
  0%|          | 0/135990 [00:00<?, ?it/s]/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'global_step': 0, 'loss_lang': 2.1875, 'segm_loss_loss_ce': 15.238483428955078, 'segm_loss_loss_bbox': 6.409528732299805, 'segm_loss_loss_giou': 3.7443103790283203, 'segm_loss_loss_mask': 3.1873254776000977, 'segm_loss_loss_dice': 9.80965518951416, 'segm_loss_loss_ce_0': 16.970569610595703, 'segm_loss_loss_bbox_0': 7.340390682220459, 'segm_loss_loss_giou_0': 3.9663851261138916, 'segm_loss_loss_boxiou_0': 1.8540560007095337, 'segm_loss_loss_mask_0': 2.6299257278442383, 'segm_loss_loss_dice_0': 9.924569129943848, 'segm_loss_loss_ce_1': 17.027713775634766, 'segm_loss_loss_bbox_1': 6.913094520568848, 'segm_loss_loss_giou_1': 3.9224424362182617, 'segm_loss_loss_boxiou_1': 1.9637547731399536, 'segm_loss_loss_mask_1': 3.1457302570343018, 'segm_loss_loss_dice_1': 9.92512321472168, 'segm_loss_loss_ce_2': 15.661853790283203, 'segm_loss_loss_bbox_2': 6.252363204956055, 'segm_loss_loss_giou_2': 3.789733409881592, 'segm_loss_loss_boxiou_2': 2.0848798751831055, 'segm_loss_loss_mask_2': 2.7451720237731934, 'segm_loss_loss_dice_2': 9.805875778198242, 'segm_loss_loss_ce_3': 14.950963973999023, 'segm_loss_loss_bbox_3': 6.423388481140137, 'segm_loss_loss_giou_3': 3.782844066619873, 'segm_loss_loss_boxiou_3': 2.432464599609375, 'segm_loss_loss_mask_3': 2.976484775543213, 'segm_loss_loss_dice_3': 9.834787368774414, 'segm_loss_loss_ce_4': 14.886210441589355, 'segm_loss_loss_bbox_4': 6.599907875061035, 'segm_loss_loss_giou_4': 3.750288724899292, 'segm_loss_loss_boxiou_4': 2.6460084915161133, 'segm_loss_loss_mask_4': 3.578193426132202, 'segm_loss_loss_dice_4': 9.813261985778809, 'segm_loss_loss_bbox_dn': 0.0, 'segm_loss_loss_giou_dn': 0.0, 'segm_loss_loss_class_dn': 0.0, 'segm_loss_loss_bbox_dn_0': 0.0, 'segm_loss_loss_giou_dn_0': 0.0, 'segm_loss_loss_class_dn_0': 0.0, 'segm_loss_loss_bbox_dn_1': 0.0, 'segm_loss_loss_giou_dn_1': 0.0, 'segm_loss_loss_class_dn_1': 0.0, 'segm_loss_loss_bbox_dn_2': 0.0, 'segm_loss_loss_giou_dn_2': 0.0, 'segm_loss_loss_class_dn_2': 0.0, 'segm_loss_loss_bbox_dn_3': 0.0, 'segm_loss_loss_giou_dn_3': 0.0, 'segm_loss_loss_class_dn_3': 0.0, 'segm_loss_loss_bbox_dn_4': 0.0, 'segm_loss_loss_giou_dn_4': 0.0, 'segm_loss_loss_class_dn_4': 0.0, 'segm_loss_loss_ce_enc': 2.2995426654815674, 'segm_loss_loss_bbox_enc': 0.8536651730537415, 'segm_loss_loss_giou_enc': 2.4502475261688232, 'segm_loss_total_loss': 251.5913848876953, 'segm_loss_iou_train': 0.0031615672633051872}
Traceback (most recent call last):
  File "/shared/nas2/jk100/partonomy_private/src/models/segllm/llava/train/train_mem_ft.py", line 17, in <module>
    train()
  File "/shared/nas2/jk100/partonomy_private/src/models/segllm/llava/train/train_eseg.py", line 1348, in train
    trainer.train()
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/transformers/trainer.py", line 2474, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/shared/nas2/jk100/partonomy_private/src/models/segllm/llava/train/llava_trainer.py", line 807, in training_step
    self.accelerator.backward(loss)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/accelerate/accelerator.py", line 2126, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 175, in backward
    self.engine.step()
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2160, in step
    self._take_model_step(lr_kwargs)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2066, in _take_model_step
    self.optimizer.step()
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1904, in step
    self._optimizer_step(i)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1810, in _optimizer_step
    self.optimizer.step()
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 391, in wrapper
    out = func(*args, **kwargs)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/adamw.py", line 177, in step
    has_complex = self._init_group(
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/adamw.py", line 128, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.49 GiB. GPU
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/segllm/llava/train/train_mem_ft.py", line 17, in <module>
[rank0]:     train()
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/segllm/llava/train/train_eseg.py", line 1348, in train
[rank0]:     trainer.train()
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/transformers/trainer.py", line 2122, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/transformers/trainer.py", line 2474, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/segllm/llava/train/llava_trainer.py", line 807, in training_step
[rank0]:     self.accelerator.backward(loss)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/accelerate/accelerator.py", line 2126, in backward
[rank0]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 175, in backward
[rank0]:     self.engine.step()
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2160, in step
[rank0]:     self._take_model_step(lr_kwargs)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2066, in _take_model_step
[rank0]:     self.optimizer.step()
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1904, in step
[rank0]:     self._optimizer_step(i)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1810, in _optimizer_step
[rank0]:     self.optimizer.step()
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
[rank0]:     return wrapped(*args, **kwargs)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 391, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
[rank0]:     ret = func(self, *args, **kwargs)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/adamw.py", line 177, in step
[rank0]:     has_complex = self._init_group(
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/adamw.py", line 128, in _init_group
[rank0]:     state["exp_avg_sq"] = torch.zeros_like(
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.49 GiB. GPU