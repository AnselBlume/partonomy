
  0%|          | 0/67994 [00:00<?, ?it/s]/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'global_step': 0, 'loss_lang': 2.1875, 'segm_loss_loss_ce': 16.473318099975586, 'segm_loss_loss_bbox': 2.7865867614746094, 'segm_loss_loss_giou': 2.6478819847106934, 'segm_loss_loss_mask': 1.4385371208190918, 'segm_loss_loss_dice': 7.281692028045654, 'segm_loss_loss_ce_0': 15.604856491088867, 'segm_loss_loss_bbox_0': 4.456837177276611, 'segm_loss_loss_giou_0': 3.2195019721984863, 'segm_loss_loss_boxiou_0': 1.223982572555542, 'segm_loss_loss_mask_0': 1.493117094039917, 'segm_loss_loss_dice_0': 7.271819591522217, 'segm_loss_loss_ce_1': 16.77682113647461, 'segm_loss_loss_bbox_1': 2.90962815284729, 'segm_loss_loss_giou_1': 2.8355584144592285, 'segm_loss_loss_boxiou_1': 1.2478750944137573, 'segm_loss_loss_mask_1': 1.3181461095809937, 'segm_loss_loss_dice_1': 6.936178207397461, 'segm_loss_loss_ce_2': 16.891878128051758, 'segm_loss_loss_bbox_2': 2.831101655960083, 'segm_loss_loss_giou_2': 2.6366336345672607, 'segm_loss_loss_boxiou_2': 1.5263514518737793, 'segm_loss_loss_mask_2': 1.4517910480499268, 'segm_loss_loss_dice_2': 6.996773719787598, 'segm_loss_loss_ce_3': 16.53441047668457, 'segm_loss_loss_bbox_3': 2.860600471496582, 'segm_loss_loss_giou_3': 2.644397735595703, 'segm_loss_loss_boxiou_3': 1.560982346534729, 'segm_loss_loss_mask_3': 1.516837477684021, 'segm_loss_loss_dice_3': 7.162938117980957, 'segm_loss_loss_ce_4': 16.502696990966797, 'segm_loss_loss_bbox_4': 2.8547964096069336, 'segm_loss_loss_giou_4': 2.6459312438964844, 'segm_loss_loss_boxiou_4': 1.629827857017517, 'segm_loss_loss_mask_4': 1.6604112386703491, 'segm_loss_loss_dice_4': 7.220188140869141, 'segm_loss_loss_bbox_dn': 0.0, 'segm_loss_loss_giou_dn': 0.0, 'segm_loss_loss_class_dn': 0.0, 'segm_loss_loss_bbox_dn_0': 0.0, 'segm_loss_loss_giou_dn_0': 0.0, 'segm_loss_loss_class_dn_0': 0.0, 'segm_loss_loss_bbox_dn_1': 0.0, 'segm_loss_loss_giou_dn_1': 0.0, 'segm_loss_loss_class_dn_1': 0.0, 'segm_loss_loss_bbox_dn_2': 0.0, 'segm_loss_loss_giou_dn_2': 0.0, 'segm_loss_loss_class_dn_2': 0.0, 'segm_loss_loss_bbox_dn_3': 0.0, 'segm_loss_loss_giou_dn_3': 0.0, 'segm_loss_loss_class_dn_3': 0.0, 'segm_loss_loss_bbox_dn_4': 0.0, 'segm_loss_loss_giou_dn_4': 0.0, 'segm_loss_loss_class_dn_4': 0.0, 'segm_loss_loss_ce_enc': 2.2208430767059326, 'segm_loss_loss_bbox_enc': 0.6945912837982178, 'segm_loss_loss_giou_enc': 1.8290281295776367, 'segm_loss_total_loss': 197.79550170898438, 'segm_loss_iou_train': 0.022747693583369255}
Traceback (most recent call last):
  File "/shared/nas2/jk100/partonomy_private/src/models/segllm/llava/train/train_mem_ft.py", line 17, in <module>
    train()
  File "/shared/nas2/jk100/partonomy_private/src/models/segllm/llava/train/train_eseg.py", line 1346, in train
    trainer.train()
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/transformers/trainer.py", line 2474, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/shared/nas2/jk100/partonomy_private/src/models/segllm/llava/train/llava_trainer.py", line 807, in training_step
    self.accelerator.backward(loss)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/accelerate/accelerator.py", line 2126, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 175, in backward
    self.engine.step()
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2160, in step
    self._take_model_step(lr_kwargs)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2066, in _take_model_step
    self.optimizer.step()
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1904, in step
    self._optimizer_step(i)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1810, in _optimizer_step
    self.optimizer.step()
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 391, in wrapper
    out = func(*args, **kwargs)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/adamw.py", line 177, in step
    has_complex = self._init_group(
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/adamw.py", line 128, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.49 GiB. GPU
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/segllm/llava/train/train_mem_ft.py", line 17, in <module>
[rank0]:     train()
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/segllm/llava/train/train_eseg.py", line 1346, in train
[rank0]:     trainer.train()
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/transformers/trainer.py", line 2122, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/transformers/trainer.py", line 2474, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/segllm/llava/train/llava_trainer.py", line 807, in training_step
[rank0]:     self.accelerator.backward(loss)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/accelerate/accelerator.py", line 2126, in backward
[rank0]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 175, in backward
[rank0]:     self.engine.step()
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2160, in step
[rank0]:     self._take_model_step(lr_kwargs)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2066, in _take_model_step
[rank0]:     self.optimizer.step()
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1904, in step
[rank0]:     self._optimizer_step(i)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1810, in _optimizer_step
[rank0]:     self.optimizer.step()
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
[rank0]:     return wrapped(*args, **kwargs)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 391, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
[rank0]:     ret = func(self, *args, **kwargs)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/adamw.py", line 177, in step
[rank0]:     has_complex = self._init_group(
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/adamw.py", line 128, in _init_group
[rank0]:     state["exp_avg_sq"] = torch.zeros_like(
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.49 GiB. GPU