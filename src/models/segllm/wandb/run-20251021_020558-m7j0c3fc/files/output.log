
  0%|          | 0/135990 [00:00<?, ?it/s]/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'global_step': 0, 'loss_lang': 2.1875, 'segm_loss_loss_ce': 14.056093215942383, 'segm_loss_loss_bbox': 5.218033790588379, 'segm_loss_loss_giou': 3.386307716369629, 'segm_loss_loss_mask': 3.0983729362487793, 'segm_loss_loss_dice': 9.71377182006836, 'segm_loss_loss_ce_0': 15.599135398864746, 'segm_loss_loss_bbox_0': 7.042691707611084, 'segm_loss_loss_giou_0': 3.930558681488037, 'segm_loss_loss_boxiou_0': 1.6975314617156982, 'segm_loss_loss_mask_0': 2.82089900970459, 'segm_loss_loss_dice_0': 9.892467498779297, 'segm_loss_loss_ce_1': 16.59603500366211, 'segm_loss_loss_bbox_1': 4.871264934539795, 'segm_loss_loss_giou_1': 3.5638065338134766, 'segm_loss_loss_boxiou_1': 1.6113189458847046, 'segm_loss_loss_mask_1': 2.1961565017700195, 'segm_loss_loss_dice_1': 9.55980110168457, 'segm_loss_loss_ce_2': 14.727685928344727, 'segm_loss_loss_bbox_2': 5.240553855895996, 'segm_loss_loss_giou_2': 3.417583465576172, 'segm_loss_loss_boxiou_2': 2.09152889251709, 'segm_loss_loss_mask_2': 2.9172987937927246, 'segm_loss_loss_dice_2': 9.686090469360352, 'segm_loss_loss_ce_3': 14.992234230041504, 'segm_loss_loss_bbox_3': 5.028469085693359, 'segm_loss_loss_giou_3': 3.396475076675415, 'segm_loss_loss_boxiou_3': 1.9983642101287842, 'segm_loss_loss_mask_3': 2.5932726860046387, 'segm_loss_loss_dice_3': 9.676131248474121, 'segm_loss_loss_ce_4': 13.820223808288574, 'segm_loss_loss_bbox_4': 5.56339168548584, 'segm_loss_loss_giou_4': 3.385199546813965, 'segm_loss_loss_boxiou_4': 2.5725810527801514, 'segm_loss_loss_mask_4': 3.650377035140991, 'segm_loss_loss_dice_4': 9.68454360961914, 'segm_loss_loss_bbox_dn': 0.0, 'segm_loss_loss_giou_dn': 0.0, 'segm_loss_loss_class_dn': 0.0, 'segm_loss_loss_bbox_dn_0': 0.0, 'segm_loss_loss_giou_dn_0': 0.0, 'segm_loss_loss_class_dn_0': 0.0, 'segm_loss_loss_bbox_dn_1': 0.0, 'segm_loss_loss_giou_dn_1': 0.0, 'segm_loss_loss_class_dn_1': 0.0, 'segm_loss_loss_bbox_dn_2': 0.0, 'segm_loss_loss_giou_dn_2': 0.0, 'segm_loss_loss_class_dn_2': 0.0, 'segm_loss_loss_bbox_dn_3': 0.0, 'segm_loss_loss_giou_dn_3': 0.0, 'segm_loss_loss_class_dn_3': 0.0, 'segm_loss_loss_bbox_dn_4': 0.0, 'segm_loss_loss_giou_dn_4': 0.0, 'segm_loss_loss_class_dn_4': 0.0, 'segm_loss_loss_ce_enc': 2.911806583404541, 'segm_loss_loss_bbox_enc': 0.6826440095901489, 'segm_loss_loss_giou_enc': 2.0256659984588623, 'segm_loss_total_loss': 234.91651916503906, 'segm_loss_iou_train': 0.0034462015610188246}
Traceback (most recent call last):
  File "/shared/nas2/jk100/partonomy_private/src/models/segllm/llava/train/train_mem_ft.py", line 17, in <module>
    train()
  File "/shared/nas2/jk100/partonomy_private/src/models/segllm/llava/train/train_eseg.py", line 1346, in train
    trainer.train()
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/transformers/trainer.py", line 2474, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/shared/nas2/jk100/partonomy_private/src/models/segllm/llava/train/llava_trainer.py", line 807, in training_step
    self.accelerator.backward(loss)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/accelerate/accelerator.py", line 2126, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 175, in backward
    self.engine.step()
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2160, in step
    self._take_model_step(lr_kwargs)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2066, in _take_model_step
    self.optimizer.step()
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1904, in step
    self._optimizer_step(i)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1810, in _optimizer_step
    self.optimizer.step()
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 391, in wrapper
    out = func(*args, **kwargs)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/adamw.py", line 177, in step
    has_complex = self._init_group(
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/adamw.py", line 128, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.49 GiB. GPU
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/segllm/llava/train/train_mem_ft.py", line 17, in <module>
[rank0]:     train()
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/segllm/llava/train/train_eseg.py", line 1346, in train
[rank0]:     trainer.train()
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/transformers/trainer.py", line 2122, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/transformers/trainer.py", line 2474, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/segllm/llava/train/llava_trainer.py", line 807, in training_step
[rank0]:     self.accelerator.backward(loss)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/accelerate/accelerator.py", line 2126, in backward
[rank0]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 175, in backward
[rank0]:     self.engine.step()
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2160, in step
[rank0]:     self._take_model_step(lr_kwargs)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2066, in _take_model_step
[rank0]:     self.optimizer.step()
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1904, in step
[rank0]:     self._optimizer_step(i)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1810, in _optimizer_step
[rank0]:     self.optimizer.step()
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
[rank0]:     return wrapped(*args, **kwargs)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 391, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
[rank0]:     ret = func(self, *args, **kwargs)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/adamw.py", line 177, in step
[rank0]:     has_complex = self._init_group(
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/adamw.py", line 128, in _init_group
[rank0]:     state["exp_avg_sq"] = torch.zeros_like(
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.49 GiB. GPU