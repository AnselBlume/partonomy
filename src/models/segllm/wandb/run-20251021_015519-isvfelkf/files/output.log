
  0%|          | 0/33996 [00:00<?, ?it/s]/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'global_step': 0, 'loss_lang': 2.484375, 'segm_loss_loss_ce': 16.543338775634766, 'segm_loss_loss_bbox': 5.308193206787109, 'segm_loss_loss_giou': 3.479041576385498, 'segm_loss_loss_mask': 3.3874545097351074, 'segm_loss_loss_dice': 8.888550758361816, 'segm_loss_loss_ce_0': 17.772384643554688, 'segm_loss_loss_bbox_0': 5.4442925453186035, 'segm_loss_loss_giou_0': 3.7268638610839844, 'segm_loss_loss_boxiou_0': 1.9945791959762573, 'segm_loss_loss_mask_0': 3.0805397033691406, 'segm_loss_loss_dice_0': 9.11640739440918, 'segm_loss_loss_ce_1': 17.95294952392578, 'segm_loss_loss_bbox_1': 5.3246355056762695, 'segm_loss_loss_giou_1': 3.5300683975219727, 'segm_loss_loss_boxiou_1': 2.170379877090454, 'segm_loss_loss_mask_1': 3.2787463665008545, 'segm_loss_loss_dice_1': 9.277148246765137, 'segm_loss_loss_ce_2': 16.863426208496094, 'segm_loss_loss_bbox_2': 4.827014923095703, 'segm_loss_loss_giou_2': 3.5017476081848145, 'segm_loss_loss_boxiou_2': 2.1863484382629395, 'segm_loss_loss_mask_2': 3.0860466957092285, 'segm_loss_loss_dice_2': 9.355585098266602, 'segm_loss_loss_ce_3': 16.570859909057617, 'segm_loss_loss_bbox_3': 4.581938743591309, 'segm_loss_loss_giou_3': 3.482346773147583, 'segm_loss_loss_boxiou_3': 2.1275219917297363, 'segm_loss_loss_mask_3': 2.831561326980591, 'segm_loss_loss_dice_3': 9.020502090454102, 'segm_loss_loss_ce_4': 16.141048431396484, 'segm_loss_loss_bbox_4': 5.303528785705566, 'segm_loss_loss_giou_4': 3.488938093185425, 'segm_loss_loss_boxiou_4': 2.3750758171081543, 'segm_loss_loss_mask_4': 3.414947509765625, 'segm_loss_loss_dice_4': 8.97676944732666, 'segm_loss_loss_bbox_dn': 0.0, 'segm_loss_loss_giou_dn': 0.0, 'segm_loss_loss_class_dn': 0.0, 'segm_loss_loss_bbox_dn_0': 0.0, 'segm_loss_loss_giou_dn_0': 0.0, 'segm_loss_loss_class_dn_0': 0.0, 'segm_loss_loss_bbox_dn_1': 0.0, 'segm_loss_loss_giou_dn_1': 0.0, 'segm_loss_loss_class_dn_1': 0.0, 'segm_loss_loss_bbox_dn_2': 0.0, 'segm_loss_loss_giou_dn_2': 0.0, 'segm_loss_loss_class_dn_2': 0.0, 'segm_loss_loss_bbox_dn_3': 0.0, 'segm_loss_loss_giou_dn_3': 0.0, 'segm_loss_loss_class_dn_3': 0.0, 'segm_loss_loss_bbox_dn_4': 0.0, 'segm_loss_loss_giou_dn_4': 0.0, 'segm_loss_loss_class_dn_4': 0.0, 'segm_loss_loss_ce_enc': 2.3741989135742188, 'segm_loss_loss_bbox_enc': 1.338163137435913, 'segm_loss_loss_giou_enc': 1.6813730001449585, 'segm_loss_total_loss': 243.8046875, 'segm_loss_iou_train': 0.005727340932935476}
Traceback (most recent call last):
  File "/shared/nas2/jk100/partonomy_private/src/models/segllm/llava/train/train_mem_ft.py", line 17, in <module>
    train()
  File "/shared/nas2/jk100/partonomy_private/src/models/segllm/llava/train/train_eseg.py", line 1346, in train
    trainer.train()
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/transformers/trainer.py", line 2474, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/shared/nas2/jk100/partonomy_private/src/models/segllm/llava/train/llava_trainer.py", line 807, in training_step
    self.accelerator.backward(loss)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/accelerate/accelerator.py", line 2126, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 175, in backward
    self.engine.step()
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2160, in step
    self._take_model_step(lr_kwargs)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2066, in _take_model_step
    self.optimizer.step()
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1904, in step
    self._optimizer_step(i)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1810, in _optimizer_step
    self.optimizer.step()
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 391, in wrapper
    out = func(*args, **kwargs)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/adamw.py", line 177, in step
    has_complex = self._init_group(
  File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/adamw.py", line 128, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.49 GiB. GPU
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/segllm/llava/train/train_mem_ft.py", line 17, in <module>
[rank0]:     train()
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/segllm/llava/train/train_eseg.py", line 1346, in train
[rank0]:     trainer.train()
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/transformers/trainer.py", line 2122, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/transformers/trainer.py", line 2474, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/segllm/llava/train/llava_trainer.py", line 807, in training_step
[rank0]:     self.accelerator.backward(loss)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/accelerate/accelerator.py", line 2126, in backward
[rank0]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 175, in backward
[rank0]:     self.engine.step()
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2160, in step
[rank0]:     self._take_model_step(lr_kwargs)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2066, in _take_model_step
[rank0]:     self.optimizer.step()
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1904, in step
[rank0]:     self._optimizer_step(i)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1810, in _optimizer_step
[rank0]:     self.optimizer.step()
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
[rank0]:     return wrapped(*args, **kwargs)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 391, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
[rank0]:     ret = func(self, *args, **kwargs)
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/adamw.py", line 177, in step
[rank0]:     has_complex = self._init_group(
[rank0]:   File "/shared/nas2/blume5/lib/conda/miniconda3/envs/segllm/lib/python3.10/site-packages/torch/optim/adamw.py", line 128, in _init_group
[rank0]:     state["exp_avg_sq"] = torch.zeros_like(
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.49 GiB. GPU