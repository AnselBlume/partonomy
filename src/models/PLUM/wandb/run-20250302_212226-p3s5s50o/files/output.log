
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565



Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:22<00:00,  7.59s/it]
trainable params: 6,553,600 || all params: 14,025,701,683 || trainable%: 0.0467256480147682
n:  base_model.model.model.embed_tokens.weight p.shape:  torch.Size([32002, 5120])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_token.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.mask_decoder.mask_tokens.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.weight p.shape:  torch.Size([256, 64, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.weight p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.weight p.shape:  torch.Size([64, 32, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.bias p.shape:  torch.Size([4])
n:  base_model.model.model.text_hidden_fcs.0.0.weight p.shape:  torch.Size([5120, 5120])
n:  base_model.model.model.text_hidden_fcs.0.0.bias p.shape:  torch.Size([5120])
n:  base_model.model.model.text_hidden_fcs.0.2.weight p.shape:  torch.Size([256, 5120])
n:  base_model.model.model.text_hidden_fcs.0.2.bias p.shape:  torch.Size([256])
n:  base_model.model.lm_head.weight p.shape:  torch.Size([32002, 5120])
>> (PLUM.py) Initializing teacher LLM...
>> (PLUM.py) Teacher LLM initialized.
ade20k:  20210
cocostuff:  118287
loading annotations into memory...
Done (t=0.56s)
creating index...
index created!
pascal_part:  4366
loading annotations into memory...
Done (t=9.07s)
creating index...
index created!
paco_lvis:  45790
mapillary:  18000
loading dataset refclef into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refclef/refs(unc).p
creating index...
index created.
DONE (t=3.34s)
dataset refclef (refs unc) (train split) has 17978 images and 99523 annotations.
loading dataset refcoco into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco/refs(unc).p
creating index...
index created.
DONE (t=5.48s)
dataset refcoco (refs unc) (train split) has 16994 images and 196771 annotations.
loading dataset refcoco+ into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco+/refs(unc).p
creating index...
index created.
DONE (t=5.75s)
dataset refcoco+ (refs unc) (train split) has 16992 images and 196737 annotations.
loading dataset refcocog into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcocog/refs(umd).p
creating index...
index created.
DONE (t=5.94s)
dataset refcocog (refs umd) (train split) has 21899 images and 208960 annotations.
vqa_data:  157712
number of reason_seg samples:  239
len(self.img_to_explanation):  239
Training with 5000 examples and validating with 200 examples.
[2025-03-02 21:24:13,652] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5, git-hash=unknown, git-branch=unknown
[2025-03-02 21:24:13,652] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2025-03-02 21:24:13,652] [INFO] [comm.py:594:init_distributed] cdb=None
[2025-03-02 21:24:13,652] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Using /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module fused_adam...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/tensor/python_tensor.cpp:78.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
[2025-03-02 21:24:34,600] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Time to load fused_adam op: 0.4725930690765381 seconds
[2025-03-02 21:24:35,309] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-03-02 21:24:36,031] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-03-02 21:24:36,031] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-03-02 21:24:36,031] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-03-02 21:24:36,031] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500000000
[2025-03-02 21:24:36,031] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500000000
[2025-03-02 21:24:36,031] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2025-03-02 21:24:36,031] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(365842916, False)]
[2025-03-02 21:24:39,797] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2025-03-02 21:24:39,798] [INFO] [utils.py:786:see_memory_usage] MA 53.71 GB         Max_MA 54.39 GB         CA 54.6 GB         Max_CA 55 GB
[2025-03-02 21:24:39,798] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 84.56 GB, percent = 8.4%
[2025-03-02 21:24:42,954] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2025-03-02 21:24:42,955] [INFO] [utils.py:786:see_memory_usage] MA 56.44 GB         Max_MA 57.8 GB         CA 58.69 GB         Max_CA 59 GB
[2025-03-02 21:24:42,955] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 84.53 GB, percent = 8.4%
[2025-03-02 21:24:42,955] [INFO] [stage_1_and_2.py:488:__init__] optimizer state initialized
[2025-03-02 21:24:45,940] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2025-03-02 21:24:45,941] [INFO] [utils.py:786:see_memory_usage] MA 56.44 GB         Max_MA 56.44 GB         CA 58.69 GB         Max_CA 59 GB
[2025-03-02 21:24:45,941] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 84.53 GB, percent = 8.4%
[2025-03-02 21:24:45,947] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-03-02 21:24:45,947] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2025-03-02 21:24:45,947] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7f60509b9c00>
[2025-03-02 21:24:45,947] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
[2025-03-02 21:24:45,954] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2025-03-02 21:24:45,955] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-03-02 21:24:45,955] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-03-02 21:24:45,955] [INFO] [config.py:964:print]   amp_enabled .................. False
[2025-03-02 21:24:45,955] [INFO] [config.py:964:print]   amp_params ................... False
[2025-03-02 21:24:45,955] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-02 21:24:45,955] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2025-03-02 21:24:45,955] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2025-03-02 21:24:45,955] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2025-03-02 21:24:45,956] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2025-03-02 21:24:45,956] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f4add804a90>
[2025-03-02 21:24:45,956] [INFO] [config.py:964:print]   communication_data_type ...... None
[2025-03-02 21:24:45,956] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-02 21:24:45,956] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2025-03-02 21:24:45,956] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2025-03-02 21:24:45,956] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-02 21:24:45,956] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2025-03-02 21:24:45,956] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2025-03-02 21:24:45,956] [INFO] [config.py:964:print]   disable_allgather ............ False
[2025-03-02 21:24:45,956] [INFO] [config.py:964:print]   dump_state ................... False
[2025-03-02 21:24:45,956] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2025-03-02 21:24:45,956] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2025-03-02 21:24:45,956] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-02 21:24:45,956] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-02 21:24:45,956] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2025-03-02 21:24:45,956] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2025-03-02 21:24:45,956] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2025-03-02 21:24:45,957] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2025-03-02 21:24:45,957] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2025-03-02 21:24:45,957] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2025-03-02 21:24:45,957] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-03-02 21:24:45,957] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2025-03-02 21:24:45,957] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2025-03-02 21:24:45,957] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2025-03-02 21:24:45,957] [INFO] [config.py:964:print]   global_rank .................. 0
[2025-03-02 21:24:45,957] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2025-03-02 21:24:45,957] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 10
[2025-03-02 21:24:45,957] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0
[2025-03-02 21:24:45,957] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2025-03-02 21:24:45,957] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-02 21:24:45,957] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2025-03-02 21:24:45,957] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2025-03-02 21:24:45,957] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2025-03-02 21:24:45,957] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2025-03-02 21:24:45,958] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2025-03-02 21:24:45,958] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2025-03-02 21:24:45,958] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-03-02 21:24:45,958] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-03-02 21:24:45,958] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2025-03-02 21:24:45,958] [INFO] [config.py:964:print]   optimizer_name ............... adamw
[2025-03-02 21:24:45,958] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.0003, 'weight_decay': 0.0, 'betas': (0.9, 0.95)}
[2025-03-02 21:24:45,958] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2025-03-02 21:24:45,958] [INFO] [config.py:964:print]   pld_enabled .................. False
[2025-03-02 21:24:45,958] [INFO] [config.py:964:print]   pld_params ................... False
[2025-03-02 21:24:45,958] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2025-03-02 21:24:45,958] [INFO] [config.py:964:print]   scheduler_name ............... WarmupDecayLR
[2025-03-02 21:24:45,958] [INFO] [config.py:964:print]   scheduler_params ............. {'total_num_steps': 5000, 'warmup_min_lr': 0, 'warmup_max_lr': 0.0003, 'warmup_num_steps': 100, 'warmup_type': 'linear'}
[2025-03-02 21:24:45,958] [INFO] [config.py:964:print]   sparse_attention ............. None
[2025-03-02 21:24:45,958] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2025-03-02 21:24:45,958] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2025-03-02 21:24:45,958] [INFO] [config.py:964:print]   train_batch_size ............. 10
[2025-03-02 21:24:45,958] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2025-03-02 21:24:45,958] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2025-03-02 21:24:45,959] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2025-03-02 21:24:45,959] [INFO] [config.py:964:print]   world_size ................... 1
[2025-03-02 21:24:45,959] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2025-03-02 21:24:45,959] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2025-03-02 21:24:45,959] [INFO] [config.py:964:print]   zero_enabled ................. True
[2025-03-02 21:24:45,959] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-02 21:24:45,959] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2025-03-02 21:24:45,959] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1,
    "gradient_accumulation_steps": 10,
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 0.0003,
            "weight_decay": 0.0,
            "betas": [0.9, 0.95]
        }
    },
    "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "total_num_steps": 5.000000e+03,
            "warmup_min_lr": 0,
            "warmup_max_lr": 0.0003,
            "warmup_num_steps": 100,
            "warmup_type": "linear"
        }
    },
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "gradient_clipping": 1.0,
    "zero_optimization": {
        "stage": 2,
        "contiguous_gradients": true,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 5.000000e+08,
        "allgather_bucket_size": 5.000000e+08
    }
}
(train) >> AFTER DEEPSPEED
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)
  return F.conv2d(input, weight, bias, self.stride,
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([1, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([2, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([5, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([9, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([3, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([7, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][  1/500]	Time  9.045 ( 9.045)	Loss 6.7255 (4.2783)	CeLoss 3.5312 (2.6480)	SegCLSLoss 1.2656 (0.7578)	KLLoss 0.1211 (0.0503)	MaskLoss 1.4742 (0.7966)	MaskBCELoss 0.6263 (0.2492)	MaskDICELoss 0.8478 (0.5474)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([13, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([20, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([15, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([4, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([8, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][  2/500]	Time  6.989 ( 6.989)	Loss 4.5334 (3.8571)	CeLoss 1.6406 (2.0129)	SegCLSLoss 1.3516 (0.8945)	KLLoss 0.2168 (0.1026)	MaskLoss 1.1687 (0.8382)	MaskBCELoss 0.1772 (0.1497)	MaskDICELoss 0.9915 (0.6885)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([12, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([18, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([11, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([6, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][  3/500]	Time  7.058 ( 7.058)	Loss 1.0391 (4.2639)	CeLoss 1.0391 (2.1461)	SegCLSLoss 0.0000 (0.9008)	KLLoss 0.0000 (0.0842)	MaskLoss 0.0000 (1.0121)	MaskBCELoss 0.0000 (0.3167)	MaskDICELoss 0.0000 (0.6954)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([10, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][  4/500]	Time  8.018 ( 8.018)	Loss 5.3778 (5.2809)	CeLoss 2.6094 (2.4250)	SegCLSLoss 1.2422 (1.1445)	KLLoss 0.0718 (0.1235)	MaskLoss 1.4293 (1.3419)	MaskBCELoss 0.4329 (0.4468)	MaskDICELoss 0.9964 (0.8952)
Epoch: [0][  5/500]	Time  8.181 ( 8.181)	Loss 5.7006 (4.6893)	CeLoss 1.7812 (2.4020)	SegCLSLoss 1.3594 (1.0219)	KLLoss 0.1924 (0.1012)	MaskLoss 1.7269 (1.0829)	MaskBCELoss 0.7393 (0.2872)	MaskDICELoss 0.9876 (0.7958)
Epoch: [0][  6/500]	Time  8.061 ( 8.061)	Loss 1.6562 (5.3951)	CeLoss 1.6562 (2.5734)	SegCLSLoss 0.0000 (1.1656)	KLLoss 0.0000 (0.1260)	MaskLoss 0.0000 (1.3115)	MaskBCELoss 0.0000 (0.4273)	MaskDICELoss 0.0000 (0.8842)
Epoch: [0][  7/500]	Time  7.048 ( 7.048)	Loss 5.6116 (3.9718)	CeLoss 2.8438 (2.2227)	SegCLSLoss 1.2344 (0.7602)	KLLoss 0.1309 (0.0614)	MaskLoss 1.2925 (0.8497)	MaskBCELoss 0.3191 (0.2745)	MaskDICELoss 0.9734 (0.5752)
Epoch: [0][  8/500]	Time  7.270 ( 7.270)	Loss 4.9818 (3.7405)	CeLoss 2.6562 (2.1453)	SegCLSLoss 1.2578 (0.7766)	KLLoss 0.0208 (0.0604)	MaskLoss 1.2716 (0.7692)	MaskBCELoss 0.3508 (0.1994)	MaskDICELoss 0.9208 (0.5697)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([14, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][  9/500]	Time  7.549 ( 7.549)	Loss 3.9568 (5.0807)	CeLoss 1.6016 (2.2648)	SegCLSLoss 1.2656 (1.2844)	KLLoss 0.0918 (0.1327)	MaskLoss 1.1413 (1.2620)	MaskBCELoss 0.2217 (0.3796)	MaskDICELoss 0.9195 (0.8824)
[2025-03-02 21:26:02,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[5.1e-05], mom=[(0.9, 0.95)]
[2025-03-02 21:26:02,386] [INFO] [timer.py:215:stop] epoch=0/micro_step=100/global_step=10, RunningAvgSamplesPerSec=1.3313363465887083, CurrSamplesPerSec=1.4446394294604488, MemAllocated=57.49GB, MaxMemAllocated=61.89GB
Epoch: [0][ 10/500]	Time  6.924 ( 6.924)	Loss 5.3215 (3.6361)	CeLoss 2.6094 (1.8996)	SegCLSLoss 1.2266 (0.7789)	KLLoss 0.1240 (0.0843)	MaskLoss 1.2639 (0.7728)	MaskBCELoss 0.3389 (0.2340)	MaskDICELoss 0.9249 (0.5389)
Epoch: [0][ 11/500]	Time  8.296 ( 8.296)	Loss 5.6140 (5.3469)	CeLoss 2.4375 (2.5492)	SegCLSLoss 1.3516 (1.2547)	KLLoss 0.1387 (0.0979)	MaskLoss 1.3494 (1.3413)	MaskBCELoss 0.6083 (0.4399)	MaskDICELoss 0.7411 (0.9014)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([23, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][ 12/500]	Time  6.888 ( 6.888)	Loss 4.5722 (3.7768)	CeLoss 1.2266 (1.8754)	SegCLSLoss 1.3359 (0.9008)	KLLoss 0.2324 (0.0741)	MaskLoss 1.3099 (0.8862)	MaskBCELoss 0.4498 (0.2691)	MaskDICELoss 0.8601 (0.6171)
Epoch: [0][ 13/500]	Time  6.871 ( 6.871)	Loss 0.6055 (3.2333)	CeLoss 0.6055 (1.6348)	SegCLSLoss 0.0000 (0.7672)	KLLoss 0.0000 (0.0824)	MaskLoss 0.0000 (0.7255)	MaskBCELoss 0.0000 (0.1598)	MaskDICELoss 0.0000 (0.5656)
Epoch: [0][ 14/500]	Time  7.628 ( 7.628)	Loss 4.8590 (4.6773)	CeLoss 2.0938 (1.7570)	SegCLSLoss 1.2734 (1.3039)	KLLoss 0.1074 (0.1439)	MaskLoss 1.3369 (1.3020)	MaskBCELoss 0.3658 (0.3940)	MaskDICELoss 0.9711 (0.9081)
Epoch: [0][ 15/500]	Time  7.297 ( 7.297)	Loss 1.2188 (2.6277)	CeLoss 1.2188 (1.3441)	SegCLSLoss 0.0000 (0.6203)	KLLoss 0.0000 (0.0609)	MaskLoss 0.0000 (0.6094)	MaskBCELoss 0.0000 (0.1218)	MaskDICELoss 0.0000 (0.4876)
Epoch: [0][ 16/500]	Time  7.747 ( 7.747)	Loss 4.5563 (3.8225)	CeLoss 1.7891 (1.5605)	SegCLSLoss 1.3359 (1.0172)	KLLoss 0.0830 (0.0704)	MaskLoss 1.3243 (1.1066)	MaskBCELoss 0.4507 (0.3671)	MaskDICELoss 0.8736 (0.7395)
Epoch: [0][ 17/500]	Time  8.477 ( 8.477)	Loss 1.1406 (3.5213)	CeLoss 1.1406 (1.4977)	SegCLSLoss 0.0000 (0.9977)	KLLoss 0.0000 (0.0826)	MaskLoss 0.0000 (0.9733)	MaskBCELoss 0.0000 (0.2199)	MaskDICELoss 0.0000 (0.7535)
Epoch: [0][ 18/500]	Time  6.012 ( 6.012)	Loss 0.9688 (2.5822)	CeLoss 0.9688 (1.2445)	SegCLSLoss 0.0000 (0.6422)	KLLoss 0.0000 (0.0544)	MaskLoss 0.0000 (0.6266)	MaskBCELoss 0.0000 (0.1720)	MaskDICELoss 0.0000 (0.4546)
Epoch: [0][ 19/500]	Time  5.866 ( 5.866)	Loss 4.1235 (2.2975)	CeLoss 1.6016 (1.2488)	SegCLSLoss 1.2266 (0.4969)	KLLoss 0.1611 (0.0533)	MaskLoss 1.1307 (0.4784)	MaskBCELoss 0.1335 (0.1094)	MaskDICELoss 0.9972 (0.3690)
[2025-03-02 21:27:15,025] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[0.00011099999999999999], mom=[(0.9, 0.95)]
[2025-03-02 21:27:15,031] [INFO] [timer.py:215:stop] epoch=0/micro_step=200/global_step=20, RunningAvgSamplesPerSec=1.356233169660628, CurrSamplesPerSec=1.3224290763231255, MemAllocated=56.71GB, MaxMemAllocated=61.89GB
Epoch: [0][ 20/500]	Time  7.564 ( 7.564)	Loss 1.1406 (2.8894)	CeLoss 1.1406 (1.0867)	SegCLSLoss 0.0000 (0.8734)	KLLoss 0.0000 (0.0955)	MaskLoss 0.0000 (0.8235)	MaskBCELoss 0.0000 (0.1589)	MaskDICELoss 0.0000 (0.6645)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([19, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([26, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][ 21/500]	Time  6.924 ( 6.924)	Loss 3.6358 (2.8486)	CeLoss 1.0391 (1.0215)	SegCLSLoss 1.2031 (0.8633)	KLLoss 0.1807 (0.1129)	MaskLoss 1.1360 (0.8086)	MaskBCELoss 0.1404 (0.1353)	MaskDICELoss 0.9956 (0.6734)
Epoch: [0][ 22/500]	Time  5.845 ( 5.845)	Loss 0.9648 (2.2661)	CeLoss 0.9648 (1.0160)	SegCLSLoss 0.0000 (0.6133)	KLLoss 0.0000 (0.0423)	MaskLoss 0.0000 (0.6138)	MaskBCELoss 0.0000 (0.1609)	MaskDICELoss 0.0000 (0.4528)
Epoch: [0][ 23/500]	Time  7.237 ( 7.237)	Loss 3.8799 (2.8777)	CeLoss 0.9336 (1.1094)	SegCLSLoss 1.1797 (0.8359)	KLLoss 0.0168 (0.0764)	MaskLoss 1.5866 (0.8510)	MaskBCELoss 0.6996 (0.1924)	MaskDICELoss 0.8871 (0.6586)
Epoch: [0][ 24/500]	Time  9.399 ( 9.399)	Loss 3.5828 (3.0430)	CeLoss 0.7109 (0.9062)	SegCLSLoss 1.1875 (1.0719)	KLLoss 0.1240 (0.1042)	MaskLoss 1.3508 (1.0285)	MaskBCELoss 0.4352 (0.1551)	MaskDICELoss 0.9156 (0.8733)
Epoch: [0][ 25/500]	Time  7.757 ( 7.757)	Loss 1.6641 (3.2511)	CeLoss 1.6641 (0.8773)	SegCLSLoss 0.0000 (1.0602)	KLLoss 0.0000 (0.1307)	MaskLoss 0.0000 (1.0920)	MaskBCELoss 0.0000 (0.2302)	MaskDICELoss 0.0000 (0.8618)
Epoch: [0][ 26/500]	Time  6.423 ( 6.423)	Loss 5.0127 (2.7495)	CeLoss 0.8750 (0.9574)	SegCLSLoss 1.1484 (0.7125)	KLLoss 0.1377 (0.0998)	MaskLoss 1.9833 (0.8037)	MaskBCELoss 1.0294 (0.2321)	MaskDICELoss 0.9539 (0.5716)
Epoch: [0][ 27/500]	Time  7.548 ( 7.548)	Loss 0.7773 (2.8700)	CeLoss 0.7773 (0.8043)	SegCLSLoss 0.0000 (0.9094)	KLLoss 0.0000 (0.1071)	MaskLoss 0.0000 (0.9684)	MaskBCELoss 0.0000 (0.2137)	MaskDICELoss 0.0000 (0.7546)
Epoch: [0][ 28/500]	Time  7.301 ( 7.301)	Loss 4.0770 (2.8853)	CeLoss 0.5859 (0.8812)	SegCLSLoss 1.1875 (0.7945)	KLLoss 0.1699 (0.0935)	MaskLoss 1.5401 (0.9344)	MaskBCELoss 0.6775 (0.2986)	MaskDICELoss 0.8626 (0.6358)
Epoch: [0][ 29/500]	Time  6.924 ( 6.924)	Loss 3.0767 (2.6241)	CeLoss 0.6797 (0.8887)	SegCLSLoss 1.1172 (0.7797)	KLLoss 0.1348 (0.0753)	MaskLoss 1.1374 (0.8524)	MaskBCELoss 0.1659 (0.1932)	MaskDICELoss 0.9715 (0.6592)
[2025-03-02 21:28:28,260] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[0.00017099999999999998], mom=[(0.9, 0.95)]
[2025-03-02 21:28:28,265] [INFO] [timer.py:215:stop] epoch=0/micro_step=300/global_step=30, RunningAvgSamplesPerSec=1.359615712684741, CurrSamplesPerSec=1.2701130951614894, MemAllocated=57.28GB, MaxMemAllocated=61.93GB
Epoch: [0][ 30/500]	Time  7.875 ( 7.875)	Loss 3.5991 (3.0850)	CeLoss 0.7109 (0.6857)	SegCLSLoss 1.1250 (0.9621)	KLLoss 0.2188 (0.1077)	MaskLoss 1.1786 (1.1469)	MaskBCELoss 0.2643 (0.3404)	MaskDICELoss 0.9143 (0.8065)
Epoch: [0][ 31/500]	Time  6.477 ( 6.477)	Loss 3.5747 (2.3304)	CeLoss 0.7695 (0.7500)	SegCLSLoss 1.0078 (0.6242)	KLLoss 0.1172 (0.0803)	MaskLoss 1.3590 (0.7477)	MaskBCELoss 0.4736 (0.1995)	MaskDICELoss 0.8854 (0.5482)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([17, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][ 32/500]	Time  8.312 ( 8.312)	Loss 2.6906 (3.0856)	CeLoss 0.5391 (0.6037)	SegCLSLoss 0.9297 (0.9859)	KLLoss 0.1157 (0.1403)	MaskLoss 1.1099 (1.1835)	MaskBCELoss 0.1198 (0.2451)	MaskDICELoss 0.9901 (0.9384)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([16, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][ 33/500]	Time  9.046 ( 9.046)	Loss 2.9267 (3.0324)	CeLoss 0.4727 (0.5941)	SegCLSLoss 0.9492 (0.8637)	KLLoss 0.1055 (0.1533)	MaskLoss 1.2539 (1.1129)	MaskBCELoss 0.3056 (0.2797)	MaskDICELoss 0.9483 (0.8332)
Epoch: [0][ 34/500]	Time  8.015 ( 8.015)	Loss 3.1196 (2.4645)	CeLoss 0.5742 (0.7740)	SegCLSLoss 0.8945 (0.6246)	KLLoss 0.0238 (0.0795)	MaskLoss 1.4544 (0.8623)	MaskBCELoss 0.5481 (0.1983)	MaskDICELoss 0.9063 (0.6639)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([21, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][ 35/500]	Time  7.470 ( 7.470)	Loss 2.9479 (2.8630)	CeLoss 0.4980 (0.7129)	SegCLSLoss 0.8125 (0.6766)	KLLoss 0.1582 (0.1179)	MaskLoss 1.1830 (1.0454)	MaskBCELoss 0.2258 (0.2934)	MaskDICELoss 0.9571 (0.7519)
Epoch: [0][ 36/500]	Time  7.245 ( 7.245)	Loss 2.7418 (2.3283)	CeLoss 0.4805 (0.6828)	SegCLSLoss 0.7344 (0.5293)	KLLoss 0.1924 (0.0793)	MaskLoss 1.0571 (0.8618)	MaskBCELoss 0.0675 (0.2012)	MaskDICELoss 0.9896 (0.6607)
Epoch: [0][ 37/500]	Time  8.613 ( 8.613)	Loss 2.7076 (2.2239)	CeLoss 0.5938 (0.6492)	SegCLSLoss 0.6406 (0.4836)	KLLoss 0.1426 (0.0819)	MaskLoss 1.1012 (0.8347)	MaskBCELoss 0.1221 (0.1708)	MaskDICELoss 0.9791 (0.6639)
Epoch: [0][ 38/500]	Time  7.656 ( 7.656)	Loss 0.6367 (1.9971)	CeLoss 0.6367 (0.4883)	SegCLSLoss 0.0000 (0.4336)	KLLoss 0.0000 (0.0720)	MaskLoss 0.0000 (0.8318)	MaskBCELoss 0.0000 (0.1723)	MaskDICELoss 0.0000 (0.6596)
Epoch: [0][ 39/500]	Time  8.194 ( 8.194)	Loss 2.8051 (2.3967)	CeLoss 0.3965 (0.5051)	SegCLSLoss 0.5859 (0.4445)	KLLoss 0.2373 (0.1315)	MaskLoss 1.0760 (0.9540)	MaskBCELoss 0.0885 (0.1900)	MaskDICELoss 0.9875 (0.7640)
[2025-03-02 21:29:45,676] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[0.00023099999999999998], mom=[(0.9, 0.95)]
[2025-03-02 21:29:45,681] [INFO] [timer.py:215:stop] epoch=0/micro_step=400/global_step=40, RunningAvgSamplesPerSec=1.3411333824326317, CurrSamplesPerSec=1.565753326984665, MemAllocated=57.11GB, MaxMemAllocated=62.46GB
Epoch: [0][ 40/500]	Time  6.388 ( 6.388)	Loss 2.6972 (1.7627)	CeLoss 0.4199 (0.6650)	SegCLSLoss 0.4707 (0.2318)	KLLoss 0.2090 (0.0650)	MaskLoss 1.1013 (0.5983)	MaskBCELoss 0.1038 (0.1238)	MaskDICELoss 0.9975 (0.4745)
Epoch: [0][ 41/500]	Time  9.071 ( 9.071)	Loss 0.7227 (2.3123)	CeLoss 0.7227 (0.4525)	SegCLSLoss 0.0000 (0.3127)	KLLoss 0.0000 (0.1163)	MaskLoss 0.0000 (0.9915)	MaskBCELoss 0.0000 (0.2477)	MaskDICELoss 0.0000 (0.7438)
Epoch: [0][ 42/500]	Time  8.944 ( 8.944)	Loss 2.6664 (2.2602)	CeLoss 0.2832 (0.5178)	SegCLSLoss 0.3555 (0.2502)	KLLoss 0.1738 (0.1268)	MaskLoss 1.2023 (0.8766)	MaskBCELoss 0.3079 (0.2340)	MaskDICELoss 0.8944 (0.6426)
Epoch: [0][ 43/500]	Time  8.163 ( 8.163)	Loss 3.2912 (2.0199)	CeLoss 0.4258 (0.5547)	SegCLSLoss 0.2871 (0.1744)	KLLoss 0.2090 (0.1109)	MaskLoss 1.3804 (0.7406)	MaskBCELoss 0.5046 (0.1934)	MaskDICELoss 0.8758 (0.5472)
Epoch: [0][ 44/500]	Time  7.254 ( 7.254)	Loss 0.8516 (2.1231)	CeLoss 0.8516 (0.5352)	SegCLSLoss 0.0000 (0.1759)	KLLoss 0.0000 (0.1103)	MaskLoss 0.0000 (0.8487)	MaskBCELoss 0.0000 (0.2096)	MaskDICELoss 0.0000 (0.6391)
Epoch: [0][ 45/500]	Time  6.949 ( 6.949)	Loss 1.4297 (2.1089)	CeLoss 1.4297 (0.5779)	SegCLSLoss 0.0000 (0.1543)	KLLoss 0.0000 (0.1106)	MaskLoss 0.0000 (0.8336)	MaskBCELoss 0.0000 (0.1776)	MaskDICELoss 0.0000 (0.6561)
Epoch: [0][ 46/500]	Time  8.284 ( 8.284)	Loss 2.5326 (2.4871)	CeLoss 0.3008 (0.4301)	SegCLSLoss 0.1934 (0.1904)	KLLoss 0.1465 (0.1337)	MaskLoss 1.2554 (1.1836)	MaskBCELoss 0.2928 (0.2430)	MaskDICELoss 0.9625 (0.9406)
Epoch: [0][ 47/500]	Time  7.298 ( 7.298)	Loss 1.0781 (2.6760)	CeLoss 1.0781 (0.5502)	SegCLSLoss 0.0000 (0.1574)	KLLoss 0.0000 (0.1435)	MaskLoss 0.0000 (1.1107)	MaskBCELoss 0.0000 (0.3630)	MaskDICELoss 0.0000 (0.7477)
Epoch: [0][ 48/500]	Time  8.539 ( 8.539)	Loss 2.4643 (2.3935)	CeLoss 0.3027 (0.4924)	SegCLSLoss 0.1445 (0.1416)	KLLoss 0.0354 (0.1005)	MaskLoss 1.3108 (1.0577)	MaskBCELoss 0.6379 (0.3698)	MaskDICELoss 0.6730 (0.6879)
Epoch: [0][ 49/500]	Time  6.240 ( 6.240)	Loss 2.5695 (2.3652)	CeLoss 0.3789 (0.4678)	SegCLSLoss 0.1357 (0.1442)	KLLoss 0.0381 (0.1246)	MaskLoss 1.3241 (1.0011)	MaskBCELoss 0.6439 (0.3270)	MaskDICELoss 0.6802 (0.6741)
[2025-03-02 21:31:06,362] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[0.00029099999999999997], mom=[(0.9, 0.95)]
[2025-03-02 21:31:06,367] [INFO] [timer.py:215:stop] epoch=0/micro_step=500/global_step=50, RunningAvgSamplesPerSec=1.3186315938285944, CurrSamplesPerSec=1.0059440171091145, MemAllocated=57.64GB, MaxMemAllocated=62.63GB
Epoch: [0][ 50/500]	Time  9.943 ( 9.943)	Loss 0.7695 (1.9392)	CeLoss 0.7695 (0.5700)	SegCLSLoss 0.0000 (0.0973)	KLLoss 0.0000 (0.0795)	MaskLoss 0.0000 (0.7690)	MaskBCELoss 0.0000 (0.2337)	MaskDICELoss 0.0000 (0.5353)
Epoch: [0][ 51/500]	Time  8.353 ( 8.353)	Loss 2.3812 (1.8197)	CeLoss 0.3594 (0.5416)	SegCLSLoss 0.1348 (0.0858)	KLLoss 0.1235 (0.0759)	MaskLoss 1.1841 (0.7339)	MaskBCELoss 0.2753 (0.1983)	MaskDICELoss 0.9087 (0.5356)
Epoch: [0][ 52/500]	Time  6.588 ( 6.588)	Loss 1.9039 (2.1172)	CeLoss 0.2988 (0.5197)	SegCLSLoss 0.1348 (0.1028)	KLLoss 0.0781 (0.0668)	MaskLoss 1.1118 (0.9822)	MaskBCELoss 0.1124 (0.2963)	MaskDICELoss 0.9993 (0.6859)
Epoch: [0][ 53/500]	Time  7.682 ( 7.682)	Loss 2.2553 (1.9251)	CeLoss 0.3672 (0.5172)	SegCLSLoss 0.1426 (0.0940)	KLLoss 0.1641 (0.0926)	MaskLoss 1.0742 (0.8288)	MaskBCELoss 0.0874 (0.1620)	MaskDICELoss 0.9868 (0.6667)
Epoch: [0][ 54/500]	Time  6.781 ( 6.781)	Loss 0.3027 (1.8709)	CeLoss 0.3027 (0.6401)	SegCLSLoss 0.0000 (0.0625)	KLLoss 0.0000 (0.0457)	MaskLoss 0.0000 (0.7469)	MaskBCELoss 0.0000 (0.2700)	MaskDICELoss 0.0000 (0.4769)
Epoch: [0][ 55/500]	Time  7.114 ( 7.114)	Loss 3.0603 (1.8780)	CeLoss 0.3633 (0.7615)	SegCLSLoss 0.1069 (0.0638)	KLLoss 0.0544 (0.0635)	MaskLoss 1.6795 (0.6516)	MaskBCELoss 0.7480 (0.1787)	MaskDICELoss 0.9315 (0.4729)
Epoch: [0][ 56/500]	Time  7.156 ( 7.156)	Loss 1.9431 (2.0416)	CeLoss 0.1973 (0.5547)	SegCLSLoss 0.1260 (0.0766)	KLLoss 0.1182 (0.0625)	MaskLoss 1.0916 (0.9227)	MaskBCELoss 0.1171 (0.2757)	MaskDICELoss 0.9745 (0.6470)
Epoch: [0][ 57/500]	Time  7.566 ( 7.566)	Loss 1.1172 (2.1438)	CeLoss 1.1172 (0.4903)	SegCLSLoss 0.0000 (0.1000)	KLLoss 0.0000 (0.1255)	MaskLoss 0.0000 (0.9120)	MaskBCELoss 0.0000 (0.1889)	MaskDICELoss 0.0000 (0.7231)
Epoch: [0][ 58/500]	Time  7.023 ( 7.023)	Loss 2.5728 (2.3829)	CeLoss 0.4219 (0.4604)	SegCLSLoss 0.0942 (0.0951)	KLLoss 0.0483 (0.0818)	MaskLoss 1.3686 (1.1491)	MaskBCELoss 0.5402 (0.3978)	MaskDICELoss 0.8284 (0.7512)
Epoch: [0][ 59/500]	Time  8.292 ( 8.292)	Loss 3.1075 (2.0741)	CeLoss 0.3652 (0.5031)	SegCLSLoss 0.1738 (0.0852)	KLLoss 0.2178 (0.1032)	MaskLoss 1.2917 (0.8778)	MaskBCELoss 0.4877 (0.2382)	MaskDICELoss 0.8040 (0.6396)
[2025-03-02 21:32:20,413] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[0.00029895918367346935], mom=[(0.9, 0.95)]
[2025-03-02 21:32:20,419] [INFO] [timer.py:215:stop] epoch=0/micro_step=600/global_step=60, RunningAvgSamplesPerSec=1.324048207401637, CurrSamplesPerSec=1.3341363866323115, MemAllocated=57.58GB, MaxMemAllocated=62.63GB
Epoch: [0][ 60/500]	Time  7.498 ( 7.498)	Loss 1.9614 (1.9773)	CeLoss 0.2598 (0.6592)	SegCLSLoss 0.1021 (0.0725)	KLLoss 0.1069 (0.0626)	MaskLoss 1.0171 (0.8035)	MaskBCELoss 0.2061 (0.2276)	MaskDICELoss 0.8110 (0.5759)
Epoch: [0][ 61/500]	Time  7.169 ( 7.169)	Loss 2.6221 (1.8936)	CeLoss 0.4316 (0.5410)	SegCLSLoss 0.1289 (0.0723)	KLLoss 0.1865 (0.0780)	MaskLoss 1.1749 (0.8042)	MaskBCELoss 0.2051 (0.2003)	MaskDICELoss 0.9698 (0.6039)
Epoch: [0][ 62/500]	Time  7.889 ( 7.889)	Loss 0.6484 (2.2027)	CeLoss 0.6484 (0.4578)	SegCLSLoss 0.0000 (0.0836)	KLLoss 0.0000 (0.0964)	MaskLoss 0.0000 (1.0183)	MaskBCELoss 0.0000 (0.2993)	MaskDICELoss 0.0000 (0.7190)
Epoch: [0][ 63/500]	Time  7.237 ( 7.237)	Loss 2.9815 (1.9837)	CeLoss 0.3672 (0.5188)	SegCLSLoss 0.0830 (0.0843)	KLLoss 0.0532 (0.1040)	MaskLoss 1.6467 (0.8390)	MaskBCELoss 0.7136 (0.1680)	MaskDICELoss 0.9331 (0.6710)
Epoch: [0][ 64/500]	Time  7.696 ( 7.696)	Loss 2.8639 (2.3397)	CeLoss 0.2949 (0.3616)	SegCLSLoss 0.1602 (0.1021)	KLLoss 0.2314 (0.1170)	MaskLoss 1.2388 (1.1483)	MaskBCELoss 0.3283 (0.3105)	MaskDICELoss 0.9105 (0.8378)
Epoch: [0][ 65/500]	Time  8.191 ( 8.191)	Loss 0.7930 (1.9303)	CeLoss 0.7930 (0.4369)	SegCLSLoss 0.0000 (0.0827)	KLLoss 0.0000 (0.1121)	MaskLoss 0.0000 (0.8909)	MaskBCELoss 0.0000 (0.1129)	MaskDICELoss 0.0000 (0.7780)
Epoch: [0][ 66/500]	Time  8.281 ( 8.281)	Loss 2.1051 (2.1721)	CeLoss 0.2402 (0.2922)	SegCLSLoss 0.0894 (0.0880)	KLLoss 0.0693 (0.0876)	MaskLoss 1.0981 (1.2080)	MaskBCELoss 0.4445 (0.2770)	MaskDICELoss 0.6536 (0.9310)
Epoch: [0][ 67/500]	Time  8.938 ( 8.938)	Loss 2.0622 (2.1266)	CeLoss 0.3105 (0.3597)	SegCLSLoss 0.0791 (0.0829)	KLLoss 0.1030 (0.0895)	MaskLoss 1.1345 (1.1057)	MaskBCELoss 0.1661 (0.2620)	MaskDICELoss 0.9684 (0.8437)
Epoch: [0][ 68/500]	Time  8.022 ( 8.022)	Loss 3.0882 (2.2085)	CeLoss 0.3242 (0.5135)	SegCLSLoss 0.1167 (0.0897)	KLLoss 0.2080 (0.1319)	MaskLoss 1.3561 (0.9314)	MaskBCELoss 0.5134 (0.1903)	MaskDICELoss 0.8427 (0.7410)
Epoch: [0][ 69/500]	Time  6.679 ( 6.679)	Loss 2.1493 (2.0045)	CeLoss 0.2305 (0.3498)	SegCLSLoss 0.1104 (0.0778)	KLLoss 0.1670 (0.0963)	MaskLoss 1.0977 (0.9312)	MaskBCELoss 0.0984 (0.2993)	MaskDICELoss 0.9993 (0.6320)
[2025-03-02 21:33:39,043] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[0.000297734693877551], mom=[(0.9, 0.95)]
[2025-03-02 21:33:39,048] [INFO] [timer.py:215:stop] epoch=0/micro_step=700/global_step=70, RunningAvgSamplesPerSec=1.316132353577674, CurrSamplesPerSec=1.173085145824556, MemAllocated=57.27GB, MaxMemAllocated=62.63GB
Epoch: [0][ 70/500]	Time  8.526 ( 8.526)	Loss 2.5215 (2.1578)	CeLoss 0.2051 (0.2910)	SegCLSLoss 0.1050 (0.0897)	KLLoss 0.1621 (0.1203)	MaskLoss 1.2442 (1.0833)	MaskBCELoss 0.3710 (0.2576)	MaskDICELoss 0.8732 (0.8257)
Epoch: [0][ 71/500]	Time  7.402 ( 7.402)	Loss 2.2184 (1.8838)	CeLoss 0.3594 (0.4501)	SegCLSLoss 0.0645 (0.0674)	KLLoss 0.0201 (0.1038)	MaskLoss 1.2398 (0.8162)	MaskBCELoss 0.5079 (0.1686)	MaskDICELoss 0.7319 (0.6476)
Epoch: [0][ 72/500]	Time  5.952 ( 5.952)	Loss 0.9180 (1.6097)	CeLoss 0.9180 (0.6096)	SegCLSLoss 0.0000 (0.0491)	KLLoss 0.0000 (0.0711)	MaskLoss 0.0000 (0.5743)	MaskBCELoss 0.0000 (0.1169)	MaskDICELoss 0.0000 (0.4574)
Epoch: [0][ 73/500]	Time  6.750 ( 6.750)	Loss 2.0219 (1.8165)	CeLoss 0.4043 (0.8341)	SegCLSLoss 0.0625 (0.0463)	KLLoss 0.0630 (0.0682)	MaskLoss 1.1382 (0.5772)	MaskBCELoss 0.1961 (0.1094)	MaskDICELoss 0.9421 (0.4678)
Epoch: [0][ 74/500]	Time  7.805 ( 7.805)	Loss 2.4194 (1.9705)	CeLoss 0.2080 (0.3981)	SegCLSLoss 0.1396 (0.0669)	KLLoss 0.2236 (0.0920)	MaskLoss 1.1034 (0.9645)	MaskBCELoss 0.1441 (0.2060)	MaskDICELoss 0.9593 (0.7586)
Epoch: [0][ 75/500]	Time  8.135 ( 8.135)	Loss 2.5489 (2.1868)	CeLoss 0.2891 (0.3823)	SegCLSLoss 0.0645 (0.0813)	KLLoss 0.0574 (0.1116)	MaskLoss 1.4898 (1.0950)	MaskBCELoss 0.5084 (0.2216)	MaskDICELoss 0.9814 (0.8734)
Epoch: [0][ 76/500]	Time  7.778 ( 7.778)	Loss 2.3043 (2.4475)	CeLoss 0.3125 (0.4312)	SegCLSLoss 0.0967 (0.0811)	KLLoss 0.1523 (0.1274)	MaskLoss 1.1279 (1.1650)	MaskBCELoss 0.2076 (0.3020)	MaskDICELoss 0.9203 (0.8630)
Epoch: [0][ 77/500]	Time  9.182 ( 9.182)	Loss 2.2108 (2.1262)	CeLoss 0.2578 (0.2999)	SegCLSLoss 0.0869 (0.0792)	KLLoss 0.1582 (0.1213)	MaskLoss 1.1260 (1.1349)	MaskBCELoss 0.1513 (0.1666)	MaskDICELoss 0.9747 (0.9683)
Epoch: [0][ 78/500]	Time  6.870 ( 6.870)	Loss 1.2812 (1.9076)	CeLoss 1.2812 (0.7286)	SegCLSLoss 0.0000 (0.0522)	KLLoss 0.0000 (0.0771)	MaskLoss 0.0000 (0.7011)	MaskBCELoss 0.0000 (0.1440)	MaskDICELoss 0.0000 (0.5571)
Epoch: [0][ 79/500]	Time  8.246 ( 8.246)	Loss 2.1051 (1.9567)	CeLoss 0.2305 (0.3209)	SegCLSLoss 0.0693 (0.0729)	KLLoss 0.0454 (0.1068)	MaskLoss 1.2210 (0.9474)	MaskBCELoss 0.4369 (0.2247)	MaskDICELoss 0.7841 (0.7227)
[2025-03-02 21:34:55,718] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[0.0002965102040816326], mom=[(0.9, 0.95)]
[2025-03-02 21:34:55,723] [INFO] [timer.py:215:stop] epoch=0/micro_step=800/global_step=80, RunningAvgSamplesPerSec=1.3146226289756273, CurrSamplesPerSec=1.169283243130255, MemAllocated=57.95GB, MaxMemAllocated=62.63GB
Epoch: [0][ 80/500]	Time  8.554 ( 8.554)	Loss 0.8164 (2.1098)	CeLoss 0.8164 (0.3394)	SegCLSLoss 0.0000 (0.0691)	KLLoss 0.0000 (0.0858)	MaskLoss 0.0000 (1.0930)	MaskBCELoss 0.0000 (0.2998)	MaskDICELoss 0.0000 (0.7933)
Epoch: [0][ 81/500]	Time  7.713 ( 7.713)	Loss 0.7148 (1.8100)	CeLoss 0.7148 (0.4977)	SegCLSLoss 0.0000 (0.0504)	KLLoss 0.0000 (0.0625)	MaskLoss 0.0000 (0.8178)	MaskBCELoss 0.0000 (0.2198)	MaskDICELoss 0.0000 (0.5980)
Epoch: [0][ 82/500]	Time  7.648 ( 7.648)	Loss 1.6285 (1.7017)	CeLoss 0.3242 (0.4510)	SegCLSLoss 0.0508 (0.0481)	KLLoss 0.0258 (0.0609)	MaskLoss 0.9697 (0.8013)	MaskBCELoss 0.2057 (0.1826)	MaskDICELoss 0.7641 (0.6187)
Epoch: [0][ 83/500]	Time  7.659 ( 7.659)	Loss 1.0703 (2.2166)	CeLoss 1.0703 (0.3431)	SegCLSLoss 0.0000 (0.0760)	KLLoss 0.0000 (0.1243)	MaskLoss 0.0000 (1.0852)	MaskBCELoss 0.0000 (0.2533)	MaskDICELoss 0.0000 (0.8319)
Epoch: [0][ 84/500]	Time  8.546 ( 8.546)	Loss 0.9023 (2.1298)	CeLoss 0.9023 (0.4105)	SegCLSLoss 0.0000 (0.0720)	KLLoss 0.0000 (0.1175)	MaskLoss 0.0000 (0.9746)	MaskBCELoss 0.0000 (0.2392)	MaskDICELoss 0.0000 (0.7354)
Epoch: [0][ 85/500]	Time  7.144 ( 7.144)	Loss 2.3922 (1.9703)	CeLoss 0.2471 (0.5819)	SegCLSLoss 0.0830 (0.0502)	KLLoss 0.1943 (0.0849)	MaskLoss 1.1290 (0.8224)	MaskBCELoss 0.2007 (0.2014)	MaskDICELoss 0.9282 (0.6210)
Epoch: [0][ 86/500]	Time  8.080 ( 8.080)	Loss 1.6064 (1.7663)	CeLoss 0.2471 (0.3685)	SegCLSLoss 0.0527 (0.0570)	KLLoss 0.0126 (0.0871)	MaskLoss 1.1154 (0.8909)	MaskBCELoss 0.1668 (0.1297)	MaskDICELoss 0.9485 (0.7612)
Epoch: [0][ 87/500]	Time  7.206 ( 7.206)	Loss 1.9259 (2.2022)	CeLoss 0.2812 (0.3663)	SegCLSLoss 0.0510 (0.0608)	KLLoss 0.0131 (0.1098)	MaskLoss 1.2430 (1.1005)	MaskBCELoss 0.3235 (0.2657)	MaskDICELoss 0.9195 (0.8348)
Epoch: [0][ 88/500]	Time  6.245 ( 6.245)	Loss 2.0454 (1.9967)	CeLoss 0.2676 (0.5791)	SegCLSLoss 0.0620 (0.0525)	KLLoss 0.1689 (0.1062)	MaskLoss 1.0340 (0.7496)	MaskBCELoss 0.0348 (0.2162)	MaskDICELoss 0.9992 (0.5334)
Epoch: [0][ 89/500]	Time  8.595 ( 8.595)	Loss 2.2686 (2.3455)	CeLoss 0.3633 (0.2983)	SegCLSLoss 0.0579 (0.0733)	KLLoss 0.0713 (0.1179)	MaskLoss 1.2238 (1.2072)	MaskBCELoss 0.3651 (0.3318)	MaskDICELoss 0.8587 (0.8754)
[2025-03-02 21:36:12,517] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[0.0002952857142857143], mom=[(0.9, 0.95)]
[2025-03-02 21:36:12,522] [INFO] [timer.py:215:stop] epoch=0/micro_step=900/global_step=90, RunningAvgSamplesPerSec=1.313214462530085, CurrSamplesPerSec=1.2560560482621175, MemAllocated=57.45GB, MaxMemAllocated=62.63GB
Epoch: [0][ 90/500]	Time  7.963 ( 7.963)	Loss 0.2656 (2.0706)	CeLoss 0.2656 (0.3395)	SegCLSLoss 0.0000 (0.0725)	KLLoss 0.0000 (0.1359)	MaskLoss 0.0000 (0.9415)	MaskBCELoss 0.0000 (0.2100)	MaskDICELoss 0.0000 (0.7315)
Epoch: [0][ 91/500]	Time  7.253 ( 7.253)	Loss 1.2578 (2.0390)	CeLoss 1.2578 (0.4564)	SegCLSLoss 0.0000 (0.0475)	KLLoss 0.0000 (0.0558)	MaskLoss 0.0000 (0.9926)	MaskBCELoss 0.0000 (0.3430)	MaskDICELoss 0.0000 (0.6496)
Epoch: [0][ 92/500]	Time  6.808 ( 6.808)	Loss 2.9166 (2.3768)	CeLoss 0.2334 (0.2805)	SegCLSLoss 0.0762 (0.0750)	KLLoss 0.2139 (0.1440)	MaskLoss 1.2729 (1.1468)	MaskBCELoss 0.5187 (0.3363)	MaskDICELoss 0.7542 (0.8104)
Epoch: [0][ 93/500]	Time  8.252 ( 8.252)	Loss 1.4219 (1.9645)	CeLoss 1.4219 (0.4860)	SegCLSLoss 0.0000 (0.0516)	KLLoss 0.0000 (0.1009)	MaskLoss 0.0000 (0.8974)	MaskBCELoss 0.0000 (0.1512)	MaskDICELoss 0.0000 (0.7462)
Epoch: [0][ 94/500]	Time  6.160 ( 6.160)	Loss 1.9458 (1.8688)	CeLoss 0.2402 (0.5585)	SegCLSLoss 0.0708 (0.0509)	KLLoss 0.1475 (0.0787)	MaskLoss 1.0326 (0.8072)	MaskBCELoss 0.0460 (0.1628)	MaskDICELoss 0.9865 (0.6444)
Epoch: [0][ 95/500]	Time  8.127 ( 8.127)	Loss 0.8984 (2.0025)	CeLoss 0.8984 (0.3249)	SegCLSLoss 0.0000 (0.0502)	KLLoss 0.0000 (0.0875)	MaskLoss 0.0000 (1.0440)	MaskBCELoss 0.0000 (0.2587)	MaskDICELoss 0.0000 (0.7853)
Epoch: [0][ 96/500]	Time  8.245 ( 8.245)	Loss 1.9699 (1.5051)	CeLoss 0.2539 (0.2966)	SegCLSLoss 0.0510 (0.0372)	KLLoss 0.0889 (0.0733)	MaskLoss 1.1465 (0.7850)	MaskBCELoss 0.1906 (0.1119)	MaskDICELoss 0.9560 (0.6731)
Epoch: [0][ 97/500]	Time  7.247 ( 7.247)	Loss 2.7361 (1.8732)	CeLoss 0.3516 (0.5513)	SegCLSLoss 0.0786 (0.0473)	KLLoss 0.1816 (0.0976)	MaskLoss 1.2696 (0.7870)	MaskBCELoss 0.3493 (0.1210)	MaskDICELoss 0.9203 (0.6661)
Epoch: [0][ 98/500]	Time  7.385 ( 7.385)	Loss 2.0049 (1.6895)	CeLoss 0.2793 (0.3979)	SegCLSLoss 0.0620 (0.0435)	KLLoss 0.1260 (0.0953)	MaskLoss 1.0918 (0.7690)	MaskBCELoss 0.1006 (0.1198)	MaskDICELoss 0.9912 (0.6491)
Epoch: [0][ 99/500]	Time  8.582 ( 8.582)	Loss 1.8879 (1.9460)	CeLoss 0.3516 (0.2486)	SegCLSLoss 0.0503 (0.0484)	KLLoss 0.1201 (0.1059)	MaskLoss 1.0118 (1.0030)	MaskBCELoss 0.0167 (0.2466)	MaskDICELoss 0.9951 (0.7564)
[2025-03-02 21:37:28,399] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[0.00029406122448979587], mom=[(0.9, 0.95)]
[2025-03-02 21:37:28,404] [INFO] [timer.py:215:stop] epoch=0/micro_step=1000/global_step=100, RunningAvgSamplesPerSec=1.3137105082309941, CurrSamplesPerSec=1.2789099915087019, MemAllocated=57.25GB, MaxMemAllocated=62.63GB
Epoch: [0][100/500]	Time  7.821 ( 7.821)	Loss 1.7380 (1.9872)	CeLoss 0.3359 (0.4065)	SegCLSLoss 0.0422 (0.0442)	KLLoss 0.0693 (0.0908)	MaskLoss 1.0473 (1.0306)	MaskBCELoss 0.0579 (0.1656)	MaskDICELoss 0.9895 (0.8651)
Epoch: [0][101/500]	Time  8.056 ( 8.056)	Loss 1.4453 (1.9854)	CeLoss 1.4453 (0.4265)	SegCLSLoss 0.0000 (0.0536)	KLLoss 0.0000 (0.1174)	MaskLoss 0.0000 (0.8963)	MaskBCELoss 0.0000 (0.1667)	MaskDICELoss 0.0000 (0.7296)
Epoch: [0][102/500]	Time  8.399 ( 8.399)	Loss 2.1101 (2.0496)	CeLoss 0.3516 (0.2730)	SegCLSLoss 0.0349 (0.0484)	KLLoss 0.0116 (0.0905)	MaskLoss 1.3006 (1.1571)	MaskBCELoss 0.3936 (0.2334)	MaskDICELoss 0.9070 (0.9237)
Epoch: [0][103/500]	Time  7.891 ( 7.891)	Loss 2.3358 (2.2030)	CeLoss 0.3340 (0.3200)	SegCLSLoss 0.0347 (0.0533)	KLLoss 0.0136 (0.1142)	MaskLoss 1.3199 (1.0828)	MaskBCELoss 0.6097 (0.3161)	MaskDICELoss 0.7102 (0.7667)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([22, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][104/500]	Time  7.065 ( 7.065)	Loss 1.5625 (1.8849)	CeLoss 1.5625 (0.4889)	SegCLSLoss 0.0000 (0.0435)	KLLoss 0.0000 (0.0888)	MaskLoss 0.0000 (0.8019)	MaskBCELoss 0.0000 (0.2162)	MaskDICELoss 0.0000 (0.5857)
Epoch: [0][105/500]	Time  5.982 ( 5.982)	Loss 0.1123 (1.9521)	CeLoss 0.1123 (0.5630)	SegCLSLoss 0.0000 (0.0395)	KLLoss 0.0000 (0.0950)	MaskLoss 0.0000 (0.7636)	MaskBCELoss 0.0000 (0.2257)	MaskDICELoss 0.0000 (0.5379)
Epoch: [0][106/500]	Time  8.017 ( 8.017)	Loss 2.1651 (1.8288)	CeLoss 0.1875 (0.5045)	SegCLSLoss 0.0625 (0.0349)	KLLoss 0.1533 (0.0767)	MaskLoss 1.1330 (0.8235)	MaskBCELoss 0.2001 (0.1764)	MaskDICELoss 0.9329 (0.6470)
Epoch: [0][107/500]	Time  6.982 ( 6.982)	Loss 1.8980 (1.6583)	CeLoss 0.2852 (0.5489)	SegCLSLoss 0.0386 (0.0268)	KLLoss 0.0102 (0.0579)	MaskLoss 1.2248 (0.7027)	MaskBCELoss 0.3275 (0.1613)	MaskDICELoss 0.8974 (0.5414)
Epoch: [0][108/500]	Time  6.732 ( 6.732)	Loss 1.3281 (1.7583)	CeLoss 1.3281 (0.5478)	SegCLSLoss 0.0000 (0.0376)	KLLoss 0.0000 (0.0892)	MaskLoss 0.0000 (0.6977)	MaskBCELoss 0.0000 (0.1375)	MaskDICELoss 0.0000 (0.5602)
Epoch: [0][109/500]	Time  7.664 ( 7.664)	Loss 1.8949 (1.9441)	CeLoss 0.2471 (0.4727)	SegCLSLoss 0.0369 (0.0366)	KLLoss 0.0928 (0.0773)	MaskLoss 1.1098 (0.9399)	MaskBCELoss 0.1484 (0.2040)	MaskDICELoss 0.9614 (0.7358)
[2025-03-02 21:38:43,031] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[0.00029283673469387755], mom=[(0.9, 0.95)]
[2025-03-02 21:38:43,037] [INFO] [timer.py:215:stop] epoch=0/micro_step=1100/global_step=110, RunningAvgSamplesPerSec=1.3161125583295958, CurrSamplesPerSec=1.2751039169332137, MemAllocated=56.95GB, MaxMemAllocated=62.63GB
Epoch: [0][110/500]	Time  7.844 ( 7.844)	Loss 2.9858 (1.9757)	CeLoss 0.5664 (0.5947)	SegCLSLoss 0.0388 (0.0319)	KLLoss 0.0806 (0.0852)	MaskLoss 1.4797 (0.8381)	MaskBCELoss 0.5999 (0.1869)	MaskDICELoss 0.8798 (0.6511)
Epoch: [0][111/500]	Time  6.637 ( 6.637)	Loss 0.9414 (1.8752)	CeLoss 0.9414 (0.6490)	SegCLSLoss 0.0000 (0.0247)	KLLoss 0.0000 (0.0519)	MaskLoss 0.0000 (0.7652)	MaskBCELoss 0.0000 (0.2409)	MaskDICELoss 0.0000 (0.5243)
Epoch: [0][112/500]	Time  7.081 ( 7.081)	Loss 2.4615 (1.5520)	CeLoss 0.3340 (0.5265)	SegCLSLoss 0.0376 (0.0243)	KLLoss 0.0781 (0.0555)	MaskLoss 1.3577 (0.6226)	MaskBCELoss 0.4397 (0.1697)	MaskDICELoss 0.9181 (0.4529)
Epoch: [0][113/500]	Time  8.417 ( 8.417)	Loss 1.8949 (2.1072)	CeLoss 0.2852 (0.3327)	SegCLSLoss 0.0292 (0.0351)	KLLoss 0.0337 (0.0827)	MaskLoss 1.1923 (1.1044)	MaskBCELoss 0.2690 (0.3220)	MaskDICELoss 0.9233 (0.7823)
Epoch: [0][114/500]	Time  6.776 ( 6.776)	Loss 0.1445 (1.2788)	CeLoss 0.1445 (0.5474)	SegCLSLoss 0.0000 (0.0169)	KLLoss 0.0000 (0.0430)	MaskLoss 0.0000 (0.4581)	MaskBCELoss 0.0000 (0.0928)	MaskDICELoss 0.0000 (0.3653)
Epoch: [0][115/500]	Time  8.345 ( 8.345)	Loss 1.8779 (1.8422)	CeLoss 0.3262 (0.3255)	SegCLSLoss 0.0439 (0.0379)	KLLoss 0.0908 (0.0871)	MaskLoss 1.0781 (0.9459)	MaskBCELoss 0.0888 (0.2040)	MaskDICELoss 0.9892 (0.7419)
Epoch: [0][116/500]	Time  8.223 ( 8.223)	Loss 1.8989 (1.9159)	CeLoss 0.2559 (0.3326)	SegCLSLoss 0.0356 (0.0426)	KLLoss 0.0679 (0.0891)	MaskLoss 1.1464 (1.0219)	MaskBCELoss 0.2056 (0.1827)	MaskDICELoss 0.9409 (0.8392)
Epoch: [0][117/500]	Time  7.952 ( 7.952)	Loss 2.2061 (2.2304)	CeLoss 0.2676 (0.2522)	SegCLSLoss 0.0332 (0.0451)	KLLoss 0.0781 (0.1300)	MaskLoss 1.2526 (1.1790)	MaskBCELoss 0.3559 (0.2565)	MaskDICELoss 0.8967 (0.9226)
Epoch: [0][118/500]	Time  7.185 ( 7.185)	Loss 1.8540 (1.6341)	CeLoss 0.3789 (0.4543)	SegCLSLoss 0.0293 (0.0260)	KLLoss 0.0747 (0.0542)	MaskLoss 1.0604 (0.7949)	MaskBCELoss 0.0982 (0.1546)	MaskDICELoss 0.9622 (0.6403)
Epoch: [0][119/500]	Time  7.414 ( 7.414)	Loss 2.4684 (2.1004)	CeLoss 0.2754 (0.4224)	SegCLSLoss 0.0298 (0.0392)	KLLoss 0.0649 (0.1036)	MaskLoss 1.4199 (1.0322)	MaskBCELoss 0.4977 (0.2120)	MaskDICELoss 0.9222 (0.8202)
[2025-03-02 21:39:58,673] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[0.0002916122448979592], mom=[(0.9, 0.95)]
[2025-03-02 21:39:58,679] [INFO] [timer.py:215:stop] epoch=0/micro_step=1200/global_step=120, RunningAvgSamplesPerSec=1.3166298639996028, CurrSamplesPerSec=1.3138521413893134, MemAllocated=57.29GB, MaxMemAllocated=62.63GB
Epoch: [0][120/500]	Time  7.613 ( 7.613)	Loss 2.5075 (1.8083)	CeLoss 0.1787 (0.3046)	SegCLSLoss 0.0757 (0.0363)	KLLoss 0.2090 (0.1113)	MaskLoss 1.2103 (0.8978)	MaskBCELoss 0.2426 (0.1420)	MaskDICELoss 0.9677 (0.7558)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([24, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][121/500]	Time  8.556 ( 8.556)	Loss 2.1184 (2.2707)	CeLoss 0.2500 (0.3335)	SegCLSLoss 0.0327 (0.0421)	KLLoss 0.0674 (0.1225)	MaskLoss 1.2321 (1.1089)	MaskBCELoss 0.3511 (0.3177)	MaskDICELoss 0.8810 (0.7913)
Epoch: [0][122/500]	Time  7.212 ( 7.212)	Loss 1.9126 (1.8384)	CeLoss 0.2383 (0.4325)	SegCLSLoss 0.0432 (0.0275)	KLLoss 0.1328 (0.0565)	MaskLoss 1.0608 (0.9309)	MaskBCELoss 0.0627 (0.2349)	MaskDICELoss 0.9981 (0.6960)
Epoch: [0][123/500]	Time  8.472 ( 8.472)	Loss 0.7461 (1.9583)	CeLoss 0.7461 (0.4662)	SegCLSLoss 0.0000 (0.0302)	KLLoss 0.0000 (0.0824)	MaskLoss 0.0000 (0.9299)	MaskBCELoss 0.0000 (0.2171)	MaskDICELoss 0.0000 (0.7128)
Epoch: [0][124/500]	Time  7.972 ( 7.972)	Loss 0.1299 (1.9317)	CeLoss 0.1299 (0.3896)	SegCLSLoss 0.0000 (0.0310)	KLLoss 0.0000 (0.0840)	MaskLoss 0.0000 (0.9381)	MaskBCELoss 0.0000 (0.2522)	MaskDICELoss 0.0000 (0.6859)
Epoch: [0][125/500]	Time  7.324 ( 7.324)	Loss 2.0671 (2.0013)	CeLoss 0.2256 (0.3472)	SegCLSLoss 0.0374 (0.0379)	KLLoss 0.1211 (0.0973)	MaskLoss 1.1579 (0.9861)	MaskBCELoss 0.1827 (0.2603)	MaskDICELoss 0.9752 (0.7258)
Epoch: [0][126/500]	Time  7.682 ( 7.682)	Loss 1.2344 (1.9482)	CeLoss 1.2344 (0.3685)	SegCLSLoss 0.0000 (0.0394)	KLLoss 0.0000 (0.1120)	MaskLoss 0.0000 (0.9552)	MaskBCELoss 0.0000 (0.1559)	MaskDICELoss 0.0000 (0.7992)
Epoch: [0][127/500]	Time  7.274 ( 7.274)	Loss 2.0399 (1.8406)	CeLoss 0.2539 (0.5583)	SegCLSLoss 0.0356 (0.0266)	KLLoss 0.1187 (0.0882)	MaskLoss 1.1135 (0.7857)	MaskBCELoss 0.1804 (0.1311)	MaskDICELoss 0.9331 (0.6547)
Epoch: [0][128/500]	Time  8.867 ( 8.867)	Loss 1.5312 (1.7218)	CeLoss 1.5312 (0.4570)	SegCLSLoss 0.0000 (0.0257)	KLLoss 0.0000 (0.0815)	MaskLoss 0.0000 (0.7755)	MaskBCELoss 0.0000 (0.1502)	MaskDICELoss 0.0000 (0.6253)
Epoch: [0][129/500]	Time  6.199 ( 6.199)	Loss 0.6992 (1.8139)	CeLoss 0.6992 (0.5306)	SegCLSLoss 0.0000 (0.0388)	KLLoss 0.0000 (0.1105)	MaskLoss 0.0000 (0.7470)	MaskBCELoss 0.0000 (0.0743)	MaskDICELoss 0.0000 (0.6726)
[2025-03-02 21:41:15,403] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[0.00029038775510204076], mom=[(0.9, 0.95)]
[2025-03-02 21:41:15,408] [INFO] [timer.py:215:stop] epoch=0/micro_step=1300/global_step=130, RunningAvgSamplesPerSec=1.3155950343994056, CurrSamplesPerSec=1.3948639007152366, MemAllocated=57.27GB, MaxMemAllocated=62.63GB
Epoch: [0][130/500]	Time  7.171 ( 7.171)	Loss 2.4094 (2.0630)	CeLoss 0.2061 (0.5258)	SegCLSLoss 0.0437 (0.0293)	KLLoss 0.1367 (0.0955)	MaskLoss 1.2867 (0.8978)	MaskBCELoss 0.3493 (0.2429)	MaskDICELoss 0.9374 (0.6549)
Epoch: [0][131/500]	Time  5.488 ( 5.488)	Loss 2.4747 (1.4689)	CeLoss 0.2070 (0.8668)	SegCLSLoss 0.0388 (0.0116)	KLLoss 0.1846 (0.0445)	MaskLoss 1.1887 (0.3471)	MaskBCELoss 0.3212 (0.0714)	MaskDICELoss 0.8675 (0.2757)
Epoch: [0][132/500]	Time  8.960 ( 8.960)	Loss 1.9522 (1.9724)	CeLoss 0.2139 (0.2747)	SegCLSLoss 0.0503 (0.0334)	KLLoss 0.1641 (0.1235)	MaskLoss 1.0267 (1.0495)	MaskBCELoss 0.0310 (0.1385)	MaskDICELoss 0.9957 (0.9110)
Epoch: [0][133/500]	Time  7.239 ( 7.239)	Loss 1.9269 (1.9981)	CeLoss 0.2051 (0.4733)	SegCLSLoss 0.0366 (0.0285)	KLLoss 0.1523 (0.0873)	MaskLoss 1.0456 (0.8764)	MaskBCELoss 0.0493 (0.2854)	MaskDICELoss 0.9963 (0.5910)
Epoch: [0][134/500]	Time  6.821 ( 6.821)	Loss 2.3633 (1.7497)	CeLoss 0.2070 (0.4999)	SegCLSLoss 0.0317 (0.0216)	KLLoss 0.1270 (0.0810)	MaskLoss 1.2379 (0.7129)	MaskBCELoss 0.3949 (0.2028)	MaskDICELoss 0.8431 (0.5101)
Epoch: [0][135/500]	Time  6.671 ( 6.671)	Loss 1.1953 (1.6778)	CeLoss 1.1953 (0.7321)	SegCLSLoss 0.0000 (0.0196)	KLLoss 0.0000 (0.0517)	MaskLoss 0.0000 (0.5731)	MaskBCELoss 0.0000 (0.1559)	MaskDICELoss 0.0000 (0.4172)
Epoch: [0][136/500]	Time  7.722 ( 7.722)	Loss 1.6406 (1.8825)	CeLoss 1.6406 (0.5595)	SegCLSLoss 0.0000 (0.0230)	KLLoss 0.0000 (0.0792)	MaskLoss 0.0000 (0.7972)	MaskBCELoss 0.0000 (0.1980)	MaskDICELoss 0.0000 (0.5992)
Epoch: [0][137/500]	Time  7.549 ( 7.549)	Loss 1.9864 (1.6968)	CeLoss 0.2695 (0.5195)	SegCLSLoss 0.0249 (0.0221)	KLLoss 0.0496 (0.0715)	MaskLoss 1.2470 (0.7231)	MaskBCELoss 0.2589 (0.1579)	MaskDICELoss 0.9881 (0.5651)
Epoch: [0][138/500]	Time  8.607 ( 8.607)	Loss 1.9889 (2.0738)	CeLoss 0.2812 (0.2456)	SegCLSLoss 0.0269 (0.0346)	KLLoss 0.0908 (0.1049)	MaskLoss 1.1369 (1.1263)	MaskBCELoss 0.1958 (0.2646)	MaskDICELoss 0.9411 (0.8617)
Epoch: [0][139/500]	Time  8.326 ( 8.326)	Loss 1.9954 (1.9977)	CeLoss 0.2139 (0.3509)	SegCLSLoss 0.0537 (0.0356)	KLLoss 0.1641 (0.1237)	MaskLoss 1.0462 (0.9817)	MaskBCELoss 0.0507 (0.1528)	MaskDICELoss 0.9955 (0.8289)
[2025-03-02 21:42:28,760] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[0.00028916326530612244], mom=[(0.9, 0.95)]
[2025-03-02 21:42:28,765] [INFO] [timer.py:215:stop] epoch=0/micro_step=1400/global_step=140, RunningAvgSamplesPerSec=1.3189493307741884, CurrSamplesPerSec=1.6744838180893424, MemAllocated=57.27GB, MaxMemAllocated=62.63GB
Epoch: [0][140/500]	Time  5.974 ( 5.974)	Loss 2.2926 (2.3253)	CeLoss 0.2139 (0.6214)	SegCLSLoss 0.0444 (0.0308)	KLLoss 0.1514 (0.1041)	MaskLoss 1.1920 (0.9378)	MaskBCELoss 0.2568 (0.3336)	MaskDICELoss 0.9352 (0.6043)
Epoch: [0][141/500]	Time  6.272 ( 6.272)	Loss 1.5938 (1.8921)	CeLoss 1.5938 (0.5483)	SegCLSLoss 0.0000 (0.0207)	KLLoss 0.0000 (0.0612)	MaskLoss 0.0000 (0.8192)	MaskBCELoss 0.0000 (0.2697)	MaskDICELoss 0.0000 (0.5495)
Epoch: [0][142/500]	Time  6.697 ( 6.697)	Loss 1.8264 (1.7434)	CeLoss 0.2354 (0.5519)	SegCLSLoss 0.0189 (0.0215)	KLLoss 0.0276 (0.0510)	MaskLoss 1.2028 (0.7614)	MaskBCELoss 0.2681 (0.2158)	MaskDICELoss 0.9347 (0.5457)
Epoch: [0][143/500]	Time  6.214 ( 6.214)	Loss 2.0429 (1.7553)	CeLoss 0.2480 (0.4216)	SegCLSLoss 0.0579 (0.0304)	KLLoss 0.1738 (0.1057)	MaskLoss 1.0276 (0.7782)	MaskBCELoss 0.0426 (0.1173)	MaskDICELoss 0.9850 (0.6610)
Epoch: [0][144/500]	Time  9.206 ( 9.206)	Loss 1.1719 (1.9691)	CeLoss 1.1719 (0.4278)	SegCLSLoss 0.0000 (0.0258)	KLLoss 0.0000 (0.1022)	MaskLoss 0.0000 (0.9171)	MaskBCELoss 0.0000 (0.2024)	MaskDICELoss 0.0000 (0.7147)
Epoch: [0][145/500]	Time  8.609 ( 8.609)	Loss 1.8885 (1.9114)	CeLoss 0.3477 (0.3331)	SegCLSLoss 0.0200 (0.0294)	KLLoss 0.0469 (0.1059)	MaskLoss 1.1195 (0.9818)	MaskBCELoss 0.2221 (0.1581)	MaskDICELoss 0.8974 (0.8237)
Epoch: [0][146/500]	Time  7.178 ( 7.178)	Loss 1.9890 (1.8437)	CeLoss 0.2314 (0.5723)	SegCLSLoss 0.0398 (0.0255)	KLLoss 0.1475 (0.0948)	MaskLoss 1.0676 (0.7473)	MaskBCELoss 0.0776 (0.1324)	MaskDICELoss 0.9900 (0.6149)
Epoch: [0][147/500]	Time  8.302 ( 8.302)	Loss 2.0504 (2.2592)	CeLoss 0.2852 (0.4151)	SegCLSLoss 0.0315 (0.0355)	KLLoss 0.1309 (0.1326)	MaskLoss 1.0713 (1.0351)	MaskBCELoss 0.1549 (0.2616)	MaskDICELoss 0.9164 (0.7735)
Epoch: [0][148/500]	Time  7.455 ( 7.455)	Loss 1.8687 (1.9398)	CeLoss 0.2559 (0.3264)	SegCLSLoss 0.0400 (0.0324)	KLLoss 0.1396 (0.1258)	MaskLoss 0.9787 (0.9125)	MaskBCELoss 0.0541 (0.1817)	MaskDICELoss 0.9246 (0.7308)
Epoch: [0][149/500]	Time  6.936 ( 6.936)	Loss 2.2214 (1.7245)	CeLoss 0.3398 (0.7708)	SegCLSLoss 0.0311 (0.0214)	KLLoss 0.1025 (0.0684)	MaskLoss 1.1836 (0.5657)	MaskBCELoss 0.2721 (0.1037)	MaskDICELoss 0.9115 (0.4620)
[2025-03-02 21:43:43,456] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[0.000287938775510204], mom=[(0.9, 0.95)]
[2025-03-02 21:43:43,461] [INFO] [timer.py:215:stop] epoch=0/micro_step=1500/global_step=150, RunningAvgSamplesPerSec=1.3202852028509224, CurrSamplesPerSec=1.2780282763219855, MemAllocated=56.8GB, MaxMemAllocated=62.63GB
Epoch: [0][150/500]	Time  7.826 ( 7.826)	Loss 2.5584 (1.8160)	CeLoss 0.3809 (0.6663)	SegCLSLoss 0.0200 (0.0142)	KLLoss 0.0072 (0.0373)	MaskLoss 1.4078 (0.7056)	MaskBCELoss 0.7307 (0.2875)	MaskDICELoss 0.6770 (0.4181)
Epoch: [0][151/500]	Time  8.118 ( 8.118)	Loss 2.3222 (1.8257)	CeLoss 0.2500 (0.3519)	SegCLSLoss 0.0361 (0.0276)	KLLoss 0.1309 (0.0887)	MaskLoss 1.2562 (0.9340)	MaskBCELoss 0.2770 (0.1721)	MaskDICELoss 0.9791 (0.7619)
Epoch: [0][152/500]	Time  8.094 ( 8.094)	Loss 1.9599 (1.7769)	CeLoss 0.2432 (0.4892)	SegCLSLoss 0.0361 (0.0252)	KLLoss 0.1445 (0.0832)	MaskLoss 1.0521 (0.7879)	MaskBCELoss 0.0680 (0.1540)	MaskDICELoss 0.9841 (0.6339)
Epoch: [0][153/500]	Time  8.375 ( 8.375)	Loss 1.7362 (1.6348)	CeLoss 0.2656 (0.3470)	SegCLSLoss 0.0219 (0.0268)	KLLoss 0.0227 (0.0995)	MaskLoss 1.1240 (0.7575)	MaskBCELoss 0.2450 (0.1192)	MaskDICELoss 0.8789 (0.6383)
Epoch: [0][154/500]	Time  7.861 ( 7.861)	Loss 1.6562 (1.6658)	CeLoss 1.6562 (0.4658)	SegCLSLoss 0.0000 (0.0255)	KLLoss 0.0000 (0.1003)	MaskLoss 0.0000 (0.6730)	MaskBCELoss 0.0000 (0.1128)	MaskDICELoss 0.0000 (0.5602)
Epoch: [0][155/500]	Time  7.925 ( 7.925)	Loss 0.1089 (2.1312)	CeLoss 0.1089 (0.2239)	SegCLSLoss 0.0000 (0.0367)	KLLoss 0.0000 (0.1192)	MaskLoss 0.0000 (1.1002)	MaskBCELoss 0.0000 (0.3103)	MaskDICELoss 0.0000 (0.7898)
Epoch: [0][156/500]	Time  6.706 ( 6.706)	Loss 2.2975 (1.8267)	CeLoss 0.3203 (0.6289)	SegCLSLoss 0.0222 (0.0217)	KLLoss 0.0708 (0.0814)	MaskLoss 1.2480 (0.6925)	MaskBCELoss 0.4324 (0.1689)	MaskDICELoss 0.8156 (0.5236)
Epoch: [0][157/500]	Time  7.511 ( 7.511)	Loss 2.7284 (1.9966)	CeLoss 0.2178 (0.4020)	SegCLSLoss 0.0437 (0.0328)	KLLoss 0.1396 (0.1064)	MaskLoss 1.4532 (0.9321)	MaskBCELoss 0.4784 (0.2216)	MaskDICELoss 0.9748 (0.7105)
Epoch: [0][158/500]	Time  6.118 ( 6.118)	Loss 1.9375 (1.4658)	CeLoss 1.9375 (0.7308)	SegCLSLoss 0.0000 (0.0106)	KLLoss 0.0000 (0.0382)	MaskLoss 0.0000 (0.4695)	MaskBCELoss 0.0000 (0.1075)	MaskDICELoss 0.0000 (0.3620)
Epoch: [0][159/500]	Time  7.492 ( 7.492)	Loss 1.8503 (1.8611)	CeLoss 0.2402 (0.4885)	SegCLSLoss 0.0258 (0.0282)	KLLoss 0.0884 (0.0877)	MaskLoss 1.1021 (0.8761)	MaskBCELoss 0.1388 (0.1311)	MaskDICELoss 0.9633 (0.7450)
[2025-03-02 21:44:58,781] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[0.0002867142857142857], mom=[(0.9, 0.95)]
[2025-03-02 21:44:58,786] [INFO] [timer.py:215:stop] epoch=0/micro_step=1600/global_step=160, RunningAvgSamplesPerSec=1.3207597247454117, CurrSamplesPerSec=1.4039508007479296, MemAllocated=57.25GB, MaxMemAllocated=62.63GB
Epoch: [0][160/500]	Time  7.124 ( 7.124)	Loss 1.9064 (1.8614)	CeLoss 0.3164 (0.6076)	SegCLSLoss 0.0184 (0.0160)	KLLoss 0.0161 (0.0493)	MaskLoss 1.1856 (0.7999)	MaskBCELoss 0.3302 (0.2486)	MaskDICELoss 0.8554 (0.5513)
Epoch: [0][161/500]	Time  7.182 ( 7.182)	Loss 0.5742 (1.6526)	CeLoss 0.5742 (0.5482)	SegCLSLoss 0.0000 (0.0135)	KLLoss 0.0000 (0.0482)	MaskLoss 0.0000 (0.7208)	MaskBCELoss 0.0000 (0.1834)	MaskDICELoss 0.0000 (0.5374)
Epoch: [0][162/500]	Time  6.065 ( 6.065)	Loss 2.4088 (1.9508)	CeLoss 0.3594 (0.4798)	SegCLSLoss 0.0364 (0.0279)	KLLoss 0.2334 (0.1080)	MaskLoss 1.0461 (0.8286)	MaskBCELoss 0.0501 (0.1971)	MaskDICELoss 0.9960 (0.6315)
Epoch: [0][163/500]	Time  8.590 ( 8.590)	Loss 3.9704 (2.0216)	CeLoss 0.2080 (0.2877)	SegCLSLoss 0.0193 (0.0251)	KLLoss 0.0830 (0.0879)	MaskLoss 2.0922 (1.0695)	MaskBCELoss 1.3274 (0.2997)	MaskDICELoss 0.7648 (0.7698)
Epoch: [0][164/500]	Time  7.002 ( 7.002)	Loss 1.9277 (1.7774)	CeLoss 0.2734 (0.4224)	SegCLSLoss 0.0171 (0.0238)	KLLoss 0.0369 (0.0770)	MaskLoss 1.1386 (0.8511)	MaskBCELoss 0.3593 (0.1848)	MaskDICELoss 0.7793 (0.6663)
Epoch: [0][165/500]	Time  7.908 ( 7.908)	Loss 1.1484 (1.5441)	CeLoss 1.1484 (0.4820)	SegCLSLoss 0.0000 (0.0139)	KLLoss 0.0000 (0.0505)	MaskLoss 0.0000 (0.7026)	MaskBCELoss 0.0000 (0.1505)	MaskDICELoss 0.0000 (0.5521)
Epoch: [0][166/500]	Time  6.702 ( 6.702)	Loss 2.3185 (1.8575)	CeLoss 0.2451 (0.5155)	SegCLSLoss 0.0505 (0.0266)	KLLoss 0.1660 (0.0869)	MaskLoss 1.0934 (0.7954)	MaskBCELoss 0.2914 (0.1859)	MaskDICELoss 0.8020 (0.6095)
Epoch: [0][167/500]	Time  6.971 ( 6.971)	Loss 2.6607 (1.7436)	CeLoss 0.1738 (0.6165)	SegCLSLoss 0.0693 (0.0246)	KLLoss 0.2246 (0.0864)	MaskLoss 1.2135 (0.6376)	MaskBCELoss 0.3379 (0.1311)	MaskDICELoss 0.8756 (0.5065)
Epoch: [0][168/500]	Time  7.506 ( 7.506)	Loss 1.9906 (2.1783)	CeLoss 0.2637 (0.4715)	SegCLSLoss 0.0217 (0.0269)	KLLoss 0.0771 (0.0917)	MaskLoss 1.1317 (1.0486)	MaskBCELoss 0.2730 (0.2773)	MaskDICELoss 0.8587 (0.7713)
Epoch: [0][169/500]	Time  7.943 ( 7.943)	Loss 2.0836 (2.1862)	CeLoss 0.3027 (0.2348)	SegCLSLoss 0.0352 (0.0407)	KLLoss 0.0781 (0.1132)	MaskLoss 1.1991 (1.1930)	MaskBCELoss 0.2517 (0.2850)	MaskDICELoss 0.9474 (0.9080)
[2025-03-02 21:46:12,518] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[0.00028548979591836734], mom=[(0.9, 0.95)]
[2025-03-02 21:46:12,523] [INFO] [timer.py:215:stop] epoch=0/micro_step=1700/global_step=170, RunningAvgSamplesPerSec=1.322828708944712, CurrSamplesPerSec=1.2711379172263102, MemAllocated=57.66GB, MaxMemAllocated=62.63GB
Epoch: [0][170/500]	Time  7.869 ( 7.869)	Loss 0.2793 (1.8504)	CeLoss 0.2793 (0.4822)	SegCLSLoss 0.0000 (0.0288)	KLLoss 0.0000 (0.0953)	MaskLoss 0.0000 (0.8119)	MaskBCELoss 0.0000 (0.1609)	MaskDICELoss 0.0000 (0.6510)
Epoch: [0][171/500]	Time  7.656 ( 7.656)	Loss 2.2540 (1.8656)	CeLoss 0.2021 (0.3042)	SegCLSLoss 0.0593 (0.0263)	KLLoss 0.1865 (0.0941)	MaskLoss 1.0973 (0.9456)	MaskBCELoss 0.1801 (0.2260)	MaskDICELoss 0.9173 (0.7196)
Epoch: [0][172/500]	Time  7.943 ( 7.943)	Loss 0.0625 (1.5618)	CeLoss 0.0625 (0.4627)	SegCLSLoss 0.0000 (0.0187)	KLLoss 0.0000 (0.0564)	MaskLoss 0.0000 (0.6778)	MaskBCELoss 0.0000 (0.1865)	MaskDICELoss 0.0000 (0.4913)
Epoch: [0][173/500]	Time  8.199 ( 8.199)	Loss 1.9991 (2.0391)	CeLoss 0.2285 (0.3721)	SegCLSLoss 0.0352 (0.0326)	KLLoss 0.0786 (0.0950)	MaskLoss 1.0762 (1.0272)	MaskBCELoss 0.3604 (0.2431)	MaskDICELoss 0.7158 (0.7841)
Epoch: [0][174/500]	Time  7.638 ( 7.638)	Loss 1.3203 (2.0185)	CeLoss 1.3203 (0.5901)	SegCLSLoss 0.0000 (0.0194)	KLLoss 0.0000 (0.0710)	MaskLoss 0.0000 (0.8680)	MaskBCELoss 0.0000 (0.2665)	MaskDICELoss 0.0000 (0.6015)
Epoch: [0][175/500]	Time  6.686 ( 6.686)	Loss 0.6641 (1.3944)	CeLoss 0.6641 (0.5676)	SegCLSLoss 0.0000 (0.0141)	KLLoss 0.0000 (0.0497)	MaskLoss 0.0000 (0.5379)	MaskBCELoss 0.0000 (0.0830)	MaskDICELoss 0.0000 (0.4548)
Epoch: [0][176/500]	Time  8.288 ( 8.288)	Loss 2.0207 (1.8739)	CeLoss 0.2754 (0.3089)	SegCLSLoss 0.0271 (0.0365)	KLLoss 0.1279 (0.1247)	MaskLoss 1.0946 (0.9511)	MaskBCELoss 0.1253 (0.0963)	MaskDICELoss 0.9693 (0.8548)
Epoch: [0][177/500]	Time  8.094 ( 8.094)	Loss 1.9041 (1.9139)	CeLoss 0.3418 (0.3810)	SegCLSLoss 0.0179 (0.0292)	KLLoss 0.0410 (0.0980)	MaskLoss 1.1397 (0.9071)	MaskBCELoss 0.2488 (0.2191)	MaskDICELoss 0.8909 (0.6880)
Epoch: [0][178/500]	Time  7.586 ( 7.586)	Loss 1.6499 (1.8736)	CeLoss 0.2100 (0.4486)	SegCLSLoss 0.0299 (0.0296)	KLLoss 0.0928 (0.0860)	MaskLoss 1.0214 (0.8864)	MaskBCELoss 0.0348 (0.1800)	MaskDICELoss 0.9866 (0.7064)
Epoch: [0][179/500]	Time  6.811 ( 6.811)	Loss 0.0664 (1.4743)	CeLoss 0.0664 (0.5574)	SegCLSLoss 0.0000 (0.0150)	KLLoss 0.0000 (0.0564)	MaskLoss 0.0000 (0.5641)	MaskBCELoss 0.0000 (0.1200)	MaskDICELoss 0.0000 (0.4440)
[2025-03-02 21:47:28,381] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[0.00028426530612244897], mom=[(0.9, 0.95)]
[2025-03-02 21:47:28,387] [INFO] [timer.py:215:stop] epoch=0/micro_step=1800/global_step=180, RunningAvgSamplesPerSec=1.322578197575221, CurrSamplesPerSec=1.4364294363402568, MemAllocated=56.76GB, MaxMemAllocated=62.63GB
Epoch: [0][180/500]	Time  6.963 ( 6.963)	Loss 0.8477 (1.6755)	CeLoss 0.8477 (0.4754)	SegCLSLoss 0.0000 (0.0191)	KLLoss 0.0000 (0.0893)	MaskLoss 0.0000 (0.6951)	MaskBCELoss 0.0000 (0.1389)	MaskDICELoss 0.0000 (0.5562)
Epoch: [0][181/500]	Time  7.226 ( 7.226)	Loss 2.0589 (2.1011)	CeLoss 0.2715 (0.4193)	SegCLSLoss 0.0172 (0.0250)	KLLoss 0.0693 (0.1109)	MaskLoss 1.2159 (0.9788)	MaskBCELoss 0.2883 (0.2470)	MaskDICELoss 0.9277 (0.7319)
Epoch: [0][182/500]	Time  6.616 ( 6.616)	Loss 1.7764 (2.1516)	CeLoss 0.1885 (0.2353)	SegCLSLoss 0.0322 (0.0384)	KLLoss 0.1260 (0.1192)	MaskLoss 1.0293 (1.1264)	MaskBCELoss 0.0362 (0.2941)	MaskDICELoss 0.9931 (0.8323)
Epoch: [0][183/500]	Time  7.722 ( 7.722)	Loss 1.7956 (1.8541)	CeLoss 0.3691 (0.4480)	SegCLSLoss 0.0171 (0.0204)	KLLoss 0.0444 (0.0683)	MaskLoss 1.0523 (0.9125)	MaskBCELoss 0.1886 (0.2101)	MaskDICELoss 0.8637 (0.7024)
Epoch: [0][184/500]	Time  7.603 ( 7.603)	Loss 1.8977 (1.9704)	CeLoss 0.2891 (0.3874)	SegCLSLoss 0.0141 (0.0196)	KLLoss 0.0067 (0.0671)	MaskLoss 1.0492 (1.0058)	MaskBCELoss 0.5242 (0.2986)	MaskDICELoss 0.5250 (0.7072)
Epoch: [0][185/500]	Time  7.405 ( 7.405)	Loss 1.8393 (2.1432)	CeLoss 0.2119 (0.3842)	SegCLSLoss 0.0337 (0.0270)	KLLoss 0.1270 (0.1165)	MaskLoss 1.0410 (1.0262)	MaskBCELoss 0.0639 (0.2529)	MaskDICELoss 0.9772 (0.7734)
Epoch: [0][186/500]	Time  8.117 ( 8.117)	Loss 1.8954 (1.8968)	CeLoss 0.3594 (0.3809)	SegCLSLoss 0.0151 (0.0233)	KLLoss 0.0327 (0.0817)	MaskLoss 1.0965 (0.9675)	MaskBCELoss 0.3008 (0.2097)	MaskDICELoss 0.7957 (0.7578)
Epoch: [0][187/500]	Time  8.113 ( 8.113)	Loss 1.7446 (1.5570)	CeLoss 0.2129 (0.2926)	SegCLSLoss 0.0262 (0.0158)	KLLoss 0.1133 (0.0643)	MaskLoss 1.0320 (0.8624)	MaskBCELoss 0.0329 (0.1362)	MaskDICELoss 0.9992 (0.7262)
Epoch: [0][188/500]	Time  6.804 ( 6.804)	Loss 2.8011 (1.6437)	CeLoss 0.2871 (0.5461)	SegCLSLoss 0.0192 (0.0147)	KLLoss 0.0732 (0.0474)	MaskLoss 1.5571 (0.6850)	MaskBCELoss 0.6541 (0.2156)	MaskDICELoss 0.9030 (0.4694)
Epoch: [0][189/500]	Time  8.338 ( 8.338)	Loss 2.0055 (1.7802)	CeLoss 0.1602 (0.2567)	SegCLSLoss 0.0554 (0.0260)	KLLoss 0.1953 (0.1008)	MaskLoss 0.9992 (0.9241)	MaskBCELoss 0.0376 (0.1832)	MaskDICELoss 0.9616 (0.7409)
[2025-03-02 21:48:44,031] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[0.0002830408163265306], mom=[(0.9, 0.95)]
[2025-03-02 21:48:44,036] [INFO] [timer.py:215:stop] epoch=0/micro_step=1900/global_step=190, RunningAvgSamplesPerSec=1.322554517685346, CurrSamplesPerSec=1.2981089124811491, MemAllocated=56.71GB, MaxMemAllocated=62.63GB
Epoch: [0][190/500]	Time  7.705 ( 7.705)	Loss 1.4375 (2.0445)	CeLoss 1.4375 (0.5549)	SegCLSLoss 0.0000 (0.0236)	KLLoss 0.0000 (0.0941)	MaskLoss 0.0000 (0.9142)	MaskBCELoss 0.0000 (0.1874)	MaskDICELoss 0.0000 (0.7268)
Epoch: [0][191/500]	Time  8.260 ( 8.260)	Loss 2.3905 (2.0013)	CeLoss 0.3906 (0.5109)	SegCLSLoss 0.0459 (0.0253)	KLLoss 0.2031 (0.1035)	MaskLoss 1.0605 (0.8974)	MaskBCELoss 0.1034 (0.1664)	MaskDICELoss 0.9570 (0.7310)
Epoch: [0][192/500]	Time  8.060 ( 8.060)	Loss 0.0957 (1.7198)	CeLoss 0.0957 (0.4396)	SegCLSLoss 0.0000 (0.0164)	KLLoss 0.0000 (0.0514)	MaskLoss 0.0000 (0.8368)	MaskBCELoss 0.0000 (0.2293)	MaskDICELoss 0.0000 (0.6075)
Epoch: [0][193/500]	Time  6.184 ( 6.184)	Loss 2.3074 (1.6720)	CeLoss 0.2129 (0.6427)	SegCLSLoss 0.0615 (0.0281)	KLLoss 0.2295 (0.0857)	MaskLoss 1.0246 (0.5631)	MaskBCELoss 0.1187 (0.1085)	MaskDICELoss 0.9059 (0.4545)
Epoch: [0][194/500]	Time  6.661 ( 6.661)	Loss 1.9109 (1.5962)	CeLoss 0.2080 (0.5067)	SegCLSLoss 0.0271 (0.0167)	KLLoss 0.1162 (0.0734)	MaskLoss 1.1078 (0.6767)	MaskBCELoss 0.1156 (0.1110)	MaskDICELoss 0.9922 (0.5657)
Epoch: [0][195/500]	Time  8.506 ( 8.506)	Loss 2.1340 (2.1506)	CeLoss 0.2041 (0.2122)	SegCLSLoss 0.0493 (0.0404)	KLLoss 0.1660 (0.1379)	MaskLoss 1.1171 (1.1421)	MaskBCELoss 0.1263 (0.2252)	MaskDICELoss 0.9908 (0.9168)
Epoch: [0][196/500]	Time  8.809 ( 8.809)	Loss 2.1158 (2.0137)	CeLoss 0.2471 (0.2395)	SegCLSLoss 0.0425 (0.0352)	KLLoss 0.1777 (0.1221)	MaskLoss 1.0682 (1.0267)	MaskBCELoss 0.0710 (0.2417)	MaskDICELoss 0.9972 (0.7850)
Epoch: [0][197/500]	Time  7.864 ( 7.864)	Loss 0.1060 (1.5840)	CeLoss 0.1060 (0.4333)	SegCLSLoss 0.0000 (0.0253)	KLLoss 0.0000 (0.0844)	MaskLoss 0.0000 (0.6635)	MaskBCELoss 0.0000 (0.1376)	MaskDICELoss 0.0000 (0.5259)
Epoch: [0][198/500]	Time  6.696 ( 6.696)	Loss 5.6375 (2.0398)	CeLoss 0.2080 (0.5185)	SegCLSLoss 0.0459 (0.0156)	KLLoss 0.2109 (0.0517)	MaskLoss 2.7375 (0.9522)	MaskBCELoss 1.8219 (0.3546)	MaskDICELoss 0.9157 (0.5977)
Epoch: [0][199/500]	Time  7.533 ( 7.533)	Loss 4.7027 (2.1255)	CeLoss 0.1167 (0.2249)	SegCLSLoss 0.0559 (0.0306)	KLLoss 0.1895 (0.1038)	MaskLoss 2.3329 (1.1499)	MaskBCELoss 1.4675 (0.3193)	MaskDICELoss 0.8655 (0.8306)
[2025-03-02 21:50:00,125] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[0.00028181632653061223], mom=[(0.9, 0.95)]
[2025-03-02 21:50:00,131] [INFO] [timer.py:215:stop] epoch=0/micro_step=2000/global_step=200, RunningAvgSamplesPerSec=1.3221393566448154, CurrSamplesPerSec=1.3294015819049165, MemAllocated=56.72GB, MaxMemAllocated=62.63GB
Epoch: [0][200/500]	Time  7.524 ( 7.524)	Loss 1.2969 (1.8939)	CeLoss 1.2969 (0.4913)	SegCLSLoss 0.0000 (0.0246)	KLLoss 0.0000 (0.0983)	MaskLoss 0.0000 (0.8149)	MaskBCELoss 0.0000 (0.1818)	MaskDICELoss 0.0000 (0.6331)
Epoch: [0][201/500]	Time  7.703 ( 7.703)	Loss 2.2174 (2.1946)	CeLoss 0.2637 (0.3813)	SegCLSLoss 0.0510 (0.0422)	KLLoss 0.2188 (0.1458)	MaskLoss 1.0265 (1.0241)	MaskBCELoss 0.0268 (0.1850)	MaskDICELoss 0.9997 (0.8391)
Epoch: [0][202/500]	Time  6.592 ( 6.592)	Loss 0.8867 (1.7618)	CeLoss 0.8867 (0.4449)	SegCLSLoss 0.0000 (0.0200)	KLLoss 0.0000 (0.0739)	MaskLoss 0.0000 (0.8057)	MaskBCELoss 0.0000 (0.2059)	MaskDICELoss 0.0000 (0.5998)
Epoch: [0][203/500]	Time  6.921 ( 6.921)	Loss 3.2915 (2.0354)	CeLoss 0.5078 (0.4188)	SegCLSLoss 0.0282 (0.0249)	KLLoss 0.1094 (0.0924)	MaskLoss 1.5857 (0.9624)	MaskBCELoss 0.7449 (0.2722)	MaskDICELoss 0.8409 (0.6903)
Epoch: [0][204/500]	Time  6.496 ( 6.496)	Loss 1.7734 (1.2200)	CeLoss 1.7734 (0.7296)	SegCLSLoss 0.0000 (0.0097)	KLLoss 0.0000 (0.0358)	MaskLoss 0.0000 (0.3188)	MaskBCELoss 0.0000 (0.0233)	MaskDICELoss 0.0000 (0.2955)
Epoch: [0][205/500]	Time  6.367 ( 6.367)	Loss 1.7741 (1.6237)	CeLoss 0.2871 (0.6724)	SegCLSLoss 0.0177 (0.0176)	KLLoss 0.0249 (0.0664)	MaskLoss 1.0463 (0.5485)	MaskBCELoss 0.3313 (0.1286)	MaskDICELoss 0.7150 (0.4199)
Epoch: [0][206/500]	Time  7.204 ( 7.204)	Loss 1.6419 (1.9147)	CeLoss 0.2578 (0.4303)	SegCLSLoss 0.0181 (0.0291)	KLLoss 0.0417 (0.1090)	MaskLoss 0.8682 (0.8388)	MaskBCELoss 0.3401 (0.1954)	MaskDICELoss 0.5281 (0.6434)
Epoch: [0][207/500]	Time  8.190 ( 8.190)	Loss 1.5255 (2.0058)	CeLoss 0.2432 (0.2403)	SegCLSLoss 0.0216 (0.0316)	KLLoss 0.0591 (0.1174)	MaskLoss 0.8913 (1.0764)	MaskBCELoss 0.1440 (0.2030)	MaskDICELoss 0.7472 (0.8734)
Epoch: [0][208/500]	Time  7.615 ( 7.615)	Loss 1.5603 (2.0159)	CeLoss 0.2598 (0.3633)	SegCLSLoss 0.0237 (0.0260)	KLLoss 0.0574 (0.0931)	MaskLoss 1.0287 (1.0475)	MaskBCELoss 0.0317 (0.2203)	MaskDICELoss 0.9970 (0.8271)
Epoch: [0][209/500]	Time  8.479 ( 8.479)	Loss 1.5938 (1.7905)	CeLoss 1.5938 (0.4591)	SegCLSLoss 0.0000 (0.0194)	KLLoss 0.0000 (0.0764)	MaskLoss 0.0000 (0.7932)	MaskBCELoss 0.0000 (0.2230)	MaskDICELoss 0.0000 (0.5702)
[2025-03-02 21:51:14,324] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=0, lr=[0.00028059183673469386], mom=[(0.9, 0.95)]
[2025-03-02 21:51:14,329] [INFO] [timer.py:215:stop] epoch=0/micro_step=2100/global_step=210, RunningAvgSamplesPerSec=1.3233591186265667, CurrSamplesPerSec=1.158667326383288, MemAllocated=57.26GB, MaxMemAllocated=62.63GB
Epoch: [0][210/500]	Time  8.632 ( 8.632)	Loss 1.8736 (1.7038)	CeLoss 0.2061 (0.2557)	SegCLSLoss 0.0417 (0.0210)	KLLoss 0.1436 (0.0675)	MaskLoss 0.9972 (0.9806)	MaskBCELoss 0.0756 (0.1872)	MaskDICELoss 0.9216 (0.7933)
Epoch: [0][211/500]	Time  7.865 ( 7.865)	Loss 2.1363 (1.8586)	CeLoss 0.3301 (0.4065)	SegCLSLoss 0.0198 (0.0281)	KLLoss 0.0791 (0.1125)	MaskLoss 1.2045 (0.8554)	MaskBCELoss 0.2756 (0.1333)	MaskDICELoss 0.9289 (0.7221)
Epoch: [0][212/500]	Time  7.341 ( 7.341)	Loss 1.6878 (1.6632)	CeLoss 0.2695 (0.4485)	SegCLSLoss 0.0220 (0.0225)	KLLoss 0.0835 (0.0887)	MaskLoss 0.9010 (0.6800)	MaskBCELoss 0.1696 (0.1682)	MaskDICELoss 0.7313 (0.5119)
Epoch: [0][213/500]	Time  7.512 ( 7.512)	Loss 1.6471 (1.8317)	CeLoss 0.2559 (0.4482)	SegCLSLoss 0.0242 (0.0189)	KLLoss 0.0771 (0.0630)	MaskLoss 0.9261 (0.9041)	MaskBCELoss 0.1428 (0.2178)	MaskDICELoss 0.7833 (0.6863)
Epoch: [0][214/500]	Time  8.297 ( 8.297)	Loss 1.4235 (1.4166)	CeLoss 0.2695 (0.3862)	SegCLSLoss 0.0243 (0.0180)	KLLoss 0.0535 (0.0711)	MaskLoss 0.8602 (0.6430)	MaskBCELoss 0.0672 (0.0938)	MaskDICELoss 0.7930 (0.5492)
Epoch: [0][215/500]	Time  6.649 ( 6.649)	Loss 2.4877 (1.6818)	CeLoss 0.2832 (0.3899)	SegCLSLoss 0.0142 (0.0229)	KLLoss 0.0057 (0.0884)	MaskLoss 1.5048 (0.7896)	MaskBCELoss 0.6684 (0.1374)	MaskDICELoss 0.8364 (0.6522)
Epoch: [0][216/500]	Time  7.654 ( 7.654)	Loss 1.9758 (1.7783)	CeLoss 0.2676 (0.4797)	SegCLSLoss 0.0189 (0.0251)	KLLoss 0.0635 (0.0902)	MaskLoss 1.1437 (0.7761)	MaskBCELoss 0.3009 (0.1492)	MaskDICELoss 0.8428 (0.6269)
Epoch: [0][217/500]	Time  6.714 ( 6.714)	Loss 1.9594 (1.8152)	CeLoss 0.1816 (0.4243)	SegCLSLoss 0.0337 (0.0305)	KLLoss 0.1973 (0.1103)	MaskLoss 0.9003 (0.7696)	MaskBCELoss 0.0709 (0.1643)	MaskDICELoss 0.8294 (0.6052)
Epoch: [0][218/500]	Time  8.474 ( 8.474)	Loss 2.0135 (2.0054)	CeLoss 0.2461 (0.3713)	SegCLSLoss 0.0247 (0.0331)	KLLoss 0.1748 (0.1271)	MaskLoss 1.0212 (0.8977)	MaskBCELoss 0.0352 (0.2116)	MaskDICELoss 0.9860 (0.6861)
Epoch: [0][219/500]	Time  7.239 ( 7.239)	Loss 0.9102 (1.5501)	CeLoss 0.9102 (0.4559)	SegCLSLoss 0.0000 (0.0189)	KLLoss 0.0000 (0.0622)	MaskLoss 0.0000 (0.6434)	MaskBCELoss 0.0000 (0.1927)	MaskDICELoss 0.0000 (0.4507)
[2025-03-02 21:52:28,844] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=0, lr=[0.0002793673469387755], mom=[(0.9, 0.95)]
[2025-03-02 21:52:28,849] [INFO] [timer.py:215:stop] epoch=0/micro_step=2200/global_step=220, RunningAvgSamplesPerSec=1.324208820233863, CurrSamplesPerSec=1.4761143195481263, MemAllocated=56.96GB, MaxMemAllocated=62.63GB
Epoch: [0][220/500]	Time  6.776 ( 6.776)	Loss 2.8494 (1.9067)	CeLoss 0.2578 (0.5611)	SegCLSLoss 0.0228 (0.0216)	KLLoss 0.0786 (0.0888)	MaskLoss 1.5140 (0.7989)	MaskBCELoss 0.7495 (0.1812)	MaskDICELoss 0.7645 (0.6177)
Epoch: [0][221/500]	Time  6.889 ( 6.889)	Loss 2.4869 (1.4059)	CeLoss 0.2217 (0.5374)	SegCLSLoss 0.0366 (0.0136)	KLLoss 0.1582 (0.0465)	MaskLoss 1.2755 (0.5615)	MaskBCELoss 0.3363 (0.1141)	MaskDICELoss 0.9392 (0.4474)
Epoch: [0][222/500]	Time  7.915 ( 7.915)	Loss 1.6833 (1.8650)	CeLoss 0.2236 (0.2596)	SegCLSLoss 0.0300 (0.0271)	KLLoss 0.1226 (0.1004)	MaskLoss 0.9349 (0.9448)	MaskBCELoss 0.0219 (0.2449)	MaskDICELoss 0.9130 (0.6998)
Epoch: [0][223/500]	Time  7.864 ( 7.864)	Loss 1.2109 (1.7658)	CeLoss 1.2109 (0.4073)	SegCLSLoss 0.0000 (0.0195)	KLLoss 0.0000 (0.0692)	MaskLoss 0.0000 (0.8616)	MaskBCELoss 0.0000 (0.2104)	MaskDICELoss 0.0000 (0.6512)
Epoch: [0][224/500]	Time  8.086 ( 8.086)	Loss 1.6552 (1.6881)	CeLoss 0.3809 (0.3436)	SegCLSLoss 0.0215 (0.0215)	KLLoss 0.0723 (0.0801)	MaskLoss 0.8879 (0.8318)	MaskBCELoss 0.0876 (0.1818)	MaskDICELoss 0.8004 (0.6500)
Epoch: [0][225/500]	Time  6.967 ( 6.967)	Loss 1.0234 (1.4994)	CeLoss 1.0234 (0.5197)	SegCLSLoss 0.0000 (0.0193)	KLLoss 0.0000 (0.0611)	MaskLoss 0.0000 (0.6217)	MaskBCELoss 0.0000 (0.1041)	MaskDICELoss 0.0000 (0.5175)
Epoch: [0][226/500]	Time  7.518 ( 7.518)	Loss 2.0146 (1.5916)	CeLoss 0.1982 (0.3920)	SegCLSLoss 0.0369 (0.0193)	KLLoss 0.1699 (0.0632)	MaskLoss 1.0475 (0.8280)	MaskBCELoss 0.0686 (0.1086)	MaskDICELoss 0.9789 (0.7194)
Epoch: [0][227/500]	Time  9.308 ( 9.308)	Loss 1.9188 (1.7658)	CeLoss 0.4551 (0.3343)	SegCLSLoss 0.0176 (0.0298)	KLLoss 0.0320 (0.1049)	MaskLoss 1.0569 (0.8472)	MaskBCELoss 0.2682 (0.1490)	MaskDICELoss 0.7887 (0.6982)
Epoch: [0][228/500]	Time  8.579 ( 8.579)	Loss 2.6412 (1.8444)	CeLoss 0.2227 (0.2400)	SegCLSLoss 0.0327 (0.0292)	KLLoss 0.1465 (0.1076)	MaskLoss 1.3677 (0.9893)	MaskBCELoss 0.4493 (0.1702)	MaskDICELoss 0.9183 (0.8191)
Epoch: [0][229/500]	Time  7.712 ( 7.712)	Loss 1.6725 (2.0883)	CeLoss 0.3750 (0.3381)	SegCLSLoss 0.0165 (0.0206)	KLLoss 0.0386 (0.0778)	MaskLoss 0.9034 (1.0653)	MaskBCELoss 0.2300 (0.3638)	MaskDICELoss 0.6734 (0.7016)
[2025-03-02 21:53:46,228] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=0, lr=[0.0002781428571428571], mom=[(0.9, 0.95)]
[2025-03-02 21:53:46,234] [INFO] [timer.py:215:stop] epoch=0/micro_step=2300/global_step=230, RunningAvgSamplesPerSec=1.322783655310758, CurrSamplesPerSec=1.5280258575405326, MemAllocated=57.24GB, MaxMemAllocated=62.63GB
Epoch: [0][230/500]	Time  6.546 ( 6.546)	Loss 1.8207 (1.5542)	CeLoss 0.3730 (0.4607)	SegCLSLoss 0.0167 (0.0162)	KLLoss 0.0300 (0.0635)	MaskLoss 0.9729 (0.6541)	MaskBCELoss 0.3478 (0.1770)	MaskDICELoss 0.6251 (0.4771)
Epoch: [0][231/500]	Time  6.851 ( 6.851)	Loss 1.0703 (1.9318)	CeLoss 1.0703 (0.4857)	SegCLSLoss 0.0000 (0.0280)	KLLoss 0.0000 (0.1019)	MaskLoss 0.0000 (0.8421)	MaskBCELoss 0.0000 (0.1825)	MaskDICELoss 0.0000 (0.6597)
Epoch: [0][232/500]	Time  6.948 ( 6.948)	Loss 2.1743 (1.7915)	CeLoss 0.2988 (0.2990)	SegCLSLoss 0.0134 (0.0337)	KLLoss 0.0063 (0.1037)	MaskLoss 1.3020 (0.8802)	MaskBCELoss 0.5421 (0.1804)	MaskDICELoss 0.7599 (0.6998)
Epoch: [0][233/500]	Time  6.468 ( 6.468)	Loss 1.8049 (1.6389)	CeLoss 0.2451 (0.6812)	SegCLSLoss 0.0265 (0.0199)	KLLoss 0.0928 (0.0793)	MaskLoss 1.0723 (0.5910)	MaskBCELoss 0.1037 (0.0396)	MaskDICELoss 0.9687 (0.5514)
Epoch: [0][234/500]	Time  7.761 ( 7.761)	Loss 0.9844 (1.5302)	CeLoss 0.9844 (0.4852)	SegCLSLoss 0.0000 (0.0184)	KLLoss 0.0000 (0.0632)	MaskLoss 0.0000 (0.6624)	MaskBCELoss 0.0000 (0.1208)	MaskDICELoss 0.0000 (0.5415)
Epoch: [0][235/500]	Time  7.745 ( 7.745)	Loss 2.3941 (1.7685)	CeLoss 0.2393 (0.6127)	SegCLSLoss 0.0349 (0.0236)	KLLoss 0.1455 (0.0869)	MaskLoss 1.2178 (0.6519)	MaskBCELoss 0.3403 (0.1439)	MaskDICELoss 0.8775 (0.5080)
Epoch: [0][236/500]	Time  6.746 ( 6.746)	Loss 2.9183 (1.9940)	CeLoss 0.3027 (0.4086)	SegCLSLoss 0.0349 (0.0272)	KLLoss 0.2139 (0.0989)	MaskLoss 1.3371 (0.9591)	MaskBCELoss 0.4093 (0.2180)	MaskDICELoss 0.9278 (0.7411)
Epoch: [0][237/500]	Time  8.056 ( 8.056)	Loss 1.9563 (1.7188)	CeLoss 0.2578 (0.3795)	SegCLSLoss 0.0231 (0.0194)	KLLoss 0.1182 (0.0720)	MaskLoss 0.9856 (0.8682)	MaskBCELoss 0.2286 (0.1736)	MaskDICELoss 0.7571 (0.6946)
Epoch: [0][238/500]	Time  8.079 ( 8.079)	Loss 1.8935 (1.7797)	CeLoss 0.2578 (0.4883)	SegCLSLoss 0.0232 (0.0295)	KLLoss 0.1187 (0.0979)	MaskLoss 1.0607 (0.7789)	MaskBCELoss 0.0906 (0.1063)	MaskDICELoss 0.9702 (0.6726)
Epoch: [0][239/500]	Time  7.826 ( 7.826)	Loss 3.2935 (2.0756)	CeLoss 0.2402 (0.4191)	SegCLSLoss 0.0757 (0.0354)	KLLoss 0.2285 (0.1318)	MaskLoss 1.4710 (0.9234)	MaskBCELoss 0.6350 (0.1894)	MaskDICELoss 0.8360 (0.7340)
[2025-03-02 21:55:01,069] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=0, lr=[0.00027691836734693875], mom=[(0.9, 0.95)]
[2025-03-02 21:55:01,073] [INFO] [timer.py:215:stop] epoch=0/micro_step=2400/global_step=240, RunningAvgSamplesPerSec=1.3233515528633557, CurrSamplesPerSec=1.196414894827806, MemAllocated=57.26GB, MaxMemAllocated=62.63GB
Epoch: [0][240/500]	Time  8.360 ( 8.360)	Loss 1.9428 (2.0185)	CeLoss 0.2266 (0.4535)	SegCLSLoss 0.0408 (0.0267)	KLLoss 0.1582 (0.0998)	MaskLoss 1.0217 (0.9333)	MaskBCELoss 0.0422 (0.2189)	MaskDICELoss 0.9795 (0.7144)
Epoch: [0][241/500]	Time  8.051 ( 8.051)	Loss 2.3181 (2.1181)	CeLoss 0.2109 (0.2282)	SegCLSLoss 0.0178 (0.0313)	KLLoss 0.0089 (0.1069)	MaskLoss 1.4045 (1.1832)	MaskBCELoss 0.6578 (0.2640)	MaskDICELoss 0.7467 (0.9192)
Epoch: [0][242/500]	Time  7.449 ( 7.449)	Loss 1.6593 (1.9930)	CeLoss 0.2598 (0.3544)	SegCLSLoss 0.0160 (0.0297)	KLLoss 0.0352 (0.1130)	MaskLoss 0.9523 (0.9722)	MaskBCELoss 0.2987 (0.2003)	MaskDICELoss 0.6536 (0.7719)
Epoch: [0][243/500]	Time  7.236 ( 7.236)	Loss 2.4583 (2.0065)	CeLoss 0.1582 (0.5142)	SegCLSLoss 0.0747 (0.0238)	KLLoss 0.2109 (0.0819)	MaskLoss 1.1656 (0.9195)	MaskBCELoss 0.2537 (0.2333)	MaskDICELoss 0.9119 (0.6863)
Epoch: [0][244/500]	Time  8.642 ( 8.642)	Loss 2.1442 (1.7861)	CeLoss 0.2578 (0.3924)	SegCLSLoss 0.0178 (0.0231)	KLLoss 0.0562 (0.0889)	MaskLoss 1.2086 (0.8515)	MaskBCELoss 0.4434 (0.1754)	MaskDICELoss 0.7653 (0.6761)
Epoch: [0][245/500]	Time  8.318 ( 8.318)	Loss 1.9714 (2.1133)	CeLoss 0.2285 (0.2300)	SegCLSLoss 0.0374 (0.0330)	KLLoss 0.1523 (0.1325)	MaskLoss 1.0374 (1.1225)	MaskBCELoss 0.0786 (0.2144)	MaskDICELoss 0.9588 (0.9081)
Epoch: [0][246/500]	Time  7.679 ( 7.679)	Loss 2.3717 (2.0908)	CeLoss 0.1367 (0.4109)	SegCLSLoss 0.0583 (0.0286)	KLLoss 0.2139 (0.1024)	MaskLoss 1.1288 (0.9672)	MaskBCELoss 0.2195 (0.2886)	MaskDICELoss 0.9092 (0.6786)
Epoch: [0][247/500]	Time  6.384 ( 6.384)	Loss 2.0076 (1.6875)	CeLoss 0.1943 (0.5465)	SegCLSLoss 0.0267 (0.0130)	KLLoss 0.1279 (0.0433)	MaskLoss 1.1331 (0.7274)	MaskBCELoss 0.1558 (0.2340)	MaskDICELoss 0.9772 (0.4934)
Epoch: [0][248/500]	Time  7.445 ( 7.445)	Loss 2.6273 (2.3533)	CeLoss 0.1855 (0.3442)	SegCLSLoss 0.0417 (0.0395)	KLLoss 0.2129 (0.1474)	MaskLoss 1.1979 (1.0957)	MaskBCELoss 0.3747 (0.3039)	MaskDICELoss 0.8233 (0.7918)
Epoch: [0][249/500]	Time  8.473 ( 8.473)	Loss 2.3345 (2.1521)	CeLoss 0.1670 (0.2778)	SegCLSLoss 0.0913 (0.0362)	KLLoss 0.1621 (0.1233)	MaskLoss 1.0905 (1.0926)	MaskBCELoss 0.3847 (0.2696)	MaskDICELoss 0.7058 (0.8230)
[2025-03-02 21:56:18,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=0, lr=[0.0002756938775510204], mom=[(0.9, 0.95)]
[2025-03-02 21:56:18,978] [INFO] [timer.py:215:stop] epoch=0/micro_step=2500/global_step=250, RunningAvgSamplesPerSec=1.321711305919437, CurrSamplesPerSec=1.2158617703049925, MemAllocated=57.26GB, MaxMemAllocated=62.63GB
Epoch: [0][250/500]	Time  8.226 ( 8.226)	Loss 1.7703 (1.7610)	CeLoss 0.2832 (0.2974)	SegCLSLoss 0.0198 (0.0219)	KLLoss 0.0796 (0.0741)	MaskLoss 1.0642 (0.9766)	MaskBCELoss 0.0967 (0.1795)	MaskDICELoss 0.9675 (0.7972)
Epoch: [0][251/500]	Time  7.780 ( 7.780)	Loss 1.2964 (1.8289)	CeLoss 0.1768 (0.3739)	SegCLSLoss 0.0142 (0.0320)	KLLoss 0.0074 (0.1013)	MaskLoss 0.9023 (0.8147)	MaskBCELoss 0.1812 (0.2191)	MaskDICELoss 0.7211 (0.5955)
Epoch: [0][252/500]	Time  7.661 ( 7.661)	Loss 1.3984 (1.5580)	CeLoss 1.3984 (0.5441)	SegCLSLoss 0.0000 (0.0152)	KLLoss 0.0000 (0.0676)	MaskLoss 0.0000 (0.6335)	MaskBCELoss 0.0000 (0.1023)	MaskDICELoss 0.0000 (0.5312)
Epoch: [0][253/500]	Time  7.687 ( 7.687)	Loss 1.1094 (1.7195)	CeLoss 1.1094 (0.2913)	SegCLSLoss 0.0000 (0.0294)	KLLoss 0.0000 (0.1059)	MaskLoss 0.0000 (0.8542)	MaskBCELoss 0.0000 (0.1350)	MaskDICELoss 0.0000 (0.7191)
Epoch: [0][254/500]	Time  7.072 ( 7.072)	Loss 1.8476 (2.0049)	CeLoss 0.2832 (0.4235)	SegCLSLoss 0.0322 (0.0201)	KLLoss 0.1309 (0.0875)	MaskLoss 1.0055 (0.9598)	MaskBCELoss 0.0217 (0.2617)	MaskDICELoss 0.9838 (0.6981)
Epoch: [0][255/500]	Time  7.684 ( 7.684)	Loss 1.7827 (1.5857)	CeLoss 0.4434 (0.3705)	SegCLSLoss 0.0172 (0.0199)	KLLoss 0.0674 (0.0661)	MaskLoss 1.0014 (0.8224)	MaskBCELoss 0.0625 (0.1188)	MaskDICELoss 0.9388 (0.7036)
Epoch: [0][256/500]	Time  8.891 ( 8.891)	Loss 1.5664 (2.2683)	CeLoss 0.2520 (0.2388)	SegCLSLoss 0.0264 (0.0333)	KLLoss 0.0977 (0.1511)	MaskLoss 0.6526 (1.1258)	MaskBCELoss 0.2576 (0.2823)	MaskDICELoss 0.3950 (0.8435)
Epoch: [0][257/500]	Time  6.935 ( 6.935)	Loss 1.6964 (1.7692)	CeLoss 0.2637 (0.5554)	SegCLSLoss 0.0277 (0.0151)	KLLoss 0.0938 (0.0454)	MaskLoss 1.0186 (0.8169)	MaskBCELoss 0.0254 (0.2082)	MaskDICELoss 0.9932 (0.6086)
Epoch: [0][258/500]	Time  8.233 ( 8.233)	Loss 2.2279 (1.9498)	CeLoss 0.2988 (0.3656)	SegCLSLoss 0.0212 (0.0219)	KLLoss 0.0474 (0.0781)	MaskLoss 1.2318 (0.9534)	MaskBCELoss 0.4981 (0.3067)	MaskDICELoss 0.7337 (0.6467)
Epoch: [0][259/500]	Time  7.558 ( 7.558)	Loss 1.8548 (1.8099)	CeLoss 0.2715 (0.3540)	SegCLSLoss 0.0325 (0.0251)	KLLoss 0.1270 (0.0873)	MaskLoss 1.0104 (0.9033)	MaskBCELoss 0.0475 (0.1901)	MaskDICELoss 0.9629 (0.7132)
[2025-03-02 21:57:35,578] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=0, lr=[0.000274469387755102], mom=[(0.9, 0.95)]
[2025-03-02 21:57:35,584] [INFO] [timer.py:215:stop] epoch=0/micro_step=2600/global_step=260, RunningAvgSamplesPerSec=1.321079633823754, CurrSamplesPerSec=1.4076832454943138, MemAllocated=56.7GB, MaxMemAllocated=62.63GB
Epoch: [0][260/500]	Time  7.106 ( 7.106)	Loss 1.4453 (2.0195)	CeLoss 1.4453 (0.4678)	SegCLSLoss 0.0000 (0.0321)	KLLoss 0.0000 (0.1050)	MaskLoss 0.0000 (0.9174)	MaskBCELoss 0.0000 (0.1984)	MaskDICELoss 0.0000 (0.7189)
Epoch: [0][261/500]	Time  7.353 ( 7.353)	Loss 2.0764 (1.6562)	CeLoss 0.2539 (0.4787)	SegCLSLoss 0.0381 (0.0205)	KLLoss 0.1689 (0.0762)	MaskLoss 1.0618 (0.7480)	MaskBCELoss 0.0654 (0.1147)	MaskDICELoss 0.9965 (0.6333)
Epoch: [0][262/500]	Time  6.783 ( 6.783)	Loss 1.4688 (1.7712)	CeLoss 1.4688 (0.6723)	SegCLSLoss 0.0000 (0.0207)	KLLoss 0.0000 (0.0686)	MaskLoss 0.0000 (0.6524)	MaskBCELoss 0.0000 (0.1615)	MaskDICELoss 0.0000 (0.4909)
Epoch: [0][263/500]	Time  7.828 ( 7.828)	Loss 1.7800 (1.7158)	CeLoss 0.2578 (0.4119)	SegCLSLoss 0.0155 (0.0220)	KLLoss 0.0072 (0.0629)	MaskLoss 0.9103 (0.8254)	MaskBCELoss 0.5748 (0.2162)	MaskDICELoss 0.3355 (0.6092)
Epoch: [0][264/500]	Time  7.370 ( 7.370)	Loss 1.4397 (1.4339)	CeLoss 0.2412 (0.4967)	SegCLSLoss 0.0195 (0.0154)	KLLoss 0.0591 (0.0532)	MaskLoss 0.9374 (0.6167)	MaskBCELoss 0.0160 (0.0998)	MaskDICELoss 0.9214 (0.5169)
Epoch: [0][265/500]	Time  7.815 ( 7.815)	Loss 1.8018 (2.0168)	CeLoss 0.2285 (0.2910)	SegCLSLoss 0.0366 (0.0363)	KLLoss 0.1187 (0.1364)	MaskLoss 1.0165 (0.9978)	MaskBCELoss 0.0627 (0.1632)	MaskDICELoss 0.9538 (0.8346)
Epoch: [0][266/500]	Time  7.375 ( 7.375)	Loss 1.8551 (1.7198)	CeLoss 0.2656 (0.5172)	SegCLSLoss 0.0376 (0.0229)	KLLoss 0.1348 (0.0805)	MaskLoss 1.0038 (0.7692)	MaskBCELoss 0.0270 (0.1000)	MaskDICELoss 0.9767 (0.6692)
Epoch: [0][267/500]	Time  8.565 ( 8.565)	Loss 2.0217 (1.6613)	CeLoss 0.3066 (0.2366)	SegCLSLoss 0.0229 (0.0255)	KLLoss 0.1001 (0.0851)	MaskLoss 1.0823 (0.9051)	MaskBCELoss 0.2207 (0.1663)	MaskDICELoss 0.8616 (0.7388)
Epoch: [0][268/500]	Time  7.472 ( 7.472)	Loss 1.6656 (1.7448)	CeLoss 0.2383 (0.3940)	SegCLSLoss 0.0199 (0.0238)	KLLoss 0.0398 (0.0823)	MaskLoss 1.1233 (0.8575)	MaskBCELoss 0.1361 (0.1518)	MaskDICELoss 0.9872 (0.7057)
Epoch: [0][269/500]	Time  7.824 ( 7.824)	Loss 0.7695 (1.7395)	CeLoss 0.7695 (0.3860)	SegCLSLoss 0.0000 (0.0282)	KLLoss 0.0000 (0.1152)	MaskLoss 0.0000 (0.7800)	MaskBCELoss 0.0000 (0.0984)	MaskDICELoss 0.0000 (0.6816)
[2025-03-02 21:58:50,706] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=0, lr=[0.00027324489795918364], mom=[(0.9, 0.95)]
[2025-03-02 21:58:50,711] [INFO] [timer.py:215:stop] epoch=0/micro_step=2700/global_step=270, RunningAvgSamplesPerSec=1.321458314489872, CurrSamplesPerSec=1.4836666648508336, MemAllocated=57.36GB, MaxMemAllocated=62.63GB
Epoch: [0][270/500]	Time  6.742 ( 6.742)	Loss 1.6410 (1.5406)	CeLoss 0.2041 (0.5938)	SegCLSLoss 0.0280 (0.0150)	KLLoss 0.0942 (0.0497)	MaskLoss 0.9368 (0.6227)	MaskBCELoss 0.1104 (0.1175)	MaskDICELoss 0.8264 (0.5051)
Epoch: [0][271/500]	Time  7.842 ( 7.842)	Loss 1.6094 (1.3788)	CeLoss 1.6094 (0.4892)	SegCLSLoss 0.0000 (0.0197)	KLLoss 0.0000 (0.0695)	MaskLoss 0.0000 (0.5260)	MaskBCELoss 0.0000 (0.0754)	MaskDICELoss 0.0000 (0.4506)
Epoch: [0][272/500]	Time  6.490 ( 6.490)	Loss 0.8477 (1.7502)	CeLoss 0.8477 (0.6306)	SegCLSLoss 0.0000 (0.0210)	KLLoss 0.0000 (0.0690)	MaskLoss 0.0000 (0.6579)	MaskBCELoss 0.0000 (0.1757)	MaskDICELoss 0.0000 (0.4822)
Epoch: [0][273/500]	Time  7.317 ( 7.317)	Loss 1.9355 (1.6232)	CeLoss 0.2617 (0.4933)	SegCLSLoss 0.0359 (0.0203)	KLLoss 0.1504 (0.0900)	MaskLoss 1.0201 (0.6502)	MaskBCELoss 0.0326 (0.1096)	MaskDICELoss 0.9876 (0.5406)
Epoch: [0][274/500]	Time  8.455 ( 8.455)	Loss 1.8652 (1.8424)	CeLoss 0.2637 (0.3033)	SegCLSLoss 0.0143 (0.0217)	KLLoss 0.0058 (0.0733)	MaskLoss 1.2344 (0.9743)	MaskBCELoss 0.3359 (0.2609)	MaskDICELoss 0.8986 (0.7134)
Epoch: [0][275/500]	Time  6.463 ( 6.463)	Loss 1.3672 (1.8941)	CeLoss 1.3672 (0.4322)	SegCLSLoss 0.0000 (0.0320)	KLLoss 0.0000 (0.1319)	MaskLoss 0.0000 (0.7757)	MaskBCELoss 0.0000 (0.1430)	MaskDICELoss 0.0000 (0.6327)
Epoch: [0][276/500]	Time  8.267 ( 8.267)	Loss 0.5625 (1.8100)	CeLoss 0.5625 (0.3025)	SegCLSLoss 0.0000 (0.0332)	KLLoss 0.0000 (0.1036)	MaskLoss 0.0000 (0.8813)	MaskBCELoss 0.0000 (0.1951)	MaskDICELoss 0.0000 (0.6861)
Epoch: [0][277/500]	Time  6.902 ( 6.902)	Loss 0.3613 (1.2524)	CeLoss 0.3613 (0.7191)	SegCLSLoss 0.0000 (0.0109)	KLLoss 0.0000 (0.0461)	MaskLoss 0.0000 (0.3075)	MaskBCELoss 0.0000 (0.0365)	MaskDICELoss 0.0000 (0.2710)
Epoch: [0][278/500]	Time  8.705 ( 8.705)	Loss 2.1630 (2.0086)	CeLoss 0.2207 (0.3555)	SegCLSLoss 0.0532 (0.0341)	KLLoss 0.2207 (0.1265)	MaskLoss 1.0119 (0.9453)	MaskBCELoss 0.0183 (0.1848)	MaskDICELoss 0.9936 (0.7605)
Epoch: [0][279/500]	Time  7.075 ( 7.075)	Loss 1.6151 (1.7256)	CeLoss 0.2217 (0.4886)	SegCLSLoss 0.0236 (0.0252)	KLLoss 0.0869 (0.0759)	MaskLoss 0.8691 (0.7293)	MaskBCELoss 0.1639 (0.1918)	MaskDICELoss 0.7052 (0.5375)
[2025-03-02 22:00:05,516] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=0, lr=[0.00027202040816326527], mom=[(0.9, 0.95)]
[2025-03-02 22:00:05,522] [INFO] [timer.py:215:stop] epoch=0/micro_step=2800/global_step=280, RunningAvgSamplesPerSec=1.322009061432222, CurrSamplesPerSec=1.370879011961376, MemAllocated=57.26GB, MaxMemAllocated=62.63GB
Epoch: [0][280/500]	Time  7.296 ( 7.296)	Loss 2.4432 (1.8803)	CeLoss 0.2256 (0.3708)	SegCLSLoss 0.0361 (0.0283)	KLLoss 0.1543 (0.1134)	MaskLoss 1.1975 (0.8472)	MaskBCELoss 0.3863 (0.1953)	MaskDICELoss 0.8113 (0.6519)
Epoch: [0][281/500]	Time  7.402 ( 7.402)	Loss 2.1567 (2.1023)	CeLoss 0.2256 (0.2836)	SegCLSLoss 0.0284 (0.0414)	KLLoss 0.1348 (0.1566)	MaskLoss 1.0894 (1.0012)	MaskBCELoss 0.2860 (0.1707)	MaskDICELoss 0.8034 (0.8305)
Epoch: [0][282/500]	Time  7.220 ( 7.220)	Loss 2.4471 (1.9193)	CeLoss 0.1885 (0.3376)	SegCLSLoss 0.0337 (0.0282)	KLLoss 0.1162 (0.0860)	MaskLoss 1.3578 (0.9386)	MaskBCELoss 0.4174 (0.2849)	MaskDICELoss 0.9404 (0.6537)
Epoch: [0][283/500]	Time  7.928 ( 7.928)	Loss 1.8465 (2.0292)	CeLoss 0.2930 (0.4521)	SegCLSLoss 0.0214 (0.0218)	KLLoss 0.1064 (0.0783)	MaskLoss 0.9576 (0.9777)	MaskBCELoss 0.1623 (0.2759)	MaskDICELoss 0.7953 (0.7018)
Epoch: [0][284/500]	Time  7.166 ( 7.166)	Loss 1.2633 (1.6274)	CeLoss 0.2695 (0.4539)	SegCLSLoss 0.0156 (0.0149)	KLLoss 0.0056 (0.0590)	MaskLoss 0.7925 (0.7057)	MaskBCELoss 0.1720 (0.2254)	MaskDICELoss 0.6205 (0.4803)
Epoch: [0][285/500]	Time  7.236 ( 7.236)	Loss 1.6199 (1.1020)	CeLoss 0.2910 (0.4656)	SegCLSLoss 0.0192 (0.0127)	KLLoss 0.0693 (0.0413)	MaskLoss 0.8888 (0.3971)	MaskBCELoss 0.1530 (0.0678)	MaskDICELoss 0.7357 (0.3293)
Epoch: [0][286/500]	Time  7.052 ( 7.052)	Loss 1.6172 (1.7795)	CeLoss 1.6172 (0.6493)	SegCLSLoss 0.0000 (0.0322)	KLLoss 0.0000 (0.1037)	MaskLoss 0.0000 (0.5679)	MaskBCELoss 0.0000 (0.1315)	MaskDICELoss 0.0000 (0.4364)
Epoch: [0][287/500]	Time  9.187 ( 9.187)	Loss 2.0917 (1.7632)	CeLoss 0.2227 (0.2500)	SegCLSLoss 0.0381 (0.0256)	KLLoss 0.1475 (0.0673)	MaskLoss 0.9421 (0.9675)	MaskBCELoss 0.3176 (0.2628)	MaskDICELoss 0.6246 (0.7047)
Epoch: [0][288/500]	Time  6.856 ( 6.856)	Loss 3.6433 (2.2380)	CeLoss 0.1138 (0.4044)	SegCLSLoss 0.0972 (0.0358)	KLLoss 0.2324 (0.1202)	MaskLoss 1.6453 (1.0293)	MaskBCELoss 0.9042 (0.3046)	MaskDICELoss 0.7411 (0.7247)
Epoch: [0][289/500]	Time  7.145 ( 7.145)	Loss 1.1328 (1.8105)	CeLoss 1.1328 (0.4398)	SegCLSLoss 0.0000 (0.0250)	KLLoss 0.0000 (0.0854)	MaskLoss 0.0000 (0.8540)	MaskBCELoss 0.0000 (0.1623)	MaskDICELoss 0.0000 (0.6917)
[2025-03-02 22:01:19,707] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=0, lr=[0.00027079591836734696], mom=[(0.9, 0.95)]
[2025-03-02 22:01:19,713] [INFO] [timer.py:215:stop] epoch=0/micro_step=2900/global_step=290, RunningAvgSamplesPerSec=1.3228986583030473, CurrSamplesPerSec=1.4292260657764353, MemAllocated=57.26GB, MaxMemAllocated=62.63GB
Epoch: [0][290/500]	Time  6.998 ( 6.998)	Loss 2.0168 (1.9711)	CeLoss 0.5703 (0.3869)	SegCLSLoss 0.0178 (0.0221)	KLLoss 0.0815 (0.0931)	MaskLoss 0.9632 (0.9527)	MaskBCELoss 0.1473 (0.2483)	MaskDICELoss 0.8159 (0.7044)
Epoch: [0][291/500]	Time  6.786 ( 6.786)	Loss 0.7070 (1.8622)	CeLoss 0.7070 (0.4586)	SegCLSLoss 0.0000 (0.0271)	KLLoss 0.0000 (0.0895)	MaskLoss 0.0000 (0.8086)	MaskBCELoss 0.0000 (0.2237)	MaskDICELoss 0.0000 (0.5849)
Epoch: [0][292/500]	Time  7.797 ( 7.797)	Loss 0.7461 (1.6039)	CeLoss 0.7461 (0.2817)	SegCLSLoss 0.0000 (0.0204)	KLLoss 0.0000 (0.0608)	MaskLoss 0.0000 (0.8625)	MaskBCELoss 0.0000 (0.2059)	MaskDICELoss 0.0000 (0.6566)
Epoch: [0][293/500]	Time  6.672 ( 6.672)	Loss 2.3282 (1.8268)	CeLoss 0.1318 (0.4761)	SegCLSLoss 0.0684 (0.0234)	KLLoss 0.2109 (0.0913)	MaskLoss 1.1349 (0.7670)	MaskBCELoss 0.1855 (0.2067)	MaskDICELoss 0.9494 (0.5603)
Epoch: [0][294/500]	Time  7.752 ( 7.752)	Loss 1.9719 (2.1247)	CeLoss 0.2275 (0.3337)	SegCLSLoss 0.0393 (0.0320)	KLLoss 0.1660 (0.1235)	MaskLoss 1.0148 (1.0155)	MaskBCELoss 0.0469 (0.2651)	MaskDICELoss 0.9679 (0.7504)
Epoch: [0][295/500]	Time  7.469 ( 7.469)	Loss 2.2838 (1.7687)	CeLoss 0.1582 (0.3458)	SegCLSLoss 0.0493 (0.0235)	KLLoss 0.1846 (0.0825)	MaskLoss 1.1369 (0.9118)	MaskBCELoss 0.2250 (0.1694)	MaskDICELoss 0.9119 (0.7424)
Epoch: [0][296/500]	Time  6.479 ( 6.479)	Loss 1.6442 (1.9533)	CeLoss 0.2207 (0.4453)	SegCLSLoss 0.0225 (0.0181)	KLLoss 0.0669 (0.0741)	MaskLoss 1.0509 (0.8499)	MaskBCELoss 0.0934 (0.3529)	MaskDICELoss 0.9575 (0.4970)
Epoch: [0][297/500]	Time  7.677 ( 7.677)	Loss 1.8020 (1.8652)	CeLoss 0.2031 (0.2956)	SegCLSLoss 0.0378 (0.0299)	KLLoss 0.1235 (0.1006)	MaskLoss 1.0309 (0.9746)	MaskBCELoss 0.0562 (0.1787)	MaskDICELoss 0.9746 (0.7960)
Epoch: [0][298/500]	Time  6.793 ( 6.793)	Loss 0.9336 (1.6057)	CeLoss 0.9336 (0.6530)	SegCLSLoss 0.0000 (0.0163)	KLLoss 0.0000 (0.0498)	MaskLoss 0.0000 (0.5804)	MaskBCELoss 0.0000 (0.1654)	MaskDICELoss 0.0000 (0.4150)
Epoch: [0][299/500]	Time  7.808 ( 7.808)	Loss 2.1320 (1.3988)	CeLoss 0.4199 (0.2115)	SegCLSLoss 0.0325 (0.0250)	KLLoss 0.1416 (0.0770)	MaskLoss 1.0158 (0.6979)	MaskBCELoss 0.1163 (0.1693)	MaskDICELoss 0.8995 (0.5286)
[2025-03-02 22:02:31,730] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=0, lr=[0.00026957142857142853], mom=[(0.9, 0.95)]
[2025-03-02 22:02:31,735] [INFO] [timer.py:215:stop] epoch=0/micro_step=3000/global_step=300, RunningAvgSamplesPerSec=1.3250061428679683, CurrSamplesPerSec=1.4732885519796415, MemAllocated=57.24GB, MaxMemAllocated=62.63GB
Epoch: [0][300/500]	Time  6.789 ( 6.789)	Loss 1.9554 (1.6262)	CeLoss 0.4004 (0.5330)	SegCLSLoss 0.0190 (0.0196)	KLLoss 0.0267 (0.0651)	MaskLoss 1.1570 (0.7120)	MaskBCELoss 0.2827 (0.1106)	MaskDICELoss 0.8743 (0.6014)
Epoch: [0][301/500]	Time  6.657 ( 6.657)	Loss 1.9019 (1.6824)	CeLoss 0.1982 (0.4678)	SegCLSLoss 0.0243 (0.0226)	KLLoss 0.0825 (0.0813)	MaskLoss 0.9916 (0.7439)	MaskBCELoss 0.3712 (0.1340)	MaskDICELoss 0.6204 (0.6098)
Epoch: [0][302/500]	Time  7.301 ( 7.301)	Loss 2.0464 (1.8271)	CeLoss 0.2812 (0.5222)	SegCLSLoss 0.0234 (0.0231)	KLLoss 0.1064 (0.0946)	MaskLoss 1.1566 (0.7631)	MaskBCELoss 0.1710 (0.1513)	MaskDICELoss 0.9856 (0.6118)
Epoch: [0][303/500]	Time  8.663 ( 8.663)	Loss 2.0145 (1.6227)	CeLoss 0.2188 (0.3348)	SegCLSLoss 0.0330 (0.0182)	KLLoss 0.1455 (0.0455)	MaskLoss 1.0379 (0.9144)	MaskBCELoss 0.1602 (0.1829)	MaskDICELoss 0.8777 (0.7315)
Epoch: [0][304/500]	Time  7.786 ( 7.786)	Loss 1.5613 (2.0389)	CeLoss 0.3008 (0.3893)	SegCLSLoss 0.0157 (0.0232)	KLLoss 0.0063 (0.0718)	MaskLoss 0.8905 (1.0626)	MaskBCELoss 0.3369 (0.2888)	MaskDICELoss 0.5536 (0.7738)
Epoch: [0][305/500]	Time  7.743 ( 7.743)	Loss 1.7685 (1.6476)	CeLoss 0.2988 (0.4676)	SegCLSLoss 0.0192 (0.0207)	KLLoss 0.0581 (0.0730)	MaskLoss 0.9860 (0.7243)	MaskBCELoss 0.2434 (0.1534)	MaskDICELoss 0.7426 (0.5710)
Epoch: [0][306/500]	Time  6.853 ( 6.853)	Loss 1.9338 (1.5968)	CeLoss 0.2500 (0.5622)	SegCLSLoss 0.0216 (0.0177)	KLLoss 0.0757 (0.0540)	MaskLoss 1.0078 (0.6417)	MaskBCELoss 0.3635 (0.1680)	MaskDICELoss 0.6443 (0.4737)
Epoch: [0][307/500]	Time  6.349 ( 6.349)	Loss 1.7866 (1.9627)	CeLoss 0.1758 (0.6667)	SegCLSLoss 0.0361 (0.0243)	KLLoss 0.1406 (0.0849)	MaskLoss 0.9903 (0.7433)	MaskBCELoss 0.0385 (0.2014)	MaskDICELoss 0.9519 (0.5420)
Epoch: [0][308/500]	Time  6.228 ( 6.228)	Loss 1.1641 (1.5482)	CeLoss 1.1641 (0.6216)	SegCLSLoss 0.0000 (0.0196)	KLLoss 0.0000 (0.0635)	MaskLoss 0.0000 (0.5364)	MaskBCELoss 0.0000 (0.1266)	MaskDICELoss 0.0000 (0.4098)
Epoch: [0][309/500]	Time  9.152 ( 9.152)	Loss 1.9220 (1.8487)	CeLoss 0.2197 (0.2479)	SegCLSLoss 0.0220 (0.0278)	KLLoss 0.0684 (0.1012)	MaskLoss 1.0907 (0.9929)	MaskBCELoss 0.3274 (0.1902)	MaskDICELoss 0.7632 (0.8027)
[2025-03-02 22:03:46,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=0, lr=[0.00026834693877551016], mom=[(0.9, 0.95)]
[2025-03-02 22:03:46,390] [INFO] [timer.py:215:stop] epoch=0/micro_step=3100/global_step=310, RunningAvgSamplesPerSec=1.325478974512699, CurrSamplesPerSec=1.2623369520612477, MemAllocated=57.57GB, MaxMemAllocated=62.63GB
Epoch: [0][310/500]	Time  7.923 ( 7.923)	Loss 1.7159 (1.5485)	CeLoss 0.2275 (0.3230)	SegCLSLoss 0.0280 (0.0223)	KLLoss 0.0889 (0.0734)	MaskLoss 1.0402 (0.7758)	MaskBCELoss 0.0781 (0.1450)	MaskDICELoss 0.9622 (0.6309)
Epoch: [0][311/500]	Time  7.037 ( 7.037)	Loss 2.4134 (2.0572)	CeLoss 0.2734 (0.5081)	SegCLSLoss 0.0305 (0.0315)	KLLoss 0.1553 (0.1225)	MaskLoss 1.2132 (0.8392)	MaskBCELoss 0.2900 (0.2032)	MaskDICELoss 0.9232 (0.6360)
Epoch: [0][312/500]	Time  6.853 ( 6.853)	Loss 0.9297 (1.8241)	CeLoss 0.9297 (0.3897)	SegCLSLoss 0.0000 (0.0290)	KLLoss 0.0000 (0.0877)	MaskLoss 0.0000 (0.8735)	MaskBCELoss 0.0000 (0.1954)	MaskDICELoss 0.0000 (0.6780)
Epoch: [0][313/500]	Time  6.365 ( 6.365)	Loss 4.6339 (1.9578)	CeLoss 0.2656 (0.5332)	SegCLSLoss 0.0297 (0.0278)	KLLoss 0.1914 (0.1028)	MaskLoss 2.0664 (0.7577)	MaskBCELoss 1.5207 (0.2419)	MaskDICELoss 0.5457 (0.5159)
Epoch: [0][314/500]	Time  6.994 ( 6.994)	Loss 2.3211 (1.9013)	CeLoss 0.2363 (0.6311)	SegCLSLoss 0.0317 (0.0222)	KLLoss 0.1279 (0.0765)	MaskLoss 1.2196 (0.7810)	MaskBCELoss 0.3359 (0.1718)	MaskDICELoss 0.8838 (0.6093)
Epoch: [0][315/500]	Time  7.669 ( 7.669)	Loss 0.7617 (1.8316)	CeLoss 0.7617 (0.5203)	SegCLSLoss 0.0000 (0.0262)	KLLoss 0.0000 (0.0996)	MaskLoss 0.0000 (0.7037)	MaskBCELoss 0.0000 (0.1963)	MaskDICELoss 0.0000 (0.5074)
Epoch: [0][316/500]	Time  6.669 ( 6.669)	Loss 0.7969 (1.9011)	CeLoss 0.7969 (0.6120)	SegCLSLoss 0.0000 (0.0283)	KLLoss 0.0000 (0.0907)	MaskLoss 0.0000 (0.7136)	MaskBCELoss 0.0000 (0.1984)	MaskDICELoss 0.0000 (0.5152)
Epoch: [0][317/500]	Time  6.876 ( 6.876)	Loss 0.8438 (1.5441)	CeLoss 0.8438 (0.5118)	SegCLSLoss 0.0000 (0.0223)	KLLoss 0.0000 (0.0705)	MaskLoss 0.0000 (0.6333)	MaskBCELoss 0.0000 (0.1058)	MaskDICELoss 0.0000 (0.5275)
Epoch: [0][318/500]	Time  8.233 ( 8.233)	Loss 1.2234 (1.7530)	CeLoss 0.2734 (0.2527)	SegCLSLoss 0.0150 (0.0326)	KLLoss 0.0067 (0.0809)	MaskLoss 0.6436 (0.9239)	MaskBCELoss 0.2712 (0.2366)	MaskDICELoss 0.3723 (0.6873)
Epoch: [0][319/500]	Time  6.763 ( 6.763)	Loss 1.1172 (1.3000)	CeLoss 1.1172 (0.3902)	SegCLSLoss 0.0000 (0.0159)	KLLoss 0.0000 (0.0535)	MaskLoss 0.0000 (0.6181)	MaskBCELoss 0.0000 (0.0697)	MaskDICELoss 0.0000 (0.5484)
[2025-03-02 22:04:56,023] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=0, lr=[0.0002671224489795918], mom=[(0.9, 0.95)]
[2025-03-02 22:04:56,028] [INFO] [timer.py:215:stop] epoch=0/micro_step=3200/global_step=320, RunningAvgSamplesPerSec=1.3287019354140768, CurrSamplesPerSec=1.6186381355226767, MemAllocated=56.7GB, MaxMemAllocated=62.63GB
Epoch: [0][320/500]	Time  6.180 ( 6.180)	Loss 1.1172 (1.3769)	CeLoss 1.1172 (0.7351)	SegCLSLoss 0.0000 (0.0109)	KLLoss 0.0000 (0.0349)	MaskLoss 0.0000 (0.4100)	MaskBCELoss 0.0000 (0.0871)	MaskDICELoss 0.0000 (0.3229)
Epoch: [0][321/500]	Time  8.219 ( 8.219)	Loss 1.9107 (1.8261)	CeLoss 0.2598 (0.3606)	SegCLSLoss 0.0280 (0.0261)	KLLoss 0.1240 (0.0918)	MaskLoss 1.0498 (0.9292)	MaskBCELoss 0.0913 (0.1566)	MaskDICELoss 0.9586 (0.7726)
Epoch: [0][322/500]	Time  6.632 ( 6.632)	Loss 2.2348 (1.4833)	CeLoss 0.2168 (0.6914)	SegCLSLoss 0.0221 (0.0110)	KLLoss 0.0918 (0.0316)	MaskLoss 1.2282 (0.5312)	MaskBCELoss 0.4129 (0.1288)	MaskDICELoss 0.8153 (0.4025)
Epoch: [0][323/500]	Time  8.451 ( 8.451)	Loss 0.7539 (1.6178)	CeLoss 0.7539 (0.4188)	SegCLSLoss 0.0000 (0.0238)	KLLoss 0.0000 (0.0984)	MaskLoss 0.0000 (0.7016)	MaskBCELoss 0.0000 (0.0923)	MaskDICELoss 0.0000 (0.6093)
Epoch: [0][324/500]	Time  8.176 ( 8.176)	Loss 2.0294 (2.1248)	CeLoss 0.2676 (0.2816)	SegCLSLoss 0.0259 (0.0343)	KLLoss 0.1240 (0.1225)	MaskLoss 1.0440 (1.0819)	MaskBCELoss 0.2080 (0.2539)	MaskDICELoss 0.8360 (0.8280)
Epoch: [0][325/500]	Time  6.416 ( 6.416)	Loss 2.7770 (1.8989)	CeLoss 0.1807 (0.5340)	SegCLSLoss 0.0850 (0.0296)	KLLoss 0.2490 (0.1059)	MaskLoss 1.2348 (0.7221)	MaskBCELoss 0.3235 (0.2042)	MaskDICELoss 0.9113 (0.5179)
Epoch: [0][326/500]	Time  7.864 ( 7.864)	Loss 2.1112 (1.8860)	CeLoss 0.1924 (0.2820)	SegCLSLoss 0.0544 (0.0291)	KLLoss 0.2119 (0.1160)	MaskLoss 1.0110 (0.9309)	MaskBCELoss 0.0299 (0.1937)	MaskDICELoss 0.9810 (0.7372)
Epoch: [0][327/500]	Time  7.147 ( 7.147)	Loss 1.0547 (1.7179)	CeLoss 1.0547 (0.5198)	SegCLSLoss 0.0000 (0.0272)	KLLoss 0.0000 (0.0920)	MaskLoss 0.0000 (0.6555)	MaskBCELoss 0.0000 (0.1608)	MaskDICELoss 0.0000 (0.4947)
Epoch: [0][328/500]	Time  8.139 ( 8.139)	Loss 1.5442 (1.7355)	CeLoss 0.2949 (0.4465)	SegCLSLoss 0.0160 (0.0272)	KLLoss 0.0069 (0.0963)	MaskLoss 1.0278 (0.7549)	MaskBCELoss 0.1863 (0.1355)	MaskDICELoss 0.8415 (0.6193)
Epoch: [0][329/500]	Time  7.695 ( 7.695)	Loss 1.7974 (1.6780)	CeLoss 0.2715 (0.5021)	SegCLSLoss 0.0299 (0.0245)	KLLoss 0.1240 (0.0945)	MaskLoss 0.9983 (0.7376)	MaskBCELoss 0.0179 (0.0479)	MaskDICELoss 0.9804 (0.6897)
[2025-03-02 22:06:12,392] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=0, lr=[0.0002658979591836734], mom=[(0.9, 0.95)]
[2025-03-02 22:06:12,397] [INFO] [timer.py:215:stop] epoch=0/micro_step=3300/global_step=330, RunningAvgSamplesPerSec=1.3281131135856077, CurrSamplesPerSec=1.311023624877624, MemAllocated=57.27GB, MaxMemAllocated=63.16GB
Epoch: [0][330/500]	Time  7.629 ( 7.629)	Loss 2.1465 (1.7105)	CeLoss 0.2617 (0.4156)	SegCLSLoss 0.0251 (0.0280)	KLLoss 0.0669 (0.0867)	MaskLoss 1.2049 (0.7793)	MaskBCELoss 0.4025 (0.1550)	MaskDICELoss 0.8024 (0.6243)
Epoch: [0][331/500]	Time  7.217 ( 7.217)	Loss 1.9083 (1.8391)	CeLoss 0.2451 (0.2923)	SegCLSLoss 0.0376 (0.0332)	KLLoss 0.1377 (0.1245)	MaskLoss 1.0409 (0.9593)	MaskBCELoss 0.0549 (0.0734)	MaskDICELoss 0.9860 (0.8859)
Epoch: [0][332/500]	Time  7.358 ( 7.358)	Loss 2.0402 (1.7569)	CeLoss 0.3145 (0.6623)	SegCLSLoss 0.0150 (0.0187)	KLLoss 0.0082 (0.0616)	MaskLoss 1.2350 (0.6621)	MaskBCELoss 0.4497 (0.1767)	MaskDICELoss 0.7852 (0.4854)
Epoch: [0][333/500]	Time  7.487 ( 7.487)	Loss 1.7932 (1.8846)	CeLoss 0.1631 (0.4143)	SegCLSLoss 0.0344 (0.0230)	KLLoss 0.1465 (0.0784)	MaskLoss 1.0078 (0.9675)	MaskBCELoss 0.0198 (0.1782)	MaskDICELoss 0.9881 (0.7893)
Epoch: [0][334/500]	Time  7.183 ( 7.183)	Loss 1.3682 (1.4585)	CeLoss 0.2314 (0.5491)	SegCLSLoss 0.0172 (0.0158)	KLLoss 0.0417 (0.0523)	MaskLoss 0.8882 (0.6120)	MaskBCELoss 0.0737 (0.0800)	MaskDICELoss 0.8145 (0.5320)
Epoch: [0][335/500]	Time  7.623 ( 7.623)	Loss 1.9025 (2.0704)	CeLoss 0.2139 (0.3425)	SegCLSLoss 0.0374 (0.0311)	KLLoss 0.1299 (0.1009)	MaskLoss 1.0733 (1.0603)	MaskBCELoss 0.0792 (0.2492)	MaskDICELoss 0.9941 (0.8111)
Epoch: [0][336/500]	Time  8.272 ( 8.272)	Loss 2.0394 (1.9655)	CeLoss 0.2578 (0.3704)	SegCLSLoss 0.0347 (0.0275)	KLLoss 0.1387 (0.1019)	MaskLoss 1.0930 (0.9350)	MaskBCELoss 0.1182 (0.2386)	MaskDICELoss 0.9748 (0.6964)
Epoch: [0][337/500]	Time  6.315 ( 6.315)	Loss 2.0365 (1.6812)	CeLoss 0.2910 (0.5319)	SegCLSLoss 0.0214 (0.0226)	KLLoss 0.0967 (0.0764)	MaskLoss 1.0706 (0.6473)	MaskBCELoss 0.2784 (0.1851)	MaskDICELoss 0.7923 (0.4622)
Epoch: [0][338/500]	Time  7.240 ( 7.240)	Loss 2.6285 (1.5713)	CeLoss 0.4199 (0.5843)	SegCLSLoss 0.0277 (0.0138)	KLLoss 0.1465 (0.0492)	MaskLoss 1.2049 (0.6336)	MaskBCELoss 0.4080 (0.1503)	MaskDICELoss 0.7968 (0.4833)
Epoch: [0][339/500]	Time  7.246 ( 7.246)	Loss 2.4991 (1.7564)	CeLoss 0.1660 (0.3706)	SegCLSLoss 0.0598 (0.0194)	KLLoss 0.2041 (0.0709)	MaskLoss 1.1513 (0.8459)	MaskBCELoss 0.3321 (0.2465)	MaskDICELoss 0.8192 (0.5994)
[2025-03-02 22:07:25,215] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=0, lr=[0.0002646734693877551], mom=[(0.9, 0.95)]
[2025-03-02 22:07:25,220] [INFO] [timer.py:215:stop] epoch=0/micro_step=3400/global_step=340, RunningAvgSamplesPerSec=1.3294113886762535, CurrSamplesPerSec=1.453638563753928, MemAllocated=57.27GB, MaxMemAllocated=63.16GB
Epoch: [0][340/500]	Time  6.881 ( 6.881)	Loss 2.0833 (1.5673)	CeLoss 0.1934 (0.5161)	SegCLSLoss 0.0396 (0.0211)	KLLoss 0.1719 (0.0742)	MaskLoss 1.0879 (0.6391)	MaskBCELoss 0.0969 (0.1046)	MaskDICELoss 0.9909 (0.5344)
Epoch: [0][341/500]	Time  8.724 ( 8.724)	Loss 0.0781 (1.5861)	CeLoss 0.0781 (0.3536)	SegCLSLoss 0.0000 (0.0226)	KLLoss 0.0000 (0.0651)	MaskLoss 0.0000 (0.7550)	MaskBCELoss 0.0000 (0.2052)	MaskDICELoss 0.0000 (0.5498)
Epoch: [0][342/500]	Time  8.327 ( 8.327)	Loss 2.2056 (2.1184)	CeLoss 0.2217 (0.2975)	SegCLSLoss 0.0405 (0.0334)	KLLoss 0.1787 (0.1129)	MaskLoss 1.0085 (1.0402)	MaskBCELoss 0.2401 (0.3129)	MaskDICELoss 0.7684 (0.7273)
Epoch: [0][343/500]	Time  7.364 ( 7.364)	Loss 2.0427 (1.9466)	CeLoss 0.2266 (0.4505)	SegCLSLoss 0.0342 (0.0255)	KLLoss 0.1504 (0.0953)	MaskLoss 1.0839 (0.8699)	MaskBCELoss 0.1150 (0.2326)	MaskDICELoss 0.9689 (0.6373)
Epoch: [0][344/500]	Time  7.030 ( 7.030)	Loss 3.6685 (1.8877)	CeLoss 0.2500 (0.4148)	SegCLSLoss 0.0598 (0.0247)	KLLoss 0.2090 (0.0944)	MaskLoss 1.6756 (0.8359)	MaskBCELoss 0.8757 (0.2469)	MaskDICELoss 0.7999 (0.5889)
Epoch: [0][345/500]	Time  8.436 ( 8.436)	Loss 1.5411 (1.7932)	CeLoss 0.2314 (0.2676)	SegCLSLoss 0.0192 (0.0279)	KLLoss 0.0435 (0.0957)	MaskLoss 0.8790 (0.9464)	MaskBCELoss 0.2480 (0.1825)	MaskDICELoss 0.6309 (0.7639)
Epoch: [0][346/500]	Time  7.422 ( 7.422)	Loss 2.5073 (1.4140)	CeLoss 0.2637 (0.4453)	SegCLSLoss 0.0410 (0.0157)	KLLoss 0.1797 (0.0517)	MaskLoss 1.0860 (0.5835)	MaskBCELoss 0.4213 (0.1713)	MaskDICELoss 0.6646 (0.4122)
Epoch: [0][347/500]	Time  7.878 ( 7.878)	Loss 2.7596 (1.6280)	CeLoss 0.3457 (0.3916)	SegCLSLoss 0.0398 (0.0225)	KLLoss 0.1934 (0.0887)	MaskLoss 1.2417 (0.7281)	MaskBCELoss 0.3773 (0.1413)	MaskDICELoss 0.8644 (0.5868)
Epoch: [0][348/500]	Time  6.820 ( 6.820)	Loss 1.7346 (1.6366)	CeLoss 0.2021 (0.5499)	SegCLSLoss 0.0270 (0.0165)	KLLoss 0.1001 (0.0592)	MaskLoss 1.0225 (0.6751)	MaskBCELoss 0.0949 (0.1664)	MaskDICELoss 0.9276 (0.5087)
Epoch: [0][349/500]	Time  9.072 ( 9.072)	Loss 1.7728 (1.8296)	CeLoss 0.2578 (0.2673)	SegCLSLoss 0.0210 (0.0241)	KLLoss 0.0923 (0.0888)	MaskLoss 0.9855 (1.0149)	MaskBCELoss 0.1506 (0.1800)	MaskDICELoss 0.8349 (0.8349)
[2025-03-02 22:08:45,159] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=0, lr=[0.0002634489795918367], mom=[(0.9, 0.95)]
[2025-03-02 22:08:45,164] [INFO] [timer.py:215:stop] epoch=0/micro_step=3500/global_step=350, RunningAvgSamplesPerSec=1.3270241732710832, CurrSamplesPerSec=1.1276641925005335, MemAllocated=57.61GB, MaxMemAllocated=63.16GB
Epoch: [0][350/500]	Time  8.870 ( 8.870)	Loss 2.1835 (1.7237)	CeLoss 0.2168 (0.3842)	SegCLSLoss 0.0339 (0.0292)	KLLoss 0.1357 (0.1058)	MaskLoss 1.1473 (0.8039)	MaskBCELoss 0.2589 (0.0973)	MaskDICELoss 0.8884 (0.7065)
Epoch: [0][351/500]	Time  6.654 ( 6.654)	Loss 2.2583 (1.6442)	CeLoss 0.1768 (0.5368)	SegCLSLoss 0.0583 (0.0215)	KLLoss 0.1670 (0.0758)	MaskLoss 1.1567 (0.6657)	MaskBCELoss 0.2266 (0.1273)	MaskDICELoss 0.9301 (0.5384)
Epoch: [0][352/500]	Time  6.148 ( 6.148)	Loss 1.8134 (1.4852)	CeLoss 0.2754 (0.7188)	SegCLSLoss 0.0181 (0.0140)	KLLoss 0.0347 (0.0618)	MaskLoss 0.9719 (0.4308)	MaskBCELoss 0.4177 (0.0816)	MaskDICELoss 0.5543 (0.3492)
Epoch: [0][353/500]	Time  7.228 ( 7.228)	Loss 1.3828 (1.9235)	CeLoss 1.3828 (0.3460)	SegCLSLoss 0.0000 (0.0251)	KLLoss 0.0000 (0.0874)	MaskLoss 0.0000 (0.9180)	MaskBCELoss 0.0000 (0.2967)	MaskDICELoss 0.0000 (0.6213)
Epoch: [0][354/500]	Time  8.533 ( 8.533)	Loss 2.1818 (1.8761)	CeLoss 0.2373 (0.3007)	SegCLSLoss 0.0317 (0.0272)	KLLoss 0.1172 (0.0958)	MaskLoss 1.1932 (0.9794)	MaskBCELoss 0.2659 (0.1994)	MaskDICELoss 0.9273 (0.7800)
Epoch: [0][355/500]	Time  7.622 ( 7.622)	Loss 2.4152 (1.8478)	CeLoss 0.2002 (0.5207)	SegCLSLoss 0.0474 (0.0225)	KLLoss 0.1797 (0.0840)	MaskLoss 1.1929 (0.8309)	MaskBCELoss 0.2809 (0.1493)	MaskDICELoss 0.9120 (0.6816)
Epoch: [0][356/500]	Time  7.208 ( 7.208)	Loss 3.0521 (1.6773)	CeLoss 0.2021 (0.4383)	SegCLSLoss 0.0275 (0.0222)	KLLoss 0.1367 (0.0842)	MaskLoss 1.6105 (0.7452)	MaskBCELoss 0.6799 (0.1459)	MaskDICELoss 0.9306 (0.5992)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([25, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][357/500]	Time  8.059 ( 8.059)	Loss 2.2794 (1.8181)	CeLoss 0.3730 (0.3249)	SegCLSLoss 0.0223 (0.0291)	KLLoss 0.0918 (0.0867)	MaskLoss 1.0667 (0.8929)	MaskBCELoss 0.4627 (0.2389)	MaskDICELoss 0.6039 (0.6540)
Epoch: [0][358/500]	Time  8.217 ( 8.217)	Loss 1.7765 (2.1408)	CeLoss 0.2354 (0.4688)	SegCLSLoss 0.0347 (0.0356)	KLLoss 0.1235 (0.1263)	MaskLoss 1.0141 (0.9238)	MaskBCELoss 0.0163 (0.2256)	MaskDICELoss 0.9978 (0.6982)
Epoch: [0][359/500]	Time  7.167 ( 7.167)	Loss 1.8240 (1.6644)	CeLoss 0.1699 (0.4861)	SegCLSLoss 0.0325 (0.0255)	KLLoss 0.1299 (0.0919)	MaskLoss 1.0559 (0.7322)	MaskBCELoss 0.0611 (0.0657)	MaskDICELoss 0.9949 (0.6665)
[2025-03-02 22:10:00,782] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=0, lr=[0.00026222448979591837], mom=[(0.9, 0.95)]
[2025-03-02 22:10:00,787] [INFO] [timer.py:215:stop] epoch=0/micro_step=3600/global_step=360, RunningAvgSamplesPerSec=1.3268996262265993, CurrSamplesPerSec=1.138452962161234, MemAllocated=57.24GB, MaxMemAllocated=63.16GB
Epoch: [0][360/500]	Time  8.786 ( 8.786)	Loss 1.6711 (1.6854)	CeLoss 0.3770 (0.3245)	SegCLSLoss 0.0153 (0.0266)	KLLoss 0.0065 (0.0859)	MaskLoss 0.9763 (0.8439)	MaskBCELoss 0.2846 (0.1602)	MaskDICELoss 0.6916 (0.6837)
Epoch: [0][361/500]	Time  6.850 ( 6.850)	Loss 1.7982 (2.0105)	CeLoss 0.2471 (0.4929)	SegCLSLoss 0.0294 (0.0294)	KLLoss 0.1309 (0.1157)	MaskLoss 0.9970 (0.8719)	MaskBCELoss 0.0161 (0.1683)	MaskDICELoss 0.9809 (0.7035)
Epoch: [0][362/500]	Time  6.963 ( 6.963)	Loss 2.5004 (1.8040)	CeLoss 0.1982 (0.4943)	SegCLSLoss 0.0381 (0.0217)	KLLoss 0.1660 (0.0799)	MaskLoss 1.2775 (0.8017)	MaskBCELoss 0.3401 (0.1769)	MaskDICELoss 0.9375 (0.6248)
Epoch: [0][363/500]	Time  6.321 ( 6.321)	Loss 1.0859 (1.5873)	CeLoss 1.0859 (0.4974)	SegCLSLoss 0.0000 (0.0235)	KLLoss 0.0000 (0.0766)	MaskLoss 0.0000 (0.6192)	MaskBCELoss 0.0000 (0.1530)	MaskDICELoss 0.0000 (0.4662)
Epoch: [0][364/500]	Time  7.902 ( 7.902)	Loss 2.2226 (1.7145)	CeLoss 0.2031 (0.4115)	SegCLSLoss 0.0208 (0.0260)	KLLoss 0.0835 (0.0981)	MaskLoss 1.2898 (0.7976)	MaskBCELoss 0.3859 (0.1001)	MaskDICELoss 0.9040 (0.6975)
Epoch: [0][365/500]	Time  7.917 ( 7.917)	Loss 2.0413 (2.1023)	CeLoss 0.1885 (0.2962)	SegCLSLoss 0.0369 (0.0378)	KLLoss 0.1738 (0.1276)	MaskLoss 0.9922 (1.0157)	MaskBCELoss 0.1468 (0.2610)	MaskDICELoss 0.8454 (0.7547)
Epoch: [0][366/500]	Time  5.459 ( 5.459)	Loss 2.1362 (1.4281)	CeLoss 0.1934 (0.7467)	SegCLSLoss 0.0459 (0.0130)	KLLoss 0.1768 (0.0487)	MaskLoss 1.0677 (0.4053)	MaskBCELoss 0.1467 (0.0752)	MaskDICELoss 0.9209 (0.3301)
Epoch: [0][367/500]	Time  7.179 ( 7.179)	Loss 1.6210 (1.5861)	CeLoss 0.3242 (0.3906)	SegCLSLoss 0.0152 (0.0229)	KLLoss 0.0066 (0.0730)	MaskLoss 0.9189 (0.6993)	MaskBCELoss 0.3427 (0.1922)	MaskDICELoss 0.5762 (0.5071)
Epoch: [0][368/500]	Time  7.261 ( 7.261)	Loss 1.8827 (1.8513)	CeLoss 0.2061 (0.4514)	SegCLSLoss 0.0354 (0.0231)	KLLoss 0.1475 (0.0862)	MaskLoss 1.0309 (0.8485)	MaskBCELoss 0.0393 (0.1958)	MaskDICELoss 0.9916 (0.6526)
Epoch: [0][369/500]	Time  8.690 ( 8.690)	Loss 1.9566 (1.7797)	CeLoss 0.3047 (0.3789)	SegCLSLoss 0.0139 (0.0260)	KLLoss 0.0052 (0.0975)	MaskLoss 1.2309 (0.8625)	MaskBCELoss 0.3917 (0.1353)	MaskDICELoss 0.8392 (0.7272)
[2025-03-02 22:11:13,474] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=0, lr=[0.000261], mom=[(0.9, 0.95)]
[2025-03-02 22:11:13,479] [INFO] [timer.py:215:stop] epoch=0/micro_step=3700/global_step=370, RunningAvgSamplesPerSec=1.328185530205546, CurrSamplesPerSec=1.2273529341104679, MemAllocated=57.25GB, MaxMemAllocated=63.16GB
Epoch: [0][370/500]	Time  8.149 ( 8.149)	Loss 1.9673 (1.7493)	CeLoss 0.2637 (0.3872)	SegCLSLoss 0.0400 (0.0286)	KLLoss 0.1611 (0.1040)	MaskLoss 1.0121 (0.8208)	MaskBCELoss 0.0256 (0.1109)	MaskDICELoss 0.9865 (0.7099)
Epoch: [0][371/500]	Time  6.875 ( 6.875)	Loss 1.1406 (1.9278)	CeLoss 1.1406 (0.5104)	SegCLSLoss 0.0000 (0.0285)	KLLoss 0.0000 (0.1008)	MaskLoss 0.0000 (0.8179)	MaskBCELoss 0.0000 (0.1827)	MaskDICELoss 0.0000 (0.6352)
Epoch: [0][372/500]	Time  7.229 ( 7.229)	Loss 1.3387 (1.6838)	CeLoss 0.4141 (0.3659)	SegCLSLoss 0.0159 (0.0260)	KLLoss 0.0062 (0.0873)	MaskLoss 0.6863 (0.7749)	MaskBCELoss 0.2051 (0.1813)	MaskDICELoss 0.4812 (0.5936)
Epoch: [0][373/500]	Time  6.758 ( 6.758)	Loss 1.6558 (1.8843)	CeLoss 0.2832 (0.6552)	SegCLSLoss 0.0178 (0.0221)	KLLoss 0.0337 (0.0685)	MaskLoss 1.0081 (0.7199)	MaskBCELoss 0.2200 (0.2247)	MaskDICELoss 0.7881 (0.4952)
Epoch: [0][374/500]	Time  7.947 ( 7.947)	Loss 1.2701 (1.6979)	CeLoss 0.4062 (0.3348)	SegCLSLoss 0.0143 (0.0263)	KLLoss 0.0056 (0.0933)	MaskLoss 0.7550 (0.8334)	MaskBCELoss 0.0795 (0.1447)	MaskDICELoss 0.6755 (0.6887)
Epoch: [0][375/500]	Time  7.187 ( 7.187)	Loss 3.6467 (1.9873)	CeLoss 0.1396 (0.4567)	SegCLSLoss 0.0996 (0.0279)	KLLoss 0.2227 (0.0893)	MaskLoss 1.6727 (0.9161)	MaskBCELoss 0.8959 (0.2432)	MaskDICELoss 0.7768 (0.6729)
Epoch: [0][376/500]	Time  6.873 ( 6.873)	Loss 2.3101 (1.8902)	CeLoss 0.2148 (0.6463)	SegCLSLoss 0.0206 (0.0188)	KLLoss 0.1045 (0.0872)	MaskLoss 1.2498 (0.7078)	MaskBCELoss 0.4158 (0.1785)	MaskDICELoss 0.8340 (0.5293)
Epoch: [0][377/500]	Time  6.534 ( 6.534)	Loss 1.0312 (1.5042)	CeLoss 1.0312 (0.6752)	SegCLSLoss 0.0000 (0.0187)	KLLoss 0.0000 (0.0643)	MaskLoss 0.0000 (0.4647)	MaskBCELoss 0.0000 (0.0977)	MaskDICELoss 0.0000 (0.3670)
Epoch: [0][378/500]	Time  7.272 ( 7.272)	Loss 2.4970 (1.6653)	CeLoss 0.2109 (0.3645)	SegCLSLoss 0.0469 (0.0217)	KLLoss 0.1836 (0.0702)	MaskLoss 1.2469 (0.8335)	MaskBCELoss 0.2813 (0.1755)	MaskDICELoss 0.9656 (0.6580)
Epoch: [0][379/500]	Time  7.605 ( 7.605)	Loss 2.0261 (1.7993)	CeLoss 0.2676 (0.3958)	SegCLSLoss 0.0200 (0.0220)	KLLoss 0.0835 (0.0900)	MaskLoss 0.9491 (0.8542)	MaskBCELoss 0.4677 (0.1775)	MaskDICELoss 0.4814 (0.6766)
[2025-03-02 22:12:26,112] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=0, lr=[0.0002597755102040816], mom=[(0.9, 0.95)]
[2025-03-02 22:12:26,117] [INFO] [timer.py:215:stop] epoch=0/micro_step=3800/global_step=380, RunningAvgSamplesPerSec=1.3294305845351126, CurrSamplesPerSec=1.196573130567021, MemAllocated=57.24GB, MaxMemAllocated=63.16GB
Epoch: [0][380/500]	Time  8.359 ( 8.359)	Loss 1.5165 (1.6165)	CeLoss 0.3281 (0.3484)	SegCLSLoss 0.0201 (0.0223)	KLLoss 0.0649 (0.0792)	MaskLoss 0.7492 (0.8197)	MaskBCELoss 0.1696 (0.1201)	MaskDICELoss 0.5796 (0.6997)
Epoch: [0][381/500]	Time  5.902 ( 5.902)	Loss 1.5599 (1.5246)	CeLoss 0.4043 (0.5559)	SegCLSLoss 0.0187 (0.0184)	KLLoss 0.0586 (0.0734)	MaskLoss 0.7243 (0.5336)	MaskBCELoss 0.1872 (0.1324)	MaskDICELoss 0.5371 (0.4012)
Epoch: [0][382/500]	Time  7.738 ( 7.738)	Loss 0.1494 (1.1360)	CeLoss 0.1494 (0.5357)	SegCLSLoss 0.0000 (0.0121)	KLLoss 0.0000 (0.0468)	MaskLoss 0.0000 (0.3367)	MaskBCELoss 0.0000 (0.0708)	MaskDICELoss 0.0000 (0.2660)
Epoch: [0][383/500]	Time  7.718 ( 7.718)	Loss 0.0991 (1.6341)	CeLoss 0.0991 (0.2958)	SegCLSLoss 0.0000 (0.0254)	KLLoss 0.0000 (0.0951)	MaskLoss 0.0000 (0.7798)	MaskBCELoss 0.0000 (0.1648)	MaskDICELoss 0.0000 (0.6150)
Epoch: [0][384/500]	Time  6.461 ( 6.461)	Loss 1.5689 (1.8707)	CeLoss 0.2578 (0.3817)	SegCLSLoss 0.0262 (0.0326)	KLLoss 0.0654 (0.0969)	MaskLoss 1.0099 (0.8973)	MaskBCELoss 0.0277 (0.1871)	MaskDICELoss 0.9822 (0.7103)
Epoch: [0][385/500]	Time  8.245 ( 8.245)	Loss 1.9292 (1.6127)	CeLoss 0.1846 (0.4689)	SegCLSLoss 0.0413 (0.0194)	KLLoss 0.1611 (0.0631)	MaskLoss 0.9731 (0.7267)	MaskBCELoss 0.1045 (0.1554)	MaskDICELoss 0.8687 (0.5714)
Epoch: [0][386/500]	Time  7.228 ( 7.228)	Loss 2.0342 (1.7759)	CeLoss 0.2236 (0.4660)	SegCLSLoss 0.0229 (0.0201)	KLLoss 0.0864 (0.0747)	MaskLoss 1.0366 (0.7880)	MaskBCELoss 0.4156 (0.2127)	MaskDICELoss 0.6211 (0.5752)
Epoch: [0][387/500]	Time  7.580 ( 7.580)	Loss 1.2422 (1.5743)	CeLoss 1.2422 (0.3791)	SegCLSLoss 0.0000 (0.0240)	KLLoss 0.0000 (0.0801)	MaskLoss 0.0000 (0.7303)	MaskBCELoss 0.0000 (0.1326)	MaskDICELoss 0.0000 (0.5977)
Epoch: [0][388/500]	Time  7.733 ( 7.733)	Loss 2.3558 (1.3875)	CeLoss 0.3223 (0.2343)	SegCLSLoss 0.0332 (0.0231)	KLLoss 0.1328 (0.0854)	MaskLoss 1.1580 (0.6606)	MaskBCELoss 0.3267 (0.1393)	MaskDICELoss 0.8313 (0.5213)
Epoch: [0][389/500]	Time  7.837 ( 7.837)	Loss 0.9727 (1.6820)	CeLoss 0.9727 (0.4545)	SegCLSLoss 0.0000 (0.0226)	KLLoss 0.0000 (0.0813)	MaskLoss 0.0000 (0.7333)	MaskBCELoss 0.0000 (0.1579)	MaskDICELoss 0.0000 (0.5754)
[2025-03-02 22:13:38,781] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=0, lr=[0.00025855102040816326], mom=[(0.9, 0.95)]
[2025-03-02 22:13:38,787] [INFO] [timer.py:215:stop] epoch=0/micro_step=3900/global_step=390, RunningAvgSamplesPerSec=1.3305992683825225, CurrSamplesPerSec=1.6062852749664185, MemAllocated=56.72GB, MaxMemAllocated=63.16GB
Epoch: [0][390/500]	Time  6.227 ( 6.227)	Loss 1.5156 (1.4361)	CeLoss 1.5156 (0.3936)	SegCLSLoss 0.0000 (0.0153)	KLLoss 0.0000 (0.0560)	MaskLoss 0.0000 (0.6363)	MaskBCELoss 0.0000 (0.1750)	MaskDICELoss 0.0000 (0.4613)
Epoch: [0][391/500]	Time  7.793 ( 7.793)	Loss 0.1885 (1.5560)	CeLoss 0.1885 (0.4933)	SegCLSLoss 0.0000 (0.0176)	KLLoss 0.0000 (0.0675)	MaskLoss 0.0000 (0.6790)	MaskBCELoss 0.0000 (0.1046)	MaskDICELoss 0.0000 (0.5744)
Epoch: [0][392/500]	Time  6.491 ( 6.491)	Loss 1.8446 (1.6723)	CeLoss 0.2520 (0.6206)	SegCLSLoss 0.0256 (0.0188)	KLLoss 0.1299 (0.0707)	MaskLoss 1.0259 (0.6758)	MaskBCELoss 0.0335 (0.0833)	MaskDICELoss 0.9923 (0.5925)
Epoch: [0][393/500]	Time  8.096 ( 8.096)	Loss 2.2072 (1.9275)	CeLoss 0.2344 (0.4810)	SegCLSLoss 0.0493 (0.0300)	KLLoss 0.2109 (0.1135)	MaskLoss 1.0519 (0.8098)	MaskBCELoss 0.0537 (0.1681)	MaskDICELoss 0.9982 (0.6418)
Epoch: [0][394/500]	Time  7.834 ( 7.834)	Loss 1.8781 (1.9158)	CeLoss 0.2559 (0.4680)	SegCLSLoss 0.0254 (0.0333)	KLLoss 0.1025 (0.1189)	MaskLoss 1.0915 (0.7865)	MaskBCELoss 0.1069 (0.1688)	MaskDICELoss 0.9846 (0.6176)
Epoch: [0][395/500]	Time  6.793 ( 6.793)	Loss 2.0269 (1.5366)	CeLoss 0.1836 (0.5492)	SegCLSLoss 0.0579 (0.0188)	KLLoss 0.1924 (0.0725)	MaskLoss 1.0005 (0.6173)	MaskBCELoss 0.0420 (0.0702)	MaskDICELoss 0.9585 (0.5472)
Epoch: [0][396/500]	Time  7.075 ( 7.075)	Loss 0.8867 (1.6707)	CeLoss 0.8867 (0.6451)	SegCLSLoss 0.0000 (0.0162)	KLLoss 0.0000 (0.0513)	MaskLoss 0.0000 (0.6306)	MaskBCELoss 0.0000 (0.1817)	MaskDICELoss 0.0000 (0.4489)
Epoch: [0][397/500]	Time  7.032 ( 7.032)	Loss 2.1815 (1.9629)	CeLoss 0.3262 (0.3799)	SegCLSLoss 0.0284 (0.0274)	KLLoss 0.1021 (0.0941)	MaskLoss 0.9167 (0.9656)	MaskBCELoss 0.5148 (0.2270)	MaskDICELoss 0.4020 (0.7386)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([28, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][398/500]	Time  8.734 ( 8.734)	Loss 1.8840 (2.0603)	CeLoss 0.2539 (0.2252)	SegCLSLoss 0.0192 (0.0371)	KLLoss 0.1104 (0.1375)	MaskLoss 1.0541 (1.0990)	MaskBCELoss 0.1269 (0.1682)	MaskDICELoss 0.9272 (0.9308)
Epoch: [0][399/500]	Time  7.537 ( 7.537)	Loss 2.4913 (1.7766)	CeLoss 0.1797 (0.4519)	SegCLSLoss 0.0703 (0.0232)	KLLoss 0.2109 (0.0924)	MaskLoss 1.1749 (0.8149)	MaskBCELoss 0.2539 (0.1287)	MaskDICELoss 0.9209 (0.6861)
[2025-03-02 22:14:53,196] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=0, lr=[0.00025732653061224484], mom=[(0.9, 0.95)]
[2025-03-02 22:14:53,202] [INFO] [timer.py:215:stop] epoch=0/micro_step=4000/global_step=400, RunningAvgSamplesPerSec=1.3309342342706887, CurrSamplesPerSec=1.4228112006000748, MemAllocated=57.26GB, MaxMemAllocated=63.16GB
Epoch: [0][400/500]	Time  7.030 ( 7.030)	Loss 2.0447 (2.4010)	CeLoss 0.2148 (0.3252)	SegCLSLoss 0.0364 (0.0293)	KLLoss 0.1504 (0.1243)	MaskLoss 1.0805 (1.1528)	MaskBCELoss 0.1282 (0.4111)	MaskDICELoss 0.9523 (0.7416)
Epoch: [0][401/500]	Time  8.671 ( 8.671)	Loss 1.8933 (1.9670)	CeLoss 0.1738 (0.3424)	SegCLSLoss 0.0334 (0.0294)	KLLoss 0.1582 (0.1148)	MaskLoss 1.0316 (0.9689)	MaskBCELoss 0.0375 (0.1808)	MaskDICELoss 0.9941 (0.7881)
Epoch: [0][402/500]	Time  8.214 ( 8.214)	Loss 1.7690 (1.6701)	CeLoss 0.2168 (0.3540)	SegCLSLoss 0.0220 (0.0265)	KLLoss 0.0559 (0.0976)	MaskLoss 1.0995 (0.7995)	MaskBCELoss 0.2183 (0.1126)	MaskDICELoss 0.8813 (0.6870)
Epoch: [0][403/500]	Time  7.656 ( 7.656)	Loss 2.7148 (1.8620)	CeLoss 0.2314 (0.4432)	SegCLSLoss 0.0439 (0.0243)	KLLoss 0.1270 (0.0912)	MaskLoss 1.3030 (0.7963)	MaskBCELoss 0.6501 (0.2460)	MaskDICELoss 0.6529 (0.5502)
Epoch: [0][404/500]	Time  7.778 ( 7.778)	Loss 2.0447 (2.0853)	CeLoss 0.2598 (0.3980)	SegCLSLoss 0.0172 (0.0409)	KLLoss 0.0549 (0.1430)	MaskLoss 1.0149 (0.8706)	MaskBCELoss 0.5415 (0.2252)	MaskDICELoss 0.4734 (0.6454)
Epoch: [0][405/500]	Time  7.009 ( 7.009)	Loss 2.0315 (2.1815)	CeLoss 0.2285 (0.3698)	SegCLSLoss 0.0491 (0.0366)	KLLoss 0.1816 (0.1256)	MaskLoss 1.0247 (1.0423)	MaskBCELoss 0.0263 (0.2486)	MaskDICELoss 0.9984 (0.7937)
Epoch: [0][406/500]	Time  6.817 ( 6.817)	Loss 1.4984 (1.5043)	CeLoss 0.3633 (0.5640)	SegCLSLoss 0.0165 (0.0167)	KLLoss 0.0425 (0.0617)	MaskLoss 0.7826 (0.5677)	MaskBCELoss 0.1767 (0.1175)	MaskDICELoss 0.6058 (0.4502)
Epoch: [0][407/500]	Time  6.160 ( 6.160)	Loss 0.9570 (1.3894)	CeLoss 0.9570 (0.4862)	SegCLSLoss 0.0000 (0.0194)	KLLoss 0.0000 (0.0659)	MaskLoss 0.0000 (0.5149)	MaskBCELoss 0.0000 (0.1149)	MaskDICELoss 0.0000 (0.4000)
Epoch: [0][408/500]	Time  9.084 ( 9.084)	Loss 1.7248 (1.9145)	CeLoss 0.2158 (0.2174)	SegCLSLoss 0.0325 (0.0332)	KLLoss 0.0928 (0.1295)	MaskLoss 1.0504 (1.0029)	MaskBCELoss 0.0729 (0.1598)	MaskDICELoss 0.9775 (0.8431)
Epoch: [0][409/500]	Time  7.290 ( 7.290)	Loss 2.2451 (1.5940)	CeLoss 0.2363 (0.4822)	SegCLSLoss 0.0264 (0.0197)	KLLoss 0.0776 (0.0689)	MaskLoss 1.2700 (0.6948)	MaskBCELoss 0.4126 (0.1312)	MaskDICELoss 0.8574 (0.5636)
[2025-03-02 22:16:10,376] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=0, lr=[0.0002561020408163265], mom=[(0.9, 0.95)]
[2025-03-02 22:16:10,381] [INFO] [timer.py:215:stop] epoch=0/micro_step=4100/global_step=410, RunningAvgSamplesPerSec=1.330053161421208, CurrSamplesPerSec=1.1767208737313188, MemAllocated=57.25GB, MaxMemAllocated=63.16GB
Epoch: [0][410/500]	Time  8.500 ( 8.500)	Loss 1.8479 (1.9717)	CeLoss 0.4844 (0.2757)	SegCLSLoss 0.0159 (0.0290)	KLLoss 0.0064 (0.1052)	MaskLoss 0.9856 (1.0567)	MaskBCELoss 0.3428 (0.2044)	MaskDICELoss 0.6428 (0.8522)
Epoch: [0][411/500]	Time  8.456 ( 8.456)	Loss 1.5000 (1.8271)	CeLoss 1.5000 (0.4062)	SegCLSLoss 0.0000 (0.0234)	KLLoss 0.0000 (0.1011)	MaskLoss 0.0000 (0.8595)	MaskBCELoss 0.0000 (0.1455)	MaskDICELoss 0.0000 (0.7140)
Epoch: [0][412/500]	Time  7.978 ( 7.978)	Loss 1.7336 (1.6253)	CeLoss 0.2041 (0.2972)	SegCLSLoss 0.0359 (0.0286)	KLLoss 0.1504 (0.1053)	MaskLoss 0.8467 (0.7771)	MaskBCELoss 0.0626 (0.1150)	MaskDICELoss 0.7841 (0.6621)
Epoch: [0][413/500]	Time  7.914 ( 7.914)	Loss 0.4980 (1.7589)	CeLoss 0.4980 (0.4226)	SegCLSLoss 0.0000 (0.0266)	KLLoss 0.0000 (0.1020)	MaskLoss 0.0000 (0.7817)	MaskBCELoss 0.0000 (0.1329)	MaskDICELoss 0.0000 (0.6488)
Epoch: [0][414/500]	Time  7.786 ( 7.786)	Loss 2.0429 (1.6310)	CeLoss 0.1729 (0.4195)	SegCLSLoss 0.0508 (0.0217)	KLLoss 0.1904 (0.0946)	MaskLoss 1.0348 (0.7145)	MaskBCELoss 0.0471 (0.1068)	MaskDICELoss 0.9876 (0.6076)
Epoch: [0][415/500]	Time  7.330 ( 7.330)	Loss 1.4297 (1.7759)	CeLoss 1.4297 (0.5336)	SegCLSLoss 0.0000 (0.0201)	KLLoss 0.0000 (0.0827)	MaskLoss 0.0000 (0.7253)	MaskBCELoss 0.0000 (0.1760)	MaskDICELoss 0.0000 (0.5493)
Epoch: [0][416/500]	Time  7.285 ( 7.285)	Loss 1.7728 (1.5204)	CeLoss 0.3535 (0.5313)	SegCLSLoss 0.0183 (0.0166)	KLLoss 0.0601 (0.0550)	MaskLoss 0.9377 (0.6616)	MaskBCELoss 0.2335 (0.0990)	MaskDICELoss 0.7042 (0.5626)
Epoch: [0][417/500]	Time  6.239 ( 6.239)	Loss 1.7775 (1.6930)	CeLoss 0.3340 (0.5559)	SegCLSLoss 0.0167 (0.0167)	KLLoss 0.0383 (0.0619)	MaskLoss 1.0617 (0.7133)	MaskBCELoss 0.2198 (0.1678)	MaskDICELoss 0.8419 (0.5454)
Epoch: [0][418/500]	Time  6.860 ( 6.860)	Loss 0.9883 (1.4638)	CeLoss 0.9883 (0.4972)	SegCLSLoss 0.0000 (0.0102)	KLLoss 0.0000 (0.0268)	MaskLoss 0.0000 (0.6281)	MaskBCELoss 0.0000 (0.2258)	MaskDICELoss 0.0000 (0.4024)
Epoch: [0][419/500]	Time  7.751 ( 7.751)	Loss 1.7893 (1.7666)	CeLoss 0.2188 (0.4794)	SegCLSLoss 0.0286 (0.0289)	KLLoss 0.1318 (0.1041)	MaskLoss 1.0129 (0.7581)	MaskBCELoss 0.0147 (0.0983)	MaskDICELoss 0.9983 (0.6598)
[2025-03-02 22:17:25,860] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=0, lr=[0.00025487755102040815], mom=[(0.9, 0.95)]
[2025-03-02 22:17:25,865] [INFO] [timer.py:215:stop] epoch=0/micro_step=4200/global_step=420, RunningAvgSamplesPerSec=1.3299318807668379, CurrSamplesPerSec=1.2684380064103795, MemAllocated=57.26GB, MaxMemAllocated=63.16GB
Epoch: [0][420/500]	Time  7.885 ( 7.885)	Loss 1.8783 (1.8945)	CeLoss 0.1992 (0.3167)	SegCLSLoss 0.0361 (0.0222)	KLLoss 0.1484 (0.0777)	MaskLoss 1.0185 (0.9707)	MaskBCELoss 0.0473 (0.2852)	MaskDICELoss 0.9711 (0.6855)
Epoch: [0][421/500]	Time  8.559 ( 8.559)	Loss 1.3758 (1.5703)	CeLoss 0.2520 (0.3213)	SegCLSLoss 0.0138 (0.0188)	KLLoss 0.0058 (0.0698)	MaskLoss 0.8132 (0.7740)	MaskBCELoss 0.2793 (0.1869)	MaskDICELoss 0.5339 (0.5871)
Epoch: [0][422/500]	Time  8.060 ( 8.060)	Loss 2.5518 (1.7695)	CeLoss 0.3301 (0.3068)	SegCLSLoss 0.0330 (0.0183)	KLLoss 0.1357 (0.0692)	MaskLoss 1.2135 (0.9350)	MaskBCELoss 0.4476 (0.2419)	MaskDICELoss 0.7659 (0.6932)
Epoch: [0][423/500]	Time  7.586 ( 7.586)	Loss 2.1573 (1.4516)	CeLoss 0.1748 (0.4839)	SegCLSLoss 0.0398 (0.0170)	KLLoss 0.2090 (0.0693)	MaskLoss 1.0283 (0.6056)	MaskBCELoss 0.0977 (0.0760)	MaskDICELoss 0.9305 (0.5296)
Epoch: [0][424/500]	Time  6.726 ( 6.726)	Loss 2.0655 (1.8521)	CeLoss 0.1406 (0.4896)	SegCLSLoss 0.0635 (0.0239)	KLLoss 0.1953 (0.1010)	MaskLoss 1.0502 (0.7847)	MaskBCELoss 0.0622 (0.1625)	MaskDICELoss 0.9880 (0.6222)
Epoch: [0][425/500]	Time  6.284 ( 6.284)	Loss 1.3125 (2.0398)	CeLoss 1.3125 (0.5708)	SegCLSLoss 0.0000 (0.0289)	KLLoss 0.0000 (0.1084)	MaskLoss 0.0000 (0.8261)	MaskBCELoss 0.0000 (0.1951)	MaskDICELoss 0.0000 (0.6310)
Epoch: [0][426/500]	Time  6.999 ( 6.999)	Loss 1.1016 (1.8392)	CeLoss 1.1016 (0.4074)	SegCLSLoss 0.0000 (0.0202)	KLLoss 0.0000 (0.1007)	MaskLoss 0.0000 (0.8581)	MaskBCELoss 0.0000 (0.1608)	MaskDICELoss 0.0000 (0.6973)
Epoch: [0][427/500]	Time  7.590 ( 7.590)	Loss 1.9473 (1.7109)	CeLoss 0.2246 (0.4686)	SegCLSLoss 0.0226 (0.0198)	KLLoss 0.1177 (0.0868)	MaskLoss 0.9225 (0.7439)	MaskBCELoss 0.3178 (0.1412)	MaskDICELoss 0.6047 (0.6027)
Epoch: [0][428/500]	Time  6.966 ( 6.966)	Loss 1.8407 (1.8186)	CeLoss 0.3379 (0.5143)	SegCLSLoss 0.0194 (0.0287)	KLLoss 0.0454 (0.1087)	MaskLoss 0.9727 (0.7397)	MaskBCELoss 0.3368 (0.1148)	MaskDICELoss 0.6359 (0.6249)
Epoch: [0][429/500]	Time  7.015 ( 7.015)	Loss 2.3628 (1.5956)	CeLoss 0.3496 (0.2711)	SegCLSLoss 0.0177 (0.0245)	KLLoss 0.0586 (0.0823)	MaskLoss 1.2899 (0.7676)	MaskBCELoss 0.4791 (0.2149)	MaskDICELoss 0.8108 (0.5526)
[2025-03-02 22:18:39,132] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=0, lr=[0.0002536530612244898], mom=[(0.9, 0.95)]
[2025-03-02 22:18:39,138] [INFO] [timer.py:215:stop] epoch=0/micro_step=4300/global_step=430, RunningAvgSamplesPerSec=1.3307309197418873, CurrSamplesPerSec=1.3359484683604144, MemAllocated=57.24GB, MaxMemAllocated=63.16GB
Epoch: [0][430/500]	Time  7.487 ( 7.487)	Loss 2.1519 (1.8511)	CeLoss 0.4492 (0.3731)	SegCLSLoss 0.0176 (0.0308)	KLLoss 0.0625 (0.1062)	MaskLoss 1.1869 (0.8941)	MaskBCELoss 0.2579 (0.1433)	MaskDICELoss 0.9290 (0.7508)
Epoch: [0][431/500]	Time  6.579 ( 6.579)	Loss 0.4785 (1.5917)	CeLoss 0.4785 (0.5204)	SegCLSLoss 0.0000 (0.0161)	KLLoss 0.0000 (0.0721)	MaskLoss 0.0000 (0.6562)	MaskBCELoss 0.0000 (0.1185)	MaskDICELoss 0.0000 (0.5378)
Epoch: [0][432/500]	Time  8.203 ( 8.203)	Loss 2.7883 (1.9516)	CeLoss 0.2217 (0.3461)	SegCLSLoss 0.0330 (0.0260)	KLLoss 0.1982 (0.1140)	MaskLoss 1.2752 (0.9533)	MaskBCELoss 0.4818 (0.1825)	MaskDICELoss 0.7934 (0.7709)
Epoch: [0][433/500]	Time  7.463 ( 7.463)	Loss 2.3607 (1.8330)	CeLoss 0.2178 (0.4284)	SegCLSLoss 0.0330 (0.0257)	KLLoss 0.1631 (0.1155)	MaskLoss 1.1782 (0.7853)	MaskBCELoss 0.2957 (0.1440)	MaskDICELoss 0.8825 (0.6413)
Epoch: [0][434/500]	Time  7.110 ( 7.110)	Loss 2.5691 (1.4220)	CeLoss 0.2295 (0.5399)	SegCLSLoss 0.0598 (0.0151)	KLLoss 0.2227 (0.0520)	MaskLoss 1.1675 (0.5381)	MaskBCELoss 0.2531 (0.1285)	MaskDICELoss 0.9144 (0.4096)
Epoch: [0][435/500]	Time  7.813 ( 7.813)	Loss 1.6008 (1.6012)	CeLoss 0.2412 (0.4702)	SegCLSLoss 0.0325 (0.0203)	KLLoss 0.1084 (0.0844)	MaskLoss 0.8499 (0.7119)	MaskBCELoss 0.0596 (0.0712)	MaskDICELoss 0.7903 (0.6407)
Epoch: [0][436/500]	Time  7.577 ( 7.577)	Loss 2.0571 (1.9158)	CeLoss 0.2324 (0.4106)	SegCLSLoss 0.0366 (0.0309)	KLLoss 0.1387 (0.1217)	MaskLoss 1.1081 (0.8092)	MaskBCELoss 0.1444 (0.1941)	MaskDICELoss 0.9637 (0.6150)
Epoch: [0][437/500]	Time  8.890 ( 8.890)	Loss 2.5550 (2.3117)	CeLoss 0.2539 (0.2343)	SegCLSLoss 0.0339 (0.0381)	KLLoss 0.0977 (0.1568)	MaskLoss 1.3647 (1.1588)	MaskBCELoss 0.5262 (0.2721)	MaskDICELoss 0.8385 (0.8867)
Epoch: [0][438/500]	Time  7.902 ( 7.902)	Loss 2.2799 (1.7259)	CeLoss 0.1738 (0.4680)	SegCLSLoss 0.0442 (0.0242)	KLLoss 0.1836 (0.0921)	MaskLoss 1.1064 (0.7682)	MaskBCELoss 0.2438 (0.1095)	MaskDICELoss 0.8626 (0.6587)
Epoch: [0][439/500]	Time  8.421 ( 8.421)	Loss 1.8242 (1.6495)	CeLoss 0.2432 (0.2617)	SegCLSLoss 0.0266 (0.0292)	KLLoss 0.1118 (0.0984)	MaskLoss 1.0153 (0.8351)	MaskBCELoss 0.1058 (0.1442)	MaskDICELoss 0.9095 (0.6908)
[2025-03-02 22:19:57,657] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=0, lr=[0.0002524285714285714], mom=[(0.9, 0.95)]
[2025-03-02 22:19:57,663] [INFO] [timer.py:215:stop] epoch=0/micro_step=4400/global_step=440, RunningAvgSamplesPerSec=1.3293717234081952, CurrSamplesPerSec=1.1676299100825887, MemAllocated=57.1GB, MaxMemAllocated=63.16GB
Epoch: [0][440/500]	Time  8.566 ( 8.566)	Loss 2.2505 (1.9130)	CeLoss 0.2773 (0.2087)	SegCLSLoss 0.0167 (0.0293)	KLLoss 0.0371 (0.1083)	MaskLoss 1.2053 (1.0058)	MaskBCELoss 0.6115 (0.2510)	MaskDICELoss 0.5938 (0.7548)
Epoch: [0][441/500]	Time  6.217 ( 6.217)	Loss 1.9246 (1.7373)	CeLoss 0.1631 (0.2784)	SegCLSLoss 0.0505 (0.0302)	KLLoss 0.1826 (0.1205)	MaskLoss 1.0017 (0.8541)	MaskBCELoss 0.0050 (0.1073)	MaskDICELoss 0.9967 (0.7468)
Epoch: [0][442/500]	Time  7.476 ( 7.476)	Loss 2.3527 (1.8877)	CeLoss 0.1670 (0.3755)	SegCLSLoss 0.0422 (0.0370)	KLLoss 0.1543 (0.1056)	MaskLoss 1.2024 (0.8751)	MaskBCELoss 0.3456 (0.1964)	MaskDICELoss 0.8569 (0.6787)
Epoch: [0][443/500]	Time  7.631 ( 7.631)	Loss 1.8494 (1.6101)	CeLoss 0.2266 (0.5369)	SegCLSLoss 0.0361 (0.0211)	KLLoss 0.1465 (0.0748)	MaskLoss 1.0080 (0.6571)	MaskBCELoss 0.0132 (0.1065)	MaskDICELoss 0.9948 (0.5507)
Epoch: [0][444/500]	Time  6.113 ( 6.113)	Loss 1.2266 (1.3762)	CeLoss 1.2266 (0.6540)	SegCLSLoss 0.0000 (0.0108)	KLLoss 0.0000 (0.0472)	MaskLoss 0.0000 (0.4270)	MaskBCELoss 0.0000 (0.1014)	MaskDICELoss 0.0000 (0.3257)
Epoch: [0][445/500]	Time  6.974 ( 6.974)	Loss 1.9570 (1.5787)	CeLoss 0.4199 (0.3396)	SegCLSLoss 0.0266 (0.0235)	KLLoss 0.1260 (0.0800)	MaskLoss 1.0076 (0.7654)	MaskBCELoss 0.0119 (0.1422)	MaskDICELoss 0.9957 (0.6231)
Epoch: [0][446/500]	Time  7.998 ( 7.998)	Loss 1.7394 (1.7965)	CeLoss 0.2217 (0.5794)	SegCLSLoss 0.0236 (0.0222)	KLLoss 0.1084 (0.0893)	MaskLoss 0.9347 (0.6913)	MaskBCELoss 0.1367 (0.1576)	MaskDICELoss 0.7980 (0.5337)
Epoch: [0][447/500]	Time  6.126 ( 6.126)	Loss 1.2266 (1.9503)	CeLoss 1.2266 (0.4732)	SegCLSLoss 0.0000 (0.0272)	KLLoss 0.0000 (0.0859)	MaskLoss 0.0000 (0.8315)	MaskBCELoss 0.0000 (0.2885)	MaskDICELoss 0.0000 (0.5430)
Epoch: [0][448/500]	Time  6.523 ( 6.523)	Loss 2.1450 (1.6625)	CeLoss 0.4219 (0.6392)	SegCLSLoss 0.0217 (0.0160)	KLLoss 0.1152 (0.0547)	MaskLoss 0.8331 (0.6338)	MaskBCELoss 0.4174 (0.1618)	MaskDICELoss 0.4158 (0.4720)
Epoch: [0][449/500]	Time  8.369 ( 8.369)	Loss 1.6206 (1.7999)	CeLoss 0.2793 (0.3271)	SegCLSLoss 0.0330 (0.0227)	KLLoss 0.1006 (0.0786)	MaskLoss 0.8430 (0.9004)	MaskBCELoss 0.0823 (0.2477)	MaskDICELoss 0.7607 (0.6527)
[2025-03-02 22:21:06,909] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=0, lr=[0.00025120408163265305], mom=[(0.9, 0.95)]
[2025-03-02 22:21:06,914] [INFO] [timer.py:215:stop] epoch=0/micro_step=4500/global_step=450, RunningAvgSamplesPerSec=1.3317367810654213, CurrSamplesPerSec=1.7176430600005324, MemAllocated=57.25GB, MaxMemAllocated=63.16GB
Epoch: [0][450/500]	Time  5.823 ( 5.823)	Loss 1.5721 (1.8033)	CeLoss 0.2559 (0.5031)	SegCLSLoss 0.0178 (0.0194)	KLLoss 0.0571 (0.0864)	MaskLoss 0.9160 (0.7571)	MaskBCELoss 0.1620 (0.1882)	MaskDICELoss 0.7540 (0.5689)
Epoch: [0][451/500]	Time  7.755 ( 7.755)	Loss 1.7808 (1.5346)	CeLoss 0.2930 (0.4564)	SegCLSLoss 0.0249 (0.0217)	KLLoss 0.0996 (0.0869)	MaskLoss 1.0080 (0.6213)	MaskBCELoss 0.0696 (0.0988)	MaskDICELoss 0.9384 (0.5225)
Epoch: [0][452/500]	Time  7.483 ( 7.483)	Loss 0.1416 (2.0638)	CeLoss 0.1416 (0.2485)	SegCLSLoss 0.0000 (0.0315)	KLLoss 0.0000 (0.0905)	MaskLoss 0.0000 (1.0834)	MaskBCELoss 0.0000 (0.3537)	MaskDICELoss 0.0000 (0.7297)
Epoch: [0][453/500]	Time  7.467 ( 7.467)	Loss 2.0209 (1.8787)	CeLoss 0.2598 (0.3315)	SegCLSLoss 0.0181 (0.0299)	KLLoss 0.0850 (0.1246)	MaskLoss 1.1836 (0.8946)	MaskBCELoss 0.2280 (0.1395)	MaskDICELoss 0.9556 (0.7551)
Epoch: [0][454/500]	Time  7.319 ( 7.319)	Loss 1.9871 (1.3648)	CeLoss 0.2285 (0.2978)	SegCLSLoss 0.0258 (0.0185)	KLLoss 0.0938 (0.0667)	MaskLoss 1.1707 (0.6906)	MaskBCELoss 0.1992 (0.1001)	MaskDICELoss 0.9715 (0.5905)
Epoch: [0][455/500]	Time  8.620 ( 8.620)	Loss 2.3962 (1.8156)	CeLoss 0.2656 (0.4259)	SegCLSLoss 0.0256 (0.0212)	KLLoss 0.1855 (0.0898)	MaskLoss 1.1599 (0.8654)	MaskBCELoss 0.2128 (0.1541)	MaskDICELoss 0.9471 (0.7112)
Epoch: [0][456/500]	Time  8.059 ( 8.059)	Loss 1.5747 (1.9336)	CeLoss 0.2773 (0.3031)	SegCLSLoss 0.0156 (0.0304)	KLLoss 0.0515 (0.1096)	MaskLoss 0.9722 (0.9913)	MaskBCELoss 0.1103 (0.1853)	MaskDICELoss 0.8619 (0.8059)
Epoch: [0][457/500]	Time  7.185 ( 7.185)	Loss 1.8192 (2.0292)	CeLoss 0.2383 (0.2286)	SegCLSLoss 0.0322 (0.0359)	KLLoss 0.1338 (0.1129)	MaskLoss 1.0091 (1.0919)	MaskBCELoss 0.0211 (0.2395)	MaskDICELoss 0.9880 (0.8523)
Epoch: [0][458/500]	Time  7.890 ( 7.890)	Loss 1.8942 (1.6168)	CeLoss 0.1865 (0.2787)	SegCLSLoss 0.0334 (0.0229)	KLLoss 0.1670 (0.0843)	MaskLoss 1.0084 (0.8048)	MaskBCELoss 0.0147 (0.1851)	MaskDICELoss 0.9937 (0.6196)
Epoch: [0][459/500]	Time  7.637 ( 7.637)	Loss 1.4394 (1.4489)	CeLoss 0.2275 (0.5071)	SegCLSLoss 0.0200 (0.0163)	KLLoss 0.0598 (0.0678)	MaskLoss 0.8710 (0.5890)	MaskBCELoss 0.0919 (0.0734)	MaskDICELoss 0.7791 (0.5156)
[2025-03-02 22:22:24,092] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=0, lr=[0.0002499795918367347], mom=[(0.9, 0.95)]
[2025-03-02 22:22:24,097] [INFO] [timer.py:215:stop] epoch=0/micro_step=4600/global_step=460, RunningAvgSamplesPerSec=1.3309319965282709, CurrSamplesPerSec=1.287668045923007, MemAllocated=57.46GB, MaxMemAllocated=63.16GB
Epoch: [0][460/500]	Time  7.768 ( 7.768)	Loss 1.2284 (1.4847)	CeLoss 0.2363 (0.2790)	SegCLSLoss 0.0175 (0.0182)	KLLoss 0.0330 (0.0654)	MaskLoss 0.8223 (0.7941)	MaskBCELoss 0.0291 (0.1408)	MaskDICELoss 0.7932 (0.6533)
Epoch: [0][461/500]	Time  7.005 ( 7.005)	Loss 2.3346 (1.7628)	CeLoss 0.2393 (0.3780)	SegCLSLoss 0.0334 (0.0240)	KLLoss 0.1357 (0.0944)	MaskLoss 1.2085 (0.8518)	MaskBCELoss 0.3292 (0.1433)	MaskDICELoss 0.8792 (0.7085)
Epoch: [0][462/500]	Time  7.557 ( 7.557)	Loss 1.6359 (1.5588)	CeLoss 0.3496 (0.3916)	SegCLSLoss 0.0145 (0.0210)	KLLoss 0.0057 (0.0846)	MaskLoss 1.0098 (0.6993)	MaskBCELoss 0.2453 (0.1187)	MaskDICELoss 0.7646 (0.5807)
Epoch: [0][463/500]	Time  7.882 ( 7.882)	Loss 2.3075 (1.5981)	CeLoss 0.1797 (0.3456)	SegCLSLoss 0.0388 (0.0269)	KLLoss 0.1592 (0.0889)	MaskLoss 1.1899 (0.7418)	MaskBCELoss 0.2817 (0.1412)	MaskDICELoss 0.9082 (0.6006)
Epoch: [0][464/500]	Time  8.025 ( 8.025)	Loss 1.7334 (1.8256)	CeLoss 0.2793 (0.4179)	SegCLSLoss 0.0259 (0.0230)	KLLoss 0.1045 (0.1018)	MaskLoss 1.0103 (0.8508)	MaskBCELoss 0.0122 (0.1377)	MaskDICELoss 0.9980 (0.7132)
Epoch: [0][465/500]	Time  7.185 ( 7.185)	Loss 1.9141 (1.7251)	CeLoss 1.9141 (0.4874)	SegCLSLoss 0.0000 (0.0247)	KLLoss 0.0000 (0.0852)	MaskLoss 0.0000 (0.7991)	MaskBCELoss 0.0000 (0.0858)	MaskDICELoss 0.0000 (0.7133)
Epoch: [0][466/500]	Time  6.854 ( 6.854)	Loss 1.8834 (1.7155)	CeLoss 0.1982 (0.5063)	SegCLSLoss 0.0381 (0.0179)	KLLoss 0.1582 (0.0774)	MaskLoss 1.0101 (0.7209)	MaskBCELoss 0.0218 (0.1696)	MaskDICELoss 0.9883 (0.5513)
Epoch: [0][467/500]	Time  8.510 ( 8.510)	Loss 2.7615 (2.2603)	CeLoss 0.2578 (0.2294)	SegCLSLoss 0.0210 (0.0375)	KLLoss 0.1245 (0.1557)	MaskLoss 1.3967 (1.1530)	MaskBCELoss 0.5992 (0.2366)	MaskDICELoss 0.7975 (0.9164)
Epoch: [0][468/500]	Time  7.270 ( 7.270)	Loss 1.6762 (1.4055)	CeLoss 0.2324 (0.4952)	SegCLSLoss 0.0272 (0.0137)	KLLoss 0.1060 (0.0426)	MaskLoss 0.9919 (0.5853)	MaskBCELoss 0.0125 (0.1477)	MaskDICELoss 0.9794 (0.4377)
Epoch: [0][469/500]	Time  6.575 ( 6.575)	Loss 0.9727 (1.9607)	CeLoss 0.9727 (0.3699)	SegCLSLoss 0.0000 (0.0268)	KLLoss 0.0000 (0.1137)	MaskLoss 0.0000 (0.8949)	MaskBCELoss 0.0000 (0.2275)	MaskDICELoss 0.0000 (0.6674)
[2025-03-02 22:23:37,802] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=0, lr=[0.0002487551020408163], mom=[(0.9, 0.95)]
[2025-03-02 22:23:37,808] [INFO] [timer.py:215:stop] epoch=0/micro_step=4700/global_step=470, RunningAvgSamplesPerSec=1.33147630047521, CurrSamplesPerSec=1.461035078428454, MemAllocated=56.81GB, MaxMemAllocated=63.16GB
Epoch: [0][470/500]	Time  6.846 ( 6.846)	Loss 1.2399 (1.6452)	CeLoss 0.2188 (0.5715)	SegCLSLoss 0.0205 (0.0150)	KLLoss 0.1050 (0.0713)	MaskLoss 0.4144 (0.5993)	MaskBCELoss 0.1771 (0.1810)	MaskDICELoss 0.2373 (0.4183)
Epoch: [0][471/500]	Time  7.226 ( 7.226)	Loss 1.1328 (1.5417)	CeLoss 1.1328 (0.5017)	SegCLSLoss 0.0000 (0.0209)	KLLoss 0.0000 (0.0743)	MaskLoss 0.0000 (0.5703)	MaskBCELoss 0.0000 (0.1617)	MaskDICELoss 0.0000 (0.4086)
Epoch: [0][472/500]	Time  8.564 ( 8.564)	Loss 1.6645 (1.8595)	CeLoss 0.2598 (0.2695)	SegCLSLoss 0.0183 (0.0340)	KLLoss 0.0762 (0.1304)	MaskLoss 0.9807 (0.9104)	MaskBCELoss 0.1096 (0.1409)	MaskDICELoss 0.8711 (0.7695)
Epoch: [0][473/500]	Time  8.628 ( 8.628)	Loss 2.3566 (1.6893)	CeLoss 0.1934 (0.3077)	SegCLSLoss 0.0400 (0.0236)	KLLoss 0.1660 (0.0888)	MaskLoss 1.1839 (0.8368)	MaskBCELoss 0.2977 (0.1785)	MaskDICELoss 0.8862 (0.6582)
Epoch: [0][474/500]	Time  7.115 ( 7.115)	Loss 2.4526 (1.7292)	CeLoss 0.1416 (0.4987)	SegCLSLoss 0.0898 (0.0280)	KLLoss 0.2061 (0.1086)	MaskLoss 1.1690 (0.6851)	MaskBCELoss 0.2758 (0.0976)	MaskDICELoss 0.8932 (0.5875)
Epoch: [0][475/500]	Time  6.793 ( 6.793)	Loss 1.9478 (1.4497)	CeLoss 0.1865 (0.5977)	SegCLSLoss 0.0454 (0.0166)	KLLoss 0.1504 (0.0597)	MaskLoss 1.0610 (0.5122)	MaskBCELoss 0.0743 (0.0924)	MaskDICELoss 0.9868 (0.4197)
Epoch: [0][476/500]	Time  6.982 ( 6.982)	Loss 2.4950 (2.0649)	CeLoss 0.1572 (0.3151)	SegCLSLoss 0.0583 (0.0338)	KLLoss 0.2100 (0.1407)	MaskLoss 1.1780 (0.9353)	MaskBCELoss 0.2936 (0.2349)	MaskDICELoss 0.8844 (0.7004)
Epoch: [0][477/500]	Time  7.191 ( 7.191)	Loss 2.7500 (1.5711)	CeLoss 0.2490 (0.3933)	SegCLSLoss 0.0388 (0.0210)	KLLoss 0.1719 (0.0787)	MaskLoss 1.3246 (0.7454)	MaskBCELoss 0.4723 (0.1083)	MaskDICELoss 0.8523 (0.6372)
Epoch: [0][478/500]	Time  6.917 ( 6.917)	Loss 1.9109 (1.7199)	CeLoss 0.2891 (0.5721)	SegCLSLoss 0.0199 (0.0161)	KLLoss 0.0908 (0.0654)	MaskLoss 1.0815 (0.7194)	MaskBCELoss 0.1653 (0.1587)	MaskDICELoss 0.9161 (0.5607)
Epoch: [0][479/500]	Time  6.938 ( 6.938)	Loss 2.8672 (1.8462)	CeLoss 0.2852 (0.4933)	SegCLSLoss 0.0537 (0.0257)	KLLoss 0.1846 (0.1030)	MaskLoss 1.3371 (0.7786)	MaskBCELoss 0.4833 (0.1502)	MaskDICELoss 0.8538 (0.6283)
[2025-03-02 22:24:51,295] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=0, lr=[0.00024753061224489794], mom=[(0.9, 0.95)]
[2025-03-02 22:24:51,300] [INFO] [timer.py:215:stop] epoch=0/micro_step=4800/global_step=480, RunningAvgSamplesPerSec=1.3320793443364325, CurrSamplesPerSec=1.4011127917604238, MemAllocated=57.25GB, MaxMemAllocated=63.16GB
Epoch: [0][480/500]	Time  7.139 ( 7.139)	Loss 1.9975 (1.4971)	CeLoss 0.3828 (0.4467)	SegCLSLoss 0.0193 (0.0181)	KLLoss 0.0654 (0.0790)	MaskLoss 0.9778 (0.6254)	MaskBCELoss 0.3634 (0.0996)	MaskDICELoss 0.6145 (0.5258)
Epoch: [0][481/500]	Time  6.929 ( 6.929)	Loss 2.8782 (1.8047)	CeLoss 0.2441 (0.6721)	SegCLSLoss 0.0408 (0.0175)	KLLoss 0.1787 (0.0718)	MaskLoss 1.3809 (0.6947)	MaskBCELoss 0.5207 (0.1424)	MaskDICELoss 0.8602 (0.5523)
Epoch: [0][482/500]	Time  7.416 ( 7.416)	Loss 1.6113 (1.8582)	CeLoss 0.2656 (0.3414)	SegCLSLoss 0.0200 (0.0239)	KLLoss 0.0649 (0.0956)	MaskLoss 0.9880 (0.9286)	MaskBCELoss 0.0881 (0.1940)	MaskDICELoss 0.8999 (0.7346)
Epoch: [0][483/500]	Time  7.758 ( 7.758)	Loss 2.1241 (1.8371)	CeLoss 0.2734 (0.3049)	SegCLSLoss 0.0193 (0.0251)	KLLoss 0.0825 (0.0945)	MaskLoss 1.0609 (0.9468)	MaskBCELoss 0.4499 (0.1947)	MaskDICELoss 0.6110 (0.7521)
Epoch: [0][484/500]	Time  7.832 ( 7.832)	Loss 1.9968 (1.7085)	CeLoss 0.1992 (0.3313)	SegCLSLoss 0.0376 (0.0276)	KLLoss 0.0952 (0.0995)	MaskLoss 1.0662 (0.8713)	MaskBCELoss 0.3330 (0.0946)	MaskDICELoss 0.7332 (0.7767)
Epoch: [0][485/500]	Time  6.991 ( 6.991)	Loss 3.0310 (2.0604)	CeLoss 0.1992 (0.4420)	SegCLSLoss 0.1318 (0.0400)	KLLoss 0.2354 (0.1367)	MaskLoss 1.2876 (0.8754)	MaskBCELoss 0.5403 (0.1768)	MaskDICELoss 0.7473 (0.6986)
Epoch: [0][486/500]	Time  6.900 ( 6.900)	Loss 2.2824 (1.1825)	CeLoss 0.2598 (0.6435)	SegCLSLoss 0.0215 (0.0080)	KLLoss 0.0986 (0.0228)	MaskLoss 1.2118 (0.3750)	MaskBCELoss 0.4066 (0.0690)	MaskDICELoss 0.8052 (0.3060)
Epoch: [0][487/500]	Time  7.750 ( 7.750)	Loss 1.4488 (1.7428)	CeLoss 0.2402 (0.3336)	SegCLSLoss 0.0184 (0.0260)	KLLoss 0.0062 (0.0793)	MaskLoss 1.0432 (0.9189)	MaskBCELoss 0.1322 (0.1603)	MaskDICELoss 0.9110 (0.7586)
Epoch: [0][488/500]	Time  7.513 ( 7.513)	Loss 2.2732 (1.6842)	CeLoss 0.2217 (0.3937)	SegCLSLoss 0.0391 (0.0229)	KLLoss 0.1299 (0.0807)	MaskLoss 1.1804 (0.8201)	MaskBCELoss 0.3311 (0.1361)	MaskDICELoss 0.8494 (0.6840)
Epoch: [0][489/500]	Time  6.615 ( 6.615)	Loss 2.4300 (1.8342)	CeLoss 0.2031 (0.4919)	SegCLSLoss 0.0359 (0.0262)	KLLoss 0.1660 (0.0921)	MaskLoss 1.2009 (0.7551)	MaskBCELoss 0.3462 (0.2062)	MaskDICELoss 0.8547 (0.5489)
[2025-03-02 22:26:04,977] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=0, lr=[0.00024630612244897957], mom=[(0.9, 0.95)]
[2025-03-02 22:26:04,982] [INFO] [timer.py:215:stop] epoch=0/micro_step=4900/global_step=490, RunningAvgSamplesPerSec=1.3325892284974512, CurrSamplesPerSec=1.2538258871767032, MemAllocated=57.27GB, MaxMemAllocated=63.16GB
Epoch: [0][490/500]	Time  7.977 ( 7.977)	Loss 2.0644 (1.6044)	CeLoss 0.2080 (0.5565)	SegCLSLoss 0.0325 (0.0157)	KLLoss 0.1855 (0.0749)	MaskLoss 1.0476 (0.6323)	MaskBCELoss 0.0481 (0.1078)	MaskDICELoss 0.9996 (0.5245)
Epoch: [0][491/500]	Time  6.919 ( 6.919)	Loss 0.0767 (1.3598)	CeLoss 0.0767 (0.5118)	SegCLSLoss 0.0000 (0.0217)	KLLoss 0.0000 (0.0622)	MaskLoss 0.0000 (0.4975)	MaskBCELoss 0.0000 (0.0908)	MaskDICELoss 0.0000 (0.4067)
Epoch: [0][492/500]	Time  6.907 ( 6.907)	Loss 1.8676 (1.5929)	CeLoss 0.1934 (0.6473)	SegCLSLoss 0.0408 (0.0195)	KLLoss 0.1582 (0.0636)	MaskLoss 0.9674 (0.5616)	MaskBCELoss 0.0526 (0.1203)	MaskDICELoss 0.9148 (0.4413)
Epoch: [0][493/500]	Time  6.111 ( 6.111)	Loss 1.6816 (1.6903)	CeLoss 0.1992 (0.6060)	SegCLSLoss 0.0288 (0.0220)	KLLoss 0.1172 (0.0743)	MaskLoss 0.9844 (0.6199)	MaskBCELoss 0.0136 (0.1559)	MaskDICELoss 0.9708 (0.4639)
Epoch: [0][494/500]	Time  5.876 ( 5.876)	Loss 1.8360 (1.4750)	CeLoss 0.2598 (0.8089)	SegCLSLoss 0.0256 (0.0099)	KLLoss 0.1377 (0.0322)	MaskLoss 0.9936 (0.4341)	MaskBCELoss 0.0181 (0.0979)	MaskDICELoss 0.9755 (0.3362)
Epoch: [0][495/500]	Time  6.658 ( 6.658)	Loss 2.0049 (1.3648)	CeLoss 0.3105 (0.6336)	SegCLSLoss 0.0369 (0.0147)	KLLoss 0.1621 (0.0591)	MaskLoss 0.9979 (0.4168)	MaskBCELoss 0.0304 (0.0712)	MaskDICELoss 0.9675 (0.3456)
Epoch: [0][496/500]	Time  6.281 ( 6.281)	Loss 1.7920 (1.6454)	CeLoss 0.2090 (0.4379)	SegCLSLoss 0.0311 (0.0193)	KLLoss 0.1348 (0.0757)	MaskLoss 1.0110 (0.7203)	MaskBCELoss 0.0154 (0.1743)	MaskDICELoss 0.9957 (0.5459)
Epoch: [0][497/500]	Time  8.406 ( 8.406)	Loss 1.9832 (1.7404)	CeLoss 0.3301 (0.2529)	SegCLSLoss 0.0195 (0.0291)	KLLoss 0.0850 (0.0928)	MaskLoss 1.1317 (0.8997)	MaskBCELoss 0.1718 (0.2016)	MaskDICELoss 0.9600 (0.6981)
Epoch: [0][498/500]	Time  7.102 ( 7.102)	Loss 1.0156 (1.8998)	CeLoss 1.0156 (0.3514)	SegCLSLoss 0.0000 (0.0193)	KLLoss 0.0000 (0.0660)	MaskLoss 0.0000 (0.9556)	MaskBCELoss 0.0000 (0.3188)	MaskDICELoss 0.0000 (0.6368)
Epoch: [0][499/500]	Time  8.253 ( 8.253)	Loss 1.9208 (1.6674)	CeLoss 0.1855 (0.3131)	SegCLSLoss 0.0391 (0.0268)	KLLoss 0.1621 (0.1002)	MaskLoss 1.0247 (0.7880)	MaskBCELoss 0.0446 (0.1518)	MaskDICELoss 0.9801 (0.6362)
[2025-03-02 22:27:14,756] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=0, lr=[0.0002450816326530612], mom=[(0.9, 0.95)]
[2025-03-02 22:27:14,762] [INFO] [timer.py:215:stop] epoch=0/micro_step=5000/global_step=500, RunningAvgSamplesPerSec=1.334472954451115, CurrSamplesPerSec=1.376271646488281, MemAllocated=57.47GB, MaxMemAllocated=63.16GB
Epoch: [0][500/500]	Time  7.268 ( 7.268)	Loss 1.7873 (1.0806)	CeLoss 0.1689 (0.6462)	SegCLSLoss 0.0281 (0.0070)	KLLoss 0.1396 (0.0281)	MaskLoss 1.0152 (0.2960)	MaskBCELoss 0.0299 (0.0225)	MaskDICELoss 0.9853 (0.2735)
  0%|                                                                                                                                                              | 0/200 [00:01<?, ?it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 659, in <module>
[rank0]:     main(sys.argv[1:])
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 412, in main
[rank0]:     giou, ciou = validate(val_loader, model_engine, epoch, writer, args)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 599, in validate
[rank0]:     for input_dict in tqdm.tqdm(val_loader):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/tqdm/std.py", line 1182, in __iter__
[rank0]:     for obj in iterable:
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: AssertionError: Caught AssertionError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 109, in collate_fn
[rank0]:     assert len(parts) == 2, (len(parts), rou)
[rank0]: AssertionError: (1, "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something showing that the man is playing sports in this image? Please output segmentation mask. ASSISTANT:")