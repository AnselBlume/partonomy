You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565



Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:15<00:00,  5.04s/it]
trainable params: 6,553,600 || all params: 14,025,701,683 || trainable%: 0.0467256480147682
n:  base_model.model.model.embed_tokens.weight p.shape:  torch.Size([32002, 5120])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_token.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.mask_decoder.mask_tokens.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.weight p.shape:  torch.Size([256, 64, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.weight p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.weight p.shape:  torch.Size([64, 32, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.bias p.shape:  torch.Size([4])
n:  base_model.model.model.text_hidden_fcs.0.0.weight p.shape:  torch.Size([5120, 5120])
n:  base_model.model.model.text_hidden_fcs.0.0.bias p.shape:  torch.Size([5120])
n:  base_model.model.model.text_hidden_fcs.0.2.weight p.shape:  torch.Size([256, 5120])
n:  base_model.model.model.text_hidden_fcs.0.2.bias p.shape:  torch.Size([256])
n:  base_model.model.lm_head.weight p.shape:  torch.Size([32002, 5120])
>> (PLUM.py) Initializing teacher LLM...
>> (PLUM.py) Teacher LLM initialized.
ade20k:  20210
cocostuff:  118287
loading annotations into memory...
Done (t=0.57s)
creating index...
index created!
pascal_part:  4366
loading annotations into memory...
Done (t=8.33s)
creating index...
index created!
paco_lvis:  45790
mapillary:  18000
loading dataset refclef into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refclef/refs(unc).p
creating index...
index created.
DONE (t=2.91s)
dataset refclef (refs unc) (train split) has 17978 images and 99523 annotations.
loading dataset refcoco into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco/refs(unc).p
creating index...
index created.
DONE (t=5.02s)
dataset refcoco (refs unc) (train split) has 16994 images and 196771 annotations.
loading dataset refcoco+ into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco+/refs(unc).p
creating index...
index created.
DONE (t=6.20s)
dataset refcoco+ (refs unc) (train split) has 16992 images and 196737 annotations.
loading dataset refcocog into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcocog/refs(umd).p
creating index...
index created.
DONE (t=6.08s)
dataset refcocog (refs umd) (train split) has 21899 images and 208960 annotations.
vqa_data:  157712
number of reason_seg samples:  239
len(self.img_to_explanation):  239
Training with 5000 examples and validating with 200 examples.
[2025-03-06 01:30:10,490] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5, git-hash=unknown, git-branch=unknown
[2025-03-06 01:30:10,491] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2025-03-06 01:30:10,491] [INFO] [comm.py:594:init_distributed] cdb=None
[2025-03-06 01:30:10,491] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-03-06 01:30:42,403] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module fused_adam...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/tensor/python_tensor.cpp:78.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
Time to load fused_adam op: 0.4619181156158447 seconds
[2025-03-06 01:30:43,072] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-03-06 01:30:43,814] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-03-06 01:30:43,816] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-03-06 01:30:43,816] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-03-06 01:30:43,816] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500000000
[2025-03-06 01:30:43,816] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500000000
[2025-03-06 01:30:43,816] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2025-03-06 01:30:43,816] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(365842916, False)]
[2025-03-06 01:30:48,125] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2025-03-06 01:30:48,126] [INFO] [utils.py:786:see_memory_usage] MA 53.71 GB         Max_MA 54.39 GB         CA 54.6 GB         Max_CA 55 GB
[2025-03-06 01:30:48,127] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 214.46 GB, percent = 21.3%
[2025-03-06 01:30:50,919] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2025-03-06 01:30:50,921] [INFO] [utils.py:786:see_memory_usage] MA 56.44 GB         Max_MA 57.8 GB         CA 58.69 GB         Max_CA 59 GB
[2025-03-06 01:30:50,921] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 214.47 GB, percent = 21.3%
[2025-03-06 01:30:50,921] [INFO] [stage_1_and_2.py:488:__init__] optimizer state initialized
[2025-03-06 01:30:53,929] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2025-03-06 01:30:53,931] [INFO] [utils.py:786:see_memory_usage] MA 56.44 GB         Max_MA 56.44 GB         CA 58.69 GB         Max_CA 59 GB
[2025-03-06 01:30:53,931] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 215.42 GB, percent = 21.4%
[2025-03-06 01:30:53,937] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-03-06 01:30:53,937] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2025-03-06 01:30:53,937] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7f0227dbe710>
[2025-03-06 01:30:53,937] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
[2025-03-06 01:30:53,944] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2025-03-06 01:30:53,944] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-03-06 01:30:53,944] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-03-06 01:30:53,945] [INFO] [config.py:964:print]   amp_enabled .................. False
[2025-03-06 01:30:53,945] [INFO] [config.py:964:print]   amp_params ................... False
[2025-03-06 01:30:53,945] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-06 01:30:53,945] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2025-03-06 01:30:53,945] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2025-03-06 01:30:53,945] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2025-03-06 01:30:53,945] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2025-03-06 01:30:53,945] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7eed637d1930>
[2025-03-06 01:30:53,945] [INFO] [config.py:964:print]   communication_data_type ...... None
[2025-03-06 01:30:53,945] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-06 01:30:53,945] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2025-03-06 01:30:53,947] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2025-03-06 01:30:53,947] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-06 01:30:53,947] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2025-03-06 01:30:53,947] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2025-03-06 01:30:53,947] [INFO] [config.py:964:print]   disable_allgather ............ False
[2025-03-06 01:30:53,947] [INFO] [config.py:964:print]   dump_state ................... False
[2025-03-06 01:30:53,947] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2025-03-06 01:30:53,947] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2025-03-06 01:30:53,947] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-06 01:30:53,947] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-06 01:30:53,947] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2025-03-06 01:30:53,948] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2025-03-06 01:30:53,948] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2025-03-06 01:30:53,948] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2025-03-06 01:30:53,948] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2025-03-06 01:30:53,948] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2025-03-06 01:30:53,948] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-03-06 01:30:53,948] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2025-03-06 01:30:53,948] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2025-03-06 01:30:53,948] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2025-03-06 01:30:53,948] [INFO] [config.py:964:print]   global_rank .................. 0
[2025-03-06 01:30:53,948] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2025-03-06 01:30:53,948] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 10
[2025-03-06 01:30:53,948] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0
[2025-03-06 01:30:53,948] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2025-03-06 01:30:53,949] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-06 01:30:53,949] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2025-03-06 01:30:53,949] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2025-03-06 01:30:53,949] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2025-03-06 01:30:53,949] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2025-03-06 01:30:53,949] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2025-03-06 01:30:53,949] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2025-03-06 01:30:53,949] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-03-06 01:30:53,949] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-03-06 01:30:53,949] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2025-03-06 01:30:53,949] [INFO] [config.py:964:print]   optimizer_name ............... adamw
[2025-03-06 01:30:53,949] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.0003, 'weight_decay': 0.0, 'betas': (0.9, 0.95)}
[2025-03-06 01:30:53,949] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2025-03-06 01:30:53,949] [INFO] [config.py:964:print]   pld_enabled .................. False
[2025-03-06 01:30:53,949] [INFO] [config.py:964:print]   pld_params ................... False
[2025-03-06 01:30:53,950] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2025-03-06 01:30:53,950] [INFO] [config.py:964:print]   scheduler_name ............... WarmupDecayLR
[2025-03-06 01:30:53,950] [INFO] [config.py:964:print]   scheduler_params ............. {'total_num_steps': 25000, 'warmup_min_lr': 0, 'warmup_max_lr': 0.0003, 'warmup_num_steps': 100, 'warmup_type': 'linear'}
[2025-03-06 01:30:53,950] [INFO] [config.py:964:print]   sparse_attention ............. None
[2025-03-06 01:30:53,950] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2025-03-06 01:30:53,950] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2025-03-06 01:30:53,950] [INFO] [config.py:964:print]   train_batch_size ............. 10
[2025-03-06 01:30:53,950] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2025-03-06 01:30:53,950] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2025-03-06 01:30:53,950] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2025-03-06 01:30:53,950] [INFO] [config.py:964:print]   world_size ................... 1
[2025-03-06 01:30:53,950] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2025-03-06 01:30:53,950] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2025-03-06 01:30:53,950] [INFO] [config.py:964:print]   zero_enabled ................. True
[2025-03-06 01:30:53,950] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-06 01:30:53,950] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2025-03-06 01:30:53,951] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1,
    "gradient_accumulation_steps": 10,
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 0.0003,
            "weight_decay": 0.0,
            "betas": [0.9, 0.95]
        }
    },
    "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "total_num_steps": 2.500000e+04,
            "warmup_min_lr": 0,
            "warmup_max_lr": 0.0003,
            "warmup_num_steps": 100,
            "warmup_type": "linear"
        }
    },
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "gradient_clipping": 1.0,
    "zero_optimization": {
        "stage": 2,
        "contiguous_gradients": true,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 5.000000e+08,
        "allgather_bucket_size": 5.000000e+08
    }
}
(train) >> AFTER DEEPSPEED
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)
  return F.conv2d(input, weight, bias, self.stride,
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([1, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([2, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([3, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([5, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([7, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([4, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([8, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][  1/500]	Time  9.063 ( 9.063)	Loss 0.6108 (0.6077)	CeLoss 0.3125 (0.3145)	SegCLSLoss 0.1226 (0.1067)	KLLoss 0.0103 (0.0087)	MaskLoss 0.1163 (0.1177)	MaskBCELoss 0.0185 (0.0305)	MaskDICELoss 0.0979 (0.0871)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([12, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][  2/500]	Time  6.001 ( 6.001)	Loss 0.7091 (0.5484)	CeLoss 0.3242 (0.2834)	SegCLSLoss 0.1182 (0.0827)	KLLoss 0.0118 (0.0058)	MaskLoss 0.1600 (0.1106)	MaskBCELoss 0.0700 (0.0456)	MaskDICELoss 0.0900 (0.0649)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([9, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([6, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][  3/500]	Time  7.133 ( 7.133)	Loss 0.6202 (0.7137)	CeLoss 0.3008 (0.2474)	SegCLSLoss 0.1221 (0.0842)	KLLoss 0.0188 (0.0107)	MaskLoss 0.1249 (0.2094)	MaskBCELoss 0.0251 (0.1421)	MaskDICELoss 0.0998 (0.0673)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([11, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][  4/500]	Time  6.648 ( 6.648)	Loss 0.1047 (1.0787)	CeLoss 0.1045 (0.2644)	SegCLSLoss 0.0000 (0.0854)	KLLoss 0.0000 (0.0067)	MaskLoss 0.0000 (0.3843)	MaskBCELoss 0.0000 (0.3321)	MaskDICELoss 0.0000 (0.0523)
Epoch: [0][  5/500]	Time  6.900 ( 6.900)	Loss 0.7716 (0.5409)	CeLoss 0.3574 (0.2983)	SegCLSLoss 0.1187 (0.0830)	KLLoss 0.0030 (0.0036)	MaskLoss 0.1764 (0.0997)	MaskBCELoss 0.0790 (0.0352)	MaskDICELoss 0.0975 (0.0645)
Epoch: [0][  6/500]	Time  6.230 ( 6.230)	Loss 0.6603 (0.5529)	CeLoss 0.2539 (0.2766)	SegCLSLoss 0.1221 (0.0834)	KLLoss 0.0178 (0.0075)	MaskLoss 0.1676 (0.1153)	MaskBCELoss 0.1278 (0.0557)	MaskDICELoss 0.0398 (0.0596)
Epoch: [0][  7/500]	Time  7.575 ( 7.575)	Loss 0.6786 (0.6935)	CeLoss 0.3594 (0.2403)	SegCLSLoss 0.1182 (0.0723)	KLLoss 0.0110 (0.0084)	MaskLoss 0.1268 (0.2064)	MaskBCELoss 0.0276 (0.1521)	MaskDICELoss 0.0992 (0.0543)
Epoch: [0][  8/500]	Time  6.563 ( 6.563)	Loss 0.6844 (0.4725)	CeLoss 0.4180 (0.2644)	SegCLSLoss 0.1187 (0.0708)	KLLoss 0.0000 (0.0044)	MaskLoss 0.1031 (0.0852)	MaskBCELoss 0.0287 (0.0307)	MaskDICELoss 0.0744 (0.0545)
Epoch: [0][  9/500]	Time  7.913 ( 7.913)	Loss 3.1644 (1.2238)	CeLoss 0.3398 (0.2654)	SegCLSLoss 0.1270 (0.0982)	KLLoss 0.0155 (0.0122)	MaskLoss 1.3775 (0.4516)	MaskBCELoss 1.2879 (0.3802)	MaskDICELoss 0.0896 (0.0714)
Epoch: [0][ 10/500]	Time  7.808 ( 7.808)	Loss 2.5714 (1.2313)	CeLoss 0.3340 (0.3070)	SegCLSLoss 0.1270 (0.1114)	KLLoss 0.0155 (0.0110)	MaskLoss 1.0826 (0.4316)	MaskBCELoss 1.0192 (0.3561)	MaskDICELoss 0.0633 (0.0755)
Epoch: [0][ 11/500]	Time  7.745 ( 7.745)	Loss 0.7681 (0.7627)	CeLoss 0.4414 (0.3369)	SegCLSLoss 0.1172 (0.1065)	KLLoss 0.0049 (0.0090)	MaskLoss 0.1325 (0.1840)	MaskBCELoss 0.0376 (0.0996)	MaskDICELoss 0.0949 (0.0844)
Epoch: [0][ 12/500]	Time  6.696 ( 6.696)	Loss 0.5660 (0.7082)	CeLoss 0.2852 (0.2496)	SegCLSLoss 0.1157 (0.0854)	KLLoss 0.0079 (0.0100)	MaskLoss 0.1095 (0.2053)	MaskBCELoss 0.0103 (0.1438)	MaskDICELoss 0.0992 (0.0615)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([15, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][ 13/500]	Time  6.934 ( 6.934)	Loss 1.9570 (0.6492)	CeLoss 0.1943 (0.2646)	SegCLSLoss 0.1270 (0.0854)	KLLoss 0.0243 (0.0070)	MaskLoss 0.8433 (0.1694)	MaskBCELoss 0.7544 (0.1058)	MaskDICELoss 0.0889 (0.0636)
Epoch: [0][ 14/500]	Time  7.568 ( 7.568)	Loss 1.8346 (0.7158)	CeLoss 0.3223 (0.3030)	SegCLSLoss 0.1270 (0.0949)	KLLoss 0.0127 (0.0078)	MaskLoss 0.7220 (0.1807)	MaskBCELoss 0.6617 (0.1062)	MaskDICELoss 0.0603 (0.0745)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([13, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][ 15/500]	Time  7.547 ( 7.547)	Loss 0.7450 (0.6530)	CeLoss 0.3320 (0.3083)	SegCLSLoss 0.1250 (0.1072)	KLLoss 0.0000 (0.0079)	MaskLoss 0.1749 (0.1435)	MaskBCELoss 0.0801 (0.0609)	MaskDICELoss 0.0948 (0.0826)
Epoch: [0][ 16/500]	Time  7.741 ( 7.741)	Loss 0.7028 (0.8023)	CeLoss 0.3672 (0.2938)	SegCLSLoss 0.1221 (0.1100)	KLLoss 0.0057 (0.0122)	MaskLoss 0.1358 (0.2237)	MaskBCELoss 0.0384 (0.1462)	MaskDICELoss 0.0974 (0.0775)
Epoch: [0][ 17/500]	Time  5.008 ( 5.008)	Loss 0.1648 (0.3784)	CeLoss 0.1650 (0.2321)	SegCLSLoss 0.0000 (0.0485)	KLLoss 0.0000 (0.0026)	MaskLoss 0.0000 (0.0603)	MaskBCELoss 0.0000 (0.0242)	MaskDICELoss 0.0000 (0.0361)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([17, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][ 18/500]	Time  6.909 ( 6.909)	Loss 0.0801 (0.6722)	CeLoss 0.0801 (0.2279)	SegCLSLoss 0.0000 (0.0839)	KLLoss 0.0000 (0.0111)	MaskLoss 0.0000 (0.1983)	MaskBCELoss 0.0000 (0.1361)	MaskDICELoss 0.0000 (0.0622)
Epoch: [0][ 19/500]	Time  5.960 ( 5.960)	Loss 0.6766 (0.3375)	CeLoss 0.3418 (0.1998)	SegCLSLoss 0.1172 (0.0469)	KLLoss 0.0097 (0.0043)	MaskLoss 0.1352 (0.0560)	MaskBCELoss 0.0414 (0.0195)	MaskDICELoss 0.0938 (0.0365)
Epoch: [0][ 20/500]	Time  7.188 ( 7.188)	Loss 0.6961 (0.7475)	CeLoss 0.3574 (0.3611)	SegCLSLoss 0.1240 (0.1092)	KLLoss 0.0099 (0.0054)	MaskLoss 0.1356 (0.1644)	MaskBCELoss 0.0413 (0.0857)	MaskDICELoss 0.0942 (0.0787)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([18, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][ 21/500]	Time  5.177 ( 5.177)	Loss 0.1039 (0.4917)	CeLoss 0.1040 (0.2258)	SegCLSLoss 0.0000 (0.0738)	KLLoss 0.0000 (0.0081)	MaskLoss 0.0000 (0.1125)	MaskBCELoss 0.0000 (0.0540)	MaskDICELoss 0.0000 (0.0586)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([10, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][ 22/500]	Time  7.735 ( 7.735)	Loss 0.8915 (0.6634)	CeLoss 0.3086 (0.3189)	SegCLSLoss 0.1270 (0.1091)	KLLoss 0.0150 (0.0084)	MaskLoss 0.2559 (0.1430)	MaskBCELoss 0.1559 (0.0559)	MaskDICELoss 0.1000 (0.0871)
Epoch: [0][ 23/500]	Time  6.472 ( 6.472)	Loss 0.7226 (0.6587)	CeLoss 0.4121 (0.3111)	SegCLSLoss 0.1187 (0.0955)	KLLoss 0.0000 (0.0056)	MaskLoss 0.1254 (0.1484)	MaskBCELoss 0.0472 (0.0794)	MaskDICELoss 0.0781 (0.0690)
Epoch: [0][ 24/500]	Time  6.236 ( 6.236)	Loss 0.1187 (0.3503)	CeLoss 0.1187 (0.2096)	SegCLSLoss 0.0000 (0.0597)	KLLoss 0.0000 (0.0056)	MaskLoss 0.0000 (0.0539)	MaskBCELoss 0.0000 (0.0069)	MaskDICELoss 0.0000 (0.0471)
Epoch: [0][ 25/500]	Time  7.317 ( 7.317)	Loss 0.7557 (0.5989)	CeLoss 0.3047 (0.2718)	SegCLSLoss 0.1270 (0.0968)	KLLoss 0.0201 (0.0111)	MaskLoss 0.1888 (0.1365)	MaskBCELoss 0.0888 (0.0581)	MaskDICELoss 0.1000 (0.0783)
Epoch: [0][ 26/500]	Time  7.061 ( 7.061)	Loss 0.6136 (0.5357)	CeLoss 0.3066 (0.2906)	SegCLSLoss 0.1133 (0.0960)	KLLoss 0.0085 (0.0072)	MaskLoss 0.1232 (0.0967)	MaskBCELoss 0.0235 (0.0192)	MaskDICELoss 0.0997 (0.0774)
Epoch: [0][ 27/500]	Time  7.189 ( 7.189)	Loss 0.6203 (0.5376)	CeLoss 0.3242 (0.2570)	SegCLSLoss 0.1162 (0.0839)	KLLoss 0.0079 (0.0078)	MaskLoss 0.1172 (0.1174)	MaskBCELoss 0.0173 (0.0487)	MaskDICELoss 0.0999 (0.0687)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([20, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([24, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][ 28/500]	Time  7.170 ( 7.170)	Loss 0.7301 (0.5084)	CeLoss 0.3164 (0.2444)	SegCLSLoss 0.1118 (0.0848)	KLLoss 0.0033 (0.0083)	MaskLoss 0.1783 (0.1088)	MaskBCELoss 0.0887 (0.0419)	MaskDICELoss 0.0896 (0.0669)
Epoch: [0][ 29/500]	Time  6.100 ( 6.100)	Loss 0.7664 (0.5092)	CeLoss 0.4180 (0.2867)	SegCLSLoss 0.1211 (0.0834)	KLLoss 0.0121 (0.0076)	MaskLoss 0.1410 (0.0884)	MaskBCELoss 0.0482 (0.0202)	MaskDICELoss 0.0928 (0.0682)
Epoch: [0][ 30/500]	Time  7.200 ( 7.200)	Loss 0.6417 (0.6039)	CeLoss 0.2539 (0.2718)	SegCLSLoss 0.1270 (0.1102)	KLLoss 0.0178 (0.0121)	MaskLoss 0.1568 (0.1353)	MaskBCELoss 0.0568 (0.0491)	MaskDICELoss 0.1000 (0.0862)
Epoch: [0][ 31/500]	Time  6.665 ( 6.665)	Loss 0.5274 (0.4174)	CeLoss 0.2539 (0.1977)	SegCLSLoss 0.1201 (0.0724)	KLLoss 0.0165 (0.0071)	MaskLoss 0.1020 (0.0899)	MaskBCELoss 0.0022 (0.0322)	MaskDICELoss 0.0997 (0.0577)
Epoch: [0][ 32/500]	Time  7.639 ( 7.639)	Loss 0.6552 (0.5848)	CeLoss 0.2539 (0.2686)	SegCLSLoss 0.1270 (0.0981)	KLLoss 0.0200 (0.0087)	MaskLoss 0.1635 (0.1311)	MaskBCELoss 0.0636 (0.0529)	MaskDICELoss 0.1000 (0.0782)
Epoch: [0][ 33/500]	Time  7.457 ( 7.457)	Loss 0.8817 (0.6166)	CeLoss 0.3848 (0.3026)	SegCLSLoss 0.1201 (0.1082)	KLLoss 0.0000 (0.0076)	MaskLoss 0.2190 (0.1280)	MaskBCELoss 0.1261 (0.0415)	MaskDICELoss 0.0928 (0.0865)
Epoch: [0][ 34/500]	Time  7.309 ( 7.309)	Loss 0.5958 (0.6956)	CeLoss 0.2451 (0.3062)	SegCLSLoss 0.1226 (0.1201)	KLLoss 0.0148 (0.0102)	MaskLoss 0.1409 (0.1622)	MaskBCELoss 0.0421 (0.0680)	MaskDICELoss 0.0987 (0.0942)
Epoch: [0][ 35/500]	Time  7.335 ( 7.335)	Loss 0.5854 (0.5661)	CeLoss 0.2559 (0.2299)	SegCLSLoss 0.1196 (0.0970)	KLLoss 0.0212 (0.0130)	MaskLoss 0.1294 (0.1409)	MaskBCELoss 0.0295 (0.0611)	MaskDICELoss 0.0999 (0.0798)
Epoch: [0][ 36/500]	Time  7.142 ( 7.142)	Loss 0.5642 (0.4875)	CeLoss 0.2949 (0.2476)	SegCLSLoss 0.1221 (0.0850)	KLLoss 0.0114 (0.0065)	MaskLoss 0.1009 (0.0971)	MaskBCELoss 0.0013 (0.0276)	MaskDICELoss 0.0995 (0.0694)
Epoch: [0][ 37/500]	Time  6.677 ( 6.677)	Loss 0.5346 (0.4483)	CeLoss 0.2598 (0.2291)	SegCLSLoss 0.1211 (0.0834)	KLLoss 0.0116 (0.0078)	MaskLoss 0.1040 (0.0868)	MaskBCELoss 0.0041 (0.0193)	MaskDICELoss 0.0999 (0.0675)
Epoch: [0][ 38/500]	Time  7.091 ( 7.091)	Loss 0.6618 (0.7203)	CeLoss 0.2539 (0.2484)	SegCLSLoss 0.1196 (0.0966)	KLLoss 0.0123 (0.0096)	MaskLoss 0.1715 (0.2094)	MaskBCELoss 0.0760 (0.1327)	MaskDICELoss 0.0955 (0.0768)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([19, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][ 39/500]	Time  6.434 ( 6.434)	Loss 0.6746 (0.4483)	CeLoss 0.2314 (0.2197)	SegCLSLoss 0.1196 (0.0830)	KLLoss 0.0116 (0.0080)	MaskLoss 0.1889 (0.0917)	MaskBCELoss 0.0905 (0.0260)	MaskDICELoss 0.0983 (0.0657)
Epoch: [0][ 40/500]	Time  7.471 ( 7.471)	Loss 0.7734 (0.7201)	CeLoss 0.2520 (0.2615)	SegCLSLoss 0.1270 (0.1091)	KLLoss 0.0173 (0.0123)	MaskLoss 0.2242 (0.1990)	MaskBCELoss 0.1243 (0.1127)	MaskDICELoss 0.0999 (0.0863)
Epoch: [0][ 41/500]	Time  6.194 ( 6.194)	Loss 0.5112 (0.4212)	CeLoss 0.1885 (0.1895)	SegCLSLoss 0.1270 (0.0851)	KLLoss 0.0225 (0.0116)	MaskLoss 0.1243 (0.0918)	MaskBCELoss 0.0253 (0.0265)	MaskDICELoss 0.0991 (0.0653)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([22, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([14, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][ 42/500]	Time  8.345 ( 8.345)	Loss 0.6904 (0.5548)	CeLoss 0.3164 (0.2499)	SegCLSLoss 0.1270 (0.1095)	KLLoss 0.0081 (0.0114)	MaskLoss 0.1522 (0.1224)	MaskBCELoss 0.0534 (0.0355)	MaskDICELoss 0.0988 (0.0870)
Epoch: [0][ 43/500]	Time  6.687 ( 6.687)	Loss 0.6311 (0.5328)	CeLoss 0.3164 (0.2536)	SegCLSLoss 0.1118 (0.0953)	KLLoss 0.0040 (0.0066)	MaskLoss 0.1288 (0.1143)	MaskBCELoss 0.0440 (0.0439)	MaskDICELoss 0.0849 (0.0703)
Epoch: [0][ 44/500]	Time  5.357 ( 5.357)	Loss 0.5994 (0.3867)	CeLoss 0.2871 (0.1898)	SegCLSLoss 0.1201 (0.0602)	KLLoss 0.0082 (0.0052)	MaskLoss 0.1239 (0.0821)	MaskBCELoss 0.0278 (0.0340)	MaskDICELoss 0.0961 (0.0481)
Epoch: [0][ 45/500]	Time  6.002 ( 6.002)	Loss 0.5855 (0.5152)	CeLoss 0.2773 (0.2518)	SegCLSLoss 0.1196 (0.0945)	KLLoss 0.0104 (0.0050)	MaskLoss 0.1224 (0.1070)	MaskBCELoss 0.0235 (0.0366)	MaskDICELoss 0.0989 (0.0704)
Epoch: [0][ 46/500]	Time  6.420 ( 6.420)	Loss 0.1000 (0.3461)	CeLoss 0.1001 (0.1810)	SegCLSLoss 0.0000 (0.0608)	KLLoss 0.0000 (0.0069)	MaskLoss 0.0000 (0.0656)	MaskBCELoss 0.0000 (0.0191)	MaskDICELoss 0.0000 (0.0464)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([16, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][ 47/500]	Time  6.977 ( 6.977)	Loss 0.5976 (0.4896)	CeLoss 0.2617 (0.2286)	SegCLSLoss 0.1182 (0.0952)	KLLoss 0.0118 (0.0094)	MaskLoss 0.1347 (0.1043)	MaskBCELoss 0.0349 (0.0286)	MaskDICELoss 0.0998 (0.0756)
Epoch: [0][ 48/500]	Time  6.982 ( 6.982)	Loss 0.5879 (0.5779)	CeLoss 0.2617 (0.2497)	SegCLSLoss 0.1172 (0.1073)	KLLoss 0.0076 (0.0109)	MaskLoss 0.1330 (0.1347)	MaskBCELoss 0.0405 (0.0513)	MaskDICELoss 0.0925 (0.0834)
Epoch: [0][ 49/500]	Time  8.093 ( 8.093)	Loss 0.1102 (0.4951)	CeLoss 0.1104 (0.2370)	SegCLSLoss 0.0000 (0.0981)	KLLoss 0.0000 (0.0105)	MaskLoss 0.0000 (0.1019)	MaskBCELoss 0.0000 (0.0245)	MaskDICELoss 0.0000 (0.0774)
Epoch: [0][ 50/500]	Time  7.896 ( 7.896)	Loss 0.4818 (0.4315)	CeLoss 0.1855 (0.2062)	SegCLSLoss 0.1196 (0.0845)	KLLoss 0.0203 (0.0106)	MaskLoss 0.1128 (0.0888)	MaskBCELoss 0.0143 (0.0217)	MaskDICELoss 0.0985 (0.0671)
Epoch: [0][ 51/500]	Time  5.363 ( 5.363)	Loss 0.5435 (0.4360)	CeLoss 0.2236 (0.1950)	SegCLSLoss 0.1147 (0.0837)	KLLoss 0.0120 (0.0060)	MaskLoss 0.1280 (0.0981)	MaskBCELoss 0.0287 (0.0340)	MaskDICELoss 0.0993 (0.0641)
Epoch: [0][ 52/500]	Time  7.703 ( 7.703)	Loss 0.6301 (0.4707)	CeLoss 0.2363 (0.2004)	SegCLSLoss 0.1221 (0.0975)	KLLoss 0.0128 (0.0077)	MaskLoss 0.1635 (0.1088)	MaskBCELoss 0.0638 (0.0329)	MaskDICELoss 0.0997 (0.0759)
Epoch: [0][ 53/500]	Time  6.887 ( 6.887)	Loss 0.7513 (0.4429)	CeLoss 0.2559 (0.1955)	SegCLSLoss 0.1187 (0.0853)	KLLoss 0.0000 (0.0074)	MaskLoss 0.2178 (0.1006)	MaskBCELoss 0.1259 (0.0392)	MaskDICELoss 0.0919 (0.0614)
Epoch: [0][ 54/500]	Time  7.230 ( 7.230)	Loss 0.5037 (0.4615)	CeLoss 0.1963 (0.2074)	SegCLSLoss 0.1196 (0.0951)	KLLoss 0.0120 (0.0071)	MaskLoss 0.1206 (0.1013)	MaskBCELoss 0.0212 (0.0270)	MaskDICELoss 0.0994 (0.0743)
Epoch: [0][ 55/500]	Time  6.931 ( 6.931)	Loss 0.1234 (0.3939)	CeLoss 0.1235 (0.1958)	SegCLSLoss 0.0000 (0.0734)	KLLoss 0.0000 (0.0059)	MaskLoss 0.0000 (0.0792)	MaskBCELoss 0.0000 (0.0225)	MaskDICELoss 0.0000 (0.0567)
Epoch: [0][ 56/500]	Time  6.363 ( 6.363)	Loss 0.6528 (0.4388)	CeLoss 0.3203 (0.2036)	SegCLSLoss 0.1182 (0.0841)	KLLoss 0.0032 (0.0065)	MaskLoss 0.1358 (0.0951)	MaskBCELoss 0.0444 (0.0341)	MaskDICELoss 0.0913 (0.0610)
Epoch: [0][ 57/500]	Time  7.570 ( 7.570)	Loss 0.4848 (0.5259)	CeLoss 0.1934 (0.2210)	SegCLSLoss 0.1123 (0.1096)	KLLoss 0.0084 (0.0113)	MaskLoss 0.1151 (0.1223)	MaskBCELoss 0.0156 (0.0436)	MaskDICELoss 0.0995 (0.0786)
Epoch: [0][ 58/500]	Time  8.079 ( 8.079)	Loss 0.5815 (0.4617)	CeLoss 0.2773 (0.1965)	SegCLSLoss 0.1172 (0.0967)	KLLoss 0.0093 (0.0102)	MaskLoss 0.1212 (0.1058)	MaskBCELoss 0.0256 (0.0332)	MaskDICELoss 0.0956 (0.0726)
Epoch: [0][ 59/500]	Time  6.364 ( 6.364)	Loss 0.6206 (0.4266)	CeLoss 0.2598 (0.1868)	SegCLSLoss 0.1260 (0.0858)	KLLoss 0.0097 (0.0096)	MaskLoss 0.1470 (0.0961)	MaskBCELoss 0.0727 (0.0355)	MaskDICELoss 0.0743 (0.0607)
Epoch: [0][ 60/500]	Time  7.148 ( 7.148)	Loss 0.1047 (0.4045)	CeLoss 0.1045 (0.1930)	SegCLSLoss 0.0000 (0.0728)	KLLoss 0.0000 (0.0068)	MaskLoss 0.0000 (0.0859)	MaskBCELoss 0.0000 (0.0302)	MaskDICELoss 0.0000 (0.0558)
Epoch: [0][ 61/500]	Time  6.611 ( 6.611)	Loss 0.6012 (0.4504)	CeLoss 0.2266 (0.1757)	SegCLSLoss 0.1211 (0.0949)	KLLoss 0.0087 (0.0104)	MaskLoss 0.1545 (0.1110)	MaskBCELoss 0.0669 (0.0402)	MaskDICELoss 0.0876 (0.0708)
Epoch: [0][ 62/500]	Time  7.310 ( 7.310)	Loss 0.5036 (0.4230)	CeLoss 0.1855 (0.1633)	SegCLSLoss 0.1260 (0.0984)	KLLoss 0.0190 (0.0109)	MaskLoss 0.1229 (0.1026)	MaskBCELoss 0.0333 (0.0328)	MaskDICELoss 0.0895 (0.0698)
Epoch: [0][ 63/500]	Time  7.982 ( 7.982)	Loss 0.5151 (0.4417)	CeLoss 0.1924 (0.1714)	SegCLSLoss 0.1221 (0.0958)	KLLoss 0.0116 (0.0102)	MaskLoss 0.1278 (0.1085)	MaskBCELoss 0.0417 (0.0348)	MaskDICELoss 0.0862 (0.0737)
Epoch: [0][ 64/500]	Time  8.261 ( 8.261)	Loss 0.5413 (0.4295)	CeLoss 0.1934 (0.1662)	SegCLSLoss 0.1143 (0.0951)	KLLoss 0.0096 (0.0080)	MaskLoss 0.1433 (0.1060)	MaskBCELoss 0.0446 (0.0317)	MaskDICELoss 0.0988 (0.0743)
Epoch: [0][ 65/500]	Time  5.406 ( 5.406)	Loss 0.5024 (0.3559)	CeLoss 0.1475 (0.1490)	SegCLSLoss 0.1221 (0.0707)	KLLoss 0.0205 (0.0083)	MaskLoss 0.1411 (0.0836)	MaskBCELoss 0.0539 (0.0298)	MaskDICELoss 0.0872 (0.0539)
Epoch: [0][ 66/500]	Time  7.795 ( 7.795)	Loss 0.1133 (0.4518)	CeLoss 0.1133 (0.1853)	SegCLSLoss 0.0000 (0.0957)	KLLoss 0.0000 (0.0092)	MaskLoss 0.0000 (0.1070)	MaskBCELoss 0.0000 (0.0363)	MaskDICELoss 0.0000 (0.0707)
Epoch: [0][ 67/500]	Time  6.258 ( 6.258)	Loss 0.0984 (0.4416)	CeLoss 0.0986 (0.1664)	SegCLSLoss 0.0000 (0.0966)	KLLoss 0.0000 (0.0108)	MaskLoss 0.0000 (0.1107)	MaskBCELoss 0.0000 (0.0351)	MaskDICELoss 0.0000 (0.0757)
Epoch: [0][ 68/500]	Time  6.517 ( 6.517)	Loss 0.4958 (0.3904)	CeLoss 0.1660 (0.1643)	SegCLSLoss 0.1260 (0.0853)	KLLoss 0.0210 (0.0093)	MaskLoss 0.1276 (0.0892)	MaskBCELoss 0.0374 (0.0273)	MaskDICELoss 0.0902 (0.0620)
Epoch: [0][ 69/500]	Time  6.883 ( 6.883)	Loss 0.5175 (0.4693)	CeLoss 0.1621 (0.1701)	SegCLSLoss 0.1196 (0.1089)	KLLoss 0.0124 (0.0127)	MaskLoss 0.1447 (0.1192)	MaskBCELoss 0.0449 (0.0376)	MaskDICELoss 0.0998 (0.0816)
Epoch: [0][ 70/500]	Time  6.213 ( 6.213)	Loss 0.5835 (0.3635)	CeLoss 0.2344 (0.1629)	SegCLSLoss 0.1196 (0.0726)	KLLoss 0.0004 (0.0064)	MaskLoss 0.1449 (0.0806)	MaskBCELoss 0.0650 (0.0272)	MaskDICELoss 0.0798 (0.0534)
Epoch: [0][ 71/500]	Time  7.157 ( 7.157)	Loss 0.4352 (0.4245)	CeLoss 0.1357 (0.1349)	SegCLSLoss 0.1147 (0.1055)	KLLoss 0.0124 (0.0093)	MaskLoss 0.1176 (0.1161)	MaskBCELoss 0.0233 (0.0338)	MaskDICELoss 0.0943 (0.0822)
Epoch: [0][ 72/500]	Time  8.380 ( 8.380)	Loss 0.4396 (0.4690)	CeLoss 0.1387 (0.1441)	SegCLSLoss 0.1123 (0.1207)	KLLoss 0.0165 (0.0143)	MaskLoss 0.1182 (0.1286)	MaskBCELoss 0.0195 (0.0360)	MaskDICELoss 0.0988 (0.0926)
Epoch: [0][ 73/500]	Time  5.970 ( 5.970)	Loss 0.4198 (0.3871)	CeLoss 0.0962 (0.1328)	SegCLSLoss 0.1240 (0.0958)	KLLoss 0.0234 (0.0119)	MaskLoss 0.1251 (0.1004)	MaskBCELoss 0.0553 (0.0300)	MaskDICELoss 0.0698 (0.0704)
Epoch: [0][ 74/500]	Time  8.554 ( 8.554)	Loss 0.5094 (0.3618)	CeLoss 0.1895 (0.1375)	SegCLSLoss 0.1133 (0.0804)	KLLoss 0.0027 (0.0072)	MaskLoss 0.1305 (0.0903)	MaskBCELoss 0.0462 (0.0256)	MaskDICELoss 0.0843 (0.0647)
Epoch: [0][ 75/500]	Time  6.031 ( 6.031)	Loss 0.5446 (0.3665)	CeLoss 0.2207 (0.1445)	SegCLSLoss 0.1235 (0.0824)	KLLoss 0.0006 (0.0055)	MaskLoss 0.1317 (0.0891)	MaskBCELoss 0.0317 (0.0244)	MaskDICELoss 0.1000 (0.0647)
Epoch: [0][ 76/500]	Time  7.461 ( 7.461)	Loss 0.4649 (0.4732)	CeLoss 0.1318 (0.1548)	SegCLSLoss 0.1123 (0.1186)	KLLoss 0.0204 (0.0106)	MaskLoss 0.1332 (0.1271)	MaskBCELoss 0.0335 (0.0384)	MaskDICELoss 0.0997 (0.0888)
Epoch: [0][ 77/500]	Time  7.780 ( 7.780)	Loss 0.4670 (0.4349)	CeLoss 0.1670 (0.1432)	SegCLSLoss 0.1162 (0.1059)	KLLoss 0.0155 (0.0076)	MaskLoss 0.1171 (0.1176)	MaskBCELoss 0.0418 (0.0425)	MaskDICELoss 0.0753 (0.0751)
Epoch: [0][ 78/500]	Time  7.717 ( 7.717)	Loss 0.4943 (0.3263)	CeLoss 0.1553 (0.1353)	SegCLSLoss 0.1147 (0.0702)	KLLoss 0.0109 (0.0069)	MaskLoss 0.1378 (0.0763)	MaskBCELoss 0.0456 (0.0209)	MaskDICELoss 0.0921 (0.0554)
Epoch: [0][ 79/500]	Time  7.554 ( 7.554)	Loss 0.4654 (0.3697)	CeLoss 0.1680 (0.1481)	SegCLSLoss 0.1250 (0.0840)	KLLoss 0.0220 (0.0076)	MaskLoss 0.1116 (0.0881)	MaskBCELoss 0.0164 (0.0222)	MaskDICELoss 0.0952 (0.0659)
Epoch: [0][ 80/500]	Time  6.422 ( 6.422)	Loss 0.6957 (0.3422)	CeLoss 0.1738 (0.1325)	SegCLSLoss 0.1240 (0.0728)	KLLoss 0.0135 (0.0060)	MaskLoss 0.2267 (0.0852)	MaskBCELoss 0.1387 (0.0310)	MaskDICELoss 0.0880 (0.0542)
Epoch: [0][ 81/500]	Time  6.846 ( 6.846)	Loss 0.3736 (0.3198)	CeLoss 0.0508 (0.1027)	SegCLSLoss 0.1162 (0.0807)	KLLoss 0.0094 (0.0069)	MaskLoss 0.1298 (0.0866)	MaskBCELoss 0.0533 (0.0226)	MaskDICELoss 0.0764 (0.0640)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([25, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][ 82/500]	Time  8.379 ( 8.379)	Loss 0.4637 (0.3089)	CeLoss 0.1660 (0.1144)	SegCLSLoss 0.1182 (0.0721)	KLLoss 0.0064 (0.0088)	MaskLoss 0.1178 (0.0771)	MaskBCELoss 0.0299 (0.0228)	MaskDICELoss 0.0878 (0.0543)
Epoch: [0][ 83/500]	Time  6.485 ( 6.485)	Loss 0.4549 (0.4147)	CeLoss 0.1465 (0.1242)	SegCLSLoss 0.1221 (0.1075)	KLLoss 0.0165 (0.0096)	MaskLoss 0.1196 (0.1159)	MaskBCELoss 0.0259 (0.0373)	MaskDICELoss 0.0937 (0.0787)
Epoch: [0][ 84/500]	Time  7.102 ( 7.102)	Loss 0.4160 (0.3673)	CeLoss 0.0879 (0.1195)	SegCLSLoss 0.1211 (0.0941)	KLLoss 0.0171 (0.0086)	MaskLoss 0.1295 (0.0982)	MaskBCELoss 0.0437 (0.0249)	MaskDICELoss 0.0858 (0.0733)
Epoch: [0][ 85/500]	Time  7.426 ( 7.426)	Loss 0.4642 (0.4476)	CeLoss 0.1387 (0.1218)	SegCLSLoss 0.1133 (0.1162)	KLLoss 0.0066 (0.0094)	MaskLoss 0.1329 (0.1315)	MaskBCELoss 0.0565 (0.0403)	MaskDICELoss 0.0763 (0.0911)
Epoch: [0][ 86/500]	Time  6.684 ( 6.684)	Loss 0.4843 (0.4090)	CeLoss 0.1582 (0.1165)	SegCLSLoss 0.1201 (0.1060)	KLLoss 0.0045 (0.0121)	MaskLoss 0.1320 (0.1168)	MaskBCELoss 0.0454 (0.0328)	MaskDICELoss 0.0866 (0.0840)
Epoch: [0][ 87/500]	Time  7.253 ( 7.253)	Loss 0.3607 (0.3172)	CeLoss 0.0801 (0.1053)	SegCLSLoss 0.1143 (0.0819)	KLLoss 0.0122 (0.0096)	MaskLoss 0.1085 (0.0831)	MaskBCELoss 0.0159 (0.0154)	MaskDICELoss 0.0926 (0.0677)
Epoch: [0][ 88/500]	Time  7.312 ( 7.312)	Loss 0.1164 (0.3851)	CeLoss 0.1162 (0.1323)	SegCLSLoss 0.0000 (0.0931)	KLLoss 0.0000 (0.0065)	MaskLoss 0.0000 (0.1014)	MaskBCELoss 0.0000 (0.0279)	MaskDICELoss 0.0000 (0.0736)
Epoch: [0][ 89/500]	Time  6.670 ( 6.670)	Loss 0.4426 (0.4224)	CeLoss 0.1211 (0.1448)	SegCLSLoss 0.1157 (0.1062)	KLLoss 0.0071 (0.0076)	MaskLoss 0.1299 (0.1104)	MaskBCELoss 0.0439 (0.0317)	MaskDICELoss 0.0860 (0.0786)
Epoch: [0][ 90/500]	Time  7.540 ( 7.540)	Loss 0.4435 (0.3818)	CeLoss 0.1318 (0.1100)	SegCLSLoss 0.1143 (0.1052)	KLLoss 0.0044 (0.0104)	MaskLoss 0.1260 (0.1070)	MaskBCELoss 0.0273 (0.0224)	MaskDICELoss 0.0988 (0.0845)
Epoch: [0][ 91/500]	Time  7.462 ( 7.462)	Loss 0.4126 (0.3464)	CeLoss 0.1279 (0.1058)	SegCLSLoss 0.1196 (0.0915)	KLLoss 0.0038 (0.0075)	MaskLoss 0.1118 (0.0957)	MaskBCELoss 0.0118 (0.0216)	MaskDICELoss 0.1000 (0.0741)
Epoch: [0][ 92/500]	Time  8.017 ( 8.017)	Loss 0.5258 (0.3641)	CeLoss 0.1562 (0.1076)	SegCLSLoss 0.1133 (0.0930)	KLLoss 0.0082 (0.0097)	MaskLoss 0.1543 (0.1025)	MaskBCELoss 0.0721 (0.0291)	MaskDICELoss 0.0822 (0.0735)
Epoch: [0][ 93/500]	Time  6.542 ( 6.542)	Loss 0.1820 (0.3161)	CeLoss 0.1816 (0.1068)	SegCLSLoss 0.0000 (0.0829)	KLLoss 0.0000 (0.0114)	MaskLoss 0.0000 (0.0811)	MaskBCELoss 0.0000 (0.0162)	MaskDICELoss 0.0000 (0.0649)
Epoch: [0][ 94/500]	Time  6.758 ( 6.758)	Loss 0.3921 (0.3642)	CeLoss 0.1108 (0.1128)	SegCLSLoss 0.1147 (0.0931)	KLLoss 0.0164 (0.0097)	MaskLoss 0.1078 (0.1000)	MaskBCELoss 0.0080 (0.0266)	MaskDICELoss 0.0998 (0.0734)
Epoch: [0][ 95/500]	Time  7.425 ( 7.425)	Loss 0.0797 (0.3524)	CeLoss 0.0796 (0.1020)	SegCLSLoss 0.0000 (0.0929)	KLLoss 0.0000 (0.0106)	MaskLoss 0.0000 (0.0993)	MaskBCELoss 0.0000 (0.0254)	MaskDICELoss 0.0000 (0.0740)
Epoch: [0][ 96/500]	Time  6.674 ( 6.674)	Loss 0.4298 (0.3193)	CeLoss 0.1235 (0.1115)	SegCLSLoss 0.1162 (0.0787)	KLLoss 0.0039 (0.0081)	MaskLoss 0.1235 (0.0822)	MaskBCELoss 0.0330 (0.0173)	MaskDICELoss 0.0905 (0.0650)
Epoch: [0][ 97/500]	Time  7.817 ( 7.817)	Loss 0.4736 (0.3383)	CeLoss 0.1240 (0.1077)	SegCLSLoss 0.1221 (0.0818)	KLLoss 0.0143 (0.0083)	MaskLoss 0.1407 (0.0927)	MaskBCELoss 0.0537 (0.0287)	MaskDICELoss 0.0870 (0.0641)
Epoch: [0][ 98/500]	Time  6.445 ( 6.445)	Loss 0.3578 (0.2394)	CeLoss 0.0625 (0.0967)	SegCLSLoss 0.1094 (0.0576)	KLLoss 0.0038 (0.0059)	MaskLoss 0.1195 (0.0555)	MaskBCELoss 0.0203 (0.0065)	MaskDICELoss 0.0992 (0.0490)
Epoch: [0][ 99/500]	Time  7.405 ( 7.405)	Loss 0.4175 (0.4254)	CeLoss 0.1118 (0.1194)	SegCLSLoss 0.1172 (0.1053)	KLLoss 0.0159 (0.0117)	MaskLoss 0.1197 (0.1238)	MaskBCELoss 0.0275 (0.0440)	MaskDICELoss 0.0922 (0.0798)
[2025-03-06 01:42:38,561] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[0.00029991566265060235], mom=[(0.9, 0.95)]
[2025-03-06 01:42:38,566] [INFO] [timer.py:215:stop] epoch=0/micro_step=100/global_step=10, RunningAvgSamplesPerSec=1.3695250798040937, CurrSamplesPerSec=1.4779184889424544, MemAllocated=57.61GB, MaxMemAllocated=62.82GB
Epoch: [0][100/500]	Time  7.296 ( 7.296)	Loss 0.3698 (0.2994)	CeLoss 0.0811 (0.1051)	SegCLSLoss 0.1143 (0.0696)	KLLoss 0.0162 (0.0083)	MaskLoss 0.1122 (0.0777)	MaskBCELoss 0.0133 (0.0217)	MaskDICELoss 0.0989 (0.0561)
Epoch: [0][101/500]	Time  7.331 ( 7.331)	Loss 0.4066 (0.3552)	CeLoss 0.0972 (0.1022)	SegCLSLoss 0.1201 (0.0908)	KLLoss 0.0111 (0.0108)	MaskLoss 0.1217 (0.1010)	MaskBCELoss 0.0295 (0.0264)	MaskDICELoss 0.0922 (0.0746)
Epoch: [0][102/500]	Time  5.776 ( 5.776)	Loss 0.3685 (0.2894)	CeLoss 0.0889 (0.1095)	SegCLSLoss 0.1045 (0.0681)	KLLoss 0.0104 (0.0073)	MaskLoss 0.1112 (0.0711)	MaskBCELoss 0.0189 (0.0158)	MaskDICELoss 0.0923 (0.0553)
Epoch: [0][103/500]	Time  8.017 ( 8.017)	Loss 0.1133 (0.3169)	CeLoss 0.1133 (0.0971)	SegCLSLoss 0.0000 (0.0792)	KLLoss 0.0000 (0.0098)	MaskLoss 0.0000 (0.0877)	MaskBCELoss 0.0000 (0.0221)	MaskDICELoss 0.0000 (0.0656)
Epoch: [0][104/500]	Time  7.517 ( 7.517)	Loss 0.1328 (0.3991)	CeLoss 0.1328 (0.1056)	SegCLSLoss 0.0000 (0.1023)	KLLoss 0.0000 (0.0107)	MaskLoss 0.0000 (0.1185)	MaskBCELoss 0.0000 (0.0354)	MaskDICELoss 0.0000 (0.0830)
Epoch: [0][105/500]	Time  8.130 ( 8.130)	Loss 0.3664 (0.3518)	CeLoss 0.0737 (0.0957)	SegCLSLoss 0.1123 (0.0943)	KLLoss 0.0083 (0.0116)	MaskLoss 0.1164 (0.1016)	MaskBCELoss 0.0214 (0.0320)	MaskDICELoss 0.0950 (0.0696)
Epoch: [0][106/500]	Time  5.783 ( 5.783)	Loss 0.1023 (0.3193)	CeLoss 0.1025 (0.0939)	SegCLSLoss 0.0000 (0.0821)	KLLoss 0.0000 (0.0107)	MaskLoss 0.0000 (0.0895)	MaskBCELoss 0.0000 (0.0271)	MaskDICELoss 0.0000 (0.0624)
Epoch: [0][107/500]	Time  6.814 ( 6.814)	Loss 0.3877 (0.3498)	CeLoss 0.0986 (0.1052)	SegCLSLoss 0.1196 (0.0928)	KLLoss 0.0227 (0.0102)	MaskLoss 0.1087 (0.0966)	MaskBCELoss 0.0094 (0.0240)	MaskDICELoss 0.0993 (0.0726)
Epoch: [0][108/500]	Time  7.781 ( 7.781)	Loss 0.1289 (0.3773)	CeLoss 0.1289 (0.1046)	SegCLSLoss 0.0000 (0.1004)	KLLoss 0.0000 (0.0078)	MaskLoss 0.0000 (0.1092)	MaskBCELoss 0.0000 (0.0253)	MaskDICELoss 0.0000 (0.0839)
Epoch: [0][109/500]	Time  7.954 ( 7.954)	Loss 0.4528 (0.3416)	CeLoss 0.1250 (0.0935)	SegCLSLoss 0.1196 (0.0929)	KLLoss 0.0059 (0.0104)	MaskLoss 0.1326 (0.0983)	MaskBCELoss 0.0487 (0.0271)	MaskDICELoss 0.0839 (0.0712)
Epoch: [0][110/500]	Time  6.399 ( 6.399)	Loss 0.4505 (0.2938)	CeLoss 0.1201 (0.1004)	SegCLSLoss 0.1157 (0.0693)	KLLoss 0.0084 (0.0064)	MaskLoss 0.1342 (0.0778)	MaskBCELoss 0.0460 (0.0224)	MaskDICELoss 0.0883 (0.0554)
Epoch: [0][111/500]	Time  7.301 ( 7.301)	Loss 0.4019 (0.3679)	CeLoss 0.0850 (0.0889)	SegCLSLoss 0.1143 (0.1021)	KLLoss 0.0089 (0.0140)	MaskLoss 0.1275 (0.1105)	MaskBCELoss 0.0370 (0.0267)	MaskDICELoss 0.0905 (0.0838)
Epoch: [0][112/500]	Time  6.016 ( 6.016)	Loss 0.3723 (0.3471)	CeLoss 0.0649 (0.0936)	SegCLSLoss 0.1118 (0.0882)	KLLoss 0.0110 (0.0068)	MaskLoss 0.1233 (0.1031)	MaskBCELoss 0.0276 (0.0314)	MaskDICELoss 0.0957 (0.0717)
Epoch: [0][113/500]	Time  6.978 ( 6.978)	Loss 0.3717 (0.3347)	CeLoss 0.0889 (0.1024)	SegCLSLoss 0.1133 (0.0894)	KLLoss 0.0096 (0.0109)	MaskLoss 0.1108 (0.0911)	MaskBCELoss 0.0112 (0.0141)	MaskDICELoss 0.0996 (0.0770)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([21, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][114/500]	Time  6.629 ( 6.629)	Loss 0.3797 (0.3060)	CeLoss 0.0796 (0.0882)	SegCLSLoss 0.1118 (0.0797)	KLLoss 0.0087 (0.0094)	MaskLoss 0.1195 (0.0866)	MaskBCELoss 0.0276 (0.0240)	MaskDICELoss 0.0919 (0.0627)
Epoch: [0][115/500]	Time  8.130 ( 8.130)	Loss 0.3490 (0.3349)	CeLoss 0.0713 (0.0849)	SegCLSLoss 0.1104 (0.0902)	KLLoss 0.0150 (0.0115)	MaskLoss 0.1073 (0.0996)	MaskBCELoss 0.0077 (0.0256)	MaskDICELoss 0.0996 (0.0739)
Epoch: [0][116/500]	Time  6.995 ( 6.995)	Loss 0.0965 (0.2785)	CeLoss 0.0967 (0.1013)	SegCLSLoss 0.0000 (0.0664)	KLLoss 0.0000 (0.0069)	MaskLoss 0.0000 (0.0703)	MaskBCELoss 0.0000 (0.0143)	MaskDICELoss 0.0000 (0.0560)
Epoch: [0][117/500]	Time  7.850 ( 7.850)	Loss 0.4169 (0.3478)	CeLoss 0.0928 (0.0806)	SegCLSLoss 0.1118 (0.1009)	KLLoss 0.0047 (0.0141)	MaskLoss 0.1331 (0.1048)	MaskBCELoss 0.0483 (0.0192)	MaskDICELoss 0.0848 (0.0857)
Epoch: [0][118/500]	Time  6.628 ( 6.628)	Loss 0.1477 (0.2820)	CeLoss 0.1475 (0.1052)	SegCLSLoss 0.0000 (0.0663)	KLLoss 0.0000 (0.0064)	MaskLoss 0.0000 (0.0702)	MaskBCELoss 0.0000 (0.0135)	MaskDICELoss 0.0000 (0.0567)
Epoch: [0][119/500]	Time  8.445 ( 8.445)	Loss 0.0859 (0.3104)	CeLoss 0.0859 (0.0946)	SegCLSLoss 0.0000 (0.0791)	KLLoss 0.0000 (0.0098)	MaskLoss 0.0000 (0.0856)	MaskBCELoss 0.0000 (0.0209)	MaskDICELoss 0.0000 (0.0647)
Epoch: [0][120/500]	Time  7.584 ( 7.584)	Loss 0.4969 (0.3236)	CeLoss 0.1289 (0.0988)	SegCLSLoss 0.1133 (0.0796)	KLLoss 0.0139 (0.0094)	MaskLoss 0.1519 (0.0901)	MaskBCELoss 0.0662 (0.0277)	MaskDICELoss 0.0858 (0.0625)
Epoch: [0][121/500]	Time  6.310 ( 6.310)	Loss 0.4057 (0.3146)	CeLoss 0.0737 (0.0970)	SegCLSLoss 0.1118 (0.0765)	KLLoss 0.0094 (0.0082)	MaskLoss 0.1356 (0.0877)	MaskBCELoss 0.0428 (0.0230)	MaskDICELoss 0.0928 (0.0647)
Epoch: [0][122/500]	Time  7.211 ( 7.211)	Loss 0.3382 (0.3259)	CeLoss 0.0522 (0.0845)	SegCLSLoss 0.1084 (0.0885)	KLLoss 0.0153 (0.0118)	MaskLoss 0.1121 (0.0957)	MaskBCELoss 0.0135 (0.0218)	MaskDICELoss 0.0986 (0.0738)
Epoch: [0][123/500]	Time  6.499 ( 6.499)	Loss 0.0758 (0.2471)	CeLoss 0.0757 (0.0847)	SegCLSLoss 0.0000 (0.0547)	KLLoss 0.0000 (0.0072)	MaskLoss 0.0000 (0.0657)	MaskBCELoss 0.0000 (0.0207)	MaskDICELoss 0.0000 (0.0450)
Epoch: [0][124/500]	Time  6.520 ( 6.520)	Loss 0.3816 (0.2940)	CeLoss 0.0942 (0.0886)	SegCLSLoss 0.1094 (0.0756)	KLLoss 0.0143 (0.0089)	MaskLoss 0.1131 (0.0815)	MaskBCELoss 0.0143 (0.0153)	MaskDICELoss 0.0988 (0.0662)
Epoch: [0][125/500]	Time  5.423 ( 5.423)	Loss 0.4075 (0.2813)	CeLoss 0.0776 (0.0994)	SegCLSLoss 0.1104 (0.0671)	KLLoss 0.0118 (0.0096)	MaskLoss 0.1342 (0.0718)	MaskBCELoss 0.0437 (0.0171)	MaskDICELoss 0.0905 (0.0547)
Epoch: [0][126/500]	Time  5.663 ( 5.663)	Loss 0.4352 (0.3339)	CeLoss 0.0869 (0.0860)	SegCLSLoss 0.1157 (0.0888)	KLLoss 0.0176 (0.0128)	MaskLoss 0.1407 (0.0986)	MaskBCELoss 0.0633 (0.0279)	MaskDICELoss 0.0774 (0.0707)
Epoch: [0][127/500]	Time  7.124 ( 7.124)	Loss 0.3445 (0.2893)	CeLoss 0.0552 (0.0883)	SegCLSLoss 0.1055 (0.0757)	KLLoss 0.0122 (0.0095)	MaskLoss 0.1152 (0.0792)	MaskBCELoss 0.0230 (0.0133)	MaskDICELoss 0.0922 (0.0660)
Epoch: [0][128/500]	Time  6.308 ( 6.308)	Loss 0.3754 (0.3170)	CeLoss 0.0835 (0.1000)	SegCLSLoss 0.1094 (0.0778)	KLLoss 0.0092 (0.0107)	MaskLoss 0.1162 (0.0864)	MaskBCELoss 0.0218 (0.0230)	MaskDICELoss 0.0945 (0.0634)
Epoch: [0][129/500]	Time  8.481 ( 8.481)	Loss 0.3769 (0.3623)	CeLoss 0.0884 (0.0864)	SegCLSLoss 0.1123 (0.1001)	KLLoss 0.0150 (0.0103)	MaskLoss 0.1123 (0.1103)	MaskBCELoss 0.0129 (0.0275)	MaskDICELoss 0.0994 (0.0829)
Epoch: [0][130/500]	Time  7.759 ( 7.759)	Loss 0.4002 (0.3673)	CeLoss 0.0840 (0.0837)	SegCLSLoss 0.1147 (0.1005)	KLLoss 0.0233 (0.0144)	MaskLoss 0.1236 (0.1131)	MaskBCELoss 0.0359 (0.0331)	MaskDICELoss 0.0877 (0.0800)
Epoch: [0][131/500]	Time  7.834 ( 7.834)	Loss 0.3273 (0.3003)	CeLoss 0.0571 (0.0866)	SegCLSLoss 0.1025 (0.0750)	KLLoss 0.0128 (0.0106)	MaskLoss 0.1066 (0.0856)	MaskBCELoss 0.0118 (0.0205)	MaskDICELoss 0.0948 (0.0651)
Epoch: [0][132/500]	Time  6.799 ( 6.799)	Loss 0.3778 (0.3530)	CeLoss 0.0732 (0.0831)	SegCLSLoss 0.1040 (0.0952)	KLLoss 0.0095 (0.0128)	MaskLoss 0.1241 (0.1080)	MaskBCELoss 0.0304 (0.0222)	MaskDICELoss 0.0937 (0.0858)
Epoch: [0][133/500]	Time  6.695 ( 6.695)	Loss 0.4040 (0.2941)	CeLoss 0.0674 (0.0743)	SegCLSLoss 0.1118 (0.0733)	KLLoss 0.0195 (0.0087)	MaskLoss 0.1356 (0.0895)	MaskBCELoss 0.0518 (0.0256)	MaskDICELoss 0.0838 (0.0639)
Epoch: [0][134/500]	Time  6.682 ( 6.682)	Loss 0.0813 (0.3481)	CeLoss 0.0811 (0.0797)	SegCLSLoss 0.0000 (0.0840)	KLLoss 0.0000 (0.0093)	MaskLoss 0.0000 (0.1108)	MaskBCELoss 0.0000 (0.0373)	MaskDICELoss 0.0000 (0.0735)
Epoch: [0][135/500]	Time  6.963 ( 6.963)	Loss 0.3613 (0.3201)	CeLoss 0.0703 (0.0863)	SegCLSLoss 0.1045 (0.0848)	KLLoss 0.0090 (0.0111)	MaskLoss 0.1170 (0.0929)	MaskBCELoss 0.0222 (0.0179)	MaskDICELoss 0.0948 (0.0751)
Epoch: [0][136/500]	Time  6.950 ( 6.950)	Loss 0.4923 (0.2794)	CeLoss 0.0540 (0.0842)	SegCLSLoss 0.1079 (0.0620)	KLLoss 0.0139 (0.0065)	MaskLoss 0.1887 (0.0805)	MaskBCELoss 0.1043 (0.0266)	MaskDICELoss 0.0844 (0.0539)
Epoch: [0][137/500]	Time  7.108 ( 7.108)	Loss 0.4153 (0.3386)	CeLoss 0.0693 (0.0891)	SegCLSLoss 0.1006 (0.0842)	KLLoss 0.0058 (0.0083)	MaskLoss 0.1463 (0.1016)	MaskBCELoss 0.0535 (0.0286)	MaskDICELoss 0.0928 (0.0730)
Epoch: [0][138/500]	Time  6.556 ( 6.556)	Loss 0.3991 (0.2444)	CeLoss 0.0752 (0.0980)	SegCLSLoss 0.1040 (0.0524)	KLLoss 0.0098 (0.0067)	MaskLoss 0.1339 (0.0584)	MaskBCELoss 0.0409 (0.0101)	MaskDICELoss 0.0930 (0.0482)
Epoch: [0][139/500]	Time  7.222 ( 7.222)	Loss 0.4186 (0.3571)	CeLoss 0.0620 (0.0850)	SegCLSLoss 0.1055 (0.0958)	KLLoss 0.0114 (0.0140)	MaskLoss 0.1491 (0.1087)	MaskBCELoss 0.0633 (0.0236)	MaskDICELoss 0.0859 (0.0851)
Epoch: [0][140/500]	Time  7.485 ( 7.485)	Loss 0.3615 (0.3197)	CeLoss 0.0674 (0.0816)	SegCLSLoss 0.1064 (0.0834)	KLLoss 0.0110 (0.0103)	MaskLoss 0.1179 (0.0956)	MaskBCELoss 0.0237 (0.0203)	MaskDICELoss 0.0941 (0.0753)
Epoch: [0][141/500]	Time  6.493 ( 6.493)	Loss 0.3284 (0.2451)	CeLoss 0.0483 (0.0780)	SegCLSLoss 0.1030 (0.0604)	KLLoss 0.0134 (0.0090)	MaskLoss 0.1107 (0.0662)	MaskBCELoss 0.0159 (0.0088)	MaskDICELoss 0.0948 (0.0574)
Epoch: [0][142/500]	Time  5.331 ( 5.331)	Loss 0.4423 (0.2225)	CeLoss 0.0669 (0.0897)	SegCLSLoss 0.1040 (0.0404)	KLLoss 0.0070 (0.0044)	MaskLoss 0.1602 (0.0553)	MaskBCELoss 0.0761 (0.0200)	MaskDICELoss 0.0842 (0.0353)
Epoch: [0][143/500]	Time  7.242 ( 7.242)	Loss 0.3983 (0.3633)	CeLoss 0.0669 (0.0648)	SegCLSLoss 0.1016 (0.1008)	KLLoss 0.0090 (0.0121)	MaskLoss 0.1382 (0.1211)	MaskBCELoss 0.0507 (0.0282)	MaskDICELoss 0.0875 (0.0928)
Epoch: [0][144/500]	Time  7.866 ( 7.866)	Loss 0.3609 (0.3394)	CeLoss 0.0613 (0.0730)	SegCLSLoss 0.0986 (0.0918)	KLLoss 0.0133 (0.0101)	MaskLoss 0.1218 (0.1077)	MaskBCELoss 0.0270 (0.0229)	MaskDICELoss 0.0948 (0.0847)
Epoch: [0][145/500]	Time  6.660 ( 6.660)	Loss 0.3406 (0.3483)	CeLoss 0.0552 (0.0712)	SegCLSLoss 0.1025 (0.0913)	KLLoss 0.0172 (0.0129)	MaskLoss 0.1129 (0.1125)	MaskBCELoss 0.0158 (0.0283)	MaskDICELoss 0.0971 (0.0842)
Epoch: [0][146/500]	Time  6.756 ( 6.756)	Loss 0.1367 (0.2304)	CeLoss 0.1367 (0.0853)	SegCLSLoss 0.0000 (0.0505)	KLLoss 0.0000 (0.0060)	MaskLoss 0.0000 (0.0585)	MaskBCELoss 0.0000 (0.0101)	MaskDICELoss 0.0000 (0.0484)
Epoch: [0][147/500]	Time  8.777 ( 8.777)	Loss 0.3357 (0.3107)	CeLoss 0.0461 (0.0716)	SegCLSLoss 0.0996 (0.0815)	KLLoss 0.0184 (0.0129)	MaskLoss 0.1151 (0.0960)	MaskBCELoss 0.0202 (0.0208)	MaskDICELoss 0.0950 (0.0752)
Epoch: [0][148/500]	Time  6.594 ( 6.594)	Loss 0.3580 (0.3564)	CeLoss 0.0771 (0.0848)	SegCLSLoss 0.1016 (0.0923)	KLLoss 0.0168 (0.0127)	MaskLoss 0.1106 (0.1094)	MaskBCELoss 0.0127 (0.0247)	MaskDICELoss 0.0979 (0.0847)
Epoch: [0][149/500]	Time  6.980 ( 6.980)	Loss 0.3609 (0.2812)	CeLoss 0.0605 (0.0789)	SegCLSLoss 0.0996 (0.0718)	KLLoss 0.0159 (0.0101)	MaskLoss 0.1215 (0.0807)	MaskBCELoss 0.0252 (0.0136)	MaskDICELoss 0.0962 (0.0671)
Epoch: [0][150/500]	Time  9.029 ( 9.029)	Loss 0.3857 (0.3463)	CeLoss 0.0684 (0.0713)	SegCLSLoss 0.0972 (0.0905)	KLLoss 0.0060 (0.0119)	MaskLoss 0.1327 (0.1119)	MaskBCELoss 0.0489 (0.0290)	MaskDICELoss 0.0838 (0.0829)
Epoch: [0][151/500]	Time  7.020 ( 7.020)	Loss 0.4313 (0.3276)	CeLoss 0.0747 (0.0774)	SegCLSLoss 0.0967 (0.0792)	KLLoss 0.0232 (0.0118)	MaskLoss 0.1484 (0.1024)	MaskBCELoss 0.0615 (0.0299)	MaskDICELoss 0.0869 (0.0725)
Epoch: [0][152/500]	Time  7.197 ( 7.197)	Loss 0.3418 (0.2490)	CeLoss 0.0593 (0.0768)	SegCLSLoss 0.1001 (0.0589)	KLLoss 0.0182 (0.0104)	MaskLoss 0.1115 (0.0688)	MaskBCELoss 0.0199 (0.0121)	MaskDICELoss 0.0917 (0.0566)
Epoch: [0][153/500]	Time  6.962 ( 6.962)	Loss 0.3674 (0.2721)	CeLoss 0.0635 (0.0882)	SegCLSLoss 0.0952 (0.0586)	KLLoss 0.0095 (0.0077)	MaskLoss 0.1259 (0.0754)	MaskBCELoss 0.0363 (0.0209)	MaskDICELoss 0.0896 (0.0545)
Epoch: [0][154/500]	Time  6.333 ( 6.333)	Loss 0.1711 (0.2105)	CeLoss 0.1709 (0.0999)	SegCLSLoss 0.0000 (0.0376)	KLLoss 0.0000 (0.0055)	MaskLoss 0.0000 (0.0445)	MaskBCELoss 0.0000 (0.0055)	MaskDICELoss 0.0000 (0.0390)
Epoch: [0][155/500]	Time  7.183 ( 7.183)	Loss 0.3384 (0.3111)	CeLoss 0.0491 (0.0691)	SegCLSLoss 0.0967 (0.0783)	KLLoss 0.0159 (0.0128)	MaskLoss 0.1165 (0.0982)	MaskBCELoss 0.0223 (0.0250)	MaskDICELoss 0.0941 (0.0733)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([26, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][156/500]	Time  9.445 ( 9.445)	Loss 0.4438 (0.3661)	CeLoss 0.0635 (0.0558)	SegCLSLoss 0.0913 (0.0981)	KLLoss 0.0056 (0.0122)	MaskLoss 0.1657 (0.1276)	MaskBCELoss 0.0762 (0.0379)	MaskDICELoss 0.0895 (0.0896)
Epoch: [0][157/500]	Time  6.797 ( 6.797)	Loss 0.1375 (0.3176)	CeLoss 0.1377 (0.0705)	SegCLSLoss 0.0000 (0.0768)	KLLoss 0.0000 (0.0110)	MaskLoss 0.0000 (0.1016)	MaskBCELoss 0.0000 (0.0292)	MaskDICELoss 0.0000 (0.0724)
Epoch: [0][158/500]	Time  6.204 ( 6.204)	Loss 0.3357 (0.2483)	CeLoss 0.0508 (0.0928)	SegCLSLoss 0.0972 (0.0492)	KLLoss 0.0240 (0.0071)	MaskLoss 0.1124 (0.0637)	MaskBCELoss 0.0175 (0.0192)	MaskDICELoss 0.0949 (0.0445)
Epoch: [0][159/500]	Time  6.477 ( 6.477)	Loss 0.3493 (0.3549)	CeLoss 0.0496 (0.0742)	SegCLSLoss 0.1016 (0.0793)	KLLoss 0.0195 (0.0120)	MaskLoss 0.1200 (0.1177)	MaskBCELoss 0.0260 (0.0478)	MaskDICELoss 0.0940 (0.0698)
Epoch: [0][160/500]	Time  7.397 ( 7.397)	Loss 0.3388 (0.3094)	CeLoss 0.0688 (0.0670)	SegCLSLoss 0.0933 (0.0782)	KLLoss 0.0087 (0.0117)	MaskLoss 0.1092 (0.0987)	MaskBCELoss 0.0127 (0.0262)	MaskDICELoss 0.0966 (0.0725)
Epoch: [0][161/500]	Time  6.022 ( 6.022)	Loss 0.0855 (0.2012)	CeLoss 0.0854 (0.0809)	SegCLSLoss 0.0000 (0.0382)	KLLoss 0.0000 (0.0057)	MaskLoss 0.0000 (0.0491)	MaskBCELoss 0.0000 (0.0130)	MaskDICELoss 0.0000 (0.0361)
Epoch: [0][162/500]	Time  7.492 ( 7.492)	Loss 0.3209 (0.2855)	CeLoss 0.0447 (0.0749)	SegCLSLoss 0.0928 (0.0637)	KLLoss 0.0164 (0.0090)	MaskLoss 0.1108 (0.0871)	MaskBCELoss 0.0111 (0.0235)	MaskDICELoss 0.0997 (0.0636)
Epoch: [0][163/500]	Time  7.057 ( 7.057)	Loss 0.3389 (0.2693)	CeLoss 0.0618 (0.0692)	SegCLSLoss 0.0908 (0.0634)	KLLoss 0.0164 (0.0108)	MaskLoss 0.1120 (0.0815)	MaskBCELoss 0.0125 (0.0146)	MaskDICELoss 0.0996 (0.0669)
Epoch: [0][164/500]	Time  7.568 ( 7.568)	Loss 0.1875 (0.2389)	CeLoss 0.1875 (0.0670)	SegCLSLoss 0.0000 (0.0556)	KLLoss 0.0000 (0.0100)	MaskLoss 0.0000 (0.0696)	MaskBCELoss 0.0000 (0.0125)	MaskDICELoss 0.0000 (0.0571)
Epoch: [0][165/500]	Time  6.427 ( 6.427)	Loss 0.3539 (0.2705)	CeLoss 0.0496 (0.0722)	SegCLSLoss 0.0923 (0.0641)	KLLoss 0.0139 (0.0120)	MaskLoss 0.1258 (0.0800)	MaskBCELoss 0.0405 (0.0125)	MaskDICELoss 0.0853 (0.0675)
Epoch: [0][166/500]	Time  7.681 ( 7.681)	Loss 0.3487 (0.3397)	CeLoss 0.0586 (0.0577)	SegCLSLoss 0.0947 (0.0817)	KLLoss 0.0209 (0.0112)	MaskLoss 0.1162 (0.1177)	MaskBCELoss 0.0201 (0.0371)	MaskDICELoss 0.0961 (0.0806)
Epoch: [0][167/500]	Time  6.658 ( 6.658)	Loss 0.1078 (0.2274)	CeLoss 0.1079 (0.1036)	SegCLSLoss 0.0000 (0.0369)	KLLoss 0.0000 (0.0043)	MaskLoss 0.0000 (0.0516)	MaskBCELoss 0.0000 (0.0149)	MaskDICELoss 0.0000 (0.0367)
Epoch: [0][168/500]	Time  7.486 ( 7.486)	Loss 0.3708 (0.3026)	CeLoss 0.0522 (0.0625)	SegCLSLoss 0.0908 (0.0732)	KLLoss 0.0108 (0.0098)	MaskLoss 0.1338 (0.0992)	MaskBCELoss 0.0485 (0.0274)	MaskDICELoss 0.0854 (0.0719)
Epoch: [0][169/500]	Time  6.773 ( 6.773)	Loss 0.1102 (0.3119)	CeLoss 0.1104 (0.0728)	SegCLSLoss 0.0000 (0.0749)	KLLoss 0.0000 (0.0121)	MaskLoss 0.0000 (0.0978)	MaskBCELoss 0.0000 (0.0239)	MaskDICELoss 0.0000 (0.0739)
Epoch: [0][170/500]	Time  6.889 ( 6.889)	Loss 0.1156 (0.3047)	CeLoss 0.1157 (0.0646)	SegCLSLoss 0.0000 (0.0757)	KLLoss 0.0000 (0.0152)	MaskLoss 0.0000 (0.0974)	MaskBCELoss 0.0000 (0.0257)	MaskDICELoss 0.0000 (0.0717)
Epoch: [0][171/500]	Time  6.220 ( 6.220)	Loss 0.1492 (0.2404)	CeLoss 0.1494 (0.0683)	SegCLSLoss 0.0000 (0.0518)	KLLoss 0.0000 (0.0115)	MaskLoss 0.0000 (0.0703)	MaskBCELoss 0.0000 (0.0146)	MaskDICELoss 0.0000 (0.0557)
Epoch: [0][172/500]	Time  5.687 ( 5.687)	Loss 0.3297 (0.2302)	CeLoss 0.0435 (0.0863)	SegCLSLoss 0.0859 (0.0426)	KLLoss 0.0146 (0.0068)	MaskLoss 0.1180 (0.0596)	MaskBCELoss 0.0226 (0.0122)	MaskDICELoss 0.0954 (0.0475)
Epoch: [0][173/500]	Time  6.948 ( 6.948)	Loss 0.3373 (0.2868)	CeLoss 0.0522 (0.0552)	SegCLSLoss 0.0825 (0.0688)	KLLoss 0.0134 (0.0128)	MaskLoss 0.1183 (0.0954)	MaskBCELoss 0.0252 (0.0205)	MaskDICELoss 0.0931 (0.0748)
Epoch: [0][174/500]	Time  6.716 ( 6.716)	Loss 0.3307 (0.2824)	CeLoss 0.0410 (0.0821)	SegCLSLoss 0.0869 (0.0604)	KLLoss 0.0117 (0.0112)	MaskLoss 0.1203 (0.0823)	MaskBCELoss 0.0298 (0.0161)	MaskDICELoss 0.0904 (0.0662)
Epoch: [0][175/500]	Time  6.482 ( 6.482)	Loss 0.3653 (0.2987)	CeLoss 0.0618 (0.0608)	SegCLSLoss 0.0889 (0.0705)	KLLoss 0.0117 (0.0123)	MaskLoss 0.1268 (0.0982)	MaskBCELoss 0.0424 (0.0250)	MaskDICELoss 0.0844 (0.0732)
Epoch: [0][176/500]	Time  6.892 ( 6.892)	Loss 0.3319 (0.2412)	CeLoss 0.0522 (0.0770)	SegCLSLoss 0.0864 (0.0444)	KLLoss 0.0183 (0.0088)	MaskLoss 0.1136 (0.0689)	MaskBCELoss 0.0208 (0.0261)	MaskDICELoss 0.0928 (0.0428)
Epoch: [0][177/500]	Time  7.326 ( 7.326)	Loss 0.2062 (0.2712)	CeLoss 0.2061 (0.0730)	SegCLSLoss 0.0000 (0.0601)	KLLoss 0.0000 (0.0105)	MaskLoss 0.0000 (0.0815)	MaskBCELoss 0.0000 (0.0147)	MaskDICELoss 0.0000 (0.0667)
Epoch: [0][178/500]	Time  7.705 ( 7.705)	Loss 0.3569 (0.3211)	CeLoss 0.0547 (0.0601)	SegCLSLoss 0.0850 (0.0781)	KLLoss 0.0099 (0.0129)	MaskLoss 0.1277 (0.1077)	MaskBCELoss 0.0391 (0.0243)	MaskDICELoss 0.0886 (0.0835)
Epoch: [0][179/500]	Time  8.130 ( 8.130)	Loss 0.3589 (0.3149)	CeLoss 0.0474 (0.0535)	SegCLSLoss 0.0825 (0.0780)	KLLoss 0.0103 (0.0148)	MaskLoss 0.1326 (0.1075)	MaskBCELoss 0.0466 (0.0247)	MaskDICELoss 0.0859 (0.0828)
Epoch: [0][180/500]	Time  6.971 ( 6.971)	Loss 0.0930 (0.2711)	CeLoss 0.0928 (0.0600)	SegCLSLoss 0.0000 (0.0604)	KLLoss 0.0000 (0.0103)	MaskLoss 0.0000 (0.0879)	MaskBCELoss 0.0000 (0.0259)	MaskDICELoss 0.0000 (0.0620)
Epoch: [0][181/500]	Time  6.517 ( 6.517)	Loss 0.3815 (0.2856)	CeLoss 0.0459 (0.0730)	SegCLSLoss 0.0801 (0.0553)	KLLoss 0.0117 (0.0086)	MaskLoss 0.1448 (0.0903)	MaskBCELoss 0.0581 (0.0271)	MaskDICELoss 0.0867 (0.0632)
Epoch: [0][182/500]	Time  7.360 ( 7.360)	Loss 0.2997 (0.2722)	CeLoss 0.0366 (0.0654)	SegCLSLoss 0.0771 (0.0562)	KLLoss 0.0182 (0.0118)	MaskLoss 0.1079 (0.0864)	MaskBCELoss 0.0104 (0.0223)	MaskDICELoss 0.0975 (0.0642)
Epoch: [0][183/500]	Time  7.079 ( 7.079)	Loss 0.3244 (0.3179)	CeLoss 0.0378 (0.0541)	SegCLSLoss 0.0845 (0.0721)	KLLoss 0.0228 (0.0122)	MaskLoss 0.1165 (0.1108)	MaskBCELoss 0.0250 (0.0268)	MaskDICELoss 0.0915 (0.0840)
Epoch: [0][184/500]	Time  5.892 ( 5.892)	Loss 0.3680 (0.2438)	CeLoss 0.0522 (0.0635)	SegCLSLoss 0.0781 (0.0473)	KLLoss 0.0114 (0.0087)	MaskLoss 0.1354 (0.0761)	MaskBCELoss 0.0509 (0.0228)	MaskDICELoss 0.0844 (0.0533)
Epoch: [0][185/500]	Time  8.780 ( 8.780)	Loss 0.3220 (0.2528)	CeLoss 0.0417 (0.0527)	SegCLSLoss 0.0762 (0.0552)	KLLoss 0.0117 (0.0096)	MaskLoss 0.1182 (0.0839)	MaskBCELoss 0.0252 (0.0191)	MaskDICELoss 0.0930 (0.0648)
Epoch: [0][186/500]	Time  7.606 ( 7.606)	Loss 0.3603 (0.2897)	CeLoss 0.0444 (0.0551)	SegCLSLoss 0.0781 (0.0622)	KLLoss 0.0107 (0.0117)	MaskLoss 0.1356 (0.0988)	MaskBCELoss 0.0486 (0.0241)	MaskDICELoss 0.0870 (0.0747)
Epoch: [0][187/500]	Time  7.615 ( 7.615)	Loss 0.3080 (0.2802)	CeLoss 0.0483 (0.0516)	SegCLSLoss 0.0771 (0.0630)	KLLoss 0.0135 (0.0125)	MaskLoss 0.1073 (0.0954)	MaskBCELoss 0.0094 (0.0209)	MaskDICELoss 0.0979 (0.0746)
Epoch: [0][188/500]	Time  7.877 ( 7.877)	Loss 0.3119 (0.3212)	CeLoss 0.0466 (0.0559)	SegCLSLoss 0.0786 (0.0720)	KLLoss 0.0201 (0.0132)	MaskLoss 0.1079 (0.1112)	MaskBCELoss 0.0113 (0.0284)	MaskDICELoss 0.0966 (0.0828)
Epoch: [0][189/500]	Time  7.418 ( 7.418)	Loss 0.3202 (0.3429)	CeLoss 0.0481 (0.0433)	SegCLSLoss 0.0767 (0.0793)	KLLoss 0.0155 (0.0162)	MaskLoss 0.1130 (0.1259)	MaskBCELoss 0.0190 (0.0357)	MaskDICELoss 0.0940 (0.0902)
Epoch: [0][190/500]	Time  7.273 ( 7.273)	Loss 0.0496 (0.2207)	CeLoss 0.0496 (0.0695)	SegCLSLoss 0.0000 (0.0399)	KLLoss 0.0000 (0.0068)	MaskLoss 0.0000 (0.0639)	MaskBCELoss 0.0000 (0.0191)	MaskDICELoss 0.0000 (0.0448)
Epoch: [0][191/500]	Time  8.478 ( 8.478)	Loss 0.3283 (0.3566)	CeLoss 0.0339 (0.0435)	SegCLSLoss 0.0757 (0.0721)	KLLoss 0.0228 (0.0164)	MaskLoss 0.1226 (0.1344)	MaskBCELoss 0.0328 (0.0435)	MaskDICELoss 0.0897 (0.0909)
Epoch: [0][192/500]	Time  8.682 ( 8.682)	Loss 0.2985 (0.3134)	CeLoss 0.0386 (0.0488)	SegCLSLoss 0.0742 (0.0652)	KLLoss 0.0227 (0.0158)	MaskLoss 0.1057 (0.1121)	MaskBCELoss 0.0059 (0.0309)	MaskDICELoss 0.0998 (0.0812)
Epoch: [0][193/500]	Time  8.624 ( 8.624)	Loss 0.3526 (0.3096)	CeLoss 0.0383 (0.0527)	SegCLSLoss 0.0747 (0.0656)	KLLoss 0.0216 (0.0132)	MaskLoss 0.1329 (0.1088)	MaskBCELoss 0.0522 (0.0262)	MaskDICELoss 0.0807 (0.0825)
Epoch: [0][194/500]	Time  7.663 ( 7.663)	Loss 0.3148 (0.2917)	CeLoss 0.0396 (0.0429)	SegCLSLoss 0.0718 (0.0640)	KLLoss 0.0184 (0.0147)	MaskLoss 0.1150 (0.1047)	MaskBCELoss 0.0208 (0.0196)	MaskDICELoss 0.0942 (0.0851)
Epoch: [0][195/500]	Time  6.452 ( 6.452)	Loss 0.4051 (0.2559)	CeLoss 0.0496 (0.0653)	SegCLSLoss 0.0713 (0.0434)	KLLoss 0.0103 (0.0091)	MaskLoss 0.1574 (0.0822)	MaskBCELoss 0.0667 (0.0278)	MaskDICELoss 0.0908 (0.0544)
Epoch: [0][196/500]	Time  7.652 ( 7.652)	Loss 0.0918 (0.2596)	CeLoss 0.0918 (0.0630)	SegCLSLoss 0.0000 (0.0500)	KLLoss 0.0000 (0.0100)	MaskLoss 0.0000 (0.0834)	MaskBCELoss 0.0000 (0.0179)	MaskDICELoss 0.0000 (0.0654)
Epoch: [0][197/500]	Time  7.244 ( 7.244)	Loss 0.3107 (0.2870)	CeLoss 0.0334 (0.0535)	SegCLSLoss 0.0674 (0.0583)	KLLoss 0.0084 (0.0124)	MaskLoss 0.1196 (0.0991)	MaskBCELoss 0.0259 (0.0246)	MaskDICELoss 0.0937 (0.0745)
Epoch: [0][198/500]	Time  7.667 ( 7.667)	Loss 0.1055 (0.2812)	CeLoss 0.1055 (0.0580)	SegCLSLoss 0.0000 (0.0574)	KLLoss 0.0000 (0.0131)	MaskLoss 0.0000 (0.0940)	MaskBCELoss 0.0000 (0.0189)	MaskDICELoss 0.0000 (0.0750)
Epoch: [0][199/500]	Time  6.510 ( 6.510)	Loss 0.3823 (0.2949)	CeLoss 0.0498 (0.0605)	SegCLSLoss 0.0732 (0.0584)	KLLoss 0.0173 (0.0139)	MaskLoss 0.1437 (0.0991)	MaskBCELoss 0.0585 (0.0268)	MaskDICELoss 0.0852 (0.0723)
[2025-03-06 01:54:31,712] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[0.0002985903614457831], mom=[(0.9, 0.95)]
[2025-03-06 01:54:31,717] [INFO] [timer.py:215:stop] epoch=0/micro_step=200/global_step=20, RunningAvgSamplesPerSec=1.3852069090827146, CurrSamplesPerSec=1.2940394273935758, MemAllocated=57.27GB, MaxMemAllocated=62.82GB
Epoch: [0][200/500]	Time  7.233 ( 7.233)	Loss 0.2932 (0.2708)	CeLoss 0.0327 (0.0516)	SegCLSLoss 0.0723 (0.0575)	KLLoss 0.0214 (0.0136)	MaskLoss 0.1070 (0.0918)	MaskBCELoss 0.0089 (0.0160)	MaskDICELoss 0.0981 (0.0758)
Epoch: [0][201/500]	Time  7.182 ( 7.182)	Loss 0.1664 (0.2839)	CeLoss 0.1660 (0.0614)	SegCLSLoss 0.0000 (0.0505)	KLLoss 0.0000 (0.0119)	MaskLoss 0.0000 (0.0957)	MaskBCELoss 0.0000 (0.0210)	MaskDICELoss 0.0000 (0.0747)
Epoch: [0][202/500]	Time  7.722 ( 7.722)	Loss 0.3051 (0.2860)	CeLoss 0.0359 (0.0609)	SegCLSLoss 0.0674 (0.0519)	KLLoss 0.0193 (0.0130)	MaskLoss 0.1129 (0.0964)	MaskBCELoss 0.0155 (0.0221)	MaskDICELoss 0.0973 (0.0743)
Epoch: [0][203/500]	Time  7.069 ( 7.069)	Loss 0.3043 (0.2666)	CeLoss 0.0393 (0.0522)	SegCLSLoss 0.0645 (0.0501)	KLLoss 0.0177 (0.0130)	MaskLoss 0.1119 (0.0914)	MaskBCELoss 0.0152 (0.0152)	MaskDICELoss 0.0967 (0.0762)
Epoch: [0][204/500]	Time  7.623 ( 7.623)	Loss 0.0688 (0.2750)	CeLoss 0.0688 (0.0472)	SegCLSLoss 0.0000 (0.0525)	KLLoss 0.0000 (0.0136)	MaskLoss 0.0000 (0.0974)	MaskBCELoss 0.0000 (0.0245)	MaskDICELoss 0.0000 (0.0729)
Epoch: [0][205/500]	Time  6.134 ( 6.134)	Loss 0.2895 (0.2615)	CeLoss 0.0287 (0.0677)	SegCLSLoss 0.0659 (0.0451)	KLLoss 0.0232 (0.0131)	MaskLoss 0.1080 (0.0823)	MaskBCELoss 0.0084 (0.0171)	MaskDICELoss 0.0997 (0.0652)
Epoch: [0][206/500]	Time  7.180 ( 7.180)	Loss 0.3198 (0.3058)	CeLoss 0.0280 (0.0438)	SegCLSLoss 0.0674 (0.0580)	KLLoss 0.0221 (0.0159)	MaskLoss 0.1236 (0.1125)	MaskBCELoss 0.0364 (0.0328)	MaskDICELoss 0.0872 (0.0797)
Epoch: [0][207/500]	Time  6.546 ( 6.546)	Loss 0.3576 (0.2698)	CeLoss 0.0322 (0.0501)	SegCLSLoss 0.0649 (0.0507)	KLLoss 0.0189 (0.0131)	MaskLoss 0.1417 (0.0939)	MaskBCELoss 0.0595 (0.0207)	MaskDICELoss 0.0822 (0.0732)
Epoch: [0][208/500]	Time  7.162 ( 7.162)	Loss 0.0895 (0.2299)	CeLoss 0.0894 (0.0597)	SegCLSLoss 0.0000 (0.0377)	KLLoss 0.0000 (0.0088)	MaskLoss 0.0000 (0.0735)	MaskBCELoss 0.0000 (0.0196)	MaskDICELoss 0.0000 (0.0539)
Epoch: [0][209/500]	Time  7.562 ( 7.562)	Loss 0.3279 (0.2929)	CeLoss 0.0461 (0.0554)	SegCLSLoss 0.0620 (0.0502)	KLLoss 0.0087 (0.0113)	MaskLoss 0.1231 (0.1034)	MaskBCELoss 0.0333 (0.0321)	MaskDICELoss 0.0898 (0.0713)
Epoch: [0][210/500]	Time  6.244 ( 6.244)	Loss 0.3204 (0.2363)	CeLoss 0.0383 (0.0662)	SegCLSLoss 0.0601 (0.0376)	KLLoss 0.0109 (0.0091)	MaskLoss 0.1233 (0.0733)	MaskBCELoss 0.0334 (0.0179)	MaskDICELoss 0.0899 (0.0555)
Epoch: [0][211/500]	Time  8.095 ( 8.095)	Loss 0.4136 (0.2620)	CeLoss 0.0469 (0.0540)	SegCLSLoss 0.0547 (0.0380)	KLLoss 0.0101 (0.0103)	MaskLoss 0.1672 (0.0919)	MaskBCELoss 0.0803 (0.0279)	MaskDICELoss 0.0868 (0.0640)
Epoch: [0][212/500]	Time  7.173 ( 7.173)	Loss 0.2837 (0.2590)	CeLoss 0.0309 (0.0473)	SegCLSLoss 0.0620 (0.0442)	KLLoss 0.0237 (0.0144)	MaskLoss 0.1051 (0.0911)	MaskBCELoss 0.0060 (0.0150)	MaskDICELoss 0.0991 (0.0761)
Epoch: [0][213/500]	Time  7.586 ( 7.586)	Loss 0.2841 (0.2971)	CeLoss 0.0337 (0.0439)	SegCLSLoss 0.0574 (0.0498)	KLLoss 0.0205 (0.0136)	MaskLoss 0.1057 (0.1108)	MaskBCELoss 0.0071 (0.0266)	MaskDICELoss 0.0986 (0.0841)
Epoch: [0][214/500]	Time  7.357 ( 7.357)	Loss 0.3548 (0.2560)	CeLoss 0.0457 (0.0584)	SegCLSLoss 0.0552 (0.0390)	KLLoss 0.0138 (0.0110)	MaskLoss 0.1372 (0.0863)	MaskBCELoss 0.0500 (0.0237)	MaskDICELoss 0.0871 (0.0626)
Epoch: [0][215/500]	Time  6.589 ( 6.589)	Loss 0.3029 (0.2921)	CeLoss 0.0474 (0.0535)	SegCLSLoss 0.0586 (0.0437)	KLLoss 0.0233 (0.0118)	MaskLoss 0.1073 (0.1054)	MaskBCELoss 0.0129 (0.0330)	MaskDICELoss 0.0945 (0.0724)
Epoch: [0][216/500]	Time  7.582 ( 7.582)	Loss 0.3061 (0.2972)	CeLoss 0.0474 (0.0490)	SegCLSLoss 0.0554 (0.0501)	KLLoss 0.0198 (0.0138)	MaskLoss 0.1107 (0.1081)	MaskBCELoss 0.0134 (0.0255)	MaskDICELoss 0.0973 (0.0826)
Epoch: [0][217/500]	Time  7.182 ( 7.182)	Loss 0.3163 (0.3007)	CeLoss 0.0481 (0.0488)	SegCLSLoss 0.0547 (0.0498)	KLLoss 0.0114 (0.0142)	MaskLoss 0.1175 (0.1100)	MaskBCELoss 0.0202 (0.0300)	MaskDICELoss 0.0973 (0.0800)
Epoch: [0][218/500]	Time  7.737 ( 7.737)	Loss 0.2950 (0.2958)	CeLoss 0.0270 (0.0423)	SegCLSLoss 0.0532 (0.0507)	KLLoss 0.0198 (0.0136)	MaskLoss 0.1159 (0.1107)	MaskBCELoss 0.0160 (0.0268)	MaskDICELoss 0.0998 (0.0839)
Epoch: [0][219/500]	Time  6.133 ( 6.133)	Loss 0.1945 (0.2248)	CeLoss 0.1943 (0.0799)	SegCLSLoss 0.0000 (0.0270)	KLLoss 0.0000 (0.0077)	MaskLoss 0.0000 (0.0637)	MaskBCELoss 0.0000 (0.0194)	MaskDICELoss 0.0000 (0.0443)
Epoch: [0][220/500]	Time  6.054 ( 6.054)	Loss 0.0973 (0.2130)	CeLoss 0.0972 (0.0809)	SegCLSLoss 0.0000 (0.0265)	KLLoss 0.0000 (0.0085)	MaskLoss 0.0000 (0.0572)	MaskBCELoss 0.0000 (0.0102)	MaskDICELoss 0.0000 (0.0470)
Epoch: [0][221/500]	Time  6.785 ( 6.785)	Loss 0.0730 (0.2429)	CeLoss 0.0732 (0.0523)	SegCLSLoss 0.0000 (0.0340)	KLLoss 0.0000 (0.0146)	MaskLoss 0.0000 (0.0831)	MaskBCELoss 0.0000 (0.0202)	MaskDICELoss 0.0000 (0.0630)
Epoch: [0][222/500]	Time  7.904 ( 7.904)	Loss 0.0961 (0.2605)	CeLoss 0.0962 (0.0630)	SegCLSLoss 0.0000 (0.0321)	KLLoss 0.0000 (0.0102)	MaskLoss 0.0000 (0.0882)	MaskBCELoss 0.0000 (0.0263)	MaskDICELoss 0.0000 (0.0619)
Epoch: [0][223/500]	Time  6.515 ( 6.515)	Loss 0.2925 (0.2469)	CeLoss 0.0334 (0.0509)	SegCLSLoss 0.0449 (0.0323)	KLLoss 0.0156 (0.0108)	MaskLoss 0.1144 (0.0872)	MaskBCELoss 0.0205 (0.0262)	MaskDICELoss 0.0939 (0.0610)
Epoch: [0][224/500]	Time  8.011 ( 8.011)	Loss 0.3316 (0.2674)	CeLoss 0.0297 (0.0448)	SegCLSLoss 0.0471 (0.0364)	KLLoss 0.0166 (0.0111)	MaskLoss 0.1351 (0.0995)	MaskBCELoss 0.0450 (0.0273)	MaskDICELoss 0.0901 (0.0721)
Epoch: [0][225/500]	Time  7.108 ( 7.108)	Loss 0.2789 (0.2345)	CeLoss 0.0299 (0.0438)	SegCLSLoss 0.0444 (0.0320)	KLLoss 0.0146 (0.0115)	MaskLoss 0.1097 (0.0844)	MaskBCELoss 0.0113 (0.0190)	MaskDICELoss 0.0984 (0.0655)
Epoch: [0][226/500]	Time  6.908 ( 6.908)	Loss 0.2792 (0.2483)	CeLoss 0.0322 (0.0663)	SegCLSLoss 0.0493 (0.0328)	KLLoss 0.0193 (0.0119)	MaskLoss 0.1062 (0.0798)	MaskBCELoss 0.0069 (0.0124)	MaskDICELoss 0.0993 (0.0674)
Epoch: [0][227/500]	Time  7.847 ( 7.847)	Loss 0.3435 (0.3062)	CeLoss 0.0374 (0.0343)	SegCLSLoss 0.0476 (0.0462)	KLLoss 0.0153 (0.0180)	MaskLoss 0.1374 (0.1199)	MaskBCELoss 0.0536 (0.0296)	MaskDICELoss 0.0837 (0.0903)
Epoch: [0][228/500]	Time  7.505 ( 7.505)	Loss 0.3197 (0.2875)	CeLoss 0.0405 (0.0381)	SegCLSLoss 0.0471 (0.0413)	KLLoss 0.0182 (0.0137)	MaskLoss 0.1233 (0.1110)	MaskBCELoss 0.0358 (0.0317)	MaskDICELoss 0.0876 (0.0793)
Epoch: [0][229/500]	Time  8.129 ( 8.129)	Loss 0.3271 (0.2831)	CeLoss 0.0376 (0.0397)	SegCLSLoss 0.0447 (0.0434)	KLLoss 0.0131 (0.0153)	MaskLoss 0.1303 (0.1070)	MaskBCELoss 0.0380 (0.0241)	MaskDICELoss 0.0923 (0.0829)
Epoch: [0][230/500]	Time  7.241 ( 7.241)	Loss 0.3286 (0.2813)	CeLoss 0.0574 (0.0400)	SegCLSLoss 0.0444 (0.0412)	KLLoss 0.0131 (0.0133)	MaskLoss 0.1211 (0.1070)	MaskBCELoss 0.0327 (0.0243)	MaskDICELoss 0.0885 (0.0827)
Epoch: [0][231/500]	Time  7.521 ( 7.521)	Loss 0.3048 (0.2634)	CeLoss 0.0347 (0.0592)	SegCLSLoss 0.0369 (0.0265)	KLLoss 0.0143 (0.0126)	MaskLoss 0.1223 (0.0923)	MaskBCELoss 0.0334 (0.0296)	MaskDICELoss 0.0889 (0.0627)
Epoch: [0][232/500]	Time  6.357 ( 6.357)	Loss 0.3160 (0.2777)	CeLoss 0.0303 (0.0532)	SegCLSLoss 0.0381 (0.0300)	KLLoss 0.0183 (0.0145)	MaskLoss 0.1287 (0.1011)	MaskBCELoss 0.0453 (0.0277)	MaskDICELoss 0.0835 (0.0734)
Epoch: [0][233/500]	Time  6.489 ( 6.489)	Loss 0.0430 (0.2032)	CeLoss 0.0430 (0.0778)	SegCLSLoss 0.0000 (0.0194)	KLLoss 0.0000 (0.0104)	MaskLoss 0.0000 (0.0553)	MaskBCELoss 0.0000 (0.0078)	MaskDICELoss 0.0000 (0.0474)
Epoch: [0][234/500]	Time  6.697 ( 6.697)	Loss 0.1562 (0.2564)	CeLoss 0.1562 (0.0752)	SegCLSLoss 0.0000 (0.0262)	KLLoss 0.0000 (0.0117)	MaskLoss 0.0000 (0.0811)	MaskBCELoss 0.0000 (0.0138)	MaskDICELoss 0.0000 (0.0673)
Epoch: [0][235/500]	Time  5.993 ( 5.993)	Loss 0.1086 (0.2409)	CeLoss 0.1084 (0.0725)	SegCLSLoss 0.0000 (0.0229)	KLLoss 0.0000 (0.0105)	MaskLoss 0.0000 (0.0758)	MaskBCELoss 0.0000 (0.0199)	MaskDICELoss 0.0000 (0.0559)
Epoch: [0][236/500]	Time  6.386 ( 6.386)	Loss 0.4362 (0.2420)	CeLoss 0.0693 (0.0438)	SegCLSLoss 0.0381 (0.0269)	KLLoss 0.0122 (0.0122)	MaskLoss 0.1707 (0.0894)	MaskBCELoss 0.0908 (0.0257)	MaskDICELoss 0.0799 (0.0637)
Epoch: [0][237/500]	Time  7.188 ( 7.188)	Loss 0.0355 (0.2095)	CeLoss 0.0356 (0.0615)	SegCLSLoss 0.0000 (0.0222)	KLLoss 0.0000 (0.0102)	MaskLoss 0.0000 (0.0659)	MaskBCELoss 0.0000 (0.0087)	MaskDICELoss 0.0000 (0.0572)
Epoch: [0][238/500]	Time  6.210 ( 6.210)	Loss 0.2750 (0.2325)	CeLoss 0.0273 (0.0686)	SegCLSLoss 0.0386 (0.0227)	KLLoss 0.0205 (0.0094)	MaskLoss 0.1092 (0.0739)	MaskBCELoss 0.0123 (0.0186)	MaskDICELoss 0.0969 (0.0554)
Epoch: [0][239/500]	Time  7.913 ( 7.913)	Loss 0.3348 (0.2685)	CeLoss 0.0386 (0.0538)	SegCLSLoss 0.0374 (0.0295)	KLLoss 0.0103 (0.0115)	MaskLoss 0.1363 (0.0972)	MaskBCELoss 0.0422 (0.0229)	MaskDICELoss 0.0941 (0.0743)
Epoch: [0][240/500]	Time  7.042 ( 7.042)	Loss 0.2701 (0.2495)	CeLoss 0.0347 (0.0468)	SegCLSLoss 0.0342 (0.0303)	KLLoss 0.0126 (0.0157)	MaskLoss 0.1061 (0.0898)	MaskBCELoss 0.0084 (0.0145)	MaskDICELoss 0.0977 (0.0754)
Epoch: [0][241/500]	Time  7.721 ( 7.721)	Loss 0.2994 (0.3137)	CeLoss 0.0303 (0.0341)	SegCLSLoss 0.0310 (0.0306)	KLLoss 0.0193 (0.0178)	MaskLoss 0.1220 (0.1277)	MaskBCELoss 0.0273 (0.0362)	MaskDICELoss 0.0947 (0.0915)
Epoch: [0][242/500]	Time  7.114 ( 7.114)	Loss 0.2680 (0.2243)	CeLoss 0.0276 (0.0452)	SegCLSLoss 0.0325 (0.0214)	KLLoss 0.0237 (0.0121)	MaskLoss 0.1063 (0.0812)	MaskBCELoss 0.0133 (0.0152)	MaskDICELoss 0.0929 (0.0660)
Epoch: [0][243/500]	Time  7.344 ( 7.344)	Loss 0.2597 (0.2679)	CeLoss 0.0276 (0.0334)	SegCLSLoss 0.0286 (0.0274)	KLLoss 0.0142 (0.0151)	MaskLoss 0.1054 (0.1066)	MaskBCELoss 0.0071 (0.0232)	MaskDICELoss 0.0983 (0.0834)
Epoch: [0][244/500]	Time  6.792 ( 6.792)	Loss 0.2646 (0.2112)	CeLoss 0.0212 (0.0550)	SegCLSLoss 0.0337 (0.0197)	KLLoss 0.0264 (0.0126)	MaskLoss 0.1067 (0.0700)	MaskBCELoss 0.0081 (0.0145)	MaskDICELoss 0.0987 (0.0555)
Epoch: [0][245/500]	Time  7.287 ( 7.287)	Loss 0.0264 (0.1988)	CeLoss 0.0264 (0.0465)	SegCLSLoss 0.0000 (0.0174)	KLLoss 0.0000 (0.0103)	MaskLoss 0.0000 (0.0693)	MaskBCELoss 0.0000 (0.0130)	MaskDICELoss 0.0000 (0.0563)
Epoch: [0][246/500]	Time  8.266 ( 8.266)	Loss 0.0840 (0.2782)	CeLoss 0.0840 (0.0413)	SegCLSLoss 0.0000 (0.0237)	KLLoss 0.0000 (0.0142)	MaskLoss 0.0000 (0.1090)	MaskBCELoss 0.0000 (0.0379)	MaskDICELoss 0.0000 (0.0711)
Epoch: [0][247/500]	Time  7.651 ( 7.651)	Loss 0.2652 (0.2928)	CeLoss 0.0256 (0.0340)	SegCLSLoss 0.0356 (0.0300)	KLLoss 0.0232 (0.0159)	MaskLoss 0.1051 (0.1180)	MaskBCELoss 0.0062 (0.0243)	MaskDICELoss 0.0989 (0.0937)
Epoch: [0][248/500]	Time  8.423 ( 8.423)	Loss 0.2825 (0.2530)	CeLoss 0.0342 (0.0376)	SegCLSLoss 0.0325 (0.0238)	KLLoss 0.0221 (0.0135)	MaskLoss 0.1106 (0.0984)	MaskBCELoss 0.0118 (0.0246)	MaskDICELoss 0.0988 (0.0738)
Epoch: [0][249/500]	Time  6.086 ( 6.086)	Loss 0.0641 (0.2426)	CeLoss 0.0640 (0.0566)	SegCLSLoss 0.0000 (0.0206)	KLLoss 0.0000 (0.0114)	MaskLoss 0.0000 (0.0850)	MaskBCELoss 0.0000 (0.0209)	MaskDICELoss 0.0000 (0.0641)
Epoch: [0][250/500]	Time  7.703 ( 7.703)	Loss 0.0570 (0.2321)	CeLoss 0.0571 (0.0507)	SegCLSLoss 0.0000 (0.0207)	KLLoss 0.0000 (0.0116)	MaskLoss 0.0000 (0.0827)	MaskBCELoss 0.0000 (0.0171)	MaskDICELoss 0.0000 (0.0656)
Epoch: [0][251/500]	Time  7.886 ( 7.886)	Loss 0.3026 (0.2570)	CeLoss 0.0359 (0.0412)	SegCLSLoss 0.0221 (0.0181)	KLLoss 0.0123 (0.0126)	MaskLoss 0.1247 (0.1002)	MaskBCELoss 0.0380 (0.0273)	MaskDICELoss 0.0868 (0.0729)
Epoch: [0][252/500]	Time  6.681 ( 6.681)	Loss 0.1297 (0.2349)	CeLoss 0.1299 (0.0682)	SegCLSLoss 0.0000 (0.0137)	KLLoss 0.0000 (0.0101)	MaskLoss 0.0000 (0.0774)	MaskBCELoss 0.0000 (0.0261)	MaskDICELoss 0.0000 (0.0513)
Epoch: [0][253/500]	Time  6.550 ( 6.550)	Loss 0.1016 (0.2369)	CeLoss 0.1016 (0.0535)	SegCLSLoss 0.0000 (0.0175)	KLLoss 0.0000 (0.0140)	MaskLoss 0.0000 (0.0839)	MaskBCELoss 0.0000 (0.0196)	MaskDICELoss 0.0000 (0.0642)
Epoch: [0][254/500]	Time  6.925 ( 6.925)	Loss 0.3210 (0.2766)	CeLoss 0.0359 (0.0561)	SegCLSLoss 0.0245 (0.0182)	KLLoss 0.0217 (0.0137)	MaskLoss 0.1310 (0.1023)	MaskBCELoss 0.0517 (0.0333)	MaskDICELoss 0.0793 (0.0690)
Epoch: [0][255/500]	Time  6.292 ( 6.292)	Loss 0.2709 (0.1770)	CeLoss 0.0287 (0.0508)	SegCLSLoss 0.0254 (0.0123)	KLLoss 0.0219 (0.0105)	MaskLoss 0.1093 (0.0574)	MaskBCELoss 0.0120 (0.0094)	MaskDICELoss 0.0973 (0.0480)
Epoch: [0][256/500]	Time  6.740 ( 6.740)	Loss 0.2961 (0.2400)	CeLoss 0.0327 (0.0520)	SegCLSLoss 0.0219 (0.0171)	KLLoss 0.0130 (0.0128)	MaskLoss 0.1229 (0.0866)	MaskBCELoss 0.0345 (0.0260)	MaskDICELoss 0.0884 (0.0606)
Epoch: [0][257/500]	Time  8.121 ( 8.121)	Loss 0.3024 (0.2730)	CeLoss 0.0325 (0.0399)	SegCLSLoss 0.0242 (0.0208)	KLLoss 0.0229 (0.0167)	MaskLoss 0.1233 (0.1072)	MaskBCELoss 0.0371 (0.0223)	MaskDICELoss 0.0861 (0.0849)
Epoch: [0][258/500]	Time  6.775 ( 6.775)	Loss 0.3386 (0.2019)	CeLoss 0.0393 (0.0428)	SegCLSLoss 0.0251 (0.0138)	KLLoss 0.0214 (0.0105)	MaskLoss 0.1381 (0.0735)	MaskBCELoss 0.0554 (0.0178)	MaskDICELoss 0.0826 (0.0557)
Epoch: [0][259/500]	Time  7.761 ( 7.761)	Loss 0.2956 (0.2698)	CeLoss 0.0435 (0.0600)	SegCLSLoss 0.0242 (0.0191)	KLLoss 0.0179 (0.0151)	MaskLoss 0.1156 (0.0964)	MaskBCELoss 0.0184 (0.0238)	MaskDICELoss 0.0972 (0.0726)
Epoch: [0][260/500]	Time  6.717 ( 6.717)	Loss 0.2956 (0.2465)	CeLoss 0.0238 (0.0607)	SegCLSLoss 0.0270 (0.0181)	KLLoss 0.0237 (0.0156)	MaskLoss 0.1232 (0.0845)	MaskBCELoss 0.0327 (0.0193)	MaskDICELoss 0.0905 (0.0652)
Epoch: [0][261/500]	Time  5.098 ( 5.098)	Loss 0.3133 (0.1775)	CeLoss 0.0310 (0.0673)	SegCLSLoss 0.0166 (0.0077)	KLLoss 0.0135 (0.0084)	MaskLoss 0.1335 (0.0511)	MaskBCELoss 0.0536 (0.0163)	MaskDICELoss 0.0799 (0.0348)
Epoch: [0][262/500]	Time  7.093 ( 7.093)	Loss 0.2691 (0.2716)	CeLoss 0.0283 (0.0392)	SegCLSLoss 0.0217 (0.0187)	KLLoss 0.0228 (0.0189)	MaskLoss 0.1092 (0.1068)	MaskBCELoss 0.0097 (0.0257)	MaskDICELoss 0.0994 (0.0811)
Epoch: [0][263/500]	Time  7.228 ( 7.228)	Loss 0.2687 (0.2529)	CeLoss 0.0271 (0.0396)	SegCLSLoss 0.0178 (0.0160)	KLLoss 0.0173 (0.0153)	MaskLoss 0.1120 (0.0988)	MaskBCELoss 0.0158 (0.0270)	MaskDICELoss 0.0962 (0.0718)
Epoch: [0][264/500]	Time  8.303 ( 8.303)	Loss 0.2846 (0.2639)	CeLoss 0.0283 (0.0313)	SegCLSLoss 0.0168 (0.0165)	KLLoss 0.0142 (0.0170)	MaskLoss 0.1204 (0.1079)	MaskBCELoss 0.0277 (0.0255)	MaskDICELoss 0.0927 (0.0825)
Epoch: [0][265/500]	Time  5.739 ( 5.739)	Loss 0.2972 (0.2390)	CeLoss 0.0315 (0.0513)	SegCLSLoss 0.0165 (0.0141)	KLLoss 0.0173 (0.0154)	MaskLoss 0.1244 (0.0865)	MaskBCELoss 0.0360 (0.0248)	MaskDICELoss 0.0884 (0.0617)
Epoch: [0][266/500]	Time  7.097 ( 7.097)	Loss 0.0242 (0.1917)	CeLoss 0.0242 (0.0455)	SegCLSLoss 0.0000 (0.0114)	KLLoss 0.0000 (0.0118)	MaskLoss 0.0000 (0.0674)	MaskBCELoss 0.0000 (0.0108)	MaskDICELoss 0.0000 (0.0566)
Epoch: [0][267/500]	Time  6.240 ( 6.240)	Loss 0.2836 (0.2390)	CeLoss 0.0277 (0.0584)	SegCLSLoss 0.0287 (0.0150)	KLLoss 0.0264 (0.0153)	MaskLoss 0.1140 (0.0827)	MaskBCELoss 0.0193 (0.0185)	MaskDICELoss 0.0948 (0.0643)
Epoch: [0][268/500]	Time  5.579 ( 5.579)	Loss 0.2879 (0.2274)	CeLoss 0.0187 (0.0691)	SegCLSLoss 0.0293 (0.0129)	KLLoss 0.0267 (0.0125)	MaskLoss 0.1207 (0.0728)	MaskBCELoss 0.0316 (0.0182)	MaskDICELoss 0.0890 (0.0545)
Epoch: [0][269/500]	Time  7.355 ( 7.355)	Loss 0.2820 (0.2250)	CeLoss 0.0283 (0.0478)	SegCLSLoss 0.0172 (0.0135)	KLLoss 0.0153 (0.0134)	MaskLoss 0.1187 (0.0819)	MaskBCELoss 0.0282 (0.0182)	MaskDICELoss 0.0906 (0.0637)
Epoch: [0][270/500]	Time  6.521 ( 6.521)	Loss 0.3005 (0.2223)	CeLoss 0.0396 (0.0729)	SegCLSLoss 0.0212 (0.0111)	KLLoss 0.0219 (0.0110)	MaskLoss 0.1198 (0.0692)	MaskBCELoss 0.0300 (0.0130)	MaskDICELoss 0.0898 (0.0562)
Epoch: [0][271/500]	Time  7.316 ( 7.316)	Loss 0.2889 (0.2046)	CeLoss 0.0369 (0.0540)	SegCLSLoss 0.0135 (0.0095)	KLLoss 0.0116 (0.0112)	MaskLoss 0.1197 (0.0702)	MaskBCELoss 0.0280 (0.0140)	MaskDICELoss 0.0916 (0.0562)
Epoch: [0][272/500]	Time  7.097 ( 7.097)	Loss 0.1227 (0.2340)	CeLoss 0.1226 (0.0548)	SegCLSLoss 0.0000 (0.0102)	KLLoss 0.0000 (0.0127)	MaskLoss 0.0000 (0.0839)	MaskBCELoss 0.0000 (0.0278)	MaskDICELoss 0.0000 (0.0561)
Epoch: [0][273/500]	Time  7.555 ( 7.555)	Loss 0.3046 (0.2777)	CeLoss 0.0310 (0.0393)	SegCLSLoss 0.0170 (0.0144)	KLLoss 0.0258 (0.0197)	MaskLoss 0.1261 (0.1107)	MaskBCELoss 0.0350 (0.0264)	MaskDICELoss 0.0912 (0.0843)
Epoch: [0][274/500]	Time  7.206 ( 7.206)	Loss 0.3474 (0.2603)	CeLoss 0.0299 (0.0557)	SegCLSLoss 0.0178 (0.0114)	KLLoss 0.0239 (0.0154)	MaskLoss 0.1483 (0.0956)	MaskBCELoss 0.0575 (0.0295)	MaskDICELoss 0.0909 (0.0660)
Epoch: [0][275/500]	Time  7.710 ( 7.710)	Loss 0.2601 (0.2112)	CeLoss 0.0286 (0.0622)	SegCLSLoss 0.0140 (0.0112)	KLLoss 0.0175 (0.0127)	MaskLoss 0.1079 (0.0685)	MaskBCELoss 0.0100 (0.0121)	MaskDICELoss 0.0979 (0.0564)
Epoch: [0][276/500]	Time  6.473 ( 6.473)	Loss 0.2911 (0.2116)	CeLoss 0.0364 (0.0610)	SegCLSLoss 0.0146 (0.0095)	KLLoss 0.0198 (0.0127)	MaskLoss 0.1188 (0.0698)	MaskBCELoss 0.0232 (0.0128)	MaskDICELoss 0.0956 (0.0570)
Epoch: [0][277/500]	Time  7.800 ( 7.800)	Loss 0.3076 (0.2745)	CeLoss 0.0356 (0.0397)	SegCLSLoss 0.0140 (0.0160)	KLLoss 0.0170 (0.0200)	MaskLoss 0.1282 (0.1083)	MaskBCELoss 0.0366 (0.0241)	MaskDICELoss 0.0916 (0.0842)
Epoch: [0][278/500]	Time  8.764 ( 8.764)	Loss 0.3262 (0.2587)	CeLoss 0.0303 (0.0356)	SegCLSLoss 0.0150 (0.0128)	KLLoss 0.0238 (0.0169)	MaskLoss 0.1382 (0.1041)	MaskBCELoss 0.0413 (0.0310)	MaskDICELoss 0.0969 (0.0731)
Epoch: [0][279/500]	Time  7.371 ( 7.371)	Loss 0.2653 (0.2887)	CeLoss 0.0310 (0.0376)	SegCLSLoss 0.0157 (0.0129)	KLLoss 0.0198 (0.0177)	MaskLoss 0.1083 (0.1179)	MaskBCELoss 0.0112 (0.0336)	MaskDICELoss 0.0970 (0.0843)
Epoch: [0][280/500]	Time  6.872 ( 6.872)	Loss 0.2537 (0.2245)	CeLoss 0.0277 (0.0480)	SegCLSLoss 0.0193 (0.0135)	KLLoss 0.0280 (0.0167)	MaskLoss 0.1013 (0.0807)	MaskBCELoss 0.0038 (0.0155)	MaskDICELoss 0.0974 (0.0652)
Epoch: [0][281/500]	Time  6.197 ( 6.197)	Loss 0.2537 (0.2194)	CeLoss 0.0223 (0.0482)	SegCLSLoss 0.0175 (0.0111)	KLLoss 0.0267 (0.0168)	MaskLoss 0.1046 (0.0786)	MaskBCELoss 0.0055 (0.0124)	MaskDICELoss 0.0991 (0.0661)
Epoch: [0][282/500]	Time  6.312 ( 6.312)	Loss 0.0895 (0.2343)	CeLoss 0.0894 (0.0600)	SegCLSLoss 0.0000 (0.0094)	KLLoss 0.0000 (0.0153)	MaskLoss 0.0000 (0.0809)	MaskBCELoss 0.0000 (0.0166)	MaskDICELoss 0.0000 (0.0643)
Epoch: [0][283/500]	Time  7.392 ( 7.392)	Loss 0.0574 (0.2141)	CeLoss 0.0574 (0.0354)	SegCLSLoss 0.0000 (0.0101)	KLLoss 0.0000 (0.0151)	MaskLoss 0.0000 (0.0831)	MaskBCELoss 0.0000 (0.0175)	MaskDICELoss 0.0000 (0.0655)
Epoch: [0][284/500]	Time  6.191 ( 6.191)	Loss 0.2731 (0.2185)	CeLoss 0.0266 (0.0543)	SegCLSLoss 0.0153 (0.0109)	KLLoss 0.0234 (0.0147)	MaskLoss 0.1136 (0.0757)	MaskBCELoss 0.0147 (0.0198)	MaskDICELoss 0.0989 (0.0559)
Epoch: [0][285/500]	Time  7.701 ( 7.701)	Loss 0.2663 (0.2474)	CeLoss 0.0396 (0.0455)	SegCLSLoss 0.0155 (0.0100)	KLLoss 0.0232 (0.0161)	MaskLoss 0.1036 (0.0944)	MaskBCELoss 0.0046 (0.0194)	MaskDICELoss 0.0990 (0.0751)
Epoch: [0][286/500]	Time  8.094 ( 8.094)	Loss 0.2644 (0.2860)	CeLoss 0.0356 (0.0304)	SegCLSLoss 0.0140 (0.0157)	KLLoss 0.0208 (0.0240)	MaskLoss 0.1056 (0.1179)	MaskBCELoss 0.0078 (0.0241)	MaskDICELoss 0.0978 (0.0938)
Epoch: [0][287/500]	Time  7.095 ( 7.095)	Loss 0.3017 (0.2297)	CeLoss 0.0303 (0.0461)	SegCLSLoss 0.0133 (0.0087)	KLLoss 0.0216 (0.0141)	MaskLoss 0.1270 (0.0861)	MaskBCELoss 0.0439 (0.0237)	MaskDICELoss 0.0832 (0.0624)
Epoch: [0][288/500]	Time  8.076 ( 8.076)	Loss 0.3065 (0.2909)	CeLoss 0.0334 (0.0296)	SegCLSLoss 0.0112 (0.0135)	KLLoss 0.0178 (0.0208)	MaskLoss 0.1292 (0.1221)	MaskBCELoss 0.0418 (0.0329)	MaskDICELoss 0.0875 (0.0891)
Epoch: [0][289/500]	Time  6.468 ( 6.468)	Loss 0.0840 (0.2103)	CeLoss 0.0840 (0.0338)	SegCLSLoss 0.0000 (0.0095)	KLLoss 0.0000 (0.0148)	MaskLoss 0.0000 (0.0822)	MaskBCELoss 0.0000 (0.0184)	MaskDICELoss 0.0000 (0.0637)
Epoch: [0][290/500]	Time  6.194 ( 6.194)	Loss 0.2812 (0.2060)	CeLoss 0.0303 (0.0549)	SegCLSLoss 0.0134 (0.0094)	KLLoss 0.0222 (0.0141)	MaskLoss 0.1166 (0.0697)	MaskBCELoss 0.0249 (0.0137)	MaskDICELoss 0.0917 (0.0560)
Epoch: [0][291/500]	Time  5.568 ( 5.568)	Loss 0.0926 (0.1881)	CeLoss 0.0928 (0.0669)	SegCLSLoss 0.0000 (0.0071)	KLLoss 0.0000 (0.0115)	MaskLoss 0.0000 (0.0560)	MaskBCELoss 0.0000 (0.0091)	MaskDICELoss 0.0000 (0.0469)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([23, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][292/500]	Time  7.846 ( 7.846)	Loss 0.2696 (0.2635)	CeLoss 0.0280 (0.0349)	SegCLSLoss 0.0143 (0.0135)	KLLoss 0.0240 (0.0207)	MaskLoss 0.1112 (0.1057)	MaskBCELoss 0.0195 (0.0226)	MaskDICELoss 0.0917 (0.0832)
Epoch: [0][293/500]	Time  6.738 ( 6.738)	Loss 0.2690 (0.2392)	CeLoss 0.0283 (0.0344)	SegCLSLoss 0.0159 (0.0118)	KLLoss 0.0260 (0.0182)	MaskLoss 0.1099 (0.0948)	MaskBCELoss 0.0149 (0.0207)	MaskDICELoss 0.0950 (0.0742)
Epoch: [0][294/500]	Time  6.818 ( 6.818)	Loss 0.1484 (0.2622)	CeLoss 0.1484 (0.0366)	SegCLSLoss 0.0000 (0.0147)	KLLoss 0.0000 (0.0226)	MaskLoss 0.0000 (0.1035)	MaskBCELoss 0.0000 (0.0178)	MaskDICELoss 0.0000 (0.0857)
Epoch: [0][295/500]	Time  6.970 ( 6.970)	Loss 0.1891 (0.2479)	CeLoss 0.1895 (0.0489)	SegCLSLoss 0.0000 (0.0100)	KLLoss 0.0000 (0.0171)	MaskLoss 0.0000 (0.0928)	MaskBCELoss 0.0000 (0.0198)	MaskDICELoss 0.0000 (0.0729)
Epoch: [0][296/500]	Time  5.788 ( 5.788)	Loss 0.2968 (0.2066)	CeLoss 0.0221 (0.0705)	SegCLSLoss 0.0168 (0.0065)	KLLoss 0.0281 (0.0111)	MaskLoss 0.1261 (0.0637)	MaskBCELoss 0.0330 (0.0188)	MaskDICELoss 0.0931 (0.0448)
Epoch: [0][297/500]	Time  8.235 ( 8.235)	Loss 0.3334 (0.2513)	CeLoss 0.0312 (0.0333)	SegCLSLoss 0.0157 (0.0112)	KLLoss 0.0273 (0.0189)	MaskLoss 0.1403 (0.1015)	MaskBCELoss 0.0566 (0.0307)	MaskDICELoss 0.0837 (0.0708)
Epoch: [0][298/500]	Time  7.195 ( 7.195)	Loss 0.3197 (0.2611)	CeLoss 0.0280 (0.0436)	SegCLSLoss 0.0148 (0.0101)	KLLoss 0.0234 (0.0191)	MaskLoss 0.1363 (0.1015)	MaskBCELoss 0.0463 (0.0276)	MaskDICELoss 0.0901 (0.0739)
Epoch: [0][299/500]	Time  5.656 ( 5.656)	Loss 0.2958 (0.1898)	CeLoss 0.0267 (0.0593)	SegCLSLoss 0.0119 (0.0073)	KLLoss 0.0228 (0.0109)	MaskLoss 0.1258 (0.0607)	MaskBCELoss 0.0378 (0.0151)	MaskDICELoss 0.0881 (0.0456)
[2025-03-06 02:06:19,062] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[0.0002972650602409638], mom=[(0.9, 0.95)]
[2025-03-06 02:06:19,067] [INFO] [timer.py:215:stop] epoch=0/micro_step=300/global_step=30, RunningAvgSamplesPerSec=1.3761552869354226, CurrSamplesPerSec=1.5926235207597137, MemAllocated=57.46GB, MaxMemAllocated=62.82GB
Epoch: [0][300/500]	Time  7.906 ( 7.906)	Loss 0.2536 (0.2733)	CeLoss 0.0258 (0.0449)	SegCLSLoss 0.0137 (0.0116)	KLLoss 0.0210 (0.0190)	MaskLoss 0.1052 (0.1065)	MaskBCELoss 0.0053 (0.0242)	MaskDICELoss 0.0999 (0.0823)
Epoch: [0][301/500]	Time  7.001 ( 7.001)	Loss 0.2795 (0.2319)	CeLoss 0.0233 (0.0563)	SegCLSLoss 0.0119 (0.0074)	KLLoss 0.0264 (0.0146)	MaskLoss 0.1185 (0.0823)	MaskBCELoss 0.0253 (0.0176)	MaskDICELoss 0.0932 (0.0647)
Epoch: [0][302/500]	Time  7.130 ( 7.130)	Loss 0.3034 (0.2685)	CeLoss 0.0270 (0.0354)	SegCLSLoss 0.0087 (0.0100)	KLLoss 0.0183 (0.0200)	MaskLoss 0.1315 (0.1090)	MaskBCELoss 0.0518 (0.0295)	MaskDICELoss 0.0797 (0.0795)
Epoch: [0][303/500]	Time  5.386 ( 5.386)	Loss 0.0422 (0.1493)	CeLoss 0.0422 (0.0708)	SegCLSLoss 0.0000 (0.0044)	KLLoss 0.0000 (0.0075)	MaskLoss 0.0000 (0.0362)	MaskBCELoss 0.0000 (0.0098)	MaskDICELoss 0.0000 (0.0264)
Epoch: [0][304/500]	Time  7.848 ( 7.848)	Loss 0.2616 (0.2564)	CeLoss 0.0249 (0.0362)	SegCLSLoss 0.0140 (0.0104)	KLLoss 0.0244 (0.0193)	MaskLoss 0.1087 (0.1027)	MaskBCELoss 0.0181 (0.0179)	MaskDICELoss 0.0906 (0.0848)
Epoch: [0][305/500]	Time  7.102 ( 7.102)	Loss 0.2723 (0.2749)	CeLoss 0.0254 (0.0372)	SegCLSLoss 0.0111 (0.0100)	KLLoss 0.0222 (0.0195)	MaskLoss 0.1151 (0.1114)	MaskBCELoss 0.0219 (0.0292)	MaskDICELoss 0.0932 (0.0823)
Epoch: [0][306/500]	Time  8.627 ( 8.627)	Loss 0.2894 (0.2550)	CeLoss 0.0250 (0.0418)	SegCLSLoss 0.0145 (0.0114)	KLLoss 0.0234 (0.0179)	MaskLoss 0.1227 (0.0992)	MaskBCELoss 0.0315 (0.0278)	MaskDICELoss 0.0912 (0.0714)
Epoch: [0][307/500]	Time  6.007 ( 6.007)	Loss 0.2895 (0.2288)	CeLoss 0.0260 (0.0707)	SegCLSLoss 0.0148 (0.0090)	KLLoss 0.0232 (0.0149)	MaskLoss 0.1223 (0.0730)	MaskBCELoss 0.0334 (0.0188)	MaskDICELoss 0.0890 (0.0542)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([27, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][308/500]	Time  6.863 ( 6.863)	Loss 0.0918 (0.2044)	CeLoss 0.0918 (0.0551)	SegCLSLoss 0.0000 (0.0093)	KLLoss 0.0000 (0.0149)	MaskLoss 0.0000 (0.0686)	MaskBCELoss 0.0000 (0.0129)	MaskDICELoss 0.0000 (0.0557)
Epoch: [0][309/500]	Time  7.611 ( 7.611)	Loss 0.0465 (0.2260)	CeLoss 0.0464 (0.0489)	SegCLSLoss 0.0000 (0.0088)	KLLoss 0.0000 (0.0155)	MaskLoss 0.0000 (0.0825)	MaskBCELoss 0.0000 (0.0191)	MaskDICELoss 0.0000 (0.0634)
Epoch: [0][310/500]	Time  8.323 ( 8.323)	Loss 0.1063 (0.2420)	CeLoss 0.1064 (0.0357)	SegCLSLoss 0.0000 (0.0098)	KLLoss 0.0000 (0.0175)	MaskLoss 0.0000 (0.0964)	MaskBCELoss 0.0000 (0.0240)	MaskDICELoss 0.0000 (0.0723)
Epoch: [0][311/500]	Time  7.661 ( 7.661)	Loss 0.2604 (0.2695)	CeLoss 0.0256 (0.0333)	SegCLSLoss 0.0131 (0.0108)	KLLoss 0.0242 (0.0198)	MaskLoss 0.1081 (0.1105)	MaskBCELoss 0.0132 (0.0300)	MaskDICELoss 0.0949 (0.0805)
Epoch: [0][312/500]	Time  7.356 ( 7.356)	Loss 0.2919 (0.2611)	CeLoss 0.0270 (0.0315)	SegCLSLoss 0.0120 (0.0128)	KLLoss 0.0244 (0.0224)	MaskLoss 0.1233 (0.1060)	MaskBCELoss 0.0354 (0.0242)	MaskDICELoss 0.0879 (0.0819)
Epoch: [0][313/500]	Time  8.200 ( 8.200)	Loss 0.2734 (0.2685)	CeLoss 0.0247 (0.0300)	SegCLSLoss 0.0080 (0.0110)	KLLoss 0.0178 (0.0202)	MaskLoss 0.1180 (0.1115)	MaskBCELoss 0.0281 (0.0298)	MaskDICELoss 0.0899 (0.0817)
Epoch: [0][314/500]	Time  7.037 ( 7.037)	Loss 0.2998 (0.2665)	CeLoss 0.0242 (0.0325)	SegCLSLoss 0.0137 (0.0121)	KLLoss 0.0248 (0.0228)	MaskLoss 0.1282 (0.1083)	MaskBCELoss 0.0444 (0.0266)	MaskDICELoss 0.0838 (0.0817)
Epoch: [0][315/500]	Time  7.102 ( 7.102)	Loss 0.2645 (0.2662)	CeLoss 0.0271 (0.0349)	SegCLSLoss 0.0148 (0.0119)	KLLoss 0.0277 (0.0230)	MaskLoss 0.1081 (0.1069)	MaskBCELoss 0.0144 (0.0252)	MaskDICELoss 0.0936 (0.0817)
Epoch: [0][316/500]	Time  7.374 ( 7.374)	Loss 0.2452 (0.2342)	CeLoss 0.0227 (0.0599)	SegCLSLoss 0.0103 (0.0079)	KLLoss 0.0225 (0.0152)	MaskLoss 0.1031 (0.0813)	MaskBCELoss 0.0052 (0.0166)	MaskDICELoss 0.0979 (0.0647)
Epoch: [0][317/500]	Time  8.050 ( 8.050)	Loss 0.3455 (0.2610)	CeLoss 0.0237 (0.0296)	SegCLSLoss 0.0110 (0.0114)	KLLoss 0.0267 (0.0217)	MaskLoss 0.1515 (0.1075)	MaskBCELoss 0.0665 (0.0241)	MaskDICELoss 0.0850 (0.0834)
Epoch: [0][318/500]	Time  6.624 ( 6.624)	Loss 0.1250 (0.2118)	CeLoss 0.1250 (0.0535)	SegCLSLoss 0.0000 (0.0075)	KLLoss 0.0000 (0.0137)	MaskLoss 0.0000 (0.0738)	MaskBCELoss 0.0000 (0.0193)	MaskDICELoss 0.0000 (0.0546)
Epoch: [0][319/500]	Time  8.427 ( 8.427)	Loss 0.2546 (0.2718)	CeLoss 0.0289 (0.0247)	SegCLSLoss 0.0128 (0.0131)	KLLoss 0.0233 (0.0239)	MaskLoss 0.1038 (0.1143)	MaskBCELoss 0.0038 (0.0207)	MaskDICELoss 0.1000 (0.0936)
Epoch: [0][320/500]	Time  6.685 ( 6.685)	Loss 0.0953 (0.1900)	CeLoss 0.0952 (0.0590)	SegCLSLoss 0.0000 (0.0055)	KLLoss 0.0000 (0.0108)	MaskLoss 0.0000 (0.0615)	MaskBCELoss 0.0000 (0.0169)	MaskDICELoss 0.0000 (0.0446)
Epoch: [0][321/500]	Time  6.821 ( 6.821)	Loss 0.2453 (0.1849)	CeLoss 0.0251 (0.0402)	SegCLSLoss 0.0092 (0.0063)	KLLoss 0.0203 (0.0130)	MaskLoss 0.1027 (0.0676)	MaskBCELoss 0.0030 (0.0114)	MaskDICELoss 0.0998 (0.0562)
Epoch: [0][322/500]	Time  6.917 ( 6.917)	Loss 0.2606 (0.2369)	CeLoss 0.0203 (0.0379)	SegCLSLoss 0.0153 (0.0095)	KLLoss 0.0283 (0.0182)	MaskLoss 0.1092 (0.0925)	MaskBCELoss 0.0188 (0.0190)	MaskDICELoss 0.0904 (0.0735)
Epoch: [0][323/500]	Time  7.220 ( 7.220)	Loss 0.2553 (0.2364)	CeLoss 0.0327 (0.0487)	SegCLSLoss 0.0109 (0.0094)	KLLoss 0.0209 (0.0186)	MaskLoss 0.1032 (0.0869)	MaskBCELoss 0.0036 (0.0114)	MaskDICELoss 0.0997 (0.0754)
Epoch: [0][324/500]	Time  9.161 ( 9.161)	Loss 0.2959 (0.2805)	CeLoss 0.0322 (0.0279)	SegCLSLoss 0.0074 (0.0101)	KLLoss 0.0148 (0.0212)	MaskLoss 0.1263 (0.1185)	MaskBCELoss 0.0448 (0.0274)	MaskDICELoss 0.0815 (0.0911)
Epoch: [0][325/500]	Time  6.816 ( 6.816)	Loss 0.2698 (0.2469)	CeLoss 0.0248 (0.0534)	SegCLSLoss 0.0087 (0.0074)	KLLoss 0.0216 (0.0161)	MaskLoss 0.1150 (0.0908)	MaskBCELoss 0.0225 (0.0195)	MaskDICELoss 0.0925 (0.0713)
Epoch: [0][326/500]	Time  6.364 ( 6.364)	Loss 0.2870 (0.2433)	CeLoss 0.0249 (0.0382)	SegCLSLoss 0.0220 (0.0079)	KLLoss 0.0266 (0.0159)	MaskLoss 0.1189 (0.0966)	MaskBCELoss 0.0249 (0.0228)	MaskDICELoss 0.0940 (0.0738)
Epoch: [0][327/500]	Time  7.571 ( 7.571)	Loss 0.2746 (0.2768)	CeLoss 0.0220 (0.0361)	SegCLSLoss 0.0154 (0.0098)	KLLoss 0.0261 (0.0198)	MaskLoss 0.1159 (0.1129)	MaskBCELoss 0.0233 (0.0308)	MaskDICELoss 0.0926 (0.0821)
Epoch: [0][328/500]	Time  6.094 ( 6.094)	Loss 0.2784 (0.2069)	CeLoss 0.0271 (0.0566)	SegCLSLoss 0.0094 (0.0061)	KLLoss 0.0217 (0.0124)	MaskLoss 0.1179 (0.0705)	MaskBCELoss 0.0204 (0.0158)	MaskDICELoss 0.0975 (0.0547)
Epoch: [0][329/500]	Time  7.347 ( 7.347)	Loss 0.0840 (0.2338)	CeLoss 0.0840 (0.0332)	SegCLSLoss 0.0000 (0.0095)	KLLoss 0.0000 (0.0172)	MaskLoss 0.0000 (0.0936)	MaskBCELoss 0.0000 (0.0214)	MaskDICELoss 0.0000 (0.0722)
Epoch: [0][330/500]	Time  7.283 ( 7.283)	Loss 0.2447 (0.2387)	CeLoss 0.0244 (0.0402)	SegCLSLoss 0.0108 (0.0111)	KLLoss 0.0232 (0.0188)	MaskLoss 0.1017 (0.0918)	MaskBCELoss 0.0066 (0.0179)	MaskDICELoss 0.0951 (0.0739)
Epoch: [0][331/500]	Time  6.461 ( 6.461)	Loss 0.2573 (0.2347)	CeLoss 0.0317 (0.0510)	SegCLSLoss 0.0114 (0.0076)	KLLoss 0.0247 (0.0148)	MaskLoss 0.1038 (0.0863)	MaskBCELoss 0.0088 (0.0246)	MaskDICELoss 0.0950 (0.0616)
Epoch: [0][332/500]	Time  5.513 ( 5.513)	Loss 0.2612 (0.2310)	CeLoss 0.0226 (0.0392)	SegCLSLoss 0.0106 (0.0074)	KLLoss 0.0221 (0.0148)	MaskLoss 0.1111 (0.0903)	MaskBCELoss 0.0125 (0.0164)	MaskDICELoss 0.0986 (0.0739)
Epoch: [0][333/500]	Time  8.442 ( 8.442)	Loss 0.2535 (0.2502)	CeLoss 0.0300 (0.0286)	SegCLSLoss 0.0068 (0.0099)	KLLoss 0.0183 (0.0201)	MaskLoss 0.1054 (0.1033)	MaskBCELoss 0.0072 (0.0180)	MaskDICELoss 0.0982 (0.0852)
Epoch: [0][334/500]	Time  7.866 ( 7.866)	Loss 0.3236 (0.2577)	CeLoss 0.0376 (0.0411)	SegCLSLoss 0.0067 (0.0079)	KLLoss 0.0140 (0.0175)	MaskLoss 0.1379 (0.1020)	MaskBCELoss 0.0485 (0.0272)	MaskDICELoss 0.0893 (0.0748)
Epoch: [0][335/500]	Time  8.020 ( 8.020)	Loss 0.2783 (0.2045)	CeLoss 0.0212 (0.0412)	SegCLSLoss 0.0104 (0.0071)	KLLoss 0.0198 (0.0145)	MaskLoss 0.1210 (0.0763)	MaskBCELoss 0.0289 (0.0092)	MaskDICELoss 0.0921 (0.0671)
Epoch: [0][336/500]	Time  7.327 ( 7.327)	Loss 0.0766 (0.2382)	CeLoss 0.0767 (0.0416)	SegCLSLoss 0.0000 (0.0088)	KLLoss 0.0000 (0.0189)	MaskLoss 0.0000 (0.0914)	MaskBCELoss 0.0000 (0.0166)	MaskDICELoss 0.0000 (0.0748)
Epoch: [0][337/500]	Time  5.986 ( 5.986)	Loss 0.3134 (0.2164)	CeLoss 0.0276 (0.0597)	SegCLSLoss 0.0114 (0.0052)	KLLoss 0.0216 (0.0124)	MaskLoss 0.1347 (0.0739)	MaskBCELoss 0.0538 (0.0215)	MaskDICELoss 0.0810 (0.0524)
Epoch: [0][338/500]	Time  7.185 ( 7.185)	Loss 0.1133 (0.2511)	CeLoss 0.1133 (0.0434)	SegCLSLoss 0.0000 (0.0098)	KLLoss 0.0000 (0.0185)	MaskLoss 0.0000 (0.0968)	MaskBCELoss 0.0000 (0.0234)	MaskDICELoss 0.0000 (0.0734)
Epoch: [0][339/500]	Time  6.161 ( 6.161)	Loss 0.3059 (0.1902)	CeLoss 0.0209 (0.0564)	SegCLSLoss 0.0104 (0.0067)	KLLoss 0.0234 (0.0123)	MaskLoss 0.1340 (0.0622)	MaskBCELoss 0.0365 (0.0156)	MaskDICELoss 0.0976 (0.0466)
Epoch: [0][340/500]	Time  7.083 ( 7.083)	Loss 0.2507 (0.2436)	CeLoss 0.0322 (0.0503)	SegCLSLoss 0.0079 (0.0065)	KLLoss 0.0197 (0.0154)	MaskLoss 0.1024 (0.0912)	MaskBCELoss 0.0037 (0.0162)	MaskDICELoss 0.0987 (0.0750)
Epoch: [0][341/500]	Time  6.800 ( 6.800)	Loss 0.2661 (0.2302)	CeLoss 0.0203 (0.0308)	SegCLSLoss 0.0134 (0.0098)	KLLoss 0.0266 (0.0178)	MaskLoss 0.1129 (0.0928)	MaskBCELoss 0.0207 (0.0186)	MaskDICELoss 0.0922 (0.0741)
Epoch: [0][342/500]	Time  8.195 ( 8.195)	Loss 0.1023 (0.2109)	CeLoss 0.1025 (0.0496)	SegCLSLoss 0.0000 (0.0055)	KLLoss 0.0000 (0.0119)	MaskLoss 0.0000 (0.0763)	MaskBCELoss 0.0000 (0.0216)	MaskDICELoss 0.0000 (0.0547)
Epoch: [0][343/500]	Time  6.095 ( 6.095)	Loss 0.2539 (0.2112)	CeLoss 0.0217 (0.0576)	SegCLSLoss 0.0110 (0.0059)	KLLoss 0.0232 (0.0126)	MaskLoss 0.1074 (0.0721)	MaskBCELoss 0.0124 (0.0172)	MaskDICELoss 0.0951 (0.0549)
Epoch: [0][344/500]	Time  7.763 ( 7.763)	Loss 0.2581 (0.2474)	CeLoss 0.0273 (0.0363)	SegCLSLoss 0.0079 (0.0086)	KLLoss 0.0198 (0.0195)	MaskLoss 0.1086 (0.0986)	MaskBCELoss 0.0175 (0.0127)	MaskDICELoss 0.0910 (0.0859)
Epoch: [0][345/500]	Time  8.672 ( 8.672)	Loss 0.0637 (0.2270)	CeLoss 0.0635 (0.0312)	SegCLSLoss 0.0000 (0.0082)	KLLoss 0.0000 (0.0172)	MaskLoss 0.0000 (0.0915)	MaskBCELoss 0.0000 (0.0170)	MaskDICELoss 0.0000 (0.0745)
Epoch: [0][346/500]	Time  6.571 ( 6.571)	Loss 0.2580 (0.2145)	CeLoss 0.0244 (0.0462)	SegCLSLoss 0.0076 (0.0069)	KLLoss 0.0203 (0.0162)	MaskLoss 0.1099 (0.0784)	MaskBCELoss 0.0178 (0.0146)	MaskDICELoss 0.0921 (0.0638)
Epoch: [0][347/500]	Time  6.860 ( 6.860)	Loss 0.2706 (0.2445)	CeLoss 0.0249 (0.0500)	SegCLSLoss 0.0063 (0.0072)	KLLoss 0.0173 (0.0162)	MaskLoss 0.1169 (0.0914)	MaskBCELoss 0.0358 (0.0191)	MaskDICELoss 0.0812 (0.0723)
Epoch: [0][348/500]	Time  5.869 ( 5.869)	Loss 0.3027 (0.1962)	CeLoss 0.0305 (0.0723)	SegCLSLoss 0.0065 (0.0045)	KLLoss 0.0155 (0.0102)	MaskLoss 0.1305 (0.0583)	MaskBCELoss 0.0532 (0.0123)	MaskDICELoss 0.0774 (0.0460)
Epoch: [0][349/500]	Time  7.758 ( 7.758)	Loss 0.2694 (0.2439)	CeLoss 0.0437 (0.0317)	SegCLSLoss 0.0176 (0.0097)	KLLoss 0.0215 (0.0187)	MaskLoss 0.1030 (0.0990)	MaskBCELoss 0.0204 (0.0144)	MaskDICELoss 0.0826 (0.0846)
Epoch: [0][350/500]	Time  7.237 ( 7.237)	Loss 0.1594 (0.2236)	CeLoss 0.1592 (0.0493)	SegCLSLoss 0.0000 (0.0067)	KLLoss 0.0000 (0.0140)	MaskLoss 0.0000 (0.0820)	MaskBCELoss 0.0000 (0.0176)	MaskDICELoss 0.0000 (0.0644)
Epoch: [0][351/500]	Time  7.946 ( 7.946)	Loss 0.0980 (0.2631)	CeLoss 0.0981 (0.0329)	SegCLSLoss 0.0000 (0.0077)	KLLoss 0.0000 (0.0177)	MaskLoss 0.0000 (0.1087)	MaskBCELoss 0.0000 (0.0284)	MaskDICELoss 0.0000 (0.0804)
Epoch: [0][352/500]	Time  6.424 ( 6.424)	Loss 0.2979 (0.2508)	CeLoss 0.0317 (0.0522)	SegCLSLoss 0.0073 (0.0065)	KLLoss 0.0188 (0.0158)	MaskLoss 0.1266 (0.0938)	MaskBCELoss 0.0583 (0.0212)	MaskDICELoss 0.0683 (0.0725)
Epoch: [0][353/500]	Time  7.105 ( 7.105)	Loss 0.0688 (0.2129)	CeLoss 0.0688 (0.0347)	SegCLSLoss 0.0000 (0.0069)	KLLoss 0.0000 (0.0142)	MaskLoss 0.0000 (0.0838)	MaskBCELoss 0.0000 (0.0221)	MaskDICELoss 0.0000 (0.0618)
Epoch: [0][354/500]	Time  7.165 ( 7.165)	Loss 0.2859 (0.2402)	CeLoss 0.0371 (0.0427)	SegCLSLoss 0.0070 (0.0073)	KLLoss 0.0171 (0.0163)	MaskLoss 0.1183 (0.0929)	MaskBCELoss 0.0375 (0.0213)	MaskDICELoss 0.0808 (0.0716)
Epoch: [0][355/500]	Time  7.199 ( 7.199)	Loss 0.2856 (0.1875)	CeLoss 0.0273 (0.0436)	SegCLSLoss 0.0082 (0.0052)	KLLoss 0.0183 (0.0117)	MaskLoss 0.1225 (0.0678)	MaskBCELoss 0.0274 (0.0112)	MaskDICELoss 0.0951 (0.0565)
Epoch: [0][356/500]	Time  7.357 ( 7.357)	Loss 0.0875 (0.2184)	CeLoss 0.0874 (0.0447)	SegCLSLoss 0.0000 (0.0049)	KLLoss 0.0000 (0.0119)	MaskLoss 0.0000 (0.0827)	MaskBCELoss 0.0000 (0.0194)	MaskDICELoss 0.0000 (0.0633)
Epoch: [0][357/500]	Time  7.179 ( 7.179)	Loss 0.3231 (0.2578)	CeLoss 0.0276 (0.0255)	SegCLSLoss 0.0079 (0.0086)	KLLoss 0.0195 (0.0179)	MaskLoss 0.1410 (0.1095)	MaskBCELoss 0.0668 (0.0288)	MaskDICELoss 0.0742 (0.0807)
Epoch: [0][358/500]	Time  6.994 ( 6.994)	Loss 0.1313 (0.2125)	CeLoss 0.1309 (0.0417)	SegCLSLoss 0.0000 (0.0068)	KLLoss 0.0000 (0.0146)	MaskLoss 0.0000 (0.0800)	MaskBCELoss 0.0000 (0.0152)	MaskDICELoss 0.0000 (0.0648)
Epoch: [0][359/500]	Time  6.599 ( 6.599)	Loss 0.3158 (0.2414)	CeLoss 0.0256 (0.0404)	SegCLSLoss 0.0059 (0.0070)	KLLoss 0.0130 (0.0148)	MaskLoss 0.1404 (0.0951)	MaskBCELoss 0.0630 (0.0234)	MaskDICELoss 0.0775 (0.0717)
Epoch: [0][360/500]	Time  7.435 ( 7.435)	Loss 0.2753 (0.2521)	CeLoss 0.0299 (0.0273)	SegCLSLoss 0.0062 (0.0086)	KLLoss 0.0146 (0.0182)	MaskLoss 0.1175 (0.1057)	MaskBCELoss 0.0290 (0.0251)	MaskDICELoss 0.0885 (0.0806)
Epoch: [0][361/500]	Time  6.690 ( 6.690)	Loss 0.2482 (0.2139)	CeLoss 0.0200 (0.0504)	SegCLSLoss 0.0098 (0.0069)	KLLoss 0.0273 (0.0145)	MaskLoss 0.1047 (0.0764)	MaskBCELoss 0.0184 (0.0110)	MaskDICELoss 0.0864 (0.0654)
Epoch: [0][362/500]	Time  7.756 ( 7.756)	Loss 0.2477 (0.2413)	CeLoss 0.0228 (0.0340)	SegCLSLoss 0.0094 (0.0098)	KLLoss 0.0195 (0.0194)	MaskLoss 0.1052 (0.0963)	MaskBCELoss 0.0058 (0.0088)	MaskDICELoss 0.0994 (0.0876)
Epoch: [0][363/500]	Time  7.698 ( 7.698)	Loss 0.2738 (0.2170)	CeLoss 0.0200 (0.0501)	SegCLSLoss 0.0109 (0.0061)	KLLoss 0.0254 (0.0140)	MaskLoss 0.1178 (0.0784)	MaskBCELoss 0.0261 (0.0153)	MaskDICELoss 0.0917 (0.0632)
Epoch: [0][364/500]	Time  7.258 ( 7.258)	Loss 0.2346 (0.2307)	CeLoss 0.0208 (0.0391)	SegCLSLoss 0.0103 (0.0065)	KLLoss 0.0223 (0.0141)	MaskLoss 0.0989 (0.0907)	MaskBCELoss 0.0071 (0.0191)	MaskDICELoss 0.0917 (0.0716)
Epoch: [0][365/500]	Time  7.035 ( 7.035)	Loss 0.2592 (0.2280)	CeLoss 0.0222 (0.0482)	SegCLSLoss 0.0087 (0.0061)	KLLoss 0.0189 (0.0138)	MaskLoss 0.1116 (0.0849)	MaskBCELoss 0.0141 (0.0220)	MaskDICELoss 0.0976 (0.0629)
Epoch: [0][366/500]	Time  7.703 ( 7.703)	Loss 0.3671 (0.2656)	CeLoss 0.0232 (0.0388)	SegCLSLoss 0.0058 (0.0062)	KLLoss 0.0130 (0.0146)	MaskLoss 0.1673 (0.1082)	MaskBCELoss 0.0745 (0.0262)	MaskDICELoss 0.0928 (0.0820)
Epoch: [0][367/500]	Time  7.501 ( 7.501)	Loss 0.3005 (0.2315)	CeLoss 0.0260 (0.0329)	SegCLSLoss 0.0086 (0.0084)	KLLoss 0.0192 (0.0167)	MaskLoss 0.1303 (0.0930)	MaskBCELoss 0.0455 (0.0196)	MaskDICELoss 0.0848 (0.0735)
Epoch: [0][368/500]	Time  7.340 ( 7.340)	Loss 0.2578 (0.2452)	CeLoss 0.0309 (0.0468)	SegCLSLoss 0.0085 (0.0083)	KLLoss 0.0232 (0.0168)	MaskLoss 0.1054 (0.0929)	MaskBCELoss 0.0060 (0.0190)	MaskDICELoss 0.0995 (0.0739)
Epoch: [0][369/500]	Time  6.869 ( 6.869)	Loss 0.2455 (0.2015)	CeLoss 0.0220 (0.0350)	SegCLSLoss 0.0090 (0.0070)	KLLoss 0.0211 (0.0144)	MaskLoss 0.1042 (0.0779)	MaskBCELoss 0.0093 (0.0163)	MaskDICELoss 0.0949 (0.0616)
Epoch: [0][370/500]	Time  6.791 ( 6.791)	Loss 0.2959 (0.2211)	CeLoss 0.0204 (0.0439)	SegCLSLoss 0.0137 (0.0083)	KLLoss 0.0227 (0.0158)	MaskLoss 0.1286 (0.0826)	MaskBCELoss 0.0378 (0.0182)	MaskDICELoss 0.0908 (0.0644)
Epoch: [0][371/500]	Time  5.975 ( 5.975)	Loss 0.2634 (0.2169)	CeLoss 0.0173 (0.0402)	SegCLSLoss 0.0177 (0.0066)	KLLoss 0.0280 (0.0142)	MaskLoss 0.1116 (0.0831)	MaskBCELoss 0.0157 (0.0203)	MaskDICELoss 0.0958 (0.0628)
Epoch: [0][372/500]	Time  8.321 ( 8.321)	Loss 0.0416 (0.2214)	CeLoss 0.0415 (0.0276)	SegCLSLoss 0.0000 (0.0061)	KLLoss 0.0000 (0.0147)	MaskLoss 0.0000 (0.0917)	MaskBCELoss 0.0000 (0.0185)	MaskDICELoss 0.0000 (0.0731)
Epoch: [0][373/500]	Time  7.884 ( 7.884)	Loss 0.2888 (0.2267)	CeLoss 0.0212 (0.0263)	SegCLSLoss 0.0061 (0.0071)	KLLoss 0.0170 (0.0159)	MaskLoss 0.1280 (0.0944)	MaskBCELoss 0.0441 (0.0235)	MaskDICELoss 0.0839 (0.0709)
Epoch: [0][374/500]	Time  7.207 ( 7.207)	Loss 0.2487 (0.2661)	CeLoss 0.0222 (0.0257)	SegCLSLoss 0.0085 (0.0112)	KLLoss 0.0211 (0.0199)	MaskLoss 0.1059 (0.1124)	MaskBCELoss 0.0080 (0.0283)	MaskDICELoss 0.0979 (0.0842)
Epoch: [0][375/500]	Time  5.983 ( 5.983)	Loss 0.2892 (0.2013)	CeLoss 0.0306 (0.0475)	SegCLSLoss 0.0053 (0.0071)	KLLoss 0.0111 (0.0128)	MaskLoss 0.1252 (0.0719)	MaskBCELoss 0.0484 (0.0176)	MaskDICELoss 0.0768 (0.0543)
Epoch: [0][376/500]	Time  6.530 ( 6.530)	Loss 0.2680 (0.2143)	CeLoss 0.0280 (0.0459)	SegCLSLoss 0.0058 (0.0076)	KLLoss 0.0145 (0.0153)	MaskLoss 0.1150 (0.0785)	MaskBCELoss 0.0283 (0.0138)	MaskDICELoss 0.0867 (0.0647)
Epoch: [0][377/500]	Time  7.848 ( 7.848)	Loss 0.2454 (0.2251)	CeLoss 0.0225 (0.0348)	SegCLSLoss 0.0098 (0.0079)	KLLoss 0.0198 (0.0154)	MaskLoss 0.1040 (0.0893)	MaskBCELoss 0.0091 (0.0166)	MaskDICELoss 0.0950 (0.0728)
Epoch: [0][378/500]	Time  7.324 ( 7.324)	Loss 0.1258 (0.2518)	CeLoss 0.1260 (0.0335)	SegCLSLoss 0.0000 (0.0084)	KLLoss 0.0000 (0.0180)	MaskLoss 0.0000 (0.1025)	MaskBCELoss 0.0000 (0.0219)	MaskDICELoss 0.0000 (0.0806)
Epoch: [0][379/500]	Time  7.005 ( 7.005)	Loss 0.1766 (0.1985)	CeLoss 0.1768 (0.0462)	SegCLSLoss 0.0000 (0.0048)	KLLoss 0.0000 (0.0120)	MaskLoss 0.0000 (0.0720)	MaskBCELoss 0.0000 (0.0165)	MaskDICELoss 0.0000 (0.0555)
Epoch: [0][380/500]	Time  6.505 ( 6.505)	Loss 0.0953 (0.2062)	CeLoss 0.0952 (0.0577)	SegCLSLoss 0.0000 (0.0077)	KLLoss 0.0000 (0.0140)	MaskLoss 0.0000 (0.0689)	MaskBCELoss 0.0000 (0.0139)	MaskDICELoss 0.0000 (0.0550)
Epoch: [0][381/500]	Time  7.467 ( 7.467)	Loss 0.2768 (0.2780)	CeLoss 0.0251 (0.0383)	SegCLSLoss 0.0107 (0.0075)	KLLoss 0.0237 (0.0176)	MaskLoss 0.1172 (0.1136)	MaskBCELoss 0.0257 (0.0378)	MaskDICELoss 0.0915 (0.0758)
Epoch: [0][382/500]	Time  7.362 ( 7.362)	Loss 0.2574 (0.2217)	CeLoss 0.0233 (0.0455)	SegCLSLoss 0.0090 (0.0094)	KLLoss 0.0203 (0.0160)	MaskLoss 0.1097 (0.0818)	MaskBCELoss 0.0163 (0.0175)	MaskDICELoss 0.0934 (0.0643)
Epoch: [0][383/500]	Time  7.394 ( 7.394)	Loss 0.2684 (0.2347)	CeLoss 0.0247 (0.0372)	SegCLSLoss 0.0058 (0.0062)	KLLoss 0.0153 (0.0141)	MaskLoss 0.1166 (0.0937)	MaskBCELoss 0.0294 (0.0217)	MaskDICELoss 0.0872 (0.0720)
Epoch: [0][384/500]	Time  7.160 ( 7.160)	Loss 0.3190 (0.2409)	CeLoss 0.0217 (0.0463)	SegCLSLoss 0.0092 (0.0058)	KLLoss 0.0183 (0.0143)	MaskLoss 0.1417 (0.0922)	MaskBCELoss 0.0524 (0.0193)	MaskDICELoss 0.0893 (0.0729)
Epoch: [0][385/500]	Time  7.300 ( 7.300)	Loss 0.2844 (0.2400)	CeLoss 0.0299 (0.0434)	SegCLSLoss 0.0057 (0.0083)	KLLoss 0.0146 (0.0173)	MaskLoss 0.1221 (0.0919)	MaskBCELoss 0.0324 (0.0184)	MaskDICELoss 0.0896 (0.0735)
Epoch: [0][386/500]	Time  8.156 ( 8.156)	Loss 0.2573 (0.2454)	CeLoss 0.0221 (0.0248)	SegCLSLoss 0.0067 (0.0077)	KLLoss 0.0148 (0.0165)	MaskLoss 0.1122 (0.1043)	MaskBCELoss 0.0239 (0.0235)	MaskDICELoss 0.0883 (0.0808)
Epoch: [0][387/500]	Time  5.998 ( 5.998)	Loss 0.2655 (0.2213)	CeLoss 0.0322 (0.0487)	SegCLSLoss 0.0096 (0.0066)	KLLoss 0.0237 (0.0149)	MaskLoss 0.1083 (0.0809)	MaskBCELoss 0.0085 (0.0143)	MaskDICELoss 0.0998 (0.0666)
Epoch: [0][388/500]	Time  7.365 ( 7.365)	Loss 0.2598 (0.2145)	CeLoss 0.0260 (0.0390)	SegCLSLoss 0.0084 (0.0054)	KLLoss 0.0192 (0.0115)	MaskLoss 0.1100 (0.0835)	MaskBCELoss 0.0102 (0.0230)	MaskDICELoss 0.0997 (0.0606)
Epoch: [0][389/500]	Time  8.505 ( 8.505)	Loss 0.2762 (0.2622)	CeLoss 0.0297 (0.0237)	SegCLSLoss 0.0057 (0.0093)	KLLoss 0.0177 (0.0190)	MaskLoss 0.1174 (0.1122)	MaskBCELoss 0.0255 (0.0204)	MaskDICELoss 0.0919 (0.0918)
Epoch: [0][390/500]	Time  8.094 ( 8.094)	Loss 0.1516 (0.2351)	CeLoss 0.1514 (0.0439)	SegCLSLoss 0.0000 (0.0074)	KLLoss 0.0000 (0.0160)	MaskLoss 0.0000 (0.0897)	MaskBCELoss 0.0000 (0.0156)	MaskDICELoss 0.0000 (0.0742)
Epoch: [0][391/500]	Time  6.269 ( 6.269)	Loss 0.1328 (0.2306)	CeLoss 0.1328 (0.0507)	SegCLSLoss 0.0000 (0.0089)	KLLoss 0.0000 (0.0150)	MaskLoss 0.0000 (0.0840)	MaskBCELoss 0.0000 (0.0213)	MaskDICELoss 0.0000 (0.0626)
Epoch: [0][392/500]	Time  7.549 ( 7.549)	Loss 0.2616 (0.2383)	CeLoss 0.0306 (0.0328)	SegCLSLoss 0.0058 (0.0092)	KLLoss 0.0140 (0.0163)	MaskLoss 0.1105 (0.0964)	MaskBCELoss 0.0179 (0.0262)	MaskDICELoss 0.0926 (0.0701)
Epoch: [0][393/500]	Time  6.977 ( 6.977)	Loss 0.2519 (0.2211)	CeLoss 0.0133 (0.0441)	SegCLSLoss 0.0168 (0.0065)	KLLoss 0.0260 (0.0135)	MaskLoss 0.1085 (0.0835)	MaskBCELoss 0.0103 (0.0187)	MaskDICELoss 0.0983 (0.0648)
Epoch: [0][394/500]	Time  7.861 ( 7.861)	Loss 0.2619 (0.2679)	CeLoss 0.0291 (0.0278)	SegCLSLoss 0.0062 (0.0093)	KLLoss 0.0135 (0.0194)	MaskLoss 0.1114 (0.1129)	MaskBCELoss 0.0182 (0.0211)	MaskDICELoss 0.0932 (0.0918)
Epoch: [0][395/500]	Time  7.123 ( 7.123)	Loss 0.2769 (0.2465)	CeLoss 0.0248 (0.0418)	SegCLSLoss 0.0057 (0.0061)	KLLoss 0.0127 (0.0134)	MaskLoss 0.1215 (0.0975)	MaskBCELoss 0.0376 (0.0287)	MaskDICELoss 0.0838 (0.0688)
Epoch: [0][396/500]	Time  7.160 ( 7.160)	Loss 0.3133 (0.2145)	CeLoss 0.0211 (0.0367)	SegCLSLoss 0.0052 (0.0080)	KLLoss 0.0112 (0.0143)	MaskLoss 0.1420 (0.0833)	MaskBCELoss 0.0801 (0.0212)	MaskDICELoss 0.0619 (0.0621)
Epoch: [0][397/500]	Time  7.536 ( 7.536)	Loss 0.2762 (0.2123)	CeLoss 0.0271 (0.0431)	SegCLSLoss 0.0058 (0.0060)	KLLoss 0.0134 (0.0132)	MaskLoss 0.1198 (0.0798)	MaskBCELoss 0.0274 (0.0177)	MaskDICELoss 0.0923 (0.0621)
Epoch: [0][398/500]	Time  6.744 ( 6.744)	Loss 0.2517 (0.2577)	CeLoss 0.0226 (0.0352)	SegCLSLoss 0.0076 (0.0084)	KLLoss 0.0175 (0.0176)	MaskLoss 0.1083 (0.1047)	MaskBCELoss 0.0100 (0.0232)	MaskDICELoss 0.0983 (0.0815)
Epoch: [0][399/500]	Time  7.136 ( 7.136)	Loss 0.2679 (0.2153)	CeLoss 0.0236 (0.0470)	SegCLSLoss 0.0098 (0.0058)	KLLoss 0.0203 (0.0115)	MaskLoss 0.1146 (0.0798)	MaskBCELoss 0.0305 (0.0166)	MaskDICELoss 0.0841 (0.0632)
[2025-03-06 02:18:18,881] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[0.00029593975903614454], mom=[(0.9, 0.95)]
[2025-03-06 02:18:18,886] [INFO] [timer.py:215:stop] epoch=0/micro_step=400/global_step=40, RunningAvgSamplesPerSec=1.3773595716379643, CurrSamplesPerSec=1.4501540823978403, MemAllocated=57.26GB, MaxMemAllocated=62.82GB
Epoch: [0][400/500]	Time  6.169 ( 6.169)	Loss 0.3027 (0.2579)	CeLoss 0.0339 (0.0384)	SegCLSLoss 0.0063 (0.0113)	KLLoss 0.0164 (0.0173)	MaskLoss 0.1287 (0.1027)	MaskBCELoss 0.0368 (0.0319)	MaskDICELoss 0.0919 (0.0707)
Epoch: [0][401/500]	Time  6.829 ( 6.829)	Loss 0.2516 (0.2217)	CeLoss 0.0247 (0.0574)	SegCLSLoss 0.0066 (0.0052)	KLLoss 0.0176 (0.0121)	MaskLoss 0.1074 (0.0779)	MaskBCELoss 0.0143 (0.0129)	MaskDICELoss 0.0932 (0.0649)
Epoch: [0][402/500]	Time  5.894 ( 5.894)	Loss 0.2699 (0.2389)	CeLoss 0.0150 (0.0367)	SegCLSLoss 0.0168 (0.0102)	KLLoss 0.0258 (0.0169)	MaskLoss 0.1168 (0.0943)	MaskBCELoss 0.0252 (0.0235)	MaskDICELoss 0.0916 (0.0707)
Epoch: [0][403/500]	Time  6.090 ( 6.090)	Loss 0.2709 (0.1799)	CeLoss 0.0248 (0.0602)	SegCLSLoss 0.0063 (0.0036)	KLLoss 0.0148 (0.0081)	MaskLoss 0.1178 (0.0569)	MaskBCELoss 0.0365 (0.0107)	MaskDICELoss 0.0813 (0.0462)
Epoch: [0][404/500]	Time  7.094 ( 7.094)	Loss 0.3018 (0.2439)	CeLoss 0.0258 (0.0495)	SegCLSLoss 0.0087 (0.0056)	KLLoss 0.0217 (0.0142)	MaskLoss 0.1304 (0.0923)	MaskBCELoss 0.0467 (0.0223)	MaskDICELoss 0.0837 (0.0699)
Epoch: [0][405/500]	Time  5.992 ( 5.992)	Loss 0.0938 (0.2076)	CeLoss 0.0938 (0.0516)	SegCLSLoss 0.0000 (0.0051)	KLLoss 0.0000 (0.0105)	MaskLoss 0.0000 (0.0741)	MaskBCELoss 0.0000 (0.0233)	MaskDICELoss 0.0000 (0.0508)
Epoch: [0][406/500]	Time  6.544 ( 6.544)	Loss 0.3172 (0.2458)	CeLoss 0.0226 (0.0364)	SegCLSLoss 0.0104 (0.0075)	KLLoss 0.0206 (0.0164)	MaskLoss 0.1396 (0.0987)	MaskBCELoss 0.0576 (0.0302)	MaskDICELoss 0.0820 (0.0685)
Epoch: [0][407/500]	Time  7.793 ( 7.793)	Loss 0.2715 (0.2534)	CeLoss 0.0247 (0.0291)	SegCLSLoss 0.0084 (0.0056)	KLLoss 0.0175 (0.0121)	MaskLoss 0.1169 (0.1077)	MaskBCELoss 0.0182 (0.0330)	MaskDICELoss 0.0987 (0.0747)
Epoch: [0][408/500]	Time  6.324 ( 6.324)	Loss 0.0151 (0.1982)	CeLoss 0.0151 (0.0479)	SegCLSLoss 0.0000 (0.0046)	KLLoss 0.0000 (0.0106)	MaskLoss 0.0000 (0.0714)	MaskBCELoss 0.0000 (0.0203)	MaskDICELoss 0.0000 (0.0511)
Epoch: [0][409/500]	Time  7.250 ( 7.250)	Loss 0.2681 (0.2500)	CeLoss 0.0286 (0.0287)	SegCLSLoss 0.0055 (0.0084)	KLLoss 0.0142 (0.0160)	MaskLoss 0.1149 (0.1045)	MaskBCELoss 0.0217 (0.0238)	MaskDICELoss 0.0932 (0.0808)
Epoch: [0][410/500]	Time  6.827 ( 6.827)	Loss 0.1445 (0.1981)	CeLoss 0.1445 (0.0562)	SegCLSLoss 0.0000 (0.0056)	KLLoss 0.0000 (0.0110)	MaskLoss 0.0000 (0.0668)	MaskBCELoss 0.0000 (0.0156)	MaskDICELoss 0.0000 (0.0512)
Epoch: [0][411/500]	Time  7.231 ( 7.231)	Loss 0.3104 (0.2454)	CeLoss 0.0435 (0.0420)	SegCLSLoss 0.0048 (0.0069)	KLLoss 0.0108 (0.0137)	MaskLoss 0.1296 (0.0966)	MaskBCELoss 0.0479 (0.0252)	MaskDICELoss 0.0818 (0.0714)
Epoch: [0][412/500]	Time  7.376 ( 7.376)	Loss 0.2667 (0.2065)	CeLoss 0.0239 (0.0373)	SegCLSLoss 0.0055 (0.0053)	KLLoss 0.0134 (0.0120)	MaskLoss 0.1167 (0.0803)	MaskBCELoss 0.0258 (0.0161)	MaskDICELoss 0.0909 (0.0642)
Epoch: [0][413/500]	Time  6.652 ( 6.652)	Loss 0.2543 (0.2210)	CeLoss 0.0221 (0.0554)	SegCLSLoss 0.0062 (0.0064)	KLLoss 0.0137 (0.0128)	MaskLoss 0.1112 (0.0779)	MaskBCELoss 0.0231 (0.0129)	MaskDICELoss 0.0880 (0.0650)
Epoch: [0][414/500]	Time  6.259 ( 6.259)	Loss 0.1430 (0.2293)	CeLoss 0.1426 (0.0422)	SegCLSLoss 0.0000 (0.0096)	KLLoss 0.0000 (0.0159)	MaskLoss 0.0000 (0.0871)	MaskBCELoss 0.0000 (0.0252)	MaskDICELoss 0.0000 (0.0620)
Epoch: [0][415/500]	Time  7.955 ( 7.955)	Loss 0.0264 (0.2200)	CeLoss 0.0264 (0.0258)	SegCLSLoss 0.0000 (0.0070)	KLLoss 0.0000 (0.0152)	MaskLoss 0.0000 (0.0916)	MaskBCELoss 0.0000 (0.0167)	MaskDICELoss 0.0000 (0.0749)
Epoch: [0][416/500]	Time  7.000 ( 7.000)	Loss 0.2663 (0.2691)	CeLoss 0.0320 (0.0268)	SegCLSLoss 0.0057 (0.0096)	KLLoss 0.0151 (0.0189)	MaskLoss 0.1120 (0.1140)	MaskBCELoss 0.0196 (0.0221)	MaskDICELoss 0.0924 (0.0919)
Epoch: [0][417/500]	Time  7.194 ( 7.194)	Loss 0.0984 (0.2100)	CeLoss 0.0986 (0.0483)	SegCLSLoss 0.0000 (0.0040)	KLLoss 0.0000 (0.0088)	MaskLoss 0.0000 (0.0776)	MaskBCELoss 0.0000 (0.0283)	MaskDICELoss 0.0000 (0.0493)
Epoch: [0][418/500]	Time  7.346 ( 7.346)	Loss 0.3246 (0.2619)	CeLoss 0.0267 (0.0315)	SegCLSLoss 0.0070 (0.0065)	KLLoss 0.0145 (0.0154)	MaskLoss 0.1436 (0.1097)	MaskBCELoss 0.0676 (0.0338)	MaskDICELoss 0.0759 (0.0759)
Epoch: [0][419/500]	Time  7.137 ( 7.137)	Loss 0.0220 (0.2441)	CeLoss 0.0220 (0.0302)	SegCLSLoss 0.0000 (0.0082)	KLLoss 0.0000 (0.0160)	MaskLoss 0.0000 (0.1009)	MaskBCELoss 0.0000 (0.0305)	MaskDICELoss 0.0000 (0.0705)
Epoch: [0][420/500]	Time  8.391 ( 8.391)	Loss 0.2769 (0.2581)	CeLoss 0.0228 (0.0333)	SegCLSLoss 0.0057 (0.0077)	KLLoss 0.0135 (0.0164)	MaskLoss 0.1222 (0.1064)	MaskBCELoss 0.0409 (0.0258)	MaskDICELoss 0.0813 (0.0806)
Epoch: [0][421/500]	Time  7.600 ( 7.600)	Loss 0.2466 (0.2112)	CeLoss 0.0197 (0.0407)	SegCLSLoss 0.0092 (0.0052)	KLLoss 0.0209 (0.0112)	MaskLoss 0.1059 (0.0811)	MaskBCELoss 0.0071 (0.0172)	MaskDICELoss 0.0988 (0.0639)
Epoch: [0][422/500]	Time  6.926 ( 6.926)	Loss 0.2839 (0.2522)	CeLoss 0.0250 (0.0432)	SegCLSLoss 0.0087 (0.0076)	KLLoss 0.0214 (0.0157)	MaskLoss 0.1219 (0.0987)	MaskBCELoss 0.0330 (0.0269)	MaskDICELoss 0.0889 (0.0717)
Epoch: [0][423/500]	Time  6.118 ( 6.118)	Loss 0.0699 (0.1790)	CeLoss 0.0698 (0.0641)	SegCLSLoss 0.0000 (0.0037)	KLLoss 0.0000 (0.0085)	MaskLoss 0.0000 (0.0544)	MaskBCELoss 0.0000 (0.0060)	MaskDICELoss 0.0000 (0.0485)
Epoch: [0][424/500]	Time  7.339 ( 7.339)	Loss 0.2819 (0.2572)	CeLoss 0.0250 (0.0332)	SegCLSLoss 0.0091 (0.0068)	KLLoss 0.0221 (0.0145)	MaskLoss 0.1206 (0.1066)	MaskBCELoss 0.0307 (0.0303)	MaskDICELoss 0.0900 (0.0763)
Epoch: [0][425/500]	Time  7.591 ( 7.591)	Loss 0.2773 (0.2528)	CeLoss 0.0248 (0.0233)	SegCLSLoss 0.0087 (0.0095)	KLLoss 0.0173 (0.0194)	MaskLoss 0.1198 (0.1075)	MaskBCELoss 0.0291 (0.0283)	MaskDICELoss 0.0907 (0.0792)
Epoch: [0][426/500]	Time  8.183 ( 8.183)	Loss 0.2403 (0.2647)	CeLoss 0.0219 (0.0348)	SegCLSLoss 0.0101 (0.0071)	KLLoss 0.0223 (0.0158)	MaskLoss 0.1011 (0.1092)	MaskBCELoss 0.0109 (0.0339)	MaskDICELoss 0.0902 (0.0754)
Epoch: [0][427/500]	Time  7.323 ( 7.323)	Loss 0.2762 (0.2524)	CeLoss 0.0277 (0.0371)	SegCLSLoss 0.0058 (0.0068)	KLLoss 0.0154 (0.0140)	MaskLoss 0.1190 (0.1024)	MaskBCELoss 0.0415 (0.0330)	MaskDICELoss 0.0774 (0.0694)
Epoch: [0][428/500]	Time  6.222 ( 6.222)	Loss 0.2569 (0.1989)	CeLoss 0.0339 (0.0522)	SegCLSLoss 0.0049 (0.0035)	KLLoss 0.0108 (0.0077)	MaskLoss 0.1075 (0.0706)	MaskBCELoss 0.0172 (0.0156)	MaskDICELoss 0.0902 (0.0550)
Epoch: [0][429/500]	Time  6.704 ( 6.704)	Loss 0.1070 (0.2059)	CeLoss 0.1069 (0.0442)	SegCLSLoss 0.0000 (0.0059)	KLLoss 0.0000 (0.0129)	MaskLoss 0.0000 (0.0762)	MaskBCELoss 0.0000 (0.0102)	MaskDICELoss 0.0000 (0.0659)
Epoch: [0][430/500]	Time  6.790 ( 6.790)	Loss 0.2847 (0.2392)	CeLoss 0.0309 (0.0389)	SegCLSLoss 0.0051 (0.0070)	KLLoss 0.0128 (0.0144)	MaskLoss 0.1224 (0.0948)	MaskBCELoss 0.0443 (0.0246)	MaskDICELoss 0.0781 (0.0702)
Epoch: [0][431/500]	Time  7.210 ( 7.210)	Loss 0.3158 (0.2447)	CeLoss 0.0260 (0.0439)	SegCLSLoss 0.0071 (0.0080)	KLLoss 0.0159 (0.0150)	MaskLoss 0.1392 (0.0947)	MaskBCELoss 0.0698 (0.0235)	MaskDICELoss 0.0694 (0.0712)
Epoch: [0][432/500]	Time  8.034 ( 8.034)	Loss 0.2741 (0.2542)	CeLoss 0.0366 (0.0348)	SegCLSLoss 0.0069 (0.0072)	KLLoss 0.0159 (0.0160)	MaskLoss 0.1130 (0.1039)	MaskBCELoss 0.0169 (0.0221)	MaskDICELoss 0.0961 (0.0817)
Epoch: [0][433/500]	Time  7.139 ( 7.139)	Loss 0.2748 (0.2476)	CeLoss 0.0203 (0.0508)	SegCLSLoss 0.0095 (0.0065)	KLLoss 0.0177 (0.0137)	MaskLoss 0.1205 (0.0933)	MaskBCELoss 0.0247 (0.0226)	MaskDICELoss 0.0958 (0.0707)
Epoch: [0][434/500]	Time  6.789 ( 6.789)	Loss 0.2545 (0.2328)	CeLoss 0.0250 (0.0286)	SegCLSLoss 0.0069 (0.0069)	KLLoss 0.0183 (0.0152)	MaskLoss 0.1084 (0.0966)	MaskBCELoss 0.0103 (0.0290)	MaskDICELoss 0.0981 (0.0676)
Epoch: [0][435/500]	Time  9.006 ( 9.006)	Loss 0.2478 (0.2376)	CeLoss 0.0204 (0.0253)	SegCLSLoss 0.0078 (0.0066)	KLLoss 0.0198 (0.0158)	MaskLoss 0.1068 (0.1005)	MaskBCELoss 0.0089 (0.0154)	MaskDICELoss 0.0979 (0.0851)
Epoch: [0][436/500]	Time  7.893 ( 7.893)	Loss 0.2886 (0.2020)	CeLoss 0.0248 (0.0433)	SegCLSLoss 0.0066 (0.0052)	KLLoss 0.0123 (0.0109)	MaskLoss 0.1271 (0.0753)	MaskBCELoss 0.0497 (0.0248)	MaskDICELoss 0.0774 (0.0505)
Epoch: [0][437/500]	Time  6.809 ( 6.809)	Loss 0.1375 (0.2031)	CeLoss 0.1377 (0.0622)	SegCLSLoss 0.0000 (0.0046)	KLLoss 0.0000 (0.0107)	MaskLoss 0.0000 (0.0666)	MaskBCELoss 0.0000 (0.0138)	MaskDICELoss 0.0000 (0.0528)
Epoch: [0][438/500]	Time  6.513 ( 6.513)	Loss 0.1289 (0.2064)	CeLoss 0.1289 (0.0552)	SegCLSLoss 0.0000 (0.0049)	KLLoss 0.0000 (0.0113)	MaskLoss 0.0000 (0.0716)	MaskBCELoss 0.0000 (0.0191)	MaskDICELoss 0.0000 (0.0525)
Epoch: [0][439/500]	Time  7.331 ( 7.331)	Loss 0.2461 (0.2385)	CeLoss 0.0242 (0.0436)	SegCLSLoss 0.0073 (0.0068)	KLLoss 0.0186 (0.0150)	MaskLoss 0.1045 (0.0920)	MaskBCELoss 0.0080 (0.0181)	MaskDICELoss 0.0965 (0.0740)
Epoch: [0][440/500]	Time  8.069 ( 8.069)	Loss 0.3004 (0.2489)	CeLoss 0.0405 (0.0278)	SegCLSLoss 0.0052 (0.0065)	KLLoss 0.0112 (0.0153)	MaskLoss 0.1258 (0.1051)	MaskBCELoss 0.0359 (0.0242)	MaskDICELoss 0.0899 (0.0808)
Epoch: [0][441/500]	Time  5.284 ( 5.284)	Loss 0.2419 (0.2290)	CeLoss 0.0215 (0.0543)	SegCLSLoss 0.0104 (0.0064)	KLLoss 0.0223 (0.0140)	MaskLoss 0.1020 (0.0822)	MaskBCELoss 0.0030 (0.0204)	MaskDICELoss 0.0991 (0.0618)
Epoch: [0][442/500]	Time  5.882 ( 5.882)	Loss 0.2833 (0.2182)	CeLoss 0.0176 (0.0545)	SegCLSLoss 0.0162 (0.0065)	KLLoss 0.0245 (0.0132)	MaskLoss 0.1226 (0.0769)	MaskBCELoss 0.0276 (0.0115)	MaskDICELoss 0.0950 (0.0654)
Epoch: [0][443/500]	Time  7.543 ( 7.543)	Loss 0.2854 (0.2626)	CeLoss 0.0281 (0.0357)	SegCLSLoss 0.0110 (0.0073)	KLLoss 0.0232 (0.0166)	MaskLoss 0.1201 (0.1074)	MaskBCELoss 0.0296 (0.0263)	MaskDICELoss 0.0904 (0.0812)
Epoch: [0][444/500]	Time  7.077 ( 7.077)	Loss 0.3110 (0.2120)	CeLoss 0.0150 (0.0523)	SegCLSLoss 0.0121 (0.0057)	KLLoss 0.0225 (0.0114)	MaskLoss 0.1394 (0.0756)	MaskBCELoss 0.0518 (0.0233)	MaskDICELoss 0.0876 (0.0523)
Epoch: [0][445/500]	Time  6.777 ( 6.777)	Loss 0.0641 (0.1901)	CeLoss 0.0640 (0.0697)	SegCLSLoss 0.0000 (0.0036)	KLLoss 0.0000 (0.0093)	MaskLoss 0.0000 (0.0570)	MaskBCELoss 0.0000 (0.0139)	MaskDICELoss 0.0000 (0.0430)
Epoch: [0][446/500]	Time  7.352 ( 7.352)	Loss 0.2470 (0.2430)	CeLoss 0.0184 (0.0297)	SegCLSLoss 0.0098 (0.0078)	KLLoss 0.0205 (0.0168)	MaskLoss 0.1067 (0.1005)	MaskBCELoss 0.0120 (0.0196)	MaskDICELoss 0.0947 (0.0809)
Epoch: [0][447/500]	Time  7.055 ( 7.055)	Loss 0.2784 (0.2500)	CeLoss 0.0148 (0.0312)	SegCLSLoss 0.0173 (0.0085)	KLLoss 0.0250 (0.0162)	MaskLoss 0.1212 (0.1032)	MaskBCELoss 0.0289 (0.0212)	MaskDICELoss 0.0924 (0.0820)
Epoch: [0][448/500]	Time  6.575 ( 6.575)	Loss 0.2514 (0.2333)	CeLoss 0.0175 (0.0371)	SegCLSLoss 0.0096 (0.0054)	KLLoss 0.0203 (0.0123)	MaskLoss 0.1095 (0.0937)	MaskBCELoss 0.0119 (0.0220)	MaskDICELoss 0.0975 (0.0716)
Epoch: [0][449/500]	Time  6.554 ( 6.554)	Loss 0.2750 (0.2317)	CeLoss 0.0200 (0.0503)	SegCLSLoss 0.0061 (0.0053)	KLLoss 0.0150 (0.0115)	MaskLoss 0.1223 (0.0865)	MaskBCELoss 0.0253 (0.0283)	MaskDICELoss 0.0970 (0.0583)
Epoch: [0][450/500]	Time  6.772 ( 6.772)	Loss 0.2396 (0.1906)	CeLoss 0.0187 (0.0624)	SegCLSLoss 0.0097 (0.0034)	KLLoss 0.0212 (0.0075)	MaskLoss 0.1027 (0.0614)	MaskBCELoss 0.0063 (0.0160)	MaskDICELoss 0.0964 (0.0454)
Epoch: [0][451/500]	Time  7.326 ( 7.326)	Loss 0.0109 (0.2209)	CeLoss 0.0109 (0.0465)	SegCLSLoss 0.0000 (0.0075)	KLLoss 0.0000 (0.0147)	MaskLoss 0.0000 (0.0816)	MaskBCELoss 0.0000 (0.0202)	MaskDICELoss 0.0000 (0.0615)
Epoch: [0][452/500]	Time  7.865 ( 7.865)	Loss 0.0551 (0.2235)	CeLoss 0.0552 (0.0365)	SegCLSLoss 0.0000 (0.0074)	KLLoss 0.0000 (0.0152)	MaskLoss 0.0000 (0.0878)	MaskBCELoss 0.0000 (0.0135)	MaskDICELoss 0.0000 (0.0744)
Epoch: [0][453/500]	Time  6.533 ( 6.533)	Loss 0.0535 (0.1539)	CeLoss 0.0535 (0.0585)	SegCLSLoss 0.0000 (0.0026)	KLLoss 0.0000 (0.0051)	MaskLoss 0.0000 (0.0458)	MaskBCELoss 0.0000 (0.0085)	MaskDICELoss 0.0000 (0.0373)
Epoch: [0][454/500]	Time  8.320 ( 8.320)	Loss 0.0131 (0.2244)	CeLoss 0.0131 (0.0371)	SegCLSLoss 0.0000 (0.0062)	KLLoss 0.0000 (0.0145)	MaskLoss 0.0000 (0.0885)	MaskBCELoss 0.0000 (0.0146)	MaskDICELoss 0.0000 (0.0739)
Epoch: [0][455/500]	Time  7.237 ( 7.237)	Loss 0.2975 (0.2042)	CeLoss 0.0352 (0.0461)	SegCLSLoss 0.0063 (0.0056)	KLLoss 0.0157 (0.0106)	MaskLoss 0.1257 (0.0750)	MaskBCELoss 0.0382 (0.0222)	MaskDICELoss 0.0876 (0.0528)
Epoch: [0][456/500]	Time  6.632 ( 6.632)	Loss 0.2546 (0.2765)	CeLoss 0.0289 (0.0250)	SegCLSLoss 0.0053 (0.0071)	KLLoss 0.0116 (0.0139)	MaskLoss 0.1085 (0.1205)	MaskBCELoss 0.0260 (0.0400)	MaskDICELoss 0.0825 (0.0804)
Epoch: [0][457/500]	Time  6.083 ( 6.083)	Loss 0.2282 (0.2133)	CeLoss 0.0215 (0.0356)	SegCLSLoss 0.0092 (0.0059)	KLLoss 0.0211 (0.0130)	MaskLoss 0.0958 (0.0841)	MaskBCELoss 0.0095 (0.0223)	MaskDICELoss 0.0863 (0.0618)
Epoch: [0][458/500]	Time  8.064 ( 8.064)	Loss 0.2467 (0.2453)	CeLoss 0.0193 (0.0288)	SegCLSLoss 0.0087 (0.0067)	KLLoss 0.0198 (0.0149)	MaskLoss 0.1066 (0.1028)	MaskBCELoss 0.0113 (0.0189)	MaskDICELoss 0.0953 (0.0839)
Epoch: [0][459/500]	Time  7.863 ( 7.863)	Loss 0.2458 (0.2410)	CeLoss 0.0193 (0.0342)	SegCLSLoss 0.0097 (0.0072)	KLLoss 0.0188 (0.0160)	MaskLoss 0.1061 (0.0976)	MaskBCELoss 0.0069 (0.0133)	MaskDICELoss 0.0992 (0.0843)
Epoch: [0][460/500]	Time  6.796 ( 6.796)	Loss 0.1398 (0.2391)	CeLoss 0.1396 (0.0472)	SegCLSLoss 0.0000 (0.0059)	KLLoss 0.0000 (0.0116)	MaskLoss 0.0000 (0.0916)	MaskBCELoss 0.0000 (0.0278)	MaskDICELoss 0.0000 (0.0638)
Epoch: [0][461/500]	Time  6.790 ( 6.790)	Loss 0.2732 (0.1659)	CeLoss 0.0181 (0.0455)	SegCLSLoss 0.0092 (0.0040)	KLLoss 0.0208 (0.0090)	MaskLoss 0.1201 (0.0569)	MaskBCELoss 0.0306 (0.0111)	MaskDICELoss 0.0895 (0.0458)
Epoch: [0][462/500]	Time  6.877 ( 6.877)	Loss 0.1031 (0.2390)	CeLoss 0.1030 (0.0410)	SegCLSLoss 0.0000 (0.0079)	KLLoss 0.0000 (0.0156)	MaskLoss 0.0000 (0.0931)	MaskBCELoss 0.0000 (0.0251)	MaskDICELoss 0.0000 (0.0680)
Epoch: [0][463/500]	Time  7.918 ( 7.918)	Loss 0.2389 (0.2558)	CeLoss 0.0238 (0.0382)	SegCLSLoss 0.0071 (0.0079)	KLLoss 0.0189 (0.0172)	MaskLoss 0.1011 (0.1026)	MaskBCELoss 0.0022 (0.0219)	MaskDICELoss 0.0988 (0.0807)
Epoch: [0][464/500]	Time  6.332 ( 6.332)	Loss 0.2965 (0.2436)	CeLoss 0.0327 (0.0582)	SegCLSLoss 0.0051 (0.0061)	KLLoss 0.0117 (0.0117)	MaskLoss 0.1277 (0.0883)	MaskBCELoss 0.0437 (0.0256)	MaskDICELoss 0.0840 (0.0626)
Epoch: [0][465/500]	Time  6.888 ( 6.888)	Loss 0.2656 (0.2366)	CeLoss 0.0261 (0.0316)	SegCLSLoss 0.0059 (0.0087)	KLLoss 0.0148 (0.0141)	MaskLoss 0.1145 (0.0968)	MaskBCELoss 0.0389 (0.0296)	MaskDICELoss 0.0756 (0.0672)
Epoch: [0][466/500]	Time  6.909 ( 6.909)	Loss 0.2407 (0.2251)	CeLoss 0.0217 (0.0363)	SegCLSLoss 0.0099 (0.0069)	KLLoss 0.0211 (0.0142)	MaskLoss 0.1018 (0.0891)	MaskBCELoss 0.0031 (0.0181)	MaskDICELoss 0.0987 (0.0711)
Epoch: [0][467/500]	Time  7.987 ( 7.987)	Loss 0.0895 (0.2232)	CeLoss 0.0894 (0.0381)	SegCLSLoss 0.0000 (0.0062)	KLLoss 0.0000 (0.0144)	MaskLoss 0.0000 (0.0874)	MaskBCELoss 0.0000 (0.0148)	MaskDICELoss 0.0000 (0.0726)
Epoch: [0][468/500]	Time  6.828 ( 6.828)	Loss 0.2606 (0.2240)	CeLoss 0.0289 (0.0570)	SegCLSLoss 0.0054 (0.0041)	KLLoss 0.0143 (0.0102)	MaskLoss 0.1110 (0.0799)	MaskBCELoss 0.0331 (0.0160)	MaskDICELoss 0.0779 (0.0639)
Epoch: [0][469/500]	Time  7.933 ( 7.933)	Loss 0.2364 (0.2451)	CeLoss 0.0210 (0.0260)	SegCLSLoss 0.0092 (0.0083)	KLLoss 0.0217 (0.0172)	MaskLoss 0.1000 (0.1032)	MaskBCELoss 0.0030 (0.0242)	MaskDICELoss 0.0970 (0.0790)
Epoch: [0][470/500]	Time  7.946 ( 7.946)	Loss 0.2519 (0.2474)	CeLoss 0.0189 (0.0380)	SegCLSLoss 0.0107 (0.0058)	KLLoss 0.0248 (0.0125)	MaskLoss 0.1076 (0.1001)	MaskBCELoss 0.0091 (0.0187)	MaskDICELoss 0.0985 (0.0814)
Epoch: [0][471/500]	Time  7.552 ( 7.552)	Loss 0.2736 (0.2601)	CeLoss 0.0205 (0.0296)	SegCLSLoss 0.0079 (0.0090)	KLLoss 0.0173 (0.0166)	MaskLoss 0.1203 (0.1089)	MaskBCELoss 0.0257 (0.0273)	MaskDICELoss 0.0946 (0.0816)
Epoch: [0][472/500]	Time  7.987 ( 7.987)	Loss 0.2383 (0.2558)	CeLoss 0.0203 (0.0253)	SegCLSLoss 0.0059 (0.0057)	KLLoss 0.0123 (0.0119)	MaskLoss 0.1045 (0.1109)	MaskBCELoss 0.0152 (0.0381)	MaskDICELoss 0.0893 (0.0728)
Epoch: [0][473/500]	Time  6.869 ( 6.869)	Loss 0.0977 (0.1900)	CeLoss 0.0977 (0.0410)	SegCLSLoss 0.0000 (0.0045)	KLLoss 0.0000 (0.0096)	MaskLoss 0.0000 (0.0709)	MaskBCELoss 0.0000 (0.0173)	MaskDICELoss 0.0000 (0.0536)
Epoch: [0][474/500]	Time  8.033 ( 8.033)	Loss 0.3142 (0.2404)	CeLoss 0.0237 (0.0224)	SegCLSLoss 0.0059 (0.0072)	KLLoss 0.0167 (0.0159)	MaskLoss 0.1395 (0.1032)	MaskBCELoss 0.0539 (0.0192)	MaskDICELoss 0.0856 (0.0840)
Epoch: [0][475/500]	Time  5.703 ( 5.703)	Loss 0.0208 (0.1803)	CeLoss 0.0208 (0.0338)	SegCLSLoss 0.0000 (0.0047)	KLLoss 0.0000 (0.0095)	MaskLoss 0.0000 (0.0697)	MaskBCELoss 0.0000 (0.0152)	MaskDICELoss 0.0000 (0.0545)
Epoch: [0][476/500]	Time  6.334 ( 6.334)	Loss 0.0742 (0.2261)	CeLoss 0.0742 (0.0463)	SegCLSLoss 0.0000 (0.0060)	KLLoss 0.0000 (0.0112)	MaskLoss 0.0000 (0.0856)	MaskBCELoss 0.0000 (0.0246)	MaskDICELoss 0.0000 (0.0610)
Epoch: [0][477/500]	Time  8.007 ( 8.007)	Loss 0.0598 (0.2208)	CeLoss 0.0598 (0.0356)	SegCLSLoss 0.0000 (0.0063)	KLLoss 0.0000 (0.0134)	MaskLoss 0.0000 (0.0877)	MaskBCELoss 0.0000 (0.0127)	MaskDICELoss 0.0000 (0.0750)
Epoch: [0][478/500]	Time  7.285 ( 7.285)	Loss 0.0109 (0.1831)	CeLoss 0.0109 (0.0415)	SegCLSLoss 0.0000 (0.0041)	KLLoss 0.0000 (0.0090)	MaskLoss 0.0000 (0.0675)	MaskBCELoss 0.0000 (0.0145)	MaskDICELoss 0.0000 (0.0531)
Epoch: [0][479/500]	Time  6.840 ( 6.840)	Loss 0.2901 (0.2355)	CeLoss 0.0239 (0.0365)	SegCLSLoss 0.0091 (0.0061)	KLLoss 0.0205 (0.0129)	MaskLoss 0.1257 (0.0948)	MaskBCELoss 0.0358 (0.0244)	MaskDICELoss 0.0899 (0.0703)
Epoch: [0][480/500]	Time  6.726 ( 6.726)	Loss 0.3224 (0.1912)	CeLoss 0.0227 (0.0373)	SegCLSLoss 0.0072 (0.0046)	KLLoss 0.0178 (0.0097)	MaskLoss 0.1436 (0.0734)	MaskBCELoss 0.0714 (0.0220)	MaskDICELoss 0.0722 (0.0514)
Epoch: [0][481/500]	Time  7.925 ( 7.925)	Loss 0.2753 (0.2620)	CeLoss 0.0148 (0.0365)	SegCLSLoss 0.0173 (0.0076)	KLLoss 0.0226 (0.0140)	MaskLoss 0.1202 (0.1074)	MaskBCELoss 0.0292 (0.0313)	MaskDICELoss 0.0911 (0.0761)
Epoch: [0][482/500]	Time  7.692 ( 7.692)	Loss 0.2492 (0.2547)	CeLoss 0.0254 (0.0293)	SegCLSLoss 0.0073 (0.0066)	KLLoss 0.0162 (0.0150)	MaskLoss 0.1061 (0.1073)	MaskBCELoss 0.0087 (0.0292)	MaskDICELoss 0.0974 (0.0780)
Epoch: [0][483/500]	Time  7.744 ( 7.744)	Loss 0.3061 (0.2341)	CeLoss 0.0247 (0.0378)	SegCLSLoss 0.0078 (0.0059)	KLLoss 0.0143 (0.0131)	MaskLoss 0.1352 (0.0934)	MaskBCELoss 0.0395 (0.0208)	MaskDICELoss 0.0957 (0.0726)
Epoch: [0][484/500]	Time  6.991 ( 6.991)	Loss 0.2838 (0.2422)	CeLoss 0.0271 (0.0404)	SegCLSLoss 0.0071 (0.0057)	KLLoss 0.0123 (0.0114)	MaskLoss 0.1236 (0.0967)	MaskBCELoss 0.0397 (0.0262)	MaskDICELoss 0.0838 (0.0705)
Epoch: [0][485/500]	Time  6.684 ( 6.684)	Loss 0.2555 (0.1942)	CeLoss 0.0276 (0.0578)	SegCLSLoss 0.0063 (0.0046)	KLLoss 0.0148 (0.0101)	MaskLoss 0.1087 (0.0646)	MaskBCELoss 0.0192 (0.0089)	MaskDICELoss 0.0895 (0.0557)
Epoch: [0][486/500]	Time  8.432 ( 8.432)	Loss 0.2487 (0.2640)	CeLoss 0.0254 (0.0281)	SegCLSLoss 0.0063 (0.0069)	KLLoss 0.0195 (0.0150)	MaskLoss 0.1052 (0.1125)	MaskBCELoss 0.0074 (0.0224)	MaskDICELoss 0.0978 (0.0901)
Epoch: [0][487/500]	Time  6.629 ( 6.629)	Loss 0.2652 (0.1761)	CeLoss 0.0232 (0.0504)	SegCLSLoss 0.0052 (0.0049)	KLLoss 0.0096 (0.0088)	MaskLoss 0.1173 (0.0594)	MaskBCELoss 0.0283 (0.0171)	MaskDICELoss 0.0889 (0.0423)
Epoch: [0][488/500]	Time  6.322 ( 6.322)	Loss 0.2806 (0.2110)	CeLoss 0.0232 (0.0316)	SegCLSLoss 0.0092 (0.0059)	KLLoss 0.0182 (0.0117)	MaskLoss 0.1219 (0.0853)	MaskBCELoss 0.0331 (0.0275)	MaskDICELoss 0.0889 (0.0578)
Epoch: [0][489/500]	Time  5.927 ( 5.927)	Loss 0.2679 (0.2164)	CeLoss 0.0200 (0.0685)	SegCLSLoss 0.0087 (0.0054)	KLLoss 0.0208 (0.0101)	MaskLoss 0.1166 (0.0700)	MaskBCELoss 0.0231 (0.0161)	MaskDICELoss 0.0935 (0.0539)
Epoch: [0][490/500]	Time  6.866 ( 6.866)	Loss 0.1289 (0.2036)	CeLoss 0.1289 (0.0489)	SegCLSLoss 0.0000 (0.0050)	KLLoss 0.0000 (0.0111)	MaskLoss 0.0000 (0.0733)	MaskBCELoss 0.0000 (0.0216)	MaskDICELoss 0.0000 (0.0517)
Epoch: [0][491/500]	Time  6.583 ( 6.583)	Loss 0.3065 (0.2162)	CeLoss 0.0376 (0.0677)	SegCLSLoss 0.0046 (0.0052)	KLLoss 0.0077 (0.0097)	MaskLoss 0.1313 (0.0705)	MaskBCELoss 0.0653 (0.0181)	MaskDICELoss 0.0660 (0.0524)
Epoch: [0][492/500]	Time  7.145 ( 7.145)	Loss 0.2627 (0.2437)	CeLoss 0.0254 (0.0293)	SegCLSLoss 0.0059 (0.0077)	KLLoss 0.0161 (0.0154)	MaskLoss 0.1132 (0.1014)	MaskBCELoss 0.0197 (0.0196)	MaskDICELoss 0.0935 (0.0819)
Epoch: [0][493/500]	Time  7.495 ( 7.495)	Loss 0.0402 (0.2442)	CeLoss 0.0403 (0.0246)	SegCLSLoss 0.0000 (0.0088)	KLLoss 0.0000 (0.0172)	MaskLoss 0.0000 (0.1033)	MaskBCELoss 0.0000 (0.0233)	MaskDICELoss 0.0000 (0.0801)
Epoch: [0][494/500]	Time  7.976 ( 7.976)	Loss 0.2429 (0.2273)	CeLoss 0.0159 (0.0304)	SegCLSLoss 0.0102 (0.0059)	KLLoss 0.0219 (0.0138)	MaskLoss 0.1056 (0.0935)	MaskBCELoss 0.0060 (0.0205)	MaskDICELoss 0.0995 (0.0730)
Epoch: [0][495/500]	Time  7.221 ( 7.221)	Loss 0.2562 (0.2431)	CeLoss 0.0356 (0.0457)	SegCLSLoss 0.0056 (0.0052)	KLLoss 0.0120 (0.0114)	MaskLoss 0.1059 (0.0946)	MaskBCELoss 0.0186 (0.0243)	MaskDICELoss 0.0874 (0.0702)
Epoch: [0][496/500]	Time  6.543 ( 6.543)	Loss 0.0934 (0.2212)	CeLoss 0.0933 (0.0464)	SegCLSLoss 0.0000 (0.0046)	KLLoss 0.0000 (0.0099)	MaskLoss 0.0000 (0.0838)	MaskBCELoss 0.0000 (0.0240)	MaskDICELoss 0.0000 (0.0597)
Epoch: [0][497/500]	Time  6.376 ( 6.376)	Loss 0.3067 (0.1748)	CeLoss 0.0153 (0.0513)	SegCLSLoss 0.0119 (0.0045)	KLLoss 0.0206 (0.0094)	MaskLoss 0.1375 (0.0583)	MaskBCELoss 0.0488 (0.0133)	MaskDICELoss 0.0888 (0.0450)
Epoch: [0][498/500]	Time  7.296 ( 7.296)	Loss 0.2854 (0.1991)	CeLoss 0.0229 (0.0504)	SegCLSLoss 0.0049 (0.0040)	KLLoss 0.0074 (0.0084)	MaskLoss 0.1282 (0.0712)	MaskBCELoss 0.0474 (0.0169)	MaskDICELoss 0.0807 (0.0544)
Epoch: [0][499/500]	Time  7.459 ( 7.459)	Loss 0.2386 (0.2453)	CeLoss 0.0221 (0.0281)	SegCLSLoss 0.0062 (0.0076)	KLLoss 0.0123 (0.0154)	MaskLoss 0.1036 (0.1028)	MaskBCELoss 0.0053 (0.0190)	MaskDICELoss 0.0983 (0.0838)
[2025-03-06 02:30:11,189] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[0.00029461445783132527], mom=[(0.9, 0.95)]
[2025-03-06 02:30:11,194] [INFO] [timer.py:215:stop] epoch=0/micro_step=500/global_step=50, RunningAvgSamplesPerSec=1.3721946658661834, CurrSamplesPerSec=1.4634711249358905, MemAllocated=57.27GB, MaxMemAllocated=62.82GB
Epoch: [0][500/500]	Time  8.900 ( 8.900)	Loss 0.2466 (0.2774)	CeLoss 0.0165 (0.0237)	SegCLSLoss 0.0099 (0.0068)	KLLoss 0.0216 (0.0144)	MaskLoss 0.1072 (0.1215)	MaskBCELoss 0.0141 (0.0351)	MaskDICELoss 0.0931 (0.0863)
