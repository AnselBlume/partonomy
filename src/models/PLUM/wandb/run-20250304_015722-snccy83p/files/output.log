You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565



Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [05:11<00:00, 103.84s/it]
trainable params: 6,553,600 || all params: 14,025,701,683 || trainable%: 0.0467256480147682
n:  base_model.model.model.embed_tokens.weight p.shape:  torch.Size([32002, 5120])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_token.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.mask_decoder.mask_tokens.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.weight p.shape:  torch.Size([256, 64, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.weight p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.weight p.shape:  torch.Size([64, 32, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.bias p.shape:  torch.Size([4])
n:  base_model.model.model.text_hidden_fcs.0.0.weight p.shape:  torch.Size([5120, 5120])
n:  base_model.model.model.text_hidden_fcs.0.0.bias p.shape:  torch.Size([5120])
n:  base_model.model.model.text_hidden_fcs.0.2.weight p.shape:  torch.Size([256, 5120])
n:  base_model.model.model.text_hidden_fcs.0.2.bias p.shape:  torch.Size([256])
n:  base_model.model.lm_head.weight p.shape:  torch.Size([32002, 5120])
>> (PLUM.py) Initializing teacher LLM...
>> (PLUM.py) Teacher LLM initialized.
ade20k:  20210
cocostuff:  118287
loading annotations into memory...
Done (t=0.58s)
creating index...
index created!
pascal_part:  4366
loading annotations into memory...
Done (t=8.18s)
creating index...
index created!
paco_lvis:  45790
mapillary:  18000
loading dataset refclef into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refclef/refs(unc).p
creating index...
index created.
DONE (t=4.21s)
dataset refclef (refs unc) (train split) has 17978 images and 99523 annotations.
loading dataset refcoco into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco/refs(unc).p
creating index...
index created.
DONE (t=5.90s)
dataset refcoco (refs unc) (train split) has 16994 images and 196771 annotations.
loading dataset refcoco+ into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco+/refs(unc).p
creating index...
index created.
DONE (t=10.50s)
dataset refcoco+ (refs unc) (train split) has 16992 images and 196737 annotations.
loading dataset refcocog into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcocog/refs(umd).p
creating index...
index created.
DONE (t=11.07s)
dataset refcocog (refs umd) (train split) has 21899 images and 208960 annotations.
vqa_data:  157712
number of reason_seg samples:  239
len(self.img_to_explanation):  239
Training with 5000 examples and validating with 200 examples.
[2025-03-04 02:05:05,514] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5, git-hash=unknown, git-branch=unknown
[2025-03-04 02:05:05,515] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2025-03-04 02:05:05,515] [INFO] [comm.py:594:init_distributed] cdb=None
[2025-03-04 02:05:05,515] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-03-04 02:05:39,288] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Time to load fused_adam op: 0.4844398498535156 seconds
[2025-03-04 02:05:39,976] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
Using /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module fused_adam...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/tensor/python_tensor.cpp:78.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
[2025-03-04 02:05:40,700] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-03-04 02:05:40,701] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-03-04 02:05:40,702] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-03-04 02:05:40,702] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500000000
[2025-03-04 02:05:40,702] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500000000
[2025-03-04 02:05:40,702] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2025-03-04 02:05:40,702] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(365842916, False)]
[2025-03-04 02:05:47,126] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2025-03-04 02:05:47,127] [INFO] [utils.py:786:see_memory_usage] MA 53.71 GB         Max_MA 54.39 GB         CA 54.6 GB         Max_CA 55 GB
[2025-03-04 02:05:47,127] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 79.02 GB, percent = 7.8%
[2025-03-04 02:05:50,763] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2025-03-04 02:05:50,765] [INFO] [utils.py:786:see_memory_usage] MA 56.44 GB         Max_MA 57.8 GB         CA 58.69 GB         Max_CA 59 GB
[2025-03-04 02:05:50,766] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 80.15 GB, percent = 8.0%
[2025-03-04 02:05:50,766] [INFO] [stage_1_and_2.py:488:__init__] optimizer state initialized
[2025-03-04 02:05:53,973] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2025-03-04 02:05:53,974] [INFO] [utils.py:786:see_memory_usage] MA 56.44 GB         Max_MA 56.44 GB         CA 58.69 GB         Max_CA 59 GB
[2025-03-04 02:05:53,974] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 80.45 GB, percent = 8.0%
[2025-03-04 02:05:53,981] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-03-04 02:05:53,981] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2025-03-04 02:05:53,981] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7f5a1d1ca020>
[2025-03-04 02:05:53,981] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
[2025-03-04 02:05:53,988] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2025-03-04 02:05:53,988] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-03-04 02:05:53,989] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-03-04 02:05:53,989] [INFO] [config.py:964:print]   amp_enabled .................. False
[2025-03-04 02:05:53,989] [INFO] [config.py:964:print]   amp_params ................... False
[2025-03-04 02:05:53,989] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-04 02:05:53,989] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2025-03-04 02:05:53,989] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2025-03-04 02:05:53,989] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2025-03-04 02:05:53,989] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2025-03-04 02:05:53,989] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f441ed98820>
[2025-03-04 02:05:53,990] [INFO] [config.py:964:print]   communication_data_type ...... None
[2025-03-04 02:05:53,990] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-04 02:05:53,990] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2025-03-04 02:05:53,990] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2025-03-04 02:05:53,990] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-04 02:05:53,990] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2025-03-04 02:05:53,990] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2025-03-04 02:05:53,990] [INFO] [config.py:964:print]   disable_allgather ............ False
[2025-03-04 02:05:53,990] [INFO] [config.py:964:print]   dump_state ................... False
[2025-03-04 02:05:53,990] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2025-03-04 02:05:53,990] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2025-03-04 02:05:53,990] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-04 02:05:53,991] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-04 02:05:53,991] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2025-03-04 02:05:53,991] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2025-03-04 02:05:53,991] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2025-03-04 02:05:53,991] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2025-03-04 02:05:53,991] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2025-03-04 02:05:53,991] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2025-03-04 02:05:53,991] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-03-04 02:05:53,991] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2025-03-04 02:05:53,991] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2025-03-04 02:05:53,991] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2025-03-04 02:05:53,991] [INFO] [config.py:964:print]   global_rank .................. 0
[2025-03-04 02:05:53,992] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2025-03-04 02:05:53,992] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 10
[2025-03-04 02:05:53,992] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0
[2025-03-04 02:05:53,992] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2025-03-04 02:05:53,992] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-04 02:05:53,992] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2025-03-04 02:05:53,992] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2025-03-04 02:05:53,992] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2025-03-04 02:05:53,992] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2025-03-04 02:05:53,992] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2025-03-04 02:05:53,992] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2025-03-04 02:05:53,993] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-03-04 02:05:53,993] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-03-04 02:05:53,993] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2025-03-04 02:05:53,993] [INFO] [config.py:964:print]   optimizer_name ............... adamw
[2025-03-04 02:05:53,993] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.0003, 'weight_decay': 0.0, 'betas': (0.9, 0.95)}
[2025-03-04 02:05:53,993] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2025-03-04 02:05:53,993] [INFO] [config.py:964:print]   pld_enabled .................. False
[2025-03-04 02:05:53,993] [INFO] [config.py:964:print]   pld_params ................... False
[2025-03-04 02:05:53,993] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2025-03-04 02:05:53,993] [INFO] [config.py:964:print]   scheduler_name ............... WarmupDecayLR
[2025-03-04 02:05:53,994] [INFO] [config.py:964:print]   scheduler_params ............. {'total_num_steps': 25000, 'warmup_min_lr': 0, 'warmup_max_lr': 0.0003, 'warmup_num_steps': 100, 'warmup_type': 'linear'}
[2025-03-04 02:05:53,994] [INFO] [config.py:964:print]   sparse_attention ............. None
[2025-03-04 02:05:53,994] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2025-03-04 02:05:53,994] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2025-03-04 02:05:53,994] [INFO] [config.py:964:print]   train_batch_size ............. 10
[2025-03-04 02:05:53,994] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2025-03-04 02:05:53,994] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2025-03-04 02:05:53,994] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2025-03-04 02:05:53,994] [INFO] [config.py:964:print]   world_size ................... 1
[2025-03-04 02:05:53,994] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2025-03-04 02:05:53,994] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2025-03-04 02:05:53,994] [INFO] [config.py:964:print]   zero_enabled ................. True
[2025-03-04 02:05:53,995] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-04 02:05:53,995] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2025-03-04 02:05:53,995] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1,
    "gradient_accumulation_steps": 10,
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 0.0003,
            "weight_decay": 0.0,
            "betas": [0.9, 0.95]
        }
    },
    "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "total_num_steps": 2.500000e+04,
            "warmup_min_lr": 0,
            "warmup_max_lr": 0.0003,
            "warmup_num_steps": 100,
            "warmup_type": "linear"
        }
    },
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "gradient_clipping": 1.0,
    "zero_optimization": {
        "stage": 2,
        "contiguous_gradients": true,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 5.000000e+08,
        "allgather_bucket_size": 5.000000e+08
    }
}
(train) >> AFTER DEEPSPEED
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)
  return F.conv2d(input, weight, bias, self.stride,
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([2, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([5, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([16, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([15, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([21, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([11, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([10, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([9, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([1, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([4, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([3, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][  1/500]	Time 14.310 (14.310)	Loss 12.4099 (14.8512)	CeLoss 3.5625 (2.9227)	SegCLSLoss 1.1250 (0.9039)	KLLoss 0.0986 (0.0894)	MaskLoss 1.1085 (3.6268)	MaskBCELoss 0.1129 (2.9384)	MaskDICELoss 0.9957 (0.6884)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([7, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([6, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([8, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][  2/500]	Time  9.367 ( 9.367)	Loss 0.9844 (7.7606)	CeLoss 0.9844 (1.9754)	SegCLSLoss 0.0000 (0.6734)	KLLoss 0.0000 (0.0674)	MaskLoss 0.0000 (0.8943)	MaskBCELoss 0.0000 (0.2954)	MaskDICELoss 0.0000 (0.5989)
Epoch: [0][  3/500]	Time  9.622 ( 9.622)	Loss 14.1803 (8.1271)	CeLoss 2.3438 (2.1164)	SegCLSLoss 1.1328 (0.6727)	KLLoss 0.2090 (0.0848)	MaskLoss 2.5423 (0.9995)	MaskBCELoss 1.5446 (0.4010)	MaskDICELoss 0.9977 (0.5985)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([12, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][  4/500]	Time  9.336 ( 9.336)	Loss 11.1325 (10.9336)	CeLoss 1.6016 (2.5586)	SegCLSLoss 1.1328 (1.0109)	KLLoss 0.2393 (0.1030)	MaskLoss 1.3734 (1.1879)	MaskBCELoss 0.3768 (0.2897)	MaskDICELoss 0.9966 (0.8982)
Epoch: [0][  5/500]	Time  8.469 ( 8.469)	Loss 11.4569 (7.7939)	CeLoss 2.5781 (2.1969)	SegCLSLoss 1.1328 (0.6875)	KLLoss 0.1758 (0.0515)	MaskLoss 1.0818 (0.8052)	MaskBCELoss 0.0850 (0.2066)	MaskDICELoss 0.9968 (0.5986)
Epoch: [0][  6/500]	Time 11.615 (11.615)	Loss 12.8380 (10.1089)	CeLoss 3.5469 (2.5352)	SegCLSLoss 1.1016 (0.9031)	KLLoss 0.1426 (0.0824)	MaskLoss 1.3141 (1.1424)	MaskBCELoss 0.3208 (0.3502)	MaskDICELoss 0.9933 (0.7922)
Epoch: [0][  7/500]	Time 11.211 (11.211)	Loss 10.9761 (9.6980)	CeLoss 2.2656 (2.4637)	SegCLSLoss 1.1328 (0.8984)	KLLoss 0.1201 (0.0787)	MaskLoss 1.0165 (0.9824)	MaskBCELoss 0.0181 (0.1926)	MaskDICELoss 0.9984 (0.7897)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([13, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][  8/500]	Time 10.529 (10.529)	Loss 12.0101 (8.7113)	CeLoss 3.2500 (2.3563)	SegCLSLoss 1.1016 (0.7859)	KLLoss 0.1289 (0.0948)	MaskLoss 1.0801 (0.8582)	MaskBCELoss 0.0921 (0.1660)	MaskDICELoss 0.9880 (0.6921)
Epoch: [0][  9/500]	Time 10.508 (10.508)	Loss 10.7417 (9.4665)	CeLoss 1.9609 (2.3219)	SegCLSLoss 1.1328 (0.8953)	KLLoss 0.2041 (0.1016)	MaskLoss 1.2706 (1.0232)	MaskBCELoss 0.3596 (0.2650)	MaskDICELoss 0.9110 (0.7582)
[2025-03-04 02:07:40,785] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[5.1e-05], mom=[(0.9, 0.95)]
[2025-03-04 02:07:40,791] [INFO] [timer.py:215:stop] epoch=0/micro_step=100/global_step=10, RunningAvgSamplesPerSec=0.9649089444400712, CurrSamplesPerSec=0.8595270976299961, MemAllocated=56.96GB, MaxMemAllocated=61.93GB
Epoch: [0][ 10/500]	Time 11.636 (11.636)	Loss 11.0770 (10.5039)	CeLoss 2.4219 (2.5977)	SegCLSLoss 1.1172 (1.0063)	KLLoss 0.1475 (0.1045)	MaskLoss 1.3078 (1.1748)	MaskBCELoss 0.4184 (0.3497)	MaskDICELoss 0.8894 (0.8251)
Epoch: [0][ 11/500]	Time  9.526 ( 9.526)	Loss 1.0859 (10.2307)	CeLoss 1.0859 (2.5117)	SegCLSLoss 0.0000 (1.0094)	KLLoss 0.0000 (0.0932)	MaskLoss 0.0000 (1.2701)	MaskBCELoss 0.0000 (0.5065)	MaskDICELoss 0.0000 (0.7636)
Epoch: [0][ 12/500]	Time 11.770 (11.770)	Loss 12.8811 (9.7529)	CeLoss 2.9844 (2.1391)	SegCLSLoss 1.0859 (0.8859)	KLLoss 0.0579 (0.1020)	MaskLoss 1.7481 (1.3002)	MaskBCELoss 0.7829 (0.5549)	MaskDICELoss 0.9652 (0.7453)
Epoch: [0][ 13/500]	Time 10.843 (10.843)	Loss 11.0117 (9.3430)	CeLoss 1.5312 (1.9578)	SegCLSLoss 1.1484 (0.8969)	KLLoss 0.1992 (0.1193)	MaskLoss 1.4196 (1.1580)	MaskBCELoss 0.4429 (0.4077)	MaskDICELoss 0.9767 (0.7503)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([17, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][ 14/500]	Time  9.968 ( 9.968)	Loss 9.9546 (5.8235)	CeLoss 1.9297 (1.4805)	SegCLSLoss 1.1094 (0.5617)	KLLoss 0.1699 (0.0567)	MaskLoss 1.3415 (0.6527)	MaskBCELoss 0.5723 (0.2027)	MaskDICELoss 0.7692 (0.4500)
Epoch: [0][ 15/500]	Time 10.263 (10.263)	Loss 10.1654 (8.7975)	CeLoss 2.0156 (1.8266)	SegCLSLoss 1.1328 (0.8891)	KLLoss 0.0972 (0.0789)	MaskLoss 1.2402 (1.0070)	MaskBCELoss 0.4047 (0.2676)	MaskDICELoss 0.8355 (0.7393)
Epoch: [0][ 16/500]	Time 10.900 (10.900)	Loss 10.1653 (9.4144)	CeLoss 1.9297 (1.6953)	SegCLSLoss 1.0938 (1.0078)	KLLoss 0.0649 (0.0954)	MaskLoss 1.3206 (1.1619)	MaskBCELoss 0.4884 (0.3622)	MaskDICELoss 0.8322 (0.7997)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([14, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][ 17/500]	Time  8.714 ( 8.714)	Loss 11.0572 (6.6249)	CeLoss 1.7109 (1.2949)	SegCLSLoss 1.1250 (0.6695)	KLLoss 0.1196 (0.1080)	MaskLoss 1.3722 (0.7991)	MaskBCELoss 0.3878 (0.2512)	MaskDICELoss 0.9844 (0.5479)
Epoch: [0][ 18/500]	Time 10.201 (10.201)	Loss 0.9258 (6.5382)	CeLoss 0.9258 (1.3199)	SegCLSLoss 0.0000 (0.6680)	KLLoss 0.0000 (0.0484)	MaskLoss 0.0000 (0.8371)	MaskBCELoss 0.0000 (0.3094)	MaskDICELoss 0.0000 (0.5278)
Epoch: [0][ 19/500]	Time  9.648 ( 9.648)	Loss 9.9299 (8.5981)	CeLoss 1.0547 (1.4320)	SegCLSLoss 1.1250 (0.8820)	KLLoss 0.0732 (0.0705)	MaskLoss 1.4216 (1.0637)	MaskBCELoss 0.5217 (0.3087)	MaskDICELoss 0.8999 (0.7550)
[2025-03-04 02:09:23,505] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[0.00011099999999999999], mom=[(0.9, 0.95)]
[2025-03-04 02:09:23,510] [INFO] [timer.py:215:stop] epoch=0/micro_step=200/global_step=20, RunningAvgSamplesPerSec=0.9698040960188147, CurrSamplesPerSec=0.9187190173844787, MemAllocated=57.1GB, MaxMemAllocated=62.08GB
Epoch: [0][ 20/500]	Time 10.886 (10.886)	Loss 9.1373 (8.9972)	CeLoss 1.1953 (1.2273)	SegCLSLoss 1.1172 (0.9969)	KLLoss 0.0078 (0.0863)	MaskLoss 1.3123 (1.1880)	MaskBCELoss 0.5199 (0.3864)	MaskDICELoss 0.7925 (0.8016)
Epoch: [0][ 21/500]	Time  9.490 ( 9.490)	Loss 9.5512 (9.0167)	CeLoss 1.1719 (1.3008)	SegCLSLoss 1.0938 (0.9898)	KLLoss 0.0532 (0.0995)	MaskLoss 1.1849 (1.1377)	MaskBCELoss 0.2836 (0.3300)	MaskDICELoss 0.9013 (0.8077)
Epoch: [0][ 22/500]	Time 10.442 (10.442)	Loss 0.8945 (7.0044)	CeLoss 0.8945 (1.0117)	SegCLSLoss 0.0000 (0.7609)	KLLoss 0.0000 (0.0625)	MaskLoss 0.0000 (0.8611)	MaskBCELoss 0.0000 (0.2228)	MaskDICELoss 0.0000 (0.6383)
Epoch: [0][ 23/500]	Time  9.211 ( 9.211)	Loss 9.6670 (7.9597)	CeLoss 1.0000 (1.0340)	SegCLSLoss 1.0625 (0.8711)	KLLoss 0.0193 (0.0686)	MaskLoss 1.6536 (1.0644)	MaskBCELoss 0.8514 (0.3488)	MaskDICELoss 0.8022 (0.7156)
Epoch: [0][ 24/500]	Time  9.244 ( 9.244)	Loss 10.4138 (7.1471)	CeLoss 1.4688 (1.0996)	SegCLSLoss 1.0703 (0.7609)	KLLoss 0.0762 (0.0960)	MaskLoss 1.3269 (0.9071)	MaskBCELoss 0.3799 (0.2805)	MaskDICELoss 0.9470 (0.6266)
Epoch: [0][ 25/500]	Time  9.367 ( 9.367)	Loss 0.6172 (8.0507)	CeLoss 0.6172 (1.0152)	SegCLSLoss 0.0000 (0.8703)	KLLoss 0.0000 (0.1128)	MaskLoss 0.0000 (1.0177)	MaskBCELoss 0.0000 (0.2757)	MaskDICELoss 0.0000 (0.7420)
Epoch: [0][ 26/500]	Time  8.623 ( 8.623)	Loss 1.3047 (6.0133)	CeLoss 1.3047 (1.0055)	SegCLSLoss 0.0000 (0.6391)	KLLoss 0.0000 (0.0759)	MaskLoss 0.0000 (0.7425)	MaskBCELoss 0.0000 (0.2213)	MaskDICELoss 0.0000 (0.5212)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([20, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][ 27/500]	Time 10.570 (10.570)	Loss 9.7869 (8.0201)	CeLoss 0.8672 (0.9914)	SegCLSLoss 1.0547 (0.8570)	KLLoss 0.1113 (0.0991)	MaskLoss 1.3756 (1.0636)	MaskBCELoss 0.4530 (0.3343)	MaskDICELoss 0.9226 (0.7293)
Epoch: [0][ 28/500]	Time 10.158 (10.158)	Loss 8.9994 (8.0234)	CeLoss 0.9844 (0.9855)	SegCLSLoss 1.0469 (0.8316)	KLLoss 0.0586 (0.1129)	MaskLoss 1.2717 (1.0074)	MaskBCELoss 0.4561 (0.2584)	MaskDICELoss 0.8156 (0.7490)
Epoch: [0][ 29/500]	Time 10.229 (10.229)	Loss 1.0000 (5.9872)	CeLoss 1.0000 (0.8016)	SegCLSLoss 0.0000 (0.6195)	KLLoss 0.0000 (0.0876)	MaskLoss 0.0000 (0.7349)	MaskBCELoss 0.0000 (0.1819)	MaskDICELoss 0.0000 (0.5530)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([19, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
[2025-03-04 02:10:59,222] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[0.00017099999999999998], mom=[(0.9, 0.95)]
[2025-03-04 02:10:59,228] [INFO] [timer.py:215:stop] epoch=0/micro_step=300/global_step=30, RunningAvgSamplesPerSec=0.995358740570889, CurrSamplesPerSec=1.1934144119128383, MemAllocated=56.71GB, MaxMemAllocated=62.08GB
Epoch: [0][ 30/500]	Time  8.381 ( 8.381)	Loss 1.7422 (7.7699)	CeLoss 1.7422 (0.8537)	SegCLSLoss 0.0000 (0.8250)	KLLoss 0.0000 (0.1354)	MaskLoss 0.0000 (0.9586)	MaskBCELoss 0.0000 (0.2168)	MaskDICELoss 0.0000 (0.7419)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([18, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][ 31/500]	Time 10.753 (10.753)	Loss 1.3125 (6.7468)	CeLoss 1.3125 (0.7846)	SegCLSLoss 0.0000 (0.6934)	KLLoss 0.0000 (0.0915)	MaskLoss 0.0000 (0.8529)	MaskBCELoss 0.0000 (0.2165)	MaskDICELoss 0.0000 (0.6364)
Epoch: [0][ 32/500]	Time 11.969 (11.969)	Loss 9.3515 (7.6828)	CeLoss 0.5977 (0.6650)	SegCLSLoss 0.9570 (0.7680)	KLLoss 0.1426 (0.0844)	MaskLoss 1.0718 (0.9691)	MaskBCELoss 0.0736 (0.2007)	MaskDICELoss 0.9982 (0.7684)
Epoch: [0][ 33/500]	Time 11.120 (11.120)	Loss 9.3990 (8.5410)	CeLoss 0.7305 (0.7410)	SegCLSLoss 0.9648 (0.8480)	KLLoss 0.2100 (0.1313)	MaskLoss 1.1370 (1.1090)	MaskBCELoss 0.1865 (0.2712)	MaskDICELoss 0.9505 (0.8378)
Epoch: [0][ 34/500]	Time  7.774 ( 7.774)	Loss 9.2625 (6.6112)	CeLoss 0.4316 (0.6539)	SegCLSLoss 0.9258 (0.6324)	KLLoss 0.2295 (0.0883)	MaskLoss 1.1068 (0.9040)	MaskBCELoss 0.1195 (0.2797)	MaskDICELoss 0.9873 (0.6243)
Epoch: [0][ 35/500]	Time  9.798 ( 9.798)	Loss 9.4217 (6.6867)	CeLoss 0.6953 (0.7529)	SegCLSLoss 0.8711 (0.6094)	KLLoss 0.0796 (0.0812)	MaskLoss 1.2007 (0.8902)	MaskBCELoss 0.2325 (0.2623)	MaskDICELoss 0.9682 (0.6279)
Epoch: [0][ 36/500]	Time 10.752 (10.752)	Loss 0.8477 (7.7448)	CeLoss 0.8477 (0.6500)	SegCLSLoss 0.0000 (0.6766)	KLLoss 0.0000 (0.1129)	MaskLoss 0.0000 (1.0409)	MaskBCELoss 0.0000 (0.2809)	MaskDICELoss 0.0000 (0.7600)
Epoch: [0][ 37/500]	Time  9.688 ( 9.688)	Loss 0.6953 (6.4839)	CeLoss 0.6953 (0.5943)	SegCLSLoss 0.0000 (0.5574)	KLLoss 0.0000 (0.0735)	MaskLoss 0.0000 (0.9319)	MaskBCELoss 0.0000 (0.3197)	MaskDICELoss 0.0000 (0.6122)
Epoch: [0][ 38/500]	Time  9.739 ( 9.739)	Loss 9.5871 (5.7310)	CeLoss 0.4902 (0.6457)	SegCLSLoss 0.7109 (0.4410)	KLLoss 0.1309 (0.0824)	MaskLoss 1.3249 (0.7926)	MaskBCELoss 0.3314 (0.2596)	MaskDICELoss 0.9935 (0.5330)
Epoch: [0][ 39/500]	Time  9.058 ( 9.058)	Loss 8.4233 (8.3073)	CeLoss 0.5156 (0.4922)	SegCLSLoss 0.7227 (0.6281)	KLLoss 0.0913 (0.1598)	MaskLoss 1.3344 (1.1967)	MaskBCELoss 0.5361 (0.3719)	MaskDICELoss 0.7983 (0.8247)
[2025-03-04 02:12:42,016] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[0.00023099999999999998], mom=[(0.9, 0.95)]
[2025-03-04 02:12:42,021] [INFO] [timer.py:215:stop] epoch=0/micro_step=400/global_step=40, RunningAvgSamplesPerSec=0.9893696757700807, CurrSamplesPerSec=0.8236694201035435, MemAllocated=57.48GB, MaxMemAllocated=62.23GB
Epoch: [0][ 40/500]	Time 12.143 (12.143)	Loss 0.6875 (7.4782)	CeLoss 0.6875 (0.5488)	SegCLSLoss 0.0000 (0.5129)	KLLoss 0.0000 (0.1320)	MaskLoss 0.0000 (1.0196)	MaskBCELoss 0.0000 (0.2692)	MaskDICELoss 0.0000 (0.7504)
Epoch: [0][ 41/500]	Time 11.182 (11.182)	Loss 9.1477 (8.7440)	CeLoss 0.4395 (0.4197)	SegCLSLoss 0.5703 (0.5578)	KLLoss 0.0874 (0.1336)	MaskLoss 1.1721 (1.2753)	MaskBCELoss 0.1736 (0.3816)	MaskDICELoss 0.9985 (0.8936)
Epoch: [0][ 42/500]	Time 11.278 (11.278)	Loss 8.4837 (8.7206)	CeLoss 0.4727 (0.4080)	SegCLSLoss 0.5430 (0.5158)	KLLoss 0.1963 (0.1445)	MaskLoss 1.3800 (1.2345)	MaskBCELoss 0.5823 (0.3276)	MaskDICELoss 0.7977 (0.9070)
Epoch: [0][ 43/500]	Time  9.169 ( 9.169)	Loss 8.8492 (6.3135)	CeLoss 0.4531 (0.5744)	SegCLSLoss 0.4082 (0.3018)	KLLoss 0.0703 (0.0892)	MaskLoss 1.1702 (0.8677)	MaskBCELoss 0.2065 (0.2405)	MaskDICELoss 0.9637 (0.6272)
Epoch: [0][ 44/500]	Time  8.171 ( 8.171)	Loss 8.3291 (6.9584)	CeLoss 0.3867 (0.4531)	SegCLSLoss 0.3887 (0.3033)	KLLoss 0.2217 (0.1414)	MaskLoss 1.2520 (0.9782)	MaskBCELoss 0.4152 (0.2689)	MaskDICELoss 0.8368 (0.7093)
Epoch: [0][ 45/500]	Time  8.462 ( 8.462)	Loss 8.8861 (7.9977)	CeLoss 0.6094 (0.5105)	SegCLSLoss 0.2949 (0.2818)	KLLoss 0.1201 (0.1340)	MaskLoss 1.1936 (1.1392)	MaskBCELoss 0.2569 (0.3169)	MaskDICELoss 0.9367 (0.8222)
Epoch: [0][ 46/500]	Time  8.965 ( 8.965)	Loss 8.6947 (7.0174)	CeLoss 0.2100 (0.4866)	SegCLSLoss 0.3105 (0.2072)	KLLoss 0.2285 (0.1358)	MaskLoss 1.2053 (1.0034)	MaskBCELoss 0.2568 (0.2894)	MaskDICELoss 0.9484 (0.7141)
Epoch: [0][ 47/500]	Time  9.936 ( 9.936)	Loss 1.4375 (7.8160)	CeLoss 1.4375 (0.4263)	SegCLSLoss 0.0000 (0.1765)	KLLoss 0.0000 (0.1301)	MaskLoss 0.0000 (1.1577)	MaskBCELoss 0.0000 (0.3484)	MaskDICELoss 0.0000 (0.8093)
Epoch: [0][ 48/500]	Time  9.338 ( 9.338)	Loss 8.5272 (6.9072)	CeLoss 0.3965 (0.4242)	SegCLSLoss 0.1807 (0.1383)	KLLoss 0.2217 (0.1394)	MaskLoss 1.3864 (1.0725)	MaskBCELoss 0.5452 (0.3842)	MaskDICELoss 0.8412 (0.6883)
Epoch: [0][ 49/500]	Time 11.470 (11.470)	Loss 8.7148 (6.1653)	CeLoss 0.4375 (0.5016)	SegCLSLoss 0.1631 (0.0991)	KLLoss 0.2246 (0.1099)	MaskLoss 1.2203 (0.8884)	MaskBCELoss 0.2983 (0.2670)	MaskDICELoss 0.9220 (0.6213)
[2025-03-04 02:14:21,519] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[0.00029099999999999997], mom=[(0.9, 0.95)]
[2025-03-04 02:14:21,524] [INFO] [timer.py:215:stop] epoch=0/micro_step=500/global_step=50, RunningAvgSamplesPerSec=0.9926166613432775, CurrSamplesPerSec=0.8673663079868555, MemAllocated=57.65GB, MaxMemAllocated=62.23GB
Epoch: [0][ 50/500]	Time 11.531 (11.531)	Loss 1.1562 (7.8389)	CeLoss 1.1562 (0.4546)	SegCLSLoss 0.0000 (0.1269)	KLLoss 0.0000 (0.1616)	MaskLoss 0.0000 (1.1219)	MaskBCELoss 0.0000 (0.3026)	MaskDICELoss 0.0000 (0.8192)
Epoch: [0][ 51/500]	Time 10.754 (10.754)	Loss 9.2168 (8.5739)	CeLoss 0.3242 (0.2959)	SegCLSLoss 0.1201 (0.1267)	KLLoss 0.1650 (0.1789)	MaskLoss 1.3816 (1.2423)	MaskBCELoss 0.3978 (0.3171)	MaskDICELoss 0.9838 (0.9252)
Epoch: [0][ 52/500]	Time 11.530 (11.530)	Loss 8.6465 (7.8785)	CeLoss 0.2432 (0.3750)	SegCLSLoss 0.1245 (0.1149)	KLLoss 0.1973 (0.1667)	MaskLoss 1.0824 (1.1980)	MaskBCELoss 0.0854 (0.3840)	MaskDICELoss 0.9970 (0.8140)
Epoch: [0][ 53/500]	Time 10.212 (10.212)	Loss 8.7749 (5.4865)	CeLoss 0.3477 (0.6404)	SegCLSLoss 0.1348 (0.0633)	KLLoss 0.2500 (0.0889)	MaskLoss 1.0663 (0.7678)	MaskBCELoss 0.0699 (0.2362)	MaskDICELoss 0.9964 (0.5317)
Epoch: [0][ 54/500]	Time 10.415 (10.415)	Loss 8.4061 (6.9540)	CeLoss 0.2832 (0.4350)	SegCLSLoss 0.1128 (0.0891)	KLLoss 0.1650 (0.1354)	MaskLoss 1.1669 (1.0449)	MaskBCELoss 0.2388 (0.3366)	MaskDICELoss 0.9281 (0.7082)
Epoch: [0][ 55/500]	Time  9.147 ( 9.147)	Loss 1.3359 (5.9211)	CeLoss 1.3359 (0.5414)	SegCLSLoss 0.0000 (0.0654)	KLLoss 0.0000 (0.0827)	MaskLoss 0.0000 (0.9257)	MaskBCELoss 0.0000 (0.3570)	MaskDICELoss 0.0000 (0.5688)
Epoch: [0][ 56/500]	Time 10.711 (10.711)	Loss 8.2181 (7.6641)	CeLoss 0.2852 (0.3560)	SegCLSLoss 0.1328 (0.0988)	KLLoss 0.2031 (0.1453)	MaskLoss 1.3667 (1.1793)	MaskBCELoss 0.5450 (0.3869)	MaskDICELoss 0.8217 (0.7924)
Epoch: [0][ 57/500]	Time  9.022 ( 9.022)	Loss 1.2891 (5.1515)	CeLoss 1.2891 (0.5373)	SegCLSLoss 0.0000 (0.0623)	KLLoss 0.0000 (0.0831)	MaskLoss 0.0000 (0.7646)	MaskBCELoss 0.0000 (0.2694)	MaskDICELoss 0.0000 (0.4952)
Epoch: [0][ 58/500]	Time  9.856 ( 9.856)	Loss 1.5625 (5.6392)	CeLoss 1.5625 (0.6531)	SegCLSLoss 0.0000 (0.0651)	KLLoss 0.0000 (0.0975)	MaskLoss 0.0000 (0.7536)	MaskBCELoss 0.0000 (0.1955)	MaskDICELoss 0.0000 (0.5581)
Epoch: [0][ 59/500]	Time 11.430 (11.430)	Loss 0.9531 (6.9304)	CeLoss 0.9531 (0.4886)	SegCLSLoss 0.0000 (0.0832)	KLLoss 0.0000 (0.1202)	MaskLoss 0.0000 (0.9700)	MaskBCELoss 0.0000 (0.2467)	MaskDICELoss 0.0000 (0.7233)
[2025-03-04 02:16:04,488] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[0.00029979518072289157], mom=[(0.9, 0.95)]
[2025-03-04 02:16:04,494] [INFO] [timer.py:215:stop] epoch=0/micro_step=600/global_step=60, RunningAvgSamplesPerSec=0.9888784465067559, CurrSamplesPerSec=1.0109361431368329, MemAllocated=57.47GB, MaxMemAllocated=62.23GB
Epoch: [0][ 60/500]	Time  9.894 ( 9.894)	Loss 8.4553 (7.3809)	CeLoss 0.5430 (0.4419)	SegCLSLoss 0.0879 (0.0920)	KLLoss 0.1602 (0.1351)	MaskLoss 1.1406 (1.0423)	MaskBCELoss 0.2359 (0.2633)	MaskDICELoss 0.9047 (0.7790)
Epoch: [0][ 61/500]	Time 10.935 (10.935)	Loss 1.2734 (6.1064)	CeLoss 1.2734 (0.5749)	SegCLSLoss 0.0000 (0.0753)	KLLoss 0.0000 (0.1079)	MaskLoss 0.0000 (0.8475)	MaskBCELoss 0.0000 (0.2324)	MaskDICELoss 0.0000 (0.6151)
Epoch: [0][ 62/500]	Time  7.628 ( 7.628)	Loss 6.8699 (4.5471)	CeLoss 0.4512 (0.7162)	SegCLSLoss 0.0991 (0.0578)	KLLoss 0.1650 (0.0757)	MaskLoss 0.9264 (0.5824)	MaskBCELoss 0.2008 (0.1555)	MaskDICELoss 0.7255 (0.4269)
Epoch: [0][ 63/500]	Time  9.542 ( 9.542)	Loss 8.2004 (7.4115)	CeLoss 0.2891 (0.3957)	SegCLSLoss 0.0835 (0.1033)	KLLoss 0.0913 (0.1423)	MaskLoss 1.2119 (1.1605)	MaskBCELoss 0.3195 (0.4102)	MaskDICELoss 0.8924 (0.7503)
Epoch: [0][ 64/500]	Time 10.327 (10.327)	Loss 8.7947 (6.9717)	CeLoss 0.2598 (0.3901)	SegCLSLoss 0.1177 (0.0912)	KLLoss 0.2061 (0.1385)	MaskLoss 1.1380 (0.9748)	MaskBCELoss 0.1387 (0.2335)	MaskDICELoss 0.9992 (0.7413)
Epoch: [0][ 65/500]	Time 11.636 (11.636)	Loss 0.9414 (7.5866)	CeLoss 0.9414 (0.3179)	SegCLSLoss 0.0000 (0.1005)	KLLoss 0.0000 (0.1472)	MaskLoss 0.0000 (1.1451)	MaskBCELoss 0.0000 (0.3482)	MaskDICELoss 0.0000 (0.7969)
Epoch: [0][ 66/500]	Time  8.986 ( 8.986)	Loss 1.1953 (6.0666)	CeLoss 1.1953 (0.5406)	SegCLSLoss 0.0000 (0.0838)	KLLoss 0.0000 (0.1131)	MaskLoss 0.0000 (0.8700)	MaskBCELoss 0.0000 (0.2649)	MaskDICELoss 0.0000 (0.6051)
Epoch: [0][ 67/500]	Time 11.667 (11.667)	Loss 9.1936 (8.3098)	CeLoss 0.3105 (0.2682)	SegCLSLoss 0.1172 (0.0979)	KLLoss 0.2090 (0.1489)	MaskLoss 1.3211 (1.2629)	MaskBCELoss 0.3255 (0.3766)	MaskDICELoss 0.9956 (0.8863)
Epoch: [0][ 68/500]	Time 11.586 (11.586)	Loss 8.5089 (7.8329)	CeLoss 0.1660 (0.2937)	SegCLSLoss 0.1113 (0.0978)	KLLoss 0.2217 (0.1519)	MaskLoss 1.0559 (1.1593)	MaskBCELoss 0.0636 (0.3226)	MaskDICELoss 0.9923 (0.8367)
Epoch: [0][ 69/500]	Time 10.423 (10.423)	Loss 8.3295 (7.4399)	CeLoss 0.2061 (0.3160)	SegCLSLoss 0.1118 (0.0980)	KLLoss 0.1738 (0.1329)	MaskLoss 1.2331 (1.1076)	MaskBCELoss 0.3284 (0.3198)	MaskDICELoss 0.9046 (0.7878)
[2025-03-04 02:17:47,660] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[0.0002995542168674699], mom=[(0.9, 0.95)]
[2025-03-04 02:17:47,665] [INFO] [timer.py:215:stop] epoch=0/micro_step=700/global_step=70, RunningAvgSamplesPerSec=0.9859669377813083, CurrSamplesPerSec=0.9579785550322331, MemAllocated=57.66GB, MaxMemAllocated=62.23GB
Epoch: [0][ 70/500]	Time 10.441 (10.441)	Loss 8.3212 (7.2457)	CeLoss 0.2578 (0.3857)	SegCLSLoss 0.0825 (0.0888)	KLLoss 0.1543 (0.1517)	MaskLoss 1.2505 (1.0203)	MaskBCELoss 0.3560 (0.2497)	MaskDICELoss 0.8945 (0.7706)
Epoch: [0][ 71/500]	Time  9.023 ( 9.023)	Loss 8.8297 (6.3115)	CeLoss 0.2832 (0.5666)	SegCLSLoss 0.1050 (0.0657)	KLLoss 0.1777 (0.1086)	MaskLoss 1.5242 (0.8737)	MaskBCELoss 0.6466 (0.2310)	MaskDICELoss 0.8776 (0.6427)
Epoch: [0][ 72/500]	Time  9.323 ( 9.323)	Loss 8.1837 (5.6071)	CeLoss 0.1631 (0.7518)	SegCLSLoss 0.1689 (0.0699)	KLLoss 0.2178 (0.1185)	MaskLoss 1.3671 (0.7873)	MaskBCELoss 0.5363 (0.2660)	MaskDICELoss 0.8308 (0.5213)
Epoch: [0][ 73/500]	Time 10.276 (10.276)	Loss 7.3726 (4.5375)	CeLoss 0.2236 (0.6687)	SegCLSLoss 0.0845 (0.0428)	KLLoss 0.1250 (0.0722)	MaskLoss 1.1405 (0.5702)	MaskBCELoss 0.3570 (0.1310)	MaskDICELoss 0.7835 (0.4392)
Epoch: [0][ 74/500]	Time  8.945 ( 8.945)	Loss 7.7762 (5.8836)	CeLoss 0.2715 (0.5789)	SegCLSLoss 0.0840 (0.0563)	KLLoss 0.1768 (0.0929)	MaskLoss 1.0205 (0.8212)	MaskBCELoss 0.1460 (0.2311)	MaskDICELoss 0.8745 (0.5902)
Epoch: [0][ 75/500]	Time 10.081 (10.081)	Loss 8.2962 (6.2174)	CeLoss 0.2305 (0.4829)	SegCLSLoss 0.0801 (0.0625)	KLLoss 0.0908 (0.1063)	MaskLoss 1.0200 (0.7852)	MaskBCELoss 0.0371 (0.1141)	MaskDICELoss 0.9828 (0.6711)
Epoch: [0][ 76/500]	Time  7.488 ( 7.488)	Loss 8.0487 (3.8448)	CeLoss 0.3086 (0.7096)	SegCLSLoss 0.0659 (0.0330)	KLLoss 0.0698 (0.0507)	MaskLoss 1.2459 (0.4773)	MaskBCELoss 0.3884 (0.1251)	MaskDICELoss 0.8575 (0.3522)
Epoch: [0][ 77/500]	Time  9.912 ( 9.912)	Loss 8.4629 (8.2389)	CeLoss 0.2383 (0.4319)	SegCLSLoss 0.0981 (0.0854)	KLLoss 0.1816 (0.1573)	MaskLoss 1.0479 (1.1051)	MaskBCELoss 0.0649 (0.2056)	MaskDICELoss 0.9831 (0.8995)
Epoch: [0][ 78/500]	Time 10.074 (10.074)	Loss 1.4844 (6.1649)	CeLoss 1.4844 (0.5387)	SegCLSLoss 0.0000 (0.0522)	KLLoss 0.0000 (0.0897)	MaskLoss 0.0000 (0.8798)	MaskBCELoss 0.0000 (0.2546)	MaskDICELoss 0.0000 (0.6251)
Epoch: [0][ 79/500]	Time  7.808 ( 7.808)	Loss 1.1719 (3.9534)	CeLoss 1.1719 (0.7235)	SegCLSLoss 0.0000 (0.0294)	KLLoss 0.0000 (0.0483)	MaskLoss 0.0000 (0.4951)	MaskBCELoss 0.0000 (0.1322)	MaskDICELoss 0.0000 (0.3629)
[2025-03-04 02:19:21,702] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[0.00029931325301204816], mom=[(0.9, 0.95)]
[2025-03-04 02:19:21,708] [INFO] [timer.py:215:stop] epoch=0/micro_step=800/global_step=80, RunningAvgSamplesPerSec=0.9952700818269029, CurrSamplesPerSec=0.90003719197419, MemAllocated=57.27GB, MaxMemAllocated=62.23GB
Epoch: [0][ 80/500]	Time 11.112 (11.112)	Loss 8.8241 (7.1719)	CeLoss 0.3594 (0.4325)	SegCLSLoss 0.0967 (0.0673)	KLLoss 0.1895 (0.1214)	MaskLoss 1.1688 (1.0690)	MaskBCELoss 0.1873 (0.3279)	MaskDICELoss 0.9815 (0.7411)
Epoch: [0][ 81/500]	Time 10.227 (10.227)	Loss 8.3605 (5.7789)	CeLoss 0.2480 (0.3911)	SegCLSLoss 0.1357 (0.0647)	KLLoss 0.2314 (0.1093)	MaskLoss 1.2244 (0.8844)	MaskBCELoss 0.3303 (0.3049)	MaskDICELoss 0.8941 (0.5796)
Epoch: [0][ 82/500]	Time  9.025 ( 9.025)	Loss 8.1976 (6.9728)	CeLoss 0.2148 (0.4513)	SegCLSLoss 0.0942 (0.0633)	KLLoss 0.2100 (0.1180)	MaskLoss 1.1101 (1.0563)	MaskBCELoss 0.1927 (0.3465)	MaskDICELoss 0.9174 (0.7098)
Epoch: [0][ 83/500]	Time 10.349 (10.349)	Loss 7.1207 (6.7493)	CeLoss 0.3164 (0.3396)	SegCLSLoss 0.0562 (0.0733)	KLLoss 0.0601 (0.1296)	MaskLoss 1.2333 (0.9929)	MaskBCELoss 0.5250 (0.2833)	MaskDICELoss 0.7083 (0.7096)
Epoch: [0][ 84/500]	Time 12.084 (12.084)	Loss 8.4478 (8.2326)	CeLoss 0.2617 (0.3036)	SegCLSLoss 0.1030 (0.0956)	KLLoss 0.2080 (0.1725)	MaskLoss 1.1806 (1.1973)	MaskBCELoss 0.2528 (0.3116)	MaskDICELoss 0.9278 (0.8857)
Epoch: [0][ 85/500]	Time 11.657 (11.657)	Loss 7.4424 (7.6873)	CeLoss 0.4375 (0.4391)	SegCLSLoss 0.0742 (0.0678)	KLLoss 0.1357 (0.1331)	MaskLoss 1.1520 (1.0918)	MaskBCELoss 0.3971 (0.2755)	MaskDICELoss 0.7548 (0.8163)
Epoch: [0][ 86/500]	Time  9.263 ( 9.263)	Loss 1.0859 (6.1216)	CeLoss 1.0859 (0.4465)	SegCLSLoss 0.0000 (0.0559)	KLLoss 0.0000 (0.1201)	MaskLoss 0.0000 (0.8407)	MaskBCELoss 0.0000 (0.1997)	MaskDICELoss 0.0000 (0.6409)
Epoch: [0][ 87/500]	Time 10.033 (10.033)	Loss 8.2961 (7.1073)	CeLoss 0.2598 (0.3069)	SegCLSLoss 0.0723 (0.0639)	KLLoss 0.1689 (0.1229)	MaskLoss 1.0407 (1.0133)	MaskBCELoss 0.0824 (0.2435)	MaskDICELoss 0.9583 (0.7699)
Epoch: [0][ 88/500]	Time  9.438 ( 9.438)	Loss 1.5156 (5.3387)	CeLoss 1.5156 (0.7032)	SegCLSLoss 0.0000 (0.0402)	KLLoss 0.0000 (0.0732)	MaskLoss 0.0000 (0.7005)	MaskBCELoss 0.0000 (0.1770)	MaskDICELoss 0.0000 (0.5235)
Epoch: [0][ 89/500]	Time 10.732 (10.732)	Loss 8.0363 (7.3794)	CeLoss 0.3652 (0.3183)	SegCLSLoss 0.0530 (0.0705)	KLLoss 0.0664 (0.1204)	MaskLoss 1.3217 (1.1236)	MaskBCELoss 0.4994 (0.3471)	MaskDICELoss 0.8223 (0.7765)
[2025-03-04 02:21:03,800] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[0.0002990722891566265], mom=[(0.9, 0.95)]
[2025-03-04 02:21:03,806] [INFO] [timer.py:215:stop] epoch=0/micro_step=900/global_step=90, RunningAvgSamplesPerSec=0.993462347515678, CurrSamplesPerSec=1.0764096113588348, MemAllocated=57.45GB, MaxMemAllocated=62.23GB
Epoch: [0][ 90/500]	Time  9.292 ( 9.292)	Loss 8.1914 (4.9330)	CeLoss 0.3164 (0.4902)	SegCLSLoss 0.0520 (0.0396)	KLLoss 0.0811 (0.0769)	MaskLoss 1.1781 (0.6823)	MaskBCELoss 0.2759 (0.1854)	MaskDICELoss 0.9022 (0.4969)
Epoch: [0][ 91/500]	Time  8.725 ( 8.725)	Loss 1.0312 (5.1788)	CeLoss 1.0312 (0.5046)	SegCLSLoss 0.0000 (0.0409)	KLLoss 0.0000 (0.0852)	MaskLoss 0.0000 (0.7328)	MaskBCELoss 0.0000 (0.2156)	MaskDICELoss 0.0000 (0.5172)
Epoch: [0][ 92/500]	Time  8.535 ( 8.535)	Loss 1.3203 (3.1655)	CeLoss 1.3203 (0.8322)	SegCLSLoss 0.0000 (0.0210)	KLLoss 0.0000 (0.0477)	MaskLoss 0.0000 (0.3603)	MaskBCELoss 0.0000 (0.1012)	MaskDICELoss 0.0000 (0.2591)
Epoch: [0][ 93/500]	Time  9.140 ( 9.140)	Loss 7.5499 (6.4496)	CeLoss 0.2490 (0.4508)	SegCLSLoss 0.0503 (0.0650)	KLLoss 0.0830 (0.1561)	MaskLoss 1.1998 (0.8867)	MaskBCELoss 0.4006 (0.2138)	MaskDICELoss 0.7991 (0.6729)
Epoch: [0][ 94/500]	Time  9.377 ( 9.377)	Loss 7.4289 (6.9984)	CeLoss 0.3066 (0.3130)	SegCLSLoss 0.0537 (0.0673)	KLLoss 0.0879 (0.1218)	MaskLoss 1.2978 (1.2063)	MaskBCELoss 0.5625 (0.5202)	MaskDICELoss 0.7352 (0.6862)
Epoch: [0][ 95/500]	Time  9.596 ( 9.596)	Loss 8.6990 (6.7876)	CeLoss 0.1855 (0.5307)	SegCLSLoss 0.0894 (0.0581)	KLLoss 0.2217 (0.1363)	MaskLoss 1.1365 (0.9631)	MaskBCELoss 0.1410 (0.2689)	MaskDICELoss 0.9955 (0.6942)
Epoch: [0][ 96/500]	Time 10.850 (10.850)	Loss 1.8984 (7.4645)	CeLoss 1.8984 (0.4700)	SegCLSLoss 0.0000 (0.0584)	KLLoss 0.0000 (0.1346)	MaskLoss 0.0000 (1.0836)	MaskBCELoss 0.0000 (0.3063)	MaskDICELoss 0.0000 (0.7772)
Epoch: [0][ 97/500]	Time  9.353 ( 9.353)	Loss 1.2188 (6.7329)	CeLoss 1.2188 (0.4300)	SegCLSLoss 0.0000 (0.0556)	KLLoss 0.0000 (0.1090)	MaskLoss 0.0000 (0.9698)	MaskBCELoss 0.0000 (0.2654)	MaskDICELoss 0.0000 (0.7044)
Epoch: [0][ 98/500]	Time  9.115 ( 9.115)	Loss 7.7937 (5.1096)	CeLoss 0.3418 (0.5281)	SegCLSLoss 0.0654 (0.0354)	KLLoss 0.1631 (0.0725)	MaskLoss 1.0485 (0.7025)	MaskBCELoss 0.1889 (0.1882)	MaskDICELoss 0.8596 (0.5143)
Epoch: [0][ 99/500]	Time  8.233 ( 8.233)	Loss 1.3125 (5.9130)	CeLoss 1.3125 (0.5493)	SegCLSLoss 0.0000 (0.0468)	KLLoss 0.0000 (0.1198)	MaskLoss 0.0000 (0.8056)	MaskBCELoss 0.0000 (0.2042)	MaskDICELoss 0.0000 (0.6015)
[2025-03-04 02:22:40,577] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[0.0002988313253012048], mom=[(0.9, 0.95)]
[2025-03-04 02:22:40,583] [INFO] [timer.py:215:stop] epoch=0/micro_step=1000/global_step=100, RunningAvgSamplesPerSec=0.9974023370675691, CurrSamplesPerSec=0.7220526651900262, MemAllocated=56.74GB, MaxMemAllocated=62.28GB
Epoch: [0][100/500]	Time 13.851 (13.851)	Loss 1.0391 (7.4109)	CeLoss 1.0391 (0.3577)	SegCLSLoss 0.0000 (0.0579)	KLLoss 0.0000 (0.1341)	MaskLoss 0.0000 (1.0434)	MaskBCELoss 0.0000 (0.2428)	MaskDICELoss 0.0000 (0.8006)
Epoch: [0][101/500]	Time  9.268 ( 9.268)	Loss 8.7690 (6.6248)	CeLoss 0.2471 (0.4573)	SegCLSLoss 0.0674 (0.0647)	KLLoss 0.1660 (0.1334)	MaskLoss 1.1676 (0.9578)	MaskBCELoss 0.1698 (0.2768)	MaskDICELoss 0.9978 (0.6810)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([24, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][102/500]	Time  9.740 ( 9.740)	Loss 6.5910 (5.8348)	CeLoss 0.2207 (0.3608)	SegCLSLoss 0.0913 (0.0605)	KLLoss 0.2109 (0.1139)	MaskLoss 0.8938 (0.8068)	MaskBCELoss 0.1726 (0.1874)	MaskDICELoss 0.7211 (0.6194)
Epoch: [0][103/500]	Time 10.868 (10.868)	Loss 8.2630 (7.6207)	CeLoss 0.1475 (0.3947)	SegCLSLoss 0.1445 (0.0647)	KLLoss 0.2520 (0.1320)	MaskLoss 1.2252 (1.0525)	MaskBCELoss 0.3353 (0.2264)	MaskDICELoss 0.8900 (0.8261)
Epoch: [0][104/500]	Time  9.067 ( 9.067)	Loss 8.6778 (6.7953)	CeLoss 0.2227 (0.3692)	SegCLSLoss 0.0776 (0.0728)	KLLoss 0.1953 (0.1448)	MaskLoss 1.1246 (0.9474)	MaskBCELoss 0.1293 (0.2225)	MaskDICELoss 0.9953 (0.7250)
Epoch: [0][105/500]	Time 11.357 (11.357)	Loss 8.9895 (7.1784)	CeLoss 0.2168 (0.2759)	SegCLSLoss 0.0752 (0.0703)	KLLoss 0.2334 (0.1436)	MaskLoss 1.4905 (1.0913)	MaskBCELoss 0.5705 (0.3344)	MaskDICELoss 0.9200 (0.7569)
Epoch: [0][106/500]	Time  9.091 ( 9.091)	Loss 8.5173 (6.7260)	CeLoss 0.2695 (0.4389)	SegCLSLoss 0.0566 (0.0586)	KLLoss 0.1357 (0.1418)	MaskLoss 1.2590 (0.9792)	MaskBCELoss 0.3314 (0.2862)	MaskDICELoss 0.9276 (0.6930)
Epoch: [0][107/500]	Time 10.479 (10.479)	Loss 0.2236 (6.4053)	CeLoss 0.2236 (0.3594)	SegCLSLoss 0.0000 (0.0607)	KLLoss 0.0000 (0.1417)	MaskLoss 0.0000 (1.0401)	MaskBCELoss 0.0000 (0.4078)	MaskDICELoss 0.0000 (0.6323)
Epoch: [0][108/500]	Time  8.189 ( 8.189)	Loss 9.2158 (7.1230)	CeLoss 0.3711 (0.5395)	SegCLSLoss 0.0645 (0.0661)	KLLoss 0.1416 (0.1423)	MaskLoss 1.3755 (1.0945)	MaskBCELoss 0.3886 (0.3913)	MaskDICELoss 0.9870 (0.7032)
Epoch: [0][109/500]	Time  8.246 ( 8.246)	Loss 0.5898 (3.1184)	CeLoss 0.5898 (0.5713)	SegCLSLoss 0.0000 (0.0265)	KLLoss 0.0000 (0.0605)	MaskLoss 0.0000 (0.4239)	MaskBCELoss 0.0000 (0.1529)	MaskDICELoss 0.0000 (0.2710)
[2025-03-04 02:24:15,565] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[0.0002985903614457831], mom=[(0.9, 0.95)]
[2025-03-04 02:24:15,570] [INFO] [timer.py:215:stop] epoch=0/micro_step=1100/global_step=110, RunningAvgSamplesPerSec=1.0022975717455602, CurrSamplesPerSec=1.1520363527096162, MemAllocated=57.59GB, MaxMemAllocated=62.28GB
Epoch: [0][110/500]	Time  8.683 ( 8.683)	Loss 8.7515 (5.5984)	CeLoss 0.2637 (0.5222)	SegCLSLoss 0.0674 (0.0413)	KLLoss 0.1846 (0.1050)	MaskLoss 1.1561 (0.7947)	MaskBCELoss 0.1630 (0.2344)	MaskDICELoss 0.9931 (0.5602)
Epoch: [0][111/500]	Time 10.610 (10.610)	Loss 8.2559 (7.0164)	CeLoss 0.2139 (0.3457)	SegCLSLoss 0.0713 (0.0595)	KLLoss 0.1885 (0.1352)	MaskLoss 1.1804 (1.0341)	MaskBCELoss 0.2708 (0.2944)	MaskDICELoss 0.9096 (0.7396)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([30, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][112/500]	Time 10.127 (10.127)	Loss 8.6426 (7.6529)	CeLoss 0.2656 (0.3116)	SegCLSLoss 0.0684 (0.0654)	KLLoss 0.1787 (0.1450)	MaskLoss 1.0831 (1.1469)	MaskBCELoss 0.0837 (0.3353)	MaskDICELoss 0.9993 (0.8116)
Epoch: [0][113/500]	Time  7.703 ( 7.703)	Loss 1.1250 (6.0356)	CeLoss 1.1250 (0.4900)	SegCLSLoss 0.0000 (0.0530)	KLLoss 0.0000 (0.1266)	MaskLoss 0.0000 (0.8812)	MaskBCELoss 0.0000 (0.2762)	MaskDICELoss 0.0000 (0.6050)
Epoch: [0][114/500]	Time 11.093 (11.093)	Loss 7.4873 (7.9023)	CeLoss 0.3145 (0.2779)	SegCLSLoss 0.0527 (0.0571)	KLLoss 0.1377 (0.1399)	MaskLoss 1.1003 (1.1866)	MaskBCELoss 0.2986 (0.3396)	MaskDICELoss 0.8017 (0.8471)
Epoch: [0][115/500]	Time 10.536 (10.536)	Loss 8.2252 (7.6090)	CeLoss 0.2715 (0.4014)	SegCLSLoss 0.0486 (0.0599)	KLLoss 0.1348 (0.1534)	MaskLoss 1.3439 (1.1538)	MaskBCELoss 0.4927 (0.3677)	MaskDICELoss 0.8513 (0.7861)
Epoch: [0][116/500]	Time  8.307 ( 8.307)	Loss 8.5712 (3.8229)	CeLoss 0.2012 (0.5484)	SegCLSLoss 0.0815 (0.0308)	KLLoss 0.2188 (0.0789)	MaskLoss 1.1468 (0.5506)	MaskBCELoss 0.1774 (0.2041)	MaskDICELoss 0.9694 (0.3465)
Epoch: [0][117/500]	Time  9.532 ( 9.532)	Loss 1.0703 (5.3990)	CeLoss 1.0703 (0.4572)	SegCLSLoss 0.0000 (0.0440)	KLLoss 0.0000 (0.1224)	MaskLoss 0.0000 (0.7697)	MaskBCELoss 0.0000 (0.2266)	MaskDICELoss 0.0000 (0.5430)
Epoch: [0][118/500]	Time  7.103 ( 7.103)	Loss 0.9727 (2.9144)	CeLoss 0.9727 (0.7598)	SegCLSLoss 0.0000 (0.0187)	KLLoss 0.0000 (0.0514)	MaskLoss 0.0000 (0.4092)	MaskBCELoss 0.0000 (0.1967)	MaskDICELoss 0.0000 (0.2125)
Epoch: [0][119/500]	Time 11.421 (11.421)	Loss 7.5495 (8.2952)	CeLoss 0.3340 (0.2509)	SegCLSLoss 0.0442 (0.0677)	KLLoss 0.1270 (0.1792)	MaskLoss 1.1818 (1.3547)	MaskBCELoss 0.3978 (0.5011)	MaskDICELoss 0.7839 (0.8536)
[2025-03-04 02:25:52,699] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[0.0002983493975903614], mom=[(0.9, 0.95)]
[2025-03-04 02:25:52,705] [INFO] [timer.py:215:stop] epoch=0/micro_step=1200/global_step=120, RunningAvgSamplesPerSec=1.0045596497475207, CurrSamplesPerSec=0.9343907624546052, MemAllocated=57.24GB, MaxMemAllocated=62.78GB
Epoch: [0][120/500]	Time 10.704 (10.704)	Loss 8.2848 (7.1126)	CeLoss 0.2559 (0.3521)	SegCLSLoss 0.0474 (0.0434)	KLLoss 0.1777 (0.1403)	MaskLoss 1.1417 (1.0473)	MaskBCELoss 0.2176 (0.2966)	MaskDICELoss 0.9241 (0.7507)
Epoch: [0][121/500]	Time  7.792 ( 7.792)	Loss 8.5899 (5.3016)	CeLoss 0.2217 (0.6147)	SegCLSLoss 0.0684 (0.0355)	KLLoss 0.2402 (0.1041)	MaskLoss 1.4120 (0.7767)	MaskBCELoss 0.5336 (0.2749)	MaskDICELoss 0.8783 (0.5019)
Epoch: [0][122/500]	Time 10.444 (10.444)	Loss 8.4454 (7.2540)	CeLoss 0.2324 (0.2352)	SegCLSLoss 0.0476 (0.0536)	KLLoss 0.1328 (0.1564)	MaskLoss 1.1373 (1.0861)	MaskBCELoss 0.1736 (0.3088)	MaskDICELoss 0.9637 (0.7773)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([29, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][123/500]	Time  9.663 ( 9.663)	Loss 8.9261 (6.4416)	CeLoss 0.2754 (0.5274)	SegCLSLoss 0.0645 (0.0482)	KLLoss 0.2129 (0.1367)	MaskLoss 1.3683 (0.9119)	MaskBCELoss 0.4239 (0.2570)	MaskDICELoss 0.9444 (0.6549)
Epoch: [0][124/500]	Time 10.818 (10.818)	Loss 8.3753 (7.0029)	CeLoss 0.2715 (0.3664)	SegCLSLoss 0.0664 (0.0539)	KLLoss 0.1885 (0.1461)	MaskLoss 1.2485 (1.0438)	MaskBCELoss 0.3508 (0.3145)	MaskDICELoss 0.8977 (0.7293)
Epoch: [0][125/500]	Time  9.002 ( 9.002)	Loss 0.1270 (3.9095)	CeLoss 0.1270 (0.5808)	SegCLSLoss 0.0000 (0.0232)	KLLoss 0.0000 (0.0687)	MaskLoss 0.0000 (0.4470)	MaskBCELoss 0.0000 (0.0546)	MaskDICELoss 0.0000 (0.3924)
Epoch: [0][126/500]	Time  9.122 ( 9.122)	Loss 7.5561 (4.1874)	CeLoss 0.2109 (0.4985)	SegCLSLoss 0.0447 (0.0248)	KLLoss 0.1143 (0.0743)	MaskLoss 1.0502 (0.5678)	MaskBCELoss 0.1989 (0.1566)	MaskDICELoss 0.8513 (0.4111)
Epoch: [0][127/500]	Time 10.173 (10.173)	Loss 8.5998 (7.1273)	CeLoss 0.2119 (0.3739)	SegCLSLoss 0.0398 (0.0415)	KLLoss 0.1387 (0.1228)	MaskLoss 1.2833 (1.0765)	MaskBCELoss 0.3396 (0.3337)	MaskDICELoss 0.9437 (0.7428)
Epoch: [0][128/500]	Time 10.741 (10.741)	Loss 7.9127 (7.6844)	CeLoss 0.2773 (0.2494)	SegCLSLoss 0.0405 (0.0548)	KLLoss 0.1729 (0.1630)	MaskLoss 1.1656 (1.1210)	MaskBCELoss 0.3134 (0.2871)	MaskDICELoss 0.8521 (0.8338)
Epoch: [0][129/500]	Time  9.863 ( 9.863)	Loss 8.1812 (6.2296)	CeLoss 0.3242 (0.5783)	SegCLSLoss 0.0503 (0.0367)	KLLoss 0.1621 (0.1180)	MaskLoss 1.2578 (0.8826)	MaskBCELoss 0.3988 (0.2577)	MaskDICELoss 0.8590 (0.6249)
[2025-03-04 02:27:28,003] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[0.00029810843373493973], mom=[(0.9, 0.95)]
[2025-03-04 02:27:28,008] [INFO] [timer.py:215:stop] epoch=0/micro_step=1300/global_step=130, RunningAvgSamplesPerSec=1.007928057217973, CurrSamplesPerSec=1.3016403265496326, MemAllocated=56.81GB, MaxMemAllocated=62.78GB
Epoch: [0][130/500]	Time  7.685 ( 7.685)	Loss 6.3939 (3.6297)	CeLoss 0.3242 (0.7125)	SegCLSLoss 0.0566 (0.0190)	KLLoss 0.1387 (0.0554)	MaskLoss 1.0742 (0.4948)	MaskBCELoss 0.4483 (0.1844)	MaskDICELoss 0.6259 (0.3104)
Epoch: [0][131/500]	Time 10.837 (10.837)	Loss 8.5489 (6.6578)	CeLoss 0.2119 (0.2467)	SegCLSLoss 0.0554 (0.0385)	KLLoss 0.1846 (0.1226)	MaskLoss 1.0913 (1.0429)	MaskBCELoss 0.1008 (0.3455)	MaskDICELoss 0.9904 (0.6974)
Epoch: [0][132/500]	Time 10.037 (10.037)	Loss 8.8317 (6.6987)	CeLoss 0.2656 (0.4594)	SegCLSLoss 0.0474 (0.0454)	KLLoss 0.1670 (0.1519)	MaskLoss 1.3127 (0.9796)	MaskBCELoss 0.3545 (0.2955)	MaskDICELoss 0.9582 (0.6842)
Epoch: [0][133/500]	Time  9.819 ( 9.819)	Loss 8.6181 (6.1334)	CeLoss 0.1748 (0.4442)	SegCLSLoss 0.0659 (0.0356)	KLLoss 0.2207 (0.1148)	MaskLoss 1.5251 (0.9050)	MaskBCELoss 0.6687 (0.2806)	MaskDICELoss 0.8564 (0.6244)
Epoch: [0][134/500]	Time 10.001 (10.001)	Loss 5.9370 (6.9665)	CeLoss 0.2617 (0.3473)	SegCLSLoss 0.0286 (0.0434)	KLLoss 0.0840 (0.1411)	MaskLoss 1.1155 (1.0817)	MaskBCELoss 0.5577 (0.3661)	MaskDICELoss 0.5578 (0.7156)
Epoch: [0][135/500]	Time  7.218 ( 7.218)	Loss 7.4828 (4.6522)	CeLoss 0.2227 (0.7271)	SegCLSLoss 0.0339 (0.0204)	KLLoss 0.0708 (0.0585)	MaskLoss 1.3001 (0.6824)	MaskBCELoss 0.5381 (0.2673)	MaskDICELoss 0.7620 (0.4152)
Epoch: [0][136/500]	Time  9.290 ( 9.290)	Loss 8.5707 (5.7430)	CeLoss 0.2617 (0.5643)	SegCLSLoss 0.0505 (0.0360)	KLLoss 0.1426 (0.0971)	MaskLoss 1.3057 (0.8166)	MaskBCELoss 0.3841 (0.2448)	MaskDICELoss 0.9216 (0.5718)
Epoch: [0][137/500]	Time  9.011 ( 9.011)	Loss 8.5249 (6.6579)	CeLoss 0.2275 (0.3214)	SegCLSLoss 0.0559 (0.0489)	KLLoss 0.1875 (0.1489)	MaskLoss 1.0676 (1.0479)	MaskBCELoss 0.0765 (0.3701)	MaskDICELoss 0.9911 (0.6778)
Epoch: [0][138/500]	Time  9.931 ( 9.931)	Loss 0.1611 (6.2127)	CeLoss 0.1611 (0.2993)	SegCLSLoss 0.0000 (0.0396)	KLLoss 0.0000 (0.1147)	MaskLoss 0.0000 (0.9583)	MaskBCELoss 0.0000 (0.3145)	MaskDICELoss 0.0000 (0.6438)
Epoch: [0][139/500]	Time 10.792 (10.792)	Loss 8.9182 (7.6836)	CeLoss 0.2520 (0.3439)	SegCLSLoss 0.0466 (0.0430)	KLLoss 0.1396 (0.1394)	MaskLoss 1.3056 (1.1323)	MaskBCELoss 0.3234 (0.3133)	MaskDICELoss 0.9822 (0.8190)
[2025-03-04 02:29:05,938] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[0.00029786746987951805], mom=[(0.9, 0.95)]
[2025-03-04 02:29:05,942] [INFO] [timer.py:215:stop] epoch=0/micro_step=1400/global_step=140, RunningAvgSamplesPerSec=1.0088817593716741, CurrSamplesPerSec=0.9095441105052863, MemAllocated=57.25GB, MaxMemAllocated=62.78GB
Epoch: [0][140/500]	Time 10.997 (10.997)	Loss 8.4056 (8.0656)	CeLoss 0.2598 (0.2252)	SegCLSLoss 0.0310 (0.0436)	KLLoss 0.0874 (0.1563)	MaskLoss 1.2991 (1.1976)	MaskBCELoss 0.3918 (0.3198)	MaskDICELoss 0.9073 (0.8778)
Epoch: [0][141/500]	Time  9.805 ( 9.805)	Loss 1.4609 (5.1854)	CeLoss 1.4609 (0.4592)	SegCLSLoss 0.0000 (0.0329)	KLLoss 0.0000 (0.1092)	MaskLoss 0.0000 (0.7018)	MaskBCELoss 0.0000 (0.1690)	MaskDICELoss 0.0000 (0.5328)
Epoch: [0][142/500]	Time  8.599 ( 8.599)	Loss 7.8300 (6.4968)	CeLoss 0.1943 (0.4675)	SegCLSLoss 0.0698 (0.0317)	KLLoss 0.1738 (0.0968)	MaskLoss 1.2999 (0.9643)	MaskBCELoss 0.4953 (0.2996)	MaskDICELoss 0.8046 (0.6647)
Epoch: [0][143/500]	Time  7.189 ( 7.189)	Loss 0.7070 (3.2332)	CeLoss 0.7070 (0.8809)	SegCLSLoss 0.0000 (0.0138)	KLLoss 0.0000 (0.0474)	MaskLoss 0.0000 (0.3423)	MaskBCELoss 0.0000 (0.0735)	MaskDICELoss 0.0000 (0.2689)
Epoch: [0][144/500]	Time  9.328 ( 9.328)	Loss 8.6151 (5.8339)	CeLoss 0.1992 (0.4701)	SegCLSLoss 0.0540 (0.0324)	KLLoss 0.1387 (0.1140)	MaskLoss 1.1839 (0.8088)	MaskBCELoss 0.2036 (0.2061)	MaskDICELoss 0.9803 (0.6027)
Epoch: [0][145/500]	Time  9.676 ( 9.676)	Loss 0.2432 (5.6898)	CeLoss 0.2432 (0.3639)	SegCLSLoss 0.0000 (0.0386)	KLLoss 0.0000 (0.1340)	MaskLoss 0.0000 (0.8212)	MaskBCELoss 0.0000 (0.2328)	MaskDICELoss 0.0000 (0.5884)
Epoch: [0][146/500]	Time 10.870 (10.870)	Loss 8.7182 (5.7692)	CeLoss 0.2930 (0.4975)	SegCLSLoss 0.0403 (0.0367)	KLLoss 0.1895 (0.1125)	MaskLoss 1.1831 (0.8084)	MaskBCELoss 0.2077 (0.2211)	MaskDICELoss 0.9753 (0.5873)
Epoch: [0][147/500]	Time 11.688 (11.688)	Loss 8.0414 (7.8331)	CeLoss 0.1992 (0.2563)	SegCLSLoss 0.0894 (0.0456)	KLLoss 0.2344 (0.1385)	MaskLoss 1.3234 (1.1985)	MaskBCELoss 0.5041 (0.3621)	MaskDICELoss 0.8193 (0.8364)
Epoch: [0][148/500]	Time  8.326 ( 8.326)	Loss 8.6962 (6.8828)	CeLoss 0.3262 (0.4107)	SegCLSLoss 0.0894 (0.0435)	KLLoss 0.2256 (0.1389)	MaskLoss 1.2635 (0.9763)	MaskBCELoss 0.3350 (0.2499)	MaskDICELoss 0.9286 (0.7264)
Epoch: [0][149/500]	Time  9.162 ( 9.162)	Loss 7.2243 (7.2040)	CeLoss 0.3223 (0.4080)	SegCLSLoss 0.0342 (0.0386)	KLLoss 0.0972 (0.1216)	MaskLoss 1.2279 (1.1518)	MaskBCELoss 0.5061 (0.4265)	MaskDICELoss 0.7218 (0.7252)
[2025-03-04 02:30:39,453] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[0.0002976265060240964], mom=[(0.9, 0.95)]
[2025-03-04 02:30:39,459] [INFO] [timer.py:215:stop] epoch=0/micro_step=1500/global_step=150, RunningAvgSamplesPerSec=1.0127602145344228, CurrSamplesPerSec=1.1272718633232748, MemAllocated=56.74GB, MaxMemAllocated=62.78GB
Epoch: [0][150/500]	Time  8.873 ( 8.873)	Loss 1.4219 (5.1193)	CeLoss 1.4219 (0.6005)	SegCLSLoss 0.0000 (0.0319)	KLLoss 0.0000 (0.1184)	MaskLoss 0.0000 (0.7243)	MaskBCELoss 0.0000 (0.2350)	MaskDICELoss 0.0000 (0.4893)
Epoch: [0][151/500]	Time  9.636 ( 9.636)	Loss 8.1722 (6.7134)	CeLoss 0.2734 (0.4271)	SegCLSLoss 0.0508 (0.0490)	KLLoss 0.1768 (0.1542)	MaskLoss 1.1366 (1.0111)	MaskBCELoss 0.2329 (0.3301)	MaskDICELoss 0.9037 (0.6809)
Epoch: [0][152/500]	Time  8.537 ( 8.537)	Loss 8.8153 (6.1810)	CeLoss 0.2773 (0.5458)	SegCLSLoss 0.0386 (0.0379)	KLLoss 0.1699 (0.1239)	MaskLoss 1.3628 (0.8549)	MaskBCELoss 0.4257 (0.2244)	MaskDICELoss 0.9371 (0.6304)
Epoch: [0][153/500]	Time 11.216 (11.216)	Loss 8.5437 (7.0093)	CeLoss 0.2461 (0.2500)	SegCLSLoss 0.0361 (0.0423)	KLLoss 0.1055 (0.1321)	MaskLoss 1.0893 (1.0344)	MaskBCELoss 0.0900 (0.2781)	MaskDICELoss 0.9993 (0.7563)
Epoch: [0][154/500]	Time 10.593 (10.593)	Loss 8.4508 (6.3266)	CeLoss 0.2139 (0.4801)	SegCLSLoss 0.0391 (0.0339)	KLLoss 0.1206 (0.1189)	MaskLoss 1.1275 (0.8848)	MaskBCELoss 0.1538 (0.2280)	MaskDICELoss 0.9737 (0.6568)
Epoch: [0][155/500]	Time  9.524 ( 9.524)	Loss 7.7158 (5.7462)	CeLoss 0.4609 (0.5926)	SegCLSLoss 0.0420 (0.0301)	KLLoss 0.1729 (0.1106)	MaskLoss 0.9506 (0.7740)	MaskBCELoss 0.0909 (0.1940)	MaskDICELoss 0.8597 (0.5800)
Epoch: [0][156/500]	Time  9.663 ( 9.663)	Loss 8.0380 (6.1763)	CeLoss 0.2119 (0.5309)	SegCLSLoss 0.0476 (0.0331)	KLLoss 0.1582 (0.1214)	MaskLoss 0.9937 (0.7842)	MaskBCELoss 0.0510 (0.1277)	MaskDICELoss 0.9427 (0.6565)
Epoch: [0][157/500]	Time  9.288 ( 9.288)	Loss 6.6735 (7.2804)	CeLoss 0.2334 (0.3556)	SegCLSLoss 0.0339 (0.0493)	KLLoss 0.1030 (0.1495)	MaskLoss 0.9854 (1.0659)	MaskBCELoss 0.2606 (0.2961)	MaskDICELoss 0.7249 (0.7698)
Epoch: [0][158/500]	Time  8.692 ( 8.692)	Loss 6.5493 (5.4456)	CeLoss 0.3965 (0.4285)	SegCLSLoss 0.0391 (0.0377)	KLLoss 0.1143 (0.1310)	MaskLoss 1.2014 (0.8090)	MaskBCELoss 0.5988 (0.2674)	MaskDICELoss 0.6025 (0.5416)
Epoch: [0][159/500]	Time  8.029 ( 8.029)	Loss 8.8174 (5.7152)	CeLoss 0.2432 (0.7536)	SegCLSLoss 0.0330 (0.0244)	KLLoss 0.1367 (0.0866)	MaskLoss 1.4622 (0.8775)	MaskBCELoss 0.5460 (0.3596)	MaskDICELoss 0.9161 (0.5180)
[2025-03-04 02:32:14,525] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[0.0002973855421686747], mom=[(0.9, 0.95)]
[2025-03-04 02:32:14,531] [INFO] [timer.py:215:stop] epoch=0/micro_step=1600/global_step=160, RunningAvgSamplesPerSec=1.015157247486228, CurrSamplesPerSec=1.0107810274391495, MemAllocated=56.72GB, MaxMemAllocated=62.78GB
Epoch: [0][160/500]	Time  9.895 ( 9.895)	Loss 1.6875 (6.1082)	CeLoss 1.6875 (0.6256)	SegCLSLoss 0.0000 (0.0398)	KLLoss 0.0000 (0.1317)	MaskLoss 0.0000 (0.8736)	MaskBCELoss 0.0000 (0.2763)	MaskDICELoss 0.0000 (0.5973)
Epoch: [0][161/500]	Time  9.134 ( 9.134)	Loss 1.1328 (5.8340)	CeLoss 1.1328 (0.4874)	SegCLSLoss 0.0000 (0.0380)	KLLoss 0.0000 (0.1235)	MaskLoss 0.0000 (0.7822)	MaskBCELoss 0.0000 (0.1756)	MaskDICELoss 0.0000 (0.6066)
Epoch: [0][162/500]	Time  9.509 ( 9.509)	Loss 7.5081 (6.5600)	CeLoss 0.2236 (0.4515)	SegCLSLoss 0.0552 (0.0435)	KLLoss 0.2217 (0.1562)	MaskLoss 1.2837 (0.9464)	MaskBCELoss 0.5390 (0.2734)	MaskDICELoss 0.7447 (0.6730)
Epoch: [0][163/500]	Time  7.154 ( 7.154)	Loss 8.4475 (3.9592)	CeLoss 0.2754 (0.7689)	SegCLSLoss 0.0410 (0.0148)	KLLoss 0.2051 (0.0619)	MaskLoss 1.3102 (0.4689)	MaskBCELoss 0.4224 (0.1051)	MaskDICELoss 0.8878 (0.3638)
Epoch: [0][164/500]	Time  9.397 ( 9.397)	Loss 7.8718 (6.4060)	CeLoss 0.2832 (0.4756)	SegCLSLoss 0.0491 (0.0378)	KLLoss 0.2217 (0.1373)	MaskLoss 1.1497 (0.8563)	MaskBCELoss 0.3095 (0.1794)	MaskDICELoss 0.8402 (0.6769)
Epoch: [0][165/500]	Time  8.520 ( 8.520)	Loss 0.8750 (6.2742)	CeLoss 0.8750 (0.6024)	SegCLSLoss 0.0000 (0.0299)	KLLoss 0.0000 (0.1001)	MaskLoss 0.0000 (0.9010)	MaskBCELoss 0.0000 (0.2754)	MaskDICELoss 0.0000 (0.6257)
Epoch: [0][166/500]	Time  8.721 ( 8.721)	Loss 7.7088 (5.7835)	CeLoss 0.2695 (0.3958)	SegCLSLoss 0.0369 (0.0325)	KLLoss 0.1074 (0.1102)	MaskLoss 0.9828 (0.7840)	MaskBCELoss 0.0914 (0.1684)	MaskDICELoss 0.8914 (0.6156)
Epoch: [0][167/500]	Time 11.024 (11.024)	Loss 8.4441 (7.6713)	CeLoss 0.2236 (0.2706)	SegCLSLoss 0.0576 (0.0478)	KLLoss 0.2207 (0.1662)	MaskLoss 1.1968 (1.1424)	MaskBCELoss 0.2671 (0.3215)	MaskDICELoss 0.9296 (0.8210)
Epoch: [0][168/500]	Time 10.342 (10.342)	Loss 8.1027 (7.0805)	CeLoss 0.2178 (0.3494)	SegCLSLoss 0.0454 (0.0391)	KLLoss 0.2002 (0.1533)	MaskLoss 1.2468 (1.0335)	MaskBCELoss 0.3856 (0.2849)	MaskDICELoss 0.8613 (0.7485)
Epoch: [0][169/500]	Time 11.252 (11.252)	Loss 7.8524 (6.9525)	CeLoss 0.2500 (0.3005)	SegCLSLoss 0.0383 (0.0333)	KLLoss 0.1738 (0.1391)	MaskLoss 1.2441 (1.0246)	MaskBCELoss 0.4239 (0.2835)	MaskDICELoss 0.8202 (0.7411)
[2025-03-04 02:33:48,181] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[0.000297144578313253], mom=[(0.9, 0.95)]
[2025-03-04 02:33:48,187] [INFO] [timer.py:215:stop] epoch=0/micro_step=1700/global_step=170, RunningAvgSamplesPerSec=1.0181515900940228, CurrSamplesPerSec=1.1628392602416424, MemAllocated=57.47GB, MaxMemAllocated=62.78GB
Epoch: [0][170/500]	Time  8.602 ( 8.602)	Loss 7.0598 (6.3225)	CeLoss 0.1807 (0.4022)	SegCLSLoss 0.0447 (0.0308)	KLLoss 0.1816 (0.1225)	MaskLoss 0.9615 (1.0398)	MaskBCELoss 0.1695 (0.4227)	MaskDICELoss 0.7920 (0.6171)
Epoch: [0][171/500]	Time 10.515 (10.515)	Loss 7.4816 (6.3193)	CeLoss 0.2373 (0.4513)	SegCLSLoss 0.0300 (0.0289)	KLLoss 0.0869 (0.1029)	MaskLoss 1.1183 (0.9279)	MaskBCELoss 0.3008 (0.2788)	MaskDICELoss 0.8175 (0.6491)
Epoch: [0][172/500]	Time 11.049 (11.049)	Loss 9.2061 (6.7006)	CeLoss 0.3965 (0.2334)	SegCLSLoss 0.0527 (0.0357)	KLLoss 0.2148 (0.1481)	MaskLoss 1.3534 (1.0237)	MaskBCELoss 0.3764 (0.3147)	MaskDICELoss 0.9771 (0.7089)
Epoch: [0][173/500]	Time  9.703 ( 9.703)	Loss 7.8961 (6.2212)	CeLoss 0.2344 (0.4120)	SegCLSLoss 0.0303 (0.0257)	KLLoss 0.1543 (0.1117)	MaskLoss 1.2780 (0.9305)	MaskBCELoss 0.4554 (0.2933)	MaskDICELoss 0.8226 (0.6372)
Epoch: [0][174/500]	Time 11.683 (11.683)	Loss 8.5114 (7.4130)	CeLoss 0.2236 (0.3135)	SegCLSLoss 0.0308 (0.0339)	KLLoss 0.1318 (0.1189)	MaskLoss 1.0816 (1.1343)	MaskBCELoss 0.0854 (0.3518)	MaskDICELoss 0.9962 (0.7825)
Epoch: [0][175/500]	Time 10.651 (10.651)	Loss 8.6271 (5.7830)	CeLoss 0.2002 (0.4322)	SegCLSLoss 0.0366 (0.0181)	KLLoss 0.1592 (0.0748)	MaskLoss 1.1458 (0.8510)	MaskBCELoss 0.1530 (0.2569)	MaskDICELoss 0.9928 (0.5941)
Epoch: [0][176/500]	Time  9.806 ( 9.806)	Loss 7.7904 (5.9231)	CeLoss 0.1865 (0.3313)	SegCLSLoss 0.0535 (0.0313)	KLLoss 0.2480 (0.1382)	MaskLoss 1.2048 (0.8670)	MaskBCELoss 0.3848 (0.2497)	MaskDICELoss 0.8200 (0.6173)
Epoch: [0][177/500]	Time 10.633 (10.633)	Loss 6.5766 (7.2493)	CeLoss 0.2617 (0.3420)	SegCLSLoss 0.0237 (0.0321)	KLLoss 0.0859 (0.1404)	MaskLoss 0.8485 (0.9515)	MaskBCELoss 0.0952 (0.1435)	MaskDICELoss 0.7533 (0.8080)
Epoch: [0][178/500]	Time  9.876 ( 9.876)	Loss 8.6802 (6.7027)	CeLoss 0.3125 (0.4545)	SegCLSLoss 0.0393 (0.0279)	KLLoss 0.1748 (0.1101)	MaskLoss 1.0964 (0.9989)	MaskBCELoss 0.0999 (0.3111)	MaskDICELoss 0.9966 (0.6877)
Epoch: [0][179/500]	Time  9.324 ( 9.324)	Loss 8.3064 (4.4283)	CeLoss 0.2148 (0.4467)	SegCLSLoss 0.0544 (0.0241)	KLLoss 0.2031 (0.0933)	MaskLoss 1.2960 (0.6148)	MaskBCELoss 0.4178 (0.1737)	MaskDICELoss 0.8782 (0.4411)
[2025-03-04 02:35:30,644] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[0.0002969036144578313], mom=[(0.9, 0.95)]
[2025-03-04 02:35:30,649] [INFO] [timer.py:215:stop] epoch=0/micro_step=1800/global_step=180, RunningAvgSamplesPerSec=1.015694211600049, CurrSamplesPerSec=1.0845136929193215, MemAllocated=57.26GB, MaxMemAllocated=62.78GB
Epoch: [0][180/500]	Time  9.223 ( 9.223)	Loss 8.7391 (5.9650)	CeLoss 0.2734 (0.4787)	SegCLSLoss 0.0452 (0.0244)	KLLoss 0.1777 (0.0967)	MaskLoss 1.1976 (0.9041)	MaskBCELoss 0.2193 (0.3092)	MaskDICELoss 0.9782 (0.5949)
Epoch: [0][181/500]	Time  9.391 ( 9.391)	Loss 6.3969 (4.6349)	CeLoss 0.2559 (0.4764)	SegCLSLoss 0.0238 (0.0163)	KLLoss 0.1187 (0.0734)	MaskLoss 1.0261 (0.6392)	MaskBCELoss 0.3664 (0.1728)	MaskDICELoss 0.6597 (0.4664)
Epoch: [0][182/500]	Time  7.977 ( 7.977)	Loss 8.5272 (4.9124)	CeLoss 0.4043 (0.7611)	SegCLSLoss 0.0297 (0.0175)	KLLoss 0.1279 (0.0756)	MaskLoss 1.0913 (0.6527)	MaskBCELoss 0.1251 (0.1925)	MaskDICELoss 0.9663 (0.4603)
Epoch: [0][183/500]	Time 10.323 (10.323)	Loss 8.4852 (7.2807)	CeLoss 0.3750 (0.3896)	SegCLSLoss 0.0422 (0.0380)	KLLoss 0.2051 (0.1624)	MaskLoss 1.2964 (1.0984)	MaskBCELoss 0.4146 (0.3463)	MaskDICELoss 0.8818 (0.7521)
Epoch: [0][184/500]	Time 11.219 (11.219)	Loss 8.1862 (6.9170)	CeLoss 0.2197 (0.4266)	SegCLSLoss 0.0630 (0.0318)	KLLoss 0.2246 (0.1237)	MaskLoss 1.2609 (1.0361)	MaskBCELoss 0.3963 (0.3229)	MaskDICELoss 0.8646 (0.7131)
Epoch: [0][185/500]	Time  8.287 ( 8.287)	Loss 8.2451 (5.9050)	CeLoss 0.3359 (0.4788)	SegCLSLoss 0.0344 (0.0310)	KLLoss 0.1865 (0.1260)	MaskLoss 1.2927 (0.9876)	MaskBCELoss 0.4393 (0.4360)	MaskDICELoss 0.8534 (0.5516)
Epoch: [0][186/500]	Time 10.682 (10.682)	Loss 1.4375 (6.5661)	CeLoss 1.4375 (0.3878)	SegCLSLoss 0.0000 (0.0293)	KLLoss 0.0000 (0.1257)	MaskLoss 0.0000 (0.9694)	MaskBCELoss 0.0000 (0.2862)	MaskDICELoss 0.0000 (0.6832)
Epoch: [0][187/500]	Time 11.965 (11.965)	Loss 8.4239 (7.9322)	CeLoss 0.2070 (0.2398)	SegCLSLoss 0.0425 (0.0357)	KLLoss 0.1846 (0.1531)	MaskLoss 1.1386 (1.2100)	MaskBCELoss 0.1831 (0.3597)	MaskDICELoss 0.9554 (0.8503)
Epoch: [0][188/500]	Time  9.259 ( 9.259)	Loss 0.0928 (5.3307)	CeLoss 0.0928 (0.5160)	SegCLSLoss 0.0000 (0.0304)	KLLoss 0.0000 (0.1182)	MaskLoss 0.0000 (0.7230)	MaskBCELoss 0.0000 (0.1837)	MaskDICELoss 0.0000 (0.5392)
Epoch: [0][189/500]	Time  7.994 ( 7.994)	Loss 1.2266 (4.7719)	CeLoss 1.2266 (0.7224)	SegCLSLoss 0.0000 (0.0200)	KLLoss 0.0000 (0.0866)	MaskLoss 0.0000 (0.5738)	MaskBCELoss 0.0000 (0.1063)	MaskDICELoss 0.0000 (0.4675)
[2025-03-04 02:37:06,033] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[0.0002966626506024096], mom=[(0.9, 0.95)]
[2025-03-04 02:37:06,037] [INFO] [timer.py:215:stop] epoch=0/micro_step=1900/global_step=190, RunningAvgSamplesPerSec=1.0173882614329914, CurrSamplesPerSec=1.2065154128939304, MemAllocated=57.45GB, MaxMemAllocated=62.78GB
Epoch: [0][190/500]	Time  8.290 ( 8.290)	Loss 7.9736 (5.3566)	CeLoss 0.2061 (0.5786)	SegCLSLoss 0.0312 (0.0229)	KLLoss 0.1235 (0.0845)	MaskLoss 1.0515 (0.7064)	MaskBCELoss 0.1307 (0.1615)	MaskDICELoss 0.9208 (0.5449)
Epoch: [0][191/500]	Time 10.233 (10.233)	Loss 8.3916 (8.0084)	CeLoss 0.2119 (0.2700)	SegCLSLoss 0.0464 (0.0337)	KLLoss 0.1738 (0.1217)	MaskLoss 1.0414 (1.2541)	MaskBCELoss 0.0584 (0.4056)	MaskDICELoss 0.9831 (0.8486)
Epoch: [0][192/500]	Time  8.965 ( 8.965)	Loss 7.4332 (5.2770)	CeLoss 0.2236 (0.6289)	SegCLSLoss 0.0508 (0.0258)	KLLoss 0.1885 (0.0918)	MaskLoss 0.9140 (0.6654)	MaskBCELoss 0.0527 (0.1300)	MaskDICELoss 0.8613 (0.5354)
Epoch: [0][193/500]	Time 10.605 (10.605)	Loss 8.3667 (6.7063)	CeLoss 0.2617 (0.4382)	SegCLSLoss 0.0454 (0.0404)	KLLoss 0.1592 (0.1326)	MaskLoss 1.0197 (0.9562)	MaskBCELoss 0.0394 (0.2558)	MaskDICELoss 0.9803 (0.7004)
Epoch: [0][194/500]	Time  9.656 ( 9.656)	Loss 1.7031 (5.3508)	CeLoss 1.7031 (0.6203)	SegCLSLoss 0.0000 (0.0308)	KLLoss 0.0000 (0.1047)	MaskLoss 0.0000 (0.6775)	MaskBCELoss 0.0000 (0.1350)	MaskDICELoss 0.0000 (0.5426)
Epoch: [0][195/500]	Time 10.281 (10.281)	Loss 7.9082 (6.4332)	CeLoss 0.3223 (0.3662)	SegCLSLoss 0.0388 (0.0336)	KLLoss 0.1543 (0.1323)	MaskLoss 1.3457 (0.9976)	MaskBCELoss 0.5589 (0.3438)	MaskDICELoss 0.7868 (0.6538)
Epoch: [0][196/500]	Time  7.664 ( 7.664)	Loss 1.0625 (4.4837)	CeLoss 1.0625 (0.6680)	SegCLSLoss 0.0000 (0.0214)	KLLoss 0.0000 (0.0824)	MaskLoss 0.0000 (0.6201)	MaskBCELoss 0.0000 (0.2064)	MaskDICELoss 0.0000 (0.4137)
Epoch: [0][197/500]	Time  8.675 ( 8.675)	Loss 7.5135 (6.5284)	CeLoss 0.2402 (0.5187)	SegCLSLoss 0.0309 (0.0326)	KLLoss 0.0469 (0.1128)	MaskLoss 1.1534 (0.9997)	MaskBCELoss 0.3361 (0.3528)	MaskDICELoss 0.8173 (0.6469)
Epoch: [0][198/500]	Time  9.889 ( 9.889)	Loss 8.9999 (8.2311)	CeLoss 0.2314 (0.2312)	SegCLSLoss 0.0583 (0.0471)	KLLoss 0.1973 (0.1775)	MaskLoss 1.4839 (1.3433)	MaskBCELoss 0.5548 (0.4912)	MaskDICELoss 0.9292 (0.8521)
Epoch: [0][199/500]	Time  7.247 ( 7.247)	Loss 8.8277 (4.5955)	CeLoss 0.2168 (0.6313)	SegCLSLoss 0.0481 (0.0203)	KLLoss 0.1680 (0.0800)	MaskLoss 1.2117 (0.6159)	MaskBCELoss 0.2127 (0.1756)	MaskDICELoss 0.9990 (0.4403)
[2025-03-04 02:38:38,681] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[0.00029642168674698794], mom=[(0.9, 0.95)]
[2025-03-04 02:38:38,687] [INFO] [timer.py:215:stop] epoch=0/micro_step=2000/global_step=200, RunningAvgSamplesPerSec=1.020354457316843, CurrSamplesPerSec=1.0601881882161217, MemAllocated=57.25GB, MaxMemAllocated=62.78GB
Epoch: [0][200/500]	Time  9.434 ( 9.434)	Loss 6.7744 (5.3694)	CeLoss 0.2793 (0.4127)	SegCLSLoss 0.0271 (0.0288)	KLLoss 0.1250 (0.1045)	MaskLoss 1.2245 (0.8281)	MaskBCELoss 0.5732 (0.2978)	MaskDICELoss 0.6512 (0.5302)
Epoch: [0][201/500]	Time 10.849 (10.849)	Loss 7.6385 (7.7077)	CeLoss 0.3496 (0.3003)	SegCLSLoss 0.0234 (0.0380)	KLLoss 0.0947 (0.1450)	MaskLoss 1.5650 (1.2383)	MaskBCELoss 0.8898 (0.4439)	MaskDICELoss 0.6752 (0.7944)
Epoch: [0][202/500]	Time  7.962 ( 7.962)	Loss 8.4871 (6.8435)	CeLoss 0.2051 (0.5034)	SegCLSLoss 0.0500 (0.0283)	KLLoss 0.2178 (0.1200)	MaskLoss 1.0550 (0.9208)	MaskBCELoss 0.0671 (0.1935)	MaskDICELoss 0.9880 (0.7274)
Epoch: [0][203/500]	Time  9.427 ( 9.427)	Loss 7.7875 (5.9563)	CeLoss 0.1406 (0.5731)	SegCLSLoss 0.0820 (0.0326)	KLLoss 0.2285 (0.1191)	MaskLoss 1.4173 (0.8293)	MaskBCELoss 0.6601 (0.2311)	MaskDICELoss 0.7571 (0.5982)
Epoch: [0][204/500]	Time 10.610 (10.610)	Loss 1.5703 (7.4873)	CeLoss 1.5703 (0.3658)	SegCLSLoss 0.0000 (0.0388)	KLLoss 0.0000 (0.1620)	MaskLoss 0.0000 (1.0780)	MaskBCELoss 0.0000 (0.2806)	MaskDICELoss 0.0000 (0.7974)
Epoch: [0][205/500]	Time  9.891 ( 9.891)	Loss 8.5786 (6.6284)	CeLoss 0.1562 (0.4925)	SegCLSLoss 0.0723 (0.0298)	KLLoss 0.2422 (0.1437)	MaskLoss 1.2016 (0.9133)	MaskBCELoss 0.2447 (0.2216)	MaskDICELoss 0.9569 (0.6918)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([25, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][206/500]	Time  9.386 ( 9.386)	Loss 0.1079 (6.0581)	CeLoss 0.1079 (0.3055)	SegCLSLoss 0.0000 (0.0337)	KLLoss 0.0000 (0.1279)	MaskLoss 0.0000 (0.8805)	MaskBCELoss 0.0000 (0.2393)	MaskDICELoss 0.0000 (0.6412)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([28, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][207/500]	Time 10.401 (10.401)	Loss 1.1797 (5.1070)	CeLoss 1.1797 (0.5251)	SegCLSLoss 0.0000 (0.0276)	KLLoss 0.0000 (0.1142)	MaskLoss 0.0000 (0.7627)	MaskBCELoss 0.0000 (0.2747)	MaskDICELoss 0.0000 (0.4881)
Epoch: [0][208/500]	Time  8.757 ( 8.757)	Loss 5.6742 (5.2707)	CeLoss 0.2334 (0.4483)	SegCLSLoss 0.0244 (0.0212)	KLLoss 0.1455 (0.1136)	MaskLoss 0.8794 (0.7691)	MaskBCELoss 0.2920 (0.2424)	MaskDICELoss 0.5875 (0.5267)
Epoch: [0][209/500]	Time  9.270 ( 9.270)	Loss 8.6840 (5.7457)	CeLoss 0.3105 (0.4260)	SegCLSLoss 0.0272 (0.0191)	KLLoss 0.1562 (0.1000)	MaskLoss 1.1461 (0.7848)	MaskBCELoss 0.1609 (0.1782)	MaskDICELoss 0.9852 (0.6067)
[2025-03-04 02:40:14,067] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=0, lr=[0.0002961807228915662], mom=[(0.9, 0.95)]
[2025-03-04 02:40:14,073] [INFO] [timer.py:215:stop] epoch=0/micro_step=2100/global_step=210, RunningAvgSamplesPerSec=1.0216762830558626, CurrSamplesPerSec=1.132469939341379, MemAllocated=56.72GB, MaxMemAllocated=62.78GB
Epoch: [0][210/500]	Time  8.832 ( 8.832)	Loss 1.0391 (6.4783)	CeLoss 1.0391 (0.4698)	SegCLSLoss 0.0000 (0.0291)	KLLoss 0.0000 (0.1363)	MaskLoss 0.0000 (0.9917)	MaskBCELoss 0.0000 (0.3459)	MaskDICELoss 0.0000 (0.6457)
Epoch: [0][211/500]	Time 11.003 (11.003)	Loss 8.1978 (7.7533)	CeLoss 0.1982 (0.2770)	SegCLSLoss 0.0349 (0.0306)	KLLoss 0.1709 (0.1630)	MaskLoss 1.0748 (1.1053)	MaskBCELoss 0.1312 (0.2574)	MaskDICELoss 0.9436 (0.8479)
Epoch: [0][212/500]	Time  9.583 ( 9.583)	Loss 8.5917 (5.0514)	CeLoss 0.2100 (0.4272)	SegCLSLoss 0.0330 (0.0220)	KLLoss 0.1865 (0.1106)	MaskLoss 1.2159 (0.7039)	MaskBCELoss 0.2583 (0.1881)	MaskDICELoss 0.9576 (0.5158)
Epoch: [0][213/500]	Time  8.001 ( 8.001)	Loss 8.4917 (6.0089)	CeLoss 0.3145 (0.5646)	SegCLSLoss 0.0339 (0.0230)	KLLoss 0.1875 (0.1158)	MaskLoss 1.1217 (0.8094)	MaskBCELoss 0.1669 (0.1930)	MaskDICELoss 0.9548 (0.6163)
Epoch: [0][214/500]	Time 10.590 (10.590)	Loss 8.4853 (5.9891)	CeLoss 0.2061 (0.4207)	SegCLSLoss 0.0432 (0.0227)	KLLoss 0.2266 (0.1227)	MaskLoss 1.0549 (0.8683)	MaskBCELoss 0.0678 (0.2520)	MaskDICELoss 0.9871 (0.6163)
Epoch: [0][215/500]	Time 10.012 (10.012)	Loss 9.4795 (7.4695)	CeLoss 0.2715 (0.3092)	SegCLSLoss 0.0383 (0.0278)	KLLoss 0.2266 (0.1448)	MaskLoss 1.6443 (1.2251)	MaskBCELoss 0.6984 (0.4666)	MaskDICELoss 0.9459 (0.7585)
Epoch: [0][216/500]	Time  9.187 ( 9.187)	Loss 5.4325 (7.6115)	CeLoss 0.3008 (0.2446)	SegCLSLoss 0.0162 (0.0398)	KLLoss 0.0825 (0.1728)	MaskLoss 0.9691 (1.1445)	MaskBCELoss 0.4518 (0.3303)	MaskDICELoss 0.5173 (0.8142)
Epoch: [0][217/500]	Time 10.696 (10.696)	Loss 0.0723 (5.2671)	CeLoss 0.0723 (0.4861)	SegCLSLoss 0.0000 (0.0176)	KLLoss 0.0000 (0.0838)	MaskLoss 0.0000 (0.6944)	MaskBCELoss 0.0000 (0.1445)	MaskDICELoss 0.0000 (0.5500)
Epoch: [0][218/500]	Time  9.933 ( 9.933)	Loss 6.7474 (7.3498)	CeLoss 0.3066 (0.3941)	SegCLSLoss 0.0308 (0.0256)	KLLoss 0.1543 (0.1259)	MaskLoss 0.8420 (1.0306)	MaskBCELoss 0.0775 (0.2380)	MaskDICELoss 0.7645 (0.7926)
Epoch: [0][219/500]	Time  9.977 ( 9.977)	Loss 7.0074 (6.1741)	CeLoss 0.2383 (0.4458)	SegCLSLoss 0.0247 (0.0272)	KLLoss 0.1064 (0.1243)	MaskLoss 1.1454 (0.9536)	MaskBCELoss 0.4185 (0.3397)	MaskDICELoss 0.7269 (0.6139)
[2025-03-04 02:41:52,808] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=0, lr=[0.00029593975903614454], mom=[(0.9, 0.95)]
[2025-03-04 02:41:52,814] [INFO] [timer.py:215:stop] epoch=0/micro_step=2200/global_step=220, RunningAvgSamplesPerSec=1.021271055788854, CurrSamplesPerSec=1.025003808050594, MemAllocated=57.25GB, MaxMemAllocated=62.78GB
Epoch: [0][220/500]	Time  9.758 ( 9.758)	Loss 7.8045 (5.7385)	CeLoss 0.3105 (0.5897)	SegCLSLoss 0.0217 (0.0193)	KLLoss 0.0581 (0.0735)	MaskLoss 1.1361 (0.8107)	MaskBCELoss 0.2776 (0.2367)	MaskDICELoss 0.8586 (0.5740)
Epoch: [0][221/500]	Time 11.243 (11.243)	Loss 8.1395 (6.6993)	CeLoss 0.1621 (0.3181)	SegCLSLoss 0.0581 (0.0301)	KLLoss 0.2373 (0.1241)	MaskLoss 1.3356 (0.9629)	MaskBCELoss 0.4959 (0.2436)	MaskDICELoss 0.8398 (0.7194)
Epoch: [0][222/500]	Time  9.710 ( 9.710)	Loss 8.1227 (4.1080)	CeLoss 0.1875 (0.3168)	SegCLSLoss 0.0476 (0.0218)	KLLoss 0.1865 (0.0865)	MaskLoss 0.9826 (0.5681)	MaskBCELoss 0.0227 (0.1419)	MaskDICELoss 0.9599 (0.4262)
Epoch: [0][223/500]	Time  9.630 ( 9.630)	Loss 7.2642 (7.3303)	CeLoss 0.2080 (0.2131)	SegCLSLoss 0.0273 (0.0330)	KLLoss 0.0830 (0.1275)	MaskLoss 1.1080 (1.0586)	MaskBCELoss 0.3174 (0.2494)	MaskDICELoss 0.7906 (0.8093)
Epoch: [0][224/500]	Time  9.500 ( 9.500)	Loss 7.2786 (6.1223)	CeLoss 0.2598 (0.3675)	SegCLSLoss 0.0498 (0.0266)	KLLoss 0.2207 (0.1121)	MaskLoss 1.2674 (0.8851)	MaskBCELoss 0.5613 (0.2420)	MaskDICELoss 0.7060 (0.6431)
Epoch: [0][225/500]	Time  9.383 ( 9.383)	Loss 8.1649 (6.8817)	CeLoss 0.3398 (0.5172)	SegCLSLoss 0.0233 (0.0297)	KLLoss 0.1221 (0.1148)	MaskLoss 1.4891 (1.1224)	MaskBCELoss 0.7034 (0.4573)	MaskDICELoss 0.7857 (0.6650)
Epoch: [0][226/500]	Time  9.677 ( 9.677)	Loss 8.1101 (5.9378)	CeLoss 0.3242 (0.5358)	SegCLSLoss 0.0256 (0.0224)	KLLoss 0.0757 (0.0978)	MaskLoss 1.3656 (0.8956)	MaskBCELoss 0.5381 (0.3121)	MaskDICELoss 0.8275 (0.5836)
Epoch: [0][227/500]	Time  9.798 ( 9.798)	Loss 8.6011 (6.2039)	CeLoss 0.2793 (0.5755)	SegCLSLoss 0.0239 (0.0259)	KLLoss 0.0674 (0.1107)	MaskLoss 1.2021 (0.8118)	MaskBCELoss 0.2291 (0.1649)	MaskDICELoss 0.9729 (0.6469)
Epoch: [0][228/500]	Time 11.453 (11.453)	Loss 8.6274 (7.2460)	CeLoss 0.2812 (0.4006)	SegCLSLoss 0.0347 (0.0275)	KLLoss 0.1494 (0.1002)	MaskLoss 1.1058 (1.1147)	MaskBCELoss 0.1113 (0.3643)	MaskDICELoss 0.9944 (0.7504)
Epoch: [0][229/500]	Time  7.938 ( 7.938)	Loss 7.5355 (4.6032)	CeLoss 0.2441 (0.5912)	SegCLSLoss 0.0322 (0.0207)	KLLoss 0.1777 (0.1010)	MaskLoss 1.1846 (0.6208)	MaskBCELoss 0.3964 (0.1777)	MaskDICELoss 0.7881 (0.4431)
[2025-03-04 02:43:31,756] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=0, lr=[0.00029569879518072286], mom=[(0.9, 0.95)]
[2025-03-04 02:43:31,762] [INFO] [timer.py:215:stop] epoch=0/micro_step=2300/global_step=230, RunningAvgSamplesPerSec=1.0208072293149841, CurrSamplesPerSec=0.9421970320967807, MemAllocated=57.34GB, MaxMemAllocated=62.78GB
Epoch: [0][230/500]	Time 10.616 (10.616)	Loss 8.0864 (7.4103)	CeLoss 0.2520 (0.3015)	SegCLSLoss 0.0242 (0.0319)	KLLoss 0.1030 (0.1517)	MaskLoss 1.1335 (0.9953)	MaskBCELoss 0.2248 (0.1702)	MaskDICELoss 0.9087 (0.8251)
Epoch: [0][231/500]	Time  9.924 ( 9.924)	Loss 7.6118 (7.3174)	CeLoss 0.3555 (0.3983)	SegCLSLoss 0.0211 (0.0282)	KLLoss 0.0747 (0.1319)	MaskLoss 1.2519 (1.0345)	MaskBCELoss 0.4739 (0.2504)	MaskDICELoss 0.7781 (0.7840)
Epoch: [0][232/500]	Time  9.531 ( 9.531)	Loss 5.8227 (5.8469)	CeLoss 0.3184 (0.5815)	SegCLSLoss 0.0204 (0.0255)	KLLoss 0.0674 (0.1186)	MaskLoss 1.0147 (0.7442)	MaskBCELoss 0.4483 (0.1366)	MaskDICELoss 0.5665 (0.6076)
Epoch: [0][233/500]	Time  9.891 ( 9.891)	Loss 0.0933 (4.6866)	CeLoss 0.0933 (0.4224)	SegCLSLoss 0.0000 (0.0176)	KLLoss 0.0000 (0.0907)	MaskLoss 0.0000 (0.6448)	MaskBCELoss 0.0000 (0.1656)	MaskDICELoss 0.0000 (0.4793)
Epoch: [0][234/500]	Time 12.740 (12.740)	Loss 8.0126 (5.1488)	CeLoss 0.2246 (0.2730)	SegCLSLoss 0.0415 (0.0187)	KLLoss 0.2061 (0.0921)	MaskLoss 1.1574 (0.7136)	MaskBCELoss 0.2830 (0.1558)	MaskDICELoss 0.8744 (0.5579)
Epoch: [0][235/500]	Time  8.364 ( 8.364)	Loss 1.6250 (6.0739)	CeLoss 1.6250 (0.5120)	SegCLSLoss 0.0000 (0.0255)	KLLoss 0.0000 (0.1224)	MaskLoss 0.0000 (0.9104)	MaskBCELoss 0.0000 (0.3094)	MaskDICELoss 0.0000 (0.6010)
Epoch: [0][236/500]	Time 10.952 (10.952)	Loss 8.1444 (6.9881)	CeLoss 0.2080 (0.3439)	SegCLSLoss 0.0288 (0.0230)	KLLoss 0.1582 (0.1326)	MaskLoss 1.0481 (1.0321)	MaskBCELoss 0.1035 (0.2928)	MaskDICELoss 0.9446 (0.7393)
Epoch: [0][237/500]	Time  9.958 ( 9.958)	Loss 8.6307 (6.6490)	CeLoss 0.3086 (0.3694)	SegCLSLoss 0.0332 (0.0240)	KLLoss 0.1680 (0.1193)	MaskLoss 1.0702 (0.9564)	MaskBCELoss 0.0705 (0.2505)	MaskDICELoss 0.9997 (0.7059)
Epoch: [0][238/500]	Time  8.844 ( 8.844)	Loss 1.3047 (4.9325)	CeLoss 1.3047 (0.4271)	SegCLSLoss 0.0000 (0.0173)	KLLoss 0.0000 (0.0908)	MaskLoss 0.0000 (0.6150)	MaskBCELoss 0.0000 (0.0857)	MaskDICELoss 0.0000 (0.5293)
Epoch: [0][239/500]	Time  9.639 ( 9.639)	Loss 7.6003 (6.0132)	CeLoss 0.2715 (0.3822)	SegCLSLoss 0.0356 (0.0226)	KLLoss 0.2061 (0.1221)	MaskLoss 1.2551 (0.8869)	MaskBCELoss 0.4894 (0.2662)	MaskDICELoss 0.7657 (0.6206)
[2025-03-04 02:45:11,607] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=0, lr=[0.0002954578313253012], mom=[(0.9, 0.95)]
[2025-03-04 02:45:11,612] [INFO] [timer.py:215:stop] epoch=0/micro_step=2400/global_step=240, RunningAvgSamplesPerSec=1.0199896620538, CurrSamplesPerSec=0.999876490590626, MemAllocated=57.28GB, MaxMemAllocated=62.78GB
Epoch: [0][240/500]	Time 10.007 (10.007)	Loss 7.7021 (6.9439)	CeLoss 0.2070 (0.3156)	SegCLSLoss 0.0483 (0.0292)	KLLoss 0.2188 (0.1668)	MaskLoss 1.1667 (1.0174)	MaskBCELoss 0.3468 (0.2820)	MaskDICELoss 0.8199 (0.7354)
Epoch: [0][241/500]	Time 11.734 (11.734)	Loss 6.9991 (6.2198)	CeLoss 0.2344 (0.3354)	SegCLSLoss 0.0153 (0.0247)	KLLoss 0.0986 (0.1238)	MaskLoss 1.1326 (0.8714)	MaskBCELoss 0.4002 (0.2037)	MaskDICELoss 0.7324 (0.6676)
Epoch: [0][242/500]	Time 10.345 (10.345)	Loss 8.1447 (5.8111)	CeLoss 0.2578 (0.3648)	SegCLSLoss 0.0258 (0.0201)	KLLoss 0.1396 (0.1071)	MaskLoss 1.0875 (0.8507)	MaskBCELoss 0.1609 (0.2461)	MaskDICELoss 0.9266 (0.6046)
Epoch: [0][243/500]	Time  8.856 ( 8.856)	Loss 1.6562 (5.6403)	CeLoss 1.6562 (0.5655)	SegCLSLoss 0.0000 (0.0199)	KLLoss 0.0000 (0.1008)	MaskLoss 0.0000 (0.7728)	MaskBCELoss 0.0000 (0.2031)	MaskDICELoss 0.0000 (0.5697)
Epoch: [0][244/500]	Time  9.361 ( 9.361)	Loss 9.0728 (7.4741)	CeLoss 0.1289 (0.3304)	SegCLSLoss 0.0664 (0.0349)	KLLoss 0.2324 (0.1588)	MaskLoss 1.4338 (1.1257)	MaskBCELoss 0.4654 (0.3397)	MaskDICELoss 0.9684 (0.7860)
Epoch: [0][245/500]	Time 10.063 (10.063)	Loss 7.4097 (7.2790)	CeLoss 0.2266 (0.3187)	SegCLSLoss 0.0396 (0.0280)	KLLoss 0.1895 (0.1571)	MaskLoss 0.9584 (0.9955)	MaskBCELoss 0.1155 (0.1959)	MaskDICELoss 0.8429 (0.7996)
Epoch: [0][246/500]	Time 10.151 (10.151)	Loss 7.2101 (6.1071)	CeLoss 0.2158 (0.4007)	SegCLSLoss 0.0464 (0.0269)	KLLoss 0.2383 (0.1424)	MaskLoss 1.1882 (0.8972)	MaskBCELoss 0.4620 (0.2712)	MaskDICELoss 0.7262 (0.6261)
Epoch: [0][247/500]	Time  9.253 ( 9.253)	Loss 8.5478 (4.7295)	CeLoss 0.2891 (0.5749)	SegCLSLoss 0.0376 (0.0146)	KLLoss 0.2246 (0.0859)	MaskLoss 1.3279 (0.6456)	MaskBCELoss 0.4344 (0.1840)	MaskDICELoss 0.8935 (0.4617)
Epoch: [0][248/500]	Time  9.436 ( 9.436)	Loss 1.1484 (5.0315)	CeLoss 1.1484 (0.5583)	SegCLSLoss 0.0000 (0.0212)	KLLoss 0.0000 (0.1075)	MaskLoss 0.0000 (0.6673)	MaskBCELoss 0.0000 (0.1638)	MaskDICELoss 0.0000 (0.5034)
Epoch: [0][249/500]	Time 10.988 (10.988)	Loss 8.5409 (6.9909)	CeLoss 0.1816 (0.2991)	SegCLSLoss 0.0308 (0.0257)	KLLoss 0.1680 (0.1478)	MaskLoss 1.1072 (1.0255)	MaskBCELoss 0.1137 (0.2788)	MaskDICELoss 0.9935 (0.7468)
[2025-03-04 02:46:52,781] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=0, lr=[0.0002952168674698795], mom=[(0.9, 0.95)]
[2025-03-04 02:46:52,787] [INFO] [timer.py:215:stop] epoch=0/micro_step=2500/global_step=250, RunningAvgSamplesPerSec=1.0186840323239816, CurrSamplesPerSec=0.9102653057428781, MemAllocated=57.25GB, MaxMemAllocated=62.78GB
Epoch: [0][250/500]	Time 10.988 (10.988)	Loss 7.5394 (5.9774)	CeLoss 0.3281 (0.2529)	SegCLSLoss 0.0157 (0.0219)	KLLoss 0.0928 (0.1316)	MaskLoss 1.2606 (0.9174)	MaskBCELoss 0.4958 (0.2928)	MaskDICELoss 0.7648 (0.6246)
Epoch: [0][251/500]	Time  9.503 ( 9.503)	Loss 0.8242 (6.1493)	CeLoss 0.8242 (0.5089)	SegCLSLoss 0.0000 (0.0168)	KLLoss 0.0000 (0.1080)	MaskLoss 0.0000 (0.9112)	MaskBCELoss 0.0000 (0.2942)	MaskDICELoss 0.0000 (0.6170)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([34, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][252/500]	Time 10.091 (10.091)	Loss 7.8650 (6.7128)	CeLoss 0.2061 (0.4935)	SegCLSLoss 0.0435 (0.0321)	KLLoss 0.1865 (0.1475)	MaskLoss 1.1501 (0.9513)	MaskBCELoss 0.2916 (0.2590)	MaskDICELoss 0.8585 (0.6922)
Epoch: [0][253/500]	Time  9.450 ( 9.450)	Loss 0.8320 (5.2970)	CeLoss 0.8320 (0.5492)	SegCLSLoss 0.0000 (0.0107)	KLLoss 0.0000 (0.0692)	MaskLoss 0.0000 (0.7546)	MaskBCELoss 0.0000 (0.2273)	MaskDICELoss 0.0000 (0.5273)
Epoch: [0][254/500]	Time 11.366 (11.366)	Loss 1.0156 (5.0036)	CeLoss 1.0156 (0.4445)	SegCLSLoss 0.0000 (0.0146)	KLLoss 0.0000 (0.0990)	MaskLoss 0.0000 (0.6704)	MaskBCELoss 0.0000 (0.1518)	MaskDICELoss 0.0000 (0.5186)
Epoch: [0][255/500]	Time  7.942 ( 7.942)	Loss 0.8398 (4.1838)	CeLoss 0.8398 (0.6286)	SegCLSLoss 0.0000 (0.0145)	KLLoss 0.0000 (0.0845)	MaskLoss 0.0000 (0.5199)	MaskBCELoss 0.0000 (0.1160)	MaskDICELoss 0.0000 (0.4040)
Epoch: [0][256/500]	Time 11.271 (11.271)	Loss 8.0727 (7.2706)	CeLoss 0.2080 (0.3932)	SegCLSLoss 0.0256 (0.0290)	KLLoss 0.1846 (0.1640)	MaskLoss 1.0434 (1.0011)	MaskBCELoss 0.1135 (0.2183)	MaskDICELoss 0.9299 (0.7828)
Epoch: [0][257/500]	Time 10.468 (10.468)	Loss 1.0938 (6.9060)	CeLoss 1.0938 (0.3126)	SegCLSLoss 0.0000 (0.0332)	KLLoss 0.0000 (0.1782)	MaskLoss 0.0000 (0.9900)	MaskBCELoss 0.0000 (0.2535)	MaskDICELoss 0.0000 (0.7365)
Epoch: [0][258/500]	Time  8.500 ( 8.500)	Loss 7.4165 (6.1537)	CeLoss 0.2559 (0.4971)	SegCLSLoss 0.0187 (0.0274)	KLLoss 0.1543 (0.1489)	MaskLoss 1.0340 (0.8791)	MaskBCELoss 0.2126 (0.2564)	MaskDICELoss 0.8214 (0.6227)
Epoch: [0][259/500]	Time  9.198 ( 9.198)	Loss 5.7672 (5.3784)	CeLoss 0.1934 (0.4616)	SegCLSLoss 0.0449 (0.0219)	KLLoss 0.1914 (0.1303)	MaskLoss 0.9033 (0.7624)	MaskBCELoss 0.3110 (0.2206)	MaskDICELoss 0.5924 (0.5418)
[2025-03-04 02:48:29,667] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=0, lr=[0.00029497590361445784], mom=[(0.9, 0.95)]
[2025-03-04 02:48:29,672] [INFO] [timer.py:215:stop] epoch=0/micro_step=2600/global_step=260, RunningAvgSamplesPerSec=1.0192059234278756, CurrSamplesPerSec=1.0997949925143844, MemAllocated=57.45GB, MaxMemAllocated=62.78GB
Epoch: [0][260/500]	Time  9.095 ( 9.095)	Loss 8.4369 (6.3801)	CeLoss 0.2412 (0.3613)	SegCLSLoss 0.0205 (0.0228)	KLLoss 0.1523 (0.1358)	MaskLoss 1.0705 (0.9793)	MaskBCELoss 0.0886 (0.3272)	MaskDICELoss 0.9819 (0.6521)
Epoch: [0][261/500]	Time 10.551 (10.551)	Loss 8.1201 (7.3798)	CeLoss 0.1250 (0.3093)	SegCLSLoss 0.0918 (0.0331)	KLLoss 0.2480 (0.1565)	MaskLoss 1.2642 (0.9853)	MaskBCELoss 0.4019 (0.1643)	MaskDICELoss 0.8623 (0.8211)
Epoch: [0][262/500]	Time  9.042 ( 9.042)	Loss 7.1635 (6.1584)	CeLoss 0.2539 (0.3886)	SegCLSLoss 0.0175 (0.0324)	KLLoss 0.1201 (0.1493)	MaskLoss 1.2821 (0.9433)	MaskBCELoss 0.5793 (0.3236)	MaskDICELoss 0.7028 (0.6197)
Epoch: [0][263/500]	Time 11.435 (11.435)	Loss 8.1916 (6.2876)	CeLoss 0.3301 (0.4577)	SegCLSLoss 0.0151 (0.0219)	KLLoss 0.0801 (0.1131)	MaskLoss 1.2738 (0.9184)	MaskBCELoss 0.4029 (0.2735)	MaskDICELoss 0.8710 (0.6449)
Epoch: [0][264/500]	Time  9.308 ( 9.308)	Loss 1.1641 (5.6025)	CeLoss 1.1641 (0.4560)	SegCLSLoss 0.0000 (0.0207)	KLLoss 0.0000 (0.1273)	MaskLoss 0.0000 (0.8727)	MaskBCELoss 0.0000 (0.3288)	MaskDICELoss 0.0000 (0.5439)
Epoch: [0][265/500]	Time  9.269 ( 9.269)	Loss 6.6710 (5.2717)	CeLoss 0.2578 (0.3923)	SegCLSLoss 0.0161 (0.0145)	KLLoss 0.1064 (0.0903)	MaskLoss 1.2061 (0.7469)	MaskBCELoss 0.5581 (0.1988)	MaskDICELoss 0.6480 (0.5480)
Epoch: [0][266/500]	Time  8.064 ( 8.064)	Loss 7.4638 (5.0084)	CeLoss 0.2041 (0.4685)	SegCLSLoss 0.0391 (0.0181)	KLLoss 0.1895 (0.1009)	MaskLoss 1.3784 (0.7534)	MaskBCELoss 0.6628 (0.2663)	MaskDICELoss 0.7155 (0.4871)
Epoch: [0][267/500]	Time  9.470 ( 9.470)	Loss 0.3223 (5.0165)	CeLoss 0.3223 (0.4756)	SegCLSLoss 0.0000 (0.0163)	KLLoss 0.0000 (0.0870)	MaskLoss 0.0000 (0.6120)	MaskBCELoss 0.0000 (0.0751)	MaskDICELoss 0.0000 (0.5369)
Epoch: [0][268/500]	Time 10.170 (10.170)	Loss 7.1039 (6.4226)	CeLoss 0.2773 (0.4757)	SegCLSLoss 0.0232 (0.0178)	KLLoss 0.1245 (0.1012)	MaskLoss 0.9187 (0.8965)	MaskBCELoss 0.1099 (0.2226)	MaskDICELoss 0.8087 (0.6739)
Epoch: [0][269/500]	Time  8.502 ( 8.502)	Loss 1.3775 (5.1461)	CeLoss 0.2070 (0.5299)	SegCLSLoss 0.0332 (0.0201)	KLLoss 0.1562 (0.1021)	MaskLoss 0.2106 (0.7176)	MaskBCELoss 0.1144 (0.2061)	MaskDICELoss 0.0962 (0.5115)
[2025-03-04 02:50:04,303] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=0, lr=[0.0002947349397590361], mom=[(0.9, 0.95)]
[2025-03-04 02:50:04,308] [INFO] [timer.py:215:stop] epoch=0/micro_step=2700/global_step=270, RunningAvgSamplesPerSec=1.0205623568117892, CurrSamplesPerSec=1.133265619721132, MemAllocated=57.56GB, MaxMemAllocated=62.78GB
Epoch: [0][270/500]	Time  8.826 ( 8.826)	Loss 7.8750 (5.7432)	CeLoss 0.2256 (0.5205)	SegCLSLoss 0.0300 (0.0213)	KLLoss 0.1475 (0.1090)	MaskLoss 1.0139 (0.7574)	MaskBCELoss 0.1038 (0.1593)	MaskDICELoss 0.9101 (0.5981)
Epoch: [0][271/500]	Time 11.137 (11.137)	Loss 0.1260 (6.2863)	CeLoss 0.1260 (0.2505)	SegCLSLoss 0.0000 (0.0278)	KLLoss 0.0000 (0.1320)	MaskLoss 0.0000 (0.9833)	MaskBCELoss 0.0000 (0.3295)	MaskDICELoss 0.0000 (0.6539)
Epoch: [0][272/500]	Time  8.843 ( 8.843)	Loss 1.1875 (5.5318)	CeLoss 1.1875 (0.4690)	SegCLSLoss 0.0000 (0.0196)	KLLoss 0.0000 (0.0838)	MaskLoss 0.0000 (0.8983)	MaskBCELoss 0.0000 (0.3696)	MaskDICELoss 0.0000 (0.5287)
Epoch: [0][273/500]	Time  9.027 ( 9.027)	Loss 7.8275 (6.3526)	CeLoss 0.2168 (0.3487)	SegCLSLoss 0.0283 (0.0317)	KLLoss 0.1157 (0.1338)	MaskLoss 1.3119 (0.9051)	MaskBCELoss 0.5022 (0.2311)	MaskDICELoss 0.8097 (0.6740)
Epoch: [0][274/500]	Time 11.588 (11.588)	Loss 6.7960 (6.2487)	CeLoss 0.1455 (0.3095)	SegCLSLoss 0.0488 (0.0273)	KLLoss 0.2324 (0.1166)	MaskLoss 0.8651 (0.8422)	MaskBCELoss 0.0879 (0.1548)	MaskDICELoss 0.7772 (0.6874)
Epoch: [0][275/500]	Time  9.498 ( 9.498)	Loss 7.9326 (7.1062)	CeLoss 0.2949 (0.3379)	SegCLSLoss 0.0371 (0.0398)	KLLoss 0.1250 (0.1474)	MaskLoss 1.1093 (0.9916)	MaskBCELoss 0.2299 (0.2220)	MaskDICELoss 0.8794 (0.7696)
Epoch: [0][276/500]	Time 11.502 (11.502)	Loss 0.9102 (6.0254)	CeLoss 0.9102 (0.3934)	SegCLSLoss 0.0000 (0.0276)	KLLoss 0.0000 (0.1188)	MaskLoss 0.0000 (0.7821)	MaskBCELoss 0.0000 (0.1263)	MaskDICELoss 0.0000 (0.6558)
Epoch: [0][277/500]	Time  9.450 ( 9.450)	Loss 7.6863 (6.6806)	CeLoss 0.1436 (0.2843)	SegCLSLoss 0.0645 (0.0375)	KLLoss 0.2412 (0.1614)	MaskLoss 1.2137 (0.9575)	MaskBCELoss 0.4069 (0.2408)	MaskDICELoss 0.8068 (0.7167)
Epoch: [0][278/500]	Time  9.040 ( 9.040)	Loss 2.0883 (5.3627)	CeLoss 0.1904 (0.5641)	SegCLSLoss 0.0461 (0.0312)	KLLoss 0.1592 (0.1275)	MaskLoss 0.2911 (0.7442)	MaskBCELoss 0.1022 (0.2164)	MaskDICELoss 0.1888 (0.5278)
Epoch: [0][279/500]	Time  9.132 ( 9.132)	Loss 1.4844 (4.9272)	CeLoss 1.4844 (0.6563)	SegCLSLoss 0.0000 (0.0247)	KLLoss 0.0000 (0.1192)	MaskLoss 0.0000 (0.6651)	MaskBCELoss 0.0000 (0.1970)	MaskDICELoss 0.0000 (0.4681)
[2025-03-04 02:51:43,814] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=0, lr=[0.00029449397590361443], mom=[(0.9, 0.95)]
[2025-03-04 02:51:43,820] [INFO] [timer.py:215:stop] epoch=0/micro_step=2800/global_step=280, RunningAvgSamplesPerSec=1.020002746086687, CurrSamplesPerSec=0.9713872386847379, MemAllocated=56.72GB, MaxMemAllocated=62.78GB
Epoch: [0][280/500]	Time 10.296 (10.296)	Loss 1.5156 (6.5961)	CeLoss 1.5156 (0.5254)	SegCLSLoss 0.0000 (0.0281)	KLLoss 0.0000 (0.1272)	MaskLoss 0.0000 (0.9056)	MaskBCELoss 0.0000 (0.2192)	MaskDICELoss 0.0000 (0.6864)
Epoch: [0][281/500]	Time  9.614 ( 9.614)	Loss 8.8540 (6.3414)	CeLoss 0.2100 (0.3715)	SegCLSLoss 0.0349 (0.0293)	KLLoss 0.1768 (0.1292)	MaskLoss 1.2608 (0.9597)	MaskBCELoss 0.2728 (0.3087)	MaskDICELoss 0.9880 (0.6511)
Epoch: [0][282/500]	Time  9.597 ( 9.597)	Loss 8.4911 (5.5681)	CeLoss 0.1416 (0.3218)	SegCLSLoss 0.0728 (0.0312)	KLLoss 0.2344 (0.1329)	MaskLoss 1.2848 (0.7937)	MaskBCELoss 0.3666 (0.2086)	MaskDICELoss 0.9182 (0.5850)
Epoch: [0][283/500]	Time 10.325 (10.325)	Loss 7.2221 (6.9099)	CeLoss 0.2139 (0.4649)	SegCLSLoss 0.0327 (0.0310)	KLLoss 0.1826 (0.1547)	MaskLoss 1.1152 (1.0408)	MaskBCELoss 0.3522 (0.3420)	MaskDICELoss 0.7629 (0.6988)
Epoch: [0][284/500]	Time  8.103 ( 8.103)	Loss 0.4824 (5.3878)	CeLoss 0.4824 (0.4963)	SegCLSLoss 0.0000 (0.0225)	KLLoss 0.0000 (0.1056)	MaskLoss 0.0000 (0.7480)	MaskBCELoss 0.0000 (0.2015)	MaskDICELoss 0.0000 (0.5465)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([23, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([26, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][285/500]	Time 11.777 (11.777)	Loss 7.2181 (7.1576)	CeLoss 0.2178 (0.2376)	SegCLSLoss 0.0354 (0.0385)	KLLoss 0.1953 (0.1835)	MaskLoss 0.9309 (1.0810)	MaskBCELoss 0.1098 (0.3218)	MaskDICELoss 0.8211 (0.7592)
Epoch: [0][286/500]	Time 10.584 (10.584)	Loss 8.3038 (5.7465)	CeLoss 0.2461 (0.5130)	SegCLSLoss 0.0236 (0.0271)	KLLoss 0.1162 (0.1188)	MaskLoss 1.6054 (0.8736)	MaskBCELoss 0.8191 (0.3147)	MaskDICELoss 0.7863 (0.5590)
Epoch: [0][287/500]	Time 11.432 (11.432)	Loss 6.8778 (6.1142)	CeLoss 0.2148 (0.3138)	SegCLSLoss 0.0282 (0.0280)	KLLoss 0.1543 (0.1430)	MaskLoss 1.1519 (0.8506)	MaskBCELoss 0.4533 (0.1937)	MaskDICELoss 0.6985 (0.6570)
Epoch: [0][288/500]	Time  8.467 ( 8.467)	Loss 8.0920 (5.7493)	CeLoss 0.2422 (0.6442)	SegCLSLoss 0.0195 (0.0184)	KLLoss 0.1494 (0.1057)	MaskLoss 1.5002 (0.8488)	MaskBCELoss 0.7186 (0.3001)	MaskDICELoss 0.7815 (0.5487)
Epoch: [0][289/500]	Time  9.053 ( 9.053)	Loss 0.8516 (4.9712)	CeLoss 0.8516 (0.6228)	SegCLSLoss 0.0000 (0.0148)	KLLoss 0.0000 (0.0868)	MaskLoss 0.0000 (0.6650)	MaskBCELoss 0.0000 (0.1775)	MaskDICELoss 0.0000 (0.4875)
[2025-03-04 02:53:23,668] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=0, lr=[0.00029425301204819276], mom=[(0.9, 0.95)]
[2025-03-04 02:53:23,673] [INFO] [timer.py:215:stop] epoch=0/micro_step=2900/global_step=290, RunningAvgSamplesPerSec=1.0193533675490625, CurrSamplesPerSec=0.9174738294465398, MemAllocated=57.26GB, MaxMemAllocated=62.78GB
Epoch: [0][290/500]	Time 10.901 (10.901)	Loss 7.8355 (5.8084)	CeLoss 0.2910 (0.3657)	SegCLSLoss 0.0306 (0.0207)	KLLoss 0.1816 (0.1196)	MaskLoss 0.9471 (0.9258)	MaskBCELoss 0.0383 (0.3489)	MaskDICELoss 0.9088 (0.5769)
Epoch: [0][291/500]	Time  6.788 ( 6.788)	Loss 2.1974 (3.9274)	CeLoss 0.2578 (0.7005)	SegCLSLoss 0.0297 (0.0137)	KLLoss 0.1924 (0.0753)	MaskLoss 0.2939 (0.4569)	MaskBCELoss 0.1031 (0.0851)	MaskDICELoss 0.1908 (0.3718)
Epoch: [0][292/500]	Time  9.637 ( 9.637)	Loss 6.0698 (5.8995)	CeLoss 0.2422 (0.3804)	SegCLSLoss 0.0483 (0.0320)	KLLoss 0.2422 (0.1477)	MaskLoss 0.8508 (0.9369)	MaskBCELoss 0.2074 (0.3566)	MaskDICELoss 0.6434 (0.5803)
Epoch: [0][293/500]	Time  9.149 ( 9.149)	Loss 7.2546 (5.8290)	CeLoss 0.2852 (0.4634)	SegCLSLoss 0.0188 (0.0238)	KLLoss 0.1064 (0.1145)	MaskLoss 1.1318 (0.8030)	MaskBCELoss 0.3670 (0.1975)	MaskDICELoss 0.7648 (0.6055)
Epoch: [0][294/500]	Time 10.257 (10.257)	Loss 9.0605 (6.7762)	CeLoss 0.2002 (0.4059)	SegCLSLoss 0.0435 (0.0284)	KLLoss 0.2012 (0.1484)	MaskLoss 1.5690 (1.0258)	MaskBCELoss 0.6523 (0.3330)	MaskDICELoss 0.9168 (0.6928)
Epoch: [0][295/500]	Time  9.025 ( 9.025)	Loss 7.3488 (4.6920)	CeLoss 0.2480 (0.5245)	SegCLSLoss 0.0226 (0.0191)	KLLoss 0.1699 (0.1065)	MaskLoss 1.0227 (0.6641)	MaskBCELoss 0.2104 (0.2103)	MaskDICELoss 0.8123 (0.4538)
Epoch: [0][296/500]	Time  8.900 ( 8.900)	Loss 8.6591 (5.7575)	CeLoss 0.2793 (0.4612)	SegCLSLoss 0.0354 (0.0276)	KLLoss 0.2090 (0.1354)	MaskLoss 1.1022 (0.8565)	MaskBCELoss 0.1111 (0.2842)	MaskDICELoss 0.9911 (0.5723)
Epoch: [0][297/500]	Time  8.981 ( 8.981)	Loss 0.0713 (4.7672)	CeLoss 0.0713 (0.4705)	SegCLSLoss 0.0000 (0.0131)	KLLoss 0.0000 (0.0949)	MaskLoss 0.0000 (0.6934)	MaskBCELoss 0.0000 (0.2253)	MaskDICELoss 0.0000 (0.4681)
Epoch: [0][298/500]	Time  8.826 ( 8.826)	Loss 7.7347 (5.1134)	CeLoss 0.2090 (0.4444)	SegCLSLoss 0.0231 (0.0185)	KLLoss 0.1436 (0.1020)	MaskLoss 1.2986 (0.7049)	MaskBCELoss 0.5030 (0.1802)	MaskDICELoss 0.7957 (0.5247)
Epoch: [0][299/500]	Time 11.281 (11.281)	Loss 8.0184 (5.9285)	CeLoss 0.2363 (0.2778)	SegCLSLoss 0.0253 (0.0221)	KLLoss 0.1187 (0.1212)	MaskLoss 1.0718 (0.8755)	MaskBCELoss 0.1539 (0.2476)	MaskDICELoss 0.9179 (0.6279)
[2025-03-04 02:54:58,038] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=0, lr=[0.0002940120481927711], mom=[(0.9, 0.95)]
[2025-03-04 02:54:58,043] [INFO] [timer.py:215:stop] epoch=0/micro_step=3000/global_step=300, RunningAvgSamplesPerSec=1.0206614230643074, CurrSamplesPerSec=0.8677055000866298, MemAllocated=57.24GB, MaxMemAllocated=62.78GB
Epoch: [0][300/500]	Time 11.527 (11.527)	Loss 8.1929 (8.2787)	CeLoss 0.2910 (0.2622)	SegCLSLoss 0.0170 (0.0247)	KLLoss 0.1123 (0.1574)	MaskLoss 1.3496 (1.2895)	MaskBCELoss 0.5023 (0.4114)	MaskDICELoss 0.8473 (0.8781)
Epoch: [0][301/500]	Time 10.704 (10.704)	Loss 6.9016 (6.3937)	CeLoss 0.2285 (0.4095)	SegCLSLoss 0.0266 (0.0306)	KLLoss 0.1699 (0.1500)	MaskLoss 0.9507 (0.8443)	MaskBCELoss 0.1861 (0.1559)	MaskDICELoss 0.7647 (0.6883)
Epoch: [0][302/500]	Time  9.796 ( 9.796)	Loss 7.3021 (5.5942)	CeLoss 0.2754 (0.3804)	SegCLSLoss 0.0206 (0.0179)	KLLoss 0.1445 (0.1118)	MaskLoss 0.9463 (0.7691)	MaskBCELoss 0.1163 (0.1766)	MaskDICELoss 0.8300 (0.5925)
Epoch: [0][303/500]	Time  9.286 ( 9.286)	Loss 8.7071 (5.9675)	CeLoss 0.2656 (0.4640)	SegCLSLoss 0.0222 (0.0197)	KLLoss 0.1445 (0.1276)	MaskLoss 1.1764 (0.8789)	MaskBCELoss 0.1876 (0.2775)	MaskDICELoss 0.9888 (0.6014)
Epoch: [0][304/500]	Time  9.104 ( 9.104)	Loss 6.6436 (5.4144)	CeLoss 0.2930 (0.4223)	SegCLSLoss 0.0193 (0.0182)	KLLoss 0.1396 (0.1090)	MaskLoss 1.4989 (0.8525)	MaskBCELoss 0.9648 (0.3242)	MaskDICELoss 0.5341 (0.5283)
Epoch: [0][305/500]	Time  8.243 ( 8.243)	Loss 7.0224 (5.6355)	CeLoss 0.2217 (0.5704)	SegCLSLoss 0.0253 (0.0255)	KLLoss 0.1846 (0.1269)	MaskLoss 0.9523 (0.7707)	MaskBCELoss 0.1690 (0.2067)	MaskDICELoss 0.7833 (0.5640)
Epoch: [0][306/500]	Time  8.787 ( 8.787)	Loss 7.8385 (4.0314)	CeLoss 0.2715 (0.6948)	SegCLSLoss 0.0167 (0.0125)	KLLoss 0.0981 (0.0693)	MaskLoss 1.1550 (0.4998)	MaskBCELoss 0.2964 (0.1230)	MaskDICELoss 0.8586 (0.3768)
Epoch: [0][307/500]	Time 11.353 (11.353)	Loss 7.2035 (7.0349)	CeLoss 0.2539 (0.3183)	SegCLSLoss 0.0500 (0.0293)	KLLoss 0.2285 (0.1587)	MaskLoss 1.1733 (0.9588)	MaskBCELoss 0.4485 (0.1879)	MaskDICELoss 0.7248 (0.7709)
Epoch: [0][308/500]	Time  9.953 ( 9.953)	Loss 1.7188 (4.3025)	CeLoss 1.7188 (0.5301)	SegCLSLoss 0.0000 (0.0162)	KLLoss 0.0000 (0.0887)	MaskLoss 0.0000 (0.5595)	MaskBCELoss 0.0000 (0.1334)	MaskDICELoss 0.0000 (0.4261)
Epoch: [0][309/500]	Time 10.761 (10.761)	Loss 8.5274 (6.9944)	CeLoss 0.2852 (0.3293)	SegCLSLoss 0.0278 (0.0240)	KLLoss 0.2207 (0.1417)	MaskLoss 1.0048 (0.9414)	MaskBCELoss 0.0051 (0.1700)	MaskDICELoss 0.9997 (0.7715)
[2025-03-04 02:56:35,307] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=0, lr=[0.00029377108433734935], mom=[(0.9, 0.95)]
[2025-03-04 02:56:35,313] [INFO] [timer.py:215:stop] epoch=0/micro_step=3100/global_step=310, RunningAvgSamplesPerSec=1.0209055540249845, CurrSamplesPerSec=1.077346290278985, MemAllocated=57.47GB, MaxMemAllocated=62.78GB
Epoch: [0][310/500]	Time  9.284 ( 9.284)	Loss 7.7698 (5.1782)	CeLoss 0.2363 (0.5257)	SegCLSLoss 0.0249 (0.0190)	KLLoss 0.1377 (0.1055)	MaskLoss 1.0496 (0.6930)	MaskBCELoss 0.1689 (0.1677)	MaskDICELoss 0.8807 (0.5253)
Epoch: [0][311/500]	Time  9.901 ( 9.901)	Loss 6.8786 (6.4970)	CeLoss 0.2656 (0.3271)	SegCLSLoss 0.0159 (0.0274)	KLLoss 0.0879 (0.1389)	MaskLoss 1.2210 (0.9451)	MaskBCELoss 0.5417 (0.2573)	MaskDICELoss 0.6792 (0.6878)
Epoch: [0][312/500]	Time 10.549 (10.549)	Loss 7.5770 (6.8427)	CeLoss 0.2812 (0.4107)	SegCLSLoss 0.0344 (0.0260)	KLLoss 0.1777 (0.1323)	MaskLoss 0.9911 (1.0556)	MaskBCELoss 0.1381 (0.3597)	MaskDICELoss 0.8530 (0.6959)
Epoch: [0][313/500]	Time  9.928 ( 9.928)	Loss 8.1384 (4.8617)	CeLoss 0.1885 (0.7264)	SegCLSLoss 0.0361 (0.0171)	KLLoss 0.1719 (0.0931)	MaskLoss 1.6103 (0.6824)	MaskBCELoss 0.8538 (0.2376)	MaskDICELoss 0.7565 (0.4448)
Epoch: [0][314/500]	Time  9.671 ( 9.671)	Loss 8.1438 (5.1125)	CeLoss 0.1846 (0.3607)	SegCLSLoss 0.0403 (0.0217)	KLLoss 0.2188 (0.1226)	MaskLoss 1.2847 (0.6848)	MaskBCELoss 0.4263 (0.1433)	MaskDICELoss 0.8584 (0.5414)
Epoch: [0][315/500]	Time  9.213 ( 9.213)	Loss 7.6683 (5.0412)	CeLoss 0.2090 (0.4504)	SegCLSLoss 0.0403 (0.0233)	KLLoss 0.2188 (0.1268)	MaskLoss 1.1080 (0.7434)	MaskBCELoss 0.2741 (0.2491)	MaskDICELoss 0.8338 (0.4943)
Epoch: [0][316/500]	Time  8.821 ( 8.821)	Loss 8.5155 (5.3160)	CeLoss 0.2578 (0.5264)	SegCLSLoss 0.0422 (0.0221)	KLLoss 0.1934 (0.1080)	MaskLoss 1.0521 (0.6605)	MaskBCELoss 0.0624 (0.1023)	MaskDICELoss 0.9898 (0.5582)
Epoch: [0][317/500]	Time  9.269 ( 9.269)	Loss 1.4609 (6.3385)	CeLoss 1.4609 (0.3592)	SegCLSLoss 0.0000 (0.0326)	KLLoss 0.0000 (0.1494)	MaskLoss 0.0000 (0.9856)	MaskBCELoss 0.0000 (0.3452)	MaskDICELoss 0.0000 (0.6404)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([27, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][318/500]	Time 11.322 (11.322)	Loss 8.3592 (6.4682)	CeLoss 0.1738 (0.3198)	SegCLSLoss 0.0515 (0.0351)	KLLoss 0.2295 (0.1615)	MaskLoss 1.1796 (0.9138)	MaskBCELoss 0.2513 (0.2236)	MaskDICELoss 0.9284 (0.6903)
Epoch: [0][319/500]	Time  9.562 ( 9.562)	Loss 5.1421 (6.3710)	CeLoss 0.3184 (0.3218)	SegCLSLoss 0.0214 (0.0218)	KLLoss 0.1377 (0.1260)	MaskLoss 0.9136 (0.9145)	MaskBCELoss 0.4386 (0.2338)	MaskDICELoss 0.4750 (0.6806)
[2025-03-04 02:58:12,778] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=0, lr=[0.0002935301204819277], mom=[(0.9, 0.95)]
[2025-03-04 02:58:12,784] [INFO] [timer.py:215:stop] epoch=0/micro_step=3200/global_step=320, RunningAvgSamplesPerSec=1.0210684709415287, CurrSamplesPerSec=1.083085122796915, MemAllocated=57.26GB, MaxMemAllocated=62.78GB
Epoch: [0][320/500]	Time  9.235 ( 9.235)	Loss 7.3055 (5.7173)	CeLoss 0.1865 (0.4361)	SegCLSLoss 0.0270 (0.0255)	KLLoss 0.1660 (0.1242)	MaskLoss 0.8905 (0.8587)	MaskBCELoss 0.0309 (0.2876)	MaskDICELoss 0.8596 (0.5711)
Epoch: [0][321/500]	Time 12.401 (12.401)	Loss 3.9930 (6.8009)	CeLoss 0.2500 (0.2971)	SegCLSLoss 0.0182 (0.0251)	KLLoss 0.1079 (0.1402)	MaskLoss 0.7579 (1.0933)	MaskBCELoss 0.4062 (0.3991)	MaskDICELoss 0.3517 (0.6942)
Epoch: [0][322/500]	Time  8.598 ( 8.598)	Loss 0.5156 (3.8425)	CeLoss 0.5156 (0.5792)	SegCLSLoss 0.0000 (0.0180)	KLLoss 0.0000 (0.0941)	MaskLoss 0.0000 (0.5674)	MaskBCELoss 0.0000 (0.2299)	MaskDICELoss 0.0000 (0.3375)
Epoch: [0][323/500]	Time  8.515 ( 8.515)	Loss 6.7661 (5.1531)	CeLoss 0.3672 (0.4647)	SegCLSLoss 0.0200 (0.0191)	KLLoss 0.1240 (0.1082)	MaskLoss 1.1928 (0.7436)	MaskBCELoss 0.5461 (0.2296)	MaskDICELoss 0.6467 (0.5139)
Epoch: [0][324/500]	Time  8.744 ( 8.744)	Loss 1.3750 (5.2945)	CeLoss 1.3750 (0.5970)	SegCLSLoss 0.0000 (0.0191)	KLLoss 0.0000 (0.1142)	MaskLoss 0.0000 (0.7236)	MaskBCELoss 0.0000 (0.2024)	MaskDICELoss 0.0000 (0.5211)
Epoch: [0][325/500]	Time  9.489 ( 9.489)	Loss 8.8187 (5.1301)	CeLoss 0.2021 (0.5124)	SegCLSLoss 0.0381 (0.0167)	KLLoss 0.2129 (0.1096)	MaskLoss 1.2183 (0.7390)	MaskBCELoss 0.2269 (0.2354)	MaskDICELoss 0.9914 (0.5036)
Epoch: [0][326/500]	Time  7.723 ( 7.723)	Loss 5.8063 (5.0866)	CeLoss 0.2598 (0.3889)	SegCLSLoss 0.0175 (0.0249)	KLLoss 0.1143 (0.1341)	MaskLoss 0.8861 (0.7586)	MaskBCELoss 0.2775 (0.2529)	MaskDICELoss 0.6086 (0.5057)
Epoch: [0][327/500]	Time  8.365 ( 8.365)	Loss 7.2452 (3.6817)	CeLoss 0.3066 (0.5820)	SegCLSLoss 0.0178 (0.0116)	KLLoss 0.1201 (0.0639)	MaskLoss 1.1505 (0.5790)	MaskBCELoss 0.3994 (0.2670)	MaskDICELoss 0.7511 (0.3120)
Epoch: [0][328/500]	Time 10.428 (10.428)	Loss 8.1758 (5.7549)	CeLoss 0.1504 (0.3800)	SegCLSLoss 0.0669 (0.0241)	KLLoss 0.2500 (0.1422)	MaskLoss 1.2715 (0.9054)	MaskBCELoss 0.4049 (0.3371)	MaskDICELoss 0.8665 (0.5683)
Epoch: [0][329/500]	Time 10.766 (10.766)	Loss 5.9924 (6.7479)	CeLoss 0.2422 (0.2948)	SegCLSLoss 0.0210 (0.0222)	KLLoss 0.1523 (0.1336)	MaskLoss 1.0846 (1.0163)	MaskBCELoss 0.5148 (0.3036)	MaskDICELoss 0.5698 (0.7127)
[2025-03-04 02:59:46,336] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=0, lr=[0.000293289156626506], mom=[(0.9, 0.95)]
[2025-03-04 02:59:46,341] [INFO] [timer.py:215:stop] epoch=0/micro_step=3300/global_step=330, RunningAvgSamplesPerSec=1.0224670731539895, CurrSamplesPerSec=1.1726823183339479, MemAllocated=57.26GB, MaxMemAllocated=62.78GB
Epoch: [0][330/500]	Time  8.529 ( 8.529)	Loss 8.1968 (4.5499)	CeLoss 0.2061 (0.5538)	SegCLSLoss 0.0189 (0.0154)	KLLoss 0.1123 (0.0833)	MaskLoss 1.1857 (0.6204)	MaskBCELoss 0.2694 (0.1764)	MaskDICELoss 0.9162 (0.4440)
Epoch: [0][331/500]	Time  8.678 ( 8.678)	Loss 8.0103 (5.2599)	CeLoss 0.2217 (0.5384)	SegCLSLoss 0.0291 (0.0175)	KLLoss 0.1777 (0.1120)	MaskLoss 1.1609 (0.8411)	MaskBCELoss 0.2818 (0.3546)	MaskDICELoss 0.8791 (0.4865)
Epoch: [0][332/500]	Time  9.883 ( 9.883)	Loss 0.8359 (4.0413)	CeLoss 0.8359 (0.3912)	SegCLSLoss 0.0000 (0.0153)	KLLoss 0.0000 (0.0880)	MaskLoss 0.0000 (0.5163)	MaskBCELoss 0.0000 (0.0960)	MaskDICELoss 0.0000 (0.4203)
Epoch: [0][333/500]	Time  9.564 ( 9.564)	Loss 8.2961 (5.9424)	CeLoss 0.1768 (0.4384)	SegCLSLoss 0.0347 (0.0171)	KLLoss 0.2119 (0.1097)	MaskLoss 1.0385 (0.8750)	MaskBCELoss 0.0697 (0.2690)	MaskDICELoss 0.9688 (0.6060)
Epoch: [0][334/500]	Time  9.920 ( 9.920)	Loss 0.1855 (5.5365)	CeLoss 0.1855 (0.3184)	SegCLSLoss 0.0000 (0.0266)	KLLoss 0.0000 (0.1341)	MaskLoss 0.0000 (0.7878)	MaskBCELoss 0.0000 (0.2053)	MaskDICELoss 0.0000 (0.5825)
Epoch: [0][335/500]	Time 10.552 (10.552)	Loss 7.4397 (5.3733)	CeLoss 0.2715 (0.4571)	SegCLSLoss 0.0281 (0.0186)	KLLoss 0.1807 (0.1167)	MaskLoss 1.2259 (0.7221)	MaskBCELoss 0.4720 (0.1644)	MaskDICELoss 0.7539 (0.5577)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([22, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][336/500]	Time  9.508 ( 9.508)	Loss 6.5037 (5.8428)	CeLoss 0.2412 (0.3884)	SegCLSLoss 0.0192 (0.0278)	KLLoss 0.1055 (0.1285)	MaskLoss 0.8902 (0.8764)	MaskBCELoss 0.1622 (0.2832)	MaskDICELoss 0.7280 (0.5932)
Epoch: [0][337/500]	Time  9.356 ( 9.356)	Loss 8.4710 (5.0332)	CeLoss 0.1377 (0.3966)	SegCLSLoss 0.0688 (0.0237)	KLLoss 0.2559 (0.1340)	MaskLoss 1.3255 (0.7035)	MaskBCELoss 0.4268 (0.1896)	MaskDICELoss 0.8987 (0.5139)
Epoch: [0][338/500]	Time  9.675 ( 9.675)	Loss 6.1570 (5.2925)	CeLoss 0.2207 (0.4236)	SegCLSLoss 0.0315 (0.0206)	KLLoss 0.1719 (0.1176)	MaskLoss 0.7927 (0.7617)	MaskBCELoss 0.0988 (0.2254)	MaskDICELoss 0.6939 (0.5363)
Epoch: [0][339/500]	Time 10.954 (10.954)	Loss 0.8086 (5.9218)	CeLoss 0.8086 (0.4192)	SegCLSLoss 0.0000 (0.0206)	KLLoss 0.0000 (0.1166)	MaskLoss 0.0000 (0.8853)	MaskBCELoss 0.0000 (0.2845)	MaskDICELoss 0.0000 (0.6008)
[2025-03-04 03:01:23,634] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=0, lr=[0.0002930481927710843], mom=[(0.9, 0.95)]
[2025-03-04 03:01:23,639] [INFO] [timer.py:215:stop] epoch=0/micro_step=3400/global_step=340, RunningAvgSamplesPerSec=1.022628424282211, CurrSamplesPerSec=1.0865896038424454, MemAllocated=56.73GB, MaxMemAllocated=62.78GB
Epoch: [0][340/500]	Time  9.206 ( 9.206)	Loss 1.4531 (5.6417)	CeLoss 1.4531 (0.5680)	SegCLSLoss 0.0000 (0.0224)	KLLoss 0.0000 (0.1235)	MaskLoss 0.0000 (0.7743)	MaskBCELoss 0.0000 (0.2093)	MaskDICELoss 0.0000 (0.5650)
Epoch: [0][341/500]	Time  9.999 ( 9.999)	Loss 8.2551 (5.0051)	CeLoss 0.2754 (0.5079)	SegCLSLoss 0.0203 (0.0171)	KLLoss 0.1006 (0.1050)	MaskLoss 1.4420 (0.7037)	MaskBCELoss 0.6113 (0.2076)	MaskDICELoss 0.8307 (0.4960)
Epoch: [0][342/500]	Time 10.998 (10.998)	Loss 7.6324 (6.2975)	CeLoss 0.1943 (0.3470)	SegCLSLoss 0.0310 (0.0263)	KLLoss 0.1865 (0.1514)	MaskLoss 1.0345 (0.8686)	MaskBCELoss 0.1733 (0.1938)	MaskDICELoss 0.8612 (0.6748)
Epoch: [0][343/500]	Time  8.812 ( 8.812)	Loss 1.1484 (5.0447)	CeLoss 1.1484 (0.4661)	SegCLSLoss 0.0000 (0.0219)	KLLoss 0.0000 (0.1318)	MaskLoss 0.0000 (0.7170)	MaskBCELoss 0.0000 (0.2168)	MaskDICELoss 0.0000 (0.5002)
Epoch: [0][344/500]	Time  8.030 ( 8.030)	Loss 5.8197 (2.8309)	CeLoss 0.3047 (0.5919)	SegCLSLoss 0.0175 (0.0111)	KLLoss 0.1504 (0.0786)	MaskLoss 0.9150 (0.4098)	MaskBCELoss 0.3271 (0.1873)	MaskDICELoss 0.5878 (0.2225)
Epoch: [0][345/500]	Time  7.542 ( 7.542)	Loss 8.7910 (3.4586)	CeLoss 0.1260 (0.7106)	SegCLSLoss 0.0654 (0.0163)	KLLoss 0.2539 (0.0820)	MaskLoss 1.2867 (0.4181)	MaskBCELoss 0.3191 (0.1145)	MaskDICELoss 0.9676 (0.3036)
Epoch: [0][346/500]	Time  8.567 ( 8.567)	Loss 1.2422 (4.4239)	CeLoss 1.2422 (0.7445)	SegCLSLoss 0.0000 (0.0136)	KLLoss 0.0000 (0.0838)	MaskLoss 0.0000 (0.5244)	MaskBCELoss 0.0000 (0.1010)	MaskDICELoss 0.0000 (0.4234)
Epoch: [0][347/500]	Time  8.891 ( 8.891)	Loss 6.1774 (5.5870)	CeLoss 0.2832 (0.5083)	SegCLSLoss 0.0179 (0.0209)	KLLoss 0.1660 (0.1302)	MaskLoss 0.9249 (0.7517)	MaskBCELoss 0.2802 (0.1793)	MaskDICELoss 0.6448 (0.5725)
Epoch: [0][348/500]	Time  8.564 ( 8.564)	Loss 8.5825 (5.4899)	CeLoss 0.2451 (0.5968)	SegCLSLoss 0.0408 (0.0217)	KLLoss 0.2422 (0.1362)	MaskLoss 1.0375 (0.7730)	MaskBCELoss 0.0375 (0.2397)	MaskDICELoss 1.0000 (0.5333)
Epoch: [0][349/500]	Time  9.815 ( 9.815)	Loss 8.8910 (4.6990)	CeLoss 0.1777 (0.5857)	SegCLSLoss 0.0364 (0.0132)	KLLoss 0.1895 (0.0768)	MaskLoss 1.2860 (0.6446)	MaskBCELoss 0.2973 (0.1878)	MaskDICELoss 0.9887 (0.4568)
[2025-03-04 03:02:55,735] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=0, lr=[0.00029280722891566265], mom=[(0.9, 0.95)]
[2025-03-04 03:02:55,740] [INFO] [timer.py:215:stop] epoch=0/micro_step=3500/global_step=350, RunningAvgSamplesPerSec=1.0243445656883667, CurrSamplesPerSec=0.9190597959107651, MemAllocated=57.25GB, MaxMemAllocated=62.78GB
Epoch: [0][350/500]	Time 10.883 (10.883)	Loss 8.7140 (8.2268)	CeLoss 0.2500 (0.2284)	SegCLSLoss 0.0179 (0.0305)	KLLoss 0.1221 (0.1755)	MaskLoss 1.8936 (1.4215)	MaskBCELoss 1.1362 (0.5941)	MaskDICELoss 0.7574 (0.8274)
Epoch: [0][351/500]	Time 10.195 (10.195)	Loss 4.0700 (6.0185)	CeLoss 0.2754 (0.3978)	SegCLSLoss 0.0201 (0.0223)	KLLoss 0.1738 (0.1351)	MaskLoss 0.8203 (0.9034)	MaskBCELoss 0.4919 (0.2922)	MaskDICELoss 0.3284 (0.6112)
Epoch: [0][352/500]	Time  9.937 ( 9.937)	Loss 6.8372 (6.2057)	CeLoss 0.3828 (0.3810)	SegCLSLoss 0.0199 (0.0236)	KLLoss 0.1187 (0.1368)	MaskLoss 1.0855 (0.9158)	MaskBCELoss 0.3931 (0.2751)	MaskDICELoss 0.6924 (0.6407)
Epoch: [0][353/500]	Time  9.316 ( 9.316)	Loss 8.1699 (5.8182)	CeLoss 0.3613 (0.4397)	SegCLSLoss 0.0198 (0.0251)	KLLoss 0.1216 (0.1275)	MaskLoss 1.3873 (0.8607)	MaskBCELoss 0.5701 (0.2746)	MaskDICELoss 0.8172 (0.5861)
Epoch: [0][354/500]	Time  8.074 ( 8.074)	Loss 4.7667 (5.5790)	CeLoss 0.3262 (0.4851)	SegCLSLoss 0.0177 (0.0192)	KLLoss 0.0957 (0.1209)	MaskLoss 0.7475 (0.7538)	MaskBCELoss 0.2742 (0.1780)	MaskDICELoss 0.4733 (0.5759)
Epoch: [0][355/500]	Time  8.227 ( 8.227)	Loss 7.3439 (6.1247)	CeLoss 0.1885 (0.5235)	SegCLSLoss 0.0309 (0.0228)	KLLoss 0.1660 (0.1383)	MaskLoss 1.0037 (0.7871)	MaskBCELoss 0.1761 (0.1409)	MaskDICELoss 0.8276 (0.6462)
Epoch: [0][356/500]	Time  9.211 ( 9.211)	Loss 8.1934 (4.6774)	CeLoss 0.2422 (0.5821)	SegCLSLoss 0.0311 (0.0180)	KLLoss 0.1787 (0.1098)	MaskLoss 1.4014 (0.6523)	MaskBCELoss 0.5758 (0.2070)	MaskDICELoss 0.8255 (0.4453)
Epoch: [0][357/500]	Time  9.943 ( 9.943)	Loss 7.6935 (6.5724)	CeLoss 0.2100 (0.4406)	SegCLSLoss 0.0381 (0.0235)	KLLoss 0.2021 (0.1369)	MaskLoss 1.2739 (0.8929)	MaskBCELoss 0.4879 (0.1932)	MaskDICELoss 0.7860 (0.6996)
Epoch: [0][358/500]	Time  9.675 ( 9.675)	Loss 1.1250 (5.0815)	CeLoss 1.1250 (0.4475)	SegCLSLoss 0.0000 (0.0195)	KLLoss 0.0000 (0.1153)	MaskLoss 0.0000 (0.6489)	MaskBCELoss 0.0000 (0.1137)	MaskDICELoss 0.0000 (0.5352)
Epoch: [0][359/500]	Time  7.228 ( 7.228)	Loss 3.9479 (2.7680)	CeLoss 0.2988 (1.0596)	SegCLSLoss 0.0179 (0.0072)	KLLoss 0.1187 (0.0433)	MaskLoss 0.7104 (0.2670)	MaskBCELoss 0.3605 (0.0790)	MaskDICELoss 0.3499 (0.1879)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([36, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
[2025-03-04 03:04:28,636] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=0, lr=[0.0002925662650602409], mom=[(0.9, 0.95)]
[2025-03-04 03:04:28,642] [INFO] [timer.py:215:stop] epoch=0/micro_step=3600/global_step=360, RunningAvgSamplesPerSec=1.0257349608250532, CurrSamplesPerSec=0.9014911009245654, MemAllocated=56.81GB, MaxMemAllocated=62.78GB
Epoch: [0][360/500]	Time 11.095 (11.095)	Loss 8.5797 (6.5953)	CeLoss 0.6094 (0.3429)	SegCLSLoss 0.0425 (0.0291)	KLLoss 0.2266 (0.1437)	MaskLoss 1.3826 (0.9724)	MaskBCELoss 0.5561 (0.2807)	MaskDICELoss 0.8265 (0.6917)
Epoch: [0][361/500]	Time  8.588 ( 8.588)	Loss 0.9062 (3.9429)	CeLoss 0.9062 (0.5975)	SegCLSLoss 0.0000 (0.0153)	KLLoss 0.0000 (0.1009)	MaskLoss 0.0000 (0.5223)	MaskBCELoss 0.0000 (0.1569)	MaskDICELoss 0.0000 (0.3654)
Epoch: [0][362/500]	Time 10.112 (10.112)	Loss 0.6719 (6.5420)	CeLoss 0.6719 (0.2862)	SegCLSLoss 0.0000 (0.0257)	KLLoss 0.0000 (0.1511)	MaskLoss 0.0000 (0.9431)	MaskBCELoss 0.0000 (0.2421)	MaskDICELoss 0.0000 (0.7010)
Epoch: [0][363/500]	Time 10.288 (10.288)	Loss 8.0558 (6.2054)	CeLoss 0.2656 (0.3062)	SegCLSLoss 0.0342 (0.0245)	KLLoss 0.2217 (0.1549)	MaskLoss 1.0661 (0.9218)	MaskBCELoss 0.1628 (0.2737)	MaskDICELoss 0.9033 (0.6481)
Epoch: [0][364/500]	Time  8.568 ( 8.568)	Loss 1.0547 (5.1480)	CeLoss 1.0547 (0.5105)	SegCLSLoss 0.0000 (0.0157)	KLLoss 0.0000 (0.1036)	MaskLoss 0.0000 (0.7456)	MaskBCELoss 0.0000 (0.2398)	MaskDICELoss 0.0000 (0.5058)
Epoch: [0][365/500]	Time 10.253 (10.253)	Loss 6.8521 (4.5121)	CeLoss 0.2285 (0.3170)	SegCLSLoss 0.0232 (0.0156)	KLLoss 0.1504 (0.0917)	MaskLoss 0.9931 (0.6876)	MaskBCELoss 0.2472 (0.2342)	MaskDICELoss 0.7459 (0.4534)
Epoch: [0][366/500]	Time  9.875 ( 9.875)	Loss 1.7969 (6.5122)	CeLoss 1.7969 (0.3783)	SegCLSLoss 0.0000 (0.0229)	KLLoss 0.0000 (0.1311)	MaskLoss 0.0000 (0.8840)	MaskBCELoss 0.0000 (0.1802)	MaskDICELoss 0.0000 (0.7038)
Epoch: [0][367/500]	Time  9.120 ( 9.120)	Loss 8.0010 (5.9204)	CeLoss 0.1953 (0.3748)	SegCLSLoss 0.0302 (0.0241)	KLLoss 0.1904 (0.1440)	MaskLoss 1.0082 (0.8784)	MaskBCELoss 0.0775 (0.2730)	MaskDICELoss 0.9307 (0.6055)
Epoch: [0][368/500]	Time  9.298 ( 9.298)	Loss 7.0188 (5.2800)	CeLoss 0.1895 (0.5859)	SegCLSLoss 0.0249 (0.0176)	KLLoss 0.1455 (0.0959)	MaskLoss 1.4412 (0.7614)	MaskBCELoss 0.8097 (0.2503)	MaskDICELoss 0.6315 (0.5112)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([37, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][369/500]	Time  9.997 ( 9.997)	Loss 8.4224 (7.0209)	CeLoss 0.1865 (0.2392)	SegCLSLoss 0.0369 (0.0361)	KLLoss 0.2441 (0.1809)	MaskLoss 1.0558 (0.9991)	MaskBCELoss 0.0789 (0.2350)	MaskDICELoss 0.9769 (0.7641)
[2025-03-04 03:06:04,773] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=0, lr=[0.00029232530120481924], mom=[(0.9, 0.95)]
[2025-03-04 03:06:04,779] [INFO] [timer.py:215:stop] epoch=0/micro_step=3700/global_step=370, RunningAvgSamplesPerSec=1.0261266704797531, CurrSamplesPerSec=0.9965717943958613, MemAllocated=57.45GB, MaxMemAllocated=62.78GB
Epoch: [0][370/500]	Time 10.036 (10.036)	Loss 7.3473 (5.5361)	CeLoss 0.2256 (0.4190)	SegCLSLoss 0.0225 (0.0209)	KLLoss 0.1157 (0.1224)	MaskLoss 1.0648 (0.8160)	MaskBCELoss 0.2538 (0.2572)	MaskDICELoss 0.8110 (0.5588)
Epoch: [0][371/500]	Time 10.155 (10.155)	Loss 5.6213 (5.7230)	CeLoss 0.2832 (0.2484)	SegCLSLoss 0.0203 (0.0194)	KLLoss 0.0889 (0.1029)	MaskLoss 0.9218 (0.8715)	MaskBCELoss 0.3560 (0.2684)	MaskDICELoss 0.5658 (0.6032)
Epoch: [0][372/500]	Time  8.628 ( 8.628)	Loss 8.1459 (5.7382)	CeLoss 0.2559 (0.4007)	SegCLSLoss 0.0258 (0.0267)	KLLoss 0.1768 (0.1461)	MaskLoss 0.9894 (0.8095)	MaskBCELoss 0.0358 (0.2164)	MaskDICELoss 0.9536 (0.5932)
Epoch: [0][373/500]	Time  8.234 ( 8.234)	Loss 0.6211 (5.5052)	CeLoss 0.6211 (0.4163)	SegCLSLoss 0.0000 (0.0234)	KLLoss 0.0000 (0.1271)	MaskLoss 0.0000 (0.7614)	MaskBCELoss 0.0000 (0.1902)	MaskDICELoss 0.0000 (0.5712)
Epoch: [0][374/500]	Time 11.326 (11.326)	Loss 7.0525 (7.5875)	CeLoss 0.1973 (0.2179)	SegCLSLoss 0.0330 (0.0370)	KLLoss 0.1895 (0.1947)	MaskLoss 0.8772 (1.0752)	MaskBCELoss 0.0613 (0.2410)	MaskDICELoss 0.8159 (0.8343)
Epoch: [0][375/500]	Time 11.903 (11.903)	Loss 4.1058 (6.7254)	CeLoss 0.3320 (0.2475)	SegCLSLoss 0.0178 (0.0264)	KLLoss 0.1079 (0.1548)	MaskLoss 0.5263 (0.9095)	MaskBCELoss 0.0923 (0.1610)	MaskDICELoss 0.4340 (0.7485)
Epoch: [0][376/500]	Time 10.716 (10.716)	Loss 1.2344 (5.4373)	CeLoss 1.2344 (0.4442)	SegCLSLoss 0.0000 (0.0235)	KLLoss 0.0000 (0.1245)	MaskLoss 0.0000 (0.7526)	MaskBCELoss 0.0000 (0.1940)	MaskDICELoss 0.0000 (0.5586)
Epoch: [0][377/500]	Time 10.557 (10.557)	Loss 8.0066 (5.8949)	CeLoss 0.3223 (0.3512)	SegCLSLoss 0.0461 (0.0269)	KLLoss 0.2422 (0.1394)	MaskLoss 1.2211 (0.8478)	MaskBCELoss 0.3914 (0.2319)	MaskDICELoss 0.8297 (0.6159)
Epoch: [0][378/500]	Time  8.417 ( 8.417)	Loss 0.6680 (4.9135)	CeLoss 0.6680 (0.5880)	SegCLSLoss 0.0000 (0.0240)	KLLoss 0.0000 (0.1210)	MaskLoss 0.0000 (0.6735)	MaskBCELoss 0.0000 (0.1992)	MaskDICELoss 0.0000 (0.4742)
Epoch: [0][379/500]	Time  9.504 ( 9.504)	Loss 1.1016 (5.4233)	CeLoss 1.1016 (0.4546)	SegCLSLoss 0.0000 (0.0237)	KLLoss 0.0000 (0.1379)	MaskLoss 0.0000 (0.7412)	MaskBCELoss 0.0000 (0.1852)	MaskDICELoss 0.0000 (0.5561)
[2025-03-04 03:07:43,968] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=0, lr=[0.00029208433734939757], mom=[(0.9, 0.95)]
[2025-03-04 03:07:43,973] [INFO] [timer.py:215:stop] epoch=0/micro_step=3800/global_step=380, RunningAvgSamplesPerSec=1.0256463069065145, CurrSamplesPerSec=1.0253789293839661, MemAllocated=56.66GB, MaxMemAllocated=62.78GB
Epoch: [0][380/500]	Time  9.754 ( 9.754)	Loss 7.5836 (6.7268)	CeLoss 0.1396 (0.3680)	SegCLSLoss 0.0486 (0.0279)	KLLoss 0.2812 (0.1600)	MaskLoss 1.0175 (0.8878)	MaskBCELoss 0.1670 (0.1528)	MaskDICELoss 0.8505 (0.7349)
Epoch: [0][381/500]	Time  9.132 ( 9.132)	Loss 6.4119 (5.1529)	CeLoss 0.2295 (0.5289)	SegCLSLoss 0.0177 (0.0162)	KLLoss 0.0884 (0.0947)	MaskLoss 1.0091 (0.6349)	MaskBCELoss 0.3312 (0.0929)	MaskDICELoss 0.6779 (0.5419)
Epoch: [0][382/500]	Time  9.174 ( 9.174)	Loss 1.3906 (4.8828)	CeLoss 1.3906 (0.6247)	SegCLSLoss 0.0000 (0.0189)	KLLoss 0.0000 (0.1042)	MaskLoss 0.0000 (0.6073)	MaskBCELoss 0.0000 (0.1189)	MaskDICELoss 0.0000 (0.4883)
Epoch: [0][383/500]	Time  9.691 ( 9.691)	Loss 7.9228 (4.6786)	CeLoss 0.2949 (0.4891)	SegCLSLoss 0.0204 (0.0157)	KLLoss 0.1040 (0.0885)	MaskLoss 1.2939 (0.6416)	MaskBCELoss 0.4728 (0.1733)	MaskDICELoss 0.8211 (0.4683)
Epoch: [0][384/500]	Time  9.328 ( 9.328)	Loss 7.6752 (4.9328)	CeLoss 0.2188 (0.3883)	SegCLSLoss 0.0264 (0.0235)	KLLoss 0.1494 (0.1179)	MaskLoss 0.9329 (0.6367)	MaskBCELoss 0.0285 (0.1131)	MaskDICELoss 0.9044 (0.5236)
Epoch: [0][385/500]	Time  9.520 ( 9.520)	Loss 0.9219 (4.7770)	CeLoss 0.9219 (0.3466)	SegCLSLoss 0.0000 (0.0209)	KLLoss 0.0000 (0.1193)	MaskLoss 0.0000 (0.7138)	MaskBCELoss 0.0000 (0.2349)	MaskDICELoss 0.0000 (0.4789)
Epoch: [0][386/500]	Time 10.670 (10.670)	Loss 8.3252 (6.7579)	CeLoss 0.1914 (0.3134)	SegCLSLoss 0.0369 (0.0254)	KLLoss 0.2344 (0.1451)	MaskLoss 1.0111 (0.9525)	MaskBCELoss 0.0348 (0.2222)	MaskDICELoss 0.9763 (0.7303)
Epoch: [0][387/500]	Time  9.332 ( 9.332)	Loss 6.8560 (5.9923)	CeLoss 0.2559 (0.4358)	SegCLSLoss 0.0201 (0.0194)	KLLoss 0.0986 (0.1050)	MaskLoss 1.2200 (0.8925)	MaskBCELoss 0.5445 (0.2831)	MaskDICELoss 0.6755 (0.6095)
Epoch: [0][388/500]	Time  8.365 ( 8.365)	Loss 7.8456 (5.9042)	CeLoss 0.2871 (0.4268)	SegCLSLoss 0.0219 (0.0290)	KLLoss 0.1084 (0.1286)	MaskLoss 1.2672 (0.9263)	MaskBCELoss 0.4497 (0.3460)	MaskDICELoss 0.8175 (0.5803)
Epoch: [0][389/500]	Time 11.027 (11.027)	Loss 5.1498 (6.7770)	CeLoss 0.3418 (0.2675)	SegCLSLoss 0.0195 (0.0245)	KLLoss 0.0771 (0.1310)	MaskLoss 0.7452 (0.9114)	MaskBCELoss 0.2069 (0.1542)	MaskDICELoss 0.5383 (0.7572)
[2025-03-04 03:09:21,349] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=0, lr=[0.0002918433734939759], mom=[(0.9, 0.95)]
[2025-03-04 03:09:21,355] [INFO] [timer.py:215:stop] epoch=0/micro_step=3900/global_step=390, RunningAvgSamplesPerSec=1.0256830664853775, CurrSamplesPerSec=0.8977417032868363, MemAllocated=57.46GB, MaxMemAllocated=62.78GB
Epoch: [0][390/500]	Time 11.142 (11.142)	Loss 6.6415 (5.2434)	CeLoss 0.2168 (0.3227)	SegCLSLoss 0.0265 (0.0219)	KLLoss 0.1689 (0.1188)	MaskLoss 0.8182 (0.6511)	MaskBCELoss 0.0504 (0.0696)	MaskDICELoss 0.7678 (0.5815)
Epoch: [0][391/500]	Time  9.095 ( 9.095)	Loss 0.0767 (5.1161)	CeLoss 0.0767 (0.4992)	SegCLSLoss 0.0000 (0.0186)	KLLoss 0.0000 (0.0941)	MaskLoss 0.0000 (0.7454)	MaskBCELoss 0.0000 (0.2416)	MaskDICELoss 0.0000 (0.5038)
Epoch: [0][392/500]	Time  8.044 ( 8.044)	Loss 6.4606 (4.9335)	CeLoss 0.3164 (0.6509)	SegCLSLoss 0.0199 (0.0153)	KLLoss 0.0972 (0.0837)	MaskLoss 0.8902 (0.6329)	MaskBCELoss 0.1808 (0.1454)	MaskDICELoss 0.7094 (0.4875)
Epoch: [0][393/500]	Time  8.765 ( 8.765)	Loss 5.9071 (5.5620)	CeLoss 0.3027 (0.3784)	SegCLSLoss 0.0188 (0.0284)	KLLoss 0.0664 (0.1125)	MaskLoss 1.0654 (0.7815)	MaskBCELoss 0.4991 (0.1992)	MaskDICELoss 0.5663 (0.5823)
Epoch: [0][394/500]	Time 10.824 (10.824)	Loss 8.3431 (6.9214)	CeLoss 0.2480 (0.3260)	SegCLSLoss 0.0236 (0.0255)	KLLoss 0.1387 (0.1430)	MaskLoss 1.0989 (0.9664)	MaskBCELoss 0.1412 (0.2152)	MaskDICELoss 0.9578 (0.7512)
Epoch: [0][395/500]	Time 10.375 (10.375)	Loss 0.7383 (4.6155)	CeLoss 0.7383 (0.4663)	SegCLSLoss 0.0000 (0.0174)	KLLoss 0.0000 (0.1071)	MaskLoss 0.0000 (0.6618)	MaskBCELoss 0.0000 (0.2102)	MaskDICELoss 0.0000 (0.4516)
Epoch: [0][396/500]	Time 10.322 (10.322)	Loss 5.4586 (5.4062)	CeLoss 0.2559 (0.4752)	SegCLSLoss 0.0172 (0.0211)	KLLoss 0.0903 (0.1139)	MaskLoss 0.8885 (0.7787)	MaskBCELoss 0.3338 (0.2372)	MaskDICELoss 0.5547 (0.5415)
Epoch: [0][397/500]	Time  9.217 ( 9.217)	Loss 0.8203 (4.6789)	CeLoss 0.8203 (0.5101)	SegCLSLoss 0.0000 (0.0130)	KLLoss 0.0000 (0.0708)	MaskLoss 0.0000 (0.6646)	MaskBCELoss 0.0000 (0.2042)	MaskDICELoss 0.0000 (0.4604)
Epoch: [0][398/500]	Time 10.674 (10.674)	Loss 7.8289 (6.8698)	CeLoss 0.1855 (0.4333)	SegCLSLoss 0.0625 (0.0247)	KLLoss 0.2402 (0.1284)	MaskLoss 1.2183 (0.9531)	MaskBCELoss 0.3958 (0.2216)	MaskDICELoss 0.8225 (0.7316)
Epoch: [0][399/500]	Time  9.963 ( 9.963)	Loss 6.4989 (4.3713)	CeLoss 0.2539 (0.4426)	SegCLSLoss 0.0270 (0.0156)	KLLoss 0.1475 (0.0910)	MaskLoss 1.2759 (0.5722)	MaskBCELoss 0.6871 (0.1247)	MaskDICELoss 0.5888 (0.4475)
[2025-03-04 03:10:59,156] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=0, lr=[0.0002916024096385542], mom=[(0.9, 0.95)]
[2025-03-04 03:10:59,161] [INFO] [timer.py:215:stop] epoch=0/micro_step=4000/global_step=400, RunningAvgSamplesPerSec=1.0256053860356136, CurrSamplesPerSec=0.9500390214854303, MemAllocated=57.26GB, MaxMemAllocated=62.78GB
Epoch: [0][400/500]	Time 10.529 (10.529)	Loss 8.1293 (6.4091)	CeLoss 0.1816 (0.4146)	SegCLSLoss 0.0294 (0.0253)	KLLoss 0.1738 (0.1559)	MaskLoss 1.0728 (0.8764)	MaskBCELoss 0.1374 (0.1976)	MaskDICELoss 0.9354 (0.6788)
Epoch: [0][401/500]	Time  9.206 ( 9.206)	Loss 0.4980 (5.4473)	CeLoss 0.4980 (0.3821)	SegCLSLoss 0.0000 (0.0210)	KLLoss 0.0000 (0.1145)	MaskLoss 0.0000 (0.7945)	MaskBCELoss 0.0000 (0.2359)	MaskDICELoss 0.0000 (0.5585)
Epoch: [0][402/500]	Time  9.760 ( 9.760)	Loss 5.5560 (6.6409)	CeLoss 0.2754 (0.3230)	SegCLSLoss 0.0170 (0.0231)	KLLoss 0.0791 (0.1328)	MaskLoss 0.9514 (0.9918)	MaskBCELoss 0.4031 (0.2935)	MaskDICELoss 0.5483 (0.6983)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([31, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][403/500]	Time 10.424 (10.424)	Loss 6.0944 (5.3385)	CeLoss 0.3359 (0.3445)	SegCLSLoss 0.0173 (0.0281)	KLLoss 0.0669 (0.1286)	MaskLoss 1.1880 (0.7789)	MaskBCELoss 0.6367 (0.2300)	MaskDICELoss 0.5514 (0.5489)
Epoch: [0][404/500]	Time 10.277 (10.277)	Loss 4.5253 (5.3326)	CeLoss 0.2871 (0.3403)	SegCLSLoss 0.0150 (0.0217)	KLLoss 0.0947 (0.1041)	MaskLoss 0.7698 (0.7333)	MaskBCELoss 0.3373 (0.1649)	MaskDICELoss 0.4325 (0.5685)
Epoch: [0][405/500]	Time  8.865 ( 8.865)	Loss 2.5488 (5.5369)	CeLoss 0.2969 (0.4232)	SegCLSLoss 0.0160 (0.0271)	KLLoss 0.0957 (0.1240)	MaskLoss 0.5974 (0.7661)	MaskBCELoss 0.4385 (0.1921)	MaskDICELoss 0.1589 (0.5740)
Epoch: [0][406/500]	Time 10.653 (10.653)	Loss 7.5884 (6.7894)	CeLoss 0.1260 (0.2054)	SegCLSLoss 0.0688 (0.0268)	KLLoss 0.2539 (0.1243)	MaskLoss 1.1452 (0.9193)	MaskBCELoss 0.3312 (0.1513)	MaskDICELoss 0.8140 (0.7679)
Epoch: [0][407/500]	Time 10.443 (10.443)	Loss 7.7171 (6.5822)	CeLoss 0.2480 (0.3143)	SegCLSLoss 0.0288 (0.0249)	KLLoss 0.2002 (0.1412)	MaskLoss 1.0472 (0.9246)	MaskBCELoss 0.1869 (0.2137)	MaskDICELoss 0.8603 (0.7109)
Epoch: [0][408/500]	Time  9.150 ( 9.150)	Loss 7.2687 (5.0828)	CeLoss 0.1748 (0.4960)	SegCLSLoss 0.0288 (0.0228)	KLLoss 0.1699 (0.1125)	MaskLoss 1.0611 (0.6392)	MaskBCELoss 0.2633 (0.1085)	MaskDICELoss 0.7978 (0.5307)
Epoch: [0][409/500]	Time  9.763 ( 9.763)	Loss 7.8332 (4.7867)	CeLoss 0.2275 (0.4777)	SegCLSLoss 0.0299 (0.0168)	KLLoss 0.1621 (0.0913)	MaskLoss 1.1952 (0.7123)	MaskBCELoss 0.3555 (0.2481)	MaskDICELoss 0.8397 (0.4641)
[2025-03-04 03:12:37,009] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=0, lr=[0.0002913614457831325], mom=[(0.9, 0.95)]
[2025-03-04 03:12:37,015] [INFO] [timer.py:215:stop] epoch=0/micro_step=4100/global_step=410, RunningAvgSamplesPerSec=1.0255193238200055, CurrSamplesPerSec=1.0741181812620015, MemAllocated=56.71GB, MaxMemAllocated=62.78GB
Epoch: [0][410/500]	Time  9.312 ( 9.312)	Loss 1.3438 (4.8926)	CeLoss 1.3438 (0.5288)	SegCLSLoss 0.0000 (0.0198)	KLLoss 0.0000 (0.1032)	MaskLoss 0.0000 (0.6066)	MaskBCELoss 0.0000 (0.1003)	MaskDICELoss 0.0000 (0.5063)
Epoch: [0][411/500]	Time  9.019 ( 9.019)	Loss 10.9214 (6.6169)	CeLoss 0.2734 (0.4049)	SegCLSLoss 0.0152 (0.0258)	KLLoss 0.0703 (0.1358)	MaskLoss 2.4709 (1.0219)	MaskBCELoss 1.5329 (0.3519)	MaskDICELoss 0.9380 (0.6700)
Epoch: [0][412/500]	Time  7.886 ( 7.886)	Loss 8.5036 (4.9602)	CeLoss 0.1992 (0.4450)	SegCLSLoss 0.0645 (0.0248)	KLLoss 0.2373 (0.1270)	MaskLoss 1.3870 (0.7487)	MaskBCELoss 0.5102 (0.2690)	MaskDICELoss 0.8768 (0.4798)
Epoch: [0][413/500]	Time  9.866 ( 9.866)	Loss 7.0125 (5.9310)	CeLoss 0.2373 (0.5268)	SegCLSLoss 0.0310 (0.0208)	KLLoss 0.1738 (0.1250)	MaskLoss 0.9101 (0.8209)	MaskBCELoss 0.1157 (0.2164)	MaskDICELoss 0.7944 (0.6045)
Epoch: [0][414/500]	Time  6.508 ( 6.508)	Loss 6.6267 (2.1713)	CeLoss 0.2178 (0.8868)	SegCLSLoss 0.0215 (0.0046)	KLLoss 0.1201 (0.0286)	MaskLoss 1.1666 (0.2049)	MaskBCELoss 0.5089 (0.0643)	MaskDICELoss 0.6577 (0.1406)
Epoch: [0][415/500]	Time  8.874 ( 8.874)	Loss 8.3993 (6.0622)	CeLoss 0.1787 (0.3842)	SegCLSLoss 0.0366 (0.0230)	KLLoss 0.1904 (0.1237)	MaskLoss 1.0557 (0.8755)	MaskBCELoss 0.0722 (0.2435)	MaskDICELoss 0.9835 (0.6320)
Epoch: [0][416/500]	Time  8.439 ( 8.439)	Loss 7.5817 (6.3527)	CeLoss 0.1309 (0.3636)	SegCLSLoss 0.0776 (0.0294)	KLLoss 0.2539 (0.1318)	MaskLoss 1.2547 (0.9013)	MaskBCELoss 0.4799 (0.2280)	MaskDICELoss 0.7748 (0.6734)
Epoch: [0][417/500]	Time  9.621 ( 9.621)	Loss 1.4766 (5.0331)	CeLoss 1.4766 (0.5226)	SegCLSLoss 0.0000 (0.0232)	KLLoss 0.0000 (0.1156)	MaskLoss 0.0000 (0.7283)	MaskBCELoss 0.0000 (0.2406)	MaskDICELoss 0.0000 (0.4877)
Epoch: [0][418/500]	Time  8.599 ( 8.599)	Loss 8.2560 (6.1226)	CeLoss 0.2080 (0.5440)	SegCLSLoss 0.0310 (0.0215)	KLLoss 0.1777 (0.1162)	MaskLoss 1.0590 (0.8211)	MaskBCELoss 0.1031 (0.1863)	MaskDICELoss 0.9559 (0.6348)
Epoch: [0][419/500]	Time  9.893 ( 9.893)	Loss 7.5510 (5.4593)	CeLoss 0.1973 (0.4465)	SegCLSLoss 0.0325 (0.0186)	KLLoss 0.1807 (0.1177)	MaskLoss 1.0364 (0.7288)	MaskBCELoss 0.1891 (0.1574)	MaskDICELoss 0.8473 (0.5714)
[2025-03-04 03:14:05,569] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=0, lr=[0.0002911204819277108], mom=[(0.9, 0.95)]
[2025-03-04 03:14:05,575] [INFO] [timer.py:215:stop] epoch=0/micro_step=4200/global_step=420, RunningAvgSamplesPerSec=1.02778037100962, CurrSamplesPerSec=1.0149809182172216, MemAllocated=57.24GB, MaxMemAllocated=62.78GB
Epoch: [0][420/500]	Time  9.854 ( 9.854)	Loss 5.5542 (4.4834)	CeLoss 0.2441 (0.4499)	SegCLSLoss 0.0173 (0.0186)	KLLoss 0.1187 (0.0967)	MaskLoss 1.0991 (0.6499)	MaskBCELoss 0.6016 (0.2120)	MaskDICELoss 0.4975 (0.4379)
Epoch: [0][421/500]	Time  8.142 ( 8.142)	Loss 6.0749 (6.0507)	CeLoss 0.2256 (0.3681)	SegCLSLoss 0.0359 (0.0267)	KLLoss 0.1572 (0.1480)	MaskLoss 1.0151 (0.8429)	MaskBCELoss 0.4078 (0.2037)	MaskDICELoss 0.6074 (0.6393)
Epoch: [0][422/500]	Time 11.154 (11.154)	Loss 8.0545 (7.1380)	CeLoss 0.2119 (0.2369)	SegCLSLoss 0.0347 (0.0270)	KLLoss 0.1543 (0.1549)	MaskLoss 1.0899 (1.0411)	MaskBCELoss 0.1746 (0.2660)	MaskDICELoss 0.9153 (0.7751)
Epoch: [0][423/500]	Time 10.229 (10.229)	Loss 0.8672 (6.2326)	CeLoss 0.8672 (0.3538)	SegCLSLoss 0.0000 (0.0189)	KLLoss 0.0000 (0.1196)	MaskLoss 0.0000 (0.8380)	MaskBCELoss 0.0000 (0.1590)	MaskDICELoss 0.0000 (0.6790)
Epoch: [0][424/500]	Time  8.361 ( 8.361)	Loss 1.0625 (5.7510)	CeLoss 1.0625 (0.4408)	SegCLSLoss 0.0000 (0.0304)	KLLoss 0.0000 (0.1450)	MaskLoss 0.0000 (0.7540)	MaskBCELoss 0.0000 (0.1470)	MaskDICELoss 0.0000 (0.6070)
Epoch: [0][425/500]	Time 10.125 (10.125)	Loss 0.9805 (6.3363)	CeLoss 0.9805 (0.3293)	SegCLSLoss 0.0000 (0.0207)	KLLoss 0.0000 (0.1328)	MaskLoss 0.0000 (0.9702)	MaskBCELoss 0.0000 (0.3163)	MaskDICELoss 0.0000 (0.6539)
Epoch: [0][426/500]	Time  8.991 ( 8.991)	Loss 7.2957 (5.7995)	CeLoss 0.2617 (0.6974)	SegCLSLoss 0.0199 (0.0166)	KLLoss 0.1445 (0.1035)	MaskLoss 0.9776 (0.8004)	MaskBCELoss 0.1568 (0.2355)	MaskDICELoss 0.8208 (0.5649)
Epoch: [0][427/500]	Time  9.013 ( 9.013)	Loss 8.3195 (6.7530)	CeLoss 0.2734 (0.3547)	SegCLSLoss 0.0300 (0.0308)	KLLoss 0.1885 (0.1525)	MaskLoss 1.0192 (0.9370)	MaskBCELoss 0.0518 (0.2109)	MaskDICELoss 0.9674 (0.7260)
Epoch: [0][428/500]	Time  9.451 ( 9.451)	Loss 1.1641 (4.2992)	CeLoss 1.1641 (0.5663)	SegCLSLoss 0.0000 (0.0182)	KLLoss 0.0000 (0.1033)	MaskLoss 0.0000 (0.6108)	MaskBCELoss 0.0000 (0.2110)	MaskDICELoss 0.0000 (0.3998)
Epoch: [0][429/500]	Time 10.327 (10.327)	Loss 5.0417 (5.3647)	CeLoss 0.2402 (0.4506)	SegCLSLoss 0.0167 (0.0245)	KLLoss 0.1387 (0.1421)	MaskLoss 0.9130 (0.7842)	MaskBCELoss 0.4415 (0.2524)	MaskDICELoss 0.4715 (0.5318)
[2025-03-04 03:15:40,379] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=0, lr=[0.00029087951807228913], mom=[(0.9, 0.95)]
[2025-03-04 03:15:40,385] [INFO] [timer.py:215:stop] epoch=0/micro_step=4300/global_step=430, RunningAvgSamplesPerSec=1.0283987570377389, CurrSamplesPerSec=1.1092724488278718, MemAllocated=57.27GB, MaxMemAllocated=62.78GB
Epoch: [0][430/500]	Time  9.017 ( 9.017)	Loss 8.0664 (5.8850)	CeLoss 0.2520 (0.4843)	SegCLSLoss 0.0175 (0.0215)	KLLoss 0.1094 (0.1198)	MaskLoss 1.1874 (0.8518)	MaskBCELoss 0.3003 (0.2574)	MaskDICELoss 0.8871 (0.5944)
Epoch: [0][431/500]	Time  9.980 ( 9.980)	Loss 0.5508 (6.0434)	CeLoss 0.5508 (0.3896)	SegCLSLoss 0.0000 (0.0269)	KLLoss 0.0000 (0.1520)	MaskLoss 0.0000 (0.8534)	MaskBCELoss 0.0000 (0.2230)	MaskDICELoss 0.0000 (0.6303)
Epoch: [0][432/500]	Time 10.970 (10.970)	Loss 8.4633 (6.3657)	CeLoss 0.1152 (0.2787)	SegCLSLoss 0.1250 (0.0418)	KLLoss 0.2383 (0.1695)	MaskLoss 1.2576 (0.9316)	MaskBCELoss 0.3356 (0.2593)	MaskDICELoss 0.9220 (0.6722)
Epoch: [0][433/500]	Time  9.815 ( 9.815)	Loss 5.5772 (5.7280)	CeLoss 0.2773 (0.4189)	SegCLSLoss 0.0216 (0.0218)	KLLoss 0.1387 (0.1384)	MaskLoss 0.6983 (0.7839)	MaskBCELoss 0.0728 (0.1852)	MaskDICELoss 0.6255 (0.5986)
Epoch: [0][434/500]	Time 10.510 (10.510)	Loss 0.6953 (5.3336)	CeLoss 0.6953 (0.4262)	SegCLSLoss 0.0000 (0.0202)	KLLoss 0.0000 (0.1101)	MaskLoss 0.0000 (0.7444)	MaskBCELoss 0.0000 (0.1947)	MaskDICELoss 0.0000 (0.5497)
Epoch: [0][435/500]	Time 10.841 (10.841)	Loss 0.1167 (6.2122)	CeLoss 0.1167 (0.2409)	SegCLSLoss 0.0000 (0.0260)	KLLoss 0.0000 (0.1524)	MaskLoss 0.0000 (0.9302)	MaskBCELoss 0.0000 (0.2726)	MaskDICELoss 0.0000 (0.6576)
Epoch: [0][436/500]	Time  8.147 ( 8.147)	Loss 1.1797 (5.5474)	CeLoss 1.1797 (0.4878)	SegCLSLoss 0.0000 (0.0184)	KLLoss 0.0000 (0.1172)	MaskLoss 0.0000 (0.7643)	MaskBCELoss 0.0000 (0.1969)	MaskDICELoss 0.0000 (0.5675)
Epoch: [0][437/500]	Time 11.023 (11.023)	Loss 7.8332 (5.7869)	CeLoss 0.2471 (0.4507)	SegCLSLoss 0.0240 (0.0197)	KLLoss 0.1157 (0.1143)	MaskLoss 0.9836 (0.7948)	MaskBCELoss 0.0685 (0.1911)	MaskDICELoss 0.9152 (0.6037)
Epoch: [0][438/500]	Time  9.929 ( 9.929)	Loss 8.4818 (6.8165)	CeLoss 0.2637 (0.2852)	SegCLSLoss 0.0320 (0.0333)	KLLoss 0.1914 (0.1742)	MaskLoss 1.0578 (0.9537)	MaskBCELoss 0.0753 (0.2149)	MaskDICELoss 0.9826 (0.7388)
Epoch: [0][439/500]	Time 10.979 (10.979)	Loss 6.0925 (7.7305)	CeLoss 0.3184 (0.2400)	SegCLSLoss 0.0201 (0.0311)	KLLoss 0.1299 (0.1771)	MaskLoss 0.8708 (1.1672)	MaskBCELoss 0.2218 (0.3399)	MaskDICELoss 0.6490 (0.8273)
[2025-03-04 03:17:22,213] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=0, lr=[0.00029063855421686746], mom=[(0.9, 0.95)]
[2025-03-04 03:17:22,218] [INFO] [timer.py:215:stop] epoch=0/micro_step=4400/global_step=440, RunningAvgSamplesPerSec=1.0272940160797615, CurrSamplesPerSec=1.037421154966749, MemAllocated=57.28GB, MaxMemAllocated=62.78GB
Epoch: [0][440/500]	Time  9.641 ( 9.641)	Loss 6.8377 (6.7116)	CeLoss 0.2061 (0.3210)	SegCLSLoss 0.0347 (0.0304)	KLLoss 0.1963 (0.1701)	MaskLoss 1.2133 (1.0383)	MaskBCELoss 0.5482 (0.3503)	MaskDICELoss 0.6652 (0.6881)
Epoch: [0][441/500]	Time  9.321 ( 9.321)	Loss 1.6875 (4.9976)	CeLoss 1.6875 (0.6799)	SegCLSLoss 0.0000 (0.0159)	KLLoss 0.0000 (0.0971)	MaskLoss 0.0000 (0.6905)	MaskBCELoss 0.0000 (0.2186)	MaskDICELoss 0.0000 (0.4720)
Epoch: [0][442/500]	Time  9.140 ( 9.140)	Loss 7.5786 (5.2242)	CeLoss 0.1914 (0.4783)	SegCLSLoss 0.0344 (0.0186)	KLLoss 0.2139 (0.1102)	MaskLoss 1.0333 (0.6813)	MaskBCELoss 0.1850 (0.1374)	MaskDICELoss 0.8483 (0.5439)
Epoch: [0][443/500]	Time 10.053 (10.053)	Loss 7.3141 (6.1227)	CeLoss 0.2871 (0.4174)	SegCLSLoss 0.0189 (0.0275)	KLLoss 0.1299 (0.1520)	MaskLoss 1.0304 (0.8354)	MaskBCELoss 0.2259 (0.1906)	MaskDICELoss 0.8046 (0.6448)
Epoch: [0][444/500]	Time 10.857 (10.857)	Loss 8.1141 (5.1448)	CeLoss 0.2930 (0.3768)	SegCLSLoss 0.0260 (0.0208)	KLLoss 0.1631 (0.1393)	MaskLoss 1.1815 (0.7522)	MaskBCELoss 0.3012 (0.2332)	MaskDICELoss 0.8804 (0.5190)
Epoch: [0][445/500]	Time  7.767 ( 7.767)	Loss 1.3047 (4.5506)	CeLoss 1.3047 (0.7383)	SegCLSLoss 0.0000 (0.0140)	KLLoss 0.0000 (0.0803)	MaskLoss 0.0000 (0.6025)	MaskBCELoss 0.0000 (0.1825)	MaskDICELoss 0.0000 (0.4200)
Epoch: [0][446/500]	Time  8.790 ( 8.790)	Loss 1.0391 (4.2330)	CeLoss 1.0391 (0.7875)	SegCLSLoss 0.0000 (0.0159)	KLLoss 0.0000 (0.0947)	MaskLoss 0.0000 (0.5467)	MaskBCELoss 0.0000 (0.1718)	MaskDICELoss 0.0000 (0.3749)
Epoch: [0][447/500]	Time 10.852 (10.852)	Loss 7.3815 (7.8731)	CeLoss 0.1787 (0.2497)	SegCLSLoss 0.0298 (0.0316)	KLLoss 0.1982 (0.1978)	MaskLoss 0.9404 (1.1739)	MaskBCELoss 0.0888 (0.3301)	MaskDICELoss 0.8517 (0.8438)
Epoch: [0][448/500]	Time  8.670 ( 8.670)	Loss 0.7734 (5.7482)	CeLoss 0.7734 (0.3830)	SegCLSLoss 0.0000 (0.0239)	KLLoss 0.0000 (0.1475)	MaskLoss 0.0000 (0.7594)	MaskBCELoss 0.0000 (0.1448)	MaskDICELoss 0.0000 (0.6145)
Epoch: [0][449/500]	Time 10.780 (10.780)	Loss 5.9247 (5.1734)	CeLoss 0.2188 (0.4334)	SegCLSLoss 0.0194 (0.0190)	KLLoss 0.1011 (0.1112)	MaskLoss 0.8341 (0.7390)	MaskBCELoss 0.1796 (0.2154)	MaskDICELoss 0.6544 (0.5236)
[2025-03-04 03:18:58,429] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=0, lr=[0.00029039759036144573], mom=[(0.9, 0.95)]
[2025-03-04 03:18:58,434] [INFO] [timer.py:215:stop] epoch=0/micro_step=4500/global_step=450, RunningAvgSamplesPerSec=1.027563254025775, CurrSamplesPerSec=1.0015921390697526, MemAllocated=56.71GB, MaxMemAllocated=62.78GB
Epoch: [0][450/500]	Time  9.986 ( 9.986)	Loss 1.5703 (5.5520)	CeLoss 1.5703 (0.3744)	SegCLSLoss 0.0000 (0.0189)	KLLoss 0.0000 (0.1297)	MaskLoss 0.0000 (0.8281)	MaskBCELoss 0.0000 (0.2643)	MaskDICELoss 0.0000 (0.5637)
Epoch: [0][451/500]	Time  9.601 ( 9.601)	Loss 2.5905 (2.3707)	CeLoss 0.2256 (0.4300)	SegCLSLoss 0.0228 (0.0077)	KLLoss 0.1562 (0.0491)	MaskLoss 0.3175 (0.2776)	MaskBCELoss 0.0574 (0.0555)	MaskDICELoss 0.2602 (0.2221)
Epoch: [0][452/500]	Time  7.572 ( 7.572)	Loss 0.5234 (4.2077)	CeLoss 0.5234 (0.5559)	SegCLSLoss 0.0000 (0.0199)	KLLoss 0.0000 (0.1268)	MaskLoss 0.0000 (0.5084)	MaskBCELoss 0.0000 (0.0921)	MaskDICELoss 0.0000 (0.4163)
Epoch: [0][453/500]	Time  9.040 ( 9.040)	Loss 5.9005 (6.7195)	CeLoss 0.2070 (0.3098)	SegCLSLoss 0.0300 (0.0250)	KLLoss 0.2002 (0.1718)	MaskLoss 0.8144 (1.1027)	MaskBCELoss 0.1727 (0.4327)	MaskDICELoss 0.6417 (0.6700)
Epoch: [0][454/500]	Time  8.139 ( 8.139)	Loss 8.6720 (6.3097)	CeLoss 0.3457 (0.5094)	SegCLSLoss 0.0194 (0.0215)	KLLoss 0.1123 (0.1456)	MaskLoss 1.9983 (1.0038)	MaskBCELoss 1.2972 (0.3977)	MaskDICELoss 0.7011 (0.6060)
Epoch: [0][455/500]	Time 10.677 (10.677)	Loss 8.5239 (5.4242)	CeLoss 0.3145 (0.3397)	SegCLSLoss 0.0250 (0.0213)	KLLoss 0.1680 (0.1306)	MaskLoss 1.0433 (0.7303)	MaskBCELoss 0.0528 (0.1499)	MaskDICELoss 0.9905 (0.5805)
Epoch: [0][456/500]	Time  9.314 ( 9.314)	Loss 5.6273 (5.6356)	CeLoss 0.2871 (0.5098)	SegCLSLoss 0.0205 (0.0208)	KLLoss 0.1406 (0.1364)	MaskLoss 0.7981 (0.7578)	MaskBCELoss 0.1991 (0.1805)	MaskDICELoss 0.5989 (0.5773)
Epoch: [0][457/500]	Time  9.378 ( 9.378)	Loss 0.5469 (5.1375)	CeLoss 0.5469 (0.3937)	SegCLSLoss 0.0000 (0.0181)	KLLoss 0.0000 (0.1044)	MaskLoss 0.0000 (0.7110)	MaskBCELoss 0.0000 (0.1762)	MaskDICELoss 0.0000 (0.5348)
Epoch: [0][458/500]	Time 10.283 (10.283)	Loss 7.2921 (5.9302)	CeLoss 0.1953 (0.4182)	SegCLSLoss 0.0337 (0.0192)	KLLoss 0.2314 (0.1410)	MaskLoss 1.2290 (0.8811)	MaskBCELoss 0.4973 (0.2812)	MaskDICELoss 0.7318 (0.5999)
Epoch: [0][459/500]	Time 10.497 (10.497)	Loss 1.0234 (5.5565)	CeLoss 1.0234 (0.3475)	SegCLSLoss 0.0000 (0.0213)	KLLoss 0.0000 (0.1319)	MaskLoss 0.0000 (0.8179)	MaskBCELoss 0.0000 (0.2463)	MaskDICELoss 0.0000 (0.5717)
[2025-03-04 03:20:33,114] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=0, lr=[0.00029015662650602405], mom=[(0.9, 0.95)]
[2025-03-04 03:20:33,119] [INFO] [timer.py:215:stop] epoch=0/micro_step=4600/global_step=460, RunningAvgSamplesPerSec=1.0281740066059482, CurrSamplesPerSec=0.9821033879478892, MemAllocated=57.16GB, MaxMemAllocated=62.78GB
Epoch: [0][460/500]	Time 10.184 (10.184)	Loss 7.9526 (6.0829)	CeLoss 0.1396 (0.3774)	SegCLSLoss 0.0742 (0.0262)	KLLoss 0.2617 (0.1502)	MaskLoss 1.3101 (0.9659)	MaskBCELoss 0.4943 (0.3641)	MaskDICELoss 0.8158 (0.6018)
Epoch: [0][461/500]	Time 10.472 (10.472)	Loss 5.1963 (4.5212)	CeLoss 0.2559 (0.3747)	SegCLSLoss 0.0161 (0.0174)	KLLoss 0.0781 (0.1025)	MaskLoss 0.7540 (0.6452)	MaskBCELoss 0.1963 (0.1877)	MaskDICELoss 0.5577 (0.4575)
Epoch: [0][462/500]	Time  8.604 ( 8.604)	Loss 0.3789 (3.7682)	CeLoss 0.3789 (0.6322)	SegCLSLoss 0.0000 (0.0098)	KLLoss 0.0000 (0.0614)	MaskLoss 0.0000 (0.4870)	MaskBCELoss 0.0000 (0.1378)	MaskDICELoss 0.0000 (0.3493)
Epoch: [0][463/500]	Time  7.316 ( 7.316)	Loss 6.1243 (3.6594)	CeLoss 0.3770 (0.5661)	SegCLSLoss 0.0170 (0.0155)	KLLoss 0.0879 (0.0823)	MaskLoss 0.8709 (0.4867)	MaskBCELoss 0.2193 (0.1484)	MaskDICELoss 0.6516 (0.3383)
Epoch: [0][464/500]	Time 10.544 (10.544)	Loss 1.3594 (5.7115)	CeLoss 1.3594 (0.5580)	SegCLSLoss 0.0000 (0.0229)	KLLoss 0.0000 (0.1362)	MaskLoss 0.0000 (0.7393)	MaskBCELoss 0.0000 (0.1514)	MaskDICELoss 0.0000 (0.5879)
Epoch: [0][465/500]	Time  8.619 ( 8.619)	Loss 0.0786 (3.2057)	CeLoss 0.0786 (0.5155)	SegCLSLoss 0.0000 (0.0089)	KLLoss 0.0000 (0.0468)	MaskLoss 0.0000 (0.3764)	MaskBCELoss 0.0000 (0.0620)	MaskDICELoss 0.0000 (0.3143)
Epoch: [0][466/500]	Time  9.372 ( 9.372)	Loss 8.7297 (6.9655)	CeLoss 0.3672 (0.3374)	SegCLSLoss 0.0303 (0.0336)	KLLoss 0.2285 (0.1815)	MaskLoss 1.0648 (0.9990)	MaskBCELoss 0.0663 (0.2603)	MaskDICELoss 0.9985 (0.7387)
Epoch: [0][467/500]	Time  9.149 ( 9.149)	Loss 7.4145 (4.7475)	CeLoss 0.3027 (0.5013)	SegCLSLoss 0.0262 (0.0218)	KLLoss 0.1621 (0.1152)	MaskLoss 1.1847 (0.6494)	MaskBCELoss 0.4236 (0.1792)	MaskDICELoss 0.7611 (0.4702)
Epoch: [0][468/500]	Time 10.774 (10.774)	Loss 0.8672 (6.8972)	CeLoss 0.8672 (0.2900)	SegCLSLoss 0.0000 (0.0308)	KLLoss 0.0000 (0.1664)	MaskLoss 0.0000 (0.9601)	MaskBCELoss 0.0000 (0.2091)	MaskDICELoss 0.0000 (0.7509)
Epoch: [0][469/500]	Time 10.500 (10.500)	Loss 5.0517 (5.7576)	CeLoss 0.2383 (0.2240)	SegCLSLoss 0.0187 (0.0211)	KLLoss 0.1221 (0.1244)	MaskLoss 0.8388 (0.8665)	MaskBCELoss 0.3383 (0.2557)	MaskDICELoss 0.5005 (0.6109)
[2025-03-04 03:22:09,353] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=0, lr=[0.0002899156626506024], mom=[(0.9, 0.95)]
[2025-03-04 03:22:09,359] [INFO] [timer.py:215:stop] epoch=0/micro_step=4700/global_step=470, RunningAvgSamplesPerSec=1.0284079316612782, CurrSamplesPerSec=0.9185093774017695, MemAllocated=57.26GB, MaxMemAllocated=62.78GB
Epoch: [0][470/500]	Time 10.889 (10.889)	Loss 8.2785 (6.6423)	CeLoss 0.2412 (0.2466)	SegCLSLoss 0.0325 (0.0256)	KLLoss 0.1855 (0.1406)	MaskLoss 1.0330 (0.9436)	MaskBCELoss 0.0714 (0.2177)	MaskDICELoss 0.9615 (0.7259)
Epoch: [0][471/500]	Time  9.881 ( 9.881)	Loss 4.5821 (4.6379)	CeLoss 0.2158 (0.4918)	SegCLSLoss 0.0195 (0.0179)	KLLoss 0.0928 (0.1058)	MaskLoss 0.6845 (0.6239)	MaskBCELoss 0.2021 (0.1599)	MaskDICELoss 0.4825 (0.4639)
Epoch: [0][472/500]	Time  7.618 ( 7.618)	Loss 0.2559 (3.6127)	CeLoss 0.2559 (0.6646)	SegCLSLoss 0.0000 (0.0112)	KLLoss 0.0000 (0.0651)	MaskLoss 0.0000 (0.5060)	MaskBCELoss 0.0000 (0.1951)	MaskDICELoss 0.0000 (0.3109)
Epoch: [0][473/500]	Time  8.262 ( 8.262)	Loss 8.4911 (4.8542)	CeLoss 0.2129 (0.5873)	SegCLSLoss 0.0347 (0.0236)	KLLoss 0.1904 (0.1221)	MaskLoss 1.0794 (0.6449)	MaskBCELoss 0.0943 (0.1710)	MaskDICELoss 0.9851 (0.4739)
Epoch: [0][474/500]	Time  9.542 ( 9.542)	Loss 5.4466 (5.7471)	CeLoss 0.2559 (0.3184)	SegCLSLoss 0.0212 (0.0218)	KLLoss 0.1416 (0.1150)	MaskLoss 0.9141 (0.9281)	MaskBCELoss 0.3787 (0.3536)	MaskDICELoss 0.5354 (0.5745)
Epoch: [0][475/500]	Time  9.423 ( 9.423)	Loss 0.8047 (5.3604)	CeLoss 0.8047 (0.3680)	SegCLSLoss 0.0000 (0.0241)	KLLoss 0.0000 (0.1252)	MaskLoss 0.0000 (0.7899)	MaskBCELoss 0.0000 (0.2440)	MaskDICELoss 0.0000 (0.5459)
Epoch: [0][476/500]	Time  9.386 ( 9.386)	Loss 6.9628 (5.5058)	CeLoss 0.2695 (0.3879)	SegCLSLoss 0.0344 (0.0252)	KLLoss 0.1953 (0.1352)	MaskLoss 1.2452 (0.8527)	MaskBCELoss 0.5802 (0.3086)	MaskDICELoss 0.6650 (0.5441)
Epoch: [0][477/500]	Time 10.839 (10.839)	Loss 4.8912 (7.0055)	CeLoss 0.2334 (0.2294)	SegCLSLoss 0.0188 (0.0259)	KLLoss 0.0674 (0.1544)	MaskLoss 0.6789 (0.9526)	MaskBCELoss 0.1417 (0.1687)	MaskDICELoss 0.5372 (0.7839)
Epoch: [0][478/500]	Time  9.659 ( 9.659)	Loss 5.7762 (4.3062)	CeLoss 0.3887 (0.5383)	SegCLSLoss 0.0192 (0.0156)	KLLoss 0.0981 (0.0821)	MaskLoss 0.7720 (0.5103)	MaskBCELoss 0.1494 (0.0673)	MaskDICELoss 0.6227 (0.4430)
Epoch: [0][479/500]	Time  8.284 ( 8.284)	Loss 7.0741 (4.0236)	CeLoss 0.3301 (0.6198)	SegCLSLoss 0.0223 (0.0167)	KLLoss 0.0947 (0.0821)	MaskLoss 1.2488 (0.5423)	MaskBCELoss 0.5589 (0.1709)	MaskDICELoss 0.6898 (0.3714)
[2025-03-04 03:23:40,999] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=0, lr=[0.0002896746987951807], mom=[(0.9, 0.95)]
[2025-03-04 03:23:41,005] [INFO] [timer.py:215:stop] epoch=0/micro_step=4800/global_step=480, RunningAvgSamplesPerSec=1.029650045805057, CurrSamplesPerSec=1.1430033345263964, MemAllocated=56.95GB, MaxMemAllocated=62.78GB
Epoch: [0][480/500]	Time  8.751 ( 8.751)	Loss 5.8689 (5.9541)	CeLoss 0.5391 (0.3787)	SegCLSLoss 0.0154 (0.0265)	KLLoss 0.0894 (0.1358)	MaskLoss 1.0022 (0.8461)	MaskBCELoss 0.4642 (0.2237)	MaskDICELoss 0.5380 (0.6223)
Epoch: [0][481/500]	Time  9.809 ( 9.809)	Loss 7.9703 (7.4207)	CeLoss 0.2910 (0.4398)	SegCLSLoss 0.0222 (0.0252)	KLLoss 0.1309 (0.1421)	MaskLoss 1.3017 (1.0202)	MaskBCELoss 0.4794 (0.2226)	MaskDICELoss 0.8222 (0.7976)
Epoch: [0][482/500]	Time  9.939 ( 9.939)	Loss 8.3271 (6.1924)	CeLoss 0.3008 (0.5578)	SegCLSLoss 0.0273 (0.0236)	KLLoss 0.1582 (0.1271)	MaskLoss 1.0087 (0.8062)	MaskBCELoss 0.0359 (0.1590)	MaskDICELoss 0.9728 (0.6472)
Epoch: [0][483/500]	Time 10.688 (10.688)	Loss 7.6367 (6.0057)	CeLoss 0.2432 (0.3454)	SegCLSLoss 0.0388 (0.0286)	KLLoss 0.2129 (0.1589)	MaskLoss 1.2433 (0.8570)	MaskBCELoss 0.4641 (0.2281)	MaskDICELoss 0.7792 (0.6289)
Epoch: [0][484/500]	Time 10.514 (10.514)	Loss 8.0535 (5.5556)	CeLoss 0.2373 (0.5352)	SegCLSLoss 0.0312 (0.0208)	KLLoss 0.1631 (0.1076)	MaskLoss 0.9967 (0.7306)	MaskBCELoss 0.0563 (0.1571)	MaskDICELoss 0.9404 (0.5735)
Epoch: [0][485/500]	Time 10.259 (10.259)	Loss 3.4989 (6.3035)	CeLoss 0.3125 (0.3823)	SegCLSLoss 0.0171 (0.0251)	KLLoss 0.0806 (0.1542)	MaskLoss 0.5753 (0.8990)	MaskBCELoss 0.2507 (0.2396)	MaskDICELoss 0.3247 (0.6594)
Epoch: [0][486/500]	Time 10.637 (10.637)	Loss 9.1494 (6.9057)	CeLoss 0.2656 (0.3544)	SegCLSLoss 0.0231 (0.0291)	KLLoss 0.1494 (0.1546)	MaskLoss 1.4188 (0.9140)	MaskBCELoss 0.4378 (0.1549)	MaskDICELoss 0.9810 (0.7591)
Epoch: [0][487/500]	Time  8.110 ( 8.110)	Loss 4.3330 (4.7603)	CeLoss 0.4746 (0.7232)	SegCLSLoss 0.0209 (0.0167)	KLLoss 0.1445 (0.0990)	MaskLoss 0.6345 (0.5960)	MaskBCELoss 0.2286 (0.1396)	MaskDICELoss 0.4059 (0.4564)
Epoch: [0][488/500]	Time  8.513 ( 8.513)	Loss 5.7878 (4.7082)	CeLoss 0.2441 (0.6682)	SegCLSLoss 0.0253 (0.0191)	KLLoss 0.1523 (0.0969)	MaskLoss 0.7557 (0.6097)	MaskBCELoss 0.1110 (0.1573)	MaskDICELoss 0.6447 (0.4524)
Epoch: [0][489/500]	Time 10.010 (10.010)	Loss 8.3589 (4.8868)	CeLoss 0.3887 (0.5526)	SegCLSLoss 0.0197 (0.0195)	KLLoss 0.1289 (0.1055)	MaskLoss 1.0144 (0.6356)	MaskBCELoss 0.0473 (0.1443)	MaskDICELoss 0.9671 (0.4913)
[2025-03-04 03:25:18,206] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=0, lr=[0.000289433734939759], mom=[(0.9, 0.95)]
[2025-03-04 03:25:18,211] [INFO] [timer.py:215:stop] epoch=0/micro_step=4900/global_step=490, RunningAvgSamplesPerSec=1.02963482571959, CurrSamplesPerSec=1.146120167099532, MemAllocated=57.24GB, MaxMemAllocated=62.78GB
Epoch: [0][490/500]	Time  8.727 ( 8.727)	Loss 7.8947 (5.9845)	CeLoss 0.2734 (0.4926)	SegCLSLoss 0.0188 (0.0282)	KLLoss 0.0840 (0.1340)	MaskLoss 1.1306 (0.8459)	MaskBCELoss 0.2529 (0.2373)	MaskDICELoss 0.8777 (0.6086)
Epoch: [0][491/500]	Time  8.990 ( 8.990)	Loss 7.1745 (6.3227)	CeLoss 0.2070 (0.3250)	SegCLSLoss 0.0439 (0.0246)	KLLoss 0.2256 (0.1481)	MaskLoss 1.1219 (0.9248)	MaskBCELoss 0.3756 (0.2602)	MaskDICELoss 0.7463 (0.6646)
Epoch: [0][492/500]	Time 10.643 (10.643)	Loss 7.3907 (5.2236)	CeLoss 0.2617 (0.5192)	SegCLSLoss 0.0295 (0.0171)	KLLoss 0.2256 (0.1177)	MaskLoss 0.9493 (0.7230)	MaskBCELoss 0.1179 (0.2011)	MaskDICELoss 0.8314 (0.5219)
Epoch: [0][493/500]	Time  8.636 ( 8.636)	Loss 6.7564 (4.9647)	CeLoss 0.2188 (0.3585)	SegCLSLoss 0.0199 (0.0204)	KLLoss 0.1582 (0.1156)	MaskLoss 1.0664 (0.6998)	MaskBCELoss 0.3602 (0.1863)	MaskDICELoss 0.7062 (0.5135)
Epoch: [0][494/500]	Time  8.563 ( 8.563)	Loss 1.4844 (5.6736)	CeLoss 1.4844 (0.5958)	SegCLSLoss 0.0000 (0.0215)	KLLoss 0.0000 (0.1372)	MaskLoss 0.0000 (0.7278)	MaskBCELoss 0.0000 (0.1488)	MaskDICELoss 0.0000 (0.5790)
Epoch: [0][495/500]	Time  8.002 ( 8.002)	Loss 0.9688 (4.7788)	CeLoss 0.9688 (0.6604)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.0718)	MaskLoss 0.0000 (0.6150)	MaskBCELoss 0.0000 (0.1468)	MaskDICELoss 0.0000 (0.4682)
Epoch: [0][496/500]	Time  9.363 ( 9.363)	Loss 7.6672 (6.7721)	CeLoss 0.1504 (0.4354)	SegCLSLoss 0.0510 (0.0280)	KLLoss 0.2559 (0.1496)	MaskLoss 1.0435 (0.9296)	MaskBCELoss 0.1854 (0.2107)	MaskDICELoss 0.8581 (0.7189)
Epoch: [0][497/500]	Time 10.506 (10.506)	Loss 6.5267 (6.8247)	CeLoss 0.2773 (0.3272)	SegCLSLoss 0.0500 (0.0259)	KLLoss 0.1621 (0.1440)	MaskLoss 1.2033 (1.0171)	MaskBCELoss 0.5942 (0.2994)	MaskDICELoss 0.6092 (0.7177)
Epoch: [0][498/500]	Time  9.849 ( 9.849)	Loss 1.0938 (6.0488)	CeLoss 1.0938 (0.4092)	SegCLSLoss 0.0000 (0.0245)	KLLoss 0.0000 (0.1320)	MaskLoss 0.0000 (0.8979)	MaskBCELoss 0.0000 (0.2814)	MaskDICELoss 0.0000 (0.6165)
Epoch: [0][499/500]	Time 10.832 (10.832)	Loss 0.1074 (4.7336)	CeLoss 0.1074 (0.4224)	SegCLSLoss 0.0000 (0.0190)	KLLoss 0.0000 (0.1196)	MaskLoss 0.0000 (0.5788)	MaskBCELoss 0.0000 (0.0748)	MaskDICELoss 0.0000 (0.5040)
[2025-03-04 03:26:53,818] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=0, lr=[0.00028919277108433735], mom=[(0.9, 0.95)]
[2025-03-04 03:26:53,823] [INFO] [timer.py:215:stop] epoch=0/micro_step=5000/global_step=500, RunningAvgSamplesPerSec=1.0299597377633964, CurrSamplesPerSec=0.9778772360151309, MemAllocated=57.26GB, MaxMemAllocated=62.78GB
Epoch: [0][500/500]	Time 10.228 (10.228)	Loss 7.6002 (6.4959)	CeLoss 0.2354 (0.3625)	SegCLSLoss 0.0283 (0.0270)	KLLoss 0.2002 (0.1536)	MaskLoss 1.3138 (0.8778)	MaskBCELoss 0.5599 (0.1760)	MaskDICELoss 0.7539 (0.7018)
  0%|▊                                                                                                                                                     | 1/200 [00:01<04:13,  1.28s/it]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([33, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
  2%|██▎                                                                                                                                                   | 3/200 [00:01<01:22,  2.40it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([45, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
  2%|███▊                                                                                                                                                  | 5/200 [00:02<00:56,  3.45it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([43, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
  5%|███████▍                                                                                                                                             | 10/200 [00:03<00:45,  4.18it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([44, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
  6%|████████▉                                                                                                                                            | 12/200 [00:03<00:43,  4.31it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([41, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
  7%|██████████▍                                                                                                                                          | 14/200 [00:04<00:58,  3.19it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([35, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
 13%|███████████████████▎                                                                                                                                 | 26/200 [00:07<00:39,  4.44it/s]
 16%|████████████████████████▌                                                                                                                            | 33/200 [00:09<00:36,  4.58it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([46, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 20%|█████████████████████████████▊                                                                                                                       | 40/200 [00:11<00:36,  4.37it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([46, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 24%|███████████████████████████████████                                                                                                                  | 47/200 [00:13<00:34,  4.49it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([38, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 27%|████████████████████████████████████████▏                                                                                                            | 54/200 [00:15<00:32,  4.50it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([50, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 32%|██████████████████████████████████████████████▉                                                                                                      | 63/200 [00:17<00:45,  2.99it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([50, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 36%|████████████████████████████████████████████████████▉                                                                                                | 71/200 [00:19<00:39,  3.31it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([32, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 40%|██████████████████████████████████████████████████████████▊                                                                                          | 79/200 [00:21<00:33,  3.66it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([32, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 44%|████████████████████████████████████████████████████████████████▊                                                                                    | 87/200 [00:23<00:27,  4.13it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([32, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 47%|██████████████████████████████████████████████████████████████████████                                                                               | 94/200 [00:25<00:27,  3.86it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([32, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 51%|███████████████████████████████████████████████████████████████████████████▍                                                                        | 102/200 [00:27<00:24,  3.99it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([40, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 55%|█████████████████████████████████████████████████████████████████████████████████▍                                                                  | 110/200 [00:29<00:17,  5.00it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([40, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 58%|██████████████████████████████████████████████████████████████████████████████████████▌                                                             | 117/200 [00:31<00:17,  4.64it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([40, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 62%|███████████████████████████████████████████████████████████████████████████████████████████▊                                                        | 124/200 [00:33<00:17,  4.39it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([40, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 66%|████████████████████████████████████████████████████████████████████████████████████████████████▉                                                   | 131/200 [00:35<00:16,  4.27it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([40, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                                             | 139/200 [00:37<00:14,  4.15it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([40, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 73%|████████████████████████████████████████████████████████████████████████████████████████████████████████████                                        | 146/200 [00:39<00:12,  4.40it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([40, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 76%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                   | 152/200 [00:41<00:10,  4.39it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([40, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                             | 160/200 [00:43<00:14,  2.76it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([40, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 84%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                        | 167/200 [00:45<00:07,  4.20it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([40, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                   | 174/200 [00:47<00:08,  2.96it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([40, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉              | 181/200 [00:49<00:04,  4.51it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([40, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████         | 188/200 [00:51<00:02,  4.84it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([40, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 196/200 [00:53<00:01,  2.80it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  warnings.warn(█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  warnings.warn(█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
giou: 0.2286, ciou: 0.2109
[2025-03-04 03:27:48,968] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step500 is about to be saved!
[2025-03-04 03:28:17,839] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./runs/plum-13b_kld_1_dice_8/ckpt_model/global_step500/mp_rank_00_model_states.pt
[2025-03-04 03:28:17,839] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./runs/plum-13b_kld_1_dice_8/ckpt_model/global_step500/mp_rank_00_model_states.pt...
[2025-03-04 03:31:34,701] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./runs/plum-13b_kld_1_dice_8/ckpt_model/global_step500/mp_rank_00_model_states.pt.
[2025-03-04 03:31:35,912] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./runs/plum-13b_kld_1_dice_8/ckpt_model/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-04 03:31:45,998] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./runs/plum-13b_kld_1_dice_8/ckpt_model/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-04 03:31:46,000] [INFO] [engine.py:3244:_save_zero_checkpoint] zero checkpoint saved ./runs/plum-13b_kld_1_dice_8/ckpt_model/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-04 03:31:46,001] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step500 is ready now!
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
Epoch: [1][  1/500]	Time 11.439 (11.439)	Loss 8.0742 (5.9259)	CeLoss 0.1670 (0.4362)	SegCLSLoss 0.0381 (0.0244)	KLLoss 0.2451 (0.1388)	MaskLoss 1.3562 (0.8544)	MaskBCELoss 0.5341 (0.2493)	MaskDICELoss 0.8220 (0.6051)
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
Epoch: [1][  2/500]	Time 10.892 (10.892)	Loss 8.4924 (5.9120)	CeLoss 0.2373 (0.2313)	SegCLSLoss 0.0317 (0.0215)	KLLoss 0.1934 (0.1277)	MaskLoss 1.0312 (0.8512)	MaskBCELoss 0.0341 (0.2112)	MaskDICELoss 0.9971 (0.6400)
Epoch: [1][  3/500]	Time  9.778 ( 9.778)	Loss 7.2167 (6.7433)	CeLoss 0.2910 (0.3674)	SegCLSLoss 0.0228 (0.0290)	KLLoss 0.1748 (0.1722)	MaskLoss 0.9709 (0.9146)	MaskBCELoss 0.1712 (0.1879)	MaskDICELoss 0.7997 (0.7267)
Epoch: [1][  4/500]	Time 11.238 (11.238)	Loss 8.3895 (6.1295)	CeLoss 0.1934 (0.3352)	SegCLSLoss 0.0347 (0.0205)	KLLoss 0.2471 (0.1326)	MaskLoss 1.2198 (0.8799)	MaskBCELoss 0.3043 (0.2313)	MaskDICELoss 0.9155 (0.6486)
Epoch: [1][  5/500]	Time  8.977 ( 8.977)	Loss 8.0171 (5.2085)	CeLoss 0.2432 (0.4562)	SegCLSLoss 0.0172 (0.0175)	KLLoss 0.0903 (0.1023)	MaskLoss 1.1538 (0.6707)	MaskBCELoss 0.2592 (0.1207)	MaskDICELoss 0.8946 (0.5500)
Epoch: [1][  6/500]	Time 10.055 (10.055)	Loss 1.4688 (6.0323)	CeLoss 1.4688 (0.4622)	SegCLSLoss 0.0000 (0.0219)	KLLoss 0.0000 (0.1312)	MaskLoss 0.0000 (0.8258)	MaskBCELoss 0.0000 (0.1965)	MaskDICELoss 0.0000 (0.6293)
Epoch: [1][  7/500]	Time  9.799 ( 9.799)	Loss 6.9878 (6.2090)	CeLoss 0.3145 (0.4012)	SegCLSLoss 0.0210 (0.0240)	KLLoss 0.1436 (0.1457)	MaskLoss 0.9444 (0.8827)	MaskBCELoss 0.1727 (0.2352)	MaskDICELoss 0.7717 (0.6474)
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
Epoch: [1][  8/500]	Time  7.904 ( 7.904)	Loss 1.9922 (5.5171)	CeLoss 1.9922 (0.4874)	SegCLSLoss 0.0000 (0.0253)	KLLoss 0.0000 (0.1156)	MaskLoss 0.0000 (0.8285)	MaskBCELoss 0.0000 (0.2879)	MaskDICELoss 0.0000 (0.5407)
Epoch: [1][  9/500]	Time 11.513 (11.513)	Loss 8.2294 (6.5868)	CeLoss 0.2217 (0.2590)	SegCLSLoss 0.0309 (0.0251)	KLLoss 0.1670 (0.1502)	MaskLoss 1.1190 (0.9125)	MaskBCELoss 0.1878 (0.1892)	MaskDICELoss 0.9312 (0.7233)
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
[2025-03-04 03:33:26,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=510, skipped=0, lr=[0.0002889638554216867], mom=[(0.9, 0.95)]
[2025-03-04 03:33:26,386] [INFO] [timer.py:215:stop] epoch=0/micro_step=5100/global_step=510, RunningAvgSamplesPerSec=1.0293542822947481, CurrSamplesPerSec=1.1399240410112443, MemAllocated=56.7GB, MaxMemAllocated=62.8GB
Epoch: [1][ 10/500]	Time  8.775 ( 8.775)	Loss 1.2188 (6.4144)	CeLoss 1.2188 (0.2994)	SegCLSLoss 0.0000 (0.0314)	KLLoss 0.0000 (0.1486)	MaskLoss 0.0000 (0.9248)	MaskBCELoss 0.0000 (0.2413)	MaskDICELoss 0.0000 (0.6835)
Epoch: [1][ 11/500]	Time  7.988 ( 7.988)	Loss 6.7792 (4.7125)	CeLoss 0.3047 (0.4031)	SegCLSLoss 0.0195 (0.0274)	KLLoss 0.1299 (0.1254)	MaskLoss 0.9773 (0.6451)	MaskBCELoss 0.2474 (0.1652)	MaskDICELoss 0.7299 (0.4799)
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
Epoch: [1][ 12/500]	Time  9.269 ( 9.269)	Loss 5.8189 (6.3974)	CeLoss 0.1904 (0.3553)	SegCLSLoss 0.0500 (0.0328)	KLLoss 0.2715 (0.1768)	MaskLoss 0.7452 (0.9352)	MaskBCELoss 0.1052 (0.2721)	MaskDICELoss 0.6400 (0.6631)
Epoch: [1][ 13/500]	Time  8.965 ( 8.965)	Loss 1.3594 (4.8607)	CeLoss 1.3594 (0.5842)	SegCLSLoss 0.0000 (0.0186)	KLLoss 0.0000 (0.0932)	MaskLoss 0.0000 (0.6429)	MaskBCELoss 0.0000 (0.1616)	MaskDICELoss 0.0000 (0.4813)
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
Epoch: [1][ 14/500]	Time  9.140 ( 9.140)	Loss 6.7252 (6.0468)	CeLoss 0.3574 (0.4722)	SegCLSLoss 0.0192 (0.0245)	KLLoss 0.1143 (0.1287)	MaskLoss 0.9712 (0.8658)	MaskBCELoss 0.2541 (0.2487)	MaskDICELoss 0.7171 (0.6170)
Epoch: [1][ 15/500]	Time  9.309 ( 9.309)	Loss 1.7578 (5.3204)	CeLoss 1.7578 (0.5837)	SegCLSLoss 0.0000 (0.0204)	KLLoss 0.0000 (0.1140)	MaskLoss 0.0000 (0.6615)	MaskBCELoss 0.0000 (0.1133)	MaskDICELoss 0.0000 (0.5482)
Epoch: [1][ 16/500]	Time  9.822 ( 9.822)	Loss 0.0786 (3.1831)	CeLoss 0.0786 (0.5792)	SegCLSLoss 0.0000 (0.0084)	KLLoss 0.0000 (0.0474)	MaskLoss 0.0000 (0.4381)	MaskBCELoss 0.0000 (0.1588)	MaskDICELoss 0.0000 (0.2793)
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
Epoch: [1][ 17/500]	Time 11.785 (11.785)	Loss 8.4740 (6.3963)	CeLoss 0.1562 (0.2474)	SegCLSLoss 0.0437 (0.0334)	KLLoss 0.2471 (0.1514)	MaskLoss 1.0335 (0.8188)	MaskBCELoss 0.0367 (0.0950)	MaskDICELoss 0.9969 (0.7239)
Epoch: [1][ 18/500]	Time  9.339 ( 9.339)	Loss 1.2969 (5.0841)	CeLoss 1.2969 (0.5146)	SegCLSLoss 0.0000 (0.0144)	KLLoss 0.0000 (0.0736)	MaskLoss 0.0000 (0.6422)	MaskBCELoss 0.0000 (0.1080)	MaskDICELoss 0.0000 (0.5341)
Epoch: [1][ 19/500]	Time  9.095 ( 9.095)	Loss 1.3828 (5.6628)	CeLoss 1.3828 (0.4896)	SegCLSLoss 0.0000 (0.0265)	KLLoss 0.0000 (0.1257)	MaskLoss 0.0000 (0.7075)	MaskBCELoss 0.0000 (0.1043)	MaskDICELoss 0.0000 (0.6032)
[2025-03-04 03:35:00,866] [INFO] [logging.py:96:log_dist] [Rank 0] step=520, skipped=0, lr=[0.00028872289156626503], mom=[(0.9, 0.95)]
[2025-03-04 03:35:00,871] [INFO] [timer.py:215:stop] epoch=0/micro_step=5200/global_step=520, RunningAvgSamplesPerSec=1.029902628566237, CurrSamplesPerSec=1.0234566634485671, MemAllocated=56.74GB, MaxMemAllocated=62.8GB
Epoch: [1][ 20/500]	Time  9.773 ( 9.773)	Loss 1.3750 (5.4888)	CeLoss 1.3750 (0.3728)	SegCLSLoss 0.0000 (0.0236)	KLLoss 0.0000 (0.1304)	MaskLoss 0.0000 (0.8295)	MaskBCELoss 0.0000 (0.2770)	MaskDICELoss 0.0000 (0.5525)
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
Epoch: [1][ 21/500]	Time  8.893 ( 8.893)	Loss 1.0234 (5.4531)	CeLoss 1.0234 (0.3431)	SegCLSLoss 0.0000 (0.0310)	KLLoss 0.0000 (0.1357)	MaskLoss 0.0000 (0.7535)	MaskBCELoss 0.0000 (0.1782)	MaskDICELoss 0.0000 (0.5753)
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
Epoch: [1][ 22/500]	Time  9.329 ( 9.329)	Loss 5.7255 (5.6959)	CeLoss 0.3652 (0.3319)	SegCLSLoss 0.0219 (0.0262)	KLLoss 0.1309 (0.1304)	MaskLoss 0.9993 (0.8600)	MaskBCELoss 0.4629 (0.2766)	MaskDICELoss 0.5365 (0.5834)
Epoch: [1][ 23/500]	Time 10.958 (10.958)	Loss 6.1984 (5.0037)	CeLoss 0.2676 (0.4042)	SegCLSLoss 0.0369 (0.0186)	KLLoss 0.2031 (0.1057)	MaskLoss 1.1480 (0.7688)	MaskBCELoss 0.5790 (0.2777)	MaskDICELoss 0.5690 (0.4912)
Epoch: [1][ 24/500]	Time  9.406 ( 9.406)	Loss 1.5625 (4.1643)	CeLoss 1.5625 (0.7084)	SegCLSLoss 0.0000 (0.0179)	KLLoss 0.0000 (0.0890)	MaskLoss 0.0000 (0.5228)	MaskBCELoss 0.0000 (0.1375)	MaskDICELoss 0.0000 (0.3853)
Epoch: [1][ 25/500]	Time  9.541 ( 9.541)	Loss 6.1983 (4.9918)	CeLoss 0.2617 (0.4682)	SegCLSLoss 0.0244 (0.0201)	KLLoss 0.1445 (0.1046)	MaskLoss 0.7910 (0.7431)	MaskBCELoss 0.0913 (0.2559)	MaskDICELoss 0.6997 (0.4871)
Epoch: [1][ 26/500]	Time  9.654 ( 9.654)	Loss 3.9032 (5.4765)	CeLoss 0.2676 (0.4619)	SegCLSLoss 0.0199 (0.0247)	KLLoss 0.1079 (0.1366)	MaskLoss 0.6322 (0.7283)	MaskBCELoss 0.2565 (0.1601)	MaskDICELoss 0.3757 (0.5682)
Epoch: [1][ 27/500]	Time  9.265 ( 9.265)	Loss 5.7486 (5.7458)	CeLoss 0.2236 (0.3828)	SegCLSLoss 0.0209 (0.0226)	KLLoss 0.1006 (0.1251)	MaskLoss 0.8487 (0.8326)	MaskBCELoss 0.2295 (0.2392)	MaskDICELoss 0.6192 (0.5935)
Epoch: [1][ 28/500]	Time  9.181 ( 9.181)	Loss 0.8477 (4.5129)	CeLoss 0.8477 (0.3980)	SegCLSLoss 0.0000 (0.0194)	KLLoss 0.0000 (0.1016)	MaskLoss 0.0000 (0.6374)	MaskBCELoss 0.0000 (0.1826)	MaskDICELoss 0.0000 (0.4548)
Epoch: [1][ 29/500]	Time 10.239 (10.239)	Loss 4.8537 (5.4111)	CeLoss 0.3262 (0.3250)	SegCLSLoss 0.0194 (0.0210)	KLLoss 0.1035 (0.1231)	MaskLoss 0.8029 (0.7449)	MaskBCELoss 0.3349 (0.1678)	MaskDICELoss 0.4681 (0.5771)
[2025-03-04 03:36:36,979] [INFO] [logging.py:96:log_dist] [Rank 0] step=530, skipped=0, lr=[0.00028848192771084336], mom=[(0.9, 0.95)]
[2025-03-04 03:36:36,984] [INFO] [timer.py:215:stop] epoch=0/micro_step=5300/global_step=530, RunningAvgSamplesPerSec=1.0301033204970873, CurrSamplesPerSec=1.0366183994387936, MemAllocated=57.27GB, MaxMemAllocated=62.8GB
Epoch: [1][ 30/500]	Time  9.649 ( 9.649)	Loss 8.5057 (6.2814)	CeLoss 0.2197 (0.5588)	SegCLSLoss 0.0327 (0.0248)	KLLoss 0.1885 (0.1329)	MaskLoss 1.1354 (0.8575)	MaskBCELoss 0.1672 (0.2138)	MaskDICELoss 0.9682 (0.6437)
Epoch: [1][ 31/500]	Time 10.443 (10.443)	Loss 8.2380 (6.2170)	CeLoss 0.3066 (0.4835)	SegCLSLoss 0.0315 (0.0264)	KLLoss 0.1699 (0.1335)	MaskLoss 1.0049 (0.9101)	MaskBCELoss 0.0490 (0.2823)	MaskDICELoss 0.9560 (0.6278)
Epoch: [1][ 32/500]	Time  7.331 ( 7.331)	Loss 1.1328 (3.9543)	CeLoss 1.1328 (0.8001)	SegCLSLoss 0.0000 (0.0179)	KLLoss 0.0000 (0.0894)	MaskLoss 0.0000 (0.5062)	MaskBCELoss 0.0000 (0.1656)	MaskDICELoss 0.0000 (0.3406)
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
Epoch: [1][ 33/500]	Time  9.818 ( 9.818)	Loss 7.6194 (7.0302)	CeLoss 0.2949 (0.2841)	SegCLSLoss 0.0258 (0.0320)	KLLoss 0.1611 (0.1734)	MaskLoss 1.2596 (1.0666)	MaskBCELoss 0.4877 (0.3293)	MaskDICELoss 0.7719 (0.7372)
Epoch: [1][ 34/500]	Time  9.619 ( 9.619)	Loss 0.8359 (5.2257)	CeLoss 0.8359 (0.5875)	SegCLSLoss 0.0000 (0.0193)	KLLoss 0.0000 (0.1143)	MaskLoss 0.0000 (0.6771)	MaskBCELoss 0.0000 (0.1504)	MaskDICELoss 0.0000 (0.5267)
Epoch: [1][ 35/500]	Time 10.922 (10.922)	Loss 7.4693 (5.9646)	CeLoss 0.2012 (0.4144)	SegCLSLoss 0.0310 (0.0275)	KLLoss 0.2109 (0.1439)	MaskLoss 0.9047 (0.7947)	MaskBCELoss 0.0326 (0.1609)	MaskDICELoss 0.8720 (0.6338)
Epoch: [1][ 36/500]	Time 10.391 (10.391)	Loss 8.4738 (5.6553)	CeLoss 0.2598 (0.4994)	SegCLSLoss 0.0197 (0.0192)	KLLoss 0.1157 (0.1115)	MaskLoss 1.1087 (0.7665)	MaskBCELoss 0.1301 (0.1828)	MaskDICELoss 0.9786 (0.5837)
Epoch: [1][ 37/500]	Time 10.432 (10.432)	Loss 8.5781 (6.2814)	CeLoss 0.2119 (0.2736)	SegCLSLoss 0.0391 (0.0218)	KLLoss 0.1973 (0.1319)	MaskLoss 1.2702 (0.8120)	MaskBCELoss 0.3355 (0.1052)	MaskDICELoss 0.9347 (0.7068)
Epoch: [1][ 38/500]	Time  9.123 ( 9.123)	Loss 7.3316 (6.1795)	CeLoss 0.1953 (0.3907)	SegCLSLoss 0.0315 (0.0222)	KLLoss 0.1934 (0.1389)	MaskLoss 0.9003 (0.8129)	MaskBCELoss 0.0459 (0.1440)	MaskDICELoss 0.8544 (0.6689)
Epoch: [1][ 39/500]	Time  9.613 ( 9.613)	Loss 0.9961 (5.5284)	CeLoss 0.9961 (0.3495)	SegCLSLoss 0.0000 (0.0247)	KLLoss 0.0000 (0.1503)	MaskLoss 0.0000 (0.7376)	MaskBCELoss 0.0000 (0.1474)	MaskDICELoss 0.0000 (0.5902)
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
[2025-03-04 03:38:15,678] [INFO] [logging.py:96:log_dist] [Rank 0] step=540, skipped=0, lr=[0.00028824096385542163], mom=[(0.9, 0.95)]
[2025-03-04 03:38:15,684] [INFO] [timer.py:215:stop] epoch=0/micro_step=5400/global_step=540, RunningAvgSamplesPerSec=1.0297864181510907, CurrSamplesPerSec=0.9085224886340031, MemAllocated=57.25GB, MaxMemAllocated=62.8GB
Epoch: [1][ 40/500]	Time 11.009 (11.009)	Loss 7.8390 (6.5685)	CeLoss 0.2891 (0.3289)	SegCLSLoss 0.0284 (0.0324)	KLLoss 0.1465 (0.1634)	MaskLoss 1.3909 (0.9558)	MaskBCELoss 0.6230 (0.2643)	MaskDICELoss 0.7680 (0.6914)
Epoch: [1][ 41/500]	Time  7.899 ( 7.899)	Loss 0.9453 (4.7786)	CeLoss 0.9453 (0.5114)	SegCLSLoss 0.0000 (0.0244)	KLLoss 0.0000 (0.1317)	MaskLoss 0.0000 (0.6983)	MaskBCELoss 0.0000 (0.2439)	MaskDICELoss 0.0000 (0.4544)
Epoch: [1][ 42/500]	Time 10.469 (10.469)	Loss 7.4377 (5.5863)	CeLoss 0.2656 (0.3511)	SegCLSLoss 0.0361 (0.0208)	KLLoss 0.1719 (0.1172)	MaskLoss 0.9438 (0.7456)	MaskBCELoss 0.0946 (0.1428)	MaskDICELoss 0.8492 (0.6028)
Epoch: [1][ 43/500]	Time 10.442 (10.442)	Loss 7.7894 (5.9598)	CeLoss 0.2734 (0.3496)	SegCLSLoss 0.0203 (0.0232)	KLLoss 0.1465 (0.1342)	MaskLoss 1.0390 (0.7778)	MaskBCELoss 0.1587 (0.1264)	MaskDICELoss 0.8803 (0.6515)
Epoch: [1][ 44/500]	Time  9.066 ( 9.066)	Loss 8.6275 (5.8811)	CeLoss 0.1289 (0.4995)	SegCLSLoss 0.0645 (0.0277)	KLLoss 0.2471 (0.1428)	MaskLoss 1.2956 (0.8121)	MaskBCELoss 0.3576 (0.2120)	MaskDICELoss 0.9380 (0.6001)
Epoch: [1][ 45/500]	Time  9.226 ( 9.226)	Loss 7.0777 (6.0198)	CeLoss 0.3281 (0.5091)	SegCLSLoss 0.0256 (0.0193)	KLLoss 0.1201 (0.1204)	MaskLoss 1.1438 (0.9318)	MaskBCELoss 0.4222 (0.3457)	MaskDICELoss 0.7215 (0.5861)
Epoch: [1][ 46/500]	Time 11.395 (11.395)	Loss 1.2422 (7.0836)	CeLoss 1.2422 (0.3622)	SegCLSLoss 0.0000 (0.0260)	KLLoss 0.0000 (0.1566)	MaskLoss 0.0000 (1.1009)	MaskBCELoss 0.0000 (0.3759)	MaskDICELoss 0.0000 (0.7250)
Epoch: [1][ 47/500]	Time  9.395 ( 9.395)	Loss 7.1611 (5.4994)	CeLoss 0.2334 (0.3334)	SegCLSLoss 0.0259 (0.0226)	KLLoss 0.1943 (0.1361)	MaskLoss 0.8744 (0.7653)	MaskBCELoss 0.0459 (0.1840)	MaskDICELoss 0.8285 (0.5813)
Epoch: [1][ 48/500]	Time  7.922 ( 7.922)	Loss 7.7802 (3.6063)	CeLoss 0.2148 (0.5541)	SegCLSLoss 0.0273 (0.0143)	KLLoss 0.1650 (0.0893)	MaskLoss 1.2236 (0.5035)	MaskBCELoss 0.4005 (0.1787)	MaskDICELoss 0.8231 (0.3248)
Epoch: [1][ 49/500]	Time 11.596 (11.596)	Loss 7.9138 (8.1609)	CeLoss 0.2080 (0.2280)	SegCLSLoss 0.0381 (0.0328)	KLLoss 0.2139 (0.1771)	MaskLoss 1.0937 (1.1769)	MaskBCELoss 0.2128 (0.2792)	MaskDICELoss 0.8808 (0.8977)
[2025-03-04 03:39:52,538] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=0, lr=[0.00028799999999999995], mom=[(0.9, 0.95)]
[2025-03-04 03:39:52,544] [INFO] [timer.py:215:stop] epoch=0/micro_step=5500/global_step=550, RunningAvgSamplesPerSec=1.029837428908465, CurrSamplesPerSec=1.0585382207570104, MemAllocated=57.24GB, MaxMemAllocated=62.8GB
Epoch: [1][ 50/500]	Time  9.449 ( 9.449)	Loss 6.4013 (7.1687)	CeLoss 0.3594 (0.3575)	SegCLSLoss 0.0201 (0.0283)	KLLoss 0.1309 (0.1547)	MaskLoss 1.0614 (1.0360)	MaskBCELoss 0.4316 (0.2743)	MaskDICELoss 0.6298 (0.7618)
Epoch: [1][ 51/500]	Time  9.961 ( 9.961)	Loss 8.5933 (5.1371)	CeLoss 0.2363 (0.5743)	SegCLSLoss 0.0400 (0.0248)	KLLoss 0.2051 (0.1275)	MaskLoss 1.5019 (0.7520)	MaskBCELoss 0.6471 (0.2655)	MaskDICELoss 0.8548 (0.4865)
Epoch: [1][ 52/500]	Time  8.324 ( 8.324)	Loss 3.5211 (4.1968)	CeLoss 0.2539 (0.5296)	SegCLSLoss 0.0231 (0.0212)	KLLoss 0.1445 (0.1132)	MaskLoss 0.4182 (0.5473)	MaskBCELoss 0.0391 (0.1391)	MaskDICELoss 0.3791 (0.4082)
Epoch: [1][ 53/500]	Time  9.379 ( 9.379)	Loss 4.4942 (4.7031)	CeLoss 0.2490 (0.4127)	SegCLSLoss 0.0309 (0.0207)	KLLoss 0.1436 (0.1052)	MaskLoss 0.5694 (0.5821)	MaskBCELoss 0.0779 (0.0803)	MaskDICELoss 0.4915 (0.5018)
Epoch: [1][ 54/500]	Time  8.889 ( 8.889)	Loss 7.1233 (4.1623)	CeLoss 0.2656 (0.5044)	SegCLSLoss 0.0258 (0.0172)	KLLoss 0.1157 (0.0902)	MaskLoss 1.1484 (0.5717)	MaskBCELoss 0.4097 (0.1692)	MaskDICELoss 0.7387 (0.4026)
Epoch: [1][ 55/500]	Time  9.528 ( 9.528)	Loss 8.1079 (4.8695)	CeLoss 0.1709 (0.3395)	SegCLSLoss 0.0342 (0.0252)	KLLoss 0.1602 (0.1209)	MaskLoss 1.0013 (0.6971)	MaskBCELoss 0.0418 (0.1967)	MaskDICELoss 0.9596 (0.5004)
Epoch: [1][ 56/500]	Time 10.012 (10.012)	Loss 7.2965 (5.3701)	CeLoss 0.3086 (0.5142)	SegCLSLoss 0.0256 (0.0200)	KLLoss 0.1309 (0.0994)	MaskLoss 1.3343 (0.7369)	MaskBCELoss 0.6386 (0.1916)	MaskDICELoss 0.6958 (0.5454)
Epoch: [1][ 57/500]	Time 11.461 (11.461)	Loss 6.1179 (6.8533)	CeLoss 0.2324 (0.3157)	SegCLSLoss 0.0344 (0.0281)	KLLoss 0.1934 (0.1453)	MaskLoss 0.7705 (1.0218)	MaskBCELoss 0.0816 (0.2993)	MaskDICELoss 0.6889 (0.7225)
Epoch: [1][ 58/500]	Time 11.142 (11.142)	Loss 6.5206 (6.9830)	CeLoss 0.2256 (0.3764)	SegCLSLoss 0.0383 (0.0322)	KLLoss 0.2031 (0.1627)	MaskLoss 0.9806 (0.9540)	MaskBCELoss 0.2955 (0.2007)	MaskDICELoss 0.6851 (0.7533)
Epoch: [1][ 59/500]	Time 10.240 (10.240)	Loss 1.1250 (5.8944)	CeLoss 1.1250 (0.4751)	SegCLSLoss 0.0000 (0.0253)	KLLoss 0.0000 (0.1316)	MaskLoss 0.0000 (0.8114)	MaskBCELoss 0.0000 (0.2027)	MaskDICELoss 0.0000 (0.6087)
[2025-03-04 03:41:33,078] [INFO] [logging.py:96:log_dist] [Rank 0] step=560, skipped=0, lr=[0.0002877590361445783], mom=[(0.9, 0.95)]
[2025-03-04 03:41:33,084] [INFO] [timer.py:215:stop] epoch=0/micro_step=5600/global_step=560, RunningAvgSamplesPerSec=1.0291874598488677, CurrSamplesPerSec=0.8619044533701463, MemAllocated=57.48GB, MaxMemAllocated=62.8GB
Epoch: [1][ 60/500]	Time 11.604 (11.604)	Loss 7.1186 (6.6066)	CeLoss 0.2578 (0.3106)	SegCLSLoss 0.0228 (0.0256)	KLLoss 0.1172 (0.1213)	MaskLoss 0.9949 (0.9831)	MaskBCELoss 0.2045 (0.2838)	MaskDICELoss 0.7904 (0.6993)
Epoch: [1][ 61/500]	Time  8.502 ( 8.502)	Loss 8.1964 (5.3918)	CeLoss 0.2324 (0.3548)	SegCLSLoss 0.0256 (0.0196)	KLLoss 0.1387 (0.1124)	MaskLoss 1.2739 (0.7993)	MaskBCELoss 0.3963 (0.2467)	MaskDICELoss 0.8776 (0.5527)
Epoch: [1][ 62/500]	Time  9.151 ( 9.151)	Loss 1.5234 (4.4341)	CeLoss 1.5234 (0.6507)	SegCLSLoss 0.0000 (0.0152)	KLLoss 0.0000 (0.0959)	MaskLoss 0.0000 (0.5733)	MaskBCELoss 0.0000 (0.1510)	MaskDICELoss 0.0000 (0.4222)
Epoch: [1][ 63/500]	Time 10.845 (10.845)	Loss 8.1115 (6.2194)	CeLoss 0.2070 (0.4396)	SegCLSLoss 0.0300 (0.0273)	KLLoss 0.2168 (0.1506)	MaskLoss 1.3752 (0.8718)	MaskBCELoss 0.5546 (0.2264)	MaskDICELoss 0.8206 (0.6454)
Epoch: [1][ 64/500]	Time  9.991 ( 9.991)	Loss 8.3372 (5.3159)	CeLoss 0.2139 (0.4505)	SegCLSLoss 0.0349 (0.0225)	KLLoss 0.2207 (0.1380)	MaskLoss 1.0387 (0.6548)	MaskBCELoss 0.0709 (0.0871)	MaskDICELoss 0.9678 (0.5677)
Epoch: [1][ 65/500]	Time  8.286 ( 8.286)	Loss 1.3203 (2.7844)	CeLoss 1.3203 (0.6592)	SegCLSLoss 0.0000 (0.0106)	KLLoss 0.0000 (0.0567)	MaskLoss 0.0000 (0.3095)	MaskBCELoss 0.0000 (0.0688)	MaskDICELoss 0.0000 (0.2407)
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
Epoch: [1][ 66/500]	Time  8.870 ( 8.870)	Loss 7.5949 (5.2727)	CeLoss 0.2168 (0.4781)	SegCLSLoss 0.0203 (0.0237)	KLLoss 0.0962 (0.1175)	MaskLoss 1.1289 (0.6640)	MaskBCELoss 0.2931 (0.1078)	MaskDICELoss 0.8358 (0.5562)
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
Epoch: [1][ 67/500]	Time 10.603 (10.603)	Loss 8.0672 (7.0100)	CeLoss 0.2754 (0.3118)	SegCLSLoss 0.0315 (0.0277)	KLLoss 0.1797 (0.1472)	MaskLoss 1.1433 (0.9517)	MaskBCELoss 0.2583 (0.1794)	MaskDICELoss 0.8850 (0.7723)
Epoch: [1][ 68/500]	Time  9.061 ( 9.061)	Loss 7.4246 (4.5644)	CeLoss 0.2090 (0.5712)	SegCLSLoss 0.0276 (0.0160)	KLLoss 0.1797 (0.0891)	MaskLoss 1.1129 (0.6418)	MaskBCELoss 0.3135 (0.2064)	MaskDICELoss 0.7994 (0.4354)
Epoch: [1][ 69/500]	Time  9.766 ( 9.766)	Loss 5.6944 (4.6461)	CeLoss 0.3301 (0.5154)	SegCLSLoss 0.0481 (0.0225)	KLLoss 0.2031 (0.1034)	MaskLoss 0.7837 (0.7013)	MaskBCELoss 0.1884 (0.2657)	MaskDICELoss 0.5954 (0.4356)
[2025-03-04 03:43:09,085] [INFO] [logging.py:96:log_dist] [Rank 0] step=570, skipped=0, lr=[0.0002875180722891566], mom=[(0.9, 0.95)]
[2025-03-04 03:43:09,091] [INFO] [timer.py:215:stop] epoch=0/micro_step=5700/global_step=570, RunningAvgSamplesPerSec=1.029406522320122, CurrSamplesPerSec=0.9149352905799689, MemAllocated=57.1GB, MaxMemAllocated=62.8GB
Epoch: [1][ 70/500]	Time 10.932 (10.932)	Loss 5.2996 (6.1840)	CeLoss 0.2988 (0.2396)	SegCLSLoss 0.0176 (0.0243)	KLLoss 0.1167 (0.1395)	MaskLoss 0.9090 (0.8842)	MaskBCELoss 0.3998 (0.2134)	MaskDICELoss 0.5093 (0.6707)
Epoch: [1][ 71/500]	Time  9.690 ( 9.690)	Loss 7.8025 (4.6259)	CeLoss 0.1494 (0.4071)	SegCLSLoss 0.0422 (0.0203)	KLLoss 0.2793 (0.1250)	MaskLoss 1.0295 (0.5956)	MaskBCELoss 0.1471 (0.1134)	MaskDICELoss 0.8824 (0.4821)
Epoch: [1][ 72/500]	Time  8.060 ( 8.060)	Loss 8.4357 (4.0624)	CeLoss 0.1416 (0.3761)	SegCLSLoss 0.0334 (0.0184)	KLLoss 0.2236 (0.1108)	MaskLoss 1.0362 (0.5510)	MaskBCELoss 0.0394 (0.1402)	MaskDICELoss 0.9968 (0.4107)
Epoch: [1][ 73/500]	Time 10.039 (10.039)	Loss 8.0330 (5.2664)	CeLoss 0.3750 (0.3635)	SegCLSLoss 0.0284 (0.0314)	KLLoss 0.1895 (0.1643)	MaskLoss 0.9880 (0.7369)	MaskBCELoss 0.0748 (0.1954)	MaskDICELoss 0.9132 (0.5415)
Epoch: [1][ 74/500]	Time  7.927 ( 7.927)	Loss 0.5391 (3.5728)	CeLoss 0.5391 (0.5768)	SegCLSLoss 0.0000 (0.0109)	KLLoss 0.0000 (0.0697)	MaskLoss 0.0000 (0.4741)	MaskBCELoss 0.0000 (0.1454)	MaskDICELoss 0.0000 (0.3287)
Epoch: [1][ 75/500]	Time 10.825 (10.825)	Loss 8.4717 (6.4136)	CeLoss 0.2207 (0.3280)	SegCLSLoss 0.0317 (0.0279)	KLLoss 0.1904 (0.1543)	MaskLoss 1.0486 (0.9106)	MaskBCELoss 0.0571 (0.2279)	MaskDICELoss 0.9915 (0.6827)
Epoch: [1][ 76/500]	Time  8.864 ( 8.864)	Loss 8.4233 (5.3435)	CeLoss 0.2715 (0.6133)	SegCLSLoss 0.0281 (0.0168)	KLLoss 0.1367 (0.0991)	MaskLoss 1.0095 (0.6329)	MaskBCELoss 0.0124 (0.0734)	MaskDICELoss 0.9971 (0.5595)
Epoch: [1][ 77/500]	Time  9.639 ( 9.639)	Loss 7.7094 (6.2376)	CeLoss 0.2656 (0.2146)	SegCLSLoss 0.0298 (0.0312)	KLLoss 0.1895 (0.1824)	MaskLoss 0.9456 (0.9141)	MaskBCELoss 0.0543 (0.2480)	MaskDICELoss 0.8913 (0.6661)
Epoch: [1][ 78/500]	Time 10.339 (10.339)	Loss 8.2466 (6.1074)	CeLoss 0.1885 (0.2775)	SegCLSLoss 0.0299 (0.0299)	KLLoss 0.1963 (0.1550)	MaskLoss 1.2878 (1.0058)	MaskBCELoss 0.4091 (0.3977)	MaskDICELoss 0.8788 (0.6081)
Epoch: [1][ 79/500]	Time 11.821 (11.821)	Loss 8.4057 (6.7796)	CeLoss 0.2754 (0.2585)	SegCLSLoss 0.0295 (0.0282)	KLLoss 0.1904 (0.1485)	MaskLoss 1.0105 (0.9433)	MaskBCELoss 0.0265 (0.1980)	MaskDICELoss 0.9840 (0.7453)
[2025-03-04 03:44:46,628] [INFO] [logging.py:96:log_dist] [Rank 0] step=580, skipped=0, lr=[0.0002872771084337349], mom=[(0.9, 0.95)]
[2025-03-04 03:44:46,634] [INFO] [timer.py:215:stop] epoch=0/micro_step=5800/global_step=580, RunningAvgSamplesPerSec=1.0293360985556543, CurrSamplesPerSec=0.9673824985881829, MemAllocated=57.62GB, MaxMemAllocated=62.8GB
Epoch: [1][ 80/500]	Time 10.339 (10.339)	Loss 0.2334 (5.5395)	CeLoss 0.2334 (0.4157)	SegCLSLoss 0.0000 (0.0211)	KLLoss 0.0000 (0.1269)	MaskLoss 0.0000 (0.7683)	MaskBCELoss 0.0000 (0.1933)	MaskDICELoss 0.0000 (0.5750)
Epoch: [1][ 81/500]	Time  9.483 ( 9.483)	Loss 8.3327 (4.7390)	CeLoss 0.1885 (0.6853)	SegCLSLoss 0.0247 (0.0126)	KLLoss 0.1484 (0.0625)	MaskLoss 1.0539 (0.6174)	MaskBCELoss 0.0747 (0.1590)	MaskDICELoss 0.9792 (0.4584)
Epoch: [1][ 82/500]	Time  9.215 ( 9.215)	Loss 7.4928 (6.1849)	CeLoss 0.2139 (0.2959)	SegCLSLoss 0.0496 (0.0249)	KLLoss 0.2109 (0.1318)	MaskLoss 1.0455 (0.9271)	MaskBCELoss 0.2200 (0.2787)	MaskDICELoss 0.8254 (0.6485)
Epoch: [1][ 83/500]	Time  9.936 ( 9.936)	Loss 6.5817 (5.5993)	CeLoss 0.2217 (0.5713)	SegCLSLoss 0.0349 (0.0185)	KLLoss 0.1426 (0.0990)	MaskLoss 0.8626 (0.7359)	MaskBCELoss 0.1170 (0.1613)	MaskDICELoss 0.7456 (0.5746)
Epoch: [1][ 84/500]	Time  8.814 ( 8.814)	Loss 5.8754 (3.4355)	CeLoss 0.2676 (0.5405)	SegCLSLoss 0.0193 (0.0180)	KLLoss 0.0957 (0.0814)	MaskLoss 1.0170 (0.4839)	MaskBCELoss 0.4390 (0.1777)	MaskDICELoss 0.5781 (0.3062)
Epoch: [1][ 85/500]	Time  9.984 ( 9.984)	Loss 1.2188 (6.4342)	CeLoss 1.2188 (0.3317)	SegCLSLoss 0.0000 (0.0305)	KLLoss 0.0000 (0.1609)	MaskLoss 0.0000 (0.8848)	MaskBCELoss 0.0000 (0.1919)	MaskDICELoss 0.0000 (0.6929)
Epoch: [1][ 86/500]	Time  9.688 ( 9.688)	Loss 7.8815 (6.2507)	CeLoss 0.2373 (0.3709)	SegCLSLoss 0.0386 (0.0251)	KLLoss 0.1631 (0.1136)	MaskLoss 1.0963 (0.8826)	MaskBCELoss 0.2178 (0.2179)	MaskDICELoss 0.8785 (0.6648)
Epoch: [1][ 87/500]	Time  8.953 ( 8.953)	Loss 8.2147 (4.9347)	CeLoss 0.2471 (0.5873)	SegCLSLoss 0.0317 (0.0157)	KLLoss 0.2002 (0.0903)	MaskLoss 1.1721 (0.6665)	MaskBCELoss 0.2711 (0.1806)	MaskDICELoss 0.9010 (0.4859)
Epoch: [1][ 88/500]	Time  9.542 ( 9.542)	Loss 0.8086 (5.6467)	CeLoss 0.8086 (0.4168)	SegCLSLoss 0.0000 (0.0241)	KLLoss 0.0000 (0.1148)	MaskLoss 0.0000 (0.7434)	MaskBCELoss 0.0000 (0.1407)	MaskDICELoss 0.0000 (0.6027)
Epoch: [1][ 89/500]	Time  9.710 ( 9.710)	Loss 7.2808 (6.6100)	CeLoss 0.2441 (0.3542)	SegCLSLoss 0.0376 (0.0329)	KLLoss 0.1816 (0.1391)	MaskLoss 1.2131 (0.9108)	MaskBCELoss 0.4782 (0.1977)	MaskDICELoss 0.7349 (0.7131)
[2025-03-04 03:46:23,400] [INFO] [logging.py:96:log_dist] [Rank 0] step=590, skipped=0, lr=[0.00028703614457831325], mom=[(0.9, 0.95)]
[2025-03-04 03:46:23,406] [INFO] [timer.py:215:stop] epoch=0/micro_step=5900/global_step=590, RunningAvgSamplesPerSec=1.0294069414013487, CurrSamplesPerSec=0.8738379828464328, MemAllocated=56.71GB, MaxMemAllocated=62.8GB
Epoch: [1][ 90/500]	Time 11.446 (11.446)	Loss 1.2969 (6.1909)	CeLoss 1.2969 (0.3459)	SegCLSLoss 0.0000 (0.0255)	KLLoss 0.0000 (0.1167)	MaskLoss 0.0000 (0.8314)	MaskBCELoss 0.0000 (0.1560)	MaskDICELoss 0.0000 (0.6754)
Epoch: [1][ 91/500]	Time  8.852 ( 8.852)	Loss 1.1406 (6.3386)	CeLoss 1.1406 (0.4917)	SegCLSLoss 0.0000 (0.0333)	KLLoss 0.0000 (0.1427)	MaskLoss 0.0000 (0.8846)	MaskBCELoss 0.0000 (0.2315)	MaskDICELoss 0.0000 (0.6531)
Epoch: [1][ 92/500]	Time 11.584 (11.584)	Loss 8.0249 (6.9881)	CeLoss 0.2119 (0.2478)	SegCLSLoss 0.0294 (0.0286)	KLLoss 0.1729 (0.1439)	MaskLoss 1.0332 (0.9718)	MaskBCELoss 0.1066 (0.1987)	MaskDICELoss 0.9267 (0.7731)
Epoch: [1][ 93/500]	Time  8.707 ( 8.707)	Loss 0.7109 (5.0309)	CeLoss 0.7109 (0.6585)	SegCLSLoss 0.0000 (0.0169)	KLLoss 0.0000 (0.0837)	MaskLoss 0.0000 (0.6981)	MaskBCELoss 0.0000 (0.2175)	MaskDICELoss 0.0000 (0.4806)
Epoch: [1][ 94/500]	Time  8.765 ( 8.765)	Loss 4.5348 (4.7550)	CeLoss 0.3633 (0.5035)	SegCLSLoss 0.0211 (0.0170)	KLLoss 0.1123 (0.0836)	MaskLoss 0.6982 (0.6746)	MaskBCELoss 0.2558 (0.2062)	MaskDICELoss 0.4424 (0.4684)
Epoch: [1][ 95/500]	Time  9.347 ( 9.347)	Loss 8.5495 (5.4977)	CeLoss 0.2422 (0.4109)	SegCLSLoss 0.0325 (0.0200)	KLLoss 0.1777 (0.1043)	MaskLoss 1.1695 (0.7299)	MaskBCELoss 0.2070 (0.1444)	MaskDICELoss 0.9625 (0.5855)
Epoch: [1][ 96/500]	Time 12.094 (12.094)	Loss 6.1650 (6.7075)	CeLoss 0.2432 (0.2670)	SegCLSLoss 0.0243 (0.0323)	KLLoss 0.1006 (0.1569)	MaskLoss 0.8203 (1.0759)	MaskBCELoss 0.1255 (0.3900)	MaskDICELoss 0.6948 (0.6859)
Epoch: [1][ 97/500]	Time 10.084 (10.084)	Loss 12.4132 (7.5734)	CeLoss 0.9727 (0.4073)	SegCLSLoss 0.0254 (0.0251)	KLLoss 0.1279 (0.1239)	MaskLoss 4.0432 (1.3325)	MaskBCELoss 3.5069 (0.6051)	MaskDICELoss 0.5362 (0.7274)
Epoch: [1][ 98/500]	Time  7.193 ( 7.193)	Loss 5.6316 (4.5718)	CeLoss 0.3906 (0.6066)	SegCLSLoss 0.0256 (0.0220)	KLLoss 0.1104 (0.0855)	MaskLoss 0.9340 (0.6517)	MaskBCELoss 0.3926 (0.2242)	MaskDICELoss 0.5413 (0.4275)
Epoch: [1][ 99/500]	Time  8.334 ( 8.334)	Loss 4.9443 (3.7893)	CeLoss 0.2812 (0.4174)	SegCLSLoss 0.0238 (0.0186)	KLLoss 0.0645 (0.0820)	MaskLoss 0.7411 (0.5644)	MaskBCELoss 0.2236 (0.2058)	MaskDICELoss 0.5175 (0.3586)
[2025-03-04 03:47:59,029] [INFO] [logging.py:96:log_dist] [Rank 0] step=600, skipped=0, lr=[0.0002867951807228916], mom=[(0.9, 0.95)]
[2025-03-04 03:47:59,035] [INFO] [timer.py:215:stop] epoch=0/micro_step=6000/global_step=600, RunningAvgSamplesPerSec=1.0296784337007994, CurrSamplesPerSec=0.9375821847796734, MemAllocated=57.1GB, MaxMemAllocated=62.8GB
Epoch: [1][100/500]	Time 10.668 (10.668)	Loss 3.2259 (5.5691)	CeLoss 0.2754 (0.4662)	SegCLSLoss 0.0197 (0.0253)	KLLoss 0.0859 (0.1213)	MaskLoss 0.4741 (0.7992)	MaskBCELoss 0.1563 (0.2374)	MaskDICELoss 0.3178 (0.5618)
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
Epoch: [1][101/500]	Time  9.958 ( 9.958)	Loss 6.4181 (6.3616)	CeLoss 0.1650 (0.2980)	SegCLSLoss 0.0537 (0.0316)	KLLoss 0.2266 (0.1557)	MaskLoss 0.8870 (0.9612)	MaskBCELoss 0.1826 (0.2996)	MaskDICELoss 0.7044 (0.6616)
Epoch: [1][102/500]	Time  9.320 ( 9.320)	Loss 5.6473 (5.8831)	CeLoss 0.3027 (0.4230)	SegCLSLoss 0.0238 (0.0209)	KLLoss 0.1455 (0.1160)	MaskLoss 1.1721 (0.8775)	MaskBCELoss 0.6984 (0.2811)	MaskDICELoss 0.4737 (0.5964)
Epoch: [1][103/500]	Time  9.548 ( 9.548)	Loss 6.9812 (5.8222)	CeLoss 0.2637 (0.3886)	SegCLSLoss 0.0194 (0.0209)	KLLoss 0.0967 (0.1122)	MaskLoss 1.1909 (0.8359)	MaskBCELoss 0.4862 (0.2294)	MaskDICELoss 0.7047 (0.6065)
Epoch: [1][104/500]	Time  9.551 ( 9.551)	Loss 6.5679 (5.3461)	CeLoss 0.1963 (0.4349)	SegCLSLoss 0.0386 (0.0210)	KLLoss 0.2061 (0.1202)	MaskLoss 0.8632 (0.6687)	MaskBCELoss 0.1265 (0.0948)	MaskDICELoss 0.7366 (0.5739)
Epoch: [1][105/500]	Time 10.521 (10.521)	Loss 1.2969 (5.4506)	CeLoss 1.2969 (0.3884)	SegCLSLoss 0.0000 (0.0186)	KLLoss 0.0000 (0.1070)	MaskLoss 0.0000 (0.7313)	MaskBCELoss 0.0000 (0.1508)	MaskDICELoss 0.0000 (0.5806)
Epoch: [1][106/500]	Time 10.847 (10.847)	Loss 7.6602 (5.2947)	CeLoss 0.2773 (0.3967)	SegCLSLoss 0.0304 (0.0199)	KLLoss 0.1885 (0.1101)	MaskLoss 1.2940 (0.7519)	MaskBCELoss 0.5287 (0.2062)	MaskDICELoss 0.7653 (0.5457)
Epoch: [1][107/500]	Time  9.227 ( 9.227)	Loss 8.5452 (5.3866)	CeLoss 0.2090 (0.2985)	SegCLSLoss 0.0223 (0.0212)	KLLoss 0.1030 (0.1166)	MaskLoss 1.1236 (0.7405)	MaskBCELoss 0.1277 (0.1605)	MaskDICELoss 0.9960 (0.5800)
Epoch: [1][108/500]	Time  9.704 ( 9.704)	Loss 4.2966 (6.1692)	CeLoss 0.2363 (0.4338)	SegCLSLoss 0.0182 (0.0205)	KLLoss 0.1289 (0.1304)	MaskLoss 0.6290 (0.8252)	MaskBCELoss 0.1851 (0.1678)	MaskDICELoss 0.4439 (0.6573)
Epoch: [1][109/500]	Time 10.378 (10.378)	Loss 8.2836 (6.5899)	CeLoss 0.1885 (0.2771)	SegCLSLoss 0.0374 (0.0255)	KLLoss 0.2031 (0.1423)	MaskLoss 1.1677 (0.9561)	MaskBCELoss 0.2447 (0.2486)	MaskDICELoss 0.9230 (0.7076)
[2025-03-04 03:49:38,673] [INFO] [logging.py:96:log_dist] [Rank 0] step=610, skipped=0, lr=[0.00028655421686746985], mom=[(0.9, 0.95)]
[2025-03-04 03:49:38,680] [INFO] [timer.py:215:stop] epoch=0/micro_step=6100/global_step=610, RunningAvgSamplesPerSec=1.029240784530196, CurrSamplesPerSec=0.9442926621395535, MemAllocated=56.97GB, MaxMemAllocated=62.8GB
Epoch: [1][110/500]	Time 10.592 (10.592)	Loss 8.7104 (5.3132)	CeLoss 0.1582 (0.4379)	SegCLSLoss 0.0515 (0.0200)	KLLoss 0.2305 (0.1152)	MaskLoss 1.3401 (0.6938)	MaskBCELoss 0.4040 (0.1333)	MaskDICELoss 0.9360 (0.5604)
Epoch: [1][111/500]	Time  8.622 ( 8.622)	Loss 7.9240 (5.2479)	CeLoss 0.3516 (0.4769)	SegCLSLoss 0.0281 (0.0260)	KLLoss 0.2080 (0.1442)	MaskLoss 1.2160 (0.7468)	MaskBCELoss 0.3964 (0.2267)	MaskDICELoss 0.8196 (0.5201)
Epoch: [1][112/500]	Time  9.359 ( 9.359)	Loss 4.5084 (5.6643)	CeLoss 0.2070 (0.4147)	SegCLSLoss 0.0269 (0.0254)	KLLoss 0.1602 (0.1469)	MaskLoss 0.5662 (0.8220)	MaskBCELoss 0.0670 (0.2478)	MaskDICELoss 0.4992 (0.5743)
Epoch: [1][113/500]	Time 10.846 (10.846)	Loss 4.7693 (5.6592)	CeLoss 0.2109 (0.3900)	SegCLSLoss 0.0327 (0.0208)	KLLoss 0.1377 (0.1139)	MaskLoss 1.0763 (0.9208)	MaskBCELoss 0.7010 (0.3702)	MaskDICELoss 0.3752 (0.5506)
Epoch: [1][114/500]	Time  8.012 ( 8.012)	Loss 4.3671 (3.8303)	CeLoss 0.3086 (0.5185)	SegCLSLoss 0.0233 (0.0165)	KLLoss 0.1133 (0.0967)	MaskLoss 0.8254 (0.5784)	MaskBCELoss 0.4450 (0.2367)	MaskDICELoss 0.3804 (0.3417)
Epoch: [1][115/500]	Time  9.219 ( 9.219)	Loss 8.2743 (5.1146)	CeLoss 0.1836 (0.4611)	SegCLSLoss 0.0374 (0.0238)	KLLoss 0.1943 (0.1033)	MaskLoss 1.3880 (0.7582)	MaskBCELoss 0.5377 (0.2545)	MaskDICELoss 0.8503 (0.5036)
Epoch: [1][116/500]	Time 11.745 (11.745)	Loss 0.1426 (5.1182)	CeLoss 0.1426 (0.3503)	SegCLSLoss 0.0000 (0.0199)	KLLoss 0.0000 (0.1127)	MaskLoss 0.0000 (0.7384)	MaskBCELoss 0.0000 (0.2102)	MaskDICELoss 0.0000 (0.5281)
Epoch: [1][117/500]	Time 10.506 (10.506)	Loss 8.7680 (7.1484)	CeLoss 0.2461 (0.3684)	SegCLSLoss 0.0322 (0.0254)	KLLoss 0.2080 (0.1473)	MaskLoss 1.1779 (1.0154)	MaskBCELoss 0.1874 (0.2505)	MaskDICELoss 0.9906 (0.7649)
Epoch: [1][118/500]	Time 11.543 (11.543)	Loss 8.6246 (6.1832)	CeLoss 0.3965 (0.3390)	SegCLSLoss 0.0206 (0.0211)	KLLoss 0.1631 (0.1333)	MaskLoss 1.0287 (0.8157)	MaskBCELoss 0.0292 (0.1376)	MaskDICELoss 0.9995 (0.6782)
Epoch: [1][119/500]	Time  9.501 ( 9.501)	Loss 8.0680 (5.3199)	CeLoss 0.1680 (0.4700)	SegCLSLoss 0.0447 (0.0242)	KLLoss 0.2090 (0.1322)	MaskLoss 1.2458 (0.6906)	MaskBCELoss 0.3829 (0.1366)	MaskDICELoss 0.8630 (0.5540)
[2025-03-04 03:51:17,991] [INFO] [logging.py:96:log_dist] [Rank 0] step=620, skipped=0, lr=[0.00028631325301204817], mom=[(0.9, 0.95)]
[2025-03-04 03:51:17,997] [INFO] [timer.py:215:stop] epoch=0/micro_step=6200/global_step=620, RunningAvgSamplesPerSec=1.0288738355404787, CurrSamplesPerSec=1.0039387257834091, MemAllocated=56.69GB, MaxMemAllocated=62.8GB
Epoch: [1][120/500]	Time  9.963 ( 9.963)	Loss 1.6016 (5.2159)	CeLoss 1.6016 (0.4235)	SegCLSLoss 0.0000 (0.0157)	KLLoss 0.0000 (0.1030)	MaskLoss 0.0000 (0.7005)	MaskBCELoss 0.0000 (0.1537)	MaskDICELoss 0.0000 (0.5468)
Epoch: [1][121/500]	Time  9.282 ( 9.282)	Loss 9.5857 (6.6658)	CeLoss 0.3027 (0.5122)	SegCLSLoss 0.0283 (0.0203)	KLLoss 0.1699 (0.1234)	MaskLoss 1.5846 (0.9588)	MaskBCELoss 0.5962 (0.2750)	MaskDICELoss 0.9884 (0.6837)
Epoch: [1][122/500]	Time  8.586 ( 8.586)	Loss 8.1868 (6.6932)	CeLoss 0.1533 (0.2443)	SegCLSLoss 0.0491 (0.0307)	KLLoss 0.2246 (0.1854)	MaskLoss 1.6110 (1.0519)	MaskBCELoss 0.8506 (0.3611)	MaskDICELoss 0.7604 (0.6908)
Epoch: [1][123/500]	Time 11.509 (11.509)	Loss 7.8520 (6.3460)	CeLoss 0.2070 (0.2403)	SegCLSLoss 0.0364 (0.0220)	KLLoss 0.2109 (0.1277)	MaskLoss 0.9482 (0.8780)	MaskBCELoss 0.0285 (0.1762)	MaskDICELoss 0.9197 (0.7018)
Epoch: [1][124/500]	Time 10.469 (10.469)	Loss 6.8572 (4.6939)	CeLoss 0.2480 (0.5273)	SegCLSLoss 0.0179 (0.0162)	KLLoss 0.1260 (0.0990)	MaskLoss 1.0060 (0.6153)	MaskBCELoss 0.2623 (0.1438)	MaskDICELoss 0.7437 (0.4715)
Epoch: [1][125/500]	Time  9.096 ( 9.096)	Loss 8.4231 (5.2022)	CeLoss 0.2354 (0.3963)	SegCLSLoss 0.0269 (0.0167)	KLLoss 0.1797 (0.1167)	MaskLoss 1.0184 (0.6744)	MaskBCELoss 0.0257 (0.1191)	MaskDICELoss 0.9928 (0.5553)
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
Epoch: [1][126/500]	Time  9.883 ( 9.883)	Loss 8.3740 (6.0983)	CeLoss 0.1973 (0.4585)	SegCLSLoss 0.0322 (0.0321)	KLLoss 0.2246 (0.1451)	MaskLoss 1.3169 (0.8645)	MaskBCELoss 0.4332 (0.2396)	MaskDICELoss 0.8838 (0.6249)
Epoch: [1][127/500]	Time  8.356 ( 8.356)	Loss 8.1783 (4.1201)	CeLoss 0.1973 (0.4820)	SegCLSLoss 0.0334 (0.0143)	KLLoss 0.2061 (0.0903)	MaskLoss 1.2937 (0.5396)	MaskBCELoss 0.4319 (0.1293)	MaskDICELoss 0.8618 (0.4103)
Epoch: [1][128/500]	Time  9.183 ( 9.183)	Loss 7.0402 (4.2070)	CeLoss 0.2334 (0.4682)	SegCLSLoss 0.0315 (0.0161)	KLLoss 0.2002 (0.0988)	MaskLoss 1.0903 (0.5845)	MaskBCELoss 0.3553 (0.1741)	MaskDICELoss 0.7350 (0.4105)
Epoch: [1][129/500]	Time  9.528 ( 9.528)	Loss 7.1369 (6.0121)	CeLoss 0.2490 (0.4719)	SegCLSLoss 0.0220 (0.0222)	KLLoss 0.1494 (0.1393)	MaskLoss 0.9104 (0.7600)	MaskBCELoss 0.0927 (0.1151)	MaskDICELoss 0.8177 (0.6450)
[2025-03-04 03:52:53,628] [INFO] [logging.py:96:log_dist] [Rank 0] step=630, skipped=0, lr=[0.00028607228915662644], mom=[(0.9, 0.95)]
[2025-03-04 03:52:53,633] [INFO] [timer.py:215:stop] epoch=0/micro_step=6300/global_step=630, RunningAvgSamplesPerSec=1.0291394069845805, CurrSamplesPerSec=1.0264254312656578, MemAllocated=57.4GB, MaxMemAllocated=62.8GB
Epoch: [1][130/500]	Time  9.745 ( 9.745)	Loss 0.0723 (5.0879)	CeLoss 0.0723 (0.5256)	SegCLSLoss 0.0000 (0.0173)	KLLoss 0.0000 (0.1119)	MaskLoss 0.0000 (0.6756)	MaskBCELoss 0.0000 (0.1606)	MaskDICELoss 0.0000 (0.5151)
Epoch: [1][131/500]	Time  9.286 ( 9.286)	Loss 3.7918 (3.9180)	CeLoss 0.2793 (0.5805)	SegCLSLoss 0.0183 (0.0124)	KLLoss 0.1147 (0.0857)	MaskLoss 0.5482 (0.5121)	MaskBCELoss 0.1664 (0.1419)	MaskDICELoss 0.3818 (0.3702)
Epoch: [1][132/500]	Time  9.235 ( 9.235)	Loss 6.8320 (4.7604)	CeLoss 0.3145 (0.4305)	SegCLSLoss 0.0239 (0.0170)	KLLoss 0.1338 (0.1021)	MaskLoss 1.2726 (0.6317)	MaskBCELoss 0.6350 (0.1391)	MaskDICELoss 0.6376 (0.4926)
Epoch: [1][133/500]	Time  9.966 ( 9.966)	Loss 5.1499 (5.7443)	CeLoss 0.2295 (0.3609)	SegCLSLoss 0.0166 (0.0216)	KLLoss 0.0938 (0.1202)	MaskLoss 0.7045 (0.7932)	MaskBCELoss 0.1364 (0.1822)	MaskDICELoss 0.5681 (0.6110)
Epoch: [1][134/500]	Time  9.294 ( 9.294)	Loss 0.1289 (4.8953)	CeLoss 0.1289 (0.4596)	SegCLSLoss 0.0000 (0.0159)	KLLoss 0.0000 (0.0959)	MaskLoss 0.0000 (0.6073)	MaskBCELoss 0.0000 (0.0877)	MaskDICELoss 0.0000 (0.5195)
Epoch: [1][135/500]	Time 10.644 (10.644)	Loss 7.3184 (5.8659)	CeLoss 0.2969 (0.4719)	SegCLSLoss 0.0220 (0.0227)	KLLoss 0.1572 (0.1396)	MaskLoss 0.8841 (0.7105)	MaskBCELoss 0.0365 (0.0735)	MaskDICELoss 0.8476 (0.6370)
Epoch: [1][136/500]	Time 10.694 (10.694)	Loss 4.3527 (5.2477)	CeLoss 0.2520 (0.3729)	SegCLSLoss 0.0173 (0.0184)	KLLoss 0.1387 (0.1148)	MaskLoss 0.9411 (0.7038)	MaskBCELoss 0.5958 (0.1467)	MaskDICELoss 0.3453 (0.5572)
Epoch: [1][137/500]	Time  9.724 ( 9.724)	Loss 7.3733 (4.8724)	CeLoss 0.1982 (0.5701)	SegCLSLoss 0.0294 (0.0187)	KLLoss 0.2031 (0.1267)	MaskLoss 1.2911 (0.6697)	MaskBCELoss 0.5619 (0.1985)	MaskDICELoss 0.7292 (0.4711)
Epoch: [1][138/500]	Time 11.351 (11.351)	Loss 8.4385 (5.8037)	CeLoss 0.2402 (0.3870)	SegCLSLoss 0.0278 (0.0221)	KLLoss 0.1396 (0.1178)	MaskLoss 1.0671 (0.8151)	MaskBCELoss 0.0821 (0.2054)	MaskDICELoss 0.9850 (0.6096)
Epoch: [1][139/500]	Time  9.384 ( 9.384)	Loss 7.2267 (5.2745)	CeLoss 0.2832 (0.4436)	SegCLSLoss 0.0206 (0.0198)	KLLoss 0.1738 (0.1246)	MaskLoss 1.1059 (0.7136)	MaskBCELoss 0.3479 (0.1687)	MaskDICELoss 0.7580 (0.5449)
[2025-03-04 03:54:32,911] [INFO] [logging.py:96:log_dist] [Rank 0] step=640, skipped=0, lr=[0.00028583132530120476], mom=[(0.9, 0.95)]
[2025-03-04 03:54:32,917] [INFO] [timer.py:215:stop] epoch=0/micro_step=6400/global_step=640, RunningAvgSamplesPerSec=1.0287909906174344, CurrSamplesPerSec=1.0303703856558468, MemAllocated=57.26GB, MaxMemAllocated=62.82GB
Epoch: [1][140/500]	Time  9.707 ( 9.707)	Loss 7.9022 (5.6943)	CeLoss 0.1680 (0.3909)	SegCLSLoss 0.0374 (0.0271)	KLLoss 0.2266 (0.1434)	MaskLoss 1.3349 (0.7921)	MaskBCELoss 0.5318 (0.1983)	MaskDICELoss 0.8031 (0.5937)
Epoch: [1][141/500]	Time 10.912 (10.912)	Loss 6.7980 (5.6592)	CeLoss 0.1875 (0.3412)	SegCLSLoss 0.0364 (0.0248)	KLLoss 0.2109 (0.1340)	MaskLoss 0.8677 (0.7836)	MaskBCELoss 0.0936 (0.1829)	MaskDICELoss 0.7741 (0.6007)
Epoch: [1][142/500]	Time  8.301 ( 8.301)	Loss 5.6720 (4.4148)	CeLoss 0.2656 (0.6024)	SegCLSLoss 0.0198 (0.0149)	KLLoss 0.1240 (0.0916)	MaskLoss 0.7452 (0.6148)	MaskBCELoss 0.1146 (0.2008)	MaskDICELoss 0.6305 (0.4140)
Epoch: [1][143/500]	Time 10.214 (10.214)	Loss 0.3281 (5.0430)	CeLoss 0.3281 (0.3854)	SegCLSLoss 0.0000 (0.0190)	KLLoss 0.0000 (0.1084)	MaskLoss 0.0000 (0.7224)	MaskBCELoss 0.0000 (0.2066)	MaskDICELoss 0.0000 (0.5158)
Epoch: [1][144/500]	Time 11.073 (11.073)	Loss 0.8477 (4.7815)	CeLoss 0.8477 (0.2974)	SegCLSLoss 0.0000 (0.0206)	KLLoss 0.0000 (0.1100)	MaskLoss 0.0000 (0.6967)	MaskBCELoss 0.0000 (0.2016)	MaskDICELoss 0.0000 (0.4951)
Epoch: [1][145/500]	Time  8.468 ( 8.468)	Loss 5.7692 (6.3256)	CeLoss 0.2471 (0.3591)	SegCLSLoss 0.0359 (0.0194)	KLLoss 0.1846 (0.1169)	MaskLoss 0.8934 (1.1321)	MaskBCELoss 0.3046 (0.5361)	MaskDICELoss 0.5889 (0.5960)
Epoch: [1][146/500]	Time  8.093 ( 8.093)	Loss 0.0708 (4.4472)	CeLoss 0.0708 (0.4928)	SegCLSLoss 0.0000 (0.0148)	KLLoss 0.0000 (0.0945)	MaskLoss 0.0000 (0.6798)	MaskBCELoss 0.0000 (0.2643)	MaskDICELoss 0.0000 (0.4155)
Epoch: [1][147/500]	Time 10.496 (10.496)	Loss 8.4372 (5.6018)	CeLoss 0.3047 (0.3097)	SegCLSLoss 0.0249 (0.0193)	KLLoss 0.1270 (0.1112)	MaskLoss 1.0137 (0.7977)	MaskBCELoss 0.0193 (0.2018)	MaskDICELoss 0.9944 (0.5960)
Epoch: [1][148/500]	Time  8.003 ( 8.003)	Loss 3.9559 (4.3279)	CeLoss 0.2100 (0.6888)	SegCLSLoss 0.0175 (0.0138)	KLLoss 0.1309 (0.0863)	MaskLoss 0.8132 (0.5553)	MaskBCELoss 0.4832 (0.1495)	MaskDICELoss 0.3300 (0.4058)
Epoch: [1][149/500]	Time  7.793 ( 7.793)	Loss 6.6596 (4.2499)	CeLoss 0.2246 (0.3658)	SegCLSLoss 0.0339 (0.0131)	KLLoss 0.1816 (0.0709)	MaskLoss 0.8081 (0.6365)	MaskBCELoss 0.0379 (0.2143)	MaskDICELoss 0.7702 (0.4222)
[2025-03-04 03:56:06,605] [INFO] [logging.py:96:log_dist] [Rank 0] step=650, skipped=0, lr=[0.0002855903614457831], mom=[(0.9, 0.95)]
[2025-03-04 03:56:06,610] [INFO] [timer.py:215:stop] epoch=0/micro_step=6500/global_step=650, RunningAvgSamplesPerSec=1.02936698062995, CurrSamplesPerSec=0.9672231509412746, MemAllocated=57.28GB, MaxMemAllocated=62.82GB
Epoch: [1][150/500]	Time 10.341 (10.341)	Loss 7.9352 (5.4864)	CeLoss 0.2197 (0.4345)	SegCLSLoss 0.0381 (0.0173)	KLLoss 0.2295 (0.1044)	MaskLoss 1.1528 (0.7287)	MaskBCELoss 0.2927 (0.1484)	MaskDICELoss 0.8601 (0.5803)
Epoch: [1][151/500]	Time  7.904 ( 7.904)	Loss 1.1797 (4.1070)	CeLoss 1.1797 (0.5640)	SegCLSLoss 0.0000 (0.0189)	KLLoss 0.0000 (0.1075)	MaskLoss 0.0000 (0.5448)	MaskBCELoss 0.0000 (0.1554)	MaskDICELoss 0.0000 (0.3895)
Epoch: [1][152/500]	Time 10.917 (10.917)	Loss 8.6836 (4.8662)	CeLoss 0.2520 (0.3355)	SegCLSLoss 0.0189 (0.0165)	KLLoss 0.1934 (0.1027)	MaskLoss 1.1238 (0.6171)	MaskBCELoss 0.1269 (0.0862)	MaskDICELoss 0.9968 (0.5309)
Epoch: [1][153/500]	Time  9.317 ( 9.317)	Loss 5.2726 (5.7308)	CeLoss 0.2969 (0.4396)	SegCLSLoss 0.0175 (0.0217)	KLLoss 0.1182 (0.1329)	MaskLoss 0.8531 (0.7829)	MaskBCELoss 0.3290 (0.1859)	MaskDICELoss 0.5241 (0.5970)
Epoch: [1][154/500]	Time 11.466 (11.466)	Loss 5.6249 (6.9004)	CeLoss 0.3262 (0.3765)	SegCLSLoss 0.0162 (0.0236)	KLLoss 0.1055 (0.1406)	MaskLoss 0.9983 (1.0024)	MaskBCELoss 0.4668 (0.2746)	MaskDICELoss 0.5315 (0.7278)
Epoch: [1][155/500]	Time  8.311 ( 8.311)	Loss 7.6701 (4.1126)	CeLoss 0.2002 (0.4169)	SegCLSLoss 0.0320 (0.0143)	KLLoss 0.1924 (0.0933)	MaskLoss 1.1490 (0.5634)	MaskBCELoss 0.3217 (0.1520)	MaskDICELoss 0.8273 (0.4114)
Epoch: [1][156/500]	Time 10.276 (10.276)	Loss 0.0859 (5.1846)	CeLoss 0.0859 (0.4444)	SegCLSLoss 0.0000 (0.0214)	KLLoss 0.0000 (0.1152)	MaskLoss 0.0000 (0.6288)	MaskBCELoss 0.0000 (0.0694)	MaskDICELoss 0.0000 (0.5594)
Epoch: [1][157/500]	Time  9.847 ( 9.847)	Loss 7.8445 (5.4530)	CeLoss 0.2129 (0.2046)	SegCLSLoss 0.0178 (0.0241)	KLLoss 0.1104 (0.1247)	MaskLoss 1.5812 (0.8809)	MaskBCELoss 0.8562 (0.3226)	MaskDICELoss 0.7250 (0.5583)
Epoch: [1][158/500]	Time 12.074 (12.074)	Loss 5.1275 (5.7351)	CeLoss 0.2578 (0.2651)	SegCLSLoss 0.0166 (0.0213)	KLLoss 0.1045 (0.1341)	MaskLoss 0.7628 (0.7989)	MaskBCELoss 0.2243 (0.1777)	MaskDICELoss 0.5385 (0.6212)
Epoch: [1][159/500]	Time 10.267 (10.267)	Loss 8.4957 (5.6951)	CeLoss 0.3105 (0.4503)	SegCLSLoss 0.0220 (0.0181)	KLLoss 0.1416 (0.1090)	MaskLoss 1.3265 (0.8191)	MaskBCELoss 0.4302 (0.2376)	MaskDICELoss 0.8963 (0.5814)
[2025-03-04 03:57:45,219] [INFO] [logging.py:96:log_dist] [Rank 0] step=660, skipped=0, lr=[0.0002853493975903614], mom=[(0.9, 0.95)]
[2025-03-04 03:57:45,225] [INFO] [timer.py:215:stop] epoch=0/micro_step=6600/global_step=660, RunningAvgSamplesPerSec=1.0291367549374513, CurrSamplesPerSec=1.2145714848319877, MemAllocated=57.26GB, MaxMemAllocated=62.82GB
Epoch: [1][160/500]	Time  8.235 ( 8.235)	Loss 8.6402 (6.3167)	CeLoss 0.1846 (0.3545)	SegCLSLoss 0.0277 (0.0228)	KLLoss 0.1689 (0.1366)	MaskLoss 1.1394 (0.8327)	MaskBCELoss 0.1404 (0.1412)	MaskDICELoss 0.9990 (0.6915)
Epoch: [1][161/500]	Time 10.674 (10.674)	Loss 5.8395 (4.7605)	CeLoss 0.3066 (0.4197)	SegCLSLoss 0.0248 (0.0192)	KLLoss 0.1387 (0.1109)	MaskLoss 0.7185 (0.6599)	MaskBCELoss 0.0609 (0.1765)	MaskDICELoss 0.6576 (0.4834)
Epoch: [1][162/500]	Time 10.669 (10.669)	Loss 9.2686 (5.7900)	CeLoss 0.2598 (0.2357)	SegCLSLoss 0.0259 (0.0226)	KLLoss 0.1738 (0.1456)	MaskLoss 1.4336 (0.8571)	MaskBCELoss 0.4413 (0.2433)	MaskDICELoss 0.9923 (0.6138)
Epoch: [1][163/500]	Time 10.787 (10.787)	Loss 7.3850 (4.9009)	CeLoss 0.2695 (0.6177)	SegCLSLoss 0.0167 (0.0174)	KLLoss 0.0986 (0.1057)	MaskLoss 1.0644 (0.5964)	MaskBCELoss 0.2508 (0.1004)	MaskDICELoss 0.8136 (0.4960)
Epoch: [1][164/500]	Time  9.680 ( 9.680)	Loss 8.5074 (5.2155)	CeLoss 0.2383 (0.4700)	SegCLSLoss 0.0339 (0.0183)	KLLoss 0.2246 (0.1074)	MaskLoss 1.0165 (0.6662)	MaskBCELoss 0.0175 (0.1168)	MaskDICELoss 0.9990 (0.5494)
Epoch: [1][165/500]	Time 10.723 (10.723)	Loss 3.1796 (5.6219)	CeLoss 0.4902 (0.3874)	SegCLSLoss 0.0165 (0.0242)	KLLoss 0.0894 (0.1207)	MaskLoss 0.6881 (0.8049)	MaskBCELoss 0.4852 (0.2229)	MaskDICELoss 0.2029 (0.5820)
Epoch: [1][166/500]	Time  9.888 ( 9.888)	Loss 7.1233 (5.5793)	CeLoss 0.2715 (0.3671)	SegCLSLoss 0.0193 (0.0200)	KLLoss 0.1396 (0.1193)	MaskLoss 1.2045 (0.7842)	MaskBCELoss 0.4891 (0.1985)	MaskDICELoss 0.7154 (0.5857)
Epoch: [1][167/500]	Time  9.562 ( 9.562)	Loss 0.7148 (4.6609)	CeLoss 0.7148 (0.3484)	SegCLSLoss 0.0000 (0.0233)	KLLoss 0.0000 (0.1173)	MaskLoss 0.0000 (0.6876)	MaskBCELoss 0.0000 (0.2195)	MaskDICELoss 0.0000 (0.4681)
Epoch: [1][168/500]	Time  8.936 ( 8.936)	Loss 0.6250 (4.5080)	CeLoss 0.6250 (0.4873)	SegCLSLoss 0.0000 (0.0137)	KLLoss 0.0000 (0.0804)	MaskLoss 0.0000 (0.6860)	MaskBCELoss 0.0000 (0.2590)	MaskDICELoss 0.0000 (0.4269)
Epoch: [1][169/500]	Time  9.824 ( 9.824)	Loss 6.7518 (5.7051)	CeLoss 0.2891 (0.4192)	SegCLSLoss 0.0354 (0.0238)	KLLoss 0.2422 (0.1325)	MaskLoss 0.8198 (0.8504)	MaskBCELoss 0.0590 (0.2769)	MaskDICELoss 0.7609 (0.5735)
[2025-03-04 03:59:25,127] [INFO] [logging.py:96:log_dist] [Rank 0] step=670, skipped=0, lr=[0.00028510843373493974], mom=[(0.9, 0.95)]
[2025-03-04 03:59:25,133] [INFO] [timer.py:215:stop] epoch=0/micro_step=6700/global_step=670, RunningAvgSamplesPerSec=1.0287050758058633, CurrSamplesPerSec=1.091396278134854, MemAllocated=57.25GB, MaxMemAllocated=62.82GB
Epoch: [1][170/500]	Time  9.165 ( 9.165)	Loss 5.1159 (4.5059)	CeLoss 0.2773 (0.4648)	SegCLSLoss 0.0173 (0.0201)	KLLoss 0.0811 (0.1165)	MaskLoss 0.8307 (0.6044)	MaskBCELoss 0.3162 (0.1534)	MaskDICELoss 0.5145 (0.4510)
Epoch: [1][171/500]	Time  9.670 ( 9.670)	Loss 7.7680 (5.7795)	CeLoss 0.1562 (0.4544)	SegCLSLoss 0.0483 (0.0264)	KLLoss 0.2344 (0.1576)	MaskLoss 1.3186 (0.8265)	MaskBCELoss 0.5325 (0.2429)	MaskDICELoss 0.7861 (0.5836)
Epoch: [1][172/500]	Time  9.737 ( 9.737)	Loss 4.4506 (4.3174)	CeLoss 0.2217 (0.6562)	SegCLSLoss 0.0188 (0.0198)	KLLoss 0.0618 (0.0897)	MaskLoss 0.5491 (0.5496)	MaskBCELoss 0.0392 (0.1392)	MaskDICELoss 0.5099 (0.4104)
Epoch: [1][173/500]	Time  9.797 ( 9.797)	Loss 7.9627 (5.8914)	CeLoss 0.2891 (0.4849)	SegCLSLoss 0.0216 (0.0212)	KLLoss 0.1436 (0.1174)	MaskLoss 1.2902 (0.8593)	MaskBCELoss 0.4674 (0.2659)	MaskDICELoss 0.8228 (0.5933)
Epoch: [1][174/500]	Time  9.076 ( 9.076)	Loss 4.5795 (5.0506)	CeLoss 0.2354 (0.5358)	SegCLSLoss 0.0203 (0.0173)	KLLoss 0.0947 (0.0962)	MaskLoss 0.6532 (0.6590)	MaskBCELoss 0.1643 (0.1437)	MaskDICELoss 0.4889 (0.5154)
Epoch: [1][175/500]	Time  9.814 ( 9.814)	Loss 0.9297 (5.9040)	CeLoss 0.9297 (0.3251)	SegCLSLoss 0.0000 (0.0286)	KLLoss 0.0000 (0.1407)	MaskLoss 0.0000 (0.8102)	MaskBCELoss 0.0000 (0.1763)	MaskDICELoss 0.0000 (0.6339)
Epoch: [1][176/500]	Time 10.478 (10.478)	Loss 7.3028 (7.1807)	CeLoss 0.3301 (0.4077)	SegCLSLoss 0.0205 (0.0250)	KLLoss 0.0884 (0.1264)	MaskLoss 1.0357 (0.9392)	MaskBCELoss 0.2351 (0.1465)	MaskDICELoss 0.8006 (0.7927)
Epoch: [1][177/500]	Time  9.971 ( 9.971)	Loss 5.6570 (5.0729)	CeLoss 0.1650 (0.3750)	SegCLSLoss 0.0525 (0.0282)	KLLoss 0.2080 (0.1396)	MaskLoss 0.9837 (0.7103)	MaskBCELoss 0.4351 (0.1896)	MaskDICELoss 0.5485 (0.5206)
Epoch: [1][178/500]	Time  7.849 ( 7.849)	Loss 0.9336 (3.4872)	CeLoss 0.9336 (0.8036)	SegCLSLoss 0.0000 (0.0111)	KLLoss 0.0000 (0.0569)	MaskLoss 0.0000 (0.4414)	MaskBCELoss 0.0000 (0.1516)	MaskDICELoss 0.0000 (0.2898)
Epoch: [1][179/500]	Time  8.680 ( 8.680)	Loss 8.1601 (4.1160)	CeLoss 0.2070 (0.4006)	SegCLSLoss 0.0366 (0.0210)	KLLoss 0.1797 (0.1027)	MaskLoss 0.9983 (0.6115)	MaskBCELoss 0.0387 (0.2150)	MaskDICELoss 0.9596 (0.3965)
[2025-03-04 04:01:01,235] [INFO] [logging.py:96:log_dist] [Rank 0] step=680, skipped=0, lr=[0.00028486746987951806], mom=[(0.9, 0.95)]
[2025-03-04 04:01:01,241] [INFO] [timer.py:215:stop] epoch=0/micro_step=6800/global_step=680, RunningAvgSamplesPerSec=1.0288795674417204, CurrSamplesPerSec=0.9064223102727527, MemAllocated=57.24GB, MaxMemAllocated=62.82GB
Epoch: [1][180/500]	Time 11.034 (11.034)	Loss 4.2669 (5.1244)	CeLoss 0.2676 (0.3244)	SegCLSLoss 0.0206 (0.0240)	KLLoss 0.0991 (0.1134)	MaskLoss 0.5420 (0.6704)	MaskBCELoss 0.0744 (0.1148)	MaskDICELoss 0.4676 (0.5556)
Epoch: [1][181/500]	Time  9.366 ( 9.366)	Loss 0.6367 (6.0588)	CeLoss 0.6367 (0.3905)	SegCLSLoss 0.0000 (0.0240)	KLLoss 0.0000 (0.1323)	MaskLoss 0.0000 (0.9663)	MaskBCELoss 0.0000 (0.3677)	MaskDICELoss 0.0000 (0.5986)
Epoch: [1][182/500]	Time  8.140 ( 8.140)	Loss 8.6137 (3.1031)	CeLoss 0.2461 (0.5521)	SegCLSLoss 0.0295 (0.0099)	KLLoss 0.1660 (0.0516)	MaskLoss 1.1127 (0.3973)	MaskBCELoss 0.1192 (0.1140)	MaskDICELoss 0.9934 (0.2833)
Epoch: [1][183/500]	Time 11.627 (11.627)	Loss 8.4166 (5.9486)	CeLoss 0.2402 (0.1963)	SegCLSLoss 0.0239 (0.0306)	KLLoss 0.0840 (0.1360)	MaskLoss 1.0462 (0.9002)	MaskBCELoss 0.0482 (0.2667)	MaskDICELoss 0.9980 (0.6334)
Epoch: [1][184/500]	Time  9.261 ( 9.261)	Loss 1.3984 (5.1554)	CeLoss 1.3984 (0.5542)	SegCLSLoss 0.0000 (0.0234)	KLLoss 0.0000 (0.1294)	MaskLoss 0.0000 (0.7764)	MaskBCELoss 0.0000 (0.2918)	MaskDICELoss 0.0000 (0.4846)
Epoch: [1][185/500]	Time 10.661 (10.661)	Loss 1.1484 (4.6171)	CeLoss 1.1484 (0.4741)	SegCLSLoss 0.0000 (0.0198)	KLLoss 0.0000 (0.0927)	MaskLoss 0.0000 (0.6492)	MaskBCELoss 0.0000 (0.1922)	MaskDICELoss 0.0000 (0.4570)
Epoch: [1][186/500]	Time  8.934 ( 8.934)	Loss 1.1172 (5.0900)	CeLoss 1.1172 (0.5464)	SegCLSLoss 0.0000 (0.0228)	KLLoss 0.0000 (0.1060)	MaskLoss 0.0000 (0.7381)	MaskBCELoss 0.0000 (0.2465)	MaskDICELoss 0.0000 (0.4916)
Epoch: [1][187/500]	Time  8.457 ( 8.457)	Loss 7.9887 (5.6753)	CeLoss 0.5039 (0.4729)	SegCLSLoss 0.0505 (0.0274)	KLLoss 0.2383 (0.1372)	MaskLoss 1.1576 (0.8497)	MaskBCELoss 0.3397 (0.2910)	MaskDICELoss 0.8180 (0.5586)
Epoch: [1][188/500]	Time  9.675 ( 9.675)	Loss 8.5104 (6.0830)	CeLoss 0.2012 (0.3509)	SegCLSLoss 0.0304 (0.0268)	KLLoss 0.1895 (0.1291)	MaskLoss 1.0636 (0.9279)	MaskBCELoss 0.0675 (0.3056)	MaskDICELoss 0.9961 (0.6223)
Epoch: [1][189/500]	Time  8.565 ( 8.565)	Loss 1.0391 (4.9571)	CeLoss 1.0391 (0.6619)	SegCLSLoss 0.0000 (0.0204)	KLLoss 0.0000 (0.1159)	MaskLoss 0.0000 (0.6445)	MaskBCELoss 0.0000 (0.1645)	MaskDICELoss 0.0000 (0.4800)
[2025-03-04 04:02:33,906] [INFO] [logging.py:96:log_dist] [Rank 0] step=690, skipped=0, lr=[0.0002846265060240964], mom=[(0.9, 0.95)]
[2025-03-04 04:02:33,912] [INFO] [timer.py:215:stop] epoch=0/micro_step=6900/global_step=690, RunningAvgSamplesPerSec=1.0295782415907593, CurrSamplesPerSec=1.2525268210640987, MemAllocated=56.74GB, MaxMemAllocated=62.82GB
Epoch: [1][190/500]	Time  7.986 ( 7.986)	Loss 1.1484 (5.1639)	CeLoss 1.1484 (0.3983)	SegCLSLoss 0.0000 (0.0256)	KLLoss 0.0000 (0.1396)	MaskLoss 0.0000 (0.7315)	MaskBCELoss 0.0000 (0.2065)	MaskDICELoss 0.0000 (0.5250)
Epoch: [1][191/500]	Time  9.339 ( 9.339)	Loss 5.2369 (6.2020)	CeLoss 0.2383 (0.2944)	SegCLSLoss 0.0206 (0.0199)	KLLoss 0.1182 (0.1201)	MaskLoss 0.7486 (0.9799)	MaskBCELoss 0.1864 (0.3436)	MaskDICELoss 0.5621 (0.6363)
Epoch: [1][192/500]	Time 10.722 (10.722)	Loss 0.6617 (5.7749)	CeLoss 0.1914 (0.4169)	SegCLSLoss 0.0156 (0.0269)	KLLoss 0.0845 (0.1418)	MaskLoss 0.0613 (0.7396)	MaskBCELoss 0.0187 (0.1190)	MaskDICELoss 0.0426 (0.6206)
Epoch: [1][193/500]	Time  8.523 ( 8.523)	Loss 7.6105 (6.4020)	CeLoss 0.2402 (0.3953)	SegCLSLoss 0.0272 (0.0235)	KLLoss 0.1670 (0.1314)	MaskLoss 0.9351 (0.8341)	MaskBCELoss 0.0487 (0.1348)	MaskDICELoss 0.8864 (0.6992)
Epoch: [1][194/500]	Time  9.518 ( 9.518)	Loss 7.6510 (5.9721)	CeLoss 0.2041 (0.3846)	SegCLSLoss 0.0284 (0.0283)	KLLoss 0.1904 (0.1521)	MaskLoss 0.9708 (0.8456)	MaskBCELoss 0.0876 (0.2240)	MaskDICELoss 0.8832 (0.6216)
Epoch: [1][195/500]	Time  8.457 ( 8.457)	Loss 1.0625 (4.7456)	CeLoss 1.0625 (0.5645)	SegCLSLoss 0.0000 (0.0157)	KLLoss 0.0000 (0.0972)	MaskLoss 0.0000 (0.6310)	MaskBCELoss 0.0000 (0.1620)	MaskDICELoss 0.0000 (0.4690)
Epoch: [1][196/500]	Time  7.699 ( 7.699)	Loss 1.4062 (3.2311)	CeLoss 1.4062 (0.6659)	SegCLSLoss 0.0000 (0.0118)	KLLoss 0.0000 (0.0666)	MaskLoss 0.0000 (0.4305)	MaskBCELoss 0.0000 (0.1586)	MaskDICELoss 0.0000 (0.2719)
Epoch: [1][197/500]	Time 10.064 (10.064)	Loss 8.2003 (5.3277)	CeLoss 0.2295 (0.4258)	SegCLSLoss 0.0205 (0.0214)	KLLoss 0.0835 (0.1065)	MaskLoss 1.0100 (0.6779)	MaskBCELoss 0.0339 (0.1064)	MaskDICELoss 0.9760 (0.5715)
Epoch: [1][198/500]	Time 11.053 (11.053)	Loss 7.6582 (6.7069)	CeLoss 0.2500 (0.3439)	SegCLSLoss 0.0234 (0.0248)	KLLoss 0.1455 (0.1496)	MaskLoss 1.0011 (0.8943)	MaskBCELoss 0.1261 (0.1588)	MaskDICELoss 0.8750 (0.7355)
Epoch: [1][199/500]	Time 10.838 (10.838)	Loss 8.1796 (5.2873)	CeLoss 0.1992 (0.2613)	SegCLSLoss 0.0332 (0.0234)	KLLoss 0.1992 (0.1342)	MaskLoss 1.0812 (0.8021)	MaskBCELoss 0.1474 (0.2561)	MaskDICELoss 0.9338 (0.5460)
[2025-03-04 04:04:08,926] [INFO] [logging.py:96:log_dist] [Rank 0] step=700, skipped=0, lr=[0.0002843855421686747], mom=[(0.9, 0.95)]
[2025-03-04 04:04:08,931] [INFO] [timer.py:215:stop] epoch=0/micro_step=7000/global_step=700, RunningAvgSamplesPerSec=1.0299009103725074, CurrSamplesPerSec=1.1359444711287487, MemAllocated=57.29GB, MaxMemAllocated=62.82GB
Epoch: [1][200/500]	Time  8.805 ( 8.805)	Loss 8.2339 (4.8783)	CeLoss 0.2500 (0.6371)	SegCLSLoss 0.0369 (0.0153)	KLLoss 0.2217 (0.0962)	MaskLoss 1.2159 (0.6754)	MaskBCELoss 0.3303 (0.2109)	MaskDICELoss 0.8856 (0.4645)
Epoch: [1][201/500]	Time  8.500 ( 8.500)	Loss 1.1797 (3.1851)	CeLoss 1.1797 (0.5735)	SegCLSLoss 0.0000 (0.0115)	KLLoss 0.0000 (0.0673)	MaskLoss 0.0000 (0.4372)	MaskBCELoss 0.0000 (0.1599)	MaskDICELoss 0.0000 (0.2773)
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
Epoch: [1][202/500]	Time 10.414 (10.414)	Loss 8.2832 (7.7048)	CeLoss 0.1855 (0.2074)	SegCLSLoss 0.0371 (0.0410)	KLLoss 0.2168 (0.2053)	MaskLoss 1.1397 (1.1073)	MaskBCELoss 0.2093 (0.2644)	MaskDICELoss 0.9303 (0.8429)
Epoch: [1][203/500]	Time  9.020 ( 9.020)	Loss 0.9102 (3.8364)	CeLoss 0.9102 (0.6514)	SegCLSLoss 0.0000 (0.0143)	KLLoss 0.0000 (0.0888)	MaskLoss 0.0000 (0.4528)	MaskBCELoss 0.0000 (0.0889)	MaskDICELoss 0.0000 (0.3640)
Epoch: [1][204/500]	Time  8.994 ( 8.994)	Loss 6.0880 (4.8161)	CeLoss 0.2354 (0.5477)	SegCLSLoss 0.0216 (0.0138)	KLLoss 0.1099 (0.0827)	MaskLoss 1.0430 (0.6541)	MaskBCELoss 0.4352 (0.1757)	MaskDICELoss 0.6078 (0.4785)
Epoch: [1][205/500]	Time 10.049 (10.049)	Loss 8.5673 (6.9436)	CeLoss 0.2539 (0.3283)	SegCLSLoss 0.0255 (0.0279)	KLLoss 0.1631 (0.1436)	MaskLoss 1.1199 (1.0404)	MaskBCELoss 0.1370 (0.3109)	MaskDICELoss 0.9829 (0.7295)
Epoch: [1][206/500]	Time 10.868 (10.868)	Loss 5.3772 (5.8443)	CeLoss 0.2852 (0.3177)	SegCLSLoss 0.0168 (0.0239)	KLLoss 0.1055 (0.1400)	MaskLoss 0.8921 (0.8431)	MaskBCELoss 0.3597 (0.2283)	MaskDICELoss 0.5324 (0.6148)
Epoch: [1][207/500]	Time  9.331 ( 9.331)	Loss 6.3396 (4.4725)	CeLoss 0.1914 (0.3888)	SegCLSLoss 0.0400 (0.0221)	KLLoss 0.2207 (0.1201)	MaskLoss 1.0137 (0.5973)	MaskBCELoss 0.3669 (0.1377)	MaskDICELoss 0.6468 (0.4597)
Epoch: [1][208/500]	Time  9.371 ( 9.371)	Loss 5.4688 (5.3214)	CeLoss 0.3301 (0.3376)	SegCLSLoss 0.0171 (0.0236)	KLLoss 0.1279 (0.1234)	MaskLoss 0.8329 (0.7195)	MaskBCELoss 0.2765 (0.1511)	MaskDICELoss 0.5564 (0.5683)
Epoch: [1][209/500]	Time  9.815 ( 9.815)	Loss 8.6288 (6.5160)	CeLoss 0.2266 (0.3877)	SegCLSLoss 0.0311 (0.0252)	KLLoss 0.1729 (0.1481)	MaskLoss 1.1448 (0.9590)	MaskBCELoss 0.1573 (0.2840)	MaskDICELoss 0.9875 (0.6750)
[2025-03-04 04:05:44,793] [INFO] [logging.py:96:log_dist] [Rank 0] step=710, skipped=0, lr=[0.000284144578313253], mom=[(0.9, 0.95)]
[2025-03-04 04:05:44,799] [INFO] [timer.py:215:stop] epoch=0/micro_step=7100/global_step=710, RunningAvgSamplesPerSec=1.0300873424622117, CurrSamplesPerSec=1.052185258482911, MemAllocated=56.71GB, MaxMemAllocated=62.82GB
Epoch: [1][210/500]	Time  9.506 ( 9.506)	Loss 0.8047 (6.7236)	CeLoss 0.8047 (0.4163)	SegCLSLoss 0.0000 (0.0270)	KLLoss 0.0000 (0.1416)	MaskLoss 0.0000 (1.1024)	MaskBCELoss 0.0000 (0.4446)	MaskDICELoss 0.0000 (0.6578)
Epoch: [1][211/500]	Time  9.645 ( 9.645)	Loss 8.2743 (6.3565)	CeLoss 0.2305 (0.4597)	SegCLSLoss 0.0352 (0.0244)	KLLoss 0.2168 (0.1555)	MaskLoss 1.1997 (0.8461)	MaskBCELoss 0.2980 (0.1732)	MaskDICELoss 0.9017 (0.6728)
Epoch: [1][212/500]	Time  9.660 ( 9.660)	Loss 0.0781 (5.9308)	CeLoss 0.0781 (0.3911)	SegCLSLoss 0.0000 (0.0216)	KLLoss 0.0000 (0.1353)	MaskLoss 0.0000 (0.9059)	MaskBCELoss 0.0000 (0.3089)	MaskDICELoss 0.0000 (0.5970)
Epoch: [1][213/500]	Time  9.502 ( 9.502)	Loss 4.7214 (4.6047)	CeLoss 0.2715 (0.3911)	SegCLSLoss 0.0243 (0.0230)	KLLoss 0.1816 (0.1221)	MaskLoss 0.6265 (0.6423)	MaskBCELoss 0.1259 (0.1764)	MaskDICELoss 0.5006 (0.4659)
Epoch: [1][214/500]	Time  8.756 ( 8.756)	Loss 0.2617 (4.6159)	CeLoss 0.2617 (0.3785)	SegCLSLoss 0.0000 (0.0186)	KLLoss 0.0000 (0.1294)	MaskLoss 0.0000 (0.5891)	MaskBCELoss 0.0000 (0.1023)	MaskDICELoss 0.0000 (0.4868)
Epoch: [1][215/500]	Time 11.645 (11.645)	Loss 0.1045 (5.6394)	CeLoss 0.1045 (0.3516)	SegCLSLoss 0.0000 (0.0200)	KLLoss 0.0000 (0.1252)	MaskLoss 0.0000 (0.7857)	MaskBCELoss 0.0000 (0.1888)	MaskDICELoss 0.0000 (0.5968)
Epoch: [1][216/500]	Time  9.363 ( 9.363)	Loss 3.9046 (5.0896)	CeLoss 0.2363 (0.5112)	SegCLSLoss 0.0216 (0.0282)	KLLoss 0.1738 (0.1383)	MaskLoss 0.5971 (0.6848)	MaskBCELoss 0.2156 (0.1753)	MaskDICELoss 0.3814 (0.5094)
Epoch: [1][217/500]	Time  9.204 ( 9.204)	Loss 7.3461 (5.2501)	CeLoss 0.2109 (0.5735)	SegCLSLoss 0.0356 (0.0191)	KLLoss 0.1924 (0.1273)	MaskLoss 1.0735 (0.6565)	MaskBCELoss 0.2773 (0.1187)	MaskDICELoss 0.7962 (0.5378)
Epoch: [1][218/500]	Time  9.876 ( 9.876)	Loss 1.7500 (4.9628)	CeLoss 1.7500 (0.3505)	SegCLSLoss 0.0000 (0.0199)	KLLoss 0.0000 (0.1417)	MaskLoss 0.0000 (0.7547)	MaskBCELoss 0.0000 (0.2629)	MaskDICELoss 0.0000 (0.4918)
Epoch: [1][219/500]	Time 10.970 (10.970)	Loss 7.8484 (5.6425)	CeLoss 0.2012 (0.4289)	SegCLSLoss 0.0334 (0.0235)	KLLoss 0.2246 (0.1387)	MaskLoss 1.0722 (0.7910)	MaskBCELoss 0.1951 (0.2107)	MaskDICELoss 0.8771 (0.5802)
[2025-03-04 04:07:21,349] [INFO] [logging.py:96:log_dist] [Rank 0] step=720, skipped=0, lr=[0.0002839036144578313], mom=[(0.9, 0.95)]
[2025-03-04 04:07:21,355] [INFO] [timer.py:215:stop] epoch=0/micro_step=7200/global_step=720, RunningAvgSamplesPerSec=1.030167327801686, CurrSamplesPerSec=1.260506350835641, MemAllocated=57.36GB, MaxMemAllocated=62.82GB
Epoch: [1][220/500]	Time  7.935 ( 7.935)	Loss 0.0830 (4.6225)	CeLoss 0.0830 (0.4909)	SegCLSLoss 0.0000 (0.0165)	KLLoss 0.0000 (0.0958)	MaskLoss 0.0000 (0.6671)	MaskBCELoss 0.0000 (0.2182)	MaskDICELoss 0.0000 (0.4489)
Epoch: [1][221/500]	Time 10.621 (10.621)	Loss 8.4909 (7.3215)	CeLoss 0.3535 (0.2303)	SegCLSLoss 0.0242 (0.0347)	KLLoss 0.1621 (0.1858)	MaskLoss 1.0999 (1.0603)	MaskBCELoss 0.1393 (0.2658)	MaskDICELoss 0.9606 (0.7945)
Epoch: [1][222/500]	Time  8.288 ( 8.288)	Loss 7.8773 (4.0592)	CeLoss 0.1777 (0.4460)	SegCLSLoss 0.0315 (0.0146)	KLLoss 0.1982 (0.1000)	MaskLoss 0.9618 (0.5407)	MaskBCELoss 0.0346 (0.1365)	MaskDICELoss 0.9272 (0.4041)
Epoch: [1][223/500]	Time 10.477 (10.477)	Loss 3.8013 (5.5641)	CeLoss 0.3223 (0.3554)	SegCLSLoss 0.0167 (0.0216)	KLLoss 0.1260 (0.1464)	MaskLoss 0.8071 (0.8104)	MaskBCELoss 0.5187 (0.2386)	MaskDICELoss 0.2884 (0.5718)
Epoch: [1][224/500]	Time  8.921 ( 8.921)	Loss 0.0664 (5.9911)	CeLoss 0.0664 (0.2474)	SegCLSLoss 0.0000 (0.0225)	KLLoss 0.0000 (0.1409)	MaskLoss 0.0000 (0.8904)	MaskBCELoss 0.0000 (0.2552)	MaskDICELoss 0.0000 (0.6351)
Epoch: [1][225/500]	Time 10.039 (10.039)	Loss 8.4000 (6.7855)	CeLoss 0.1963 (0.3053)	SegCLSLoss 0.0330 (0.0325)	KLLoss 0.2002 (0.1799)	MaskLoss 1.2402 (0.9988)	MaskBCELoss 0.3226 (0.2845)	MaskDICELoss 0.9176 (0.7143)
Epoch: [1][226/500]	Time 11.541 (11.541)	Loss 3.4749 (5.8173)	CeLoss 0.2188 (0.3596)	SegCLSLoss 0.0260 (0.0262)	KLLoss 0.2061 (0.1586)	MaskLoss 0.6173 (0.7862)	MaskBCELoss 0.3168 (0.1673)	MaskDICELoss 0.3005 (0.6190)
Epoch: [1][227/500]	Time 10.176 (10.176)	Loss 0.9453 (5.1475)	CeLoss 0.9453 (0.4502)	SegCLSLoss 0.0000 (0.0166)	KLLoss 0.0000 (0.1124)	MaskLoss 0.0000 (0.7042)	MaskBCELoss 0.0000 (0.1761)	MaskDICELoss 0.0000 (0.5281)
Epoch: [1][228/500]	Time 10.026 (10.026)	Loss 8.8587 (5.8939)	CeLoss 0.3066 (0.6001)	SegCLSLoss 0.0229 (0.0219)	KLLoss 0.1226 (0.1434)	MaskLoss 1.2611 (0.8386)	MaskBCELoss 0.2785 (0.2615)	MaskDICELoss 0.9825 (0.5771)
Epoch: [1][229/500]	Time  9.461 ( 9.461)	Loss 5.6750 (5.2776)	CeLoss 0.2656 (0.3563)	SegCLSLoss 0.0176 (0.0222)	KLLoss 0.1328 (0.1423)	MaskLoss 1.0353 (0.7692)	MaskBCELoss 0.5022 (0.2310)	MaskDICELoss 0.5330 (0.5383)
[2025-03-04 04:09:02,211] [INFO] [logging.py:96:log_dist] [Rank 0] step=730, skipped=0, lr=[0.00028366265060240963], mom=[(0.9, 0.95)]
[2025-03-04 04:09:02,217] [INFO] [timer.py:215:stop] epoch=0/micro_step=7300/global_step=730, RunningAvgSamplesPerSec=1.029617270247864, CurrSamplesPerSec=0.8841743462063638, MemAllocated=57.26GB, MaxMemAllocated=62.82GB
Epoch: [1][230/500]	Time 11.312 (11.312)	Loss 6.6396 (5.9419)	CeLoss 0.2559 (0.3261)	SegCLSLoss 0.0173 (0.0220)	KLLoss 0.1680 (0.1533)	MaskLoss 0.9505 (0.8156)	MaskBCELoss 0.2327 (0.1788)	MaskDICELoss 0.7178 (0.6368)
Epoch: [1][231/500]	Time  9.903 ( 9.903)	Loss 8.9068 (5.5799)	CeLoss 0.1914 (0.4912)	SegCLSLoss 0.0439 (0.0186)	KLLoss 0.2246 (0.1159)	MaskLoss 1.4634 (0.7567)	MaskBCELoss 0.5396 (0.1817)	MaskDICELoss 0.9238 (0.5750)
Epoch: [1][232/500]	Time 10.761 (10.761)	Loss 5.6148 (5.5600)	CeLoss 0.2578 (0.4141)	SegCLSLoss 0.0239 (0.0211)	KLLoss 0.1299 (0.1425)	MaskLoss 0.7519 (0.8152)	MaskBCELoss 0.1332 (0.2548)	MaskDICELoss 0.6188 (0.5605)
Epoch: [1][233/500]	Time 10.551 (10.551)	Loss 8.4723 (5.7705)	CeLoss 0.2402 (0.6481)	SegCLSLoss 0.0237 (0.0155)	KLLoss 0.1328 (0.1005)	MaskLoss 1.0772 (0.7414)	MaskBCELoss 0.0884 (0.1528)	MaskDICELoss 0.9888 (0.5885)
Epoch: [1][234/500]	Time  8.035 ( 8.035)	Loss 8.3275 (4.0381)	CeLoss 0.2148 (0.5960)	SegCLSLoss 0.0261 (0.0140)	KLLoss 0.1943 (0.0982)	MaskLoss 1.4097 (0.5599)	MaskBCELoss 0.5620 (0.1903)	MaskDICELoss 0.8477 (0.3696)
Epoch: [1][235/500]	Time  9.242 ( 9.242)	Loss 6.5372 (5.3642)	CeLoss 0.3223 (0.5520)	SegCLSLoss 0.0176 (0.0186)	KLLoss 0.1406 (0.1257)	MaskLoss 1.0490 (0.7964)	MaskBCELoss 0.3879 (0.2823)	MaskDICELoss 0.6611 (0.5141)
Epoch: [1][236/500]	Time 11.475 (11.475)	Loss 7.8071 (5.8493)	CeLoss 0.2178 (0.2303)	SegCLSLoss 0.0344 (0.0246)	KLLoss 0.2246 (0.1616)	MaskLoss 1.3478 (0.8687)	MaskBCELoss 0.5727 (0.2508)	MaskDICELoss 0.7751 (0.6179)
Epoch: [1][237/500]	Time 10.010 (10.010)	Loss 6.2559 (4.2747)	CeLoss 0.2637 (0.4618)	SegCLSLoss 0.0192 (0.0157)	KLLoss 0.1318 (0.0962)	MaskLoss 1.0690 (0.6671)	MaskBCELoss 0.4504 (0.2714)	MaskDICELoss 0.6186 (0.3957)
Epoch: [1][238/500]	Time 11.081 (11.081)	Loss 0.0684 (5.7085)	CeLoss 0.0684 (0.3422)	SegCLSLoss 0.0000 (0.0182)	KLLoss 0.0000 (0.1103)	MaskLoss 0.0000 (0.8330)	MaskBCELoss 0.0000 (0.2361)	MaskDICELoss 0.0000 (0.5968)
Epoch: [1][239/500]	Time 10.284 (10.284)	Loss 0.0737 (4.3865)	CeLoss 0.0737 (0.4259)	SegCLSLoss 0.0000 (0.0166)	KLLoss 0.0000 (0.0961)	MaskLoss 0.0000 (0.5692)	MaskBCELoss 0.0000 (0.1163)	MaskDICELoss 0.0000 (0.4529)
[2025-03-04 04:10:44,302] [INFO] [logging.py:96:log_dist] [Rank 0] step=740, skipped=0, lr=[0.0002834216867469879], mom=[(0.9, 0.95)]
[2025-03-04 04:10:44,308] [INFO] [timer.py:215:stop] epoch=0/micro_step=7400/global_step=740, RunningAvgSamplesPerSec=1.0289065131809758, CurrSamplesPerSec=0.9305553617347205, MemAllocated=57.11GB, MaxMemAllocated=62.82GB
Epoch: [1][240/500]	Time 10.748 (10.748)	Loss 4.0829 (5.6815)	CeLoss 0.2949 (0.5195)	SegCLSLoss 0.0187 (0.0204)	KLLoss 0.1436 (0.1200)	MaskLoss 0.6187 (0.8093)	MaskBCELoss 0.2193 (0.2404)	MaskDICELoss 0.3994 (0.5689)
Epoch: [1][241/500]	Time  7.301 ( 7.301)	Loss 5.5998 (3.4269)	CeLoss 0.2578 (0.6758)	SegCLSLoss 0.0160 (0.0106)	KLLoss 0.0850 (0.0759)	MaskLoss 0.7362 (0.5166)	MaskBCELoss 0.1068 (0.2439)	MaskDICELoss 0.6293 (0.2728)
Epoch: [1][242/500]	Time  8.975 ( 8.975)	Loss 0.5781 (4.4705)	CeLoss 0.5781 (0.5039)	SegCLSLoss 0.0000 (0.0185)	KLLoss 0.0000 (0.1008)	MaskLoss 0.0000 (0.6637)	MaskBCELoss 0.0000 (0.2422)	MaskDICELoss 0.0000 (0.4215)
Epoch: [1][243/500]	Time  9.906 ( 9.906)	Loss 7.8086 (6.5384)	CeLoss 0.2139 (0.3286)	SegCLSLoss 0.0228 (0.0260)	KLLoss 0.1436 (0.1396)	MaskLoss 1.3082 (0.9587)	MaskBCELoss 0.5044 (0.2687)	MaskDICELoss 0.8038 (0.6900)
Epoch: [1][244/500]	Time  7.365 ( 7.365)	Loss 6.9418 (4.4078)	CeLoss 0.2012 (0.7965)	SegCLSLoss 0.0334 (0.0188)	KLLoss 0.2031 (0.0994)	MaskLoss 0.8778 (0.5695)	MaskBCELoss 0.0838 (0.1756)	MaskDICELoss 0.7940 (0.3939)
Epoch: [1][245/500]	Time  9.604 ( 9.604)	Loss 7.4642 (6.1356)	CeLoss 0.2598 (0.4326)	SegCLSLoss 0.0305 (0.0230)	KLLoss 0.1934 (0.1203)	MaskLoss 1.1624 (0.8439)	MaskBCELoss 0.3840 (0.1967)	MaskDICELoss 0.7784 (0.6472)
Epoch: [1][246/500]	Time  9.662 ( 9.662)	Loss 7.5946 (3.9303)	CeLoss 0.3320 (0.5665)	SegCLSLoss 0.0361 (0.0132)	KLLoss 0.1924 (0.0771)	MaskLoss 1.1240 (0.5740)	MaskBCELoss 0.3234 (0.2186)	MaskDICELoss 0.8006 (0.3554)
Epoch: [1][247/500]	Time  8.281 ( 8.281)	Loss 7.5356 (4.5085)	CeLoss 0.1865 (0.6591)	SegCLSLoss 0.0283 (0.0190)	KLLoss 0.1680 (0.1007)	MaskLoss 0.9654 (0.5396)	MaskBCELoss 0.0928 (0.0962)	MaskDICELoss 0.8726 (0.4433)
Epoch: [1][248/500]	Time  9.917 ( 9.917)	Loss 7.0653 (6.1189)	CeLoss 0.2109 (0.3303)	SegCLSLoss 0.0315 (0.0280)	KLLoss 0.2109 (0.1509)	MaskLoss 0.9165 (0.8751)	MaskBCELoss 0.1173 (0.2296)	MaskDICELoss 0.7991 (0.6455)
Epoch: [1][249/500]	Time  9.273 ( 9.273)	Loss 1.2109 (4.0362)	CeLoss 1.2109 (0.6176)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.0830)	MaskLoss 0.0000 (0.4720)	MaskBCELoss 0.0000 (0.0746)	MaskDICELoss 0.0000 (0.3974)
[2025-03-04 04:12:14,959] [INFO] [logging.py:96:log_dist] [Rank 0] step=750, skipped=0, lr=[0.0002831807228915662], mom=[(0.9, 0.95)]
[2025-03-04 04:12:14,963] [INFO] [timer.py:215:stop] epoch=0/micro_step=7500/global_step=750, RunningAvgSamplesPerSec=1.0298346045139846, CurrSamplesPerSec=0.9644253106275231, MemAllocated=56.96GB, MaxMemAllocated=62.82GB
Epoch: [1][250/500]	Time 10.371 (10.371)	Loss 4.9096 (6.1201)	CeLoss 0.1934 (0.3911)	SegCLSLoss 0.0352 (0.0239)	KLLoss 0.2217 (0.1411)	MaskLoss 0.9301 (0.8607)	MaskBCELoss 0.4941 (0.2183)	MaskDICELoss 0.4360 (0.6424)
Epoch: [1][251/500]	Time 10.362 (10.362)	Loss 0.1006 (3.7420)	CeLoss 0.1006 (0.4764)	SegCLSLoss 0.0000 (0.0117)	KLLoss 0.0000 (0.0622)	MaskLoss 0.0000 (0.4909)	MaskBCELoss 0.0000 (0.1216)	MaskDICELoss 0.0000 (0.3693)
Epoch: [1][252/500]	Time  9.995 ( 9.995)	Loss 2.0002 (4.9476)	CeLoss 0.2559 (0.3763)	SegCLSLoss 0.0189 (0.0214)	KLLoss 0.1167 (0.1189)	MaskLoss 0.3313 (0.7372)	MaskBCELoss 0.1722 (0.2427)	MaskDICELoss 0.1591 (0.4945)
Epoch: [1][253/500]	Time 11.504 (11.504)	Loss 8.4436 (5.7054)	CeLoss 0.2305 (0.4825)	SegCLSLoss 0.0303 (0.0232)	KLLoss 0.1602 (0.1234)	MaskLoss 1.0738 (0.7614)	MaskBCELoss 0.0922 (0.1671)	MaskDICELoss 0.9816 (0.5943)
Epoch: [1][254/500]	Time 10.657 (10.657)	Loss 4.0416 (5.0426)	CeLoss 0.2305 (0.3946)	SegCLSLoss 0.0200 (0.0238)	KLLoss 0.1289 (0.1299)	MaskLoss 0.7248 (0.7666)	MaskBCELoss 0.3544 (0.2711)	MaskDICELoss 0.3705 (0.4955)
Epoch: [1][255/500]	Time 10.161 (10.161)	Loss 8.4081 (5.3264)	CeLoss 0.2314 (0.3221)	SegCLSLoss 0.0417 (0.0247)	KLLoss 0.2383 (0.1386)	MaskLoss 1.0337 (0.8551)	MaskBCELoss 0.0586 (0.3311)	MaskDICELoss 0.9751 (0.5239)
Epoch: [1][256/500]	Time 10.638 (10.638)	Loss 8.4388 (5.8171)	CeLoss 0.1777 (0.4702)	SegCLSLoss 0.0300 (0.0219)	KLLoss 0.1660 (0.1248)	MaskLoss 1.0660 (0.7377)	MaskBCELoss 0.0747 (0.1151)	MaskDICELoss 0.9912 (0.6226)
Epoch: [1][257/500]	Time  9.279 ( 9.279)	Loss 1.0391 (3.9246)	CeLoss 1.0391 (0.6455)	SegCLSLoss 0.0000 (0.0148)	KLLoss 0.0000 (0.0825)	MaskLoss 0.0000 (0.5104)	MaskBCELoss 0.0000 (0.1491)	MaskDICELoss 0.0000 (0.3614)
Epoch: [1][258/500]	Time 10.406 (10.406)	Loss 8.8601 (7.0591)	CeLoss 0.1738 (0.1968)	SegCLSLoss 0.0312 (0.0249)	KLLoss 0.1748 (0.1464)	MaskLoss 1.3226 (1.1171)	MaskBCELoss 0.3473 (0.3723)	MaskDICELoss 0.9753 (0.7448)
Epoch: [1][259/500]	Time 10.395 (10.395)	Loss 5.4728 (5.0402)	CeLoss 0.2656 (0.5888)	SegCLSLoss 0.0190 (0.0201)	KLLoss 0.1562 (0.1174)	MaskLoss 0.8036 (0.7608)	MaskBCELoss 0.2313 (0.2937)	MaskDICELoss 0.5723 (0.4671)
[2025-03-04 04:13:57,925] [INFO] [logging.py:96:log_dist] [Rank 0] step=760, skipped=0, lr=[0.00028293975903614455], mom=[(0.9, 0.95)]
[2025-03-04 04:13:57,931] [INFO] [timer.py:215:stop] epoch=0/micro_step=7600/global_step=760, RunningAvgSamplesPerSec=1.029017173751843, CurrSamplesPerSec=1.0451688280083549, MemAllocated=56.83GB, MaxMemAllocated=62.82GB
Epoch: [1][260/500]	Time  9.570 ( 9.570)	Loss 7.7682 (7.1967)	CeLoss 0.1357 (0.3655)	SegCLSLoss 0.0811 (0.0321)	KLLoss 0.2715 (0.1742)	MaskLoss 1.1233 (1.0703)	MaskBCELoss 0.2776 (0.3202)	MaskDICELoss 0.8457 (0.7501)
Epoch: [1][261/500]	Time 10.007 (10.007)	Loss 6.9286 (3.6939)	CeLoss 0.1494 (0.4565)	SegCLSLoss 0.0581 (0.0161)	KLLoss 0.2480 (0.0835)	MaskLoss 1.0231 (0.4980)	MaskBCELoss 0.2803 (0.1397)	MaskDICELoss 0.7428 (0.3583)
Epoch: [1][262/500]	Time  9.430 ( 9.430)	Loss 7.8644 (6.9447)	CeLoss 0.3438 (0.2937)	SegCLSLoss 0.0204 (0.0238)	KLLoss 0.1738 (0.1583)	MaskLoss 1.3526 (1.0098)	MaskBCELoss 0.5807 (0.2662)	MaskDICELoss 0.7720 (0.7436)
Epoch: [1][263/500]	Time  8.774 ( 8.774)	Loss 7.3092 (4.4504)	CeLoss 0.3711 (0.5569)	SegCLSLoss 0.0361 (0.0143)	KLLoss 0.2139 (0.0789)	MaskLoss 1.0550 (0.5691)	MaskBCELoss 0.2887 (0.1242)	MaskDICELoss 0.7663 (0.4449)
Epoch: [1][264/500]	Time  9.275 ( 9.275)	Loss 5.3898 (4.9142)	CeLoss 0.2031 (0.4271)	SegCLSLoss 0.0393 (0.0213)	KLLoss 0.2197 (0.1137)	MaskLoss 0.8006 (0.6658)	MaskBCELoss 0.2427 (0.1606)	MaskDICELoss 0.5579 (0.5052)
Epoch: [1][265/500]	Time  9.343 ( 9.343)	Loss 6.2357 (5.7290)	CeLoss 0.3438 (0.4455)	SegCLSLoss 0.0173 (0.0213)	KLLoss 0.0879 (0.1362)	MaskLoss 0.9980 (0.8351)	MaskBCELoss 0.3646 (0.2574)	MaskDICELoss 0.6334 (0.5777)
Epoch: [1][266/500]	Time  9.256 ( 9.256)	Loss 6.8639 (4.0809)	CeLoss 0.3301 (0.4234)	SegCLSLoss 0.0211 (0.0198)	KLLoss 0.1147 (0.1020)	MaskLoss 0.9821 (0.5757)	MaskBCELoss 0.2413 (0.1767)	MaskDICELoss 0.7408 (0.3990)
Epoch: [1][267/500]	Time  9.266 ( 9.266)	Loss 1.2422 (6.1410)	CeLoss 1.2422 (0.3935)	SegCLSLoss 0.0000 (0.0250)	KLLoss 0.0000 (0.1434)	MaskLoss 0.0000 (0.7752)	MaskBCELoss 0.0000 (0.1016)	MaskDICELoss 0.0000 (0.6736)
Epoch: [1][268/500]	Time  8.796 ( 8.796)	Loss 1.7188 (5.5080)	CeLoss 1.7188 (0.5210)	SegCLSLoss 0.0000 (0.0193)	KLLoss 0.0000 (0.1267)	MaskLoss 0.0000 (0.8020)	MaskBCELoss 0.0000 (0.2609)	MaskDICELoss 0.0000 (0.5411)
Epoch: [1][269/500]	Time 10.253 (10.253)	Loss 5.4197 (5.6251)	CeLoss 0.2930 (0.4527)	SegCLSLoss 0.0215 (0.0216)	KLLoss 0.1270 (0.1250)	MaskLoss 0.8584 (0.7390)	MaskBCELoss 0.3131 (0.1460)	MaskDICELoss 0.5452 (0.5931)
[2025-03-04 04:15:31,781] [INFO] [logging.py:96:log_dist] [Rank 0] step=770, skipped=0, lr=[0.00028269879518072287], mom=[(0.9, 0.95)]
[2025-03-04 04:15:31,787] [INFO] [timer.py:215:stop] epoch=0/micro_step=7700/global_step=770, RunningAvgSamplesPerSec=1.029480616717083, CurrSamplesPerSec=1.0579214666521115, MemAllocated=57.26GB, MaxMemAllocated=62.82GB
Epoch: [1][270/500]	Time  9.454 ( 9.454)	Loss 7.7568 (4.7866)	CeLoss 0.2061 (0.4386)	SegCLSLoss 0.0259 (0.0178)	KLLoss 0.1572 (0.1008)	MaskLoss 1.2835 (0.6978)	MaskBCELoss 0.4811 (0.2240)	MaskDICELoss 0.8025 (0.4738)
Epoch: [1][271/500]	Time 10.686 (10.686)	Loss 6.5393 (6.0255)	CeLoss 0.3340 (0.4947)	SegCLSLoss 0.0173 (0.0192)	KLLoss 0.1123 (0.1241)	MaskLoss 0.8679 (0.8200)	MaskBCELoss 0.1429 (0.1937)	MaskDICELoss 0.7251 (0.6263)
Epoch: [1][272/500]	Time  8.033 ( 8.033)	Loss 6.9634 (4.0899)	CeLoss 0.2871 (0.6345)	SegCLSLoss 0.0276 (0.0146)	KLLoss 0.1621 (0.0953)	MaskLoss 0.9198 (0.4887)	MaskBCELoss 0.1429 (0.0928)	MaskDICELoss 0.7768 (0.3959)
Epoch: [1][273/500]	Time 10.946 (10.946)	Loss 7.2087 (5.8770)	CeLoss 0.2275 (0.4991)	SegCLSLoss 0.0325 (0.0265)	KLLoss 0.1621 (0.1440)	MaskLoss 0.9070 (0.7725)	MaskBCELoss 0.0756 (0.1599)	MaskDICELoss 0.8314 (0.6126)
Epoch: [1][274/500]	Time 10.862 (10.862)	Loss 7.8296 (5.4028)	CeLoss 0.3457 (0.4447)	SegCLSLoss 0.0189 (0.0191)	KLLoss 0.1270 (0.1229)	MaskLoss 1.2118 (0.7436)	MaskBCELoss 0.3912 (0.1872)	MaskDICELoss 0.8206 (0.5564)
Epoch: [1][275/500]	Time 10.773 (10.773)	Loss 8.3986 (6.1269)	CeLoss 0.2539 (0.2315)	SegCLSLoss 0.0222 (0.0177)	KLLoss 0.1660 (0.1125)	MaskLoss 1.1023 (0.8528)	MaskBCELoss 0.1420 (0.1747)	MaskDICELoss 0.9604 (0.6781)
Epoch: [1][276/500]	Time  7.879 ( 7.879)	Loss 8.4281 (5.2353)	CeLoss 0.2412 (0.3852)	SegCLSLoss 0.0498 (0.0230)	KLLoss 0.2715 (0.1290)	MaskLoss 1.4374 (0.7625)	MaskBCELoss 0.6018 (0.2317)	MaskDICELoss 0.8357 (0.5307)
Epoch: [1][277/500]	Time  9.651 ( 9.651)	Loss 8.7589 (5.5730)	CeLoss 0.2324 (0.3260)	SegCLSLoss 0.0278 (0.0219)	KLLoss 0.1953 (0.1340)	MaskLoss 1.2678 (0.8171)	MaskBCELoss 0.3041 (0.2390)	MaskDICELoss 0.9637 (0.5781)
Epoch: [1][278/500]	Time  9.995 ( 9.995)	Loss 8.5542 (5.5331)	CeLoss 0.1377 (0.2195)	SegCLSLoss 0.0571 (0.0240)	KLLoss 0.2324 (0.1269)	MaskLoss 1.8146 (0.9346)	MaskBCELoss 1.0601 (0.3836)	MaskDICELoss 0.7544 (0.5509)
Epoch: [1][279/500]	Time 10.906 (10.906)	Loss 7.6738 (5.7441)	CeLoss 0.1348 (0.2593)	SegCLSLoss 0.0532 (0.0306)	KLLoss 0.2500 (0.1792)	MaskLoss 1.2187 (0.8594)	MaskBCELoss 0.4143 (0.2640)	MaskDICELoss 0.8044 (0.5953)
[2025-03-04 04:17:11,787] [INFO] [logging.py:96:log_dist] [Rank 0] step=780, skipped=0, lr=[0.0002824578313253012], mom=[(0.9, 0.95)]
[2025-03-04 04:17:11,792] [INFO] [timer.py:215:stop] epoch=0/micro_step=7800/global_step=780, RunningAvgSamplesPerSec=1.0290922126146325, CurrSamplesPerSec=0.9734959973507231, MemAllocated=56.82GB, MaxMemAllocated=62.82GB
Epoch: [1][280/500]	Time 10.274 (10.274)	Loss 7.5469 (6.4774)	CeLoss 0.1592 (0.3528)	SegCLSLoss 0.0488 (0.0238)	KLLoss 0.2402 (0.1571)	MaskLoss 1.1509 (0.9227)	MaskBCELoss 0.3473 (0.2376)	MaskDICELoss 0.8036 (0.6851)
Epoch: [1][281/500]	Time  8.841 ( 8.841)	Loss 6.5158 (4.3491)	CeLoss 0.2148 (0.5434)	SegCLSLoss 0.0276 (0.0159)	KLLoss 0.1504 (0.1080)	MaskLoss 0.8219 (0.5878)	MaskBCELoss 0.0731 (0.1688)	MaskDICELoss 0.7488 (0.4190)
Epoch: [1][282/500]	Time  9.752 ( 9.752)	Loss 1.1094 (4.7421)	CeLoss 1.1094 (0.2949)	SegCLSLoss 0.0000 (0.0202)	KLLoss 0.0000 (0.1268)	MaskLoss 0.0000 (0.6821)	MaskBCELoss 0.0000 (0.1910)	MaskDICELoss 0.0000 (0.4910)
Epoch: [1][283/500]	Time  9.349 ( 9.349)	Loss 6.4815 (5.6679)	CeLoss 0.2012 (0.3114)	SegCLSLoss 0.0247 (0.0257)	KLLoss 0.1279 (0.1361)	MaskLoss 0.7955 (0.9102)	MaskBCELoss 0.0374 (0.3456)	MaskDICELoss 0.7581 (0.5646)
Epoch: [1][284/500]	Time 10.665 (10.665)	Loss 3.6883 (6.4647)	CeLoss 0.2236 (0.2587)	SegCLSLoss 0.0146 (0.0268)	KLLoss 0.0776 (0.1570)	MaskLoss 0.5432 (0.9499)	MaskBCELoss 0.1610 (0.2606)	MaskDICELoss 0.3822 (0.6893)
Epoch: [1][285/500]	Time  8.331 ( 8.331)	Loss 0.7461 (5.3588)	CeLoss 0.7461 (0.5234)	SegCLSLoss 0.0000 (0.0252)	KLLoss 0.0000 (0.1340)	MaskLoss 0.0000 (0.7788)	MaskBCELoss 0.0000 (0.2569)	MaskDICELoss 0.0000 (0.5219)
Epoch: [1][286/500]	Time 10.487 (10.487)	Loss 8.8162 (5.6243)	CeLoss 0.2461 (0.4432)	SegCLSLoss 0.0315 (0.0198)	KLLoss 0.1797 (0.1264)	MaskLoss 1.2003 (0.7619)	MaskBCELoss 0.2046 (0.1751)	MaskDICELoss 0.9957 (0.5868)
Epoch: [1][287/500]	Time  7.842 ( 7.842)	Loss 0.8984 (4.0297)	CeLoss 0.8984 (0.6505)	SegCLSLoss 0.0000 (0.0104)	KLLoss 0.0000 (0.0664)	MaskLoss 0.0000 (0.4607)	MaskBCELoss 0.0000 (0.0629)	MaskDICELoss 0.0000 (0.3978)
Epoch: [1][288/500]	Time  9.077 ( 9.077)	Loss 8.3210 (4.5930)	CeLoss 0.1787 (0.5239)	SegCLSLoss 0.0405 (0.0165)	KLLoss 0.2314 (0.1069)	MaskLoss 1.1097 (0.5753)	MaskBCELoss 0.1644 (0.1082)	MaskDICELoss 0.9453 (0.4672)
Epoch: [1][289/500]	Time  9.848 ( 9.848)	Loss 0.0850 (4.7666)	CeLoss 0.0850 (0.4715)	SegCLSLoss 0.0000 (0.0139)	KLLoss 0.0000 (0.0865)	MaskLoss 0.0000 (0.6579)	MaskBCELoss 0.0000 (0.1770)	MaskDICELoss 0.0000 (0.4809)
[2025-03-04 04:18:46,647] [INFO] [logging.py:96:log_dist] [Rank 0] step=790, skipped=0, lr=[0.0002822168674698795], mom=[(0.9, 0.95)]
[2025-03-04 04:18:46,653] [INFO] [timer.py:215:stop] epoch=0/micro_step=7900/global_step=790, RunningAvgSamplesPerSec=1.0294052197390164, CurrSamplesPerSec=0.9373508817253515, MemAllocated=56.96GB, MaxMemAllocated=62.82GB
Epoch: [1][290/500]	Time 10.670 (10.670)	Loss 7.8447 (6.1820)	CeLoss 0.2373 (0.2456)	SegCLSLoss 0.0383 (0.0268)	KLLoss 0.2158 (0.1568)	MaskLoss 1.0071 (0.8733)	MaskBCELoss 0.1142 (0.2034)	MaskDICELoss 0.8930 (0.6699)
Epoch: [1][291/500]	Time  9.842 ( 9.842)	Loss 6.9000 (5.1750)	CeLoss 0.2852 (0.2878)	SegCLSLoss 0.0173 (0.0230)	KLLoss 0.1079 (0.1300)	MaskLoss 0.9191 (0.7539)	MaskBCELoss 0.1422 (0.2142)	MaskDICELoss 0.7769 (0.5397)
Epoch: [1][292/500]	Time 11.225 (11.225)	Loss 3.9806 (6.6557)	CeLoss 0.3730 (0.2372)	SegCLSLoss 0.0164 (0.0260)	KLLoss 0.1196 (0.1729)	MaskLoss 0.5425 (0.9328)	MaskBCELoss 0.1432 (0.2050)	MaskDICELoss 0.3993 (0.7278)
Epoch: [1][293/500]	Time  7.797 ( 7.797)	Loss 1.1484 (3.5996)	CeLoss 1.1484 (0.6487)	SegCLSLoss 0.0000 (0.0120)	KLLoss 0.0000 (0.0747)	MaskLoss 0.0000 (0.4281)	MaskBCELoss 0.0000 (0.0925)	MaskDICELoss 0.0000 (0.3357)
Epoch: [1][294/500]	Time  9.583 ( 9.583)	Loss 6.8416 (5.4804)	CeLoss 0.2891 (0.4567)	SegCLSLoss 0.0179 (0.0192)	KLLoss 0.1270 (0.1236)	MaskLoss 0.9741 (0.7816)	MaskBCELoss 0.2295 (0.2271)	MaskDICELoss 0.7446 (0.5545)
Epoch: [1][295/500]	Time  6.840 ( 6.840)	Loss 5.7505 (3.4724)	CeLoss 0.2324 (0.7740)	SegCLSLoss 0.0165 (0.0131)	KLLoss 0.0967 (0.0713)	MaskLoss 0.7391 (0.3890)	MaskBCELoss 0.0830 (0.0819)	MaskDICELoss 0.6561 (0.3071)
Epoch: [1][296/500]	Time  8.467 ( 8.467)	Loss 4.4650 (3.8075)	CeLoss 0.2539 (0.6927)	SegCLSLoss 0.0250 (0.0111)	KLLoss 0.1748 (0.0633)	MaskLoss 0.8284 (0.4849)	MaskBCELoss 0.4340 (0.1389)	MaskDICELoss 0.3945 (0.3460)
Epoch: [1][297/500]	Time  8.587 ( 8.587)	Loss 8.2551 (4.7832)	CeLoss 0.2480 (0.6377)	SegCLSLoss 0.0330 (0.0177)	KLLoss 0.1885 (0.0984)	MaskLoss 1.3378 (0.6985)	MaskBCELoss 0.4834 (0.2583)	MaskDICELoss 0.8544 (0.4402)
Epoch: [1][298/500]	Time  8.757 ( 8.757)	Loss 6.6863 (4.6369)	CeLoss 0.2402 (0.5688)	SegCLSLoss 0.0198 (0.0143)	KLLoss 0.1406 (0.0817)	MaskLoss 0.8140 (0.5993)	MaskBCELoss 0.0360 (0.1359)	MaskDICELoss 0.7780 (0.4634)
Epoch: [1][299/500]	Time  8.967 ( 8.967)	Loss 5.1902 (5.2847)	CeLoss 0.2812 (0.6224)	SegCLSLoss 0.0175 (0.0187)	KLLoss 0.1348 (0.1044)	MaskLoss 0.7693 (0.7507)	MaskBCELoss 0.2313 (0.2428)	MaskDICELoss 0.5380 (0.5079)
[2025-03-04 04:20:16,240] [INFO] [logging.py:96:log_dist] [Rank 0] step=800, skipped=0, lr=[0.00028197590361445785], mom=[(0.9, 0.95)]
[2025-03-04 04:20:16,246] [INFO] [timer.py:215:stop] epoch=0/micro_step=8000/global_step=800, RunningAvgSamplesPerSec=1.030410902311393, CurrSamplesPerSec=1.0498038768594717, MemAllocated=57.74GB, MaxMemAllocated=62.82GB
Epoch: [1][300/500]	Time  9.528 ( 9.528)	Loss 0.7539 (3.4138)	CeLoss 0.7539 (0.4540)	SegCLSLoss 0.0000 (0.0159)	KLLoss 0.0000 (0.0945)	MaskLoss 0.0000 (0.4659)	MaskBCELoss 0.0000 (0.1449)	MaskDICELoss 0.0000 (0.3209)
Epoch: [1][301/500]	Time 10.695 (10.695)	Loss 0.9453 (5.1040)	CeLoss 0.9453 (0.3557)	SegCLSLoss 0.0000 (0.0205)	KLLoss 0.0000 (0.1195)	MaskLoss 0.0000 (0.6874)	MaskBCELoss 0.0000 (0.1468)	MaskDICELoss 0.0000 (0.5406)
Epoch: [1][302/500]	Time  8.004 ( 8.004)	Loss 5.4202 (4.4398)	CeLoss 0.2734 (0.5847)	SegCLSLoss 0.0415 (0.0139)	KLLoss 0.1562 (0.0786)	MaskLoss 0.9184 (0.6694)	MaskBCELoss 0.3964 (0.2643)	MaskDICELoss 0.5220 (0.4051)
Epoch: [1][303/500]	Time 11.336 (11.336)	Loss 4.5930 (5.7191)	CeLoss 0.2314 (0.2611)	SegCLSLoss 0.0171 (0.0230)	KLLoss 0.1084 (0.1459)	MaskLoss 0.8166 (0.8473)	MaskBCELoss 0.3812 (0.2462)	MaskDICELoss 0.4354 (0.6010)
Epoch: [1][304/500]	Time  9.943 ( 9.943)	Loss 6.1446 (6.8194)	CeLoss 0.2520 (0.3544)	SegCLSLoss 0.0168 (0.0214)	KLLoss 0.0889 (0.1386)	MaskLoss 0.8542 (1.0757)	MaskBCELoss 0.1728 (0.3816)	MaskDICELoss 0.6814 (0.6941)
Epoch: [1][305/500]	Time  9.712 ( 9.712)	Loss 0.1147 (4.3844)	CeLoss 0.1147 (0.3293)	SegCLSLoss 0.0000 (0.0255)	KLLoss 0.0000 (0.1332)	MaskLoss 0.0000 (0.6843)	MaskBCELoss 0.0000 (0.2609)	MaskDICELoss 0.0000 (0.4234)
Epoch: [1][306/500]	Time 10.619 (10.619)	Loss 9.2259 (6.6020)	CeLoss 0.2217 (0.3376)	SegCLSLoss 0.0244 (0.0243)	KLLoss 0.1562 (0.1479)	MaskLoss 1.4274 (1.0053)	MaskBCELoss 0.4306 (0.3230)	MaskDICELoss 0.9968 (0.6823)
Epoch: [1][307/500]	Time 11.267 (11.267)	Loss 4.4483 (6.3358)	CeLoss 0.2520 (0.3500)	SegCLSLoss 0.0173 (0.0219)	KLLoss 0.1001 (0.1349)	MaskLoss 0.7722 (0.8957)	MaskBCELoss 0.3481 (0.2210)	MaskDICELoss 0.4241 (0.6748)
Epoch: [1][308/500]	Time 10.891 (10.891)	Loss 1.4609 (4.8670)	CeLoss 1.4609 (0.3300)	SegCLSLoss 0.0000 (0.0234)	KLLoss 0.0000 (0.1272)	MaskLoss 0.0000 (0.7494)	MaskBCELoss 0.0000 (0.2661)	MaskDICELoss 0.0000 (0.4833)
Epoch: [1][309/500]	Time  7.677 ( 7.677)	Loss 0.9062 (4.6110)	CeLoss 0.9062 (0.5057)	SegCLSLoss 0.0000 (0.0175)	KLLoss 0.0000 (0.1094)	MaskLoss 0.0000 (0.5727)	MaskBCELoss 0.0000 (0.0991)	MaskDICELoss 0.0000 (0.4736)
[2025-03-04 04:21:55,910] [INFO] [logging.py:96:log_dist] [Rank 0] step=810, skipped=0, lr=[0.0002817349397590361], mom=[(0.9, 0.95)]
[2025-03-04 04:21:55,916] [INFO] [timer.py:215:stop] epoch=0/micro_step=8100/global_step=810, RunningAvgSamplesPerSec=1.0300689823281601, CurrSamplesPerSec=1.0499400295894412, MemAllocated=57.26GB, MaxMemAllocated=62.82GB
Epoch: [1][310/500]	Time  9.526 ( 9.526)	Loss 8.5294 (5.2569)	CeLoss 0.2246 (0.5108)	SegCLSLoss 0.0264 (0.0199)	KLLoss 0.1797 (0.1158)	MaskLoss 1.1421 (0.6717)	MaskBCELoss 0.1709 (0.1255)	MaskDICELoss 0.9712 (0.5462)
Epoch: [1][311/500]	Time 11.020 (11.020)	Loss 2.2043 (4.8461)	CeLoss 0.2578 (0.2786)	SegCLSLoss 0.0156 (0.0173)	KLLoss 0.0825 (0.1072)	MaskLoss 0.3242 (0.6649)	MaskBCELoss 0.1228 (0.1447)	MaskDICELoss 0.2014 (0.5203)
Epoch: [1][312/500]	Time  9.869 ( 9.869)	Loss 6.9815 (5.2725)	CeLoss 0.2217 (0.3736)	SegCLSLoss 0.0322 (0.0184)	KLLoss 0.1582 (0.1069)	MaskLoss 0.8421 (0.7804)	MaskBCELoss 0.0254 (0.2434)	MaskDICELoss 0.8168 (0.5370)
Epoch: [1][313/500]	Time  9.000 ( 9.000)	Loss 8.7751 (4.2819)	CeLoss 0.2168 (0.5576)	SegCLSLoss 0.0311 (0.0156)	KLLoss 0.2002 (0.0983)	MaskLoss 1.1734 (0.5552)	MaskBCELoss 0.1742 (0.1372)	MaskDICELoss 0.9991 (0.4180)
Epoch: [1][314/500]	Time 11.659 (11.659)	Loss 7.2763 (6.3967)	CeLoss 0.2061 (0.2504)	SegCLSLoss 0.0258 (0.0239)	KLLoss 0.1641 (0.1491)	MaskLoss 1.1955 (0.9980)	MaskBCELoss 0.4451 (0.3333)	MaskDICELoss 0.7504 (0.6647)
Epoch: [1][315/500]	Time 10.660 (10.660)	Loss 1.8203 (5.6288)	CeLoss 1.8203 (0.5702)	SegCLSLoss 0.0000 (0.0194)	KLLoss 0.0000 (0.1242)	MaskLoss 0.0000 (0.9163)	MaskBCELoss 0.0000 (0.4011)	MaskDICELoss 0.0000 (0.5153)
Epoch: [1][316/500]	Time  8.659 ( 8.659)	Loss 1.0312 (4.7428)	CeLoss 1.0312 (0.5394)	SegCLSLoss 0.0000 (0.0179)	KLLoss 0.0000 (0.1026)	MaskLoss 0.0000 (0.5963)	MaskBCELoss 0.0000 (0.1130)	MaskDICELoss 0.0000 (0.4833)
Epoch: [1][317/500]	Time 10.005 (10.005)	Loss 7.6471 (6.7162)	CeLoss 0.1904 (0.2181)	SegCLSLoss 0.0332 (0.0359)	KLLoss 0.1943 (0.1963)	MaskLoss 1.2995 (1.0348)	MaskBCELoss 0.5252 (0.3325)	MaskDICELoss 0.7743 (0.7024)
Epoch: [1][318/500]	Time 10.202 (10.202)	Loss 3.8156 (5.9697)	CeLoss 0.2178 (0.3007)	SegCLSLoss 0.0166 (0.0228)	KLLoss 0.0962 (0.1420)	MaskLoss 0.5999 (0.8993)	MaskBCELoss 0.2176 (0.2798)	MaskDICELoss 0.3823 (0.6195)
Epoch: [1][319/500]	Time  8.819 ( 8.819)	Loss 1.1406 (4.9959)	CeLoss 1.1406 (0.4245)	SegCLSLoss 0.0000 (0.0228)	KLLoss 0.0000 (0.1399)	MaskLoss 0.0000 (0.7457)	MaskBCELoss 0.0000 (0.2576)	MaskDICELoss 0.0000 (0.4881)
[2025-03-04 04:23:37,369] [INFO] [logging.py:96:log_dist] [Rank 0] step=820, skipped=0, lr=[0.00028149397590361444], mom=[(0.9, 0.95)]
[2025-03-04 04:23:37,374] [INFO] [timer.py:215:stop] epoch=0/micro_step=8200/global_step=820, RunningAvgSamplesPerSec=1.0295038880592837, CurrSamplesPerSec=0.8648857305641124, MemAllocated=57.26GB, MaxMemAllocated=62.82GB
Epoch: [1][320/500]	Time 11.564 (11.564)	Loss 5.7117 (4.4888)	CeLoss 0.3535 (0.4371)	SegCLSLoss 0.0203 (0.0155)	KLLoss 0.1309 (0.0987)	MaskLoss 0.9408 (0.6266)	MaskBCELoss 0.3848 (0.1780)	MaskDICELoss 0.5560 (0.4486)
Epoch: [1][321/500]	Time  6.972 ( 6.972)	Loss 1.5469 (3.0331)	CeLoss 1.5469 (0.8413)	SegCLSLoss 0.0000 (0.0088)	KLLoss 0.0000 (0.0557)	MaskLoss 0.0000 (0.2982)	MaskBCELoss 0.0000 (0.0423)	MaskDICELoss 0.0000 (0.2559)
Epoch: [1][322/500]	Time  9.978 ( 9.978)	Loss 6.7680 (4.9365)	CeLoss 0.4004 (0.4966)	SegCLSLoss 0.0178 (0.0194)	KLLoss 0.1309 (0.1173)	MaskLoss 0.9789 (0.6802)	MaskBCELoss 0.2670 (0.1880)	MaskDICELoss 0.7119 (0.4922)
Epoch: [1][323/500]	Time  8.957 ( 8.957)	Loss 6.5483 (4.2578)	CeLoss 0.2432 (0.4958)	SegCLSLoss 0.0172 (0.0172)	KLLoss 0.0981 (0.1035)	MaskLoss 1.0787 (0.5753)	MaskBCELoss 0.4051 (0.1587)	MaskDICELoss 0.6736 (0.4166)
Epoch: [1][324/500]	Time  9.323 ( 9.323)	Loss 5.6784 (5.5674)	CeLoss 0.1992 (0.5578)	SegCLSLoss 0.0164 (0.0167)	KLLoss 0.1045 (0.1065)	MaskLoss 0.8622 (0.7087)	MaskBCELoss 0.2553 (0.1292)	MaskDICELoss 0.6069 (0.5796)
Epoch: [1][325/500]	Time  9.610 ( 9.610)	Loss 6.4967 (4.3330)	CeLoss 0.1895 (0.3085)	SegCLSLoss 0.0302 (0.0175)	KLLoss 0.1650 (0.1039)	MaskLoss 0.8646 (0.6699)	MaskBCELoss 0.1315 (0.2413)	MaskDICELoss 0.7331 (0.4286)
Epoch: [1][326/500]	Time  9.382 ( 9.382)	Loss 5.5010 (4.2509)	CeLoss 0.3457 (0.3344)	SegCLSLoss 0.0170 (0.0125)	KLLoss 0.0820 (0.0934)	MaskLoss 0.7941 (0.6492)	MaskBCELoss 0.2146 (0.2294)	MaskDICELoss 0.5795 (0.4198)
Epoch: [1][327/500]	Time  8.191 ( 8.191)	Loss 0.4414 (2.4515)	CeLoss 0.4414 (0.8145)	SegCLSLoss 0.0000 (0.0069)	KLLoss 0.0000 (0.0454)	MaskLoss 0.0000 (0.2292)	MaskBCELoss 0.0000 (0.0409)	MaskDICELoss 0.0000 (0.1883)
Epoch: [1][328/500]	Time  8.136 ( 8.136)	Loss 4.6083 (4.2159)	CeLoss 0.2910 (0.8336)	SegCLSLoss 0.0177 (0.0128)	KLLoss 0.1079 (0.0842)	MaskLoss 0.6131 (0.5324)	MaskBCELoss 0.1175 (0.1613)	MaskDICELoss 0.4956 (0.3711)
Epoch: [1][329/500]	Time  8.615 ( 8.615)	Loss 1.1094 (6.5421)	CeLoss 1.1094 (0.3464)	SegCLSLoss 0.0000 (0.0227)	KLLoss 0.0000 (0.1480)	MaskLoss 0.0000 (1.0013)	MaskBCELoss 0.0000 (0.3291)	MaskDICELoss 0.0000 (0.6723)
[2025-03-04 04:25:05,180] [INFO] [logging.py:96:log_dist] [Rank 0] step=830, skipped=0, lr=[0.00028125301204819276], mom=[(0.9, 0.95)]
[2025-03-04 04:25:05,186] [INFO] [timer.py:215:stop] epoch=0/micro_step=8300/global_step=830, RunningAvgSamplesPerSec=1.0307006195898578, CurrSamplesPerSec=1.1564755441210455, MemAllocated=56.74GB, MaxMemAllocated=62.82GB
Epoch: [1][330/500]	Time  8.649 ( 8.649)	Loss 1.2500 (5.6215)	CeLoss 1.2500 (0.5307)	SegCLSLoss 0.0000 (0.0198)	KLLoss 0.0000 (0.1331)	MaskLoss 0.0000 (0.8013)	MaskBCELoss 0.0000 (0.2437)	MaskDICELoss 0.0000 (0.5576)
Epoch: [1][331/500]	Time  7.323 ( 7.323)	Loss 1.0156 (3.7187)	CeLoss 1.0156 (0.7090)	SegCLSLoss 0.0000 (0.0143)	KLLoss 0.0000 (0.0885)	MaskLoss 0.0000 (0.4912)	MaskBCELoss 0.0000 (0.1692)	MaskDICELoss 0.0000 (0.3220)
Epoch: [1][332/500]	Time  8.706 ( 8.706)	Loss 9.8106 (5.5640)	CeLoss 0.2031 (0.4163)	SegCLSLoss 0.0203 (0.0167)	KLLoss 0.1299 (0.1113)	MaskLoss 2.0857 (0.8522)	MaskBCELoss 1.2031 (0.2983)	MaskDICELoss 0.8826 (0.5539)
Epoch: [1][333/500]	Time  9.469 ( 9.469)	Loss 3.8193 (6.1622)	CeLoss 0.2139 (0.3504)	SegCLSLoss 0.0203 (0.0214)	KLLoss 0.1582 (0.1462)	MaskLoss 0.5649 (0.9140)	MaskBCELoss 0.1805 (0.2761)	MaskDICELoss 0.3844 (0.6378)
Epoch: [1][334/500]	Time 12.189 (12.189)	Loss 8.4262 (6.0148)	CeLoss 0.1729 (0.2168)	SegCLSLoss 0.0356 (0.0243)	KLLoss 0.2373 (0.1667)	MaskLoss 1.0036 (0.7838)	MaskBCELoss 0.0050 (0.1086)	MaskDICELoss 0.9986 (0.6752)
Epoch: [1][335/500]	Time  9.226 ( 9.226)	Loss 0.0879 (4.7634)	CeLoss 0.0879 (0.3436)	SegCLSLoss 0.0000 (0.0206)	KLLoss 0.0000 (0.1169)	MaskLoss 0.0000 (0.6680)	MaskBCELoss 0.0000 (0.1752)	MaskDICELoss 0.0000 (0.4928)
Epoch: [1][336/500]	Time  7.180 ( 7.180)	Loss 2.7074 (4.5278)	CeLoss 0.2656 (0.4921)	SegCLSLoss 0.0153 (0.0211)	KLLoss 0.1045 (0.1120)	MaskLoss 0.3980 (0.6431)	MaskBCELoss 0.1425 (0.2053)	MaskDICELoss 0.2554 (0.4378)
Epoch: [1][337/500]	Time  8.621 ( 8.621)	Loss 0.8125 (5.2714)	CeLoss 0.8125 (0.3644)	SegCLSLoss 0.0000 (0.0220)	KLLoss 0.0000 (0.1183)	MaskLoss 0.0000 (0.7326)	MaskBCELoss 0.0000 (0.1806)	MaskDICELoss 0.0000 (0.5521)
Epoch: [1][338/500]	Time 11.207 (11.207)	Loss 7.9742 (6.9102)	CeLoss 0.2148 (0.2273)	SegCLSLoss 0.0221 (0.0252)	KLLoss 0.1660 (0.1591)	MaskLoss 0.9855 (1.0067)	MaskBCELoss 0.0500 (0.2571)	MaskDICELoss 0.9354 (0.7496)
Epoch: [1][339/500]	Time 10.662 (10.662)	Loss 6.9557 (4.6296)	CeLoss 0.1943 (0.2925)	SegCLSLoss 0.0422 (0.0171)	KLLoss 0.2363 (0.1199)	MaskLoss 1.1193 (0.6822)	MaskBCELoss 0.4086 (0.2082)	MaskDICELoss 0.7107 (0.4740)
[2025-03-04 04:26:40,355] [INFO] [logging.py:96:log_dist] [Rank 0] step=840, skipped=0, lr=[0.00028101204819277103], mom=[(0.9, 0.95)]
[2025-03-04 04:26:40,360] [INFO] [timer.py:215:stop] epoch=0/micro_step=8400/global_step=840, RunningAvgSamplesPerSec=1.0309369239211092, CurrSamplesPerSec=0.9444012894894201, MemAllocated=57.27GB, MaxMemAllocated=62.82GB
Epoch: [1][340/500]	Time 10.591 (10.591)	Loss 7.9345 (5.6491)	CeLoss 0.2334 (0.3576)	SegCLSLoss 0.0325 (0.0210)	KLLoss 0.2031 (0.1583)	MaskLoss 0.9835 (0.8028)	MaskBCELoss 0.0644 (0.2168)	MaskDICELoss 0.9191 (0.5860)
Epoch: [1][341/500]	Time  9.941 ( 9.941)	Loss 0.0659 (5.1350)	CeLoss 0.0659 (0.4420)	SegCLSLoss 0.0000 (0.0176)	KLLoss 0.0000 (0.1364)	MaskLoss 0.0000 (0.7112)	MaskBCELoss 0.0000 (0.1903)	MaskDICELoss 0.0000 (0.5209)
Epoch: [1][342/500]	Time 10.868 (10.868)	Loss 6.1125 (6.7114)	CeLoss 0.2695 (0.3743)	SegCLSLoss 0.0291 (0.0239)	KLLoss 0.1641 (0.1670)	MaskLoss 0.7715 (0.9059)	MaskBCELoss 0.0845 (0.1816)	MaskDICELoss 0.6870 (0.7243)
Epoch: [1][343/500]	Time  8.119 ( 8.119)	Loss 1.7500 (5.6173)	CeLoss 1.7500 (0.6026)	SegCLSLoss 0.0000 (0.0254)	KLLoss 0.0000 (0.1406)	MaskLoss 0.0000 (0.9890)	MaskBCELoss 0.0000 (0.5085)	MaskDICELoss 0.0000 (0.4805)
Epoch: [1][344/500]	Time  9.536 ( 9.536)	Loss 8.1876 (5.9474)	CeLoss 0.2402 (0.4674)	SegCLSLoss 0.0270 (0.0217)	KLLoss 0.1973 (0.1415)	MaskLoss 1.0079 (0.8204)	MaskBCELoss 0.0544 (0.2059)	MaskDICELoss 0.9534 (0.6145)
Epoch: [1][345/500]	Time 10.749 (10.749)	Loss 1.2812 (4.6989)	CeLoss 1.2812 (0.3770)	SegCLSLoss 0.0000 (0.0131)	KLLoss 0.0000 (0.0889)	MaskLoss 0.0000 (0.6119)	MaskBCELoss 0.0000 (0.1114)	MaskDICELoss 0.0000 (0.5005)
Epoch: [1][346/500]	Time 10.022 (10.022)	Loss 6.0349 (5.6582)	CeLoss 0.3203 (0.3745)	SegCLSLoss 0.0144 (0.0196)	KLLoss 0.0889 (0.1298)	MaskLoss 0.9553 (0.7558)	MaskBCELoss 0.3375 (0.1503)	MaskDICELoss 0.6177 (0.6055)
Epoch: [1][347/500]	Time 11.090 (11.090)	Loss 5.1618 (6.8021)	CeLoss 0.2637 (0.4105)	SegCLSLoss 0.0172 (0.0213)	KLLoss 0.1235 (0.1384)	MaskLoss 0.7755 (0.8569)	MaskBCELoss 0.2394 (0.1021)	MaskDICELoss 0.5361 (0.7548)
Epoch: [1][348/500]	Time  8.556 ( 8.556)	Loss 1.5234 (2.9945)	CeLoss 1.5234 (0.6666)	SegCLSLoss 0.0000 (0.0100)	KLLoss 0.0000 (0.0665)	MaskLoss 0.0000 (0.3759)	MaskBCELoss 0.0000 (0.1251)	MaskDICELoss 0.0000 (0.2508)
Epoch: [1][349/500]	Time 10.363 (10.363)	Loss 6.0771 (5.8616)	CeLoss 0.2012 (0.3462)	SegCLSLoss 0.0217 (0.0183)	KLLoss 0.1426 (0.1218)	MaskLoss 0.7401 (0.7512)	MaskBCELoss 0.0332 (0.1042)	MaskDICELoss 0.7069 (0.6470)
[2025-03-04 04:28:17,856] [INFO] [logging.py:96:log_dist] [Rank 0] step=850, skipped=0, lr=[0.00028077108433734936], mom=[(0.9, 0.95)]
[2025-03-04 04:28:17,861] [INFO] [timer.py:215:stop] epoch=0/micro_step=8500/global_step=850, RunningAvgSamplesPerSec=1.030876145034495, CurrSamplesPerSec=1.2113006447759052, MemAllocated=56.96GB, MaxMemAllocated=62.82GB
Epoch: [1][350/500]	Time  8.258 ( 8.258)	Loss 6.1701 (4.9248)	CeLoss 0.2676 (0.5611)	SegCLSLoss 0.0388 (0.0232)	KLLoss 0.1943 (0.1208)	MaskLoss 1.3471 (0.7150)	MaskBCELoss 0.8479 (0.2481)	MaskDICELoss 0.4992 (0.4669)
Epoch: [1][351/500]	Time  9.764 ( 9.764)	Loss 6.5374 (4.8968)	CeLoss 0.2441 (0.2522)	SegCLSLoss 0.0167 (0.0191)	KLLoss 0.0884 (0.1121)	MaskLoss 1.0164 (0.7099)	MaskBCELoss 0.3222 (0.1927)	MaskDICELoss 0.6941 (0.5172)
Epoch: [1][352/500]	Time 11.226 (11.226)	Loss 7.6452 (6.1533)	CeLoss 0.2441 (0.2830)	SegCLSLoss 0.0277 (0.0197)	KLLoss 0.1543 (0.1172)	MaskLoss 1.0048 (0.9133)	MaskBCELoss 0.1342 (0.2605)	MaskDICELoss 0.8706 (0.6528)
Epoch: [1][353/500]	Time  9.315 ( 9.315)	Loss 6.8872 (4.3300)	CeLoss 0.2461 (0.4729)	SegCLSLoss 0.0184 (0.0135)	KLLoss 0.1069 (0.0869)	MaskLoss 0.9219 (0.5730)	MaskBCELoss 0.1419 (0.1367)	MaskDICELoss 0.7800 (0.4362)
Epoch: [1][354/500]	Time  8.556 ( 8.556)	Loss 6.1538 (3.3187)	CeLoss 0.2275 (0.6653)	SegCLSLoss 0.0267 (0.0110)	KLLoss 0.1299 (0.0615)	MaskLoss 0.7629 (0.4301)	MaskBCELoss 0.0534 (0.1424)	MaskDICELoss 0.7095 (0.2877)
Epoch: [1][355/500]	Time  9.781 ( 9.781)	Loss 8.0900 (5.6029)	CeLoss 0.1543 (0.4487)	SegCLSLoss 0.0552 (0.0255)	KLLoss 0.2461 (0.1385)	MaskLoss 1.1225 (0.7089)	MaskBCELoss 0.2196 (0.1114)	MaskDICELoss 0.9029 (0.5975)
Epoch: [1][356/500]	Time  8.872 ( 8.872)	Loss 7.5285 (5.8226)	CeLoss 0.2217 (0.3970)	SegCLSLoss 0.0283 (0.0243)	KLLoss 0.1592 (0.1326)	MaskLoss 1.4090 (0.8134)	MaskBCELoss 0.6897 (0.2043)	MaskDICELoss 0.7193 (0.6090)
Epoch: [1][357/500]	Time 10.047 (10.047)	Loss 6.2053 (5.5960)	CeLoss 0.2598 (0.4164)	SegCLSLoss 0.0177 (0.0232)	KLLoss 0.0854 (0.1252)	MaskLoss 0.8850 (0.7857)	MaskBCELoss 0.2051 (0.2071)	MaskDICELoss 0.6800 (0.5786)
Epoch: [1][358/500]	Time  8.751 ( 8.751)	Loss 5.3028 (5.6393)	CeLoss 0.3086 (0.5849)	SegCLSLoss 0.0184 (0.0235)	KLLoss 0.1299 (0.1422)	MaskLoss 0.8436 (0.8122)	MaskBCELoss 0.3159 (0.2662)	MaskDICELoss 0.5277 (0.5460)
Epoch: [1][359/500]	Time  8.619 ( 8.619)	Loss 5.1004 (5.8653)	CeLoss 0.3633 (0.4770)	SegCLSLoss 0.0278 (0.0189)	KLLoss 0.1475 (0.1165)	MaskLoss 0.8528 (0.8159)	MaskBCELoss 0.3742 (0.2108)	MaskDICELoss 0.4786 (0.6051)
[2025-03-04 04:29:52,904] [INFO] [logging.py:96:log_dist] [Rank 0] step=860, skipped=0, lr=[0.0002805301204819277], mom=[(0.9, 0.95)]
[2025-03-04 04:29:52,909] [INFO] [timer.py:215:stop] epoch=0/micro_step=8600/global_step=860, RunningAvgSamplesPerSec=1.0311205444790412, CurrSamplesPerSec=0.9885881963817985, MemAllocated=57.26GB, MaxMemAllocated=62.82GB
Epoch: [1][360/500]	Time 10.117 (10.117)	Loss 4.2153 (5.2775)	CeLoss 0.3457 (0.4012)	SegCLSLoss 0.0176 (0.0189)	KLLoss 0.0781 (0.1144)	MaskLoss 0.5514 (0.7185)	MaskBCELoss 0.1049 (0.1660)	MaskDICELoss 0.4465 (0.5525)
Epoch: [1][361/500]	Time  9.924 ( 9.924)	Loss 6.6369 (5.1833)	CeLoss 0.2246 (0.3805)	SegCLSLoss 0.0369 (0.0188)	KLLoss 0.2139 (0.1120)	MaskLoss 0.8686 (0.7269)	MaskBCELoss 0.1282 (0.1890)	MaskDICELoss 0.7404 (0.5379)
Epoch: [1][362/500]	Time  7.737 ( 7.737)	Loss 1.3906 (3.3740)	CeLoss 1.3906 (0.8925)	SegCLSLoss 0.0000 (0.0101)	KLLoss 0.0000 (0.0712)	MaskLoss 0.0000 (0.4391)	MaskBCELoss 0.0000 (0.1845)	MaskDICELoss 0.0000 (0.2545)
Epoch: [1][363/500]	Time  9.582 ( 9.582)	Loss 8.2909 (5.5812)	CeLoss 0.2656 (0.6107)	SegCLSLoss 0.0276 (0.0176)	KLLoss 0.1562 (0.1092)	MaskLoss 1.2487 (0.7385)	MaskBCELoss 0.3557 (0.1759)	MaskDICELoss 0.8930 (0.5626)
Epoch: [1][364/500]	Time  9.284 ( 9.284)	Loss 1.4375 (4.8689)	CeLoss 1.4375 (0.3917)	SegCLSLoss 0.0000 (0.0237)	KLLoss 0.0000 (0.1219)	MaskLoss 0.0000 (0.7044)	MaskBCELoss 0.0000 (0.2153)	MaskDICELoss 0.0000 (0.4891)
Epoch: [1][365/500]	Time  9.671 ( 9.671)	Loss 1.0703 (4.7166)	CeLoss 1.0703 (0.4615)	SegCLSLoss 0.0000 (0.0178)	KLLoss 0.0000 (0.0994)	MaskLoss 0.0000 (0.6438)	MaskBCELoss 0.0000 (0.1672)	MaskDICELoss 0.0000 (0.4765)
Epoch: [1][366/500]	Time  9.219 ( 9.219)	Loss 1.8021 (4.4466)	CeLoss 0.2969 (0.6026)	SegCLSLoss 0.0197 (0.0186)	KLLoss 0.1118 (0.1025)	MaskLoss 0.2380 (0.5636)	MaskBCELoss 0.0867 (0.1294)	MaskDICELoss 0.1513 (0.4342)
Epoch: [1][367/500]	Time  9.252 ( 9.252)	Loss 8.3391 (4.6660)	CeLoss 0.2949 (0.6467)	SegCLSLoss 0.0359 (0.0157)	KLLoss 0.1787 (0.1004)	MaskLoss 1.4599 (0.6664)	MaskBCELoss 0.6387 (0.2368)	MaskDICELoss 0.8212 (0.4296)
Epoch: [1][368/500]	Time 10.295 (10.295)	Loss 6.6476 (6.0369)	CeLoss 0.2246 (0.2338)	SegCLSLoss 0.0337 (0.0255)	KLLoss 0.2080 (0.1362)	MaskLoss 1.1618 (0.8632)	MaskBCELoss 0.5160 (0.2086)	MaskDICELoss 0.6458 (0.6546)
Epoch: [1][369/500]	Time 10.800 (10.800)	Loss 7.4737 (5.8137)	CeLoss 0.2197 (0.3053)	SegCLSLoss 0.0354 (0.0262)	KLLoss 0.2217 (0.1472)	MaskLoss 0.9284 (0.8302)	MaskBCELoss 0.0687 (0.2155)	MaskDICELoss 0.8597 (0.6147)
[2025-03-04 04:31:26,964] [INFO] [logging.py:96:log_dist] [Rank 0] step=870, skipped=0, lr=[0.000280289156626506], mom=[(0.9, 0.95)]
[2025-03-04 04:31:26,969] [INFO] [timer.py:215:stop] epoch=0/micro_step=8700/global_step=870, RunningAvgSamplesPerSec=1.031480533786807, CurrSamplesPerSec=1.2058524743268206, MemAllocated=56.66GB, MaxMemAllocated=62.82GB
Epoch: [1][370/500]	Time  8.295 ( 8.295)	Loss 8.7744 (5.6854)	CeLoss 0.1738 (0.3298)	SegCLSLoss 0.0236 (0.0287)	KLLoss 0.1182 (0.1423)	MaskLoss 1.2383 (0.8658)	MaskBCELoss 0.2394 (0.2879)	MaskDICELoss 0.9989 (0.5779)
Epoch: [1][371/500]	Time  9.167 ( 9.167)	Loss 7.2309 (4.4820)	CeLoss 0.2578 (0.4049)	SegCLSLoss 0.0233 (0.0222)	KLLoss 0.2148 (0.1124)	MaskLoss 0.9561 (0.6427)	MaskBCELoss 0.1503 (0.1980)	MaskDICELoss 0.8057 (0.4447)
Epoch: [1][372/500]	Time 10.748 (10.748)	Loss 8.2443 (6.6961)	CeLoss 0.3633 (0.2936)	SegCLSLoss 0.0244 (0.0280)	KLLoss 0.1475 (0.1539)	MaskLoss 1.2496 (1.0127)	MaskBCELoss 0.3794 (0.3112)	MaskDICELoss 0.8703 (0.7015)
Epoch: [1][373/500]	Time  7.045 ( 7.045)	Loss 0.7266 (3.6272)	CeLoss 0.7266 (0.8592)	SegCLSLoss 0.0000 (0.0103)	KLLoss 0.0000 (0.0627)	MaskLoss 0.0000 (0.4535)	MaskBCELoss 0.0000 (0.1546)	MaskDICELoss 0.0000 (0.2988)
Epoch: [1][374/500]	Time 11.107 (11.107)	Loss 6.7386 (5.2307)	CeLoss 0.2354 (0.3007)	SegCLSLoss 0.0216 (0.0233)	KLLoss 0.1387 (0.1303)	MaskLoss 1.1046 (0.7830)	MaskBCELoss 0.4138 (0.2460)	MaskDICELoss 0.6908 (0.5370)
Epoch: [1][375/500]	Time  8.613 ( 8.613)	Loss 1.3438 (4.0128)	CeLoss 1.3438 (0.6664)	SegCLSLoss 0.0000 (0.0158)	KLLoss 0.0000 (0.0950)	MaskLoss 0.0000 (0.4548)	MaskBCELoss 0.0000 (0.0659)	MaskDICELoss 0.0000 (0.3890)
Epoch: [1][376/500]	Time 11.887 (11.887)	Loss 0.1079 (4.5156)	CeLoss 0.1079 (0.2654)	SegCLSLoss 0.0000 (0.0164)	KLLoss 0.0000 (0.0892)	MaskLoss 0.0000 (0.5728)	MaskBCELoss 0.0000 (0.0717)	MaskDICELoss 0.0000 (0.5011)
Epoch: [1][377/500]	Time  9.905 ( 9.905)	Loss 6.0325 (5.5902)	CeLoss 0.1748 (0.4653)	SegCLSLoss 0.0315 (0.0221)	KLLoss 0.1885 (0.1365)	MaskLoss 0.8454 (0.8231)	MaskBCELoss 0.1849 (0.2680)	MaskDICELoss 0.6605 (0.5552)
Epoch: [1][378/500]	Time 10.503 (10.503)	Loss 8.4949 (6.7398)	CeLoss 0.3164 (0.4353)	SegCLSLoss 0.0254 (0.0244)	KLLoss 0.1445 (0.1466)	MaskLoss 1.0364 (0.9976)	MaskBCELoss 0.0448 (0.3058)	MaskDICELoss 0.9916 (0.6918)
Epoch: [1][379/500]	Time 10.405 (10.405)	Loss 8.3930 (5.6668)	CeLoss 0.2520 (0.4377)	SegCLSLoss 0.0266 (0.0184)	KLLoss 0.2061 (0.1256)	MaskLoss 1.0466 (0.7172)	MaskBCELoss 0.0754 (0.1073)	MaskDICELoss 0.9712 (0.6100)
[2025-03-04 04:33:07,607] [INFO] [logging.py:96:log_dist] [Rank 0] step=880, skipped=0, lr=[0.00028004819277108433], mom=[(0.9, 0.95)]
[2025-03-04 04:33:07,613] [INFO] [timer.py:215:stop] epoch=0/micro_step=8800/global_step=880, RunningAvgSamplesPerSec=1.0310347866955634, CurrSamplesPerSec=0.8879796989611327, MemAllocated=56.72GB, MaxMemAllocated=62.82GB
Epoch: [1][380/500]	Time 11.264 (11.264)	Loss 1.2031 (4.9051)	CeLoss 1.2031 (0.4423)	SegCLSLoss 0.0000 (0.0218)	KLLoss 0.0000 (0.1163)	MaskLoss 0.0000 (0.6999)	MaskBCELoss 0.0000 (0.2106)	MaskDICELoss 0.0000 (0.4893)
Epoch: [1][381/500]	Time 10.385 (10.385)	Loss 5.3824 (5.1590)	CeLoss 0.3145 (0.4491)	SegCLSLoss 0.0182 (0.0236)	KLLoss 0.1079 (0.1256)	MaskLoss 0.7366 (0.6771)	MaskBCELoss 0.1570 (0.1407)	MaskDICELoss 0.5796 (0.5364)
Epoch: [1][382/500]	Time 10.392 (10.392)	Loss 1.0469 (5.5528)	CeLoss 1.0469 (0.3285)	SegCLSLoss 0.0000 (0.0216)	KLLoss 0.0000 (0.1426)	MaskLoss 0.0000 (0.7708)	MaskBCELoss 0.0000 (0.1826)	MaskDICELoss 0.0000 (0.5882)
Epoch: [1][383/500]	Time  9.822 ( 9.822)	Loss 5.3675 (5.9186)	CeLoss 0.3359 (0.2489)	SegCLSLoss 0.0175 (0.0220)	KLLoss 0.1201 (0.1234)	MaskLoss 0.6541 (0.8941)	MaskBCELoss 0.0550 (0.2696)	MaskDICELoss 0.5991 (0.6245)
Epoch: [1][384/500]	Time 10.580 (10.580)	Loss 7.3930 (5.4559)	CeLoss 0.1895 (0.3188)	SegCLSLoss 0.0425 (0.0279)	KLLoss 0.2451 (0.1450)	MaskLoss 1.0746 (0.8865)	MaskBCELoss 0.2768 (0.3523)	MaskDICELoss 0.7978 (0.5341)
Epoch: [1][385/500]	Time  7.993 ( 7.993)	Loss 1.8828 (4.9647)	CeLoss 1.8828 (0.5253)	SegCLSLoss 0.0000 (0.0171)	KLLoss 0.0000 (0.1218)	MaskLoss 0.0000 (0.7216)	MaskBCELoss 0.0000 (0.2439)	MaskDICELoss 0.0000 (0.4777)
Epoch: [1][386/500]	Time 10.486 (10.486)	Loss 1.5781 (4.9082)	CeLoss 1.5781 (0.6506)	SegCLSLoss 0.0000 (0.0164)	KLLoss 0.0000 (0.1007)	MaskLoss 0.0000 (0.6448)	MaskBCELoss 0.0000 (0.1682)	MaskDICELoss 0.0000 (0.4766)
Epoch: [1][387/500]	Time 11.062 (11.062)	Loss 4.6077 (5.5528)	CeLoss 0.2852 (0.2476)	SegCLSLoss 0.0183 (0.0200)	KLLoss 0.1543 (0.1350)	MaskLoss 0.7515 (0.7548)	MaskBCELoss 0.3089 (0.1464)	MaskDICELoss 0.4426 (0.6085)
Epoch: [1][388/500]	Time  8.584 ( 8.584)	Loss 4.9576 (5.5255)	CeLoss 0.2793 (0.5776)	SegCLSLoss 0.0153 (0.0176)	KLLoss 0.1108 (0.1240)	MaskLoss 0.7722 (0.7051)	MaskBCELoss 0.2697 (0.1376)	MaskDICELoss 0.5025 (0.5675)
Epoch: [1][389/500]	Time  9.674 ( 9.674)	Loss 8.1458 (5.5202)	CeLoss 0.3008 (0.3553)	SegCLSLoss 0.0265 (0.0189)	KLLoss 0.2080 (0.1379)	MaskLoss 1.2692 (0.7282)	MaskBCELoss 0.4219 (0.1347)	MaskDICELoss 0.8473 (0.5935)
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
[2025-03-04 04:34:46,923] [INFO] [logging.py:96:log_dist] [Rank 0] step=890, skipped=0, lr=[0.00027980722891566266], mom=[(0.9, 0.95)]
[2025-03-04 04:34:46,929] [INFO] [timer.py:215:stop] epoch=0/micro_step=8900/global_step=890, RunningAvgSamplesPerSec=1.0307585009207114, CurrSamplesPerSec=0.9676095754568879, MemAllocated=57.26GB, MaxMemAllocated=62.82GB
Epoch: [1][390/500]	Time 10.338 (10.338)	Loss 8.4332 (5.6234)	CeLoss 0.1631 (0.4094)	SegCLSLoss 0.0337 (0.0231)	KLLoss 0.2373 (0.1413)	MaskLoss 1.0187 (0.7590)	MaskBCELoss 0.0225 (0.1685)	MaskDICELoss 0.9963 (0.5905)
Epoch: [1][391/500]	Time 10.370 (10.370)	Loss 7.9869 (6.7632)	CeLoss 0.2002 (0.2087)	SegCLSLoss 0.0186 (0.0280)	KLLoss 0.1123 (0.1800)	MaskLoss 0.9964 (0.9384)	MaskBCELoss 0.0507 (0.1911)	MaskDICELoss 0.9456 (0.7473)
Epoch: [1][392/500]	Time 10.659 (10.659)	Loss 3.6180 (4.0724)	CeLoss 0.2871 (0.2976)	SegCLSLoss 0.0152 (0.0138)	KLLoss 0.1348 (0.0959)	MaskLoss 0.5142 (0.5332)	MaskBCELoss 0.1543 (0.0989)	MaskDICELoss 0.3600 (0.4343)
Epoch: [1][393/500]	Time  6.893 ( 6.893)	Loss 7.8728 (3.7950)	CeLoss 0.3066 (0.6479)	SegCLSLoss 0.0198 (0.0107)	KLLoss 0.1494 (0.0720)	MaskLoss 1.7813 (0.5222)	MaskBCELoss 1.1405 (0.1846)	MaskDICELoss 0.6409 (0.3376)
Epoch: [1][394/500]	Time 10.093 (10.093)	Loss 7.5460 (6.1488)	CeLoss 0.2793 (0.4813)	SegCLSLoss 0.0190 (0.0243)	KLLoss 0.1143 (0.1459)	MaskLoss 1.1558 (0.8572)	MaskBCELoss 0.3505 (0.2247)	MaskDICELoss 0.8053 (0.6325)
Epoch: [1][395/500]	Time 10.664 (10.664)	Loss 2.7918 (4.7793)	CeLoss 0.2891 (0.3493)	SegCLSLoss 0.0272 (0.0188)	KLLoss 0.1689 (0.1143)	MaskLoss 0.4682 (0.6739)	MaskBCELoss 0.2377 (0.1808)	MaskDICELoss 0.2305 (0.4930)
Epoch: [1][396/500]	Time  9.094 ( 9.094)	Loss 7.0784 (5.5461)	CeLoss 0.1992 (0.5783)	SegCLSLoss 0.0330 (0.0199)	KLLoss 0.2080 (0.1284)	MaskLoss 0.8822 (0.7396)	MaskBCELoss 0.0671 (0.1812)	MaskDICELoss 0.8150 (0.5583)
Epoch: [1][397/500]	Time 10.970 (10.970)	Loss 6.2287 (6.7366)	CeLoss 0.3867 (0.3463)	SegCLSLoss 0.0172 (0.0228)	KLLoss 0.1455 (0.1471)	MaskLoss 0.8265 (0.9559)	MaskBCELoss 0.1537 (0.2359)	MaskDICELoss 0.6728 (0.7200)
Epoch: [1][398/500]	Time  9.509 ( 9.509)	Loss 4.1327 (5.1878)	CeLoss 0.1855 (0.3969)	SegCLSLoss 0.0244 (0.0201)	KLLoss 0.1514 (0.1400)	MaskLoss 0.5655 (0.7913)	MaskBCELoss 0.1232 (0.2815)	MaskDICELoss 0.4423 (0.5097)
Epoch: [1][399/500]	Time  8.121 ( 8.121)	Loss 1.6641 (5.1413)	CeLoss 1.6641 (0.5870)	SegCLSLoss 0.0000 (0.0203)	KLLoss 0.0000 (0.1205)	MaskLoss 0.0000 (0.6477)	MaskBCELoss 0.0000 (0.1263)	MaskDICELoss 0.0000 (0.5214)
[2025-03-04 04:36:22,299] [INFO] [logging.py:96:log_dist] [Rank 0] step=900, skipped=0, lr=[0.0002795662650602409], mom=[(0.9, 0.95)]
[2025-03-04 04:36:22,306] [INFO] [timer.py:215:stop] epoch=0/micro_step=9000/global_step=900, RunningAvgSamplesPerSec=1.0309544814752583, CurrSamplesPerSec=1.111033527470207, MemAllocated=56.74GB, MaxMemAllocated=62.82GB
Epoch: [1][400/500]	Time  9.003 ( 9.003)	Loss 0.9609 (3.8693)	CeLoss 0.9609 (0.6149)	SegCLSLoss 0.0000 (0.0135)	KLLoss 0.0000 (0.0875)	MaskLoss 0.0000 (0.4869)	MaskBCELoss 0.0000 (0.1225)	MaskDICELoss 0.0000 (0.3644)
Epoch: [1][401/500]	Time 11.410 (11.410)	Loss 2.2437 (5.5376)	CeLoss 0.2793 (0.2724)	SegCLSLoss 0.0161 (0.0222)	KLLoss 0.1309 (0.1366)	MaskLoss 0.3124 (0.7918)	MaskBCELoss 0.1123 (0.2028)	MaskDICELoss 0.2002 (0.5890)
Epoch: [1][402/500]	Time  9.478 ( 9.478)	Loss 6.9561 (6.5895)	CeLoss 0.2109 (0.2272)	SegCLSLoss 0.0435 (0.0278)	KLLoss 0.2324 (0.1771)	MaskLoss 0.8969 (1.0563)	MaskBCELoss 0.1140 (0.3798)	MaskDICELoss 0.7829 (0.6764)
Epoch: [1][403/500]	Time 10.406 (10.406)	Loss 6.7931 (5.4297)	CeLoss 0.2061 (0.3045)	SegCLSLoss 0.0275 (0.0229)	KLLoss 0.1777 (0.1338)	MaskLoss 0.9108 (0.7579)	MaskBCELoss 0.1486 (0.1805)	MaskDICELoss 0.7622 (0.5774)
Epoch: [1][404/500]	Time  9.513 ( 9.513)	Loss 7.8517 (4.8967)	CeLoss 0.1953 (0.5667)	SegCLSLoss 0.0366 (0.0166)	KLLoss 0.2422 (0.1129)	MaskLoss 1.1224 (0.6753)	MaskBCELoss 0.2641 (0.1990)	MaskDICELoss 0.8583 (0.4763)
Epoch: [1][405/500]	Time 10.052 (10.052)	Loss 8.1264 (4.9093)	CeLoss 0.1162 (0.2593)	SegCLSLoss 0.0786 (0.0253)	KLLoss 0.2598 (0.1359)	MaskLoss 1.1821 (0.6695)	MaskBCELoss 0.2907 (0.1424)	MaskDICELoss 0.8914 (0.5271)
Epoch: [1][406/500]	Time  9.368 ( 9.368)	Loss 0.4883 (4.9987)	CeLoss 0.4883 (0.3079)	SegCLSLoss 0.0000 (0.0236)	KLLoss 0.0000 (0.1396)	MaskLoss 0.0000 (0.7033)	MaskBCELoss 0.0000 (0.1811)	MaskDICELoss 0.0000 (0.5222)
Epoch: [1][407/500]	Time  7.973 ( 7.973)	Loss 5.1395 (4.3411)	CeLoss 0.4941 (0.6537)	SegCLSLoss 0.0150 (0.0203)	KLLoss 0.1045 (0.1115)	MaskLoss 0.6256 (0.5142)	MaskBCELoss 0.0784 (0.0912)	MaskDICELoss 0.5471 (0.4230)
Epoch: [1][408/500]	Time 10.957 (10.957)	Loss 5.9897 (6.6241)	CeLoss 0.2363 (0.4214)	SegCLSLoss 0.0273 (0.0214)	KLLoss 0.1621 (0.1427)	MaskLoss 0.7169 (0.8423)	MaskBCELoss 0.0263 (0.1148)	MaskDICELoss 0.6906 (0.7275)
Epoch: [1][409/500]	Time  9.362 ( 9.362)	Loss 7.6714 (5.2254)	CeLoss 0.2197 (0.4119)	SegCLSLoss 0.0253 (0.0237)	KLLoss 0.1719 (0.1448)	MaskLoss 0.9624 (0.6642)	MaskBCELoss 0.0720 (0.1095)	MaskDICELoss 0.8904 (0.5547)
[2025-03-04 04:38:00,168] [INFO] [logging.py:96:log_dist] [Rank 0] step=910, skipped=0, lr=[0.00027932530120481925], mom=[(0.9, 0.95)]
[2025-03-04 04:38:00,174] [INFO] [timer.py:215:stop] epoch=0/micro_step=9100/global_step=910, RunningAvgSamplesPerSec=1.0308542306778063, CurrSamplesPerSec=1.0698848156315728, MemAllocated=57.26GB, MaxMemAllocated=62.82GB
Epoch: [1][410/500]	Time  9.349 ( 9.349)	Loss 7.8595 (5.6010)	CeLoss 0.1846 (0.6156)	SegCLSLoss 0.0325 (0.0230)	KLLoss 0.2109 (0.1320)	MaskLoss 1.0210 (0.7990)	MaskBCELoss 0.1202 (0.2583)	MaskDICELoss 0.9009 (0.5407)
Epoch: [1][411/500]	Time 11.163 (11.163)	Loss 5.9265 (6.2884)	CeLoss 0.2412 (0.3768)	SegCLSLoss 0.0168 (0.0203)	KLLoss 0.1201 (0.1472)	MaskLoss 0.8731 (0.8859)	MaskBCELoss 0.2382 (0.2222)	MaskDICELoss 0.6349 (0.6637)
Epoch: [1][412/500]	Time 10.350 (10.350)	Loss 7.9026 (6.2727)	CeLoss 0.2393 (0.3354)	SegCLSLoss 0.0334 (0.0219)	KLLoss 0.1895 (0.1532)	MaskLoss 1.0699 (0.8590)	MaskBCELoss 0.1836 (0.1832)	MaskDICELoss 0.8863 (0.6758)
Epoch: [1][413/500]	Time 10.535 (10.535)	Loss 6.8373 (4.8448)	CeLoss 0.2412 (0.3739)	SegCLSLoss 0.0221 (0.0202)	KLLoss 0.1592 (0.1198)	MaskLoss 0.8808 (0.6512)	MaskBCELoss 0.1033 (0.1448)	MaskDICELoss 0.7776 (0.5064)
Epoch: [1][414/500]	Time  9.262 ( 9.262)	Loss 9.1504 (6.0025)	CeLoss 0.1748 (0.4382)	SegCLSLoss 0.0645 (0.0248)	KLLoss 0.2480 (0.1433)	MaskLoss 1.3552 (0.7585)	MaskBCELoss 0.3577 (0.1099)	MaskDICELoss 0.9975 (0.6486)
Epoch: [1][415/500]	Time 11.047 (11.047)	Loss 7.6333 (6.7627)	CeLoss 0.4062 (0.2571)	SegCLSLoss 0.0247 (0.0266)	KLLoss 0.1387 (0.1768)	MaskLoss 1.1359 (1.0497)	MaskBCELoss 0.3348 (0.3469)	MaskDICELoss 0.8011 (0.7028)
Epoch: [1][416/500]	Time  8.553 ( 8.553)	Loss 7.8170 (5.9500)	CeLoss 0.2061 (0.3613)	SegCLSLoss 0.0265 (0.0249)	KLLoss 0.2061 (0.1511)	MaskLoss 0.9597 (0.8019)	MaskBCELoss 0.0477 (0.1649)	MaskDICELoss 0.9120 (0.6369)
Epoch: [1][417/500]	Time 10.030 (10.030)	Loss 6.7955 (5.5330)	CeLoss 0.2334 (0.3229)	SegCLSLoss 0.0243 (0.0209)	KLLoss 0.1807 (0.1348)	MaskLoss 0.8120 (0.7583)	MaskBCELoss 0.0210 (0.1669)	MaskDICELoss 0.7910 (0.5914)
Epoch: [1][418/500]	Time  8.111 ( 8.111)	Loss 6.0872 (4.4663)	CeLoss 0.2256 (0.7603)	SegCLSLoss 0.0227 (0.0163)	KLLoss 0.1504 (0.1046)	MaskLoss 0.7365 (0.5596)	MaskBCELoss 0.0319 (0.1472)	MaskDICELoss 0.7046 (0.4124)
Epoch: [1][419/500]	Time  7.844 ( 7.844)	Loss 1.1953 (4.2727)	CeLoss 1.1953 (0.4529)	SegCLSLoss 0.0000 (0.0217)	KLLoss 0.0000 (0.1225)	MaskLoss 0.0000 (0.6759)	MaskBCELoss 0.0000 (0.2867)	MaskDICELoss 0.0000 (0.3892)
[2025-03-04 04:39:36,773] [INFO] [logging.py:96:log_dist] [Rank 0] step=920, skipped=0, lr=[0.0002790843373493976], mom=[(0.9, 0.95)]
[2025-03-04 04:39:36,779] [INFO] [timer.py:215:stop] epoch=0/micro_step=9200/global_step=920, RunningAvgSamplesPerSec=1.0309026917141308, CurrSamplesPerSec=1.0301591242248456, MemAllocated=56.72GB, MaxMemAllocated=62.82GB
Epoch: [1][420/500]	Time  9.709 ( 9.709)	Loss 1.3594 (5.8623)	CeLoss 1.3594 (0.4329)	SegCLSLoss 0.0000 (0.0185)	KLLoss 0.0000 (0.1178)	MaskLoss 0.0000 (0.8275)	MaskBCELoss 0.0000 (0.2195)	MaskDICELoss 0.0000 (0.6080)
Epoch: [1][421/500]	Time 10.687 (10.687)	Loss 8.4777 (6.4390)	CeLoss 0.1455 (0.3570)	SegCLSLoss 0.0483 (0.0256)	KLLoss 0.2520 (0.1558)	MaskLoss 1.3167 (0.9458)	MaskBCELoss 0.4130 (0.2755)	MaskDICELoss 0.9037 (0.6703)
Epoch: [1][422/500]	Time 10.785 (10.785)	Loss 7.7973 (6.9332)	CeLoss 0.2451 (0.3199)	SegCLSLoss 0.0420 (0.0265)	KLLoss 0.2148 (0.1698)	MaskLoss 1.1922 (1.0515)	MaskBCELoss 0.3701 (0.3304)	MaskDICELoss 0.8221 (0.7211)
Epoch: [1][423/500]	Time 10.707 (10.707)	Loss 4.1929 (5.3034)	CeLoss 0.3066 (0.3854)	SegCLSLoss 0.0181 (0.0177)	KLLoss 0.1416 (0.1260)	MaskLoss 0.5211 (0.7139)	MaskBCELoss 0.0722 (0.1548)	MaskDICELoss 0.4489 (0.5592)
Epoch: [1][424/500]	Time  9.902 ( 9.902)	Loss 8.0375 (6.6173)	CeLoss 0.2148 (0.4217)	SegCLSLoss 0.0237 (0.0331)	KLLoss 0.1660 (0.1582)	MaskLoss 1.4977 (1.0248)	MaskBCELoss 0.7227 (0.3630)	MaskDICELoss 0.7749 (0.6619)
Epoch: [1][425/500]	Time 10.764 (10.764)	Loss 7.9235 (6.6717)	CeLoss 0.1523 (0.3227)	SegCLSLoss 0.0664 (0.0271)	KLLoss 0.2539 (0.1640)	MaskLoss 1.3177 (0.9505)	MaskBCELoss 0.5096 (0.2388)	MaskDICELoss 0.8081 (0.7117)
Epoch: [1][426/500]	Time  9.169 ( 9.169)	Loss 7.3761 (5.3597)	CeLoss 0.2324 (0.4229)	SegCLSLoss 0.0258 (0.0203)	KLLoss 0.1709 (0.1246)	MaskLoss 1.2027 (0.7814)	MaskBCELoss 0.4436 (0.2415)	MaskDICELoss 0.7591 (0.5399)
Epoch: [1][427/500]	Time  9.493 ( 9.493)	Loss 3.5215 (3.7066)	CeLoss 0.2383 (0.5641)	SegCLSLoss 0.0172 (0.0115)	KLLoss 0.1118 (0.0781)	MaskLoss 0.4658 (0.4439)	MaskBCELoss 0.0940 (0.0821)	MaskDICELoss 0.3718 (0.3618)
Epoch: [1][428/500]	Time 10.534 (10.534)	Loss 6.4754 (6.2788)	CeLoss 0.2432 (0.3472)	SegCLSLoss 0.0212 (0.0212)	KLLoss 0.1357 (0.1395)	MaskLoss 0.8604 (0.9225)	MaskBCELoss 0.1330 (0.2664)	MaskDICELoss 0.7273 (0.6561)
Epoch: [1][429/500]	Time  9.770 ( 9.770)	Loss 6.7737 (5.0070)	CeLoss 0.2871 (0.5498)	SegCLSLoss 0.0166 (0.0186)	KLLoss 0.1387 (0.1254)	MaskLoss 1.0228 (0.7082)	MaskBCELoss 0.3070 (0.2237)	MaskDICELoss 0.7158 (0.4844)
[2025-03-04 04:41:17,894] [INFO] [logging.py:96:log_dist] [Rank 0] step=930, skipped=0, lr=[0.0002788433734939759], mom=[(0.9, 0.95)]
[2025-03-04 04:41:17,900] [INFO] [timer.py:215:stop] epoch=0/micro_step=9300/global_step=930, RunningAvgSamplesPerSec=1.0304334568728162, CurrSamplesPerSec=1.0742368323323388, MemAllocated=57.25GB, MaxMemAllocated=62.82GB
Epoch: [1][430/500]	Time  9.311 ( 9.311)	Loss 4.8964 (5.5300)	CeLoss 0.2773 (0.5740)	SegCLSLoss 0.0195 (0.0196)	KLLoss 0.1367 (0.1228)	MaskLoss 0.6532 (0.6956)	MaskBCELoss 0.1254 (0.1236)	MaskDICELoss 0.5277 (0.5720)
Epoch: [1][431/500]	Time  9.286 ( 9.286)	Loss 1.1778 (4.7520)	CeLoss 0.2295 (0.3434)	SegCLSLoss 0.0170 (0.0248)	KLLoss 0.0952 (0.1462)	MaskLoss 0.1246 (0.6921)	MaskBCELoss 0.0255 (0.2146)	MaskDICELoss 0.0991 (0.4775)
Epoch: [1][432/500]	Time  9.109 ( 9.109)	Loss 0.8672 (3.8509)	CeLoss 0.8672 (0.5909)	SegCLSLoss 0.0000 (0.0131)	KLLoss 0.0000 (0.0844)	MaskLoss 0.0000 (0.5011)	MaskBCELoss 0.0000 (0.1399)	MaskDICELoss 0.0000 (0.3612)
Epoch: [1][433/500]	Time 10.572 (10.572)	Loss 0.6836 (6.7404)	CeLoss 0.6836 (0.3072)	SegCLSLoss 0.0000 (0.0210)	KLLoss 0.0000 (0.1456)	MaskLoss 0.0000 (0.8951)	MaskBCELoss 0.0000 (0.1472)	MaskDICELoss 0.0000 (0.7479)
Epoch: [1][434/500]	Time  9.467 ( 9.467)	Loss 1.2578 (5.1159)	CeLoss 1.2578 (0.5303)	SegCLSLoss 0.0000 (0.0183)	KLLoss 0.0000 (0.1235)	MaskLoss 0.0000 (0.6718)	MaskBCELoss 0.0000 (0.1536)	MaskDICELoss 0.0000 (0.5182)
Epoch: [1][435/500]	Time 12.286 (12.286)	Loss 5.3937 (6.0925)	CeLoss 0.2275 (0.2382)	SegCLSLoss 0.0247 (0.0226)	KLLoss 0.1445 (0.1437)	MaskLoss 0.6949 (0.8092)	MaskBCELoss 0.0917 (0.1291)	MaskDICELoss 0.6032 (0.6801)
Epoch: [1][436/500]	Time 11.838 (11.838)	Loss 8.5177 (6.7186)	CeLoss 0.2236 (0.2515)	SegCLSLoss 0.0308 (0.0276)	KLLoss 0.1982 (0.1760)	MaskLoss 1.0469 (0.9067)	MaskBCELoss 0.0491 (0.1627)	MaskDICELoss 0.9977 (0.7440)
Epoch: [1][437/500]	Time  8.979 ( 8.979)	Loss 7.1808 (5.0100)	CeLoss 0.2520 (0.5594)	SegCLSLoss 0.0255 (0.0178)	KLLoss 0.1406 (0.1131)	MaskLoss 0.8766 (0.6646)	MaskBCELoss 0.0398 (0.1647)	MaskDICELoss 0.8369 (0.5000)
Epoch: [1][438/500]	Time  9.380 ( 9.380)	Loss 0.9805 (5.3410)	CeLoss 0.9805 (0.3531)	SegCLSLoss 0.0000 (0.0249)	KLLoss 0.0000 (0.1200)	MaskLoss 0.0000 (0.7557)	MaskBCELoss 0.0000 (0.1982)	MaskDICELoss 0.0000 (0.5574)
Epoch: [1][439/500]	Time 11.626 (11.626)	Loss 5.7405 (6.6078)	CeLoss 0.3105 (0.2314)	SegCLSLoss 0.0200 (0.0225)	KLLoss 0.1387 (0.1513)	MaskLoss 0.7454 (0.8873)	MaskBCELoss 0.1136 (0.1474)	MaskDICELoss 0.6318 (0.7399)
[2025-03-04 04:42:57,425] [INFO] [logging.py:96:log_dist] [Rank 0] step=940, skipped=0, lr=[0.00027860240963855417], mom=[(0.9, 0.95)]
[2025-03-04 04:42:57,431] [INFO] [timer.py:215:stop] epoch=0/micro_step=9400/global_step=940, RunningAvgSamplesPerSec=1.030154086417241, CurrSamplesPerSec=1.4314136693851545, MemAllocated=56.73GB, MaxMemAllocated=62.82GB
Epoch: [1][440/500]	Time  6.988 ( 6.988)	Loss 1.1484 (3.8221)	CeLoss 1.1484 (0.8736)	SegCLSLoss 0.0000 (0.0102)	KLLoss 0.0000 (0.0641)	MaskLoss 0.0000 (0.4572)	MaskBCELoss 0.0000 (0.1296)	MaskDICELoss 0.0000 (0.3275)
Epoch: [1][441/500]	Time 11.353 (11.353)	Loss 1.0625 (6.5087)	CeLoss 1.0625 (0.3264)	SegCLSLoss 0.0000 (0.0217)	KLLoss 0.0000 (0.1452)	MaskLoss 0.0000 (0.8781)	MaskBCELoss 0.0000 (0.1664)	MaskDICELoss 0.0000 (0.7116)
Epoch: [1][442/500]	Time  9.243 ( 9.243)	Loss 8.7377 (4.7604)	CeLoss 0.1133 (0.4840)	SegCLSLoss 0.0762 (0.0236)	KLLoss 0.2656 (0.1204)	MaskLoss 1.3006 (0.6587)	MaskBCELoss 0.3475 (0.1877)	MaskDICELoss 0.9531 (0.4711)
Epoch: [1][443/500]	Time  8.140 ( 8.140)	Loss 6.3698 (4.1532)	CeLoss 0.2275 (0.5947)	SegCLSLoss 0.0201 (0.0153)	KLLoss 0.1504 (0.1031)	MaskLoss 1.1306 (0.5839)	MaskBCELoss 0.5103 (0.2039)	MaskDICELoss 0.6203 (0.3801)
Epoch: [1][444/500]	Time  8.802 ( 8.802)	Loss 7.2643 (4.2814)	CeLoss 0.2520 (0.6031)	SegCLSLoss 0.0347 (0.0153)	KLLoss 0.2080 (0.0982)	MaskLoss 0.9110 (0.4973)	MaskBCELoss 0.0834 (0.0676)	MaskDICELoss 0.8276 (0.4297)
Epoch: [1][445/500]	Time 11.047 (11.047)	Loss 4.1653 (5.4301)	CeLoss 0.2910 (0.4418)	SegCLSLoss 0.0160 (0.0237)	KLLoss 0.0986 (0.1356)	MaskLoss 0.6254 (0.6962)	MaskBCELoss 0.2060 (0.1214)	MaskDICELoss 0.4194 (0.5747)
Epoch: [1][446/500]	Time  5.863 ( 5.863)	Loss 1.8984 (2.9108)	CeLoss 1.8984 (1.0226)	SegCLSLoss 0.0000 (0.0082)	KLLoss 0.0000 (0.0524)	MaskLoss 0.0000 (0.2972)	MaskBCELoss 0.0000 (0.0911)	MaskDICELoss 0.0000 (0.2062)
Epoch: [1][447/500]	Time  8.034 ( 8.034)	Loss 6.6587 (4.5658)	CeLoss 0.2178 (0.5261)	SegCLSLoss 0.0317 (0.0157)	KLLoss 0.2080 (0.1023)	MaskLoss 0.8274 (0.6218)	MaskBCELoss 0.0670 (0.1742)	MaskDICELoss 0.7604 (0.4477)
Epoch: [1][448/500]	Time  9.779 ( 9.779)	Loss 5.3635 (4.8936)	CeLoss 0.3359 (0.4699)	SegCLSLoss 0.0161 (0.0203)	KLLoss 0.1201 (0.1264)	MaskLoss 0.9199 (0.7065)	MaskBCELoss 0.4101 (0.2275)	MaskDICELoss 0.5098 (0.4790)
Epoch: [1][449/500]	Time  8.885 ( 8.885)	Loss 6.0016 (5.9655)	CeLoss 0.2354 (0.3666)	SegCLSLoss 0.0236 (0.0216)	KLLoss 0.1289 (0.1343)	MaskLoss 0.8969 (0.7720)	MaskBCELoss 0.2581 (0.1204)	MaskDICELoss 0.6388 (0.6516)
[2025-03-04 04:44:30,602] [INFO] [logging.py:96:log_dist] [Rank 0] step=950, skipped=0, lr=[0.0002783614457831325], mom=[(0.9, 0.95)]
[2025-03-04 04:44:30,608] [INFO] [timer.py:215:stop] epoch=0/micro_step=9500/global_step=950, RunningAvgSamplesPerSec=1.0305921517859755, CurrSamplesPerSec=0.8313332107896138, MemAllocated=57.26GB, MaxMemAllocated=62.82GB
Epoch: [1][450/500]	Time 12.031 (12.031)	Loss 6.7181 (6.4485)	CeLoss 0.2617 (0.2629)	SegCLSLoss 0.0173 (0.0254)	KLLoss 0.1367 (0.1541)	MaskLoss 1.2708 (0.9323)	MaskBCELoss 0.6425 (0.2399)	MaskDICELoss 0.6284 (0.6924)
Epoch: [1][451/500]	Time  9.158 ( 9.158)	Loss 5.7018 (4.3570)	CeLoss 0.2285 (0.4120)	SegCLSLoss 0.0181 (0.0179)	KLLoss 0.1108 (0.1173)	MaskLoss 0.8137 (0.6311)	MaskBCELoss 0.1925 (0.2050)	MaskDICELoss 0.6211 (0.4261)
Epoch: [1][452/500]	Time 10.070 (10.070)	Loss 1.1172 (4.4197)	CeLoss 1.1172 (0.5027)	SegCLSLoss 0.0000 (0.0149)	KLLoss 0.0000 (0.0973)	MaskLoss 0.0000 (0.5815)	MaskBCELoss 0.0000 (0.1399)	MaskDICELoss 0.0000 (0.4416)
Epoch: [1][453/500]	Time  9.533 ( 9.533)	Loss 6.3628 (5.6456)	CeLoss 0.2676 (0.3545)	SegCLSLoss 0.0194 (0.0221)	KLLoss 0.1367 (0.1346)	MaskLoss 0.7768 (0.7604)	MaskBCELoss 0.0442 (0.1562)	MaskDICELoss 0.7325 (0.6041)
Epoch: [1][454/500]	Time 10.573 (10.573)	Loss 5.1037 (5.5847)	CeLoss 0.1826 (0.3497)	SegCLSLoss 0.0200 (0.0212)	KLLoss 0.1514 (0.1473)	MaskLoss 0.6562 (0.7252)	MaskBCELoss 0.0817 (0.1207)	MaskDICELoss 0.5746 (0.6045)
Epoch: [1][455/500]	Time  8.923 ( 8.923)	Loss 7.9457 (5.2353)	CeLoss 0.2432 (0.4862)	SegCLSLoss 0.0281 (0.0179)	KLLoss 0.1816 (0.1150)	MaskLoss 1.2029 (0.7634)	MaskBCELoss 0.3528 (0.2470)	MaskDICELoss 0.8501 (0.5164)
Epoch: [1][456/500]	Time 11.879 (11.879)	Loss 8.3877 (6.2748)	CeLoss 0.1660 (0.2423)	SegCLSLoss 0.0264 (0.0263)	KLLoss 0.1445 (0.1624)	MaskLoss 1.4149 (0.8881)	MaskBCELoss 0.5426 (0.2081)	MaskDICELoss 0.8723 (0.6801)
Epoch: [1][457/500]	Time 10.642 (10.642)	Loss 0.3691 (4.9819)	CeLoss 0.3691 (0.5118)	SegCLSLoss 0.0000 (0.0166)	KLLoss 0.0000 (0.1067)	MaskLoss 0.0000 (0.6617)	MaskBCELoss 0.0000 (0.1564)	MaskDICELoss 0.0000 (0.5053)
Epoch: [1][458/500]	Time  9.665 ( 9.665)	Loss 7.7182 (5.4104)	CeLoss 0.2402 (0.5204)	SegCLSLoss 0.0206 (0.0190)	KLLoss 0.1709 (0.1255)	MaskLoss 0.9851 (0.6862)	MaskBCELoss 0.0974 (0.1225)	MaskDICELoss 0.8877 (0.5637)
Epoch: [1][459/500]	Time  9.213 ( 9.213)	Loss 8.1832 (6.5035)	CeLoss 0.2949 (0.4322)	SegCLSLoss 0.0269 (0.0239)	KLLoss 0.1934 (0.1512)	MaskLoss 1.0405 (0.9751)	MaskBCELoss 0.1068 (0.3154)	MaskDICELoss 0.9337 (0.6597)
[2025-03-04 04:46:09,621] [INFO] [logging.py:96:log_dist] [Rank 0] step=960, skipped=0, lr=[0.0002781204819277108], mom=[(0.9, 0.95)]
[2025-03-04 04:46:09,627] [INFO] [timer.py:215:stop] epoch=0/micro_step=9600/global_step=960, RunningAvgSamplesPerSec=1.0303738121962862, CurrSamplesPerSec=1.0684536451714757, MemAllocated=56.81GB, MaxMemAllocated=62.82GB
Epoch: [1][460/500]	Time  9.362 ( 9.362)	Loss 1.2508 (3.5626)	CeLoss 0.2578 (0.3827)	SegCLSLoss 0.0193 (0.0116)	KLLoss 0.1250 (0.0863)	MaskLoss 0.2098 (0.5007)	MaskBCELoss 0.1367 (0.1530)	MaskDICELoss 0.0731 (0.3477)
Epoch: [1][461/500]	Time  9.115 ( 9.115)	Loss 8.4287 (5.9953)	CeLoss 0.2080 (0.2888)	SegCLSLoss 0.0304 (0.0196)	KLLoss 0.2158 (0.1302)	MaskLoss 1.0075 (0.8303)	MaskBCELoss 0.0118 (0.1793)	MaskDICELoss 0.9957 (0.6510)
Epoch: [1][462/500]	Time  8.239 ( 8.239)	Loss 7.9249 (4.9282)	CeLoss 0.2207 (0.5609)	SegCLSLoss 0.0312 (0.0171)	KLLoss 0.2061 (0.1116)	MaskLoss 1.1676 (0.6578)	MaskBCELoss 0.3095 (0.1692)	MaskDICELoss 0.8581 (0.4886)
Epoch: [1][463/500]	Time 11.575 (11.575)	Loss 4.3895 (4.2684)	CeLoss 0.2266 (0.4041)	SegCLSLoss 0.0197 (0.0206)	KLLoss 0.1143 (0.1213)	MaskLoss 0.5727 (0.5956)	MaskBCELoss 0.0906 (0.1719)	MaskDICELoss 0.4821 (0.4236)
Epoch: [1][464/500]	Time 10.491 (10.491)	Loss 7.3485 (6.9757)	CeLoss 0.3184 (0.2210)	SegCLSLoss 0.0264 (0.0272)	KLLoss 0.1484 (0.1673)	MaskLoss 1.1286 (1.0095)	MaskBCELoss 0.3601 (0.2504)	MaskDICELoss 0.7685 (0.7592)
Epoch: [1][465/500]	Time  7.985 ( 7.985)	Loss 0.1040 (3.3508)	CeLoss 0.1040 (0.5844)	SegCLSLoss 0.0000 (0.0126)	KLLoss 0.0000 (0.0774)	MaskLoss 0.0000 (0.4041)	MaskBCELoss 0.0000 (0.0917)	MaskDICELoss 0.0000 (0.3124)
Epoch: [1][466/500]	Time 10.555 (10.555)	Loss 1.8672 (4.7046)	CeLoss 1.8672 (0.3453)	SegCLSLoss 0.0000 (0.0213)	KLLoss 0.0000 (0.1248)	MaskLoss 0.0000 (0.6859)	MaskBCELoss 0.0000 (0.2106)	MaskDICELoss 0.0000 (0.4753)
Epoch: [1][467/500]	Time  9.699 ( 9.699)	Loss 2.5168 (5.5948)	CeLoss 0.2656 (0.3541)	SegCLSLoss 0.0178 (0.0278)	KLLoss 0.1221 (0.1437)	MaskLoss 0.4983 (0.8120)	MaskBCELoss 0.3113 (0.2354)	MaskDICELoss 0.1870 (0.5766)
Epoch: [1][468/500]	Time  9.226 ( 9.226)	Loss 8.7064 (5.9115)	CeLoss 0.2773 (0.4872)	SegCLSLoss 0.0242 (0.0213)	KLLoss 0.1738 (0.1450)	MaskLoss 1.2542 (0.8047)	MaskBCELoss 0.2984 (0.1948)	MaskDICELoss 0.9558 (0.6099)
Epoch: [1][469/500]	Time 12.031 (12.031)	Loss 6.2779 (5.9004)	CeLoss 0.1943 (0.3244)	SegCLSLoss 0.0260 (0.0215)	KLLoss 0.1758 (0.1385)	MaskLoss 0.9474 (0.8204)	MaskBCELoss 0.2807 (0.1894)	MaskDICELoss 0.6667 (0.6310)
[2025-03-04 04:47:47,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=970, skipped=0, lr=[0.00027787951807228914], mom=[(0.9, 0.95)]
[2025-03-04 04:47:47,458] [INFO] [timer.py:215:stop] epoch=0/micro_step=9700/global_step=970, RunningAvgSamplesPerSec=1.0302902405373002, CurrSamplesPerSec=1.121928348587671, MemAllocated=56.74GB, MaxMemAllocated=62.82GB
Epoch: [1][470/500]	Time  8.915 ( 8.915)	Loss 0.9141 (6.3455)	CeLoss 0.9141 (0.3831)	SegCLSLoss 0.0000 (0.0224)	KLLoss 0.0000 (0.1408)	MaskLoss 0.0000 (0.9997)	MaskBCELoss 0.0000 (0.3646)	MaskDICELoss 0.0000 (0.6351)
Epoch: [1][471/500]	Time  9.021 ( 9.021)	Loss 0.0933 (4.2732)	CeLoss 0.0933 (0.5040)	SegCLSLoss 0.0000 (0.0188)	KLLoss 0.0000 (0.1137)	MaskLoss 0.0000 (0.5452)	MaskBCELoss 0.0000 (0.1193)	MaskDICELoss 0.0000 (0.4260)
Epoch: [1][472/500]	Time  7.800 ( 7.800)	Loss 1.6641 (5.6911)	CeLoss 1.6641 (0.3573)	SegCLSLoss 0.0000 (0.0337)	KLLoss 0.0000 (0.1636)	MaskLoss 0.0000 (0.8374)	MaskBCELoss 0.0000 (0.2576)	MaskDICELoss 0.0000 (0.5798)
Epoch: [1][473/500]	Time 10.155 (10.155)	Loss 0.9844 (5.2948)	CeLoss 0.9844 (0.2779)	SegCLSLoss 0.0000 (0.0225)	KLLoss 0.0000 (0.1434)	MaskLoss 0.0000 (0.7560)	MaskBCELoss 0.0000 (0.1976)	MaskDICELoss 0.0000 (0.5584)
Epoch: [1][474/500]	Time  9.239 ( 9.239)	Loss 1.1875 (5.2444)	CeLoss 1.1875 (0.4873)	SegCLSLoss 0.0000 (0.0205)	KLLoss 0.0000 (0.1448)	MaskLoss 0.0000 (0.6900)	MaskBCELoss 0.0000 (0.1530)	MaskDICELoss 0.0000 (0.5370)
Epoch: [1][475/500]	Time  8.393 ( 8.393)	Loss 7.6355 (3.4891)	CeLoss 0.2930 (0.7021)	SegCLSLoss 0.0239 (0.0094)	KLLoss 0.1475 (0.0676)	MaskLoss 0.9169 (0.4658)	MaskBCELoss 0.0255 (0.1687)	MaskDICELoss 0.8914 (0.2972)
Epoch: [1][476/500]	Time  8.680 ( 8.680)	Loss 6.2144 (6.5351)	CeLoss 0.1826 (0.3792)	SegCLSLoss 0.0317 (0.0219)	KLLoss 0.1816 (0.1459)	MaskLoss 0.7494 (1.0352)	MaskBCELoss 0.0266 (0.3803)	MaskDICELoss 0.7228 (0.6548)
Epoch: [1][477/500]	Time 10.862 (10.862)	Loss 2.5773 (4.5927)	CeLoss 0.2617 (0.3254)	SegCLSLoss 0.0210 (0.0159)	KLLoss 0.1426 (0.1191)	MaskLoss 0.4729 (0.5997)	MaskBCELoss 0.2700 (0.1096)	MaskDICELoss 0.2029 (0.4901)
Epoch: [1][478/500]	Time  9.016 ( 9.016)	Loss 1.5312 (3.7418)	CeLoss 1.5312 (0.4025)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.1034)	MaskLoss 0.0000 (0.5801)	MaskBCELoss 0.0000 (0.2354)	MaskDICELoss 0.0000 (0.3447)
Epoch: [1][479/500]	Time  9.445 ( 9.445)	Loss 8.0103 (5.9545)	CeLoss 0.2217 (0.5407)	SegCLSLoss 0.0192 (0.0199)	KLLoss 0.1689 (0.1387)	MaskLoss 1.4049 (0.8086)	MaskBCELoss 0.6049 (0.2006)	MaskDICELoss 0.8000 (0.6080)
[2025-03-04 04:49:19,890] [INFO] [logging.py:96:log_dist] [Rank 0] step=980, skipped=0, lr=[0.00027763855421686747], mom=[(0.9, 0.95)]
[2025-03-04 04:49:19,896] [INFO] [timer.py:215:stop] epoch=0/micro_step=9800/global_step=980, RunningAvgSamplesPerSec=1.0307938099232157, CurrSamplesPerSec=1.0178192083702258, MemAllocated=57.26GB, MaxMemAllocated=62.82GB
Epoch: [1][480/500]	Time  9.827 ( 9.827)	Loss 8.4634 (6.3741)	CeLoss 0.2227 (0.3681)	SegCLSLoss 0.0280 (0.0214)	KLLoss 0.2090 (0.1588)	MaskLoss 1.0920 (0.8695)	MaskBCELoss 0.1197 (0.1866)	MaskDICELoss 0.9723 (0.6829)
Epoch: [1][481/500]	Time  8.858 ( 8.858)	Loss 3.7555 (4.0608)	CeLoss 0.2832 (0.6470)	SegCLSLoss 0.0195 (0.0152)	KLLoss 0.1807 (0.1131)	MaskLoss 0.5135 (0.4891)	MaskBCELoss 0.1375 (0.1032)	MaskDICELoss 0.3760 (0.3859)
Epoch: [1][482/500]	Time 10.305 (10.305)	Loss 7.6732 (5.8799)	CeLoss 0.1865 (0.4296)	SegCLSLoss 0.0396 (0.0238)	KLLoss 0.2266 (0.1548)	MaskLoss 0.9413 (0.7561)	MaskBCELoss 0.0485 (0.1275)	MaskDICELoss 0.8928 (0.6286)
Epoch: [1][483/500]	Time  9.278 ( 9.278)	Loss 1.4297 (4.7092)	CeLoss 1.4297 (0.4725)	SegCLSLoss 0.0000 (0.0171)	KLLoss 0.0000 (0.1365)	MaskLoss 0.0000 (0.6077)	MaskBCELoss 0.0000 (0.1283)	MaskDICELoss 0.0000 (0.4794)
Epoch: [1][484/500]	Time  9.473 ( 9.473)	Loss 8.0416 (5.9243)	CeLoss 0.2197 (0.3806)	SegCLSLoss 0.0182 (0.0182)	KLLoss 0.1338 (0.1442)	MaskLoss 1.2160 (0.7872)	MaskBCELoss 0.3416 (0.1512)	MaskDICELoss 0.8744 (0.6360)
Epoch: [1][485/500]	Time  8.923 ( 8.923)	Loss 8.1027 (5.3116)	CeLoss 0.1582 (0.3740)	SegCLSLoss 0.0737 (0.0287)	KLLoss 0.2314 (0.1620)	MaskLoss 1.3673 (0.7666)	MaskBCELoss 0.5435 (0.2286)	MaskDICELoss 0.8237 (0.5380)
Epoch: [1][486/500]	Time  9.897 ( 9.897)	Loss 7.1692 (5.7267)	CeLoss 0.2471 (0.4037)	SegCLSLoss 0.0212 (0.0239)	KLLoss 0.2158 (0.1763)	MaskLoss 0.9205 (0.7785)	MaskBCELoss 0.1112 (0.1822)	MaskDICELoss 0.8093 (0.5963)
Epoch: [1][487/500]	Time  9.410 ( 9.410)	Loss 0.7344 (5.2202)	CeLoss 0.7344 (0.3602)	SegCLSLoss 0.0000 (0.0196)	KLLoss 0.0000 (0.1412)	MaskLoss 0.0000 (0.7434)	MaskBCELoss 0.0000 (0.2064)	MaskDICELoss 0.0000 (0.5371)
Epoch: [1][488/500]	Time  9.209 ( 9.209)	Loss 8.2657 (6.2580)	CeLoss 0.2383 (0.3285)	SegCLSLoss 0.0289 (0.0176)	KLLoss 0.2070 (0.1269)	MaskLoss 1.1677 (1.1166)	MaskBCELoss 0.2558 (0.5232)	MaskDICELoss 0.9119 (0.5934)
Epoch: [1][489/500]	Time  9.438 ( 9.438)	Loss 2.0469 (5.8626)	CeLoss 2.0469 (0.4329)	SegCLSLoss 0.0000 (0.0215)	KLLoss 0.0000 (0.1548)	MaskLoss 0.0000 (0.8736)	MaskBCELoss 0.0000 (0.2875)	MaskDICELoss 0.0000 (0.5862)
[2025-03-04 04:50:54,657] [INFO] [logging.py:96:log_dist] [Rank 0] step=990, skipped=0, lr=[0.00027739759036144574], mom=[(0.9, 0.95)]
[2025-03-04 04:50:54,662] [INFO] [timer.py:215:stop] epoch=0/micro_step=9900/global_step=990, RunningAvgSamplesPerSec=1.0310371265779674, CurrSamplesPerSec=1.0025931912028228, MemAllocated=57.26GB, MaxMemAllocated=62.82GB
Epoch: [1][490/500]	Time  9.976 ( 9.976)	Loss 8.6091 (5.7199)	CeLoss 0.3691 (0.6185)	SegCLSLoss 0.0234 (0.0178)	KLLoss 0.1582 (0.1268)	MaskLoss 1.0586 (0.6992)	MaskBCELoss 0.0665 (0.1047)	MaskDICELoss 0.9921 (0.5946)
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
Epoch: [1][491/500]	Time 10.734 (10.734)	Loss 0.7578 (5.0869)	CeLoss 0.7578 (0.2759)	SegCLSLoss 0.0000 (0.0283)	KLLoss 0.0000 (0.1490)	MaskLoss 0.0000 (0.7357)	MaskBCELoss 0.0000 (0.2063)	MaskDICELoss 0.0000 (0.5294)
Epoch: [1][492/500]	Time 11.700 (11.700)	Loss 7.0416 (7.1587)	CeLoss 0.3125 (0.2474)	SegCLSLoss 0.0181 (0.0261)	KLLoss 0.1621 (0.1839)	MaskLoss 1.1654 (1.0024)	MaskBCELoss 0.4610 (0.2174)	MaskDICELoss 0.7044 (0.7849)
Epoch: [1][493/500]	Time  8.091 ( 8.091)	Loss 3.3858 (5.1557)	CeLoss 0.2617 (0.4620)	SegCLSLoss 0.0161 (0.0173)	KLLoss 0.1982 (0.1422)	MaskLoss 0.6190 (0.7002)	MaskBCELoss 0.3391 (0.1764)	MaskDICELoss 0.2799 (0.5238)
Epoch: [1][494/500]	Time 10.065 (10.065)	Loss 1.7565 (4.0469)	CeLoss 0.2930 (0.4423)	SegCLSLoss 0.0182 (0.0180)	KLLoss 0.1221 (0.1114)	MaskLoss 0.3308 (0.5400)	MaskBCELoss 0.2193 (0.1393)	MaskDICELoss 0.1115 (0.4007)
Epoch: [1][495/500]	Time  9.828 ( 9.828)	Loss 7.4598 (5.8931)	CeLoss 0.2051 (0.3733)	SegCLSLoss 0.0204 (0.0226)	KLLoss 0.1826 (0.1534)	MaskLoss 0.9284 (0.8094)	MaskBCELoss 0.0610 (0.1868)	MaskDICELoss 0.8674 (0.6226)
Epoch: [1][496/500]	Time  8.035 ( 8.035)	Loss 0.3984 (3.3830)	CeLoss 0.3984 (0.8397)	SegCLSLoss 0.0000 (0.0109)	KLLoss 0.0000 (0.0712)	MaskLoss 0.0000 (0.3781)	MaskBCELoss 0.0000 (0.0931)	MaskDICELoss 0.0000 (0.2851)
Epoch: [1][497/500]	Time 10.786 (10.786)	Loss 8.2714 (5.7458)	CeLoss 0.2598 (0.3187)	SegCLSLoss 0.0262 (0.0194)	KLLoss 0.1973 (0.1441)	MaskLoss 0.9901 (0.7364)	MaskBCELoss 0.0200 (0.1030)	MaskDICELoss 0.9701 (0.6334)
Epoch: [1][498/500]	Time  9.896 ( 9.896)	Loss 8.1635 (5.0943)	CeLoss 0.2832 (0.4468)	SegCLSLoss 0.0233 (0.0183)	KLLoss 0.1670 (0.1122)	MaskLoss 1.2451 (0.6676)	MaskBCELoss 0.3764 (0.1358)	MaskDICELoss 0.8687 (0.5318)
Epoch: [1][499/500]	Time  9.495 ( 9.495)	Loss 4.6450 (5.8504)	CeLoss 0.2471 (0.3729)	SegCLSLoss 0.0142 (0.0199)	KLLoss 0.1299 (0.1446)	MaskLoss 0.8765 (0.8415)	MaskBCELoss 0.4583 (0.2349)	MaskDICELoss 0.4182 (0.6066)
[2025-03-04 04:52:30,594] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[0.00027715662650602406], mom=[(0.9, 0.95)]
[2025-03-04 04:52:30,600] [INFO] [timer.py:215:stop] epoch=0/micro_step=10000/global_step=1000, RunningAvgSamplesPerSec=1.031150970105719, CurrSamplesPerSec=1.36884429418346, MemAllocated=56.81GB, MaxMemAllocated=62.82GB
Epoch: [1][500/500]	Time  7.307 ( 7.307)	Loss 7.1707 (3.3424)	CeLoss 0.1963 (0.6921)	SegCLSLoss 0.0156 (0.0110)	KLLoss 0.1289 (0.0790)	MaskLoss 0.9773 (0.4030)	MaskBCELoss 0.1633 (0.1097)	MaskDICELoss 0.8140 (0.2933)
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
giou: 0.2336, ciou: 0.2320
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')ngine to run the backward pass██████████████████████████████████████████████████| 200/200 [00:54<00:00,  3.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
[2025-03-04 04:53:31,285] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1000 is about to be saved!
[2025-03-04 04:53:59,551] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./runs/plum-13b_kld_1_dice_8/ckpt_model/global_step1000/mp_rank_00_model_states.pt
[2025-03-04 04:53:59,552] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./runs/plum-13b_kld_1_dice_8/ckpt_model/global_step1000/mp_rank_00_model_states.pt...
[2025-03-04 04:56:57,279] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./runs/plum-13b_kld_1_dice_8/ckpt_model/global_step1000/mp_rank_00_model_states.pt.
[2025-03-04 04:56:58,823] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./runs/plum-13b_kld_1_dice_8/ckpt_model/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-04 04:57:09,252] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./runs/plum-13b_kld_1_dice_8/ckpt_model/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-04 04:57:09,254] [INFO] [engine.py:3244:_save_zero_checkpoint] zero checkpoint saved ./runs/plum-13b_kld_1_dice_8/ckpt_model/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-04 04:57:09,255] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
Epoch: [2][  1/500]	Time 10.606 (10.606)	Loss 4.3899 (5.8485)	CeLoss 0.2910 (0.3557)	SegCLSLoss 0.0194 (0.0234)	KLLoss 0.1123 (0.1391)	MaskLoss 0.5635 (0.8200)	MaskBCELoss 0.0887 (0.2029)	MaskDICELoss 0.4748 (0.6170)
Epoch: [2][  2/500]	Time  9.674 ( 9.674)	Loss 3.3247 (4.0431)	CeLoss 0.3633 (0.6021)	SegCLSLoss 0.0159 (0.0125)	KLLoss 0.1030 (0.0838)	MaskLoss 0.4000 (0.4598)	MaskBCELoss 0.0583 (0.0546)	MaskDICELoss 0.3417 (0.4052)
Epoch: [2][  3/500]	Time 10.517 (10.517)	Loss 5.9790 (5.5610)	CeLoss 0.2988 (0.3895)	SegCLSLoss 0.0239 (0.0180)	KLLoss 0.1611 (0.1325)	MaskLoss 0.8918 (0.8109)	MaskBCELoss 0.2713 (0.2429)	MaskDICELoss 0.6205 (0.5680)
Epoch: [2][  4/500]	Time  8.646 ( 8.646)	Loss 7.1133 (6.6785)	CeLoss 0.2207 (0.3591)	SegCLSLoss 0.0187 (0.0258)	KLLoss 0.1279 (0.1539)	MaskLoss 0.8991 (1.0721)	MaskBCELoss 0.0731 (0.4040)	MaskDICELoss 0.8260 (0.6681)
Epoch: [2][  5/500]	Time 11.008 (11.008)	Loss 8.0848 (6.6115)	CeLoss 0.2051 (0.2077)	SegCLSLoss 0.0217 (0.0256)	KLLoss 0.1475 (0.1725)	MaskLoss 1.0005 (0.9580)	MaskBCELoss 0.0471 (0.2409)	MaskDICELoss 0.9534 (0.7171)
Epoch: [2][  6/500]	Time  8.952 ( 8.952)	Loss 8.4353 (4.9071)	CeLoss 0.1865 (0.6809)	SegCLSLoss 0.0254 (0.0140)	KLLoss 0.1699 (0.0963)	MaskLoss 1.0412 (0.6098)	MaskBCELoss 0.0439 (0.1259)	MaskDICELoss 0.9973 (0.4839)
Epoch: [2][  7/500]	Time  7.470 ( 7.470)	Loss 3.9967 (4.5252)	CeLoss 0.2637 (0.7459)	SegCLSLoss 0.0199 (0.0136)	KLLoss 0.1670 (0.0958)	MaskLoss 0.5465 (0.5380)	MaskBCELoss 0.1361 (0.1045)	MaskDICELoss 0.4104 (0.4334)
Epoch: [2][  8/500]	Time  7.697 ( 7.697)	Loss 5.4359 (4.3790)	CeLoss 0.3184 (0.6029)	SegCLSLoss 0.0142 (0.0166)	KLLoss 0.0918 (0.0938)	MaskLoss 1.1514 (0.5731)	MaskBCELoss 0.6989 (0.1518)	MaskDICELoss 0.4525 (0.4213)
Epoch: [2][  9/500]	Time 11.809 (11.809)	Loss 7.6201 (6.8385)	CeLoss 0.1709 (0.2418)	SegCLSLoss 0.0415 (0.0292)	KLLoss 0.2539 (0.1891)	MaskLoss 1.1041 (1.0598)	MaskBCELoss 0.2763 (0.3475)	MaskDICELoss 0.8278 (0.7123)
[2025-03-04 04:58:44,170] [INFO] [logging.py:96:log_dist] [Rank 0] step=1010, skipped=0, lr=[0.0002769277108433735], mom=[(0.9, 0.95)]
[2025-03-04 04:58:44,176] [INFO] [timer.py:215:stop] epoch=0/micro_step=10100/global_step=1010, RunningAvgSamplesPerSec=1.031409761688541, CurrSamplesPerSec=1.1729781311086436, MemAllocated=57.58GB, MaxMemAllocated=62.82GB
Epoch: [2][ 10/500]	Time  8.527 ( 8.527)	Loss 0.4453 (3.4698)	CeLoss 0.4453 (0.8581)	SegCLSLoss 0.0000 (0.0096)	KLLoss 0.0000 (0.0721)	MaskLoss 0.0000 (0.3902)	MaskBCELoss 0.0000 (0.0979)	MaskDICELoss 0.0000 (0.2924)
Epoch: [2][ 11/500]	Time  8.800 ( 8.800)	Loss 6.6692 (6.3502)	CeLoss 0.3301 (0.4188)	SegCLSLoss 0.0176 (0.0288)	KLLoss 0.1201 (0.1333)	MaskLoss 0.8799 (1.0622)	MaskBCELoss 0.1384 (0.4524)	MaskDICELoss 0.7414 (0.6098)
Epoch: [2][ 12/500]	Time  9.504 ( 9.504)	Loss 1.6128 (4.8956)	CeLoss 0.2490 (0.2876)	SegCLSLoss 0.0193 (0.0203)	KLLoss 0.1348 (0.1443)	MaskLoss 0.2689 (0.7430)	MaskBCELoss 0.1552 (0.2484)	MaskDICELoss 0.1137 (0.4946)
Epoch: [2][ 13/500]	Time  8.731 ( 8.731)	Loss 8.3271 (5.0698)	CeLoss 0.2002 (0.4922)	SegCLSLoss 0.0281 (0.0246)	KLLoss 0.2012 (0.1367)	MaskLoss 1.0110 (0.7055)	MaskBCELoss 0.0292 (0.2025)	MaskDICELoss 0.9818 (0.5029)
Epoch: [2][ 14/500]	Time  9.213 ( 9.213)	Loss 6.1751 (4.0144)	CeLoss 0.2637 (0.5426)	SegCLSLoss 0.0173 (0.0111)	KLLoss 0.1572 (0.0784)	MaskLoss 0.8104 (0.4638)	MaskBCELoss 0.1229 (0.0538)	MaskDICELoss 0.6874 (0.4100)
Epoch: [2][ 15/500]	Time  9.134 ( 9.134)	Loss 8.2741 (5.0801)	CeLoss 0.1943 (0.3614)	SegCLSLoss 0.0150 (0.0191)	KLLoss 0.0898 (0.1229)	MaskLoss 1.0212 (0.7086)	MaskBCELoss 0.0314 (0.1804)	MaskDICELoss 0.9898 (0.5281)
Epoch: [2][ 16/500]	Time 11.393 (11.393)	Loss 6.8841 (5.6797)	CeLoss 0.2559 (0.3849)	SegCLSLoss 0.0265 (0.0221)	KLLoss 0.1699 (0.1438)	MaskLoss 1.0537 (0.8421)	MaskBCELoss 0.3308 (0.2661)	MaskDICELoss 0.7229 (0.5759)
Epoch: [2][ 17/500]	Time 10.283 (10.283)	Loss 8.5378 (6.3637)	CeLoss 0.2236 (0.4677)	SegCLSLoss 0.0273 (0.0210)	KLLoss 0.1553 (0.1296)	MaskLoss 1.0816 (0.8944)	MaskBCELoss 0.0846 (0.2331)	MaskDICELoss 0.9970 (0.6612)
Epoch: [2][ 18/500]	Time  7.850 ( 7.850)	Loss 1.1016 (4.9812)	CeLoss 1.1016 (0.6511)	SegCLSLoss 0.0000 (0.0134)	KLLoss 0.0000 (0.0778)	MaskLoss 0.0000 (0.5962)	MaskBCELoss 0.0000 (0.0874)	MaskDICELoss 0.0000 (0.5088)
Epoch: [2][ 19/500]	Time  9.434 ( 9.434)	Loss 5.3475 (5.4455)	CeLoss 0.2910 (0.3550)	SegCLSLoss 0.0243 (0.0215)	KLLoss 0.1650 (0.1482)	MaskLoss 0.6765 (0.7807)	MaskBCELoss 0.0889 (0.2190)	MaskDICELoss 0.5876 (0.5617)
[2025-03-04 05:00:18,023] [INFO] [logging.py:96:log_dist] [Rank 0] step=1020, skipped=0, lr=[0.0002766867469879518], mom=[(0.9, 0.95)]
[2025-03-04 05:00:18,029] [INFO] [timer.py:215:stop] epoch=0/micro_step=10200/global_step=1020, RunningAvgSamplesPerSec=1.0317357689861881, CurrSamplesPerSec=1.0517553512512976, MemAllocated=56.81GB, MaxMemAllocated=62.82GB
Epoch: [2][ 20/500]	Time  9.510 ( 9.510)	Loss 1.3743 (6.3812)	CeLoss 0.2119 (0.3335)	SegCLSLoss 0.0149 (0.0292)	KLLoss 0.0786 (0.1709)	MaskLoss 0.2750 (0.9369)	MaskBCELoss 0.1875 (0.2722)	MaskDICELoss 0.0876 (0.6646)
Epoch: [2][ 21/500]	Time  8.836 ( 8.836)	Loss 8.5209 (5.4719)	CeLoss 0.2461 (0.3654)	SegCLSLoss 0.0239 (0.0220)	KLLoss 0.1562 (0.1127)	MaskLoss 1.2380 (0.8133)	MaskBCELoss 0.2995 (0.2539)	MaskDICELoss 0.9385 (0.5594)
Epoch: [2][ 22/500]	Time  8.995 ( 8.995)	Loss 0.9922 (4.4585)	CeLoss 0.9922 (0.6656)	SegCLSLoss 0.0000 (0.0152)	KLLoss 0.0000 (0.0953)	MaskLoss 0.0000 (0.5001)	MaskBCELoss 0.0000 (0.0519)	MaskDICELoss 0.0000 (0.4483)
Epoch: [2][ 23/500]	Time  9.332 ( 9.332)	Loss 7.9743 (6.0580)	CeLoss 0.2314 (0.4288)	SegCLSLoss 0.0282 (0.0229)	KLLoss 0.1777 (0.1436)	MaskLoss 1.2841 (0.9103)	MaskBCELoss 0.4534 (0.3013)	MaskDICELoss 0.8307 (0.6090)
Epoch: [2][ 24/500]	Time  9.896 ( 9.896)	Loss 7.4827 (5.8969)	CeLoss 0.2168 (0.2839)	SegCLSLoss 0.0327 (0.0301)	KLLoss 0.1523 (0.1593)	MaskLoss 0.9316 (0.8819)	MaskBCELoss 0.0595 (0.2694)	MaskDICELoss 0.8721 (0.6124)
Epoch: [2][ 25/500]	Time  8.742 ( 8.742)	Loss 6.2171 (5.4703)	CeLoss 0.1943 (0.4479)	SegCLSLoss 0.0233 (0.0213)	KLLoss 0.1533 (0.1218)	MaskLoss 0.8058 (0.7068)	MaskBCELoss 0.0982 (0.1274)	MaskDICELoss 0.7077 (0.5794)
Epoch: [2][ 26/500]	Time  9.870 ( 9.870)	Loss 7.7747 (5.4638)	CeLoss 0.1904 (0.2756)	SegCLSLoss 0.0310 (0.0249)	KLLoss 0.2090 (0.1391)	MaskLoss 0.9455 (0.7866)	MaskBCELoss 0.0338 (0.2094)	MaskDICELoss 0.9116 (0.5773)
Epoch: [2][ 27/500]	Time 11.996 (11.996)	Loss 6.4700 (6.6770)	CeLoss 0.3223 (0.2335)	SegCLSLoss 0.0204 (0.0242)	KLLoss 0.1299 (0.1562)	MaskLoss 0.8431 (0.9015)	MaskBCELoss 0.1226 (0.1561)	MaskDICELoss 0.7205 (0.7454)
Epoch: [2][ 28/500]	Time 10.072 (10.072)	Loss 6.1645 (5.2955)	CeLoss 0.2090 (0.4306)	SegCLSLoss 0.0249 (0.0231)	KLLoss 0.1338 (0.1306)	MaskLoss 0.7843 (0.6772)	MaskBCELoss 0.0775 (0.1159)	MaskDICELoss 0.7067 (0.5614)
Epoch: [2][ 29/500]	Time  7.951 ( 7.951)	Loss 1.5312 (5.1785)	CeLoss 1.5312 (0.6274)	SegCLSLoss 0.0000 (0.0148)	KLLoss 0.0000 (0.0950)	MaskLoss 0.0000 (0.7837)	MaskBCELoss 0.0000 (0.3034)	MaskDICELoss 0.0000 (0.4803)
[2025-03-04 05:01:50,984] [INFO] [logging.py:96:log_dist] [Rank 0] step=1030, skipped=0, lr=[0.00027644578313253007], mom=[(0.9, 0.95)]
[2025-03-04 05:01:50,989] [INFO] [timer.py:215:stop] epoch=0/micro_step=10300/global_step=1030, RunningAvgSamplesPerSec=1.0321480177114322, CurrSamplesPerSec=1.375458895780041, MemAllocated=56.7GB, MaxMemAllocated=62.82GB
Epoch: [2][ 30/500]	Time  7.272 ( 7.272)	Loss 1.7656 (3.7188)	CeLoss 1.7656 (0.9843)	SegCLSLoss 0.0000 (0.0094)	KLLoss 0.0000 (0.0617)	MaskLoss 0.0000 (0.3622)	MaskBCELoss 0.0000 (0.0382)	MaskDICELoss 0.0000 (0.3240)
Epoch: [2][ 31/500]	Time  8.801 ( 8.801)	Loss 8.7486 (4.4044)	CeLoss 0.2012 (0.4769)	SegCLSLoss 0.0256 (0.0201)	KLLoss 0.2051 (0.1236)	MaskLoss 1.1761 (0.5744)	MaskBCELoss 0.1796 (0.1335)	MaskDICELoss 0.9964 (0.4409)
Epoch: [2][ 32/500]	Time  9.398 ( 9.398)	Loss 7.9106 (7.2579)	CeLoss 0.2021 (0.2929)	SegCLSLoss 0.0281 (0.0317)	KLLoss 0.1895 (0.1852)	MaskLoss 0.9599 (1.0718)	MaskBCELoss 0.0292 (0.3017)	MaskDICELoss 0.9307 (0.7701)
Epoch: [2][ 33/500]	Time  9.566 ( 9.566)	Loss 5.3591 (4.9724)	CeLoss 0.1826 (0.4215)	SegCLSLoss 0.0197 (0.0177)	KLLoss 0.1309 (0.1186)	MaskLoss 0.7798 (0.6617)	MaskBCELoss 0.2006 (0.1451)	MaskDICELoss 0.5792 (0.5166)
Epoch: [2][ 34/500]	Time  8.542 ( 8.542)	Loss 7.0367 (4.7451)	CeLoss 0.2061 (0.5934)	SegCLSLoss 0.0332 (0.0171)	KLLoss 0.1709 (0.0991)	MaskLoss 0.9400 (0.6016)	MaskBCELoss 0.1463 (0.1282)	MaskDICELoss 0.7937 (0.4734)
Epoch: [2][ 35/500]	Time  9.812 ( 9.812)	Loss 7.0826 (4.8145)	CeLoss 0.2656 (0.3236)	SegCLSLoss 0.0211 (0.0166)	KLLoss 0.1543 (0.1116)	MaskLoss 0.8608 (0.6464)	MaskBCELoss 0.0389 (0.1333)	MaskDICELoss 0.8219 (0.5131)
Epoch: [2][ 36/500]	Time  9.761 ( 9.761)	Loss 6.1536 (5.6394)	CeLoss 0.2197 (0.3536)	SegCLSLoss 0.0325 (0.0216)	KLLoss 0.1963 (0.1356)	MaskLoss 0.7819 (0.8971)	MaskBCELoss 0.0892 (0.3396)	MaskDICELoss 0.6927 (0.5575)
Epoch: [2][ 37/500]	Time 10.276 (10.276)	Loss 8.9665 (5.3291)	CeLoss 0.1719 (0.4443)	SegCLSLoss 0.0505 (0.0214)	KLLoss 0.2539 (0.1331)	MaskLoss 1.3766 (0.7123)	MaskBCELoss 0.4163 (0.1596)	MaskDICELoss 0.9603 (0.5527)
Epoch: [2][ 38/500]	Time  8.804 ( 8.804)	Loss 4.9988 (4.1936)	CeLoss 0.3164 (0.5499)	SegCLSLoss 0.0182 (0.0135)	KLLoss 0.1289 (0.0937)	MaskLoss 0.7729 (0.5242)	MaskBCELoss 0.2732 (0.1084)	MaskDICELoss 0.4997 (0.4158)
Epoch: [2][ 39/500]	Time  8.994 ( 8.994)	Loss 8.5414 (5.5178)	CeLoss 0.2734 (0.5573)	SegCLSLoss 0.0190 (0.0179)	KLLoss 0.1084 (0.1038)	MaskLoss 1.0913 (0.6759)	MaskBCELoss 0.0966 (0.0931)	MaskDICELoss 0.9947 (0.5827)
[2025-03-04 05:03:26,050] [INFO] [logging.py:96:log_dist] [Rank 0] step=1040, skipped=0, lr=[0.0002762048192771084], mom=[(0.9, 0.95)]
[2025-03-04 05:03:26,056] [INFO] [timer.py:215:stop] epoch=0/micro_step=10400/global_step=1040, RunningAvgSamplesPerSec=1.032336369089757, CurrSamplesPerSec=0.9000948464678034, MemAllocated=57.3GB, MaxMemAllocated=62.82GB
Epoch: [2][ 40/500]	Time 11.112 (11.112)	Loss 7.8983 (6.0561)	CeLoss 0.1455 (0.3946)	SegCLSLoss 0.0598 (0.0221)	KLLoss 0.2373 (0.1213)	MaskLoss 1.2338 (0.8543)	MaskBCELoss 0.3977 (0.2176)	MaskDICELoss 0.8361 (0.6367)
Epoch: [2][ 41/500]	Time  9.631 ( 9.631)	Loss 8.3479 (6.3232)	CeLoss 0.2344 (0.3179)	SegCLSLoss 0.0190 (0.0202)	KLLoss 0.1196 (0.1269)	MaskLoss 1.0053 (0.8840)	MaskBCELoss 0.0096 (0.2007)	MaskDICELoss 0.9957 (0.6833)
Epoch: [2][ 42/500]	Time  8.871 ( 8.871)	Loss 6.0216 (4.7778)	CeLoss 0.2637 (0.4750)	SegCLSLoss 0.0237 (0.0161)	KLLoss 0.1738 (0.1095)	MaskLoss 0.9473 (0.6518)	MaskBCELoss 0.3343 (0.1715)	MaskDICELoss 0.6130 (0.4803)
Epoch: [2][ 43/500]	Time 10.657 (10.657)	Loss 7.2788 (4.9730)	CeLoss 0.2061 (0.3728)	SegCLSLoss 0.0295 (0.0200)	KLLoss 0.1816 (0.1161)	MaskLoss 1.2193 (0.7009)	MaskBCELoss 0.4797 (0.1889)	MaskDICELoss 0.7396 (0.5120)
Epoch: [2][ 44/500]	Time 10.073 (10.073)	Loss 7.8053 (6.5685)	CeLoss 0.1826 (0.2411)	SegCLSLoss 0.0464 (0.0286)	KLLoss 0.2324 (0.1759)	MaskLoss 1.2106 (0.9869)	MaskBCELoss 0.3862 (0.2929)	MaskDICELoss 0.8244 (0.6939)
Epoch: [2][ 45/500]	Time  9.337 ( 9.337)	Loss 3.3424 (5.3333)	CeLoss 0.3262 (0.4535)	SegCLSLoss 0.0179 (0.0246)	KLLoss 0.0688 (0.1401)	MaskLoss 0.4054 (0.7337)	MaskBCELoss 0.0508 (0.1904)	MaskDICELoss 0.3546 (0.5433)
Epoch: [2][ 46/500]	Time  7.123 ( 7.123)	Loss 7.2451 (4.7547)	CeLoss 0.1699 (0.5273)	SegCLSLoss 0.0420 (0.0162)	KLLoss 0.2363 (0.1064)	MaskLoss 0.9292 (0.6901)	MaskBCELoss 0.1027 (0.2347)	MaskDICELoss 0.8265 (0.4554)
Epoch: [2][ 47/500]	Time  9.635 ( 9.635)	Loss 0.0840 (4.6776)	CeLoss 0.0840 (0.4021)	SegCLSLoss 0.0000 (0.0168)	KLLoss 0.0000 (0.0996)	MaskLoss 0.0000 (0.6187)	MaskBCELoss 0.0000 (0.1303)	MaskDICELoss 0.0000 (0.4884)
Epoch: [2][ 48/500]	Time  9.175 ( 9.175)	Loss 0.1030 (5.5773)	CeLoss 0.1030 (0.3923)	SegCLSLoss 0.0000 (0.0275)	KLLoss 0.0000 (0.1455)	MaskLoss 0.0000 (0.8398)	MaskBCELoss 0.0000 (0.2821)	MaskDICELoss 0.0000 (0.5577)
Epoch: [2][ 49/500]	Time  8.351 ( 8.351)	Loss 7.1546 (3.7929)	CeLoss 0.2520 (0.5951)	SegCLSLoss 0.0183 (0.0119)	KLLoss 0.1270 (0.0828)	MaskLoss 1.3233 (0.5352)	MaskBCELoss 0.6368 (0.1954)	MaskDICELoss 0.6866 (0.3397)
[2025-03-04 05:04:57,160] [INFO] [logging.py:96:log_dist] [Rank 0] step=1050, skipped=0, lr=[0.0002759638554216867], mom=[(0.9, 0.95)]
[2025-03-04 05:04:57,165] [INFO] [timer.py:215:stop] epoch=0/micro_step=10500/global_step=1050, RunningAvgSamplesPerSec=1.0329240083506366, CurrSamplesPerSec=1.2117006904835552, MemAllocated=57.38GB, MaxMemAllocated=62.82GB
Epoch: [2][ 50/500]	Time  8.255 ( 8.255)	Loss 7.9763 (5.4784)	CeLoss 0.2285 (0.3920)	SegCLSLoss 0.0271 (0.0210)	KLLoss 0.1953 (0.1377)	MaskLoss 0.9744 (0.7781)	MaskBCELoss 0.0428 (0.2145)	MaskDICELoss 0.9317 (0.5636)
Epoch: [2][ 51/500]	Time  9.906 ( 9.906)	Loss 1.0234 (5.5450)	CeLoss 1.0234 (0.4147)	SegCLSLoss 0.0000 (0.0187)	KLLoss 0.0000 (0.1200)	MaskLoss 0.0000 (0.7733)	MaskBCELoss 0.0000 (0.1976)	MaskDICELoss 0.0000 (0.5757)
Epoch: [2][ 52/500]	Time  9.804 ( 9.804)	Loss 6.8686 (6.3734)	CeLoss 0.2100 (0.2856)	SegCLSLoss 0.0190 (0.0224)	KLLoss 0.1235 (0.1506)	MaskLoss 0.9058 (0.8691)	MaskBCELoss 0.1202 (0.1711)	MaskDICELoss 0.7855 (0.6980)
Epoch: [2][ 53/500]	Time 12.816 (12.816)	Loss 1.5703 (4.9462)	CeLoss 1.5703 (0.5938)	SegCLSLoss 0.0000 (0.0209)	KLLoss 0.0000 (0.1216)	MaskLoss 0.0000 (0.6854)	MaskBCELoss 0.0000 (0.2104)	MaskDICELoss 0.0000 (0.4749)
Epoch: [2][ 54/500]	Time  8.359 ( 8.359)	Loss 1.0391 (5.0589)	CeLoss 1.0391 (0.7237)	SegCLSLoss 0.0000 (0.0165)	KLLoss 0.0000 (0.1110)	MaskLoss 0.0000 (0.6761)	MaskBCELoss 0.0000 (0.1988)	MaskDICELoss 0.0000 (0.4773)
Epoch: [2][ 55/500]	Time 10.018 (10.018)	Loss 6.5482 (6.3098)	CeLoss 0.2461 (0.2576)	SegCLSLoss 0.0201 (0.0235)	KLLoss 0.1611 (0.1511)	MaskLoss 1.1592 (1.0106)	MaskBCELoss 0.5239 (0.3660)	MaskDICELoss 0.6353 (0.6446)
Epoch: [2][ 56/500]	Time  9.538 ( 9.538)	Loss 0.7188 (5.6227)	CeLoss 0.7188 (0.3037)	SegCLSLoss 0.0000 (0.0216)	KLLoss 0.0000 (0.1437)	MaskLoss 0.0000 (0.9226)	MaskBCELoss 0.0000 (0.3695)	MaskDICELoss 0.0000 (0.5532)
Epoch: [2][ 57/500]	Time  8.797 ( 8.797)	Loss 8.8711 (5.1537)	CeLoss 0.2656 (0.4895)	SegCLSLoss 0.0325 (0.0238)	KLLoss 0.2246 (0.1277)	MaskLoss 1.5172 (0.7675)	MaskBCELoss 0.6291 (0.2691)	MaskDICELoss 0.8882 (0.4983)
Epoch: [2][ 58/500]	Time 10.821 (10.821)	Loss 1.3516 (5.4937)	CeLoss 1.3516 (0.3638)	SegCLSLoss 0.0000 (0.0179)	KLLoss 0.0000 (0.1159)	MaskLoss 0.0000 (0.8333)	MaskBCELoss 0.0000 (0.2769)	MaskDICELoss 0.0000 (0.5564)
Epoch: [2][ 59/500]	Time  9.193 ( 9.193)	Loss 0.7188 (4.2336)	CeLoss 0.7188 (0.5096)	SegCLSLoss 0.0000 (0.0140)	KLLoss 0.0000 (0.0947)	MaskLoss 0.0000 (0.6095)	MaskBCELoss 0.0000 (0.2089)	MaskDICELoss 0.0000 (0.4006)
[2025-03-04 05:06:34,422] [INFO] [logging.py:96:log_dist] [Rank 0] step=1060, skipped=0, lr=[0.00027572289156626504], mom=[(0.9, 0.95)]
[2025-03-04 05:06:34,428] [INFO] [timer.py:215:stop] epoch=0/micro_step=10600/global_step=1060, RunningAvgSamplesPerSec=1.0328803438755743, CurrSamplesPerSec=1.248498464955292, MemAllocated=57.27GB, MaxMemAllocated=62.82GB
Epoch: [2][ 60/500]	Time  8.012 ( 8.012)	Loss 9.1648 (4.2467)	CeLoss 0.2041 (0.5523)	SegCLSLoss 0.0322 (0.0160)	KLLoss 0.2188 (0.1026)	MaskLoss 1.4294 (0.6034)	MaskBCELoss 0.4517 (0.2072)	MaskDICELoss 0.9777 (0.3961)
Epoch: [2][ 61/500]	Time 10.541 (10.541)	Loss 1.3438 (6.2411)	CeLoss 1.3438 (0.3446)	SegCLSLoss 0.0000 (0.0224)	KLLoss 0.0000 (0.1492)	MaskLoss 0.0000 (0.9269)	MaskBCELoss 0.0000 (0.2799)	MaskDICELoss 0.0000 (0.6470)
Epoch: [2][ 62/500]	Time  9.852 ( 9.852)	Loss 3.4208 (5.7228)	CeLoss 0.2637 (0.2174)	SegCLSLoss 0.0166 (0.0251)	KLLoss 0.1123 (0.1649)	MaskLoss 0.4775 (0.7789)	MaskBCELoss 0.1303 (0.1505)	MaskDICELoss 0.3472 (0.6284)
Epoch: [2][ 63/500]	Time  9.978 ( 9.978)	Loss 4.7476 (5.6672)	CeLoss 0.2676 (0.3493)	SegCLSLoss 0.0171 (0.0182)	KLLoss 0.1309 (0.1373)	MaskLoss 0.5926 (0.8819)	MaskBCELoss 0.0666 (0.3140)	MaskDICELoss 0.5260 (0.5680)
Epoch: [2][ 64/500]	Time 10.027 (10.027)	Loss 7.1991 (4.9890)	CeLoss 0.2402 (0.3680)	SegCLSLoss 0.0283 (0.0186)	KLLoss 0.2168 (0.1304)	MaskLoss 1.1289 (0.7787)	MaskBCELoss 0.3838 (0.2914)	MaskDICELoss 0.7451 (0.4872)
Epoch: [2][ 65/500]	Time  9.180 ( 9.180)	Loss 6.9416 (4.4737)	CeLoss 0.1904 (0.5115)	SegCLSLoss 0.0349 (0.0152)	KLLoss 0.2080 (0.0989)	MaskLoss 1.1877 (0.5629)	MaskBCELoss 0.4961 (0.1080)	MaskDICELoss 0.6917 (0.4550)
Epoch: [2][ 66/500]	Time  9.967 ( 9.967)	Loss 8.8843 (6.4280)	CeLoss 0.2002 (0.3759)	SegCLSLoss 0.0476 (0.0216)	KLLoss 0.2227 (0.1517)	MaskLoss 1.3864 (0.9031)	MaskBCELoss 0.4420 (0.2226)	MaskDICELoss 0.9444 (0.6806)
Epoch: [2][ 67/500]	Time  9.380 ( 9.380)	Loss 6.1572 (5.0898)	CeLoss 0.4121 (0.3960)	SegCLSLoss 0.0223 (0.0230)	KLLoss 0.1157 (0.1359)	MaskLoss 0.9421 (0.7422)	MaskBCELoss 0.3198 (0.2318)	MaskDICELoss 0.6223 (0.5104)
Epoch: [2][ 68/500]	Time  8.515 ( 8.515)	Loss 4.8905 (4.1979)	CeLoss 0.2871 (0.3050)	SegCLSLoss 0.0179 (0.0157)	KLLoss 0.1299 (0.1180)	MaskLoss 0.6573 (0.5838)	MaskBCELoss 0.1323 (0.1506)	MaskDICELoss 0.5250 (0.4332)
Epoch: [2][ 69/500]	Time  9.960 ( 9.960)	Loss 7.2839 (5.7439)	CeLoss 0.2695 (0.3692)	SegCLSLoss 0.0157 (0.0174)	KLLoss 0.1328 (0.1250)	MaskLoss 0.9291 (0.7569)	MaskBCELoss 0.0932 (0.1356)	MaskDICELoss 0.8359 (0.6212)
[2025-03-04 05:08:11,076] [INFO] [logging.py:96:log_dist] [Rank 0] step=1070, skipped=0, lr=[0.00027548192771084337], mom=[(0.9, 0.95)]
[2025-03-04 05:08:11,082] [INFO] [timer.py:215:stop] epoch=0/micro_step=10700/global_step=1070, RunningAvgSamplesPerSec=1.0328983020368743, CurrSamplesPerSec=1.0808861737722524, MemAllocated=57.46GB, MaxMemAllocated=62.82GB
Epoch: [2][ 70/500]	Time  9.254 ( 9.254)	Loss 5.7058 (4.4996)	CeLoss 0.2275 (0.6073)	SegCLSLoss 0.0239 (0.0170)	KLLoss 0.1943 (0.1113)	MaskLoss 0.7058 (0.6121)	MaskBCELoss 0.0624 (0.1874)	MaskDICELoss 0.6434 (0.4247)
Epoch: [2][ 71/500]	Time 10.837 (10.837)	Loss 7.1242 (5.3180)	CeLoss 0.2617 (0.2357)	SegCLSLoss 0.0178 (0.0213)	KLLoss 0.1494 (0.1582)	MaskLoss 0.8775 (0.7246)	MaskBCELoss 0.0530 (0.1471)	MaskDICELoss 0.8245 (0.5774)
Epoch: [2][ 72/500]	Time  8.711 ( 8.711)	Loss 8.2760 (7.2656)	CeLoss 0.1328 (0.4088)	SegCLSLoss 0.0659 (0.0280)	KLLoss 0.2402 (0.1575)	MaskLoss 1.3412 (0.9716)	MaskBCELoss 0.4766 (0.1813)	MaskDICELoss 0.8646 (0.7903)
Epoch: [2][ 73/500]	Time  7.851 ( 7.851)	Loss 7.2469 (4.7185)	CeLoss 0.3340 (0.4740)	SegCLSLoss 0.0295 (0.0141)	KLLoss 0.2061 (0.0959)	MaskLoss 1.1956 (0.6455)	MaskBCELoss 0.4787 (0.1705)	MaskDICELoss 0.7168 (0.4751)
Epoch: [2][ 74/500]	Time  7.649 ( 7.649)	Loss 8.3755 (5.6992)	CeLoss 0.2930 (0.5345)	SegCLSLoss 0.0269 (0.0172)	KLLoss 0.1641 (0.1085)	MaskLoss 1.9095 (1.0189)	MaskBCELoss 1.2286 (0.5172)	MaskDICELoss 0.6810 (0.5016)
Epoch: [2][ 75/500]	Time 10.302 (10.302)	Loss 6.6051 (5.6620)	CeLoss 0.2969 (0.4471)	SegCLSLoss 0.0168 (0.0175)	KLLoss 0.1387 (0.1270)	MaskLoss 1.1453 (0.7532)	MaskBCELoss 0.5002 (0.1578)	MaskDICELoss 0.6452 (0.5955)
Epoch: [2][ 76/500]	Time  8.266 ( 8.266)	Loss 3.6977 (4.9574)	CeLoss 0.2461 (0.4688)	SegCLSLoss 0.0138 (0.0147)	KLLoss 0.1138 (0.1108)	MaskLoss 0.7624 (0.6925)	MaskBCELoss 0.4614 (0.1949)	MaskDICELoss 0.3010 (0.4976)
Epoch: [2][ 77/500]	Time  7.374 ( 7.374)	Loss 5.3683 (4.8853)	CeLoss 0.2559 (0.5490)	SegCLSLoss 0.0179 (0.0162)	KLLoss 0.1426 (0.1084)	MaskLoss 0.8155 (0.6333)	MaskBCELoss 0.2607 (0.1411)	MaskDICELoss 0.5548 (0.4922)
Epoch: [2][ 78/500]	Time  7.228 ( 7.228)	Loss 1.6406 (2.9602)	CeLoss 1.6406 (0.8851)	SegCLSLoss 0.0000 (0.0085)	KLLoss 0.0000 (0.0600)	MaskLoss 0.0000 (0.3292)	MaskBCELoss 0.0000 (0.1038)	MaskDICELoss 0.0000 (0.2254)
Epoch: [2][ 79/500]	Time  8.436 ( 8.436)	Loss 1.1406 (2.8394)	CeLoss 1.1406 (0.7375)	SegCLSLoss 0.0000 (0.0068)	KLLoss 0.0000 (0.0446)	MaskLoss 0.0000 (0.3094)	MaskBCELoss 0.0000 (0.0703)	MaskDICELoss 0.0000 (0.2392)
[2025-03-04 05:09:38,106] [INFO] [logging.py:96:log_dist] [Rank 0] step=1080, skipped=0, lr=[0.00027524096385542164], mom=[(0.9, 0.95)]
[2025-03-04 05:09:38,112] [INFO] [timer.py:215:stop] epoch=0/micro_step=10800/global_step=1080, RunningAvgSamplesPerSec=1.033869278068462, CurrSamplesPerSec=0.9639100675959015, MemAllocated=57.27GB, MaxMemAllocated=62.82GB
Epoch: [2][ 80/500]	Time 10.376 (10.376)	Loss 7.8034 (4.9304)	CeLoss 0.1895 (0.3475)	SegCLSLoss 0.0330 (0.0197)	KLLoss 0.1895 (0.1344)	MaskLoss 1.3302 (0.7511)	MaskBCELoss 0.5388 (0.2617)	MaskDICELoss 0.7914 (0.4894)
Epoch: [2][ 81/500]	Time  8.605 ( 8.605)	Loss 1.6719 (4.7863)	CeLoss 1.6719 (0.5290)	SegCLSLoss 0.0000 (0.0199)	KLLoss 0.0000 (0.1099)	MaskLoss 0.0000 (0.6609)	MaskBCELoss 0.0000 (0.1916)	MaskDICELoss 0.0000 (0.4693)
Epoch: [2][ 82/500]	Time 10.008 (10.008)	Loss 8.1072 (5.5144)	CeLoss 0.1846 (0.2587)	SegCLSLoss 0.0311 (0.0217)	KLLoss 0.2139 (0.1520)	MaskLoss 1.2581 (0.7852)	MaskBCELoss 0.3953 (0.1982)	MaskDICELoss 0.8628 (0.5870)
Epoch: [2][ 83/500]	Time 11.510 (11.510)	Loss 6.1872 (6.7860)	CeLoss 0.2119 (0.2412)	SegCLSLoss 0.0216 (0.0232)	KLLoss 0.1592 (0.1665)	MaskLoss 0.7630 (1.0249)	MaskBCELoss 0.0499 (0.3054)	MaskDICELoss 0.7131 (0.7195)
Epoch: [2][ 84/500]	Time  8.285 ( 8.285)	Loss 4.9445 (6.4718)	CeLoss 0.3750 (0.3679)	SegCLSLoss 0.0164 (0.0236)	KLLoss 0.1309 (0.1608)	MaskLoss 0.8643 (0.9900)	MaskBCELoss 0.4143 (0.3314)	MaskDICELoss 0.4500 (0.6586)
Epoch: [2][ 85/500]	Time 10.473 (10.473)	Loss 1.2031 (5.9070)	CeLoss 1.2031 (0.4331)	SegCLSLoss 0.0000 (0.0182)	KLLoss 0.0000 (0.1325)	MaskLoss 0.0000 (0.8062)	MaskBCELoss 0.0000 (0.1862)	MaskDICELoss 0.0000 (0.6199)
Epoch: [2][ 86/500]	Time  8.925 ( 8.925)	Loss 7.2499 (4.4670)	CeLoss 0.2090 (0.7455)	SegCLSLoss 0.0280 (0.0141)	KLLoss 0.1934 (0.0985)	MaskLoss 0.9343 (0.5126)	MaskBCELoss 0.1067 (0.0807)	MaskDICELoss 0.8276 (0.4319)
Epoch: [2][ 87/500]	Time  9.230 ( 9.230)	Loss 7.4818 (5.6491)	CeLoss 0.2031 (0.3937)	SegCLSLoss 0.0253 (0.0219)	KLLoss 0.1787 (0.1466)	MaskLoss 0.9239 (0.7516)	MaskBCELoss 0.0507 (0.1525)	MaskDICELoss 0.8732 (0.5991)
Epoch: [2][ 88/500]	Time  7.730 ( 7.730)	Loss 7.4473 (4.4184)	CeLoss 0.1592 (0.4358)	SegCLSLoss 0.0503 (0.0214)	KLLoss 0.2197 (0.1218)	MaskLoss 1.2468 (0.6296)	MaskBCELoss 0.4886 (0.1979)	MaskDICELoss 0.7582 (0.4318)
Epoch: [2][ 89/500]	Time  8.484 ( 8.484)	Loss 7.7714 (5.2748)	CeLoss 0.2412 (0.5806)	SegCLSLoss 0.0410 (0.0230)	KLLoss 0.2207 (0.1312)	MaskLoss 1.1467 (0.6913)	MaskBCELoss 0.3140 (0.1632)	MaskDICELoss 0.8326 (0.5282)
[2025-03-04 05:11:09,440] [INFO] [logging.py:96:log_dist] [Rank 0] step=1090, skipped=0, lr=[0.00027499999999999996], mom=[(0.9, 0.95)]
[2025-03-04 05:11:09,446] [INFO] [timer.py:215:stop] epoch=0/micro_step=10900/global_step=1090, RunningAvgSamplesPerSec=1.0344007017974348, CurrSamplesPerSec=1.2374789936643837, MemAllocated=57.26GB, MaxMemAllocated=62.82GB
Epoch: [2][ 90/500]	Time  8.083 ( 8.083)	Loss 8.3250 (5.5914)	CeLoss 0.1836 (0.4211)	SegCLSLoss 0.0283 (0.0203)	KLLoss 0.1855 (0.1387)	MaskLoss 1.0343 (0.7340)	MaskBCELoss 0.0554 (0.1417)	MaskDICELoss 0.9789 (0.5923)
Epoch: [2][ 91/500]	Time 10.700 (10.700)	Loss 7.9983 (6.6614)	CeLoss 0.1670 (0.2453)	SegCLSLoss 0.0425 (0.0256)	KLLoss 0.2207 (0.1633)	MaskLoss 1.2793 (0.9505)	MaskBCELoss 0.4410 (0.2273)	MaskDICELoss 0.8383 (0.7232)
Epoch: [2][ 92/500]	Time  9.992 ( 9.992)	Loss 8.1583 (6.3152)	CeLoss 0.2871 (0.4708)	SegCLSLoss 0.0364 (0.0193)	KLLoss 0.2314 (0.1310)	MaskLoss 0.9765 (1.0030)	MaskBCELoss 0.0315 (0.3866)	MaskDICELoss 0.9450 (0.6163)
Epoch: [2][ 93/500]	Time 10.222 (10.222)	Loss 0.7695 (4.4468)	CeLoss 0.7695 (0.4997)	SegCLSLoss 0.0000 (0.0168)	KLLoss 0.0000 (0.0975)	MaskLoss 0.0000 (0.5616)	MaskBCELoss 0.0000 (0.1087)	MaskDICELoss 0.0000 (0.4529)
Epoch: [2][ 94/500]	Time  9.953 ( 9.953)	Loss 6.0538 (5.1594)	CeLoss 0.2070 (0.3547)	SegCLSLoss 0.0361 (0.0198)	KLLoss 0.2012 (0.1409)	MaskLoss 1.0829 (0.7251)	MaskBCELoss 0.5059 (0.1912)	MaskDICELoss 0.5770 (0.5339)
Epoch: [2][ 95/500]	Time  9.024 ( 9.024)	Loss 7.1080 (5.2154)	CeLoss 0.3066 (0.5646)	SegCLSLoss 0.0144 (0.0150)	KLLoss 0.0771 (0.0972)	MaskLoss 1.1365 (0.7668)	MaskBCELoss 0.3957 (0.2646)	MaskDICELoss 0.7407 (0.5021)
Epoch: [2][ 96/500]	Time  9.252 ( 9.252)	Loss 7.0325 (5.4707)	CeLoss 0.2041 (0.5549)	SegCLSLoss 0.0242 (0.0185)	KLLoss 0.1768 (0.1165)	MaskLoss 0.8943 (0.7847)	MaskBCELoss 0.0857 (0.2479)	MaskDICELoss 0.8086 (0.5368)
Epoch: [2][ 97/500]	Time  7.763 ( 7.763)	Loss 7.7113 (5.2965)	CeLoss 0.3008 (0.5211)	SegCLSLoss 0.0183 (0.0154)	KLLoss 0.1455 (0.1052)	MaskLoss 1.5064 (0.7408)	MaskBCELoss 0.7995 (0.2107)	MaskDICELoss 0.7069 (0.5301)
Epoch: [2][ 98/500]	Time  8.590 ( 8.590)	Loss 7.4696 (5.2841)	CeLoss 0.1553 (0.5434)	SegCLSLoss 0.0461 (0.0261)	KLLoss 0.2246 (0.1380)	MaskLoss 1.1081 (0.7136)	MaskBCELoss 0.2996 (0.1865)	MaskDICELoss 0.8085 (0.5271)
Epoch: [2][ 99/500]	Time  9.193 ( 9.193)	Loss 3.2001 (5.8228)	CeLoss 0.2715 (0.3468)	SegCLSLoss 0.0146 (0.0216)	KLLoss 0.1177 (0.1458)	MaskLoss 0.4116 (0.8013)	MaskBCELoss 0.0815 (0.1818)	MaskDICELoss 0.3301 (0.6195)
[2025-03-04 05:12:42,237] [INFO] [logging.py:96:log_dist] [Rank 0] step=1100, skipped=0, lr=[0.0002747590361445783], mom=[(0.9, 0.95)]
[2025-03-04 05:12:42,243] [INFO] [timer.py:215:stop] epoch=0/micro_step=11000/global_step=1100, RunningAvgSamplesPerSec=1.0347806434511249, CurrSamplesPerSec=1.2335698435503297, MemAllocated=56.81GB, MaxMemAllocated=62.82GB
Epoch: [2][100/500]	Time  8.109 ( 8.109)	Loss 7.7119 (3.9659)	CeLoss 0.2090 (0.6439)	SegCLSLoss 0.0242 (0.0187)	KLLoss 0.1436 (0.0964)	MaskLoss 1.1986 (0.5056)	MaskBCELoss 0.3733 (0.1381)	MaskDICELoss 0.8253 (0.3675)
Epoch: [2][101/500]	Time 10.496 (10.496)	Loss 6.3293 (6.0599)	CeLoss 0.2490 (0.2694)	SegCLSLoss 0.0160 (0.0182)	KLLoss 0.1309 (0.1267)	MaskLoss 0.8846 (0.8495)	MaskBCELoss 0.1894 (0.1903)	MaskDICELoss 0.6952 (0.6592)
Epoch: [2][102/500]	Time  9.312 ( 9.312)	Loss 1.7578 (5.4258)	CeLoss 1.7578 (0.5330)	SegCLSLoss 0.0000 (0.0154)	KLLoss 0.0000 (0.0994)	MaskLoss 0.0000 (0.7339)	MaskBCELoss 0.0000 (0.1809)	MaskDICELoss 0.0000 (0.5530)
Epoch: [2][103/500]	Time  8.715 ( 8.715)	Loss 0.9727 (5.1803)	CeLoss 0.9727 (0.5841)	SegCLSLoss 0.0000 (0.0161)	KLLoss 0.0000 (0.0979)	MaskLoss 0.0000 (0.6830)	MaskBCELoss 0.0000 (0.1623)	MaskDICELoss 0.0000 (0.5207)
Epoch: [2][104/500]	Time  9.541 ( 9.541)	Loss 5.8140 (5.8598)	CeLoss 0.2129 (0.2230)	SegCLSLoss 0.0330 (0.0240)	KLLoss 0.1934 (0.1463)	MaskLoss 1.0956 (0.9223)	MaskBCELoss 0.5621 (0.3166)	MaskDICELoss 0.5335 (0.6057)
Epoch: [2][105/500]	Time  9.313 ( 9.313)	Loss 7.5919 (6.0114)	CeLoss 0.1992 (0.4545)	SegCLSLoss 0.0200 (0.0252)	KLLoss 0.1758 (0.1493)	MaskLoss 0.9968 (0.8097)	MaskBCELoss 0.1278 (0.1804)	MaskDICELoss 0.8689 (0.6293)
Epoch: [2][106/500]	Time  9.748 ( 9.748)	Loss 0.7539 (4.5914)	CeLoss 0.7539 (0.3546)	SegCLSLoss 0.0000 (0.0192)	KLLoss 0.0000 (0.1104)	MaskLoss 0.0000 (0.6105)	MaskBCELoss 0.0000 (0.1280)	MaskDICELoss 0.0000 (0.4826)
Epoch: [2][107/500]	Time  9.302 ( 9.302)	Loss 1.8828 (4.6213)	CeLoss 1.8828 (0.7075)	SegCLSLoss 0.0000 (0.0148)	KLLoss 0.0000 (0.0931)	MaskLoss 0.0000 (0.5606)	MaskBCELoss 0.0000 (0.1120)	MaskDICELoss 0.0000 (0.4486)
Epoch: [2][108/500]	Time  8.617 ( 8.617)	Loss 1.8516 (4.5954)	CeLoss 1.8516 (0.7568)	SegCLSLoss 0.0000 (0.0132)	KLLoss 0.0000 (0.0958)	MaskLoss 0.0000 (0.5342)	MaskBCELoss 0.0000 (0.0896)	MaskDICELoss 0.0000 (0.4446)
Epoch: [2][109/500]	Time 10.826 (10.826)	Loss 7.6139 (6.2958)	CeLoss 0.2227 (0.2380)	SegCLSLoss 0.0366 (0.0251)	KLLoss 0.2217 (0.1484)	MaskLoss 1.0471 (0.9051)	MaskBCELoss 0.2043 (0.2241)	MaskDICELoss 0.8428 (0.6810)
[2025-03-04 05:14:17,534] [INFO] [logging.py:96:log_dist] [Rank 0] step=1110, skipped=0, lr=[0.0002745180722891566], mom=[(0.9, 0.95)]
[2025-03-04 05:14:17,539] [INFO] [timer.py:215:stop] epoch=0/micro_step=11100/global_step=1110, RunningAvgSamplesPerSec=1.0349121135321397, CurrSamplesPerSec=1.0611201915119877, MemAllocated=56.95GB, MaxMemAllocated=62.82GB
Epoch: [2][110/500]	Time  9.426 ( 9.426)	Loss 5.6134 (5.9716)	CeLoss 0.3301 (0.3591)	SegCLSLoss 0.0184 (0.0181)	KLLoss 0.0942 (0.1113)	MaskLoss 0.8687 (0.7695)	MaskBCELoss 0.2949 (0.1107)	MaskDICELoss 0.5737 (0.6588)
Epoch: [2][111/500]	Time  9.290 ( 9.290)	Loss 3.6375 (5.2270)	CeLoss 0.2139 (0.4375)	SegCLSLoss 0.0203 (0.0239)	KLLoss 0.1494 (0.1341)	MaskLoss 0.7790 (0.7458)	MaskBCELoss 0.4946 (0.2205)	MaskDICELoss 0.2844 (0.5253)
Epoch: [2][112/500]	Time  8.597 ( 8.597)	Loss 7.3211 (4.4691)	CeLoss 0.2012 (0.2719)	SegCLSLoss 0.0277 (0.0185)	KLLoss 0.2080 (0.1271)	MaskLoss 0.9644 (0.6547)	MaskBCELoss 0.1360 (0.1962)	MaskDICELoss 0.8284 (0.4586)
Epoch: [2][113/500]	Time  6.449 ( 6.449)	Loss 0.8711 (3.6324)	CeLoss 0.8711 (0.7781)	SegCLSLoss 0.0000 (0.0082)	KLLoss 0.0000 (0.0595)	MaskLoss 0.0000 (0.6833)	MaskBCELoss 0.0000 (0.4460)	MaskDICELoss 0.0000 (0.2373)
Epoch: [2][114/500]	Time 10.952 (10.952)	Loss 8.4516 (6.4870)	CeLoss 0.2539 (0.3595)	SegCLSLoss 0.0151 (0.0210)	KLLoss 0.0854 (0.1376)	MaskLoss 1.0541 (0.8983)	MaskBCELoss 0.0548 (0.2012)	MaskDICELoss 0.9993 (0.6970)
Epoch: [2][115/500]	Time 10.793 (10.793)	Loss 8.1817 (6.5365)	CeLoss 0.2852 (0.2376)	SegCLSLoss 0.0239 (0.0270)	KLLoss 0.1660 (0.1690)	MaskLoss 1.0349 (0.8992)	MaskBCELoss 0.0933 (0.1796)	MaskDICELoss 0.9415 (0.7196)
Epoch: [2][116/500]	Time 10.569 (10.569)	Loss 7.9036 (5.5345)	CeLoss 0.1875 (0.2885)	SegCLSLoss 0.0255 (0.0200)	KLLoss 0.1621 (0.1321)	MaskLoss 1.0583 (0.9023)	MaskBCELoss 0.1543 (0.3524)	MaskDICELoss 0.9040 (0.5499)
Epoch: [2][117/500]	Time 10.276 (10.276)	Loss 1.4766 (4.5997)	CeLoss 1.4766 (0.4475)	SegCLSLoss 0.0000 (0.0163)	KLLoss 0.0000 (0.1104)	MaskLoss 0.0000 (0.5678)	MaskBCELoss 0.0000 (0.0848)	MaskDICELoss 0.0000 (0.4830)
Epoch: [2][118/500]	Time  9.334 ( 9.334)	Loss 8.2154 (5.7750)	CeLoss 0.1992 (0.5256)	SegCLSLoss 0.0298 (0.0228)	KLLoss 0.1895 (0.1452)	MaskLoss 1.2244 (0.7978)	MaskBCELoss 0.3303 (0.2148)	MaskDICELoss 0.8941 (0.5829)
Epoch: [2][119/500]	Time  8.297 ( 8.297)	Loss 0.9453 (2.7803)	CeLoss 0.9453 (0.5756)	SegCLSLoss 0.0000 (0.0097)	KLLoss 0.0000 (0.0560)	MaskLoss 0.0000 (0.3688)	MaskBCELoss 0.0000 (0.1343)	MaskDICELoss 0.0000 (0.2344)
[2025-03-04 05:15:49,583] [INFO] [logging.py:96:log_dist] [Rank 0] step=1120, skipped=0, lr=[0.00027427710843373494], mom=[(0.9, 0.95)]
[2025-03-04 05:15:49,589] [INFO] [timer.py:215:stop] epoch=0/micro_step=11200/global_step=1120, RunningAvgSamplesPerSec=1.0353522970528086, CurrSamplesPerSec=1.3351221102542488, MemAllocated=57.26GB, MaxMemAllocated=62.82GB
Epoch: [2][120/500]	Time  7.492 ( 7.492)	Loss 5.8499 (3.1234)	CeLoss 0.3047 (0.6095)	SegCLSLoss 0.0161 (0.0106)	KLLoss 0.1592 (0.0757)	MaskLoss 0.9408 (0.3847)	MaskBCELoss 0.3582 (0.1074)	MaskDICELoss 0.5826 (0.2773)
Epoch: [2][121/500]	Time 10.544 (10.544)	Loss 4.9933 (5.3423)	CeLoss 0.4746 (0.3627)	SegCLSLoss 0.0168 (0.0172)	KLLoss 0.1240 (0.1096)	MaskLoss 0.6352 (0.7550)	MaskBCELoss 0.1156 (0.1965)	MaskDICELoss 0.5196 (0.5585)
Epoch: [2][122/500]	Time  8.957 ( 8.957)	Loss 6.6168 (5.1488)	CeLoss 0.2324 (0.5021)	SegCLSLoss 0.0219 (0.0164)	KLLoss 0.1455 (0.1140)	MaskLoss 0.8783 (0.7352)	MaskBCELoss 0.1331 (0.2262)	MaskDICELoss 0.7452 (0.5090)
Epoch: [2][123/500]	Time  9.646 ( 9.646)	Loss 7.7269 (5.0061)	CeLoss 0.4434 (0.4424)	SegCLSLoss 0.0153 (0.0186)	KLLoss 0.1279 (0.1332)	MaskLoss 1.0457 (0.6531)	MaskBCELoss 0.2029 (0.1340)	MaskDICELoss 0.8429 (0.5191)
Epoch: [2][124/500]	Time  7.874 ( 7.874)	Loss 7.0682 (3.7310)	CeLoss 0.2129 (0.8512)	SegCLSLoss 0.0216 (0.0129)	KLLoss 0.1660 (0.0760)	MaskLoss 0.8514 (0.4510)	MaskBCELoss 0.0223 (0.1351)	MaskDICELoss 0.8291 (0.3159)
Epoch: [2][125/500]	Time 10.978 (10.978)	Loss 7.8053 (5.2778)	CeLoss 0.1738 (0.3099)	SegCLSLoss 0.0374 (0.0266)	KLLoss 0.2070 (0.1489)	MaskLoss 1.4042 (0.7422)	MaskBCELoss 0.6377 (0.1886)	MaskDICELoss 0.7664 (0.5536)
Epoch: [2][126/500]	Time  7.288 ( 7.288)	Loss 6.5436 (4.7723)	CeLoss 0.2578 (0.6246)	SegCLSLoss 0.0156 (0.0171)	KLLoss 0.1187 (0.1063)	MaskLoss 1.1924 (0.6443)	MaskBCELoss 0.5634 (0.1869)	MaskDICELoss 0.6290 (0.4574)
Epoch: [2][127/500]	Time 11.103 (11.103)	Loss 6.6898 (5.6836)	CeLoss 0.2031 (0.2015)	SegCLSLoss 0.0317 (0.0221)	KLLoss 0.1992 (0.1460)	MaskLoss 0.8195 (0.7979)	MaskBCELoss 0.0474 (0.1764)	MaskDICELoss 0.7721 (0.6216)
Epoch: [2][128/500]	Time 11.219 (11.219)	Loss 7.2631 (5.1007)	CeLoss 0.2676 (0.3940)	SegCLSLoss 0.0162 (0.0156)	KLLoss 0.1162 (0.1029)	MaskLoss 1.0180 (0.7365)	MaskBCELoss 0.2119 (0.2160)	MaskDICELoss 0.8061 (0.5205)
Epoch: [2][129/500]	Time  9.413 ( 9.413)	Loss 0.0771 (5.0069)	CeLoss 0.0771 (0.4020)	SegCLSLoss 0.0000 (0.0161)	KLLoss 0.0000 (0.1089)	MaskLoss 0.0000 (0.6616)	MaskBCELoss 0.0000 (0.1341)	MaskDICELoss 0.0000 (0.5275)
[2025-03-04 05:17:27,664] [INFO] [logging.py:96:log_dist] [Rank 0] step=1130, skipped=0, lr=[0.0002740361445783132], mom=[(0.9, 0.95)]
[2025-03-04 05:17:27,670] [INFO] [timer.py:215:stop] epoch=0/micro_step=11300/global_step=1130, RunningAvgSamplesPerSec=1.0352118243163992, CurrSamplesPerSec=0.9043241174924681, MemAllocated=56.74GB, MaxMemAllocated=62.82GB
Epoch: [2][130/500]	Time 11.060 (11.060)	Loss 1.1016 (6.3649)	CeLoss 1.1016 (0.3101)	SegCLSLoss 0.0000 (0.0244)	KLLoss 0.0000 (0.1594)	MaskLoss 0.0000 (0.9699)	MaskBCELoss 0.0000 (0.3126)	MaskDICELoss 0.0000 (0.6573)
Epoch: [2][131/500]	Time  9.095 ( 9.095)	Loss 1.4844 (4.2116)	CeLoss 1.4844 (0.5306)	SegCLSLoss 0.0000 (0.0136)	KLLoss 0.0000 (0.0890)	MaskLoss 0.0000 (0.5714)	MaskBCELoss 0.0000 (0.1643)	MaskDICELoss 0.0000 (0.4071)
Epoch: [2][132/500]	Time  7.927 ( 7.927)	Loss 7.4399 (5.8675)	CeLoss 0.1748 (0.5418)	SegCLSLoss 0.0292 (0.0174)	KLLoss 0.1885 (0.1178)	MaskLoss 0.9418 (0.8965)	MaskBCELoss 0.0789 (0.3287)	MaskDICELoss 0.8629 (0.5678)
Epoch: [2][133/500]	Time  9.935 ( 9.935)	Loss 7.3902 (3.9713)	CeLoss 0.3047 (0.5166)	SegCLSLoss 0.0181 (0.0152)	KLLoss 0.1279 (0.1026)	MaskLoss 1.1420 (0.5192)	MaskBCELoss 0.3645 (0.1349)	MaskDICELoss 0.7775 (0.3843)
Epoch: [2][134/500]	Time  8.746 ( 8.746)	Loss 8.3316 (5.5523)	CeLoss 0.1777 (0.5189)	SegCLSLoss 0.0332 (0.0204)	KLLoss 0.2051 (0.1477)	MaskLoss 1.1586 (0.7200)	MaskBCELoss 0.2226 (0.1473)	MaskDICELoss 0.9360 (0.5727)
Epoch: [2][135/500]	Time 10.207 (10.207)	Loss 1.6328 (5.0271)	CeLoss 1.6328 (0.4310)	SegCLSLoss 0.0000 (0.0179)	KLLoss 0.0000 (0.1286)	MaskLoss 0.0000 (0.6414)	MaskBCELoss 0.0000 (0.1121)	MaskDICELoss 0.0000 (0.5293)
Epoch: [2][136/500]	Time  9.713 ( 9.713)	Loss 7.0762 (5.3426)	CeLoss 0.2070 (0.3666)	SegCLSLoss 0.0259 (0.0171)	KLLoss 0.1934 (0.1239)	MaskLoss 1.2827 (0.7240)	MaskBCELoss 0.6000 (0.1581)	MaskDICELoss 0.6828 (0.5659)
Epoch: [2][137/500]	Time  9.306 ( 9.306)	Loss 1.7500 (4.8747)	CeLoss 1.7500 (0.6111)	SegCLSLoss 0.0000 (0.0166)	KLLoss 0.0000 (0.1089)	MaskLoss 0.0000 (0.6350)	MaskBCELoss 0.0000 (0.1555)	MaskDICELoss 0.0000 (0.4794)
Epoch: [2][138/500]	Time  9.707 ( 9.707)	Loss 6.1907 (5.8005)	CeLoss 0.1885 (0.3536)	SegCLSLoss 0.0223 (0.0174)	KLLoss 0.1416 (0.1215)	MaskLoss 1.2111 (0.9493)	MaskBCELoss 0.6396 (0.3796)	MaskDICELoss 0.5714 (0.5697)
Epoch: [2][139/500]	Time  9.423 ( 9.423)	Loss 7.8144 (6.4428)	CeLoss 0.2734 (0.2928)	SegCLSLoss 0.0255 (0.0239)	KLLoss 0.1602 (0.1592)	MaskLoss 1.0086 (1.1303)	MaskBCELoss 0.1170 (0.5105)	MaskDICELoss 0.8916 (0.6198)
[2025-03-04 05:19:02,271] [INFO] [logging.py:96:log_dist] [Rank 0] step=1140, skipped=0, lr=[0.00027379518072289153], mom=[(0.9, 0.95)]
[2025-03-04 05:19:02,277] [INFO] [timer.py:215:stop] epoch=0/micro_step=11400/global_step=1140, RunningAvgSamplesPerSec=1.0354010498723294, CurrSamplesPerSec=0.9483641181126543, MemAllocated=57.26GB, MaxMemAllocated=62.82GB
Epoch: [2][140/500]	Time 10.547 (10.547)	Loss 6.9303 (5.6955)	CeLoss 0.1924 (0.3453)	SegCLSLoss 0.0248 (0.0218)	KLLoss 0.1807 (0.1496)	MaskLoss 0.9200 (0.7620)	MaskBCELoss 0.1360 (0.1511)	MaskDICELoss 0.7839 (0.6109)
Epoch: [2][141/500]	Time  9.682 ( 9.682)	Loss 7.7889 (6.5451)	CeLoss 0.1943 (0.5291)	SegCLSLoss 0.0244 (0.0261)	KLLoss 0.1699 (0.1622)	MaskLoss 0.9573 (0.8791)	MaskBCELoss 0.0411 (0.1988)	MaskDICELoss 0.9162 (0.6803)
Epoch: [2][142/500]	Time 10.305 (10.305)	Loss 1.1250 (4.1953)	CeLoss 1.1250 (0.5251)	SegCLSLoss 0.0000 (0.0167)	KLLoss 0.0000 (0.1045)	MaskLoss 0.0000 (0.5585)	MaskBCELoss 0.0000 (0.1517)	MaskDICELoss 0.0000 (0.4068)
Epoch: [2][143/500]	Time  7.602 ( 7.602)	Loss 5.0770 (4.2264)	CeLoss 0.2383 (0.5779)	SegCLSLoss 0.0149 (0.0146)	KLLoss 0.0898 (0.0914)	MaskLoss 0.7737 (0.5212)	MaskBCELoss 0.2414 (0.1033)	MaskDICELoss 0.5323 (0.4179)
Epoch: [2][144/500]	Time  9.568 ( 9.568)	Loss 3.9171 (4.5769)	CeLoss 0.2793 (0.4632)	SegCLSLoss 0.0170 (0.0183)	KLLoss 0.1196 (0.1104)	MaskLoss 0.5021 (0.6260)	MaskBCELoss 0.0843 (0.1690)	MaskDICELoss 0.4178 (0.4570)
Epoch: [2][145/500]	Time  9.445 ( 9.445)	Loss 8.4723 (5.1372)	CeLoss 0.1934 (0.4153)	SegCLSLoss 0.0320 (0.0182)	KLLoss 0.2002 (0.1257)	MaskLoss 1.0607 (0.6527)	MaskBCELoss 0.0706 (0.1058)	MaskDICELoss 0.9901 (0.5469)
Epoch: [2][146/500]	Time 10.377 (10.377)	Loss 0.4531 (4.3125)	CeLoss 0.4531 (0.4455)	SegCLSLoss 0.0000 (0.0143)	KLLoss 0.0000 (0.1010)	MaskLoss 0.0000 (0.5539)	MaskBCELoss 0.0000 (0.1121)	MaskDICELoss 0.0000 (0.4418)
Epoch: [2][147/500]	Time  9.836 ( 9.836)	Loss 4.3092 (6.6147)	CeLoss 0.2637 (0.2539)	SegCLSLoss 0.0161 (0.0253)	KLLoss 0.1426 (0.1658)	MaskLoss 0.5522 (0.8570)	MaskBCELoss 0.0870 (0.1125)	MaskDICELoss 0.4651 (0.7446)
Epoch: [2][148/500]	Time  8.772 ( 8.772)	Loss 1.6484 (4.3067)	CeLoss 1.6484 (0.6523)	SegCLSLoss 0.0000 (0.0138)	KLLoss 0.0000 (0.0973)	MaskLoss 0.0000 (0.5015)	MaskBCELoss 0.0000 (0.0770)	MaskDICELoss 0.0000 (0.4245)
Epoch: [2][149/500]	Time  9.653 ( 9.653)	Loss 7.4796 (6.3405)	CeLoss 0.3184 (0.4060)	SegCLSLoss 0.0197 (0.0306)	KLLoss 0.1309 (0.1751)	MaskLoss 0.9580 (0.9079)	MaskBCELoss 0.1072 (0.2532)	MaskDICELoss 0.8508 (0.6548)
[2025-03-04 05:20:37,796] [INFO] [logging.py:96:log_dist] [Rank 0] step=1150, skipped=0, lr=[0.00027355421686746985], mom=[(0.9, 0.95)]
[2025-03-04 05:20:37,801] [INFO] [timer.py:215:stop] epoch=0/micro_step=11500/global_step=1150, RunningAvgSamplesPerSec=1.0355014025708296, CurrSamplesPerSec=0.9724560091188154, MemAllocated=57.27GB, MaxMemAllocated=62.82GB
Epoch: [2][150/500]	Time 10.285 (10.285)	Loss 5.7890 (4.8669)	CeLoss 0.1953 (0.4229)	SegCLSLoss 0.0352 (0.0175)	KLLoss 0.2295 (0.1245)	MaskLoss 0.6813 (0.6864)	MaskBCELoss 0.0171 (0.1967)	MaskDICELoss 0.6642 (0.4897)
Epoch: [2][151/500]	Time  9.512 ( 9.512)	Loss 3.6704 (5.8776)	CeLoss 0.2139 (0.3509)	SegCLSLoss 0.0171 (0.0200)	KLLoss 0.1709 (0.1479)	MaskLoss 0.5305 (0.7672)	MaskBCELoss 0.1613 (0.1282)	MaskDICELoss 0.3692 (0.6390)
Epoch: [2][152/500]	Time  8.753 ( 8.753)	Loss 7.5526 (3.9894)	CeLoss 0.2656 (0.4363)	SegCLSLoss 0.0234 (0.0135)	KLLoss 0.1689 (0.0910)	MaskLoss 0.9121 (0.4840)	MaskBCELoss 0.0316 (0.0695)	MaskDICELoss 0.8805 (0.4146)
Epoch: [2][153/500]	Time  8.175 ( 8.175)	Loss 4.1939 (4.4777)	CeLoss 0.3848 (0.5829)	SegCLSLoss 0.0152 (0.0183)	KLLoss 0.1221 (0.1190)	MaskLoss 0.7227 (0.5878)	MaskBCELoss 0.3506 (0.1559)	MaskDICELoss 0.3721 (0.4319)
Epoch: [2][154/500]	Time  7.742 ( 7.742)	Loss 1.5156 (5.4533)	CeLoss 1.5156 (0.6641)	SegCLSLoss 0.0000 (0.0176)	KLLoss 0.0000 (0.1189)	MaskLoss 0.0000 (0.7664)	MaskBCELoss 0.0000 (0.2450)	MaskDICELoss 0.0000 (0.5214)
Epoch: [2][155/500]	Time 10.262 (10.262)	Loss 0.6523 (5.8691)	CeLoss 0.6523 (0.4392)	SegCLSLoss 0.0000 (0.0215)	KLLoss 0.0000 (0.1290)	MaskLoss 0.0000 (0.8461)	MaskBCELoss 0.0000 (0.2464)	MaskDICELoss 0.0000 (0.5997)
Epoch: [2][156/500]	Time  9.616 ( 9.616)	Loss 0.1562 (4.2235)	CeLoss 0.1562 (0.3438)	SegCLSLoss 0.0000 (0.0174)	KLLoss 0.0000 (0.1090)	MaskLoss 0.0000 (0.6032)	MaskBCELoss 0.0000 (0.1774)	MaskDICELoss 0.0000 (0.4259)
Epoch: [2][157/500]	Time  9.244 ( 9.244)	Loss 1.0312 (4.6862)	CeLoss 1.0312 (0.5456)	SegCLSLoss 0.0000 (0.0177)	KLLoss 0.0000 (0.1267)	MaskLoss 0.0000 (0.6110)	MaskBCELoss 0.0000 (0.1472)	MaskDICELoss 0.0000 (0.4638)
Epoch: [2][158/500]	Time  9.329 ( 9.329)	Loss 4.3856 (4.5434)	CeLoss 0.2676 (0.4302)	SegCLSLoss 0.0192 (0.0136)	KLLoss 0.1396 (0.1004)	MaskLoss 0.7405 (0.6516)	MaskBCELoss 0.3260 (0.2011)	MaskDICELoss 0.4144 (0.4504)
Epoch: [2][159/500]	Time 11.263 (11.263)	Loss 0.1514 (5.7812)	CeLoss 0.1514 (0.2709)	SegCLSLoss 0.0000 (0.0204)	KLLoss 0.0000 (0.1571)	MaskLoss 0.0000 (0.7615)	MaskBCELoss 0.0000 (0.1248)	MaskDICELoss 0.0000 (0.6367)
[2025-03-04 05:22:08,694] [INFO] [logging.py:96:log_dist] [Rank 0] step=1160, skipped=0, lr=[0.0002733132530120482], mom=[(0.9, 0.95)]
[2025-03-04 05:22:08,700] [INFO] [timer.py:215:stop] epoch=0/micro_step=11600/global_step=1160, RunningAvgSamplesPerSec=1.036028632383723, CurrSamplesPerSec=1.4284360293841438, MemAllocated=57.29GB, MaxMemAllocated=62.82GB
Epoch: [2][160/500]	Time  7.003 ( 7.003)	Loss 8.3974 (5.7396)	CeLoss 0.1699 (0.6640)	SegCLSLoss 0.0347 (0.0318)	KLLoss 0.2266 (0.1487)	MaskLoss 1.3205 (0.8008)	MaskBCELoss 0.4301 (0.2492)	MaskDICELoss 0.8904 (0.5516)
Epoch: [2][161/500]	Time  8.604 ( 8.604)	Loss 0.8828 (4.6856)	CeLoss 0.8828 (0.5056)	SegCLSLoss 0.0000 (0.0172)	KLLoss 0.0000 (0.1124)	MaskLoss 0.0000 (0.6671)	MaskBCELoss 0.0000 (0.2129)	MaskDICELoss 0.0000 (0.4542)
Epoch: [2][162/500]	Time 10.255 (10.255)	Loss 7.6727 (5.6526)	CeLoss 0.2500 (0.5062)	SegCLSLoss 0.0349 (0.0179)	KLLoss 0.2139 (0.1287)	MaskLoss 0.9448 (0.7794)	MaskBCELoss 0.0611 (0.2044)	MaskDICELoss 0.8838 (0.5750)
Epoch: [2][163/500]	Time  8.653 ( 8.653)	Loss 7.3848 (5.0200)	CeLoss 0.1758 (0.4126)	SegCLSLoss 0.0306 (0.0211)	KLLoss 0.1826 (0.1287)	MaskLoss 1.1262 (0.6805)	MaskBCELoss 0.3333 (0.1628)	MaskDICELoss 0.7929 (0.5178)
Epoch: [2][164/500]	Time 10.105 (10.105)	Loss 6.5856 (5.6142)	CeLoss 0.2227 (0.3435)	SegCLSLoss 0.0354 (0.0196)	KLLoss 0.2676 (0.1451)	MaskLoss 0.8224 (0.6940)	MaskBCELoss 0.0836 (0.0727)	MaskDICELoss 0.7388 (0.6213)
Epoch: [2][165/500]	Time 10.225 (10.225)	Loss 5.6494 (4.8112)	CeLoss 0.2520 (0.4325)	SegCLSLoss 0.0165 (0.0172)	KLLoss 0.1108 (0.1359)	MaskLoss 0.8281 (0.6392)	MaskBCELoss 0.2244 (0.1467)	MaskDICELoss 0.6037 (0.4926)
Epoch: [2][166/500]	Time  9.632 ( 9.632)	Loss 7.0726 (4.3909)	CeLoss 0.2012 (0.4709)	SegCLSLoss 0.0371 (0.0226)	KLLoss 0.2168 (0.1245)	MaskLoss 0.9480 (0.5573)	MaskBCELoss 0.1582 (0.1124)	MaskDICELoss 0.7898 (0.4449)
Epoch: [2][167/500]	Time  9.251 ( 9.251)	Loss 5.4997 (6.9194)	CeLoss 0.3047 (0.3135)	SegCLSLoss 0.0152 (0.0229)	KLLoss 0.1289 (0.1426)	MaskLoss 0.7926 (1.1399)	MaskBCELoss 0.2138 (0.4446)	MaskDICELoss 0.5788 (0.6953)
Epoch: [2][168/500]	Time  9.621 ( 9.621)	Loss 7.3002 (5.5479)	CeLoss 0.2695 (0.4535)	SegCLSLoss 0.0184 (0.0157)	KLLoss 0.1797 (0.1219)	MaskLoss 1.6193 (0.7497)	MaskBCELoss 1.0189 (0.1722)	MaskDICELoss 0.6004 (0.5775)
Epoch: [2][169/500]	Time 10.601 (10.601)	Loss 8.1100 (5.0439)	CeLoss 0.2314 (0.5015)	SegCLSLoss 0.0237 (0.0166)	KLLoss 0.1895 (0.1256)	MaskLoss 1.0079 (0.6485)	MaskBCELoss 0.0645 (0.1300)	MaskDICELoss 0.9434 (0.5185)
[2025-03-04 05:23:45,833] [INFO] [logging.py:96:log_dist] [Rank 0] step=1170, skipped=0, lr=[0.00027307228915662645], mom=[(0.9, 0.95)]
[2025-03-04 05:23:45,839] [INFO] [timer.py:215:stop] epoch=0/micro_step=11700/global_step=1170, RunningAvgSamplesPerSec=1.0359735815679292, CurrSamplesPerSec=0.9813073102821129, MemAllocated=57.25GB, MaxMemAllocated=62.82GB
Epoch: [2][170/500]	Time 10.193 (10.193)	Loss 6.0555 (6.5251)	CeLoss 0.2754 (0.3596)	SegCLSLoss 0.0176 (0.0211)	KLLoss 0.1367 (0.1495)	MaskLoss 0.7931 (0.9169)	MaskBCELoss 0.1185 (0.2217)	MaskDICELoss 0.6746 (0.6952)
Epoch: [2][171/500]	Time 10.443 (10.443)	Loss 0.5898 (6.8525)	CeLoss 0.5898 (0.2805)	SegCLSLoss 0.0000 (0.0251)	KLLoss 0.0000 (0.1729)	MaskLoss 0.0000 (1.0654)	MaskBCELoss 0.0000 (0.3560)	MaskDICELoss 0.0000 (0.7094)
Epoch: [2][172/500]	Time  9.475 ( 9.475)	Loss 8.4609 (4.7775)	CeLoss 0.0742 (0.3219)	SegCLSLoss 0.1011 (0.0232)	KLLoss 0.2754 (0.1293)	MaskLoss 1.2949 (0.6785)	MaskBCELoss 0.3831 (0.1856)	MaskDICELoss 0.9118 (0.4929)
Epoch: [2][173/500]	Time  9.563 ( 9.563)	Loss 6.4216 (6.6830)	CeLoss 0.2031 (0.2538)	SegCLSLoss 0.0286 (0.0257)	KLLoss 0.1982 (0.1726)	MaskLoss 1.1169 (1.0571)	MaskBCELoss 0.4883 (0.3689)	MaskDICELoss 0.6286 (0.6882)
Epoch: [2][174/500]	Time  8.602 ( 8.602)	Loss 9.1052 (4.4465)	CeLoss 0.2168 (0.6119)	SegCLSLoss 0.0332 (0.0149)	KLLoss 0.2090 (0.1134)	MaskLoss 1.3789 (0.5698)	MaskBCELoss 0.3946 (0.1408)	MaskDICELoss 0.9843 (0.4290)
Epoch: [2][175/500]	Time  9.268 ( 9.268)	Loss 7.1287 (4.7036)	CeLoss 0.2051 (0.5104)	SegCLSLoss 0.0311 (0.0200)	KLLoss 0.1934 (0.1256)	MaskLoss 1.0591 (0.6372)	MaskBCELoss 0.2931 (0.1733)	MaskDICELoss 0.7661 (0.4639)
Epoch: [2][176/500]	Time  9.866 ( 9.866)	Loss 7.8027 (4.6128)	CeLoss 0.3574 (0.3146)	SegCLSLoss 0.0294 (0.0199)	KLLoss 0.1807 (0.1312)	MaskLoss 1.4745 (0.7097)	MaskBCELoss 0.7580 (0.2534)	MaskDICELoss 0.7165 (0.4562)
Epoch: [2][177/500]	Time  9.286 ( 9.286)	Loss 7.1705 (5.5181)	CeLoss 0.2051 (0.3678)	SegCLSLoss 0.0320 (0.0247)	KLLoss 0.2070 (0.1615)	MaskLoss 0.8793 (0.7921)	MaskBCELoss 0.0486 (0.2267)	MaskDICELoss 0.8307 (0.5654)
Epoch: [2][178/500]	Time  9.564 ( 9.564)	Loss 7.1580 (5.3253)	CeLoss 0.2539 (0.4850)	SegCLSLoss 0.0317 (0.0189)	KLLoss 0.2080 (0.1404)	MaskLoss 1.3219 (0.8058)	MaskBCELoss 0.6489 (0.2926)	MaskDICELoss 0.6730 (0.5132)
Epoch: [2][179/500]	Time  8.878 ( 8.878)	Loss 7.4814 (4.4189)	CeLoss 0.1963 (0.5295)	SegCLSLoss 0.0317 (0.0188)	KLLoss 0.2109 (0.1161)	MaskLoss 1.3156 (0.6169)	MaskBCELoss 0.5776 (0.1952)	MaskDICELoss 0.7380 (0.4217)
[2025-03-04 05:25:20,444] [INFO] [logging.py:96:log_dist] [Rank 0] step=1180, skipped=0, lr=[0.0002728313253012048], mom=[(0.9, 0.95)]
[2025-03-04 05:25:20,450] [INFO] [timer.py:215:stop] epoch=0/micro_step=11800/global_step=1180, RunningAvgSamplesPerSec=1.0361500291359893, CurrSamplesPerSec=1.0347858487773303, MemAllocated=57.24GB, MaxMemAllocated=62.82GB
Epoch: [2][180/500]	Time  9.666 ( 9.666)	Loss 3.8622 (5.9437)	CeLoss 0.2295 (0.3747)	SegCLSLoss 0.0146 (0.0229)	KLLoss 0.1387 (0.1614)	MaskLoss 0.6038 (0.8259)	MaskBCELoss 0.2238 (0.2019)	MaskDICELoss 0.3799 (0.6240)
Epoch: [2][181/500]	Time  8.805 ( 8.805)	Loss 8.0666 (5.3470)	CeLoss 0.2383 (0.5868)	SegCLSLoss 0.0271 (0.0199)	KLLoss 0.1748 (0.1249)	MaskLoss 1.0477 (0.6981)	MaskBCELoss 0.1234 (0.1599)	MaskDICELoss 0.9242 (0.5382)
Epoch: [2][182/500]	Time 10.987 (10.987)	Loss 5.9767 (7.1397)	CeLoss 0.2441 (0.2347)	SegCLSLoss 0.0171 (0.0267)	KLLoss 0.1211 (0.1809)	MaskLoss 0.7936 (0.9826)	MaskBCELoss 0.1242 (0.1916)	MaskDICELoss 0.6694 (0.7909)
Epoch: [2][183/500]	Time  9.981 ( 9.981)	Loss 1.3970 (5.8505)	CeLoss 0.1973 (0.3700)	SegCLSLoss 0.0182 (0.0182)	KLLoss 0.1553 (0.1467)	MaskLoss 0.2220 (0.8793)	MaskBCELoss 0.1235 (0.2850)	MaskDICELoss 0.0986 (0.5943)
Epoch: [2][184/500]	Time  8.868 ( 8.868)	Loss 8.4530 (5.6552)	CeLoss 0.2100 (0.4930)	SegCLSLoss 0.0302 (0.0259)	KLLoss 0.1826 (0.1354)	MaskLoss 1.0258 (0.7280)	MaskBCELoss 0.0267 (0.1349)	MaskDICELoss 0.9992 (0.5930)
Epoch: [2][185/500]	Time  8.784 ( 8.784)	Loss 8.5247 (6.3051)	CeLoss 0.2070 (0.3383)	SegCLSLoss 0.0396 (0.0252)	KLLoss 0.2656 (0.1577)	MaskLoss 1.0951 (0.8662)	MaskBCELoss 0.1214 (0.1888)	MaskDICELoss 0.9737 (0.6773)
Epoch: [2][186/500]	Time  9.672 ( 9.672)	Loss 3.9371 (5.9099)	CeLoss 0.2734 (0.2173)	SegCLSLoss 0.0184 (0.0226)	KLLoss 0.1357 (0.1555)	MaskLoss 0.7117 (0.9008)	MaskBCELoss 0.3624 (0.2801)	MaskDICELoss 0.3493 (0.6207)
Epoch: [2][187/500]	Time  9.207 ( 9.207)	Loss 9.7865 (6.2097)	CeLoss 0.3027 (0.3489)	SegCLSLoss 0.0229 (0.0249)	KLLoss 0.2119 (0.1454)	MaskLoss 1.7670 (0.8547)	MaskBCELoss 0.8128 (0.1892)	MaskDICELoss 0.9542 (0.6655)
Epoch: [2][188/500]	Time 11.440 (11.440)	Loss 0.5625 (6.5327)	CeLoss 0.5625 (0.2613)	SegCLSLoss 0.0000 (0.0259)	KLLoss 0.0000 (0.1679)	MaskLoss 0.0000 (0.8248)	MaskBCELoss 0.0000 (0.0846)	MaskDICELoss 0.0000 (0.7402)
Epoch: [2][189/500]	Time  9.629 ( 9.629)	Loss 5.8179 (5.5087)	CeLoss 0.2734 (0.4547)	SegCLSLoss 0.0187 (0.0172)	KLLoss 0.1465 (0.1110)	MaskLoss 0.7347 (0.7593)	MaskBCELoss 0.0815 (0.1900)	MaskDICELoss 0.6532 (0.5693)
[2025-03-04 05:26:56,497] [INFO] [logging.py:96:log_dist] [Rank 0] step=1190, skipped=0, lr=[0.0002725903614457831], mom=[(0.9, 0.95)]
[2025-03-04 05:26:56,503] [INFO] [timer.py:215:stop] epoch=0/micro_step=11900/global_step=1190, RunningAvgSamplesPerSec=1.036192888379281, CurrSamplesPerSec=1.1524123264342523, MemAllocated=56.7GB, MaxMemAllocated=62.82GB
Epoch: [2][190/500]	Time  8.680 ( 8.680)	Loss 1.4766 (3.9135)	CeLoss 1.4766 (0.7080)	SegCLSLoss 0.0000 (0.0126)	KLLoss 0.0000 (0.0909)	MaskLoss 0.0000 (0.4497)	MaskBCELoss 0.0000 (0.0815)	MaskDICELoss 0.0000 (0.3682)
Epoch: [2][191/500]	Time  9.607 ( 9.607)	Loss 4.6663 (5.8125)	CeLoss 0.3027 (0.2827)	SegCLSLoss 0.0149 (0.0247)	KLLoss 0.0962 (0.1492)	MaskLoss 0.8109 (0.8411)	MaskBCELoss 0.3712 (0.2267)	MaskDICELoss 0.4397 (0.6144)
Epoch: [2][192/500]	Time  8.718 ( 8.718)	Loss 4.0257 (4.3000)	CeLoss 0.2695 (0.5475)	SegCLSLoss 0.0143 (0.0228)	KLLoss 0.1030 (0.1224)	MaskLoss 0.7191 (0.5936)	MaskBCELoss 0.3513 (0.1884)	MaskDICELoss 0.3678 (0.4053)
Epoch: [2][193/500]	Time  8.109 ( 8.109)	Loss 3.0101 (5.0337)	CeLoss 0.2305 (0.5881)	SegCLSLoss 0.0221 (0.0162)	KLLoss 0.1641 (0.1186)	MaskLoss 0.4811 (0.6551)	MaskBCELoss 0.2075 (0.1537)	MaskDICELoss 0.2736 (0.5014)
Epoch: [2][194/500]	Time  8.953 ( 8.953)	Loss 2.3022 (4.8260)	CeLoss 0.3281 (0.2888)	SegCLSLoss 0.0155 (0.0208)	KLLoss 0.1260 (0.1336)	MaskLoss 0.3187 (0.6678)	MaskBCELoss 0.1181 (0.1583)	MaskDICELoss 0.2006 (0.5096)
Epoch: [2][195/500]	Time  8.943 ( 8.943)	Loss 6.8913 (3.5727)	CeLoss 0.2051 (0.4879)	SegCLSLoss 0.0258 (0.0128)	KLLoss 0.1523 (0.0843)	MaskLoss 0.8864 (0.4248)	MaskBCELoss 0.0951 (0.0674)	MaskDICELoss 0.7912 (0.3574)
Epoch: [2][196/500]	Time 11.164 (11.164)	Loss 0.2266 (3.3372)	CeLoss 0.2266 (0.2884)	SegCLSLoss 0.0000 (0.0125)	KLLoss 0.0000 (0.0885)	MaskLoss 0.0000 (0.4402)	MaskBCELoss 0.0000 (0.0946)	MaskDICELoss 0.0000 (0.3457)
Epoch: [2][197/500]	Time  9.627 ( 9.627)	Loss 7.7243 (6.1975)	CeLoss 0.2070 (0.3695)	SegCLSLoss 0.0339 (0.0235)	KLLoss 0.1777 (0.1367)	MaskLoss 0.9725 (0.9119)	MaskBCELoss 0.0763 (0.2693)	MaskDICELoss 0.8962 (0.6426)
Epoch: [2][198/500]	Time 10.188 (10.188)	Loss 0.7734 (4.9636)	CeLoss 0.7734 (0.3957)	SegCLSLoss 0.0000 (0.0190)	KLLoss 0.0000 (0.1329)	MaskLoss 0.0000 (0.6156)	MaskBCELoss 0.0000 (0.0832)	MaskDICELoss 0.0000 (0.5324)
Epoch: [2][199/500]	Time  9.810 ( 9.810)	Loss 7.6744 (5.6274)	CeLoss 0.3730 (0.3727)	SegCLSLoss 0.0273 (0.0219)	KLLoss 0.2148 (0.1527)	MaskLoss 1.1226 (0.8113)	MaskBCELoss 0.3180 (0.2333)	MaskDICELoss 0.8046 (0.5780)
[2025-03-04 05:28:29,509] [INFO] [logging.py:96:log_dist] [Rank 0] step=1200, skipped=0, lr=[0.0002723493975903614], mom=[(0.9, 0.95)]
[2025-03-04 05:28:29,514] [INFO] [timer.py:215:stop] epoch=0/micro_step=12000/global_step=1200, RunningAvgSamplesPerSec=1.0365079011091773, CurrSamplesPerSec=1.2677058300237296, MemAllocated=57.28GB, MaxMemAllocated=62.82GB
Epoch: [2][200/500]	Time  7.890 ( 7.890)	Loss 8.0963 (4.8279)	CeLoss 0.1621 (0.6898)	SegCLSLoss 0.0479 (0.0186)	KLLoss 0.2393 (0.1120)	MaskLoss 1.2446 (0.6743)	MaskBCELoss 0.3811 (0.2296)	MaskDICELoss 0.8635 (0.4447)
Epoch: [2][201/500]	Time 10.488 (10.488)	Loss 7.2379 (7.0014)	CeLoss 0.1250 (0.2334)	SegCLSLoss 0.0723 (0.0335)	KLLoss 0.2598 (0.1943)	MaskLoss 1.1635 (1.0158)	MaskBCELoss 0.4154 (0.2617)	MaskDICELoss 0.7482 (0.7542)
Epoch: [2][202/500]	Time  7.358 ( 7.358)	Loss 7.6864 (3.9180)	CeLoss 0.2256 (0.7479)	SegCLSLoss 0.0259 (0.0118)	KLLoss 0.1826 (0.0846)	MaskLoss 0.9973 (0.5474)	MaskBCELoss 0.1190 (0.2166)	MaskDICELoss 0.8783 (0.3307)
Epoch: [2][203/500]	Time 10.939 (10.939)	Loss 8.5573 (5.9948)	CeLoss 0.2197 (0.3598)	SegCLSLoss 0.0305 (0.0244)	KLLoss 0.2051 (0.1484)	MaskLoss 1.2032 (0.7897)	MaskBCELoss 0.2516 (0.1406)	MaskDICELoss 0.9516 (0.6491)
Epoch: [2][204/500]	Time  9.480 ( 9.480)	Loss 8.3205 (4.1407)	CeLoss 0.1816 (0.4250)	SegCLSLoss 0.0273 (0.0149)	KLLoss 0.1768 (0.1086)	MaskLoss 1.0094 (0.5599)	MaskBCELoss 0.0209 (0.1466)	MaskDICELoss 0.9884 (0.4133)
Epoch: [2][205/500]	Time  9.603 ( 9.603)	Loss 4.6247 (5.6066)	CeLoss 0.2197 (0.4125)	SegCLSLoss 0.0239 (0.0243)	KLLoss 0.1650 (0.1588)	MaskLoss 0.5887 (0.7228)	MaskBCELoss 0.0802 (0.1266)	MaskDICELoss 0.5085 (0.5962)
Epoch: [2][206/500]	Time  7.252 ( 7.252)	Loss 1.1719 (3.8430)	CeLoss 1.1719 (0.8149)	SegCLSLoss 0.0000 (0.0195)	KLLoss 0.0000 (0.0867)	MaskLoss 0.0000 (0.4303)	MaskBCELoss 0.0000 (0.0851)	MaskDICELoss 0.0000 (0.3452)
Epoch: [2][207/500]	Time  9.844 ( 9.844)	Loss 5.1010 (5.4900)	CeLoss 0.2441 (0.4719)	SegCLSLoss 0.0159 (0.0169)	KLLoss 0.1436 (0.1269)	MaskLoss 0.9451 (0.7371)	MaskBCELoss 0.4758 (0.1689)	MaskDICELoss 0.4694 (0.5682)
Epoch: [2][208/500]	Time  8.432 ( 8.432)	Loss 8.2249 (6.6521)	CeLoss 0.2412 (0.3691)	SegCLSLoss 0.0347 (0.0297)	KLLoss 0.2158 (0.1612)	MaskLoss 0.9799 (0.8779)	MaskBCELoss 0.0146 (0.1527)	MaskDICELoss 0.9654 (0.7252)
Epoch: [2][209/500]	Time 10.550 (10.550)	Loss 7.0161 (5.0143)	CeLoss 0.2178 (0.3686)	SegCLSLoss 0.0281 (0.0175)	KLLoss 0.1895 (0.1306)	MaskLoss 0.9069 (0.6556)	MaskBCELoss 0.1101 (0.1230)	MaskDICELoss 0.7967 (0.5326)
[2025-03-04 05:30:01,255] [INFO] [logging.py:96:log_dist] [Rank 0] step=1210, skipped=0, lr=[0.00027210843373493975], mom=[(0.9, 0.95)]
[2025-03-04 05:30:01,260] [INFO] [timer.py:215:stop] epoch=0/micro_step=12100/global_step=1210, RunningAvgSamplesPerSec=1.0369303393309832, CurrSamplesPerSec=1.2822210739250812, MemAllocated=56.71GB, MaxMemAllocated=62.82GB
Epoch: [2][210/500]	Time  7.801 ( 7.801)	Loss 0.4707 (3.0503)	CeLoss 0.4707 (0.5286)	SegCLSLoss 0.0000 (0.0107)	KLLoss 0.0000 (0.0729)	MaskLoss 0.0000 (0.4562)	MaskBCELoss 0.0000 (0.2010)	MaskDICELoss 0.0000 (0.2552)
Epoch: [2][211/500]	Time  9.927 ( 9.927)	Loss 0.4316 (5.1047)	CeLoss 0.4316 (0.3465)	SegCLSLoss 0.0000 (0.0235)	KLLoss 0.0000 (0.1315)	MaskLoss 0.0000 (0.7394)	MaskBCELoss 0.0000 (0.2167)	MaskDICELoss 0.0000 (0.5227)
Epoch: [2][212/500]	Time 11.377 (11.377)	Loss 7.6370 (5.6016)	CeLoss 0.2080 (0.2471)	SegCLSLoss 0.0203 (0.0218)	KLLoss 0.1338 (0.1345)	MaskLoss 0.9170 (0.7744)	MaskBCELoss 0.0084 (0.1643)	MaskDICELoss 0.9086 (0.6101)
Epoch: [2][213/500]	Time 10.228 (10.228)	Loss 6.5234 (5.7872)	CeLoss 0.2334 (0.5189)	SegCLSLoss 0.0210 (0.0207)	KLLoss 0.1855 (0.1392)	MaskLoss 1.0873 (0.7752)	MaskBCELoss 0.4341 (0.1804)	MaskDICELoss 0.6532 (0.5947)
Epoch: [2][214/500]	Time  8.706 ( 8.706)	Loss 5.3831 (5.8168)	CeLoss 0.2637 (0.4503)	SegCLSLoss 0.0159 (0.0247)	KLLoss 0.1426 (0.1489)	MaskLoss 1.0064 (0.8686)	MaskBCELoss 0.5138 (0.2906)	MaskDICELoss 0.4927 (0.5780)
Epoch: [2][215/500]	Time  8.186 ( 8.186)	Loss 1.0625 (4.5196)	CeLoss 1.0625 (0.4908)	SegCLSLoss 0.0000 (0.0272)	KLLoss 0.0000 (0.1297)	MaskLoss 0.0000 (0.6260)	MaskBCELoss 0.0000 (0.1871)	MaskDICELoss 0.0000 (0.4389)
Epoch: [2][216/500]	Time  9.107 ( 9.107)	Loss 0.9258 (4.4921)	CeLoss 0.9258 (0.5176)	SegCLSLoss 0.0000 (0.0173)	KLLoss 0.0000 (0.1151)	MaskLoss 0.0000 (0.5791)	MaskBCELoss 0.0000 (0.1303)	MaskDICELoss 0.0000 (0.4488)
Epoch: [2][217/500]	Time  8.629 ( 8.629)	Loss 1.7266 (4.4301)	CeLoss 1.7266 (0.6764)	SegCLSLoss 0.0000 (0.0187)	KLLoss 0.0000 (0.1102)	MaskLoss 0.0000 (0.5285)	MaskBCELoss 0.0000 (0.0990)	MaskDICELoss 0.0000 (0.4295)
Epoch: [2][218/500]	Time 11.542 (11.542)	Loss 7.4962 (5.2421)	CeLoss 0.2412 (0.5188)	SegCLSLoss 0.0157 (0.0138)	KLLoss 0.1152 (0.0994)	MaskLoss 1.4330 (0.7749)	MaskBCELoss 0.7219 (0.2636)	MaskDICELoss 0.7111 (0.5113)
Epoch: [2][219/500]	Time  9.419 ( 9.419)	Loss 7.4526 (5.3650)	CeLoss 0.2148 (0.3179)	SegCLSLoss 0.0311 (0.0276)	KLLoss 0.2051 (0.1674)	MaskLoss 0.9553 (0.7168)	MaskBCELoss 0.1042 (0.1448)	MaskDICELoss 0.8511 (0.5720)
[2025-03-04 05:31:36,075] [INFO] [logging.py:96:log_dist] [Rank 0] step=1220, skipped=0, lr=[0.00027186746987951807], mom=[(0.9, 0.95)]
[2025-03-04 05:31:36,080] [INFO] [timer.py:215:stop] epoch=0/micro_step=12200/global_step=1220, RunningAvgSamplesPerSec=1.0370747606966149, CurrSamplesPerSec=1.299606254662472, MemAllocated=56.71GB, MaxMemAllocated=62.82GB
Epoch: [2][220/500]	Time  7.697 ( 7.697)	Loss 1.2422 (4.9039)	CeLoss 1.2422 (0.5084)	SegCLSLoss 0.0000 (0.0155)	KLLoss 0.0000 (0.1075)	MaskLoss 0.0000 (0.6064)	MaskBCELoss 0.0000 (0.0952)	MaskDICELoss 0.0000 (0.5112)
Epoch: [2][221/500]	Time  9.102 ( 9.102)	Loss 8.7565 (6.0670)	CeLoss 0.2334 (0.5026)	SegCLSLoss 0.0300 (0.0205)	KLLoss 0.1982 (0.1320)	MaskLoss 1.1572 (0.8038)	MaskBCELoss 0.1577 (0.1680)	MaskDICELoss 0.9995 (0.6358)
Epoch: [2][222/500]	Time  8.889 ( 8.889)	Loss 1.7422 (4.0503)	CeLoss 1.7422 (0.4921)	SegCLSLoss 0.0000 (0.0173)	KLLoss 0.0000 (0.1177)	MaskLoss 0.0000 (0.5944)	MaskBCELoss 0.0000 (0.2205)	MaskDICELoss 0.0000 (0.3739)
Epoch: [2][223/500]	Time  9.556 ( 9.556)	Loss 1.5469 (5.7056)	CeLoss 1.5469 (0.4794)	SegCLSLoss 0.0000 (0.0250)	KLLoss 0.0000 (0.1488)	MaskLoss 0.0000 (0.7780)	MaskBCELoss 0.0000 (0.1933)	MaskDICELoss 0.0000 (0.5848)
Epoch: [2][224/500]	Time 10.550 (10.550)	Loss 8.7451 (5.0706)	CeLoss 0.2109 (0.3085)	SegCLSLoss 0.0359 (0.0220)	KLLoss 0.2158 (0.1426)	MaskLoss 1.4223 (0.7684)	MaskBCELoss 0.5131 (0.2565)	MaskDICELoss 0.9092 (0.5119)
Epoch: [2][225/500]	Time 10.588 (10.588)	Loss 7.1423 (6.7705)	CeLoss 0.2275 (0.3647)	SegCLSLoss 0.0176 (0.0303)	KLLoss 0.1235 (0.1737)	MaskLoss 1.2186 (1.0050)	MaskBCELoss 0.4943 (0.3039)	MaskDICELoss 0.7243 (0.7011)
Epoch: [2][226/500]	Time  8.823 ( 8.823)	Loss 7.2735 (4.7037)	CeLoss 0.2520 (0.6000)	SegCLSLoss 0.0327 (0.0216)	KLLoss 0.2168 (0.1303)	MaskLoss 0.8929 (0.6384)	MaskBCELoss 0.0591 (0.1908)	MaskDICELoss 0.8339 (0.4477)
Epoch: [2][227/500]	Time  7.628 ( 7.628)	Loss 1.0938 (5.4199)	CeLoss 1.0938 (0.5160)	SegCLSLoss 0.0000 (0.0200)	KLLoss 0.0000 (0.1271)	MaskLoss 0.0000 (0.8088)	MaskBCELoss 0.0000 (0.2839)	MaskDICELoss 0.0000 (0.5248)
Epoch: [2][228/500]	Time  8.530 ( 8.530)	Loss 8.5899 (3.5592)	CeLoss 0.2637 (0.4197)	SegCLSLoss 0.0177 (0.0172)	KLLoss 0.1406 (0.0997)	MaskLoss 1.0896 (0.4871)	MaskBCELoss 0.0902 (0.1443)	MaskDICELoss 0.9994 (0.3428)
Epoch: [2][229/500]	Time 10.518 (10.518)	Loss 8.4659 (5.3387)	CeLoss 0.2773 (0.3477)	SegCLSLoss 0.0211 (0.0230)	KLLoss 0.1504 (0.1482)	MaskLoss 1.1350 (0.7866)	MaskBCELoss 0.1752 (0.2436)	MaskDICELoss 0.9597 (0.5430)
[2025-03-04 05:33:08,468] [INFO] [logging.py:96:log_dist] [Rank 0] step=1230, skipped=0, lr=[0.0002716265060240964], mom=[(0.9, 0.95)]
[2025-03-04 05:33:08,474] [INFO] [timer.py:215:stop] epoch=0/micro_step=12300/global_step=1230, RunningAvgSamplesPerSec=1.0374293494784996, CurrSamplesPerSec=1.2183918637075621, MemAllocated=57.11GB, MaxMemAllocated=62.82GB
Epoch: [2][230/500]	Time  8.210 ( 8.210)	Loss 6.9537 (5.6573)	CeLoss 0.1914 (0.6079)	SegCLSLoss 0.0309 (0.0182)	KLLoss 0.2178 (0.1231)	MaskLoss 0.8262 (0.8153)	MaskBCELoss 0.0136 (0.2676)	MaskDICELoss 0.8126 (0.5477)
Epoch: [2][231/500]	Time  9.104 ( 9.104)	Loss 7.3859 (5.4569)	CeLoss 0.2324 (0.4007)	SegCLSLoss 0.0187 (0.0188)	KLLoss 0.1436 (0.1301)	MaskLoss 1.0399 (0.7183)	MaskBCELoss 0.2200 (0.1384)	MaskDICELoss 0.8199 (0.5799)
Epoch: [2][232/500]	Time 10.716 (10.716)	Loss 5.7491 (5.6406)	CeLoss 0.2178 (0.3889)	SegCLSLoss 0.0262 (0.0211)	KLLoss 0.1797 (0.1472)	MaskLoss 0.7405 (0.8138)	MaskBCELoss 0.0975 (0.2360)	MaskDICELoss 0.6430 (0.5778)
Epoch: [2][233/500]	Time 10.733 (10.733)	Loss 5.2016 (5.7887)	CeLoss 0.2275 (0.2305)	SegCLSLoss 0.0280 (0.0267)	KLLoss 0.1992 (0.1744)	MaskLoss 0.8003 (0.8143)	MaskBCELoss 0.2737 (0.1906)	MaskDICELoss 0.5266 (0.6237)
Epoch: [2][234/500]	Time 10.184 (10.184)	Loss 6.6470 (5.1144)	CeLoss 0.2021 (0.5728)	SegCLSLoss 0.0260 (0.0167)	KLLoss 0.2139 (0.1143)	MaskLoss 1.0730 (0.7640)	MaskBCELoss 0.3945 (0.2822)	MaskDICELoss 0.6786 (0.4818)
Epoch: [2][235/500]	Time  8.006 ( 8.006)	Loss 1.1562 (4.5653)	CeLoss 1.1562 (0.5119)	SegCLSLoss 0.0000 (0.0230)	KLLoss 0.0000 (0.1375)	MaskLoss 0.0000 (0.6271)	MaskBCELoss 0.0000 (0.1854)	MaskDICELoss 0.0000 (0.4417)
Epoch: [2][236/500]	Time  9.168 ( 9.168)	Loss 8.8409 (5.2509)	CeLoss 0.2910 (0.5700)	SegCLSLoss 0.0266 (0.0245)	KLLoss 0.1826 (0.1404)	MaskLoss 1.2621 (0.7453)	MaskBCELoss 0.2907 (0.2391)	MaskDICELoss 0.9714 (0.5062)
Epoch: [2][237/500]	Time  8.475 ( 8.475)	Loss 7.0233 (3.4495)	CeLoss 0.2617 (0.4921)	SegCLSLoss 0.0220 (0.0131)	KLLoss 0.1494 (0.1040)	MaskLoss 0.9524 (0.4299)	MaskBCELoss 0.1696 (0.0988)	MaskDICELoss 0.7828 (0.3312)
Epoch: [2][238/500]	Time  9.566 ( 9.566)	Loss 7.0543 (5.7139)	CeLoss 0.2520 (0.3302)	SegCLSLoss 0.0206 (0.0232)	KLLoss 0.1699 (0.1373)	MaskLoss 0.9099 (0.8748)	MaskBCELoss 0.1095 (0.2940)	MaskDICELoss 0.8005 (0.5808)
Epoch: [2][239/500]	Time 11.564 (11.564)	Loss 6.8084 (4.5559)	CeLoss 0.1855 (0.2614)	SegCLSLoss 0.0352 (0.0196)	KLLoss 0.2441 (0.1441)	MaskLoss 1.2239 (0.6483)	MaskBCELoss 0.5717 (0.1743)	MaskDICELoss 0.6522 (0.4740)
[2025-03-04 05:34:44,970] [INFO] [logging.py:96:log_dist] [Rank 0] step=1240, skipped=0, lr=[0.00027138554216867467], mom=[(0.9, 0.95)]
[2025-03-04 05:34:44,975] [INFO] [timer.py:215:stop] epoch=0/micro_step=12400/global_step=1240, RunningAvgSamplesPerSec=1.0374213603681448, CurrSamplesPerSec=1.113145862441598, MemAllocated=56.65GB, MaxMemAllocated=62.82GB
Epoch: [2][240/500]	Time  8.986 ( 8.986)	Loss 8.3714 (3.9957)	CeLoss 0.1875 (0.3505)	SegCLSLoss 0.0193 (0.0130)	KLLoss 0.1953 (0.0979)	MaskLoss 1.0262 (0.5566)	MaskBCELoss 0.0384 (0.1520)	MaskDICELoss 0.9878 (0.4046)
Epoch: [2][241/500]	Time 10.282 (10.282)	Loss 8.3013 (7.0536)	CeLoss 0.2598 (0.2287)	SegCLSLoss 0.0233 (0.0315)	KLLoss 0.1660 (0.1910)	MaskLoss 1.0179 (0.9759)	MaskBCELoss 0.0465 (0.1982)	MaskDICELoss 0.9713 (0.7777)
Epoch: [2][242/500]	Time  9.301 ( 9.301)	Loss 0.6484 (4.4432)	CeLoss 0.6484 (0.6093)	SegCLSLoss 0.0000 (0.0138)	KLLoss 0.0000 (0.0959)	MaskLoss 0.0000 (0.6294)	MaskBCELoss 0.0000 (0.2174)	MaskDICELoss 0.0000 (0.4121)
Epoch: [2][243/500]	Time  9.786 ( 9.786)	Loss 1.4688 (5.6646)	CeLoss 1.4688 (0.4361)	SegCLSLoss 0.0000 (0.0214)	KLLoss 0.0000 (0.1473)	MaskLoss 0.0000 (0.7978)	MaskBCELoss 0.0000 (0.2186)	MaskDICELoss 0.0000 (0.5791)
Epoch: [2][244/500]	Time 10.340 (10.340)	Loss 8.0004 (5.8047)	CeLoss 0.1226 (0.3922)	SegCLSLoss 0.0674 (0.0265)	KLLoss 0.2832 (0.1535)	MaskLoss 1.1689 (0.7710)	MaskBCELoss 0.2984 (0.1536)	MaskDICELoss 0.8705 (0.6173)
Epoch: [2][245/500]	Time  9.364 ( 9.364)	Loss 1.7969 (5.1462)	CeLoss 1.7969 (0.4174)	SegCLSLoss 0.0000 (0.0198)	KLLoss 0.0000 (0.1305)	MaskLoss 0.0000 (0.6945)	MaskBCELoss 0.0000 (0.1612)	MaskDICELoss 0.0000 (0.5332)
Epoch: [2][246/500]	Time  8.790 ( 8.790)	Loss 1.1875 (5.2856)	CeLoss 1.1875 (0.3811)	SegCLSLoss 0.0000 (0.0179)	KLLoss 0.0000 (0.1259)	MaskLoss 0.0000 (0.7139)	MaskBCELoss 0.0000 (0.1569)	MaskDICELoss 0.0000 (0.5570)
Epoch: [2][247/500]	Time  8.234 ( 8.234)	Loss 0.6094 (4.7109)	CeLoss 0.6094 (0.4893)	SegCLSLoss 0.0000 (0.0208)	KLLoss 0.0000 (0.1267)	MaskLoss 0.0000 (0.7330)	MaskBCELoss 0.0000 (0.2966)	MaskDICELoss 0.0000 (0.4364)
Epoch: [2][248/500]	Time  9.412 ( 9.412)	Loss 0.1416 (4.3466)	CeLoss 0.1416 (0.4983)	SegCLSLoss 0.0000 (0.0195)	KLLoss 0.0000 (0.1385)	MaskLoss 0.0000 (0.5559)	MaskBCELoss 0.0000 (0.1245)	MaskDICELoss 0.0000 (0.4314)
Epoch: [2][249/500]	Time  9.987 ( 9.987)	Loss 7.9396 (6.7413)	CeLoss 0.1953 (0.3962)	SegCLSLoss 0.0286 (0.0205)	KLLoss 0.2002 (0.1497)	MaskLoss 1.0178 (0.9227)	MaskBCELoss 0.1021 (0.1995)	MaskDICELoss 0.9156 (0.7233)
[2025-03-04 05:36:18,056] [INFO] [logging.py:96:log_dist] [Rank 0] step=1250, skipped=0, lr=[0.000271144578313253], mom=[(0.9, 0.95)]
[2025-03-04 05:36:18,062] [INFO] [timer.py:215:stop] epoch=0/micro_step=12500/global_step=1250, RunningAvgSamplesPerSec=1.0377093112894626, CurrSamplesPerSec=1.3176536168619541, MemAllocated=56.82GB, MaxMemAllocated=62.82GB
Epoch: [2][250/500]	Time  7.591 ( 7.591)	Loss 1.3991 (3.8095)	CeLoss 0.1133 (0.7564)	SegCLSLoss 0.0488 (0.0150)	KLLoss 0.2441 (0.0956)	MaskLoss 0.1408 (0.4701)	MaskBCELoss 0.0184 (0.1352)	MaskDICELoss 0.1224 (0.3349)
Epoch: [2][251/500]	Time  9.266 ( 9.266)	Loss 3.4194 (5.5648)	CeLoss 0.2480 (0.4705)	SegCLSLoss 0.0182 (0.0176)	KLLoss 0.1504 (0.1349)	MaskLoss 0.4607 (0.8327)	MaskBCELoss 0.1124 (0.2852)	MaskDICELoss 0.3483 (0.5475)
Epoch: [2][252/500]	Time  9.528 ( 9.528)	Loss 1.4297 (5.7128)	CeLoss 1.4297 (0.3388)	SegCLSLoss 0.0000 (0.0251)	KLLoss 0.0000 (0.1505)	MaskLoss 0.0000 (0.8580)	MaskBCELoss 0.0000 (0.2754)	MaskDICELoss 0.0000 (0.5826)
Epoch: [2][253/500]	Time  9.944 ( 9.944)	Loss 0.4375 (3.5298)	CeLoss 0.4375 (0.6532)	SegCLSLoss 0.0000 (0.0114)	KLLoss 0.0000 (0.0916)	MaskLoss 0.0000 (0.4247)	MaskBCELoss 0.0000 (0.1030)	MaskDICELoss 0.0000 (0.3217)
Epoch: [2][254/500]	Time  8.804 ( 8.804)	Loss 0.0781 (3.3820)	CeLoss 0.0781 (0.7733)	SegCLSLoss 0.0000 (0.0112)	KLLoss 0.0000 (0.0735)	MaskLoss 0.0000 (0.3804)	MaskBCELoss 0.0000 (0.0856)	MaskDICELoss 0.0000 (0.2948)
Epoch: [2][255/500]	Time 10.017 (10.017)	Loss 5.1089 (6.0503)	CeLoss 0.2451 (0.3573)	SegCLSLoss 0.0156 (0.0186)	KLLoss 0.1416 (0.1398)	MaskLoss 0.6556 (0.8338)	MaskBCELoss 0.0884 (0.1878)	MaskDICELoss 0.5672 (0.6460)
Epoch: [2][256/500]	Time  9.768 ( 9.768)	Loss 0.0679 (5.2062)	CeLoss 0.0679 (0.3689)	SegCLSLoss 0.0000 (0.0179)	KLLoss 0.0000 (0.1323)	MaskLoss 0.0000 (0.7578)	MaskBCELoss 0.0000 (0.2277)	MaskDICELoss 0.0000 (0.5301)
Epoch: [2][257/500]	Time  7.982 ( 7.982)	Loss 7.2518 (3.2164)	CeLoss 0.1865 (0.7010)	SegCLSLoss 0.0284 (0.0098)	KLLoss 0.1924 (0.0625)	MaskLoss 0.9601 (0.3588)	MaskBCELoss 0.1372 (0.0704)	MaskDICELoss 0.8229 (0.2884)
Epoch: [2][258/500]	Time  9.374 ( 9.374)	Loss 8.4042 (5.4555)	CeLoss 0.2168 (0.3790)	SegCLSLoss 0.0233 (0.0219)	KLLoss 0.1650 (0.1510)	MaskLoss 1.2824 (0.7234)	MaskBCELoss 0.3749 (0.1454)	MaskDICELoss 0.9075 (0.5780)
Epoch: [2][259/500]	Time  8.586 ( 8.586)	Loss 7.6033 (6.0323)	CeLoss 0.1357 (0.4079)	SegCLSLoss 0.0571 (0.0295)	KLLoss 0.2051 (0.1624)	MaskLoss 1.1229 (0.9269)	MaskBCELoss 0.2915 (0.3280)	MaskDICELoss 0.8314 (0.5989)
[2025-03-04 05:37:49,662] [INFO] [logging.py:96:log_dist] [Rank 0] step=1260, skipped=0, lr=[0.00027090361445783126], mom=[(0.9, 0.95)]
[2025-03-04 05:37:49,669] [INFO] [timer.py:215:stop] epoch=0/micro_step=12600/global_step=1260, RunningAvgSamplesPerSec=1.0381184566428712, CurrSamplesPerSec=1.1997245581720979, MemAllocated=56.72GB, MaxMemAllocated=62.82GB
Epoch: [2][260/500]	Time  8.338 ( 8.338)	Loss 1.7656 (3.2002)	CeLoss 1.7656 (0.5451)	SegCLSLoss 0.0000 (0.0108)	KLLoss 0.0000 (0.0928)	MaskLoss 0.0000 (0.4165)	MaskBCELoss 0.0000 (0.1292)	MaskDICELoss 0.0000 (0.2873)
Epoch: [2][261/500]	Time  8.994 ( 8.994)	Loss 6.8738 (5.7384)	CeLoss 0.1846 (0.3968)	SegCLSLoss 0.0400 (0.0233)	KLLoss 0.2637 (0.1571)	MaskLoss 0.8521 (0.8050)	MaskBCELoss 0.0686 (0.2112)	MaskDICELoss 0.7835 (0.5938)
Epoch: [2][262/500]	Time 10.949 (10.949)	Loss 7.3178 (6.5643)	CeLoss 0.2012 (0.3544)	SegCLSLoss 0.0221 (0.0277)	KLLoss 0.1582 (0.1778)	MaskLoss 0.9586 (0.9050)	MaskBCELoss 0.1204 (0.2036)	MaskDICELoss 0.8382 (0.7014)
Epoch: [2][263/500]	Time  7.899 ( 7.899)	Loss 8.0070 (5.0090)	CeLoss 0.1357 (0.5136)	SegCLSLoss 0.0635 (0.0255)	KLLoss 0.2539 (0.1424)	MaskLoss 1.2649 (0.7943)	MaskBCELoss 0.4223 (0.3356)	MaskDICELoss 0.8426 (0.4587)
Epoch: [2][264/500]	Time  6.961 ( 6.961)	Loss 0.5508 (5.2765)	CeLoss 0.5508 (0.5282)	SegCLSLoss 0.0000 (0.0182)	KLLoss 0.0000 (0.1119)	MaskLoss 0.0000 (0.8343)	MaskBCELoss 0.0000 (0.3412)	MaskDICELoss 0.0000 (0.4931)
Epoch: [2][265/500]	Time  9.110 ( 9.110)	Loss 0.8555 (5.0083)	CeLoss 0.8555 (0.6040)	SegCLSLoss 0.0000 (0.0155)	KLLoss 0.0000 (0.1044)	MaskLoss 0.0000 (0.6644)	MaskBCELoss 0.0000 (0.1704)	MaskDICELoss 0.0000 (0.4939)
Epoch: [2][266/500]	Time  7.536 ( 7.536)	Loss 5.2907 (3.5706)	CeLoss 0.2637 (0.6372)	SegCLSLoss 0.0173 (0.0147)	KLLoss 0.1377 (0.1023)	MaskLoss 0.8436 (0.4345)	MaskBCELoss 0.3114 (0.1087)	MaskDICELoss 0.5322 (0.3258)
Epoch: [2][267/500]	Time  8.030 ( 8.030)	Loss 6.6069 (4.8737)	CeLoss 0.2539 (0.5065)	SegCLSLoss 0.0171 (0.0171)	KLLoss 0.1123 (0.1090)	MaskLoss 0.9853 (0.6376)	MaskBCELoss 0.2751 (0.1418)	MaskDICELoss 0.7102 (0.4958)
Epoch: [2][268/500]	Time  8.836 ( 8.836)	Loss 3.2467 (4.3003)	CeLoss 0.2773 (0.7385)	SegCLSLoss 0.0154 (0.0130)	KLLoss 0.0869 (0.0870)	MaskLoss 0.5359 (0.5067)	MaskBCELoss 0.2352 (0.0976)	MaskDICELoss 0.3006 (0.4091)
Epoch: [2][269/500]	Time  9.987 ( 9.987)	Loss 5.4478 (6.5564)	CeLoss 0.2148 (0.3036)	SegCLSLoss 0.0206 (0.0264)	KLLoss 0.1543 (0.1425)	MaskLoss 0.7569 (1.0132)	MaskBCELoss 0.1643 (0.3346)	MaskDICELoss 0.5925 (0.6785)
[2025-03-04 05:39:16,593] [INFO] [logging.py:96:log_dist] [Rank 0] step=1270, skipped=0, lr=[0.0002706626506024096], mom=[(0.9, 0.95)]
[2025-03-04 05:39:16,599] [INFO] [timer.py:215:stop] epoch=0/micro_step=12700/global_step=1270, RunningAvgSamplesPerSec=1.038919206646053, CurrSamplesPerSec=1.1593627317272082, MemAllocated=56.72GB, MaxMemAllocated=62.82GB
Epoch: [2][270/500]	Time  8.628 ( 8.628)	Loss 0.3984 (5.3956)	CeLoss 0.3984 (0.4577)	SegCLSLoss 0.0000 (0.0179)	KLLoss 0.0000 (0.1217)	MaskLoss 0.0000 (0.7724)	MaskBCELoss 0.0000 (0.2287)	MaskDICELoss 0.0000 (0.5437)
Epoch: [2][271/500]	Time  7.942 ( 7.942)	Loss 8.0430 (3.6614)	CeLoss 0.1846 (0.6424)	SegCLSLoss 0.0356 (0.0149)	KLLoss 0.1992 (0.0878)	MaskLoss 1.2071 (0.4508)	MaskBCELoss 0.3361 (0.1137)	MaskDICELoss 0.8711 (0.3370)
Epoch: [2][272/500]	Time  9.536 ( 9.536)	Loss 6.0223 (4.6448)	CeLoss 0.2031 (0.2818)	SegCLSLoss 0.0291 (0.0226)	KLLoss 0.1582 (0.1228)	MaskLoss 0.7491 (0.6376)	MaskBCELoss 0.0576 (0.1454)	MaskDICELoss 0.6915 (0.4923)
Epoch: [2][273/500]	Time  9.775 ( 9.775)	Loss 0.4766 (4.9881)	CeLoss 0.4766 (0.5120)	SegCLSLoss 0.0000 (0.0189)	KLLoss 0.0000 (0.1065)	MaskLoss 0.0000 (0.6150)	MaskBCELoss 0.0000 (0.0933)	MaskDICELoss 0.0000 (0.5217)
Epoch: [2][274/500]	Time  8.467 ( 8.467)	Loss 1.2344 (4.7522)	CeLoss 1.2344 (0.5813)	SegCLSLoss 0.0000 (0.0208)	KLLoss 0.0000 (0.1374)	MaskLoss 0.0000 (0.6068)	MaskBCELoss 0.0000 (0.1386)	MaskDICELoss 0.0000 (0.4682)
Epoch: [2][275/500]	Time  9.261 ( 9.261)	Loss 0.6172 (5.1886)	CeLoss 0.6172 (0.3786)	SegCLSLoss 0.0000 (0.0218)	KLLoss 0.0000 (0.1407)	MaskLoss 0.0000 (0.7508)	MaskBCELoss 0.0000 (0.2247)	MaskDICELoss 0.0000 (0.5262)
Epoch: [2][276/500]	Time  9.774 ( 9.774)	Loss 0.0708 (4.5775)	CeLoss 0.0708 (0.3730)	SegCLSLoss 0.0000 (0.0161)	KLLoss 0.0000 (0.1089)	MaskLoss 0.0000 (0.6744)	MaskBCELoss 0.0000 (0.2178)	MaskDICELoss 0.0000 (0.4565)
Epoch: [2][277/500]	Time  8.006 ( 8.006)	Loss 5.4740 (4.3291)	CeLoss 0.2305 (0.8553)	SegCLSLoss 0.0277 (0.0114)	KLLoss 0.2031 (0.0770)	MaskLoss 0.6350 (0.5283)	MaskBCELoss 0.0088 (0.1392)	MaskDICELoss 0.6261 (0.3891)
Epoch: [2][278/500]	Time  9.117 ( 9.117)	Loss 1.2500 (4.1799)	CeLoss 1.2500 (0.4168)	SegCLSLoss 0.0000 (0.0160)	KLLoss 0.0000 (0.0896)	MaskLoss 0.0000 (0.5565)	MaskBCELoss 0.0000 (0.1311)	MaskDICELoss 0.0000 (0.4254)
Epoch: [2][279/500]	Time  8.330 ( 8.330)	Loss 1.3438 (5.2451)	CeLoss 1.3438 (0.4574)	SegCLSLoss 0.0000 (0.0217)	KLLoss 0.0000 (0.1318)	MaskLoss 0.0000 (0.7046)	MaskBCELoss 0.0000 (0.1654)	MaskDICELoss 0.0000 (0.5393)
[2025-03-04 05:40:45,384] [INFO] [logging.py:96:log_dist] [Rank 0] step=1280, skipped=0, lr=[0.0002704216867469879], mom=[(0.9, 0.95)]
[2025-03-04 05:40:45,390] [INFO] [timer.py:215:stop] epoch=0/micro_step=12800/global_step=1280, RunningAvgSamplesPerSec=1.03955131048842, CurrSamplesPerSec=1.165572754674611, MemAllocated=57.56GB, MaxMemAllocated=62.82GB
Epoch: [2][280/500]	Time  8.582 ( 8.582)	Loss 7.8417 (7.1346)	CeLoss 0.2471 (0.2334)	SegCLSLoss 0.0170 (0.0326)	KLLoss 0.1436 (0.1831)	MaskLoss 0.9971 (1.0813)	MaskBCELoss 0.0889 (0.3248)	MaskDICELoss 0.9082 (0.7566)
Epoch: [2][281/500]	Time  7.051 ( 7.051)	Loss 5.7999 (2.5163)	CeLoss 0.2500 (0.5062)	SegCLSLoss 0.0305 (0.0129)	KLLoss 0.1904 (0.0624)	MaskLoss 1.3126 (0.3632)	MaskBCELoss 0.8596 (0.1607)	MaskDICELoss 0.4530 (0.2025)
Epoch: [2][282/500]	Time  8.668 ( 8.668)	Loss 1.5547 (4.4721)	CeLoss 1.5547 (0.6483)	SegCLSLoss 0.0000 (0.0138)	KLLoss 0.0000 (0.0980)	MaskLoss 0.0000 (0.5411)	MaskBCELoss 0.0000 (0.1016)	MaskDICELoss 0.0000 (0.4395)
Epoch: [2][283/500]	Time 10.779 (10.779)	Loss 8.2909 (6.1884)	CeLoss 0.1953 (0.3230)	SegCLSLoss 0.0227 (0.0210)	KLLoss 0.1953 (0.1420)	MaskLoss 1.0175 (0.8444)	MaskBCELoss 0.0419 (0.1738)	MaskDICELoss 0.9756 (0.6706)
Epoch: [2][284/500]	Time  7.245 ( 7.245)	Loss 1.2422 (3.7895)	CeLoss 1.2422 (0.7718)	SegCLSLoss 0.0000 (0.0152)	KLLoss 0.0000 (0.0989)	MaskLoss 0.0000 (0.3935)	MaskBCELoss 0.0000 (0.0395)	MaskDICELoss 0.0000 (0.3540)
Epoch: [2][285/500]	Time 10.522 (10.522)	Loss 4.5098 (5.4151)	CeLoss 0.2656 (0.2398)	SegCLSLoss 0.0166 (0.0203)	KLLoss 0.1226 (0.1413)	MaskLoss 0.6339 (0.7767)	MaskBCELoss 0.1597 (0.1984)	MaskDICELoss 0.4742 (0.5784)
Epoch: [2][286/500]	Time  9.623 ( 9.623)	Loss 4.0879 (5.5887)	CeLoss 0.2451 (0.3491)	SegCLSLoss 0.0156 (0.0214)	KLLoss 0.1357 (0.1500)	MaskLoss 0.6145 (0.8195)	MaskBCELoss 0.2024 (0.2462)	MaskDICELoss 0.4120 (0.5733)
Epoch: [2][287/500]	Time  7.560 ( 7.560)	Loss 5.1717 (3.2686)	CeLoss 0.2656 (0.6405)	SegCLSLoss 0.0151 (0.0090)	KLLoss 0.1416 (0.0653)	MaskLoss 0.7467 (0.3878)	MaskBCELoss 0.2027 (0.0907)	MaskDICELoss 0.5440 (0.2971)
Epoch: [2][288/500]	Time 10.468 (10.468)	Loss 5.7433 (4.8603)	CeLoss 0.2246 (0.3089)	SegCLSLoss 0.0175 (0.0196)	KLLoss 0.1475 (0.1473)	MaskLoss 0.7948 (0.7335)	MaskBCELoss 0.1660 (0.2456)	MaskDICELoss 0.6288 (0.4879)
Epoch: [2][289/500]	Time  9.974 ( 9.974)	Loss 0.9648 (5.8806)	CeLoss 0.9648 (0.4003)	SegCLSLoss 0.0000 (0.0177)	KLLoss 0.0000 (0.1281)	MaskLoss 0.0000 (0.7778)	MaskBCELoss 0.0000 (0.1466)	MaskDICELoss 0.0000 (0.6312)
[2025-03-04 05:42:16,901] [INFO] [logging.py:96:log_dist] [Rank 0] step=1290, skipped=0, lr=[0.00027018072289156623], mom=[(0.9, 0.95)]
[2025-03-04 05:42:16,907] [INFO] [timer.py:215:stop] epoch=0/micro_step=12900/global_step=1290, RunningAvgSamplesPerSec=1.039945352365096, CurrSamplesPerSec=1.0389872387231855, MemAllocated=57.25GB, MaxMemAllocated=62.82GB
Epoch: [2][290/500]	Time  9.627 ( 9.627)	Loss 4.9069 (4.8628)	CeLoss 0.2695 (0.4684)	SegCLSLoss 0.0161 (0.0182)	KLLoss 0.1123 (0.1183)	MaskLoss 0.6327 (0.6317)	MaskBCELoss 0.0909 (0.1310)	MaskDICELoss 0.5418 (0.5006)
Epoch: [2][291/500]	Time  9.546 ( 9.546)	Loss 7.4014 (5.6642)	CeLoss 0.2324 (0.3178)	SegCLSLoss 0.0266 (0.0215)	KLLoss 0.1719 (0.1423)	MaskLoss 0.9113 (0.7335)	MaskBCELoss 0.0511 (0.1125)	MaskDICELoss 0.8601 (0.6210)
Epoch: [2][292/500]	Time  8.592 ( 8.592)	Loss 3.4772 (3.7542)	CeLoss 0.3574 (0.6157)	SegCLSLoss 0.0153 (0.0140)	KLLoss 0.1416 (0.0937)	MaskLoss 0.6160 (0.4694)	MaskBCELoss 0.3265 (0.1196)	MaskDICELoss 0.2895 (0.3498)
Epoch: [2][293/500]	Time  9.760 ( 9.760)	Loss 8.6533 (5.2930)	CeLoss 0.1836 (0.3877)	SegCLSLoss 0.0339 (0.0177)	KLLoss 0.2158 (0.1213)	MaskLoss 1.1772 (0.6905)	MaskBCELoss 0.1967 (0.1248)	MaskDICELoss 0.9805 (0.5657)
Epoch: [2][294/500]	Time  9.179 ( 9.179)	Loss 6.9197 (5.5312)	CeLoss 0.2295 (0.5446)	SegCLSLoss 0.0334 (0.0155)	KLLoss 0.1699 (0.1100)	MaskLoss 0.8396 (1.0767)	MaskBCELoss 0.0355 (0.6241)	MaskDICELoss 0.8041 (0.4526)
Epoch: [2][295/500]	Time 10.677 (10.677)	Loss 8.2732 (4.3986)	CeLoss 0.1963 (0.2709)	SegCLSLoss 0.0303 (0.0195)	KLLoss 0.2080 (0.1239)	MaskLoss 1.7810 (0.6892)	MaskBCELoss 1.0655 (0.2532)	MaskDICELoss 0.7155 (0.4360)
Epoch: [2][296/500]	Time 10.408 (10.408)	Loss 1.4297 (5.3543)	CeLoss 1.4297 (0.3626)	SegCLSLoss 0.0000 (0.0184)	KLLoss 0.0000 (0.1394)	MaskLoss 0.0000 (0.7880)	MaskBCELoss 0.0000 (0.2435)	MaskDICELoss 0.0000 (0.5444)
Epoch: [2][297/500]	Time  9.955 ( 9.955)	Loss 0.9336 (4.5913)	CeLoss 0.9336 (0.3936)	SegCLSLoss 0.0000 (0.0199)	KLLoss 0.0000 (0.1292)	MaskLoss 0.0000 (0.6729)	MaskBCELoss 0.0000 (0.2207)	MaskDICELoss 0.0000 (0.4521)
Epoch: [2][298/500]	Time  9.475 ( 9.475)	Loss 7.7467 (6.0614)	CeLoss 0.1592 (0.4095)	SegCLSLoss 0.0549 (0.0249)	KLLoss 0.2217 (0.1616)	MaskLoss 1.3662 (0.8291)	MaskBCELoss 0.5986 (0.1925)	MaskDICELoss 0.7677 (0.6366)
Epoch: [2][299/500]	Time  8.832 ( 8.832)	Loss 1.2422 (6.4815)	CeLoss 1.2422 (0.4369)	SegCLSLoss 0.0000 (0.0230)	KLLoss 0.0000 (0.1498)	MaskLoss 0.0000 (0.8558)	MaskBCELoss 0.0000 (0.1605)	MaskDICELoss 0.0000 (0.6953)
[2025-03-04 05:43:52,696] [INFO] [logging.py:96:log_dist] [Rank 0] step=1300, skipped=0, lr=[0.00026993975903614456], mom=[(0.9, 0.95)]
[2025-03-04 05:43:52,702] [INFO] [timer.py:215:stop] epoch=0/micro_step=13000/global_step=1300, RunningAvgSamplesPerSec=1.03997717099863, CurrSamplesPerSec=1.0673874767826967, MemAllocated=57.26GB, MaxMemAllocated=62.82GB
Epoch: [2][300/500]	Time  9.371 ( 9.371)	Loss 7.9477 (6.4038)	CeLoss 0.1904 (0.4030)	SegCLSLoss 0.0315 (0.0253)	KLLoss 0.1934 (0.1574)	MaskLoss 1.0474 (0.8801)	MaskBCELoss 0.1383 (0.2017)	MaskDICELoss 0.9091 (0.6785)
Epoch: [2][301/500]	Time  7.374 ( 7.374)	Loss 4.2636 (3.6488)	CeLoss 0.2285 (0.8590)	SegCLSLoss 0.0164 (0.0118)	KLLoss 0.1348 (0.0777)	MaskLoss 0.6974 (0.4389)	MaskBCELoss 0.2811 (0.1342)	MaskDICELoss 0.4163 (0.3047)
Epoch: [2][302/500]	Time 10.875 (10.875)	Loss 8.6178 (6.1688)	CeLoss 0.2314 (0.2058)	SegCLSLoss 0.0186 (0.0277)	KLLoss 0.1445 (0.1647)	MaskLoss 1.5229 (0.9483)	MaskBCELoss 0.6584 (0.3003)	MaskDICELoss 0.8645 (0.6480)
Epoch: [2][303/500]	Time  9.261 ( 9.261)	Loss 4.0355 (6.3298)	CeLoss 0.2314 (0.3143)	SegCLSLoss 0.0258 (0.0261)	KLLoss 0.1523 (0.1597)	MaskLoss 0.6223 (1.0214)	MaskBCELoss 0.2232 (0.3880)	MaskDICELoss 0.3991 (0.6333)
Epoch: [2][304/500]	Time  9.548 ( 9.548)	Loss 4.7226 (4.7946)	CeLoss 0.3125 (0.4556)	SegCLSLoss 0.0164 (0.0161)	KLLoss 0.0942 (0.1164)	MaskLoss 0.6468 (0.6034)	MaskBCELoss 0.1444 (0.1020)	MaskDICELoss 0.5025 (0.5014)
Epoch: [2][305/500]	Time 10.684 (10.684)	Loss 6.9440 (5.2633)	CeLoss 0.2871 (0.2517)	SegCLSLoss 0.0189 (0.0199)	KLLoss 0.1416 (0.1357)	MaskLoss 1.1529 (0.8093)	MaskBCELoss 0.4528 (0.2681)	MaskDICELoss 0.7001 (0.5413)
Epoch: [2][306/500]	Time 10.059 (10.059)	Loss 5.9234 (5.4696)	CeLoss 0.2324 (0.3358)	SegCLSLoss 0.0269 (0.0250)	KLLoss 0.1328 (0.1585)	MaskLoss 0.7807 (0.7690)	MaskBCELoss 0.1169 (0.1983)	MaskDICELoss 0.6638 (0.5708)
Epoch: [2][307/500]	Time  9.565 ( 9.565)	Loss 1.2203 (4.7606)	CeLoss 0.2949 (0.3977)	SegCLSLoss 0.0164 (0.0182)	KLLoss 0.1973 (0.1384)	MaskLoss 0.0924 (0.6646)	MaskBCELoss 0.0032 (0.1836)	MaskDICELoss 0.0892 (0.4810)
Epoch: [2][308/500]	Time  7.923 ( 7.923)	Loss 8.1168 (4.5096)	CeLoss 0.2158 (0.4786)	SegCLSLoss 0.0337 (0.0187)	KLLoss 0.2080 (0.1215)	MaskLoss 1.3116 (0.6290)	MaskBCELoss 0.4696 (0.1885)	MaskDICELoss 0.8420 (0.4404)
Epoch: [2][309/500]	Time 10.051 (10.051)	Loss 8.4561 (6.5971)	CeLoss 0.2812 (0.3080)	SegCLSLoss 0.0266 (0.0227)	KLLoss 0.1582 (0.1547)	MaskLoss 1.0153 (0.9633)	MaskBCELoss 0.0200 (0.2639)	MaskDICELoss 0.9954 (0.6994)
[2025-03-04 05:45:28,235] [INFO] [logging.py:96:log_dist] [Rank 0] step=1310, skipped=0, lr=[0.0002696987951807229], mom=[(0.9, 0.95)]
[2025-03-04 05:45:28,241] [INFO] [timer.py:215:stop] epoch=0/micro_step=13100/global_step=1310, RunningAvgSamplesPerSec=1.0400296026588889, CurrSamplesPerSec=0.9806737568090393, MemAllocated=57.26GB, MaxMemAllocated=62.82GB
Epoch: [2][310/500]	Time 10.199 (10.199)	Loss 8.8135 (4.5610)	CeLoss 0.2041 (0.4697)	SegCLSLoss 0.0265 (0.0167)	KLLoss 0.2285 (0.1197)	MaskLoss 1.2240 (0.6133)	MaskBCELoss 0.2373 (0.1572)	MaskDICELoss 0.9867 (0.4561)
Epoch: [2][311/500]	Time  8.651 ( 8.651)	Loss 8.7019 (4.8102)	CeLoss 0.2354 (0.4665)	SegCLSLoss 0.0383 (0.0185)	KLLoss 0.2754 (0.1327)	MaskLoss 1.3760 (0.6261)	MaskBCELoss 0.4729 (0.1345)	MaskDICELoss 0.9031 (0.4916)
Epoch: [2][312/500]	Time 10.745 (10.745)	Loss 5.5454 (5.9299)	CeLoss 0.2891 (0.4244)	SegCLSLoss 0.0186 (0.0203)	KLLoss 0.1475 (0.1458)	MaskLoss 0.8594 (0.8293)	MaskBCELoss 0.2959 (0.2143)	MaskDICELoss 0.5635 (0.6151)
Epoch: [2][313/500]	Time  9.276 ( 9.276)	Loss 4.1670 (4.8589)	CeLoss 0.2910 (0.4233)	SegCLSLoss 0.0166 (0.0163)	KLLoss 0.1221 (0.1228)	MaskLoss 0.7044 (0.6679)	MaskBCELoss 0.3150 (0.1732)	MaskDICELoss 0.3894 (0.4948)
Epoch: [2][314/500]	Time 10.610 (10.610)	Loss 7.2784 (6.1261)	CeLoss 0.2773 (0.3895)	SegCLSLoss 0.0166 (0.0190)	KLLoss 0.1328 (0.1356)	MaskLoss 1.0946 (0.8955)	MaskBCELoss 0.3160 (0.2621)	MaskDICELoss 0.7785 (0.6334)
Epoch: [2][315/500]	Time  7.892 ( 7.892)	Loss 7.3272 (4.1873)	CeLoss 0.2617 (0.5364)	SegCLSLoss 0.0164 (0.0223)	KLLoss 0.1123 (0.1168)	MaskLoss 1.0288 (0.5948)	MaskBCELoss 0.2143 (0.2059)	MaskDICELoss 0.8145 (0.3889)
Epoch: [2][316/500]	Time 10.912 (10.912)	Loss 9.0915 (6.5301)	CeLoss 0.2461 (0.2627)	SegCLSLoss 0.0167 (0.0206)	KLLoss 0.1406 (0.1346)	MaskLoss 1.4131 (0.8924)	MaskBCELoss 0.4346 (0.1694)	MaskDICELoss 0.9785 (0.7229)
Epoch: [2][317/500]	Time 10.352 (10.352)	Loss 7.6079 (6.3435)	CeLoss 0.2734 (0.4103)	SegCLSLoss 0.0223 (0.0215)	KLLoss 0.1748 (0.1593)	MaskLoss 0.9072 (0.8478)	MaskBCELoss 0.0185 (0.1698)	MaskDICELoss 0.8887 (0.6780)
Epoch: [2][318/500]	Time  8.634 ( 8.634)	Loss 1.4141 (6.2647)	CeLoss 1.4141 (0.4750)	SegCLSLoss 0.0000 (0.0295)	KLLoss 0.0000 (0.1709)	MaskLoss 0.0000 (0.8636)	MaskBCELoss 0.0000 (0.2174)	MaskDICELoss 0.0000 (0.6462)
Epoch: [2][319/500]	Time  8.794 ( 8.794)	Loss 8.5855 (5.8125)	CeLoss 0.2695 (0.4671)	SegCLSLoss 0.0194 (0.0218)	KLLoss 0.1699 (0.1470)	MaskLoss 1.0682 (0.8034)	MaskBCELoss 0.0682 (0.2066)	MaskDICELoss 1.0000 (0.5968)
[2025-03-04 05:47:04,106] [INFO] [logging.py:96:log_dist] [Rank 0] step=1320, skipped=0, lr=[0.0002694578313253012], mom=[(0.9, 0.95)]
[2025-03-04 05:47:04,112] [INFO] [timer.py:215:stop] epoch=0/micro_step=13200/global_step=1320, RunningAvgSamplesPerSec=1.0400538916653879, CurrSamplesPerSec=0.9996235357911263, MemAllocated=57.26GB, MaxMemAllocated=62.82GB
Epoch: [2][320/500]	Time 10.005 (10.005)	Loss 8.8646 (5.4123)	CeLoss 0.2832 (0.3743)	SegCLSLoss 0.0317 (0.0208)	KLLoss 0.1855 (0.1377)	MaskLoss 1.2197 (0.7315)	MaskBCELoss 0.2295 (0.1604)	MaskDICELoss 0.9901 (0.5711)
Epoch: [2][321/500]	Time 10.491 (10.491)	Loss 0.0566 (4.7584)	CeLoss 0.0566 (0.3063)	SegCLSLoss 0.0000 (0.0157)	KLLoss 0.0000 (0.1137)	MaskLoss 0.0000 (0.6871)	MaskBCELoss 0.0000 (0.1944)	MaskDICELoss 0.0000 (0.4927)
Epoch: [2][322/500]	Time  9.226 ( 9.226)	Loss 0.0771 (3.8393)	CeLoss 0.0771 (0.3655)	SegCLSLoss 0.0000 (0.0126)	KLLoss 0.0000 (0.0862)	MaskLoss 0.0000 (0.4919)	MaskBCELoss 0.0000 (0.0923)	MaskDICELoss 0.0000 (0.3996)
Epoch: [2][323/500]	Time  9.242 ( 9.242)	Loss 6.7415 (4.2655)	CeLoss 0.2793 (0.4670)	SegCLSLoss 0.0179 (0.0199)	KLLoss 0.1143 (0.1062)	MaskLoss 0.9480 (0.5958)	MaskBCELoss 0.2075 (0.1807)	MaskDICELoss 0.7405 (0.4151)
Epoch: [2][324/500]	Time  9.567 ( 9.567)	Loss 5.6791 (4.8660)	CeLoss 0.2812 (0.5958)	SegCLSLoss 0.0211 (0.0168)	KLLoss 0.1235 (0.1129)	MaskLoss 0.7029 (0.6105)	MaskBCELoss 0.0597 (0.1225)	MaskDICELoss 0.6432 (0.4880)
Epoch: [2][325/500]	Time  9.958 ( 9.958)	Loss 1.1406 (5.0686)	CeLoss 1.1406 (0.4706)	SegCLSLoss 0.0000 (0.0202)	KLLoss 0.0000 (0.1313)	MaskLoss 0.0000 (0.7144)	MaskBCELoss 0.0000 (0.2098)	MaskDICELoss 0.0000 (0.5046)
Epoch: [2][326/500]	Time  8.743 ( 8.743)	Loss 1.6641 (4.4671)	CeLoss 1.6641 (0.4364)	SegCLSLoss 0.0000 (0.0141)	KLLoss 0.0000 (0.1013)	MaskLoss 0.0000 (0.5924)	MaskBCELoss 0.0000 (0.1361)	MaskDICELoss 0.0000 (0.4562)
Epoch: [2][327/500]	Time 10.726 (10.726)	Loss 8.1593 (5.6609)	CeLoss 0.1650 (0.1933)	SegCLSLoss 0.0310 (0.0297)	KLLoss 0.1973 (0.1708)	MaskLoss 1.3367 (0.8100)	MaskBCELoss 0.4856 (0.1996)	MaskDICELoss 0.8511 (0.6104)
Epoch: [2][328/500]	Time 10.673 (10.673)	Loss 3.8145 (5.0249)	CeLoss 0.2373 (0.4568)	SegCLSLoss 0.0205 (0.0183)	KLLoss 0.1641 (0.1303)	MaskLoss 0.4701 (0.6624)	MaskBCELoss 0.0598 (0.1452)	MaskDICELoss 0.4104 (0.5173)
Epoch: [2][329/500]	Time  8.013 ( 8.013)	Loss 7.6818 (6.1745)	CeLoss 0.2363 (0.3946)	SegCLSLoss 0.0282 (0.0238)	KLLoss 0.2109 (0.1688)	MaskLoss 1.0045 (0.7882)	MaskBCELoss 0.1359 (0.1177)	MaskDICELoss 0.8686 (0.6705)
[2025-03-04 05:48:39,968] [INFO] [logging.py:96:log_dist] [Rank 0] step=1330, skipped=0, lr=[0.00026921686746987953], mom=[(0.9, 0.95)]
[2025-03-04 05:48:39,974] [INFO] [timer.py:215:stop] epoch=0/micro_step=13300/global_step=1330, RunningAvgSamplesPerSec=1.0400786096262256, CurrSamplesPerSec=1.0844988308222738, MemAllocated=56.69GB, MaxMemAllocated=62.82GB
Epoch: [2][330/500]	Time  9.223 ( 9.223)	Loss 1.9453 (6.6631)	CeLoss 1.9453 (0.3941)	SegCLSLoss 0.0000 (0.0291)	KLLoss 0.0000 (0.1893)	MaskLoss 0.0000 (0.9042)	MaskBCELoss 0.0000 (0.1948)	MaskDICELoss 0.0000 (0.7095)
Epoch: [2][331/500]	Time 10.704 (10.704)	Loss 5.8305 (5.7562)	CeLoss 0.1934 (0.2114)	SegCLSLoss 0.0310 (0.0252)	KLLoss 0.1904 (0.1629)	MaskLoss 0.7455 (0.8584)	MaskBCELoss 0.0887 (0.2496)	MaskDICELoss 0.6568 (0.6088)
Epoch: [2][332/500]	Time  8.757 ( 8.757)	Loss 6.5409 (3.9785)	CeLoss 0.1836 (0.5170)	SegCLSLoss 0.0320 (0.0181)	KLLoss 0.2217 (0.1059)	MaskLoss 1.1123 (0.5073)	MaskBCELoss 0.4632 (0.1187)	MaskDICELoss 0.6491 (0.3886)
Epoch: [2][333/500]	Time 10.457 (10.457)	Loss 6.8227 (6.0670)	CeLoss 0.2285 (0.2757)	SegCLSLoss 0.0240 (0.0239)	KLLoss 0.1553 (0.1411)	MaskLoss 0.9296 (0.8300)	MaskBCELoss 0.1681 (0.1669)	MaskDICELoss 0.7615 (0.6631)
Epoch: [2][334/500]	Time 10.223 (10.223)	Loss 8.4019 (6.0641)	CeLoss 0.1855 (0.3084)	SegCLSLoss 0.0305 (0.0202)	KLLoss 0.2373 (0.1534)	MaskLoss 1.0100 (0.8887)	MaskBCELoss 0.0193 (0.2528)	MaskDICELoss 0.9907 (0.6358)
Epoch: [2][335/500]	Time  7.677 ( 7.677)	Loss 6.4427 (4.1686)	CeLoss 0.2061 (0.5714)	SegCLSLoss 0.0374 (0.0153)	KLLoss 0.2383 (0.1031)	MaskLoss 1.0227 (0.5533)	MaskBCELoss 0.3670 (0.1567)	MaskDICELoss 0.6557 (0.3966)
Epoch: [2][336/500]	Time  9.694 ( 9.694)	Loss 7.8528 (5.0631)	CeLoss 0.1865 (0.3044)	SegCLSLoss 0.0325 (0.0185)	KLLoss 0.2080 (0.1211)	MaskLoss 1.2154 (0.7292)	MaskBCELoss 0.3801 (0.2008)	MaskDICELoss 0.8353 (0.5284)
Epoch: [2][337/500]	Time 10.689 (10.689)	Loss 5.1627 (6.0770)	CeLoss 0.2988 (0.3401)	SegCLSLoss 0.0178 (0.0264)	KLLoss 0.1196 (0.1664)	MaskLoss 0.6980 (0.8358)	MaskBCELoss 0.1416 (0.1882)	MaskDICELoss 0.5565 (0.6475)
Epoch: [2][338/500]	Time  9.406 ( 9.406)	Loss 7.7181 (6.6316)	CeLoss 0.3242 (0.2692)	SegCLSLoss 0.0182 (0.0229)	KLLoss 0.1045 (0.1662)	MaskLoss 1.4390 (0.9695)	MaskBCELoss 0.7052 (0.2619)	MaskDICELoss 0.7338 (0.7076)
Epoch: [2][339/500]	Time  8.516 ( 8.516)	Loss 1.0625 (4.7037)	CeLoss 1.0625 (0.4682)	SegCLSLoss 0.0000 (0.0210)	KLLoss 0.0000 (0.1236)	MaskLoss 0.0000 (0.6366)	MaskBCELoss 0.0000 (0.1652)	MaskDICELoss 0.0000 (0.4714)
[2025-03-04 05:50:16,215] [INFO] [logging.py:96:log_dist] [Rank 0] step=1340, skipped=0, lr=[0.0002689759036144578], mom=[(0.9, 0.95)]
[2025-03-04 05:50:16,221] [INFO] [timer.py:215:stop] epoch=0/micro_step=13400/global_step=1340, RunningAvgSamplesPerSec=1.0400719215051413, CurrSamplesPerSec=0.9879985951367327, MemAllocated=56.96GB, MaxMemAllocated=62.82GB
Epoch: [2][340/500]	Time 10.125 (10.125)	Loss 6.3112 (5.5797)	CeLoss 0.2119 (0.2252)	SegCLSLoss 0.0366 (0.0211)	KLLoss 0.2676 (0.1532)	MaskLoss 0.8429 (0.7822)	MaskBCELoss 0.1550 (0.1778)	MaskDICELoss 0.6879 (0.6044)
Epoch: [2][341/500]	Time  8.970 ( 8.970)	Loss 1.0859 (5.1370)	CeLoss 1.0859 (0.5332)	SegCLSLoss 0.0000 (0.0202)	KLLoss 0.0000 (0.1150)	MaskLoss 0.0000 (0.6894)	MaskBCELoss 0.0000 (0.1727)	MaskDICELoss 0.0000 (0.5167)
Epoch: [2][342/500]	Time  9.001 ( 9.001)	Loss 7.8793 (4.3852)	CeLoss 0.1338 (0.5519)	SegCLSLoss 0.0540 (0.0170)	KLLoss 0.2363 (0.0986)	MaskLoss 1.2305 (0.5597)	MaskBCELoss 0.3939 (0.1252)	MaskDICELoss 0.8366 (0.4345)
Epoch: [2][343/500]	Time  9.260 ( 9.260)	Loss 1.5234 (4.5426)	CeLoss 1.5234 (0.4987)	SegCLSLoss 0.0000 (0.0155)	KLLoss 0.0000 (0.1076)	MaskLoss 0.0000 (0.6429)	MaskBCELoss 0.0000 (0.2024)	MaskDICELoss 0.0000 (0.4405)
Epoch: [2][344/500]	Time  8.880 ( 8.880)	Loss 7.1831 (5.0602)	CeLoss 0.2178 (0.3220)	SegCLSLoss 0.0303 (0.0275)	KLLoss 0.2139 (0.1468)	MaskLoss 1.2936 (0.8539)	MaskBCELoss 0.6018 (0.3756)	MaskDICELoss 0.6918 (0.4783)
Epoch: [2][345/500]	Time  9.453 ( 9.453)	Loss 0.0845 (3.4322)	CeLoss 0.0845 (0.3648)	SegCLSLoss 0.0000 (0.0143)	KLLoss 0.0000 (0.0862)	MaskLoss 0.0000 (0.4874)	MaskBCELoss 0.0000 (0.1542)	MaskDICELoss 0.0000 (0.3332)
Epoch: [2][346/500]	Time  9.568 ( 9.568)	Loss 3.8343 (5.1063)	CeLoss 0.2295 (0.3818)	SegCLSLoss 0.0175 (0.0315)	KLLoss 0.1084 (0.1510)	MaskLoss 0.4878 (0.7068)	MaskBCELoss 0.0693 (0.1828)	MaskDICELoss 0.4185 (0.5240)
Epoch: [2][347/500]	Time 10.670 (10.670)	Loss 8.3906 (6.5200)	CeLoss 0.2041 (0.4101)	SegCLSLoss 0.0327 (0.0194)	KLLoss 0.1895 (0.1208)	MaskLoss 1.2830 (0.8935)	MaskBCELoss 0.3806 (0.1948)	MaskDICELoss 0.9024 (0.6987)
Epoch: [2][348/500]	Time  9.595 ( 9.595)	Loss 0.7188 (4.6219)	CeLoss 0.7188 (0.4930)	SegCLSLoss 0.0000 (0.0185)	KLLoss 0.0000 (0.1150)	MaskLoss 0.0000 (0.5821)	MaskBCELoss 0.0000 (0.1087)	MaskDICELoss 0.0000 (0.4734)
Epoch: [2][349/500]	Time  8.694 ( 8.694)	Loss 9.1066 (5.3706)	CeLoss 0.2676 (0.4541)	SegCLSLoss 0.0211 (0.0184)	KLLoss 0.1396 (0.1244)	MaskLoss 1.4542 (0.6790)	MaskBCELoss 0.4909 (0.1082)	MaskDICELoss 0.9634 (0.5708)
[2025-03-04 05:51:50,055] [INFO] [logging.py:96:log_dist] [Rank 0] step=1350, skipped=0, lr=[0.0002687349397590361], mom=[(0.9, 0.95)]
[2025-03-04 05:51:50,061] [INFO] [timer.py:215:stop] epoch=0/micro_step=13500/global_step=1350, RunningAvgSamplesPerSec=1.0402585080313822, CurrSamplesPerSec=1.0260263017480713, MemAllocated=57.26GB, MaxMemAllocated=62.82GB
Epoch: [2][350/500]	Time  9.748 ( 9.748)	Loss 8.4962 (6.2609)	CeLoss 0.1953 (0.3840)	SegCLSLoss 0.0212 (0.0255)	KLLoss 0.2520 (0.1601)	MaskLoss 1.0252 (0.8215)	MaskBCELoss 0.0270 (0.1447)	MaskDICELoss 0.9981 (0.6769)
Epoch: [2][351/500]	Time 10.657 (10.657)	Loss 6.9919 (5.8396)	CeLoss 0.2021 (0.2911)	SegCLSLoss 0.0415 (0.0241)	KLLoss 0.2598 (0.1545)	MaskLoss 0.9483 (0.8294)	MaskBCELoss 0.1796 (0.2088)	MaskDICELoss 0.7688 (0.6206)
Epoch: [2][352/500]	Time  7.732 ( 7.732)	Loss 6.3896 (3.7801)	CeLoss 0.2061 (0.5429)	SegCLSLoss 0.0332 (0.0165)	KLLoss 0.2012 (0.1018)	MaskLoss 1.0471 (0.4862)	MaskBCELoss 0.4018 (0.1269)	MaskDICELoss 0.6453 (0.3592)
Epoch: [2][353/500]	Time  9.789 ( 9.789)	Loss 0.1030 (4.4986)	CeLoss 0.1030 (0.3617)	SegCLSLoss 0.0000 (0.0172)	KLLoss 0.0000 (0.1196)	MaskLoss 0.0000 (0.6691)	MaskBCELoss 0.0000 (0.2240)	MaskDICELoss 0.0000 (0.4451)
Epoch: [2][354/500]	Time  9.218 ( 9.218)	Loss 8.0809 (4.5092)	CeLoss 0.2656 (0.3857)	SegCLSLoss 0.0239 (0.0206)	KLLoss 0.1367 (0.1281)	MaskLoss 1.0184 (0.6222)	MaskBCELoss 0.0801 (0.1655)	MaskDICELoss 0.9383 (0.4567)
Epoch: [2][355/500]	Time  8.209 ( 8.209)	Loss 1.2109 (3.7384)	CeLoss 1.2109 (0.7383)	SegCLSLoss 0.0000 (0.0117)	KLLoss 0.0000 (0.0833)	MaskLoss 0.0000 (0.4384)	MaskBCELoss 0.0000 (0.0994)	MaskDICELoss 0.0000 (0.3390)
Epoch: [2][356/500]	Time  9.130 ( 9.130)	Loss 1.3281 (3.7647)	CeLoss 1.3281 (0.6311)	SegCLSLoss 0.0000 (0.0132)	KLLoss 0.0000 (0.0811)	MaskLoss 0.0000 (0.4496)	MaskBCELoss 0.0000 (0.0918)	MaskDICELoss 0.0000 (0.3578)
Epoch: [2][357/500]	Time  9.096 ( 9.096)	Loss 6.2759 (5.8237)	CeLoss 0.2314 (0.4307)	SegCLSLoss 0.0342 (0.0201)	KLLoss 0.1953 (0.1279)	MaskLoss 0.7557 (0.8340)	MaskBCELoss 0.0356 (0.2362)	MaskDICELoss 0.7202 (0.5978)
Epoch: [2][358/500]	Time  8.397 ( 8.397)	Loss 8.4419 (4.6015)	CeLoss 0.2695 (0.6013)	SegCLSLoss 0.0287 (0.0138)	KLLoss 0.1934 (0.0951)	MaskLoss 1.0318 (0.5544)	MaskBCELoss 0.0481 (0.0895)	MaskDICELoss 0.9836 (0.4649)
Epoch: [2][359/500]	Time  8.712 ( 8.712)	Loss 4.4567 (5.1255)	CeLoss 0.2461 (0.3137)	SegCLSLoss 0.0190 (0.0213)	KLLoss 0.1328 (0.1360)	MaskLoss 0.5642 (0.8153)	MaskBCELoss 0.0743 (0.3096)	MaskDICELoss 0.4899 (0.5057)
[2025-03-04 05:53:21,432] [INFO] [logging.py:96:log_dist] [Rank 0] step=1360, skipped=0, lr=[0.0002684939759036144], mom=[(0.9, 0.95)]
[2025-03-04 05:53:21,438] [INFO] [timer.py:215:stop] epoch=0/micro_step=13600/global_step=1360, RunningAvgSamplesPerSec=1.0406386309331914, CurrSamplesPerSec=0.9583497216623115, MemAllocated=56.74GB, MaxMemAllocated=62.82GB
Epoch: [2][360/500]	Time 10.437 (10.437)	Loss 1.5859 (5.2939)	CeLoss 1.5859 (0.4688)	SegCLSLoss 0.0000 (0.0216)	KLLoss 0.0000 (0.1327)	MaskLoss 0.0000 (0.7108)	MaskBCELoss 0.0000 (0.1675)	MaskDICELoss 0.0000 (0.5433)
Epoch: [2][361/500]	Time  8.661 ( 8.661)	Loss 5.7988 (5.0717)	CeLoss 0.3281 (0.5193)	SegCLSLoss 0.0227 (0.0166)	KLLoss 0.1758 (0.1198)	MaskLoss 0.7465 (0.6169)	MaskBCELoss 0.1148 (0.0853)	MaskDICELoss 0.6317 (0.5317)
Epoch: [2][362/500]	Time  9.513 ( 9.513)	Loss 5.5808 (5.0102)	CeLoss 0.1992 (0.4190)	SegCLSLoss 0.0248 (0.0164)	KLLoss 0.1748 (0.1272)	MaskLoss 0.6800 (0.6759)	MaskBCELoss 0.0410 (0.1586)	MaskDICELoss 0.6390 (0.5173)
Epoch: [2][363/500]	Time 10.609 (10.609)	Loss 1.9609 (6.9231)	CeLoss 1.9609 (0.3971)	SegCLSLoss 0.0000 (0.0251)	KLLoss 0.0000 (0.1612)	MaskLoss 0.0000 (1.0127)	MaskBCELoss 0.0000 (0.2916)	MaskDICELoss 0.0000 (0.7211)
Epoch: [2][364/500]	Time  8.411 ( 8.411)	Loss 6.6625 (5.3728)	CeLoss 0.2451 (0.4276)	SegCLSLoss 0.0344 (0.0182)	KLLoss 0.2617 (0.1370)	MaskLoss 0.8174 (0.7232)	MaskBCELoss 0.0667 (0.1643)	MaskDICELoss 0.7507 (0.5588)
Epoch: [2][365/500]	Time  8.941 ( 8.941)	Loss 7.6770 (5.8135)	CeLoss 0.1934 (0.6445)	SegCLSLoss 0.0388 (0.0181)	KLLoss 0.2158 (0.1192)	MaskLoss 1.0278 (0.8317)	MaskBCELoss 0.1625 (0.2689)	MaskDICELoss 0.8653 (0.5628)
Epoch: [2][366/500]	Time  7.998 ( 7.998)	Loss 0.1211 (3.4882)	CeLoss 0.1211 (0.6061)	SegCLSLoss 0.0000 (0.0099)	KLLoss 0.0000 (0.0639)	MaskLoss 0.0000 (0.4618)	MaskBCELoss 0.0000 (0.1468)	MaskDICELoss 0.0000 (0.3150)
Epoch: [2][367/500]	Time 10.101 (10.101)	Loss 6.8581 (5.4205)	CeLoss 0.2422 (0.4008)	SegCLSLoss 0.0199 (0.0221)	KLLoss 0.1235 (0.1464)	MaskLoss 0.8324 (0.7479)	MaskBCELoss 0.0293 (0.1869)	MaskDICELoss 0.8031 (0.5610)
Epoch: [2][368/500]	Time 10.274 (10.274)	Loss 5.8998 (6.5160)	CeLoss 0.1494 (0.3406)	SegCLSLoss 0.0427 (0.0250)	KLLoss 0.2832 (0.1730)	MaskLoss 0.7490 (0.9224)	MaskBCELoss 0.0909 (0.2315)	MaskDICELoss 0.6581 (0.6908)
Epoch: [2][369/500]	Time  9.903 ( 9.903)	Loss 8.7186 (5.8534)	CeLoss 0.2197 (0.2405)	SegCLSLoss 0.0265 (0.0179)	KLLoss 0.2520 (0.1349)	MaskLoss 1.1172 (0.8500)	MaskBCELoss 0.1173 (0.2218)	MaskDICELoss 1.0000 (0.6282)
[2025-03-04 05:54:53,412] [INFO] [logging.py:96:log_dist] [Rank 0] step=1370, skipped=0, lr=[0.0002682530120481927], mom=[(0.9, 0.95)]
[2025-03-04 05:54:53,418] [INFO] [timer.py:215:stop] epoch=0/micro_step=13700/global_step=1370, RunningAvgSamplesPerSec=1.040965836890158, CurrSamplesPerSec=1.3214825669888595, MemAllocated=57.46GB, MaxMemAllocated=62.82GB
Epoch: [2][370/500]	Time  7.569 ( 7.569)	Loss 6.2430 (5.5039)	CeLoss 0.2676 (0.3894)	SegCLSLoss 0.0260 (0.0255)	KLLoss 0.1807 (0.1507)	MaskLoss 0.7940 (0.7645)	MaskBCELoss 0.0950 (0.1942)	MaskDICELoss 0.6990 (0.5703)
Epoch: [2][371/500]	Time 10.374 (10.374)	Loss 7.8125 (5.7007)	CeLoss 0.2012 (0.4052)	SegCLSLoss 0.0344 (0.0196)	KLLoss 0.2188 (0.1269)	MaskLoss 0.9446 (0.7870)	MaskBCELoss 0.0303 (0.1896)	MaskDICELoss 0.9143 (0.5975)
Epoch: [2][372/500]	Time  8.638 ( 8.638)	Loss 6.9225 (4.7674)	CeLoss 0.2373 (0.4670)	SegCLSLoss 0.0315 (0.0222)	KLLoss 0.1680 (0.1312)	MaskLoss 0.8572 (0.5778)	MaskBCELoss 0.0595 (0.0774)	MaskDICELoss 0.7977 (0.5004)
Epoch: [2][373/500]	Time  9.249 ( 9.249)	Loss 5.6121 (3.7935)	CeLoss 0.2275 (0.3074)	SegCLSLoss 0.0193 (0.0174)	KLLoss 0.1641 (0.1144)	MaskLoss 1.0038 (0.6190)	MaskBCELoss 0.4701 (0.2648)	MaskDICELoss 0.5337 (0.3542)
Epoch: [2][374/500]	Time 11.315 (11.315)	Loss 7.0023 (5.6122)	CeLoss 0.2021 (0.3828)	SegCLSLoss 0.0291 (0.0214)	KLLoss 0.2295 (0.1470)	MaskLoss 0.8472 (0.7486)	MaskBCELoss 0.0368 (0.1528)	MaskDICELoss 0.8104 (0.5958)
Epoch: [2][375/500]	Time  9.195 ( 9.195)	Loss 4.8932 (5.7218)	CeLoss 0.2656 (0.4101)	SegCLSLoss 0.0177 (0.0238)	KLLoss 0.1309 (0.1531)	MaskLoss 0.6101 (0.7647)	MaskBCELoss 0.0657 (0.1618)	MaskDICELoss 0.5445 (0.6028)
Epoch: [2][376/500]	Time  8.656 ( 8.656)	Loss 1.2109 (4.4460)	CeLoss 1.2109 (0.6149)	SegCLSLoss 0.0000 (0.0143)	KLLoss 0.0000 (0.0911)	MaskLoss 0.0000 (0.5949)	MaskBCELoss 0.0000 (0.1711)	MaskDICELoss 0.0000 (0.4239)
Epoch: [2][377/500]	Time  7.983 ( 7.983)	Loss 8.2127 (5.0056)	CeLoss 0.2520 (0.5191)	SegCLSLoss 0.0579 (0.0251)	KLLoss 0.2617 (0.1370)	MaskLoss 1.3129 (0.6861)	MaskBCELoss 0.4722 (0.1920)	MaskDICELoss 0.8407 (0.4941)
Epoch: [2][378/500]	Time 10.208 (10.208)	Loss 8.3745 (5.3359)	CeLoss 0.2148 (0.3646)	SegCLSLoss 0.0281 (0.0193)	KLLoss 0.1904 (0.1392)	MaskLoss 1.0165 (0.7516)	MaskBCELoss 0.0292 (0.1984)	MaskDICELoss 0.9873 (0.5532)
Epoch: [2][379/500]	Time  9.333 ( 9.333)	Loss 8.9553 (6.2213)	CeLoss 0.1719 (0.3214)	SegCLSLoss 0.0664 (0.0285)	KLLoss 0.2363 (0.1604)	MaskLoss 1.3830 (0.8903)	MaskBCELoss 0.4250 (0.2329)	MaskDICELoss 0.9580 (0.6574)
[2025-03-04 05:56:27,331] [INFO] [logging.py:96:log_dist] [Rank 0] step=1380, skipped=0, lr=[0.00026801204819277104], mom=[(0.9, 0.95)]
[2025-03-04 05:56:27,337] [INFO] [timer.py:215:stop] epoch=0/micro_step=13800/global_step=1380, RunningAvgSamplesPerSec=1.0411359195843062, CurrSamplesPerSec=1.1152321426491774, MemAllocated=56.95GB, MaxMemAllocated=62.82GB
Epoch: [2][380/500]	Time  8.969 ( 8.969)	Loss 6.2662 (6.7562)	CeLoss 0.2715 (0.3054)	SegCLSLoss 0.0161 (0.0204)	KLLoss 0.1108 (0.1471)	MaskLoss 0.9940 (1.0765)	MaskBCELoss 0.3461 (0.3864)	MaskDICELoss 0.6479 (0.6900)
Epoch: [2][381/500]	Time 10.828 (10.828)	Loss 7.1948 (4.5233)	CeLoss 0.2559 (0.3770)	SegCLSLoss 0.0225 (0.0170)	KLLoss 0.1543 (0.1100)	MaskLoss 1.1429 (0.6365)	MaskBCELoss 0.3950 (0.1773)	MaskDICELoss 0.7479 (0.4592)
Epoch: [2][382/500]	Time 10.273 (10.273)	Loss 4.4346 (6.4337)	CeLoss 0.2598 (0.2287)	SegCLSLoss 0.0166 (0.0239)	KLLoss 0.1445 (0.1794)	MaskLoss 0.9379 (0.9845)	MaskBCELoss 0.5802 (0.3104)	MaskDICELoss 0.3578 (0.6741)
Epoch: [2][383/500]	Time  9.518 ( 9.518)	Loss 8.2418 (6.5144)	CeLoss 0.1660 (0.3051)	SegCLSLoss 0.0339 (0.0286)	KLLoss 0.1973 (0.1722)	MaskLoss 1.0496 (1.0087)	MaskBCELoss 0.0890 (0.3410)	MaskDICELoss 0.9606 (0.6677)
Epoch: [2][384/500]	Time 11.284 (11.284)	Loss 6.4285 (5.3197)	CeLoss 0.2090 (0.3541)	SegCLSLoss 0.0297 (0.0184)	KLLoss 0.1660 (0.1206)	MaskLoss 0.7827 (0.7198)	MaskBCELoss 0.0373 (0.1537)	MaskDICELoss 0.7454 (0.5660)
Epoch: [2][385/500]	Time  8.186 ( 8.186)	Loss 1.2734 (4.7205)	CeLoss 1.2734 (0.4440)	SegCLSLoss 0.0000 (0.0281)	KLLoss 0.0000 (0.1265)	MaskLoss 0.0000 (0.7305)	MaskBCELoss 0.0000 (0.2848)	MaskDICELoss 0.0000 (0.4458)
Epoch: [2][386/500]	Time  9.723 ( 9.723)	Loss 8.4187 (6.1869)	CeLoss 0.2061 (0.3320)	SegCLSLoss 0.0322 (0.0221)	KLLoss 0.1963 (0.1348)	MaskLoss 1.0338 (0.8134)	MaskBCELoss 0.0450 (0.1330)	MaskDICELoss 0.9888 (0.6804)
Epoch: [2][387/500]	Time  7.664 ( 7.664)	Loss 6.4588 (3.9510)	CeLoss 0.3340 (0.6184)	SegCLSLoss 0.0371 (0.0139)	KLLoss 0.1816 (0.0855)	MaskLoss 0.8790 (0.4810)	MaskBCELoss 0.1841 (0.1013)	MaskDICELoss 0.6949 (0.3797)
Epoch: [2][388/500]	Time 11.607 (11.607)	Loss 7.2897 (4.8376)	CeLoss 0.2676 (0.3760)	SegCLSLoss 0.0378 (0.0164)	KLLoss 0.2002 (0.1134)	MaskLoss 1.1790 (0.6264)	MaskBCELoss 0.4384 (0.1119)	MaskDICELoss 0.7406 (0.5145)
Epoch: [2][389/500]	Time 10.641 (10.641)	Loss 7.4038 (6.3251)	CeLoss 0.2275 (0.2633)	SegCLSLoss 0.0277 (0.0236)	KLLoss 0.1895 (0.1612)	MaskLoss 0.9564 (0.8855)	MaskBCELoss 0.1129 (0.1991)	MaskDICELoss 0.8436 (0.6863)
[2025-03-04 05:58:06,023] [INFO] [logging.py:96:log_dist] [Rank 0] step=1390, skipped=0, lr=[0.00026777108433734937], mom=[(0.9, 0.95)]
[2025-03-04 05:58:06,028] [INFO] [timer.py:215:stop] epoch=0/micro_step=13900/global_step=1390, RunningAvgSamplesPerSec=1.0409308747720458, CurrSamplesPerSec=1.115307644648427, MemAllocated=57.26GB, MaxMemAllocated=62.82GB
Epoch: [2][390/500]	Time  8.968 ( 8.968)	Loss 7.1990 (5.8968)	CeLoss 0.2432 (0.5646)	SegCLSLoss 0.0231 (0.0180)	KLLoss 0.1660 (0.1334)	MaskLoss 1.1256 (0.8375)	MaskBCELoss 0.3710 (0.2517)	MaskDICELoss 0.7547 (0.5858)
Epoch: [2][391/500]	Time 10.051 (10.051)	Loss 8.3354 (6.0144)	CeLoss 0.2246 (0.2701)	SegCLSLoss 0.0277 (0.0242)	KLLoss 0.2090 (0.1658)	MaskLoss 1.0793 (0.8588)	MaskBCELoss 0.1244 (0.2172)	MaskDICELoss 0.9549 (0.6416)
Epoch: [2][392/500]	Time  8.384 ( 8.384)	Loss 5.0750 (4.3937)	CeLoss 0.2988 (0.5107)	SegCLSLoss 0.0150 (0.0151)	KLLoss 0.0957 (0.0938)	MaskLoss 0.6840 (0.5501)	MaskBCELoss 0.1333 (0.1032)	MaskDICELoss 0.5508 (0.4469)
Epoch: [2][393/500]	Time 10.235 (10.235)	Loss 7.0967 (6.2830)	CeLoss 0.2617 (0.2384)	SegCLSLoss 0.0243 (0.0287)	KLLoss 0.1445 (0.1822)	MaskLoss 1.1508 (0.9309)	MaskBCELoss 0.4213 (0.2665)	MaskDICELoss 0.7295 (0.6643)
Epoch: [2][394/500]	Time 10.397 (10.397)	Loss 1.6016 (5.2570)	CeLoss 1.6016 (0.4928)	SegCLSLoss 0.0000 (0.0149)	KLLoss 0.0000 (0.1069)	MaskLoss 0.0000 (0.8097)	MaskBCELoss 0.0000 (0.3046)	MaskDICELoss 0.0000 (0.5051)
Epoch: [2][395/500]	Time  9.587 ( 9.587)	Loss 5.7020 (5.0611)	CeLoss 0.3555 (0.4564)	SegCLSLoss 0.0176 (0.0169)	KLLoss 0.1270 (0.1135)	MaskLoss 0.7369 (0.6676)	MaskBCELoss 0.1139 (0.1430)	MaskDICELoss 0.6230 (0.5246)
Epoch: [2][396/500]	Time  9.567 ( 9.567)	Loss 7.7575 (6.8435)	CeLoss 0.1826 (0.3173)	SegCLSLoss 0.0405 (0.0277)	KLLoss 0.2207 (0.1819)	MaskLoss 1.2008 (0.8944)	MaskBCELoss 0.3789 (0.1374)	MaskDICELoss 0.8220 (0.7570)
Epoch: [2][397/500]	Time  8.941 ( 8.941)	Loss 6.6175 (5.3672)	CeLoss 0.3301 (0.4354)	SegCLSLoss 0.0177 (0.0241)	KLLoss 0.2002 (0.1686)	MaskLoss 1.1261 (0.8162)	MaskBCELoss 0.4884 (0.2965)	MaskDICELoss 0.6377 (0.5198)
Epoch: [2][398/500]	Time  9.808 ( 9.808)	Loss 4.7853 (6.0401)	CeLoss 0.3086 (0.2789)	SegCLSLoss 0.0300 (0.0221)	KLLoss 0.2080 (0.1526)	MaskLoss 0.7269 (0.8951)	MaskBCELoss 0.2602 (0.2605)	MaskDICELoss 0.4667 (0.6346)
Epoch: [2][399/500]	Time  8.440 ( 8.440)	Loss 0.8008 (4.8582)	CeLoss 0.8008 (0.5113)	SegCLSLoss 0.0000 (0.0149)	KLLoss 0.0000 (0.1062)	MaskLoss 0.0000 (0.6438)	MaskBCELoss 0.0000 (0.1528)	MaskDICELoss 0.0000 (0.4909)
[2025-03-04 05:59:39,367] [INFO] [logging.py:96:log_dist] [Rank 0] step=1400, skipped=0, lr=[0.0002675301204819277], mom=[(0.9, 0.95)]
[2025-03-04 05:59:39,373] [INFO] [timer.py:215:stop] epoch=0/micro_step=14000/global_step=1400, RunningAvgSamplesPerSec=1.0411432601130721, CurrSamplesPerSec=1.2607522135358662, MemAllocated=56.81GB, MaxMemAllocated=62.82GB
Epoch: [2][400/500]	Time  7.934 ( 7.934)	Loss 5.0634 (4.3954)	CeLoss 0.1963 (0.5334)	SegCLSLoss 0.0294 (0.0161)	KLLoss 0.2188 (0.1160)	MaskLoss 0.6891 (0.5355)	MaskBCELoss 0.1465 (0.0911)	MaskDICELoss 0.5426 (0.4444)
Epoch: [2][401/500]	Time 10.428 (10.428)	Loss 2.9505 (4.8667)	CeLoss 0.3574 (0.2325)	SegCLSLoss 0.0165 (0.0207)	KLLoss 0.1621 (0.1528)	MaskLoss 0.3511 (0.6846)	MaskBCELoss 0.0642 (0.1676)	MaskDICELoss 0.2868 (0.5170)
Epoch: [2][402/500]	Time  9.162 ( 9.162)	Loss 0.1387 (5.2762)	CeLoss 0.1387 (0.3547)	SegCLSLoss 0.0000 (0.0224)	KLLoss 0.0000 (0.1605)	MaskLoss 0.0000 (0.7835)	MaskBCELoss 0.0000 (0.2530)	MaskDICELoss 0.0000 (0.5304)
Epoch: [2][403/500]	Time 10.973 (10.973)	Loss 7.8755 (6.2585)	CeLoss 0.1904 (0.2346)	SegCLSLoss 0.0364 (0.0213)	KLLoss 0.2090 (0.1403)	MaskLoss 1.0068 (0.8790)	MaskBCELoss 0.0995 (0.1931)	MaskDICELoss 0.9073 (0.6858)
Epoch: [2][404/500]	Time 10.596 (10.596)	Loss 7.0155 (6.5489)	CeLoss 0.1680 (0.3680)	SegCLSLoss 0.0276 (0.0260)	KLLoss 0.1953 (0.1682)	MaskLoss 0.8606 (0.8807)	MaskBCELoss 0.0410 (0.1743)	MaskDICELoss 0.8196 (0.7064)
Epoch: [2][405/500]	Time  9.698 ( 9.698)	Loss 0.7109 (5.9221)	CeLoss 0.7109 (0.5607)	SegCLSLoss 0.0000 (0.0255)	KLLoss 0.0000 (0.1353)	MaskLoss 0.0000 (0.7616)	MaskBCELoss 0.0000 (0.1466)	MaskDICELoss 0.0000 (0.6150)
Epoch: [2][406/500]	Time 10.605 (10.605)	Loss 8.1905 (7.1631)	CeLoss 0.2197 (0.2285)	SegCLSLoss 0.0255 (0.0279)	KLLoss 0.1807 (0.1724)	MaskLoss 1.1682 (1.1361)	MaskBCELoss 0.2615 (0.3901)	MaskDICELoss 0.9067 (0.7460)
Epoch: [2][407/500]	Time  7.736 ( 7.736)	Loss 6.9531 (4.8846)	CeLoss 0.2148 (0.5627)	SegCLSLoss 0.0262 (0.0146)	KLLoss 0.1689 (0.1056)	MaskLoss 0.9995 (0.7351)	MaskBCELoss 0.2399 (0.2786)	MaskDICELoss 0.7596 (0.4565)
Epoch: [2][408/500]	Time 11.396 (11.396)	Loss 8.0495 (6.0179)	CeLoss 0.2129 (0.3832)	SegCLSLoss 0.0413 (0.0215)	KLLoss 0.2139 (0.1363)	MaskLoss 1.2293 (0.8194)	MaskBCELoss 0.3721 (0.1779)	MaskDICELoss 0.8573 (0.6414)
Epoch: [2][409/500]	Time 10.536 (10.536)	Loss 7.9632 (5.5971)	CeLoss 0.2812 (0.3479)	SegCLSLoss 0.0262 (0.0178)	KLLoss 0.1768 (0.1183)	MaskLoss 1.0326 (0.7699)	MaskBCELoss 0.1284 (0.1729)	MaskDICELoss 0.9042 (0.5970)
[2025-03-04 06:01:19,977] [INFO] [logging.py:96:log_dist] [Rank 0] step=1410, skipped=0, lr=[0.000267289156626506], mom=[(0.9, 0.95)]
[2025-03-04 06:01:19,983] [INFO] [timer.py:215:stop] epoch=0/micro_step=14100/global_step=1410, RunningAvgSamplesPerSec=1.0407934879458824, CurrSamplesPerSec=1.0549767549677402, MemAllocated=57.46GB, MaxMemAllocated=62.82GB
Epoch: [2][410/500]	Time  9.481 ( 9.481)	Loss 8.3511 (4.6136)	CeLoss 0.3789 (0.5996)	SegCLSLoss 0.0258 (0.0142)	KLLoss 0.1846 (0.0971)	MaskLoss 1.0621 (0.5420)	MaskBCELoss 0.1206 (0.0711)	MaskDICELoss 0.9415 (0.4709)
Epoch: [2][411/500]	Time 10.312 (10.312)	Loss 6.8308 (3.7283)	CeLoss 0.2832 (0.5400)	SegCLSLoss 0.0223 (0.0133)	KLLoss 0.1465 (0.0808)	MaskLoss 0.8139 (0.4831)	MaskBCELoss 0.0203 (0.1274)	MaskDICELoss 0.7936 (0.3557)
Epoch: [2][412/500]	Time  8.462 ( 8.462)	Loss 6.3491 (5.0320)	CeLoss 0.2266 (0.4127)	SegCLSLoss 0.0205 (0.0168)	KLLoss 0.1147 (0.1076)	MaskLoss 1.1020 (0.7658)	MaskBCELoss 0.4697 (0.2704)	MaskDICELoss 0.6323 (0.4953)
Epoch: [2][413/500]	Time 10.332 (10.332)	Loss 8.4343 (5.7336)	CeLoss 0.2354 (0.3693)	SegCLSLoss 0.0374 (0.0185)	KLLoss 0.1982 (0.1276)	MaskLoss 1.3688 (0.8344)	MaskBCELoss 0.4948 (0.2414)	MaskDICELoss 0.8740 (0.5930)
Epoch: [2][414/500]	Time  7.694 ( 7.694)	Loss 0.1074 (4.3168)	CeLoss 0.1074 (0.4749)	SegCLSLoss 0.0000 (0.0177)	KLLoss 0.0000 (0.1154)	MaskLoss 0.0000 (0.5784)	MaskBCELoss 0.0000 (0.1517)	MaskDICELoss 0.0000 (0.4268)
Epoch: [2][415/500]	Time  9.361 ( 9.361)	Loss 6.8233 (5.3523)	CeLoss 0.1992 (0.6555)	SegCLSLoss 0.0386 (0.0193)	KLLoss 0.2158 (0.1131)	MaskLoss 1.0540 (0.6397)	MaskBCELoss 0.3404 (0.0905)	MaskDICELoss 0.7136 (0.5492)
Epoch: [2][416/500]	Time 11.210 (11.210)	Loss 7.9847 (7.0466)	CeLoss 0.3418 (0.2658)	SegCLSLoss 0.0173 (0.0214)	KLLoss 0.1357 (0.1430)	MaskLoss 1.2391 (1.0099)	MaskBCELoss 0.4021 (0.2420)	MaskDICELoss 0.8370 (0.7679)
Epoch: [2][417/500]	Time  8.547 ( 8.547)	Loss 7.5094 (4.2372)	CeLoss 0.2314 (0.6250)	SegCLSLoss 0.0242 (0.0157)	KLLoss 0.1611 (0.0990)	MaskLoss 0.9540 (0.4971)	MaskBCELoss 0.0878 (0.0786)	MaskDICELoss 0.8662 (0.4185)
Epoch: [2][418/500]	Time  6.653 ( 6.653)	Loss 2.7698 (3.9106)	CeLoss 0.1895 (0.6359)	SegCLSLoss 0.0231 (0.0153)	KLLoss 0.1670 (0.1076)	MaskLoss 0.5128 (0.4976)	MaskBCELoss 0.2834 (0.1370)	MaskDICELoss 0.2295 (0.3607)
Epoch: [2][419/500]	Time 10.751 (10.751)	Loss 5.8640 (6.0258)	CeLoss 0.2539 (0.3283)	SegCLSLoss 0.0249 (0.0240)	KLLoss 0.1924 (0.1496)	MaskLoss 1.1428 (0.9432)	MaskBCELoss 0.6225 (0.3349)	MaskDICELoss 0.5202 (0.6082)
[2025-03-04 06:02:54,225] [INFO] [logging.py:96:log_dist] [Rank 0] step=1420, skipped=0, lr=[0.00026704819277108434], mom=[(0.9, 0.95)]
[2025-03-04 06:02:54,230] [INFO] [timer.py:215:stop] epoch=0/micro_step=14200/global_step=1420, RunningAvgSamplesPerSec=1.0409349186236492, CurrSamplesPerSec=0.915439412354066, MemAllocated=56.72GB, MaxMemAllocated=62.82GB
Epoch: [2][420/500]	Time 10.926 (10.926)	Loss 1.6641 (6.0145)	CeLoss 1.6641 (0.3917)	SegCLSLoss 0.0000 (0.0253)	KLLoss 0.0000 (0.1546)	MaskLoss 0.0000 (0.8656)	MaskBCELoss 0.0000 (0.2448)	MaskDICELoss 0.0000 (0.6208)
Epoch: [2][421/500]	Time  7.596 ( 7.596)	Loss 7.6929 (5.3985)	CeLoss 0.1494 (0.4751)	SegCLSLoss 0.0217 (0.0269)	KLLoss 0.1992 (0.1490)	MaskLoss 0.9283 (0.7323)	MaskBCELoss 0.0155 (0.1829)	MaskDICELoss 0.9128 (0.5494)
Epoch: [2][422/500]	Time 10.950 (10.950)	Loss 7.8999 (5.2584)	CeLoss 0.2695 (0.2858)	SegCLSLoss 0.0238 (0.0229)	KLLoss 0.1680 (0.1429)	MaskLoss 0.9633 (0.6525)	MaskBCELoss 0.0426 (0.0669)	MaskDICELoss 0.9207 (0.5856)
Epoch: [2][423/500]	Time  8.018 ( 8.018)	Loss 1.8281 (4.0171)	CeLoss 1.8281 (0.6810)	SegCLSLoss 0.0000 (0.0176)	KLLoss 0.0000 (0.1100)	MaskLoss 0.0000 (0.4477)	MaskBCELoss 0.0000 (0.0607)	MaskDICELoss 0.0000 (0.3870)
Epoch: [2][424/500]	Time  8.664 ( 8.664)	Loss 5.0222 (3.8277)	CeLoss 0.2559 (0.5163)	SegCLSLoss 0.0156 (0.0121)	KLLoss 0.1001 (0.0766)	MaskLoss 0.6990 (0.5111)	MaskBCELoss 0.1555 (0.1433)	MaskDICELoss 0.5435 (0.3678)
Epoch: [2][425/500]	Time  9.400 ( 9.400)	Loss 1.6172 (6.4686)	CeLoss 1.6172 (0.3503)	SegCLSLoss 0.0000 (0.0294)	KLLoss 0.0000 (0.1780)	MaskLoss 0.0000 (0.9448)	MaskBCELoss 0.0000 (0.2722)	MaskDICELoss 0.0000 (0.6727)
Epoch: [2][426/500]	Time  9.036 ( 9.036)	Loss 0.7148 (3.3171)	CeLoss 0.7148 (0.6358)	SegCLSLoss 0.0000 (0.0132)	KLLoss 0.0000 (0.0781)	MaskLoss 0.0000 (0.4583)	MaskBCELoss 0.0000 (0.1783)	MaskDICELoss 0.0000 (0.2800)
Epoch: [2][427/500]	Time  9.846 ( 9.846)	Loss 1.0156 (5.4282)	CeLoss 1.0156 (0.4600)	SegCLSLoss 0.0000 (0.0210)	KLLoss 0.0000 (0.1362)	MaskLoss 0.0000 (0.7305)	MaskBCELoss 0.0000 (0.1704)	MaskDICELoss 0.0000 (0.5601)
Epoch: [2][428/500]	Time  9.910 ( 9.910)	Loss 7.7986 (4.6399)	CeLoss 0.1533 (0.5446)	SegCLSLoss 0.0613 (0.0185)	KLLoss 0.2373 (0.0979)	MaskLoss 1.2183 (0.6006)	MaskBCELoss 0.3950 (0.1362)	MaskDICELoss 0.8233 (0.4645)
Epoch: [2][429/500]	Time  9.262 ( 9.262)	Loss 5.9553 (4.9030)	CeLoss 0.2637 (0.3219)	SegCLSLoss 0.0242 (0.0186)	KLLoss 0.1641 (0.1125)	MaskLoss 0.7170 (0.6986)	MaskBCELoss 0.0367 (0.1882)	MaskDICELoss 0.6803 (0.5104)
[2025-03-04 06:04:27,173] [INFO] [logging.py:96:log_dist] [Rank 0] step=1430, skipped=0, lr=[0.00026680722891566267], mom=[(0.9, 0.95)]
[2025-03-04 06:04:27,178] [INFO] [timer.py:215:stop] epoch=0/micro_step=14300/global_step=1430, RunningAvgSamplesPerSec=1.0411730698674604, CurrSamplesPerSec=0.9741336105549783, MemAllocated=57.26GB, MaxMemAllocated=62.82GB
Epoch: [2][430/500]	Time 10.268 (10.268)	Loss 7.9914 (5.7078)	CeLoss 0.2656 (0.3461)	SegCLSLoss 0.0303 (0.0238)	KLLoss 0.2012 (0.1544)	MaskLoss 1.0256 (0.8425)	MaskBCELoss 0.1160 (0.2575)	MaskDICELoss 0.9096 (0.5850)
Epoch: [2][431/500]	Time  7.248 ( 7.248)	Loss 1.2734 (3.6173)	CeLoss 1.2734 (0.8010)	SegCLSLoss 0.0000 (0.0141)	KLLoss 0.0000 (0.0816)	MaskLoss 0.0000 (0.4097)	MaskBCELoss 0.0000 (0.0916)	MaskDICELoss 0.0000 (0.3180)
Epoch: [2][432/500]	Time 10.212 (10.212)	Loss 5.6345 (6.5987)	CeLoss 0.2002 (0.2303)	SegCLSLoss 0.0179 (0.0273)	KLLoss 0.1182 (0.1616)	MaskLoss 1.0103 (0.9109)	MaskBCELoss 0.4627 (0.1823)	MaskDICELoss 0.5476 (0.7285)
Epoch: [2][433/500]	Time  9.208 ( 9.208)	Loss 7.7662 (6.0032)	CeLoss 0.2246 (0.2726)	SegCLSLoss 0.0162 (0.0276)	KLLoss 0.1133 (0.1531)	MaskLoss 1.1212 (0.8450)	MaskBCELoss 0.2582 (0.1995)	MaskDICELoss 0.8630 (0.6456)
Epoch: [2][434/500]	Time 10.427 (10.427)	Loss 8.2107 (5.8187)	CeLoss 0.2559 (0.3430)	SegCLSLoss 0.0225 (0.0190)	KLLoss 0.1641 (0.1223)	MaskLoss 1.0029 (0.7924)	MaskBCELoss 0.0407 (0.1658)	MaskDICELoss 0.9622 (0.6266)
Epoch: [2][435/500]	Time  9.196 ( 9.196)	Loss 5.9964 (5.3022)	CeLoss 0.2188 (0.5767)	SegCLSLoss 0.0352 (0.0221)	KLLoss 0.1816 (0.1243)	MaskLoss 0.7359 (0.7832)	MaskBCELoss 0.0514 (0.2792)	MaskDICELoss 0.6844 (0.5040)
Epoch: [2][436/500]	Time  9.652 ( 9.652)	Loss 1.6562 (4.5299)	CeLoss 1.6562 (0.5205)	SegCLSLoss 0.0000 (0.0180)	KLLoss 0.0000 (0.1198)	MaskLoss 0.0000 (0.5777)	MaskBCELoss 0.0000 (0.1235)	MaskDICELoss 0.0000 (0.4542)
Epoch: [2][437/500]	Time  7.716 ( 7.716)	Loss 0.9570 (4.6557)	CeLoss 0.9570 (0.7121)	SegCLSLoss 0.0000 (0.0196)	KLLoss 0.0000 (0.1161)	MaskLoss 0.0000 (0.5969)	MaskBCELoss 0.0000 (0.1596)	MaskDICELoss 0.0000 (0.4373)
Epoch: [2][438/500]	Time  8.293 ( 8.293)	Loss 0.9844 (4.8042)	CeLoss 0.9844 (0.5554)	SegCLSLoss 0.0000 (0.0173)	KLLoss 0.0000 (0.1046)	MaskLoss 0.0000 (0.6173)	MaskBCELoss 0.0000 (0.1338)	MaskDICELoss 0.0000 (0.4835)
Epoch: [2][439/500]	Time  7.879 ( 7.879)	Loss 0.0850 (3.5752)	CeLoss 0.0850 (0.5861)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.0770)	MaskLoss 0.0000 (0.4305)	MaskBCELoss 0.0000 (0.0898)	MaskDICELoss 0.0000 (0.3407)
[2025-03-04 06:05:56,597] [INFO] [logging.py:96:log_dist] [Rank 0] step=1440, skipped=0, lr=[0.00026656626506024094], mom=[(0.9, 0.95)]
[2025-03-04 06:05:56,602] [INFO] [timer.py:215:stop] epoch=0/micro_step=14400/global_step=1440, RunningAvgSamplesPerSec=1.0416737558916904, CurrSamplesPerSec=1.0427659168644148, MemAllocated=57.25GB, MaxMemAllocated=62.82GB
Epoch: [2][440/500]	Time  9.592 ( 9.592)	Loss 7.1875 (5.8653)	CeLoss 0.2393 (0.4086)	SegCLSLoss 0.0150 (0.0221)	KLLoss 0.1235 (0.1354)	MaskLoss 1.1313 (0.8161)	MaskBCELoss 0.3723 (0.2031)	MaskDICELoss 0.7590 (0.6130)
Epoch: [2][441/500]	Time 10.466 (10.466)	Loss 7.0188 (5.5367)	CeLoss 0.2217 (0.2156)	SegCLSLoss 0.0181 (0.0231)	KLLoss 0.1226 (0.1599)	MaskLoss 0.8636 (0.7743)	MaskBCELoss 0.0406 (0.1742)	MaskDICELoss 0.8230 (0.6001)
Epoch: [2][442/500]	Time  9.364 ( 9.364)	Loss 1.2344 (5.3487)	CeLoss 1.2344 (0.4247)	SegCLSLoss 0.0000 (0.0197)	KLLoss 0.0000 (0.1281)	MaskLoss 0.0000 (0.7667)	MaskBCELoss 0.0000 (0.2246)	MaskDICELoss 0.0000 (0.5420)
Epoch: [2][443/500]	Time  8.537 ( 8.537)	Loss 3.9022 (4.8183)	CeLoss 0.2451 (0.5774)	SegCLSLoss 0.0153 (0.0221)	KLLoss 0.0752 (0.1146)	MaskLoss 0.5233 (0.5770)	MaskBCELoss 0.1021 (0.0835)	MaskDICELoss 0.4212 (0.4936)
Epoch: [2][444/500]	Time  9.340 ( 9.340)	Loss 4.7460 (4.6557)	CeLoss 0.2100 (0.4146)	SegCLSLoss 0.0435 (0.0179)	KLLoss 0.2119 (0.1112)	MaskLoss 0.8084 (0.6259)	MaskBCELoss 0.3607 (0.1477)	MaskDICELoss 0.4477 (0.4782)
Epoch: [2][445/500]	Time  9.840 ( 9.840)	Loss 7.3522 (5.7618)	CeLoss 0.2334 (0.3067)	SegCLSLoss 0.0251 (0.0210)	KLLoss 0.1738 (0.1334)	MaskLoss 0.8784 (0.7860)	MaskBCELoss 0.0158 (0.1629)	MaskDICELoss 0.8626 (0.6232)
Epoch: [2][446/500]	Time 10.278 (10.278)	Loss 0.1855 (4.7915)	CeLoss 0.1855 (0.2449)	SegCLSLoss 0.0000 (0.0172)	KLLoss 0.0000 (0.1083)	MaskLoss 0.0000 (0.6439)	MaskBCELoss 0.0000 (0.1202)	MaskDICELoss 0.0000 (0.5237)
Epoch: [2][447/500]	Time  7.966 ( 7.966)	Loss 7.9330 (4.6349)	CeLoss 0.2148 (0.6134)	SegCLSLoss 0.0157 (0.0152)	KLLoss 0.1094 (0.1045)	MaskLoss 1.4135 (0.5819)	MaskBCELoss 0.6178 (0.1244)	MaskDICELoss 0.7957 (0.4576)
Epoch: [2][448/500]	Time  8.558 ( 8.558)	Loss 7.2235 (4.8448)	CeLoss 0.2461 (0.5924)	SegCLSLoss 0.0262 (0.0159)	KLLoss 0.2344 (0.1183)	MaskLoss 1.1837 (0.6930)	MaskBCELoss 0.4567 (0.2363)	MaskDICELoss 0.7270 (0.4567)
Epoch: [2][449/500]	Time  7.700 ( 7.700)	Loss 3.4523 (6.2467)	CeLoss 0.2178 (0.4135)	SegCLSLoss 0.0139 (0.0214)	KLLoss 0.1099 (0.1394)	MaskLoss 0.9110 (0.9447)	MaskBCELoss 0.6949 (0.3123)	MaskDICELoss 0.2161 (0.6324)
[2025-03-04 06:07:28,140] [INFO] [logging.py:96:log_dist] [Rank 0] step=1450, skipped=0, lr=[0.00026632530120481926], mom=[(0.9, 0.95)]
[2025-03-04 06:07:28,146] [INFO] [timer.py:215:stop] epoch=0/micro_step=14500/global_step=1450, RunningAvgSamplesPerSec=1.042008885888994, CurrSamplesPerSec=1.0534925708160556, MemAllocated=57.26GB, MaxMemAllocated=62.82GB
Epoch: [2][450/500]	Time  9.494 ( 9.494)	Loss 7.0875 (3.4529)	CeLoss 0.2637 (0.3945)	SegCLSLoss 0.0146 (0.0104)	KLLoss 0.1079 (0.0805)	MaskLoss 1.4896 (0.5074)	MaskBCELoss 0.8680 (0.1811)	MaskDICELoss 0.6216 (0.3263)
Epoch: [2][451/500]	Time 10.473 (10.473)	Loss 7.0275 (6.0771)	CeLoss 0.2070 (0.4523)	SegCLSLoss 0.0273 (0.0204)	KLLoss 0.1992 (0.1408)	MaskLoss 1.0308 (0.8495)	MaskBCELoss 0.2732 (0.2204)	MaskDICELoss 0.7577 (0.6291)
Epoch: [2][452/500]	Time 10.033 (10.033)	Loss 0.6289 (4.8815)	CeLoss 0.6289 (0.3767)	SegCLSLoss 0.0000 (0.0245)	KLLoss 0.0000 (0.1063)	MaskLoss 0.0000 (0.7151)	MaskBCELoss 0.0000 (0.2224)	MaskDICELoss 0.0000 (0.4926)
Epoch: [2][453/500]	Time  9.237 ( 9.237)	Loss 8.6361 (5.6706)	CeLoss 0.2676 (0.4276)	SegCLSLoss 0.0278 (0.0193)	KLLoss 0.1475 (0.1238)	MaskLoss 2.0230 (0.8323)	MaskBCELoss 1.3297 (0.2581)	MaskDICELoss 0.6934 (0.5742)
Epoch: [2][454/500]	Time  8.447 ( 8.447)	Loss 0.0679 (4.0065)	CeLoss 0.0679 (0.3445)	SegCLSLoss 0.0000 (0.0133)	KLLoss 0.0000 (0.0854)	MaskLoss 0.0000 (0.5525)	MaskBCELoss 0.0000 (0.1417)	MaskDICELoss 0.0000 (0.4108)
Epoch: [2][455/500]	Time 12.342 (12.342)	Loss 5.9454 (5.8497)	CeLoss 0.2051 (0.2349)	SegCLSLoss 0.0344 (0.0237)	KLLoss 0.1729 (0.1429)	MaskLoss 0.8743 (0.8778)	MaskBCELoss 0.2406 (0.2603)	MaskDICELoss 0.6337 (0.6174)
Epoch: [2][456/500]	Time  9.458 ( 9.458)	Loss 6.4473 (6.1623)	CeLoss 0.2773 (0.4653)	SegCLSLoss 0.0260 (0.0225)	KLLoss 0.1621 (0.1298)	MaskLoss 0.7788 (0.7514)	MaskBCELoss 0.0393 (0.0758)	MaskDICELoss 0.7394 (0.6755)
Epoch: [2][457/500]	Time  8.753 ( 8.753)	Loss 7.0605 (3.2822)	CeLoss 0.3105 (0.5518)	SegCLSLoss 0.0258 (0.0134)	KLLoss 0.2002 (0.0814)	MaskLoss 1.0515 (0.4426)	MaskBCELoss 0.3125 (0.1498)	MaskDICELoss 0.7390 (0.2929)
Epoch: [2][458/500]	Time  9.495 ( 9.495)	Loss 1.1875 (6.0535)	CeLoss 1.1875 (0.4631)	SegCLSLoss 0.0000 (0.0265)	KLLoss 0.0000 (0.1611)	MaskLoss 0.0000 (0.8221)	MaskBCELoss 0.0000 (0.1935)	MaskDICELoss 0.0000 (0.6287)
Epoch: [2][459/500]	Time  9.515 ( 9.515)	Loss 2.9340 (5.4862)	CeLoss 0.2578 (0.4687)	SegCLSLoss 0.0155 (0.0188)	KLLoss 0.1064 (0.1307)	MaskLoss 0.3416 (0.7297)	MaskBCELoss 0.0283 (0.1599)	MaskDICELoss 0.3133 (0.5697)
[2025-03-04 06:09:04,889] [INFO] [logging.py:96:log_dist] [Rank 0] step=1460, skipped=0, lr=[0.0002660843373493976], mom=[(0.9, 0.95)]
[2025-03-04 06:09:04,895] [INFO] [timer.py:215:stop] epoch=0/micro_step=14600/global_step=1460, RunningAvgSamplesPerSec=1.0419520137385907, CurrSamplesPerSec=1.1120132712902828, MemAllocated=57.24GB, MaxMemAllocated=62.82GB
Epoch: [2][460/500]	Time  8.995 ( 8.995)	Loss 3.4558 (5.3114)	CeLoss 0.2451 (0.5897)	SegCLSLoss 0.0178 (0.0176)	KLLoss 0.1426 (0.1218)	MaskLoss 0.5543 (0.6579)	MaskBCELoss 0.2292 (0.1120)	MaskDICELoss 0.3251 (0.5459)
Epoch: [2][461/500]	Time  8.987 ( 8.987)	Loss 0.8789 (4.1386)	CeLoss 0.8789 (0.5182)	SegCLSLoss 0.0000 (0.0134)	KLLoss 0.0000 (0.0959)	MaskLoss 0.0000 (0.5740)	MaskBCELoss 0.0000 (0.1791)	MaskDICELoss 0.0000 (0.3949)
Epoch: [2][462/500]	Time  9.060 ( 9.060)	Loss 8.4408 (4.8621)	CeLoss 0.2148 (0.4310)	SegCLSLoss 0.0188 (0.0172)	KLLoss 0.1328 (0.1132)	MaskLoss 1.2800 (0.6720)	MaskBCELoss 0.3595 (0.1777)	MaskDICELoss 0.9205 (0.4942)
Epoch: [2][463/500]	Time  8.523 ( 8.523)	Loss 1.6562 (5.9758)	CeLoss 1.6562 (0.4507)	SegCLSLoss 0.0000 (0.0199)	KLLoss 0.0000 (0.1392)	MaskLoss 0.0000 (0.8345)	MaskBCELoss 0.0000 (0.2166)	MaskDICELoss 0.0000 (0.6179)
Epoch: [2][464/500]	Time  9.358 ( 9.358)	Loss 8.4006 (5.6282)	CeLoss 0.1885 (0.3176)	SegCLSLoss 0.0195 (0.0222)	KLLoss 0.2334 (0.1522)	MaskLoss 1.0313 (0.8322)	MaskBCELoss 0.0469 (0.2516)	MaskDICELoss 0.9844 (0.5805)
Epoch: [2][465/500]	Time  8.972 ( 8.972)	Loss 7.9701 (4.5870)	CeLoss 0.1836 (0.4979)	SegCLSLoss 0.0381 (0.0156)	KLLoss 0.2090 (0.1027)	MaskLoss 1.1930 (0.6508)	MaskBCELoss 0.3310 (0.2045)	MaskDICELoss 0.8620 (0.4462)
Epoch: [2][466/500]	Time  9.874 ( 9.874)	Loss 4.4787 (5.5250)	CeLoss 0.3223 (0.3383)	SegCLSLoss 0.0139 (0.0226)	KLLoss 0.1147 (0.1472)	MaskLoss 0.6559 (0.7574)	MaskBCELoss 0.2023 (0.1718)	MaskDICELoss 0.4536 (0.5856)
Epoch: [2][467/500]	Time  9.277 ( 9.277)	Loss 7.8620 (3.8527)	CeLoss 0.2139 (0.4240)	SegCLSLoss 0.0337 (0.0149)	KLLoss 0.2031 (0.1134)	MaskLoss 0.9607 (0.5578)	MaskBCELoss 0.0429 (0.1924)	MaskDICELoss 0.9178 (0.3654)
Epoch: [2][468/500]	Time  9.211 ( 9.211)	Loss 6.0315 (4.4615)	CeLoss 0.3086 (0.4137)	SegCLSLoss 0.0193 (0.0193)	KLLoss 0.1738 (0.1179)	MaskLoss 0.9541 (0.6527)	MaskBCELoss 0.3490 (0.2170)	MaskDICELoss 0.6052 (0.4357)
Epoch: [2][469/500]	Time 10.541 (10.541)	Loss 6.8915 (5.2077)	CeLoss 0.1689 (0.3453)	SegCLSLoss 0.0361 (0.0180)	KLLoss 0.1914 (0.1111)	MaskLoss 0.8648 (0.8249)	MaskBCELoss 0.0676 (0.3095)	MaskDICELoss 0.7972 (0.5154)
[2025-03-04 06:10:37,631] [INFO] [logging.py:96:log_dist] [Rank 0] step=1470, skipped=0, lr=[0.00026584337349397586], mom=[(0.9, 0.95)]
[2025-03-04 06:10:37,637] [INFO] [timer.py:215:stop] epoch=0/micro_step=14700/global_step=1470, RunningAvgSamplesPerSec=1.0421922997951742, CurrSamplesPerSec=1.1189633547920934, MemAllocated=56.66GB, MaxMemAllocated=62.82GB
Epoch: [2][470/500]	Time  8.939 ( 8.939)	Loss 7.8159 (6.3631)	CeLoss 0.1367 (0.2548)	SegCLSLoss 0.0195 (0.0258)	KLLoss 0.1543 (0.1709)	MaskLoss 0.9564 (0.9129)	MaskBCELoss 0.0227 (0.2297)	MaskDICELoss 0.9337 (0.6832)
Epoch: [2][471/500]	Time 10.480 (10.480)	Loss 5.8121 (6.7205)	CeLoss 0.2871 (0.2779)	SegCLSLoss 0.0170 (0.0222)	KLLoss 0.1484 (0.1569)	MaskLoss 0.8553 (1.0507)	MaskBCELoss 0.2456 (0.3552)	MaskDICELoss 0.6097 (0.6956)
Epoch: [2][472/500]	Time 10.212 (10.212)	Loss 8.4955 (6.3317)	CeLoss 0.2500 (0.5196)	SegCLSLoss 0.0217 (0.0208)	KLLoss 0.1543 (0.1479)	MaskLoss 1.1774 (0.8443)	MaskBCELoss 0.2233 (0.1833)	MaskDICELoss 0.9541 (0.6609)
Epoch: [2][473/500]	Time  7.850 ( 7.850)	Loss 1.1953 (3.7455)	CeLoss 1.1953 (0.5039)	SegCLSLoss 0.0000 (0.0168)	KLLoss 0.0000 (0.1014)	MaskLoss 0.0000 (0.4507)	MaskBCELoss 0.0000 (0.0790)	MaskDICELoss 0.0000 (0.3718)
Epoch: [2][474/500]	Time 10.360 (10.360)	Loss 7.0990 (4.3298)	CeLoss 0.2715 (0.3305)	SegCLSLoss 0.0225 (0.0137)	KLLoss 0.1973 (0.1132)	MaskLoss 0.8871 (0.6207)	MaskBCELoss 0.0797 (0.1811)	MaskDICELoss 0.8074 (0.4396)
Epoch: [2][475/500]	Time  9.589 ( 9.589)	Loss 1.0234 (6.2704)	CeLoss 1.0234 (0.4573)	SegCLSLoss 0.0000 (0.0243)	KLLoss 0.0000 (0.1533)	MaskLoss 0.0000 (0.9262)	MaskBCELoss 0.0000 (0.2937)	MaskDICELoss 0.0000 (0.6325)
Epoch: [2][476/500]	Time  8.705 ( 8.705)	Loss 1.5625 (5.0636)	CeLoss 1.5625 (0.4715)	SegCLSLoss 0.0000 (0.0167)	KLLoss 0.0000 (0.1202)	MaskLoss 0.0000 (0.7051)	MaskBCELoss 0.0000 (0.1962)	MaskDICELoss 0.0000 (0.5088)
Epoch: [2][477/500]	Time  9.402 ( 9.402)	Loss 7.7488 (4.4082)	CeLoss 0.2129 (0.4758)	SegCLSLoss 0.0182 (0.0143)	KLLoss 0.1582 (0.1178)	MaskLoss 1.2995 (0.6237)	MaskBCELoss 0.5043 (0.1970)	MaskDICELoss 0.7951 (0.4267)
Epoch: [2][478/500]	Time 10.378 (10.378)	Loss 0.0835 (5.9470)	CeLoss 0.0835 (0.3697)	SegCLSLoss 0.0000 (0.0172)	KLLoss 0.0000 (0.1335)	MaskLoss 0.0000 (0.8181)	MaskBCELoss 0.0000 (0.1849)	MaskDICELoss 0.0000 (0.6331)
Epoch: [2][479/500]	Time 10.370 (10.370)	Loss 4.5016 (5.1042)	CeLoss 0.2422 (0.2992)	SegCLSLoss 0.0197 (0.0187)	KLLoss 0.1523 (0.1384)	MaskLoss 0.6003 (0.6967)	MaskBCELoss 0.1175 (0.1526)	MaskDICELoss 0.4828 (0.5441)
[2025-03-04 06:12:14,435] [INFO] [logging.py:96:log_dist] [Rank 0] step=1480, skipped=0, lr=[0.0002656024096385542], mom=[(0.9, 0.95)]
[2025-03-04 06:12:14,441] [INFO] [timer.py:215:stop] epoch=0/micro_step=14800/global_step=1480, RunningAvgSamplesPerSec=1.042130906313782, CurrSamplesPerSec=1.0575650651012207, MemAllocated=57.27GB, MaxMemAllocated=62.82GB
Epoch: [2][480/500]	Time  9.458 ( 9.458)	Loss 8.4551 (5.1092)	CeLoss 0.1729 (0.5202)	SegCLSLoss 0.0334 (0.0174)	KLLoss 0.2236 (0.1325)	MaskLoss 1.0335 (0.6354)	MaskBCELoss 0.0378 (0.1059)	MaskDICELoss 0.9957 (0.5295)
Epoch: [2][481/500]	Time  9.862 ( 9.862)	Loss 5.2953 (6.2006)	CeLoss 0.2432 (0.4114)	SegCLSLoss 0.0176 (0.0247)	KLLoss 0.1582 (0.1652)	MaskLoss 0.8322 (0.8613)	MaskBCELoss 0.2954 (0.2130)	MaskDICELoss 0.5368 (0.6483)
Epoch: [2][482/500]	Time 10.167 (10.167)	Loss 0.0771 (3.1426)	CeLoss 0.0771 (0.3420)	SegCLSLoss 0.0000 (0.0135)	KLLoss 0.0000 (0.1067)	MaskLoss 0.0000 (0.4683)	MaskBCELoss 0.0000 (0.1765)	MaskDICELoss 0.0000 (0.2917)
Epoch: [2][483/500]	Time  7.682 ( 7.682)	Loss 0.5977 (2.4370)	CeLoss 0.5977 (0.6782)	SegCLSLoss 0.0000 (0.0079)	KLLoss 0.0000 (0.0558)	MaskLoss 0.0000 (0.2211)	MaskBCELoss 0.0000 (0.0117)	MaskDICELoss 0.0000 (0.2095)
Epoch: [2][484/500]	Time  8.992 ( 8.992)	Loss 8.6070 (5.9575)	CeLoss 0.2139 (0.5671)	SegCLSLoss 0.0354 (0.0196)	KLLoss 0.2285 (0.1315)	MaskLoss 1.0811 (0.7790)	MaskBCELoss 0.0838 (0.1638)	MaskDICELoss 0.9973 (0.6152)
Epoch: [2][485/500]	Time  9.657 ( 9.657)	Loss 1.6172 (5.1393)	CeLoss 1.6172 (0.4815)	SegCLSLoss 0.0000 (0.0188)	KLLoss 0.0000 (0.1309)	MaskLoss 0.0000 (0.6766)	MaskBCELoss 0.0000 (0.1493)	MaskDICELoss 0.0000 (0.5273)
Epoch: [2][486/500]	Time  8.957 ( 8.957)	Loss 7.8654 (5.6298)	CeLoss 0.2197 (0.4165)	SegCLSLoss 0.0300 (0.0259)	KLLoss 0.2617 (0.1526)	MaskLoss 1.0137 (0.8191)	MaskBCELoss 0.1234 (0.2509)	MaskDICELoss 0.8903 (0.5683)
Epoch: [2][487/500]	Time  8.163 ( 8.163)	Loss 1.4142 (4.3077)	CeLoss 0.1904 (0.4905)	SegCLSLoss 0.0194 (0.0151)	KLLoss 0.1602 (0.1187)	MaskLoss 0.2673 (0.6931)	MaskBCELoss 0.1806 (0.3089)	MaskDICELoss 0.0867 (0.3841)
Epoch: [2][488/500]	Time  9.320 ( 9.320)	Loss 7.3448 (4.7574)	CeLoss 0.1904 (0.3316)	SegCLSLoss 0.0315 (0.0213)	KLLoss 0.2129 (0.1404)	MaskLoss 0.9683 (0.6722)	MaskBCELoss 0.1366 (0.1837)	MaskDICELoss 0.8317 (0.4884)
Epoch: [2][489/500]	Time  9.505 ( 9.505)	Loss 7.4206 (5.7525)	CeLoss 0.2070 (0.5804)	SegCLSLoss 0.0270 (0.0186)	KLLoss 0.2012 (0.1357)	MaskLoss 1.1462 (0.7812)	MaskBCELoss 0.3619 (0.2037)	MaskDICELoss 0.7844 (0.5775)
[2025-03-04 06:13:45,977] [INFO] [logging.py:96:log_dist] [Rank 0] step=1490, skipped=0, lr=[0.0002653614457831325], mom=[(0.9, 0.95)]
[2025-03-04 06:13:45,982] [INFO] [timer.py:215:stop] epoch=0/micro_step=14900/global_step=1490, RunningAvgSamplesPerSec=1.0424544923025323, CurrSamplesPerSec=1.0830177235114653, MemAllocated=57.24GB, MaxMemAllocated=62.82GB
Epoch: [2][490/500]	Time  9.236 ( 9.236)	Loss 4.2391 (4.8693)	CeLoss 0.2832 (0.4952)	SegCLSLoss 0.0175 (0.0181)	KLLoss 0.1475 (0.1251)	MaskLoss 0.5543 (0.5922)	MaskBCELoss 0.1054 (0.0829)	MaskDICELoss 0.4488 (0.5093)
Epoch: [2][491/500]	Time  8.701 ( 8.701)	Loss 6.3065 (4.5588)	CeLoss 0.2344 (0.5646)	SegCLSLoss 0.0152 (0.0163)	KLLoss 0.1582 (0.1403)	MaskLoss 1.0795 (0.6016)	MaskBCELoss 0.4550 (0.1612)	MaskDICELoss 0.6245 (0.4405)
Epoch: [2][492/500]	Time 11.606 (11.606)	Loss 7.2678 (5.3503)	CeLoss 0.2441 (0.3342)	SegCLSLoss 0.0310 (0.0173)	KLLoss 0.2002 (0.1305)	MaskLoss 1.1749 (0.7963)	MaskBCELoss 0.4320 (0.2490)	MaskDICELoss 0.7429 (0.5474)
Epoch: [2][493/500]	Time  7.964 ( 7.964)	Loss 5.5106 (4.3744)	CeLoss 0.2598 (0.5713)	SegCLSLoss 0.0150 (0.0174)	KLLoss 0.1016 (0.1045)	MaskLoss 0.9102 (0.5599)	MaskBCELoss 0.3567 (0.1316)	MaskDICELoss 0.5535 (0.4284)
Epoch: [2][494/500]	Time  9.586 ( 9.586)	Loss 7.0411 (4.6776)	CeLoss 0.2383 (0.4217)	SegCLSLoss 0.0221 (0.0170)	KLLoss 0.1621 (0.1254)	MaskLoss 1.0837 (0.7212)	MaskBCELoss 0.3397 (0.2746)	MaskDICELoss 0.7439 (0.4466)
Epoch: [2][495/500]	Time 11.325 (11.325)	Loss 5.3128 (5.5274)	CeLoss 0.2676 (0.2436)	SegCLSLoss 0.0176 (0.0217)	KLLoss 0.1455 (0.1716)	MaskLoss 0.7588 (0.8458)	MaskBCELoss 0.1966 (0.2775)	MaskDICELoss 0.5622 (0.5683)
Epoch: [2][496/500]	Time 10.407 (10.407)	Loss 7.8310 (6.3133)	CeLoss 0.2334 (0.4663)	SegCLSLoss 0.0320 (0.0235)	KLLoss 0.2324 (0.1633)	MaskLoss 1.1419 (0.8454)	MaskBCELoss 0.2975 (0.1818)	MaskDICELoss 0.8445 (0.6635)
Epoch: [2][497/500]	Time  9.572 ( 9.572)	Loss 5.2786 (3.4261)	CeLoss 0.2988 (0.4028)	SegCLSLoss 0.0146 (0.0134)	KLLoss 0.1289 (0.1015)	MaskLoss 0.8183 (0.4772)	MaskBCELoss 0.2838 (0.1505)	MaskDICELoss 0.5344 (0.3268)
Epoch: [2][498/500]	Time  8.997 ( 8.997)	Loss 8.3481 (5.4780)	CeLoss 0.2266 (0.4427)	SegCLSLoss 0.0142 (0.0250)	KLLoss 0.1201 (0.1362)	MaskLoss 1.0073 (0.7046)	MaskBCELoss 0.0106 (0.1249)	MaskDICELoss 0.9967 (0.5797)
  0%|                                                                                                                                                              | 0/200 [00:00<?, ?it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  0%|                                                                                                                                                              | 0/200 [00:00<?, ?it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
[2025-03-04 06:15:24,862] [INFO] [logging.py:96:log_dist] [Rank 0] step=1500, skipped=0, lr=[0.00026512048192771083], mom=[(0.9, 0.95)]
[2025-03-04 06:15:24,868] [INFO] [timer.py:215:stop] epoch=0/micro_step=15000/global_step=1500, RunningAvgSamplesPerSec=1.0422411856282485, CurrSamplesPerSec=1.0046593509101485, MemAllocated=56.82GB, MaxMemAllocated=62.86GB
  2%|███                                                                                                                                                   | 4/200 [00:01<01:06,  2.95it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  6%|████████▏                                                                                                                                            | 11/200 [00:03<00:43,  4.37it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 10%|██████████████▏                                                                                                                                      | 19/200 [00:05<00:38,  4.65it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 14%|████████████████████                                                                                                                                 | 27/200 [00:07<00:34,  5.02it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 18%|██████████████████████████                                                                                                                           | 35/200 [00:10<00:51,  3.22it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 22%|████████████████████████████████                                                                                                                     | 43/200 [00:12<00:40,  3.87it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 26%|█████████████████████████████████████▉                                                                                                               | 51/200 [00:14<00:35,  4.15it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 30%|███████████████████████████████████████████▉                                                                                                         | 59/200 [00:16<00:30,  4.58it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 34%|█████████████████████████████████████████████████▉                                                                                                   | 67/200 [00:18<00:29,  4.52it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 38%|███████████████████████████████████████████████████████▉                                                                                             | 75/200 [00:20<00:26,  4.78it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 41%|█████████████████████████████████████████████████████████████                                                                                        | 82/200 [00:22<00:24,  4.73it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 44%|██████████████████████████████████████████████████████████████████▎                                                                                  | 89/200 [00:23<00:25,  4.44it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 48%|███████████████████████████████████████████████████████████████████████▌                                                                             | 96/200 [00:25<00:22,  4.64it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 52%|████████████████████████████████████████████████████████████████████████████▉                                                                       | 104/200 [00:27<00:21,  4.54it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 56%|██████████████████████████████████████████████████████████████████████████████████▉                                                                 | 112/200 [00:30<00:30,  2.86it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 60%|████████████████████████████████████████████████████████████████████████████████████████▊                                                           | 120/200 [00:32<00:25,  3.20it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 64%|███████████████████████████████████████████████████████████████████████████████████████████████▍                                                    | 129/200 [00:34<00:15,  4.44it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 68%|█████████████████████████████████████████████████████████████████████████████████████████████████████▍                                              | 137/200 [00:36<00:13,  4.62it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                        | 145/200 [00:38<00:11,  4.90it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 76%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                  | 153/200 [00:40<00:09,  5.04it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                            | 161/200 [00:42<00:08,  4.80it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 84%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                       | 168/200 [00:44<00:06,  4.78it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                 | 176/200 [00:46<00:05,  4.73it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍            | 183/200 [00:47<00:03,  4.48it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌       | 190/200 [00:49<00:02,  4.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 195/200 [00:52<00:02,  2.31it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 195/200 [00:52<00:02,  2.31it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
Epoch: [3][  1/500]	Time 11.337 (11.337)	Loss 5.6094 (5.2888)	CeLoss 0.2217 (0.3558)	SegCLSLoss 0.0240 (0.0186)	KLLoss 0.1543 (0.1428)	MaskLoss 0.6965 (0.7362)	MaskBCELoss 0.0583 (0.1846)	MaskDICELoss 0.6383 (0.5515)
Epoch: [3][  2/500]	Time  8.641 ( 8.641)	Loss 5.4543 (3.3484)	CeLoss 0.2090 (0.4106)	SegCLSLoss 0.0272 (0.0173)	KLLoss 0.2002 (0.1232)	MaskLoss 0.6763 (0.4491)	MaskBCELoss 0.0630 (0.1312)	MaskDICELoss 0.6133 (0.3180)
Epoch: [3][  3/500]	Time  7.635 ( 7.635)	Loss 7.8179 (3.6524)	CeLoss 0.4688 (0.6114)	SegCLSLoss 0.0192 (0.0111)	KLLoss 0.2188 (0.0980)	MaskLoss 1.6402 (0.5358)	MaskBCELoss 0.9998 (0.2248)	MaskDICELoss 0.6404 (0.3110)
Epoch: [3][  4/500]	Time  8.927 ( 8.927)	Loss 8.4224 (5.7078)	CeLoss 0.2412 (0.4004)	SegCLSLoss 0.0288 (0.0224)	KLLoss 0.1992 (0.1414)	MaskLoss 1.0153 (0.7711)	MaskBCELoss 0.0259 (0.1690)	MaskDICELoss 0.9894 (0.6021)
Epoch: [3][  5/500]	Time  9.568 ( 9.568)	Loss 4.1495 (5.1795)	CeLoss 0.3164 (0.4516)	SegCLSLoss 0.0197 (0.0245)	KLLoss 0.1230 (0.1524)	MaskLoss 0.5576 (0.7289)	MaskBCELoss 0.1268 (0.2113)	MaskDICELoss 0.4308 (0.5176)
Epoch: [3][  6/500]	Time 10.359 (10.359)	Loss 1.9062 (6.3279)	CeLoss 1.9062 (0.5146)	SegCLSLoss 0.0000 (0.0203)	KLLoss 0.0000 (0.1474)	MaskLoss 0.0000 (0.8588)	MaskBCELoss 0.0000 (0.2025)	MaskDICELoss 0.0000 (0.6564)
Epoch: [3][  7/500]	Time 10.042 (10.042)	Loss 8.3048 (5.8923)	CeLoss 0.1953 (0.3381)	SegCLSLoss 0.0233 (0.0229)	KLLoss 0.1738 (0.1589)	MaskLoss 1.3534 (0.8282)	MaskBCELoss 0.4838 (0.2070)	MaskDICELoss 0.8695 (0.6212)
Epoch: [3][  8/500]	Time 12.064 (12.064)	Loss 8.0049 (6.8834)	CeLoss 0.2412 (0.2617)	SegCLSLoss 0.0243 (0.0241)	KLLoss 0.1621 (0.1659)	MaskLoss 1.1733 (1.0229)	MaskBCELoss 0.2996 (0.2899)	MaskDICELoss 0.8737 (0.7331)
Epoch: [3][  9/500]	Time  9.397 ( 9.397)	Loss 0.8555 (4.9460)	CeLoss 0.8555 (0.4210)	SegCLSLoss 0.0000 (0.0198)	KLLoss 0.0000 (0.1356)	MaskLoss 0.0000 (0.7130)	MaskBCELoss 0.0000 (0.2208)	MaskDICELoss 0.0000 (0.4923)
[2025-03-04 06:17:56,287] [INFO] [logging.py:96:log_dist] [Rank 0] step=1510, skipped=0, lr=[0.00026489156626506024], mom=[(0.9, 0.95)]
[2025-03-04 06:17:56,293] [INFO] [timer.py:215:stop] epoch=0/micro_step=15100/global_step=1510, RunningAvgSamplesPerSec=1.042117412428729, CurrSamplesPerSec=0.9922353159876013, MemAllocated=57.25GB, MaxMemAllocated=62.86GB
Epoch: [3][ 10/500]	Time 10.080 (10.080)	Loss 4.5639 (5.0049)	CeLoss 0.2539 (0.2817)	SegCLSLoss 0.0266 (0.0246)	KLLoss 0.1719 (0.1503)	MaskLoss 0.5920 (0.7136)	MaskBCELoss 0.1019 (0.1915)	MaskDICELoss 0.4901 (0.5222)
Epoch: [3][ 11/500]	Time  8.269 ( 8.269)	Loss 0.6055 (4.8994)	CeLoss 0.6055 (0.6215)	SegCLSLoss 0.0000 (0.0241)	KLLoss 0.0000 (0.1176)	MaskLoss 0.0000 (0.6592)	MaskBCELoss 0.0000 (0.1875)	MaskDICELoss 0.0000 (0.4717)
Epoch: [3][ 12/500]	Time  7.946 ( 7.946)	Loss 6.7635 (3.6336)	CeLoss 0.2471 (0.6783)	SegCLSLoss 0.0194 (0.0098)	KLLoss 0.1934 (0.0860)	MaskLoss 1.4228 (0.4685)	MaskBCELoss 0.8447 (0.1472)	MaskDICELoss 0.5781 (0.3212)
Epoch: [3][ 13/500]	Time  9.715 ( 9.715)	Loss 7.6680 (5.6335)	CeLoss 0.2715 (0.1999)	SegCLSLoss 0.0264 (0.0289)	KLLoss 0.1689 (0.1708)	MaskLoss 1.1825 (0.8172)	MaskBCELoss 0.3742 (0.2149)	MaskDICELoss 0.8083 (0.6023)
Epoch: [3][ 14/500]	Time  9.718 ( 9.718)	Loss 8.2129 (5.9289)	CeLoss 0.1846 (0.3318)	SegCLSLoss 0.0396 (0.0248)	KLLoss 0.1621 (0.1443)	MaskLoss 1.4467 (0.8569)	MaskBCELoss 0.6214 (0.2359)	MaskDICELoss 0.8254 (0.6210)
Epoch: [3][ 15/500]	Time  9.344 ( 9.344)	Loss 8.1205 (4.7635)	CeLoss 0.2061 (0.5966)	SegCLSLoss 0.0270 (0.0130)	KLLoss 0.1650 (0.1009)	MaskLoss 1.4604 (0.6685)	MaskBCELoss 0.6579 (0.2148)	MaskDICELoss 0.8025 (0.4537)
Epoch: [3][ 16/500]	Time  8.796 ( 8.796)	Loss 7.3697 (4.1988)	CeLoss 0.2041 (0.4970)	SegCLSLoss 0.0243 (0.0145)	KLLoss 0.1191 (0.1085)	MaskLoss 0.9391 (0.5790)	MaskBCELoss 0.0799 (0.1743)	MaskDICELoss 0.8592 (0.4047)
Epoch: [3][ 17/500]	Time 10.152 (10.152)	Loss 4.1808 (5.7611)	CeLoss 0.2285 (0.3289)	SegCLSLoss 0.0151 (0.0212)	KLLoss 0.1143 (0.1536)	MaskLoss 0.5908 (0.7593)	MaskBCELoss 0.1496 (0.1343)	MaskDICELoss 0.4413 (0.6249)
Epoch: [3][ 18/500]	Time  9.171 ( 9.171)	Loss 4.9639 (4.7844)	CeLoss 0.2559 (0.6137)	SegCLSLoss 0.0266 (0.0164)	KLLoss 0.1924 (0.1187)	MaskLoss 0.6510 (0.5730)	MaskBCELoss 0.1175 (0.0901)	MaskDICELoss 0.5335 (0.4829)
Epoch: [3][ 19/500]	Time  7.446 ( 7.446)	Loss 1.4141 (2.6259)	CeLoss 1.4141 (0.7589)	SegCLSLoss 0.0000 (0.0082)	KLLoss 0.0000 (0.0581)	MaskLoss 0.0000 (0.3146)	MaskBCELoss 0.0000 (0.1186)	MaskDICELoss 0.0000 (0.1960)
[2025-03-04 06:19:26,773] [INFO] [logging.py:96:log_dist] [Rank 0] step=1520, skipped=0, lr=[0.00026465060240963857], mom=[(0.9, 0.95)]
[2025-03-04 06:19:26,779] [INFO] [timer.py:215:stop] epoch=0/micro_step=15200/global_step=1520, RunningAvgSamplesPerSec=1.04251034557397, CurrSamplesPerSec=1.0073636209815704, MemAllocated=57.47GB, MaxMemAllocated=62.86GB
Epoch: [3][ 20/500]	Time  9.930 ( 9.930)	Loss 7.6311 (5.0301)	CeLoss 0.2021 (0.3714)	SegCLSLoss 0.0287 (0.0187)	KLLoss 0.1777 (0.1385)	MaskLoss 0.9462 (0.6926)	MaskBCELoss 0.0555 (0.1717)	MaskDICELoss 0.8907 (0.5210)
Epoch: [3][ 21/500]	Time 11.097 (11.097)	Loss 7.8914 (6.0317)	CeLoss 0.2070 (0.3571)	SegCLSLoss 0.0337 (0.0262)	KLLoss 0.2500 (0.1647)	MaskLoss 0.9956 (0.8867)	MaskBCELoss 0.0910 (0.2660)	MaskDICELoss 0.9046 (0.6207)
Epoch: [3][ 22/500]	Time 10.570 (10.570)	Loss 7.8109 (5.4944)	CeLoss 0.1250 (0.3277)	SegCLSLoss 0.0547 (0.0235)	KLLoss 0.2441 (0.1531)	MaskLoss 1.5149 (0.7748)	MaskBCELoss 0.7841 (0.1993)	MaskDICELoss 0.7308 (0.5754)
Epoch: [3][ 23/500]	Time  9.245 ( 9.245)	Loss 8.2132 (5.0037)	CeLoss 0.2451 (0.4724)	SegCLSLoss 0.0227 (0.0174)	KLLoss 0.1670 (0.1257)	MaskLoss 1.0272 (0.6646)	MaskBCELoss 0.0710 (0.1532)	MaskDICELoss 0.9562 (0.5114)
Epoch: [3][ 24/500]	Time  9.787 ( 9.787)	Loss 4.0061 (5.8392)	CeLoss 0.2812 (0.3050)	SegCLSLoss 0.0183 (0.0264)	KLLoss 0.1387 (0.1656)	MaskLoss 0.6013 (0.9895)	MaskBCELoss 0.2057 (0.4267)	MaskDICELoss 0.3956 (0.5628)
Epoch: [3][ 25/500]	Time  9.439 ( 9.439)	Loss 8.2466 (4.8162)	CeLoss 0.2363 (0.5019)	SegCLSLoss 0.0251 (0.0199)	KLLoss 0.1553 (0.1294)	MaskLoss 0.9955 (0.6559)	MaskBCELoss 0.0203 (0.1787)	MaskDICELoss 0.9752 (0.4772)
Epoch: [3][ 26/500]	Time  9.058 ( 9.058)	Loss 5.2561 (4.2752)	CeLoss 0.2676 (0.4840)	SegCLSLoss 0.0177 (0.0154)	KLLoss 0.1709 (0.1198)	MaskLoss 0.7538 (0.5812)	MaskBCELoss 0.2039 (0.1643)	MaskDICELoss 0.5499 (0.4169)
Epoch: [3][ 27/500]	Time  9.172 ( 9.172)	Loss 8.7559 (5.5407)	CeLoss 0.1914 (0.4604)	SegCLSLoss 0.0251 (0.0175)	KLLoss 0.1689 (0.1203)	MaskLoss 1.6308 (0.7804)	MaskBCELoss 0.7773 (0.2153)	MaskDICELoss 0.8535 (0.5651)
Epoch: [3][ 28/500]	Time 11.341 (11.341)	Loss 1.0235 (6.2507)	CeLoss 0.2676 (0.2184)	SegCLSLoss 0.0179 (0.0272)	KLLoss 0.1396 (0.1782)	MaskLoss 0.1398 (0.8772)	MaskBCELoss 0.0855 (0.1963)	MaskDICELoss 0.0543 (0.6809)
Epoch: [3][ 29/500]	Time  8.931 ( 8.931)	Loss 3.8380 (5.8025)	CeLoss 0.2334 (0.4144)	SegCLSLoss 0.0240 (0.0284)	KLLoss 0.1865 (0.1717)	MaskLoss 0.5164 (0.7922)	MaskBCELoss 0.1208 (0.1893)	MaskDICELoss 0.3956 (0.6030)
[2025-03-04 06:21:05,384] [INFO] [logging.py:96:log_dist] [Rank 0] step=1530, skipped=0, lr=[0.00026440963855421684], mom=[(0.9, 0.95)]
[2025-03-04 06:21:05,390] [INFO] [timer.py:215:stop] epoch=0/micro_step=15300/global_step=1530, RunningAvgSamplesPerSec=1.04232028143445, CurrSamplesPerSec=1.0031173786601744, MemAllocated=57.25GB, MaxMemAllocated=62.86GB
Epoch: [3][ 30/500]	Time  9.971 ( 9.971)	Loss 5.7547 (6.1364)	CeLoss 0.3398 (0.2548)	SegCLSLoss 0.0172 (0.0208)	KLLoss 0.1299 (0.1698)	MaskLoss 0.8734 (0.9079)	MaskBCELoss 0.2848 (0.2603)	MaskDICELoss 0.5886 (0.6476)
Epoch: [3][ 31/500]	Time  7.989 ( 7.989)	Loss 1.3203 (4.3777)	CeLoss 1.3203 (0.6982)	SegCLSLoss 0.0000 (0.0213)	KLLoss 0.0000 (0.0995)	MaskLoss 0.0000 (0.5929)	MaskBCELoss 0.0000 (0.1956)	MaskDICELoss 0.0000 (0.3972)
Epoch: [3][ 32/500]	Time  9.811 ( 9.811)	Loss 3.7170 (4.3248)	CeLoss 0.2236 (0.4491)	SegCLSLoss 0.0182 (0.0233)	KLLoss 0.1445 (0.1149)	MaskLoss 0.5262 (0.6706)	MaskBCELoss 0.1450 (0.2693)	MaskDICELoss 0.3813 (0.4013)
Epoch: [3][ 33/500]	Time  9.552 ( 9.552)	Loss 1.8672 (4.9391)	CeLoss 1.8672 (0.4611)	SegCLSLoss 0.0000 (0.0207)	KLLoss 0.0000 (0.1332)	MaskLoss 0.0000 (0.6484)	MaskBCELoss 0.0000 (0.1421)	MaskDICELoss 0.0000 (0.5063)
Epoch: [3][ 34/500]	Time 10.886 (10.886)	Loss 4.4287 (6.4661)	CeLoss 0.3965 (0.4074)	SegCLSLoss 0.0157 (0.0253)	KLLoss 0.0967 (0.1549)	MaskLoss 0.6835 (0.9080)	MaskBCELoss 0.2565 (0.2288)	MaskDICELoss 0.4270 (0.6792)
Epoch: [3][ 35/500]	Time  9.932 ( 9.932)	Loss 7.2583 (4.6705)	CeLoss 0.2021 (0.4904)	SegCLSLoss 0.0278 (0.0160)	KLLoss 0.1973 (0.1211)	MaskLoss 1.2779 (0.6508)	MaskBCELoss 0.5631 (0.1925)	MaskDICELoss 0.7148 (0.4582)
Epoch: [3][ 36/500]	Time  9.840 ( 9.840)	Loss 7.0548 (4.7084)	CeLoss 0.2832 (0.4518)	SegCLSLoss 0.0209 (0.0165)	KLLoss 0.0889 (0.1191)	MaskLoss 1.1496 (0.6625)	MaskBCELoss 0.4208 (0.1952)	MaskDICELoss 0.7288 (0.4673)
Epoch: [3][ 37/500]	Time  9.215 ( 9.215)	Loss 7.6143 (5.6544)	CeLoss 0.1816 (0.5132)	SegCLSLoss 0.0381 (0.0189)	KLLoss 0.2734 (0.1287)	MaskLoss 1.0791 (0.7964)	MaskBCELoss 0.2489 (0.2281)	MaskDICELoss 0.8302 (0.5683)
Epoch: [3][ 38/500]	Time 10.005 (10.005)	Loss 5.4249 (6.5530)	CeLoss 0.2715 (0.2423)	SegCLSLoss 0.0435 (0.0291)	KLLoss 0.2461 (0.1874)	MaskLoss 0.9569 (1.0436)	MaskBCELoss 0.4616 (0.3733)	MaskDICELoss 0.4953 (0.6703)
Epoch: [3][ 39/500]	Time  8.920 ( 8.920)	Loss 8.7085 (5.3822)	CeLoss 0.1055 (0.5170)	SegCLSLoss 0.0525 (0.0216)	KLLoss 0.2285 (0.1275)	MaskLoss 1.3503 (0.7607)	MaskBCELoss 0.4088 (0.2265)	MaskDICELoss 0.9414 (0.5342)
[2025-03-04 06:22:40,036] [INFO] [logging.py:96:log_dist] [Rank 0] step=1540, skipped=0, lr=[0.00026416867469879516], mom=[(0.9, 0.95)]
[2025-03-04 06:22:40,041] [INFO] [timer.py:215:stop] epoch=0/micro_step=15400/global_step=1540, RunningAvgSamplesPerSec=1.0424124366223961, CurrSamplesPerSec=1.1763120818631434, MemAllocated=57.26GB, MaxMemAllocated=62.86GB
Epoch: [3][ 40/500]	Time  8.503 ( 8.503)	Loss 7.1028 (4.4259)	CeLoss 0.2432 (0.5264)	SegCLSLoss 0.0165 (0.0145)	KLLoss 0.1426 (0.1127)	MaskLoss 1.1342 (0.5939)	MaskBCELoss 0.3942 (0.1620)	MaskDICELoss 0.7400 (0.4319)
Epoch: [3][ 41/500]	Time 10.441 (10.441)	Loss 2.8522 (5.5950)	CeLoss 0.2344 (0.2852)	SegCLSLoss 0.0201 (0.0214)	KLLoss 0.1211 (0.1583)	MaskLoss 0.3472 (0.7649)	MaskBCELoss 0.0485 (0.1630)	MaskDICELoss 0.2988 (0.6019)
Epoch: [3][ 42/500]	Time 11.045 (11.045)	Loss 5.1346 (6.6792)	CeLoss 0.3086 (0.2436)	SegCLSLoss 0.0146 (0.0259)	KLLoss 0.0952 (0.1668)	MaskLoss 1.1538 (1.0223)	MaskBCELoss 0.7513 (0.3204)	MaskDICELoss 0.4025 (0.7018)
Epoch: [3][ 43/500]	Time  9.956 ( 9.956)	Loss 8.1299 (6.7877)	CeLoss 0.2334 (0.2043)	SegCLSLoss 0.0256 (0.0275)	KLLoss 0.1777 (0.1725)	MaskLoss 1.0365 (0.9278)	MaskBCELoss 0.0977 (0.1708)	MaskDICELoss 0.9388 (0.7570)
Epoch: [3][ 44/500]	Time 10.707 (10.707)	Loss 5.2055 (5.7593)	CeLoss 0.2539 (0.3131)	SegCLSLoss 0.0232 (0.0198)	KLLoss 0.1689 (0.1409)	MaskLoss 0.6339 (0.8348)	MaskBCELoss 0.0499 (0.2304)	MaskDICELoss 0.5840 (0.6044)
Epoch: [3][ 45/500]	Time 10.118 (10.118)	Loss 8.0963 (5.6153)	CeLoss 0.2021 (0.4045)	SegCLSLoss 0.0253 (0.0203)	KLLoss 0.2031 (0.1256)	MaskLoss 0.9650 (0.7823)	MaskBCELoss 0.0070 (0.1972)	MaskDICELoss 0.9580 (0.5851)
Epoch: [3][ 46/500]	Time  7.819 ( 7.819)	Loss 1.4531 (2.3416)	CeLoss 1.4531 (0.5189)	SegCLSLoss 0.0000 (0.0078)	KLLoss 0.0000 (0.0620)	MaskLoss 0.0000 (0.2927)	MaskBCELoss 0.0000 (0.0975)	MaskDICELoss 0.0000 (0.1952)
Epoch: [3][ 47/500]	Time  9.133 ( 9.133)	Loss 1.5078 (5.2953)	CeLoss 1.5078 (0.4200)	SegCLSLoss 0.0000 (0.0210)	KLLoss 0.0000 (0.1219)	MaskLoss 0.0000 (0.7692)	MaskBCELoss 0.0000 (0.2351)	MaskDICELoss 0.0000 (0.5341)
Epoch: [3][ 48/500]	Time  7.992 ( 7.992)	Loss 1.3203 (3.8391)	CeLoss 1.3203 (0.7575)	SegCLSLoss 0.0000 (0.0155)	KLLoss 0.0000 (0.0965)	MaskLoss 0.0000 (0.4506)	MaskBCELoss 0.0000 (0.1045)	MaskDICELoss 0.0000 (0.3461)
Epoch: [3][ 49/500]	Time  8.803 ( 8.803)	Loss 7.4922 (5.5917)	CeLoss 0.2344 (0.3994)	SegCLSLoss 0.0220 (0.0217)	KLLoss 0.1152 (0.1270)	MaskLoss 2.0352 (0.9401)	MaskBCELoss 1.5248 (0.4110)	MaskDICELoss 0.5104 (0.5291)
[2025-03-04 06:24:15,282] [INFO] [logging.py:96:log_dist] [Rank 0] step=1550, skipped=0, lr=[0.0002639277108433735], mom=[(0.9, 0.95)]
[2025-03-04 06:24:15,288] [INFO] [timer.py:215:stop] epoch=0/micro_step=15500/global_step=1550, RunningAvgSamplesPerSec=1.0424615743508356, CurrSamplesPerSec=1.0832533499712844, MemAllocated=56.71GB, MaxMemAllocated=62.86GB
Epoch: [3][ 50/500]	Time  9.233 ( 9.233)	Loss 1.6016 (3.1780)	CeLoss 1.6016 (0.4975)	SegCLSLoss 0.0000 (0.0116)	KLLoss 0.0000 (0.0764)	MaskLoss 0.0000 (0.3768)	MaskBCELoss 0.0000 (0.0693)	MaskDICELoss 0.0000 (0.3075)
Epoch: [3][ 51/500]	Time  8.302 ( 8.302)	Loss 7.8387 (4.7167)	CeLoss 0.1118 (0.2336)	SegCLSLoss 0.0515 (0.0194)	KLLoss 0.2373 (0.1255)	MaskLoss 1.1285 (0.6549)	MaskBCELoss 0.2607 (0.1486)	MaskDICELoss 0.8678 (0.5063)
Epoch: [3][ 52/500]	Time  8.875 ( 8.875)	Loss 1.7578 (5.3369)	CeLoss 1.7578 (0.4059)	SegCLSLoss 0.0000 (0.0164)	KLLoss 0.0000 (0.1189)	MaskLoss 0.0000 (0.7708)	MaskBCELoss 0.0000 (0.2272)	MaskDICELoss 0.0000 (0.5436)
Epoch: [3][ 53/500]	Time  9.388 ( 9.388)	Loss 1.1016 (4.8707)	CeLoss 1.1016 (0.3363)	SegCLSLoss 0.0000 (0.0232)	KLLoss 0.0000 (0.1330)	MaskLoss 0.0000 (0.7680)	MaskBCELoss 0.0000 (0.2923)	MaskDICELoss 0.0000 (0.4757)
Epoch: [3][ 54/500]	Time  9.320 ( 9.320)	Loss 7.9358 (5.5743)	CeLoss 0.2266 (0.3055)	SegCLSLoss 0.0215 (0.0235)	KLLoss 0.1807 (0.1633)	MaskLoss 1.0245 (0.8463)	MaskBCELoss 0.1130 (0.2795)	MaskDICELoss 0.9115 (0.5668)
Epoch: [3][ 55/500]	Time  9.168 ( 9.168)	Loss 0.5234 (4.7253)	CeLoss 0.5234 (0.5967)	SegCLSLoss 0.0000 (0.0151)	KLLoss 0.0000 (0.0998)	MaskLoss 0.0000 (0.5899)	MaskBCELoss 0.0000 (0.1163)	MaskDICELoss 0.0000 (0.4736)
Epoch: [3][ 56/500]	Time 10.778 (10.778)	Loss 3.1441 (6.3361)	CeLoss 0.3145 (0.4128)	SegCLSLoss 0.0143 (0.0237)	KLLoss 0.0889 (0.1585)	MaskLoss 0.4854 (0.8626)	MaskBCELoss 0.1916 (0.1914)	MaskDICELoss 0.2938 (0.6713)
Epoch: [3][ 57/500]	Time 10.663 (10.663)	Loss 6.5188 (5.8078)	CeLoss 0.1797 (0.3228)	SegCLSLoss 0.0356 (0.0209)	KLLoss 0.2676 (0.1454)	MaskLoss 0.8419 (0.7746)	MaskBCELoss 0.1135 (0.1446)	MaskDICELoss 0.7284 (0.6301)
Epoch: [3][ 58/500]	Time  9.177 ( 9.177)	Loss 8.2973 (6.8280)	CeLoss 0.2354 (0.3701)	SegCLSLoss 0.0383 (0.0274)	KLLoss 0.2080 (0.1719)	MaskLoss 1.0682 (0.9552)	MaskBCELoss 0.1182 (0.2281)	MaskDICELoss 0.9500 (0.7271)
Epoch: [3][ 59/500]	Time  9.296 ( 9.296)	Loss 1.2266 (4.8425)	CeLoss 1.2266 (0.3589)	SegCLSLoss 0.0000 (0.0201)	KLLoss 0.0000 (0.1358)	MaskLoss 0.0000 (0.6991)	MaskBCELoss 0.0000 (0.2091)	MaskDICELoss 0.0000 (0.4899)
[2025-03-04 06:25:49,787] [INFO] [logging.py:96:log_dist] [Rank 0] step=1560, skipped=0, lr=[0.00026368674698795176], mom=[(0.9, 0.95)]
[2025-03-04 06:25:49,793] [INFO] [timer.py:215:stop] epoch=0/micro_step=15600/global_step=1560, RunningAvgSamplesPerSec=1.0425619735933662, CurrSamplesPerSec=1.0485605599457548, MemAllocated=57.11GB, MaxMemAllocated=62.86GB
Epoch: [3][ 60/500]	Time  9.539 ( 9.539)	Loss 4.2722 (5.5309)	CeLoss 0.2344 (0.2576)	SegCLSLoss 0.0166 (0.0255)	KLLoss 0.1621 (0.1564)	MaskLoss 1.0750 (0.7988)	MaskBCELoss 0.7887 (0.2144)	MaskDICELoss 0.2863 (0.5844)
Epoch: [3][ 61/500]	Time  9.935 ( 9.935)	Loss 5.9617 (6.0132)	CeLoss 0.2041 (0.4648)	SegCLSLoss 0.0280 (0.0176)	KLLoss 0.1982 (0.1356)	MaskLoss 0.7728 (0.7475)	MaskBCELoss 0.1062 (0.0959)	MaskDICELoss 0.6667 (0.6515)
Epoch: [3][ 62/500]	Time 10.787 (10.787)	Loss 3.8233 (4.7568)	CeLoss 0.2812 (0.3188)	SegCLSLoss 0.0165 (0.0180)	KLLoss 0.1309 (0.1296)	MaskLoss 0.7040 (0.7416)	MaskBCELoss 0.3714 (0.2722)	MaskDICELoss 0.3326 (0.4694)
Epoch: [3][ 63/500]	Time  8.183 ( 8.183)	Loss 7.1916 (3.5095)	CeLoss 0.2910 (0.7143)	SegCLSLoss 0.0167 (0.0141)	KLLoss 0.1738 (0.0964)	MaskLoss 0.9604 (0.3885)	MaskBCELoss 0.1606 (0.0694)	MaskDICELoss 0.7997 (0.3191)
Epoch: [3][ 64/500]	Time  9.603 ( 9.603)	Loss 6.7563 (5.1904)	CeLoss 0.1660 (0.3532)	SegCLSLoss 0.0364 (0.0235)	KLLoss 0.2314 (0.1430)	MaskLoss 0.8384 (0.7384)	MaskBCELoss 0.0612 (0.2041)	MaskDICELoss 0.7772 (0.5343)
Epoch: [3][ 65/500]	Time  9.322 ( 9.322)	Loss 7.5431 (5.3151)	CeLoss 0.2090 (0.3903)	SegCLSLoss 0.0273 (0.0195)	KLLoss 0.1719 (0.1322)	MaskLoss 1.0124 (0.7036)	MaskBCELoss 0.1584 (0.1411)	MaskDICELoss 0.8540 (0.5625)
Epoch: [3][ 66/500]	Time  7.986 ( 7.986)	Loss 1.6484 (5.2976)	CeLoss 1.6484 (0.5237)	SegCLSLoss 0.0000 (0.0155)	KLLoss 0.0000 (0.1137)	MaskLoss 0.0000 (0.6746)	MaskBCELoss 0.0000 (0.1240)	MaskDICELoss 0.0000 (0.5505)
Epoch: [3][ 67/500]	Time  9.832 ( 9.832)	Loss 5.2458 (5.0536)	CeLoss 0.2197 (0.4474)	SegCLSLoss 0.0203 (0.0161)	KLLoss 0.1357 (0.1162)	MaskLoss 0.7626 (0.6479)	MaskBCELoss 0.2034 (0.1169)	MaskDICELoss 0.5592 (0.5310)
Epoch: [3][ 68/500]	Time  8.383 ( 8.383)	Loss 2.6741 (4.6548)	CeLoss 0.1895 (0.4291)	SegCLSLoss 0.0264 (0.0171)	KLLoss 0.1807 (0.1217)	MaskLoss 0.4520 (0.6632)	MaskBCELoss 0.2208 (0.2017)	MaskDICELoss 0.2312 (0.4615)
Epoch: [3][ 69/500]	Time 10.364 (10.364)	Loss 0.9102 (5.3832)	CeLoss 0.9102 (0.3104)	SegCLSLoss 0.0000 (0.0230)	KLLoss 0.0000 (0.1446)	MaskLoss 0.0000 (0.8035)	MaskBCELoss 0.0000 (0.2519)	MaskDICELoss 0.0000 (0.5516)
[2025-03-04 06:27:22,388] [INFO] [logging.py:96:log_dist] [Rank 0] step=1570, skipped=0, lr=[0.0002634457831325301], mom=[(0.9, 0.95)]
[2025-03-04 06:27:22,393] [INFO] [timer.py:215:stop] epoch=0/micro_step=15700/global_step=1570, RunningAvgSamplesPerSec=1.0427931841789493, CurrSamplesPerSec=1.2189548327431248, MemAllocated=57.26GB, MaxMemAllocated=62.86GB
Epoch: [3][ 70/500]	Time  8.206 ( 8.206)	Loss 7.2816 (4.6426)	CeLoss 0.2158 (0.5108)	SegCLSLoss 0.0315 (0.0191)	KLLoss 0.2188 (0.1258)	MaskLoss 1.1861 (0.6283)	MaskBCELoss 0.4428 (0.1717)	MaskDICELoss 0.7434 (0.4567)
Epoch: [3][ 71/500]	Time  8.237 ( 8.237)	Loss 1.1172 (3.0586)	CeLoss 1.1172 (0.8571)	SegCLSLoss 0.0000 (0.0086)	KLLoss 0.0000 (0.0612)	MaskLoss 0.0000 (0.4219)	MaskBCELoss 0.0000 (0.2065)	MaskDICELoss 0.0000 (0.2154)
Epoch: [3][ 72/500]	Time 11.139 (11.139)	Loss 8.2156 (7.3530)	CeLoss 0.1904 (0.2170)	SegCLSLoss 0.0303 (0.0303)	KLLoss 0.1924 (0.1830)	MaskLoss 1.3472 (1.0871)	MaskBCELoss 0.4934 (0.2931)	MaskDICELoss 0.8538 (0.7939)
Epoch: [3][ 73/500]	Time  8.672 ( 8.672)	Loss 7.5989 (4.7312)	CeLoss 0.1973 (0.2750)	SegCLSLoss 0.0275 (0.0157)	KLLoss 0.1807 (0.1178)	MaskLoss 1.3769 (0.6706)	MaskBCELoss 0.6345 (0.1724)	MaskDICELoss 0.7424 (0.4982)
Epoch: [3][ 74/500]	Time  9.016 ( 9.016)	Loss 0.2871 (4.6098)	CeLoss 0.2871 (0.3757)	SegCLSLoss 0.0000 (0.0141)	KLLoss 0.0000 (0.0952)	MaskLoss 0.0000 (0.6588)	MaskBCELoss 0.0000 (0.1897)	MaskDICELoss 0.0000 (0.4691)
Epoch: [3][ 75/500]	Time  8.300 ( 8.300)	Loss 7.5570 (5.5070)	CeLoss 0.2148 (0.5648)	SegCLSLoss 0.0292 (0.0190)	KLLoss 0.1934 (0.1306)	MaskLoss 0.9715 (0.7105)	MaskBCELoss 0.1062 (0.1469)	MaskDICELoss 0.8654 (0.5636)
Epoch: [3][ 76/500]	Time 11.248 (11.248)	Loss 0.0713 (5.3471)	CeLoss 0.0713 (0.3620)	SegCLSLoss 0.0000 (0.0185)	KLLoss 0.0000 (0.1226)	MaskLoss 0.0000 (0.7388)	MaskBCELoss 0.0000 (0.1763)	MaskDICELoss 0.0000 (0.5626)
Epoch: [3][ 77/500]	Time  7.485 ( 7.485)	Loss 4.5004 (4.1197)	CeLoss 0.2275 (0.4964)	SegCLSLoss 0.0152 (0.0139)	KLLoss 0.1094 (0.0957)	MaskLoss 0.7183 (0.4928)	MaskBCELoss 0.2650 (0.0703)	MaskDICELoss 0.4533 (0.4225)
Epoch: [3][ 78/500]	Time 10.602 (10.602)	Loss 5.4183 (5.3840)	CeLoss 0.2812 (0.4252)	SegCLSLoss 0.0236 (0.0197)	KLLoss 0.1240 (0.1381)	MaskLoss 0.7537 (0.7442)	MaskBCELoss 0.1715 (0.1905)	MaskDICELoss 0.5822 (0.5537)
Epoch: [3][ 79/500]	Time 10.147 (10.147)	Loss 6.9069 (6.4615)	CeLoss 0.4629 (0.3029)	SegCLSLoss 0.0172 (0.0229)	KLLoss 0.1572 (0.1578)	MaskLoss 0.9321 (0.9623)	MaskBCELoss 0.1965 (0.2847)	MaskDICELoss 0.7356 (0.6775)
[2025-03-04 06:28:56,393] [INFO] [logging.py:96:log_dist] [Rank 0] step=1580, skipped=0, lr=[0.0002632048192771084], mom=[(0.9, 0.95)]
[2025-03-04 06:28:56,398] [INFO] [timer.py:215:stop] epoch=0/micro_step=15800/global_step=1580, RunningAvgSamplesPerSec=1.0429246447615592, CurrSamplesPerSec=1.091986702888283, MemAllocated=56.73GB, MaxMemAllocated=62.86GB
Epoch: [3][ 80/500]	Time  9.160 ( 9.160)	Loss 1.3984 (4.5401)	CeLoss 1.3984 (0.4024)	SegCLSLoss 0.0000 (0.0165)	KLLoss 0.0000 (0.1024)	MaskLoss 0.0000 (0.5550)	MaskBCELoss 0.0000 (0.0689)	MaskDICELoss 0.0000 (0.4861)
Epoch: [3][ 81/500]	Time 10.304 (10.304)	Loss 4.8226 (4.6689)	CeLoss 0.1982 (0.3173)	SegCLSLoss 0.0327 (0.0219)	KLLoss 0.1982 (0.1166)	MaskLoss 0.8186 (0.6593)	MaskBCELoss 0.3567 (0.1751)	MaskDICELoss 0.4619 (0.4842)
Epoch: [3][ 82/500]	Time  9.378 ( 9.378)	Loss 2.7021 (4.8192)	CeLoss 0.3125 (0.4995)	SegCLSLoss 0.0144 (0.0160)	KLLoss 0.1064 (0.1163)	MaskLoss 0.4074 (0.6244)	MaskBCELoss 0.1638 (0.1332)	MaskDICELoss 0.2436 (0.4911)
Epoch: [3][ 83/500]	Time 11.877 (11.877)	Loss 5.9766 (6.1635)	CeLoss 0.2559 (0.2483)	SegCLSLoss 0.0248 (0.0235)	KLLoss 0.2002 (0.1586)	MaskLoss 0.9033 (0.9408)	MaskBCELoss 0.2864 (0.2970)	MaskDICELoss 0.6169 (0.6438)
Epoch: [3][ 84/500]	Time 11.500 (11.500)	Loss 1.4141 (6.7490)	CeLoss 1.4141 (0.3669)	SegCLSLoss 0.0000 (0.0257)	KLLoss 0.0000 (0.1529)	MaskLoss 0.0000 (0.9509)	MaskBCELoss 0.0000 (0.2318)	MaskDICELoss 0.0000 (0.7191)
Epoch: [3][ 85/500]	Time 10.660 (10.660)	Loss 7.3098 (5.7547)	CeLoss 0.2490 (0.3252)	SegCLSLoss 0.0223 (0.0213)	KLLoss 0.1543 (0.1497)	MaskLoss 0.8845 (0.7704)	MaskBCELoss 0.0301 (0.1490)	MaskDICELoss 0.8544 (0.6214)
Epoch: [3][ 86/500]	Time  9.247 ( 9.247)	Loss 0.1260 (3.7121)	CeLoss 0.1260 (0.5777)	SegCLSLoss 0.0000 (0.0102)	KLLoss 0.0000 (0.0689)	MaskLoss 0.0000 (0.4671)	MaskBCELoss 0.0000 (0.1127)	MaskDICELoss 0.0000 (0.3544)
Epoch: [3][ 87/500]	Time 10.259 (10.259)	Loss 8.6009 (6.1677)	CeLoss 0.2598 (0.2109)	SegCLSLoss 0.0283 (0.0235)	KLLoss 0.1748 (0.1584)	MaskLoss 2.0062 (0.9218)	MaskBCELoss 1.3163 (0.2646)	MaskDICELoss 0.6899 (0.6572)
Epoch: [3][ 88/500]	Time  8.405 ( 8.405)	Loss 7.4570 (4.5096)	CeLoss 0.2148 (0.6940)	SegCLSLoss 0.0284 (0.0135)	KLLoss 0.1934 (0.1065)	MaskLoss 0.9103 (0.5217)	MaskBCELoss 0.0412 (0.0786)	MaskDICELoss 0.8691 (0.4432)
Epoch: [3][ 89/500]	Time 10.881 (10.881)	Loss 8.3991 (5.2328)	CeLoss 0.1934 (0.3060)	SegCLSLoss 0.0518 (0.0229)	KLLoss 0.2393 (0.1426)	MaskLoss 1.0120 (0.7085)	MaskBCELoss 0.0256 (0.1492)	MaskDICELoss 0.9863 (0.5593)
[2025-03-04 06:30:35,973] [INFO] [logging.py:96:log_dist] [Rank 0] step=1590, skipped=0, lr=[0.00026296385542168673], mom=[(0.9, 0.95)]
[2025-03-04 06:30:35,979] [INFO] [timer.py:215:stop] epoch=0/micro_step=15900/global_step=1590, RunningAvgSamplesPerSec=1.0426727401114837, CurrSamplesPerSec=1.415036967438628, MemAllocated=56.73GB, MaxMemAllocated=62.86GB
Epoch: [3][ 90/500]	Time  7.069 ( 7.069)	Loss 1.2109 (3.8310)	CeLoss 1.2109 (0.6901)	SegCLSLoss 0.0000 (0.0172)	KLLoss 0.0000 (0.1124)	MaskLoss 0.0000 (0.4522)	MaskBCELoss 0.0000 (0.0996)	MaskDICELoss 0.0000 (0.3526)
Epoch: [3][ 91/500]	Time  9.001 ( 9.001)	Loss 6.2347 (5.8573)	CeLoss 0.2334 (0.4301)	SegCLSLoss 0.0223 (0.0262)	KLLoss 0.1826 (0.1477)	MaskLoss 0.8150 (0.7746)	MaskBCELoss 0.1185 (0.1550)	MaskDICELoss 0.6965 (0.6196)
Epoch: [3][ 92/500]	Time  9.184 ( 9.184)	Loss 3.3531 (5.0984)	CeLoss 0.2256 (0.5276)	SegCLSLoss 0.0187 (0.0203)	KLLoss 0.1562 (0.1451)	MaskLoss 0.4258 (0.6369)	MaskBCELoss 0.0740 (0.1133)	MaskDICELoss 0.3518 (0.5237)
Epoch: [3][ 93/500]	Time  9.558 ( 9.558)	Loss 9.0281 (5.2875)	CeLoss 0.1875 (0.5864)	SegCLSLoss 0.0231 (0.0168)	KLLoss 0.1621 (0.1174)	MaskLoss 1.4595 (0.6654)	MaskBCELoss 0.5015 (0.1247)	MaskDICELoss 0.9580 (0.5407)
Epoch: [3][ 94/500]	Time  8.926 ( 8.926)	Loss 7.3982 (4.5373)	CeLoss 0.2061 (0.4568)	SegCLSLoss 0.0312 (0.0144)	KLLoss 0.2246 (0.1090)	MaskLoss 1.1491 (0.5650)	MaskBCELoss 0.3733 (0.0927)	MaskDICELoss 0.7758 (0.4723)
Epoch: [3][ 95/500]	Time  8.827 ( 8.827)	Loss 7.1414 (5.7020)	CeLoss 0.2266 (0.3210)	SegCLSLoss 0.0231 (0.0238)	KLLoss 0.1484 (0.1445)	MaskLoss 1.1646 (0.8233)	MaskBCELoss 0.4271 (0.2270)	MaskDICELoss 0.7376 (0.5963)
Epoch: [3][ 96/500]	Time  8.487 ( 8.487)	Loss 1.3984 (5.9792)	CeLoss 1.3984 (0.5385)	SegCLSLoss 0.0000 (0.0236)	KLLoss 0.0000 (0.1630)	MaskLoss 0.0000 (0.8514)	MaskBCELoss 0.0000 (0.2575)	MaskDICELoss 0.0000 (0.5939)
Epoch: [3][ 97/500]	Time  9.629 ( 9.629)	Loss 7.0718 (4.8298)	CeLoss 0.2520 (0.2888)	SegCLSLoss 0.0173 (0.0170)	KLLoss 0.1523 (0.1220)	MaskLoss 1.1276 (0.6797)	MaskBCELoss 0.3935 (0.1712)	MaskDICELoss 0.7341 (0.5086)
Epoch: [3][ 98/500]	Time  7.268 ( 7.268)	Loss 1.5078 (3.5850)	CeLoss 1.5078 (0.9086)	SegCLSLoss 0.0000 (0.0149)	KLLoss 0.0000 (0.0771)	MaskLoss 0.0000 (0.3781)	MaskBCELoss 0.0000 (0.0723)	MaskDICELoss 0.0000 (0.3059)
Epoch: [3][ 99/500]	Time  8.967 ( 8.967)	Loss 5.6076 (5.4237)	CeLoss 0.2031 (0.5147)	SegCLSLoss 0.0435 (0.0253)	KLLoss 0.3008 (0.1540)	MaskLoss 0.9398 (0.7553)	MaskBCELoss 0.4057 (0.2166)	MaskDICELoss 0.5341 (0.5387)
[2025-03-04 06:32:05,319] [INFO] [logging.py:96:log_dist] [Rank 0] step=1600, skipped=0, lr=[0.00026272289156626505], mom=[(0.9, 0.95)]
[2025-03-04 06:32:05,325] [INFO] [timer.py:215:stop] epoch=0/micro_step=16000/global_step=1600, RunningAvgSamplesPerSec=1.0431203720622884, CurrSamplesPerSec=1.0529782652067197, MemAllocated=57.34GB, MaxMemAllocated=62.86GB
Epoch: [3][100/500]	Time  9.499 ( 9.499)	Loss 6.5756 (4.2539)	CeLoss 0.2119 (0.3348)	SegCLSLoss 0.0289 (0.0211)	KLLoss 0.1836 (0.1245)	MaskLoss 0.8305 (0.5650)	MaskBCELoss 0.0798 (0.1226)	MaskDICELoss 0.7507 (0.4423)
Epoch: [3][101/500]	Time  8.834 ( 8.834)	Loss 6.8543 (4.9584)	CeLoss 0.1709 (0.3346)	SegCLSLoss 0.0293 (0.0216)	KLLoss 0.2061 (0.1495)	MaskLoss 1.1733 (0.8654)	MaskBCELoss 0.4872 (0.4100)	MaskDICELoss 0.6862 (0.4555)
Epoch: [3][102/500]	Time  9.378 ( 9.378)	Loss 1.5990 (6.4851)	CeLoss 0.3086 (0.3547)	SegCLSLoss 0.0154 (0.0260)	KLLoss 0.1035 (0.1688)	MaskLoss 0.2837 (0.9174)	MaskBCELoss 0.1818 (0.2319)	MaskDICELoss 0.1019 (0.6856)
Epoch: [3][103/500]	Time  7.586 ( 7.586)	Loss 0.5938 (4.1915)	CeLoss 0.5938 (0.6736)	SegCLSLoss 0.0000 (0.0129)	KLLoss 0.0000 (0.1009)	MaskLoss 0.0000 (0.4991)	MaskBCELoss 0.0000 (0.0970)	MaskDICELoss 0.0000 (0.4020)
Epoch: [3][104/500]	Time  8.522 ( 8.522)	Loss 8.1084 (3.6397)	CeLoss 0.2578 (0.7587)	SegCLSLoss 0.0187 (0.0115)	KLLoss 0.1523 (0.0877)	MaskLoss 0.9958 (0.3960)	MaskBCELoss 0.0463 (0.0634)	MaskDICELoss 0.9495 (0.3326)
Epoch: [3][105/500]	Time  9.632 ( 9.632)	Loss 7.8256 (4.8238)	CeLoss 0.1719 (0.3250)	SegCLSLoss 0.0305 (0.0195)	KLLoss 0.2129 (0.1407)	MaskLoss 1.1940 (0.6422)	MaskBCELoss 0.3545 (0.1315)	MaskDICELoss 0.8395 (0.5107)
Epoch: [3][106/500]	Time  9.654 ( 9.654)	Loss 6.7397 (6.0955)	CeLoss 0.2559 (0.3548)	SegCLSLoss 0.0264 (0.0199)	KLLoss 0.2139 (0.1553)	MaskLoss 1.2520 (0.9067)	MaskBCELoss 0.6268 (0.2798)	MaskDICELoss 0.6252 (0.6270)
Epoch: [3][107/500]	Time  9.311 ( 9.311)	Loss 6.3069 (4.5407)	CeLoss 0.2520 (0.2484)	SegCLSLoss 0.0388 (0.0254)	KLLoss 0.1816 (0.1379)	MaskLoss 1.3927 (0.6617)	MaskBCELoss 0.8812 (0.1920)	MaskDICELoss 0.5114 (0.4697)
Epoch: [3][108/500]	Time  9.766 ( 9.766)	Loss 7.1553 (5.6663)	CeLoss 0.2656 (0.3991)	SegCLSLoss 0.0151 (0.0199)	KLLoss 0.0874 (0.1376)	MaskLoss 1.1270 (0.7667)	MaskBCELoss 0.3703 (0.1690)	MaskDICELoss 0.7567 (0.5977)
Epoch: [3][109/500]	Time  8.293 ( 8.293)	Loss 1.5547 (3.3113)	CeLoss 1.5547 (0.7467)	SegCLSLoss 0.0000 (0.0105)	KLLoss 0.0000 (0.0726)	MaskLoss 0.0000 (0.3949)	MaskBCELoss 0.0000 (0.1121)	MaskDICELoss 0.0000 (0.2828)
[2025-03-04 06:33:35,649] [INFO] [logging.py:96:log_dist] [Rank 0] step=1610, skipped=0, lr=[0.0002624819277108434], mom=[(0.9, 0.95)]
[2025-03-04 06:33:35,655] [INFO] [timer.py:215:stop] epoch=0/micro_step=16100/global_step=1610, RunningAvgSamplesPerSec=1.0434961731519297, CurrSamplesPerSec=1.0692457772377948, MemAllocated=57.26GB, MaxMemAllocated=62.86GB
Epoch: [3][110/500]	Time  9.354 ( 9.354)	Loss 8.4233 (6.2230)	CeLoss 0.1895 (0.3602)	SegCLSLoss 0.0281 (0.0215)	KLLoss 0.1855 (0.1508)	MaskLoss 1.0757 (0.8774)	MaskBCELoss 0.0952 (0.2196)	MaskDICELoss 0.9805 (0.6577)
Epoch: [3][111/500]	Time 10.043 (10.043)	Loss 3.9877 (4.5914)	CeLoss 0.2949 (0.4508)	SegCLSLoss 0.0135 (0.0156)	KLLoss 0.0952 (0.1079)	MaskLoss 0.7581 (0.6202)	MaskBCELoss 0.4123 (0.1561)	MaskDICELoss 0.3458 (0.4641)
Epoch: [3][112/500]	Time  9.320 ( 9.320)	Loss 5.9129 (5.8296)	CeLoss 0.2324 (0.4681)	SegCLSLoss 0.0172 (0.0300)	KLLoss 0.1357 (0.1483)	MaskLoss 0.8066 (0.8523)	MaskBCELoss 0.1528 (0.2701)	MaskDICELoss 0.6538 (0.5822)
Epoch: [3][113/500]	Time  9.545 ( 9.545)	Loss 6.3904 (5.4274)	CeLoss 0.2676 (0.3560)	SegCLSLoss 0.0173 (0.0198)	KLLoss 0.1367 (0.1435)	MaskLoss 1.0565 (0.7680)	MaskBCELoss 0.4122 (0.2044)	MaskDICELoss 0.6442 (0.5637)
Epoch: [3][114/500]	Time  8.915 ( 8.915)	Loss 7.1147 (5.3444)	CeLoss 0.3125 (0.5333)	SegCLSLoss 0.0156 (0.0195)	KLLoss 0.1025 (0.1444)	MaskLoss 1.5633 (0.7401)	MaskBCELoss 0.9689 (0.2106)	MaskDICELoss 0.5944 (0.5295)
Epoch: [3][115/500]	Time 10.346 (10.346)	Loss 5.2949 (5.2624)	CeLoss 0.2500 (0.2906)	SegCLSLoss 0.0184 (0.0258)	KLLoss 0.1426 (0.1432)	MaskLoss 0.7051 (0.7957)	MaskBCELoss 0.1247 (0.2583)	MaskDICELoss 0.5804 (0.5375)
Epoch: [3][116/500]	Time  9.262 ( 9.262)	Loss 6.5022 (6.2654)	CeLoss 0.3164 (0.3499)	SegCLSLoss 0.0276 (0.0227)	KLLoss 0.1729 (0.1544)	MaskLoss 0.9413 (0.9153)	MaskBCELoss 0.2554 (0.2621)	MaskDICELoss 0.6859 (0.6532)
Epoch: [3][117/500]	Time  9.170 ( 9.170)	Loss 6.2161 (5.5683)	CeLoss 0.3164 (0.4705)	SegCLSLoss 0.0254 (0.0178)	KLLoss 0.1562 (0.1191)	MaskLoss 0.8379 (0.7690)	MaskBCELoss 0.1619 (0.1970)	MaskDICELoss 0.6760 (0.5720)
Epoch: [3][118/500]	Time  9.187 ( 9.187)	Loss 4.2285 (4.4479)	CeLoss 0.2090 (0.2772)	SegCLSLoss 0.0157 (0.0206)	KLLoss 0.1157 (0.1338)	MaskLoss 0.7489 (0.6075)	MaskBCELoss 0.3491 (0.1389)	MaskDICELoss 0.3998 (0.4686)
Epoch: [3][119/500]	Time  8.373 ( 8.373)	Loss 1.4062 (5.6593)	CeLoss 1.4062 (0.4493)	SegCLSLoss 0.0000 (0.0221)	KLLoss 0.0000 (0.1520)	MaskLoss 0.0000 (0.7859)	MaskBCELoss 0.0000 (0.2066)	MaskDICELoss 0.0000 (0.5792)
[2025-03-04 06:35:09,134] [INFO] [logging.py:96:log_dist] [Rank 0] step=1620, skipped=0, lr=[0.00026224096385542165], mom=[(0.9, 0.95)]
[2025-03-04 06:35:09,140] [INFO] [timer.py:215:stop] epoch=0/micro_step=16200/global_step=1620, RunningAvgSamplesPerSec=1.0436552408945736, CurrSamplesPerSec=1.072875393864324, MemAllocated=57.25GB, MaxMemAllocated=62.86GB
Epoch: [3][120/500]	Time  9.323 ( 9.323)	Loss 6.1101 (3.0174)	CeLoss 0.2197 (0.6019)	SegCLSLoss 0.0302 (0.0104)	KLLoss 0.2061 (0.0714)	MaskLoss 0.9438 (0.3915)	MaskBCELoss 0.3137 (0.1322)	MaskDICELoss 0.6302 (0.2593)
Epoch: [3][121/500]	Time  9.703 ( 9.703)	Loss 7.7745 (6.1584)	CeLoss 0.2852 (0.2256)	SegCLSLoss 0.0247 (0.0254)	KLLoss 0.1855 (0.1747)	MaskLoss 1.0159 (0.9000)	MaskBCELoss 0.1392 (0.2424)	MaskDICELoss 0.8767 (0.6576)
Epoch: [3][122/500]	Time  8.987 ( 8.987)	Loss 3.1575 (5.1181)	CeLoss 0.3535 (0.5075)	SegCLSLoss 0.0165 (0.0168)	KLLoss 0.1069 (0.1099)	MaskLoss 0.5370 (0.8501)	MaskBCELoss 0.2679 (0.3847)	MaskDICELoss 0.2691 (0.4654)
Epoch: [3][123/500]	Time  9.075 ( 9.075)	Loss 4.7700 (3.6682)	CeLoss 0.2910 (0.5111)	SegCLSLoss 0.0157 (0.0187)	KLLoss 0.1172 (0.1062)	MaskLoss 0.7692 (0.5334)	MaskBCELoss 0.2999 (0.2043)	MaskDICELoss 0.4693 (0.3291)
Epoch: [3][124/500]	Time  7.668 ( 7.668)	Loss 1.9098 (3.7111)	CeLoss 0.1963 (0.6133)	SegCLSLoss 0.0259 (0.0172)	KLLoss 0.1826 (0.1046)	MaskLoss 0.3005 (0.4834)	MaskBCELoss 0.1475 (0.1471)	MaskDICELoss 0.1530 (0.3363)
Epoch: [3][125/500]	Time  9.732 ( 9.732)	Loss 1.5000 (4.2463)	CeLoss 1.5000 (0.6527)	SegCLSLoss 0.0000 (0.0174)	KLLoss 0.0000 (0.1099)	MaskLoss 0.0000 (0.5448)	MaskBCELoss 0.0000 (0.1473)	MaskDICELoss 0.0000 (0.3975)
Epoch: [3][126/500]	Time 10.754 (10.754)	Loss 8.4654 (6.2548)	CeLoss 0.1602 (0.2061)	SegCLSLoss 0.0464 (0.0319)	KLLoss 0.2520 (0.1857)	MaskLoss 1.4396 (0.9742)	MaskBCELoss 0.5811 (0.3244)	MaskDICELoss 0.8584 (0.6498)
Epoch: [3][127/500]	Time  9.771 ( 9.771)	Loss 8.0040 (5.9823)	CeLoss 0.2471 (0.2599)	SegCLSLoss 0.0315 (0.0183)	KLLoss 0.2285 (0.1447)	MaskLoss 1.2147 (0.8867)	MaskBCELoss 0.3676 (0.2542)	MaskDICELoss 0.8471 (0.6325)
Epoch: [3][128/500]	Time  7.570 ( 7.570)	Loss 0.0728 (3.0436)	CeLoss 0.0728 (0.6776)	SegCLSLoss 0.0000 (0.0094)	KLLoss 0.0000 (0.0756)	MaskLoss 0.0000 (0.3791)	MaskBCELoss 0.0000 (0.1245)	MaskDICELoss 0.0000 (0.2546)
Epoch: [3][129/500]	Time  8.698 ( 8.698)	Loss 1.3591 (5.2497)	CeLoss 0.1562 (0.2696)	SegCLSLoss 0.0496 (0.0268)	KLLoss 0.2715 (0.1608)	MaskLoss 0.1204 (0.8619)	MaskBCELoss 0.0095 (0.3483)	MaskDICELoss 0.1109 (0.5137)
[2025-03-04 06:36:38,620] [INFO] [logging.py:96:log_dist] [Rank 0] step=1630, skipped=0, lr=[0.00026199999999999997], mom=[(0.9, 0.95)]
[2025-03-04 06:36:38,626] [INFO] [timer.py:215:stop] epoch=0/micro_step=16300/global_step=1630, RunningAvgSamplesPerSec=1.0440800246368802, CurrSamplesPerSec=1.3283998815865163, MemAllocated=56.66GB, MaxMemAllocated=62.86GB
Epoch: [3][130/500]	Time  7.530 ( 7.530)	Loss 4.7173 (3.8036)	CeLoss 0.2178 (0.4973)	SegCLSLoss 0.0162 (0.0160)	KLLoss 0.1963 (0.1024)	MaskLoss 0.5464 (0.4951)	MaskBCELoss 0.0126 (0.1275)	MaskDICELoss 0.5338 (0.3676)
Epoch: [3][131/500]	Time  9.347 ( 9.347)	Loss 1.3594 (4.5491)	CeLoss 1.3594 (0.4338)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.1105)	MaskLoss 0.0000 (0.6140)	MaskBCELoss 0.0000 (0.1524)	MaskDICELoss 0.0000 (0.4616)
Epoch: [3][132/500]	Time  8.469 ( 8.469)	Loss 6.0657 (5.7658)	CeLoss 0.3008 (0.5051)	SegCLSLoss 0.0175 (0.0219)	KLLoss 0.1196 (0.1276)	MaskLoss 0.8910 (0.8788)	MaskBCELoss 0.2483 (0.3180)	MaskDICELoss 0.6427 (0.5608)
Epoch: [3][133/500]	Time 10.061 (10.061)	Loss 4.9749 (4.7491)	CeLoss 0.2520 (0.4727)	SegCLSLoss 0.0149 (0.0160)	KLLoss 0.1226 (0.1335)	MaskLoss 0.6725 (0.6098)	MaskBCELoss 0.1313 (0.1239)	MaskDICELoss 0.5412 (0.4859)
Epoch: [3][134/500]	Time 12.409 (12.409)	Loss 7.7542 (5.2709)	CeLoss 0.2754 (0.2348)	SegCLSLoss 0.0159 (0.0215)	KLLoss 0.1582 (0.1513)	MaskLoss 1.2368 (0.7584)	MaskBCELoss 0.4303 (0.1988)	MaskDICELoss 0.8065 (0.5595)
Epoch: [3][135/500]	Time 10.418 (10.418)	Loss 7.1757 (5.8207)	CeLoss 0.2275 (0.3558)	SegCLSLoss 0.0243 (0.0205)	KLLoss 0.1514 (0.1460)	MaskLoss 1.1892 (0.8370)	MaskBCELoss 0.4547 (0.2312)	MaskDICELoss 0.7345 (0.6058)
Epoch: [3][136/500]	Time  8.326 ( 8.326)	Loss 0.8750 (3.7039)	CeLoss 0.8750 (0.4555)	SegCLSLoss 0.0000 (0.0150)	KLLoss 0.0000 (0.1161)	MaskLoss 0.0000 (0.5237)	MaskBCELoss 0.0000 (0.1775)	MaskDICELoss 0.0000 (0.3462)
Epoch: [3][137/500]	Time 10.903 (10.903)	Loss 6.2225 (5.2239)	CeLoss 0.1807 (0.2077)	SegCLSLoss 0.0320 (0.0261)	KLLoss 0.1924 (0.1633)	MaskLoss 0.7709 (0.7285)	MaskBCELoss 0.0555 (0.1647)	MaskDICELoss 0.7154 (0.5638)
Epoch: [3][138/500]	Time  8.774 ( 8.774)	Loss 1.2031 (3.6035)	CeLoss 1.2031 (0.5725)	SegCLSLoss 0.0000 (0.0127)	KLLoss 0.0000 (0.1135)	MaskLoss 0.0000 (0.5520)	MaskBCELoss 0.0000 (0.2507)	MaskDICELoss 0.0000 (0.3013)
Epoch: [3][139/500]	Time 10.255 (10.255)	Loss 7.2691 (4.7052)	CeLoss 0.1426 (0.3861)	SegCLSLoss 0.0579 (0.0203)	KLLoss 0.2539 (0.1249)	MaskLoss 1.1598 (0.6744)	MaskBCELoss 0.4058 (0.2019)	MaskDICELoss 0.7540 (0.4725)
[2025-03-04 06:38:17,279] [INFO] [logging.py:96:log_dist] [Rank 0] step=1640, skipped=0, lr=[0.0002617590361445783], mom=[(0.9, 0.95)]
[2025-03-04 06:38:17,285] [INFO] [timer.py:215:stop] epoch=0/micro_step=16400/global_step=1640, RunningAvgSamplesPerSec=1.0438894461115478, CurrSamplesPerSec=1.031486156843342, MemAllocated=57.73GB, MaxMemAllocated=62.86GB
Epoch: [3][140/500]	Time  9.697 ( 9.697)	Loss 0.3262 (4.0797)	CeLoss 0.3262 (0.5559)	SegCLSLoss 0.0000 (0.0111)	KLLoss 0.0000 (0.0896)	MaskLoss 0.0000 (0.5116)	MaskBCELoss 0.0000 (0.1108)	MaskDICELoss 0.0000 (0.4009)
Epoch: [3][141/500]	Time 10.536 (10.536)	Loss 6.6080 (5.5500)	CeLoss 0.2100 (0.2269)	SegCLSLoss 0.0278 (0.0193)	KLLoss 0.2090 (0.1433)	MaskLoss 0.8138 (0.8105)	MaskBCELoss 0.0560 (0.2190)	MaskDICELoss 0.7578 (0.5915)
Epoch: [3][142/500]	Time  8.979 ( 8.979)	Loss 1.5234 (5.4385)	CeLoss 1.5234 (0.4161)	SegCLSLoss 0.0000 (0.0216)	KLLoss 0.0000 (0.1477)	MaskLoss 0.0000 (0.7622)	MaskBCELoss 0.0000 (0.2057)	MaskDICELoss 0.0000 (0.5565)
Epoch: [3][143/500]	Time  7.960 ( 7.960)	Loss 7.3248 (5.7090)	CeLoss 0.1904 (0.5492)	SegCLSLoss 0.0425 (0.0172)	KLLoss 0.2275 (0.1197)	MaskLoss 1.2779 (0.8689)	MaskBCELoss 0.5563 (0.3200)	MaskDICELoss 0.7216 (0.5490)
Epoch: [3][144/500]	Time  9.929 ( 9.929)	Loss 8.0320 (5.7558)	CeLoss 0.2031 (0.3675)	SegCLSLoss 0.0283 (0.0193)	KLLoss 0.1934 (0.1442)	MaskLoss 1.0158 (0.8845)	MaskBCELoss 0.0841 (0.3070)	MaskDICELoss 0.9317 (0.5775)
Epoch: [3][145/500]	Time  9.832 ( 9.832)	Loss 8.1984 (5.5001)	CeLoss 0.2988 (0.3914)	SegCLSLoss 0.0243 (0.0185)	KLLoss 0.1846 (0.1272)	MaskLoss 0.9910 (0.7609)	MaskBCELoss 0.0377 (0.1858)	MaskDICELoss 0.9534 (0.5751)
Epoch: [3][146/500]	Time  9.264 ( 9.264)	Loss 0.0859 (4.9934)	CeLoss 0.0859 (0.5990)	SegCLSLoss 0.0000 (0.0209)	KLLoss 0.0000 (0.1320)	MaskLoss 0.0000 (0.6419)	MaskBCELoss 0.0000 (0.1473)	MaskDICELoss 0.0000 (0.4947)
Epoch: [3][147/500]	Time  9.030 ( 9.030)	Loss 8.0532 (4.0696)	CeLoss 0.3496 (0.5125)	SegCLSLoss 0.0233 (0.0137)	KLLoss 0.1689 (0.1097)	MaskLoss 0.9570 (0.5211)	MaskBCELoss 0.0223 (0.1214)	MaskDICELoss 0.9347 (0.3997)
Epoch: [3][148/500]	Time  7.336 ( 7.336)	Loss 5.5977 (3.8224)	CeLoss 0.2197 (0.4778)	SegCLSLoss 0.0229 (0.0124)	KLLoss 0.1777 (0.0925)	MaskLoss 0.6765 (0.4890)	MaskBCELoss 0.0374 (0.1110)	MaskDICELoss 0.6391 (0.3779)
Epoch: [3][149/500]	Time  8.844 ( 8.844)	Loss 6.8465 (4.9615)	CeLoss 0.2578 (0.6311)	SegCLSLoss 0.0171 (0.0126)	KLLoss 0.1436 (0.0971)	MaskLoss 1.0470 (0.7084)	MaskBCELoss 0.3233 (0.2400)	MaskDICELoss 0.7237 (0.4684)
[2025-03-04 06:39:49,086] [INFO] [logging.py:96:log_dist] [Rank 0] step=1650, skipped=0, lr=[0.0002615180722891566], mom=[(0.9, 0.95)]
[2025-03-04 06:39:49,092] [INFO] [timer.py:215:stop] epoch=0/micro_step=16500/global_step=1650, RunningAvgSamplesPerSec=1.0441542876833176, CurrSamplesPerSec=0.9905016267349555, MemAllocated=57.25GB, MaxMemAllocated=62.86GB
Epoch: [3][150/500]	Time 10.098 (10.098)	Loss 4.1760 (6.8548)	CeLoss 0.3008 (0.2726)	SegCLSLoss 0.0149 (0.0243)	KLLoss 0.0874 (0.1748)	MaskLoss 0.7129 (0.9624)	MaskBCELoss 0.3206 (0.2173)	MaskDICELoss 0.3923 (0.7451)
Epoch: [3][151/500]	Time  8.428 ( 8.428)	Loss 0.3809 (3.2740)	CeLoss 0.3809 (0.5629)	SegCLSLoss 0.0000 (0.0139)	KLLoss 0.0000 (0.1006)	MaskLoss 0.0000 (0.4488)	MaskBCELoss 0.0000 (0.1645)	MaskDICELoss 0.0000 (0.2843)
Epoch: [3][152/500]	Time  9.079 ( 9.079)	Loss 3.4228 (4.5397)	CeLoss 0.1934 (0.3798)	SegCLSLoss 0.0383 (0.0196)	KLLoss 0.2773 (0.1390)	MaskLoss 0.4159 (0.7076)	MaskBCELoss 0.0658 (0.2749)	MaskDICELoss 0.3501 (0.4326)
Epoch: [3][153/500]	Time 10.405 (10.405)	Loss 5.4943 (6.1492)	CeLoss 0.2852 (0.2994)	SegCLSLoss 0.0197 (0.0284)	KLLoss 0.1377 (0.1732)	MaskLoss 0.6562 (0.8436)	MaskBCELoss 0.0314 (0.1811)	MaskDICELoss 0.6247 (0.6625)
Epoch: [3][154/500]	Time  9.430 ( 9.430)	Loss 7.0750 (5.1320)	CeLoss 0.1904 (0.3422)	SegCLSLoss 0.0245 (0.0232)	KLLoss 0.1875 (0.1458)	MaskLoss 0.9521 (0.7654)	MaskBCELoss 0.1553 (0.2485)	MaskDICELoss 0.7967 (0.5169)
Epoch: [3][155/500]	Time  9.111 ( 9.111)	Loss 6.2975 (4.5395)	CeLoss 0.2539 (0.6593)	SegCLSLoss 0.0160 (0.0178)	KLLoss 0.1260 (0.1213)	MaskLoss 0.9802 (0.5791)	MaskBCELoss 0.3218 (0.1471)	MaskDICELoss 0.6584 (0.4320)
Epoch: [3][156/500]	Time 11.735 (11.735)	Loss 7.1876 (7.0529)	CeLoss 0.1787 (0.2587)	SegCLSLoss 0.0266 (0.0273)	KLLoss 0.1699 (0.1718)	MaskLoss 0.8957 (0.9408)	MaskBCELoss 0.0569 (0.1529)	MaskDICELoss 0.8388 (0.7879)
Epoch: [3][157/500]	Time  7.936 ( 7.936)	Loss 6.1937 (4.3367)	CeLoss 0.2520 (0.6661)	SegCLSLoss 0.0156 (0.0136)	KLLoss 0.0903 (0.0977)	MaskLoss 0.8751 (0.5179)	MaskBCELoss 0.1928 (0.0961)	MaskDICELoss 0.6823 (0.4218)
Epoch: [3][158/500]	Time  9.890 ( 9.890)	Loss 6.8808 (5.5867)	CeLoss 0.2012 (0.3716)	SegCLSLoss 0.0275 (0.0225)	KLLoss 0.1973 (0.1605)	MaskLoss 0.8178 (0.7661)	MaskBCELoss 0.0122 (0.1810)	MaskDICELoss 0.8055 (0.5852)
Epoch: [3][159/500]	Time 10.035 (10.035)	Loss 1.0781 (5.1175)	CeLoss 1.0781 (0.3797)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.1023)	MaskLoss 0.0000 (0.6430)	MaskBCELoss 0.0000 (0.0859)	MaskDICELoss 0.0000 (0.5571)
[2025-03-04 06:41:25,192] [INFO] [logging.py:96:log_dist] [Rank 0] step=1660, skipped=0, lr=[0.0002612771084337349], mom=[(0.9, 0.95)]
[2025-03-04 06:41:25,198] [INFO] [timer.py:215:stop] epoch=0/micro_step=16600/global_step=1660, RunningAvgSamplesPerSec=1.044133406242189, CurrSamplesPerSec=0.9943565229991222, MemAllocated=56.73GB, MaxMemAllocated=62.86GB
Epoch: [3][160/500]	Time 10.059 (10.059)	Loss 1.4453 (4.7699)	CeLoss 1.4453 (0.4413)	SegCLSLoss 0.0000 (0.0151)	KLLoss 0.0000 (0.1134)	MaskLoss 0.0000 (0.5908)	MaskBCELoss 0.0000 (0.0865)	MaskDICELoss 0.0000 (0.5043)
Epoch: [3][161/500]	Time  6.876 ( 6.876)	Loss 7.6147 (3.9759)	CeLoss 0.2217 (0.6234)	SegCLSLoss 0.0176 (0.0133)	KLLoss 0.1426 (0.0929)	MaskLoss 0.9546 (0.4750)	MaskBCELoss 0.0659 (0.0912)	MaskDICELoss 0.8887 (0.3838)
Epoch: [3][162/500]	Time  9.712 ( 9.712)	Loss 4.5300 (5.5558)	CeLoss 0.2930 (0.2857)	SegCLSLoss 0.0146 (0.0185)	KLLoss 0.1187 (0.1463)	MaskLoss 0.8035 (0.7405)	MaskBCELoss 0.3863 (0.1349)	MaskDICELoss 0.4172 (0.6056)
Epoch: [3][163/500]	Time  9.874 ( 9.874)	Loss 6.3426 (4.7259)	CeLoss 0.2197 (0.3859)	SegCLSLoss 0.0234 (0.0243)	KLLoss 0.1475 (0.1601)	MaskLoss 0.8920 (0.6946)	MaskBCELoss 0.1954 (0.2316)	MaskDICELoss 0.6966 (0.4630)
Epoch: [3][164/500]	Time 10.191 (10.191)	Loss 3.0872 (4.3239)	CeLoss 0.2490 (0.1695)	SegCLSLoss 0.0182 (0.0222)	KLLoss 0.1348 (0.1351)	MaskLoss 0.5078 (0.6751)	MaskBCELoss 0.2279 (0.2322)	MaskDICELoss 0.2798 (0.4430)
Epoch: [3][165/500]	Time  9.443 ( 9.443)	Loss 8.1248 (5.2181)	CeLoss 0.1904 (0.3937)	SegCLSLoss 0.0342 (0.0169)	KLLoss 0.2480 (0.1293)	MaskLoss 0.9939 (0.6641)	MaskBCELoss 0.0469 (0.1042)	MaskDICELoss 0.9470 (0.5598)
Epoch: [3][166/500]	Time  8.507 ( 8.507)	Loss 1.2969 (4.4642)	CeLoss 1.2969 (0.6256)	SegCLSLoss 0.0000 (0.0109)	KLLoss 0.0000 (0.0912)	MaskLoss 0.0000 (0.5399)	MaskBCELoss 0.0000 (0.0962)	MaskDICELoss 0.0000 (0.4437)
Epoch: [3][167/500]	Time 11.615 (11.615)	Loss 7.6412 (6.4829)	CeLoss 0.2754 (0.2298)	SegCLSLoss 0.0194 (0.0285)	KLLoss 0.1504 (0.1679)	MaskLoss 0.9958 (0.9130)	MaskBCELoss 0.1268 (0.2056)	MaskDICELoss 0.8690 (0.7074)
Epoch: [3][168/500]	Time  9.006 ( 9.006)	Loss 8.4205 (5.1560)	CeLoss 0.1953 (0.4998)	SegCLSLoss 0.0256 (0.0157)	KLLoss 0.2422 (0.1226)	MaskLoss 0.9991 (0.6636)	MaskBCELoss 0.0036 (0.1306)	MaskDICELoss 0.9955 (0.5330)
Epoch: [3][169/500]	Time 10.918 (10.918)	Loss 5.7226 (6.7140)	CeLoss 0.2285 (0.2175)	SegCLSLoss 0.0153 (0.0310)	KLLoss 0.1108 (0.1868)	MaskLoss 0.8568 (0.9511)	MaskBCELoss 0.2466 (0.2192)	MaskDICELoss 0.6102 (0.7320)
[2025-03-04 06:43:02,653] [INFO] [logging.py:96:log_dist] [Rank 0] step=1670, skipped=0, lr=[0.0002610361445783132], mom=[(0.9, 0.95)]
[2025-03-04 06:43:02,659] [INFO] [timer.py:215:stop] epoch=0/micro_step=16700/global_step=1670, RunningAvgSamplesPerSec=1.0440241825322314, CurrSamplesPerSec=0.8836560094323969, MemAllocated=57.26GB, MaxMemAllocated=62.86GB
Epoch: [3][170/500]	Time 11.319 (11.319)	Loss 8.4289 (6.1814)	CeLoss 0.2246 (0.2104)	SegCLSLoss 0.0250 (0.0182)	KLLoss 0.1807 (0.1342)	MaskLoss 1.0937 (0.8872)	MaskBCELoss 0.1231 (0.2116)	MaskDICELoss 0.9706 (0.6755)
Epoch: [3][171/500]	Time 10.787 (10.787)	Loss 6.3655 (5.1002)	CeLoss 0.2539 (0.2725)	SegCLSLoss 0.0182 (0.0178)	KLLoss 0.1709 (0.1320)	MaskLoss 0.8278 (0.7172)	MaskBCELoss 0.1150 (0.1751)	MaskDICELoss 0.7127 (0.5421)
Epoch: [3][172/500]	Time 10.161 (10.161)	Loss 7.1296 (5.3978)	CeLoss 0.3105 (0.3954)	SegCLSLoss 0.0173 (0.0183)	KLLoss 0.1309 (0.1341)	MaskLoss 0.9339 (0.7558)	MaskBCELoss 0.1318 (0.1979)	MaskDICELoss 0.8021 (0.5579)
Epoch: [3][173/500]	Time 10.683 (10.683)	Loss 7.7794 (4.7631)	CeLoss 0.2207 (0.2962)	SegCLSLoss 0.0276 (0.0221)	KLLoss 0.1895 (0.1198)	MaskLoss 0.9590 (0.6811)	MaskBCELoss 0.0528 (0.1854)	MaskDICELoss 0.9062 (0.4957)
Epoch: [3][174/500]	Time  7.656 ( 7.656)	Loss 6.3237 (4.1918)	CeLoss 0.1924 (0.6934)	SegCLSLoss 0.0170 (0.0116)	KLLoss 0.1250 (0.0856)	MaskLoss 0.7958 (0.5775)	MaskBCELoss 0.0615 (0.2021)	MaskDICELoss 0.7343 (0.3754)
Epoch: [3][175/500]	Time 10.531 (10.531)	Loss 5.6968 (6.8476)	CeLoss 0.2344 (0.2381)	SegCLSLoss 0.0269 (0.0317)	KLLoss 0.1787 (0.1993)	MaskLoss 0.7403 (1.0102)	MaskBCELoss 0.1086 (0.2811)	MaskDICELoss 0.6317 (0.7291)
Epoch: [3][176/500]	Time  8.486 ( 8.486)	Loss 8.2942 (5.2598)	CeLoss 0.1475 (0.6447)	SegCLSLoss 0.0723 (0.0204)	KLLoss 0.2539 (0.1182)	MaskLoss 1.2581 (0.6270)	MaskBCELoss 0.3681 (0.0883)	MaskDICELoss 0.8901 (0.5388)
Epoch: [3][177/500]	Time 10.255 (10.255)	Loss 8.1643 (5.6053)	CeLoss 0.1777 (0.3328)	SegCLSLoss 0.0278 (0.0200)	KLLoss 0.2246 (0.1329)	MaskLoss 1.0271 (0.7126)	MaskBCELoss 0.0781 (0.0952)	MaskDICELoss 0.9490 (0.6174)
Epoch: [3][178/500]	Time  9.343 ( 9.343)	Loss 7.8273 (7.1142)	CeLoss 0.2363 (0.4208)	SegCLSLoss 0.0206 (0.0266)	KLLoss 0.1895 (0.1646)	MaskLoss 1.0911 (0.9910)	MaskBCELoss 0.2232 (0.2354)	MaskDICELoss 0.8679 (0.7556)
Epoch: [3][179/500]	Time  9.288 ( 9.288)	Loss 1.7734 (4.8357)	CeLoss 1.7734 (0.6448)	SegCLSLoss 0.0000 (0.0222)	KLLoss 0.0000 (0.1217)	MaskLoss 0.0000 (0.6793)	MaskBCELoss 0.0000 (0.2294)	MaskDICELoss 0.0000 (0.4499)
[2025-03-04 06:44:40,557] [INFO] [logging.py:96:log_dist] [Rank 0] step=1680, skipped=0, lr=[0.00026079518072289154], mom=[(0.9, 0.95)]
[2025-03-04 06:44:40,561] [INFO] [timer.py:215:stop] epoch=0/micro_step=16800/global_step=1680, RunningAvgSamplesPerSec=1.0438875930345883, CurrSamplesPerSec=0.9337413179927682, MemAllocated=56.82GB, MaxMemAllocated=62.86GB
Epoch: [3][180/500]	Time 10.712 (10.712)	Loss 7.5295 (5.9133)	CeLoss 0.1895 (0.2317)	SegCLSLoss 0.0605 (0.0219)	KLLoss 0.2520 (0.1311)	MaskLoss 1.1968 (0.7971)	MaskBCELoss 0.4196 (0.1395)	MaskDICELoss 0.7772 (0.6575)
Epoch: [3][181/500]	Time 10.899 (10.899)	Loss 5.8232 (4.4240)	CeLoss 0.2480 (0.3908)	SegCLSLoss 0.0275 (0.0130)	KLLoss 0.1982 (0.0933)	MaskLoss 0.9033 (0.6342)	MaskBCELoss 0.3107 (0.1901)	MaskDICELoss 0.5926 (0.4442)
Epoch: [3][182/500]	Time 10.455 (10.455)	Loss 8.0274 (4.6756)	CeLoss 0.2354 (0.3971)	SegCLSLoss 0.0250 (0.0169)	KLLoss 0.1689 (0.1053)	MaskLoss 1.0307 (0.6327)	MaskBCELoss 0.1061 (0.1495)	MaskDICELoss 0.9247 (0.4832)
Epoch: [3][183/500]	Time  8.441 ( 8.441)	Loss 5.9538 (4.9625)	CeLoss 0.2988 (0.5425)	SegCLSLoss 0.0200 (0.0183)	KLLoss 0.1592 (0.1276)	MaskLoss 0.7357 (0.6777)	MaskBCELoss 0.0667 (0.1896)	MaskDICELoss 0.6690 (0.4880)
Epoch: [3][184/500]	Time  7.892 ( 7.892)	Loss 1.5938 (4.4812)	CeLoss 1.5938 (0.6784)	SegCLSLoss 0.0000 (0.0128)	KLLoss 0.0000 (0.0944)	MaskLoss 0.0000 (0.5701)	MaskBCELoss 0.0000 (0.1431)	MaskDICELoss 0.0000 (0.4270)
Epoch: [3][185/500]	Time  9.225 ( 9.225)	Loss 7.6297 (4.9546)	CeLoss 0.2363 (0.3685)	SegCLSLoss 0.0292 (0.0198)	KLLoss 0.1914 (0.1238)	MaskLoss 1.0318 (0.7947)	MaskBCELoss 0.1777 (0.3175)	MaskDICELoss 0.8541 (0.4772)
Epoch: [3][186/500]	Time  8.777 ( 8.777)	Loss 6.7752 (4.6373)	CeLoss 0.1855 (0.5657)	SegCLSLoss 0.0376 (0.0174)	KLLoss 0.2139 (0.1061)	MaskLoss 0.8077 (0.5949)	MaskBCELoss 0.0173 (0.1338)	MaskDICELoss 0.7903 (0.4612)
Epoch: [3][187/500]	Time  9.805 ( 9.805)	Loss 1.0380 (5.8724)	CeLoss 0.2559 (0.2781)	SegCLSLoss 0.0164 (0.0199)	KLLoss 0.0830 (0.1250)	MaskLoss 0.1212 (0.8904)	MaskBCELoss 0.0466 (0.2773)	MaskDICELoss 0.0747 (0.6131)
Epoch: [3][188/500]	Time  8.174 ( 8.174)	Loss 8.8502 (5.4614)	CeLoss 0.1982 (0.4458)	SegCLSLoss 0.0359 (0.0208)	KLLoss 0.2236 (0.1208)	MaskLoss 1.3504 (0.8341)	MaskBCELoss 0.3988 (0.2981)	MaskDICELoss 0.9517 (0.5361)
Epoch: [3][189/500]	Time  9.731 ( 9.731)	Loss 6.9720 (5.1922)	CeLoss 0.1855 (0.4756)	SegCLSLoss 0.0332 (0.0194)	KLLoss 0.1768 (0.1208)	MaskLoss 0.8571 (0.6105)	MaskBCELoss 0.0440 (0.0497)	MaskDICELoss 0.8131 (0.5608)
[2025-03-04 06:46:15,007] [INFO] [logging.py:96:log_dist] [Rank 0] step=1690, skipped=0, lr=[0.00026055421686746986], mom=[(0.9, 0.95)]
[2025-03-04 06:46:15,013] [INFO] [timer.py:215:stop] epoch=0/micro_step=16900/global_step=1690, RunningAvgSamplesPerSec=1.0439754975123874, CurrSamplesPerSec=0.9049350293385685, MemAllocated=57.27GB, MaxMemAllocated=62.86GB
Epoch: [3][190/500]	Time 11.053 (11.053)	Loss 7.3948 (6.5671)	CeLoss 0.2471 (0.2990)	SegCLSLoss 0.0334 (0.0239)	KLLoss 0.2061 (0.1469)	MaskLoss 0.8765 (0.9688)	MaskBCELoss 0.0144 (0.2735)	MaskDICELoss 0.8622 (0.6953)
Epoch: [3][191/500]	Time  8.625 ( 8.625)	Loss 7.0333 (5.5362)	CeLoss 0.2207 (0.5066)	SegCLSLoss 0.0219 (0.0212)	KLLoss 0.1777 (0.1172)	MaskLoss 0.9857 (0.8181)	MaskBCELoss 0.2104 (0.2739)	MaskDICELoss 0.7753 (0.5442)
Epoch: [3][192/500]	Time  9.175 ( 9.175)	Loss 6.1457 (5.3601)	CeLoss 0.2275 (0.3595)	SegCLSLoss 0.0232 (0.0189)	KLLoss 0.1543 (0.1302)	MaskLoss 0.7436 (0.8190)	MaskBCELoss 0.0330 (0.2819)	MaskDICELoss 0.7107 (0.5371)
Epoch: [3][193/500]	Time  7.852 ( 7.852)	Loss 7.4980 (4.8205)	CeLoss 0.2734 (0.5261)	SegCLSLoss 0.0182 (0.0154)	KLLoss 0.0913 (0.0842)	MaskLoss 1.0077 (0.5794)	MaskBCELoss 0.1564 (0.0721)	MaskDICELoss 0.8513 (0.5073)
Epoch: [3][194/500]	Time 10.108 (10.108)	Loss 3.5950 (6.1356)	CeLoss 0.2080 (0.3352)	SegCLSLoss 0.0312 (0.0187)	KLLoss 0.1797 (0.1095)	MaskLoss 0.5590 (1.3676)	MaskBCELoss 0.2132 (0.8765)	MaskDICELoss 0.3458 (0.4911)
Epoch: [3][195/500]	Time  8.507 ( 8.507)	Loss 6.9902 (5.2479)	CeLoss 0.2207 (0.4623)	SegCLSLoss 0.0201 (0.0187)	KLLoss 0.2061 (0.1084)	MaskLoss 1.9426 (0.8290)	MaskBCELoss 1.4980 (0.3274)	MaskDICELoss 0.4446 (0.5016)
Epoch: [3][196/500]	Time  9.655 ( 9.655)	Loss 5.0945 (4.1373)	CeLoss 0.2158 (0.5072)	SegCLSLoss 0.0280 (0.0187)	KLLoss 0.1855 (0.1243)	MaskLoss 0.6825 (0.5471)	MaskBCELoss 0.1300 (0.1468)	MaskDICELoss 0.5526 (0.4004)
Epoch: [3][197/500]	Time  9.771 ( 9.771)	Loss 8.5389 (4.8964)	CeLoss 0.2539 (0.3675)	SegCLSLoss 0.0299 (0.0199)	KLLoss 0.1914 (0.1272)	MaskLoss 1.0536 (0.6733)	MaskBCELoss 0.0585 (0.1658)	MaskDICELoss 0.9951 (0.5075)
Epoch: [3][198/500]	Time  9.031 ( 9.031)	Loss 0.1055 (3.8134)	CeLoss 0.1055 (0.5821)	SegCLSLoss 0.0000 (0.0121)	KLLoss 0.0000 (0.0775)	MaskLoss 0.0000 (0.5070)	MaskBCELoss 0.0000 (0.1514)	MaskDICELoss 0.0000 (0.3556)
Epoch: [3][199/500]	Time  9.339 ( 9.339)	Loss 6.0211 (4.8362)	CeLoss 0.2578 (0.5375)	SegCLSLoss 0.0248 (0.0163)	KLLoss 0.1592 (0.1084)	MaskLoss 0.9756 (0.5960)	MaskBCELoss 0.3689 (0.0976)	MaskDICELoss 0.6067 (0.4984)
[2025-03-04 06:47:46,309] [INFO] [logging.py:96:log_dist] [Rank 0] step=1700, skipped=0, lr=[0.0002603132530120482], mom=[(0.9, 0.95)]
[2025-03-04 06:47:46,315] [INFO] [timer.py:215:stop] epoch=0/micro_step=17000/global_step=1700, RunningAvgSamplesPerSec=1.0442646162185247, CurrSamplesPerSec=1.0828563628794001, MemAllocated=57.1GB, MaxMemAllocated=62.86GB
Epoch: [3][200/500]	Time  9.238 ( 9.238)	Loss 3.4512 (5.8737)	CeLoss 0.3125 (0.3912)	SegCLSLoss 0.0204 (0.0254)	KLLoss 0.1631 (0.1479)	MaskLoss 0.5375 (0.7947)	MaskBCELoss 0.2222 (0.1726)	MaskDICELoss 0.3153 (0.6221)
Epoch: [3][201/500]	Time 10.002 (10.002)	Loss 4.9294 (5.9443)	CeLoss 0.2812 (0.4875)	SegCLSLoss 0.0146 (0.0191)	KLLoss 0.0923 (0.1357)	MaskLoss 0.7294 (0.8037)	MaskBCELoss 0.2144 (0.1864)	MaskDICELoss 0.5150 (0.6173)
Epoch: [3][202/500]	Time  8.931 ( 8.931)	Loss 5.4261 (5.6079)	CeLoss 0.2598 (0.3410)	SegCLSLoss 0.0183 (0.0199)	KLLoss 0.1797 (0.1361)	MaskLoss 0.8933 (0.7910)	MaskBCELoss 0.3616 (0.2013)	MaskDICELoss 0.5317 (0.5898)
Epoch: [3][203/500]	Time  8.010 ( 8.010)	Loss 1.1484 (5.3853)	CeLoss 1.1484 (0.4158)	SegCLSLoss 0.0000 (0.0210)	KLLoss 0.0000 (0.1218)	MaskLoss 0.0000 (0.7307)	MaskBCELoss 0.0000 (0.1680)	MaskDICELoss 0.0000 (0.5626)
Epoch: [3][204/500]	Time  8.779 ( 8.779)	Loss 1.5469 (3.6685)	CeLoss 1.5469 (0.4645)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.0841)	MaskLoss 0.0000 (0.5092)	MaskBCELoss 0.0000 (0.1601)	MaskDICELoss 0.0000 (0.3491)
Epoch: [3][205/500]	Time 10.161 (10.161)	Loss 1.9766 (5.4887)	CeLoss 1.9766 (0.4017)	SegCLSLoss 0.0000 (0.0212)	KLLoss 0.0000 (0.1388)	MaskLoss 0.0000 (0.7343)	MaskBCELoss 0.0000 (0.1560)	MaskDICELoss 0.0000 (0.5782)
Epoch: [3][206/500]	Time  8.795 ( 8.795)	Loss 6.3592 (4.4020)	CeLoss 0.2100 (0.4182)	SegCLSLoss 0.0157 (0.0176)	KLLoss 0.1108 (0.1296)	MaskLoss 0.9838 (0.5829)	MaskBCELoss 0.3066 (0.1363)	MaskDICELoss 0.6772 (0.4466)
Epoch: [3][207/500]	Time  7.659 ( 7.659)	Loss 8.3947 (5.0817)	CeLoss 0.2188 (0.5303)	SegCLSLoss 0.0162 (0.0196)	KLLoss 0.1289 (0.1122)	MaskLoss 1.3104 (0.7157)	MaskBCELoss 0.4073 (0.2159)	MaskDICELoss 0.9031 (0.4998)
Epoch: [3][208/500]	Time  9.746 ( 9.746)	Loss 1.7578 (5.5196)	CeLoss 1.7578 (0.5481)	SegCLSLoss 0.0000 (0.0171)	KLLoss 0.0000 (0.1231)	MaskLoss 0.0000 (0.7519)	MaskBCELoss 0.0000 (0.1961)	MaskDICELoss 0.0000 (0.5559)
Epoch: [3][209/500]	Time  8.959 ( 8.959)	Loss 7.8499 (4.8771)	CeLoss 0.1914 (0.4619)	SegCLSLoss 0.0388 (0.0229)	KLLoss 0.1787 (0.1315)	MaskLoss 1.2451 (0.6538)	MaskBCELoss 0.4170 (0.1597)	MaskDICELoss 0.8282 (0.4941)
[2025-03-04 06:49:16,132] [INFO] [logging.py:96:log_dist] [Rank 0] step=1710, skipped=0, lr=[0.00026007228915662646], mom=[(0.9, 0.95)]
[2025-03-04 06:49:16,138] [INFO] [timer.py:215:stop] epoch=0/micro_step=17100/global_step=1710, RunningAvgSamplesPerSec=1.044644863534937, CurrSamplesPerSec=1.138982230476635, MemAllocated=56.96GB, MaxMemAllocated=62.86GB
Epoch: [3][210/500]	Time  8.782 ( 8.782)	Loss 6.1849 (4.2964)	CeLoss 0.1582 (0.6029)	SegCLSLoss 0.0403 (0.0159)	KLLoss 0.2812 (0.1081)	MaskLoss 0.7420 (0.5277)	MaskBCELoss 0.0353 (0.1074)	MaskDICELoss 0.7067 (0.4203)
Epoch: [3][211/500]	Time  8.152 ( 8.152)	Loss 1.5781 (3.3306)	CeLoss 1.5781 (0.4854)	SegCLSLoss 0.0000 (0.0124)	KLLoss 0.0000 (0.0877)	MaskLoss 0.0000 (0.4362)	MaskBCELoss 0.0000 (0.1231)	MaskDICELoss 0.0000 (0.3131)
Epoch: [3][212/500]	Time  9.010 ( 9.010)	Loss 0.2275 (3.2773)	CeLoss 0.2275 (0.4554)	SegCLSLoss 0.0000 (0.0128)	KLLoss 0.0000 (0.0829)	MaskLoss 0.0000 (0.4397)	MaskBCELoss 0.0000 (0.1308)	MaskDICELoss 0.0000 (0.3089)
Epoch: [3][213/500]	Time  9.526 ( 9.526)	Loss 7.1408 (6.7273)	CeLoss 0.2676 (0.3745)	SegCLSLoss 0.0256 (0.0266)	KLLoss 0.1494 (0.1666)	MaskLoss 1.4036 (0.9609)	MaskBCELoss 0.7529 (0.2524)	MaskDICELoss 0.6507 (0.7085)
Epoch: [3][214/500]	Time 12.086 (12.086)	Loss 5.1996 (6.4442)	CeLoss 0.2129 (0.2219)	SegCLSLoss 0.0151 (0.0277)	KLLoss 0.0869 (0.1621)	MaskLoss 0.7213 (0.8693)	MaskBCELoss 0.1466 (0.1512)	MaskDICELoss 0.5747 (0.7180)
Epoch: [3][215/500]	Time 10.434 (10.434)	Loss 8.8973 (5.3794)	CeLoss 0.1953 (0.4876)	SegCLSLoss 0.0258 (0.0193)	KLLoss 0.1875 (0.1294)	MaskLoss 1.4494 (0.7423)	MaskBCELoss 0.5155 (0.1976)	MaskDICELoss 0.9340 (0.5447)
Epoch: [3][216/500]	Time  9.175 ( 9.175)	Loss 5.2129 (4.8918)	CeLoss 0.2041 (0.3640)	SegCLSLoss 0.0231 (0.0177)	KLLoss 0.2002 (0.1289)	MaskLoss 0.6528 (0.6543)	MaskBCELoss 0.0709 (0.1408)	MaskDICELoss 0.5819 (0.5135)
Epoch: [3][217/500]	Time  8.941 ( 8.941)	Loss 8.7157 (3.9025)	CeLoss 0.3555 (0.3252)	SegCLSLoss 0.0376 (0.0217)	KLLoss 0.2471 (0.1330)	MaskLoss 1.0693 (0.5616)	MaskBCELoss 0.0766 (0.1766)	MaskDICELoss 0.9927 (0.3850)
Epoch: [3][218/500]	Time 11.008 (11.008)	Loss 8.3557 (6.1140)	CeLoss 0.2344 (0.3994)	SegCLSLoss 0.0408 (0.0215)	KLLoss 0.2217 (0.1622)	MaskLoss 1.2179 (0.7907)	MaskBCELoss 0.3107 (0.1307)	MaskDICELoss 0.9072 (0.6600)
Epoch: [3][219/500]	Time  7.644 ( 7.644)	Loss 0.8281 (4.3840)	CeLoss 0.8281 (0.6155)	SegCLSLoss 0.0000 (0.0195)	KLLoss 0.0000 (0.1172)	MaskLoss 0.0000 (0.5405)	MaskBCELoss 0.0000 (0.1137)	MaskDICELoss 0.0000 (0.4268)
[2025-03-04 06:50:52,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=1720, skipped=0, lr=[0.0002598313253012048], mom=[(0.9, 0.95)]
[2025-03-04 06:50:52,735] [INFO] [timer.py:215:stop] epoch=0/micro_step=17200/global_step=1720, RunningAvgSamplesPerSec=1.0445906942886964, CurrSamplesPerSec=0.9418671389463903, MemAllocated=57.67GB, MaxMemAllocated=62.86GB
Epoch: [3][220/500]	Time 10.619 (10.619)	Loss 4.8013 (5.9828)	CeLoss 0.2471 (0.4085)	SegCLSLoss 0.0245 (0.0254)	KLLoss 0.1611 (0.1451)	MaskLoss 0.5890 (0.8586)	MaskBCELoss 0.0555 (0.2421)	MaskDICELoss 0.5336 (0.6165)
Epoch: [3][221/500]	Time  9.615 ( 9.615)	Loss 7.5730 (4.9263)	CeLoss 0.1865 (0.3878)	SegCLSLoss 0.0369 (0.0176)	KLLoss 0.2451 (0.1269)	MaskLoss 1.1179 (0.6779)	MaskBCELoss 0.3032 (0.1701)	MaskDICELoss 0.8147 (0.5078)
Epoch: [3][222/500]	Time  8.922 ( 8.922)	Loss 4.1199 (3.0079)	CeLoss 0.2852 (0.5029)	SegCLSLoss 0.0464 (0.0126)	KLLoss 0.2441 (0.0831)	MaskLoss 0.7003 (0.3896)	MaskBCELoss 0.3396 (0.1168)	MaskDICELoss 0.3608 (0.2727)
Epoch: [3][223/500]	Time  8.059 ( 8.059)	Loss 1.8672 (4.8904)	CeLoss 1.8672 (0.5281)	SegCLSLoss 0.0000 (0.0169)	KLLoss 0.0000 (0.1086)	MaskLoss 0.0000 (0.6078)	MaskBCELoss 0.0000 (0.1029)	MaskDICELoss 0.0000 (0.5049)
Epoch: [3][224/500]	Time  8.716 ( 8.716)	Loss 6.0576 (3.3750)	CeLoss 0.2471 (0.6762)	SegCLSLoss 0.0189 (0.0098)	KLLoss 0.1602 (0.0793)	MaskLoss 0.7654 (0.4077)	MaskBCELoss 0.0803 (0.1078)	MaskDICELoss 0.6851 (0.2999)
Epoch: [3][225/500]	Time  8.795 ( 8.795)	Loss 1.0547 (4.4776)	CeLoss 1.0547 (0.4253)	SegCLSLoss 0.0000 (0.0185)	KLLoss 0.0000 (0.1116)	MaskLoss 0.0000 (0.5931)	MaskBCELoss 0.0000 (0.1354)	MaskDICELoss 0.0000 (0.4576)
Epoch: [3][226/500]	Time  6.538 ( 6.538)	Loss 1.2891 (2.1171)	CeLoss 1.2891 (1.0283)	SegCLSLoss 0.0000 (0.0049)	KLLoss 0.0000 (0.0441)	MaskLoss 0.0000 (0.1630)	MaskBCELoss 0.0000 (0.0437)	MaskDICELoss 0.0000 (0.1193)
Epoch: [3][227/500]	Time  7.720 ( 7.720)	Loss 8.5869 (4.6713)	CeLoss 0.2021 (0.5753)	SegCLSLoss 0.0272 (0.0152)	KLLoss 0.1895 (0.1249)	MaskLoss 1.1239 (0.5426)	MaskBCELoss 0.1351 (0.0629)	MaskDICELoss 0.9888 (0.4797)
Epoch: [3][228/500]	Time  8.510 ( 8.510)	Loss 5.9493 (5.3089)	CeLoss 0.1973 (0.6332)	SegCLSLoss 0.0215 (0.0232)	KLLoss 0.1934 (0.1474)	MaskLoss 0.7662 (0.7017)	MaskBCELoss 0.0972 (0.1828)	MaskDICELoss 0.6691 (0.5189)
Epoch: [3][229/500]	Time 10.704 (10.704)	Loss 7.7770 (5.6896)	CeLoss 0.0977 (0.2446)	SegCLSLoss 0.0732 (0.0234)	KLLoss 0.2832 (0.1482)	MaskLoss 1.4553 (0.8547)	MaskBCELoss 0.7139 (0.2589)	MaskDICELoss 0.7414 (0.5958)
[2025-03-04 06:52:20,820] [INFO] [logging.py:96:log_dist] [Rank 0] step=1730, skipped=0, lr=[0.0002595903614457831], mom=[(0.9, 0.95)]
[2025-03-04 06:52:20,825] [INFO] [timer.py:215:stop] epoch=0/micro_step=17300/global_step=1730, RunningAvgSamplesPerSec=1.0450744939801733, CurrSamplesPerSec=0.9517266755596968, MemAllocated=57.27GB, MaxMemAllocated=62.86GB
Epoch: [3][230/500]	Time 10.509 (10.509)	Loss 7.8250 (4.4990)	CeLoss 0.1914 (0.3488)	SegCLSLoss 0.0275 (0.0156)	KLLoss 0.2207 (0.1308)	MaskLoss 0.9432 (0.6098)	MaskBCELoss 0.0244 (0.1446)	MaskDICELoss 0.9188 (0.4652)
Epoch: [3][231/500]	Time 10.384 (10.384)	Loss 2.1639 (5.7130)	CeLoss 0.3105 (0.2475)	SegCLSLoss 0.0140 (0.0233)	KLLoss 0.1030 (0.1789)	MaskLoss 0.3450 (0.8883)	MaskBCELoss 0.1696 (0.3053)	MaskDICELoss 0.1753 (0.5831)
Epoch: [3][232/500]	Time 10.475 (10.475)	Loss 5.7754 (4.7898)	CeLoss 0.2852 (0.3103)	SegCLSLoss 0.0212 (0.0177)	KLLoss 0.1357 (0.1361)	MaskLoss 0.6898 (0.6095)	MaskBCELoss 0.0287 (0.0903)	MaskDICELoss 0.6610 (0.5193)
Epoch: [3][233/500]	Time 11.174 (11.174)	Loss 0.3887 (4.4290)	CeLoss 0.3887 (0.4364)	SegCLSLoss 0.0000 (0.0140)	KLLoss 0.0000 (0.1036)	MaskLoss 0.0000 (0.6527)	MaskBCELoss 0.0000 (0.2232)	MaskDICELoss 0.0000 (0.4294)
Epoch: [3][234/500]	Time  9.455 ( 9.455)	Loss 5.1410 (5.2385)	CeLoss 0.2598 (0.2860)	SegCLSLoss 0.0155 (0.0193)	KLLoss 0.1025 (0.1431)	MaskLoss 0.6911 (0.7222)	MaskBCELoss 0.1265 (0.1630)	MaskDICELoss 0.5646 (0.5592)
Epoch: [3][235/500]	Time  9.135 ( 9.135)	Loss 7.7502 (4.3995)	CeLoss 0.2383 (0.3232)	SegCLSLoss 0.0215 (0.0164)	KLLoss 0.1787 (0.1291)	MaskLoss 1.3756 (0.6773)	MaskBCELoss 0.6137 (0.2465)	MaskDICELoss 0.7619 (0.4308)
Epoch: [3][236/500]	Time 11.494 (11.494)	Loss 6.5525 (6.9902)	CeLoss 0.3047 (0.3492)	SegCLSLoss 0.0187 (0.0253)	KLLoss 0.1660 (0.1718)	MaskLoss 0.9649 (1.0013)	MaskBCELoss 0.2746 (0.2591)	MaskDICELoss 0.6904 (0.7423)
Epoch: [3][237/500]	Time 10.890 (10.890)	Loss 7.8668 (5.6824)	CeLoss 0.3867 (0.3487)	SegCLSLoss 0.0603 (0.0246)	KLLoss 0.2617 (0.1659)	MaskLoss 1.2000 (0.8724)	MaskBCELoss 0.4021 (0.3040)	MaskDICELoss 0.7979 (0.5684)
Epoch: [3][238/500]	Time 10.318 (10.318)	Loss 6.8705 (4.7888)	CeLoss 0.1553 (0.4789)	SegCLSLoss 0.0645 (0.0193)	KLLoss 0.2773 (0.1276)	MaskLoss 1.3562 (0.7667)	MaskBCELoss 0.7406 (0.3268)	MaskDICELoss 0.6156 (0.4399)
Epoch: [3][239/500]	Time  9.839 ( 9.839)	Loss 7.3992 (6.4881)	CeLoss 0.1777 (0.2833)	SegCLSLoss 0.0305 (0.0236)	KLLoss 0.2500 (0.1696)	MaskLoss 0.9117 (0.9250)	MaskBCELoss 0.0563 (0.2294)	MaskDICELoss 0.8554 (0.6955)
[2025-03-04 06:54:02,468] [INFO] [logging.py:96:log_dist] [Rank 0] step=1740, skipped=0, lr=[0.00025934939759036143], mom=[(0.9, 0.95)]
[2025-03-04 06:54:02,473] [INFO] [timer.py:215:stop] epoch=0/micro_step=17400/global_step=1740, RunningAvgSamplesPerSec=1.0447009913411087, CurrSamplesPerSec=1.1789115045103449, MemAllocated=57.3GB, MaxMemAllocated=62.86GB
Epoch: [3][240/500]	Time  8.484 ( 8.484)	Loss 8.1088 (4.8131)	CeLoss 0.1582 (0.6502)	SegCLSLoss 0.0398 (0.0191)	KLLoss 0.1934 (0.1323)	MaskLoss 1.3225 (0.6659)	MaskBCELoss 0.4737 (0.2177)	MaskDICELoss 0.8488 (0.4482)
Epoch: [3][241/500]	Time  8.082 ( 8.082)	Loss 8.5692 (5.2097)	CeLoss 0.3555 (0.4289)	SegCLSLoss 0.0439 (0.0203)	KLLoss 0.2500 (0.1470)	MaskLoss 1.2751 (0.7044)	MaskBCELoss 0.3762 (0.1685)	MaskDICELoss 0.8990 (0.5358)
Epoch: [3][242/500]	Time  9.451 ( 9.451)	Loss 1.2344 (4.2178)	CeLoss 1.2344 (0.3238)	SegCLSLoss 0.0000 (0.0159)	KLLoss 0.0000 (0.1081)	MaskLoss 0.0000 (0.6090)	MaskBCELoss 0.0000 (0.1823)	MaskDICELoss 0.0000 (0.4267)
Epoch: [3][243/500]	Time 11.283 (11.283)	Loss 6.0002 (5.6905)	CeLoss 0.3301 (0.3315)	SegCLSLoss 0.0173 (0.0275)	KLLoss 0.1445 (0.1687)	MaskLoss 0.8488 (0.8518)	MaskBCELoss 0.2120 (0.2730)	MaskDICELoss 0.6367 (0.5788)
Epoch: [3][244/500]	Time 11.494 (11.494)	Loss 7.5911 (6.8546)	CeLoss 0.2656 (0.2374)	SegCLSLoss 0.0175 (0.0246)	KLLoss 0.1631 (0.1778)	MaskLoss 1.2018 (0.9466)	MaskBCELoss 0.4101 (0.1909)	MaskDICELoss 0.7917 (0.7557)
Epoch: [3][245/500]	Time 10.956 (10.956)	Loss 0.0752 (5.6466)	CeLoss 0.0752 (0.2028)	SegCLSLoss 0.0000 (0.0195)	KLLoss 0.0000 (0.1614)	MaskLoss 0.0000 (0.7551)	MaskBCELoss 0.0000 (0.1281)	MaskDICELoss 0.0000 (0.6270)
Epoch: [3][246/500]	Time 10.247 (10.247)	Loss 6.8659 (4.3635)	CeLoss 0.2969 (0.3462)	SegCLSLoss 0.0203 (0.0192)	KLLoss 0.1973 (0.1320)	MaskLoss 0.8673 (0.6111)	MaskBCELoss 0.0961 (0.1689)	MaskDICELoss 0.7712 (0.4423)
Epoch: [3][247/500]	Time  7.336 ( 7.336)	Loss 8.4742 (4.2578)	CeLoss 0.2480 (0.7821)	SegCLSLoss 0.0140 (0.0111)	KLLoss 0.1406 (0.0860)	MaskLoss 1.0418 (0.4426)	MaskBCELoss 0.0428 (0.0261)	MaskDICELoss 0.9990 (0.4165)
Epoch: [3][248/500]	Time  9.527 ( 9.527)	Loss 6.4493 (5.9747)	CeLoss 0.2871 (0.4869)	SegCLSLoss 0.0176 (0.0217)	KLLoss 0.1367 (0.1471)	MaskLoss 0.8536 (0.7758)	MaskBCELoss 0.1355 (0.1461)	MaskDICELoss 0.7181 (0.6297)
Epoch: [3][249/500]	Time 10.458 (10.458)	Loss 6.9462 (3.8095)	CeLoss 0.2412 (0.3178)	SegCLSLoss 0.0251 (0.0179)	KLLoss 0.2031 (0.1142)	MaskLoss 0.8623 (0.5045)	MaskBCELoss 0.0682 (0.1113)	MaskDICELoss 0.7941 (0.3932)
[2025-03-04 06:55:41,095] [INFO] [logging.py:96:log_dist] [Rank 0] step=1750, skipped=0, lr=[0.00025910843373493976], mom=[(0.9, 0.95)]
[2025-03-04 06:55:41,102] [INFO] [timer.py:215:stop] epoch=0/micro_step=17500/global_step=1750, RunningAvgSamplesPerSec=1.0445204792954146, CurrSamplesPerSec=1.0211722304914244, MemAllocated=57.27GB, MaxMemAllocated=62.86GB
Epoch: [3][250/500]	Time  9.795 ( 9.795)	Loss 6.6818 (6.1814)	CeLoss 0.1592 (0.3841)	SegCLSLoss 0.0330 (0.0239)	KLLoss 0.2637 (0.1773)	MaskLoss 0.8077 (0.8208)	MaskBCELoss 0.0365 (0.1597)	MaskDICELoss 0.7712 (0.6611)
Epoch: [3][251/500]	Time  7.849 ( 7.849)	Loss 4.2815 (4.0507)	CeLoss 0.4609 (0.6702)	SegCLSLoss 0.0139 (0.0134)	KLLoss 0.1196 (0.1001)	MaskLoss 0.5589 (0.5483)	MaskBCELoss 0.1300 (0.1853)	MaskDICELoss 0.4290 (0.3629)
Epoch: [3][252/500]	Time  9.067 ( 9.067)	Loss 7.6194 (4.8206)	CeLoss 0.2227 (0.5725)	SegCLSLoss 0.0300 (0.0156)	KLLoss 0.2852 (0.1174)	MaskLoss 1.0615 (0.6377)	MaskBCELoss 0.2327 (0.1632)	MaskDICELoss 0.8288 (0.4746)
Epoch: [3][253/500]	Time  8.530 ( 8.530)	Loss 0.8359 (4.5859)	CeLoss 0.8359 (0.5715)	SegCLSLoss 0.0000 (0.0209)	KLLoss 0.0000 (0.1139)	MaskLoss 0.0000 (0.5932)	MaskBCELoss 0.0000 (0.1426)	MaskDICELoss 0.0000 (0.4506)
Epoch: [3][254/500]	Time  9.564 ( 9.564)	Loss 7.4942 (4.9383)	CeLoss 0.1309 (0.3590)	SegCLSLoss 0.0605 (0.0202)	KLLoss 0.2637 (0.1443)	MaskLoss 1.2083 (0.7031)	MaskBCELoss 0.4329 (0.2001)	MaskDICELoss 0.7753 (0.5030)
Epoch: [3][255/500]	Time  9.692 ( 9.692)	Loss 8.4882 (5.6424)	CeLoss 0.1777 (0.2347)	SegCLSLoss 0.0400 (0.0273)	KLLoss 0.2188 (0.1842)	MaskLoss 1.3137 (0.8135)	MaskBCELoss 0.4062 (0.2163)	MaskDICELoss 0.9075 (0.5971)
Epoch: [3][256/500]	Time 11.012 (11.012)	Loss 1.5469 (6.0011)	CeLoss 1.5469 (0.4037)	SegCLSLoss 0.0000 (0.0174)	KLLoss 0.0000 (0.1309)	MaskLoss 0.0000 (0.9057)	MaskBCELoss 0.0000 (0.2980)	MaskDICELoss 0.0000 (0.6077)
Epoch: [3][257/500]	Time  9.302 ( 9.302)	Loss 6.4421 (4.7300)	CeLoss 0.3398 (0.5805)	SegCLSLoss 0.0244 (0.0139)	KLLoss 0.1299 (0.1097)	MaskLoss 0.8516 (0.6089)	MaskBCELoss 0.1418 (0.1398)	MaskDICELoss 0.7097 (0.4692)
Epoch: [3][258/500]	Time  9.558 ( 9.558)	Loss 8.1711 (4.8719)	CeLoss 0.2578 (0.3743)	SegCLSLoss 0.0160 (0.0167)	KLLoss 0.1641 (0.1209)	MaskLoss 1.0572 (0.6533)	MaskBCELoss 0.1194 (0.1430)	MaskDICELoss 0.9378 (0.5103)
Epoch: [3][259/500]	Time 10.684 (10.684)	Loss 5.3928 (6.1162)	CeLoss 0.3750 (0.3185)	SegCLSLoss 0.0151 (0.0206)	KLLoss 0.1426 (0.1502)	MaskLoss 0.8250 (0.9970)	MaskBCELoss 0.2885 (0.3899)	MaskDICELoss 0.5366 (0.6072)
[2025-03-04 06:57:15,333] [INFO] [logging.py:96:log_dist] [Rank 0] step=1760, skipped=0, lr=[0.000258867469879518], mom=[(0.9, 0.95)]
[2025-03-04 06:57:15,336] [INFO] [timer.py:215:stop] epoch=0/micro_step=17600/global_step=1760, RunningAvgSamplesPerSec=1.0446147418249911, CurrSamplesPerSec=1.1142277473711817, MemAllocated=57.46GB, MaxMemAllocated=62.86GB
Epoch: [3][260/500]	Time  8.977 ( 8.977)	Loss 7.2258 (6.2780)	CeLoss 0.2148 (0.4189)	SegCLSLoss 0.0299 (0.0266)	KLLoss 0.1875 (0.1531)	MaskLoss 0.9160 (0.9641)	MaskBCELoss 0.0867 (0.3367)	MaskDICELoss 0.8293 (0.6274)
Epoch: [3][261/500]	Time  9.811 ( 9.811)	Loss 1.0781 (4.2094)	CeLoss 1.0781 (0.5011)	SegCLSLoss 0.0000 (0.0117)	KLLoss 0.0000 (0.0945)	MaskLoss 0.0000 (0.5708)	MaskBCELoss 0.0000 (0.1598)	MaskDICELoss 0.0000 (0.4110)
Epoch: [3][262/500]	Time  9.405 ( 9.405)	Loss 6.4862 (5.3905)	CeLoss 0.1992 (0.3435)	SegCLSLoss 0.0317 (0.0279)	KLLoss 0.2158 (0.1603)	MaskLoss 0.7698 (0.7126)	MaskBCELoss 0.0170 (0.1380)	MaskDICELoss 0.7528 (0.5746)
Epoch: [3][263/500]	Time  9.550 ( 9.550)	Loss 7.1162 (6.5251)	CeLoss 0.2520 (0.3821)	SegCLSLoss 0.0240 (0.0297)	KLLoss 0.1748 (0.1855)	MaskLoss 0.8470 (0.9437)	MaskBCELoss 0.0162 (0.2678)	MaskDICELoss 0.8308 (0.6759)
Epoch: [3][264/500]	Time  9.527 ( 9.527)	Loss 0.7344 (4.5661)	CeLoss 0.7344 (0.2957)	SegCLSLoss 0.0000 (0.0154)	KLLoss 0.0000 (0.1185)	MaskLoss 0.0000 (0.6269)	MaskBCELoss 0.0000 (0.1452)	MaskDICELoss 0.0000 (0.4817)
Epoch: [3][265/500]	Time  9.101 ( 9.101)	Loss 7.5627 (5.4112)	CeLoss 0.1934 (0.6337)	SegCLSLoss 0.0264 (0.0218)	KLLoss 0.1826 (0.1391)	MaskLoss 1.2657 (0.7213)	MaskBCELoss 0.4922 (0.1906)	MaskDICELoss 0.7735 (0.5308)
Epoch: [3][266/500]	Time 10.180 (10.180)	Loss 0.6094 (4.4158)	CeLoss 0.6094 (0.3095)	SegCLSLoss 0.0000 (0.0180)	KLLoss 0.0000 (0.1150)	MaskLoss 0.0000 (0.5943)	MaskBCELoss 0.0000 (0.1287)	MaskDICELoss 0.0000 (0.4656)
Epoch: [3][267/500]	Time  8.724 ( 8.724)	Loss 7.1577 (5.7831)	CeLoss 0.3086 (0.3770)	SegCLSLoss 0.0206 (0.0217)	KLLoss 0.1299 (0.1527)	MaskLoss 1.2354 (0.7969)	MaskBCELoss 0.5291 (0.1888)	MaskDICELoss 0.7063 (0.6081)
Epoch: [3][268/500]	Time 10.562 (10.562)	Loss 0.0767 (4.6874)	CeLoss 0.0767 (0.3929)	SegCLSLoss 0.0000 (0.0158)	KLLoss 0.0000 (0.1279)	MaskLoss 0.0000 (0.6346)	MaskBCELoss 0.0000 (0.1530)	MaskDICELoss 0.0000 (0.4816)
Epoch: [3][269/500]	Time  9.371 ( 9.371)	Loss 2.6298 (3.8301)	CeLoss 0.7422 (0.3881)	SegCLSLoss 0.0261 (0.0161)	KLLoss 0.2100 (0.1238)	MaskLoss 0.3232 (0.5107)	MaskBCELoss 0.1534 (0.1292)	MaskDICELoss 0.1698 (0.3815)
[2025-03-04 06:58:51,651] [INFO] [logging.py:96:log_dist] [Rank 0] step=1770, skipped=0, lr=[0.00025862650602409635], mom=[(0.9, 0.95)]
[2025-03-04 06:58:51,656] [INFO] [timer.py:215:stop] epoch=0/micro_step=17700/global_step=1770, RunningAvgSamplesPerSec=1.0445794680975262, CurrSamplesPerSec=0.9914563177664699, MemAllocated=57.68GB, MaxMemAllocated=62.86GB
Epoch: [3][270/500]	Time 10.088 (10.088)	Loss 0.3184 (3.0125)	CeLoss 0.3184 (0.5967)	SegCLSLoss 0.0000 (0.0069)	KLLoss 0.0000 (0.0540)	MaskLoss 0.0000 (0.3389)	MaskBCELoss 0.0000 (0.0588)	MaskDICELoss 0.0000 (0.2801)
Epoch: [3][271/500]	Time 10.399 (10.399)	Loss 7.0305 (6.4187)	CeLoss 0.2969 (0.2334)	SegCLSLoss 0.0322 (0.0252)	KLLoss 0.2148 (0.1836)	MaskLoss 1.0075 (0.8700)	MaskBCELoss 0.2594 (0.1618)	MaskDICELoss 0.7480 (0.7082)
Epoch: [3][272/500]	Time  8.563 ( 8.563)	Loss 4.5081 (4.5360)	CeLoss 0.2344 (0.7622)	SegCLSLoss 0.0153 (0.0120)	KLLoss 0.1387 (0.0911)	MaskLoss 0.6145 (0.5577)	MaskBCELoss 0.1315 (0.1308)	MaskDICELoss 0.4830 (0.4269)
Epoch: [3][273/500]	Time  8.023 ( 8.023)	Loss 3.6881 (5.0044)	CeLoss 0.4121 (0.4123)	SegCLSLoss 0.0228 (0.0205)	KLLoss 0.1914 (0.1533)	MaskLoss 0.5961 (0.6982)	MaskBCELoss 0.2830 (0.1928)	MaskDICELoss 0.3131 (0.5054)
Epoch: [3][274/500]	Time  9.341 ( 9.341)	Loss 6.9656 (5.8020)	CeLoss 0.1914 (0.3240)	SegCLSLoss 0.0320 (0.0228)	KLLoss 0.2246 (0.1810)	MaskLoss 0.8921 (0.7454)	MaskBCELoss 0.1005 (0.1129)	MaskDICELoss 0.7916 (0.6325)
Epoch: [3][275/500]	Time  7.082 ( 7.082)	Loss 3.5793 (3.9028)	CeLoss 0.2852 (0.7642)	SegCLSLoss 0.0142 (0.0139)	KLLoss 0.1157 (0.0904)	MaskLoss 0.4682 (0.5691)	MaskBCELoss 0.0957 (0.2519)	MaskDICELoss 0.3725 (0.3172)
Epoch: [3][276/500]	Time  8.464 ( 8.464)	Loss 1.7734 (4.5609)	CeLoss 1.7734 (0.7556)	SegCLSLoss 0.0000 (0.0125)	KLLoss 0.0000 (0.0993)	MaskLoss 0.0000 (0.5878)	MaskBCELoss 0.0000 (0.1671)	MaskDICELoss 0.0000 (0.4207)
Epoch: [3][277/500]	Time 10.579 (10.579)	Loss 7.5048 (6.5239)	CeLoss 0.1670 (0.2940)	SegCLSLoss 0.0398 (0.0263)	KLLoss 0.2598 (0.1765)	MaskLoss 1.1039 (1.0951)	MaskBCELoss 0.2952 (0.4533)	MaskDICELoss 0.8086 (0.6417)
Epoch: [3][278/500]	Time 11.165 (11.165)	Loss 6.3526 (5.4838)	CeLoss 0.2041 (0.2104)	SegCLSLoss 0.0288 (0.0220)	KLLoss 0.2129 (0.1614)	MaskLoss 0.7561 (0.7855)	MaskBCELoss 0.0213 (0.1972)	MaskDICELoss 0.7348 (0.5884)
Epoch: [3][279/500]	Time  9.236 ( 9.236)	Loss 1.3281 (4.5012)	CeLoss 1.3281 (0.5223)	SegCLSLoss 0.0000 (0.0168)	KLLoss 0.0000 (0.1249)	MaskLoss 0.0000 (0.5620)	MaskBCELoss 0.0000 (0.1084)	MaskDICELoss 0.0000 (0.4536)
[2025-03-04 07:00:24,567] [INFO] [logging.py:96:log_dist] [Rank 0] step=1780, skipped=0, lr=[0.0002583855421686747], mom=[(0.9, 0.95)]
[2025-03-04 07:00:24,572] [INFO] [timer.py:215:stop] epoch=0/micro_step=17800/global_step=1780, RunningAvgSamplesPerSec=1.0447533067777788, CurrSamplesPerSec=0.9938333720506921, MemAllocated=57.28GB, MaxMemAllocated=62.86GB
Epoch: [3][280/500]	Time 10.064 (10.064)	Loss 7.9151 (5.4896)	CeLoss 0.1816 (0.2261)	SegCLSLoss 0.0381 (0.0185)	KLLoss 0.1992 (0.1503)	MaskLoss 1.2705 (0.8185)	MaskBCELoss 0.4415 (0.2407)	MaskDICELoss 0.8290 (0.5778)
Epoch: [3][281/500]	Time 10.004 (10.004)	Loss 7.8016 (5.1145)	CeLoss 0.2021 (0.3563)	SegCLSLoss 0.0315 (0.0177)	KLLoss 0.2559 (0.1435)	MaskLoss 0.9619 (0.7151)	MaskBCELoss 0.0611 (0.1858)	MaskDICELoss 0.9008 (0.5293)
Epoch: [3][282/500]	Time  8.931 ( 8.931)	Loss 10.0730 (5.7005)	CeLoss 0.3652 (0.3375)	SegCLSLoss 0.0134 (0.0182)	KLLoss 0.1641 (0.1425)	MaskLoss 2.2189 (0.8905)	MaskBCELoss 1.3689 (0.3189)	MaskDICELoss 0.8500 (0.5717)
Epoch: [3][283/500]	Time 10.359 (10.359)	Loss 5.5617 (5.0867)	CeLoss 0.2178 (0.2122)	SegCLSLoss 0.0170 (0.0217)	KLLoss 0.1465 (0.1446)	MaskLoss 0.7329 (0.7270)	MaskBCELoss 0.1125 (0.1828)	MaskDICELoss 0.6205 (0.5442)
Epoch: [3][284/500]	Time  9.768 ( 9.768)	Loss 8.4108 (5.1935)	CeLoss 0.1953 (0.4625)	SegCLSLoss 0.0432 (0.0193)	KLLoss 0.2373 (0.1438)	MaskLoss 1.4045 (0.7730)	MaskBCELoss 0.5464 (0.2678)	MaskDICELoss 0.8581 (0.5053)
Epoch: [3][285/500]	Time  8.226 ( 8.226)	Loss 1.1328 (3.5255)	CeLoss 1.1328 (0.6055)	SegCLSLoss 0.0000 (0.0103)	KLLoss 0.0000 (0.0771)	MaskLoss 0.0000 (0.4200)	MaskBCELoss 0.0000 (0.0871)	MaskDICELoss 0.0000 (0.3330)
Epoch: [3][286/500]	Time  9.536 ( 9.536)	Loss 8.5609 (6.1319)	CeLoss 0.2295 (0.4114)	SegCLSLoss 0.0479 (0.0250)	KLLoss 0.2334 (0.1570)	MaskLoss 1.3859 (0.8831)	MaskBCELoss 0.5025 (0.2523)	MaskDICELoss 0.8835 (0.6308)
Epoch: [3][287/500]	Time 10.568 (10.568)	Loss 8.1504 (6.2392)	CeLoss 0.1982 (0.3494)	SegCLSLoss 0.0304 (0.0257)	KLLoss 0.2158 (0.1590)	MaskLoss 1.2471 (0.9509)	MaskBCELoss 0.3760 (0.3149)	MaskDICELoss 0.8711 (0.6360)
Epoch: [3][288/500]	Time  7.320 ( 7.320)	Loss 8.4283 (3.8801)	CeLoss 0.2256 (0.6243)	SegCLSLoss 0.0272 (0.0139)	KLLoss 0.2109 (0.1079)	MaskLoss 1.2312 (0.4754)	MaskBCELoss 0.3117 (0.1103)	MaskDICELoss 0.9194 (0.3650)
Epoch: [3][289/500]	Time  8.901 ( 8.901)	Loss 0.0552 (5.2194)	CeLoss 0.0552 (0.3599)	SegCLSLoss 0.0000 (0.0244)	KLLoss 0.0000 (0.1446)	MaskLoss 0.0000 (0.6749)	MaskBCELoss 0.0000 (0.1163)	MaskDICELoss 0.0000 (0.5587)
[2025-03-04 07:01:57,012] [INFO] [logging.py:96:log_dist] [Rank 0] step=1790, skipped=0, lr=[0.000258144578313253], mom=[(0.9, 0.95)]
[2025-03-04 07:01:57,018] [INFO] [timer.py:215:stop] epoch=0/micro_step=17900/global_step=1790, RunningAvgSamplesPerSec=1.0449540547842586, CurrSamplesPerSec=1.132360026052652, MemAllocated=57.71GB, MaxMemAllocated=62.86GB
Epoch: [3][290/500]	Time  8.833 ( 8.833)	Loss 0.5430 (5.1872)	CeLoss 0.5430 (0.5778)	SegCLSLoss 0.0000 (0.0160)	KLLoss 0.0000 (0.1168)	MaskLoss 0.0000 (0.6403)	MaskBCELoss 0.0000 (0.1063)	MaskDICELoss 0.0000 (0.5340)
Epoch: [3][291/500]	Time 10.523 (10.523)	Loss 5.6757 (5.5017)	CeLoss 0.2324 (0.3225)	SegCLSLoss 0.0166 (0.0188)	KLLoss 0.1475 (0.1465)	MaskLoss 0.7108 (0.7677)	MaskBCELoss 0.0662 (0.1864)	MaskDICELoss 0.6446 (0.5813)
Epoch: [3][292/500]	Time 10.786 (10.786)	Loss 7.8080 (5.0848)	CeLoss 0.2539 (0.3005)	SegCLSLoss 0.0181 (0.0182)	KLLoss 0.1279 (0.1236)	MaskLoss 0.9830 (0.6734)	MaskBCELoss 0.0745 (0.1226)	MaskDICELoss 0.9085 (0.5508)
Epoch: [3][293/500]	Time  8.725 ( 8.725)	Loss 4.6413 (4.5825)	CeLoss 0.2656 (0.3839)	SegCLSLoss 0.0168 (0.0171)	KLLoss 0.1172 (0.1380)	MaskLoss 0.6598 (0.6627)	MaskBCELoss 0.1713 (0.2082)	MaskDICELoss 0.4885 (0.4545)
Epoch: [3][294/500]	Time  9.120 ( 9.120)	Loss 5.8516 (5.7901)	CeLoss 0.3301 (0.5957)	SegCLSLoss 0.0214 (0.0224)	KLLoss 0.1465 (0.1322)	MaskLoss 0.7461 (0.7894)	MaskBCELoss 0.1005 (0.2107)	MaskDICELoss 0.6455 (0.5787)
Epoch: [3][295/500]	Time  9.583 ( 9.583)	Loss 1.3047 (6.2626)	CeLoss 1.3047 (0.4633)	SegCLSLoss 0.0000 (0.0235)	KLLoss 0.0000 (0.1410)	MaskLoss 0.0000 (1.1194)	MaskBCELoss 0.0000 (0.5514)	MaskDICELoss 0.0000 (0.5680)
Epoch: [3][296/500]	Time  8.947 ( 8.947)	Loss 7.7637 (4.6750)	CeLoss 0.2158 (0.4034)	SegCLSLoss 0.0211 (0.0181)	KLLoss 0.1719 (0.1347)	MaskLoss 0.9749 (0.6470)	MaskBCELoss 0.0723 (0.1746)	MaskDICELoss 0.9026 (0.4724)
Epoch: [3][297/500]	Time 10.510 (10.510)	Loss 3.8446 (5.7868)	CeLoss 0.1924 (0.3460)	SegCLSLoss 0.0137 (0.0226)	KLLoss 0.0933 (0.1528)	MaskLoss 0.5290 (0.7937)	MaskBCELoss 0.1135 (0.1789)	MaskDICELoss 0.4156 (0.6149)
Epoch: [3][298/500]	Time 10.527 (10.527)	Loss 5.5812 (6.1485)	CeLoss 0.2852 (0.3828)	SegCLSLoss 0.0137 (0.0251)	KLLoss 0.1309 (0.1640)	MaskLoss 0.8954 (0.8497)	MaskBCELoss 0.3343 (0.2014)	MaskDICELoss 0.5611 (0.6483)
Epoch: [3][299/500]	Time  9.354 ( 9.354)	Loss 1.3750 (4.3664)	CeLoss 1.3750 (0.5137)	SegCLSLoss 0.0000 (0.0176)	KLLoss 0.0000 (0.1092)	MaskLoss 0.0000 (0.6578)	MaskBCELoss 0.0000 (0.2546)	MaskDICELoss 0.0000 (0.4032)
[2025-03-04 07:03:32,830] [INFO] [logging.py:96:log_dist] [Rank 0] step=1800, skipped=0, lr=[0.00025790361445783127], mom=[(0.9, 0.95)]
[2025-03-04 07:03:32,835] [INFO] [timer.py:215:stop] epoch=0/micro_step=18000/global_step=1800, RunningAvgSamplesPerSec=1.0449477991592206, CurrSamplesPerSec=1.2918956671191875, MemAllocated=56.74GB, MaxMemAllocated=62.86GB
Epoch: [3][300/500]	Time  7.743 ( 7.743)	Loss 1.1797 (3.5492)	CeLoss 1.1797 (0.5720)	SegCLSLoss 0.0000 (0.0147)	KLLoss 0.0000 (0.0866)	MaskLoss 0.0000 (0.4973)	MaskBCELoss 0.0000 (0.1825)	MaskDICELoss 0.0000 (0.3148)
Epoch: [3][301/500]	Time  9.831 ( 9.831)	Loss 4.8131 (5.1471)	CeLoss 0.2100 (0.4149)	SegCLSLoss 0.0164 (0.0181)	KLLoss 0.1455 (0.1092)	MaskLoss 0.8347 (0.7624)	MaskBCELoss 0.3714 (0.2476)	MaskDICELoss 0.4634 (0.5148)
Epoch: [3][302/500]	Time  9.917 ( 9.917)	Loss 8.4751 (6.3730)	CeLoss 0.1992 (0.4228)	SegCLSLoss 0.0374 (0.0264)	KLLoss 0.2168 (0.1529)	MaskLoss 1.2019 (0.8647)	MaskBCELoss 0.2623 (0.1889)	MaskDICELoss 0.9396 (0.6759)
Epoch: [3][303/500]	Time  8.799 ( 8.799)	Loss 1.8203 (4.3122)	CeLoss 1.8203 (0.5679)	SegCLSLoss 0.0000 (0.0217)	KLLoss 0.0000 (0.1133)	MaskLoss 0.0000 (0.5890)	MaskBCELoss 0.0000 (0.1820)	MaskDICELoss 0.0000 (0.4071)
Epoch: [3][304/500]	Time  8.534 ( 8.534)	Loss 7.6548 (4.8572)	CeLoss 0.2031 (0.4384)	SegCLSLoss 0.0349 (0.0206)	KLLoss 0.2236 (0.1231)	MaskLoss 0.9603 (0.6674)	MaskBCELoss 0.0788 (0.1756)	MaskDICELoss 0.8815 (0.4918)
Epoch: [3][305/500]	Time  9.452 ( 9.452)	Loss 7.5536 (6.2839)	CeLoss 0.1719 (0.4036)	SegCLSLoss 0.0388 (0.0253)	KLLoss 0.2031 (0.1624)	MaskLoss 1.1291 (1.0491)	MaskBCELoss 0.3123 (0.4480)	MaskDICELoss 0.8168 (0.6011)
Epoch: [3][306/500]	Time  8.737 ( 8.737)	Loss 0.9336 (4.7269)	CeLoss 0.9336 (0.4914)	SegCLSLoss 0.0000 (0.0183)	KLLoss 0.0000 (0.1187)	MaskLoss 0.0000 (0.6946)	MaskBCELoss 0.0000 (0.2415)	MaskDICELoss 0.0000 (0.4530)
Epoch: [3][307/500]	Time  9.449 ( 9.449)	Loss 4.8997 (4.5690)	CeLoss 0.1758 (0.3573)	SegCLSLoss 0.0201 (0.0210)	KLLoss 0.1689 (0.1348)	MaskLoss 0.6584 (0.6216)	MaskBCELoss 0.1205 (0.1510)	MaskDICELoss 0.5379 (0.4706)
Epoch: [3][308/500]	Time 10.375 (10.375)	Loss 7.5428 (5.8098)	CeLoss 0.2246 (0.3710)	SegCLSLoss 0.0248 (0.0182)	KLLoss 0.1670 (0.1264)	MaskLoss 0.9581 (0.7651)	MaskBCELoss 0.0877 (0.1363)	MaskDICELoss 0.8704 (0.6289)
Epoch: [3][309/500]	Time  9.019 ( 9.019)	Loss 1.1953 (6.8595)	CeLoss 1.1953 (0.2892)	SegCLSLoss 0.0000 (0.0426)	KLLoss 0.0000 (0.1953)	MaskLoss 0.0000 (0.9686)	MaskBCELoss 0.0000 (0.2324)	MaskDICELoss 0.0000 (0.7361)
[2025-03-04 07:05:06,192] [INFO] [logging.py:96:log_dist] [Rank 0] step=1810, skipped=0, lr=[0.0002576626506024096], mom=[(0.9, 0.95)]
[2025-03-04 07:05:06,199] [INFO] [timer.py:215:stop] epoch=0/micro_step=18100/global_step=1810, RunningAvgSamplesPerSec=1.0450898268700897, CurrSamplesPerSec=1.081398164530566, MemAllocated=57.58GB, MaxMemAllocated=62.86GB
Epoch: [3][310/500]	Time  9.249 ( 9.249)	Loss 0.2246 (4.4849)	CeLoss 0.2246 (0.5114)	SegCLSLoss 0.0000 (0.0131)	KLLoss 0.0000 (0.0867)	MaskLoss 0.0000 (0.7455)	MaskBCELoss 0.0000 (0.3473)	MaskDICELoss 0.0000 (0.3982)
Epoch: [3][311/500]	Time  8.012 ( 8.012)	Loss 8.0846 (6.9925)	CeLoss 0.2451 (0.2840)	SegCLSLoss 0.0302 (0.0255)	KLLoss 0.2197 (0.1510)	MaskLoss 1.2724 (1.0169)	MaskBCELoss 0.4291 (0.2652)	MaskDICELoss 0.8432 (0.7518)
Epoch: [3][312/500]	Time  8.824 ( 8.824)	Loss 1.3203 (3.5550)	CeLoss 1.3203 (0.7896)	SegCLSLoss 0.0000 (0.0132)	KLLoss 0.0000 (0.0822)	MaskLoss 0.0000 (0.3823)	MaskBCELoss 0.0000 (0.0636)	MaskDICELoss 0.0000 (0.3187)
Epoch: [3][313/500]	Time 10.536 (10.536)	Loss 6.3197 (5.7609)	CeLoss 0.1953 (0.3608)	SegCLSLoss 0.0383 (0.0205)	KLLoss 0.2207 (0.1412)	MaskLoss 0.9059 (0.7426)	MaskBCELoss 0.2272 (0.1152)	MaskDICELoss 0.6787 (0.6273)
Epoch: [3][314/500]	Time  9.005 ( 9.005)	Loss 6.0496 (5.6471)	CeLoss 0.2021 (0.3057)	SegCLSLoss 0.0327 (0.0201)	KLLoss 0.1992 (0.1342)	MaskLoss 0.7866 (0.7723)	MaskBCELoss 0.1102 (0.1636)	MaskDICELoss 0.6764 (0.6087)
Epoch: [3][315/500]	Time 10.436 (10.436)	Loss 7.1892 (6.3100)	CeLoss 0.2236 (0.2570)	SegCLSLoss 0.0286 (0.0245)	KLLoss 0.1943 (0.1594)	MaskLoss 1.1415 (0.9514)	MaskBCELoss 0.3961 (0.2883)	MaskDICELoss 0.7454 (0.6631)
Epoch: [3][316/500]	Time 10.607 (10.607)	Loss 7.2133 (5.3549)	CeLoss 0.1943 (0.3631)	SegCLSLoss 0.0247 (0.0191)	KLLoss 0.1758 (0.1230)	MaskLoss 0.8619 (0.7441)	MaskBCELoss 0.0108 (0.1822)	MaskDICELoss 0.8511 (0.5618)
Epoch: [3][317/500]	Time 10.275 (10.275)	Loss 8.0205 (5.8184)	CeLoss 0.2139 (0.2240)	SegCLSLoss 0.0251 (0.0212)	KLLoss 0.2559 (0.1526)	MaskLoss 1.4226 (0.8631)	MaskBCELoss 0.6405 (0.2457)	MaskDICELoss 0.7821 (0.6174)
Epoch: [3][318/500]	Time  8.741 ( 8.741)	Loss 1.1641 (3.3277)	CeLoss 1.1641 (0.4293)	SegCLSLoss 0.0000 (0.0119)	KLLoss 0.0000 (0.0862)	MaskLoss 0.0000 (0.4608)	MaskBCELoss 0.0000 (0.1467)	MaskDICELoss 0.0000 (0.3141)
Epoch: [3][319/500]	Time  9.571 ( 9.571)	Loss 3.5673 (5.6287)	CeLoss 0.3770 (0.3571)	SegCLSLoss 0.0172 (0.0222)	KLLoss 0.1602 (0.1466)	MaskLoss 0.4568 (0.7508)	MaskBCELoss 0.1057 (0.1487)	MaskDICELoss 0.3511 (0.6021)
[2025-03-04 07:06:40,918] [INFO] [logging.py:96:log_dist] [Rank 0] step=1820, skipped=0, lr=[0.0002574216867469879], mom=[(0.9, 0.95)]
[2025-03-04 07:06:40,924] [INFO] [timer.py:215:stop] epoch=0/micro_step=18200/global_step=1820, RunningAvgSamplesPerSec=1.0451484702289553, CurrSamplesPerSec=1.147380271244125, MemAllocated=56.74GB, MaxMemAllocated=62.86GB
Epoch: [3][320/500]	Time  8.717 ( 8.717)	Loss 1.1172 (4.5209)	CeLoss 1.1172 (0.4243)	SegCLSLoss 0.0000 (0.0164)	KLLoss 0.0000 (0.1107)	MaskLoss 0.0000 (0.6960)	MaskBCELoss 0.0000 (0.2651)	MaskDICELoss 0.0000 (0.4309)
Epoch: [3][321/500]	Time  9.604 ( 9.604)	Loss 1.4062 (5.3854)	CeLoss 1.4062 (0.4892)	SegCLSLoss 0.0000 (0.0195)	KLLoss 0.0000 (0.1349)	MaskLoss 0.0000 (0.7637)	MaskBCELoss 0.0000 (0.2263)	MaskDICELoss 0.0000 (0.5374)
Epoch: [3][322/500]	Time 10.378 (10.378)	Loss 5.1006 (6.4137)	CeLoss 0.3203 (0.3918)	SegCLSLoss 0.0154 (0.0247)	KLLoss 0.1108 (0.1477)	MaskLoss 0.6922 (0.9346)	MaskBCELoss 0.1461 (0.2692)	MaskDICELoss 0.5461 (0.6654)
Epoch: [3][323/500]	Time  9.182 ( 9.182)	Loss 8.5564 (6.2304)	CeLoss 0.3027 (0.3922)	SegCLSLoss 0.0208 (0.0234)	KLLoss 0.1582 (0.1626)	MaskLoss 1.0431 (0.8443)	MaskBCELoss 0.0432 (0.1818)	MaskDICELoss 0.9999 (0.6625)
Epoch: [3][324/500]	Time  8.717 ( 8.717)	Loss 7.8958 (5.1641)	CeLoss 0.2383 (0.6286)	SegCLSLoss 0.0293 (0.0196)	KLLoss 0.2090 (0.1327)	MaskLoss 1.2605 (0.7148)	MaskBCELoss 0.4419 (0.2210)	MaskDICELoss 0.8186 (0.4939)
Epoch: [3][325/500]	Time  7.780 ( 7.780)	Loss 6.9947 (5.2750)	CeLoss 0.2119 (0.5731)	SegCLSLoss 0.0253 (0.0189)	KLLoss 0.1914 (0.1266)	MaskLoss 0.8722 (0.7021)	MaskBCELoss 0.0665 (0.1753)	MaskDICELoss 0.8057 (0.5269)
Epoch: [3][326/500]	Time  8.942 ( 8.942)	Loss 7.4945 (4.6455)	CeLoss 0.2314 (0.3425)	SegCLSLoss 0.0236 (0.0177)	KLLoss 0.1426 (0.1046)	MaskLoss 1.0224 (0.6504)	MaskBCELoss 0.1786 (0.1689)	MaskDICELoss 0.8438 (0.4814)
Epoch: [3][327/500]	Time 10.179 (10.179)	Loss 1.9358 (4.8483)	CeLoss 0.2812 (0.3660)	SegCLSLoss 0.0183 (0.0171)	KLLoss 0.1211 (0.1254)	MaskLoss 0.3506 (0.7156)	MaskBCELoss 0.2135 (0.2294)	MaskDICELoss 0.1371 (0.4862)
Epoch: [3][328/500]	Time  9.509 ( 9.509)	Loss 3.5331 (5.4185)	CeLoss 0.3398 (0.2779)	SegCLSLoss 0.0154 (0.0172)	KLLoss 0.0864 (0.1250)	MaskLoss 0.4878 (0.7239)	MaskBCELoss 0.1338 (0.1308)	MaskDICELoss 0.3540 (0.5931)
Epoch: [3][329/500]	Time  9.687 ( 9.687)	Loss 7.8157 (4.4782)	CeLoss 0.2285 (0.2423)	SegCLSLoss 0.0308 (0.0156)	KLLoss 0.2109 (0.1148)	MaskLoss 1.1570 (0.6962)	MaskBCELoss 0.3159 (0.2427)	MaskDICELoss 0.8411 (0.4535)
[2025-03-04 07:08:13,870] [INFO] [logging.py:96:log_dist] [Rank 0] step=1830, skipped=0, lr=[0.00025718072289156624], mom=[(0.9, 0.95)]
[2025-03-04 07:08:13,875] [INFO] [timer.py:215:stop] epoch=0/micro_step=18300/global_step=1830, RunningAvgSamplesPerSec=1.0453124991384213, CurrSamplesPerSec=1.1146769269750045, MemAllocated=56.82GB, MaxMemAllocated=62.86GB
Epoch: [3][330/500]	Time  8.973 ( 8.973)	Loss 6.7571 (5.2801)	CeLoss 0.1895 (0.4201)	SegCLSLoss 0.0388 (0.0247)	KLLoss 0.2285 (0.1629)	MaskLoss 1.1369 (0.7264)	MaskBCELoss 0.4627 (0.1878)	MaskDICELoss 0.6743 (0.5386)
Epoch: [3][331/500]	Time  7.403 ( 7.403)	Loss 5.7869 (4.5294)	CeLoss 0.2598 (0.7177)	SegCLSLoss 0.0186 (0.0106)	KLLoss 0.1338 (0.0820)	MaskLoss 1.3235 (0.6342)	MaskBCELoss 0.8672 (0.2248)	MaskDICELoss 0.4563 (0.4094)
Epoch: [3][332/500]	Time 10.176 (10.176)	Loss 4.2402 (5.8351)	CeLoss 0.2969 (0.3220)	SegCLSLoss 0.0177 (0.0226)	KLLoss 0.1147 (0.1617)	MaskLoss 0.5359 (0.7753)	MaskBCELoss 0.0782 (0.1437)	MaskDICELoss 0.4577 (0.6315)
Epoch: [3][333/500]	Time 10.410 (10.410)	Loss 7.6020 (6.6860)	CeLoss 0.1924 (0.2320)	SegCLSLoss 0.0269 (0.0266)	KLLoss 0.1924 (0.1802)	MaskLoss 0.9339 (0.9464)	MaskBCELoss 0.0446 (0.2184)	MaskDICELoss 0.8893 (0.7280)
Epoch: [3][334/500]	Time 10.050 (10.050)	Loss 1.5000 (5.9055)	CeLoss 1.5000 (0.4521)	SegCLSLoss 0.0000 (0.0203)	KLLoss 0.0000 (0.1510)	MaskLoss 0.0000 (0.7330)	MaskBCELoss 0.0000 (0.0953)	MaskDICELoss 0.0000 (0.6377)
Epoch: [3][335/500]	Time  8.652 ( 8.652)	Loss 7.6311 (4.0529)	CeLoss 0.1855 (0.5057)	SegCLSLoss 0.0288 (0.0157)	KLLoss 0.1934 (0.1111)	MaskLoss 0.9289 (0.5163)	MaskBCELoss 0.0325 (0.1170)	MaskDICELoss 0.8965 (0.3993)
Epoch: [3][336/500]	Time 10.854 (10.854)	Loss 8.8087 (6.9370)	CeLoss 0.2266 (0.2491)	SegCLSLoss 0.0483 (0.0246)	KLLoss 0.2812 (0.1772)	MaskLoss 1.1388 (0.9767)	MaskBCELoss 0.1388 (0.2191)	MaskDICELoss 1.0000 (0.7575)
Epoch: [3][337/500]	Time  9.711 ( 9.711)	Loss 0.8711 (4.1914)	CeLoss 0.8711 (0.4519)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.1117)	MaskLoss 0.0000 (0.5770)	MaskBCELoss 0.0000 (0.1658)	MaskDICELoss 0.0000 (0.4112)
Epoch: [3][338/500]	Time 10.525 (10.525)	Loss 5.6793 (5.5967)	CeLoss 0.2891 (0.3486)	SegCLSLoss 0.0261 (0.0243)	KLLoss 0.1455 (0.1624)	MaskLoss 0.6770 (0.8542)	MaskBCELoss 0.0310 (0.2934)	MaskDICELoss 0.6460 (0.5608)
Epoch: [3][339/500]	Time  7.465 ( 7.465)	Loss 1.1328 (3.5819)	CeLoss 1.1328 (0.7077)	SegCLSLoss 0.0000 (0.0122)	KLLoss 0.0000 (0.0864)	MaskLoss 0.0000 (0.4657)	MaskBCELoss 0.0000 (0.1574)	MaskDICELoss 0.0000 (0.3083)
[2025-03-04 07:09:46,420] [INFO] [logging.py:96:log_dist] [Rank 0] step=1840, skipped=0, lr=[0.00025693975903614457], mom=[(0.9, 0.95)]
[2025-03-04 07:09:46,426] [INFO] [timer.py:215:stop] epoch=0/micro_step=18400/global_step=1840, RunningAvgSamplesPerSec=1.0454986363674987, CurrSamplesPerSec=1.3692535347293788, MemAllocated=56.7GB, MaxMemAllocated=62.86GB
Epoch: [3][340/500]	Time  7.305 ( 7.305)	Loss 0.7109 (4.3870)	CeLoss 0.7109 (0.6387)	SegCLSLoss 0.0000 (0.0148)	KLLoss 0.0000 (0.0928)	MaskLoss 0.0000 (0.5932)	MaskBCELoss 0.0000 (0.1829)	MaskDICELoss 0.0000 (0.4103)
Epoch: [3][341/500]	Time  9.161 ( 9.161)	Loss 7.3414 (3.7572)	CeLoss 0.2246 (0.2644)	SegCLSLoss 0.0229 (0.0196)	KLLoss 0.2158 (0.1287)	MaskLoss 0.8884 (0.5315)	MaskBCELoss 0.0364 (0.1496)	MaskDICELoss 0.8519 (0.3819)
Epoch: [3][342/500]	Time  8.985 ( 8.985)	Loss 7.3599 (5.0979)	CeLoss 0.3105 (0.3488)	SegCLSLoss 0.0187 (0.0213)	KLLoss 0.1992 (0.1549)	MaskLoss 1.4253 (0.7097)	MaskBCELoss 0.7604 (0.1823)	MaskDICELoss 0.6650 (0.5274)
Epoch: [3][343/500]	Time  9.736 ( 9.736)	Loss 3.1741 (4.9404)	CeLoss 0.3281 (0.2276)	SegCLSLoss 0.0154 (0.0187)	KLLoss 0.0981 (0.1342)	MaskLoss 0.5765 (0.7628)	MaskBCELoss 0.3120 (0.2556)	MaskDICELoss 0.2646 (0.5072)
Epoch: [3][344/500]	Time 10.864 (10.864)	Loss 8.4660 (7.5560)	CeLoss 0.1904 (0.1995)	SegCLSLoss 0.0273 (0.0335)	KLLoss 0.2109 (0.2000)	MaskLoss 1.0362 (1.0399)	MaskBCELoss 0.0396 (0.1966)	MaskDICELoss 0.9966 (0.8433)
Epoch: [3][345/500]	Time  7.593 ( 7.593)	Loss 5.6129 (4.1497)	CeLoss 0.2334 (0.7314)	SegCLSLoss 0.0212 (0.0159)	KLLoss 0.1680 (0.1192)	MaskLoss 0.7623 (0.4767)	MaskBCELoss 0.1496 (0.0871)	MaskDICELoss 0.6127 (0.3896)
Epoch: [3][346/500]	Time  8.629 ( 8.629)	Loss 4.0498 (4.3633)	CeLoss 0.3086 (0.6763)	SegCLSLoss 0.0148 (0.0155)	KLLoss 0.1338 (0.1162)	MaskLoss 0.4771 (0.5685)	MaskBCELoss 0.0360 (0.1641)	MaskDICELoss 0.4411 (0.4044)
Epoch: [3][347/500]	Time 10.573 (10.573)	Loss 7.8856 (4.5830)	CeLoss 0.1885 (0.3619)	SegCLSLoss 0.0398 (0.0166)	KLLoss 0.2441 (0.1134)	MaskLoss 1.2088 (0.6288)	MaskBCELoss 0.3731 (0.1552)	MaskDICELoss 0.8358 (0.4736)
Epoch: [3][348/500]	Time 11.252 (11.252)	Loss 6.3226 (4.8286)	CeLoss 0.2969 (0.2902)	SegCLSLoss 0.0193 (0.0180)	KLLoss 0.1699 (0.1269)	MaskLoss 0.7733 (0.7108)	MaskBCELoss 0.0567 (0.2140)	MaskDICELoss 0.7166 (0.4969)
Epoch: [3][349/500]	Time 10.394 (10.394)	Loss 8.4202 (6.3179)	CeLoss 0.2256 (0.3625)	SegCLSLoss 0.0233 (0.0247)	KLLoss 0.2002 (0.1734)	MaskLoss 1.0689 (0.8181)	MaskBCELoss 0.0948 (0.1293)	MaskDICELoss 0.9741 (0.6889)
[2025-03-04 07:11:21,680] [INFO] [logging.py:96:log_dist] [Rank 0] step=1850, skipped=0, lr=[0.0002566987951807229], mom=[(0.9, 0.95)]
[2025-03-04 07:11:21,685] [INFO] [timer.py:215:stop] epoch=0/micro_step=18500/global_step=1850, RunningAvgSamplesPerSec=1.0455226530556687, CurrSamplesPerSec=1.2392624978423799, MemAllocated=57.67GB, MaxMemAllocated=62.86GB
Epoch: [3][350/500]	Time  8.071 ( 8.071)	Loss 0.2500 (3.7811)	CeLoss 0.2500 (0.6621)	SegCLSLoss 0.0000 (0.0152)	KLLoss 0.0000 (0.0918)	MaskLoss 0.0000 (0.4744)	MaskBCELoss 0.0000 (0.1292)	MaskDICELoss 0.0000 (0.3452)
Epoch: [3][351/500]	Time 10.512 (10.512)	Loss 3.4848 (4.9169)	CeLoss 0.2422 (0.2316)	SegCLSLoss 0.0303 (0.0193)	KLLoss 0.2178 (0.1388)	MaskLoss 0.5855 (0.6767)	MaskBCELoss 0.2793 (0.1461)	MaskDICELoss 0.3062 (0.5306)
Epoch: [3][352/500]	Time 10.891 (10.891)	Loss 9.7129 (4.9620)	CeLoss 0.2461 (0.2076)	SegCLSLoss 0.0184 (0.0177)	KLLoss 0.1973 (0.1394)	MaskLoss 2.7783 (0.8495)	MaskBCELoss 2.1611 (0.3650)	MaskDICELoss 0.6172 (0.4845)
Epoch: [3][353/500]	Time 10.025 (10.025)	Loss 0.9922 (6.0517)	CeLoss 0.9922 (0.4408)	SegCLSLoss 0.0000 (0.0275)	KLLoss 0.0000 (0.1513)	MaskLoss 0.0000 (0.8700)	MaskBCELoss 0.0000 (0.2524)	MaskDICELoss 0.0000 (0.6177)
Epoch: [3][354/500]	Time  8.910 ( 8.910)	Loss 7.3200 (4.8210)	CeLoss 0.2949 (0.4840)	SegCLSLoss 0.0170 (0.0194)	KLLoss 0.1357 (0.1365)	MaskLoss 0.9997 (0.5948)	MaskBCELoss 0.1858 (0.0945)	MaskDICELoss 0.8139 (0.5003)
Epoch: [3][355/500]	Time 10.257 (10.257)	Loss 0.5703 (4.4708)	CeLoss 0.5703 (0.4713)	SegCLSLoss 0.0000 (0.0149)	KLLoss 0.0000 (0.1135)	MaskLoss 0.0000 (0.6309)	MaskBCELoss 0.0000 (0.1948)	MaskDICELoss 0.0000 (0.4361)
Epoch: [3][356/500]	Time  9.609 ( 9.609)	Loss 1.5547 (5.6750)	CeLoss 1.5547 (0.3337)	SegCLSLoss 0.0000 (0.0212)	KLLoss 0.0000 (0.1410)	MaskLoss 0.0000 (0.8378)	MaskBCELoss 0.0000 (0.2521)	MaskDICELoss 0.0000 (0.5857)
Epoch: [3][357/500]	Time 10.111 (10.111)	Loss 7.8200 (4.1710)	CeLoss 0.3242 (0.4725)	SegCLSLoss 0.0215 (0.0168)	KLLoss 0.1680 (0.1215)	MaskLoss 1.0022 (0.5345)	MaskBCELoss 0.1169 (0.1179)	MaskDICELoss 0.8853 (0.4166)
Epoch: [3][358/500]	Time  9.040 ( 9.040)	Loss 0.3828 (4.3557)	CeLoss 0.3828 (0.3686)	SegCLSLoss 0.0000 (0.0204)	KLLoss 0.0000 (0.1358)	MaskLoss 0.0000 (0.6242)	MaskBCELoss 0.0000 (0.1920)	MaskDICELoss 0.0000 (0.4322)
Epoch: [3][359/500]	Time  8.725 ( 8.725)	Loss 8.0953 (4.4934)	CeLoss 0.2041 (0.7664)	SegCLSLoss 0.0322 (0.0141)	KLLoss 0.2266 (0.1038)	MaskLoss 0.9972 (0.5380)	MaskBCELoss 0.0550 (0.1146)	MaskDICELoss 0.9423 (0.4234)
[2025-03-04 07:12:59,864] [INFO] [logging.py:96:log_dist] [Rank 0] step=1860, skipped=0, lr=[0.0002564578313253012], mom=[(0.9, 0.95)]
[2025-03-04 07:12:59,871] [INFO] [timer.py:215:stop] epoch=0/micro_step=18600/global_step=1860, RunningAvgSamplesPerSec=1.0453742560571777, CurrSamplesPerSec=0.9897052161707338, MemAllocated=56.71GB, MaxMemAllocated=62.86GB
Epoch: [3][360/500]	Time 10.106 (10.106)	Loss 1.8594 (4.1526)	CeLoss 1.8594 (0.6080)	SegCLSLoss 0.0000 (0.0165)	KLLoss 0.0000 (0.1048)	MaskLoss 0.0000 (0.4949)	MaskBCELoss 0.0000 (0.0880)	MaskDICELoss 0.0000 (0.4069)
Epoch: [3][361/500]	Time  9.158 ( 9.158)	Loss 1.1406 (4.9487)	CeLoss 1.1406 (0.5056)	SegCLSLoss 0.0000 (0.0188)	KLLoss 0.0000 (0.1278)	MaskLoss 0.0000 (0.6961)	MaskBCELoss 0.0000 (0.2105)	MaskDICELoss 0.0000 (0.4856)
Epoch: [3][362/500]	Time 11.240 (11.240)	Loss 7.8766 (5.4778)	CeLoss 0.1924 (0.1932)	SegCLSLoss 0.0190 (0.0273)	KLLoss 0.1680 (0.1646)	MaskLoss 1.5391 (0.8729)	MaskBCELoss 0.8013 (0.3128)	MaskDICELoss 0.7379 (0.5601)
Epoch: [3][363/500]	Time  9.755 ( 9.755)	Loss 0.0742 (5.0332)	CeLoss 0.0742 (0.2067)	SegCLSLoss 0.0000 (0.0214)	KLLoss 0.0000 (0.1595)	MaskLoss 0.0000 (0.6477)	MaskBCELoss 0.0000 (0.0875)	MaskDICELoss 0.0000 (0.5602)
Epoch: [3][364/500]	Time  6.982 ( 6.982)	Loss 6.2374 (4.0302)	CeLoss 0.1924 (0.6438)	SegCLSLoss 0.0415 (0.0157)	KLLoss 0.2637 (0.1209)	MaskLoss 0.9588 (0.5850)	MaskBCELoss 0.3183 (0.2371)	MaskDICELoss 0.6405 (0.3479)
Epoch: [3][365/500]	Time  9.273 ( 9.273)	Loss 8.4364 (6.3907)	CeLoss 0.1602 (0.3535)	SegCLSLoss 0.0430 (0.0308)	KLLoss 0.2314 (0.1958)	MaskLoss 1.2961 (0.9605)	MaskBCELoss 0.3911 (0.3097)	MaskDICELoss 0.9050 (0.6508)
Epoch: [3][366/500]	Time  9.898 ( 9.898)	Loss 7.9439 (5.2219)	CeLoss 0.2363 (0.4991)	SegCLSLoss 0.0308 (0.0198)	KLLoss 0.2061 (0.1278)	MaskLoss 1.0098 (0.6616)	MaskBCELoss 0.0986 (0.1179)	MaskDICELoss 0.9112 (0.5437)
Epoch: [3][367/500]	Time 10.228 (10.228)	Loss 8.9444 (5.4849)	CeLoss 0.1826 (0.3135)	SegCLSLoss 0.0261 (0.0197)	KLLoss 0.1865 (0.1449)	MaskLoss 1.3011 (0.8458)	MaskBCELoss 0.3079 (0.2917)	MaskDICELoss 0.9932 (0.5541)
Epoch: [3][368/500]	Time  9.158 ( 9.158)	Loss 1.3047 (4.3176)	CeLoss 1.3047 (0.4627)	SegCLSLoss 0.0000 (0.0176)	KLLoss 0.0000 (0.1156)	MaskLoss 0.0000 (0.5848)	MaskBCELoss 0.0000 (0.1580)	MaskDICELoss 0.0000 (0.4268)
Epoch: [3][369/500]	Time  9.108 ( 9.108)	Loss 3.1301 (5.6993)	CeLoss 0.2363 (0.4336)	SegCLSLoss 0.0212 (0.0228)	KLLoss 0.1826 (0.1479)	MaskLoss 0.3635 (0.7205)	MaskBCELoss 0.0346 (0.1096)	MaskDICELoss 0.3289 (0.6109)
[2025-03-04 07:14:35,161] [INFO] [logging.py:96:log_dist] [Rank 0] step=1870, skipped=0, lr=[0.0002562168674698795], mom=[(0.9, 0.95)]
[2025-03-04 07:14:35,167] [INFO] [timer.py:215:stop] epoch=0/micro_step=18700/global_step=1870, RunningAvgSamplesPerSec=1.0453965582179816, CurrSamplesPerSec=0.9529605593350353, MemAllocated=57.18GB, MaxMemAllocated=62.86GB
Epoch: [3][370/500]	Time 10.496 (10.496)	Loss 7.0528 (5.6933)	CeLoss 0.1011 (0.3246)	SegCLSLoss 0.0889 (0.0269)	KLLoss 0.2578 (0.1542)	MaskLoss 1.1842 (0.7766)	MaskBCELoss 0.4705 (0.1685)	MaskDICELoss 0.7137 (0.6081)
Epoch: [3][371/500]	Time  9.680 ( 9.680)	Loss 0.0625 (5.0237)	CeLoss 0.0625 (0.3480)	SegCLSLoss 0.0000 (0.0213)	KLLoss 0.0000 (0.1426)	MaskLoss 0.0000 (0.6076)	MaskBCELoss 0.0000 (0.0563)	MaskDICELoss 0.0000 (0.5513)
Epoch: [3][372/500]	Time  8.916 ( 8.916)	Loss 1.8438 (5.8601)	CeLoss 1.8438 (0.5429)	SegCLSLoss 0.0000 (0.0181)	KLLoss 0.0000 (0.1174)	MaskLoss 0.0000 (0.8345)	MaskBCELoss 0.0000 (0.2475)	MaskDICELoss 0.0000 (0.5870)
Epoch: [3][373/500]	Time 10.963 (10.963)	Loss 3.3867 (5.8910)	CeLoss 0.2412 (0.3253)	SegCLSLoss 0.0181 (0.0265)	KLLoss 0.1523 (0.1688)	MaskLoss 0.4382 (0.8067)	MaskBCELoss 0.0869 (0.1784)	MaskDICELoss 0.3513 (0.6283)
Epoch: [3][374/500]	Time 10.395 (10.395)	Loss 6.5715 (6.1426)	CeLoss 0.1836 (0.2642)	SegCLSLoss 0.0283 (0.0272)	KLLoss 0.1797 (0.1830)	MaskLoss 1.1052 (0.9394)	MaskBCELoss 0.4411 (0.3057)	MaskDICELoss 0.6640 (0.6337)
Epoch: [3][375/500]	Time  8.726 ( 8.726)	Loss 7.5229 (4.2743)	CeLoss 0.4121 (0.6068)	SegCLSLoss 0.0304 (0.0175)	KLLoss 0.2080 (0.1282)	MaskLoss 1.0805 (0.5690)	MaskBCELoss 0.2930 (0.1702)	MaskDICELoss 0.7875 (0.3987)
Epoch: [3][376/500]	Time 10.279 (10.279)	Loss 8.5412 (5.8620)	CeLoss 0.1846 (0.2043)	SegCLSLoss 0.0320 (0.0316)	KLLoss 0.2217 (0.1821)	MaskLoss 1.1276 (0.9311)	MaskBCELoss 0.1503 (0.3314)	MaskDICELoss 0.9773 (0.5996)
Epoch: [3][377/500]	Time  8.758 ( 8.758)	Loss 5.3742 (5.6108)	CeLoss 0.3164 (0.4583)	SegCLSLoss 0.0184 (0.0225)	KLLoss 0.0898 (0.1425)	MaskLoss 0.7905 (0.6884)	MaskBCELoss 0.2276 (0.0848)	MaskDICELoss 0.5629 (0.6037)
Epoch: [3][378/500]	Time 10.672 (10.672)	Loss 7.6691 (5.7905)	CeLoss 0.2314 (0.3396)	SegCLSLoss 0.0295 (0.0215)	KLLoss 0.1826 (0.1533)	MaskLoss 1.2221 (0.8047)	MaskBCELoss 0.4229 (0.1919)	MaskDICELoss 0.7992 (0.6129)
Epoch: [3][379/500]	Time 11.072 (11.072)	Loss 8.4609 (6.4892)	CeLoss 0.1816 (0.2075)	SegCLSLoss 0.0339 (0.0305)	KLLoss 0.2012 (0.1977)	MaskLoss 1.4152 (0.9424)	MaskBCELoss 0.5432 (0.2450)	MaskDICELoss 0.8720 (0.6974)
[2025-03-04 07:16:15,671] [INFO] [logging.py:96:log_dist] [Rank 0] step=1880, skipped=0, lr=[0.0002559759036144578], mom=[(0.9, 0.95)]
[2025-03-04 07:16:15,677] [INFO] [timer.py:215:stop] epoch=0/micro_step=18800/global_step=1880, RunningAvgSamplesPerSec=1.0451151116725892, CurrSamplesPerSec=0.9050714855388949, MemAllocated=57.26GB, MaxMemAllocated=62.86GB
Epoch: [3][380/500]	Time 11.051 (11.051)	Loss 7.3462 (6.4809)	CeLoss 0.2715 (0.3797)	SegCLSLoss 0.0211 (0.0273)	KLLoss 0.1523 (0.1674)	MaskLoss 0.9972 (0.8775)	MaskBCELoss 0.1774 (0.1833)	MaskDICELoss 0.8197 (0.6942)
Epoch: [3][381/500]	Time  9.131 ( 9.131)	Loss 7.9429 (4.9979)	CeLoss 0.2188 (0.3970)	SegCLSLoss 0.0352 (0.0254)	KLLoss 0.2129 (0.1374)	MaskLoss 0.9436 (0.6732)	MaskBCELoss 0.0092 (0.1558)	MaskDICELoss 0.9344 (0.5174)
Epoch: [3][382/500]	Time 11.119 (11.119)	Loss 2.0377 (5.2126)	CeLoss 0.2793 (0.2576)	SegCLSLoss 0.0165 (0.0202)	KLLoss 0.1250 (0.1646)	MaskLoss 0.3119 (0.7194)	MaskBCELoss 0.1449 (0.1624)	MaskDICELoss 0.1670 (0.5570)
Epoch: [3][383/500]	Time  8.997 ( 8.997)	Loss 7.5435 (3.2897)	CeLoss 0.1914 (0.5299)	SegCLSLoss 0.0203 (0.0092)	KLLoss 0.1680 (0.0690)	MaskLoss 1.9389 (0.5152)	MaskBCELoss 1.3894 (0.2393)	MaskDICELoss 0.5494 (0.2760)
Epoch: [3][384/500]	Time  7.889 ( 7.889)	Loss 3.9745 (5.0993)	CeLoss 0.3691 (0.3325)	SegCLSLoss 0.0153 (0.0226)	KLLoss 0.1543 (0.1669)	MaskLoss 0.5422 (0.6754)	MaskBCELoss 0.1491 (0.1357)	MaskDICELoss 0.3931 (0.5397)
Epoch: [3][385/500]	Time  9.676 ( 9.676)	Loss 0.1494 (4.2829)	CeLoss 0.1494 (0.3116)	SegCLSLoss 0.0000 (0.0199)	KLLoss 0.0000 (0.1343)	MaskLoss 0.0000 (0.5901)	MaskBCELoss 0.0000 (0.1490)	MaskDICELoss 0.0000 (0.4411)
Epoch: [3][386/500]	Time  7.334 ( 7.334)	Loss 0.9837 (4.2985)	CeLoss 0.3047 (0.5367)	SegCLSLoss 0.0209 (0.0174)	KLLoss 0.2168 (0.1262)	MaskLoss 0.1521 (0.5832)	MaskBCELoss 0.1274 (0.1732)	MaskDICELoss 0.0247 (0.4101)
Epoch: [3][387/500]	Time 11.005 (11.005)	Loss 3.0065 (5.5193)	CeLoss 0.2520 (0.3886)	SegCLSLoss 0.0181 (0.0214)	KLLoss 0.1553 (0.1443)	MaskLoss 0.4790 (0.7509)	MaskBCELoss 0.2073 (0.1720)	MaskDICELoss 0.2717 (0.5789)
Epoch: [3][388/500]	Time  7.405 ( 7.405)	Loss 8.1695 (4.6046)	CeLoss 0.1953 (0.6414)	SegCLSLoss 0.0280 (0.0192)	KLLoss 0.1748 (0.1250)	MaskLoss 1.1255 (0.6301)	MaskBCELoss 0.2028 (0.2020)	MaskDICELoss 0.9226 (0.4281)
Epoch: [3][389/500]	Time  8.855 ( 8.855)	Loss 0.7812 (4.1496)	CeLoss 0.7812 (0.5124)	SegCLSLoss 0.0000 (0.0178)	KLLoss 0.0000 (0.1152)	MaskLoss 0.0000 (0.7014)	MaskBCELoss 0.0000 (0.3497)	MaskDICELoss 0.0000 (0.3517)
[2025-03-04 07:17:45,640] [INFO] [logging.py:96:log_dist] [Rank 0] step=1890, skipped=0, lr=[0.0002557349397590361], mom=[(0.9, 0.95)]
[2025-03-04 07:17:45,646] [INFO] [timer.py:215:stop] epoch=0/micro_step=18900/global_step=1890, RunningAvgSamplesPerSec=1.0454468006282498, CurrSamplesPerSec=1.168829308614283, MemAllocated=56.73GB, MaxMemAllocated=62.86GB
Epoch: [3][390/500]	Time  8.557 ( 8.557)	Loss 1.9766 (4.1688)	CeLoss 1.9766 (0.8248)	SegCLSLoss 0.0000 (0.0181)	KLLoss 0.0000 (0.1155)	MaskLoss 0.0000 (0.4890)	MaskBCELoss 0.0000 (0.1154)	MaskDICELoss 0.0000 (0.3736)
Epoch: [3][391/500]	Time  9.229 ( 9.229)	Loss 3.9209 (3.0086)	CeLoss 0.3008 (0.4253)	SegCLSLoss 0.0182 (0.0144)	KLLoss 0.1572 (0.1049)	MaskLoss 0.5809 (0.4426)	MaskBCELoss 0.1992 (0.1784)	MaskDICELoss 0.3817 (0.2643)
Epoch: [3][392/500]	Time  9.635 ( 9.635)	Loss 8.0975 (5.3408)	CeLoss 0.2266 (0.4718)	SegCLSLoss 0.0265 (0.0193)	KLLoss 0.2988 (0.1421)	MaskLoss 0.9610 (0.7040)	MaskBCELoss 0.0215 (0.1524)	MaskDICELoss 0.9394 (0.5516)
Epoch: [3][393/500]	Time  8.401 ( 8.401)	Loss 1.4766 (4.5914)	CeLoss 1.4766 (0.5743)	SegCLSLoss 0.0000 (0.0156)	KLLoss 0.0000 (0.1204)	MaskLoss 0.0000 (0.6293)	MaskBCELoss 0.0000 (0.1910)	MaskDICELoss 0.0000 (0.4383)
Epoch: [3][394/500]	Time  8.197 ( 8.197)	Loss 1.5938 (4.1752)	CeLoss 1.5938 (0.6494)	SegCLSLoss 0.0000 (0.0149)	KLLoss 0.0000 (0.1126)	MaskLoss 0.0000 (0.5397)	MaskBCELoss 0.0000 (0.1519)	MaskDICELoss 0.0000 (0.3878)
Epoch: [3][395/500]	Time 10.109 (10.109)	Loss 4.8457 (6.2442)	CeLoss 0.2285 (0.3709)	SegCLSLoss 0.0160 (0.0260)	KLLoss 0.1377 (0.1712)	MaskLoss 0.6957 (0.9215)	MaskBCELoss 0.1825 (0.2804)	MaskDICELoss 0.5132 (0.6410)
Epoch: [3][396/500]	Time 10.463 (10.463)	Loss 3.9118 (5.8105)	CeLoss 0.2217 (0.2146)	SegCLSLoss 0.0175 (0.0262)	KLLoss 0.1855 (0.1682)	MaskLoss 0.6143 (0.9075)	MaskBCELoss 0.2365 (0.3077)	MaskDICELoss 0.3779 (0.5999)
Epoch: [3][397/500]	Time 10.580 (10.580)	Loss 5.5486 (4.5087)	CeLoss 0.2773 (0.3146)	SegCLSLoss 0.0194 (0.0172)	KLLoss 0.1719 (0.1412)	MaskLoss 0.7827 (0.6205)	MaskBCELoss 0.1953 (0.1534)	MaskDICELoss 0.5874 (0.4672)
Epoch: [3][398/500]	Time  9.981 ( 9.981)	Loss 9.1394 (5.4649)	CeLoss 0.4043 (0.2444)	SegCLSLoss 0.0184 (0.0172)	KLLoss 0.1484 (0.1324)	MaskLoss 1.5762 (0.7843)	MaskBCELoss 0.6722 (0.1992)	MaskDICELoss 0.9041 (0.5851)
Epoch: [3][399/500]	Time  8.028 ( 8.028)	Loss 1.4844 (3.3454)	CeLoss 1.4844 (0.8892)	SegCLSLoss 0.0000 (0.0110)	KLLoss 0.0000 (0.0826)	MaskLoss 0.0000 (0.3283)	MaskBCELoss 0.0000 (0.0430)	MaskDICELoss 0.0000 (0.2853)
[2025-03-04 07:19:18,397] [INFO] [logging.py:96:log_dist] [Rank 0] step=1900, skipped=0, lr=[0.0002554939759036144], mom=[(0.9, 0.95)]
[2025-03-04 07:19:18,403] [INFO] [timer.py:215:stop] epoch=0/micro_step=19000/global_step=1900, RunningAvgSamplesPerSec=1.0456144957205609, CurrSamplesPerSec=1.2297219431963, MemAllocated=57.1GB, MaxMemAllocated=62.86GB
Epoch: [3][400/500]	Time  8.134 ( 8.134)	Loss 2.5830 (3.9246)	CeLoss 0.2715 (0.6050)	SegCLSLoss 0.0176 (0.0113)	KLLoss 0.1426 (0.0856)	MaskLoss 0.3310 (0.5322)	MaskBCELoss 0.0814 (0.1716)	MaskDICELoss 0.2495 (0.3606)
Epoch: [3][401/500]	Time  7.382 ( 7.382)	Loss 3.7148 (3.9553)	CeLoss 0.2754 (0.5834)	SegCLSLoss 0.0168 (0.0169)	KLLoss 0.1445 (0.1067)	MaskLoss 0.6488 (0.5253)	MaskBCELoss 0.3172 (0.1576)	MaskDICELoss 0.3316 (0.3677)
Epoch: [3][402/500]	Time 11.562 (11.562)	Loss 5.4086 (5.5560)	CeLoss 0.2451 (0.3355)	SegCLSLoss 0.0172 (0.0205)	KLLoss 0.1494 (0.1535)	MaskLoss 0.8469 (0.7575)	MaskBCELoss 0.2948 (0.1673)	MaskDICELoss 0.5521 (0.5903)
Epoch: [3][403/500]	Time  9.280 ( 9.280)	Loss 9.1903 (4.5547)	CeLoss 0.3242 (0.4430)	SegCLSLoss 0.0309 (0.0193)	KLLoss 0.2598 (0.1344)	MaskLoss 1.5579 (0.6074)	MaskBCELoss 0.6457 (0.1486)	MaskDICELoss 0.9122 (0.4587)
Epoch: [3][404/500]	Time  8.928 ( 8.928)	Loss 4.7817 (5.2860)	CeLoss 0.2344 (0.4922)	SegCLSLoss 0.0327 (0.0272)	KLLoss 0.2051 (0.1630)	MaskLoss 0.7728 (0.7503)	MaskBCELoss 0.3093 (0.2308)	MaskDICELoss 0.4635 (0.5195)
Epoch: [3][405/500]	Time 10.942 (10.942)	Loss 0.1807 (4.5240)	CeLoss 0.1807 (0.3074)	SegCLSLoss 0.0000 (0.0184)	KLLoss 0.0000 (0.1184)	MaskLoss 0.0000 (0.6612)	MaskBCELoss 0.0000 (0.2001)	MaskDICELoss 0.0000 (0.4611)
Epoch: [3][406/500]	Time  8.049 ( 8.049)	Loss 4.8583 (3.4205)	CeLoss 0.1836 (0.4450)	SegCLSLoss 0.0352 (0.0136)	KLLoss 0.2031 (0.0975)	MaskLoss 1.0183 (0.4600)	MaskBCELoss 0.6153 (0.1348)	MaskDICELoss 0.4029 (0.3252)
Epoch: [3][407/500]	Time  9.114 ( 9.114)	Loss 4.1570 (4.7536)	CeLoss 0.2373 (0.4703)	SegCLSLoss 0.0178 (0.0254)	KLLoss 0.1562 (0.1586)	MaskLoss 0.5030 (0.6554)	MaskBCELoss 0.0449 (0.1885)	MaskDICELoss 0.4581 (0.4669)
Epoch: [3][408/500]	Time  9.326 ( 9.326)	Loss 8.1110 (5.5734)	CeLoss 0.2051 (0.4601)	SegCLSLoss 0.0204 (0.0164)	KLLoss 0.1846 (0.1357)	MaskLoss 1.3361 (0.7282)	MaskBCELoss 0.4960 (0.1427)	MaskDICELoss 0.8401 (0.5856)
Epoch: [3][409/500]	Time  8.777 ( 8.777)	Loss 1.3906 (4.3155)	CeLoss 1.3906 (0.6952)	SegCLSLoss 0.0000 (0.0170)	KLLoss 0.0000 (0.1054)	MaskLoss 0.0000 (0.5642)	MaskBCELoss 0.0000 (0.1680)	MaskDICELoss 0.0000 (0.3963)
[2025-03-04 07:20:51,709] [INFO] [logging.py:96:log_dist] [Rank 0] step=1910, skipped=0, lr=[0.00025525301204819273], mom=[(0.9, 0.95)]
[2025-03-04 07:20:51,715] [INFO] [timer.py:215:stop] epoch=0/micro_step=19100/global_step=1910, RunningAvgSamplesPerSec=1.045748744701809, CurrSamplesPerSec=1.0051314785152903, MemAllocated=57.25GB, MaxMemAllocated=62.86GB
Epoch: [3][410/500]	Time  9.951 ( 9.951)	Loss 7.4658 (5.9906)	CeLoss 0.2637 (0.3362)	SegCLSLoss 0.0141 (0.0208)	KLLoss 0.1094 (0.1556)	MaskLoss 1.1606 (0.7869)	MaskBCELoss 0.3666 (0.1345)	MaskDICELoss 0.7940 (0.6524)
Epoch: [3][411/500]	Time  8.705 ( 8.705)	Loss 6.3123 (6.1975)	CeLoss 0.2217 (0.4191)	SegCLSLoss 0.0302 (0.0207)	KLLoss 0.1602 (0.1478)	MaskLoss 0.7875 (0.7590)	MaskBCELoss 0.0641 (0.0752)	MaskDICELoss 0.7235 (0.6838)
Epoch: [3][412/500]	Time 11.209 (11.209)	Loss 5.5720 (5.7648)	CeLoss 0.2139 (0.2220)	SegCLSLoss 0.0225 (0.0198)	KLLoss 0.1309 (0.1442)	MaskLoss 0.6756 (0.7717)	MaskBCELoss 0.0313 (0.1309)	MaskDICELoss 0.6442 (0.6409)
Epoch: [3][413/500]	Time  7.988 ( 7.988)	Loss 7.6135 (5.4054)	CeLoss 0.2656 (0.7016)	SegCLSLoss 0.0293 (0.0201)	KLLoss 0.2051 (0.1134)	MaskLoss 1.4562 (0.7910)	MaskBCELoss 0.7537 (0.2913)	MaskDICELoss 0.7025 (0.4997)
Epoch: [3][414/500]	Time  9.375 ( 9.375)	Loss 7.1877 (5.5465)	CeLoss 0.1885 (0.5521)	SegCLSLoss 0.0278 (0.0159)	KLLoss 0.2031 (0.1275)	MaskLoss 0.9556 (0.6620)	MaskBCELoss 0.1440 (0.0728)	MaskDICELoss 0.8117 (0.5892)
Epoch: [3][415/500]	Time  9.911 ( 9.911)	Loss 8.1804 (5.6404)	CeLoss 0.2109 (0.3611)	SegCLSLoss 0.0306 (0.0235)	KLLoss 0.1992 (0.1727)	MaskLoss 1.3729 (0.8773)	MaskBCELoss 0.5380 (0.3207)	MaskDICELoss 0.8348 (0.5566)
Epoch: [3][416/500]	Time  9.950 ( 9.950)	Loss 1.5156 (4.0413)	CeLoss 1.5156 (0.4875)	SegCLSLoss 0.0000 (0.0144)	KLLoss 0.0000 (0.0951)	MaskLoss 0.0000 (0.5308)	MaskBCELoss 0.0000 (0.1324)	MaskDICELoss 0.0000 (0.3984)
Epoch: [3][417/500]	Time  9.207 ( 9.207)	Loss 0.7617 (4.7034)	CeLoss 0.7617 (0.5119)	SegCLSLoss 0.0000 (0.0187)	KLLoss 0.0000 (0.1302)	MaskLoss 0.0000 (0.6136)	MaskBCELoss 0.0000 (0.1428)	MaskDICELoss 0.0000 (0.4708)
Epoch: [3][418/500]	Time  9.519 ( 9.519)	Loss 6.9397 (6.4063)	CeLoss 0.6406 (0.5101)	SegCLSLoss 0.0280 (0.0209)	KLLoss 0.1953 (0.1434)	MaskLoss 1.5301 (0.9543)	MaskBCELoss 1.0254 (0.3153)	MaskDICELoss 0.5047 (0.6390)
Epoch: [3][419/500]	Time  9.733 ( 9.733)	Loss 7.1829 (6.1360)	CeLoss 0.2393 (0.2722)	SegCLSLoss 0.0240 (0.0246)	KLLoss 0.1826 (0.1663)	MaskLoss 0.9728 (0.9497)	MaskBCELoss 0.1722 (0.3188)	MaskDICELoss 0.8006 (0.6310)
[2025-03-04 07:22:27,004] [INFO] [logging.py:96:log_dist] [Rank 0] step=1920, skipped=0, lr=[0.00025501204819277105], mom=[(0.9, 0.95)]
[2025-03-04 07:22:27,009] [INFO] [timer.py:215:stop] epoch=0/micro_step=19200/global_step=1920, RunningAvgSamplesPerSec=1.04576849659357, CurrSamplesPerSec=1.0313765582394867, MemAllocated=57.27GB, MaxMemAllocated=62.86GB
Epoch: [3][420/500]	Time  9.698 ( 9.698)	Loss 6.8959 (6.0632)	CeLoss 0.2031 (0.2965)	SegCLSLoss 0.0251 (0.0257)	KLLoss 0.1934 (0.1553)	MaskLoss 0.8623 (0.8608)	MaskBCELoss 0.0688 (0.2147)	MaskDICELoss 0.7935 (0.6462)
Epoch: [3][421/500]	Time  9.715 ( 9.715)	Loss 6.2601 (6.4170)	CeLoss 0.2158 (0.2585)	SegCLSLoss 0.0275 (0.0248)	KLLoss 0.1846 (0.1777)	MaskLoss 0.8182 (1.0649)	MaskBCELoss 0.1166 (0.4251)	MaskDICELoss 0.7016 (0.6398)
Epoch: [3][422/500]	Time 10.260 (10.260)	Loss 6.4902 (4.8099)	CeLoss 0.2412 (0.5131)	SegCLSLoss 0.0181 (0.0174)	KLLoss 0.0859 (0.1110)	MaskLoss 1.3333 (0.6503)	MaskBCELoss 0.7520 (0.1709)	MaskDICELoss 0.5813 (0.4794)
Epoch: [3][423/500]	Time  9.594 ( 9.594)	Loss 8.3902 (5.8344)	CeLoss 0.2041 (0.3590)	SegCLSLoss 0.0254 (0.0243)	KLLoss 0.1855 (0.1444)	MaskLoss 1.0467 (0.8220)	MaskBCELoss 0.0643 (0.2096)	MaskDICELoss 0.9824 (0.6124)
Epoch: [3][424/500]	Time 10.148 (10.148)	Loss 2.5632 (5.0110)	CeLoss 0.6289 (0.3061)	SegCLSLoss 0.0150 (0.0244)	KLLoss 0.0923 (0.1685)	MaskLoss 0.4434 (0.7597)	MaskBCELoss 0.2858 (0.2590)	MaskDICELoss 0.1576 (0.5007)
Epoch: [3][425/500]	Time 10.568 (10.568)	Loss 3.1074 (4.4743)	CeLoss 0.2051 (0.2995)	SegCLSLoss 0.0364 (0.0193)	KLLoss 0.2217 (0.1312)	MaskLoss 0.4185 (0.6375)	MaskBCELoss 0.1143 (0.1777)	MaskDICELoss 0.3042 (0.4598)
Epoch: [3][426/500]	Time  8.862 ( 8.862)	Loss 1.6094 (5.6897)	CeLoss 1.6094 (0.4647)	SegCLSLoss 0.0000 (0.0229)	KLLoss 0.0000 (0.1508)	MaskLoss 0.0000 (0.8536)	MaskBCELoss 0.0000 (0.2944)	MaskDICELoss 0.0000 (0.5593)
Epoch: [3][427/500]	Time  9.389 ( 9.389)	Loss 6.5423 (5.4962)	CeLoss 0.3262 (0.4504)	SegCLSLoss 0.0154 (0.0268)	KLLoss 0.1211 (0.1534)	MaskLoss 0.9493 (0.7602)	MaskBCELoss 0.2512 (0.2005)	MaskDICELoss 0.6981 (0.5597)
Epoch: [3][428/500]	Time  9.037 ( 9.037)	Loss 7.3556 (4.1721)	CeLoss 0.2402 (0.5734)	SegCLSLoss 0.0237 (0.0127)	KLLoss 0.1699 (0.0939)	MaskLoss 0.9055 (0.5559)	MaskBCELoss 0.0517 (0.1581)	MaskDICELoss 0.8538 (0.3977)
Epoch: [3][429/500]	Time 11.189 (11.189)	Loss 7.6662 (5.5228)	CeLoss 0.2197 (0.3104)	SegCLSLoss 0.0272 (0.0220)	KLLoss 0.1914 (0.1543)	MaskLoss 0.9955 (0.7745)	MaskBCELoss 0.1207 (0.1915)	MaskDICELoss 0.8749 (0.5830)
[2025-03-04 07:24:03,402] [INFO] [logging.py:96:log_dist] [Rank 0] step=1930, skipped=0, lr=[0.0002547710843373494], mom=[(0.9, 0.95)]
[2025-03-04 07:24:03,408] [INFO] [timer.py:215:stop] epoch=0/micro_step=19300/global_step=1930, RunningAvgSamplesPerSec=1.0457254672271519, CurrSamplesPerSec=1.310119637676035, MemAllocated=56.74GB, MaxMemAllocated=62.86GB
Epoch: [3][430/500]	Time  7.635 ( 7.635)	Loss 1.0938 (3.9572)	CeLoss 1.0938 (0.8352)	SegCLSLoss 0.0000 (0.0126)	KLLoss 0.0000 (0.0742)	MaskLoss 0.0000 (0.5117)	MaskBCELoss 0.0000 (0.1753)	MaskDICELoss 0.0000 (0.3364)
Epoch: [3][431/500]	Time  8.698 ( 8.698)	Loss 1.3438 (5.0195)	CeLoss 1.3438 (0.4289)	SegCLSLoss 0.0000 (0.0168)	KLLoss 0.0000 (0.1262)	MaskLoss 0.0000 (0.6255)	MaskBCELoss 0.0000 (0.0912)	MaskDICELoss 0.0000 (0.5343)
Epoch: [3][432/500]	Time  8.894 ( 8.894)	Loss 0.0977 (4.6353)	CeLoss 0.0977 (0.4279)	SegCLSLoss 0.0000 (0.0132)	KLLoss 0.0000 (0.0725)	MaskLoss 0.0000 (0.6071)	MaskBCELoss 0.0000 (0.1214)	MaskDICELoss 0.0000 (0.4857)
Epoch: [3][433/500]	Time  9.604 ( 9.604)	Loss 4.8537 (4.3167)	CeLoss 0.2490 (0.5416)	SegCLSLoss 0.0197 (0.0156)	KLLoss 0.1895 (0.1185)	MaskLoss 0.8328 (0.5892)	MaskBCELoss 0.3764 (0.1775)	MaskDICELoss 0.4565 (0.4117)
Epoch: [3][434/500]	Time 11.036 (11.036)	Loss 6.7553 (6.0166)	CeLoss 0.2617 (0.2614)	SegCLSLoss 0.0253 (0.0200)	KLLoss 0.1592 (0.1484)	MaskLoss 0.8423 (0.8690)	MaskBCELoss 0.0695 (0.2259)	MaskDICELoss 0.7728 (0.6431)
Epoch: [3][435/500]	Time 11.229 (11.229)	Loss 5.1929 (6.6559)	CeLoss 0.2793 (0.2501)	SegCLSLoss 0.0178 (0.0283)	KLLoss 0.1660 (0.1812)	MaskLoss 0.7475 (1.0086)	MaskBCELoss 0.2070 (0.3096)	MaskDICELoss 0.5405 (0.6989)
Epoch: [3][436/500]	Time 11.961 (11.961)	Loss 4.1503 (6.4634)	CeLoss 0.2500 (0.2328)	SegCLSLoss 0.0170 (0.0277)	KLLoss 0.1348 (0.1795)	MaskLoss 0.5492 (0.8945)	MaskBCELoss 0.1059 (0.1864)	MaskDICELoss 0.4432 (0.7081)
Epoch: [3][437/500]	Time  8.885 ( 8.885)	Loss 6.8139 (5.1321)	CeLoss 0.3398 (0.5813)	SegCLSLoss 0.0164 (0.0195)	KLLoss 0.1318 (0.1164)	MaskLoss 1.1378 (0.6944)	MaskBCELoss 0.4615 (0.1885)	MaskDICELoss 0.6763 (0.5059)
Epoch: [3][438/500]	Time  9.132 ( 9.132)	Loss 6.8953 (5.0609)	CeLoss 0.1992 (0.5487)	SegCLSLoss 0.0283 (0.0214)	KLLoss 0.1865 (0.1327)	MaskLoss 0.8619 (0.6635)	MaskBCELoss 0.0664 (0.1565)	MaskDICELoss 0.7955 (0.5070)
Epoch: [3][439/500]	Time  8.131 ( 8.131)	Loss 5.8037 (3.7404)	CeLoss 0.1924 (0.6687)	SegCLSLoss 0.0339 (0.0168)	KLLoss 0.2158 (0.0920)	MaskLoss 0.7232 (0.4703)	MaskBCELoss 0.0680 (0.1319)	MaskDICELoss 0.6553 (0.3384)
[2025-03-04 07:25:42,011] [INFO] [logging.py:96:log_dist] [Rank 0] step=1940, skipped=0, lr=[0.0002545301204819277], mom=[(0.9, 0.95)]
[2025-03-04 07:25:42,016] [INFO] [timer.py:215:stop] epoch=0/micro_step=19400/global_step=1940, RunningAvgSamplesPerSec=1.0455581991385583, CurrSamplesPerSec=0.9060976100566867, MemAllocated=57.58GB, MaxMemAllocated=62.86GB
Epoch: [3][440/500]	Time 11.038 (11.038)	Loss 6.7807 (5.7134)	CeLoss 0.1992 (0.2160)	SegCLSLoss 0.0256 (0.0292)	KLLoss 0.1963 (0.1683)	MaskLoss 0.8211 (0.8795)	MaskBCELoss 0.0327 (0.2870)	MaskDICELoss 0.7884 (0.5925)
Epoch: [3][441/500]	Time  8.868 ( 8.868)	Loss 0.4238 (4.2506)	CeLoss 0.4238 (0.5439)	SegCLSLoss 0.0000 (0.0131)	KLLoss 0.0000 (0.0927)	MaskLoss 0.0000 (0.5147)	MaskBCELoss 0.0000 (0.0849)	MaskDICELoss 0.0000 (0.4298)
Epoch: [3][442/500]	Time  8.671 ( 8.671)	Loss 7.8676 (2.9471)	CeLoss 0.3379 (0.5145)	SegCLSLoss 0.0192 (0.0118)	KLLoss 0.1602 (0.0841)	MaskLoss 1.0033 (0.3781)	MaskBCELoss 0.1110 (0.1137)	MaskDICELoss 0.8922 (0.2644)
Epoch: [3][443/500]	Time  9.703 ( 9.703)	Loss 8.4826 (5.2307)	CeLoss 0.2314 (0.2521)	SegCLSLoss 0.0201 (0.0199)	KLLoss 0.1260 (0.1472)	MaskLoss 1.8009 (0.7515)	MaskBCELoss 1.0486 (0.1985)	MaskDICELoss 0.7523 (0.5531)
Epoch: [3][444/500]	Time 10.683 (10.683)	Loss 6.0890 (4.9984)	CeLoss 0.2021 (0.2278)	SegCLSLoss 0.0154 (0.0198)	KLLoss 0.0991 (0.1322)	MaskLoss 0.9089 (0.6844)	MaskBCELoss 0.2485 (0.1410)	MaskDICELoss 0.6604 (0.5433)
Epoch: [3][445/500]	Time 10.037 (10.037)	Loss 6.5504 (5.5238)	CeLoss 0.2207 (0.3324)	SegCLSLoss 0.0204 (0.0194)	KLLoss 0.1523 (0.1357)	MaskLoss 0.9296 (0.7883)	MaskBCELoss 0.2115 (0.2100)	MaskDICELoss 0.7181 (0.5782)
Epoch: [3][446/500]	Time 10.674 (10.674)	Loss 5.9233 (5.1525)	CeLoss 0.2432 (0.3243)	SegCLSLoss 0.0186 (0.0180)	KLLoss 0.1118 (0.1201)	MaskLoss 0.8518 (0.7097)	MaskBCELoss 0.2090 (0.1631)	MaskDICELoss 0.6427 (0.5466)
Epoch: [3][447/500]	Time 11.197 (11.197)	Loss 5.4166 (4.7981)	CeLoss 0.2080 (0.3527)	SegCLSLoss 0.0156 (0.0194)	KLLoss 0.1025 (0.1291)	MaskLoss 0.7050 (0.6491)	MaskBCELoss 0.0903 (0.1477)	MaskDICELoss 0.6147 (0.5014)
Epoch: [3][448/500]	Time  9.220 ( 9.220)	Loss 1.8906 (4.9499)	CeLoss 1.8906 (0.4294)	SegCLSLoss 0.0000 (0.0168)	KLLoss 0.0000 (0.1231)	MaskLoss 0.0000 (0.6927)	MaskBCELoss 0.0000 (0.1920)	MaskDICELoss 0.0000 (0.5007)
Epoch: [3][449/500]	Time 10.347 (10.347)	Loss 7.4354 (6.4816)	CeLoss 0.2930 (0.2323)	SegCLSLoss 0.0184 (0.0288)	KLLoss 0.1621 (0.1813)	MaskLoss 1.0598 (0.9294)	MaskBCELoss 0.2513 (0.2303)	MaskDICELoss 0.8085 (0.6991)
[2025-03-04 07:27:21,228] [INFO] [logging.py:96:log_dist] [Rank 0] step=1950, skipped=0, lr=[0.000254289156626506], mom=[(0.9, 0.95)]
[2025-03-04 07:27:21,234] [INFO] [timer.py:215:stop] epoch=0/micro_step=19500/global_step=1950, RunningAvgSamplesPerSec=1.0453584672800342, CurrSamplesPerSec=1.0186580141251445, MemAllocated=56.8GB, MaxMemAllocated=62.86GB
Epoch: [3][450/500]	Time  9.819 ( 9.819)	Loss 7.5798 (5.4521)	CeLoss 0.6875 (0.4016)	SegCLSLoss 0.0234 (0.0182)	KLLoss 0.1201 (0.1256)	MaskLoss 1.6938 (0.8676)	MaskBCELoss 1.1318 (0.3376)	MaskDICELoss 0.5620 (0.5301)
Epoch: [3][451/500]	Time  9.567 ( 9.567)	Loss 8.4409 (6.1541)	CeLoss 0.2578 (0.2423)	SegCLSLoss 0.0356 (0.0237)	KLLoss 0.2158 (0.1523)	MaskLoss 1.2540 (0.9708)	MaskBCELoss 0.3472 (0.3365)	MaskDICELoss 0.9068 (0.6343)
Epoch: [3][452/500]	Time  7.846 ( 7.846)	Loss 8.1609 (4.8180)	CeLoss 0.2139 (0.5809)	SegCLSLoss 0.0302 (0.0150)	KLLoss 0.2002 (0.1032)	MaskLoss 1.3337 (0.6365)	MaskBCELoss 0.4897 (0.1609)	MaskDICELoss 0.8440 (0.4756)
Epoch: [3][453/500]	Time 11.670 (11.670)	Loss 0.0664 (4.7211)	CeLoss 0.0664 (0.2029)	SegCLSLoss 0.0000 (0.0176)	KLLoss 0.0000 (0.1128)	MaskLoss 0.0000 (0.6826)	MaskBCELoss 0.0000 (0.1774)	MaskDICELoss 0.0000 (0.5052)
Epoch: [3][454/500]	Time  9.414 ( 9.414)	Loss 8.4418 (4.5943)	CeLoss 0.2334 (0.5908)	SegCLSLoss 0.0267 (0.0167)	KLLoss 0.1914 (0.1106)	MaskLoss 1.0699 (0.6007)	MaskBCELoss 0.0925 (0.1535)	MaskDICELoss 0.9774 (0.4472)
Epoch: [3][455/500]	Time  9.901 ( 9.901)	Loss 8.1113 (6.1810)	CeLoss 0.1357 (0.3486)	SegCLSLoss 0.0442 (0.0299)	KLLoss 0.2656 (0.1583)	MaskLoss 1.1080 (0.8862)	MaskBCELoss 0.1961 (0.2384)	MaskDICELoss 0.9119 (0.6478)
Epoch: [3][456/500]	Time 10.209 (10.209)	Loss 7.7447 (5.2580)	CeLoss 0.2949 (0.4086)	SegCLSLoss 0.0256 (0.0171)	KLLoss 0.1777 (0.1126)	MaskLoss 1.2570 (0.7143)	MaskBCELoss 0.4663 (0.1643)	MaskDICELoss 0.7907 (0.5499)
Epoch: [3][457/500]	Time  8.928 ( 8.928)	Loss 5.3397 (4.4361)	CeLoss 0.2559 (0.3764)	SegCLSLoss 0.0192 (0.0235)	KLLoss 0.1426 (0.1365)	MaskLoss 0.7139 (0.5779)	MaskBCELoss 0.1299 (0.1185)	MaskDICELoss 0.5840 (0.4593)
Epoch: [3][458/500]	Time  9.259 ( 9.259)	Loss 1.7188 (5.4218)	CeLoss 1.7188 (0.5474)	SegCLSLoss 0.0000 (0.0182)	KLLoss 0.0000 (0.1244)	MaskLoss 0.0000 (0.7216)	MaskBCELoss 0.0000 (0.1720)	MaskDICELoss 0.0000 (0.5496)
Epoch: [3][459/500]	Time  9.286 ( 9.286)	Loss 7.4717 (4.9363)	CeLoss 0.1768 (0.5657)	SegCLSLoss 0.0352 (0.0209)	KLLoss 0.1973 (0.1274)	MaskLoss 1.1065 (0.6197)	MaskBCELoss 0.2951 (0.1208)	MaskDICELoss 0.8114 (0.4989)
[2025-03-04 07:28:54,738] [INFO] [logging.py:96:log_dist] [Rank 0] step=1960, skipped=0, lr=[0.00025404819277108435], mom=[(0.9, 0.95)]
[2025-03-04 07:28:54,743] [INFO] [timer.py:215:stop] epoch=0/micro_step=19600/global_step=1960, RunningAvgSamplesPerSec=1.0454794616871714, CurrSamplesPerSec=1.346504449787981, MemAllocated=56.74GB, MaxMemAllocated=62.86GB
Epoch: [3][460/500]	Time  7.429 ( 7.429)	Loss 0.7617 (4.5163)	CeLoss 0.7617 (0.6448)	SegCLSLoss 0.0000 (0.0150)	KLLoss 0.0000 (0.1035)	MaskLoss 0.0000 (0.5380)	MaskBCELoss 0.0000 (0.0906)	MaskDICELoss 0.0000 (0.4474)
Epoch: [3][461/500]	Time  8.965 ( 8.965)	Loss 5.1273 (5.0146)	CeLoss 0.3340 (0.3863)	SegCLSLoss 0.0173 (0.0210)	KLLoss 0.1108 (0.1247)	MaskLoss 0.7421 (0.6965)	MaskBCELoss 0.2104 (0.1798)	MaskDICELoss 0.5317 (0.5167)
Epoch: [3][462/500]	Time 10.586 (10.586)	Loss 7.9154 (5.5734)	CeLoss 0.1885 (0.3617)	SegCLSLoss 0.0337 (0.0228)	KLLoss 0.2051 (0.1332)	MaskLoss 0.9804 (0.6930)	MaskBCELoss 0.0564 (0.0794)	MaskDICELoss 0.9241 (0.6136)
Epoch: [3][463/500]	Time  7.744 ( 7.744)	Loss 1.5781 (3.8458)	CeLoss 1.5781 (0.6448)	SegCLSLoss 0.0000 (0.0118)	KLLoss 0.0000 (0.0696)	MaskLoss 0.0000 (0.4698)	MaskBCELoss 0.0000 (0.1055)	MaskDICELoss 0.0000 (0.3643)
Epoch: [3][464/500]	Time  8.407 ( 8.407)	Loss 4.4929 (4.5325)	CeLoss 0.2539 (0.6517)	SegCLSLoss 0.0168 (0.0189)	KLLoss 0.0991 (0.1015)	MaskLoss 0.6671 (0.6110)	MaskBCELoss 0.2008 (0.1863)	MaskDICELoss 0.4662 (0.4247)
Epoch: [3][465/500]	Time 10.125 (10.125)	Loss 9.0471 (6.3579)	CeLoss 0.3984 (0.2221)	SegCLSLoss 0.0304 (0.0334)	KLLoss 0.1973 (0.1983)	MaskLoss 1.4458 (0.9490)	MaskBCELoss 0.5215 (0.2786)	MaskDICELoss 0.9243 (0.6704)
Epoch: [3][466/500]	Time 10.679 (10.679)	Loss 6.7716 (3.8768)	CeLoss 0.2480 (0.3644)	SegCLSLoss 0.0210 (0.0141)	KLLoss 0.1738 (0.0936)	MaskLoss 0.9098 (0.4891)	MaskBCELoss 0.1564 (0.0835)	MaskDICELoss 0.7534 (0.4056)
Epoch: [3][467/500]	Time  9.379 ( 9.379)	Loss 8.0155 (5.0287)	CeLoss 0.4043 (0.4104)	SegCLSLoss 0.0260 (0.0177)	KLLoss 0.2031 (0.1218)	MaskLoss 1.1930 (0.6798)	MaskBCELoss 0.3583 (0.1584)	MaskDICELoss 0.8347 (0.5214)
Epoch: [3][468/500]	Time  8.041 ( 8.041)	Loss 1.2266 (4.4516)	CeLoss 1.2266 (0.6040)	SegCLSLoss 0.0000 (0.0117)	KLLoss 0.0000 (0.0764)	MaskLoss 0.0000 (0.6298)	MaskBCELoss 0.0000 (0.2121)	MaskDICELoss 0.0000 (0.4176)
Epoch: [3][469/500]	Time  9.241 ( 9.241)	Loss 7.1542 (5.5182)	CeLoss 0.2178 (0.2757)	SegCLSLoss 0.0203 (0.0251)	KLLoss 0.1611 (0.1435)	MaskLoss 0.8671 (0.7901)	MaskBCELoss 0.0285 (0.2058)	MaskDICELoss 0.8386 (0.5843)
[2025-03-04 07:30:28,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=1970, skipped=0, lr=[0.0002538072289156626], mom=[(0.9, 0.95)]
[2025-03-04 07:30:28,328] [INFO] [timer.py:215:stop] epoch=0/micro_step=19700/global_step=1970, RunningAvgSamplesPerSec=1.0455950907872522, CurrSamplesPerSec=0.9600623770996352, MemAllocated=57.25GB, MaxMemAllocated=62.86GB
Epoch: [3][470/500]	Time 10.419 (10.419)	Loss 8.8047 (6.4860)	CeLoss 0.3223 (0.2459)	SegCLSLoss 0.0237 (0.0247)	KLLoss 0.1934 (0.1527)	MaskLoss 1.1469 (0.9469)	MaskBCELoss 0.1496 (0.2500)	MaskDICELoss 0.9973 (0.6969)
Epoch: [3][471/500]	Time 11.500 (11.500)	Loss 9.2508 (6.0123)	CeLoss 0.3105 (0.2544)	SegCLSLoss 0.0376 (0.0204)	KLLoss 0.2266 (0.1288)	MaskLoss 1.6155 (0.8827)	MaskBCELoss 0.7047 (0.2405)	MaskDICELoss 0.9109 (0.6422)
Epoch: [3][472/500]	Time 11.454 (11.454)	Loss 5.1979 (6.2019)	CeLoss 0.2754 (0.2512)	SegCLSLoss 0.0159 (0.0243)	KLLoss 0.0830 (0.1484)	MaskLoss 0.8768 (0.9184)	MaskBCELoss 0.3640 (0.2596)	MaskDICELoss 0.5128 (0.6588)
Epoch: [3][473/500]	Time  9.073 ( 9.073)	Loss 7.0040 (5.4467)	CeLoss 0.2891 (0.4194)	SegCLSLoss 0.0300 (0.0230)	KLLoss 0.2012 (0.1583)	MaskLoss 0.8522 (0.7130)	MaskBCELoss 0.0536 (0.1411)	MaskDICELoss 0.7986 (0.5719)
Epoch: [3][474/500]	Time 10.782 (10.782)	Loss 6.3307 (5.2001)	CeLoss 0.1914 (0.3429)	SegCLSLoss 0.0284 (0.0184)	KLLoss 0.1934 (0.1223)	MaskLoss 0.8814 (0.6522)	MaskBCELoss 0.1865 (0.0820)	MaskDICELoss 0.6949 (0.5702)
Epoch: [3][475/500]	Time  8.057 ( 8.057)	Loss 4.8670 (3.8440)	CeLoss 0.2266 (0.6599)	SegCLSLoss 0.0250 (0.0136)	KLLoss 0.1660 (0.0993)	MaskLoss 0.6750 (0.4660)	MaskBCELoss 0.1566 (0.1083)	MaskDICELoss 0.5185 (0.3576)
Epoch: [3][476/500]	Time  8.575 ( 8.575)	Loss 6.4311 (4.5169)	CeLoss 0.2217 (0.4805)	SegCLSLoss 0.0249 (0.0150)	KLLoss 0.1465 (0.1001)	MaskLoss 0.8146 (0.6035)	MaskBCELoss 0.0778 (0.1498)	MaskDICELoss 0.7368 (0.4536)
Epoch: [3][477/500]	Time  9.896 ( 9.896)	Loss 6.9337 (5.1589)	CeLoss 0.1836 (0.3985)	SegCLSLoss 0.0284 (0.0197)	KLLoss 0.2061 (0.1479)	MaskLoss 0.8740 (0.6942)	MaskBCELoss 0.0771 (0.1586)	MaskDICELoss 0.7969 (0.5357)
Epoch: [3][478/500]	Time  9.548 ( 9.548)	Loss 5.8537 (5.8072)	CeLoss 0.2227 (0.3627)	SegCLSLoss 0.0189 (0.0203)	KLLoss 0.1270 (0.1513)	MaskLoss 0.7933 (0.7765)	MaskBCELoss 0.1420 (0.1548)	MaskDICELoss 0.6513 (0.6216)
Epoch: [3][479/500]	Time  8.655 ( 8.655)	Loss 1.1250 (3.0350)	CeLoss 1.1250 (0.7819)	SegCLSLoss 0.0000 (0.0091)	KLLoss 0.0000 (0.0540)	MaskLoss 0.0000 (0.3158)	MaskBCELoss 0.0000 (0.0553)	MaskDICELoss 0.0000 (0.2605)
[2025-03-04 07:32:05,638] [INFO] [logging.py:96:log_dist] [Rank 0] step=1980, skipped=0, lr=[0.00025356626506024095], mom=[(0.9, 0.95)]
[2025-03-04 07:32:05,644] [INFO] [timer.py:215:stop] epoch=0/micro_step=19800/global_step=1980, RunningAvgSamplesPerSec=1.0455033345323028, CurrSamplesPerSec=1.0229079658317897, MemAllocated=57.56GB, MaxMemAllocated=62.86GB
Epoch: [3][480/500]	Time  9.778 ( 9.778)	Loss 5.7751 (5.3999)	CeLoss 0.2305 (0.5070)	SegCLSLoss 0.0211 (0.0213)	KLLoss 0.1387 (0.1526)	MaskLoss 0.8035 (0.7375)	MaskBCELoss 0.1720 (0.1951)	MaskDICELoss 0.6315 (0.5424)
Epoch: [3][481/500]	Time  9.657 ( 9.657)	Loss 3.5207 (5.6196)	CeLoss 0.2471 (0.4698)	SegCLSLoss 0.0156 (0.0198)	KLLoss 0.0991 (0.1284)	MaskLoss 0.5870 (0.8295)	MaskBCELoss 0.2548 (0.2708)	MaskDICELoss 0.3322 (0.5588)
Epoch: [3][482/500]	Time 10.105 (10.105)	Loss 1.8594 (5.7893)	CeLoss 1.8594 (0.4230)	SegCLSLoss 0.0000 (0.0249)	KLLoss 0.0000 (0.1573)	MaskLoss 0.0000 (0.8353)	MaskBCELoss 0.0000 (0.2477)	MaskDICELoss 0.0000 (0.5876)
Epoch: [3][483/500]	Time  8.369 ( 8.369)	Loss 5.4096 (4.7815)	CeLoss 0.1807 (0.4451)	SegCLSLoss 0.0430 (0.0181)	KLLoss 0.2559 (0.1167)	MaskLoss 0.6461 (0.6555)	MaskBCELoss 0.0360 (0.1722)	MaskDICELoss 0.6101 (0.4833)
Epoch: [3][484/500]	Time 10.676 (10.676)	Loss 5.9706 (4.8614)	CeLoss 0.2363 (0.4331)	SegCLSLoss 0.0267 (0.0177)	KLLoss 0.1875 (0.1171)	MaskLoss 0.7516 (0.6373)	MaskBCELoss 0.0800 (0.1326)	MaskDICELoss 0.6716 (0.5046)
Epoch: [3][485/500]	Time  9.126 ( 9.126)	Loss 0.7578 (4.0473)	CeLoss 0.7578 (0.4871)	SegCLSLoss 0.0000 (0.0152)	KLLoss 0.0000 (0.1046)	MaskLoss 0.0000 (0.5367)	MaskBCELoss 0.0000 (0.1409)	MaskDICELoss 0.0000 (0.3958)
Epoch: [3][486/500]	Time 10.315 (10.315)	Loss 5.2648 (5.7311)	CeLoss 0.1963 (0.3461)	SegCLSLoss 0.0171 (0.0223)	KLLoss 0.1240 (0.1508)	MaskLoss 0.7227 (0.8539)	MaskBCELoss 0.1408 (0.2681)	MaskDICELoss 0.5819 (0.5859)
Epoch: [3][487/500]	Time 10.329 (10.329)	Loss 3.4137 (5.8905)	CeLoss 0.3145 (0.2586)	SegCLSLoss 0.0166 (0.0221)	KLLoss 0.1309 (0.1423)	MaskLoss 0.6024 (0.8741)	MaskBCELoss 0.3098 (0.2523)	MaskDICELoss 0.2926 (0.6218)
Epoch: [3][488/500]	Time  8.980 ( 8.980)	Loss 5.5281 (4.9387)	CeLoss 0.3145 (0.5255)	SegCLSLoss 0.0172 (0.0173)	KLLoss 0.1235 (0.1169)	MaskLoss 0.8061 (0.6915)	MaskBCELoss 0.2276 (0.2073)	MaskDICELoss 0.5784 (0.4842)
Epoch: [3][489/500]	Time  8.881 ( 8.881)	Loss 6.5334 (4.0149)	CeLoss 0.2227 (0.4337)	SegCLSLoss 0.0199 (0.0136)	KLLoss 0.1250 (0.0953)	MaskLoss 0.8239 (0.5289)	MaskBCELoss 0.0693 (0.1253)	MaskDICELoss 0.7547 (0.4036)
[2025-03-04 07:33:43,588] [INFO] [logging.py:96:log_dist] [Rank 0] step=1990, skipped=0, lr=[0.0002533253012048192], mom=[(0.9, 0.95)]
[2025-03-04 07:33:43,594] [INFO] [timer.py:215:stop] epoch=0/micro_step=19900/global_step=1990, RunningAvgSamplesPerSec=1.0453776853630226, CurrSamplesPerSec=0.868965447515432, MemAllocated=56.8GB, MaxMemAllocated=62.86GB
Epoch: [3][490/500]	Time 11.510 (11.510)	Loss 1.1012 (5.6754)	CeLoss 0.3594 (0.2613)	SegCLSLoss 0.0138 (0.0244)	KLLoss 0.0688 (0.1478)	MaskLoss 0.1819 (0.8124)	MaskBCELoss 0.1315 (0.2076)	MaskDICELoss 0.0503 (0.6048)
Epoch: [3][491/500]	Time 10.250 (10.250)	Loss 0.1836 (6.1169)	CeLoss 0.1836 (0.3164)	SegCLSLoss 0.0000 (0.0231)	KLLoss 0.0000 (0.1481)	MaskLoss 0.0000 (0.9158)	MaskBCELoss 0.0000 (0.2810)	MaskDICELoss 0.0000 (0.6348)
Epoch: [3][492/500]	Time  9.696 ( 9.696)	Loss 8.8088 (4.6092)	CeLoss 0.2656 (0.3991)	SegCLSLoss 0.0305 (0.0191)	KLLoss 0.1748 (0.1275)	MaskLoss 2.8053 (0.8882)	MaskBCELoss 2.3485 (0.5054)	MaskDICELoss 0.4569 (0.3828)
Epoch: [3][493/500]	Time  9.506 ( 9.506)	Loss 1.0312 (5.4661)	CeLoss 0.2637 (0.3232)	SegCLSLoss 0.0141 (0.0184)	KLLoss 0.1011 (0.1353)	MaskLoss 0.1328 (0.7608)	MaskBCELoss 0.0674 (0.1814)	MaskDICELoss 0.0654 (0.5794)
Epoch: [3][494/500]	Time  9.289 ( 9.289)	Loss 0.0737 (4.8137)	CeLoss 0.0737 (0.4639)	SegCLSLoss 0.0000 (0.0143)	KLLoss 0.0000 (0.1005)	MaskLoss 0.0000 (0.6646)	MaskBCELoss 0.0000 (0.1790)	MaskDICELoss 0.0000 (0.4855)
Epoch: [3][495/500]	Time  9.257 ( 9.257)	Loss 1.6953 (4.8448)	CeLoss 1.6953 (0.3980)	SegCLSLoss 0.0000 (0.0134)	KLLoss 0.0000 (0.1137)	MaskLoss 0.0000 (0.6852)	MaskBCELoss 0.0000 (0.1926)	MaskDICELoss 0.0000 (0.4926)
Epoch: [3][496/500]	Time  8.853 ( 8.853)	Loss 8.1733 (6.4737)	CeLoss 0.1758 (0.3316)	SegCLSLoss 0.0625 (0.0378)	KLLoss 0.2520 (0.2085)	MaskLoss 1.2171 (0.9177)	MaskBCELoss 0.3371 (0.2378)	MaskDICELoss 0.8800 (0.6799)
Epoch: [3][497/500]	Time  9.359 ( 9.359)	Loss 1.4219 (4.8269)	CeLoss 1.4219 (0.4777)	SegCLSLoss 0.0000 (0.0171)	KLLoss 0.0000 (0.1224)	MaskLoss 0.0000 (0.6338)	MaskBCELoss 0.0000 (0.1422)	MaskDICELoss 0.0000 (0.4917)
Epoch: [3][498/500]	Time 11.155 (11.155)	Loss 6.2799 (4.0800)	CeLoss 0.2256 (0.1782)	SegCLSLoss 0.0227 (0.0154)	KLLoss 0.1572 (0.1063)	MaskLoss 0.7672 (0.6215)	MaskBCELoss 0.0420 (0.1974)	MaskDICELoss 0.7252 (0.4241)
Epoch: [3][499/500]	Time  8.681 ( 8.681)	Loss 0.3379 (3.6498)	CeLoss 0.3379 (0.5857)	SegCLSLoss 0.0000 (0.0114)	KLLoss 0.0000 (0.0773)	MaskLoss 0.0000 (0.4990)	MaskBCELoss 0.0000 (0.1685)	MaskDICELoss 0.0000 (0.3305)
[2025-03-04 07:35:19,362] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=0, lr=[0.00025308433734939754], mom=[(0.9, 0.95)]
[2025-03-04 07:35:19,368] [INFO] [timer.py:215:stop] epoch=0/micro_step=20000/global_step=2000, RunningAvgSamplesPerSec=1.0453722806253183, CurrSamplesPerSec=1.0280568589438117, MemAllocated=57.25GB, MaxMemAllocated=62.86GB
Epoch: [3][500/500]	Time  9.729 ( 9.729)	Loss 7.8747 (5.1419)	CeLoss 0.3242 (0.5011)	SegCLSLoss 0.0168 (0.0144)	KLLoss 0.0581 (0.0978)	MaskLoss 1.4973 (0.6546)	MaskBCELoss 0.7490 (0.1168)	MaskDICELoss 0.7482 (0.5378)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
giou: 0.1665, ciou: 0.1164
Epoch: [4][  1/500]	Time 10.648 (10.648)	Loss 7.5036 (5.9734)	CeLoss 0.3457 (0.3056)	SegCLSLoss 0.0220 (0.0221)	KLLoss 0.1514 (0.1462)	MaskLoss 1.3052 (0.8359)	MaskBCELoss 0.5743 (0.1961)	MaskDICELoss 0.7309 (0.6398)
Epoch: [4][  2/500]	Time  9.225 ( 9.225)	Loss 7.1859 (3.9180)	CeLoss 0.1943 (0.4474)	SegCLSLoss 0.0312 (0.0129)	KLLoss 0.2324 (0.0942)	MaskLoss 0.8774 (0.5185)	MaskBCELoss 0.0458 (0.1297)	MaskDICELoss 0.8316 (0.3888)
Epoch: [4][  3/500]	Time  9.201 ( 9.201)	Loss 6.5913 (6.0494)	CeLoss 0.1914 (0.5062)	SegCLSLoss 0.0266 (0.0248)	KLLoss 0.1650 (0.1527)	MaskLoss 0.7927 (0.7425)	MaskBCELoss 0.0202 (0.0937)	MaskDICELoss 0.7725 (0.6488)
Epoch: [4][  4/500]	Time  9.840 ( 9.840)	Loss 7.8485 (5.2003)	CeLoss 0.2295 (0.4245)	SegCLSLoss 0.0354 (0.0204)	KLLoss 0.2158 (0.1361)	MaskLoss 0.9347 (0.6836)	MaskBCELoss 0.0153 (0.1399)	MaskDICELoss 0.9194 (0.5437)
Epoch: [4][  5/500]	Time 10.348 (10.348)	Loss 8.4790 (6.6919)	CeLoss 0.1660 (0.2276)	SegCLSLoss 0.0378 (0.0279)	KLLoss 0.1914 (0.1635)	MaskLoss 1.3141 (1.0072)	MaskBCELoss 0.4014 (0.2951)	MaskDICELoss 0.9126 (0.7121)
Epoch: [4][  6/500]	Time  9.007 ( 9.007)	Loss 8.0648 (5.4483)	CeLoss 0.2188 (0.4646)	SegCLSLoss 0.0308 (0.0167)	KLLoss 0.1729 (0.1161)	MaskLoss 1.1804 (0.8348)	MaskBCELoss 0.2974 (0.3031)	MaskDICELoss 0.8830 (0.5316)
Epoch: [4][  7/500]	Time  9.150 ( 9.150)	Loss 8.4543 (4.8622)	CeLoss 0.2812 (0.4479)	SegCLSLoss 0.0315 (0.0189)	KLLoss 0.1943 (0.1177)	MaskLoss 1.0247 (0.5890)	MaskBCELoss 0.0393 (0.0709)	MaskDICELoss 0.9854 (0.5181)
Epoch: [4][  8/500]	Time  9.968 ( 9.968)	Loss 1.6406 (5.3703)	CeLoss 1.6406 (0.3940)	SegCLSLoss 0.0000 (0.0200)	KLLoss 0.0000 (0.1441)	MaskLoss 0.0000 (0.7510)	MaskBCELoss 0.0000 (0.1977)	MaskDICELoss 0.0000 (0.5534)
Epoch: [4][  9/500]	Time 10.294 (10.294)	Loss 7.0180 (5.6122)	CeLoss 0.1924 (0.4327)	SegCLSLoss 0.0330 (0.0185)	KLLoss 0.2207 (0.1418)	MaskLoss 1.1608 (0.7568)	MaskBCELoss 0.4496 (0.1709)	MaskDICELoss 0.7111 (0.5858)
[2025-03-04 07:37:51,735] [INFO] [logging.py:96:log_dist] [Rank 0] step=2010, skipped=0, lr=[0.00025285542168674695], mom=[(0.9, 0.95)]
[2025-03-04 07:37:51,740] [INFO] [timer.py:215:stop] epoch=0/micro_step=20100/global_step=2010, RunningAvgSamplesPerSec=1.0452772828345709, CurrSamplesPerSec=0.990815049545666, MemAllocated=57.27GB, MaxMemAllocated=62.86GB
Epoch: [4][ 10/500]	Time 10.095 (10.095)	Loss 6.8501 (6.1370)	CeLoss 0.2373 (0.3998)	SegCLSLoss 0.0291 (0.0214)	KLLoss 0.1855 (0.1439)	MaskLoss 0.8477 (0.8416)	MaskBCELoss 0.0615 (0.1917)	MaskDICELoss 0.7862 (0.6498)
Epoch: [4][ 11/500]	Time  8.163 ( 8.163)	Loss 0.3359 (3.6182)	CeLoss 0.3359 (0.4470)	SegCLSLoss 0.0000 (0.0123)	KLLoss 0.0000 (0.0853)	MaskLoss 0.0000 (0.4394)	MaskBCELoss 0.0000 (0.0726)	MaskDICELoss 0.0000 (0.3668)
Epoch: [4][ 12/500]	Time 10.106 (10.106)	Loss 1.2734 (5.7335)	CeLoss 1.2734 (0.4068)	SegCLSLoss 0.0000 (0.0157)	KLLoss 0.0000 (0.1170)	MaskLoss 0.0000 (0.8221)	MaskBCELoss 0.0000 (0.2292)	MaskDICELoss 0.0000 (0.5929)
Epoch: [4][ 13/500]	Time 11.113 (11.113)	Loss 8.3000 (5.7162)	CeLoss 0.1631 (0.2218)	SegCLSLoss 0.0410 (0.0221)	KLLoss 0.2012 (0.1468)	MaskLoss 1.3639 (0.7958)	MaskBCELoss 0.4993 (0.1716)	MaskDICELoss 0.8646 (0.6242)
Epoch: [4][ 14/500]	Time  7.314 ( 7.314)	Loss 7.2011 (3.6520)	CeLoss 0.2051 (0.6910)	SegCLSLoss 0.0308 (0.0099)	KLLoss 0.1953 (0.0733)	MaskLoss 0.9291 (0.4403)	MaskBCELoss 0.1080 (0.1066)	MaskDICELoss 0.8211 (0.3337)
Epoch: [4][ 15/500]	Time  9.622 ( 9.622)	Loss 1.9144 (4.9094)	CeLoss 0.3125 (0.3444)	SegCLSLoss 0.0217 (0.0203)	KLLoss 0.1631 (0.1473)	MaskLoss 0.3308 (0.6928)	MaskBCELoss 0.2034 (0.1891)	MaskDICELoss 0.1274 (0.5036)
Epoch: [4][ 16/500]	Time  9.410 ( 9.410)	Loss 8.2131 (5.9408)	CeLoss 0.2090 (0.4360)	SegCLSLoss 0.0250 (0.0192)	KLLoss 0.2002 (0.1456)	MaskLoss 1.0530 (0.7629)	MaskBCELoss 0.1055 (0.1256)	MaskDICELoss 0.9475 (0.6373)
Epoch: [4][ 17/500]	Time  8.567 ( 8.567)	Loss 5.3574 (5.2560)	CeLoss 0.2061 (0.4400)	SegCLSLoss 0.0303 (0.0229)	KLLoss 0.2236 (0.1538)	MaskLoss 0.6477 (0.6931)	MaskBCELoss 0.0449 (0.1489)	MaskDICELoss 0.6028 (0.5441)
Epoch: [4][ 18/500]	Time 11.305 (11.305)	Loss 7.3835 (5.9348)	CeLoss 0.1396 (0.3843)	SegCLSLoss 0.0640 (0.0237)	KLLoss 0.2520 (0.1627)	MaskLoss 1.1752 (0.8965)	MaskBCELoss 0.4070 (0.2994)	MaskDICELoss 0.7682 (0.5972)
Epoch: [4][ 19/500]	Time  9.226 ( 9.226)	Loss 4.1721 (4.2455)	CeLoss 0.2617 (0.5399)	SegCLSLoss 0.0134 (0.0145)	KLLoss 0.0972 (0.1113)	MaskLoss 0.6500 (0.5401)	MaskBCELoss 0.2322 (0.1223)	MaskDICELoss 0.4178 (0.4178)
[2025-03-04 07:39:26,920] [INFO] [logging.py:96:log_dist] [Rank 0] step=2020, skipped=0, lr=[0.0002526144578313253], mom=[(0.9, 0.95)]
[2025-03-04 07:39:26,925] [INFO] [timer.py:215:stop] epoch=0/micro_step=20200/global_step=2020, RunningAvgSamplesPerSec=1.0453042734062084, CurrSamplesPerSec=0.9654787190098673, MemAllocated=56.74GB, MaxMemAllocated=62.86GB
Epoch: [4][ 20/500]	Time 10.360 (10.360)	Loss 1.1953 (5.2431)	CeLoss 1.1953 (0.3312)	SegCLSLoss 0.0000 (0.0175)	KLLoss 0.0000 (0.1511)	MaskLoss 0.0000 (0.7215)	MaskBCELoss 0.0000 (0.1700)	MaskDICELoss 0.0000 (0.5515)
Epoch: [4][ 21/500]	Time  8.138 ( 8.138)	Loss 5.6459 (3.8337)	CeLoss 0.2676 (0.6177)	SegCLSLoss 0.0149 (0.0123)	KLLoss 0.1309 (0.0930)	MaskLoss 0.9276 (0.4829)	MaskBCELoss 0.3635 (0.1244)	MaskDICELoss 0.5641 (0.3585)
Epoch: [4][ 22/500]	Time  7.664 ( 7.664)	Loss 4.8764 (4.4231)	CeLoss 0.2656 (0.5836)	SegCLSLoss 0.0145 (0.0116)	KLLoss 0.1064 (0.0901)	MaskLoss 0.8033 (0.6450)	MaskBCELoss 0.3215 (0.2360)	MaskDICELoss 0.4818 (0.4090)
Epoch: [4][ 23/500]	Time  9.446 ( 9.446)	Loss 7.1798 (4.8353)	CeLoss 0.2119 (0.4178)	SegCLSLoss 0.0248 (0.0189)	KLLoss 0.1777 (0.1271)	MaskLoss 0.8758 (0.6406)	MaskBCELoss 0.0382 (0.1406)	MaskDICELoss 0.8376 (0.5000)
Epoch: [4][ 24/500]	Time 12.165 (12.165)	Loss 7.8793 (5.4552)	CeLoss 0.2070 (0.2607)	SegCLSLoss 0.0267 (0.0200)	KLLoss 0.1748 (0.1412)	MaskLoss 1.1627 (0.7519)	MaskBCELoss 0.3028 (0.1620)	MaskDICELoss 0.8599 (0.5899)
Epoch: [4][ 25/500]	Time  9.284 ( 9.284)	Loss 5.5295 (5.3931)	CeLoss 0.1973 (0.6545)	SegCLSLoss 0.0449 (0.0218)	KLLoss 0.2637 (0.1354)	MaskLoss 0.7340 (0.7073)	MaskBCELoss 0.1378 (0.1776)	MaskDICELoss 0.5962 (0.5297)
Epoch: [4][ 26/500]	Time 10.674 (10.674)	Loss 8.4232 (6.2322)	CeLoss 0.2305 (0.2536)	SegCLSLoss 0.0227 (0.0251)	KLLoss 0.1426 (0.1628)	MaskLoss 1.1577 (0.8408)	MaskBCELoss 0.2039 (0.1539)	MaskDICELoss 0.9538 (0.6869)
Epoch: [4][ 27/500]	Time  9.511 ( 9.511)	Loss 1.6719 (4.9324)	CeLoss 1.6719 (0.6199)	SegCLSLoss 0.0000 (0.0175)	KLLoss 0.0000 (0.1225)	MaskLoss 0.0000 (0.6053)	MaskBCELoss 0.0000 (0.1102)	MaskDICELoss 0.0000 (0.4951)
Epoch: [4][ 28/500]	Time  9.256 ( 9.256)	Loss 2.6551 (5.1153)	CeLoss 0.2061 (0.5022)	SegCLSLoss 0.0156 (0.0172)	KLLoss 0.1494 (0.1255)	MaskLoss 0.5340 (0.7025)	MaskBCELoss 0.3300 (0.1903)	MaskDICELoss 0.2040 (0.5123)
Epoch: [4][ 29/500]	Time  7.658 ( 7.658)	Loss 6.9512 (5.1225)	CeLoss 0.2129 (0.6552)	SegCLSLoss 0.0232 (0.0190)	KLLoss 0.1826 (0.1257)	MaskLoss 0.8413 (0.6925)	MaskBCELoss 0.0309 (0.2013)	MaskDICELoss 0.8104 (0.4912)
[2025-03-04 07:41:01,448] [INFO] [logging.py:96:log_dist] [Rank 0] step=2030, skipped=0, lr=[0.0002523734939759036], mom=[(0.9, 0.95)]
[2025-03-04 07:41:01,455] [INFO] [timer.py:215:stop] epoch=0/micro_step=20300/global_step=2030, RunningAvgSamplesPerSec=1.0453663465383534, CurrSamplesPerSec=0.9317884952544323, MemAllocated=57.47GB, MaxMemAllocated=62.86GB
Epoch: [4][ 30/500]	Time 10.734 (10.734)	Loss 8.0546 (6.1855)	CeLoss 0.2334 (0.2247)	SegCLSLoss 0.0226 (0.0197)	KLLoss 0.1504 (0.1399)	MaskLoss 1.1571 (0.8979)	MaskBCELoss 0.2661 (0.2287)	MaskDICELoss 0.8910 (0.6692)
Epoch: [4][ 31/500]	Time  7.572 ( 7.572)	Loss 7.2758 (4.5845)	CeLoss 0.2129 (0.7867)	SegCLSLoss 0.0249 (0.0149)	KLLoss 0.2129 (0.1107)	MaskLoss 0.9258 (0.5466)	MaskBCELoss 0.0946 (0.1155)	MaskDICELoss 0.8311 (0.4311)
Epoch: [4][ 32/500]	Time  9.961 ( 9.961)	Loss 4.5415 (4.9306)	CeLoss 0.3477 (0.4421)	SegCLSLoss 0.0176 (0.0204)	KLLoss 0.1465 (0.1373)	MaskLoss 0.5368 (0.6910)	MaskBCELoss 0.0421 (0.1977)	MaskDICELoss 0.4947 (0.4932)
Epoch: [4][ 33/500]	Time 11.008 (11.008)	Loss 4.4534 (5.0256)	CeLoss 0.2266 (0.3201)	SegCLSLoss 0.0178 (0.0192)	KLLoss 0.1299 (0.1450)	MaskLoss 0.5523 (0.6928)	MaskBCELoss 0.0551 (0.1652)	MaskDICELoss 0.4972 (0.5275)
Epoch: [4][ 34/500]	Time 10.748 (10.748)	Loss 8.4971 (7.4360)	CeLoss 0.2295 (0.2267)	SegCLSLoss 0.0256 (0.0283)	KLLoss 0.2539 (0.2002)	MaskLoss 1.0035 (0.9877)	MaskBCELoss 0.0045 (0.1511)	MaskDICELoss 0.9990 (0.8366)
Epoch: [4][ 35/500]	Time  9.642 ( 9.642)	Loss 6.2208 (5.3966)	CeLoss 0.3027 (0.4034)	SegCLSLoss 0.0184 (0.0188)	KLLoss 0.1611 (0.1367)	MaskLoss 0.8678 (0.6890)	MaskBCELoss 0.1990 (0.1109)	MaskDICELoss 0.6688 (0.5781)
Epoch: [4][ 36/500]	Time 10.255 (10.255)	Loss 8.4399 (5.7678)	CeLoss 0.2031 (0.3587)	SegCLSLoss 0.0276 (0.0251)	KLLoss 0.1924 (0.1578)	MaskLoss 1.1563 (0.8216)	MaskBCELoss 0.2034 (0.2223)	MaskDICELoss 0.9529 (0.5993)
Epoch: [4][ 37/500]	Time  9.133 ( 9.133)	Loss 6.1403 (5.0068)	CeLoss 0.2451 (0.4598)	SegCLSLoss 0.0242 (0.0223)	KLLoss 0.1348 (0.1221)	MaskLoss 0.7446 (0.6703)	MaskBCELoss 0.0349 (0.1581)	MaskDICELoss 0.7097 (0.5122)
Epoch: [4][ 38/500]	Time 11.125 (11.125)	Loss 7.9246 (5.9684)	CeLoss 0.1455 (0.2119)	SegCLSLoss 0.0649 (0.0256)	KLLoss 0.2578 (0.1686)	MaskLoss 1.3305 (0.9051)	MaskBCELoss 0.5258 (0.2776)	MaskDICELoss 0.8047 (0.6275)
Epoch: [4][ 39/500]	Time  8.972 ( 8.972)	Loss 8.3658 (5.4575)	CeLoss 0.1484 (0.4108)	SegCLSLoss 0.0396 (0.0181)	KLLoss 0.2109 (0.1322)	MaskLoss 1.3040 (0.8345)	MaskBCELoss 0.4075 (0.2952)	MaskDICELoss 0.8965 (0.5394)
[2025-03-04 07:42:38,457] [INFO] [logging.py:96:log_dist] [Rank 0] step=2040, skipped=0, lr=[0.0002521325301204819], mom=[(0.9, 0.95)]
[2025-03-04 07:42:38,463] [INFO] [timer.py:215:stop] epoch=0/micro_step=20400/global_step=2040, RunningAvgSamplesPerSec=1.0452949150717419, CurrSamplesPerSec=1.164183431145595, MemAllocated=57.48GB, MaxMemAllocated=62.86GB
Epoch: [4][ 40/500]	Time  8.592 ( 8.592)	Loss 5.8279 (6.0842)	CeLoss 0.2383 (0.3595)	SegCLSLoss 0.0214 (0.0276)	KLLoss 0.1299 (0.1729)	MaskLoss 0.7069 (0.8414)	MaskBCELoss 0.0344 (0.1989)	MaskDICELoss 0.6725 (0.6426)
Epoch: [4][ 41/500]	Time  9.402 ( 9.402)	Loss 8.4782 (5.9237)	CeLoss 0.1846 (0.4595)	SegCLSLoss 0.0227 (0.0205)	KLLoss 0.1719 (0.1474)	MaskLoss 1.0805 (0.7871)	MaskBCELoss 0.0888 (0.1651)	MaskDICELoss 0.9917 (0.6221)
Epoch: [4][ 42/500]	Time  9.138 ( 9.138)	Loss 7.5719 (5.1872)	CeLoss 0.0874 (0.3581)	SegCLSLoss 0.0874 (0.0246)	KLLoss 0.2363 (0.1397)	MaskLoss 1.3057 (0.7799)	MaskBCELoss 0.5402 (0.2603)	MaskDICELoss 0.7655 (0.5196)
Epoch: [4][ 43/500]	Time 10.995 (10.995)	Loss 8.0660 (6.4552)	CeLoss 0.1885 (0.2097)	SegCLSLoss 0.0334 (0.0234)	KLLoss 0.2109 (0.1568)	MaskLoss 1.0181 (1.0308)	MaskBCELoss 0.0825 (0.3616)	MaskDICELoss 0.9356 (0.6692)
Epoch: [4][ 44/500]	Time  8.448 ( 8.448)	Loss 4.8194 (4.1585)	CeLoss 0.2539 (0.5690)	SegCLSLoss 0.0166 (0.0164)	KLLoss 0.1328 (0.1208)	MaskLoss 0.8021 (0.5473)	MaskBCELoss 0.3319 (0.1529)	MaskDICELoss 0.4701 (0.3943)
Epoch: [4][ 45/500]	Time  8.533 ( 8.533)	Loss 0.0747 (4.6457)	CeLoss 0.0747 (0.4645)	SegCLSLoss 0.0000 (0.0162)	KLLoss 0.0000 (0.1037)	MaskLoss 0.0000 (0.7639)	MaskBCELoss 0.0000 (0.3403)	MaskDICELoss 0.0000 (0.4236)
Epoch: [4][ 46/500]	Time 10.768 (10.768)	Loss 7.0606 (5.6084)	CeLoss 0.1826 (0.3892)	SegCLSLoss 0.0305 (0.0263)	KLLoss 0.2188 (0.1682)	MaskLoss 1.0866 (0.7782)	MaskBCELoss 0.3418 (0.1979)	MaskDICELoss 0.7449 (0.5803)
Epoch: [4][ 47/500]	Time 10.639 (10.639)	Loss 6.9939 (7.4475)	CeLoss 0.2139 (0.2350)	SegCLSLoss 0.0272 (0.0287)	KLLoss 0.2168 (0.1770)	MaskLoss 1.1517 (1.1175)	MaskBCELoss 0.4441 (0.3199)	MaskDICELoss 0.7075 (0.7976)
Epoch: [4][ 48/500]	Time  8.513 ( 8.513)	Loss 5.4859 (4.7621)	CeLoss 0.2656 (0.5238)	SegCLSLoss 0.0151 (0.0184)	KLLoss 0.1348 (0.1255)	MaskLoss 0.8946 (0.6691)	MaskBCELoss 0.3465 (0.2082)	MaskDICELoss 0.5481 (0.4609)
Epoch: [4][ 49/500]	Time 10.526 (10.526)	Loss 8.2722 (6.1621)	CeLoss 0.2109 (0.3651)	SegCLSLoss 0.0223 (0.0224)	KLLoss 0.1689 (0.1504)	MaskLoss 1.0405 (0.8066)	MaskBCELoss 0.0737 (0.1362)	MaskDICELoss 0.9668 (0.6704)
[2025-03-04 07:44:14,177] [INFO] [logging.py:96:log_dist] [Rank 0] step=2050, skipped=0, lr=[0.00025189156626506025], mom=[(0.9, 0.95)]
[2025-03-04 07:44:14,183] [INFO] [timer.py:215:stop] epoch=0/micro_step=20500/global_step=2050, RunningAvgSamplesPerSec=1.0452929077970092, CurrSamplesPerSec=1.1418464420408603, MemAllocated=56.96GB, MaxMemAllocated=62.86GB
Epoch: [4][ 50/500]	Time  8.760 ( 8.760)	Loss 5.4462 (5.7285)	CeLoss 0.2119 (0.3510)	SegCLSLoss 0.0214 (0.0234)	KLLoss 0.1562 (0.1445)	MaskLoss 0.7452 (0.8448)	MaskBCELoss 0.1490 (0.2561)	MaskDICELoss 0.5962 (0.5886)
Epoch: [4][ 51/500]	Time  9.612 ( 9.612)	Loss 8.3481 (5.5370)	CeLoss 0.1885 (0.3962)	SegCLSLoss 0.0344 (0.0237)	KLLoss 0.2080 (0.1592)	MaskLoss 0.9981 (0.7918)	MaskBCELoss 0.0085 (0.2274)	MaskDICELoss 0.9896 (0.5644)
Epoch: [4][ 52/500]	Time  9.177 ( 9.177)	Loss 5.3458 (5.5703)	CeLoss 0.3320 (0.4887)	SegCLSLoss 0.0204 (0.0260)	KLLoss 0.1201 (0.1433)	MaskLoss 0.6930 (0.8065)	MaskBCELoss 0.1099 (0.2543)	MaskDICELoss 0.5831 (0.5521)
Epoch: [4][ 53/500]	Time  8.908 ( 8.908)	Loss 8.5153 (5.6573)	CeLoss 0.1953 (0.5222)	SegCLSLoss 0.0260 (0.0190)	KLLoss 0.1738 (0.1275)	MaskLoss 1.5034 (0.7622)	MaskBCELoss 0.6491 (0.1833)	MaskDICELoss 0.8543 (0.5789)
Epoch: [4][ 54/500]	Time  8.751 ( 8.751)	Loss 7.3198 (5.8281)	CeLoss 0.1309 (0.4398)	SegCLSLoss 0.0598 (0.0277)	KLLoss 0.2344 (0.1638)	MaskLoss 1.3488 (0.8161)	MaskBCELoss 0.6442 (0.2197)	MaskDICELoss 0.7046 (0.5964)
Epoch: [4][ 55/500]	Time  7.697 ( 7.697)	Loss 6.8485 (3.3219)	CeLoss 0.2236 (0.9968)	SegCLSLoss 0.0220 (0.0089)	KLLoss 0.1221 (0.0558)	MaskLoss 1.2630 (0.3393)	MaskBCELoss 0.6019 (0.0749)	MaskDICELoss 0.6612 (0.2644)
Epoch: [4][ 56/500]	Time  9.372 ( 9.372)	Loss 7.2036 (3.9417)	CeLoss 0.3848 (0.7031)	SegCLSLoss 0.0270 (0.0138)	KLLoss 0.1787 (0.0853)	MaskLoss 0.8751 (0.4986)	MaskBCELoss 0.0626 (0.1404)	MaskDICELoss 0.8125 (0.3582)
Epoch: [4][ 57/500]	Time 11.360 (11.360)	Loss 7.7760 (7.3246)	CeLoss 0.3223 (0.2572)	SegCLSLoss 0.0344 (0.0248)	KLLoss 0.1680 (0.1521)	MaskLoss 0.9859 (1.1128)	MaskBCELoss 0.1032 (0.3332)	MaskDICELoss 0.8827 (0.7795)
Epoch: [4][ 58/500]	Time  8.509 ( 8.509)	Loss 5.4547 (5.8026)	CeLoss 0.4688 (0.4155)	SegCLSLoss 0.0339 (0.0214)	KLLoss 0.2207 (0.1465)	MaskLoss 0.8908 (0.8789)	MaskBCELoss 0.3965 (0.3002)	MaskDICELoss 0.4943 (0.5787)
Epoch: [4][ 59/500]	Time  8.435 ( 8.435)	Loss 6.9702 (4.2965)	CeLoss 0.1904 (0.2666)	SegCLSLoss 0.0305 (0.0246)	KLLoss 0.1738 (0.1236)	MaskLoss 0.8637 (0.6039)	MaskBCELoss 0.0530 (0.1561)	MaskDICELoss 0.8107 (0.4477)
[2025-03-04 07:45:47,377] [INFO] [logging.py:96:log_dist] [Rank 0] step=2060, skipped=0, lr=[0.0002516506024096385], mom=[(0.9, 0.95)]
[2025-03-04 07:45:47,383] [INFO] [timer.py:215:stop] epoch=0/micro_step=20600/global_step=2060, RunningAvgSamplesPerSec=1.045424790303454, CurrSamplesPerSec=0.8789979931210085, MemAllocated=57.28GB, MaxMemAllocated=62.86GB
Epoch: [4][ 60/500]	Time 11.379 (11.379)	Loss 6.6756 (5.9735)	CeLoss 0.2109 (0.2135)	SegCLSLoss 0.0291 (0.0261)	KLLoss 0.1582 (0.1647)	MaskLoss 1.1980 (0.8429)	MaskBCELoss 0.5486 (0.1935)	MaskDICELoss 0.6495 (0.6494)
Epoch: [4][ 61/500]	Time 10.219 (10.219)	Loss 7.6909 (5.3034)	CeLoss 0.2559 (0.5162)	SegCLSLoss 0.0228 (0.0195)	KLLoss 0.1299 (0.1069)	MaskLoss 0.9362 (0.6626)	MaskBCELoss 0.0328 (0.1051)	MaskDICELoss 0.9034 (0.5576)
Epoch: [4][ 62/500]	Time  9.175 ( 9.175)	Loss 6.6996 (4.5041)	CeLoss 0.1914 (0.4541)	SegCLSLoss 0.0427 (0.0226)	KLLoss 0.2441 (0.1404)	MaskLoss 0.8720 (0.6227)	MaskBCELoss 0.1222 (0.1805)	MaskDICELoss 0.7498 (0.4422)
Epoch: [4][ 63/500]	Time  8.091 ( 8.091)	Loss 1.3516 (4.7101)	CeLoss 1.3516 (0.5180)	SegCLSLoss 0.0000 (0.0268)	KLLoss 0.0000 (0.1336)	MaskLoss 0.0000 (0.7069)	MaskBCELoss 0.0000 (0.2684)	MaskDICELoss 0.0000 (0.4386)
Epoch: [4][ 64/500]	Time  8.905 ( 8.905)	Loss 1.9297 (4.5126)	CeLoss 1.9297 (0.4930)	SegCLSLoss 0.0000 (0.0165)	KLLoss 0.0000 (0.1022)	MaskLoss 0.0000 (0.6505)	MaskBCELoss 0.0000 (0.2158)	MaskDICELoss 0.0000 (0.4347)
Epoch: [4][ 65/500]	Time 10.186 (10.186)	Loss 8.6974 (6.8316)	CeLoss 0.2188 (0.2420)	SegCLSLoss 0.0344 (0.0268)	KLLoss 0.2773 (0.1811)	MaskLoss 1.1164 (0.9250)	MaskBCELoss 0.1249 (0.1676)	MaskDICELoss 0.9915 (0.7574)
Epoch: [4][ 66/500]	Time 11.413 (11.413)	Loss 6.4096 (5.7133)	CeLoss 0.3047 (0.4889)	SegCLSLoss 0.0181 (0.0200)	KLLoss 0.1079 (0.1272)	MaskLoss 0.9079 (0.7663)	MaskBCELoss 0.2125 (0.1739)	MaskDICELoss 0.6953 (0.5924)
Epoch: [4][ 67/500]	Time  8.185 ( 8.185)	Loss 1.9375 (4.9465)	CeLoss 1.9375 (0.4803)	SegCLSLoss 0.0000 (0.0178)	KLLoss 0.0000 (0.1129)	MaskLoss 0.0000 (0.6388)	MaskBCELoss 0.0000 (0.1276)	MaskDICELoss 0.0000 (0.5111)
Epoch: [4][ 68/500]	Time  9.530 ( 9.530)	Loss 3.6024 (4.4390)	CeLoss 0.2432 (0.4225)	SegCLSLoss 0.0168 (0.0183)	KLLoss 0.1016 (0.1182)	MaskLoss 0.4977 (0.5815)	MaskBCELoss 0.1221 (0.1271)	MaskDICELoss 0.3756 (0.4544)
Epoch: [4][ 69/500]	Time 10.669 (10.669)	Loss 7.6273 (5.0729)	CeLoss 0.2012 (0.2264)	SegCLSLoss 0.0366 (0.0213)	KLLoss 0.2285 (0.1210)	MaskLoss 0.9394 (0.7176)	MaskBCELoss 0.0562 (0.1710)	MaskDICELoss 0.8832 (0.5466)
[2025-03-04 07:47:23,150] [INFO] [logging.py:96:log_dist] [Rank 0] step=2070, skipped=0, lr=[0.00025140963855421685], mom=[(0.9, 0.95)]
[2025-03-04 07:47:23,155] [INFO] [timer.py:215:stop] epoch=0/micro_step=20700/global_step=2070, RunningAvgSamplesPerSec=1.0454194426520167, CurrSamplesPerSec=1.064056829809799, MemAllocated=56.82GB, MaxMemAllocated=62.86GB
Epoch: [4][ 70/500]	Time  9.400 ( 9.400)	Loss 8.2685 (7.2661)	CeLoss 0.1621 (0.2055)	SegCLSLoss 0.0449 (0.0416)	KLLoss 0.2070 (0.2062)	MaskLoss 1.2997 (1.1127)	MaskBCELoss 0.4200 (0.3446)	MaskDICELoss 0.8797 (0.7681)
Epoch: [4][ 71/500]	Time 10.445 (10.445)	Loss 7.4384 (6.8189)	CeLoss 0.2832 (0.3721)	SegCLSLoss 0.0356 (0.0297)	KLLoss 0.2080 (0.1695)	MaskLoss 0.8866 (0.9067)	MaskBCELoss 0.0270 (0.1651)	MaskDICELoss 0.8596 (0.7415)
Epoch: [4][ 72/500]	Time  9.660 ( 9.660)	Loss 5.6915 (5.7609)	CeLoss 0.4688 (0.3529)	SegCLSLoss 0.0181 (0.0250)	KLLoss 0.1426 (0.1446)	MaskLoss 0.8178 (0.7668)	MaskBCELoss 0.2453 (0.1472)	MaskDICELoss 0.5725 (0.6196)
Epoch: [4][ 73/500]	Time  9.878 ( 9.878)	Loss 3.4800 (4.8898)	CeLoss 0.2852 (0.4472)	SegCLSLoss 0.0226 (0.0195)	KLLoss 0.1523 (0.1353)	MaskLoss 0.5109 (0.6467)	MaskBCELoss 0.1761 (0.1460)	MaskDICELoss 0.3348 (0.5007)
Epoch: [4][ 74/500]	Time  9.884 ( 9.884)	Loss 2.7949 (5.8708)	CeLoss 0.2227 (0.4666)	SegCLSLoss 0.0206 (0.0234)	KLLoss 0.1660 (0.1657)	MaskLoss 0.3496 (0.7288)	MaskBCELoss 0.0667 (0.1006)	MaskDICELoss 0.2829 (0.6282)
Epoch: [4][ 75/500]	Time 10.556 (10.556)	Loss 6.9466 (5.6881)	CeLoss 0.4961 (0.3818)	SegCLSLoss 0.0171 (0.0198)	KLLoss 0.0913 (0.1284)	MaskLoss 0.8463 (0.7096)	MaskBCELoss 0.0695 (0.0848)	MaskDICELoss 0.7767 (0.6249)
Epoch: [4][ 76/500]	Time 10.970 (10.970)	Loss 5.7409 (5.8133)	CeLoss 0.2168 (0.2205)	SegCLSLoss 0.0258 (0.0226)	KLLoss 0.1865 (0.1514)	MaskLoss 0.7650 (0.8975)	MaskBCELoss 0.1325 (0.2917)	MaskDICELoss 0.6325 (0.6058)
Epoch: [4][ 77/500]	Time  8.475 ( 8.475)	Loss 1.0938 (3.7051)	CeLoss 1.0938 (0.6049)	SegCLSLoss 0.0000 (0.0132)	KLLoss 0.0000 (0.0843)	MaskLoss 0.0000 (0.4450)	MaskBCELoss 0.0000 (0.0918)	MaskDICELoss 0.0000 (0.3532)
Epoch: [4][ 78/500]	Time  9.948 ( 9.948)	Loss 4.1527 (4.6263)	CeLoss 0.3027 (0.4256)	SegCLSLoss 0.0201 (0.0178)	KLLoss 0.1514 (0.1263)	MaskLoss 0.5643 (0.5872)	MaskBCELoss 0.1378 (0.1054)	MaskDICELoss 0.4265 (0.4818)
Epoch: [4][ 79/500]	Time 10.318 (10.318)	Loss 5.6031 (5.2341)	CeLoss 0.2441 (0.3146)	SegCLSLoss 0.0264 (0.0198)	KLLoss 0.1914 (0.1461)	MaskLoss 0.7696 (0.7109)	MaskBCELoss 0.1671 (0.1539)	MaskDICELoss 0.6024 (0.5570)
[2025-03-04 07:49:02,534] [INFO] [logging.py:96:log_dist] [Rank 0] step=2080, skipped=0, lr=[0.00025116867469879517], mom=[(0.9, 0.95)]
[2025-03-04 07:49:02,539] [INFO] [timer.py:215:stop] epoch=0/micro_step=20800/global_step=2080, RunningAvgSamplesPerSec=1.0452242128321108, CurrSamplesPerSec=1.0813518558522714, MemAllocated=57.26GB, MaxMemAllocated=62.86GB
Epoch: [4][ 80/500]	Time  9.250 ( 9.250)	Loss 7.6947 (4.8458)	CeLoss 0.2100 (0.5499)	SegCLSLoss 0.0227 (0.0208)	KLLoss 0.1680 (0.1315)	MaskLoss 0.9552 (0.6379)	MaskBCELoss 0.0563 (0.1583)	MaskDICELoss 0.8989 (0.4796)
Epoch: [4][ 81/500]	Time  8.919 ( 8.919)	Loss 7.4069 (4.2274)	CeLoss 0.1982 (0.3208)	SegCLSLoss 0.0569 (0.0226)	KLLoss 0.2500 (0.1264)	MaskLoss 1.3232 (0.5629)	MaskBCELoss 0.6092 (0.1224)	MaskDICELoss 0.7140 (0.4405)
Epoch: [4][ 82/500]	Time  9.734 ( 9.734)	Loss 0.0947 (5.1257)	CeLoss 0.0947 (0.3394)	SegCLSLoss 0.0000 (0.0213)	KLLoss 0.0000 (0.1216)	MaskLoss 0.0000 (0.7204)	MaskBCELoss 0.0000 (0.1848)	MaskDICELoss 0.0000 (0.5356)
Epoch: [4][ 83/500]	Time  6.668 ( 6.668)	Loss 9.0661 (5.2880)	CeLoss 0.2617 (0.4230)	SegCLSLoss 0.0226 (0.0201)	KLLoss 0.1553 (0.1319)	MaskLoss 1.3183 (0.6908)	MaskBCELoss 0.3183 (0.1339)	MaskDICELoss 1.0000 (0.5569)
Epoch: [4][ 84/500]	Time 10.103 (10.103)	Loss 8.7406 (5.0043)	CeLoss 0.1533 (0.3635)	SegCLSLoss 0.0525 (0.0196)	KLLoss 0.2383 (0.1189)	MaskLoss 1.6434 (0.6924)	MaskBCELoss 0.8041 (0.1711)	MaskDICELoss 0.8393 (0.5212)
Epoch: [4][ 85/500]	Time 10.352 (10.352)	Loss 3.0544 (5.0801)	CeLoss 0.2383 (0.3677)	SegCLSLoss 0.0140 (0.0207)	KLLoss 0.0806 (0.1416)	MaskLoss 0.5441 (0.7651)	MaskBCELoss 0.2708 (0.2600)	MaskDICELoss 0.2733 (0.5051)
Epoch: [4][ 86/500]	Time  8.694 ( 8.694)	Loss 7.6463 (4.8378)	CeLoss 0.2734 (0.4915)	SegCLSLoss 0.0187 (0.0202)	KLLoss 0.1562 (0.1199)	MaskLoss 1.0439 (0.6157)	MaskBCELoss 0.1907 (0.1182)	MaskDICELoss 0.8532 (0.4975)
Epoch: [4][ 87/500]	Time  9.234 ( 9.234)	Loss 8.5573 (3.7179)	CeLoss 0.2051 (0.6817)	SegCLSLoss 0.0320 (0.0133)	KLLoss 0.2217 (0.0795)	MaskLoss 1.1728 (0.4689)	MaskBCELoss 0.2111 (0.1335)	MaskDICELoss 0.9617 (0.3354)
Epoch: [4][ 88/500]	Time  9.274 ( 9.274)	Loss 6.4793 (4.6833)	CeLoss 0.2988 (0.4006)	SegCLSLoss 0.0148 (0.0178)	KLLoss 0.1187 (0.1191)	MaskLoss 1.0013 (0.6863)	MaskBCELoss 0.3262 (0.2226)	MaskDICELoss 0.6751 (0.4637)
Epoch: [4][ 89/500]	Time  9.450 ( 9.450)	Loss 7.3396 (5.8279)	CeLoss 0.2227 (0.4306)	SegCLSLoss 0.0159 (0.0177)	KLLoss 0.1377 (0.1141)	MaskLoss 1.3333 (0.8680)	MaskBCELoss 0.6157 (0.2782)	MaskDICELoss 0.7176 (0.5898)
[2025-03-04 07:50:35,262] [INFO] [logging.py:96:log_dist] [Rank 0] step=2090, skipped=0, lr=[0.00025092771084337344], mom=[(0.9, 0.95)]
[2025-03-04 07:50:35,267] [INFO] [timer.py:215:stop] epoch=0/micro_step=20900/global_step=2090, RunningAvgSamplesPerSec=1.045379144437669, CurrSamplesPerSec=0.9711013857774659, MemAllocated=57.26GB, MaxMemAllocated=62.86GB
Epoch: [4][ 90/500]	Time 10.300 (10.300)	Loss 6.7147 (5.2442)	CeLoss 0.2090 (0.2938)	SegCLSLoss 0.0232 (0.0203)	KLLoss 0.1621 (0.1379)	MaskLoss 0.8880 (0.7010)	MaskBCELoss 0.1287 (0.1341)	MaskDICELoss 0.7593 (0.5668)
Epoch: [4][ 91/500]	Time  6.140 ( 6.140)	Loss 1.7734 (4.3850)	CeLoss 1.7734 (0.8828)	SegCLSLoss 0.0000 (0.0099)	KLLoss 0.0000 (0.0804)	MaskLoss 0.0000 (0.6455)	MaskBCELoss 0.0000 (0.2912)	MaskDICELoss 0.0000 (0.3543)
Epoch: [4][ 92/500]	Time  8.603 ( 8.603)	Loss 0.0747 (4.2140)	CeLoss 0.0747 (0.4447)	SegCLSLoss 0.0000 (0.0174)	KLLoss 0.0000 (0.1015)	MaskLoss 0.0000 (0.6409)	MaskBCELoss 0.0000 (0.2448)	MaskDICELoss 0.0000 (0.3961)
Epoch: [4][ 93/500]	Time  8.099 ( 8.099)	Loss 3.9212 (4.7750)	CeLoss 0.2002 (0.6724)	SegCLSLoss 0.0238 (0.0152)	KLLoss 0.1641 (0.1126)	MaskLoss 0.4894 (0.7306)	MaskBCELoss 0.0615 (0.3104)	MaskDICELoss 0.4279 (0.4202)
Epoch: [4][ 94/500]	Time  8.122 ( 8.122)	Loss 6.1619 (4.0990)	CeLoss 0.2598 (0.9849)	SegCLSLoss 0.0281 (0.0112)	KLLoss 0.1953 (0.0800)	MaskLoss 0.8728 (0.4803)	MaskBCELoss 0.2149 (0.1356)	MaskDICELoss 0.6579 (0.3447)
Epoch: [4][ 95/500]	Time  9.420 ( 9.420)	Loss 1.8109 (4.3935)	CeLoss 0.3359 (0.4857)	SegCLSLoss 0.0211 (0.0163)	KLLoss 0.1748 (0.1128)	MaskLoss 0.1811 (0.6233)	MaskBCELoss 0.0262 (0.1999)	MaskDICELoss 0.1549 (0.4233)
Epoch: [4][ 96/500]	Time 10.007 (10.007)	Loss 8.3680 (4.8359)	CeLoss 0.2451 (0.4086)	SegCLSLoss 0.0244 (0.0182)	KLLoss 0.1787 (0.1200)	MaskLoss 1.3355 (0.7025)	MaskBCELoss 0.4590 (0.2203)	MaskDICELoss 0.8766 (0.4822)
Epoch: [4][ 97/500]	Time 10.023 (10.023)	Loss 8.2854 (5.8921)	CeLoss 0.2109 (0.3144)	SegCLSLoss 0.0344 (0.0219)	KLLoss 0.1992 (0.1421)	MaskLoss 1.2703 (0.8764)	MaskBCELoss 0.3842 (0.2644)	MaskDICELoss 0.8862 (0.6120)
Epoch: [4][ 98/500]	Time  9.077 ( 9.077)	Loss 7.6640 (4.6701)	CeLoss 0.2119 (0.3960)	SegCLSLoss 0.0260 (0.0152)	KLLoss 0.1768 (0.1066)	MaskLoss 0.9519 (0.6014)	MaskBCELoss 0.0589 (0.1086)	MaskDICELoss 0.8930 (0.4929)
Epoch: [4][ 99/500]	Time  9.687 ( 9.687)	Loss 8.7023 (5.2364)	CeLoss 0.1631 (0.4728)	SegCLSLoss 0.0452 (0.0270)	KLLoss 0.2207 (0.1366)	MaskLoss 1.3766 (0.7605)	MaskBCELoss 0.4528 (0.2452)	MaskDICELoss 0.9238 (0.5154)
[2025-03-04 07:52:03,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=2100, skipped=0, lr=[0.00025068674698795176], mom=[(0.9, 0.95)]
[2025-03-04 07:52:03,127] [INFO] [timer.py:215:stop] epoch=0/micro_step=21000/global_step=2100, RunningAvgSamplesPerSec=1.0457863048045124, CurrSamplesPerSec=1.1521956003932454, MemAllocated=57.24GB, MaxMemAllocated=62.86GB
Epoch: [4][100/500]	Time  8.681 ( 8.681)	Loss 4.0187 (3.7975)	CeLoss 0.2871 (0.5343)	SegCLSLoss 0.0149 (0.0109)	KLLoss 0.1201 (0.0762)	MaskLoss 0.6832 (0.5846)	MaskBCELoss 0.3102 (0.2491)	MaskDICELoss 0.3730 (0.3354)
Epoch: [4][101/500]	Time  9.621 ( 9.621)	Loss 6.7815 (5.0858)	CeLoss 0.1943 (0.5485)	SegCLSLoss 0.0183 (0.0182)	KLLoss 0.1465 (0.1145)	MaskLoss 0.9707 (0.6571)	MaskBCELoss 0.2222 (0.1405)	MaskDICELoss 0.7484 (0.5166)
Epoch: [4][102/500]	Time 10.179 (10.179)	Loss 8.3920 (6.4211)	CeLoss 0.1729 (0.3739)	SegCLSLoss 0.0295 (0.0261)	KLLoss 0.2227 (0.1548)	MaskLoss 1.0048 (0.9247)	MaskBCELoss 0.0095 (0.2530)	MaskDICELoss 0.9954 (0.6717)
Epoch: [4][103/500]	Time 10.116 (10.116)	Loss 2.3470 (5.3391)	CeLoss 0.2373 (0.3379)	SegCLSLoss 0.0131 (0.0218)	KLLoss 0.0645 (0.1198)	MaskLoss 0.3106 (0.6814)	MaskBCELoss 0.0743 (0.0968)	MaskDICELoss 0.2362 (0.5846)
Epoch: [4][104/500]	Time  9.400 ( 9.400)	Loss 5.0766 (4.5856)	CeLoss 0.1826 (0.5074)	SegCLSLoss 0.0155 (0.0156)	KLLoss 0.1123 (0.0876)	MaskLoss 0.6854 (0.6676)	MaskBCELoss 0.1182 (0.2264)	MaskDICELoss 0.5672 (0.4412)
Epoch: [4][105/500]	Time  9.166 ( 9.166)	Loss 4.7633 (3.9415)	CeLoss 0.2598 (0.5751)	SegCLSLoss 0.0183 (0.0145)	KLLoss 0.1357 (0.0926)	MaskLoss 0.6453 (0.4849)	MaskBCELoss 0.1343 (0.1021)	MaskDICELoss 0.5111 (0.3828)
Epoch: [4][106/500]	Time  9.509 ( 9.509)	Loss 6.1566 (3.6268)	CeLoss 0.3027 (0.4472)	SegCLSLoss 0.0293 (0.0151)	KLLoss 0.2373 (0.1109)	MaskLoss 0.8258 (0.4996)	MaskBCELoss 0.1674 (0.1558)	MaskDICELoss 0.6584 (0.3437)
Epoch: [4][107/500]	Time  9.247 ( 9.247)	Loss 7.2843 (4.8338)	CeLoss 0.2197 (0.4536)	SegCLSLoss 0.0249 (0.0183)	KLLoss 0.1865 (0.1249)	MaskLoss 0.8794 (0.6753)	MaskBCELoss 0.0281 (0.1928)	MaskDICELoss 0.8513 (0.4826)
Epoch: [4][108/500]	Time 10.572 (10.572)	Loss 6.7682 (5.6964)	CeLoss 0.2100 (0.3932)	SegCLSLoss 0.0197 (0.0255)	KLLoss 0.1240 (0.1547)	MaskLoss 0.8561 (0.7306)	MaskBCELoss 0.0708 (0.1182)	MaskDICELoss 0.7854 (0.6124)
Epoch: [4][109/500]	Time  8.917 ( 8.917)	Loss 4.3817 (4.8097)	CeLoss 0.3086 (0.5108)	SegCLSLoss 0.0161 (0.0202)	KLLoss 0.0747 (0.1343)	MaskLoss 0.5797 (0.6029)	MaskBCELoss 0.1077 (0.1114)	MaskDICELoss 0.4720 (0.4915)
[2025-03-04 07:53:39,024] [INFO] [logging.py:96:log_dist] [Rank 0] step=2110, skipped=0, lr=[0.0002504457831325301], mom=[(0.9, 0.95)]
[2025-03-04 07:53:39,028] [INFO] [timer.py:215:stop] epoch=0/micro_step=21100/global_step=2110, RunningAvgSamplesPerSec=1.0457726379198788, CurrSamplesPerSec=1.0904117173171348, MemAllocated=57.25GB, MaxMemAllocated=62.86GB
Epoch: [4][110/500]	Time  9.173 ( 9.173)	Loss 3.9106 (4.3943)	CeLoss 0.2031 (0.3768)	SegCLSLoss 0.0146 (0.0127)	KLLoss 0.0820 (0.0869)	MaskLoss 0.5369 (0.7613)	MaskBCELoss 0.1129 (0.3611)	MaskDICELoss 0.4240 (0.4002)
Epoch: [4][111/500]	Time  9.405 ( 9.405)	Loss 6.1356 (5.1126)	CeLoss 0.2334 (0.4052)	SegCLSLoss 0.0156 (0.0178)	KLLoss 0.1348 (0.1253)	MaskLoss 0.7702 (0.7273)	MaskBCELoss 0.0668 (0.2076)	MaskDICELoss 0.7034 (0.5197)
Epoch: [4][112/500]	Time  8.349 ( 8.349)	Loss 7.9639 (4.1762)	CeLoss 0.2061 (0.4637)	SegCLSLoss 0.0276 (0.0200)	KLLoss 0.1816 (0.1182)	MaskLoss 1.0521 (0.6083)	MaskBCELoss 0.1425 (0.2136)	MaskDICELoss 0.9096 (0.3946)
Epoch: [4][113/500]	Time  9.644 ( 9.644)	Loss 8.2241 (5.3529)	CeLoss 0.2109 (0.4233)	SegCLSLoss 0.0339 (0.0245)	KLLoss 0.2109 (0.1379)	MaskLoss 1.0480 (0.7634)	MaskBCELoss 0.0995 (0.2212)	MaskDICELoss 0.9484 (0.5421)
Epoch: [4][114/500]	Time  9.304 ( 9.304)	Loss 7.8716 (4.1330)	CeLoss 0.1187 (0.3187)	SegCLSLoss 0.0737 (0.0226)	KLLoss 0.2520 (0.1194)	MaskLoss 1.2949 (0.5906)	MaskBCELoss 0.4824 (0.1735)	MaskDICELoss 0.8126 (0.4171)
Epoch: [4][115/500]	Time  8.119 ( 8.119)	Loss 5.5630 (4.2667)	CeLoss 0.2100 (0.5646)	SegCLSLoss 0.0248 (0.0128)	KLLoss 0.1396 (0.0886)	MaskLoss 0.6799 (0.5110)	MaskBCELoss 0.0399 (0.0801)	MaskDICELoss 0.6400 (0.4308)
Epoch: [4][116/500]	Time 10.833 (10.833)	Loss 9.3983 (5.4844)	CeLoss 0.2715 (0.3655)	SegCLSLoss 0.0156 (0.0206)	KLLoss 0.1123 (0.1327)	MaskLoss 1.6144 (0.8185)	MaskBCELoss 0.6513 (0.2621)	MaskDICELoss 0.9631 (0.5564)
Epoch: [4][117/500]	Time  9.690 ( 9.690)	Loss 6.1136 (4.7960)	CeLoss 0.1387 (0.5276)	SegCLSLoss 0.0752 (0.0209)	KLLoss 0.2441 (0.1071)	MaskLoss 1.1893 (0.6884)	MaskBCELoss 0.6368 (0.2260)	MaskDICELoss 0.5525 (0.4624)
Epoch: [4][118/500]	Time  9.337 ( 9.337)	Loss 9.4108 (6.8592)	CeLoss 0.1387 (0.3524)	SegCLSLoss 0.0491 (0.0264)	KLLoss 0.2461 (0.1630)	MaskLoss 1.5763 (1.0116)	MaskBCELoss 0.6016 (0.2937)	MaskDICELoss 0.9747 (0.7179)
Epoch: [4][119/500]	Time 10.637 (10.637)	Loss 8.4294 (4.7809)	CeLoss 0.2090 (0.2838)	SegCLSLoss 0.0273 (0.0181)	KLLoss 0.2285 (0.1329)	MaskLoss 1.1944 (0.6759)	MaskBCELoss 0.2629 (0.1753)	MaskDICELoss 0.9316 (0.5006)
[2025-03-04 07:55:14,062] [INFO] [logging.py:96:log_dist] [Rank 0] step=2120, skipped=0, lr=[0.0002502048192771084], mom=[(0.9, 0.95)]
[2025-03-04 07:55:14,068] [INFO] [timer.py:215:stop] epoch=0/micro_step=21200/global_step=2120, RunningAvgSamplesPerSec=1.0458035856314831, CurrSamplesPerSec=1.0288555400431652, MemAllocated=56.8GB, MaxMemAllocated=62.86GB
Epoch: [4][120/500]	Time  9.722 ( 9.722)	Loss 5.4242 (5.4316)	CeLoss 0.2500 (0.2925)	SegCLSLoss 0.0204 (0.0210)	KLLoss 0.1494 (0.1384)	MaskLoss 1.1864 (0.8357)	MaskBCELoss 0.7462 (0.2826)	MaskDICELoss 0.4402 (0.5531)
Epoch: [4][121/500]	Time  9.144 ( 9.144)	Loss 1.4141 (4.0231)	CeLoss 1.4141 (0.4092)	SegCLSLoss 0.0000 (0.0130)	KLLoss 0.0000 (0.0875)	MaskLoss 0.0000 (0.5347)	MaskBCELoss 0.0000 (0.1263)	MaskDICELoss 0.0000 (0.4084)
Epoch: [4][122/500]	Time  9.942 ( 9.942)	Loss 1.1484 (6.4767)	CeLoss 1.1484 (0.3194)	SegCLSLoss 0.0000 (0.0241)	KLLoss 0.0000 (0.1499)	MaskLoss 0.0000 (0.9928)	MaskBCELoss 0.0000 (0.3245)	MaskDICELoss 0.0000 (0.6683)
Epoch: [4][123/500]	Time 11.004 (11.004)	Loss 0.0962 (4.2238)	CeLoss 0.0962 (0.5434)	SegCLSLoss 0.0000 (0.0121)	KLLoss 0.0000 (0.0924)	MaskLoss 0.0000 (0.5728)	MaskBCELoss 0.0000 (0.1668)	MaskDICELoss 0.0000 (0.4060)
Epoch: [4][124/500]	Time  9.249 ( 9.249)	Loss 3.0819 (4.5547)	CeLoss 0.2275 (0.6459)	SegCLSLoss 0.0164 (0.0171)	KLLoss 0.1416 (0.1162)	MaskLoss 0.5193 (0.6337)	MaskBCELoss 0.2416 (0.2142)	MaskDICELoss 0.2777 (0.4195)
Epoch: [4][125/500]	Time 10.322 (10.322)	Loss 7.3258 (7.0561)	CeLoss 0.1836 (0.3415)	SegCLSLoss 0.0282 (0.0238)	KLLoss 0.1768 (0.1552)	MaskLoss 0.9223 (1.0883)	MaskBCELoss 0.0712 (0.3598)	MaskDICELoss 0.8511 (0.7285)
Epoch: [4][126/500]	Time 11.350 (11.350)	Loss 7.6646 (4.9328)	CeLoss 0.1504 (0.2316)	SegCLSLoss 0.0359 (0.0169)	KLLoss 0.2158 (0.1085)	MaskLoss 1.1935 (0.6983)	MaskBCELoss 0.3777 (0.1670)	MaskDICELoss 0.8158 (0.5313)
Epoch: [4][127/500]	Time 10.443 (10.443)	Loss 8.5154 (4.7604)	CeLoss 0.2812 (0.5946)	SegCLSLoss 0.0259 (0.0163)	KLLoss 0.1953 (0.1181)	MaskLoss 1.0471 (0.5728)	MaskBCELoss 0.0586 (0.0906)	MaskDICELoss 0.9885 (0.4823)
Epoch: [4][128/500]	Time 11.074 (11.074)	Loss 7.6615 (5.3419)	CeLoss 0.2441 (0.5111)	SegCLSLoss 0.0332 (0.0197)	KLLoss 0.2051 (0.1228)	MaskLoss 0.9060 (0.7238)	MaskBCELoss 0.0089 (0.1821)	MaskDICELoss 0.8971 (0.5418)
Epoch: [4][129/500]	Time  8.762 ( 8.762)	Loss 7.6282 (4.3200)	CeLoss 0.1934 (0.7734)	SegCLSLoss 0.0265 (0.0174)	KLLoss 0.1895 (0.0940)	MaskLoss 0.9501 (0.4904)	MaskBCELoss 0.0615 (0.0800)	MaskDICELoss 0.8886 (0.4104)
[2025-03-04 07:56:54,881] [INFO] [logging.py:96:log_dist] [Rank 0] step=2130, skipped=0, lr=[0.00024996385542168674], mom=[(0.9, 0.95)]
[2025-03-04 07:56:54,886] [INFO] [timer.py:215:stop] epoch=0/micro_step=21300/global_step=2130, RunningAvgSamplesPerSec=1.0455373884541748, CurrSamplesPerSec=1.0496460616775771, MemAllocated=57.47GB, MaxMemAllocated=62.86GB
Epoch: [4][130/500]	Time  9.529 ( 9.529)	Loss 5.2501 (5.3237)	CeLoss 0.2148 (0.5007)	SegCLSLoss 0.0208 (0.0259)	KLLoss 0.1562 (0.1332)	MaskLoss 0.6506 (0.7353)	MaskBCELoss 0.0562 (0.2009)	MaskDICELoss 0.5944 (0.5344)
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 954.00 MiB. GPU epspeed/runtime/zero/stage_1_and_2.py", line 1890, in backward 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 954.00 MiB. GPU epspeed/runtime/zero/stage_1_and_2.py", line 1890, in backward 200/200 [00:53<00:00,  3.75it/s]/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([42, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.