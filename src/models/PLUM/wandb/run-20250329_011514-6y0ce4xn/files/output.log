You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565



Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [03:29<00:00, 69.89s/it]
Traceback (most recent call last):
  File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 747, in <module>
    main(sys.argv[1:])
  File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 226, in main
    model.get_model().initialize_plum_modules(model.get_model().config)
  File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py", line 308, in initialize_plum_modules
    self.visual_model = build_sam_vit_h(self.vision_pretrained)
  File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/segment_anything/build_sam.py", line 16, in build_sam_vit_h
    return _build_sam(
  File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/segment_anything/build_sam.py", line 105, in _build_sam
    with open(checkpoint, "rb") as f:
FileNotFoundError: [Errno 2] No such file or directory: 'PATH_TO_SAM_ViT-H'