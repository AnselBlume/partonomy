You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565



Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:15<00:00,  5.21s/it]
trainable params: 6,553,600 || all params: 14,025,701,683 || trainable%: 0.0467256480147682
n:  base_model.model.model.embed_tokens.weight p.shape:  torch.Size([32002, 5120])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_token.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.mask_decoder.mask_tokens.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.weight p.shape:  torch.Size([256, 64, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.weight p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.weight p.shape:  torch.Size([64, 32, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.bias p.shape:  torch.Size([4])
n:  base_model.model.model.text_hidden_fcs.0.0.weight p.shape:  torch.Size([5120, 5120])
n:  base_model.model.model.text_hidden_fcs.0.0.bias p.shape:  torch.Size([5120])
n:  base_model.model.model.text_hidden_fcs.0.2.weight p.shape:  torch.Size([256, 5120])
n:  base_model.model.model.text_hidden_fcs.0.2.bias p.shape:  torch.Size([256])
n:  base_model.model.lm_head.weight p.shape:  torch.Size([32002, 5120])
>> (PLUM.py) Initializing teacher LLM...
>> (PLUM.py) Teacher LLM initialized.
ade20k:  20210
cocostuff:  118287
loading annotations into memory...
Done (t=0.55s)
creating index...
index created!
pascal_part:  4366
loading annotations into memory...
Done (t=8.12s)
creating index...
index created!
paco_lvis:  45790
mapillary:  18000
loading dataset refclef into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refclef/refs(unc).p
creating index...
index created.
DONE (t=2.63s)
dataset refclef (refs unc) (train split) has 17978 images and 99523 annotations.
loading dataset refcoco into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco/refs(unc).p
creating index...
index created.
DONE (t=5.31s)
dataset refcoco (refs unc) (train split) has 16994 images and 196771 annotations.
loading dataset refcoco+ into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco+/refs(unc).p
creating index...
index created.
DONE (t=5.85s)
dataset refcoco+ (refs unc) (train split) has 16992 images and 196737 annotations.
loading dataset refcocog into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcocog/refs(umd).p
creating index...
index created.
DONE (t=6.14s)
dataset refcocog (refs umd) (train split) has 21899 images and 208960 annotations.
vqa_data:  157712
number of reason_seg samples:  239
len(self.img_to_explanation):  239
Training with 20000 examples and validating with 200 examples.
[2025-03-09 09:23:35,688] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5, git-hash=unknown, git-branch=unknown
[2025-03-09 09:23:35,688] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2025-03-09 09:23:35,688] [INFO] [comm.py:594:init_distributed] cdb=None
[2025-03-09 09:23:35,688] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Using /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[2025-03-09 09:24:12,511] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Loading extension module fused_adam...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/tensor/python_tensor.cpp:78.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
Time to load fused_adam op: 0.6379883289337158 seconds
[2025-03-09 09:24:13,393] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-03-09 09:24:14,167] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-03-09 09:24:14,168] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-03-09 09:24:14,168] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-03-09 09:24:14,168] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500000000
[2025-03-09 09:24:14,168] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500000000
[2025-03-09 09:24:14,168] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2025-03-09 09:24:14,168] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(365842916, False)]
[2025-03-09 09:24:19,109] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2025-03-09 09:24:19,110] [INFO] [utils.py:786:see_memory_usage] MA 53.71 GB         Max_MA 54.39 GB         CA 54.6 GB         Max_CA 55 GB
[2025-03-09 09:24:19,110] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 125.17 GB, percent = 12.4%
[2025-03-09 09:24:22,401] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2025-03-09 09:24:22,403] [INFO] [utils.py:786:see_memory_usage] MA 56.44 GB         Max_MA 57.8 GB         CA 58.69 GB         Max_CA 59 GB
[2025-03-09 09:24:22,403] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 125.16 GB, percent = 12.4%
[2025-03-09 09:24:22,403] [INFO] [stage_1_and_2.py:488:__init__] optimizer state initialized
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)
  return F.conv2d(input, weight, bias, self.stride,
[2025-03-09 09:24:25,983] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2025-03-09 09:24:25,984] [INFO] [utils.py:786:see_memory_usage] MA 56.44 GB         Max_MA 56.44 GB         CA 58.69 GB         Max_CA 59 GB
[2025-03-09 09:24:25,984] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 125.29 GB, percent = 12.4%
[2025-03-09 09:24:25,990] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-03-09 09:24:25,991] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2025-03-09 09:24:25,991] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7fbd3e635270>
[2025-03-09 09:24:25,991] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
[2025-03-09 09:24:25,998] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2025-03-09 09:24:25,999] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-03-09 09:24:25,999] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-03-09 09:24:25,999] [INFO] [config.py:964:print]   amp_enabled .................. False
[2025-03-09 09:24:25,999] [INFO] [config.py:964:print]   amp_params ................... False
[2025-03-09 09:24:25,999] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-09 09:24:25,999] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2025-03-09 09:24:25,999] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2025-03-09 09:24:25,999] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2025-03-09 09:24:25,999] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2025-03-09 09:24:25,999] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fbd3e634a90>
[2025-03-09 09:24:26,000] [INFO] [config.py:964:print]   communication_data_type ...... None
[2025-03-09 09:24:26,000] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-09 09:24:26,000] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2025-03-09 09:24:26,000] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2025-03-09 09:24:26,000] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-09 09:24:26,000] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2025-03-09 09:24:26,000] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2025-03-09 09:24:26,000] [INFO] [config.py:964:print]   disable_allgather ............ False
[2025-03-09 09:24:26,000] [INFO] [config.py:964:print]   dump_state ................... False
[2025-03-09 09:24:26,000] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2025-03-09 09:24:26,000] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2025-03-09 09:24:26,000] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-09 09:24:26,000] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-09 09:24:26,000] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2025-03-09 09:24:26,000] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2025-03-09 09:24:26,000] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2025-03-09 09:24:26,000] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2025-03-09 09:24:26,001] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2025-03-09 09:24:26,001] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2025-03-09 09:24:26,001] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-03-09 09:24:26,001] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2025-03-09 09:24:26,001] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2025-03-09 09:24:26,001] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2025-03-09 09:24:26,001] [INFO] [config.py:964:print]   global_rank .................. 0
[2025-03-09 09:24:26,001] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2025-03-09 09:24:26,001] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 10
[2025-03-09 09:24:26,001] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0
[2025-03-09 09:24:26,001] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2025-03-09 09:24:26,001] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-09 09:24:26,001] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2025-03-09 09:24:26,001] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2025-03-09 09:24:26,001] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2025-03-09 09:24:26,001] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2025-03-09 09:24:26,001] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2025-03-09 09:24:26,002] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2025-03-09 09:24:26,002] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-03-09 09:24:26,002] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-03-09 09:24:26,002] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2025-03-09 09:24:26,002] [INFO] [config.py:964:print]   optimizer_name ............... adamw
[2025-03-09 09:24:26,002] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.0003, 'weight_decay': 0.0, 'betas': (0.9, 0.95)}
[2025-03-09 09:24:26,002] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2025-03-09 09:24:26,002] [INFO] [config.py:964:print]   pld_enabled .................. False
[2025-03-09 09:24:26,002] [INFO] [config.py:964:print]   pld_params ................... False
[2025-03-09 09:24:26,002] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2025-03-09 09:24:26,002] [INFO] [config.py:964:print]   scheduler_name ............... WarmupDecayLR
[2025-03-09 09:24:26,002] [INFO] [config.py:964:print]   scheduler_params ............. {'total_num_steps': 25000, 'warmup_min_lr': 0, 'warmup_max_lr': 0.0003, 'warmup_num_steps': 100, 'warmup_type': 'linear'}
[2025-03-09 09:24:26,002] [INFO] [config.py:964:print]   sparse_attention ............. None
[2025-03-09 09:24:26,002] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2025-03-09 09:24:26,002] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2025-03-09 09:24:26,003] [INFO] [config.py:964:print]   train_batch_size ............. 40
[2025-03-09 09:24:26,003] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  4
[2025-03-09 09:24:26,003] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2025-03-09 09:24:26,003] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2025-03-09 09:24:26,003] [INFO] [config.py:964:print]   world_size ................... 1
[2025-03-09 09:24:26,003] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2025-03-09 09:24:26,003] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2025-03-09 09:24:26,003] [INFO] [config.py:964:print]   zero_enabled ................. True
[2025-03-09 09:24:26,003] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-09 09:24:26,003] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2025-03-09 09:24:26,003] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 4,
    "gradient_accumulation_steps": 10,
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 0.0003,
            "weight_decay": 0.0,
            "betas": [0.9, 0.95]
        }
    },
    "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "total_num_steps": 2.500000e+04,
            "warmup_min_lr": 0,
            "warmup_max_lr": 0.0003,
            "warmup_num_steps": 100,
            "warmup_type": "linear"
        }
    },
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "gradient_clipping": 1.0,
    "zero_optimization": {
        "stage": 2,
        "contiguous_gradients": true,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 5.000000e+08,
        "allgather_bucket_size": 5.000000e+08
    }
}
(train) >> AFTER DEEPSPEED
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch: [0][  1/500]	Time 29.602 (29.602)	Loss 0.6185 (0.4832)	CeLoss 0.3223 (0.2642)	SegCLSLoss 0.1162 (0.1080)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1630 (0.1434)	MaskBCELoss 0.1049 (0.0624)	MaskDICELoss 0.0581 (0.0810)
Epoch: [0][  2/500]	Time 28.564 (28.564)	Loss 0.3863 (0.4224)	CeLoss 0.2188 (0.2440)	SegCLSLoss 0.1260 (0.1229)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1241 (0.1294)	MaskBCELoss 0.0287 (0.0347)	MaskDICELoss 0.0954 (0.0946)
Epoch: [0][  3/500]	Time 21.548 (21.548)	Loss 0.5003 (0.4346)	CeLoss 0.1963 (0.2341)	SegCLSLoss 0.1260 (0.1100)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1783 (0.1294)	MaskBCELoss 0.1012 (0.0539)	MaskDICELoss 0.0771 (0.0755)
Epoch: [0][  4/500]	Time 31.293 (31.293)	Loss 0.3635 (0.4778)	CeLoss 0.1484 (0.2694)	SegCLSLoss 0.1201 (0.1209)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1479 (0.1469)	MaskBCELoss 0.0535 (0.0498)	MaskDICELoss 0.0944 (0.0972)
Epoch: [0][  5/500]	Time 29.532 (29.532)	Loss 0.3069 (0.3651)	CeLoss 0.1836 (0.2347)	SegCLSLoss 0.1235 (0.1111)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1051 (0.1040)	MaskBCELoss 0.0060 (0.0153)	MaskDICELoss 0.0991 (0.0887)
Epoch: [0][  6/500]	Time 29.917 (29.917)	Loss 0.4103 (0.4303)	CeLoss 0.2012 (0.2177)	SegCLSLoss 0.1289 (0.1245)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1261 (0.1446)	MaskBCELoss 0.0544 (0.0521)	MaskDICELoss 0.0716 (0.0925)
Epoch: [0][  7/500]	Time 26.432 (26.432)	Loss 0.3921 (0.4052)	CeLoss 0.2451 (0.2465)	SegCLSLoss 0.1279 (0.1103)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1160 (0.1185)	MaskBCELoss 0.0165 (0.0295)	MaskDICELoss 0.0995 (0.0890)
Epoch: [0][  8/500]	Time 29.317 (29.317)	Loss 0.3982 (0.5047)	CeLoss 0.2090 (0.2562)	SegCLSLoss 0.1250 (0.1222)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1226 (0.1621)	MaskBCELoss 0.0433 (0.0707)	MaskDICELoss 0.0793 (0.0914)
Epoch: [0][  9/500]	Time 27.045 (27.045)	Loss 0.3558 (0.3769)	CeLoss 0.2324 (0.2331)	SegCLSLoss 0.1235 (0.1107)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0429 (0.0948)	MaskBCELoss 0.0271 (0.0275)	MaskDICELoss 0.0158 (0.0673)
Epoch: [0][ 10/500]	Time 27.863 (27.863)	Loss 0.3906 (0.4272)	CeLoss 0.1416 (0.2575)	SegCLSLoss 0.1162 (0.1218)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1398 (0.1210)	MaskBCELoss 0.0805 (0.0323)	MaskDICELoss 0.0593 (0.0887)
Epoch: [0][ 11/500]	Time 28.857 (28.857)	Loss 0.5236 (0.4587)	CeLoss 0.3066 (0.2736)	SegCLSLoss 0.1162 (0.1191)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1487 (0.1310)	MaskBCELoss 0.0568 (0.0401)	MaskDICELoss 0.0919 (0.0909)
Epoch: [0][ 12/500]	Time 26.984 (26.984)	Loss 0.5399 (0.4535)	CeLoss 0.2930 (0.2378)	SegCLSLoss 0.1226 (0.1211)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1647 (0.1461)	MaskBCELoss 0.0686 (0.0548)	MaskDICELoss 0.0961 (0.0913)
Epoch: [0][ 13/500]	Time 27.736 (27.736)	Loss 0.3292 (0.5642)	CeLoss 0.1309 (0.2109)	SegCLSLoss 0.1221 (0.1215)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1323 (0.2185)	MaskBCELoss 0.0478 (0.1224)	MaskDICELoss 0.0845 (0.0960)
Epoch: [0][ 14/500]	Time 27.359 (27.359)	Loss 0.4560 (0.4324)	CeLoss 0.2910 (0.2286)	SegCLSLoss 0.1133 (0.1084)	KLLoss 0.0001 (0.0000)	MaskLoss 0.1267 (0.1375)	MaskBCELoss 0.0305 (0.0540)	MaskDICELoss 0.0962 (0.0835)
Epoch: [0][ 15/500]	Time 28.835 (28.835)	Loss 0.3185 (0.4431)	CeLoss 0.1982 (0.2383)	SegCLSLoss 0.1211 (0.1227)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1045 (0.1403)	MaskBCELoss 0.0045 (0.0489)	MaskDICELoss 0.1000 (0.0914)
Epoch: [0][ 16/500]	Time 29.807 (29.807)	Loss 0.3793 (0.3545)	CeLoss 0.1797 (0.2159)	SegCLSLoss 0.1289 (0.0863)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1379 (0.0964)	MaskBCELoss 0.0444 (0.0315)	MaskDICELoss 0.0935 (0.0649)
Epoch: [0][ 17/500]	Time 29.121 (29.121)	Loss 0.3862 (0.3913)	CeLoss 0.1982 (0.2194)	SegCLSLoss 0.1221 (0.1237)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1357 (0.1235)	MaskBCELoss 0.0393 (0.0320)	MaskDICELoss 0.0964 (0.0915)
Epoch: [0][ 18/500]	Time 28.905 (28.905)	Loss 0.3890 (0.4241)	CeLoss 0.1348 (0.2296)	SegCLSLoss 0.1211 (0.1222)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1644 (0.1340)	MaskBCELoss 0.0743 (0.0441)	MaskDICELoss 0.0901 (0.0899)
Epoch: [0][ 19/500]	Time 28.287 (28.287)	Loss 0.5836 (0.4792)	CeLoss 0.2363 (0.2476)	SegCLSLoss 0.1270 (0.1094)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1972 (0.1520)	MaskBCELoss 0.1233 (0.0671)	MaskDICELoss 0.0739 (0.0848)
Epoch: [0][ 20/500]	Time 29.189 (29.189)	Loss 0.4242 (0.3596)	CeLoss 0.2734 (0.2096)	SegCLSLoss 0.1182 (0.0991)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1138 (0.1061)	MaskBCELoss 0.0230 (0.0317)	MaskDICELoss 0.0908 (0.0744)
Epoch: [0][ 21/500]	Time 27.847 (27.847)	Loss 0.5047 (0.4551)	CeLoss 0.3203 (0.2692)	SegCLSLoss 0.1172 (0.1188)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1380 (0.1342)	MaskBCELoss 0.0384 (0.0398)	MaskDICELoss 0.0996 (0.0945)
Epoch: [0][ 22/500]	Time 27.047 (27.047)	Loss 0.1102 (0.3897)	CeLoss 0.1104 (0.2150)	SegCLSLoss 0.0000 (0.0984)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0000 (0.1190)	MaskBCELoss 0.0000 (0.0441)	MaskDICELoss 0.0000 (0.0748)
Epoch: [0][ 23/500]	Time 27.754 (27.754)	Loss 0.3427 (0.3993)	CeLoss 0.2031 (0.2418)	SegCLSLoss 0.1260 (0.1235)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1119 (0.1123)	MaskBCELoss 0.0141 (0.0266)	MaskDICELoss 0.0979 (0.0856)
Epoch: [0][ 24/500]	Time 25.434 (25.434)	Loss 0.4466 (0.4650)	CeLoss 0.3320 (0.2274)	SegCLSLoss 0.1143 (0.1219)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1029 (0.1548)	MaskBCELoss 0.0030 (0.0657)	MaskDICELoss 0.0999 (0.0892)
Epoch: [0][ 25/500]	Time 28.259 (28.259)	Loss 0.4776 (0.3852)	CeLoss 0.2930 (0.2233)	SegCLSLoss 0.1187 (0.1238)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1092 (0.1151)	MaskBCELoss 0.0476 (0.0281)	MaskDICELoss 0.0616 (0.0869)
Epoch: [0][ 26/500]	Time 30.438 (30.438)	Loss 0.5929 (0.3865)	CeLoss 0.2715 (0.2219)	SegCLSLoss 0.1162 (0.1064)	KLLoss 0.0000 (0.0000)	MaskLoss 0.2039 (0.1140)	MaskBCELoss 0.1075 (0.0363)	MaskDICELoss 0.0964 (0.0777)
Epoch: [0][ 27/500]	Time 27.408 (27.408)	Loss 0.5143 (0.3878)	CeLoss 0.3086 (0.2372)	SegCLSLoss 0.1162 (0.0970)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1481 (0.1070)	MaskBCELoss 0.0487 (0.0323)	MaskDICELoss 0.0994 (0.0747)
Epoch: [0][ 28/500]	Time 28.838 (28.838)	Loss 0.3642 (0.3401)	CeLoss 0.1748 (0.1846)	SegCLSLoss 0.1211 (0.1115)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1391 (0.1097)	MaskBCELoss 0.0392 (0.0298)	MaskDICELoss 0.0999 (0.0798)
Epoch: [0][ 29/500]	Time 31.003 (31.003)	Loss 0.4844 (0.3228)	CeLoss 0.2988 (0.2008)	SegCLSLoss 0.1157 (0.0983)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1365 (0.0957)	MaskBCELoss 0.0399 (0.0166)	MaskDICELoss 0.0966 (0.0791)
Epoch: [0][ 30/500]	Time 27.855 (27.855)	Loss 0.1516 (0.3311)	CeLoss 0.1514 (0.2134)	SegCLSLoss 0.0000 (0.0852)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0000 (0.0850)	MaskBCELoss 0.0000 (0.0217)	MaskDICELoss 0.0000 (0.0633)
Epoch: [0][ 31/500]	Time 30.125 (30.125)	Loss 0.3680 (0.3659)	CeLoss 0.2295 (0.2237)	SegCLSLoss 0.1240 (0.1105)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1128 (0.1084)	MaskBCELoss 0.0129 (0.0217)	MaskDICELoss 0.0998 (0.0867)
Epoch: [0][ 32/500]	Time 27.626 (27.626)	Loss 0.2770 (0.4211)	CeLoss 0.1426 (0.2469)	SegCLSLoss 0.1157 (0.1201)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1132 (0.1255)	MaskBCELoss 0.0136 (0.0344)	MaskDICELoss 0.0996 (0.0911)
Epoch: [0][ 33/500]	Time 28.514 (28.514)	Loss 0.2271 (0.4083)	CeLoss 0.1289 (0.2215)	SegCLSLoss 0.1235 (0.1207)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0673 (0.1287)	MaskBCELoss 0.0019 (0.0414)	MaskDICELoss 0.0654 (0.0872)
Epoch: [0][ 34/500]	Time 28.111 (28.111)	Loss 0.4696 (0.3538)	CeLoss 0.2793 (0.2077)	SegCLSLoss 0.1187 (0.1083)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1266 (0.1065)	MaskBCELoss 0.0448 (0.0259)	MaskDICELoss 0.0818 (0.0806)
Epoch: [0][ 35/500]	Time 25.140 (25.140)	Loss 0.2983 (0.3861)	CeLoss 0.1807 (0.2130)	SegCLSLoss 0.1260 (0.1077)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1023 (0.1229)	MaskBCELoss 0.0023 (0.0388)	MaskDICELoss 0.1000 (0.0840)
Epoch: [0][ 36/500]	Time 29.355 (29.355)	Loss 0.2654 (0.3384)	CeLoss 0.1465 (0.1985)	SegCLSLoss 0.1235 (0.1112)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1029 (0.1071)	MaskBCELoss 0.0030 (0.0207)	MaskDICELoss 0.0998 (0.0864)
Epoch: [0][ 37/500]	Time 28.265 (28.265)	Loss 0.4479 (0.3845)	CeLoss 0.3086 (0.2516)	SegCLSLoss 0.1157 (0.1080)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1150 (0.1050)	MaskBCELoss 0.0155 (0.0177)	MaskDICELoss 0.0995 (0.0874)
Epoch: [0][ 38/500]	Time 30.395 (30.395)	Loss 0.3776 (0.3960)	CeLoss 0.1943 (0.2437)	SegCLSLoss 0.1226 (0.1196)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1144 (0.1107)	MaskBCELoss 0.0428 (0.0249)	MaskDICELoss 0.0716 (0.0858)
Epoch: [0][ 39/500]	Time 29.629 (29.629)	Loss 0.3967 (0.3658)	CeLoss 0.2168 (0.2376)	SegCLSLoss 0.1279 (0.0970)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1249 (0.0969)	MaskBCELoss 0.0353 (0.0208)	MaskDICELoss 0.0896 (0.0761)
Epoch: [0][ 40/500]	Time 29.508 (29.508)	Loss 0.4371 (0.4304)	CeLoss 0.3281 (0.1995)	SegCLSLoss 0.1162 (0.1118)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0996 (0.1474)	MaskBCELoss 0.0009 (0.0676)	MaskDICELoss 0.0986 (0.0798)
Epoch: [0][ 41/500]	Time 25.835 (25.835)	Loss 0.2858 (0.3264)	CeLoss 0.1172 (0.1710)	SegCLSLoss 0.1226 (0.1101)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1260 (0.1149)	MaskBCELoss 0.0298 (0.0288)	MaskDICELoss 0.0962 (0.0861)
Epoch: [0][ 42/500]	Time 30.549 (30.549)	Loss 0.3234 (0.3520)	CeLoss 0.2002 (0.1979)	SegCLSLoss 0.1279 (0.1091)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0987 (0.1140)	MaskBCELoss 0.0067 (0.0284)	MaskDICELoss 0.0921 (0.0856)
Epoch: [0][ 43/500]	Time 27.001 (27.001)	Loss 0.2945 (0.4062)	CeLoss 0.1748 (0.1959)	SegCLSLoss 0.1221 (0.1082)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1021 (0.1434)	MaskBCELoss 0.0050 (0.0564)	MaskDICELoss 0.0971 (0.0869)
Epoch: [0][ 44/500]	Time 27.885 (27.885)	Loss 0.2811 (0.3829)	CeLoss 0.1621 (0.1951)	SegCLSLoss 0.1226 (0.1232)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1036 (0.1355)	MaskBCELoss 0.0039 (0.0391)	MaskDICELoss 0.0997 (0.0964)
Epoch: [0][ 45/500]	Time 28.194 (28.194)	Loss 0.3417 (0.3709)	CeLoss 0.1934 (0.1930)	SegCLSLoss 0.1260 (0.1099)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1147 (0.1273)	MaskBCELoss 0.0187 (0.0395)	MaskDICELoss 0.0960 (0.0878)
Epoch: [0][ 46/500]	Time 30.058 (30.058)	Loss 0.2508 (0.3202)	CeLoss 0.1416 (0.1621)	SegCLSLoss 0.1162 (0.1217)	KLLoss 0.0001 (0.0000)	MaskLoss 0.1003 (0.1154)	MaskBCELoss 0.0005 (0.0263)	MaskDICELoss 0.0999 (0.0891)
Epoch: [0][ 47/500]	Time 30.116 (30.116)	Loss 0.3248 (0.3350)	CeLoss 0.1914 (0.1796)	SegCLSLoss 0.1240 (0.1083)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0974 (0.1110)	MaskBCELoss 0.0153 (0.0306)	MaskDICELoss 0.0820 (0.0804)
Epoch: [0][ 48/500]	Time 27.616 (27.616)	Loss 0.3475 (0.4047)	CeLoss 0.2188 (0.2073)	SegCLSLoss 0.1211 (0.1077)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1043 (0.1379)	MaskBCELoss 0.0105 (0.0496)	MaskDICELoss 0.0938 (0.0884)
Epoch: [0][ 49/500]	Time 29.450 (29.450)	Loss 0.2840 (0.3691)	CeLoss 0.1709 (0.2011)	SegCLSLoss 0.1240 (0.0964)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1006 (0.1176)	MaskBCELoss 0.0006 (0.0411)	MaskDICELoss 0.1000 (0.0765)
Epoch: [0][ 50/500]	Time 29.539 (29.539)	Loss 0.1187 (0.4573)	CeLoss 0.1187 (0.2049)	SegCLSLoss 0.0000 (0.1086)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0000 (0.1625)	MaskBCELoss 0.0000 (0.0778)	MaskDICELoss 0.0000 (0.0848)
Epoch: [0][ 51/500]	Time 26.355 (26.355)	Loss 0.3457 (0.3033)	CeLoss 0.2236 (0.1670)	SegCLSLoss 0.1143 (0.1081)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1070 (0.0945)	MaskBCELoss 0.0073 (0.0233)	MaskDICELoss 0.0996 (0.0712)
Epoch: [0][ 52/500]	Time 28.300 (28.300)	Loss 0.2716 (0.3415)	CeLoss 0.1396 (0.1891)	SegCLSLoss 0.1279 (0.1210)	KLLoss 0.0001 (0.0001)	MaskLoss 0.1088 (0.1150)	MaskBCELoss 0.0094 (0.0229)	MaskDICELoss 0.0994 (0.0921)
Epoch: [0][ 53/500]	Time 27.762 (27.762)	Loss 0.3659 (0.3290)	CeLoss 0.2539 (0.1886)	SegCLSLoss 0.1147 (0.1067)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1016 (0.1064)	MaskBCELoss 0.0028 (0.0226)	MaskDICELoss 0.0989 (0.0838)
Epoch: [0][ 54/500]	Time 27.910 (27.910)	Loss 0.4020 (0.3738)	CeLoss 0.1963 (0.1921)	SegCLSLoss 0.1260 (0.1195)	KLLoss 0.0001 (0.0000)	MaskLoss 0.1234 (0.1272)	MaskBCELoss 0.0540 (0.0388)	MaskDICELoss 0.0694 (0.0884)
Epoch: [0][ 55/500]	Time 28.227 (28.227)	Loss 0.3306 (0.3110)	CeLoss 0.1758 (0.1712)	SegCLSLoss 0.1235 (0.1088)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1127 (0.1024)	MaskBCELoss 0.0245 (0.0228)	MaskDICELoss 0.0883 (0.0796)
Epoch: [0][ 56/500]	Time 30.544 (30.544)	Loss 0.2961 (0.3069)	CeLoss 0.1484 (0.1783)	SegCLSLoss 0.1226 (0.0965)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1158 (0.0952)	MaskBCELoss 0.0192 (0.0217)	MaskDICELoss 0.0966 (0.0735)
Epoch: [0][ 57/500]	Time 29.879 (29.879)	Loss 0.2824 (0.3187)	CeLoss 0.1348 (0.1892)	SegCLSLoss 0.1270 (0.1068)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1043 (0.0960)	MaskBCELoss 0.0212 (0.0188)	MaskDICELoss 0.0831 (0.0773)
Epoch: [0][ 58/500]	Time 28.272 (28.272)	Loss 0.2852 (0.3004)	CeLoss 0.1670 (0.1630)	SegCLSLoss 0.1182 (0.1196)	KLLoss 0.0001 (0.0000)	MaskLoss 0.1037 (0.1104)	MaskBCELoss 0.0045 (0.0149)	MaskDICELoss 0.0992 (0.0956)
Epoch: [0][ 59/500]	Time 28.397 (28.397)	Loss 0.5691 (0.3730)	CeLoss 0.1777 (0.1932)	SegCLSLoss 0.1260 (0.1197)	KLLoss 0.0000 (0.0000)	MaskLoss 0.2215 (0.1266)	MaskBCELoss 0.1451 (0.0379)	MaskDICELoss 0.0764 (0.0886)
Epoch: [0][ 60/500]	Time 26.990 (26.990)	Loss 0.2923 (0.4130)	CeLoss 0.1553 (0.1921)	SegCLSLoss 0.1221 (0.1164)	KLLoss 0.0000 (0.0001)	MaskLoss 0.1033 (0.1472)	MaskBCELoss 0.0167 (0.0593)	MaskDICELoss 0.0866 (0.0880)
Epoch: [0][ 61/500]	Time 29.341 (29.341)	Loss 0.3295 (0.3254)	CeLoss 0.1963 (0.1609)	SegCLSLoss 0.1118 (0.1166)	KLLoss 0.0001 (0.0001)	MaskLoss 0.1091 (0.1196)	MaskBCELoss 0.0156 (0.0310)	MaskDICELoss 0.0935 (0.0886)
Epoch: [0][ 62/500]	Time 28.327 (28.327)	Loss 0.3278 (0.3066)	CeLoss 0.1738 (0.1509)	SegCLSLoss 0.1182 (0.1184)	KLLoss 0.0001 (0.0001)	MaskLoss 0.1166 (0.1161)	MaskBCELoss 0.0244 (0.0255)	MaskDICELoss 0.0922 (0.0906)
Epoch: [0][ 63/500]	Time 27.233 (27.233)	Loss 0.2881 (0.3296)	CeLoss 0.1738 (0.1607)	SegCLSLoss 0.1104 (0.1167)	KLLoss 0.0001 (0.0002)	MaskLoss 0.1012 (0.1237)	MaskBCELoss 0.0052 (0.0324)	MaskDICELoss 0.0959 (0.0914)
Epoch: [0][ 64/500]	Time 25.185 (25.185)	Loss 0.4168 (0.3404)	CeLoss 0.1484 (0.1522)	SegCLSLoss 0.1118 (0.1157)	KLLoss 0.0001 (0.0002)	MaskLoss 0.1782 (0.1292)	MaskBCELoss 0.0820 (0.0439)	MaskDICELoss 0.0962 (0.0853)
Epoch: [0][ 65/500]	Time 27.561 (27.561)	Loss 0.3476 (0.3126)	CeLoss 0.1953 (0.1507)	SegCLSLoss 0.1143 (0.1175)	KLLoss 0.0001 (0.0001)	MaskLoss 0.1091 (0.1154)	MaskBCELoss 0.0266 (0.0301)	MaskDICELoss 0.0825 (0.0853)
Epoch: [0][ 66/500]	Time 25.203 (25.203)	Loss 0.4118 (0.3229)	CeLoss 0.1631 (0.1636)	SegCLSLoss 0.1182 (0.1063)	KLLoss 0.0001 (0.0001)	MaskLoss 0.1581 (0.1131)	MaskBCELoss 0.0739 (0.0331)	MaskDICELoss 0.0841 (0.0800)
Epoch: [0][ 67/500]	Time 26.258 (26.258)	Loss 0.3801 (0.3195)	CeLoss 0.1221 (0.1501)	SegCLSLoss 0.1226 (0.1073)	KLLoss 0.0001 (0.0001)	MaskLoss 0.1575 (0.1191)	MaskBCELoss 0.0791 (0.0375)	MaskDICELoss 0.0784 (0.0815)
Epoch: [0][ 68/500]	Time 27.332 (27.332)	Loss 0.2757 (0.2854)	CeLoss 0.1387 (0.1397)	SegCLSLoss 0.1221 (0.1069)	KLLoss 0.0001 (0.0001)	MaskLoss 0.1127 (0.1104)	MaskBCELoss 0.0129 (0.0246)	MaskDICELoss 0.0999 (0.0858)
Epoch: [0][ 69/500]	Time 27.986 (27.986)	Loss 0.2924 (0.3186)	CeLoss 0.1387 (0.1518)	SegCLSLoss 0.1196 (0.1169)	KLLoss 0.0002 (0.0002)	MaskLoss 0.1008 (0.1180)	MaskBCELoss 0.0290 (0.0329)	MaskDICELoss 0.0718 (0.0851)
Epoch: [0][ 70/500]	Time 28.228 (28.228)	Loss 0.2975 (0.2974)	CeLoss 0.1533 (0.1379)	SegCLSLoss 0.1211 (0.0941)	KLLoss 0.0002 (0.0001)	MaskLoss 0.1118 (0.1099)	MaskBCELoss 0.0183 (0.0383)	MaskDICELoss 0.0935 (0.0715)
Epoch: [0][ 71/500]	Time 27.544 (27.544)	Loss 0.4038 (0.2957)	CeLoss 0.1445 (0.1340)	SegCLSLoss 0.1182 (0.1031)	KLLoss 0.0002 (0.0004)	MaskLoss 0.1641 (0.1130)	MaskBCELoss 0.0791 (0.0357)	MaskDICELoss 0.0851 (0.0773)
Epoch: [0][ 72/500]	Time 25.628 (25.628)	Loss 0.2908 (0.3129)	CeLoss 0.1348 (0.1328)	SegCLSLoss 0.1133 (0.1145)	KLLoss 0.0003 (0.0006)	MaskLoss 0.1076 (0.1261)	MaskBCELoss 0.0309 (0.0399)	MaskDICELoss 0.0767 (0.0862)
Epoch: [0][ 73/500]	Time 26.630 (26.630)	Loss 0.3006 (0.3278)	CeLoss 0.1318 (0.1384)	SegCLSLoss 0.1143 (0.1140)	KLLoss 0.0005 (0.0006)	MaskLoss 0.1089 (0.1326)	MaskBCELoss 0.0380 (0.0440)	MaskDICELoss 0.0709 (0.0886)
Epoch: [0][ 74/500]	Time 30.509 (30.509)	Loss 0.2403 (0.2926)	CeLoss 0.0913 (0.1241)	SegCLSLoss 0.1182 (0.1160)	KLLoss 0.0003 (0.0004)	MaskLoss 0.1190 (0.1218)	MaskBCELoss 0.0205 (0.0331)	MaskDICELoss 0.0985 (0.0887)
Epoch: [0][ 75/500]	Time 25.338 (25.338)	Loss 0.3303 (0.2762)	CeLoss 0.1562 (0.1266)	SegCLSLoss 0.1118 (0.1042)	KLLoss 0.0004 (0.0003)	MaskLoss 0.1148 (0.1112)	MaskBCELoss 0.0403 (0.0279)	MaskDICELoss 0.0745 (0.0834)
Epoch: [0][ 76/500]	Time 27.780 (27.780)	Loss 0.2687 (0.2974)	CeLoss 0.1201 (0.1230)	SegCLSLoss 0.1143 (0.1146)	KLLoss 0.0002 (0.0004)	MaskLoss 0.1206 (0.1246)	MaskBCELoss 0.0207 (0.0365)	MaskDICELoss 0.1000 (0.0881)
Epoch: [0][ 77/500]	Time 28.329 (28.329)	Loss 0.2912 (0.2619)	CeLoss 0.1211 (0.1340)	SegCLSLoss 0.1211 (0.0914)	KLLoss 0.0007 (0.0003)	MaskLoss 0.1183 (0.0960)	MaskBCELoss 0.0339 (0.0229)	MaskDICELoss 0.0844 (0.0731)
Epoch: [0][ 78/500]	Time 28.117 (28.117)	Loss 0.4432 (0.3168)	CeLoss 0.1631 (0.1337)	SegCLSLoss 0.1133 (0.1134)	KLLoss 0.0008 (0.0004)	MaskLoss 0.1662 (0.1281)	MaskBCELoss 0.0932 (0.0415)	MaskDICELoss 0.0729 (0.0866)
Epoch: [0][ 79/500]	Time 27.509 (27.509)	Loss 0.3815 (0.3005)	CeLoss 0.1582 (0.1267)	SegCLSLoss 0.1123 (0.1023)	KLLoss 0.0003 (0.0004)	MaskLoss 0.1400 (0.1219)	MaskBCELoss 0.0639 (0.0412)	MaskDICELoss 0.0761 (0.0807)
Epoch: [0][ 80/500]	Time 27.454 (27.454)	Loss 0.2925 (0.2934)	CeLoss 0.1318 (0.1241)	SegCLSLoss 0.1133 (0.1156)	KLLoss 0.0006 (0.0005)	MaskLoss 0.1221 (0.1237)	MaskBCELoss 0.0282 (0.0330)	MaskDICELoss 0.0938 (0.0906)
Epoch: [0][ 81/500]	Time 28.607 (28.607)	Loss 0.2489 (0.2719)	CeLoss 0.1104 (0.1154)	SegCLSLoss 0.1104 (0.1084)	KLLoss 0.0011 (0.0009)	MaskLoss 0.1121 (0.1192)	MaskBCELoss 0.0181 (0.0285)	MaskDICELoss 0.0940 (0.0908)
Epoch: [0][ 82/500]	Time 26.565 (26.565)	Loss 0.2425 (0.2787)	CeLoss 0.1118 (0.1143)	SegCLSLoss 0.1147 (0.1084)	KLLoss 0.0006 (0.0009)	MaskLoss 0.1047 (0.1236)	MaskBCELoss 0.0143 (0.0322)	MaskDICELoss 0.0904 (0.0914)
Epoch: [0][ 83/500]	Time 26.539 (26.539)	Loss 0.2564 (0.2596)	CeLoss 0.1260 (0.1140)	SegCLSLoss 0.1069 (0.1083)	KLLoss 0.0008 (0.0010)	MaskLoss 0.1088 (0.1150)	MaskBCELoss 0.0149 (0.0225)	MaskDICELoss 0.0939 (0.0924)
Epoch: [0][ 84/500]	Time 26.593 (26.593)	Loss 0.2649 (0.2686)	CeLoss 0.1211 (0.1136)	SegCLSLoss 0.1118 (0.1090)	KLLoss 0.0015 (0.0011)	MaskLoss 0.1172 (0.1203)	MaskBCELoss 0.0198 (0.0270)	MaskDICELoss 0.0973 (0.0933)
Epoch: [0][ 85/500]	Time 26.073 (26.073)	Loss 0.2998 (0.2683)	CeLoss 0.1157 (0.1069)	SegCLSLoss 0.1094 (0.0977)	KLLoss 0.0008 (0.0008)	MaskLoss 0.1207 (0.1129)	MaskBCELoss 0.0461 (0.0374)	MaskDICELoss 0.0746 (0.0755)
Epoch: [0][ 86/500]	Time 25.632 (25.632)	Loss 0.1008 (0.2287)	CeLoss 0.1006 (0.1139)	SegCLSLoss 0.0000 (0.0883)	KLLoss 0.0000 (0.0008)	MaskLoss 0.0000 (0.0912)	MaskBCELoss 0.0000 (0.0168)	MaskDICELoss 0.0000 (0.0744)
Epoch: [0][ 87/500]	Time 27.874 (27.874)	Loss 0.2454 (0.2555)	CeLoss 0.1187 (0.1126)	SegCLSLoss 0.1118 (0.1089)	KLLoss 0.0001 (0.0008)	MaskLoss 0.1102 (0.1130)	MaskBCELoss 0.0102 (0.0213)	MaskDICELoss 0.1000 (0.0917)
Epoch: [0][ 88/500]	Time 28.834 (28.834)	Loss 0.2492 (0.2367)	CeLoss 0.1221 (0.1155)	SegCLSLoss 0.1084 (0.0992)	KLLoss 0.0010 (0.0008)	MaskLoss 0.1111 (0.0980)	MaskBCELoss 0.0114 (0.0149)	MaskDICELoss 0.0997 (0.0831)
Epoch: [0][ 89/500]	Time 31.743 (31.743)	Loss 0.3181 (0.2682)	CeLoss 0.0850 (0.1079)	SegCLSLoss 0.1064 (0.0967)	KLLoss 0.0023 (0.0014)	MaskLoss 0.1463 (0.1173)	MaskBCELoss 0.0716 (0.0356)	MaskDICELoss 0.0747 (0.0817)
Epoch: [0][ 90/500]	Time 26.120 (26.120)	Loss 0.2739 (0.2714)	CeLoss 0.1211 (0.1143)	SegCLSLoss 0.1069 (0.1101)	KLLoss 0.0013 (0.0010)	MaskLoss 0.1034 (0.1171)	MaskBCELoss 0.0315 (0.0290)	MaskDICELoss 0.0719 (0.0881)
Epoch: [0][ 91/500]	Time 28.142 (28.142)	Loss 0.2291 (0.2459)	CeLoss 0.1064 (0.1039)	SegCLSLoss 0.1084 (0.0933)	KLLoss 0.0012 (0.0018)	MaskLoss 0.1076 (0.1113)	MaskBCELoss 0.0096 (0.0265)	MaskDICELoss 0.0980 (0.0848)
Epoch: [0][ 92/500]	Time 27.975 (27.975)	Loss 0.2593 (0.2528)	CeLoss 0.1123 (0.1073)	SegCLSLoss 0.1069 (0.1043)	KLLoss 0.0019 (0.0025)	MaskLoss 0.1179 (0.1154)	MaskBCELoss 0.0231 (0.0238)	MaskDICELoss 0.0948 (0.0916)
Epoch: [0][ 93/500]	Time 29.510 (29.510)	Loss 0.0855 (0.2432)	CeLoss 0.0854 (0.1011)	SegCLSLoss 0.0000 (0.0930)	KLLoss 0.0000 (0.0022)	MaskLoss 0.0000 (0.1089)	MaskBCELoss 0.0000 (0.0274)	MaskDICELoss 0.0000 (0.0815)
Epoch: [0][ 94/500]	Time 28.102 (28.102)	Loss 0.2120 (0.2829)	CeLoss 0.0908 (0.1062)	SegCLSLoss 0.1045 (0.0942)	KLLoss 0.0012 (0.0018)	MaskLoss 0.1085 (0.1288)	MaskBCELoss 0.0094 (0.0433)	MaskDICELoss 0.0991 (0.0855)
Epoch: [0][ 95/500]	Time 28.734 (28.734)	Loss 0.2231 (0.2485)	CeLoss 0.1118 (0.1051)	SegCLSLoss 0.1040 (0.1042)	KLLoss 0.0010 (0.0018)	MaskLoss 0.1039 (0.1166)	MaskBCELoss 0.0047 (0.0219)	MaskDICELoss 0.0991 (0.0946)
Epoch: [0][ 96/500]	Time 30.565 (30.565)	Loss 0.2256 (0.2117)	CeLoss 0.1025 (0.0980)	SegCLSLoss 0.1045 (0.0819)	KLLoss 0.0020 (0.0017)	MaskLoss 0.1053 (0.0924)	MaskBCELoss 0.0122 (0.0178)	MaskDICELoss 0.0932 (0.0746)
Epoch: [0][ 97/500]	Time 26.270 (26.270)	Loss 0.4938 (0.3533)	CeLoss 0.0903 (0.1006)	SegCLSLoss 0.1025 (0.1036)	KLLoss 0.0045 (0.0026)	MaskLoss 0.2418 (0.1683)	MaskBCELoss 0.1543 (0.0779)	MaskDICELoss 0.0875 (0.0904)
Epoch: [0][ 98/500]	Time 26.759 (26.759)	Loss 0.2196 (0.2529)	CeLoss 0.1016 (0.1044)	SegCLSLoss 0.0986 (0.0946)	KLLoss 0.0016 (0.0018)	MaskLoss 0.1092 (0.1144)	MaskBCELoss 0.0095 (0.0293)	MaskDICELoss 0.0997 (0.0851)
Epoch: [0][ 99/500]	Time 26.379 (26.379)	Loss 0.1997 (0.2414)	CeLoss 0.0796 (0.1023)	SegCLSLoss 0.0986 (0.1040)	KLLoss 0.0041 (0.0021)	MaskLoss 0.1070 (0.1104)	MaskBCELoss 0.0115 (0.0213)	MaskDICELoss 0.0954 (0.0891)
[2025-03-09 10:11:12,201] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[0.00029991566265060235], mom=[(0.9, 0.95)]
[2025-03-09 10:11:12,206] [INFO] [timer.py:215:stop] epoch=0/micro_step=100/global_step=10, RunningAvgSamplesPerSec=1.4204386409304692, CurrSamplesPerSec=1.3150677990751516, MemAllocated=59.86GB, MaxMemAllocated=73.04GB
Epoch: [0][100/500]	Time 27.825 (27.825)	Loss 0.2082 (0.2385)	CeLoss 0.0967 (0.1051)	SegCLSLoss 0.0991 (0.0921)	KLLoss 0.0013 (0.0011)	MaskLoss 0.1045 (0.1079)	MaskBCELoss 0.0060 (0.0223)	MaskDICELoss 0.0985 (0.0856)
Epoch: [0][101/500]	Time 29.683 (29.683)	Loss 0.4480 (0.2281)	CeLoss 0.0913 (0.0917)	SegCLSLoss 0.1040 (0.0894)	KLLoss 0.0027 (0.0023)	MaskLoss 0.2146 (0.1092)	MaskBCELoss 0.1313 (0.0249)	MaskDICELoss 0.0833 (0.0844)
Epoch: [0][102/500]	Time 27.754 (27.754)	Loss 0.2174 (0.2082)	CeLoss 0.0967 (0.0971)	SegCLSLoss 0.1016 (0.0905)	KLLoss 0.0033 (0.0018)	MaskLoss 0.1094 (0.0971)	MaskBCELoss 0.0106 (0.0116)	MaskDICELoss 0.0988 (0.0856)
Epoch: [0][103/500]	Time 27.063 (27.063)	Loss 0.2511 (0.2273)	CeLoss 0.0850 (0.1034)	SegCLSLoss 0.1045 (0.0985)	KLLoss 0.0021 (0.0031)	MaskLoss 0.1250 (0.1084)	MaskBCELoss 0.0341 (0.0136)	MaskDICELoss 0.0909 (0.0948)
Epoch: [0][104/500]	Time 29.357 (29.357)	Loss 0.0883 (0.2419)	CeLoss 0.0884 (0.0932)	SegCLSLoss 0.0000 (0.0818)	KLLoss 0.0000 (0.0022)	MaskLoss 0.0000 (0.1111)	MaskBCELoss 0.0000 (0.0350)	MaskDICELoss 0.0000 (0.0761)
Epoch: [0][105/500]	Time 28.887 (28.887)	Loss 0.2313 (0.2621)	CeLoss 0.0889 (0.1048)	SegCLSLoss 0.1040 (0.0999)	KLLoss 0.0060 (0.0044)	MaskLoss 0.1132 (0.1234)	MaskBCELoss 0.0227 (0.0304)	MaskDICELoss 0.0904 (0.0930)
Epoch: [0][106/500]	Time 29.010 (29.010)	Loss 0.0922 (0.1858)	CeLoss 0.0923 (0.0942)	SegCLSLoss 0.0000 (0.0808)	KLLoss 0.0000 (0.0020)	MaskLoss 0.0000 (0.0824)	MaskBCELoss 0.0000 (0.0067)	MaskDICELoss 0.0000 (0.0757)
Epoch: [0][107/500]	Time 31.012 (31.012)	Loss 0.2498 (0.2197)	CeLoss 0.1201 (0.1058)	SegCLSLoss 0.0967 (0.0878)	KLLoss 0.0029 (0.0021)	MaskLoss 0.1118 (0.0988)	MaskBCELoss 0.0168 (0.0137)	MaskDICELoss 0.0950 (0.0851)
Epoch: [0][108/500]	Time 30.136 (30.136)	Loss 0.2218 (0.2046)	CeLoss 0.1147 (0.1004)	SegCLSLoss 0.0991 (0.0699)	KLLoss 0.0021 (0.0020)	MaskLoss 0.1035 (0.0840)	MaskBCELoss 0.0040 (0.0181)	MaskDICELoss 0.0995 (0.0659)
Epoch: [0][109/500]	Time 26.336 (26.336)	Loss 0.2525 (0.2403)	CeLoss 0.1226 (0.0994)	SegCLSLoss 0.0957 (0.0910)	KLLoss 0.0039 (0.0025)	MaskLoss 0.1146 (0.1090)	MaskBCELoss 0.0166 (0.0273)	MaskDICELoss 0.0980 (0.0817)
Epoch: [0][110/500]	Time 31.011 (31.011)	Loss 0.2041 (0.2660)	CeLoss 0.1001 (0.1010)	SegCLSLoss 0.0938 (0.0875)	KLLoss 0.0037 (0.0032)	MaskLoss 0.1021 (0.1221)	MaskBCELoss 0.0041 (0.0400)	MaskDICELoss 0.0980 (0.0821)
Epoch: [0][111/500]	Time 27.038 (27.038)	Loss 0.1932 (0.2257)	CeLoss 0.0801 (0.0880)	SegCLSLoss 0.0972 (0.0946)	KLLoss 0.0032 (0.0031)	MaskLoss 0.1052 (0.1145)	MaskBCELoss 0.0078 (0.0220)	MaskDICELoss 0.0974 (0.0925)
Epoch: [0][112/500]	Time 29.221 (29.221)	Loss 0.2466 (0.2641)	CeLoss 0.1055 (0.0953)	SegCLSLoss 0.0933 (0.0943)	KLLoss 0.0043 (0.0033)	MaskLoss 0.1152 (0.1319)	MaskBCELoss 0.0244 (0.0372)	MaskDICELoss 0.0907 (0.0947)
Epoch: [0][113/500]	Time 26.585 (26.585)	Loss 0.2033 (0.1939)	CeLoss 0.0879 (0.0995)	SegCLSLoss 0.1001 (0.0739)	KLLoss 0.0043 (0.0025)	MaskLoss 0.1078 (0.0852)	MaskBCELoss 0.0079 (0.0099)	MaskDICELoss 0.0999 (0.0753)
Epoch: [0][114/500]	Time 29.592 (29.592)	Loss 0.4212 (0.2670)	CeLoss 0.0908 (0.0939)	SegCLSLoss 0.1006 (0.0868)	KLLoss 0.0040 (0.0036)	MaskLoss 0.2016 (0.1269)	MaskBCELoss 0.1198 (0.0441)	MaskDICELoss 0.0817 (0.0828)
Epoch: [0][115/500]	Time 31.444 (31.444)	Loss 0.0844 (0.2329)	CeLoss 0.0845 (0.0889)	SegCLSLoss 0.0000 (0.0865)	KLLoss 0.0000 (0.0030)	MaskLoss 0.0000 (0.1145)	MaskBCELoss 0.0000 (0.0291)	MaskDICELoss 0.0000 (0.0854)
Epoch: [0][116/500]	Time 27.529 (27.529)	Loss 0.1897 (0.2273)	CeLoss 0.0830 (0.0919)	SegCLSLoss 0.0981 (0.0843)	KLLoss 0.0047 (0.0031)	MaskLoss 0.1037 (0.1091)	MaskBCELoss 0.0039 (0.0258)	MaskDICELoss 0.0999 (0.0833)
Epoch: [0][117/500]	Time 28.980 (28.980)	Loss 0.2314 (0.2291)	CeLoss 0.1040 (0.0974)	SegCLSLoss 0.0894 (0.0962)	KLLoss 0.0055 (0.0038)	MaskLoss 0.1139 (0.1091)	MaskBCELoss 0.0174 (0.0194)	MaskDICELoss 0.0965 (0.0897)
Epoch: [0][118/500]	Time 25.361 (25.361)	Loss 0.2781 (0.2240)	CeLoss 0.0913 (0.0965)	SegCLSLoss 0.1030 (0.0957)	KLLoss 0.0072 (0.0041)	MaskLoss 0.1317 (0.1124)	MaskBCELoss 0.0462 (0.0155)	MaskDICELoss 0.0855 (0.0969)
Epoch: [0][119/500]	Time 28.444 (28.444)	Loss 0.1965 (0.2308)	CeLoss 0.0894 (0.1023)	SegCLSLoss 0.0894 (0.0750)	KLLoss 0.0029 (0.0029)	MaskLoss 0.1057 (0.1022)	MaskBCELoss 0.0062 (0.0267)	MaskDICELoss 0.0995 (0.0756)
Epoch: [0][120/500]	Time 28.462 (28.462)	Loss 0.2081 (0.2284)	CeLoss 0.0854 (0.0955)	SegCLSLoss 0.1006 (0.0947)	KLLoss 0.0057 (0.0040)	MaskLoss 0.1102 (0.1127)	MaskBCELoss 0.0114 (0.0194)	MaskDICELoss 0.0988 (0.0933)
Epoch: [0][121/500]	Time 29.863 (29.863)	Loss 0.2183 (0.2173)	CeLoss 0.1084 (0.0908)	SegCLSLoss 0.0845 (0.0858)	KLLoss 0.0034 (0.0045)	MaskLoss 0.1076 (0.1114)	MaskBCELoss 0.0091 (0.0186)	MaskDICELoss 0.0984 (0.0928)
Epoch: [0][122/500]	Time 27.356 (27.356)	Loss 0.2326 (0.1917)	CeLoss 0.0947 (0.0818)	SegCLSLoss 0.0840 (0.0830)	KLLoss 0.0031 (0.0036)	MaskLoss 0.1192 (0.0982)	MaskBCELoss 0.0242 (0.0128)	MaskDICELoss 0.0951 (0.0853)
Epoch: [0][123/500]	Time 27.696 (27.696)	Loss 0.1895 (0.2161)	CeLoss 0.0845 (0.0949)	SegCLSLoss 0.0854 (0.0682)	KLLoss 0.0059 (0.0035)	MaskLoss 0.1053 (0.1022)	MaskBCELoss 0.0064 (0.0240)	MaskDICELoss 0.0989 (0.0781)
Epoch: [0][124/500]	Time 27.670 (27.670)	Loss 0.4586 (0.2581)	CeLoss 0.0757 (0.0772)	SegCLSLoss 0.0986 (0.0905)	KLLoss 0.0045 (0.0046)	MaskLoss 0.2245 (0.1354)	MaskBCELoss 0.1476 (0.0452)	MaskDICELoss 0.0769 (0.0902)
Epoch: [0][125/500]	Time 25.830 (25.830)	Loss 0.2078 (0.2238)	CeLoss 0.0972 (0.0987)	SegCLSLoss 0.0845 (0.0779)	KLLoss 0.0040 (0.0038)	MaskLoss 0.1085 (0.1052)	MaskBCELoss 0.0096 (0.0224)	MaskDICELoss 0.0988 (0.0828)
Epoch: [0][126/500]	Time 27.747 (27.747)	Loss 0.2092 (0.2384)	CeLoss 0.1006 (0.0857)	SegCLSLoss 0.0835 (0.0865)	KLLoss 0.0023 (0.0035)	MaskLoss 0.1062 (0.1215)	MaskBCELoss 0.0093 (0.0325)	MaskDICELoss 0.0969 (0.0890)
Epoch: [0][127/500]	Time 30.718 (30.718)	Loss 0.1928 (0.2432)	CeLoss 0.0908 (0.0860)	SegCLSLoss 0.0811 (0.0868)	KLLoss 0.0041 (0.0040)	MaskLoss 0.1057 (0.1277)	MaskBCELoss 0.0058 (0.0334)	MaskDICELoss 0.0999 (0.0943)
Epoch: [0][128/500]	Time 28.036 (28.036)	Loss 0.1219 (0.1910)	CeLoss 0.1221 (0.0960)	SegCLSLoss 0.0000 (0.0674)	KLLoss 0.0000 (0.0040)	MaskLoss 0.0000 (0.0870)	MaskBCELoss 0.0000 (0.0119)	MaskDICELoss 0.0000 (0.0752)
Epoch: [0][129/500]	Time 26.913 (26.913)	Loss 0.1885 (0.2589)	CeLoss 0.0947 (0.0941)	SegCLSLoss 0.0830 (0.0757)	KLLoss 0.0021 (0.0039)	MaskLoss 0.0854 (0.1232)	MaskBCELoss 0.0061 (0.0435)	MaskDICELoss 0.0793 (0.0797)
Epoch: [0][130/500]	Time 29.745 (29.745)	Loss 0.1903 (0.1847)	CeLoss 0.0728 (0.0901)	SegCLSLoss 0.0928 (0.0607)	KLLoss 0.0025 (0.0024)	MaskLoss 0.1054 (0.0796)	MaskBCELoss 0.0125 (0.0162)	MaskDICELoss 0.0928 (0.0634)
Epoch: [0][131/500]	Time 30.965 (30.965)	Loss 0.3354 (0.2126)	CeLoss 0.0742 (0.0808)	SegCLSLoss 0.0713 (0.0688)	KLLoss 0.0043 (0.0036)	MaskLoss 0.1724 (0.1080)	MaskBCELoss 0.0933 (0.0289)	MaskDICELoss 0.0791 (0.0790)
Epoch: [0][132/500]	Time 27.291 (27.291)	Loss 0.1000 (0.1844)	CeLoss 0.1001 (0.0873)	SegCLSLoss 0.0000 (0.0656)	KLLoss 0.0000 (0.0037)	MaskLoss 0.0000 (0.0906)	MaskBCELoss 0.0000 (0.0126)	MaskDICELoss 0.0000 (0.0780)
Epoch: [0][133/500]	Time 29.348 (29.348)	Loss 0.2691 (0.2176)	CeLoss 0.0713 (0.0792)	SegCLSLoss 0.0732 (0.0713)	KLLoss 0.0127 (0.0047)	MaskLoss 0.1346 (0.1102)	MaskBCELoss 0.0627 (0.0318)	MaskDICELoss 0.0719 (0.0784)
Epoch: [0][134/500]	Time 26.960 (26.960)	Loss 0.1915 (0.2058)	CeLoss 0.0757 (0.0842)	SegCLSLoss 0.0894 (0.0795)	KLLoss 0.0032 (0.0041)	MaskLoss 0.0970 (0.1107)	MaskBCELoss 0.0151 (0.0177)	MaskDICELoss 0.0819 (0.0930)
Epoch: [0][135/500]	Time 30.671 (30.671)	Loss 0.1930 (0.2062)	CeLoss 0.0698 (0.0815)	SegCLSLoss 0.0898 (0.0804)	KLLoss 0.0049 (0.0045)	MaskLoss 0.1133 (0.1129)	MaskBCELoss 0.0144 (0.0188)	MaskDICELoss 0.0990 (0.0942)
Epoch: [0][136/500]	Time 27.047 (27.047)	Loss 0.2283 (0.2166)	CeLoss 0.0806 (0.0849)	SegCLSLoss 0.0786 (0.0792)	KLLoss 0.0068 (0.0048)	MaskLoss 0.1064 (0.1147)	MaskBCELoss 0.0371 (0.0232)	MaskDICELoss 0.0694 (0.0916)
Epoch: [0][137/500]	Time 29.837 (29.837)	Loss 0.1901 (0.1953)	CeLoss 0.0664 (0.0732)	SegCLSLoss 0.0918 (0.0847)	KLLoss 0.0043 (0.0054)	MaskLoss 0.1103 (0.1110)	MaskBCELoss 0.0149 (0.0162)	MaskDICELoss 0.0954 (0.0948)
Epoch: [0][138/500]	Time 28.780 (28.780)	Loss 0.4005 (0.2296)	CeLoss 0.0708 (0.0758)	SegCLSLoss 0.0889 (0.0819)	KLLoss 0.0042 (0.0046)	MaskLoss 0.2044 (0.1244)	MaskBCELoss 0.1218 (0.0339)	MaskDICELoss 0.0825 (0.0905)
Epoch: [0][139/500]	Time 29.059 (29.059)	Loss 0.2077 (0.2195)	CeLoss 0.0693 (0.0911)	SegCLSLoss 0.0889 (0.0717)	KLLoss 0.0060 (0.0041)	MaskLoss 0.1221 (0.1075)	MaskBCELoss 0.0222 (0.0260)	MaskDICELoss 0.0999 (0.0815)
Epoch: [0][140/500]	Time 28.646 (28.646)	Loss 0.1806 (0.2166)	CeLoss 0.0654 (0.0800)	SegCLSLoss 0.0889 (0.0795)	KLLoss 0.0044 (0.0041)	MaskLoss 0.1094 (0.1174)	MaskBCELoss 0.0105 (0.0254)	MaskDICELoss 0.0989 (0.0920)
Epoch: [0][141/500]	Time 26.201 (26.201)	Loss 0.2620 (0.2102)	CeLoss 0.0669 (0.0686)	SegCLSLoss 0.0830 (0.0756)	KLLoss 0.0077 (0.0047)	MaskLoss 0.1370 (0.1206)	MaskBCELoss 0.0566 (0.0290)	MaskDICELoss 0.0804 (0.0916)
Epoch: [0][142/500]	Time 26.018 (26.018)	Loss 0.1780 (0.2100)	CeLoss 0.0601 (0.0802)	SegCLSLoss 0.0806 (0.0716)	KLLoss 0.0042 (0.0054)	MaskLoss 0.1107 (0.1133)	MaskBCELoss 0.0151 (0.0248)	MaskDICELoss 0.0956 (0.0885)
Epoch: [0][143/500]	Time 30.570 (30.570)	Loss 0.0527 (0.1791)	CeLoss 0.0527 (0.0768)	SegCLSLoss 0.0000 (0.0537)	KLLoss 0.0000 (0.0041)	MaskLoss 0.0000 (0.0936)	MaskBCELoss 0.0000 (0.0191)	MaskDICELoss 0.0000 (0.0744)
Epoch: [0][144/500]	Time 27.776 (27.776)	Loss 0.3626 (0.1879)	CeLoss 0.0991 (0.0663)	SegCLSLoss 0.0625 (0.0584)	KLLoss 0.0121 (0.0038)	MaskLoss 0.1685 (0.0963)	MaskBCELoss 0.0986 (0.0295)	MaskDICELoss 0.0699 (0.0669)
Epoch: [0][145/500]	Time 30.194 (30.194)	Loss 0.2244 (0.2277)	CeLoss 0.0986 (0.0886)	SegCLSLoss 0.0669 (0.0612)	KLLoss 0.0053 (0.0045)	MaskLoss 0.1083 (0.1106)	MaskBCELoss 0.0255 (0.0354)	MaskDICELoss 0.0827 (0.0752)
Epoch: [0][146/500]	Time 27.302 (27.302)	Loss 0.2737 (0.2077)	CeLoss 0.1055 (0.0904)	SegCLSLoss 0.0610 (0.0622)	KLLoss 0.0055 (0.0044)	MaskLoss 0.1358 (0.1058)	MaskBCELoss 0.0466 (0.0223)	MaskDICELoss 0.0892 (0.0835)
Epoch: [0][147/500]	Time 27.155 (27.155)	Loss 0.2327 (0.2295)	CeLoss 0.0635 (0.0780)	SegCLSLoss 0.0845 (0.0716)	KLLoss 0.0067 (0.0047)	MaskLoss 0.1329 (0.1229)	MaskBCELoss 0.0405 (0.0362)	MaskDICELoss 0.0924 (0.0867)
Epoch: [0][148/500]	Time 27.351 (27.351)	Loss 0.3196 (0.2506)	CeLoss 0.0559 (0.0720)	SegCLSLoss 0.0825 (0.0758)	KLLoss 0.0023 (0.0047)	MaskLoss 0.1662 (0.1391)	MaskBCELoss 0.0931 (0.0476)	MaskDICELoss 0.0731 (0.0915)
Epoch: [0][149/500]	Time 27.309 (27.309)	Loss 0.1850 (0.2031)	CeLoss 0.0635 (0.0760)	SegCLSLoss 0.0598 (0.0689)	KLLoss 0.0072 (0.0060)	MaskLoss 0.1144 (0.1162)	MaskBCELoss 0.0227 (0.0229)	MaskDICELoss 0.0917 (0.0933)
Epoch: [0][150/500]	Time 27.024 (27.024)	Loss 0.2460 (0.2606)	CeLoss 0.0996 (0.0864)	SegCLSLoss 0.0635 (0.0696)	KLLoss 0.0065 (0.0059)	MaskLoss 0.1137 (0.1349)	MaskBCELoss 0.0386 (0.0479)	MaskDICELoss 0.0751 (0.0870)
Epoch: [0][151/500]	Time 25.227 (25.227)	Loss 0.2069 (0.2058)	CeLoss 0.1030 (0.0640)	SegCLSLoss 0.0552 (0.0677)	KLLoss 0.0052 (0.0072)	MaskLoss 0.1101 (0.1212)	MaskBCELoss 0.0143 (0.0316)	MaskDICELoss 0.0958 (0.0896)
Epoch: [0][152/500]	Time 25.403 (25.403)	Loss 0.3755 (0.1944)	CeLoss 0.0605 (0.0587)	SegCLSLoss 0.0732 (0.0636)	KLLoss 0.0094 (0.0052)	MaskLoss 0.1988 (0.1131)	MaskBCELoss 0.1195 (0.0316)	MaskDICELoss 0.0792 (0.0816)
Epoch: [0][153/500]	Time 28.880 (28.880)	Loss 0.1663 (0.1805)	CeLoss 0.0483 (0.0771)	SegCLSLoss 0.0737 (0.0507)	KLLoss 0.0071 (0.0048)	MaskLoss 0.1146 (0.0936)	MaskBCELoss 0.0159 (0.0208)	MaskDICELoss 0.0987 (0.0728)
Epoch: [0][154/500]	Time 30.534 (30.534)	Loss 0.1543 (0.2352)	CeLoss 0.0698 (0.0697)	SegCLSLoss 0.0481 (0.0626)	KLLoss 0.0053 (0.0068)	MaskLoss 0.1053 (0.1341)	MaskBCELoss 0.0053 (0.0448)	MaskDICELoss 0.1000 (0.0894)
Epoch: [0][155/500]	Time 25.710 (25.710)	Loss 0.2360 (0.2211)	CeLoss 0.0947 (0.0758)	SegCLSLoss 0.0491 (0.0608)	KLLoss 0.0050 (0.0055)	MaskLoss 0.1268 (0.1247)	MaskBCELoss 0.0359 (0.0351)	MaskDICELoss 0.0909 (0.0896)
Epoch: [0][156/500]	Time 28.493 (28.493)	Loss 0.2475 (0.2220)	CeLoss 0.0947 (0.0750)	SegCLSLoss 0.0562 (0.0609)	KLLoss 0.0065 (0.0081)	MaskLoss 0.1277 (0.1264)	MaskBCELoss 0.0401 (0.0355)	MaskDICELoss 0.0875 (0.0909)
Epoch: [0][157/500]	Time 25.751 (25.751)	Loss 0.2118 (0.2033)	CeLoss 0.0811 (0.0784)	SegCLSLoss 0.0520 (0.0604)	KLLoss 0.0079 (0.0063)	MaskLoss 0.1083 (0.1177)	MaskBCELoss 0.0337 (0.0239)	MaskDICELoss 0.0745 (0.0939)
Epoch: [0][158/500]	Time 25.460 (25.460)	Loss 0.2016 (0.1875)	CeLoss 0.0972 (0.0701)	SegCLSLoss 0.0579 (0.0581)	KLLoss 0.0072 (0.0062)	MaskLoss 0.1121 (0.1069)	MaskBCELoss 0.0126 (0.0232)	MaskDICELoss 0.0995 (0.0837)
Epoch: [0][159/500]	Time 28.128 (28.128)	Loss 0.0531 (0.1741)	CeLoss 0.0532 (0.0743)	SegCLSLoss 0.0000 (0.0458)	KLLoss 0.0000 (0.0047)	MaskLoss 0.0000 (0.0851)	MaskBCELoss 0.0000 (0.0229)	MaskDICELoss 0.0000 (0.0622)
Epoch: [0][160/500]	Time 27.426 (27.426)	Loss 0.1959 (0.1974)	CeLoss 0.0923 (0.0695)	SegCLSLoss 0.0515 (0.0567)	KLLoss 0.0050 (0.0050)	MaskLoss 0.1045 (0.1078)	MaskBCELoss 0.0172 (0.0304)	MaskDICELoss 0.0873 (0.0774)
Epoch: [0][161/500]	Time 29.235 (29.235)	Loss 0.1682 (0.1833)	CeLoss 0.0684 (0.0564)	SegCLSLoss 0.0432 (0.0546)	KLLoss 0.0104 (0.0072)	MaskLoss 0.1134 (0.1165)	MaskBCELoss 0.0144 (0.0276)	MaskDICELoss 0.0990 (0.0890)
Epoch: [0][162/500]	Time 28.983 (28.983)	Loss 0.0539 (0.1516)	CeLoss 0.0540 (0.0526)	SegCLSLoss 0.0000 (0.0503)	KLLoss 0.0000 (0.0065)	MaskLoss 0.0000 (0.1008)	MaskBCELoss 0.0000 (0.0156)	MaskDICELoss 0.0000 (0.0852)
Epoch: [0][163/500]	Time 27.332 (27.332)	Loss 0.1482 (0.1894)	CeLoss 0.0520 (0.0661)	SegCLSLoss 0.0659 (0.0532)	KLLoss 0.0065 (0.0070)	MaskLoss 0.1064 (0.1156)	MaskBCELoss 0.0065 (0.0259)	MaskDICELoss 0.0999 (0.0897)
Epoch: [0][164/500]	Time 29.053 (29.053)	Loss 0.1562 (0.1887)	CeLoss 0.0454 (0.0715)	SegCLSLoss 0.0610 (0.0523)	KLLoss 0.0077 (0.0070)	MaskLoss 0.1129 (0.1140)	MaskBCELoss 0.0160 (0.0226)	MaskDICELoss 0.0969 (0.0914)
Epoch: [0][165/500]	Time 26.300 (26.300)	Loss 0.1716 (0.1912)	CeLoss 0.0520 (0.0632)	SegCLSLoss 0.0586 (0.0522)	KLLoss 0.0090 (0.0064)	MaskLoss 0.1192 (0.1177)	MaskBCELoss 0.0205 (0.0287)	MaskDICELoss 0.0988 (0.0890)
Epoch: [0][166/500]	Time 26.673 (26.673)	Loss 0.2105 (0.1968)	CeLoss 0.1157 (0.0696)	SegCLSLoss 0.0474 (0.0538)	KLLoss 0.0047 (0.0068)	MaskLoss 0.1083 (0.1181)	MaskBCELoss 0.0115 (0.0276)	MaskDICELoss 0.0968 (0.0905)
Epoch: [0][167/500]	Time 26.282 (26.282)	Loss 0.1776 (0.2072)	CeLoss 0.0859 (0.0756)	SegCLSLoss 0.0503 (0.0538)	KLLoss 0.0071 (0.0066)	MaskLoss 0.1080 (0.1186)	MaskBCELoss 0.0084 (0.0302)	MaskDICELoss 0.0996 (0.0883)
Epoch: [0][168/500]	Time 27.836 (27.836)	Loss 0.1585 (0.2114)	CeLoss 0.0457 (0.0692)	SegCLSLoss 0.0640 (0.0557)	KLLoss 0.0046 (0.0079)	MaskLoss 0.1065 (0.1244)	MaskBCELoss 0.0183 (0.0347)	MaskDICELoss 0.0882 (0.0897)
Epoch: [0][169/500]	Time 29.654 (29.654)	Loss 0.1753 (0.1831)	CeLoss 0.0786 (0.0793)	SegCLSLoss 0.0435 (0.0426)	KLLoss 0.0093 (0.0061)	MaskLoss 0.1111 (0.1022)	MaskBCELoss 0.0132 (0.0210)	MaskDICELoss 0.0979 (0.0812)
Epoch: [0][170/500]	Time 26.822 (26.822)	Loss 0.1452 (0.1928)	CeLoss 0.0464 (0.0665)	SegCLSLoss 0.0669 (0.0504)	KLLoss 0.0052 (0.0070)	MaskLoss 0.1068 (0.1098)	MaskBCELoss 0.0081 (0.0307)	MaskDICELoss 0.0987 (0.0790)
Epoch: [0][171/500]	Time 24.150 (24.150)	Loss 0.1374 (0.1865)	CeLoss 0.0461 (0.0610)	SegCLSLoss 0.0562 (0.0478)	KLLoss 0.0081 (0.0085)	MaskLoss 0.1064 (0.1215)	MaskBCELoss 0.0067 (0.0272)	MaskDICELoss 0.0997 (0.0943)
Epoch: [0][172/500]	Time 24.276 (24.276)	Loss 0.1798 (0.1854)	CeLoss 0.0669 (0.0687)	SegCLSLoss 0.0400 (0.0408)	KLLoss 0.0091 (0.0082)	MaskLoss 0.1196 (0.1174)	MaskBCELoss 0.0222 (0.0251)	MaskDICELoss 0.0974 (0.0923)
Epoch: [0][173/500]	Time 25.676 (25.676)	Loss 0.2501 (0.1817)	CeLoss 0.0630 (0.0661)	SegCLSLoss 0.0310 (0.0261)	KLLoss 0.0117 (0.0070)	MaskLoss 0.1413 (0.0975)	MaskBCELoss 0.0673 (0.0359)	MaskDICELoss 0.0740 (0.0617)
Epoch: [0][174/500]	Time 27.005 (27.005)	Loss 0.2236 (0.1983)	CeLoss 0.0664 (0.0659)	SegCLSLoss 0.0364 (0.0379)	KLLoss 0.0063 (0.0082)	MaskLoss 0.1217 (0.1151)	MaskBCELoss 0.0523 (0.0373)	MaskDICELoss 0.0694 (0.0778)
Epoch: [0][175/500]	Time 28.712 (28.712)	Loss 0.1377 (0.1835)	CeLoss 0.0447 (0.0715)	SegCLSLoss 0.0520 (0.0357)	KLLoss 0.0058 (0.0083)	MaskLoss 0.0948 (0.1052)	MaskBCELoss 0.0130 (0.0277)	MaskDICELoss 0.0817 (0.0776)
Epoch: [0][176/500]	Time 26.663 (26.663)	Loss 0.1711 (0.1978)	CeLoss 0.0586 (0.0636)	SegCLSLoss 0.0352 (0.0469)	KLLoss 0.0088 (0.0105)	MaskLoss 0.1107 (0.1217)	MaskBCELoss 0.0264 (0.0332)	MaskDICELoss 0.0843 (0.0885)
Epoch: [0][177/500]	Time 26.543 (26.543)	Loss 0.1541 (0.2044)	CeLoss 0.0396 (0.0602)	SegCLSLoss 0.0508 (0.0447)	KLLoss 0.0118 (0.0083)	MaskLoss 0.1133 (0.1290)	MaskBCELoss 0.0218 (0.0382)	MaskDICELoss 0.0915 (0.0908)
Epoch: [0][178/500]	Time 25.011 (25.011)	Loss 0.1953 (0.1752)	CeLoss 0.0854 (0.0749)	SegCLSLoss 0.0447 (0.0344)	KLLoss 0.0188 (0.0087)	MaskLoss 0.1118 (0.1030)	MaskBCELoss 0.0211 (0.0211)	MaskDICELoss 0.0907 (0.0819)
Epoch: [0][179/500]	Time 28.941 (28.941)	Loss 0.0887 (0.1697)	CeLoss 0.0889 (0.0716)	SegCLSLoss 0.0000 (0.0328)	KLLoss 0.0000 (0.0061)	MaskLoss 0.0000 (0.0937)	MaskBCELoss 0.0000 (0.0232)	MaskDICELoss 0.0000 (0.0705)
Epoch: [0][180/500]	Time 27.920 (27.920)	Loss 0.2190 (0.1851)	CeLoss 0.0620 (0.0574)	SegCLSLoss 0.0349 (0.0435)	KLLoss 0.0085 (0.0097)	MaskLoss 0.1352 (0.1184)	MaskBCELoss 0.0478 (0.0311)	MaskDICELoss 0.0873 (0.0874)
Epoch: [0][181/500]	Time 29.527 (29.527)	Loss 0.1823 (0.1693)	CeLoss 0.1001 (0.0584)	SegCLSLoss 0.0264 (0.0358)	KLLoss 0.0067 (0.0106)	MaskLoss 0.0996 (0.1167)	MaskBCELoss 0.0128 (0.0230)	MaskDICELoss 0.0868 (0.0937)
Epoch: [0][182/500]	Time 28.835 (28.835)	Loss 0.0594 (0.1473)	CeLoss 0.0593 (0.0577)	SegCLSLoss 0.0000 (0.0268)	KLLoss 0.0000 (0.0090)	MaskLoss 0.0000 (0.0938)	MaskBCELoss 0.0000 (0.0195)	MaskDICELoss 0.0000 (0.0743)
Epoch: [0][183/500]	Time 29.425 (29.425)	Loss 0.1764 (0.1716)	CeLoss 0.0410 (0.0759)	SegCLSLoss 0.0425 (0.0277)	KLLoss 0.0109 (0.0091)	MaskLoss 0.1213 (0.1019)	MaskBCELoss 0.0355 (0.0206)	MaskDICELoss 0.0859 (0.0813)
Epoch: [0][184/500]	Time 27.053 (27.053)	Loss 0.1506 (0.2156)	CeLoss 0.0620 (0.0596)	SegCLSLoss 0.0220 (0.0365)	KLLoss 0.0183 (0.0110)	MaskLoss 0.1137 (0.1351)	MaskBCELoss 0.0138 (0.0468)	MaskDICELoss 0.0998 (0.0883)
Epoch: [0][185/500]	Time 27.061 (27.061)	Loss 0.1604 (0.1562)	CeLoss 0.0405 (0.0622)	SegCLSLoss 0.0427 (0.0287)	KLLoss 0.0051 (0.0074)	MaskLoss 0.1100 (0.0959)	MaskBCELoss 0.0289 (0.0211)	MaskDICELoss 0.0811 (0.0748)
Epoch: [0][186/500]	Time 28.642 (28.642)	Loss 0.1567 (0.1904)	CeLoss 0.0757 (0.0594)	SegCLSLoss 0.0234 (0.0334)	KLLoss 0.0083 (0.0105)	MaskLoss 0.1038 (0.1259)	MaskBCELoss 0.0115 (0.0342)	MaskDICELoss 0.0923 (0.0916)
Epoch: [0][187/500]	Time 27.146 (27.146)	Loss 0.4032 (0.1998)	CeLoss 0.1045 (0.0711)	SegCLSLoss 0.0261 (0.0329)	KLLoss 0.0151 (0.0096)	MaskLoss 0.2048 (0.1246)	MaskBCELoss 0.1219 (0.0333)	MaskDICELoss 0.0830 (0.0913)
Epoch: [0][188/500]	Time 31.829 (31.829)	Loss 0.1526 (0.1382)	CeLoss 0.0757 (0.0589)	SegCLSLoss 0.0261 (0.0268)	KLLoss 0.0063 (0.0086)	MaskLoss 0.1066 (0.0960)	MaskBCELoss 0.0068 (0.0120)	MaskDICELoss 0.0998 (0.0840)
Epoch: [0][189/500]	Time 28.071 (28.071)	Loss 0.1854 (0.1944)	CeLoss 0.0447 (0.0665)	SegCLSLoss 0.0444 (0.0320)	KLLoss 0.0131 (0.0121)	MaskLoss 0.1305 (0.1254)	MaskBCELoss 0.0356 (0.0328)	MaskDICELoss 0.0949 (0.0926)
Epoch: [0][190/500]	Time 29.003 (29.003)	Loss 0.0809 (0.1758)	CeLoss 0.0811 (0.0628)	SegCLSLoss 0.0000 (0.0256)	KLLoss 0.0000 (0.0089)	MaskLoss 0.0000 (0.1054)	MaskBCELoss 0.0000 (0.0317)	MaskDICELoss 0.0000 (0.0737)
Epoch: [0][191/500]	Time 28.866 (28.866)	Loss 0.1323 (0.1583)	CeLoss 0.0591 (0.0610)	SegCLSLoss 0.0177 (0.0217)	KLLoss 0.0140 (0.0097)	MaskLoss 0.1052 (0.1053)	MaskBCELoss 0.0078 (0.0226)	MaskDICELoss 0.0974 (0.0827)
Epoch: [0][192/500]	Time 27.296 (27.296)	Loss 0.0527 (0.1481)	CeLoss 0.0527 (0.0520)	SegCLSLoss 0.0000 (0.0237)	KLLoss 0.0000 (0.0104)	MaskLoss 0.0000 (0.0961)	MaskBCELoss 0.0000 (0.0241)	MaskDICELoss 0.0000 (0.0720)
Epoch: [0][193/500]	Time 28.253 (28.253)	Loss 0.1525 (0.1258)	CeLoss 0.0435 (0.0531)	SegCLSLoss 0.0337 (0.0217)	KLLoss 0.0145 (0.0099)	MaskLoss 0.1177 (0.0870)	MaskBCELoss 0.0223 (0.0122)	MaskDICELoss 0.0954 (0.0748)
Epoch: [0][194/500]	Time 30.485 (30.485)	Loss 0.1257 (0.1614)	CeLoss 0.0427 (0.0586)	SegCLSLoss 0.0327 (0.0210)	KLLoss 0.0060 (0.0092)	MaskLoss 0.1081 (0.1050)	MaskBCELoss 0.0082 (0.0266)	MaskDICELoss 0.0999 (0.0784)
Epoch: [0][195/500]	Time 23.294 (23.294)	Loss 0.2088 (0.1634)	CeLoss 0.1240 (0.0713)	SegCLSLoss 0.0271 (0.0262)	KLLoss 0.0168 (0.0110)	MaskLoss 0.1040 (0.1030)	MaskBCELoss 0.0129 (0.0183)	MaskDICELoss 0.0912 (0.0847)
Epoch: [0][196/500]	Time 27.834 (27.834)	Loss 0.1396 (0.1414)	CeLoss 0.0415 (0.0570)	SegCLSLoss 0.0337 (0.0245)	KLLoss 0.0156 (0.0119)	MaskLoss 0.1139 (0.1096)	MaskBCELoss 0.0163 (0.0116)	MaskDICELoss 0.0976 (0.0980)
Epoch: [0][197/500]	Time 28.275 (28.275)	Loss 0.1591 (0.1765)	CeLoss 0.0981 (0.0754)	SegCLSLoss 0.0198 (0.0173)	KLLoss 0.0087 (0.0084)	MaskLoss 0.0807 (0.0976)	MaskBCELoss 0.0073 (0.0291)	MaskDICELoss 0.0734 (0.0685)
Epoch: [0][198/500]	Time 25.079 (25.079)	Loss 0.1446 (0.1443)	CeLoss 0.0679 (0.0517)	SegCLSLoss 0.0168 (0.0282)	KLLoss 0.0143 (0.0107)	MaskLoss 0.1087 (0.1091)	MaskBCELoss 0.0091 (0.0160)	MaskDICELoss 0.0996 (0.0931)
Epoch: [0][199/500]	Time 25.427 (25.427)	Loss 0.1477 (0.1217)	CeLoss 0.0806 (0.0675)	SegCLSLoss 0.0198 (0.0141)	KLLoss 0.0081 (0.0048)	MaskLoss 0.1037 (0.0565)	MaskBCELoss 0.0038 (0.0126)	MaskDICELoss 0.1000 (0.0439)
[2025-03-09 10:57:47,610] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[0.0002985903614457831], mom=[(0.9, 0.95)]
[2025-03-09 10:57:47,615] [INFO] [timer.py:215:stop] epoch=0/micro_step=200/global_step=20, RunningAvgSamplesPerSec=1.4364436097055606, CurrSamplesPerSec=1.5799250603268022, MemAllocated=60.0GB, MaxMemAllocated=73.04GB
Epoch: [0][200/500]	Time 29.948 (29.948)	Loss 0.1511 (0.1876)	CeLoss 0.0579 (0.0759)	SegCLSLoss 0.0171 (0.0196)	KLLoss 0.0092 (0.0106)	MaskLoss 0.1105 (0.1178)	MaskBCELoss 0.0197 (0.0287)	MaskDICELoss 0.0908 (0.0891)
Epoch: [0][201/500]	Time 28.107 (28.107)	Loss 0.1051 (0.1586)	CeLoss 0.0376 (0.0537)	SegCLSLoss 0.0212 (0.0148)	KLLoss 0.0178 (0.0121)	MaskLoss 0.0974 (0.1101)	MaskBCELoss 0.0056 (0.0283)	MaskDICELoss 0.0918 (0.0817)
Epoch: [0][202/500]	Time 27.521 (27.521)	Loss 0.1682 (0.1707)	CeLoss 0.0942 (0.0683)	SegCLSLoss 0.0133 (0.0172)	KLLoss 0.0104 (0.0131)	MaskLoss 0.1065 (0.1127)	MaskBCELoss 0.0095 (0.0249)	MaskDICELoss 0.0970 (0.0878)
Epoch: [0][203/500]	Time 26.811 (26.811)	Loss 0.0762 (0.1336)	CeLoss 0.0762 (0.0551)	SegCLSLoss 0.0000 (0.0135)	KLLoss 0.0000 (0.0103)	MaskLoss 0.0000 (0.0834)	MaskBCELoss 0.0000 (0.0200)	MaskDICELoss 0.0000 (0.0633)
Epoch: [0][204/500]	Time 29.782 (29.782)	Loss 0.1524 (0.1294)	CeLoss 0.0791 (0.0582)	SegCLSLoss 0.0132 (0.0154)	KLLoss 0.0154 (0.0101)	MaskLoss 0.1083 (0.0856)	MaskBCELoss 0.0085 (0.0138)	MaskDICELoss 0.0997 (0.0719)
Epoch: [0][205/500]	Time 28.211 (28.211)	Loss 0.1417 (0.1524)	CeLoss 0.0371 (0.0641)	SegCLSLoss 0.0237 (0.0146)	KLLoss 0.0123 (0.0109)	MaskLoss 0.1175 (0.1001)	MaskBCELoss 0.0227 (0.0206)	MaskDICELoss 0.0947 (0.0796)
Epoch: [0][206/500]	Time 29.154 (29.154)	Loss 0.1244 (0.1630)	CeLoss 0.0354 (0.0520)	SegCLSLoss 0.0184 (0.0197)	KLLoss 0.0124 (0.0142)	MaskLoss 0.1134 (0.1161)	MaskBCELoss 0.0154 (0.0288)	MaskDICELoss 0.0980 (0.0874)
Epoch: [0][207/500]	Time 27.248 (27.248)	Loss 0.1385 (0.1691)	CeLoss 0.0679 (0.0517)	SegCLSLoss 0.0129 (0.0190)	KLLoss 0.0110 (0.0112)	MaskLoss 0.1069 (0.1178)	MaskBCELoss 0.0072 (0.0327)	MaskDICELoss 0.0997 (0.0850)
Epoch: [0][208/500]	Time 27.424 (27.424)	Loss 0.1758 (0.2032)	CeLoss 0.0923 (0.0753)	SegCLSLoss 0.0164 (0.0160)	KLLoss 0.0129 (0.0130)	MaskLoss 0.1122 (0.1198)	MaskBCELoss 0.0131 (0.0400)	MaskDICELoss 0.0990 (0.0798)
Epoch: [0][209/500]	Time 27.694 (27.694)	Loss 0.1142 (0.1504)	CeLoss 0.0405 (0.0657)	SegCLSLoss 0.0247 (0.0199)	KLLoss 0.0123 (0.0120)	MaskLoss 0.1054 (0.1107)	MaskBCELoss 0.0056 (0.0129)	MaskDICELoss 0.0998 (0.0978)
Epoch: [0][210/500]	Time 29.274 (29.274)	Loss 0.1502 (0.1634)	CeLoss 0.0649 (0.0757)	SegCLSLoss 0.0208 (0.0129)	KLLoss 0.0204 (0.0119)	MaskLoss 0.1020 (0.0935)	MaskBCELoss 0.0159 (0.0230)	MaskDICELoss 0.0861 (0.0705)
Epoch: [0][211/500]	Time 29.186 (29.186)	Loss 0.1412 (0.1620)	CeLoss 0.0325 (0.0666)	SegCLSLoss 0.0173 (0.0131)	KLLoss 0.0107 (0.0156)	MaskLoss 0.1145 (0.1140)	MaskBCELoss 0.0286 (0.0212)	MaskDICELoss 0.0859 (0.0929)
Epoch: [0][212/500]	Time 25.763 (25.763)	Loss 0.1117 (0.1482)	CeLoss 0.0435 (0.0562)	SegCLSLoss 0.0131 (0.0126)	KLLoss 0.0129 (0.0157)	MaskLoss 0.1031 (0.1027)	MaskBCELoss 0.0067 (0.0229)	MaskDICELoss 0.0964 (0.0798)
Epoch: [0][213/500]	Time 26.574 (26.574)	Loss 0.2124 (0.1569)	CeLoss 0.0605 (0.0617)	SegCLSLoss 0.0084 (0.0116)	KLLoss 0.0238 (0.0151)	MaskLoss 0.1444 (0.0979)	MaskBCELoss 0.0502 (0.0270)	MaskDICELoss 0.0942 (0.0710)
Epoch: [0][214/500]	Time 29.598 (29.598)	Loss 0.2384 (0.1308)	CeLoss 0.0869 (0.0632)	SegCLSLoss 0.0091 (0.0108)	KLLoss 0.0193 (0.0137)	MaskLoss 0.1314 (0.0858)	MaskBCELoss 0.0539 (0.0129)	MaskDICELoss 0.0774 (0.0729)
Epoch: [0][215/500]	Time 27.726 (27.726)	Loss 0.0311 (0.1324)	CeLoss 0.0310 (0.0534)	SegCLSLoss 0.0000 (0.0128)	KLLoss 0.0000 (0.0136)	MaskLoss 0.0000 (0.1002)	MaskBCELoss 0.0000 (0.0150)	MaskDICELoss 0.0000 (0.0851)
Epoch: [0][216/500]	Time 28.873 (28.873)	Loss 0.1885 (0.1701)	CeLoss 0.0635 (0.0568)	SegCLSLoss 0.0085 (0.0114)	KLLoss 0.0156 (0.0170)	MaskLoss 0.1267 (0.1236)	MaskBCELoss 0.0382 (0.0305)	MaskDICELoss 0.0885 (0.0931)
Epoch: [0][217/500]	Time 28.809 (28.809)	Loss 0.1909 (0.1421)	CeLoss 0.1118 (0.0593)	SegCLSLoss 0.0157 (0.0136)	KLLoss 0.0114 (0.0138)	MaskLoss 0.0931 (0.1021)	MaskBCELoss 0.0165 (0.0167)	MaskDICELoss 0.0765 (0.0854)
Epoch: [0][218/500]	Time 29.775 (29.775)	Loss 0.1432 (0.1519)	CeLoss 0.0299 (0.0566)	SegCLSLoss 0.0153 (0.0123)	KLLoss 0.0225 (0.0142)	MaskLoss 0.1135 (0.1045)	MaskBCELoss 0.0326 (0.0246)	MaskDICELoss 0.0808 (0.0799)
Epoch: [0][219/500]	Time 26.732 (26.732)	Loss 0.1646 (0.1563)	CeLoss 0.0874 (0.0493)	SegCLSLoss 0.0116 (0.0146)	KLLoss 0.0231 (0.0194)	MaskLoss 0.1081 (0.1175)	MaskBCELoss 0.0115 (0.0273)	MaskDICELoss 0.0966 (0.0903)
Epoch: [0][220/500]	Time 25.501 (25.501)	Loss 0.1403 (0.1612)	CeLoss 0.0303 (0.0525)	SegCLSLoss 0.0240 (0.0164)	KLLoss 0.0138 (0.0168)	MaskLoss 0.1153 (0.1168)	MaskBCELoss 0.0270 (0.0281)	MaskDICELoss 0.0884 (0.0887)
Epoch: [0][221/500]	Time 25.054 (25.054)	Loss 0.1275 (0.1422)	CeLoss 0.0547 (0.0560)	SegCLSLoss 0.0065 (0.0109)	KLLoss 0.0160 (0.0179)	MaskLoss 0.1032 (0.1082)	MaskBCELoss 0.0120 (0.0178)	MaskDICELoss 0.0912 (0.0904)
Epoch: [0][222/500]	Time 26.182 (26.182)	Loss 0.1332 (0.1504)	CeLoss 0.0586 (0.0563)	SegCLSLoss 0.0067 (0.0098)	KLLoss 0.0154 (0.0187)	MaskLoss 0.0954 (0.1091)	MaskBCELoss 0.0156 (0.0230)	MaskDICELoss 0.0798 (0.0861)
Epoch: [0][223/500]	Time 28.631 (28.631)	Loss 0.1282 (0.1406)	CeLoss 0.0703 (0.0578)	SegCLSLoss 0.0065 (0.0072)	KLLoss 0.0250 (0.0175)	MaskLoss 0.1003 (0.1008)	MaskBCELoss 0.0031 (0.0192)	MaskDICELoss 0.0972 (0.0817)
Epoch: [0][224/500]	Time 27.063 (27.063)	Loss 0.1691 (0.1700)	CeLoss 0.0261 (0.0618)	SegCLSLoss 0.0102 (0.0097)	KLLoss 0.0138 (0.0176)	MaskLoss 0.1185 (0.1179)	MaskBCELoss 0.0524 (0.0296)	MaskDICELoss 0.0661 (0.0883)
Epoch: [0][225/500]	Time 27.380 (27.380)	Loss 0.1030 (0.1894)	CeLoss 0.0344 (0.0565)	SegCLSLoss 0.0145 (0.0101)	KLLoss 0.0170 (0.0161)	MaskLoss 0.1057 (0.1245)	MaskBCELoss 0.0057 (0.0438)	MaskDICELoss 0.1000 (0.0807)
Epoch: [0][226/500]	Time 29.470 (29.470)	Loss 0.1197 (0.1712)	CeLoss 0.0444 (0.0513)	SegCLSLoss 0.0072 (0.0084)	KLLoss 0.0145 (0.0157)	MaskLoss 0.1028 (0.1212)	MaskBCELoss 0.0135 (0.0367)	MaskDICELoss 0.0894 (0.0844)
Epoch: [0][227/500]	Time 29.425 (29.425)	Loss 0.2882 (0.1679)	CeLoss 0.0693 (0.0687)	SegCLSLoss 0.0099 (0.0083)	KLLoss 0.0192 (0.0183)	MaskLoss 0.1628 (0.1141)	MaskBCELoss 0.0881 (0.0254)	MaskDICELoss 0.0747 (0.0888)
Epoch: [0][228/500]	Time 30.108 (30.108)	Loss 0.1228 (0.1629)	CeLoss 0.0291 (0.0656)	SegCLSLoss 0.0110 (0.0067)	KLLoss 0.0188 (0.0170)	MaskLoss 0.1053 (0.0983)	MaskBCELoss 0.0237 (0.0298)	MaskDICELoss 0.0816 (0.0686)
Epoch: [0][229/500]	Time 27.914 (27.914)	Loss 0.1185 (0.1816)	CeLoss 0.0601 (0.0459)	SegCLSLoss 0.0056 (0.0095)	KLLoss 0.0173 (0.0156)	MaskLoss 0.1025 (0.1277)	MaskBCELoss 0.0029 (0.0448)	MaskDICELoss 0.0996 (0.0830)
Epoch: [0][230/500]	Time 24.452 (24.452)	Loss 0.1997 (0.1852)	CeLoss 0.0947 (0.0787)	SegCLSLoss 0.0104 (0.0093)	KLLoss 0.0204 (0.0164)	MaskLoss 0.1222 (0.1187)	MaskBCELoss 0.0260 (0.0283)	MaskDICELoss 0.0962 (0.0904)
Epoch: [0][231/500]	Time 27.047 (27.047)	Loss 0.1099 (0.1620)	CeLoss 0.0342 (0.0581)	SegCLSLoss 0.0090 (0.0070)	KLLoss 0.0270 (0.0236)	MaskLoss 0.0949 (0.1136)	MaskBCELoss 0.0159 (0.0291)	MaskDICELoss 0.0790 (0.0845)
Epoch: [0][232/500]	Time 27.177 (27.177)	Loss 0.0229 (0.1298)	CeLoss 0.0229 (0.0646)	SegCLSLoss 0.0000 (0.0069)	KLLoss 0.0000 (0.0201)	MaskLoss 0.0000 (0.0944)	MaskBCELoss 0.0000 (0.0098)	MaskDICELoss 0.0000 (0.0846)
Epoch: [0][233/500]	Time 29.654 (29.654)	Loss 0.1260 (0.1965)	CeLoss 0.0299 (0.0503)	SegCLSLoss 0.0050 (0.0071)	KLLoss 0.0322 (0.0235)	MaskLoss 0.1205 (0.1396)	MaskBCELoss 0.0222 (0.0485)	MaskDICELoss 0.0982 (0.0911)
Epoch: [0][234/500]	Time 27.961 (27.961)	Loss 0.1259 (0.1790)	CeLoss 0.0574 (0.0640)	SegCLSLoss 0.0043 (0.0061)	KLLoss 0.0188 (0.0216)	MaskLoss 0.1062 (0.1246)	MaskBCELoss 0.0087 (0.0330)	MaskDICELoss 0.0975 (0.0915)
Epoch: [0][235/500]	Time 31.244 (31.244)	Loss 0.1232 (0.1525)	CeLoss 0.0535 (0.0544)	SegCLSLoss 0.0069 (0.0068)	KLLoss 0.0175 (0.0201)	MaskLoss 0.1040 (0.1151)	MaskBCELoss 0.0094 (0.0247)	MaskDICELoss 0.0946 (0.0903)
Epoch: [0][236/500]	Time 30.423 (30.423)	Loss 0.1773 (0.1695)	CeLoss 0.0713 (0.0751)	SegCLSLoss 0.0055 (0.0061)	KLLoss 0.0242 (0.0219)	MaskLoss 0.1114 (0.1153)	MaskBCELoss 0.0318 (0.0225)	MaskDICELoss 0.0796 (0.0928)
Epoch: [0][237/500]	Time 31.302 (31.302)	Loss 0.1285 (0.1082)	CeLoss 0.0554 (0.0503)	SegCLSLoss 0.0089 (0.0049)	KLLoss 0.0164 (0.0152)	MaskLoss 0.1093 (0.0706)	MaskBCELoss 0.0094 (0.0135)	MaskDICELoss 0.1000 (0.0572)
Epoch: [0][238/500]	Time 28.662 (28.662)	Loss 0.1258 (0.1088)	CeLoss 0.0620 (0.0441)	SegCLSLoss 0.0054 (0.0055)	KLLoss 0.0177 (0.0139)	MaskLoss 0.1045 (0.0770)	MaskBCELoss 0.0058 (0.0157)	MaskDICELoss 0.0987 (0.0614)
Epoch: [0][239/500]	Time 26.801 (26.801)	Loss 0.1297 (0.1373)	CeLoss 0.0654 (0.0512)	SegCLSLoss 0.0050 (0.0081)	KLLoss 0.0237 (0.0244)	MaskLoss 0.1055 (0.1051)	MaskBCELoss 0.0060 (0.0197)	MaskDICELoss 0.0995 (0.0855)
Epoch: [0][240/500]	Time 25.696 (25.696)	Loss 0.1411 (0.1542)	CeLoss 0.0459 (0.0618)	SegCLSLoss 0.0095 (0.0077)	KLLoss 0.0150 (0.0230)	MaskLoss 0.1058 (0.1154)	MaskBCELoss 0.0250 (0.0206)	MaskDICELoss 0.0809 (0.0948)
Epoch: [0][241/500]	Time 29.370 (29.370)	Loss 0.1065 (0.1425)	CeLoss 0.0342 (0.0558)	SegCLSLoss 0.0079 (0.0062)	KLLoss 0.0238 (0.0235)	MaskLoss 0.1008 (0.1098)	MaskBCELoss 0.0120 (0.0191)	MaskDICELoss 0.0888 (0.0907)
Epoch: [0][242/500]	Time 30.707 (30.707)	Loss 0.1341 (0.1366)	CeLoss 0.0515 (0.0427)	SegCLSLoss 0.0035 (0.0063)	KLLoss 0.0317 (0.0250)	MaskLoss 0.0943 (0.1121)	MaskBCELoss 0.0225 (0.0231)	MaskDICELoss 0.0718 (0.0890)
Epoch: [0][243/500]	Time 28.293 (28.293)	Loss 0.1138 (0.1356)	CeLoss 0.0435 (0.0591)	SegCLSLoss 0.0048 (0.0040)	KLLoss 0.0244 (0.0214)	MaskLoss 0.1087 (0.0997)	MaskBCELoss 0.0092 (0.0165)	MaskDICELoss 0.0995 (0.0832)
Epoch: [0][244/500]	Time 25.853 (25.853)	Loss 0.1497 (0.1493)	CeLoss 0.0344 (0.0517)	SegCLSLoss 0.0069 (0.0064)	KLLoss 0.0260 (0.0250)	MaskLoss 0.1271 (0.1140)	MaskBCELoss 0.0322 (0.0249)	MaskDICELoss 0.0949 (0.0891)
Epoch: [0][245/500]	Time 27.613 (27.613)	Loss 0.1166 (0.1330)	CeLoss 0.0215 (0.0463)	SegCLSLoss 0.0044 (0.0047)	KLLoss 0.0297 (0.0267)	MaskLoss 0.1179 (0.1131)	MaskBCELoss 0.0226 (0.0185)	MaskDICELoss 0.0953 (0.0946)
Epoch: [0][246/500]	Time 27.215 (27.215)	Loss 0.0559 (0.1474)	CeLoss 0.0559 (0.0490)	SegCLSLoss 0.0000 (0.0053)	KLLoss 0.0000 (0.0248)	MaskLoss 0.0000 (0.1074)	MaskBCELoss 0.0000 (0.0281)	MaskDICELoss 0.0000 (0.0793)
Epoch: [0][247/500]	Time 29.802 (29.802)	Loss 0.1699 (0.1537)	CeLoss 0.0986 (0.0635)	SegCLSLoss 0.0099 (0.0053)	KLLoss 0.0219 (0.0278)	MaskLoss 0.1073 (0.1138)	MaskBCELoss 0.0082 (0.0204)	MaskDICELoss 0.0991 (0.0933)
Epoch: [0][248/500]	Time 28.192 (28.192)	Loss 0.2156 (0.1491)	CeLoss 0.1162 (0.0672)	SegCLSLoss 0.0053 (0.0048)	KLLoss 0.0237 (0.0210)	MaskLoss 0.1163 (0.1031)	MaskBCELoss 0.0258 (0.0187)	MaskDICELoss 0.0905 (0.0844)
Epoch: [0][249/500]	Time 30.496 (30.496)	Loss 0.0527 (0.1338)	CeLoss 0.0527 (0.0548)	SegCLSLoss 0.0000 (0.0050)	KLLoss 0.0000 (0.0226)	MaskLoss 0.0000 (0.0971)	MaskBCELoss 0.0000 (0.0186)	MaskDICELoss 0.0000 (0.0785)
Epoch: [0][250/500]	Time 29.549 (29.549)	Loss 0.0949 (0.1258)	CeLoss 0.0325 (0.0544)	SegCLSLoss 0.0086 (0.0055)	KLLoss 0.0227 (0.0194)	MaskLoss 0.1041 (0.0891)	MaskBCELoss 0.0041 (0.0160)	MaskDICELoss 0.1000 (0.0730)
Epoch: [0][251/500]	Time 28.793 (28.793)	Loss 0.1986 (0.1763)	CeLoss 0.0466 (0.0526)	SegCLSLoss 0.0045 (0.0046)	KLLoss 0.0258 (0.0259)	MaskLoss 0.1327 (0.1276)	MaskBCELoss 0.0556 (0.0384)	MaskDICELoss 0.0771 (0.0892)
Epoch: [0][252/500]	Time 29.023 (29.023)	Loss 0.0691 (0.0997)	CeLoss 0.0693 (0.0441)	SegCLSLoss 0.0000 (0.0042)	KLLoss 0.0000 (0.0208)	MaskLoss 0.0000 (0.0845)	MaskBCELoss 0.0000 (0.0075)	MaskDICELoss 0.0000 (0.0770)
Epoch: [0][253/500]	Time 26.517 (26.517)	Loss 0.1259 (0.1523)	CeLoss 0.0327 (0.0548)	SegCLSLoss 0.0042 (0.0047)	KLLoss 0.0312 (0.0258)	MaskLoss 0.0916 (0.1063)	MaskBCELoss 0.0302 (0.0280)	MaskDICELoss 0.0614 (0.0783)
Epoch: [0][254/500]	Time 29.368 (29.368)	Loss 0.1577 (0.1947)	CeLoss 0.0457 (0.0579)	SegCLSLoss 0.0075 (0.0047)	KLLoss 0.0211 (0.0269)	MaskLoss 0.1268 (0.1339)	MaskBCELoss 0.0299 (0.0450)	MaskDICELoss 0.0970 (0.0889)
Epoch: [0][255/500]	Time 27.268 (27.268)	Loss 0.0652 (0.1176)	CeLoss 0.0654 (0.0467)	SegCLSLoss 0.0000 (0.0031)	KLLoss 0.0000 (0.0204)	MaskLoss 0.0000 (0.0806)	MaskBCELoss 0.0000 (0.0193)	MaskDICELoss 0.0000 (0.0613)
Epoch: [0][256/500]	Time 28.639 (28.639)	Loss 0.1297 (0.1494)	CeLoss 0.0659 (0.0590)	SegCLSLoss 0.0039 (0.0043)	KLLoss 0.0378 (0.0303)	MaskLoss 0.1057 (0.1129)	MaskBCELoss 0.0059 (0.0211)	MaskDICELoss 0.0998 (0.0918)
Epoch: [0][257/500]	Time 25.467 (25.467)	Loss 0.1163 (0.1727)	CeLoss 0.0520 (0.0685)	SegCLSLoss 0.0073 (0.0040)	KLLoss 0.0305 (0.0279)	MaskLoss 0.1010 (0.1179)	MaskBCELoss 0.0069 (0.0288)	MaskDICELoss 0.0941 (0.0892)
Epoch: [0][258/500]	Time 28.292 (28.292)	Loss 0.1159 (0.1455)	CeLoss 0.0471 (0.0539)	SegCLSLoss 0.0022 (0.0032)	KLLoss 0.0216 (0.0248)	MaskLoss 0.1088 (0.1092)	MaskBCELoss 0.0089 (0.0236)	MaskDICELoss 0.0999 (0.0857)
Epoch: [0][259/500]	Time 23.864 (23.864)	Loss 0.0656 (0.1346)	CeLoss 0.0654 (0.0548)	SegCLSLoss 0.0000 (0.0035)	KLLoss 0.0000 (0.0258)	MaskLoss 0.0000 (0.0944)	MaskBCELoss 0.0000 (0.0206)	MaskDICELoss 0.0000 (0.0738)
Epoch: [0][260/500]	Time 30.805 (30.805)	Loss 0.0897 (0.1237)	CeLoss 0.0291 (0.0523)	SegCLSLoss 0.0074 (0.0054)	KLLoss 0.0247 (0.0228)	MaskLoss 0.1028 (0.0989)	MaskBCELoss 0.0037 (0.0128)	MaskDICELoss 0.0991 (0.0861)
Epoch: [0][261/500]	Time 29.203 (29.203)	Loss 0.1337 (0.1528)	CeLoss 0.0601 (0.0573)	SegCLSLoss 0.0052 (0.0041)	KLLoss 0.0264 (0.0314)	MaskLoss 0.0989 (0.1139)	MaskBCELoss 0.0142 (0.0243)	MaskDICELoss 0.0847 (0.0897)
Epoch: [0][262/500]	Time 27.420 (27.420)	Loss 0.1109 (0.1428)	CeLoss 0.0322 (0.0518)	SegCLSLoss 0.0061 (0.0051)	KLLoss 0.0286 (0.0276)	MaskLoss 0.1000 (0.1063)	MaskBCELoss 0.0170 (0.0235)	MaskDICELoss 0.0829 (0.0828)
Epoch: [0][263/500]	Time 27.403 (27.403)	Loss 0.1175 (0.1323)	CeLoss 0.0635 (0.0497)	SegCLSLoss 0.0030 (0.0036)	KLLoss 0.0332 (0.0254)	MaskLoss 0.0911 (0.0991)	MaskBCELoss 0.0045 (0.0209)	MaskDICELoss 0.0866 (0.0782)
Epoch: [0][264/500]	Time 27.640 (27.640)	Loss 0.0637 (0.1778)	CeLoss 0.0635 (0.0595)	SegCLSLoss 0.0000 (0.0037)	KLLoss 0.0000 (0.0276)	MaskLoss 0.0000 (0.1142)	MaskBCELoss 0.0000 (0.0395)	MaskDICELoss 0.0000 (0.0747)
Epoch: [0][265/500]	Time 29.803 (29.803)	Loss 0.1992 (0.1491)	CeLoss 0.0889 (0.0548)	SegCLSLoss 0.0024 (0.0032)	KLLoss 0.0457 (0.0265)	MaskLoss 0.1170 (0.0963)	MaskBCELoss 0.0337 (0.0297)	MaskDICELoss 0.0833 (0.0666)
Epoch: [0][266/500]	Time 25.983 (25.983)	Loss 0.1190 (0.1506)	CeLoss 0.0405 (0.0561)	SegCLSLoss 0.0019 (0.0051)	KLLoss 0.0293 (0.0277)	MaskLoss 0.1117 (0.1138)	MaskBCELoss 0.0145 (0.0234)	MaskDICELoss 0.0971 (0.0904)
Epoch: [0][267/500]	Time 25.651 (25.651)	Loss 0.1465 (0.1416)	CeLoss 0.0718 (0.0577)	SegCLSLoss 0.0036 (0.0046)	KLLoss 0.0325 (0.0276)	MaskLoss 0.1090 (0.1029)	MaskBCELoss 0.0122 (0.0201)	MaskDICELoss 0.0968 (0.0828)
Epoch: [0][268/500]	Time 28.897 (28.897)	Loss 0.1951 (0.1223)	CeLoss 0.0869 (0.0554)	SegCLSLoss 0.0025 (0.0041)	KLLoss 0.0251 (0.0258)	MaskLoss 0.1090 (0.0949)	MaskBCELoss 0.0351 (0.0116)	MaskDICELoss 0.0739 (0.0832)
Epoch: [0][269/500]	Time 26.022 (26.022)	Loss 0.2152 (0.1604)	CeLoss 0.0869 (0.0438)	SegCLSLoss 0.0020 (0.0049)	KLLoss 0.0398 (0.0320)	MaskLoss 0.1375 (0.1238)	MaskBCELoss 0.0390 (0.0348)	MaskDICELoss 0.0985 (0.0890)
Epoch: [0][270/500]	Time 28.182 (28.182)	Loss 0.1751 (0.1435)	CeLoss 0.0270 (0.0435)	SegCLSLoss 0.0057 (0.0052)	KLLoss 0.0293 (0.0278)	MaskLoss 0.1435 (0.1142)	MaskBCELoss 0.0490 (0.0269)	MaskDICELoss 0.0945 (0.0873)
Epoch: [0][271/500]	Time 26.339 (26.339)	Loss 0.1701 (0.1615)	CeLoss 0.1016 (0.0572)	SegCLSLoss 0.0058 (0.0041)	KLLoss 0.0330 (0.0327)	MaskLoss 0.1056 (0.1196)	MaskBCELoss 0.0084 (0.0283)	MaskDICELoss 0.0972 (0.0912)
Epoch: [0][272/500]	Time 29.175 (29.175)	Loss 0.2353 (0.1651)	CeLoss 0.0289 (0.0626)	SegCLSLoss 0.0028 (0.0029)	KLLoss 0.0422 (0.0301)	MaskLoss 0.1557 (0.1116)	MaskBCELoss 0.0848 (0.0302)	MaskDICELoss 0.0709 (0.0814)
Epoch: [0][273/500]	Time 27.712 (27.712)	Loss 0.1393 (0.1484)	CeLoss 0.0303 (0.0536)	SegCLSLoss 0.0033 (0.0042)	KLLoss 0.0378 (0.0289)	MaskLoss 0.1241 (0.1125)	MaskBCELoss 0.0303 (0.0243)	MaskDICELoss 0.0938 (0.0883)
Epoch: [0][274/500]	Time 26.007 (26.007)	Loss 0.1060 (0.1347)	CeLoss 0.0317 (0.0450)	SegCLSLoss 0.0074 (0.0052)	KLLoss 0.0327 (0.0286)	MaskLoss 0.1036 (0.1040)	MaskBCELoss 0.0125 (0.0234)	MaskDICELoss 0.0911 (0.0806)
Epoch: [0][275/500]	Time 30.657 (30.657)	Loss 0.1737 (0.1290)	CeLoss 0.0732 (0.0425)	SegCLSLoss 0.0052 (0.0050)	KLLoss 0.0254 (0.0272)	MaskLoss 0.1097 (0.1085)	MaskBCELoss 0.0287 (0.0198)	MaskDICELoss 0.0810 (0.0887)
Epoch: [0][276/500]	Time 27.956 (27.956)	Loss 0.1138 (0.1319)	CeLoss 0.0347 (0.0599)	SegCLSLoss 0.0039 (0.0036)	KLLoss 0.0317 (0.0260)	MaskLoss 0.1135 (0.0973)	MaskBCELoss 0.0137 (0.0144)	MaskDICELoss 0.0998 (0.0829)
Epoch: [0][277/500]	Time 27.526 (27.526)	Loss 0.0363 (0.1425)	CeLoss 0.0364 (0.0461)	SegCLSLoss 0.0000 (0.0035)	KLLoss 0.0000 (0.0294)	MaskLoss 0.0000 (0.1068)	MaskBCELoss 0.0000 (0.0275)	MaskDICELoss 0.0000 (0.0793)
Epoch: [0][278/500]	Time 27.961 (27.961)	Loss 0.1416 (0.1415)	CeLoss 0.0649 (0.0610)	SegCLSLoss 0.0020 (0.0040)	KLLoss 0.0309 (0.0280)	MaskLoss 0.1122 (0.1091)	MaskBCELoss 0.0130 (0.0160)	MaskDICELoss 0.0993 (0.0931)
Epoch: [0][279/500]	Time 30.249 (30.249)	Loss 0.1697 (0.1201)	CeLoss 0.0796 (0.0466)	SegCLSLoss 0.0024 (0.0038)	KLLoss 0.0437 (0.0288)	MaskLoss 0.1183 (0.0999)	MaskBCELoss 0.0198 (0.0144)	MaskDICELoss 0.0985 (0.0855)
Epoch: [0][280/500]	Time 27.554 (27.554)	Loss 0.1267 (0.1261)	CeLoss 0.0270 (0.0533)	SegCLSLoss 0.0107 (0.0045)	KLLoss 0.0300 (0.0315)	MaskLoss 0.1208 (0.1087)	MaskBCELoss 0.0227 (0.0108)	MaskDICELoss 0.0980 (0.0979)
Epoch: [0][281/500]	Time 25.798 (25.798)	Loss 0.1411 (0.1317)	CeLoss 0.0635 (0.0538)	SegCLSLoss 0.0052 (0.0051)	KLLoss 0.0206 (0.0284)	MaskLoss 0.1117 (0.0991)	MaskBCELoss 0.0125 (0.0172)	MaskDICELoss 0.0992 (0.0819)
Epoch: [0][282/500]	Time 24.335 (24.335)	Loss 0.1996 (0.1673)	CeLoss 0.0967 (0.0608)	SegCLSLoss 0.0028 (0.0050)	KLLoss 0.0337 (0.0316)	MaskLoss 0.1189 (0.1140)	MaskBCELoss 0.0281 (0.0313)	MaskDICELoss 0.0908 (0.0827)
Epoch: [0][283/500]	Time 26.337 (26.337)	Loss 0.1902 (0.1511)	CeLoss 0.0635 (0.0659)	SegCLSLoss 0.0023 (0.0030)	KLLoss 0.0386 (0.0265)	MaskLoss 0.1291 (0.0977)	MaskBCELoss 0.0405 (0.0232)	MaskDICELoss 0.0886 (0.0745)
Epoch: [0][284/500]	Time 31.921 (31.921)	Loss 0.1477 (0.1265)	CeLoss 0.0258 (0.0484)	SegCLSLoss 0.0052 (0.0030)	KLLoss 0.0271 (0.0287)	MaskLoss 0.1144 (0.0978)	MaskBCELoss 0.0415 (0.0186)	MaskDICELoss 0.0729 (0.0792)
Epoch: [0][285/500]	Time 28.320 (28.320)	Loss 0.1923 (0.1811)	CeLoss 0.0825 (0.0609)	SegCLSLoss 0.0028 (0.0034)	KLLoss 0.0277 (0.0352)	MaskLoss 0.1204 (0.1248)	MaskBCELoss 0.0321 (0.0374)	MaskDICELoss 0.0883 (0.0874)
Epoch: [0][286/500]	Time 28.998 (28.998)	Loss 0.2202 (0.1411)	CeLoss 0.0713 (0.0581)	SegCLSLoss 0.0026 (0.0029)	KLLoss 0.0334 (0.0324)	MaskLoss 0.1383 (0.1087)	MaskBCELoss 0.0525 (0.0182)	MaskDICELoss 0.0858 (0.0905)
Epoch: [0][287/500]	Time 27.215 (27.215)	Loss 0.3111 (0.1356)	CeLoss 0.0581 (0.0474)	SegCLSLoss 0.0015 (0.0054)	KLLoss 0.0391 (0.0308)	MaskLoss 0.1889 (0.1145)	MaskBCELoss 0.1051 (0.0188)	MaskDICELoss 0.0838 (0.0958)
Epoch: [0][288/500]	Time 28.830 (28.830)	Loss 0.1448 (0.1159)	CeLoss 0.0806 (0.0520)	SegCLSLoss 0.0020 (0.0030)	KLLoss 0.0386 (0.0261)	MaskLoss 0.1062 (0.0856)	MaskBCELoss 0.0067 (0.0131)	MaskDICELoss 0.0995 (0.0725)
Epoch: [0][289/500]	Time 27.494 (27.494)	Loss 0.1529 (0.1180)	CeLoss 0.0928 (0.0504)	SegCLSLoss 0.0041 (0.0044)	KLLoss 0.0234 (0.0322)	MaskLoss 0.1011 (0.1046)	MaskBCELoss 0.0049 (0.0087)	MaskDICELoss 0.0962 (0.0959)
Epoch: [0][290/500]	Time 25.480 (25.480)	Loss 0.0824 (0.1402)	CeLoss 0.0270 (0.0585)	SegCLSLoss 0.0036 (0.0035)	KLLoss 0.0347 (0.0285)	MaskLoss 0.1013 (0.1025)	MaskBCELoss 0.0020 (0.0192)	MaskDICELoss 0.0994 (0.0834)
Epoch: [0][291/500]	Time 30.389 (30.389)	Loss 0.0500 (0.1287)	CeLoss 0.0500 (0.0557)	SegCLSLoss 0.0000 (0.0023)	KLLoss 0.0000 (0.0278)	MaskLoss 0.0000 (0.0902)	MaskBCELoss 0.0000 (0.0178)	MaskDICELoss 0.0000 (0.0724)
Epoch: [0][292/500]	Time 27.284 (27.284)	Loss 0.0965 (0.1249)	CeLoss 0.0425 (0.0483)	SegCLSLoss 0.0025 (0.0035)	KLLoss 0.0200 (0.0319)	MaskLoss 0.1015 (0.0986)	MaskBCELoss 0.0015 (0.0171)	MaskDICELoss 0.1000 (0.0815)
Epoch: [0][293/500]	Time 25.352 (25.352)	Loss 0.2432 (0.1537)	CeLoss 0.0869 (0.0428)	SegCLSLoss 0.0031 (0.0050)	KLLoss 0.0376 (0.0304)	MaskLoss 0.1413 (0.1215)	MaskBCELoss 0.0562 (0.0318)	MaskDICELoss 0.0851 (0.0897)
Epoch: [0][294/500]	Time 32.204 (32.204)	Loss 0.0625 (0.1030)	CeLoss 0.0625 (0.0524)	SegCLSLoss 0.0000 (0.0024)	KLLoss 0.0000 (0.0225)	MaskLoss 0.0000 (0.0752)	MaskBCELoss 0.0000 (0.0079)	MaskDICELoss 0.0000 (0.0673)
Epoch: [0][295/500]	Time 26.253 (26.253)	Loss 0.1164 (0.1470)	CeLoss 0.0591 (0.0599)	SegCLSLoss 0.0047 (0.0056)	KLLoss 0.0221 (0.0301)	MaskLoss 0.1025 (0.1096)	MaskBCELoss 0.0025 (0.0196)	MaskDICELoss 0.1000 (0.0900)
Epoch: [0][296/500]	Time 29.150 (29.150)	Loss 0.2316 (0.1299)	CeLoss 0.0287 (0.0418)	SegCLSLoss 0.0030 (0.0028)	KLLoss 0.0356 (0.0286)	MaskLoss 0.1570 (0.1024)	MaskBCELoss 0.0819 (0.0236)	MaskDICELoss 0.0751 (0.0788)
Epoch: [0][297/500]	Time 28.001 (28.001)	Loss 0.0617 (0.1269)	CeLoss 0.0618 (0.0462)	SegCLSLoss 0.0000 (0.0035)	KLLoss 0.0000 (0.0290)	MaskLoss 0.0000 (0.0992)	MaskBCELoss 0.0000 (0.0196)	MaskDICELoss 0.0000 (0.0796)
Epoch: [0][298/500]	Time 29.108 (29.108)	Loss 0.1604 (0.1624)	CeLoss 0.0659 (0.0454)	SegCLSLoss 0.0032 (0.0039)	KLLoss 0.0254 (0.0285)	MaskLoss 0.1111 (0.1179)	MaskBCELoss 0.0248 (0.0374)	MaskDICELoss 0.0863 (0.0805)
Epoch: [0][299/500]	Time 26.529 (26.529)	Loss 0.2153 (0.1808)	CeLoss 0.0762 (0.0414)	SegCLSLoss 0.0031 (0.0054)	KLLoss 0.0325 (0.0307)	MaskLoss 0.1303 (0.1343)	MaskBCELoss 0.0483 (0.0463)	MaskDICELoss 0.0820 (0.0880)
[2025-03-09 11:44:28,077] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[0.0002972650602409638], mom=[(0.9, 0.95)]
[2025-03-09 11:44:28,082] [INFO] [timer.py:215:stop] epoch=0/micro_step=300/global_step=30, RunningAvgSamplesPerSec=1.4185427903646268, CurrSamplesPerSec=1.3399510210559686, MemAllocated=59.77GB, MaxMemAllocated=73.04GB
Epoch: [0][300/500]	Time 27.593 (27.593)	Loss 0.3444 (0.1795)	CeLoss 0.0562 (0.0676)	SegCLSLoss 0.0020 (0.0039)	KLLoss 0.0439 (0.0311)	MaskLoss 0.2173 (0.1256)	MaskBCELoss 0.1189 (0.0315)	MaskDICELoss 0.0985 (0.0941)
Epoch: [0][301/500]	Time 29.200 (29.200)	Loss 0.1116 (0.1243)	CeLoss 0.0347 (0.0437)	SegCLSLoss 0.0049 (0.0034)	KLLoss 0.0403 (0.0356)	MaskLoss 0.1120 (0.1089)	MaskBCELoss 0.0123 (0.0163)	MaskDICELoss 0.0998 (0.0926)
Epoch: [0][302/500]	Time 27.819 (27.819)	Loss 0.2097 (0.1718)	CeLoss 0.1094 (0.0605)	SegCLSLoss 0.0023 (0.0037)	KLLoss 0.0386 (0.0324)	MaskLoss 0.1184 (0.1218)	MaskBCELoss 0.0269 (0.0324)	MaskDICELoss 0.0915 (0.0894)
Epoch: [0][303/500]	Time 29.133 (29.133)	Loss 0.1331 (0.1584)	CeLoss 0.0610 (0.0552)	SegCLSLoss 0.0046 (0.0034)	KLLoss 0.0327 (0.0345)	MaskLoss 0.1002 (0.1156)	MaskBCELoss 0.0131 (0.0292)	MaskDICELoss 0.0871 (0.0863)
Epoch: [0][304/500]	Time 27.187 (27.187)	Loss 0.0469 (0.1429)	CeLoss 0.0469 (0.0505)	SegCLSLoss 0.0000 (0.0034)	KLLoss 0.0000 (0.0287)	MaskLoss 0.0000 (0.1056)	MaskBCELoss 0.0000 (0.0253)	MaskDICELoss 0.0000 (0.0804)
Epoch: [0][305/500]	Time 24.550 (24.550)	Loss 0.1902 (0.1183)	CeLoss 0.0938 (0.0452)	SegCLSLoss 0.0023 (0.0032)	KLLoss 0.0309 (0.0291)	MaskLoss 0.1196 (0.0927)	MaskBCELoss 0.0236 (0.0168)	MaskDICELoss 0.0960 (0.0759)
Epoch: [0][306/500]	Time 25.356 (25.356)	Loss 0.1532 (0.1616)	CeLoss 0.0251 (0.0403)	SegCLSLoss 0.0073 (0.0035)	KLLoss 0.0254 (0.0329)	MaskLoss 0.1164 (0.1230)	MaskBCELoss 0.0441 (0.0387)	MaskDICELoss 0.0724 (0.0843)
Epoch: [0][307/500]	Time 27.674 (27.674)	Loss 0.1201 (0.1315)	CeLoss 0.0256 (0.0458)	SegCLSLoss 0.0058 (0.0029)	KLLoss 0.0376 (0.0260)	MaskLoss 0.1170 (0.0936)	MaskBCELoss 0.0221 (0.0250)	MaskDICELoss 0.0949 (0.0686)
Epoch: [0][308/500]	Time 30.636 (30.636)	Loss 0.0996 (0.1288)	CeLoss 0.0222 (0.0506)	SegCLSLoss 0.0021 (0.0029)	KLLoss 0.0366 (0.0314)	MaskLoss 0.1067 (0.0993)	MaskBCELoss 0.0152 (0.0180)	MaskDICELoss 0.0915 (0.0813)
Epoch: [0][309/500]	Time 28.001 (28.001)	Loss 0.1558 (0.1569)	CeLoss 0.0299 (0.0568)	SegCLSLoss 0.0028 (0.0037)	KLLoss 0.0378 (0.0345)	MaskLoss 0.1160 (0.1118)	MaskBCELoss 0.0444 (0.0282)	MaskDICELoss 0.0717 (0.0836)
Epoch: [0][310/500]	Time 25.121 (25.121)	Loss 0.1746 (0.1846)	CeLoss 0.0593 (0.0589)	SegCLSLoss 0.0015 (0.0036)	KLLoss 0.0330 (0.0336)	MaskLoss 0.1223 (0.1254)	MaskBCELoss 0.0355 (0.0408)	MaskDICELoss 0.0868 (0.0847)
Epoch: [0][311/500]	Time 28.234 (28.234)	Loss 0.1330 (0.1475)	CeLoss 0.0718 (0.0597)	SegCLSLoss 0.0023 (0.0038)	KLLoss 0.0344 (0.0296)	MaskLoss 0.1049 (0.1094)	MaskBCELoss 0.0050 (0.0208)	MaskDICELoss 0.0999 (0.0886)
Epoch: [0][312/500]	Time 27.709 (27.709)	Loss 0.3195 (0.1323)	CeLoss 0.0835 (0.0502)	SegCLSLoss 0.0017 (0.0037)	KLLoss 0.0447 (0.0264)	MaskLoss 0.1567 (0.0882)	MaskBCELoss 0.1045 (0.0241)	MaskDICELoss 0.0522 (0.0640)
Epoch: [0][313/500]	Time 26.171 (26.171)	Loss 0.0214 (0.1407)	CeLoss 0.0214 (0.0555)	SegCLSLoss 0.0000 (0.0034)	KLLoss 0.0000 (0.0303)	MaskLoss 0.0000 (0.1004)	MaskBCELoss 0.0000 (0.0222)	MaskDICELoss 0.0000 (0.0782)
Epoch: [0][314/500]	Time 28.641 (28.641)	Loss 0.1050 (0.1380)	CeLoss 0.0250 (0.0554)	SegCLSLoss 0.0030 (0.0027)	KLLoss 0.0309 (0.0319)	MaskLoss 0.1121 (0.1134)	MaskBCELoss 0.0149 (0.0164)	MaskDICELoss 0.0972 (0.0969)
Epoch: [0][315/500]	Time 27.888 (27.888)	Loss 0.1446 (0.1345)	CeLoss 0.0713 (0.0369)	SegCLSLoss 0.0037 (0.0036)	KLLoss 0.0342 (0.0290)	MaskLoss 0.1107 (0.1070)	MaskBCELoss 0.0108 (0.0283)	MaskDICELoss 0.0999 (0.0787)
Epoch: [0][316/500]	Time 27.934 (27.934)	Loss 0.2018 (0.1562)	CeLoss 0.0520 (0.0537)	SegCLSLoss 0.0028 (0.0035)	KLLoss 0.0376 (0.0340)	MaskLoss 0.1319 (0.1184)	MaskBCELoss 0.0549 (0.0277)	MaskDICELoss 0.0771 (0.0907)
Epoch: [0][317/500]	Time 24.399 (24.399)	Loss 0.0500 (0.1342)	CeLoss 0.0500 (0.0472)	SegCLSLoss 0.0000 (0.0039)	KLLoss 0.0000 (0.0288)	MaskLoss 0.0000 (0.1034)	MaskBCELoss 0.0000 (0.0223)	MaskDICELoss 0.0000 (0.0811)
Epoch: [0][318/500]	Time 28.574 (28.574)	Loss 0.1249 (0.1339)	CeLoss 0.0227 (0.0411)	SegCLSLoss 0.0028 (0.0035)	KLLoss 0.0437 (0.0317)	MaskLoss 0.1239 (0.1149)	MaskBCELoss 0.0259 (0.0224)	MaskDICELoss 0.0979 (0.0925)
Epoch: [0][319/500]	Time 27.739 (27.739)	Loss 0.1003 (0.1320)	CeLoss 0.0422 (0.0373)	SegCLSLoss 0.0020 (0.0027)	KLLoss 0.0258 (0.0283)	MaskLoss 0.1033 (0.1047)	MaskBCELoss 0.0037 (0.0273)	MaskDICELoss 0.0996 (0.0775)
Epoch: [0][320/500]	Time 26.965 (26.965)	Loss 0.0332 (0.1237)	CeLoss 0.0332 (0.0555)	SegCLSLoss 0.0000 (0.0029)	KLLoss 0.0000 (0.0285)	MaskLoss 0.0000 (0.0898)	MaskBCELoss 0.0000 (0.0146)	MaskDICELoss 0.0000 (0.0752)
Epoch: [0][321/500]	Time 28.966 (28.966)	Loss 0.1263 (0.1338)	CeLoss 0.0532 (0.0531)	SegCLSLoss 0.0027 (0.0026)	KLLoss 0.0332 (0.0325)	MaskLoss 0.1090 (0.1045)	MaskBCELoss 0.0114 (0.0180)	MaskDICELoss 0.0976 (0.0864)
Epoch: [0][322/500]	Time 30.218 (30.218)	Loss 0.2102 (0.1189)	CeLoss 0.0850 (0.0484)	SegCLSLoss 0.0064 (0.0030)	KLLoss 0.0261 (0.0297)	MaskLoss 0.1209 (0.0976)	MaskBCELoss 0.0410 (0.0134)	MaskDICELoss 0.0799 (0.0842)
Epoch: [0][323/500]	Time 30.071 (30.071)	Loss 0.1049 (0.1419)	CeLoss 0.0457 (0.0494)	SegCLSLoss 0.0033 (0.0038)	KLLoss 0.0286 (0.0327)	MaskLoss 0.1035 (0.1112)	MaskBCELoss 0.0038 (0.0234)	MaskDICELoss 0.0998 (0.0878)
Epoch: [0][324/500]	Time 27.126 (27.126)	Loss 0.1447 (0.1630)	CeLoss 0.0601 (0.0435)	SegCLSLoss 0.0019 (0.0043)	KLLoss 0.0349 (0.0347)	MaskLoss 0.0973 (0.1243)	MaskBCELoss 0.0234 (0.0369)	MaskDICELoss 0.0739 (0.0874)
Epoch: [0][325/500]	Time 24.919 (24.919)	Loss 0.2131 (0.1960)	CeLoss 0.0884 (0.0678)	SegCLSLoss 0.0020 (0.0031)	KLLoss 0.0415 (0.0367)	MaskLoss 0.1322 (0.1252)	MaskBCELoss 0.0384 (0.0428)	MaskDICELoss 0.0938 (0.0824)
Epoch: [0][326/500]	Time 26.487 (26.487)	Loss 0.1678 (0.1376)	CeLoss 0.0679 (0.0452)	SegCLSLoss 0.0035 (0.0044)	KLLoss 0.0383 (0.0286)	MaskLoss 0.1192 (0.1051)	MaskBCELoss 0.0258 (0.0251)	MaskDICELoss 0.0934 (0.0799)
Epoch: [0][327/500]	Time 26.915 (26.915)	Loss 0.1969 (0.1711)	CeLoss 0.1030 (0.0491)	SegCLSLoss 0.0040 (0.0045)	KLLoss 0.0376 (0.0314)	MaskLoss 0.1173 (0.1261)	MaskBCELoss 0.0219 (0.0378)	MaskDICELoss 0.0954 (0.0883)
Epoch: [0][328/500]	Time 29.556 (29.556)	Loss 0.1018 (0.1355)	CeLoss 0.0251 (0.0384)	SegCLSLoss 0.0056 (0.0037)	KLLoss 0.0369 (0.0278)	MaskLoss 0.1117 (0.1066)	MaskBCELoss 0.0120 (0.0280)	MaskDICELoss 0.0996 (0.0786)
Epoch: [0][329/500]	Time 26.029 (26.029)	Loss 0.1360 (0.1261)	CeLoss 0.0466 (0.0452)	SegCLSLoss 0.0027 (0.0026)	KLLoss 0.0266 (0.0268)	MaskLoss 0.0901 (0.0931)	MaskBCELoss 0.0286 (0.0220)	MaskDICELoss 0.0616 (0.0711)
Epoch: [0][330/500]	Time 29.477 (29.477)	Loss 0.0539 (0.0967)	CeLoss 0.0540 (0.0504)	SegCLSLoss 0.0000 (0.0018)	KLLoss 0.0000 (0.0189)	MaskLoss 0.0000 (0.0626)	MaskBCELoss 0.0000 (0.0094)	MaskDICELoss 0.0000 (0.0532)
Epoch: [0][331/500]	Time 25.356 (25.356)	Loss 0.2376 (0.1324)	CeLoss 0.0977 (0.0484)	SegCLSLoss 0.0027 (0.0035)	KLLoss 0.0361 (0.0287)	MaskLoss 0.1204 (0.1008)	MaskBCELoss 0.0521 (0.0213)	MaskDICELoss 0.0683 (0.0796)
Epoch: [0][332/500]	Time 28.074 (28.074)	Loss 0.1332 (0.1296)	CeLoss 0.0488 (0.0390)	SegCLSLoss 0.0018 (0.0031)	KLLoss 0.0201 (0.0285)	MaskLoss 0.0846 (0.0993)	MaskBCELoss 0.0274 (0.0262)	MaskDICELoss 0.0572 (0.0731)
Epoch: [0][333/500]	Time 25.499 (25.499)	Loss 0.1411 (0.1689)	CeLoss 0.0635 (0.0464)	SegCLSLoss 0.0049 (0.0032)	KLLoss 0.0277 (0.0299)	MaskLoss 0.1006 (0.1193)	MaskBCELoss 0.0168 (0.0409)	MaskDICELoss 0.0839 (0.0784)
Epoch: [0][334/500]	Time 22.930 (22.930)	Loss 0.1098 (0.1396)	CeLoss 0.0251 (0.0472)	SegCLSLoss 0.0063 (0.0052)	KLLoss 0.0366 (0.0304)	MaskLoss 0.1130 (0.1075)	MaskBCELoss 0.0166 (0.0240)	MaskDICELoss 0.0964 (0.0834)
Epoch: [0][335/500]	Time 27.133 (27.133)	Loss 0.1063 (0.1292)	CeLoss 0.0251 (0.0465)	SegCLSLoss 0.0058 (0.0052)	KLLoss 0.0391 (0.0302)	MaskLoss 0.0944 (0.1024)	MaskBCELoss 0.0206 (0.0192)	MaskDICELoss 0.0737 (0.0831)
Epoch: [0][336/500]	Time 30.198 (30.198)	Loss 0.0986 (0.1283)	CeLoss 0.0206 (0.0379)	SegCLSLoss 0.0027 (0.0040)	KLLoss 0.0356 (0.0344)	MaskLoss 0.1113 (0.1154)	MaskBCELoss 0.0140 (0.0205)	MaskDICELoss 0.0973 (0.0949)
Epoch: [0][337/500]	Time 27.831 (27.831)	Loss 0.0837 (0.1329)	CeLoss 0.0223 (0.0479)	SegCLSLoss 0.0028 (0.0033)	KLLoss 0.0337 (0.0292)	MaskLoss 0.1039 (0.0998)	MaskBCELoss 0.0053 (0.0224)	MaskDICELoss 0.0986 (0.0774)
Epoch: [0][338/500]	Time 30.099 (30.099)	Loss 0.1005 (0.1124)	CeLoss 0.0258 (0.0406)	SegCLSLoss 0.0067 (0.0033)	KLLoss 0.0232 (0.0276)	MaskLoss 0.1103 (0.0932)	MaskBCELoss 0.0108 (0.0157)	MaskDICELoss 0.0995 (0.0775)
Epoch: [0][339/500]	Time 28.472 (28.472)	Loss 0.0211 (0.1233)	CeLoss 0.0211 (0.0377)	SegCLSLoss 0.0000 (0.0030)	KLLoss 0.0000 (0.0287)	MaskLoss 0.0000 (0.1009)	MaskBCELoss 0.0000 (0.0224)	MaskDICELoss 0.0000 (0.0785)
Epoch: [0][340/500]	Time 28.370 (28.370)	Loss 0.1982 (0.1751)	CeLoss 0.0579 (0.0464)	SegCLSLoss 0.0015 (0.0027)	KLLoss 0.0359 (0.0323)	MaskLoss 0.1130 (0.1142)	MaskBCELoss 0.0554 (0.0468)	MaskDICELoss 0.0576 (0.0674)
Epoch: [0][341/500]	Time 27.933 (27.933)	Loss 0.1455 (0.1485)	CeLoss 0.0708 (0.0352)	SegCLSLoss 0.0070 (0.0032)	KLLoss 0.0320 (0.0328)	MaskLoss 0.0889 (0.1194)	MaskBCELoss 0.0179 (0.0347)	MaskDICELoss 0.0710 (0.0847)
Epoch: [0][342/500]	Time 27.695 (27.695)	Loss 0.0906 (0.1275)	CeLoss 0.0187 (0.0425)	SegCLSLoss 0.0024 (0.0030)	KLLoss 0.0417 (0.0331)	MaskLoss 0.1103 (0.1017)	MaskBCELoss 0.0104 (0.0217)	MaskDICELoss 0.0999 (0.0800)
Epoch: [0][343/500]	Time 29.511 (29.511)	Loss 0.1500 (0.1329)	CeLoss 0.0640 (0.0402)	SegCLSLoss 0.0016 (0.0045)	KLLoss 0.0250 (0.0287)	MaskLoss 0.1131 (0.1117)	MaskBCELoss 0.0191 (0.0230)	MaskDICELoss 0.0940 (0.0887)
Epoch: [0][344/500]	Time 30.291 (30.291)	Loss 0.1458 (0.1190)	CeLoss 0.0500 (0.0351)	SegCLSLoss 0.0031 (0.0036)	KLLoss 0.0232 (0.0283)	MaskLoss 0.1202 (0.0989)	MaskBCELoss 0.0227 (0.0218)	MaskDICELoss 0.0975 (0.0771)
Epoch: [0][345/500]	Time 28.627 (28.627)	Loss 0.1579 (0.1557)	CeLoss 0.0894 (0.0539)	SegCLSLoss 0.0022 (0.0037)	KLLoss 0.0349 (0.0339)	MaskLoss 0.1041 (0.1140)	MaskBCELoss 0.0102 (0.0286)	MaskDICELoss 0.0939 (0.0854)
Epoch: [0][346/500]	Time 26.206 (26.206)	Loss 0.1499 (0.1239)	CeLoss 0.0781 (0.0448)	SegCLSLoss 0.0028 (0.0038)	KLLoss 0.0261 (0.0278)	MaskLoss 0.1059 (0.0992)	MaskBCELoss 0.0115 (0.0184)	MaskDICELoss 0.0944 (0.0809)
Epoch: [0][347/500]	Time 27.728 (27.728)	Loss 0.1228 (0.1584)	CeLoss 0.0315 (0.0429)	SegCLSLoss 0.0031 (0.0028)	KLLoss 0.0374 (0.0331)	MaskLoss 0.1040 (0.1169)	MaskBCELoss 0.0252 (0.0371)	MaskDICELoss 0.0788 (0.0797)
Epoch: [0][348/500]	Time 26.834 (26.834)	Loss 0.1583 (0.1472)	CeLoss 0.0535 (0.0611)	SegCLSLoss 0.0034 (0.0042)	KLLoss 0.0322 (0.0292)	MaskLoss 0.1094 (0.1033)	MaskBCELoss 0.0323 (0.0215)	MaskDICELoss 0.0770 (0.0818)
Epoch: [0][349/500]	Time 27.970 (27.970)	Loss 0.2448 (0.1348)	CeLoss 0.0281 (0.0439)	SegCLSLoss 0.0049 (0.0035)	KLLoss 0.0398 (0.0301)	MaskLoss 0.1682 (0.1032)	MaskBCELoss 0.0867 (0.0250)	MaskDICELoss 0.0815 (0.0781)
Epoch: [0][350/500]	Time 28.581 (28.581)	Loss 0.1289 (0.1488)	CeLoss 0.0742 (0.0521)	SegCLSLoss 0.0031 (0.0029)	KLLoss 0.0297 (0.0309)	MaskLoss 0.1015 (0.1139)	MaskBCELoss 0.0016 (0.0256)	MaskDICELoss 0.1000 (0.0883)
Epoch: [0][351/500]	Time 26.146 (26.146)	Loss 0.1003 (0.1337)	CeLoss 0.0466 (0.0583)	SegCLSLoss 0.0031 (0.0038)	KLLoss 0.0356 (0.0290)	MaskLoss 0.1010 (0.0973)	MaskBCELoss 0.0010 (0.0165)	MaskDICELoss 0.1000 (0.0808)
Epoch: [0][352/500]	Time 28.359 (28.359)	Loss 0.1511 (0.1134)	CeLoss 0.0679 (0.0471)	SegCLSLoss 0.0020 (0.0024)	KLLoss 0.0388 (0.0229)	MaskLoss 0.0887 (0.0776)	MaskBCELoss 0.0251 (0.0175)	MaskDICELoss 0.0636 (0.0601)
Epoch: [0][353/500]	Time 26.385 (26.385)	Loss 0.1897 (0.1629)	CeLoss 0.0835 (0.0517)	SegCLSLoss 0.0039 (0.0032)	KLLoss 0.0383 (0.0326)	MaskLoss 0.1180 (0.1220)	MaskBCELoss 0.0301 (0.0324)	MaskDICELoss 0.0879 (0.0895)
Epoch: [0][354/500]	Time 29.545 (29.545)	Loss 0.0762 (0.1379)	CeLoss 0.0206 (0.0503)	SegCLSLoss 0.0038 (0.0042)	KLLoss 0.0273 (0.0290)	MaskLoss 0.1017 (0.1100)	MaskBCELoss 0.0019 (0.0204)	MaskDICELoss 0.0997 (0.0897)
Epoch: [0][355/500]	Time 30.536 (30.536)	Loss 0.1227 (0.1380)	CeLoss 0.0276 (0.0456)	SegCLSLoss 0.0040 (0.0028)	KLLoss 0.0203 (0.0329)	MaskLoss 0.1201 (0.1089)	MaskBCELoss 0.0221 (0.0244)	MaskDICELoss 0.0980 (0.0844)
Epoch: [0][356/500]	Time 27.574 (27.574)	Loss 0.0944 (0.1300)	CeLoss 0.0356 (0.0484)	SegCLSLoss 0.0055 (0.0031)	KLLoss 0.0297 (0.0224)	MaskLoss 0.1014 (0.0883)	MaskBCELoss 0.0036 (0.0239)	MaskDICELoss 0.0978 (0.0643)
Epoch: [0][357/500]	Time 28.395 (28.395)	Loss 0.1015 (0.1366)	CeLoss 0.0266 (0.0465)	SegCLSLoss 0.0046 (0.0027)	KLLoss 0.0300 (0.0279)	MaskLoss 0.1113 (0.1040)	MaskBCELoss 0.0113 (0.0245)	MaskDICELoss 0.1000 (0.0795)
Epoch: [0][358/500]	Time 23.665 (23.665)	Loss 0.1575 (0.1232)	CeLoss 0.0918 (0.0485)	SegCLSLoss 0.0079 (0.0049)	KLLoss 0.0289 (0.0297)	MaskLoss 0.1006 (0.1045)	MaskBCELoss 0.0077 (0.0134)	MaskDICELoss 0.0929 (0.0912)
Epoch: [0][359/500]	Time 28.748 (28.748)	Loss 0.0590 (0.1474)	CeLoss 0.0591 (0.0630)	SegCLSLoss 0.0000 (0.0028)	KLLoss 0.0000 (0.0292)	MaskLoss 0.0000 (0.0990)	MaskBCELoss 0.0000 (0.0223)	MaskDICELoss 0.0000 (0.0767)
Epoch: [0][360/500]	Time 28.648 (28.648)	Loss 0.1617 (0.1384)	CeLoss 0.0552 (0.0421)	SegCLSLoss 0.0018 (0.0036)	KLLoss 0.0287 (0.0318)	MaskLoss 0.1107 (0.1140)	MaskBCELoss 0.0337 (0.0250)	MaskDICELoss 0.0771 (0.0890)
Epoch: [0][361/500]	Time 27.489 (27.489)	Loss 0.0186 (0.1389)	CeLoss 0.0186 (0.0440)	SegCLSLoss 0.0000 (0.0033)	KLLoss 0.0000 (0.0279)	MaskLoss 0.0000 (0.0996)	MaskBCELoss 0.0000 (0.0290)	MaskDICELoss 0.0000 (0.0707)
Epoch: [0][362/500]	Time 27.989 (27.989)	Loss 0.1024 (0.1258)	CeLoss 0.0251 (0.0351)	SegCLSLoss 0.0089 (0.0049)	KLLoss 0.0281 (0.0313)	MaskLoss 0.1104 (0.1120)	MaskBCELoss 0.0117 (0.0215)	MaskDICELoss 0.0987 (0.0905)
Epoch: [0][363/500]	Time 28.124 (28.124)	Loss 0.1434 (0.1226)	CeLoss 0.0474 (0.0398)	SegCLSLoss 0.0031 (0.0042)	KLLoss 0.0356 (0.0294)	MaskLoss 0.0978 (0.0952)	MaskBCELoss 0.0303 (0.0220)	MaskDICELoss 0.0675 (0.0731)
Epoch: [0][364/500]	Time 29.144 (29.144)	Loss 0.2010 (0.1336)	CeLoss 0.0728 (0.0515)	SegCLSLoss 0.0018 (0.0022)	KLLoss 0.0364 (0.0335)	MaskLoss 0.1261 (0.1027)	MaskBCELoss 0.0430 (0.0198)	MaskDICELoss 0.0831 (0.0828)
Epoch: [0][365/500]	Time 26.804 (26.804)	Loss 0.1825 (0.1274)	CeLoss 0.0688 (0.0475)	SegCLSLoss 0.0023 (0.0040)	KLLoss 0.0245 (0.0283)	MaskLoss 0.1295 (0.0974)	MaskBCELoss 0.0319 (0.0194)	MaskDICELoss 0.0977 (0.0780)
Epoch: [0][366/500]	Time 27.325 (27.325)	Loss 0.0938 (0.1352)	CeLoss 0.0131 (0.0476)	SegCLSLoss 0.0032 (0.0043)	KLLoss 0.0240 (0.0323)	MaskLoss 0.0978 (0.1088)	MaskBCELoss 0.0202 (0.0207)	MaskDICELoss 0.0777 (0.0881)
Epoch: [0][367/500]	Time 26.407 (26.407)	Loss 0.1016 (0.1688)	CeLoss 0.0203 (0.0450)	SegCLSLoss 0.0132 (0.0051)	KLLoss 0.0266 (0.0345)	MaskLoss 0.0934 (0.1223)	MaskBCELoss 0.0188 (0.0400)	MaskDICELoss 0.0746 (0.0823)
Epoch: [0][368/500]	Time 27.898 (27.898)	Loss 0.0795 (0.1289)	CeLoss 0.0211 (0.0378)	SegCLSLoss 0.0051 (0.0030)	KLLoss 0.0320 (0.0359)	MaskLoss 0.1007 (0.1152)	MaskBCELoss 0.0036 (0.0214)	MaskDICELoss 0.0971 (0.0938)
Epoch: [0][369/500]	Time 28.156 (28.156)	Loss 0.0924 (0.1641)	CeLoss 0.0306 (0.0466)	SegCLSLoss 0.0031 (0.0026)	KLLoss 0.0320 (0.0312)	MaskLoss 0.0931 (0.1185)	MaskBCELoss 0.0091 (0.0380)	MaskDICELoss 0.0841 (0.0805)
Epoch: [0][370/500]	Time 28.812 (28.812)	Loss 0.2157 (0.1736)	CeLoss 0.0791 (0.0625)	SegCLSLoss 0.0045 (0.0028)	KLLoss 0.0312 (0.0328)	MaskLoss 0.1274 (0.1176)	MaskBCELoss 0.0469 (0.0339)	MaskDICELoss 0.0804 (0.0837)
Epoch: [0][371/500]	Time 27.784 (27.784)	Loss 0.1511 (0.1235)	CeLoss 0.0280 (0.0444)	SegCLSLoss 0.0028 (0.0035)	KLLoss 0.0403 (0.0325)	MaskLoss 0.1240 (0.0994)	MaskBCELoss 0.0399 (0.0184)	MaskDICELoss 0.0841 (0.0810)
Epoch: [0][372/500]	Time 29.065 (29.065)	Loss 0.2276 (0.1674)	CeLoss 0.0693 (0.0552)	SegCLSLoss 0.0016 (0.0025)	KLLoss 0.0376 (0.0339)	MaskLoss 0.1177 (0.1178)	MaskBCELoss 0.0656 (0.0347)	MaskDICELoss 0.0520 (0.0832)
Epoch: [0][373/500]	Time 25.742 (25.742)	Loss 0.2027 (0.1496)	CeLoss 0.0674 (0.0621)	SegCLSLoss 0.0045 (0.0041)	KLLoss 0.0310 (0.0296)	MaskLoss 0.1287 (0.1067)	MaskBCELoss 0.0459 (0.0215)	MaskDICELoss 0.0828 (0.0852)
Epoch: [0][374/500]	Time 29.978 (29.978)	Loss 0.0996 (0.1327)	CeLoss 0.0457 (0.0518)	SegCLSLoss 0.0045 (0.0028)	KLLoss 0.0317 (0.0348)	MaskLoss 0.1009 (0.1101)	MaskBCELoss 0.0009 (0.0163)	MaskDICELoss 0.1000 (0.0938)
Epoch: [0][375/500]	Time 27.869 (27.869)	Loss 0.1468 (0.1438)	CeLoss 0.0542 (0.0567)	SegCLSLoss 0.0027 (0.0032)	KLLoss 0.0347 (0.0294)	MaskLoss 0.0978 (0.1083)	MaskBCELoss 0.0282 (0.0210)	MaskDICELoss 0.0696 (0.0873)
Epoch: [0][376/500]	Time 26.705 (26.705)	Loss 0.1877 (0.1545)	CeLoss 0.0264 (0.0477)	SegCLSLoss 0.0023 (0.0030)	KLLoss 0.0337 (0.0301)	MaskLoss 0.1462 (0.1056)	MaskBCELoss 0.0580 (0.0350)	MaskDICELoss 0.0882 (0.0706)
Epoch: [0][377/500]	Time 28.211 (28.211)	Loss 0.1349 (0.1268)	CeLoss 0.0742 (0.0582)	SegCLSLoss 0.0029 (0.0028)	KLLoss 0.0251 (0.0284)	MaskLoss 0.1038 (0.0977)	MaskBCELoss 0.0048 (0.0122)	MaskDICELoss 0.0990 (0.0854)
Epoch: [0][378/500]	Time 28.754 (28.754)	Loss 0.3528 (0.1667)	CeLoss 0.0840 (0.0414)	SegCLSLoss 0.0038 (0.0032)	KLLoss 0.0337 (0.0276)	MaskLoss 0.2075 (0.1194)	MaskBCELoss 0.1087 (0.0427)	MaskDICELoss 0.0988 (0.0768)
Epoch: [0][379/500]	Time 24.573 (24.573)	Loss 0.1853 (0.1551)	CeLoss 0.0342 (0.0402)	SegCLSLoss 0.0023 (0.0037)	KLLoss 0.0430 (0.0307)	MaskLoss 0.1380 (0.1192)	MaskBCELoss 0.0540 (0.0357)	MaskDICELoss 0.0840 (0.0835)
Epoch: [0][380/500]	Time 28.674 (28.674)	Loss 0.1141 (0.1631)	CeLoss 0.0219 (0.0560)	SegCLSLoss 0.0058 (0.0031)	KLLoss 0.0320 (0.0365)	MaskLoss 0.1045 (0.1139)	MaskBCELoss 0.0247 (0.0324)	MaskDICELoss 0.0798 (0.0814)
Epoch: [0][381/500]	Time 29.244 (29.244)	Loss 0.1009 (0.1382)	CeLoss 0.0217 (0.0438)	SegCLSLoss 0.0065 (0.0042)	KLLoss 0.0280 (0.0336)	MaskLoss 0.1065 (0.1125)	MaskBCELoss 0.0151 (0.0240)	MaskDICELoss 0.0914 (0.0885)
Epoch: [0][382/500]	Time 26.635 (26.635)	Loss 0.1085 (0.1215)	CeLoss 0.0498 (0.0390)	SegCLSLoss 0.0034 (0.0031)	KLLoss 0.0312 (0.0345)	MaskLoss 0.1034 (0.1046)	MaskBCELoss 0.0035 (0.0191)	MaskDICELoss 0.1000 (0.0854)
Epoch: [0][383/500]	Time 28.149 (28.149)	Loss 0.0603 (0.1321)	CeLoss 0.0117 (0.0445)	SegCLSLoss 0.0025 (0.0034)	KLLoss 0.0315 (0.0321)	MaskLoss 0.0667 (0.0997)	MaskBCELoss 0.0093 (0.0241)	MaskDICELoss 0.0574 (0.0757)
Epoch: [0][384/500]	Time 26.822 (26.822)	Loss 0.1984 (0.1555)	CeLoss 0.0635 (0.0554)	SegCLSLoss 0.0018 (0.0030)	KLLoss 0.0327 (0.0351)	MaskLoss 0.1311 (0.1169)	MaskBCELoss 0.0456 (0.0268)	MaskDICELoss 0.0855 (0.0901)
Epoch: [0][385/500]	Time 30.147 (30.147)	Loss 0.0391 (0.1316)	CeLoss 0.0391 (0.0552)	SegCLSLoss 0.0000 (0.0032)	KLLoss 0.0000 (0.0270)	MaskLoss 0.0000 (0.0921)	MaskBCELoss 0.0000 (0.0192)	MaskDICELoss 0.0000 (0.0729)
Epoch: [0][386/500]	Time 29.184 (29.184)	Loss 0.0861 (0.1303)	CeLoss 0.0232 (0.0415)	SegCLSLoss 0.0033 (0.0049)	KLLoss 0.0352 (0.0273)	MaskLoss 0.0854 (0.0969)	MaskBCELoss 0.0124 (0.0253)	MaskDICELoss 0.0730 (0.0716)
Epoch: [0][387/500]	Time 27.575 (27.575)	Loss 0.0373 (0.1439)	CeLoss 0.0374 (0.0473)	SegCLSLoss 0.0000 (0.0028)	KLLoss 0.0000 (0.0262)	MaskLoss 0.0000 (0.1026)	MaskBCELoss 0.0000 (0.0292)	MaskDICELoss 0.0000 (0.0734)
Epoch: [0][388/500]	Time 28.446 (28.446)	Loss 0.1186 (0.1295)	CeLoss 0.0635 (0.0486)	SegCLSLoss 0.0046 (0.0034)	KLLoss 0.0281 (0.0325)	MaskLoss 0.0967 (0.1023)	MaskBCELoss 0.0028 (0.0187)	MaskDICELoss 0.0940 (0.0836)
Epoch: [0][389/500]	Time 26.919 (26.919)	Loss 0.0768 (0.1176)	CeLoss 0.0219 (0.0457)	SegCLSLoss 0.0035 (0.0030)	KLLoss 0.0356 (0.0241)	MaskLoss 0.1016 (0.0874)	MaskBCELoss 0.0016 (0.0178)	MaskDICELoss 0.1000 (0.0696)
Epoch: [0][390/500]	Time 30.280 (30.280)	Loss 0.1115 (0.1191)	CeLoss 0.0574 (0.0478)	SegCLSLoss 0.0018 (0.0027)	KLLoss 0.0322 (0.0281)	MaskLoss 0.1015 (0.0933)	MaskBCELoss 0.0017 (0.0155)	MaskDICELoss 0.0998 (0.0778)
Epoch: [0][391/500]	Time 27.991 (27.991)	Loss 0.0883 (0.1043)	CeLoss 0.0183 (0.0400)	SegCLSLoss 0.0023 (0.0043)	KLLoss 0.0383 (0.0286)	MaskLoss 0.1089 (0.0929)	MaskBCELoss 0.0095 (0.0105)	MaskDICELoss 0.0994 (0.0824)
Epoch: [0][392/500]	Time 25.302 (25.302)	Loss 0.0570 (0.1128)	CeLoss 0.0571 (0.0481)	SegCLSLoss 0.0000 (0.0048)	KLLoss 0.0000 (0.0267)	MaskLoss 0.0000 (0.0916)	MaskBCELoss 0.0000 (0.0110)	MaskDICELoss 0.0000 (0.0805)
Epoch: [0][393/500]	Time 27.408 (27.408)	Loss 0.0749 (0.1211)	CeLoss 0.0210 (0.0302)	SegCLSLoss 0.0041 (0.0042)	KLLoss 0.0315 (0.0300)	MaskLoss 0.1009 (0.1076)	MaskBCELoss 0.0010 (0.0233)	MaskDICELoss 0.1000 (0.0844)
Epoch: [0][394/500]	Time 27.158 (27.158)	Loss 0.1695 (0.1291)	CeLoss 0.0435 (0.0438)	SegCLSLoss 0.0020 (0.0030)	KLLoss 0.0354 (0.0343)	MaskLoss 0.1151 (0.1025)	MaskBCELoss 0.0451 (0.0217)	MaskDICELoss 0.0700 (0.0809)
Epoch: [0][395/500]	Time 26.778 (26.778)	Loss 0.0630 (0.1576)	CeLoss 0.0236 (0.0461)	SegCLSLoss 0.0025 (0.0033)	KLLoss 0.0376 (0.0313)	MaskLoss 0.0657 (0.1065)	MaskBCELoss 0.0036 (0.0377)	MaskDICELoss 0.0621 (0.0688)
Epoch: [0][396/500]	Time 30.911 (30.911)	Loss 0.1403 (0.1383)	CeLoss 0.0371 (0.0531)	SegCLSLoss 0.0045 (0.0035)	KLLoss 0.0366 (0.0249)	MaskLoss 0.1074 (0.0976)	MaskBCELoss 0.0314 (0.0231)	MaskDICELoss 0.0760 (0.0745)
Epoch: [0][397/500]	Time 29.148 (29.148)	Loss 0.2500 (0.1395)	CeLoss 0.0359 (0.0364)	SegCLSLoss 0.0033 (0.0041)	KLLoss 0.0309 (0.0324)	MaskLoss 0.1685 (0.1200)	MaskBCELoss 0.0855 (0.0274)	MaskDICELoss 0.0829 (0.0926)
Epoch: [0][398/500]	Time 28.918 (28.918)	Loss 0.0840 (0.1165)	CeLoss 0.0251 (0.0489)	SegCLSLoss 0.0034 (0.0031)	KLLoss 0.0327 (0.0287)	MaskLoss 0.0991 (0.0975)	MaskBCELoss 0.0050 (0.0115)	MaskDICELoss 0.0941 (0.0860)
Epoch: [0][399/500]	Time 25.857 (25.857)	Loss 0.2988 (0.1430)	CeLoss 0.1030 (0.0442)	SegCLSLoss 0.0040 (0.0044)	KLLoss 0.0508 (0.0337)	MaskLoss 0.1469 (0.1092)	MaskBCELoss 0.0799 (0.0280)	MaskDICELoss 0.0670 (0.0812)
[2025-03-09 12:30:47,352] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[0.00029593975903614454], mom=[(0.9, 0.95)]
[2025-03-09 12:30:47,358] [INFO] [timer.py:215:stop] epoch=0/micro_step=400/global_step=40, RunningAvgSamplesPerSec=1.4240766421253976, CurrSamplesPerSec=1.5133129095998088, MemAllocated=59.62GB, MaxMemAllocated=73.04GB
Epoch: [0][400/500]	Time 28.866 (28.866)	Loss 0.1832 (0.1575)	CeLoss 0.0226 (0.0466)	SegCLSLoss 0.0025 (0.0032)	KLLoss 0.0231 (0.0329)	MaskLoss 0.1365 (0.1174)	MaskBCELoss 0.0608 (0.0338)	MaskDICELoss 0.0757 (0.0836)
Epoch: [0][401/500]	Time 25.388 (25.388)	Loss 0.1357 (0.1306)	CeLoss 0.0369 (0.0483)	SegCLSLoss 0.0081 (0.0038)	KLLoss 0.0219 (0.0267)	MaskLoss 0.1150 (0.0995)	MaskBCELoss 0.0248 (0.0204)	MaskDICELoss 0.0903 (0.0791)
Epoch: [0][402/500]	Time 26.839 (26.839)	Loss 0.0942 (0.1237)	CeLoss 0.0170 (0.0511)	SegCLSLoss 0.0022 (0.0026)	KLLoss 0.0352 (0.0264)	MaskLoss 0.1105 (0.0909)	MaskBCELoss 0.0139 (0.0173)	MaskDICELoss 0.0966 (0.0736)
Epoch: [0][403/500]	Time 23.933 (23.933)	Loss 0.1600 (0.1847)	CeLoss 0.0222 (0.0502)	SegCLSLoss 0.0066 (0.0048)	KLLoss 0.0303 (0.0326)	MaskLoss 0.1072 (0.1283)	MaskBCELoss 0.0540 (0.0453)	MaskDICELoss 0.0531 (0.0829)
Epoch: [0][404/500]	Time 27.683 (27.683)	Loss 0.0808 (0.1203)	CeLoss 0.0209 (0.0376)	SegCLSLoss 0.0056 (0.0030)	KLLoss 0.0320 (0.0335)	MaskLoss 0.0922 (0.1105)	MaskBCELoss 0.0074 (0.0174)	MaskDICELoss 0.0848 (0.0931)
Epoch: [0][405/500]	Time 27.037 (27.037)	Loss 0.1071 (0.1332)	CeLoss 0.0337 (0.0472)	SegCLSLoss 0.0022 (0.0033)	KLLoss 0.0317 (0.0269)	MaskLoss 0.1099 (0.1012)	MaskBCELoss 0.0116 (0.0225)	MaskDICELoss 0.0983 (0.0787)
Epoch: [0][406/500]	Time 28.188 (28.188)	Loss 0.1013 (0.1105)	CeLoss 0.0339 (0.0346)	SegCLSLoss 0.0037 (0.0046)	KLLoss 0.0293 (0.0326)	MaskLoss 0.1075 (0.1031)	MaskBCELoss 0.0077 (0.0147)	MaskDICELoss 0.0998 (0.0884)
Epoch: [0][407/500]	Time 27.431 (27.431)	Loss 0.0785 (0.1371)	CeLoss 0.0243 (0.0344)	SegCLSLoss 0.0036 (0.0034)	KLLoss 0.0317 (0.0346)	MaskLoss 0.0983 (0.1130)	MaskBCELoss 0.0021 (0.0296)	MaskDICELoss 0.0962 (0.0833)
Epoch: [0][408/500]	Time 30.213 (30.213)	Loss 0.1733 (0.1642)	CeLoss 0.0791 (0.0455)	SegCLSLoss 0.0035 (0.0035)	KLLoss 0.0342 (0.0322)	MaskLoss 0.0978 (0.1230)	MaskBCELoss 0.0290 (0.0370)	MaskDICELoss 0.0688 (0.0861)
Epoch: [0][409/500]	Time 27.702 (27.702)	Loss 0.0723 (0.1243)	CeLoss 0.0176 (0.0435)	SegCLSLoss 0.0030 (0.0038)	KLLoss 0.0317 (0.0315)	MaskLoss 0.1014 (0.1081)	MaskBCELoss 0.0017 (0.0165)	MaskDICELoss 0.0997 (0.0916)
Epoch: [0][410/500]	Time 30.027 (30.027)	Loss 0.0947 (0.1638)	CeLoss 0.0312 (0.0488)	SegCLSLoss 0.0037 (0.0032)	KLLoss 0.0306 (0.0293)	MaskLoss 0.1054 (0.1211)	MaskBCELoss 0.0060 (0.0353)	MaskDICELoss 0.0994 (0.0858)
Epoch: [0][411/500]	Time 28.309 (28.309)	Loss 0.1100 (0.1384)	CeLoss 0.0688 (0.0421)	SegCLSLoss 0.0052 (0.0041)	KLLoss 0.0270 (0.0281)	MaskLoss 0.0572 (0.1024)	MaskBCELoss 0.0066 (0.0287)	MaskDICELoss 0.0506 (0.0737)
Epoch: [0][412/500]	Time 32.039 (32.039)	Loss 0.0543 (0.1209)	CeLoss 0.0542 (0.0389)	SegCLSLoss 0.0000 (0.0031)	KLLoss 0.0000 (0.0271)	MaskLoss 0.0000 (0.0937)	MaskBCELoss 0.0000 (0.0224)	MaskDICELoss 0.0000 (0.0713)
Epoch: [0][413/500]	Time 27.923 (27.923)	Loss 0.1331 (0.1252)	CeLoss 0.0559 (0.0400)	SegCLSLoss 0.0038 (0.0039)	KLLoss 0.0250 (0.0289)	MaskLoss 0.0972 (0.1072)	MaskBCELoss 0.0178 (0.0198)	MaskDICELoss 0.0794 (0.0875)
Epoch: [0][414/500]	Time 29.146 (29.146)	Loss 0.0997 (0.1176)	CeLoss 0.0236 (0.0419)	SegCLSLoss 0.0042 (0.0040)	KLLoss 0.0325 (0.0307)	MaskLoss 0.0858 (0.0988)	MaskBCELoss 0.0208 (0.0162)	MaskDICELoss 0.0650 (0.0826)
Epoch: [0][415/500]	Time 24.416 (24.416)	Loss 0.1449 (0.1412)	CeLoss 0.0231 (0.0415)	SegCLSLoss 0.0037 (0.0026)	KLLoss 0.0349 (0.0329)	MaskLoss 0.1251 (0.1038)	MaskBCELoss 0.0383 (0.0310)	MaskDICELoss 0.0868 (0.0727)
Epoch: [0][416/500]	Time 31.052 (31.052)	Loss 0.1035 (0.1337)	CeLoss 0.0216 (0.0373)	SegCLSLoss 0.0047 (0.0030)	KLLoss 0.0254 (0.0316)	MaskLoss 0.1113 (0.1137)	MaskBCELoss 0.0160 (0.0254)	MaskDICELoss 0.0953 (0.0883)
Epoch: [0][417/500]	Time 26.288 (26.288)	Loss 0.1141 (0.0863)	CeLoss 0.0420 (0.0311)	SegCLSLoss 0.0022 (0.0030)	KLLoss 0.0309 (0.0260)	MaskLoss 0.1103 (0.0822)	MaskBCELoss 0.0105 (0.0084)	MaskDICELoss 0.0999 (0.0738)
Epoch: [0][418/500]	Time 28.363 (28.363)	Loss 0.1265 (0.1418)	CeLoss 0.0598 (0.0531)	SegCLSLoss 0.0043 (0.0030)	KLLoss 0.0371 (0.0316)	MaskLoss 0.1030 (0.1094)	MaskBCELoss 0.0086 (0.0216)	MaskDICELoss 0.0944 (0.0878)
Epoch: [0][419/500]	Time 29.809 (29.809)	Loss 0.1581 (0.1142)	CeLoss 0.0299 (0.0380)	SegCLSLoss 0.0028 (0.0033)	KLLoss 0.0312 (0.0282)	MaskLoss 0.1034 (0.0915)	MaskBCELoss 0.0501 (0.0192)	MaskDICELoss 0.0532 (0.0723)
Epoch: [0][420/500]	Time 28.381 (28.381)	Loss 0.1421 (0.1309)	CeLoss 0.0747 (0.0473)	SegCLSLoss 0.0029 (0.0024)	KLLoss 0.0349 (0.0255)	MaskLoss 0.1079 (0.0953)	MaskBCELoss 0.0080 (0.0231)	MaskDICELoss 0.0999 (0.0722)
Epoch: [0][421/500]	Time 26.905 (26.905)	Loss 0.0355 (0.1348)	CeLoss 0.0356 (0.0543)	SegCLSLoss 0.0000 (0.0029)	KLLoss 0.0000 (0.0299)	MaskLoss 0.0000 (0.0944)	MaskBCELoss 0.0000 (0.0213)	MaskDICELoss 0.0000 (0.0731)
Epoch: [0][422/500]	Time 27.568 (27.568)	Loss 0.1506 (0.1363)	CeLoss 0.0386 (0.0522)	SegCLSLoss 0.0017 (0.0032)	KLLoss 0.0231 (0.0321)	MaskLoss 0.1298 (0.1024)	MaskBCELoss 0.0310 (0.0209)	MaskDICELoss 0.0988 (0.0815)
Epoch: [0][423/500]	Time 27.039 (27.039)	Loss 0.1030 (0.1289)	CeLoss 0.0260 (0.0460)	SegCLSLoss 0.0035 (0.0047)	KLLoss 0.0349 (0.0336)	MaskLoss 0.1120 (0.1095)	MaskBCELoss 0.0128 (0.0171)	MaskDICELoss 0.0992 (0.0923)
Epoch: [0][424/500]	Time 30.535 (30.535)	Loss 0.0107 (0.0969)	CeLoss 0.0107 (0.0286)	SegCLSLoss 0.0000 (0.0031)	KLLoss 0.0000 (0.0284)	MaskLoss 0.0000 (0.0952)	MaskBCELoss 0.0000 (0.0128)	MaskDICELoss 0.0000 (0.0824)
Epoch: [0][425/500]	Time 29.279 (29.279)	Loss 0.1000 (0.1360)	CeLoss 0.0327 (0.0414)	SegCLSLoss 0.0023 (0.0040)	KLLoss 0.0306 (0.0300)	MaskLoss 0.0890 (0.1113)	MaskBCELoss 0.0145 (0.0246)	MaskDICELoss 0.0745 (0.0867)
Epoch: [0][426/500]	Time 24.920 (24.920)	Loss 0.1561 (0.1235)	CeLoss 0.0908 (0.0561)	SegCLSLoss 0.0019 (0.0043)	KLLoss 0.0219 (0.0305)	MaskLoss 0.1064 (0.0999)	MaskBCELoss 0.0077 (0.0102)	MaskDICELoss 0.0987 (0.0897)
Epoch: [0][427/500]	Time 28.026 (28.026)	Loss 0.1411 (0.1222)	CeLoss 0.0806 (0.0460)	SegCLSLoss 0.0027 (0.0043)	KLLoss 0.0398 (0.0330)	MaskLoss 0.0974 (0.1053)	MaskBCELoss 0.0069 (0.0142)	MaskDICELoss 0.0905 (0.0910)
Epoch: [0][428/500]	Time 24.728 (24.728)	Loss 0.2374 (0.1457)	CeLoss 0.1123 (0.0550)	SegCLSLoss 0.0072 (0.0044)	KLLoss 0.0359 (0.0297)	MaskLoss 0.1281 (0.1088)	MaskBCELoss 0.0380 (0.0227)	MaskDICELoss 0.0901 (0.0862)
Epoch: [0][429/500]	Time 30.056 (30.056)	Loss 0.1333 (0.1377)	CeLoss 0.0796 (0.0508)	SegCLSLoss 0.0027 (0.0035)	KLLoss 0.0229 (0.0290)	MaskLoss 0.1012 (0.1147)	MaskBCELoss 0.0012 (0.0185)	MaskDICELoss 0.1000 (0.0962)
Epoch: [0][430/500]	Time 26.180 (26.180)	Loss 0.0860 (0.1108)	CeLoss 0.0227 (0.0383)	SegCLSLoss 0.0056 (0.0040)	KLLoss 0.0391 (0.0305)	MaskLoss 0.1053 (0.1062)	MaskBCELoss 0.0053 (0.0116)	MaskDICELoss 0.1000 (0.0946)
Epoch: [0][431/500]	Time 25.842 (25.842)	Loss 0.1475 (0.1193)	CeLoss 0.0908 (0.0527)	SegCLSLoss 0.0087 (0.0036)	KLLoss 0.0309 (0.0279)	MaskLoss 0.1010 (0.0996)	MaskBCELoss 0.0011 (0.0100)	MaskDICELoss 0.1000 (0.0897)
Epoch: [0][432/500]	Time 25.831 (25.831)	Loss 0.1144 (0.1258)	CeLoss 0.0542 (0.0543)	SegCLSLoss 0.0019 (0.0039)	KLLoss 0.0300 (0.0285)	MaskLoss 0.0991 (0.0970)	MaskBCELoss 0.0065 (0.0141)	MaskDICELoss 0.0926 (0.0829)
Epoch: [0][433/500]	Time 30.378 (30.378)	Loss 0.0563 (0.1369)	CeLoss 0.0562 (0.0616)	SegCLSLoss 0.0000 (0.0031)	KLLoss 0.0000 (0.0298)	MaskLoss 0.0000 (0.0970)	MaskBCELoss 0.0000 (0.0168)	MaskDICELoss 0.0000 (0.0802)
Epoch: [0][434/500]	Time 28.237 (28.237)	Loss 0.0160 (0.1042)	CeLoss 0.0160 (0.0373)	SegCLSLoss 0.0000 (0.0033)	KLLoss 0.0000 (0.0253)	MaskLoss 0.0000 (0.0812)	MaskBCELoss 0.0000 (0.0164)	MaskDICELoss 0.0000 (0.0648)
Epoch: [0][435/500]	Time 28.090 (28.090)	Loss 0.1068 (0.1099)	CeLoss 0.0227 (0.0396)	SegCLSLoss 0.0032 (0.0038)	KLLoss 0.0264 (0.0274)	MaskLoss 0.1082 (0.0966)	MaskBCELoss 0.0189 (0.0134)	MaskDICELoss 0.0893 (0.0832)
Epoch: [0][436/500]	Time 26.215 (26.215)	Loss 0.1599 (0.1269)	CeLoss 0.0605 (0.0510)	SegCLSLoss 0.0028 (0.0049)	KLLoss 0.0344 (0.0319)	MaskLoss 0.0877 (0.1024)	MaskBCELoss 0.0359 (0.0148)	MaskDICELoss 0.0517 (0.0876)
Epoch: [0][437/500]	Time 30.953 (30.953)	Loss 0.1512 (0.1311)	CeLoss 0.0225 (0.0478)	SegCLSLoss 0.0026 (0.0027)	KLLoss 0.0280 (0.0353)	MaskLoss 0.1220 (0.1052)	MaskBCELoss 0.0442 (0.0195)	MaskDICELoss 0.0778 (0.0857)
Epoch: [0][438/500]	Time 26.046 (26.046)	Loss 0.1630 (0.1363)	CeLoss 0.0148 (0.0505)	SegCLSLoss 0.0023 (0.0032)	KLLoss 0.0403 (0.0343)	MaskLoss 0.1378 (0.1097)	MaskBCELoss 0.0521 (0.0196)	MaskDICELoss 0.0858 (0.0901)
Epoch: [0][439/500]	Time 27.485 (27.485)	Loss 0.1748 (0.1205)	CeLoss 0.0840 (0.0342)	SegCLSLoss 0.0030 (0.0046)	KLLoss 0.0300 (0.0330)	MaskLoss 0.0963 (0.1067)	MaskBCELoss 0.0274 (0.0205)	MaskDICELoss 0.0689 (0.0862)
Epoch: [0][440/500]	Time 28.816 (28.816)	Loss 0.0498 (0.1324)	CeLoss 0.0498 (0.0467)	SegCLSLoss 0.0000 (0.0030)	KLLoss 0.0000 (0.0295)	MaskLoss 0.0000 (0.0983)	MaskBCELoss 0.0000 (0.0234)	MaskDICELoss 0.0000 (0.0749)
Epoch: [0][441/500]	Time 29.813 (29.813)	Loss 0.1045 (0.1281)	CeLoss 0.0248 (0.0494)	SegCLSLoss 0.0042 (0.0034)	KLLoss 0.0347 (0.0346)	MaskLoss 0.1027 (0.1079)	MaskBCELoss 0.0175 (0.0154)	MaskDICELoss 0.0852 (0.0925)
Epoch: [0][442/500]	Time 26.416 (26.416)	Loss 0.1726 (0.1185)	CeLoss 0.0781 (0.0513)	SegCLSLoss 0.0044 (0.0035)	KLLoss 0.0264 (0.0293)	MaskLoss 0.1111 (0.0924)	MaskBCELoss 0.0244 (0.0127)	MaskDICELoss 0.0867 (0.0797)
Epoch: [0][443/500]	Time 28.496 (28.496)	Loss 0.1912 (0.1419)	CeLoss 0.0767 (0.0515)	SegCLSLoss 0.0017 (0.0030)	KLLoss 0.0295 (0.0257)	MaskLoss 0.1309 (0.1028)	MaskBCELoss 0.0323 (0.0250)	MaskDICELoss 0.0986 (0.0778)
Epoch: [0][444/500]	Time 26.423 (26.423)	Loss 0.1480 (0.1376)	CeLoss 0.0613 (0.0547)	SegCLSLoss 0.0041 (0.0033)	KLLoss 0.0277 (0.0283)	MaskLoss 0.1134 (0.0980)	MaskBCELoss 0.0187 (0.0215)	MaskDICELoss 0.0947 (0.0765)
Epoch: [0][445/500]	Time 26.769 (26.769)	Loss 0.1711 (0.1270)	CeLoss 0.0396 (0.0436)	SegCLSLoss 0.0024 (0.0033)	KLLoss 0.0277 (0.0279)	MaskLoss 0.1146 (0.0968)	MaskBCELoss 0.0488 (0.0222)	MaskDICELoss 0.0658 (0.0746)
Epoch: [0][446/500]	Time 28.549 (28.549)	Loss 0.2441 (0.1455)	CeLoss 0.0258 (0.0401)	SegCLSLoss 0.0030 (0.0032)	KLLoss 0.0266 (0.0286)	MaskLoss 0.1622 (0.1100)	MaskBCELoss 0.0904 (0.0326)	MaskDICELoss 0.0717 (0.0774)
Epoch: [0][447/500]	Time 25.225 (25.225)	Loss 0.2118 (0.1610)	CeLoss 0.0796 (0.0587)	SegCLSLoss 0.0025 (0.0031)	KLLoss 0.0386 (0.0308)	MaskLoss 0.1375 (0.1098)	MaskBCELoss 0.0415 (0.0306)	MaskDICELoss 0.0961 (0.0792)
Epoch: [0][448/500]	Time 27.936 (27.936)	Loss 0.1772 (0.1248)	CeLoss 0.0339 (0.0430)	SegCLSLoss 0.0042 (0.0046)	KLLoss 0.0247 (0.0315)	MaskLoss 0.1316 (0.1013)	MaskBCELoss 0.0502 (0.0193)	MaskDICELoss 0.0815 (0.0820)
Epoch: [0][449/500]	Time 28.223 (28.223)	Loss 0.1910 (0.1252)	CeLoss 0.0752 (0.0410)	SegCLSLoss 0.0044 (0.0032)	KLLoss 0.0461 (0.0296)	MaskLoss 0.1180 (0.0987)	MaskBCELoss 0.0362 (0.0221)	MaskDICELoss 0.0818 (0.0766)
Epoch: [0][450/500]	Time 28.102 (28.102)	Loss 0.1613 (0.1201)	CeLoss 0.0830 (0.0576)	SegCLSLoss 0.0036 (0.0032)	KLLoss 0.0234 (0.0312)	MaskLoss 0.0979 (0.0957)	MaskBCELoss 0.0181 (0.0087)	MaskDICELoss 0.0797 (0.0869)
Epoch: [0][451/500]	Time 28.350 (28.350)	Loss 0.0289 (0.1151)	CeLoss 0.0289 (0.0422)	SegCLSLoss 0.0000 (0.0032)	KLLoss 0.0000 (0.0299)	MaskLoss 0.0000 (0.0872)	MaskBCELoss 0.0000 (0.0185)	MaskDICELoss 0.0000 (0.0687)
Epoch: [0][452/500]	Time 28.132 (28.132)	Loss 0.0756 (0.1330)	CeLoss 0.0212 (0.0378)	SegCLSLoss 0.0057 (0.0035)	KLLoss 0.0325 (0.0278)	MaskLoss 0.1007 (0.1072)	MaskBCELoss 0.0008 (0.0265)	MaskDICELoss 0.0999 (0.0807)
Epoch: [0][453/500]	Time 28.891 (28.891)	Loss 0.1588 (0.1482)	CeLoss 0.0559 (0.0541)	SegCLSLoss 0.0085 (0.0035)	KLLoss 0.0277 (0.0275)	MaskLoss 0.1004 (0.1117)	MaskBCELoss 0.0323 (0.0243)	MaskDICELoss 0.0680 (0.0874)
Epoch: [0][454/500]	Time 28.466 (28.466)	Loss 0.1012 (0.1199)	CeLoss 0.0281 (0.0377)	SegCLSLoss 0.0041 (0.0034)	KLLoss 0.0383 (0.0310)	MaskLoss 0.0973 (0.0961)	MaskBCELoss 0.0150 (0.0216)	MaskDICELoss 0.0824 (0.0745)
Epoch: [0][455/500]	Time 28.055 (28.055)	Loss 0.1139 (0.1243)	CeLoss 0.0317 (0.0557)	SegCLSLoss 0.0034 (0.0031)	KLLoss 0.0347 (0.0277)	MaskLoss 0.0928 (0.0910)	MaskBCELoss 0.0228 (0.0144)	MaskDICELoss 0.0700 (0.0766)
Epoch: [0][456/500]	Time 28.509 (28.509)	Loss 0.0908 (0.1066)	CeLoss 0.0327 (0.0349)	SegCLSLoss 0.0023 (0.0038)	KLLoss 0.0327 (0.0268)	MaskLoss 0.1022 (0.0973)	MaskBCELoss 0.0039 (0.0141)	MaskDICELoss 0.0982 (0.0831)
Epoch: [0][457/500]	Time 26.837 (26.837)	Loss 0.1141 (0.1510)	CeLoss 0.0610 (0.0527)	SegCLSLoss 0.0021 (0.0032)	KLLoss 0.0461 (0.0336)	MaskLoss 0.1010 (0.1151)	MaskBCELoss 0.0010 (0.0261)	MaskDICELoss 0.1000 (0.0890)
Epoch: [0][458/500]	Time 27.457 (27.457)	Loss 0.1038 (0.1082)	CeLoss 0.0337 (0.0460)	SegCLSLoss 0.0052 (0.0029)	KLLoss 0.0320 (0.0272)	MaskLoss 0.1061 (0.0858)	MaskBCELoss 0.0096 (0.0119)	MaskDICELoss 0.0965 (0.0739)
Epoch: [0][459/500]	Time 27.302 (27.302)	Loss 0.0966 (0.1337)	CeLoss 0.0250 (0.0542)	SegCLSLoss 0.0043 (0.0038)	KLLoss 0.0393 (0.0353)	MaskLoss 0.1017 (0.1096)	MaskBCELoss 0.0124 (0.0151)	MaskDICELoss 0.0893 (0.0945)
Epoch: [0][460/500]	Time 28.561 (28.561)	Loss 0.1748 (0.1349)	CeLoss 0.0444 (0.0473)	SegCLSLoss 0.0056 (0.0029)	KLLoss 0.0366 (0.0289)	MaskLoss 0.1080 (0.0967)	MaskBCELoss 0.0491 (0.0253)	MaskDICELoss 0.0589 (0.0715)
Epoch: [0][461/500]	Time 27.205 (27.205)	Loss 0.0851 (0.1091)	CeLoss 0.0225 (0.0431)	SegCLSLoss 0.0029 (0.0032)	KLLoss 0.0283 (0.0307)	MaskLoss 0.1002 (0.0954)	MaskBCELoss 0.0074 (0.0110)	MaskDICELoss 0.0928 (0.0844)
Epoch: [0][462/500]	Time 26.131 (26.131)	Loss 0.2877 (0.1470)	CeLoss 0.0972 (0.0655)	SegCLSLoss 0.0046 (0.0033)	KLLoss 0.0383 (0.0307)	MaskLoss 0.1316 (0.0998)	MaskBCELoss 0.0815 (0.0200)	MaskDICELoss 0.0501 (0.0798)
Epoch: [0][463/500]	Time 33.305 (33.305)	Loss 0.0355 (0.1134)	CeLoss 0.0356 (0.0504)	SegCLSLoss 0.0000 (0.0027)	KLLoss 0.0000 (0.0280)	MaskLoss 0.0000 (0.0895)	MaskBCELoss 0.0000 (0.0113)	MaskDICELoss 0.0000 (0.0782)
Epoch: [0][464/500]	Time 30.244 (30.244)	Loss 0.1278 (0.1265)	CeLoss 0.0688 (0.0510)	SegCLSLoss 0.0029 (0.0034)	KLLoss 0.0266 (0.0298)	MaskLoss 0.1028 (0.1042)	MaskBCELoss 0.0041 (0.0144)	MaskDICELoss 0.0987 (0.0898)
Epoch: [0][465/500]	Time 26.066 (26.066)	Loss 0.0312 (0.0959)	CeLoss 0.0312 (0.0327)	SegCLSLoss 0.0000 (0.0047)	KLLoss 0.0000 (0.0227)	MaskLoss 0.0000 (0.0717)	MaskBCELoss 0.0000 (0.0167)	MaskDICELoss 0.0000 (0.0550)
Epoch: [0][466/500]	Time 25.364 (25.364)	Loss 0.0903 (0.1248)	CeLoss 0.0238 (0.0489)	SegCLSLoss 0.0029 (0.0044)	KLLoss 0.0352 (0.0306)	MaskLoss 0.1075 (0.1076)	MaskBCELoss 0.0076 (0.0133)	MaskDICELoss 0.0999 (0.0944)
Epoch: [0][467/500]	Time 26.666 (26.666)	Loss 0.1442 (0.1273)	CeLoss 0.0859 (0.0364)	SegCLSLoss 0.0042 (0.0032)	KLLoss 0.0234 (0.0350)	MaskLoss 0.1019 (0.1056)	MaskBCELoss 0.0036 (0.0244)	MaskDICELoss 0.0983 (0.0812)
Epoch: [0][468/500]	Time 27.092 (27.092)	Loss 0.1110 (0.1037)	CeLoss 0.0559 (0.0441)	SegCLSLoss 0.0059 (0.0033)	KLLoss 0.0364 (0.0216)	MaskLoss 0.1010 (0.0729)	MaskBCELoss 0.0010 (0.0143)	MaskDICELoss 0.0999 (0.0586)
Epoch: [0][469/500]	Time 28.724 (28.724)	Loss 0.1234 (0.1158)	CeLoss 0.0444 (0.0516)	SegCLSLoss 0.0021 (0.0035)	KLLoss 0.0297 (0.0255)	MaskLoss 0.1131 (0.0907)	MaskBCELoss 0.0144 (0.0113)	MaskDICELoss 0.0987 (0.0794)
Epoch: [0][470/500]	Time 25.515 (25.515)	Loss 0.1119 (0.1329)	CeLoss 0.0579 (0.0482)	SegCLSLoss 0.0023 (0.0033)	KLLoss 0.0254 (0.0317)	MaskLoss 0.1015 (0.1062)	MaskBCELoss 0.0015 (0.0199)	MaskDICELoss 0.1000 (0.0863)
Epoch: [0][471/500]	Time 27.100 (27.100)	Loss 0.2279 (0.1875)	CeLoss 0.0728 (0.0634)	SegCLSLoss 0.0020 (0.0028)	KLLoss 0.0359 (0.0298)	MaskLoss 0.1271 (0.1111)	MaskBCELoss 0.0603 (0.0448)	MaskDICELoss 0.0668 (0.0664)
Epoch: [0][472/500]	Time 25.893 (25.893)	Loss 0.1214 (0.1049)	CeLoss 0.0737 (0.0398)	SegCLSLoss 0.0065 (0.0034)	KLLoss 0.0254 (0.0313)	MaskLoss 0.0757 (0.0991)	MaskBCELoss 0.0044 (0.0092)	MaskDICELoss 0.0713 (0.0898)
Epoch: [0][473/500]	Time 27.676 (27.676)	Loss 0.2828 (0.1110)	CeLoss 0.0371 (0.0499)	SegCLSLoss 0.0035 (0.0025)	KLLoss 0.0270 (0.0235)	MaskLoss 0.1889 (0.0775)	MaskBCELoss 0.0997 (0.0140)	MaskDICELoss 0.0893 (0.0635)
Epoch: [0][474/500]	Time 26.273 (26.273)	Loss 0.0625 (0.1221)	CeLoss 0.0625 (0.0423)	SegCLSLoss 0.0000 (0.0028)	KLLoss 0.0000 (0.0289)	MaskLoss 0.0000 (0.1005)	MaskBCELoss 0.0000 (0.0188)	MaskDICELoss 0.0000 (0.0817)
Epoch: [0][475/500]	Time 29.802 (29.802)	Loss 0.1037 (0.1264)	CeLoss 0.0242 (0.0491)	SegCLSLoss 0.0062 (0.0036)	KLLoss 0.0325 (0.0319)	MaskLoss 0.1112 (0.0937)	MaskBCELoss 0.0140 (0.0191)	MaskDICELoss 0.0972 (0.0746)
Epoch: [0][476/500]	Time 27.537 (27.537)	Loss 0.0450 (0.1194)	CeLoss 0.0242 (0.0424)	SegCLSLoss 0.0029 (0.0034)	KLLoss 0.0481 (0.0298)	MaskLoss 0.0293 (0.0946)	MaskBCELoss 0.0032 (0.0186)	MaskDICELoss 0.0261 (0.0760)
Epoch: [0][477/500]	Time 27.839 (27.839)	Loss 0.2221 (0.1396)	CeLoss 0.0503 (0.0436)	SegCLSLoss 0.0020 (0.0040)	KLLoss 0.0347 (0.0341)	MaskLoss 0.1477 (0.1110)	MaskBCELoss 0.0644 (0.0257)	MaskDICELoss 0.0833 (0.0853)
Epoch: [0][478/500]	Time 28.476 (28.476)	Loss 0.1752 (0.1374)	CeLoss 0.0771 (0.0406)	SegCLSLoss 0.0033 (0.0039)	KLLoss 0.0352 (0.0317)	MaskLoss 0.1109 (0.1142)	MaskBCELoss 0.0275 (0.0252)	MaskDICELoss 0.0835 (0.0890)
Epoch: [0][479/500]	Time 28.392 (28.392)	Loss 0.1035 (0.1382)	CeLoss 0.0234 (0.0538)	SegCLSLoss 0.0040 (0.0035)	KLLoss 0.0300 (0.0345)	MaskLoss 0.0786 (0.1021)	MaskBCELoss 0.0259 (0.0210)	MaskDICELoss 0.0527 (0.0811)
Epoch: [0][480/500]	Time 27.730 (27.730)	Loss 0.1331 (0.1246)	CeLoss 0.0591 (0.0290)	SegCLSLoss 0.0075 (0.0040)	KLLoss 0.0317 (0.0237)	MaskLoss 0.1047 (0.1009)	MaskBCELoss 0.0119 (0.0288)	MaskDICELoss 0.0928 (0.0721)
Epoch: [0][481/500]	Time 27.067 (27.067)	Loss 0.1175 (0.1182)	CeLoss 0.0562 (0.0559)	SegCLSLoss 0.0019 (0.0028)	KLLoss 0.0354 (0.0317)	MaskLoss 0.1037 (0.0953)	MaskBCELoss 0.0058 (0.0088)	MaskDICELoss 0.0980 (0.0865)
Epoch: [0][482/500]	Time 25.954 (25.954)	Loss 0.1029 (0.1678)	CeLoss 0.0256 (0.0453)	SegCLSLoss 0.0045 (0.0033)	KLLoss 0.0248 (0.0273)	MaskLoss 0.0973 (0.1193)	MaskBCELoss 0.0176 (0.0408)	MaskDICELoss 0.0798 (0.0784)
Epoch: [0][483/500]	Time 28.886 (28.886)	Loss 0.1467 (0.1091)	CeLoss 0.0928 (0.0454)	SegCLSLoss 0.0051 (0.0032)	KLLoss 0.0267 (0.0295)	MaskLoss 0.1006 (0.0972)	MaskBCELoss 0.0007 (0.0090)	MaskDICELoss 0.0999 (0.0882)
Epoch: [0][484/500]	Time 30.559 (30.559)	Loss 0.1506 (0.1389)	CeLoss 0.0732 (0.0442)	SegCLSLoss 0.0019 (0.0020)	KLLoss 0.0276 (0.0299)	MaskLoss 0.0805 (0.1010)	MaskBCELoss 0.0241 (0.0288)	MaskDICELoss 0.0564 (0.0723)
Epoch: [0][485/500]	Time 28.587 (28.587)	Loss 0.1007 (0.1248)	CeLoss 0.0220 (0.0407)	SegCLSLoss 0.0092 (0.0038)	KLLoss 0.0264 (0.0290)	MaskLoss 0.1050 (0.0988)	MaskBCELoss 0.0144 (0.0219)	MaskDICELoss 0.0907 (0.0769)
Epoch: [0][486/500]	Time 28.153 (28.153)	Loss 0.0811 (0.1745)	CeLoss 0.0139 (0.0482)	SegCLSLoss 0.0056 (0.0037)	KLLoss 0.0233 (0.0335)	MaskLoss 0.0834 (0.1196)	MaskBCELoss 0.0152 (0.0431)	MaskDICELoss 0.0682 (0.0765)
Epoch: [0][487/500]	Time 25.608 (25.608)	Loss 0.1896 (0.1681)	CeLoss 0.0513 (0.0497)	SegCLSLoss 0.0019 (0.0031)	KLLoss 0.0242 (0.0314)	MaskLoss 0.1362 (0.1285)	MaskBCELoss 0.0464 (0.0350)	MaskDICELoss 0.0898 (0.0935)
Epoch: [0][488/500]	Time 29.052 (29.052)	Loss 0.0848 (0.1067)	CeLoss 0.0850 (0.0570)	SegCLSLoss 0.0000 (0.0021)	KLLoss 0.0000 (0.0249)	MaskLoss 0.0000 (0.0823)	MaskBCELoss 0.0000 (0.0051)	MaskDICELoss 0.0000 (0.0772)
Epoch: [0][489/500]	Time 26.684 (26.684)	Loss 0.1186 (0.1666)	CeLoss 0.0654 (0.0561)	SegCLSLoss 0.0053 (0.0039)	KLLoss 0.0356 (0.0302)	MaskLoss 0.0985 (0.1143)	MaskBCELoss 0.0009 (0.0343)	MaskDICELoss 0.0976 (0.0800)
Epoch: [0][490/500]	Time 28.034 (28.034)	Loss 0.1991 (0.0961)	CeLoss 0.1123 (0.0348)	SegCLSLoss 0.0098 (0.0042)	KLLoss 0.0339 (0.0241)	MaskLoss 0.1125 (0.0864)	MaskBCELoss 0.0171 (0.0107)	MaskDICELoss 0.0953 (0.0758)
Epoch: [0][491/500]	Time 25.684 (25.684)	Loss 0.0927 (0.1397)	CeLoss 0.0190 (0.0526)	SegCLSLoss 0.0057 (0.0041)	KLLoss 0.0195 (0.0268)	MaskLoss 0.1092 (0.1000)	MaskBCELoss 0.0108 (0.0234)	MaskDICELoss 0.0984 (0.0766)
Epoch: [0][492/500]	Time 31.036 (31.036)	Loss 0.2416 (0.1547)	CeLoss 0.0884 (0.0532)	SegCLSLoss 0.0084 (0.0039)	KLLoss 0.0198 (0.0258)	MaskLoss 0.1249 (0.1054)	MaskBCELoss 0.0577 (0.0312)	MaskDICELoss 0.0672 (0.0742)
Epoch: [0][493/500]	Time 28.037 (28.037)	Loss 0.1055 (0.0975)	CeLoss 0.0425 (0.0435)	SegCLSLoss 0.0078 (0.0031)	KLLoss 0.0264 (0.0214)	MaskLoss 0.1027 (0.0764)	MaskBCELoss 0.0051 (0.0095)	MaskDICELoss 0.0975 (0.0669)
Epoch: [0][494/500]	Time 29.910 (29.910)	Loss 0.1691 (0.1259)	CeLoss 0.1006 (0.0549)	SegCLSLoss 0.0043 (0.0032)	KLLoss 0.0251 (0.0300)	MaskLoss 0.1079 (0.0960)	MaskBCELoss 0.0081 (0.0142)	MaskDICELoss 0.0998 (0.0817)
Epoch: [0][495/500]	Time 26.879 (26.879)	Loss 0.1236 (0.1403)	CeLoss 0.0605 (0.0431)	SegCLSLoss 0.0053 (0.0033)	KLLoss 0.0222 (0.0279)	MaskLoss 0.0795 (0.1071)	MaskBCELoss 0.0137 (0.0280)	MaskDICELoss 0.0658 (0.0791)
Epoch: [0][496/500]	Time 30.953 (30.953)	Loss 0.1790 (0.1326)	CeLoss 0.0542 (0.0428)	SegCLSLoss 0.0015 (0.0038)	KLLoss 0.0339 (0.0313)	MaskLoss 0.1279 (0.1055)	MaskBCELoss 0.0400 (0.0234)	MaskDICELoss 0.0880 (0.0821)
Epoch: [0][497/500]	Time 28.739 (28.739)	Loss 0.0856 (0.1455)	CeLoss 0.0212 (0.0537)	SegCLSLoss 0.0027 (0.0040)	KLLoss 0.0195 (0.0257)	MaskLoss 0.1057 (0.1061)	MaskBCELoss 0.0067 (0.0244)	MaskDICELoss 0.0989 (0.0817)
Epoch: [0][498/500]	Time 31.355 (31.355)	Loss 0.1524 (0.1087)	CeLoss 0.0967 (0.0485)	SegCLSLoss 0.0057 (0.0033)	KLLoss 0.0300 (0.0275)	MaskLoss 0.0958 (0.0861)	MaskBCELoss 0.0032 (0.0103)	MaskDICELoss 0.0926 (0.0758)
Epoch: [0][499/500]	Time 29.630 (29.630)	Loss 0.0537 (0.1137)	CeLoss 0.0317 (0.0449)	SegCLSLoss 0.0038 (0.0034)	KLLoss 0.0359 (0.0275)	MaskLoss 0.0266 (0.0817)	MaskBCELoss 0.0044 (0.0175)	MaskDICELoss 0.0223 (0.0642)
[2025-03-09 13:17:17,601] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[0.00029461445783132527], mom=[(0.9, 0.95)]
[2025-03-09 13:17:17,606] [INFO] [timer.py:215:stop] epoch=0/micro_step=500/global_step=50, RunningAvgSamplesPerSec=1.421815374303068, CurrSamplesPerSec=1.4693057254193098, MemAllocated=60.24GB, MaxMemAllocated=73.04GB
Epoch: [0][500/500]	Time 26.602 (26.602)	Loss 0.1130 (0.1402)	CeLoss 0.0571 (0.0484)	SegCLSLoss 0.0017 (0.0028)	KLLoss 0.0330 (0.0275)	MaskLoss 0.1025 (0.1039)	MaskBCELoss 0.0026 (0.0256)	MaskDICELoss 0.0999 (0.0783)




























 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 198/200 [00:57<00:00,  4.06it/s]
giou: 0.0344, ciou: 0.0228
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:58<00:00,  3.42it/s]
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[34m[1mwandb[39m[22m: [33mWARNING[39m Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 499. Dropping entry: {'val/giou': 0.03437110409140587, 'val/ciou': 0.022807853296399117, '_timestamp': 1741544296.4134963}).
[2025-03-09 13:18:43,914] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./runs/plum-13b_kld_0_dice_0.5/plum-13b_kld_0_dice_0.5_accum-10_maxlen512_epochs50_bsz4_kld_loss_0.0_dice_loss_0.5_1000.0_lr0.0003_teacher_ckpt_model/global_step50/mp_rank_00_model_states.pt
[2025-03-09 13:18:43,915] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./runs/plum-13b_kld_0_dice_0.5/plum-13b_kld_0_dice_0.5_accum-10_maxlen512_epochs50_bsz4_kld_loss_0.0_dice_loss_0.5_1000.0_lr0.0003_teacher_ckpt_model/global_step50/mp_rank_00_model_states.pt...
[2025-03-09 13:22:09,857] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./runs/plum-13b_kld_0_dice_0.5/plum-13b_kld_0_dice_0.5_accum-10_maxlen512_epochs50_bsz4_kld_loss_0.0_dice_loss_0.5_1000.0_lr0.0003_teacher_ckpt_model/global_step50/mp_rank_00_model_states.pt.
[2025-03-09 13:22:11,881] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./runs/plum-13b_kld_0_dice_0.5/plum-13b_kld_0_dice_0.5_accum-10_maxlen512_epochs50_bsz4_kld_loss_0.0_dice_loss_0.5_1000.0_lr0.0003_teacher_ckpt_model/global_step50/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-09 13:22:21,502] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./runs/plum-13b_kld_0_dice_0.5/plum-13b_kld_0_dice_0.5_accum-10_maxlen512_epochs50_bsz4_kld_loss_0.0_dice_loss_0.5_1000.0_lr0.0003_teacher_ckpt_model/global_step50/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-09 13:22:21,504] [INFO] [engine.py:3244:_save_zero_checkpoint] zero checkpoint saved ./runs/plum-13b_kld_0_dice_0.5/plum-13b_kld_0_dice_0.5_accum-10_maxlen512_epochs50_bsz4_kld_loss_0.0_dice_loss_0.5_1000.0_lr0.0003_teacher_ckpt_model/global_step50/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-09 13:22:21,505] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step50 is ready now!
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch: [1][  1/500]	Time 28.565 (28.565)	Loss 0.3027 (0.1479)	CeLoss 0.0547 (0.0438)	SegCLSLoss 0.0018 (0.0038)	KLLoss 0.0417 (0.0306)	MaskLoss 0.1691 (0.1164)	MaskBCELoss 0.1085 (0.0293)	MaskDICELoss 0.0606 (0.0870)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 499. Dropping entry: {'train/loss': 0.14792468696832656, 'train/ce_loss': 0.04381103515625, 'train/seg_cls_loss': 0.00384368896484375, 'train/kl_loss': 0.030615234375, 'train/mask_bce_loss': 0.029329315875656903, 'train/mask_dice_loss': 0.08704773038625717, 'train/mask_loss': 0.11637704744935036, 'metrics/total_secs_per_batch': 28.56511616706848, 'metrics/data_secs_per_batch': 13.165747213363648, '_timestamp': 1741544570.0891814}).
Epoch: [1][  2/500]	Time 27.081 (27.081)	Loss 0.1050 (0.1519)	CeLoss 0.0236 (0.0485)	SegCLSLoss 0.0034 (0.0034)	KLLoss 0.0352 (0.0286)	MaskLoss 0.1017 (0.1128)	MaskBCELoss 0.0193 (0.0303)	MaskDICELoss 0.0823 (0.0825)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 1 is less than current step: 499. Dropping entry: {'train/loss': 0.15188410133123398, 'train/ce_loss': 0.0484619140625, 'train/seg_cls_loss': 0.003424072265625, 'train/kl_loss': 0.02857666015625, 'train/mask_bce_loss': 0.030307000572793185, 'train/mask_dice_loss': 0.08246207311749458, 'train/mask_loss': 0.11276907324790955, 'metrics/total_secs_per_batch': 27.081013917922974, 'metrics/data_secs_per_batch': 11.908275508880616, '_timestamp': 1741544597.169912}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 1 is less than current step: 499. Dropping entry: {'train/lr': 0.0002945903614457831, '_timestamp': 1741544597.1702049}).
Epoch: [1][  3/500]	Time 29.280 (29.280)	Loss 0.0765 (0.1334)	CeLoss 0.0133 (0.0479)	SegCLSLoss 0.0023 (0.0023)	KLLoss 0.0366 (0.0269)	MaskLoss 0.1034 (0.1037)	MaskBCELoss 0.0069 (0.0217)	MaskDICELoss 0.0965 (0.0820)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 2 is less than current step: 499. Dropping entry: {'train/loss': 0.1334034209139645, 'train/ce_loss': 0.047894287109375, 'train/seg_cls_loss': 0.002325439453125, 'train/kl_loss': 0.02686767578125, 'train/mask_bce_loss': 0.021650523832067847, 'train/mask_dice_loss': 0.082040686160326, 'train/mask_loss': 0.10369121059775352, 'metrics/total_secs_per_batch': 29.280494213104248, 'metrics/data_secs_per_batch': 13.089408731460571, '_timestamp': 1741544626.4504037}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 2 is less than current step: 499. Dropping entry: {'train/lr': 0.000294578313253012, '_timestamp': 1741544626.4506626}).
Epoch: [1][  4/500]	Time 30.922 (30.922)	Loss 0.1256 (0.1426)	CeLoss 0.0247 (0.0417)	SegCLSLoss 0.0039 (0.0034)	KLLoss 0.0286 (0.0282)	MaskLoss 0.1171 (0.1166)	MaskBCELoss 0.0269 (0.0272)	MaskDICELoss 0.0902 (0.0893)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 3 is less than current step: 499. Dropping entry: {'train/loss': 0.1426062785089016, 'train/ce_loss': 0.04173583984375, 'train/seg_cls_loss': 0.003438568115234375, 'train/kl_loss': 0.02821044921875, 'train/mask_bce_loss': 0.02723904000595212, 'train/mask_dice_loss': 0.08934232592582703, 'train/mask_loss': 0.11658136546611786, 'metrics/total_secs_per_batch': 30.921514749526978, 'metrics/data_secs_per_batch': 13.845545530319214, '_timestamp': 1741544657.3718896}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 3 is less than current step: 499. Dropping entry: {'train/lr': 0.0002945662650602409, '_timestamp': 1741544657.3721411}).
Epoch: [1][  5/500]	Time 28.506 (28.506)	Loss 0.0795 (0.1422)	CeLoss 0.0164 (0.0495)	SegCLSLoss 0.0023 (0.0030)	KLLoss 0.0320 (0.0260)	MaskLoss 0.1039 (0.1053)	MaskBCELoss 0.0066 (0.0257)	MaskDICELoss 0.0972 (0.0796)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 4 is less than current step: 499. Dropping entry: {'train/loss': 0.14219361506402492, 'train/ce_loss': 0.04951171875, 'train/seg_cls_loss': 0.003040313720703125, 'train/kl_loss': 0.02596435546875, 'train/mask_bce_loss': 0.025658767204731703, 'train/mask_dice_loss': 0.07964278012514114, 'train/mask_loss': 0.10530154928565025, 'metrics/total_secs_per_batch': 28.506368160247803, 'metrics/data_secs_per_batch': 13.246676230430603, '_timestamp': 1741544685.878367}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 4 is less than current step: 499. Dropping entry: {'train/lr': 0.0002945542168674699, '_timestamp': 1741544685.8786407}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 5 is less than current step: 499. Dropping entry: {'train/loss': 0.12002759277820588, 'train/ce_loss': 0.03525390625, 'train/seg_cls_loss': 0.003369140625, 'train/kl_loss': 0.03070068359375, 'train/mask_bce_loss': 0.02036583002191037, 'train/mask_dice_loss': 0.0848418552428484, 'train/mask_loss': 0.10520768463611603, 'metrics/total_secs_per_batch': 27.654805660247803, 'metrics/data_secs_per_batch': 12.778410339355469, '_timestamp': 1741544713.5331423}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 5 is less than current step: 499. Dropping entry: {'train/lr': 0.0002945421686746988, '_timestamp': 1741544713.5335424}).
Epoch: [1][  6/500]	Time 27.655 (27.655)	Loss 0.1609 (0.1200)	CeLoss 0.0635 (0.0353)	SegCLSLoss 0.0049 (0.0034)	KLLoss 0.0374 (0.0307)	MaskLoss 0.0902 (0.1052)	MaskBCELoss 0.0335 (0.0204)	MaskDICELoss 0.0567 (0.0848)
Epoch: [1][  7/500]	Time 29.055 (29.055)	Loss 0.1016 (0.1090)	CeLoss 0.0256 (0.0444)	SegCLSLoss 0.0039 (0.0026)	KLLoss 0.0347 (0.0238)	MaskLoss 0.0976 (0.0780)	MaskBCELoss 0.0168 (0.0162)	MaskDICELoss 0.0807 (0.0618)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 6 is less than current step: 499. Dropping entry: {'train/loss': 0.10904266312718391, 'train/ce_loss': 0.0443603515625, 'train/seg_cls_loss': 0.002630615234375, 'train/kl_loss': 0.02381591796875, 'train/mask_bce_loss': 0.016233533853664994, 'train/mask_dice_loss': 0.06178400125354529, 'train/mask_loss': 0.07801753394305706, 'metrics/total_secs_per_batch': 29.054922819137573, 'metrics/data_secs_per_batch': 12.58230917453766, '_timestamp': 1741544742.5880196}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 6 is less than current step: 499. Dropping entry: {'train/lr': 0.0002945301204819277, '_timestamp': 1741544742.5883992}).
Epoch: [1][  8/500]	Time 26.334 (26.334)	Loss 0.1835 (0.1265)	CeLoss 0.0239 (0.0441)	SegCLSLoss 0.0028 (0.0024)	KLLoss 0.0374 (0.0293)	MaskLoss 0.1416 (0.0875)	MaskBCELoss 0.0582 (0.0250)	MaskDICELoss 0.0834 (0.0626)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 7 is less than current step: 499. Dropping entry: {'train/loss': 0.12649196125566958, 'train/ce_loss': 0.044085693359375, 'train/seg_cls_loss': 0.002384185791015625, 'train/kl_loss': 0.0292724609375, 'train/mask_bce_loss': 0.024952382361516357, 'train/mask_dice_loss': 0.06256892438977957, 'train/mask_loss': 0.08752130400389432, 'metrics/total_secs_per_batch': 26.33381199836731, 'metrics/data_secs_per_batch': 12.29996223449707, '_timestamp': 1741544768.9222405}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 7 is less than current step: 499. Dropping entry: {'train/lr': 0.0002945180722891566, '_timestamp': 1741544768.9227688}).
Epoch: [1][  9/500]	Time 29.196 (29.196)	Loss 0.0959 (0.1017)	CeLoss 0.0449 (0.0436)	SegCLSLoss 0.0047 (0.0044)	KLLoss 0.0325 (0.0273)	MaskLoss 0.0744 (0.0883)	MaskBCELoss 0.0076 (0.0079)	MaskDICELoss 0.0668 (0.0804)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 8 is less than current step: 499. Dropping entry: {'train/loss': 0.1017296090722084, 'train/ce_loss': 0.04361572265625, 'train/seg_cls_loss': 0.00435791015625, 'train/kl_loss': 0.027294921875, 'train/mask_bce_loss': 0.007881231210194527, 'train/mask_dice_loss': 0.08044991493225098, 'train/mask_loss': 0.0883311465382576, 'metrics/total_secs_per_batch': 29.19562840461731, 'metrics/data_secs_per_batch': 12.896644282341004, '_timestamp': 1741544798.1175764}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 8 is less than current step: 499. Dropping entry: {'train/lr': 0.0002945060240963855, '_timestamp': 1741544798.1179893}).
Epoch: [1][ 10/500]	Time 26.696 (26.696)	Loss 0.0879 (0.1354)	CeLoss 0.0227 (0.0519)	SegCLSLoss 0.0055 (0.0038)	KLLoss 0.0247 (0.0298)	MaskLoss 0.1061 (0.1081)	MaskBCELoss 0.0063 (0.0184)	MaskDICELoss 0.0998 (0.0897)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 9 is less than current step: 499. Dropping entry: {'train/loss': 0.13540427386760712, 'train/ce_loss': 0.0519287109375, 'train/seg_cls_loss': 0.003816986083984375, 'train/kl_loss': 0.0298095703125, 'train/mask_bce_loss': 0.018363147787749768, 'train/mask_dice_loss': 0.08973876759409904, 'train/mask_loss': 0.10810191407799721, 'metrics/total_secs_per_batch': 26.69627070426941, 'metrics/data_secs_per_batch': 11.362096190452576, '_timestamp': 1741544824.8137496}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 9 is less than current step: 499. Dropping entry: {'train/lr': 0.00029448192771084334, '_timestamp': 1741544824.8141217}).
Epoch: [1][ 11/500]	Time 28.958 (28.958)	Loss 0.1093 (0.1386)	CeLoss 0.0522 (0.0419)	SegCLSLoss 0.0018 (0.0028)	KLLoss 0.0300 (0.0233)	MaskLoss 0.1021 (0.1080)	MaskBCELoss 0.0035 (0.0275)	MaskDICELoss 0.0986 (0.0805)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 10 is less than current step: 499. Dropping entry: {'train/loss': 0.13857114538550377, 'train/ce_loss': 0.04193115234375, 'train/seg_cls_loss': 0.0027557373046875, 'train/kl_loss': 0.023291015625, 'train/mask_bce_loss': 0.027488597785122693, 'train/mask_dice_loss': 0.08050820529460907, 'train/mask_loss': 0.10799680277705193, 'metrics/total_secs_per_batch': 28.958236694335938, 'metrics/data_secs_per_batch': 13.237845301628113, '_timestamp': 1741544853.7719972}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 10 is less than current step: 499. Dropping entry: {'train/lr': 0.00029446987951807225, '_timestamp': 1741544853.7722657}).
Epoch: [1][ 12/500]	Time 26.957 (26.957)	Loss 0.1326 (0.1190)	CeLoss 0.0220 (0.0475)	SegCLSLoss 0.0111 (0.0044)	KLLoss 0.0258 (0.0248)	MaskLoss 0.1234 (0.0956)	MaskBCELoss 0.0289 (0.0143)	MaskDICELoss 0.0945 (0.0813)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 11 is less than current step: 499. Dropping entry: {'train/loss': 0.11899493038654327, 'train/ce_loss': 0.04752197265625, 'train/seg_cls_loss': 0.0043548583984375, 'train/kl_loss': 0.024755859375, 'train/mask_bce_loss': 0.014340662921313196, 'train/mask_dice_loss': 0.08127174153923988, 'train/mask_loss': 0.09561240375041961, 'metrics/total_secs_per_batch': 26.956594944000244, 'metrics/data_secs_per_batch': 13.406776237487794, '_timestamp': 1741544880.7285922}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 11 is less than current step: 499. Dropping entry: {'train/lr': 0.00029445783132530116, '_timestamp': 1741544880.7288516}).
Epoch: [1][ 13/500]	Time 26.075 (26.075)	Loss 0.1373 (0.1054)	CeLoss 0.0791 (0.0388)	SegCLSLoss 0.0034 (0.0027)	KLLoss 0.0210 (0.0227)	MaskLoss 0.1033 (0.0852)	MaskBCELoss 0.0034 (0.0152)	MaskDICELoss 0.0999 (0.0700)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 12 is less than current step: 499. Dropping entry: {'train/loss': 0.10541367745026946, 'train/ce_loss': 0.038763427734375, 'train/seg_cls_loss': 0.002742767333984375, 'train/kl_loss': 0.0226806640625, 'train/mask_bce_loss': 0.015175645984709264, 'train/mask_dice_loss': 0.06998805217444896, 'train/mask_loss': 0.08516369834542274, 'metrics/total_secs_per_batch': 26.07539677619934, 'metrics/data_secs_per_batch': 11.514736866950988, '_timestamp': 1741544906.8039997}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 12 is less than current step: 499. Dropping entry: {'train/lr': 0.00029444578313253013, '_timestamp': 1741544906.8043766}).
Epoch: [1][ 14/500]	Time 29.577 (29.577)	Loss 0.1472 (0.1328)	CeLoss 0.0527 (0.0404)	SegCLSLoss 0.0011 (0.0031)	KLLoss 0.0309 (0.0294)	MaskLoss 0.1136 (0.1123)	MaskBCELoss 0.0249 (0.0231)	MaskDICELoss 0.0887 (0.0892)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 13 is less than current step: 499. Dropping entry: {'train/loss': 0.13283264636993408, 'train/ce_loss': 0.04041748046875, 'train/seg_cls_loss': 0.003125762939453125, 'train/kl_loss': 0.0294189453125, 'train/mask_bce_loss': 0.023143540130695327, 'train/mask_dice_loss': 0.08918488398194313, 'train/mask_loss': 0.11232842281460761, 'metrics/total_secs_per_batch': 29.576926469802856, 'metrics/data_secs_per_batch': 13.746027112007141, '_timestamp': 1741544936.3809273}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 13 is less than current step: 499. Dropping entry: {'train/lr': 0.00029443373493975904, '_timestamp': 1741544936.38132}).
Epoch: [1][ 15/500]	Time 25.587 (25.587)	Loss 0.0962 (0.1168)	CeLoss 0.0229 (0.0369)	SegCLSLoss 0.0038 (0.0031)	KLLoss 0.0264 (0.0224)	MaskLoss 0.1018 (0.0901)	MaskBCELoss 0.0136 (0.0222)	MaskDICELoss 0.0882 (0.0679)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 14 is less than current step: 499. Dropping entry: {'train/loss': 0.11676429007202387, 'train/ce_loss': 0.03692626953125, 'train/seg_cls_loss': 0.003058624267578125, 'train/kl_loss': 0.02236328125, 'train/mask_bce_loss': 0.02219637508969754, 'train/mask_dice_loss': 0.06786807738244534, 'train/mask_loss': 0.09006445407867432, 'metrics/total_secs_per_batch': 25.58703851699829, 'metrics/data_secs_per_batch': 11.71604356765747, '_timestamp': 1741544961.9679785}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 14 is less than current step: 499. Dropping entry: {'train/lr': 0.00029442168674698795, '_timestamp': 1741544961.9683557}).
Epoch: [1][ 16/500]	Time 28.062 (28.062)	Loss 0.1745 (0.1294)	CeLoss 0.0237 (0.0455)	SegCLSLoss 0.0031 (0.0025)	KLLoss 0.0320 (0.0217)	MaskLoss 0.1314 (0.0941)	MaskBCELoss 0.0558 (0.0237)	MaskDICELoss 0.0756 (0.0704)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 15 is less than current step: 499. Dropping entry: {'train/loss': 0.12940750224515796, 'train/ce_loss': 0.045489501953125, 'train/seg_cls_loss': 0.002520751953125, 'train/kl_loss': 0.02174072265625, 'train/mask_bce_loss': 0.02374716252088547, 'train/mask_dice_loss': 0.07036932483315468, 'train/mask_loss': 0.09411648735404014, 'metrics/total_secs_per_batch': 28.06232190132141, 'metrics/data_secs_per_batch': 12.634759545326233, '_timestamp': 1741544990.0302992}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 15 is less than current step: 499. Dropping entry: {'train/lr': 0.00029440963855421686, '_timestamp': 1741544990.0306582}).
Epoch: [1][ 17/500]	Time 24.886 (24.886)	Loss 0.1234 (0.1351)	CeLoss 0.0693 (0.0620)	SegCLSLoss 0.0021 (0.0039)	KLLoss 0.0293 (0.0264)	MaskLoss 0.1014 (0.0829)	MaskBCELoss 0.0016 (0.0198)	MaskDICELoss 0.0998 (0.0631)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 16 is less than current step: 499. Dropping entry: {'train/loss': 0.1351128650829196, 'train/ce_loss': 0.06197509765625, 'train/seg_cls_loss': 0.00387115478515625, 'train/kl_loss': 0.02635498046875, 'train/mask_bce_loss': 0.01981778781628236, 'train/mask_dice_loss': 0.06312254220247268, 'train/mask_loss': 0.08294033110141755, 'metrics/total_secs_per_batch': 24.885515213012695, 'metrics/data_secs_per_batch': 11.5806720495224, '_timestamp': 1741545014.915791}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 16 is less than current step: 499. Dropping entry: {'train/lr': 0.00029439759036144577, '_timestamp': 1741545014.916039}).
Epoch: [1][ 18/500]	Time 28.116 (28.116)	Loss 0.1855 (0.1412)	CeLoss 0.0835 (0.0445)	SegCLSLoss 0.0042 (0.0040)	KLLoss 0.0286 (0.0285)	MaskLoss 0.1222 (0.1144)	MaskBCELoss 0.0259 (0.0250)	MaskDICELoss 0.0964 (0.0893)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 17 is less than current step: 499. Dropping entry: {'train/loss': 0.14116354957222937, 'train/ce_loss': 0.0444580078125, 'train/seg_cls_loss': 0.0039764404296875, 'train/kl_loss': 0.0284912109375, 'train/mask_bce_loss': 0.025047350977547466, 'train/mask_dice_loss': 0.08930566720664501, 'train/mask_loss': 0.11435301825404168, 'metrics/total_secs_per_batch': 28.116103649139404, 'metrics/data_secs_per_batch': 12.894721865653992, '_timestamp': 1741545043.0319128}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 17 is less than current step: 499. Dropping entry: {'train/lr': 0.0002943855421686747, '_timestamp': 1741545043.032292}).
Epoch: [1][ 19/500]	Time 27.546 (27.546)	Loss 0.1525 (0.1307)	CeLoss 0.0869 (0.0370)	SegCLSLoss 0.0052 (0.0037)	KLLoss 0.0342 (0.0293)	MaskLoss 0.1026 (0.1134)	MaskBCELoss 0.0079 (0.0234)	MaskDICELoss 0.0947 (0.0899)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 18 is less than current step: 499. Dropping entry: {'train/loss': 0.13066781535744668, 'train/ce_loss': 0.03704833984375, 'train/seg_cls_loss': 0.0037322998046875, 'train/kl_loss': 0.029296875, 'train/mask_bce_loss': 0.023415266955271363, 'train/mask_dice_loss': 0.08994019255042077, 'train/mask_loss': 0.11335545852780342, 'metrics/total_secs_per_batch': 27.545594930648804, 'metrics/data_secs_per_batch': 12.036887693405152, '_timestamp': 1741545070.5775864}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 18 is less than current step: 499. Dropping entry: {'train/lr': 0.0002943734939759036, '_timestamp': 1741545070.5781229}).
Epoch: [1][ 20/500]	Time 27.537 (27.537)	Loss 0.2790 (0.1374)	CeLoss 0.0403 (0.0485)	SegCLSLoss 0.0041 (0.0035)	KLLoss 0.0289 (0.0232)	MaskLoss 0.1697 (0.0958)	MaskBCELoss 0.1012 (0.0262)	MaskDICELoss 0.0685 (0.0696)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 19 is less than current step: 499. Dropping entry: {'train/loss': 0.13743555173277855, 'train/ce_loss': 0.04847412109375, 'train/seg_cls_loss': 0.003485107421875, 'train/kl_loss': 0.02318115234375, 'train/mask_bce_loss': 0.026178609603084622, 'train/mask_dice_loss': 0.0696293331682682, 'train/mask_loss': 0.09580794051289558, 'metrics/total_secs_per_batch': 27.53664803504944, 'metrics/data_secs_per_batch': 12.413483023643494, '_timestamp': 1741545098.114189}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 19 is less than current step: 499. Dropping entry: {'train/lr': 0.0002943493975903614, '_timestamp': 1741545098.1145673}).
Epoch: [1][ 21/500]	Time 28.290 (28.290)	Loss 0.1190 (0.1015)	CeLoss 0.0496 (0.0490)	SegCLSLoss 0.0063 (0.0035)	KLLoss 0.0283 (0.0243)	MaskLoss 0.1011 (0.0726)	MaskBCELoss 0.0105 (0.0096)	MaskDICELoss 0.0906 (0.0630)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 20 is less than current step: 499. Dropping entry: {'train/loss': 0.10150575339794159, 'train/ce_loss': 0.04898681640625, 'train/seg_cls_loss': 0.0035064697265625, 'train/kl_loss': 0.0242919921875, 'train/mask_bce_loss': 0.009622782314545475, 'train/mask_dice_loss': 0.06299693956971168, 'train/mask_loss': 0.07261972334235907, 'metrics/total_secs_per_batch': 28.29001259803772, 'metrics/data_secs_per_batch': 12.929828929901124, '_timestamp': 1741545126.4042194}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 20 is less than current step: 499. Dropping entry: {'train/lr': 0.0002943373493975903, '_timestamp': 1741545126.404618}).
Epoch: [1][ 22/500]	Time 27.709 (27.709)	Loss 0.1253 (0.1256)	CeLoss 0.0352 (0.0523)	SegCLSLoss 0.0014 (0.0035)	KLLoss 0.0214 (0.0290)	MaskLoss 0.1177 (0.1072)	MaskBCELoss 0.0204 (0.0120)	MaskDICELoss 0.0973 (0.0952)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 21 is less than current step: 499. Dropping entry: {'train/loss': 0.1255580849945545, 'train/ce_loss': 0.05225830078125, 'train/seg_cls_loss': 0.00352783203125, 'train/kl_loss': 0.02904052734375, 'train/mask_bce_loss': 0.011959317838773131, 'train/mask_dice_loss': 0.09521249309182167, 'train/mask_loss': 0.10717180743813515, 'metrics/total_secs_per_batch': 27.709184408187866, 'metrics/data_secs_per_batch': 11.495277309417725, '_timestamp': 1741545154.1134088}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 21 is less than current step: 499. Dropping entry: {'train/lr': 0.00029432530120481924, '_timestamp': 1741545154.113772}).
Epoch: [1][ 23/500]	Time 28.388 (28.388)	Loss 0.0900 (0.1285)	CeLoss 0.0256 (0.0464)	SegCLSLoss 0.0025 (0.0045)	KLLoss 0.0261 (0.0281)	MaskLoss 0.1055 (0.1032)	MaskBCELoss 0.0070 (0.0188)	MaskDICELoss 0.0985 (0.0843)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 22 is less than current step: 499. Dropping entry: {'train/loss': 0.12849930636584758, 'train/ce_loss': 0.04638671875, 'train/seg_cls_loss': 0.0044677734375, 'train/kl_loss': 0.028125, 'train/mask_bce_loss': 0.018845711241010577, 'train/mask_dice_loss': 0.08431108072400093, 'train/mask_loss': 0.10315679274499416, 'metrics/total_secs_per_batch': 28.38806700706482, 'metrics/data_secs_per_batch': 13.460753250122071, '_timestamp': 1741545182.5017955}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 22 is less than current step: 499. Dropping entry: {'train/lr': 0.00029431325301204815, '_timestamp': 1741545182.5022528}).
Epoch: [1][ 24/500]	Time 27.159 (27.159)	Loss 0.1317 (0.1204)	CeLoss 0.0586 (0.0490)	SegCLSLoss 0.0029 (0.0033)	KLLoss 0.0339 (0.0263)	MaskLoss 0.1074 (0.0927)	MaskBCELoss 0.0119 (0.0155)	MaskDICELoss 0.0956 (0.0772)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 23 is less than current step: 499. Dropping entry: {'train/loss': 0.12040231488645077, 'train/ce_loss': 0.04901123046875, 'train/seg_cls_loss': 0.00332489013671875, 'train/kl_loss': 0.026318359375, 'train/mask_bce_loss': 0.015536238998174667, 'train/mask_dice_loss': 0.0771948292851448, 'train/mask_loss': 0.09273106902837754, 'metrics/total_secs_per_batch': 27.159497499465942, 'metrics/data_secs_per_batch': 11.952285504341125, '_timestamp': 1741545209.6609597}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 23 is less than current step: 499. Dropping entry: {'train/lr': 0.00029430120481927706, '_timestamp': 1741545209.661212}).
Epoch: [1][ 25/500]	Time 26.523 (26.523)	Loss 0.0305 (0.1079)	CeLoss 0.0305 (0.0423)	SegCLSLoss 0.0000 (0.0025)	KLLoss 0.0000 (0.0235)	MaskLoss 0.0000 (0.0879)	MaskBCELoss 0.0000 (0.0135)	MaskDICELoss 0.0000 (0.0743)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 24 is less than current step: 499. Dropping entry: {'train/loss': 0.10785350054502488, 'train/ce_loss': 0.04234619140625, 'train/seg_cls_loss': 0.00251617431640625, 'train/kl_loss': 0.0235107421875, 'train/mask_bce_loss': 0.013549114018678666, 'train/mask_dice_loss': 0.07434257566928863, 'train/mask_loss': 0.08789169043302536, 'metrics/total_secs_per_batch': 26.523300647735596, 'metrics/data_secs_per_batch': 11.94939820766449, '_timestamp': 1741545236.1842394}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 24 is less than current step: 499. Dropping entry: {'train/lr': 0.00029428915662650597, '_timestamp': 1741545236.1846173}).
Epoch: [1][ 26/500]	Time 28.217 (28.217)	Loss 0.1228 (0.1297)	CeLoss 0.0610 (0.0404)	SegCLSLoss 0.0014 (0.0023)	KLLoss 0.0413 (0.0264)	MaskLoss 0.1038 (0.1009)	MaskBCELoss 0.0061 (0.0252)	MaskDICELoss 0.0977 (0.0757)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 25 is less than current step: 499. Dropping entry: {'train/loss': 0.1297348588705063, 'train/ce_loss': 0.0404296875, 'train/seg_cls_loss': 0.00225830078125, 'train/kl_loss': 0.02642822265625, 'train/mask_bce_loss': 0.025166530668502673, 'train/mask_dice_loss': 0.07571765407919884, 'train/mask_loss': 0.10088418498635292, 'metrics/total_secs_per_batch': 28.216938972473145, 'metrics/data_secs_per_batch': 12.736835646629334, '_timestamp': 1741545264.4012508}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 25 is less than current step: 499. Dropping entry: {'train/lr': 0.00029427710843373493, '_timestamp': 1741545264.4016526}).
Epoch: [1][ 27/500]	Time 24.999 (24.999)	Loss 0.1488 (0.1218)	CeLoss 0.0193 (0.0309)	SegCLSLoss 0.0052 (0.0039)	KLLoss 0.0248 (0.0277)	MaskLoss 0.1275 (0.1099)	MaskBCELoss 0.0422 (0.0227)	MaskDICELoss 0.0854 (0.0872)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 26 is less than current step: 499. Dropping entry: {'train/loss': 0.12179438248276711, 'train/ce_loss': 0.030908203125, 'train/seg_cls_loss': 0.00385589599609375, 'train/kl_loss': 0.027734375, 'train/mask_bce_loss': 0.022692228376399726, 'train/mask_dice_loss': 0.08723390810191631, 'train/mask_loss': 0.1099261350929737, 'metrics/total_secs_per_batch': 24.999297380447388, 'metrics/data_secs_per_batch': 10.87864682674408, '_timestamp': 1741545289.4005027}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 26 is less than current step: 499. Dropping entry: {'train/lr': 0.00029426506024096384, '_timestamp': 1741545289.4007533}).
Epoch: [1][ 28/500]	Time 30.002 (30.002)	Loss 0.1578 (0.1272)	CeLoss 0.0593 (0.0469)	SegCLSLoss 0.0037 (0.0029)	KLLoss 0.0242 (0.0297)	MaskLoss 0.1097 (0.1017)	MaskBCELoss 0.0277 (0.0186)	MaskDICELoss 0.0820 (0.0831)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 27 is less than current step: 499. Dropping entry: {'train/loss': 0.1272153565660119, 'train/ce_loss': 0.04693603515625, 'train/seg_cls_loss': 0.0029205322265625, 'train/kl_loss': 0.0296875, 'train/mask_bce_loss': 0.018598787195514888, 'train/mask_dice_loss': 0.0831410363316536, 'train/mask_loss': 0.10173982456326484, 'metrics/total_secs_per_batch': 30.002450466156006, 'metrics/data_secs_per_batch': 12.968882131576539, '_timestamp': 1741545319.4030244}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 27 is less than current step: 499. Dropping entry: {'train/lr': 0.00029425301204819276, '_timestamp': 1741545319.403428}).
Epoch: [1][ 29/500]	Time 29.298 (29.298)	Loss 0.1486 (0.1341)	CeLoss 0.0520 (0.0521)	SegCLSLoss 0.0028 (0.0025)	KLLoss 0.0232 (0.0230)	MaskLoss 0.1197 (0.0993)	MaskBCELoss 0.0235 (0.0207)	MaskDICELoss 0.0963 (0.0786)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 28 is less than current step: 499. Dropping entry: {'train/loss': 0.13409451115876436, 'train/ce_loss': 0.052099609375, 'train/seg_cls_loss': 0.00252685546875, 'train/kl_loss': 0.022998046875, 'train/mask_bce_loss': 0.02074642211664468, 'train/mask_dice_loss': 0.07856270559132099, 'train/mask_loss': 0.09930912926793098, 'metrics/total_secs_per_batch': 29.297857999801636, 'metrics/data_secs_per_batch': 12.452979254722596, '_timestamp': 1741545348.700944}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 28 is less than current step: 499. Dropping entry: {'train/lr': 0.00029424096385542167, '_timestamp': 1741545348.7013698}).
Epoch: [1][ 30/500]	Time 30.445 (30.445)	Loss 0.0910 (0.1371)	CeLoss 0.0276 (0.0512)	SegCLSLoss 0.0030 (0.0029)	KLLoss 0.0315 (0.0300)	MaskLoss 0.1054 (0.1114)	MaskBCELoss 0.0061 (0.0192)	MaskDICELoss 0.0993 (0.0923)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 29 is less than current step: 499. Dropping entry: {'train/loss': 0.13714574724435807, 'train/ce_loss': 0.05123291015625, 'train/seg_cls_loss': 0.002935791015625, 'train/kl_loss': 0.02996826171875, 'train/mask_bce_loss': 0.019152481760829686, 'train/mask_dice_loss': 0.09225188009440899, 'train/mask_loss': 0.11140436083078384, 'metrics/total_secs_per_batch': 30.44455337524414, 'metrics/data_secs_per_batch': 13.93171489238739, '_timestamp': 1741545379.1459804}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 29 is less than current step: 499. Dropping entry: {'train/lr': 0.0002942168674698795, '_timestamp': 1741545379.1462739}).
Epoch: [1][ 31/500]	Time 27.700 (27.700)	Loss 0.3124 (0.1410)	CeLoss 0.0435 (0.0381)	SegCLSLoss 0.0026 (0.0036)	KLLoss 0.0281 (0.0272)	MaskLoss 0.1835 (0.1133)	MaskBCELoss 0.1171 (0.0297)	MaskDICELoss 0.0664 (0.0836)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 30 is less than current step: 499. Dropping entry: {'train/loss': 0.1410225212574005, 'train/ce_loss': 0.0380615234375, 'train/seg_cls_loss': 0.00359649658203125, 'train/kl_loss': 0.027197265625, 'train/mask_bce_loss': 0.029703492810949684, 'train/mask_dice_loss': 0.08358263410627842, 'train/mask_loss': 0.11328612491488457, 'metrics/total_secs_per_batch': 27.699755668640137, 'metrics/data_secs_per_batch': 12.948782324790955, '_timestamp': 1741545406.8451803}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 30 is less than current step: 499. Dropping entry: {'train/lr': 0.0002942048192771084, '_timestamp': 1741545406.8454406}).
Epoch: [1][ 32/500]	Time 24.996 (24.996)	Loss 0.1120 (0.1390)	CeLoss 0.0422 (0.0608)	SegCLSLoss 0.0040 (0.0026)	KLLoss 0.0245 (0.0238)	MaskLoss 0.1058 (0.0923)	MaskBCELoss 0.0099 (0.0205)	MaskDICELoss 0.0959 (0.0718)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 31 is less than current step: 499. Dropping entry: {'train/loss': 0.13902920261025428, 'train/ce_loss': 0.06080322265625, 'train/seg_cls_loss': 0.0025970458984375, 'train/kl_loss': 0.023779296875, 'train/mask_bce_loss': 0.020531393820419908, 'train/mask_dice_loss': 0.07175313383340835, 'train/mask_loss': 0.0922845296561718, 'metrics/total_secs_per_batch': 24.995872497558594, 'metrics/data_secs_per_batch': 11.279327774047852, '_timestamp': 1741545431.8410704}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 31 is less than current step: 499. Dropping entry: {'train/lr': 0.0002941927710843373, '_timestamp': 1741545431.8414736}).
Epoch: [1][ 33/500]	Time 27.140 (27.140)	Loss 0.0801 (0.0983)	CeLoss 0.0217 (0.0402)	SegCLSLoss 0.0049 (0.0030)	KLLoss 0.0320 (0.0232)	MaskLoss 0.1016 (0.0874)	MaskBCELoss 0.0034 (0.0086)	MaskDICELoss 0.0982 (0.0788)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 32 is less than current step: 499. Dropping entry: {'train/loss': 0.09832730405032634, 'train/ce_loss': 0.0402099609375, 'train/seg_cls_loss': 0.00299224853515625, 'train/kl_loss': 0.02315673828125, 'train/mask_bce_loss': 0.008599646796938032, 'train/mask_dice_loss': 0.0788185179233551, 'train/mask_loss': 0.08741816505789757, 'metrics/total_secs_per_batch': 27.140273809432983, 'metrics/data_secs_per_batch': 12.538253593444825, '_timestamp': 1741545458.98135}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 32 is less than current step: 499. Dropping entry: {'train/lr': 0.0002941807228915662, '_timestamp': 1741545458.981735}).
Epoch: [1][ 34/500]	Time 27.843 (27.843)	Loss 0.0907 (0.1102)	CeLoss 0.0189 (0.0426)	SegCLSLoss 0.0056 (0.0035)	KLLoss 0.0277 (0.0259)	MaskLoss 0.1011 (0.0964)	MaskBCELoss 0.0123 (0.0118)	MaskDICELoss 0.0889 (0.0846)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 33 is less than current step: 499. Dropping entry: {'train/loss': 0.11018490921705962, 'train/ce_loss': 0.042584228515625, 'train/seg_cls_loss': 0.00345611572265625, 'train/kl_loss': 0.0258544921875, 'train/mask_bce_loss': 0.011806685535702854, 'train/mask_dice_loss': 0.0846079207956791, 'train/mask_loss': 0.0964146077632904, 'metrics/total_secs_per_batch': 27.84343695640564, 'metrics/data_secs_per_batch': 12.392907285690308, '_timestamp': 1741545486.824744}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 33 is less than current step: 499. Dropping entry: {'train/lr': 0.0002941686746987952, '_timestamp': 1741545486.8251224}).
Epoch: [1][ 35/500]	Time 28.359 (28.359)	Loss 0.1918 (0.1536)	CeLoss 0.0237 (0.0441)	SegCLSLoss 0.0046 (0.0036)	KLLoss 0.0376 (0.0254)	MaskLoss 0.1355 (0.1199)	MaskBCELoss 0.0654 (0.0318)	MaskDICELoss 0.0701 (0.0881)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 34 is less than current step: 499. Dropping entry: {'train/loss': 0.15363410189747811, 'train/ce_loss': 0.04407958984375, 'train/seg_cls_loss': 0.003641510009765625, 'train/kl_loss': 0.02535400390625, 'train/mask_bce_loss': 0.03182620609295554, 'train/mask_dice_loss': 0.08810790851712227, 'train/mask_loss': 0.1199341133236885, 'metrics/total_secs_per_batch': 28.358788013458252, 'metrics/data_secs_per_batch': 12.290403127670288, '_timestamp': 1741545515.1837177}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 34 is less than current step: 499. Dropping entry: {'train/lr': 0.0002941566265060241, '_timestamp': 1741545515.1845279}).
Epoch: [1][ 36/500]	Time 25.705 (25.705)	Loss 0.1299 (0.1324)	CeLoss 0.0396 (0.0456)	SegCLSLoss 0.0057 (0.0037)	KLLoss 0.0258 (0.0235)	MaskLoss 0.0935 (0.0942)	MaskBCELoss 0.0272 (0.0252)	MaskDICELoss 0.0663 (0.0690)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 35 is less than current step: 499. Dropping entry: {'train/loss': 0.1323731337673962, 'train/ce_loss': 0.045587158203125, 'train/seg_cls_loss': 0.0037109375, 'train/kl_loss': 0.02353515625, 'train/mask_bce_loss': 0.025221587507985532, 'train/mask_dice_loss': 0.06895757503807545, 'train/mask_loss': 0.09417916163802147, 'metrics/total_secs_per_batch': 25.705334424972534, 'metrics/data_secs_per_batch': 11.221816730499267, '_timestamp': 1741545540.8888886}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 35 is less than current step: 499. Dropping entry: {'train/lr': 0.000294144578313253, '_timestamp': 1741545540.8891432}).
Epoch: [1][ 37/500]	Time 30.775 (30.775)	Loss 0.1331 (0.1272)	CeLoss 0.0752 (0.0402)	SegCLSLoss 0.0016 (0.0036)	KLLoss 0.0286 (0.0275)	MaskLoss 0.1023 (0.1118)	MaskBCELoss 0.0041 (0.0196)	MaskDICELoss 0.0982 (0.0922)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 36 is less than current step: 499. Dropping entry: {'train/loss': 0.1272431418299675, 'train/ce_loss': 0.04019775390625, 'train/seg_cls_loss': 0.003578948974609375, 'train/kl_loss': 0.02747802734375, 'train/mask_bce_loss': 0.019583542924374343, 'train/mask_dice_loss': 0.09218726307153702, 'train/mask_loss': 0.11177080720663071, 'metrics/total_secs_per_batch': 30.774688720703125, 'metrics/data_secs_per_batch': 14.44824514389038, '_timestamp': 1741545571.663551}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 36 is less than current step: 499. Dropping entry: {'train/lr': 0.0002941325301204819, '_timestamp': 1741545571.663824}).
Epoch: [1][ 38/500]	Time 30.934 (30.934)	Loss 0.1485 (0.0928)	CeLoss 0.0229 (0.0308)	SegCLSLoss 0.0021 (0.0027)	KLLoss 0.0295 (0.0224)	MaskLoss 0.1283 (0.0851)	MaskBCELoss 0.0402 (0.0120)	MaskDICELoss 0.0881 (0.0731)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 37 is less than current step: 499. Dropping entry: {'train/loss': 0.0927941646426916, 'train/ce_loss': 0.03084716796875, 'train/seg_cls_loss': 0.002690887451171875, 'train/kl_loss': 0.02244873046875, 'train/mask_bce_loss': 0.012012432934716344, 'train/mask_dice_loss': 0.07308546900749206, 'train/mask_loss': 0.08509790226817131, 'metrics/total_secs_per_batch': 30.93428063392639, 'metrics/data_secs_per_batch': 14.569207692146302, '_timestamp': 1741545602.5982902}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 37 is less than current step: 499. Dropping entry: {'train/lr': 0.00029412048192771083, '_timestamp': 1741545602.599384}).
Epoch: [1][ 39/500]	Time 28.320 (28.320)	Loss 0.1161 (0.1696)	CeLoss 0.0481 (0.0546)	SegCLSLoss 0.0020 (0.0027)	KLLoss 0.0286 (0.0327)	MaskLoss 0.1084 (0.1166)	MaskBCELoss 0.0086 (0.0369)	MaskDICELoss 0.0999 (0.0797)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 38 is less than current step: 499. Dropping entry: {'train/loss': 0.1696143001317978, 'train/ce_loss': 0.05457763671875, 'train/seg_cls_loss': 0.002663421630859375, 'train/kl_loss': 0.03271484375, 'train/mask_bce_loss': 0.036909395921975376, 'train/mask_dice_loss': 0.07973554842174053, 'train/mask_loss': 0.11664494574069977, 'metrics/total_secs_per_batch': 28.319949626922607, 'metrics/data_secs_per_batch': 12.12846736907959, '_timestamp': 1741545630.9178689}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 38 is less than current step: 499. Dropping entry: {'train/lr': 0.00029410843373493974, '_timestamp': 1741545630.9181452}).
Epoch: [1][ 40/500]	Time 31.115 (31.115)	Loss 0.1281 (0.1622)	CeLoss 0.0801 (0.0479)	SegCLSLoss 0.0014 (0.0036)	KLLoss 0.0381 (0.0258)	MaskLoss 0.0426 (0.1185)	MaskBCELoss 0.0173 (0.0355)	MaskDICELoss 0.0253 (0.0830)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 39 is less than current step: 499. Dropping entry: {'train/loss': 0.16215420961380006, 'train/ce_loss': 0.04791259765625, 'train/seg_cls_loss': 0.003627777099609375, 'train/kl_loss': 0.0257568359375, 'train/mask_bce_loss': 0.035485482646618036, 'train/mask_dice_loss': 0.08303055725991726, 'train/mask_loss': 0.11851604059338569, 'metrics/total_secs_per_batch': 31.114980220794678, 'metrics/data_secs_per_batch': 13.985700440406799, '_timestamp': 1741545662.0329595}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 39 is less than current step: 499. Dropping entry: {'train/lr': 0.00029408433734939756, '_timestamp': 1741545662.03416}).
Epoch: [1][ 41/500]	Time 24.740 (24.740)	Loss 0.2398 (0.1562)	CeLoss 0.0815 (0.0517)	SegCLSLoss 0.0042 (0.0027)	KLLoss 0.0306 (0.0328)	MaskLoss 0.1303 (0.1115)	MaskBCELoss 0.0607 (0.0316)	MaskDICELoss 0.0696 (0.0799)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 40 is less than current step: 499. Dropping entry: {'train/loss': 0.15623457133769988, 'train/ce_loss': 0.0517333984375, 'train/seg_cls_loss': 0.002684783935546875, 'train/kl_loss': 0.03282470703125, 'train/mask_bce_loss': 0.03160976255312562, 'train/mask_dice_loss': 0.07991680447012187, 'train/mask_loss': 0.11152656637132168, 'metrics/total_secs_per_batch': 24.739588737487793, 'metrics/data_secs_per_batch': 11.695393896102905, '_timestamp': 1741545686.7724102}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 40 is less than current step: 499. Dropping entry: {'train/lr': 0.00029407228915662647, '_timestamp': 1741545686.7728353}).
Epoch: [1][ 42/500]	Time 28.666 (28.666)	Loss 0.1721 (0.1352)	CeLoss 0.0240 (0.0420)	SegCLSLoss 0.0017 (0.0030)	KLLoss 0.0339 (0.0296)	MaskLoss 0.1261 (0.1141)	MaskBCELoss 0.0561 (0.0231)	MaskDICELoss 0.0700 (0.0910)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 41 is less than current step: 499. Dropping entry: {'train/loss': 0.13520409911870956, 'train/ce_loss': 0.0420166015625, 'train/seg_cls_loss': 0.002972412109375, 'train/kl_loss': 0.0295654296875, 'train/mask_bce_loss': 0.02310728160664439, 'train/mask_dice_loss': 0.091025947406888, 'train/mask_loss': 0.1141332283616066, 'metrics/total_secs_per_batch': 28.666218996047974, 'metrics/data_secs_per_batch': 12.608831524848938, '_timestamp': 1741545715.4386086}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 41 is less than current step: 499. Dropping entry: {'train/lr': 0.0002940602409638554, '_timestamp': 1741545715.4389849}).
Epoch: [1][ 43/500]	Time 24.717 (24.717)	Loss 0.0887 (0.1335)	CeLoss 0.0212 (0.0560)	SegCLSLoss 0.0048 (0.0035)	KLLoss 0.0361 (0.0272)	MaskLoss 0.1071 (0.0971)	MaskBCELoss 0.0076 (0.0181)	MaskDICELoss 0.0995 (0.0790)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 42 is less than current step: 499. Dropping entry: {'train/loss': 0.13347467333078383, 'train/ce_loss': 0.05596923828125, 'train/seg_cls_loss': 0.003484344482421875, 'train/kl_loss': 0.02723388671875, 'train/mask_bce_loss': 0.018141988664865494, 'train/mask_dice_loss': 0.07895170450210572, 'train/mask_loss': 0.09709369316697121, 'metrics/total_secs_per_batch': 24.717156171798706, 'metrics/data_secs_per_batch': 12.668490839004516, '_timestamp': 1741545740.1562436}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 42 is less than current step: 499. Dropping entry: {'train/lr': 0.0002940481927710843, '_timestamp': 1741545740.1566818}).
Epoch: [1][ 44/500]	Time 27.678 (27.678)	Loss 0.0915 (0.1247)	CeLoss 0.0145 (0.0412)	SegCLSLoss 0.0030 (0.0032)	KLLoss 0.0309 (0.0271)	MaskLoss 0.1023 (0.0956)	MaskBCELoss 0.0161 (0.0227)	MaskDICELoss 0.0862 (0.0730)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 43 is less than current step: 499. Dropping entry: {'train/loss': 0.12466816008090972, 'train/ce_loss': 0.04119873046875, 'train/seg_cls_loss': 0.003185272216796875, 'train/kl_loss': 0.0270751953125, 'train/mask_bce_loss': 0.02266922885319218, 'train/mask_dice_loss': 0.07297581098973752, 'train/mask_loss': 0.09564504250884057, 'metrics/total_secs_per_batch': 27.6776921749115, 'metrics/data_secs_per_batch': 12.67095513343811, '_timestamp': 1741545767.8335776}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 43 is less than current step: 499. Dropping entry: {'train/lr': 0.0002940361445783132, '_timestamp': 1741545767.8339992}).
Epoch: [1][ 45/500]	Time 25.818 (25.818)	Loss 0.1334 (0.1243)	CeLoss 0.0635 (0.0399)	SegCLSLoss 0.0025 (0.0031)	KLLoss 0.0287 (0.0284)	MaskLoss 0.1074 (0.1008)	MaskBCELoss 0.0101 (0.0216)	MaskDICELoss 0.0973 (0.0792)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 44 is less than current step: 499. Dropping entry: {'train/loss': 0.12428524121642112, 'train/ce_loss': 0.0398681640625, 'train/seg_cls_loss': 0.003142547607421875, 'train/kl_loss': 0.0283935546875, 'train/mask_bce_loss': 0.021646685153245925, 'train/mask_dice_loss': 0.07915171310305595, 'train/mask_loss': 0.10079839825630188, 'metrics/total_secs_per_batch': 25.818092346191406, 'metrics/data_secs_per_batch': 11.492665719985961, '_timestamp': 1741545793.6515727}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 44 is less than current step: 499. Dropping entry: {'train/lr': 0.0002940240963855421, '_timestamp': 1741545793.6519485}).
Epoch: [1][ 46/500]	Time 27.536 (27.536)	Loss 0.1183 (0.1450)	CeLoss 0.0547 (0.0486)	SegCLSLoss 0.0021 (0.0026)	KLLoss 0.0334 (0.0280)	MaskLoss 0.1061 (0.1141)	MaskBCELoss 0.0062 (0.0253)	MaskDICELoss 0.0999 (0.0888)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 45 is less than current step: 499. Dropping entry: {'train/loss': 0.14496005177497864, 'train/ce_loss': 0.04864501953125, 'train/seg_cls_loss': 0.002587890625, 'train/kl_loss': 0.02802734375, 'train/mask_bce_loss': 0.025313683645799757, 'train/mask_dice_loss': 0.08876302242279052, 'train/mask_loss': 0.11407670527696609, 'metrics/total_secs_per_batch': 27.536129236221313, 'metrics/data_secs_per_batch': 11.993074250221252, '_timestamp': 1741545821.188303}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 45 is less than current step: 499. Dropping entry: {'train/lr': 0.0002940120481927711, '_timestamp': 1741545821.1888278}).
Epoch: [1][ 47/500]	Time 30.714 (30.714)	Loss 0.0849 (0.1143)	CeLoss 0.0128 (0.0497)	SegCLSLoss 0.0048 (0.0025)	KLLoss 0.0216 (0.0199)	MaskLoss 0.1076 (0.0858)	MaskBCELoss 0.0106 (0.0136)	MaskDICELoss 0.0970 (0.0721)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 46 is less than current step: 499. Dropping entry: {'train/loss': 0.11428316663950681, 'train/ce_loss': 0.04967041015625, 'train/seg_cls_loss': 0.00247344970703125, 'train/kl_loss': 0.01988525390625, 'train/mask_bce_loss': 0.01362596855033189, 'train/mask_dice_loss': 0.072148397564888, 'train/mask_loss': 0.08577436432242394, 'metrics/total_secs_per_batch': 30.714269399642944, 'metrics/data_secs_per_batch': 14.177505016326904, '_timestamp': 1741545851.9021611}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 46 is less than current step: 499. Dropping entry: {'train/lr': 0.000294, '_timestamp': 1741545851.902486}).
Epoch: [1][ 48/500]	Time 27.537 (27.537)	Loss 0.1034 (0.1392)	CeLoss 0.0398 (0.0487)	SegCLSLoss 0.0011 (0.0028)	KLLoss 0.0386 (0.0279)	MaskLoss 0.1065 (0.1154)	MaskBCELoss 0.0065 (0.0209)	MaskDICELoss 0.1000 (0.0945)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 47 is less than current step: 499. Dropping entry: {'train/loss': 0.13921163678169252, 'train/ce_loss': 0.04873046875, 'train/seg_cls_loss': 0.0028289794921875, 'train/kl_loss': 0.027880859375, 'train/mask_bce_loss': 0.020883886620867997, 'train/mask_dice_loss': 0.09453615620732307, 'train/mask_loss': 0.11542004570364953, 'metrics/total_secs_per_batch': 27.536659002304077, 'metrics/data_secs_per_batch': 12.327947092056274, '_timestamp': 1741545879.4389977}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 47 is less than current step: 499. Dropping entry: {'train/lr': 0.0002939879518072289, '_timestamp': 1741545879.4395068}).
Epoch: [1][ 49/500]	Time 28.459 (28.459)	Loss 0.1679 (0.1267)	CeLoss 0.0240 (0.0422)	SegCLSLoss 0.0023 (0.0030)	KLLoss 0.0228 (0.0257)	MaskLoss 0.1347 (0.1018)	MaskBCELoss 0.0502 (0.0214)	MaskDICELoss 0.0845 (0.0803)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 48 is less than current step: 499. Dropping entry: {'train/loss': 0.12673731632530688, 'train/ce_loss': 0.04217529296875, 'train/seg_cls_loss': 0.00302734375, 'train/kl_loss': 0.02572021484375, 'train/mask_bce_loss': 0.02141405499423854, 'train/mask_dice_loss': 0.08033794313669204, 'train/mask_loss': 0.10175199657678605, 'metrics/total_secs_per_batch': 28.45911478996277, 'metrics/data_secs_per_batch': 13.116156315803527, '_timestamp': 1741545907.8978024}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 48 is less than current step: 499. Dropping entry: {'train/lr': 0.0002939759036144578, '_timestamp': 1741545907.8982363}).
Epoch: [1][ 50/500]	Time 27.735 (27.735)	Loss 0.1860 (0.1473)	CeLoss 0.0786 (0.0539)	SegCLSLoss 0.0022 (0.0027)	KLLoss 0.0297 (0.0255)	MaskLoss 0.1173 (0.0998)	MaskBCELoss 0.0318 (0.0281)	MaskDICELoss 0.0855 (0.0717)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 49 is less than current step: 499. Dropping entry: {'train/loss': 0.14729707064107062, 'train/ce_loss': 0.053863525390625, 'train/seg_cls_loss': 0.002740478515625, 'train/kl_loss': 0.02548828125, 'train/mask_bce_loss': 0.028106241766363383, 'train/mask_dice_loss': 0.0717199532315135, 'train/mask_loss': 0.09982619546353817, 'metrics/total_secs_per_batch': 27.735076665878296, 'metrics/data_secs_per_batch': 12.611364102363586, '_timestamp': 1741545935.6328454}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 49 is less than current step: 499. Dropping entry: {'train/lr': 0.00029395180722891563, '_timestamp': 1741545935.6331944}).
Epoch: [1][ 51/500]	Time 32.309 (32.309)	Loss 0.1458 (0.1130)	CeLoss 0.0439 (0.0358)	SegCLSLoss 0.0046 (0.0027)	KLLoss 0.0403 (0.0270)	MaskLoss 0.1124 (0.0927)	MaskBCELoss 0.0289 (0.0196)	MaskDICELoss 0.0835 (0.0731)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 50 is less than current step: 499. Dropping entry: {'train/loss': 0.11295328866690398, 'train/ce_loss': 0.03583984375, 'train/seg_cls_loss': 0.002712249755859375, 'train/kl_loss': 0.02703857421875, 'train/mask_bce_loss': 0.019589909305796026, 'train/mask_dice_loss': 0.07311334423720836, 'train/mask_loss': 0.09270325303077698, 'metrics/total_secs_per_batch': 32.30916500091553, 'metrics/data_secs_per_batch': 15.839995837211609, '_timestamp': 1741545967.9423814}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 50 is less than current step: 499. Dropping entry: {'train/lr': 0.00029393975903614454, '_timestamp': 1741545967.947795}).
Epoch: [1][ 52/500]	Time 27.179 (27.179)	Loss 0.1256 (0.1215)	CeLoss 0.0593 (0.0386)	SegCLSLoss 0.0033 (0.0041)	KLLoss 0.0286 (0.0260)	MaskLoss 0.1068 (0.1010)	MaskBCELoss 0.0075 (0.0202)	MaskDICELoss 0.0993 (0.0808)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 51 is less than current step: 499. Dropping entry: {'train/loss': 0.12148774564266204, 'train/ce_loss': 0.03863525390625, 'train/seg_cls_loss': 0.0041351318359375, 'train/kl_loss': 0.02596435546875, 'train/mask_bce_loss': 0.020208246912807225, 'train/mask_dice_loss': 0.08083389922976494, 'train/mask_loss': 0.10104214549064636, 'metrics/total_secs_per_batch': 27.179065942764282, 'metrics/data_secs_per_batch': 12.782001566886901, '_timestamp': 1741545995.1210682}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 51 is less than current step: 499. Dropping entry: {'train/lr': 0.00029392771084337346, '_timestamp': 1741545995.1214404}).
Epoch: [1][ 53/500]	Time 30.494 (30.494)	Loss 0.2305 (0.1246)	CeLoss 0.0593 (0.0415)	SegCLSLoss 0.0012 (0.0030)	KLLoss 0.0170 (0.0227)	MaskLoss 0.1386 (0.0937)	MaskBCELoss 0.0673 (0.0231)	MaskDICELoss 0.0713 (0.0707)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 52 is less than current step: 499. Dropping entry: {'train/loss': 0.12459277398884297, 'train/ce_loss': 0.0415283203125, 'train/seg_cls_loss': 0.002950286865234375, 'train/kl_loss': 0.02271728515625, 'train/mask_bce_loss': 0.023075191862881185, 'train/mask_dice_loss': 0.07065430618822574, 'train/mask_loss': 0.09372949711978436, 'metrics/total_secs_per_batch': 30.493653535842896, 'metrics/data_secs_per_batch': 13.058347749710084, '_timestamp': 1741546025.6146908}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 52 is less than current step: 499. Dropping entry: {'train/lr': 0.00029391566265060237, '_timestamp': 1741546025.6150568}).
Epoch: [1][ 54/500]	Time 25.073 (25.073)	Loss 0.0578 (0.1370)	CeLoss 0.0579 (0.0493)	SegCLSLoss 0.0000 (0.0025)	KLLoss 0.0000 (0.0278)	MaskLoss 0.0000 (0.0980)	MaskBCELoss 0.0000 (0.0250)	MaskDICELoss 0.0000 (0.0731)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 53 is less than current step: 499. Dropping entry: {'train/loss': 0.13702655471861364, 'train/ce_loss': 0.0493408203125, 'train/seg_cls_loss': 0.00251922607421875, 'train/kl_loss': 0.02779541015625, 'train/mask_bce_loss': 0.02495596806984395, 'train/mask_dice_loss': 0.07307689636945724, 'train/mask_loss': 0.09803286418318749, 'metrics/total_secs_per_batch': 25.07335901260376, 'metrics/data_secs_per_batch': 12.090296983718872, '_timestamp': 1741546050.68812}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 53 is less than current step: 499. Dropping entry: {'train/lr': 0.00029390361445783133, '_timestamp': 1741546050.6883802}).
Epoch: [1][ 55/500]	Time 27.480 (27.480)	Loss 0.1776 (0.1447)	CeLoss 0.0212 (0.0482)	SegCLSLoss 0.0018 (0.0026)	KLLoss 0.0334 (0.0269)	MaskLoss 0.1229 (0.1015)	MaskBCELoss 0.0626 (0.0296)	MaskDICELoss 0.0603 (0.0719)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 54 is less than current step: 499. Dropping entry: {'train/loss': 0.14469889365136623, 'train/ce_loss': 0.0482177734375, 'train/seg_cls_loss': 0.00262298583984375, 'train/kl_loss': 0.0269287109375, 'train/mask_bce_loss': 0.02958727751392871, 'train/mask_dice_loss': 0.07192757651209832, 'train/mask_loss': 0.10151485279202462, 'metrics/total_secs_per_batch': 27.479865074157715, 'metrics/data_secs_per_batch': 12.584033679962157, '_timestamp': 1741546078.168021}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 54 is less than current step: 499. Dropping entry: {'train/lr': 0.00029389156626506024, '_timestamp': 1741546078.1684134}).
Epoch: [1][ 56/500]	Time 29.357 (29.357)	Loss 0.1252 (0.1465)	CeLoss 0.0182 (0.0512)	SegCLSLoss 0.0048 (0.0031)	KLLoss 0.0242 (0.0302)	MaskLoss 0.1212 (0.1018)	MaskBCELoss 0.0294 (0.0285)	MaskDICELoss 0.0918 (0.0732)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 55 is less than current step: 499. Dropping entry: {'train/loss': 0.14651621431112288, 'train/ce_loss': 0.05120849609375, 'train/seg_cls_loss': 0.003063201904296875, 'train/kl_loss': 0.03017578125, 'train/mask_bce_loss': 0.028540567960590124, 'train/mask_dice_loss': 0.07322562141343951, 'train/mask_loss': 0.1017661850899458, 'metrics/total_secs_per_batch': 29.357117176055908, 'metrics/data_secs_per_batch': 12.847111225128174, '_timestamp': 1741546107.5251381}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 55 is less than current step: 499. Dropping entry: {'train/lr': 0.00029387951807228915, '_timestamp': 1741546107.5254014}).
Epoch: [1][ 57/500]	Time 27.914 (27.914)	Loss 0.2145 (0.1436)	CeLoss 0.0830 (0.0454)	SegCLSLoss 0.0046 (0.0030)	KLLoss 0.0317 (0.0289)	MaskLoss 0.1164 (0.1119)	MaskBCELoss 0.0472 (0.0271)	MaskDICELoss 0.0692 (0.0847)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 56 is less than current step: 499. Dropping entry: {'train/loss': 0.14364911690354348, 'train/ce_loss': 0.04542236328125, 'train/seg_cls_loss': 0.003021240234375, 'train/kl_loss': 0.0288818359375, 'train/mask_bce_loss': 0.027142268093302845, 'train/mask_dice_loss': 0.08474478982388974, 'train/mask_loss': 0.11188705787062644, 'metrics/total_secs_per_batch': 27.913812160491943, 'metrics/data_secs_per_batch': 12.788240098953247, '_timestamp': 1741546135.4389434}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 56 is less than current step: 499. Dropping entry: {'train/lr': 0.00029386746987951806, '_timestamp': 1741546135.4393172}).
Epoch: [1][ 58/500]	Time 23.497 (23.497)	Loss 0.0955 (0.1353)	CeLoss 0.0250 (0.0489)	SegCLSLoss 0.0023 (0.0036)	KLLoss 0.0208 (0.0267)	MaskLoss 0.1083 (0.0988)	MaskBCELoss 0.0101 (0.0235)	MaskDICELoss 0.0982 (0.0753)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 57 is less than current step: 499. Dropping entry: {'train/loss': 0.13525803238153458, 'train/ce_loss': 0.04888916015625, 'train/seg_cls_loss': 0.003591156005859375, 'train/kl_loss': 0.02674560546875, 'train/mask_bce_loss': 0.023475076258182525, 'train/mask_dice_loss': 0.07530715428292752, 'train/mask_loss': 0.09878223165869712, 'metrics/total_secs_per_batch': 23.497365951538086, 'metrics/data_secs_per_batch': 10.910625624656678, '_timestamp': 1741546158.936633}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 57 is less than current step: 499. Dropping entry: {'train/lr': 0.000293855421686747, '_timestamp': 1741546158.9369605}).
Epoch: [1][ 59/500]	Time 29.847 (29.847)	Loss 0.0857 (0.1212)	CeLoss 0.0127 (0.0460)	SegCLSLoss 0.0032 (0.0023)	KLLoss 0.0325 (0.0253)	MaskLoss 0.0815 (0.0897)	MaskBCELoss 0.0205 (0.0194)	MaskDICELoss 0.0610 (0.0703)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 58 is less than current step: 499. Dropping entry: {'train/loss': 0.12115773819386959, 'train/ce_loss': 0.0460205078125, 'train/seg_cls_loss': 0.00227813720703125, 'train/kl_loss': 0.0253173828125, 'train/mask_bce_loss': 0.019405432045459747, 'train/mask_dice_loss': 0.07031873688101768, 'train/mask_loss': 0.08972416892647743, 'metrics/total_secs_per_batch': 29.84683394432068, 'metrics/data_secs_per_batch': 13.001317143440247, '_timestamp': 1741546188.7834017}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 58 is less than current step: 499. Dropping entry: {'train/lr': 0.0002938433734939759, '_timestamp': 1741546188.7836926}).
Epoch: [1][ 60/500]	Time 27.640 (27.640)	Loss 0.0943 (0.1266)	CeLoss 0.0251 (0.0501)	SegCLSLoss 0.0021 (0.0029)	KLLoss 0.0420 (0.0257)	MaskLoss 0.1088 (0.0892)	MaskBCELoss 0.0091 (0.0203)	MaskDICELoss 0.0996 (0.0689)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 59 is less than current step: 499. Dropping entry: {'train/loss': 0.12658169753849507, 'train/ce_loss': 0.0501220703125, 'train/seg_cls_loss': 0.00286407470703125, 'train/kl_loss': 0.02568359375, 'train/mask_bce_loss': 0.020264190435409547, 'train/mask_dice_loss': 0.06894256919622421, 'train/mask_loss': 0.0892067577689886, 'metrics/total_secs_per_batch': 27.639899253845215, 'metrics/data_secs_per_batch': 12.860305166244506, '_timestamp': 1741546216.4230106}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 59 is less than current step: 499. Dropping entry: {'train/lr': 0.0002938192771084337, '_timestamp': 1741546216.4232373}).
Epoch: [1][ 61/500]	Time 26.160 (26.160)	Loss 0.0890 (0.1051)	CeLoss 0.0297 (0.0425)	SegCLSLoss 0.0020 (0.0019)	KLLoss 0.0354 (0.0292)	MaskLoss 0.1028 (0.0852)	MaskBCELoss 0.0046 (0.0127)	MaskDICELoss 0.0982 (0.0725)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 60 is less than current step: 499. Dropping entry: {'train/loss': 0.1051239762455225, 'train/ce_loss': 0.0424560546875, 'train/seg_cls_loss': 0.00191192626953125, 'train/kl_loss': 0.02919921875, 'train/mask_bce_loss': 0.01274923449382186, 'train/mask_dice_loss': 0.07249319776892663, 'train/mask_loss': 0.08524243161082268, 'metrics/total_secs_per_batch': 26.16048550605774, 'metrics/data_secs_per_batch': 11.659605598449707, '_timestamp': 1741546242.5836139}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 60 is less than current step: 499. Dropping entry: {'train/lr': 0.0002938072289156626, '_timestamp': 1741546242.5839844}).
Epoch: [1][ 62/500]	Time 28.797 (28.797)	Loss 0.0880 (0.1234)	CeLoss 0.0238 (0.0371)	SegCLSLoss 0.0035 (0.0039)	KLLoss 0.0320 (0.0290)	MaskLoss 0.1042 (0.1082)	MaskBCELoss 0.0069 (0.0202)	MaskDICELoss 0.0972 (0.0880)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 61 is less than current step: 499. Dropping entry: {'train/loss': 0.1234095685184002, 'train/ce_loss': 0.0370849609375, 'train/seg_cls_loss': 0.003905487060546875, 'train/kl_loss': 0.02899169921875, 'train/mask_bce_loss': 0.0202111955935834, 'train/mask_dice_loss': 0.0879665408283472, 'train/mask_loss': 0.10817773640155792, 'metrics/total_secs_per_batch': 28.79736614227295, 'metrics/data_secs_per_batch': 12.203307628631592, '_timestamp': 1741546271.380975}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 61 is less than current step: 499. Dropping entry: {'train/lr': 0.00029379518072289153, '_timestamp': 1741546271.38138}).
Epoch: [1][ 63/500]	Time 28.020 (28.020)	Loss 0.1585 (0.1252)	CeLoss 0.0264 (0.0475)	SegCLSLoss 0.0020 (0.0028)	KLLoss 0.0270 (0.0296)	MaskLoss 0.1337 (0.1102)	MaskBCELoss 0.0429 (0.0141)	MaskDICELoss 0.0908 (0.0961)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 62 is less than current step: 499. Dropping entry: {'train/loss': 0.1252244792878628, 'train/ce_loss': 0.0475341796875, 'train/seg_cls_loss': 0.00279388427734375, 'train/kl_loss': 0.0296142578125, 'train/mask_bce_loss': 0.014090918854344636, 'train/mask_dice_loss': 0.09611652940511703, 'train/mask_loss': 0.110207449644804, 'metrics/total_secs_per_batch': 28.01987051963806, 'metrics/data_secs_per_batch': 12.619326615333557, '_timestamp': 1741546299.4007971}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 62 is less than current step: 499. Dropping entry: {'train/lr': 0.00029378313253012044, '_timestamp': 1741546299.4011886}).
Epoch: [1][ 64/500]	Time 29.953 (29.953)	Loss 0.1793 (0.1679)	CeLoss 0.0796 (0.0539)	SegCLSLoss 0.0020 (0.0023)	KLLoss 0.0291 (0.0275)	MaskLoss 0.1146 (0.1122)	MaskBCELoss 0.0274 (0.0377)	MaskDICELoss 0.0871 (0.0745)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 63 is less than current step: 499. Dropping entry: {'train/loss': 0.1678679332137108, 'train/ce_loss': 0.0539306640625, 'train/seg_cls_loss': 0.002317047119140625, 'train/kl_loss': 0.02752685546875, 'train/mask_bce_loss': 0.03771214422304183, 'train/mask_dice_loss': 0.07453572861850262, 'train/mask_loss': 0.11224787086248397, 'metrics/total_secs_per_batch': 29.953177452087402, 'metrics/data_secs_per_batch': 13.076911807060242, '_timestamp': 1741546329.3542187}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 63 is less than current step: 499. Dropping entry: {'train/lr': 0.00029377108433734935, '_timestamp': 1741546329.3547454}).
Epoch: [1][ 65/500]	Time 25.780 (25.780)	Loss 0.1026 (0.1166)	CeLoss 0.0247 (0.0372)	SegCLSLoss 0.0047 (0.0021)	KLLoss 0.0286 (0.0240)	MaskLoss 0.1123 (0.0913)	MaskBCELoss 0.0130 (0.0218)	MaskDICELoss 0.0993 (0.0695)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 64 is less than current step: 499. Dropping entry: {'train/loss': 0.11662473492324352, 'train/ce_loss': 0.037158203125, 'train/seg_cls_loss': 0.00210723876953125, 'train/kl_loss': 0.0240234375, 'train/mask_bce_loss': 0.02184004511218518, 'train/mask_dice_loss': 0.06946351230144501, 'train/mask_loss': 0.09130355715751648, 'metrics/total_secs_per_batch': 25.78026580810547, 'metrics/data_secs_per_batch': 11.83311505317688, '_timestamp': 1741546355.1342435}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 64 is less than current step: 499. Dropping entry: {'train/lr': 0.00029375903614457826, '_timestamp': 1741546355.1345062}).
Epoch: [1][ 66/500]	Time 28.109 (28.109)	Loss 0.1588 (0.1402)	CeLoss 0.0654 (0.0464)	SegCLSLoss 0.0029 (0.0030)	KLLoss 0.0356 (0.0298)	MaskLoss 0.1113 (0.1136)	MaskBCELoss 0.0242 (0.0237)	MaskDICELoss 0.0871 (0.0899)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 65 is less than current step: 499. Dropping entry: {'train/loss': 0.14020258337259292, 'train/ce_loss': 0.0464111328125, 'train/seg_cls_loss': 0.00301055908203125, 'train/kl_loss': 0.0297607421875, 'train/mask_bce_loss': 0.023666989372577517, 'train/mask_dice_loss': 0.08991689011454582, 'train/mask_loss': 0.11358387917280197, 'metrics/total_secs_per_batch': 28.108928203582764, 'metrics/data_secs_per_batch': 12.461515879631042, '_timestamp': 1741546383.2431417}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 65 is less than current step: 499. Dropping entry: {'train/lr': 0.0002937469879518072, '_timestamp': 1741546383.2433863}).
Epoch: [1][ 67/500]	Time 30.365 (30.365)	Loss 0.0995 (0.1196)	CeLoss 0.0250 (0.0453)	SegCLSLoss 0.0028 (0.0023)	KLLoss 0.0327 (0.0304)	MaskLoss 0.1115 (0.0943)	MaskBCELoss 0.0116 (0.0174)	MaskDICELoss 0.0999 (0.0769)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 66 is less than current step: 499. Dropping entry: {'train/loss': 0.11964834555983543, 'train/ce_loss': 0.04532470703125, 'train/seg_cls_loss': 0.002265167236328125, 'train/kl_loss': 0.030419921875, 'train/mask_bce_loss': 0.01738199417013675, 'train/mask_dice_loss': 0.07687809020280838, 'train/mask_loss': 0.09426008462905884, 'metrics/total_secs_per_batch': 30.365333795547485, 'metrics/data_secs_per_batch': 13.450341033935548, '_timestamp': 1741546413.6085293}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 66 is less than current step: 499. Dropping entry: {'train/lr': 0.00029373493975903614, '_timestamp': 1741546413.6087954}).
Epoch: [1][ 68/500]	Time 27.676 (27.676)	Loss 0.1120 (0.1275)	CeLoss 0.0356 (0.0429)	SegCLSLoss 0.0011 (0.0026)	KLLoss 0.0322 (0.0275)	MaskLoss 0.1116 (0.0968)	MaskBCELoss 0.0132 (0.0232)	MaskDICELoss 0.0984 (0.0736)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 67 is less than current step: 499. Dropping entry: {'train/loss': 0.12748321071267127, 'train/ce_loss': 0.0429443359375, 'train/seg_cls_loss': 0.002562713623046875, 'train/kl_loss': 0.0275146484375, 'train/mask_bce_loss': 0.0232274325331673, 'train/mask_dice_loss': 0.07356059830635786, 'train/mask_loss': 0.09678803235292435, 'metrics/total_secs_per_batch': 27.676067352294922, 'metrics/data_secs_per_batch': 12.59293520450592, '_timestamp': 1741546441.2845814}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 67 is less than current step: 499. Dropping entry: {'train/lr': 0.00029372289156626505, '_timestamp': 1741546441.2848496}).
Epoch: [1][ 69/500]	Time 28.420 (28.420)	Loss 0.1054 (0.1456)	CeLoss 0.0464 (0.0539)	SegCLSLoss 0.0025 (0.0026)	KLLoss 0.0189 (0.0246)	MaskLoss 0.1035 (0.1076)	MaskBCELoss 0.0040 (0.0244)	MaskDICELoss 0.0995 (0.0832)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 68 is less than current step: 499. Dropping entry: {'train/loss': 0.14561297791078687, 'train/ce_loss': 0.053936767578125, 'train/seg_cls_loss': 0.002639007568359375, 'train/kl_loss': 0.02462158203125, 'train/mask_bce_loss': 0.02438685572706163, 'train/mask_dice_loss': 0.08324884660542012, 'train/mask_loss': 0.1076357014477253, 'metrics/total_secs_per_batch': 28.41951847076416, 'metrics/data_secs_per_batch': 12.025462985038757, '_timestamp': 1741546469.7041833}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 68 is less than current step: 499. Dropping entry: {'train/lr': 0.00029371084337349396, '_timestamp': 1741546469.7045794}).
Epoch: [1][ 70/500]	Time 29.808 (29.808)	Loss 0.0453 (0.1042)	CeLoss 0.0454 (0.0482)	SegCLSLoss 0.0000 (0.0026)	KLLoss 0.0000 (0.0197)	MaskLoss 0.0000 (0.0831)	MaskBCELoss 0.0000 (0.0088)	MaskDICELoss 0.0000 (0.0742)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 69 is less than current step: 499. Dropping entry: {'train/loss': 0.10422394629567862, 'train/ce_loss': 0.04815673828125, 'train/seg_cls_loss': 0.002581787109375, 'train/kl_loss': 0.01971435546875, 'train/mask_bce_loss': 0.008829087787307798, 'train/mask_dice_loss': 0.07424481734633445, 'train/mask_loss': 0.08307390585541725, 'metrics/total_secs_per_batch': 29.808152437210083, 'metrics/data_secs_per_batch': 14.320035719871521, '_timestamp': 1741546499.5127854}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 69 is less than current step: 499. Dropping entry: {'train/lr': 0.0002936867469879518, '_timestamp': 1741546499.5133872}).
Epoch: [1][ 71/500]	Time 29.172 (29.172)	Loss 0.1056 (0.1035)	CeLoss 0.0520 (0.0416)	SegCLSLoss 0.0027 (0.0022)	KLLoss 0.0267 (0.0256)	MaskLoss 0.1009 (0.0795)	MaskBCELoss 0.0014 (0.0141)	MaskDICELoss 0.0995 (0.0654)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 70 is less than current step: 499. Dropping entry: {'train/loss': 0.1034834049642086, 'train/ce_loss': 0.041552734375, 'train/seg_cls_loss': 0.00216522216796875, 'train/kl_loss': 0.0255615234375, 'train/mask_bce_loss': 0.01406614831648767, 'train/mask_dice_loss': 0.06544831059873105, 'train/mask_loss': 0.07951446101069451, 'metrics/total_secs_per_batch': 29.17242431640625, 'metrics/data_secs_per_batch': 13.450662112236023, '_timestamp': 1741546528.6852226}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 70 is less than current step: 499. Dropping entry: {'train/lr': 0.0002936746987951807, '_timestamp': 1741546528.6856782}).
Epoch: [1][ 72/500]	Time 28.481 (28.481)	Loss 0.0829 (0.1120)	CeLoss 0.0222 (0.0460)	SegCLSLoss 0.0057 (0.0031)	KLLoss 0.0256 (0.0287)	MaskLoss 0.1001 (0.1033)	MaskBCELoss 0.0053 (0.0085)	MaskDICELoss 0.0948 (0.0947)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 71 is less than current step: 499. Dropping entry: {'train/loss': 0.11198379397392273, 'train/ce_loss': 0.04599609375, 'train/seg_cls_loss': 0.00305023193359375, 'train/kl_loss': 0.02874755859375, 'train/mask_bce_loss': 0.008525235042907298, 'train/mask_dice_loss': 0.09472992718219757, 'train/mask_loss': 0.10325516238808632, 'metrics/total_secs_per_batch': 28.48056721687317, 'metrics/data_secs_per_batch': 13.062746119499206, '_timestamp': 1741546557.1652596}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 71 is less than current step: 499. Dropping entry: {'train/lr': 0.0002936626506024096, '_timestamp': 1741546557.165612}).
Epoch: [1][ 73/500]	Time 31.366 (31.366)	Loss 0.0964 (0.1237)	CeLoss 0.0378 (0.0494)	SegCLSLoss 0.0031 (0.0036)	KLLoss 0.0327 (0.0268)	MaskLoss 0.1009 (0.1004)	MaskBCELoss 0.0043 (0.0148)	MaskDICELoss 0.0966 (0.0856)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 72 is less than current step: 499. Dropping entry: {'train/loss': 0.12372714877128602, 'train/ce_loss': 0.04940185546875, 'train/seg_cls_loss': 0.00364837646484375, 'train/kl_loss': 0.0267822265625, 'train/mask_bce_loss': 0.014823963504750282, 'train/mask_dice_loss': 0.08558032251894473, 'train/mask_loss': 0.10040428526699544, 'metrics/total_secs_per_batch': 31.365607500076294, 'metrics/data_secs_per_batch': 14.363763570785522, '_timestamp': 1741546588.5309014}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 72 is less than current step: 499. Dropping entry: {'train/lr': 0.0002936506024096385, '_timestamp': 1741546588.5312917}).
Epoch: [1][ 74/500]	Time 28.764 (28.764)	Loss 0.2659 (0.1432)	CeLoss 0.0479 (0.0493)	SegCLSLoss 0.0043 (0.0030)	KLLoss 0.0403 (0.0302)	MaskLoss 0.1600 (0.1117)	MaskBCELoss 0.0906 (0.0244)	MaskDICELoss 0.0693 (0.0873)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 73 is less than current step: 499. Dropping entry: {'train/loss': 0.14319275468587875, 'train/ce_loss': 0.049267578125, 'train/seg_cls_loss': 0.0029510498046875, 'train/kl_loss': 0.03023681640625, 'train/mask_bce_loss': 0.024378389352932572, 'train/mask_dice_loss': 0.08730945345014333, 'train/mask_loss': 0.11168784387409687, 'metrics/total_secs_per_batch': 28.76370120048523, 'metrics/data_secs_per_batch': 12.571019387245178, '_timestamp': 1741546617.2947645}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 73 is less than current step: 499. Dropping entry: {'train/lr': 0.0002936385542168674, '_timestamp': 1741546617.2952938}).
Epoch: [1][ 75/500]	Time 28.684 (28.684)	Loss 0.2371 (0.1495)	CeLoss 0.0571 (0.0531)	SegCLSLoss 0.0034 (0.0032)	KLLoss 0.0258 (0.0265)	MaskLoss 0.1593 (0.1083)	MaskBCELoss 0.0659 (0.0271)	MaskDICELoss 0.0934 (0.0812)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 74 is less than current step: 499. Dropping entry: {'train/loss': 0.14945528656244278, 'train/ce_loss': 0.05308837890625, 'train/seg_cls_loss': 0.0032135009765625, 'train/kl_loss': 0.02652587890625, 'train/mask_bce_loss': 0.027076091227354483, 'train/mask_dice_loss': 0.08119213953614235, 'train/mask_loss': 0.10826823115348816, 'metrics/total_secs_per_batch': 28.683746337890625, 'metrics/data_secs_per_batch': 13.600277280807495, '_timestamp': 1741546645.978324}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 74 is less than current step: 499. Dropping entry: {'train/lr': 0.0002936265060240964, '_timestamp': 1741546645.978696}).
Epoch: [1][ 76/500]	Time 28.022 (28.022)	Loss 0.1277 (0.1544)	CeLoss 0.0398 (0.0580)	SegCLSLoss 0.0016 (0.0040)	KLLoss 0.0310 (0.0275)	MaskLoss 0.1151 (0.1181)	MaskBCELoss 0.0197 (0.0236)	MaskDICELoss 0.0955 (0.0945)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 75 is less than current step: 499. Dropping entry: {'train/loss': 0.15439006462693214, 'train/ce_loss': 0.0580322265625, 'train/seg_cls_loss': 0.00395660400390625, 'train/kl_loss': 0.027490234375, 'train/mask_bce_loss': 0.023606832325458526, 'train/mask_dice_loss': 0.09445045962929725, 'train/mask_loss': 0.11805729120969773, 'metrics/total_secs_per_batch': 28.02160406112671, 'metrics/data_secs_per_batch': 12.593767619132995, '_timestamp': 1741546674.0001485}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 75 is less than current step: 499. Dropping entry: {'train/lr': 0.0002936144578313253, '_timestamp': 1741546674.000602}).
Epoch: [1][ 77/500]	Time 27.999 (27.999)	Loss 0.1240 (0.1636)	CeLoss 0.0654 (0.0532)	SegCLSLoss 0.0028 (0.0026)	KLLoss 0.0251 (0.0275)	MaskLoss 0.1025 (0.1195)	MaskBCELoss 0.0040 (0.0330)	MaskDICELoss 0.0985 (0.0866)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 76 is less than current step: 499. Dropping entry: {'train/loss': 0.16363076567649842, 'train/ce_loss': 0.05318603515625, 'train/seg_cls_loss': 0.0026153564453125, 'train/kl_loss': 0.02745361328125, 'train/mask_bce_loss': 0.032959933509118855, 'train/mask_dice_loss': 0.08657413870096206, 'train/mask_loss': 0.11953407302498817, 'metrics/total_secs_per_batch': 27.998998165130615, 'metrics/data_secs_per_batch': 12.171508383750915, '_timestamp': 1741546701.9995337}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 76 is less than current step: 499. Dropping entry: {'train/lr': 0.0002936024096385542, '_timestamp': 1741546702.0001326}).
Epoch: [1][ 78/500]	Time 28.459 (28.459)	Loss 0.0996 (0.1207)	CeLoss 0.0187 (0.0464)	SegCLSLoss 0.0019 (0.0025)	KLLoss 0.0231 (0.0301)	MaskLoss 0.1066 (0.1038)	MaskBCELoss 0.0178 (0.0141)	MaskDICELoss 0.0888 (0.0897)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 77 is less than current step: 499. Dropping entry: {'train/loss': 0.12066043391823769, 'train/ce_loss': 0.046380615234375, 'train/seg_cls_loss': 0.00254974365234375, 'train/kl_loss': 0.03006591796875, 'train/mask_bce_loss': 0.014068586155190133, 'train/mask_dice_loss': 0.08969494886696339, 'train/mask_loss': 0.10376353561878204, 'metrics/total_secs_per_batch': 28.459198236465454, 'metrics/data_secs_per_batch': 12.473910236358643, '_timestamp': 1741546730.4581678}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 77 is less than current step: 499. Dropping entry: {'train/lr': 0.0002935903614457831, '_timestamp': 1741546730.4584289}).
Epoch: [1][ 79/500]	Time 31.017 (31.017)	Loss 0.1126 (0.1113)	CeLoss 0.0579 (0.0479)	SegCLSLoss 0.0019 (0.0019)	KLLoss 0.0201 (0.0218)	MaskLoss 0.1020 (0.0827)	MaskBCELoss 0.0020 (0.0141)	MaskDICELoss 0.1000 (0.0686)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 78 is less than current step: 499. Dropping entry: {'train/loss': 0.11133741848170757, 'train/ce_loss': 0.047900390625, 'train/seg_cls_loss': 0.001895904541015625, 'train/kl_loss': 0.02183837890625, 'train/mask_bce_loss': 0.014089094859082251, 'train/mask_dice_loss': 0.06856455504894257, 'train/mask_loss': 0.08265364915132523, 'metrics/total_secs_per_batch': 31.01684546470642, 'metrics/data_secs_per_batch': 14.10111095905304, '_timestamp': 1741546761.4754522}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 78 is less than current step: 499. Dropping entry: {'train/lr': 0.00029357831325301203, '_timestamp': 1741546761.4758866}).
Epoch: [1][ 80/500]	Time 28.821 (28.821)	Loss 0.2284 (0.2220)	CeLoss 0.0654 (0.0463)	SegCLSLoss 0.0019 (0.0033)	KLLoss 0.0359 (0.0312)	MaskLoss 0.1437 (0.1518)	MaskBCELoss 0.0601 (0.0655)	MaskDICELoss 0.0836 (0.0862)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 79 is less than current step: 499. Dropping entry: {'train/loss': 0.22203458100557327, 'train/ce_loss': 0.04627685546875, 'train/seg_cls_loss': 0.00329132080078125, 'train/kl_loss': 0.031201171875, 'train/mask_bce_loss': 0.06551021203631535, 'train/mask_dice_loss': 0.08624705784022808, 'train/mask_loss': 0.15175727009773254, 'metrics/total_secs_per_batch': 28.820897579193115, 'metrics/data_secs_per_batch': 12.432389187812806, '_timestamp': 1741546790.295912}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 79 is less than current step: 499. Dropping entry: {'train/lr': 0.00029355421686746985, '_timestamp': 1741546790.296143}).
Epoch: [1][ 81/500]	Time 26.619 (26.619)	Loss 0.1525 (0.1321)	CeLoss 0.0713 (0.0550)	SegCLSLoss 0.0041 (0.0028)	KLLoss 0.0205 (0.0306)	MaskLoss 0.1012 (0.0973)	MaskBCELoss 0.0190 (0.0181)	MaskDICELoss 0.0822 (0.0792)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 80 is less than current step: 499. Dropping entry: {'train/loss': 0.13212965577840804, 'train/ce_loss': 0.0550048828125, 'train/seg_cls_loss': 0.0028045654296875, 'train/kl_loss': 0.03056640625, 'train/mask_bce_loss': 0.01807451075874269, 'train/mask_dice_loss': 0.07920734956860542, 'train/mask_loss': 0.09728185907006263, 'metrics/total_secs_per_batch': 26.619457483291626, 'metrics/data_secs_per_batch': 11.894320082664489, '_timestamp': 1741546816.9155867}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 80 is less than current step: 499. Dropping entry: {'train/lr': 0.00029354216867469876, '_timestamp': 1741546816.9160016}).
Epoch: [1][ 82/500]	Time 26.125 (26.125)	Loss 0.0243 (0.1256)	CeLoss 0.0243 (0.0302)	SegCLSLoss 0.0000 (0.0025)	KLLoss 0.0000 (0.0277)	MaskLoss 0.0000 (0.1052)	MaskBCELoss 0.0000 (0.0277)	MaskDICELoss 0.0000 (0.0775)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 81 is less than current step: 499. Dropping entry: {'train/loss': 0.12562984507530928, 'train/ce_loss': 0.03023681640625, 'train/seg_cls_loss': 0.002527618408203125, 'train/kl_loss': 0.0276611328125, 'train/mask_bce_loss': 0.02770233009941876, 'train/mask_dice_loss': 0.0775206781923771, 'train/mask_loss': 0.1052230078727007, 'metrics/total_secs_per_batch': 26.124701738357544, 'metrics/data_secs_per_batch': 11.054283571243285, '_timestamp': 1741546843.0401082}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 81 is less than current step: 499. Dropping entry: {'train/lr': 0.0002935301204819277, '_timestamp': 1741546843.0404913}).
Epoch: [1][ 83/500]	Time 31.403 (31.403)	Loss 0.1831 (0.1288)	CeLoss 0.0571 (0.0548)	SegCLSLoss 0.0030 (0.0022)	KLLoss 0.0332 (0.0277)	MaskLoss 0.1223 (0.0961)	MaskBCELoss 0.0422 (0.0166)	MaskDICELoss 0.0801 (0.0795)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 82 is less than current step: 499. Dropping entry: {'train/loss': 0.1288014005869627, 'train/ce_loss': 0.0547607421875, 'train/seg_cls_loss': 0.002176666259765625, 'train/kl_loss': 0.027679443359375, 'train/mask_bce_loss': 0.016595709318062292, 'train/mask_dice_loss': 0.07954026833176613, 'train/mask_loss': 0.0961359791457653, 'metrics/total_secs_per_batch': 31.403198719024658, 'metrics/data_secs_per_batch': 13.708755016326904, '_timestamp': 1741546874.443351}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 82 is less than current step: 499. Dropping entry: {'train/lr': 0.0002935180722891566, '_timestamp': 1741546874.4437826}).
Epoch: [1][ 84/500]	Time 29.553 (29.553)	Loss 0.1266 (0.1262)	CeLoss 0.0645 (0.0542)	SegCLSLoss 0.0064 (0.0034)	KLLoss 0.0361 (0.0319)	MaskLoss 0.1036 (0.0970)	MaskBCELoss 0.0048 (0.0145)	MaskDICELoss 0.0987 (0.0825)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 83 is less than current step: 499. Dropping entry: {'train/loss': 0.12617744952440263, 'train/ce_loss': 0.0542236328125, 'train/seg_cls_loss': 0.003420257568359375, 'train/kl_loss': 0.03194580078125, 'train/mask_bce_loss': 0.014475782826775686, 'train/mask_dice_loss': 0.08247910663485528, 'train/mask_loss': 0.09695489145815372, 'metrics/total_secs_per_batch': 29.553430318832397, 'metrics/data_secs_per_batch': 13.286353039741517, '_timestamp': 1741546903.996816}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 83 is less than current step: 499. Dropping entry: {'train/lr': 0.0002935060240963855, '_timestamp': 1741546903.9972353}).
Epoch: [1][ 85/500]	Time 26.454 (26.454)	Loss 0.0905 (0.1222)	CeLoss 0.0189 (0.0523)	SegCLSLoss 0.0043 (0.0029)	KLLoss 0.0310 (0.0266)	MaskLoss 0.1077 (0.0959)	MaskBCELoss 0.0103 (0.0137)	MaskDICELoss 0.0973 (0.0822)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 84 is less than current step: 499. Dropping entry: {'train/loss': 0.12223244849592448, 'train/ce_loss': 0.05230712890625, 'train/seg_cls_loss': 0.0029144287109375, 'train/kl_loss': 0.0265869140625, 'train/mask_bce_loss': 0.013709957286482676, 'train/mask_dice_loss': 0.08218365460634232, 'train/mask_loss': 0.09589361101388931, 'metrics/total_secs_per_batch': 26.454225778579712, 'metrics/data_secs_per_batch': 12.177698469161987, '_timestamp': 1741546930.4509914}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 84 is less than current step: 499. Dropping entry: {'train/lr': 0.0002934939759036144, '_timestamp': 1741546930.4514084}).
Epoch: [1][ 86/500]	Time 29.746 (29.746)	Loss 0.3725 (0.1574)	CeLoss 0.0306 (0.0352)	SegCLSLoss 0.0029 (0.0034)	KLLoss 0.0396 (0.0295)	MaskLoss 0.2289 (0.1255)	MaskBCELoss 0.1506 (0.0385)	MaskDICELoss 0.0783 (0.0870)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 85 is less than current step: 499. Dropping entry: {'train/loss': 0.15743742510676384, 'train/ce_loss': 0.03515625, 'train/seg_cls_loss': 0.00335693359375, 'train/kl_loss': 0.0294677734375, 'train/mask_bce_loss': 0.038525249483063816, 'train/mask_dice_loss': 0.08700431287288665, 'train/mask_loss': 0.12552955970168114, 'metrics/total_secs_per_batch': 29.746487855911255, 'metrics/data_secs_per_batch': 13.884079241752625, '_timestamp': 1741546960.1974552}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 85 is less than current step: 499. Dropping entry: {'train/lr': 0.00029348192771084337, '_timestamp': 1741546960.1978378}).
Epoch: [1][ 87/500]	Time 28.255 (28.255)	Loss 0.1110 (0.1262)	CeLoss 0.0547 (0.0515)	SegCLSLoss 0.0065 (0.0031)	KLLoss 0.0398 (0.0320)	MaskLoss 0.0987 (0.0965)	MaskBCELoss 0.0025 (0.0166)	MaskDICELoss 0.0962 (0.0799)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 86 is less than current step: 499. Dropping entry: {'train/loss': 0.1261606127023697, 'train/ce_loss': 0.0515380859375, 'train/seg_cls_loss': 0.003076171875, 'train/kl_loss': 0.03203125, 'train/mask_bce_loss': 0.016588300262810662, 'train/mask_dice_loss': 0.07989380024373531, 'train/mask_loss': 0.09648210108280182, 'metrics/total_secs_per_batch': 28.25528621673584, 'metrics/data_secs_per_batch': 12.50317165851593, '_timestamp': 1741546988.452954}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 86 is less than current step: 499. Dropping entry: {'train/lr': 0.0002934698795180723, '_timestamp': 1741546988.4532747}).
Epoch: [1][ 88/500]	Time 29.646 (29.646)	Loss 0.0831 (0.1624)	CeLoss 0.0129 (0.0389)	SegCLSLoss 0.0048 (0.0031)	KLLoss 0.0376 (0.0305)	MaskLoss 0.1071 (0.1255)	MaskBCELoss 0.0095 (0.0394)	MaskDICELoss 0.0976 (0.0860)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 87 is less than current step: 499. Dropping entry: {'train/loss': 0.1624236822128296, 'train/ce_loss': 0.038946533203125, 'train/seg_cls_loss': 0.003128814697265625, 'train/kl_loss': 0.03050537109375, 'train/mask_bce_loss': 0.039433776121586564, 'train/mask_dice_loss': 0.08601850681006909, 'train/mask_loss': 0.1254522830247879, 'metrics/total_secs_per_batch': 29.646247625350952, 'metrics/data_secs_per_batch': 13.76792860031128, '_timestamp': 1741547018.0989819}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 87 is less than current step: 499. Dropping entry: {'train/lr': 0.0002934578313253012, '_timestamp': 1741547018.0993586}).
Epoch: [1][ 89/500]	Time 27.035 (27.035)	Loss 0.1572 (0.1173)	CeLoss 0.0791 (0.0486)	SegCLSLoss 0.0030 (0.0023)	KLLoss 0.0309 (0.0258)	MaskLoss 0.1041 (0.0793)	MaskBCELoss 0.0164 (0.0187)	MaskDICELoss 0.0876 (0.0606)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 88 is less than current step: 499. Dropping entry: {'train/loss': 0.1173417555168271, 'train/ce_loss': 0.0485595703125, 'train/seg_cls_loss': 0.002324676513671875, 'train/kl_loss': 0.025830078125, 'train/mask_bce_loss': 0.01866456982679665, 'train/mask_dice_loss': 0.06059163808822632, 'train/mask_loss': 0.07925620675086975, 'metrics/total_secs_per_batch': 27.03470468521118, 'metrics/data_secs_per_batch': 12.126804232597351, '_timestamp': 1741547045.1336627}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 88 is less than current step: 499. Dropping entry: {'train/lr': 0.0002934457831325301, '_timestamp': 1741547045.1339493}).
Epoch: [1][ 90/500]	Time 27.677 (27.677)	Loss 0.1017 (0.1425)	CeLoss 0.0254 (0.0486)	SegCLSLoss 0.0048 (0.0034)	KLLoss 0.0300 (0.0286)	MaskLoss 0.1108 (0.1070)	MaskBCELoss 0.0124 (0.0258)	MaskDICELoss 0.0984 (0.0812)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 89 is less than current step: 499. Dropping entry: {'train/loss': 0.14252049326896668, 'train/ce_loss': 0.04857177734375, 'train/seg_cls_loss': 0.003430938720703125, 'train/kl_loss': 0.02857666015625, 'train/mask_bce_loss': 0.025818488700315355, 'train/mask_dice_loss': 0.08118109814822674, 'train/mask_loss': 0.10699958950281144, 'metrics/total_secs_per_batch': 27.67688536643982, 'metrics/data_secs_per_batch': 13.216667795181275, '_timestamp': 1741547072.8116915}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 89 is less than current step: 499. Dropping entry: {'train/lr': 0.0002934216867469879, '_timestamp': 1741547072.8123405}).
Epoch: [1][ 91/500]	Time 30.202 (30.202)	Loss 0.1565 (0.1181)	CeLoss 0.0752 (0.0432)	SegCLSLoss 0.0020 (0.0025)	KLLoss 0.0410 (0.0307)	MaskLoss 0.0614 (0.0924)	MaskBCELoss 0.0329 (0.0183)	MaskDICELoss 0.0285 (0.0741)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 90 is less than current step: 499. Dropping entry: {'train/loss': 0.11810704721137881, 'train/ce_loss': 0.043157958984375, 'train/seg_cls_loss': 0.002475738525390625, 'train/kl_loss': 0.0306884765625, 'train/mask_bce_loss': 0.018321517715230586, 'train/mask_dice_loss': 0.07411943059414625, 'train/mask_loss': 0.09244094975292683, 'metrics/total_secs_per_batch': 30.201762437820435, 'metrics/data_secs_per_batch': 13.986658906936645, '_timestamp': 1741547103.0126054}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 90 is less than current step: 499. Dropping entry: {'train/lr': 0.00029340963855421684, '_timestamp': 1741547103.0129035}).
Epoch: [1][ 92/500]	Time 28.432 (28.432)	Loss 0.0424 (0.1135)	CeLoss 0.0425 (0.0484)	SegCLSLoss 0.0000 (0.0027)	KLLoss 0.0000 (0.0283)	MaskLoss 0.0000 (0.0894)	MaskBCELoss 0.0000 (0.0127)	MaskDICELoss 0.0000 (0.0767)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 91 is less than current step: 499. Dropping entry: {'train/loss': 0.11352806612849235, 'train/ce_loss': 0.048394775390625, 'train/seg_cls_loss': 0.002689361572265625, 'train/kl_loss': 0.02828369140625, 'train/mask_bce_loss': 0.012743013503495604, 'train/mask_dice_loss': 0.07665048372000456, 'train/mask_loss': 0.08939349688589573, 'metrics/total_secs_per_batch': 28.432034015655518, 'metrics/data_secs_per_batch': 12.900612998008729, '_timestamp': 1741547131.4450128}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 91 is less than current step: 499. Dropping entry: {'train/lr': 0.00029339759036144575, '_timestamp': 1741547131.4455945}).
Epoch: [1][ 93/500]	Time 26.805 (26.805)	Loss 0.0652 (0.1097)	CeLoss 0.0236 (0.0284)	SegCLSLoss 0.0023 (0.0027)	KLLoss 0.0299 (0.0294)	MaskLoss 0.0605 (0.0977)	MaskBCELoss 0.0068 (0.0207)	MaskDICELoss 0.0537 (0.0769)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 92 is less than current step: 499. Dropping entry: {'train/loss': 0.10972845293581486, 'train/ce_loss': 0.0284423828125, 'train/seg_cls_loss': 0.00266571044921875, 'train/kl_loss': 0.02943115234375, 'train/mask_bce_loss': 0.02074479798320681, 'train/mask_dice_loss': 0.07694646269083023, 'train/mask_loss': 0.0976912621408701, 'metrics/total_secs_per_batch': 26.805216312408447, 'metrics/data_secs_per_batch': 12.20583713054657, '_timestamp': 1741547158.2497797}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 92 is less than current step: 499. Dropping entry: {'train/lr': 0.00029338554216867466, '_timestamp': 1741547158.2503455}).
Epoch: [1][ 94/500]	Time 29.276 (29.276)	Loss 0.0352 (0.0957)	CeLoss 0.0159 (0.0334)	SegCLSLoss 0.0015 (0.0024)	KLLoss 0.0378 (0.0263)	MaskLoss 0.0235 (0.0799)	MaskBCELoss 0.0046 (0.0141)	MaskDICELoss 0.0189 (0.0658)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 93 is less than current step: 499. Dropping entry: {'train/loss': 0.09567834287881852, 'train/ce_loss': 0.03338623046875, 'train/seg_cls_loss': 0.002355194091796875, 'train/kl_loss': 0.02628173828125, 'train/mask_bce_loss': 0.014096489921212197, 'train/mask_dice_loss': 0.0658496307209134, 'train/mask_loss': 0.07994612138718367, 'metrics/total_secs_per_batch': 29.27565622329712, 'metrics/data_secs_per_batch': 13.569502925872802, '_timestamp': 1741547187.5253017}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 93 is less than current step: 499. Dropping entry: {'train/lr': 0.00029337349397590357, '_timestamp': 1741547187.525679}).
Epoch: [1][ 95/500]	Time 28.048 (28.048)	Loss 0.1186 (0.1376)	CeLoss 0.0488 (0.0402)	SegCLSLoss 0.0045 (0.0035)	KLLoss 0.0315 (0.0285)	MaskLoss 0.1078 (0.1085)	MaskBCELoss 0.0090 (0.0276)	MaskDICELoss 0.0988 (0.0808)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 94 is less than current step: 499. Dropping entry: {'train/loss': 0.13759613558650016, 'train/ce_loss': 0.04017333984375, 'train/seg_cls_loss': 0.003522491455078125, 'train/kl_loss': 0.02845458984375, 'train/mask_bce_loss': 0.02764391414821148, 'train/mask_dice_loss': 0.08082754872739314, 'train/mask_loss': 0.10847146175801754, 'metrics/total_secs_per_batch': 28.048085689544678, 'metrics/data_secs_per_batch': 12.250749683380127, '_timestamp': 1741547215.573406}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 94 is less than current step: 499. Dropping entry: {'train/lr': 0.00029336144578313253, '_timestamp': 1741547215.573782}).
Epoch: [1][ 96/500]	Time 27.195 (27.195)	Loss 0.0792 (0.1265)	CeLoss 0.0229 (0.0495)	SegCLSLoss 0.0023 (0.0025)	KLLoss 0.0315 (0.0279)	MaskLoss 0.1017 (0.0969)	MaskBCELoss 0.0028 (0.0182)	MaskDICELoss 0.0988 (0.0787)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 95 is less than current step: 499. Dropping entry: {'train/loss': 0.12646974250674248, 'train/ce_loss': 0.049462890625, 'train/seg_cls_loss': 0.002454376220703125, 'train/kl_loss': 0.0278564453125, 'train/mask_bce_loss': 0.018213482550345363, 'train/mask_dice_loss': 0.07865977324545384, 'train/mask_loss': 0.09687325358390808, 'metrics/total_secs_per_batch': 27.19502902030945, 'metrics/data_secs_per_batch': 12.496683692932129, '_timestamp': 1741547242.7684417}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 95 is less than current step: 499. Dropping entry: {'train/lr': 0.00029334939759036144, '_timestamp': 1741547242.7688344}).
Epoch: [1][ 97/500]	Time 29.057 (29.057)	Loss 0.1798 (0.1588)	CeLoss 0.0444 (0.0522)	SegCLSLoss 0.0018 (0.0040)	KLLoss 0.0267 (0.0300)	MaskLoss 0.1327 (0.1184)	MaskBCELoss 0.0453 (0.0302)	MaskDICELoss 0.0874 (0.0881)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 96 is less than current step: 499. Dropping entry: {'train/loss': 0.1587524928152561, 'train/ce_loss': 0.05224609375, 'train/seg_cls_loss': 0.003966522216796875, 'train/kl_loss': 0.02999267578125, 'train/mask_bce_loss': 0.03022454269230366, 'train/mask_dice_loss': 0.08813025951385497, 'train/mask_loss': 0.11835480257868766, 'metrics/total_secs_per_batch': 29.057469367980957, 'metrics/data_secs_per_batch': 12.417148566246032, '_timestamp': 1741547271.825964}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 96 is less than current step: 499. Dropping entry: {'train/lr': 0.00029333734939759036, '_timestamp': 1741547271.826375}).
Epoch: [1][ 98/500]	Time 25.064 (25.064)	Loss 0.1075 (0.1070)	CeLoss 0.0481 (0.0399)	SegCLSLoss 0.0021 (0.0023)	KLLoss 0.0251 (0.0294)	MaskLoss 0.1041 (0.0911)	MaskBCELoss 0.0043 (0.0137)	MaskDICELoss 0.0998 (0.0774)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 97 is less than current step: 499. Dropping entry: {'train/loss': 0.1070407759398222, 'train/ce_loss': 0.03988037109375, 'train/seg_cls_loss': 0.00229644775390625, 'train/kl_loss': 0.0294189453125, 'train/mask_bce_loss': 0.013684661220759153, 'train/mask_dice_loss': 0.0773702485486865, 'train/mask_loss': 0.09105490893125534, 'metrics/total_secs_per_batch': 25.064011096954346, 'metrics/data_secs_per_batch': 11.281421303749084, '_timestamp': 1741547296.8898745}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 97 is less than current step: 499. Dropping entry: {'train/lr': 0.00029332530120481927, '_timestamp': 1741547296.8901238}).
Epoch: [1][ 99/500]	Time 27.294 (27.294)	Loss 0.0954 (0.0984)	CeLoss 0.0256 (0.0286)	SegCLSLoss 0.0015 (0.0023)	KLLoss 0.0293 (0.0283)	MaskLoss 0.1088 (0.0933)	MaskBCELoss 0.0097 (0.0146)	MaskDICELoss 0.0991 (0.0787)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 98 is less than current step: 499. Dropping entry: {'train/loss': 0.09835757873952389, 'train/ce_loss': 0.02860107421875, 'train/seg_cls_loss': 0.00229339599609375, 'train/kl_loss': 0.0282958984375, 'train/mask_bce_loss': 0.014617631561122835, 'train/mask_dice_loss': 0.07871337532997132, 'train/mask_loss': 0.09333100765943528, 'metrics/total_secs_per_batch': 27.29401421546936, 'metrics/data_secs_per_batch': 12.357536458969117, '_timestamp': 1741547324.1839538}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 98 is less than current step: 499. Dropping entry: {'train/lr': 0.0002933132530120482, '_timestamp': 1741547324.1843376}).
[2025-03-09 14:09:12,711] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[0.0002933012048192771], mom=[(0.9, 0.95)]
[2025-03-09 14:09:12,717] [INFO] [timer.py:215:stop] epoch=0/micro_step=600/global_step=60, RunningAvgSamplesPerSec=1.4210922308112302, CurrSamplesPerSec=1.4188094484033842, MemAllocated=58.91GB, MaxMemAllocated=73.04GB
Epoch: [1][100/500]	Time 28.534 (28.534)	Loss 0.2263 (0.1263)	CeLoss 0.0908 (0.0495)	SegCLSLoss 0.0037 (0.0029)	KLLoss 0.0315 (0.0231)	MaskLoss 0.1296 (0.0884)	MaskBCELoss 0.0459 (0.0207)	MaskDICELoss 0.0836 (0.0677)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 99 is less than current step: 499. Dropping entry: {'train/loss': 0.12625483684241773, 'train/ce_loss': 0.04951171875, 'train/seg_cls_loss': 0.002901458740234375, 'train/kl_loss': 0.02308349609375, 'train/mask_bce_loss': 0.02074509267695248, 'train/mask_dice_loss': 0.06767382845282555, 'train/mask_loss': 0.08841891959309578, 'metrics/total_secs_per_batch': 28.534367561340332, 'metrics/data_secs_per_batch': 13.0000789642334, '_timestamp': 1741547352.718017}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 99 is less than current step: 499. Dropping entry: {'train/lr': 0.000293289156626506, '_timestamp': 1741547352.7183657}).
Epoch: [1][101/500]	Time 26.788 (26.788)	Loss 0.0806 (0.1117)	CeLoss 0.0222 (0.0374)	SegCLSLoss 0.0020 (0.0023)	KLLoss 0.0317 (0.0280)	MaskLoss 0.1032 (0.0858)	MaskBCELoss 0.0038 (0.0201)	MaskDICELoss 0.0994 (0.0657)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 100 is less than current step: 499. Dropping entry: {'train/loss': 0.11165757998824119, 'train/ce_loss': 0.03741455078125, 'train/seg_cls_loss': 0.00230712890625, 'train/kl_loss': 0.027978515625, 'train/mask_bce_loss': 0.020108074136078357, 'train/mask_dice_loss': 0.06573442360386253, 'train/mask_loss': 0.08584249801933766, 'metrics/total_secs_per_batch': 26.787935495376587, 'metrics/data_secs_per_batch': 11.468419528007507, '_timestamp': 1741547379.5063167}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 100 is less than current step: 499. Dropping entry: {'train/lr': 0.0002932771084337349, '_timestamp': 1741547379.5067184}).
Epoch: [1][102/500]	Time 27.003 (27.003)	Loss 0.0873 (0.1393)	CeLoss 0.0630 (0.0427)	SegCLSLoss 0.0022 (0.0025)	KLLoss 0.0289 (0.0312)	MaskLoss 0.0289 (0.1073)	MaskBCELoss 0.0059 (0.0278)	MaskDICELoss 0.0230 (0.0796)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 101 is less than current step: 499. Dropping entry: {'train/loss': 0.13928057998418808, 'train/ce_loss': 0.04271240234375, 'train/seg_cls_loss': 0.0024566650390625, 'train/kl_loss': 0.03118896484375, 'train/mask_bce_loss': 0.02776356730610132, 'train/mask_dice_loss': 0.07957719881087541, 'train/mask_loss': 0.10734076611697674, 'metrics/total_secs_per_batch': 27.002826929092407, 'metrics/data_secs_per_batch': 12.139582991600037, '_timestamp': 1741547406.5091047}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 101 is less than current step: 499. Dropping entry: {'train/lr': 0.0002932650602409638, '_timestamp': 1741547406.5095108}).
Epoch: [1][103/500]	Time 28.776 (28.776)	Loss 0.0803 (0.0894)	CeLoss 0.0217 (0.0364)	SegCLSLoss 0.0018 (0.0026)	KLLoss 0.0449 (0.0238)	MaskLoss 0.1023 (0.0765)	MaskBCELoss 0.0043 (0.0089)	MaskDICELoss 0.0980 (0.0675)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 102 is less than current step: 499. Dropping entry: {'train/loss': 0.08937332034111023, 'train/ce_loss': 0.03638916015625, 'train/seg_cls_loss': 0.00263671875, 'train/kl_loss': 0.02384033203125, 'train/mask_bce_loss': 0.008934108726680278, 'train/mask_dice_loss': 0.06752192229032516, 'train/mask_loss': 0.0764560304582119, 'metrics/total_secs_per_batch': 28.776151180267334, 'metrics/data_secs_per_batch': 12.625952434539794, '_timestamp': 1741547435.2852142}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 102 is less than current step: 499. Dropping entry: {'train/lr': 0.00029325301204819273, '_timestamp': 1741547435.2854607}).
Epoch: [1][104/500]	Time 29.587 (29.587)	Loss 0.0933 (0.1003)	CeLoss 0.0221 (0.0412)	SegCLSLoss 0.0017 (0.0019)	KLLoss 0.0444 (0.0319)	MaskLoss 0.0999 (0.0765)	MaskBCELoss 0.0136 (0.0133)	MaskDICELoss 0.0864 (0.0632)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 103 is less than current step: 499. Dropping entry: {'train/loss': 0.10031620636582375, 'train/ce_loss': 0.04124755859375, 'train/seg_cls_loss': 0.00185699462890625, 'train/kl_loss': 0.03189697265625, 'train/mask_bce_loss': 0.013272067694924772, 'train/mask_dice_loss': 0.06320820469409227, 'train/mask_loss': 0.0764802735298872, 'metrics/total_secs_per_batch': 29.586666345596313, 'metrics/data_secs_per_batch': 13.344016671180725, '_timestamp': 1741547464.87189}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 103 is less than current step: 499. Dropping entry: {'train/lr': 0.00029324096385542164, '_timestamp': 1741547464.8721528}).
Epoch: [1][105/500]	Time 24.071 (24.071)	Loss 0.1113 (0.1072)	CeLoss 0.0194 (0.0481)	SegCLSLoss 0.0056 (0.0034)	KLLoss 0.0266 (0.0206)	MaskLoss 0.1182 (0.0857)	MaskBCELoss 0.0199 (0.0097)	MaskDICELoss 0.0983 (0.0761)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 104 is less than current step: 499. Dropping entry: {'train/loss': 0.10716335885226727, 'train/ce_loss': 0.0480712890625, 'train/seg_cls_loss': 0.00337982177734375, 'train/kl_loss': 0.02061767578125, 'train/mask_bce_loss': 0.009651320916600526, 'train/mask_dice_loss': 0.076092529296875, 'train/mask_loss': 0.08574384897947311, 'metrics/total_secs_per_batch': 24.071484565734863, 'metrics/data_secs_per_batch': 10.338541722297668, '_timestamp': 1741547488.943539}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 104 is less than current step: 499. Dropping entry: {'train/lr': 0.00029322891566265055, '_timestamp': 1741547488.9440725}).
Epoch: [1][106/500]	Time 29.294 (29.294)	Loss 0.1029 (0.1374)	CeLoss 0.0366 (0.0482)	SegCLSLoss 0.0044 (0.0025)	KLLoss 0.0247 (0.0297)	MaskLoss 0.1062 (0.1004)	MaskBCELoss 0.0074 (0.0252)	MaskDICELoss 0.0989 (0.0753)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 105 is less than current step: 499. Dropping entry: {'train/loss': 0.13736485540866852, 'train/ce_loss': 0.04820556640625, 'train/seg_cls_loss': 0.002471160888671875, 'train/kl_loss': 0.02965087890625, 'train/mask_bce_loss': 0.025158338318578898, 'train/mask_dice_loss': 0.075268230214715, 'train/mask_loss': 0.1004265658557415, 'metrics/total_secs_per_batch': 29.294383764266968, 'metrics/data_secs_per_batch': 12.572893571853637, '_timestamp': 1741547518.237772}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 105 is less than current step: 499. Dropping entry: {'train/lr': 0.0002932168674698795, '_timestamp': 1741547518.2381518}).
Epoch: [1][107/500]	Time 27.532 (27.532)	Loss 0.0830 (0.1242)	CeLoss 0.0223 (0.0451)	SegCLSLoss 0.0034 (0.0030)	KLLoss 0.0247 (0.0245)	MaskLoss 0.1040 (0.0903)	MaskBCELoss 0.0046 (0.0216)	MaskDICELoss 0.0995 (0.0687)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 106 is less than current step: 499. Dropping entry: {'train/loss': 0.12417400665581227, 'train/ce_loss': 0.0450927734375, 'train/seg_cls_loss': 0.00298004150390625, 'train/kl_loss': 0.02449951171875, 'train/mask_bce_loss': 0.021613254654221238, 'train/mask_dice_loss': 0.0687114030122757, 'train/mask_loss': 0.0903246570378542, 'metrics/total_secs_per_batch': 27.531869411468506, 'metrics/data_secs_per_batch': 12.52857415676117, '_timestamp': 1741547545.769666}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 106 is less than current step: 499. Dropping entry: {'train/lr': 0.00029320481927710843, '_timestamp': 1741547545.7700634}).
Epoch: [1][108/500]	Time 27.913 (27.913)	Loss 0.1867 (0.1288)	CeLoss 0.0581 (0.0414)	SegCLSLoss 0.0013 (0.0029)	KLLoss 0.0293 (0.0263)	MaskLoss 0.1303 (0.1047)	MaskBCELoss 0.0417 (0.0224)	MaskDICELoss 0.0887 (0.0824)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 107 is less than current step: 499. Dropping entry: {'train/loss': 0.12876664102077484, 'train/ce_loss': 0.041357421875, 'train/seg_cls_loss': 0.00290374755859375, 'train/kl_loss': 0.02628173828125, 'train/mask_bce_loss': 0.022384170768782498, 'train/mask_dice_loss': 0.08235206454992294, 'train/mask_loss': 0.10473623275756835, 'metrics/total_secs_per_batch': 27.913495779037476, 'metrics/data_secs_per_batch': 12.978212666511535, '_timestamp': 1741547573.6832669}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 107 is less than current step: 499. Dropping entry: {'train/lr': 0.00029319277108433734, '_timestamp': 1741547573.6838193}).
Epoch: [1][109/500]	Time 29.214 (29.214)	Loss 0.1443 (0.1388)	CeLoss 0.0248 (0.0367)	SegCLSLoss 0.0016 (0.0033)	KLLoss 0.0400 (0.0301)	MaskLoss 0.1245 (0.1121)	MaskBCELoss 0.0376 (0.0296)	MaskDICELoss 0.0869 (0.0825)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 108 is less than current step: 499. Dropping entry: {'train/loss': 0.13877745047211648, 'train/ce_loss': 0.036737060546875, 'train/seg_cls_loss': 0.003340911865234375, 'train/kl_loss': 0.0300537109375, 'train/mask_bce_loss': 0.029579067789018153, 'train/mask_dice_loss': 0.08250034525990486, 'train/mask_loss': 0.11207941174507141, 'metrics/total_secs_per_batch': 29.213691473007202, 'metrics/data_secs_per_batch': 13.640872192382812, '_timestamp': 1741547602.8968394}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 108 is less than current step: 499. Dropping entry: {'train/lr': 0.00029318072289156625, '_timestamp': 1741547602.8972208}).
Epoch: [1][110/500]	Time 29.182 (29.182)	Loss 0.1069 (0.1176)	CeLoss 0.0222 (0.0409)	SegCLSLoss 0.0037 (0.0029)	KLLoss 0.0371 (0.0334)	MaskLoss 0.1152 (0.0888)	MaskBCELoss 0.0168 (0.0206)	MaskDICELoss 0.0985 (0.0682)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 109 is less than current step: 499. Dropping entry: {'train/loss': 0.11764647392556071, 'train/ce_loss': 0.040875244140625, 'train/seg_cls_loss': 0.002921295166015625, 'train/kl_loss': 0.03341064453125, 'train/mask_bce_loss': 0.02062708935700357, 'train/mask_dice_loss': 0.068175214715302, 'train/mask_loss': 0.08880230262875558, 'metrics/total_secs_per_batch': 29.182384252548218, 'metrics/data_secs_per_batch': 12.722193837165833, '_timestamp': 1741547632.0792692}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 109 is less than current step: 499. Dropping entry: {'train/lr': 0.00029315662650602407, '_timestamp': 1741547632.079644}).
Epoch: [1][111/500]	Time 28.033 (28.033)	Loss 0.1262 (0.1348)	CeLoss 0.0645 (0.0480)	SegCLSLoss 0.0012 (0.0034)	KLLoss 0.0232 (0.0294)	MaskLoss 0.1037 (0.0991)	MaskBCELoss 0.0061 (0.0237)	MaskDICELoss 0.0977 (0.0754)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 110 is less than current step: 499. Dropping entry: {'train/loss': 0.13476700484752654, 'train/ce_loss': 0.048046875, 'train/seg_cls_loss': 0.0033843994140625, 'train/kl_loss': 0.02935791015625, 'train/mask_bce_loss': 0.02367397919879295, 'train/mask_dice_loss': 0.07538496553897858, 'train/mask_loss': 0.09905894473195076, 'metrics/total_secs_per_batch': 28.0332133769989, 'metrics/data_secs_per_batch': 12.202513813972473, '_timestamp': 1741547660.1124969}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 110 is less than current step: 499. Dropping entry: {'train/lr': 0.000293144578313253, '_timestamp': 1741547660.112764}).
Epoch: [1][112/500]	Time 28.618 (28.618)	Loss 0.1310 (0.1117)	CeLoss 0.0598 (0.0434)	SegCLSLoss 0.0041 (0.0035)	KLLoss 0.0325 (0.0319)	MaskLoss 0.1092 (0.0921)	MaskBCELoss 0.0098 (0.0136)	MaskDICELoss 0.0994 (0.0784)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 111 is less than current step: 499. Dropping entry: {'train/loss': 0.1116584800183773, 'train/ce_loss': 0.04339599609375, 'train/seg_cls_loss': 0.00349884033203125, 'train/kl_loss': 0.031884765625, 'train/mask_bce_loss': 0.013635993108619005, 'train/mask_dice_loss': 0.07842142470180988, 'train/mask_loss': 0.09205741845071316, 'metrics/total_secs_per_batch': 28.618139028549194, 'metrics/data_secs_per_batch': 12.337842440605163, '_timestamp': 1741547688.7308917}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 111 is less than current step: 499. Dropping entry: {'train/lr': 0.0002931325301204819, '_timestamp': 1741547688.7312531}).
Epoch: [1][113/500]	Time 28.354 (28.354)	Loss 0.1244 (0.0963)	CeLoss 0.0233 (0.0328)	SegCLSLoss 0.0028 (0.0020)	KLLoss 0.0339 (0.0299)	MaskLoss 0.0979 (0.0933)	MaskBCELoss 0.0338 (0.0106)	MaskDICELoss 0.0641 (0.0827)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 112 is less than current step: 499. Dropping entry: {'train/loss': 0.09627503994852304, 'train/ce_loss': 0.03280029296875, 'train/seg_cls_loss': 0.002020263671875, 'train/kl_loss': 0.0298583984375, 'train/mask_bce_loss': 0.010553838894702494, 'train/mask_dice_loss': 0.08269800767302513, 'train/mask_loss': 0.09325184747576713, 'metrics/total_secs_per_batch': 28.354458808898926, 'metrics/data_secs_per_batch': 13.11125955581665, '_timestamp': 1741547717.0852892}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 112 is less than current step: 499. Dropping entry: {'train/lr': 0.0002931204819277108, '_timestamp': 1741547717.0856807}).
Epoch: [1][114/500]	Time 31.178 (31.178)	Loss 0.1214 (0.1151)	CeLoss 0.0240 (0.0320)	SegCLSLoss 0.0038 (0.0035)	KLLoss 0.0337 (0.0308)	MaskLoss 0.1091 (0.1029)	MaskBCELoss 0.0272 (0.0199)	MaskDICELoss 0.0819 (0.0830)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 113 is less than current step: 499. Dropping entry: {'train/loss': 0.11506523713469505, 'train/ce_loss': 0.0320068359375, 'train/seg_cls_loss': 0.00348663330078125, 'train/kl_loss': 0.03082275390625, 'train/mask_bce_loss': 0.019916972774080933, 'train/mask_dice_loss': 0.08302117800340056, 'train/mask_loss': 0.1029381511732936, 'metrics/total_secs_per_batch': 31.177717208862305, 'metrics/data_secs_per_batch': 12.966124081611634, '_timestamp': 1741547748.2627993}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 113 is less than current step: 499. Dropping entry: {'train/lr': 0.0002931084337349397, '_timestamp': 1741547748.2630692}).
Epoch: [1][115/500]	Time 25.461 (25.461)	Loss 0.0828 (0.1454)	CeLoss 0.0232 (0.0582)	SegCLSLoss 0.0021 (0.0042)	KLLoss 0.0270 (0.0266)	MaskLoss 0.0999 (0.1083)	MaskBCELoss 0.0057 (0.0207)	MaskDICELoss 0.0942 (0.0876)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 114 is less than current step: 499. Dropping entry: {'train/loss': 0.14541031792759895, 'train/ce_loss': 0.05816650390625, 'train/seg_cls_loss': 0.00416717529296875, 'train/kl_loss': 0.026611328125, 'train/mask_bce_loss': 0.02067369261640124, 'train/mask_dice_loss': 0.08757899031043052, 'train/mask_loss': 0.10825268253684044, 'metrics/total_secs_per_batch': 25.460670709609985, 'metrics/data_secs_per_batch': 11.866013622283935, '_timestamp': 1741547773.7234776}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 114 is less than current step: 499. Dropping entry: {'train/lr': 0.0002930963855421686, '_timestamp': 1741547773.7238455}).
Epoch: [1][116/500]	Time 28.086 (28.086)	Loss 0.1967 (0.1425)	CeLoss 0.0762 (0.0615)	SegCLSLoss 0.0014 (0.0029)	KLLoss 0.0420 (0.0263)	MaskLoss 0.1232 (0.1035)	MaskBCELoss 0.0388 (0.0186)	MaskDICELoss 0.0844 (0.0849)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 115 is less than current step: 499. Dropping entry: {'train/loss': 0.14252125322818757, 'train/ce_loss': 0.0614990234375, 'train/seg_cls_loss': 0.002884674072265625, 'train/kl_loss': 0.02628173828125, 'train/mask_bce_loss': 0.01857397594721988, 'train/mask_dice_loss': 0.0849458135664463, 'train/mask_loss': 0.10351978838443757, 'metrics/total_secs_per_batch': 28.086330890655518, 'metrics/data_secs_per_batch': 12.384806060791016, '_timestamp': 1741547801.8098772}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 115 is less than current step: 499. Dropping entry: {'train/lr': 0.0002930843373493976, '_timestamp': 1741547801.810308}).
Epoch: [1][117/500]	Time 27.034 (27.034)	Loss 0.0787 (0.1027)	CeLoss 0.0250 (0.0419)	SegCLSLoss 0.0027 (0.0025)	KLLoss 0.0297 (0.0302)	MaskLoss 0.0451 (0.0843)	MaskBCELoss 0.0198 (0.0116)	MaskDICELoss 0.0253 (0.0727)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 116 is less than current step: 499. Dropping entry: {'train/loss': 0.10267031565308571, 'train/ce_loss': 0.0419189453125, 'train/seg_cls_loss': 0.002458953857421875, 'train/kl_loss': 0.03021240234375, 'train/mask_bce_loss': 0.011648507078643889, 'train/mask_dice_loss': 0.07265285346657038, 'train/mask_loss': 0.08430136181414127, 'metrics/total_secs_per_batch': 27.03449273109436, 'metrics/data_secs_per_batch': 13.722596406936646, '_timestamp': 1741547828.8443668}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 116 is less than current step: 499. Dropping entry: {'train/lr': 0.0002930722891566265, '_timestamp': 1741547828.844767}).
Epoch: [1][118/500]	Time 26.380 (26.380)	Loss 0.1325 (0.1315)	CeLoss 0.0747 (0.0548)	SegCLSLoss 0.0025 (0.0035)	KLLoss 0.0277 (0.0265)	MaskLoss 0.1029 (0.0989)	MaskBCELoss 0.0035 (0.0170)	MaskDICELoss 0.0994 (0.0819)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 117 is less than current step: 499. Dropping entry: {'train/loss': 0.13151563741266728, 'train/ce_loss': 0.05482177734375, 'train/seg_cls_loss': 0.0034576416015625, 'train/kl_loss': 0.02650146484375, 'train/mask_bce_loss': 0.017028638953343035, 'train/mask_dice_loss': 0.08185031414031982, 'train/mask_loss': 0.09887895137071609, 'metrics/total_secs_per_batch': 26.379640102386475, 'metrics/data_secs_per_batch': 11.521037364006043, '_timestamp': 1741547855.2239661}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 117 is less than current step: 499. Dropping entry: {'train/lr': 0.0002930602409638554, '_timestamp': 1741547855.2242322}).
Epoch: [1][119/500]	Time 27.903 (27.903)	Loss 0.2642 (0.1103)	CeLoss 0.0732 (0.0345)	SegCLSLoss 0.0011 (0.0019)	KLLoss 0.0356 (0.0293)	MaskLoss 0.1494 (0.0920)	MaskBCELoss 0.0771 (0.0193)	MaskDICELoss 0.0723 (0.0728)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 118 is less than current step: 499. Dropping entry: {'train/loss': 0.11033752672374249, 'train/ce_loss': 0.034527587890625, 'train/seg_cls_loss': 0.001888275146484375, 'train/kl_loss': 0.02928466796875, 'train/mask_bce_loss': 0.019254742923658343, 'train/mask_dice_loss': 0.07277717478573323, 'train/mask_loss': 0.09203191921114921, 'metrics/total_secs_per_batch': 27.902684450149536, 'metrics/data_secs_per_batch': 12.791968131065369, '_timestamp': 1741547883.1266332}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 118 is less than current step: 499. Dropping entry: {'train/lr': 0.0002930481927710843, '_timestamp': 1741547883.1268883}).
Epoch: [1][120/500]	Time 28.505 (28.505)	Loss 0.0167 (0.1137)	CeLoss 0.0167 (0.0448)	SegCLSLoss 0.0000 (0.0017)	KLLoss 0.0000 (0.0282)	MaskLoss 0.0000 (0.0808)	MaskBCELoss 0.0000 (0.0185)	MaskDICELoss 0.0000 (0.0624)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 119 is less than current step: 499. Dropping entry: {'train/loss': 0.11369532849639655, 'train/ce_loss': 0.04476318359375, 'train/seg_cls_loss': 0.0017261505126953125, 'train/kl_loss': 0.02823486328125, 'train/mask_bce_loss': 0.018457097286591305, 'train/mask_dice_loss': 0.06237085573375225, 'train/mask_loss': 0.08082795348018408, 'metrics/total_secs_per_batch': 28.505391359329224, 'metrics/data_secs_per_batch': 12.613256573677063, '_timestamp': 1741547911.6324995}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 119 is less than current step: 499. Dropping entry: {'train/lr': 0.00029302409638554214, '_timestamp': 1741547911.6328251}).
Epoch: [1][121/500]	Time 25.858 (25.858)	Loss 0.0835 (0.1228)	CeLoss 0.0220 (0.0396)	SegCLSLoss 0.0015 (0.0025)	KLLoss 0.0286 (0.0340)	MaskLoss 0.1051 (0.1128)	MaskBCELoss 0.0055 (0.0170)	MaskDICELoss 0.0996 (0.0958)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 120 is less than current step: 499. Dropping entry: {'train/loss': 0.1227691039443016, 'train/ce_loss': 0.0396240234375, 'train/seg_cls_loss': 0.0024871826171875, 'train/kl_loss': 0.034033203125, 'train/mask_bce_loss': 0.01700484518369194, 'train/mask_dice_loss': 0.09576101750135421, 'train/mask_loss': 0.11276586353778839, 'metrics/total_secs_per_batch': 25.858002185821533, 'metrics/data_secs_per_batch': 11.490075039863587, '_timestamp': 1741547937.4900503}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 120 is less than current step: 499. Dropping entry: {'train/lr': 0.00029301204819277106, '_timestamp': 1741547937.4904175}).
Epoch: [1][122/500]	Time 29.580 (29.580)	Loss 0.1839 (0.1958)	CeLoss 0.1045 (0.0638)	SegCLSLoss 0.0024 (0.0022)	KLLoss 0.0396 (0.0330)	MaskLoss 0.0992 (0.1285)	MaskBCELoss 0.0187 (0.0444)	MaskDICELoss 0.0805 (0.0841)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 121 is less than current step: 499. Dropping entry: {'train/loss': 0.19581945836544037, 'train/ce_loss': 0.06378173828125, 'train/seg_cls_loss': 0.002164459228515625, 'train/kl_loss': 0.032958984375, 'train/mask_bce_loss': 0.044418297958327455, 'train/mask_dice_loss': 0.08412197269499302, 'train/mask_loss': 0.12854027301073073, 'metrics/total_secs_per_batch': 29.580130100250244, 'metrics/data_secs_per_batch': 13.822951364517213, '_timestamp': 1741547967.070171}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 121 is less than current step: 499. Dropping entry: {'train/lr': 0.00029299999999999997, '_timestamp': 1741547967.0705564}).
Epoch: [1][123/500]	Time 28.388 (28.388)	Loss 0.1129 (0.1370)	CeLoss 0.0579 (0.0539)	SegCLSLoss 0.0018 (0.0022)	KLLoss 0.0361 (0.0328)	MaskLoss 0.1022 (0.1096)	MaskBCELoss 0.0022 (0.0181)	MaskDICELoss 0.1000 (0.0914)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 122 is less than current step: 499. Dropping entry: {'train/loss': 0.13700393661856652, 'train/ce_loss': 0.05394287109375, 'train/seg_cls_loss': 0.002199554443359375, 'train/kl_loss': 0.032763671875, 'train/mask_bce_loss': 0.018124671427358408, 'train/mask_dice_loss': 0.09143106490373612, 'train/mask_loss': 0.10955573692917824, 'metrics/total_secs_per_batch': 28.387878894805908, 'metrics/data_secs_per_batch': 12.416705131530762, '_timestamp': 1741547995.458212}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 122 is less than current step: 499. Dropping entry: {'train/lr': 0.0002929879518072289, '_timestamp': 1741547995.4586453}).
Epoch: [1][124/500]	Time 29.282 (29.282)	Loss 0.0798 (0.0994)	CeLoss 0.0266 (0.0406)	SegCLSLoss 0.0020 (0.0028)	KLLoss 0.0488 (0.0307)	MaskLoss 0.1008 (0.0924)	MaskBCELoss 0.0012 (0.0075)	MaskDICELoss 0.0996 (0.0849)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 123 is less than current step: 499. Dropping entry: {'train/loss': 0.09939587451517581, 'train/ce_loss': 0.04056396484375, 'train/seg_cls_loss': 0.00278778076171875, 'train/kl_loss': 0.0306640625, 'train/mask_bce_loss': 0.007521880225976929, 'train/mask_dice_loss': 0.08489563539624215, 'train/mask_loss': 0.09241751804947854, 'metrics/total_secs_per_batch': 29.2818865776062, 'metrics/data_secs_per_batch': 14.049729418754577, '_timestamp': 1741548024.7400138}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 123 is less than current step: 499. Dropping entry: {'train/lr': 0.0002929759036144578, '_timestamp': 1741548024.740459}).
Epoch: [1][125/500]	Time 30.098 (30.098)	Loss 0.0696 (0.1381)	CeLoss 0.0208 (0.0390)	SegCLSLoss 0.0016 (0.0022)	KLLoss 0.0339 (0.0328)	MaskLoss 0.0518 (0.1124)	MaskBCELoss 0.0148 (0.0279)	MaskDICELoss 0.0369 (0.0846)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 124 is less than current step: 499. Dropping entry: {'train/loss': 0.1381305806338787, 'train/ce_loss': 0.0389892578125, 'train/seg_cls_loss': 0.00218658447265625, 'train/kl_loss': 0.03277587890625, 'train/mask_bce_loss': 0.027887242403812705, 'train/mask_dice_loss': 0.08455594070255756, 'train/mask_loss': 0.11244318149983883, 'metrics/total_secs_per_batch': 30.09812617301941, 'metrics/data_secs_per_batch': 14.511497116088867, '_timestamp': 1741548054.8380802}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 124 is less than current step: 499. Dropping entry: {'train/lr': 0.0002929638554216867, '_timestamp': 1741548054.8384497}).
Epoch: [1][126/500]	Time 28.521 (28.521)	Loss 0.1376 (0.1188)	CeLoss 0.0728 (0.0417)	SegCLSLoss 0.0015 (0.0029)	KLLoss 0.0317 (0.0324)	MaskLoss 0.1033 (0.1107)	MaskBCELoss 0.0083 (0.0136)	MaskDICELoss 0.0950 (0.0971)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 125 is less than current step: 499. Dropping entry: {'train/loss': 0.11882073953747749, 'train/ce_loss': 0.04168701171875, 'train/seg_cls_loss': 0.00287628173828125, 'train/kl_loss': 0.03236083984375, 'train/mask_bce_loss': 0.013591792597435415, 'train/mask_dice_loss': 0.09706336632370949, 'train/mask_loss': 0.11065515875816345, 'metrics/total_secs_per_batch': 28.520529747009277, 'metrics/data_secs_per_batch': 12.57563111782074, '_timestamp': 1741548083.3587375}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 125 is less than current step: 499. Dropping entry: {'train/lr': 0.0002929518072289156, '_timestamp': 1741548083.3591561}).
Epoch: [1][127/500]	Time 28.030 (28.030)	Loss 0.1385 (0.1625)	CeLoss 0.0674 (0.0584)	SegCLSLoss 0.0015 (0.0023)	KLLoss 0.0332 (0.0280)	MaskLoss 0.1089 (0.1101)	MaskBCELoss 0.0107 (0.0319)	MaskDICELoss 0.0982 (0.0783)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 126 is less than current step: 499. Dropping entry: {'train/loss': 0.1624977935105562, 'train/ce_loss': 0.0584228515625, 'train/seg_cls_loss': 0.002265167236328125, 'train/kl_loss': 0.02796630859375, 'train/mask_bce_loss': 0.03187575798947364, 'train/mask_dice_loss': 0.07825426440685987, 'train/mask_loss': 0.11013002246618271, 'metrics/total_secs_per_batch': 28.030407428741455, 'metrics/data_secs_per_batch': 12.37529296875, '_timestamp': 1741548111.3890293}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 126 is less than current step: 499. Dropping entry: {'train/lr': 0.0002929397590361446, '_timestamp': 1741548111.3893983}).
Epoch: [1][128/500]	Time 25.884 (25.884)	Loss 0.1166 (0.1205)	CeLoss 0.0464 (0.0381)	SegCLSLoss 0.0026 (0.0023)	KLLoss 0.0347 (0.0313)	MaskLoss 0.1026 (0.1115)	MaskBCELoss 0.0118 (0.0169)	MaskDICELoss 0.0908 (0.0946)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 127 is less than current step: 499. Dropping entry: {'train/loss': 0.12048195376992225, 'train/ce_loss': 0.038134765625, 'train/seg_cls_loss': 0.002298736572265625, 'train/kl_loss': 0.03131103515625, 'train/mask_bce_loss': 0.016918737406376748, 'train/mask_dice_loss': 0.09459755197167397, 'train/mask_loss': 0.11151629015803337, 'metrics/total_secs_per_batch': 25.883515119552612, 'metrics/data_secs_per_batch': 11.1862224817276, '_timestamp': 1741548137.2725358}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 127 is less than current step: 499. Dropping entry: {'train/lr': 0.0002929277108433735, '_timestamp': 1741548137.2727952}).
Epoch: [1][129/500]	Time 26.385 (26.385)	Loss 0.1618 (0.1089)	CeLoss 0.0493 (0.0334)	SegCLSLoss 0.0019 (0.0027)	KLLoss 0.0327 (0.0308)	MaskLoss 0.0920 (0.1010)	MaskBCELoss 0.0436 (0.0157)	MaskDICELoss 0.0484 (0.0853)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 128 is less than current step: 499. Dropping entry: {'train/loss': 0.10894143730401992, 'train/ce_loss': 0.033447265625, 'train/seg_cls_loss': 0.00274505615234375, 'train/kl_loss': 0.0307861328125, 'train/mask_bce_loss': 0.015723164661903866, 'train/mask_dice_loss': 0.08532224446535111, 'train/mask_loss': 0.10104540809988975, 'metrics/total_secs_per_batch': 26.385416269302368, 'metrics/data_secs_per_batch': 11.554629158973693, '_timestamp': 1741548163.6585398}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 128 is less than current step: 499. Dropping entry: {'train/lr': 0.0002929156626506024, '_timestamp': 1741548163.659133}).
Epoch: [1][130/500]	Time 27.957 (27.957)	Loss 0.0480 (0.1165)	CeLoss 0.0481 (0.0513)	SegCLSLoss 0.0000 (0.0022)	KLLoss 0.0000 (0.0307)	MaskLoss 0.0000 (0.0910)	MaskBCELoss 0.0000 (0.0124)	MaskDICELoss 0.0000 (0.0786)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 129 is less than current step: 499. Dropping entry: {'train/loss': 0.11654753722250462, 'train/ce_loss': 0.051318359375, 'train/seg_cls_loss': 0.002188873291015625, 'train/kl_loss': 0.03074951171875, 'train/mask_bce_loss': 0.0124038040463347, 'train/mask_dice_loss': 0.07861658185720444, 'train/mask_loss': 0.09102038592100144, 'metrics/total_secs_per_batch': 27.957467079162598, 'metrics/data_secs_per_batch': 12.46156096458435, '_timestamp': 1741548191.6154678}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 129 is less than current step: 499. Dropping entry: {'train/lr': 0.0002928915662650602, '_timestamp': 1741548191.6159086}).
Epoch: [1][131/500]	Time 27.906 (27.906)	Loss 0.1280 (0.1118)	CeLoss 0.0752 (0.0383)	SegCLSLoss 0.0033 (0.0024)	KLLoss 0.0320 (0.0259)	MaskLoss 0.1005 (0.0897)	MaskBCELoss 0.0006 (0.0183)	MaskDICELoss 0.1000 (0.0714)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 130 is less than current step: 499. Dropping entry: {'train/loss': 0.11178680676966905, 'train/ce_loss': 0.0383056640625, 'train/seg_cls_loss': 0.00235748291015625, 'train/kl_loss': 0.02586669921875, 'train/mask_bce_loss': 0.018281868170015515, 'train/mask_dice_loss': 0.07144223600625992, 'train/mask_loss': 0.0897241037338972, 'metrics/total_secs_per_batch': 27.905858278274536, 'metrics/data_secs_per_batch': 12.738947582244872, '_timestamp': 1741548219.5213919}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 130 is less than current step: 499. Dropping entry: {'train/lr': 0.00029287951807228913, '_timestamp': 1741548219.5218048}).
Epoch: [1][132/500]	Time 27.483 (27.483)	Loss 0.0924 (0.1781)	CeLoss 0.0168 (0.0631)	SegCLSLoss 0.0025 (0.0027)	KLLoss 0.0312 (0.0328)	MaskLoss 0.0663 (0.1196)	MaskBCELoss 0.0274 (0.0360)	MaskDICELoss 0.0389 (0.0836)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 131 is less than current step: 499. Dropping entry: {'train/loss': 0.1781486839056015, 'train/ce_loss': 0.0630859375, 'train/seg_cls_loss': 0.002733612060546875, 'train/kl_loss': 0.03284912109375, 'train/mask_bce_loss': 0.035972362238680944, 'train/mask_dice_loss': 0.08357979170978069, 'train/mask_loss': 0.1195521518588066, 'metrics/total_secs_per_batch': 27.4825496673584, 'metrics/data_secs_per_batch': 10.974292278289795, '_timestamp': 1741548247.0042014}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 131 is less than current step: 499. Dropping entry: {'train/lr': 0.00029286746987951804, '_timestamp': 1741548247.0047107}).
Epoch: [1][133/500]	Time 30.873 (30.873)	Loss 0.0829 (0.1370)	CeLoss 0.0276 (0.0507)	SegCLSLoss 0.0041 (0.0028)	KLLoss 0.0322 (0.0338)	MaskLoss 0.0730 (0.0872)	MaskBCELoss 0.0113 (0.0276)	MaskDICELoss 0.0617 (0.0596)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 132 is less than current step: 499. Dropping entry: {'train/loss': 0.137032300978899, 'train/ce_loss': 0.0506591796875, 'train/seg_cls_loss': 0.00276641845703125, 'train/kl_loss': 0.03380126953125, 'train/mask_bce_loss': 0.027575369126861916, 'train/mask_dice_loss': 0.05964202508330345, 'train/mask_loss': 0.0872173935174942, 'metrics/total_secs_per_batch': 30.873392820358276, 'metrics/data_secs_per_batch': 14.513091540336609, '_timestamp': 1741548277.877284}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 132 is less than current step: 499. Dropping entry: {'train/lr': 0.00029285542168674695, '_timestamp': 1741548277.8776627}).
Epoch: [1][134/500]	Time 27.251 (27.251)	Loss 0.1069 (0.1139)	CeLoss 0.0830 (0.0404)	SegCLSLoss 0.0030 (0.0026)	KLLoss 0.0405 (0.0291)	MaskLoss 0.0192 (0.0830)	MaskBCELoss 0.0086 (0.0205)	MaskDICELoss 0.0106 (0.0624)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 133 is less than current step: 499. Dropping entry: {'train/loss': 0.11393432607874274, 'train/ce_loss': 0.040423583984375, 'train/seg_cls_loss': 0.00256805419921875, 'train/kl_loss': 0.0291259765625, 'train/mask_bce_loss': 0.020543811563402414, 'train/mask_dice_loss': 0.06243168311193585, 'train/mask_loss': 0.08297549467533827, 'metrics/total_secs_per_batch': 27.251464128494263, 'metrics/data_secs_per_batch': 12.276271414756774, '_timestamp': 1741548305.1287751}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 133 is less than current step: 499. Dropping entry: {'train/lr': 0.00029284337349397586, '_timestamp': 1741548305.1290371}).
Epoch: [1][135/500]	Time 28.723 (28.723)	Loss 0.1241 (0.1454)	CeLoss 0.0728 (0.0554)	SegCLSLoss 0.0025 (0.0030)	KLLoss 0.0305 (0.0294)	MaskLoss 0.0997 (0.1097)	MaskBCELoss 0.0003 (0.0225)	MaskDICELoss 0.0994 (0.0872)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 134 is less than current step: 499. Dropping entry: {'train/loss': 0.14537502825260162, 'train/ce_loss': 0.05537109375, 'train/seg_cls_loss': 0.0029510498046875, 'train/kl_loss': 0.029443359375, 'train/mask_bce_loss': 0.022457110261893832, 'train/mask_dice_loss': 0.08721067756414413, 'train/mask_loss': 0.10966778621077537, 'metrics/total_secs_per_batch': 28.72277307510376, 'metrics/data_secs_per_batch': 13.258027172088623, '_timestamp': 1741548333.8515546}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 134 is less than current step: 499. Dropping entry: {'train/lr': 0.00029283132530120477, '_timestamp': 1741548333.85181}).
Epoch: [1][136/500]	Time 30.984 (30.984)	Loss 0.1980 (0.1188)	CeLoss 0.0559 (0.0340)	SegCLSLoss 0.0019 (0.0026)	KLLoss 0.0247 (0.0308)	MaskLoss 0.1289 (0.1070)	MaskBCELoss 0.0512 (0.0201)	MaskDICELoss 0.0777 (0.0869)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 135 is less than current step: 499. Dropping entry: {'train/loss': 0.11881187558174133, 'train/ce_loss': 0.0339599609375, 'train/seg_cls_loss': 0.00262451171875, 'train/kl_loss': 0.0307861328125, 'train/mask_bce_loss': 0.020054533070651814, 'train/mask_dice_loss': 0.0869173377752304, 'train/mask_loss': 0.10697187185287475, 'metrics/total_secs_per_batch': 30.984476327896118, 'metrics/data_secs_per_batch': 14.680360865592956, '_timestamp': 1741548364.83606}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 135 is less than current step: 499. Dropping entry: {'train/lr': 0.00029281927710843374, '_timestamp': 1741548364.8363338}).
Epoch: [1][137/500]	Time 26.591 (26.591)	Loss 0.0882 (0.1304)	CeLoss 0.0254 (0.0498)	SegCLSLoss 0.0049 (0.0038)	KLLoss 0.0277 (0.0319)	MaskLoss 0.1050 (0.1024)	MaskBCELoss 0.0053 (0.0183)	MaskDICELoss 0.0997 (0.0841)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 136 is less than current step: 499. Dropping entry: {'train/loss': 0.13044537454843522, 'train/ce_loss': 0.0498046875, 'train/seg_cls_loss': 0.00375518798828125, 'train/kl_loss': 0.031884765625, 'train/mask_bce_loss': 0.018334372839308343, 'train/mask_dice_loss': 0.08407669849693775, 'train/mask_loss': 0.10241107121109963, 'metrics/total_secs_per_batch': 26.591313123703003, 'metrics/data_secs_per_batch': 11.724456572532654, '_timestamp': 1741548391.427419}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 136 is less than current step: 499. Dropping entry: {'train/lr': 0.00029280722891566265, '_timestamp': 1741548391.4278204}).
Epoch: [1][138/500]	Time 25.463 (25.463)	Loss 0.0801 (0.1130)	CeLoss 0.0203 (0.0408)	SegCLSLoss 0.0020 (0.0038)	KLLoss 0.0289 (0.0319)	MaskLoss 0.1043 (0.1015)	MaskBCELoss 0.0044 (0.0130)	MaskDICELoss 0.0999 (0.0884)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 137 is less than current step: 499. Dropping entry: {'train/loss': 0.11296806633472442, 'train/ce_loss': 0.0407958984375, 'train/seg_cls_loss': 0.0038421630859375, 'train/kl_loss': 0.03194580078125, 'train/mask_bce_loss': 0.013009717699605971, 'train/mask_dice_loss': 0.0884480407461524, 'train/mask_loss': 0.10145775973796844, 'metrics/total_secs_per_batch': 25.463260412216187, 'metrics/data_secs_per_batch': 11.404928398132324, '_timestamp': 1741548416.8905888}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 137 is less than current step: 499. Dropping entry: {'train/lr': 0.00029279518072289156, '_timestamp': 1741548416.890982}).
Epoch: [1][139/500]	Time 27.468 (27.468)	Loss 0.1544 (0.1053)	CeLoss 0.0476 (0.0345)	SegCLSLoss 0.0028 (0.0024)	KLLoss 0.0293 (0.0263)	MaskLoss 0.1228 (0.0930)	MaskBCELoss 0.0293 (0.0154)	MaskDICELoss 0.0935 (0.0776)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 138 is less than current step: 499. Dropping entry: {'train/loss': 0.10526466500014067, 'train/ce_loss': 0.0344970703125, 'train/seg_cls_loss': 0.002425384521484375, 'train/kl_loss': 0.0262939453125, 'train/mask_bce_loss': 0.015369405632372945, 'train/mask_dice_loss': 0.07764545269310474, 'train/mask_loss': 0.09301485940814018, 'metrics/total_secs_per_batch': 27.468174934387207, 'metrics/data_secs_per_batch': 13.038139510154725, '_timestamp': 1741548444.358875}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 138 is less than current step: 499. Dropping entry: {'train/lr': 0.00029278313253012047, '_timestamp': 1741548444.359274}).
Epoch: [1][140/500]	Time 30.448 (30.448)	Loss 0.1559 (0.1177)	CeLoss 0.0659 (0.0459)	SegCLSLoss 0.0021 (0.0021)	KLLoss 0.0378 (0.0316)	MaskLoss 0.1053 (0.0860)	MaskBCELoss 0.0240 (0.0185)	MaskDICELoss 0.0813 (0.0674)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 139 is less than current step: 499. Dropping entry: {'train/loss': 0.11771736163645982, 'train/ce_loss': 0.045941162109375, 'train/seg_cls_loss': 0.0020538330078125, 'train/kl_loss': 0.0316162109375, 'train/mask_bce_loss': 0.01853146282956004, 'train/mask_dice_loss': 0.06744167730212211, 'train/mask_loss': 0.0859731413424015, 'metrics/total_secs_per_batch': 30.44784903526306, 'metrics/data_secs_per_batch': 14.112826776504516, '_timestamp': 1741548474.806632}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 139 is less than current step: 499. Dropping entry: {'train/lr': 0.0002927590361445783, '_timestamp': 1741548474.8069768}).
Epoch: [1][141/500]	Time 28.337 (28.337)	Loss 0.1895 (0.1200)	CeLoss 0.0703 (0.0424)	SegCLSLoss 0.0025 (0.0027)	KLLoss 0.0232 (0.0320)	MaskLoss 0.1144 (0.1002)	MaskBCELoss 0.0406 (0.0174)	MaskDICELoss 0.0738 (0.0828)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 140 is less than current step: 499. Dropping entry: {'train/loss': 0.11998172029852867, 'train/ce_loss': 0.0424072265625, 'train/seg_cls_loss': 0.0027435302734375, 'train/kl_loss': 0.03199462890625, 'train/mask_bce_loss': 0.017399476328864693, 'train/mask_dice_loss': 0.08284600079059601, 'train/mask_loss': 0.10024547837674617, 'metrics/total_secs_per_batch': 28.337175846099854, 'metrics/data_secs_per_batch': 13.664129137992859, '_timestamp': 1741548503.1438253}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 140 is less than current step: 499. Dropping entry: {'train/lr': 0.0002927469879518072, '_timestamp': 1741548503.1442015}).
Epoch: [1][142/500]	Time 25.944 (25.944)	Loss 0.1275 (0.1352)	CeLoss 0.0674 (0.0614)	SegCLSLoss 0.0046 (0.0027)	KLLoss 0.0398 (0.0287)	MaskLoss 0.1031 (0.0937)	MaskBCELoss 0.0040 (0.0170)	MaskDICELoss 0.0990 (0.0767)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 141 is less than current step: 499. Dropping entry: {'train/loss': 0.13516198806464672, 'train/ce_loss': 0.06143798828125, 'train/seg_cls_loss': 0.002732086181640625, 'train/kl_loss': 0.0286865234375, 'train/mask_bce_loss': 0.016988436703104525, 'train/mask_dice_loss': 0.07666710652410984, 'train/mask_loss': 0.09365554377436638, 'metrics/total_secs_per_batch': 25.944130897521973, 'metrics/data_secs_per_batch': 11.767927122116088, '_timestamp': 1741548529.0883944}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 141 is less than current step: 499. Dropping entry: {'train/lr': 0.0002927349397590361, '_timestamp': 1741548529.0887492}).
Epoch: [1][143/500]	Time 28.687 (28.687)	Loss 0.2937 (0.1307)	CeLoss 0.0586 (0.0499)	SegCLSLoss 0.0012 (0.0024)	KLLoss 0.0271 (0.0288)	MaskLoss 0.1776 (0.0944)	MaskBCELoss 0.0970 (0.0215)	MaskDICELoss 0.0806 (0.0729)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 142 is less than current step: 499. Dropping entry: {'train/loss': 0.13068451844155787, 'train/ce_loss': 0.049853515625, 'train/seg_cls_loss': 0.00242767333984375, 'train/kl_loss': 0.02884521484375, 'train/mask_bce_loss': 0.021534121572040023, 'train/mask_dice_loss': 0.07290832996368408, 'train/mask_loss': 0.09444245211780071, 'metrics/total_secs_per_batch': 28.687323331832886, 'metrics/data_secs_per_batch': 12.744480085372924, '_timestamp': 1741548557.7753167}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 142 is less than current step: 499. Dropping entry: {'train/lr': 0.000292722891566265, '_timestamp': 1741548557.7756124}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 143 is less than current step: 499. Dropping entry: {'train/loss': 0.10797244496643543, 'train/ce_loss': 0.04300537109375, 'train/seg_cls_loss': 0.00331268310546875, 'train/kl_loss': 0.02864990234375, 'train/mask_bce_loss': 0.01282682818127796, 'train/mask_dice_loss': 0.07541883084923029, 'train/mask_loss': 0.08824565894901752, 'metrics/total_secs_per_batch': 27.979117155075073, 'metrics/data_secs_per_batch': 12.871633410453796, '_timestamp': 1741548585.7544868}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 143 is less than current step: 499. Dropping entry: {'train/lr': 0.00029271084337349393, '_timestamp': 1741548585.7548923}).
Epoch: [1][144/500]	Time 27.979 (27.979)	Loss 0.1150 (0.1080)	CeLoss 0.0574 (0.0430)	SegCLSLoss 0.0018 (0.0033)	KLLoss 0.0237 (0.0286)	MaskLoss 0.1014 (0.0882)	MaskBCELoss 0.0041 (0.0128)	MaskDICELoss 0.0974 (0.0754)
Epoch: [1][145/500]	Time 27.517 (27.517)	Loss 0.1454 (0.1128)	CeLoss 0.0664 (0.0365)	SegCLSLoss 0.0033 (0.0027)	KLLoss 0.0310 (0.0302)	MaskLoss 0.1093 (0.0944)	MaskBCELoss 0.0152 (0.0185)	MaskDICELoss 0.0941 (0.0759)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 144 is less than current step: 499. Dropping entry: {'train/loss': 0.11281029116362333, 'train/ce_loss': 0.0365234375, 'train/seg_cls_loss': 0.002713775634765625, 'train/kl_loss': 0.03017578125, 'train/mask_bce_loss': 0.018500687793130056, 'train/mask_dice_loss': 0.07585610672831536, 'train/mask_loss': 0.09435679502785206, 'metrics/total_secs_per_batch': 27.5167179107666, 'metrics/data_secs_per_batch': 12.849353456497193, '_timestamp': 1741548613.271167}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 144 is less than current step: 499. Dropping entry: {'train/lr': 0.00029269879518072284, '_timestamp': 1741548613.2715552}).
Epoch: [1][146/500]	Time 28.998 (28.998)	Loss 0.2231 (0.1228)	CeLoss 0.0742 (0.0505)	SegCLSLoss 0.0016 (0.0024)	KLLoss 0.0366 (0.0295)	MaskLoss 0.1317 (0.0876)	MaskBCELoss 0.0549 (0.0182)	MaskDICELoss 0.0768 (0.0695)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 145 is less than current step: 499. Dropping entry: {'train/loss': 0.12275098972022533, 'train/ce_loss': 0.0504638671875, 'train/seg_cls_loss': 0.002385711669921875, 'train/kl_loss': 0.02950439453125, 'train/mask_bce_loss': 0.01815421711653471, 'train/mask_dice_loss': 0.06946713980287314, 'train/mask_loss': 0.08762135542929173, 'metrics/total_secs_per_batch': 28.99778962135315, 'metrics/data_secs_per_batch': 13.130613040924072, '_timestamp': 1741548642.268989}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 145 is less than current step: 499. Dropping entry: {'train/lr': 0.00029268674698795176, '_timestamp': 1741548642.2693963}).
Epoch: [1][147/500]	Time 27.647 (27.647)	Loss 0.1188 (0.1067)	CeLoss 0.0128 (0.0373)	SegCLSLoss 0.0034 (0.0025)	KLLoss 0.0327 (0.0235)	MaskLoss 0.1266 (0.0726)	MaskBCELoss 0.0274 (0.0212)	MaskDICELoss 0.0992 (0.0513)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 146 is less than current step: 499. Dropping entry: {'train/loss': 0.10672826301306486, 'train/ce_loss': 0.03734130859375, 'train/seg_cls_loss': 0.002526092529296875, 'train/kl_loss': 0.02353515625, 'train/mask_bce_loss': 0.021216881461441517, 'train/mask_dice_loss': 0.051342904660850765, 'train/mask_loss': 0.07255978714674712, 'metrics/total_secs_per_batch': 27.646557569503784, 'metrics/data_secs_per_batch': 12.577508354187012, '_timestamp': 1741548669.915785}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 146 is less than current step: 499. Dropping entry: {'train/lr': 0.0002926746987951807, '_timestamp': 1741548669.916163}).
Epoch: [1][148/500]	Time 29.416 (29.416)	Loss 0.2160 (0.1154)	CeLoss 0.0762 (0.0405)	SegCLSLoss 0.0048 (0.0023)	KLLoss 0.0231 (0.0357)	MaskLoss 0.1138 (0.1010)	MaskBCELoss 0.0537 (0.0155)	MaskDICELoss 0.0600 (0.0855)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 147 is less than current step: 499. Dropping entry: {'train/loss': 0.11544564887881278, 'train/ce_loss': 0.0405029296875, 'train/seg_cls_loss': 0.002336883544921875, 'train/kl_loss': 0.035693359375, 'train/mask_bce_loss': 0.01552085466682911, 'train/mask_dice_loss': 0.08546802764758468, 'train/mask_loss': 0.10098888427019119, 'metrics/total_secs_per_batch': 29.4162278175354, 'metrics/data_secs_per_batch': 13.052690267562866, '_timestamp': 1741548699.3319945}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 147 is less than current step: 499. Dropping entry: {'train/lr': 0.00029266265060240963, '_timestamp': 1741548699.3324711}).
Epoch: [1][149/500]	Time 27.988 (27.988)	Loss 0.0270 (0.1316)	CeLoss 0.0270 (0.0515)	SegCLSLoss 0.0000 (0.0029)	KLLoss 0.0000 (0.0319)	MaskLoss 0.0000 (0.0822)	MaskBCELoss 0.0000 (0.0250)	MaskDICELoss 0.0000 (0.0573)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 148 is less than current step: 499. Dropping entry: {'train/loss': 0.13155695516616106, 'train/ce_loss': 0.05150146484375, 'train/seg_cls_loss': 0.0029415130615234376, 'train/kl_loss': 0.03194580078125, 'train/mask_bce_loss': 0.024984028516337277, 'train/mask_dice_loss': 0.057259826362133025, 'train/mask_loss': 0.08224385492503643, 'metrics/total_secs_per_batch': 27.988482236862183, 'metrics/data_secs_per_batch': 12.330697751045227, '_timestamp': 1741548727.3202026}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 148 is less than current step: 499. Dropping entry: {'train/lr': 0.00029265060240963854, '_timestamp': 1741548727.3205655}).
Epoch: [1][150/500]	Time 29.052 (29.052)	Loss 0.1205 (0.1272)	CeLoss 0.0796 (0.0477)	SegCLSLoss 0.0012 (0.0031)	KLLoss 0.0403 (0.0335)	MaskLoss 0.0322 (0.0984)	MaskBCELoss 0.0160 (0.0192)	MaskDICELoss 0.0163 (0.0792)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 149 is less than current step: 499. Dropping entry: {'train/loss': 0.12721303775906562, 'train/ce_loss': 0.047650146484375, 'train/seg_cls_loss': 0.003112030029296875, 'train/kl_loss': 0.033544921875, 'train/mask_bce_loss': 0.019213192560710014, 'train/mask_dice_loss': 0.07915533371269703, 'train/mask_loss': 0.0983685277402401, 'metrics/total_secs_per_batch': 29.05217981338501, 'metrics/data_secs_per_batch': 13.09023721218109, '_timestamp': 1741548756.3723788}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 149 is less than current step: 499. Dropping entry: {'train/lr': 0.00029262650602409636, '_timestamp': 1741548756.3727086}).
Epoch: [1][151/500]	Time 29.784 (29.784)	Loss 0.0674 (0.1120)	CeLoss 0.0146 (0.0320)	SegCLSLoss 0.0026 (0.0027)	KLLoss 0.0332 (0.0328)	MaskLoss 0.1003 (0.1079)	MaskBCELoss 0.0008 (0.0164)	MaskDICELoss 0.0994 (0.0915)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 150 is less than current step: 499. Dropping entry: {'train/loss': 0.11198574006557464, 'train/ce_loss': 0.031982421875, 'train/seg_cls_loss': 0.00274810791015625, 'train/kl_loss': 0.03282470703125, 'train/mask_bce_loss': 0.01644622889580205, 'train/mask_dice_loss': 0.09148733876645565, 'train/mask_loss': 0.10793356522917748, 'metrics/total_secs_per_batch': 29.78390097618103, 'metrics/data_secs_per_batch': 13.324225735664367, '_timestamp': 1741548786.1564417}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 150 is less than current step: 499. Dropping entry: {'train/lr': 0.0002926144578313253, '_timestamp': 1741548786.156733}).
Epoch: [1][152/500]	Time 28.289 (28.289)	Loss 0.2625 (0.1376)	CeLoss 0.0967 (0.0583)	SegCLSLoss 0.0052 (0.0031)	KLLoss 0.0339 (0.0323)	MaskLoss 0.1186 (0.0895)	MaskBCELoss 0.0691 (0.0219)	MaskDICELoss 0.0495 (0.0676)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 151 is less than current step: 499. Dropping entry: {'train/loss': 0.13757162243127824, 'train/ce_loss': 0.0583251953125, 'train/seg_cls_loss': 0.003142547607421875, 'train/kl_loss': 0.03233642578125, 'train/mask_bce_loss': 0.02192329114768654, 'train/mask_dice_loss': 0.06756726391613484, 'train/mask_loss': 0.08949055299162864, 'metrics/total_secs_per_batch': 28.289257049560547, 'metrics/data_secs_per_batch': 14.061422157287598, '_timestamp': 1741548814.4456248}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 151 is less than current step: 499. Dropping entry: {'train/lr': 0.0002926024096385542, '_timestamp': 1741548814.4459262}).
Epoch: [1][153/500]	Time 27.963 (27.963)	Loss 0.1527 (0.1344)	CeLoss 0.0247 (0.0545)	SegCLSLoss 0.0019 (0.0025)	KLLoss 0.0398 (0.0333)	MaskLoss 0.1307 (0.0900)	MaskBCELoss 0.0412 (0.0225)	MaskDICELoss 0.0895 (0.0675)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 152 is less than current step: 499. Dropping entry: {'train/loss': 0.13440418429672718, 'train/ce_loss': 0.0545166015625, 'train/seg_cls_loss': 0.002472686767578125, 'train/kl_loss': 0.033251953125, 'train/mask_bce_loss': 0.022463286691345275, 'train/mask_dice_loss': 0.06749037355184555, 'train/mask_loss': 0.0899536607787013, 'metrics/total_secs_per_batch': 27.963366746902466, 'metrics/data_secs_per_batch': 12.929327869415284, '_timestamp': 1741548842.4089882}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 152 is less than current step: 499. Dropping entry: {'train/lr': 0.0002925903614457831, '_timestamp': 1741548842.4092493}).
Epoch: [1][154/500]	Time 30.813 (30.813)	Loss 0.0876 (0.1112)	CeLoss 0.0159 (0.0394)	SegCLSLoss 0.0069 (0.0030)	KLLoss 0.0266 (0.0329)	MaskLoss 0.1046 (0.1044)	MaskBCELoss 0.0107 (0.0121)	MaskDICELoss 0.0938 (0.0922)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 153 is less than current step: 499. Dropping entry: {'train/loss': 0.11120462119579315, 'train/ce_loss': 0.03936767578125, 'train/seg_cls_loss': 0.00295867919921875, 'train/kl_loss': 0.0329345703125, 'train/mask_bce_loss': 0.012133686582092195, 'train/mask_dice_loss': 0.09223387166857719, 'train/mask_loss': 0.10436755567789077, 'metrics/total_secs_per_batch': 30.81285572052002, 'metrics/data_secs_per_batch': 13.92206380367279, '_timestamp': 1741548873.2218034}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 153 is less than current step: 499. Dropping entry: {'train/lr': 0.000292578313253012, '_timestamp': 1741548873.2220652}).
Epoch: [1][155/500]	Time 25.758 (25.758)	Loss 0.0831 (0.1176)	CeLoss 0.0297 (0.0400)	SegCLSLoss 0.0019 (0.0023)	KLLoss 0.0283 (0.0338)	MaskLoss 0.0957 (0.1022)	MaskBCELoss 0.0030 (0.0168)	MaskDICELoss 0.0927 (0.0854)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 154 is less than current step: 499. Dropping entry: {'train/loss': 0.11762021481990814, 'train/ce_loss': 0.0400390625, 'train/seg_cls_loss': 0.002321624755859375, 'train/kl_loss': 0.03375244140625, 'train/mask_bce_loss': 0.016832737158983944, 'train/mask_dice_loss': 0.0853704173117876, 'train/mask_loss': 0.10220315530896187, 'metrics/total_secs_per_batch': 25.758066654205322, 'metrics/data_secs_per_batch': 12.00345561504364, '_timestamp': 1741548898.9802332}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 154 is less than current step: 499. Dropping entry: {'train/lr': 0.0002925662650602409, '_timestamp': 1741548898.9885988}).
Epoch: [1][156/500]	Time 25.267 (25.267)	Loss 0.1148 (0.1261)	CeLoss 0.0522 (0.0478)	SegCLSLoss 0.0036 (0.0031)	KLLoss 0.0289 (0.0249)	MaskLoss 0.1050 (0.0993)	MaskBCELoss 0.0053 (0.0180)	MaskDICELoss 0.0996 (0.0813)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 155 is less than current step: 499. Dropping entry: {'train/loss': 0.12606253549456597, 'train/ce_loss': 0.047802734375, 'train/seg_cls_loss': 0.00305023193359375, 'train/kl_loss': 0.02489013671875, 'train/mask_bce_loss': 0.018005065282341092, 'train/mask_dice_loss': 0.08127668164670468, 'train/mask_loss': 0.09928174838423728, 'metrics/total_secs_per_batch': 25.266538381576538, 'metrics/data_secs_per_batch': 10.595732760429382, '_timestamp': 1741548924.246456}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 155 is less than current step: 499. Dropping entry: {'train/lr': 0.00029255421686746983, '_timestamp': 1741548924.2468429}).
Epoch: [1][157/500]	Time 28.641 (28.641)	Loss 0.0927 (0.1225)	CeLoss 0.0238 (0.0500)	SegCLSLoss 0.0067 (0.0034)	KLLoss 0.0261 (0.0275)	MaskLoss 0.1077 (0.0954)	MaskBCELoss 0.0078 (0.0154)	MaskDICELoss 0.0999 (0.0801)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 156 is less than current step: 499. Dropping entry: {'train/loss': 0.12246711887419223, 'train/ce_loss': 0.050048828125, 'train/seg_cls_loss': 0.00335845947265625, 'train/kl_loss': 0.02747802734375, 'train/mask_bce_loss': 0.015355087106581778, 'train/mask_dice_loss': 0.08005686234682799, 'train/mask_loss': 0.09541195146739483, 'metrics/total_secs_per_batch': 28.64135456085205, 'metrics/data_secs_per_batch': 12.538523483276368, '_timestamp': 1741548952.88782}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 156 is less than current step: 499. Dropping entry: {'train/lr': 0.0002925421686746988, '_timestamp': 1741548952.8882072}).
Epoch: [1][158/500]	Time 28.795 (28.795)	Loss 0.1263 (0.0991)	CeLoss 0.0635 (0.0412)	SegCLSLoss 0.0022 (0.0024)	KLLoss 0.0303 (0.0207)	MaskLoss 0.1029 (0.0711)	MaskBCELoss 0.0067 (0.0140)	MaskDICELoss 0.0962 (0.0571)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 157 is less than current step: 499. Dropping entry: {'train/loss': 0.09908015094697475, 'train/ce_loss': 0.04119873046875, 'train/seg_cls_loss': 0.00244293212890625, 'train/kl_loss': 0.02066650390625, 'train/mask_bce_loss': 0.014027197530958801, 'train/mask_dice_loss': 0.05707104243338108, 'train/mask_loss': 0.07109823897480964, 'metrics/total_secs_per_batch': 28.794904947280884, 'metrics/data_secs_per_batch': 12.603193521499634, '_timestamp': 1741548981.682828}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 157 is less than current step: 499. Dropping entry: {'train/lr': 0.0002925301204819277, '_timestamp': 1741548981.6834955}).
Epoch: [1][159/500]	Time 30.877 (30.877)	Loss 0.1429 (0.1188)	CeLoss 0.0210 (0.0422)	SegCLSLoss 0.0028 (0.0021)	KLLoss 0.0444 (0.0340)	MaskLoss 0.1227 (0.0902)	MaskBCELoss 0.0394 (0.0203)	MaskDICELoss 0.0833 (0.0699)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 158 is less than current step: 499. Dropping entry: {'train/loss': 0.11881250273436308, 'train/ce_loss': 0.04222412109375, 'train/seg_cls_loss': 0.00210418701171875, 'train/kl_loss': 0.03397216796875, 'train/mask_bce_loss': 0.0203008721844526, 'train/mask_dice_loss': 0.06991760609671474, 'train/mask_loss': 0.09021847900003195, 'metrics/total_secs_per_batch': 30.876868963241577, 'metrics/data_secs_per_batch': 14.064070105552673, '_timestamp': 1741549012.559607}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 158 is less than current step: 499. Dropping entry: {'train/lr': 0.0002925180722891566, '_timestamp': 1741549012.5599952}).
Epoch: [1][160/500]	Time 31.885 (31.885)	Loss 0.1353 (0.1228)	CeLoss 0.0225 (0.0513)	SegCLSLoss 0.0031 (0.0035)	KLLoss 0.0237 (0.0258)	MaskLoss 0.1202 (0.0925)	MaskBCELoss 0.0341 (0.0158)	MaskDICELoss 0.0861 (0.0767)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 159 is less than current step: 499. Dropping entry: {'train/loss': 0.12283713240176439, 'train/ce_loss': 0.05125732421875, 'train/seg_cls_loss': 0.00350494384765625, 'train/kl_loss': 0.02579345703125, 'train/mask_bce_loss': 0.015764477953780442, 'train/mask_dice_loss': 0.07671791575849056, 'train/mask_loss': 0.09248239248991012, 'metrics/total_secs_per_batch': 31.885076999664307, 'metrics/data_secs_per_batch': 12.991143441200256, '_timestamp': 1741549044.4449205}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 159 is less than current step: 499. Dropping entry: {'train/lr': 0.00029249397590361444, '_timestamp': 1741549044.4480994}).
Epoch: [1][161/500]	Time 26.804 (26.804)	Loss 0.0744 (0.1144)	CeLoss 0.0129 (0.0526)	SegCLSLoss 0.0022 (0.0026)	KLLoss 0.0391 (0.0253)	MaskLoss 0.1052 (0.0853)	MaskBCELoss 0.0052 (0.0119)	MaskDICELoss 0.1000 (0.0734)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 160 is less than current step: 499. Dropping entry: {'train/loss': 0.11444424688816071, 'train/ce_loss': 0.052630615234375, 'train/seg_cls_loss': 0.00256500244140625, 'train/kl_loss': 0.02525634765625, 'train/mask_bce_loss': 0.011938808823470027, 'train/mask_dice_loss': 0.07337934747338296, 'train/mask_loss': 0.08531815633177757, 'metrics/total_secs_per_batch': 26.80372953414917, 'metrics/data_secs_per_batch': 11.961461400985717, '_timestamp': 1741549071.2484455}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 160 is less than current step: 499. Dropping entry: {'train/lr': 0.00029248192771084335, '_timestamp': 1741549071.2487288}).
Epoch: [1][162/500]	Time 29.261 (29.261)	Loss 0.1028 (0.1136)	CeLoss 0.0503 (0.0387)	SegCLSLoss 0.0023 (0.0041)	KLLoss 0.0354 (0.0330)	MaskLoss 0.1005 (0.1091)	MaskBCELoss 0.0007 (0.0122)	MaskDICELoss 0.0998 (0.0969)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 161 is less than current step: 499. Dropping entry: {'train/loss': 0.11362053900957107, 'train/ce_loss': 0.03865966796875, 'train/seg_cls_loss': 0.004131317138671875, 'train/kl_loss': 0.032958984375, 'train/mask_bce_loss': 0.012220065330620856, 'train/mask_dice_loss': 0.09691549986600875, 'train/mask_loss': 0.1091355673968792, 'metrics/total_secs_per_batch': 29.2608482837677, 'metrics/data_secs_per_batch': 13.230761814117432, '_timestamp': 1741549100.5092998}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 161 is less than current step: 499. Dropping entry: {'train/lr': 0.00029246987951807226, '_timestamp': 1741549100.5095615}).
Epoch: [1][163/500]	Time 27.289 (27.289)	Loss 0.1396 (0.1454)	CeLoss 0.0815 (0.0428)	SegCLSLoss 0.0026 (0.0031)	KLLoss 0.0277 (0.0325)	MaskLoss 0.0904 (0.1195)	MaskBCELoss 0.0077 (0.0275)	MaskDICELoss 0.0827 (0.0920)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 162 is less than current step: 499. Dropping entry: {'train/loss': 0.1453995332121849, 'train/ce_loss': 0.04281005859375, 'train/seg_cls_loss': 0.00313873291015625, 'train/kl_loss': 0.03248291015625, 'train/mask_bce_loss': 0.027540010452503337, 'train/mask_dice_loss': 0.09198667779564858, 'train/mask_loss': 0.11952668875455856, 'metrics/total_secs_per_batch': 27.289056539535522, 'metrics/data_secs_per_batch': 12.62684109210968, '_timestamp': 1741549127.7983031}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 162 is less than current step: 499. Dropping entry: {'train/lr': 0.00029245783132530117, '_timestamp': 1741549127.79857}).
Epoch: [1][164/500]	Time 27.927 (27.927)	Loss 0.0215 (0.1152)	CeLoss 0.0215 (0.0402)	SegCLSLoss 0.0000 (0.0027)	KLLoss 0.0000 (0.0288)	MaskLoss 0.0000 (0.0910)	MaskBCELoss 0.0000 (0.0188)	MaskDICELoss 0.0000 (0.0722)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 163 is less than current step: 499. Dropping entry: {'train/loss': 0.11520641073584556, 'train/ce_loss': 0.0402099609375, 'train/seg_cls_loss': 0.00274810791015625, 'train/kl_loss': 0.028759765625, 'train/mask_bce_loss': 0.018774539534933864, 'train/mask_dice_loss': 0.07218965813517571, 'train/mask_loss': 0.09096419923007489, 'metrics/total_secs_per_batch': 27.927284717559814, 'metrics/data_secs_per_batch': 13.0988609790802, '_timestamp': 1741549155.7256618}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 163 is less than current step: 499. Dropping entry: {'train/lr': 0.0002924457831325301, '_timestamp': 1741549155.7260685}).
Epoch: [1][165/500]	Time 28.260 (28.260)	Loss 0.0957 (0.1129)	CeLoss 0.0189 (0.0431)	SegCLSLoss 0.0014 (0.0021)	KLLoss 0.0270 (0.0314)	MaskLoss 0.0932 (0.0915)	MaskBCELoss 0.0197 (0.0153)	MaskDICELoss 0.0735 (0.0763)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 164 is less than current step: 499. Dropping entry: {'train/loss': 0.1128611471503973, 'train/ce_loss': 0.0431396484375, 'train/seg_cls_loss': 0.002138519287109375, 'train/kl_loss': 0.03140869140625, 'train/mask_bce_loss': 0.015267843869514764, 'train/mask_dice_loss': 0.07627200782299041, 'train/mask_loss': 0.09153985157608986, 'metrics/total_secs_per_batch': 28.260226488113403, 'metrics/data_secs_per_batch': 12.765471506118775, '_timestamp': 1741549183.9858353}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 164 is less than current step: 499. Dropping entry: {'train/lr': 0.000292433734939759, '_timestamp': 1741549183.9862275}).
Epoch: [1][166/500]	Time 29.191 (29.191)	Loss 0.1716 (0.1241)	CeLoss 0.0942 (0.0572)	SegCLSLoss 0.0020 (0.0031)	KLLoss 0.0277 (0.0295)	MaskLoss 0.0617 (0.0970)	MaskBCELoss 0.0303 (0.0112)	MaskDICELoss 0.0314 (0.0858)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 165 is less than current step: 499. Dropping entry: {'train/loss': 0.12406533360481262, 'train/ce_loss': 0.05723876953125, 'train/seg_cls_loss': 0.003073883056640625, 'train/kl_loss': 0.02952880859375, 'train/mask_bce_loss': 0.01117089053732343, 'train/mask_dice_loss': 0.08582991361618042, 'train/mask_loss': 0.09700080566108227, 'metrics/total_secs_per_batch': 29.19126844406128, 'metrics/data_secs_per_batch': 14.120693254470826, '_timestamp': 1741549213.177106}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 165 is less than current step: 499. Dropping entry: {'train/lr': 0.0002924216867469879, '_timestamp': 1741549213.1774924}).
Epoch: [1][167/500]	Time 29.644 (29.644)	Loss 0.1115 (0.1268)	CeLoss 0.0598 (0.0358)	SegCLSLoss 0.0011 (0.0024)	KLLoss 0.0417 (0.0370)	MaskLoss 0.0961 (0.0983)	MaskBCELoss 0.0022 (0.0272)	MaskDICELoss 0.0940 (0.0711)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 166 is less than current step: 499. Dropping entry: {'train/loss': 0.12683432176709175, 'train/ce_loss': 0.03575439453125, 'train/seg_cls_loss': 0.0023876190185546874, 'train/kl_loss': 0.03699951171875, 'train/mask_bce_loss': 0.027201570756733417, 'train/mask_dice_loss': 0.07109283152967691, 'train/mask_loss': 0.09829440172761679, 'metrics/total_secs_per_batch': 29.64390778541565, 'metrics/data_secs_per_batch': 12.964259481430053, '_timestamp': 1741549242.82101}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 166 is less than current step: 499. Dropping entry: {'train/lr': 0.00029240963855421687, '_timestamp': 1741549242.8213887}).
Epoch: [1][168/500]	Time 30.699 (30.699)	Loss 0.0734 (0.1364)	CeLoss 0.0126 (0.0378)	SegCLSLoss 0.0023 (0.0030)	KLLoss 0.0281 (0.0270)	MaskLoss 0.1048 (0.1042)	MaskBCELoss 0.0049 (0.0300)	MaskDICELoss 0.0999 (0.0743)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 167 is less than current step: 499. Dropping entry: {'train/loss': 0.13643906079232693, 'train/ce_loss': 0.03782958984375, 'train/seg_cls_loss': 0.00298004150390625, 'train/kl_loss': 0.0269775390625, 'train/mask_bce_loss': 0.029972870915662496, 'train/mask_dice_loss': 0.07427570139989256, 'train/mask_loss': 0.10424857251346112, 'metrics/total_secs_per_batch': 30.698655605316162, 'metrics/data_secs_per_batch': 12.946272802352905, '_timestamp': 1741549273.5199952}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 167 is less than current step: 499. Dropping entry: {'train/lr': 0.0002923975903614458, '_timestamp': 1741549273.5204082}).
Epoch: [1][169/500]	Time 26.976 (26.976)	Loss 0.1455 (0.1399)	CeLoss 0.0762 (0.0560)	SegCLSLoss 0.0015 (0.0032)	KLLoss 0.0291 (0.0316)	MaskLoss 0.1081 (0.1041)	MaskBCELoss 0.0097 (0.0201)	MaskDICELoss 0.0984 (0.0840)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 168 is less than current step: 499. Dropping entry: {'train/loss': 0.1398940309882164, 'train/ce_loss': 0.0560302734375, 'train/seg_cls_loss': 0.003216552734375, 'train/kl_loss': 0.0315673828125, 'train/mask_bce_loss': 0.020082668407121675, 'train/mask_dice_loss': 0.08404722753912211, 'train/mask_loss': 0.10412989370524883, 'metrics/total_secs_per_batch': 26.976157188415527, 'metrics/data_secs_per_batch': 12.064673972129821, '_timestamp': 1741549300.4958467}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 168 is less than current step: 499. Dropping entry: {'train/lr': 0.0002923855421686747, '_timestamp': 1741549300.496221}).
Epoch: [1][170/500]	Time 24.536 (24.536)	Loss 0.1675 (0.1553)	CeLoss 0.0613 (0.0625)	SegCLSLoss 0.0013 (0.0026)	KLLoss 0.0396 (0.0275)	MaskLoss 0.1055 (0.1001)	MaskBCELoss 0.0351 (0.0276)	MaskDICELoss 0.0704 (0.0725)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 169 is less than current step: 499. Dropping entry: {'train/loss': 0.1552712582051754, 'train/ce_loss': 0.06248779296875, 'train/seg_cls_loss': 0.002581787109375, 'train/kl_loss': 0.027490234375, 'train/mask_bce_loss': 0.027577287942403928, 'train/mask_dice_loss': 0.07249898947775364, 'train/mask_loss': 0.10007627531886101, 'metrics/total_secs_per_batch': 24.535539865493774, 'metrics/data_secs_per_batch': 10.430786156654358, '_timestamp': 1741549325.0313532}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 169 is less than current step: 499. Dropping entry: {'train/lr': 0.0002923614457831325, '_timestamp': 1741549325.0315783}).
Epoch: [1][171/500]	Time 26.106 (26.106)	Loss 0.0798 (0.1235)	CeLoss 0.0250 (0.0471)	SegCLSLoss 0.0037 (0.0034)	KLLoss 0.0215 (0.0325)	MaskLoss 0.0645 (0.1001)	MaskBCELoss 0.0137 (0.0164)	MaskDICELoss 0.0508 (0.0837)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 170 is less than current step: 499. Dropping entry: {'train/loss': 0.12350166738033294, 'train/ce_loss': 0.0470703125, 'train/seg_cls_loss': 0.00338592529296875, 'train/kl_loss': 0.03250732421875, 'train/mask_bce_loss': 0.016438421967905015, 'train/mask_dice_loss': 0.08371058404445648, 'train/mask_loss': 0.10014900341629981, 'metrics/total_secs_per_batch': 26.106250524520874, 'metrics/data_secs_per_batch': 12.186284303665161, '_timestamp': 1741549351.1378794}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 170 is less than current step: 499. Dropping entry: {'train/lr': 0.0002923493975903614, '_timestamp': 1741549351.1386456}).
Epoch: [1][172/500]	Time 32.004 (32.004)	Loss 0.0951 (0.1271)	CeLoss 0.0347 (0.0371)	SegCLSLoss 0.0025 (0.0025)	KLLoss 0.0305 (0.0338)	MaskLoss 0.1026 (0.1114)	MaskBCELoss 0.0051 (0.0221)	MaskDICELoss 0.0976 (0.0893)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 171 is less than current step: 499. Dropping entry: {'train/loss': 0.12710852399468422, 'train/ce_loss': 0.03707275390625, 'train/seg_cls_loss': 0.002451324462890625, 'train/kl_loss': 0.033837890625, 'train/mask_bce_loss': 0.022065430949442088, 'train/mask_dice_loss': 0.08931470438838005, 'train/mask_loss': 0.11138013377785683, 'metrics/total_secs_per_batch': 32.00384521484375, 'metrics/data_secs_per_batch': 14.765661716461182, '_timestamp': 1741549383.1414733}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 171 is less than current step: 499. Dropping entry: {'train/lr': 0.00029233734939759033, '_timestamp': 1741549383.1417296}).
Epoch: [1][173/500]	Time 28.040 (28.040)	Loss 0.1501 (0.1247)	CeLoss 0.0967 (0.0560)	SegCLSLoss 0.0030 (0.0028)	KLLoss 0.0305 (0.0315)	MaskLoss 0.1007 (0.1006)	MaskBCELoss 0.0009 (0.0114)	MaskDICELoss 0.0998 (0.0892)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 172 is less than current step: 499. Dropping entry: {'train/loss': 0.12468145489692688, 'train/ce_loss': 0.05599365234375, 'train/seg_cls_loss': 0.00282135009765625, 'train/kl_loss': 0.0314697265625, 'train/mask_bce_loss': 0.011357337015215307, 'train/mask_dice_loss': 0.08924604989588261, 'train/mask_loss': 0.10060338824987411, 'metrics/total_secs_per_batch': 28.040430545806885, 'metrics/data_secs_per_batch': 12.911815690994263, '_timestamp': 1741549411.1819522}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 172 is less than current step: 499. Dropping entry: {'train/lr': 0.00029232530120481924, '_timestamp': 1741549411.1823528}).
Epoch: [1][174/500]	Time 29.345 (29.345)	Loss 0.0930 (0.1220)	CeLoss 0.0381 (0.0397)	SegCLSLoss 0.0022 (0.0027)	KLLoss 0.0332 (0.0344)	MaskLoss 0.0994 (0.1016)	MaskBCELoss 0.0027 (0.0201)	MaskDICELoss 0.0967 (0.0816)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 173 is less than current step: 499. Dropping entry: {'train/loss': 0.1219621941447258, 'train/ce_loss': 0.03970947265625, 'train/seg_cls_loss': 0.00272369384765625, 'train/kl_loss': 0.0344482421875, 'train/mask_bce_loss': 0.020062238350510596, 'train/mask_dice_loss': 0.08156605362892151, 'train/mask_loss': 0.10162829235196114, 'metrics/total_secs_per_batch': 29.345044136047363, 'metrics/data_secs_per_batch': 13.82530107498169, '_timestamp': 1741549440.5270028}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 173 is less than current step: 499. Dropping entry: {'train/lr': 0.00029231325301204815, '_timestamp': 1741549440.5274}).
Epoch: [1][175/500]	Time 28.449 (28.449)	Loss 0.0892 (0.1040)	CeLoss 0.0150 (0.0469)	SegCLSLoss 0.0022 (0.0026)	KLLoss 0.0297 (0.0312)	MaskLoss 0.1089 (0.0820)	MaskBCELoss 0.0124 (0.0099)	MaskDICELoss 0.0965 (0.0721)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 174 is less than current step: 499. Dropping entry: {'train/loss': 0.10399234145879746, 'train/ce_loss': 0.046923828125, 'train/seg_cls_loss': 0.00257110595703125, 'train/kl_loss': 0.03115234375, 'train/mask_bce_loss': 0.009865203534718602, 'train/mask_dice_loss': 0.07213715203106404, 'train/mask_loss': 0.08200235459953546, 'metrics/total_secs_per_batch': 28.449389219284058, 'metrics/data_secs_per_batch': 12.992134594917298, '_timestamp': 1741549468.9764163}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 174 is less than current step: 499. Dropping entry: {'train/lr': 0.00029230120481927706, '_timestamp': 1741549468.9766967}).
Epoch: [1][176/500]	Time 28.372 (28.372)	Loss 0.0190 (0.1282)	CeLoss 0.0190 (0.0400)	SegCLSLoss 0.0000 (0.0027)	KLLoss 0.0000 (0.0245)	MaskLoss 0.0000 (0.0895)	MaskBCELoss 0.0000 (0.0281)	MaskDICELoss 0.0000 (0.0614)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 175 is less than current step: 499. Dropping entry: {'train/loss': 0.12823521457612513, 'train/ce_loss': 0.04002685546875, 'train/seg_cls_loss': 0.0027069091796875, 'train/kl_loss': 0.0244873046875, 'train/mask_bce_loss': 0.028066566330380738, 'train/mask_dice_loss': 0.06143072322010994, 'train/mask_loss': 0.0894972912967205, 'metrics/total_secs_per_batch': 28.371517181396484, 'metrics/data_secs_per_batch': 13.186646199226379, '_timestamp': 1741549497.3480902}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 175 is less than current step: 499. Dropping entry: {'train/lr': 0.000292289156626506, '_timestamp': 1741549497.3484821}).
Epoch: [1][177/500]	Time 27.131 (27.131)	Loss 0.1022 (0.1092)	CeLoss 0.0210 (0.0370)	SegCLSLoss 0.0016 (0.0037)	KLLoss 0.0300 (0.0300)	MaskLoss 0.0938 (0.0998)	MaskBCELoss 0.0223 (0.0136)	MaskDICELoss 0.0714 (0.0862)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 176 is less than current step: 499. Dropping entry: {'train/loss': 0.10915603339672089, 'train/ce_loss': 0.03702392578125, 'train/seg_cls_loss': 0.003699493408203125, 'train/kl_loss': 0.03001708984375, 'train/mask_bce_loss': 0.013610323192551732, 'train/mask_dice_loss': 0.08619499020278454, 'train/mask_loss': 0.09980531185865402, 'metrics/total_secs_per_batch': 27.13066816329956, 'metrics/data_secs_per_batch': 12.421989393234252, '_timestamp': 1741549524.478666}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 176 is less than current step: 499. Dropping entry: {'train/lr': 0.00029227710843373494, '_timestamp': 1741549524.47908}).
Epoch: [1][178/500]	Time 23.652 (23.652)	Loss 0.0779 (0.1076)	CeLoss 0.0203 (0.0383)	SegCLSLoss 0.0062 (0.0040)	KLLoss 0.0231 (0.0288)	MaskLoss 0.0599 (0.0800)	MaskBCELoss 0.0164 (0.0182)	MaskDICELoss 0.0434 (0.0617)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 177 is less than current step: 499. Dropping entry: {'train/loss': 0.10760579183697701, 'train/ce_loss': 0.03826904296875, 'train/seg_cls_loss': 0.00401458740234375, 'train/kl_loss': 0.0287841796875, 'train/mask_bce_loss': 0.018230640701949595, 'train/mask_dice_loss': 0.06172261340543628, 'train/mask_loss': 0.07995325364172459, 'metrics/total_secs_per_batch': 23.65244197845459, 'metrics/data_secs_per_batch': 11.484499168395995, '_timestamp': 1741549548.1309803}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 177 is less than current step: 499. Dropping entry: {'train/lr': 0.00029226506024096385, '_timestamp': 1741549548.1312318}).
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 670, in <module>
[rank0]:     main(sys.argv[1:])
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 403, in main
[rank0]:     train_iter = train(
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 499, in train
[rank0]:     output_dict = model(**input_dict)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1735, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/peft/peft_model.py", line 922, in forward
[rank0]:     return self.base_model(
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py", line 216, in forward
[rank0]:     return self.model_forward(**kwargs)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py", line 388, in model_forward
[rank0]:     gt_mask = masks_list[batch_idx][mask_idx].unsqueeze(0)  # [1, H, W]
[rank0]: IndexError: index 1 is out of bounds for dimension 0 with size 1