
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")


Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:15<00:00,  5.05s/it]
Some weights of PLUMForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-llama-2-13b-chat-lightning-preview and are newly initialized: ['model.vision_tower.visual.transformer.resblocks.22.mlp.c_proj.weight', 'model.vision_tower.transformer.resblocks.6.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.21.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.13.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.10.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.20.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.7.ln_2.weight', 'bio_encoder.encoder.layers.0.norm1.bias', 'model.vision_tower.visual.transformer.resblocks.11.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.14.attn.in_proj_bias', 'bio_encoder.encoder.layers.0.linear2.weight', 'model.vision_tower.visual.transformer.resblocks.4.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.23.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.2.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.4.mlp.c_fc.weight', 'model.vision_tower.transformer.resblocks.4.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.8.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.1.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.20.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.6.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.14.ln_1.weight', 'model.vision_tower.transformer.resblocks.10.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.11.ln_1.weight', 'model.vision_tower.transformer.resblocks.0.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.13.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.18.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.21.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.8.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.10.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.5.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.3.attn.in_proj_weight', 'model.vision_tower.transformer.resblocks.4.mlp.c_proj.bias', 'bio_encoder.encoder.layers.0.self_attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.9.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.17.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.10.ln_2.weight', 'model.vision_tower.transformer.resblocks.8.ln_2.weight', 'model.vision_tower.transformer.resblocks.6.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.9.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.0.ln_2.bias', 'model.vision_tower.transformer.resblocks.1.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.7.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.9.mlp.c_fc.weight', 'model.vision_tower.transformer.resblocks.1.ln_1.bias', 'model.vision_tower.transformer.resblocks.2.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.9.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.16.ln_1.bias', 'model.vision_tower.transformer.resblocks.8.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.18.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.11.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.19.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.19.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.21.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.13.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.0.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.5.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.23.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.21.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.2.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.3.mlp.c_fc.bias', 'model.vision_tower.transformer.resblocks.3.ln_1.weight', 'model.vision_tower.visual.positional_embedding', 'model.vision_tower.visual.transformer.resblocks.19.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.19.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.13.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.11.mlp.c_fc.bias', 'model.vision_tower.transformer.resblocks.8.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.4.ln_2.weight', 'model.vision_tower.transformer.resblocks.0.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.9.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.15.attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.7.mlp.c_proj.bias', 'model.vision_tower.transformer.resblocks.5.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.11.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.4.attn.in_proj_bias', 'model.vision_tower.visual.ln_pre.weight', 'model.vision_tower.transformer.resblocks.3.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.22.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.16.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.3.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.11.mlp.c_fc.bias', 'model.vision_tower.transformer.resblocks.3.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.15.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.18.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.14.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.22.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.0.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.12.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.3.mlp.c_fc.bias', 'model.vision_tower.transformer.resblocks.10.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.9.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.14.ln_2.bias', 'model.vision_tower.transformer.resblocks.4.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.11.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.9.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.0.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.23.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.15.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.20.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.15.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.17.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.11.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.15.ln_2.bias', 'model.vision_tower.transformer.resblocks.5.attn.out_proj.weight', 'bio_encoder.encoder.layers.0.linear1.weight', 'model.vision_tower.transformer.resblocks.6.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.16.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.16.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.1.mlp.c_proj.weight', 'model.vision_tower.transformer.resblocks.7.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.19.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.7.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.4.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.22.attn.in_proj_weight', 'bio_encoder.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.12.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.12.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.16.attn.in_proj_weight', 'model.vision_tower.transformer.resblocks.6.attn.out_proj.bias', 'model.vision_tower.visual.ln_post.bias', 'model.vision_tower.visual.transformer.resblocks.5.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.18.mlp.c_proj.bias', 'model.vision_tower.transformer.resblocks.10.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.6.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.6.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.10.mlp.c_proj.weight', 'model.vision_tower.transformer.resblocks.7.attn.in_proj_weight', 'model.vision_tower.transformer.resblocks.1.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.9.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.21.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.18.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.14.attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.5.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.23.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.13.ln_2.bias', 'model.vision_tower.transformer.resblocks.7.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.23.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.23.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.20.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.3.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.10.attn.in_proj_weight', 'bio_encoder.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.6.ln_1.bias', 'model.vision_tower.transformer.resblocks.5.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.1.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.20.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.7.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.4.ln_2.bias', 'model.vision_tower.transformer.resblocks.8.ln_2.bias', 'model.vision_tower.transformer.resblocks.11.attn.out_proj.weight', 'model.vision_tower.visual.class_embedding', 'model.vision_tower.transformer.resblocks.11.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.20.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.9.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.21.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.21.ln_1.weight', 'model.vision_tower.transformer.resblocks.7.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.4.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.10.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.14.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.6.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.7.ln_1.weight', 'model.vision_tower.transformer.resblocks.0.ln_1.bias', 'model.vision_tower.transformer.resblocks.6.ln_1.weight', 'model.vision_tower.transformer.resblocks.11.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.3.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.8.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.22.ln_2.bias', 'model.vision_tower.transformer.resblocks.8.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.20.ln_1.bias', 'model.vision_tower.transformer.resblocks.9.ln_1.bias', 'model.vision_tower.transformer.resblocks.10.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.4.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.17.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.14.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.18.ln_1.bias', 'bio_encoder.encoder.layers.0.linear2.bias', 'model.vision_tower.visual.transformer.resblocks.4.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.13.ln_1.bias', 'model.vision_tower.transformer.resblocks.4.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.3.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.15.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.4.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.0.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.8.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.18.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.4.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.19.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.8.ln_2.bias', 'model.vision_tower.transformer.resblocks.6.ln_2.weight', 'model.vision_tower.transformer.resblocks.2.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.17.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.16.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.5.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.17.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.4.ln_1.bias', 'model.vision_tower.transformer.resblocks.11.ln_2.weight', 'model.vision_tower.transformer.resblocks.8.ln_1.bias', 'model.vision_tower.transformer.resblocks.6.mlp.c_proj.weight', 'model.vision_tower.transformer.resblocks.9.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.7.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.1.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.2.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.8.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.8.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.7.mlp.c_fc.bias', 'model.vision_tower.transformer.resblocks.10.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.11.ln_1.weight', 'model.vision_tower.transformer.resblocks.8.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.6.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.23.attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.7.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.1.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.18.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.12.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.19.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.11.ln_2.weight', 'model.vision_tower.transformer.resblocks.7.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.19.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.11.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.5.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.12.mlp.c_fc.bias', 'model.vision_tower.transformer.resblocks.1.mlp.c_fc.weight', 'model.vision_tower.transformer.resblocks.11.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.10.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.8.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.6.mlp.c_proj.bias', 'model.vision_tower.transformer.resblocks.2.mlp.c_fc.bias', 'model.vision_tower.transformer.resblocks.0.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.8.mlp.c_proj.weight', 'model.vision_tower.ln_final.bias', 'model.vision_tower.transformer.resblocks.3.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.11.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.0.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.21.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.5.mlp.c_proj.bias', 'model.vision_tower.transformer.resblocks.0.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.8.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.10.mlp.c_fc.weight', 'model.vision_tower.transformer.resblocks.1.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.2.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.11.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.2.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.16.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.6.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.1.ln_2.weight', 'model.vision_tower.transformer.resblocks.4.ln_2.bias', 'model.vision_tower.transformer.resblocks.9.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.8.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.6.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.21.mlp.c_fc.bias', 'model.vision_tower.transformer.resblocks.7.mlp.c_proj.weight', 'model.vision_tower.transformer.resblocks.10.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.13.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.19.ln_1.bias', 'bio_encoder.encoder.layers.0.norm2.weight', 'model.vision_tower.visual.transformer.resblocks.2.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.12.ln_2.weight', 'model.vision_tower.visual.ln_post.weight', 'model.vision_tower.visual.transformer.resblocks.19.mlp.c_proj.weight', 'model.vision_tower.transformer.resblocks.2.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.9.ln_2.weight', 'model.vision_tower.transformer.resblocks.2.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.6.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.19.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.2.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.7.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.2.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.17.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.22.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.0.ln_1.weight', 'model.vision_tower.transformer.resblocks.0.mlp.c_fc.bias', 'model.vision_tower.transformer.resblocks.10.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.4.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.0.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.6.mlp.c_fc.weight', 'model.vision_tower.transformer.resblocks.1.attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.8.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.13.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.0.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.0.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.22.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.13.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.7.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.18.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.15.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.1.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.17.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.18.ln_2.bias', 'model.vision_tower.transformer.resblocks.4.ln_2.weight', 'model.vision_tower.transformer.resblocks.1.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.7.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.7.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.22.ln_2.weight', 'model.vision_tower.transformer.resblocks.5.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.16.attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.0.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.10.mlp.c_fc.bias', 'model.vision_tower.visual.conv1.weight', 'model.vision_tower.transformer.resblocks.9.mlp.c_fc.weight', 'model.vision_tower.visual.proj', 'model.vision_tower.visual.transformer.resblocks.0.ln_2.weight', 'model.vision_tower.transformer.resblocks.9.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.8.attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.0.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.17.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.3.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.10.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.1.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.11.ln_1.bias', 'model.vision_tower.transformer.resblocks.6.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.1.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.0.mlp.c_proj.bias', 'bio_encoder.encoder.layers.0.norm1.weight', 'model.vision_tower.visual.transformer.resblocks.3.ln_2.bias', 'model.vision_tower.transformer.resblocks.5.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.8.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.2.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.9.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.2.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.3.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.12.mlp.c_fc.weight', 'model.vision_tower.transformer.resblocks.7.attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.8.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.9.mlp.c_fc.bias', 'model.vision_tower.token_embedding.weight', 'model.vision_tower.visual.transformer.resblocks.16.mlp.c_fc.weight', 'model.vision_tower.text_projection', 'model.vision_tower.transformer.resblocks.4.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.17.attn.in_proj_weight', 'model.vision_tower.transformer.resblocks.6.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.8.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.14.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.5.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.10.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.15.attn.in_proj_weight', 'model.vision_tower.transformer.resblocks.11.mlp.c_fc.weight', 'model.vision_tower.ln_final.weight', 'model.vision_tower.visual.transformer.resblocks.5.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.6.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.15.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.15.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.11.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.12.attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.4.mlp.c_proj.weight', 'model.vision_tower.transformer.resblocks.1.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.5.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.9.attn.in_proj_weight', 'model.vision_tower.transformer.resblocks.1.mlp.c_proj.bias', 'model.vision_tower.transformer.resblocks.2.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.3.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.3.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.16.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.2.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.6.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.23.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.22.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.20.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.7.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.3.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.2.ln_2.weight', 'model.vision_tower.positional_embedding', 'model.vision_tower.transformer.resblocks.1.mlp.c_proj.weight', 'model.vision_tower.transformer.resblocks.9.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.2.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.21.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.17.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.8.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.12.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.1.mlp.c_proj.bias', 'model.vision_tower.transformer.resblocks.1.ln_1.weight', 'model.vision_tower.transformer.resblocks.6.attn.in_proj_weight', 'model.vision_tower.transformer.resblocks.3.ln_2.bias', 'model.vision_tower.transformer.resblocks.11.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.14.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.9.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.14.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.4.mlp.c_proj.weight', 'model.vision_tower.transformer.resblocks.0.attn.in_proj_weight', 'model.vision_tower.transformer.resblocks.9.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.12.attn.in_proj_weight', 'model.vision_tower.transformer.resblocks.3.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.2.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.20.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.14.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.15.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.5.ln_2.bias', 'model.vision_tower.transformer.resblocks.4.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.7.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.1.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.22.attn.out_proj.bias', 'model.vision_tower.visual.ln_pre.bias', 'model.vision_tower.transformer.resblocks.1.ln_2.weight', 'model.vision_tower.transformer.resblocks.2.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.10.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.3.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.12.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.21.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.20.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.14.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.23.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.5.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.11.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.12.ln_2.bias', 'model.vision_tower.transformer.resblocks.3.ln_1.bias', 'model.vision_tower.transformer.resblocks.5.ln_1.weight', 'model.vision_tower.transformer.resblocks.9.mlp.c_proj.bias', 'model.vision_tower.transformer.resblocks.2.ln_1.bias', 'bio_encoder.encoder.layers.0.norm2.bias', 'model.vision_tower.visual.transformer.resblocks.13.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.20.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.9.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.2.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.10.ln_1.weight', 'model.vision_tower.transformer.resblocks.0.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.1.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.18.attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.5.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.22.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.13.ln_1.weight', 'model.vision_tower.logit_scale', 'model.vision_tower.transformer.resblocks.2.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.13.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.16.mlp.c_fc.bias', 'model.vision_tower.transformer.resblocks.10.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.5.ln_1.bias', 'model.vision_tower.transformer.resblocks.7.mlp.c_fc.weight', 'model.vision_tower.transformer.resblocks.5.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.17.attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.11.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.10.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.23.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.5.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.22.mlp.c_fc.bias', 'model.vision_tower.transformer.resblocks.7.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.3.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.10.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.3.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.18.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.5.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.23.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.5.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.23.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.19.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.15.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.0.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.16.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.17.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.21.attn.out_proj.weight', 'bio_encoder.encoder.layers.0.self_attn.in_proj_weight', 'model.vision_tower.transformer.resblocks.0.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.6.ln_1.weight', 'model.vision_tower.transformer.resblocks.4.attn.in_proj_bias', 'bio_encoder.encoder.layers.0.linear1.bias', 'model.vision_tower.visual.transformer.resblocks.20.mlp.c_proj.bias', 'model.vision_tower.transformer.resblocks.9.attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.3.attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.10.mlp.c_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
> /shared/nas2/jk100/partonomy_private/src/models/PLUM/model/llava/model/llava_arch.py(60)initialize_vision_modules()
-> self.config.use_mm_proj = True
CLIP(
  (visual): VisionTransformer(
    (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
    (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (12): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (13): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (14): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (15): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (16): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (17): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (18): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (19): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (20): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (21): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (22): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (23): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (transformer): Transformer(
    (resblocks): Sequential(
      (0): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (6): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (7): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (8): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (9): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (10): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (11): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (token_embedding): Embedding(49408, 768)
  (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
*** TypeError: 'CLIP' object is not subscriptable
VisionTransformer(
  (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
  (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (transformer): Transformer(
    (resblocks): Sequential(
      (0): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (12): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (13): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (14): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (15): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (16): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (17): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (18): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (19): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (20): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (21): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (22): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (23): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
VisionTransformer(
  (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
  (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (transformer): Transformer(
    (resblocks): Sequential(
      (0): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (12): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (13): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (14): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (15): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (16): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (17): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (18): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (19): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (20): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (21): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (22): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (23): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
*** TypeError: 'VisionTransformer' object is not subscriptable
*** TypeError: 'VisionTransformer' object is not subscriptable
*** SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 908, in <module>
    main(sys.argv[1:])
  File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 284, in main
    model.get_model().initialize_vision_modules(model.get_model().config)
  File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/llava/model/llava_arch.py", line 60, in initialize_vision_modules
    self.config.use_mm_proj = True
  File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/llava/model/llava_arch.py", line 60, in initialize_vision_modules
    self.config.use_mm_proj = True
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
