
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")


Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:15<00:00,  5.04s/it]
Some weights of PLUMForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-llama-2-13b-chat-lightning-preview and are newly initialized: ['bio_encoder.encoder.layers.0.linear2.bias', 'bio_encoder.encoder.layers.0.linear1.weight', 'bio_encoder.encoder.layers.0.self_attn.in_proj_bias', 'bio_encoder.encoder.layers.0.self_attn.in_proj_weight', 'bio_encoder.encoder.layers.0.linear1.bias', 'bio_encoder.encoder.layers.0.norm1.bias', 'bio_encoder.encoder.layers.0.norm1.weight', 'bio_encoder.encoder.layers.0.norm2.bias', 'bio_encoder.encoder.layers.0.self_attn.out_proj.weight', 'bio_encoder.encoder.layers.0.linear2.weight', 'bio_encoder.encoder.layers.0.self_attn.out_proj.bias', 'bio_encoder.encoder.layers.0.norm2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 6,553,600 || all params: 14,151,568,689 || trainable%: 0.04631006034754371
>> model.config.train_mask_prompt_encoder:  True
n:  base_model.model.model.embed_tokens.weight p.shape:  torch.Size([32002, 5120])
n:  base_model.model.model.visual_model.prompt_encoder.point_embeddings.0.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.prompt_encoder.point_embeddings.1.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.prompt_encoder.point_embeddings.2.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.prompt_encoder.point_embeddings.3.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.prompt_encoder.not_a_point_embed.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.0.weight p.shape:  torch.Size([4, 1, 2, 2])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.0.bias p.shape:  torch.Size([4])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.1.weight p.shape:  torch.Size([4])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.1.bias p.shape:  torch.Size([4])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.3.weight p.shape:  torch.Size([16, 4, 2, 2])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.3.bias p.shape:  torch.Size([16])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.4.weight p.shape:  torch.Size([16])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.4.bias p.shape:  torch.Size([16])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.6.weight p.shape:  torch.Size([256, 16, 1, 1])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.6.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.prompt_encoder.no_mask_embed.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_token.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.mask_decoder.mask_tokens.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.weight p.shape:  torch.Size([256, 64, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.weight p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.weight p.shape:  torch.Size([64, 32, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.bias p.shape:  torch.Size([4])
n:  base_model.model.model.text_hidden_fcs.0.0.weight p.shape:  torch.Size([5120, 5120])
n:  base_model.model.model.text_hidden_fcs.0.0.bias p.shape:  torch.Size([5120])
n:  base_model.model.model.text_hidden_fcs.0.2.weight p.shape:  torch.Size([256, 5120])
n:  base_model.model.model.text_hidden_fcs.0.2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.token_to_mask_fcs.0.0.weight p.shape:  torch.Size([5120, 5120])
n:  base_model.model.model.token_to_mask_fcs.0.0.bias p.shape:  torch.Size([5120])
n:  base_model.model.model.token_to_mask_fcs.0.2.weight p.shape:  torch.Size([1, 5120])
n:  base_model.model.model.token_to_mask_fcs.0.2.bias p.shape:  torch.Size([1])
n:  base_model.model.lm_head.weight p.shape:  torch.Size([32002, 5120])
n:  base_model.model.bio_encoder.encoder.layers.0.self_attn.in_proj_weight p.shape:  torch.Size([15360, 5120])
n:  base_model.model.bio_encoder.encoder.layers.0.self_attn.in_proj_bias p.shape:  torch.Size([15360])
n:  base_model.model.bio_encoder.encoder.layers.0.self_attn.out_proj.weight p.shape:  torch.Size([5120, 5120])
n:  base_model.model.bio_encoder.encoder.layers.0.self_attn.out_proj.bias p.shape:  torch.Size([5120])
n:  base_model.model.bio_encoder.encoder.layers.0.linear1.weight p.shape:  torch.Size([2048, 5120])
n:  base_model.model.bio_encoder.encoder.layers.0.linear1.bias p.shape:  torch.Size([2048])
n:  base_model.model.bio_encoder.encoder.layers.0.linear2.weight p.shape:  torch.Size([5120, 2048])
n:  base_model.model.bio_encoder.encoder.layers.0.linear2.bias p.shape:  torch.Size([5120])
n:  base_model.model.bio_encoder.encoder.layers.0.norm1.weight p.shape:  torch.Size([5120])
n:  base_model.model.bio_encoder.encoder.layers.0.norm1.bias p.shape:  torch.Size([5120])
n:  base_model.model.bio_encoder.encoder.layers.0.norm2.weight p.shape:  torch.Size([5120])
n:  base_model.model.bio_encoder.encoder.layers.0.norm2.bias p.shape:  torch.Size([5120])
>> (plum_train_ds) loading ExplanatorySegDataset...
>> (plum_train_ds) question_types:  QuestionType.POSITIVE
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partonomy/partonomy_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partonomy/partonomy_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/pascal_part/pascal_part_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/pascal_part/pascal_part_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partimagenet/partimagenet_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partimagenet/partimagenet_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/paco_lvis/paco_lvis_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/paco_lvis/paco_lvis_qa_pairs.json
>> (plum_train_ds) question_types:  QuestionType.POSITIVE_WITH_LABEL
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partonomy/partonomy_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partonomy/partonomy_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/pascal_part/pascal_part_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/pascal_part/pascal_part_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partimagenet/partimagenet_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partimagenet/partimagenet_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/paco_lvis/paco_lvis_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/paco_lvis/paco_lvis_qa_pairs.json
>> (plum_train_ds) question_types:  QuestionType.NEGATIVE
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partonomy/partonomy_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partonomy/partonomy_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/pascal_part/pascal_part_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/pascal_part/pascal_part_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partimagenet/partimagenet_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partimagenet/partimagenet_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/paco_lvis/paco_lvis_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/paco_lvis/paco_lvis_qa_pairs.json
>> (plum_train_ds) question_types:  QuestionType.NEGATIVE_WITH_LABEL
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partonomy/partonomy_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partonomy/partonomy_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/pascal_part/pascal_part_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/pascal_part/pascal_part_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partimagenet/partimagenet_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partimagenet/partimagenet_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/paco_lvis/paco_lvis_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/paco_lvis/paco_lvis_qa_pairs.json
>> (plum_train_ds) question_types:  QuestionType.DIFFERENCE
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partonomy/partonomy_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partonomy/partonomy_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/pascal_part/pascal_part_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/pascal_part/pascal_part_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partimagenet/partimagenet_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partimagenet/partimagenet_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/paco_lvis/paco_lvis_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/paco_lvis/paco_lvis_qa_pairs.json
>> (plum_train_ds) question_types:  QuestionType.DIFFERENCE_WITH_LABEL
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partonomy/partonomy_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partonomy/partonomy_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/pascal_part/pascal_part_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/pascal_part/pascal_part_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partimagenet/partimagenet_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partimagenet/partimagenet_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/paco_lvis/paco_lvis_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/paco_lvis/paco_lvis_qa_pairs.json
ade20k:  20210
cocostuff:  118287
loading annotations into memory...
Done (t=0.21s)
creating index...
index created!
pascal_part:  4366
loading annotations into memory...
Done (t=10.48s)
creating index...
index created!
paco_lvis:  45790
mapillary:  18000
loading dataset refclef into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refclef/refs(unc).p
creating index...
index created.
DONE (t=8.47s)
dataset refclef (refs unc) (train split) has 17978 images and 99523 annotations.
loading dataset refcoco into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco/refs(unc).p
creating index...
index created.
DONE (t=3.62s)
dataset refcoco (refs unc) (train split) has 16994 images and 196771 annotations.
loading dataset refcoco+ into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco+/refs(unc).p
creating index...
index created.
DONE (t=3.72s)
dataset refcoco+ (refs unc) (train split) has 16992 images and 196737 annotations.
loading dataset refcocog into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcocog/refs(umd).p
creating index...
index created.
DONE (t=12.28s)
dataset refcocog (refs umd) (train split) has 21899 images and 208960 annotations.
vqa_data:  157712
number of reason_seg samples:  239
len(self.img_to_explanation):  239
Training with 40000 examples and validating with 200 examples.
[2025-04-12 14:41:17,582] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5, git-hash=unknown, git-branch=unknown
[2025-04-12 14:41:17,582] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2025-04-12 14:41:17,583] [INFO] [comm.py:594:init_distributed] cdb=None
[2025-04-12 14:41:17,583] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Using /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
[2025-04-12 14:42:05,050] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Detected CUDA files, patching ldflags
Emitting ninja build file /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module fused_adam...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/tensor/python_tensor.cpp:78.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
Time to load fused_adam op: 0.6182498931884766 seconds
[2025-04-12 14:42:05,856] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-12 14:42:06,042] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-12 14:42:06,042] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-12 14:42:06,043] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-04-12 14:42:06,043] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500000000
[2025-04-12 14:42:06,043] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500000000
[2025-04-12 14:42:06,043] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2025-04-12 14:42:06,043] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(517951026, False)]
[2025-04-12 14:42:19,799] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2025-04-12 14:42:19,800] [INFO] [utils.py:786:see_memory_usage] MA 28.49 GB         Max_MA 29.45 GB         CA 29.59 GB         Max_CA 30 GB
[2025-04-12 14:42:19,801] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 192.31 GB, percent = 19.1%
[2025-04-12 14:42:29,900] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2025-04-12 14:42:29,900] [INFO] [utils.py:786:see_memory_usage] MA 32.35 GB         Max_MA 34.28 GB         CA 35.38 GB         Max_CA 35 GB
[2025-04-12 14:42:29,901] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 192.27 GB, percent = 19.1%
[2025-04-12 14:42:29,901] [INFO] [stage_1_and_2.py:488:__init__] optimizer state initialized
[2025-04-12 14:42:38,693] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2025-04-12 14:42:38,694] [INFO] [utils.py:786:see_memory_usage] MA 32.35 GB         Max_MA 32.35 GB         CA 35.38 GB         Max_CA 35 GB
[2025-04-12 14:42:38,695] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 192.23 GB, percent = 19.1%
[2025-04-12 14:42:38,702] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-04-12 14:42:38,702] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2025-04-12 14:42:38,703] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7fb46d7793c0>
[2025-04-12 14:42:38,703] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
[2025-04-12 14:42:38,706] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2025-04-12 14:42:38,706] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-04-12 14:42:38,706] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-04-12 14:42:38,706] [INFO] [config.py:964:print]   amp_enabled .................. False
[2025-04-12 14:42:38,706] [INFO] [config.py:964:print]   amp_params ................... False
[2025-04-12 14:42:38,707] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-12 14:42:38,707] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2025-04-12 14:42:38,707] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2025-04-12 14:42:38,707] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2025-04-12 14:42:38,707] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2025-04-12 14:42:38,707] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fabf1d20160>
[2025-04-12 14:42:38,707] [INFO] [config.py:964:print]   communication_data_type ...... None
[2025-04-12 14:42:38,707] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-12 14:42:38,707] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2025-04-12 14:42:38,707] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2025-04-12 14:42:38,707] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-12 14:42:38,707] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2025-04-12 14:42:38,707] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2025-04-12 14:42:38,707] [INFO] [config.py:964:print]   disable_allgather ............ False
[2025-04-12 14:42:38,707] [INFO] [config.py:964:print]   dump_state ................... False
[2025-04-12 14:42:38,707] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2025-04-12 14:42:38,707] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2025-04-12 14:42:38,708] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-12 14:42:38,708] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-12 14:42:38,708] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2025-04-12 14:42:38,708] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2025-04-12 14:42:38,708] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2025-04-12 14:42:38,708] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2025-04-12 14:42:38,708] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2025-04-12 14:42:38,708] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2025-04-12 14:42:38,708] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-04-12 14:42:38,708] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2025-04-12 14:42:38,708] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2025-04-12 14:42:38,708] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2025-04-12 14:42:38,708] [INFO] [config.py:964:print]   global_rank .................. 0
[2025-04-12 14:42:38,708] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2025-04-12 14:42:38,708] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 10
[2025-04-12 14:42:38,708] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0
[2025-04-12 14:42:38,708] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2025-04-12 14:42:38,709] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-12 14:42:38,709] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2025-04-12 14:42:38,709] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2025-04-12 14:42:38,709] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2025-04-12 14:42:38,709] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2025-04-12 14:42:38,709] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2025-04-12 14:42:38,709] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2025-04-12 14:42:38,709] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-04-12 14:42:38,709] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-04-12 14:42:38,709] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2025-04-12 14:42:38,709] [INFO] [config.py:964:print]   optimizer_name ............... adamw
[2025-04-12 14:42:38,709] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.0003, 'weight_decay': 0.0, 'betas': (0.9, 0.95)}
[2025-04-12 14:42:38,709] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2025-04-12 14:42:38,709] [INFO] [config.py:964:print]   pld_enabled .................. False
[2025-04-12 14:42:38,709] [INFO] [config.py:964:print]   pld_params ................... False
[2025-04-12 14:42:38,709] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2025-04-12 14:42:38,709] [INFO] [config.py:964:print]   scheduler_name ............... WarmupDecayLR
[2025-04-12 14:42:38,709] [INFO] [config.py:964:print]   scheduler_params ............. {'total_num_steps': 25000, 'warmup_min_lr': 0, 'warmup_max_lr': 0.0003, 'warmup_num_steps': 100, 'warmup_type': 'linear'}
[2025-04-12 14:42:38,710] [INFO] [config.py:964:print]   sparse_attention ............. None
[2025-04-12 14:42:38,710] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2025-04-12 14:42:38,710] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2025-04-12 14:42:38,710] [INFO] [config.py:964:print]   train_batch_size ............. 80
[2025-04-12 14:42:38,710] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  8
[2025-04-12 14:42:38,710] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2025-04-12 14:42:38,710] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2025-04-12 14:42:38,710] [INFO] [config.py:964:print]   world_size ................... 1
[2025-04-12 14:42:38,710] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2025-04-12 14:42:38,710] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2025-04-12 14:42:38,710] [INFO] [config.py:964:print]   zero_enabled ................. True
[2025-04-12 14:42:38,710] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-12 14:42:38,710] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2025-04-12 14:42:38,710] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 8,
    "gradient_accumulation_steps": 10,
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 0.0003,
            "weight_decay": 0.0,
            "betas": [0.9, 0.95]
        }
    },
    "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "total_num_steps": 2.500000e+04,
            "warmup_min_lr": 0,
            "warmup_max_lr": 0.0003,
            "warmup_num_steps": 100,
            "warmup_type": "linear"
        }
    },
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "gradient_clipping": 1.0,
    "zero_optimization": {
        "stage": 2,
        "contiguous_gradients": true,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 5.000000e+08,
        "allgather_bucket_size": 5.000000e+08
    }
}
(train) >> AFTER DEEPSPEED
>> (train) Auto-resume from:  ./runs/plum-13b_kld_0_dice_6_v1_partonomy_bidirbio_768/plum-13b_kld_0_dice_6_v1_partonomy_bidirbio_768_accum_10_maxlen512_epochs50_segloss_2_bce_loss_2_kld_loss_0_dice_loss_6_bidir_bio_exp_seg_train_prompt_enc_binary_span_ckpt_model
>> (train) resume exists:  False
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)
  return F.conv2d(input, weight, bias, self.stride,
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:654: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  weight=torch.tensor(pos_weight_val, device=logits_bin.device),
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py:952: UserWarning: Plan failed with a CuDNNError: cuDNN error: CUDNN_STATUS_BAD_PARAM
Exception raised from run_conv_plan at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:374 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fb73f956897 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe1640b (0x7fb6de9fb40b in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x108f133 (0x7fb6dec74133 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x1091043 (0x7fb6dec76043 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x109176b (0x7fb6dec7676b in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x1075c7d (0x7fb6dec5ac7d in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x107656a (0x7fb6dec5b56a in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #7: at::native::cudnn_convolution_transpose(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, bool) + 0xa4 (0x7fb6dec5b714 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #8: <unknown function> + 0x32f2cc2 (0x7fb6e0ed7cc2 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #9: <unknown function> + 0x32ff147 (0x7fb6e0ee4147 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #10: at::_ops::cudnn_convolution_transpose::call(at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool) + 0x2fb (0x7fb73383c2fb in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #11: at::native::_convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool, bool) + 0x166d (0x7fb732f6348d in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #12: <unknown function> + 0x2a8d27f (0x7fb7340f127f in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #13: <unknown function> + 0x2a93bdc (0x7fb7340f7bdc in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #14: at::_ops::_convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool, bool) + 0x344 (0x7fb73383a0d4 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #15: at::native::convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long) + 0x3b8 (0x7fb732f56868 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x2a8cb1c (0x7fb7340f0b1c in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x2a93a48 (0x7fb7340f7a48 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #18: at::_ops::convolution::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt) + 0x17b (0x7fb7337f7d6b in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0x41902e1 (0x7fb7357f42e1 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #20: <unknown function> + 0x4191259 (0x7fb7357f5259 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #21: at::_ops::convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt) + 0x2d4 (0x7fb733838ed4 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #22: <unknown function> + 0x164a2e0 (0x7fb732cae2e0 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #23: at::native::conv_transpose2d_symint(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, c10::ArrayRef<c10::SymInt>) + 0x125 (0x7fb732f5bb05 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #24: <unknown function> + 0x2c879c9 (0x7fb7342eb9c9 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #25: <unknown function> + 0x2c87b03 (0x7fb7342ebb03 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #26: at::_ops::conv_transpose2d_input::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, c10::ArrayRef<c10::SymInt>) + 0x2cb (0x7fb733b9509b in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #27: <unknown function> + 0x61476f (0x7fb73e61c76f in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #28: /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python() [0x4fc697]
frame #29: _PyObject_MakeTpCall + 0x25b (0x4f614b in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x53d6 (0x4f2376 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #31: /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python() [0x508006]
frame #32: _PyEval_EvalFrameDefault + 0x2b79 (0x4efb19 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #33: /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python() [0x508006]
frame #34: _PyEval_EvalFrameDefault + 0x2b79 (0x4efb19 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #35: _PyObject_FastCallDictTstate + 0xcd (0x4f561d in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #36: _PyObject_Call_Prepend + 0x66 (0x506596 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #37: /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python() [0x5cc323]
frame #38: _PyObject_MakeTpCall + 0x25b (0x4f614b in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #39: _PyEval_EvalFrameDefault + 0x4dde (0x4f1d7e in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #40: /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python() [0x508006]
frame #41: _PyEval_EvalFrameDefault + 0x2b79 (0x4efb19 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #42: /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python() [0x508006]
frame #43: _PyEval_EvalFrameDefault + 0x2b79 (0x4efb19 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #44: _PyObject_FastCallDictTstate + 0xcd (0x4f561d in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #45: _PyObject_Call_Prepend + 0x66 (0x506596 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #46: /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python() [0x5cc323]
frame #47: _PyObject_MakeTpCall + 0x25b (0x4f614b in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #48: _PyEval_EvalFrameDefault + 0x53d6 (0x4f2376 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #49: /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python() [0x507eae]
frame #50: _PyEval_EvalFrameDefault + 0x13b3 (0x4ee353 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #51: /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python() [0x507eae]
frame #52: PyObject_Call + 0xb8 (0x508858 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #53: _PyEval_EvalFrameDefault + 0x2b79 (0x4efb19 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #54: /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python() [0x507eae]
frame #55: PyObject_Call + 0xb8 (0x508858 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #56: _PyEval_EvalFrameDefault + 0x2b79 (0x4efb19 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #57: _PyFunction_Vectorcall + 0x6f (0x4fcadf in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #58: _PyObject_FastCallDictTstate + 0x17d (0x4f56cd in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #59: _PyObject_Call_Prepend + 0x66 (0x506596 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #60: /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python() [0x5cc323]
frame #61: _PyObject_MakeTpCall + 0x25b (0x4f614b in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #62: _PyEval_EvalFrameDefault + 0x5757 (0x4f26f7 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #63: /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python() [0x507eae]
 (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:921.)
  return F.conv_transpose2d(
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py:952: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv_transpose2d(
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a CuDNNError: cuDNN error: CUDNN_STATUS_BAD_PARAM
Exception raised from run_conv_plan at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:374 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fb73f956897 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe1640b (0x7fb6de9fb40b in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x108f133 (0x7fb6dec74133 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x1091043 (0x7fb6dec76043 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x10915bb (0x7fb6dec765bb in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x1074062 (0x7fb6dec59062 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x107527f (0x7fb6dec5a27f in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0x1078184 (0x7fb6dec5d184 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #8: at::native::convolution_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::OptionalArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, std::array<bool, 3ul>) + 0x1872 (0x7fb732f661e2 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #9: <unknown function> + 0x32f8375 (0x7fb6e0edd375 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #10: <unknown function> + 0x3300422 (0x7fb6e0ee5422 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #11: at::_ops::convolution_backward::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::OptionalArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, std::array<bool, 3ul>) + 0x26b (0x7fb733e3510b in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #12: <unknown function> + 0x45578f3 (0x7fb735bbb8f3 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #13: <unknown function> + 0x45594a3 (0x7fb735bbd4a3 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #14: at::_ops::convolution_backward::call(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::OptionalArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, std::array<bool, 3ul>) + 0x3a3 (0x7fb733e5b7c3 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #15: <unknown function> + 0x164a4b1 (0x7fb732cae4b1 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #16: torch::autograd::generated::ConvolutionBackward0::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x300 (0x7fb7356cd290 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4dcc3ab (0x7fb7364303ab in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #18: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1526 (0x7fb73642a4a6 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #19: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x698 (0x7fb73642b108 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #20: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x13f (0x7fb73642203f in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #21: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x5c (0x7fb73e830e1c in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #22: <unknown function> + 0xdbbf4 (0x7fb752bf4bf4 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/../../libstdc++.so.6)
frame #23: <unknown function> + 0x94ac3 (0x7fb75415dac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #24: <unknown function> + 0x126850 (0x7fb7541ef850 in /lib/x86_64-linux-gnu/libc.so.6)
 (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:921.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch: [0][  1/500]	Time 141.590 (141.590)	Loss 2.3966 (2.4119)	CeLoss 0.2656 (0.2754)	SegCLSLoss 0.7361 (0.7358)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1443 (0.1519)	MaskBCELoss 0.0517 (0.0616)	MaskDICELoss 0.0926 (0.0902)
Epoch: [0][  2/500]	Time 125.760 (125.760)	Loss 2.4936 (2.4481)	CeLoss 0.3379 (0.3045)	SegCLSLoss 0.7408 (0.7355)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1472 (0.1511)	MaskBCELoss 0.0522 (0.0585)	MaskDICELoss 0.0950 (0.0926)
Epoch: [0][  3/500]	Time 128.437 (128.437)	Loss 2.4259 (2.4099)	CeLoss 0.2695 (0.2806)	SegCLSLoss 0.7514 (0.7387)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1455 (0.1443)	MaskBCELoss 0.0550 (0.0534)	MaskDICELoss 0.0905 (0.0908)
Epoch: [0][  4/500]	Time 118.399 (118.399)	Loss 2.4568 (2.4284)	CeLoss 0.3223 (0.2941)	SegCLSLoss 0.7444 (0.7392)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1427 (0.1476)	MaskBCELoss 0.0525 (0.0574)	MaskDICELoss 0.0902 (0.0902)
Epoch: [0][  5/500]	Time 144.831 (144.831)	Loss 2.4246 (2.4273)	CeLoss 0.2949 (0.2953)	SegCLSLoss 0.7524 (0.7372)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1422 (0.1488)	MaskBCELoss 0.0571 (0.0590)	MaskDICELoss 0.0850 (0.0899)
Epoch: [0][  6/500]	Time 128.182 (128.182)	Loss 2.4181 (2.3982)	CeLoss 0.2871 (0.2698)	SegCLSLoss 0.7458 (0.7396)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1350 (0.1472)	MaskBCELoss 0.0428 (0.0585)	MaskDICELoss 0.0922 (0.0887)
Epoch: [0][  7/500]	Time 111.604 (111.604)	Loss 2.4416 (2.4266)	CeLoss 0.3027 (0.3043)	SegCLSLoss 0.7497 (0.7374)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1472 (0.1484)	MaskBCELoss 0.0610 (0.0606)	MaskDICELoss 0.0862 (0.0878)
Epoch: [0][  8/500]	Time 132.814 (132.814)	Loss 2.4695 (2.4353)	CeLoss 0.2969 (0.2863)	SegCLSLoss 0.7368 (0.7392)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1717 (0.1547)	MaskBCELoss 0.0827 (0.0644)	MaskDICELoss 0.0889 (0.0903)
Epoch: [0][  9/500]	Time 135.244 (135.244)	Loss 2.3845 (2.4230)	CeLoss 0.2539 (0.2787)	SegCLSLoss 0.7521 (0.7396)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1345 (0.1497)	MaskBCELoss 0.0450 (0.0582)	MaskDICELoss 0.0895 (0.0915)
Epoch: [0][ 10/500]	Time 117.785 (117.785)	Loss 2.4403 (2.4628)	CeLoss 0.2773 (0.3041)	SegCLSLoss 0.7254 (0.7326)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1630 (0.1661)	MaskBCELoss 0.0667 (0.0758)	MaskDICELoss 0.0963 (0.0902)
Epoch: [0][ 11/500]	Time 128.870 (128.870)	Loss 1.7249 (1.7067)	CeLoss 0.3105 (0.2936)	SegCLSLoss 0.4040 (0.3893)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1265 (0.1394)	MaskBCELoss 0.0383 (0.0505)	MaskDICELoss 0.0882 (0.0889)
Epoch: [0][ 12/500]	Time 152.895 (152.895)	Loss 1.8367 (1.7162)	CeLoss 0.3477 (0.2822)	SegCLSLoss 0.3835 (0.3900)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1729 (0.1478)	MaskBCELoss 0.0790 (0.0583)	MaskDICELoss 0.0939 (0.0895)
Epoch: [0][ 13/500]	Time 116.773 (116.773)	Loss 1.6687 (1.6970)	CeLoss 0.2393 (0.2719)	SegCLSLoss 0.4012 (0.3906)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1369 (0.1387)	MaskBCELoss 0.0486 (0.0472)	MaskDICELoss 0.0883 (0.0916)
Epoch: [0][ 14/500]	Time 140.230 (140.230)	Loss 1.7190 (1.6955)	CeLoss 0.2773 (0.2869)	SegCLSLoss 0.3919 (0.3901)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1384 (0.1344)	MaskBCELoss 0.0433 (0.0444)	MaskDICELoss 0.0951 (0.0899)
Epoch: [0][ 15/500]	Time 127.885 (127.885)	Loss 1.6862 (1.7008)	CeLoss 0.2480 (0.2865)	SegCLSLoss 0.3983 (0.3900)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1406 (0.1393)	MaskBCELoss 0.0506 (0.0504)	MaskDICELoss 0.0900 (0.0889)
Epoch: [0][ 16/500]	Time 114.453 (114.453)	Loss 1.7019 (1.6948)	CeLoss 0.3008 (0.2803)	SegCLSLoss 0.3723 (0.3891)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1414 (0.1427)	MaskBCELoss 0.0481 (0.0550)	MaskDICELoss 0.0933 (0.0877)
Epoch: [0][ 17/500]	Time 64.223 (64.223)	Loss 1.6972 (1.6973)	CeLoss 0.2754 (0.2873)	SegCLSLoss 0.3892 (0.3897)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1474 (0.1374)	MaskBCELoss 0.0601 (0.0483)	MaskDICELoss 0.0873 (0.0890)
Epoch: [0][ 18/500]	Time 66.370 (66.370)	Loss 1.7275 (1.7531)	CeLoss 0.3086 (0.2918)	SegCLSLoss 0.3822 (0.3857)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1419 (0.1606)	MaskBCELoss 0.0491 (0.0685)	MaskDICELoss 0.0929 (0.0921)
Epoch: [0][ 19/500]	Time 66.687 (66.687)	Loss 1.7509 (1.6935)	CeLoss 0.3379 (0.2787)	SegCLSLoss 0.3848 (0.3921)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1319 (0.1371)	MaskBCELoss 0.0369 (0.0480)	MaskDICELoss 0.0950 (0.0891)
Epoch: [0][ 20/500]	Time 66.107 (66.107)	Loss 1.7233 (1.6785)	CeLoss 0.3281 (0.2758)	SegCLSLoss 0.3731 (0.3896)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1402 (0.1340)	MaskBCELoss 0.0480 (0.0452)	MaskDICELoss 0.0921 (0.0888)
Epoch: [0][ 21/500]	Time 71.047 (71.047)	Loss 1.0598 (1.0317)	CeLoss 0.2852 (0.2590)	SegCLSLoss 0.0735 (0.0680)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1236 (0.1340)	MaskBCELoss 0.0283 (0.0418)	MaskDICELoss 0.0953 (0.0922)
Epoch: [0][ 22/500]	Time 66.367 (66.367)	Loss 1.0654 (0.9918)	CeLoss 0.3105 (0.2562)	SegCLSLoss 0.0493 (0.0677)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1432 (0.1257)	MaskBCELoss 0.0508 (0.0385)	MaskDICELoss 0.0924 (0.0872)
Epoch: [0][ 23/500]	Time 70.979 (70.979)	Loss 1.0905 (1.0128)	CeLoss 0.3027 (0.2623)	SegCLSLoss 0.0567 (0.0657)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1448 (0.1318)	MaskBCELoss 0.0487 (0.0429)	MaskDICELoss 0.0961 (0.0889)
Epoch: [0][ 24/500]	Time 90.169 (90.169)	Loss 1.0101 (1.0226)	CeLoss 0.2422 (0.2642)	SegCLSLoss 0.0767 (0.0653)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1273 (0.1313)	MaskBCELoss 0.0373 (0.0400)	MaskDICELoss 0.0900 (0.0913)
Epoch: [0][ 25/500]	Time 73.555 (73.555)	Loss 1.1551 (1.0342)	CeLoss 0.3906 (0.2866)	SegCLSLoss 0.0494 (0.0602)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1362 (0.1304)	MaskBCELoss 0.0379 (0.0388)	MaskDICELoss 0.0983 (0.0916)
Epoch: [0][ 26/500]	Time 58.631 (58.631)	Loss 1.0010 (1.0276)	CeLoss 0.2715 (0.2685)	SegCLSLoss 0.0572 (0.0668)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1278 (0.1282)	MaskBCELoss 0.0380 (0.0359)	MaskDICELoss 0.0898 (0.0923)
Epoch: [0][ 27/500]	Time 71.886 (71.886)	Loss 1.0302 (1.0252)	CeLoss 0.3047 (0.2734)	SegCLSLoss 0.0569 (0.0635)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1195 (0.1274)	MaskBCELoss 0.0263 (0.0349)	MaskDICELoss 0.0932 (0.0925)
Epoch: [0][ 28/500]	Time 64.548 (64.548)	Loss 1.0732 (1.0120)	CeLoss 0.2676 (0.2495)	SegCLSLoss 0.0647 (0.0714)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1538 (0.1311)	MaskBCELoss 0.0615 (0.0417)	MaskDICELoss 0.0923 (0.0894)
Epoch: [0][ 29/500]	Time 69.479 (69.479)	Loss 0.9689 (1.0194)	CeLoss 0.2852 (0.2607)	SegCLSLoss 0.0546 (0.0646)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1189 (0.1356)	MaskBCELoss 0.0345 (0.0461)	MaskDICELoss 0.0844 (0.0895)
Epoch: [0][ 30/500]	Time 71.513 (71.513)	Loss 1.0395 (0.9959)	CeLoss 0.2266 (0.2713)	SegCLSLoss 0.0887 (0.0594)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1334 (0.1252)	MaskBCELoss 0.0412 (0.0364)	MaskDICELoss 0.0922 (0.0888)
Epoch: [0][ 31/500]	Time 71.688 (71.688)	Loss 0.9919 (1.0754)	CeLoss 0.2285 (0.2246)	SegCLSLoss 0.1055 (0.1319)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1139 (0.1198)	MaskBCELoss 0.0327 (0.0329)	MaskDICELoss 0.0812 (0.0868)
Epoch: [0][ 32/500]	Time 69.986 (69.986)	Loss 1.1001 (1.0687)	CeLoss 0.2578 (0.2457)	SegCLSLoss 0.1092 (0.1154)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1271 (0.1243)	MaskBCELoss 0.0347 (0.0384)	MaskDICELoss 0.0924 (0.0859)
Epoch: [0][ 33/500]	Time 69.235 (69.235)	Loss 1.0432 (1.0882)	CeLoss 0.2441 (0.2402)	SegCLSLoss 0.1142 (0.1331)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1207 (0.1173)	MaskBCELoss 0.0383 (0.0305)	MaskDICELoss 0.0824 (0.0868)
Epoch: [0][ 34/500]	Time 65.390 (65.390)	Loss 0.9979 (1.0343)	CeLoss 0.2314 (0.2576)	SegCLSLoss 0.1059 (0.1018)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1142 (0.1175)	MaskBCELoss 0.0325 (0.0329)	MaskDICELoss 0.0816 (0.0846)
Epoch: [0][ 35/500]	Time 62.422 (62.422)	Loss 1.0645 (1.0614)	CeLoss 0.2158 (0.2638)	SegCLSLoss 0.1494 (0.1071)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1174 (0.1211)	MaskBCELoss 0.0386 (0.0357)	MaskDICELoss 0.0788 (0.0853)
Epoch: [0][ 36/500]	Time 61.623 (61.623)	Loss 1.0943 (1.0726)	CeLoss 0.2051 (0.2492)	SegCLSLoss 0.1685 (0.1212)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1116 (0.1184)	MaskBCELoss 0.0292 (0.0324)	MaskDICELoss 0.0824 (0.0860)
Epoch: [0][ 37/500]	Time 63.183 (63.183)	Loss 1.0260 (1.0740)	CeLoss 0.2949 (0.2651)	SegCLSLoss 0.0873 (0.1107)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1152 (0.1189)	MaskBCELoss 0.0338 (0.0315)	MaskDICELoss 0.0814 (0.0874)
Epoch: [0][ 38/500]	Time 67.314 (67.314)	Loss 1.0189 (1.0685)	CeLoss 0.2578 (0.2277)	SegCLSLoss 0.0855 (0.1311)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1303 (0.1196)	MaskBCELoss 0.0479 (0.0347)	MaskDICELoss 0.0824 (0.0849)
Epoch: [0][ 39/500]	Time 64.594 (64.594)	Loss 1.1328 (1.0740)	CeLoss 0.1982 (0.2564)	SegCLSLoss 0.1782 (0.1089)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1150 (0.1261)	MaskBCELoss 0.0280 (0.0393)	MaskDICELoss 0.0870 (0.0869)
Epoch: [0][ 40/500]	Time 68.496 (68.496)	Loss 1.0936 (1.0484)	CeLoss 0.2441 (0.2437)	SegCLSLoss 0.1277 (0.1139)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1105 (0.1183)	MaskBCELoss 0.0170 (0.0332)	MaskDICELoss 0.0934 (0.0851)
Epoch: [0][ 41/500]	Time 66.459 (66.459)	Loss 1.0815 (1.1463)	CeLoss 0.1973 (0.1995)	SegCLSLoss 0.1366 (0.1761)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1328 (0.1277)	MaskBCELoss 0.0464 (0.0429)	MaskDICELoss 0.0864 (0.0848)
Epoch: [0][ 42/500]	Time 62.364 (62.364)	Loss 1.0577 (1.1157)	CeLoss 0.2061 (0.2157)	SegCLSLoss 0.1315 (0.1514)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1275 (0.1249)	MaskBCELoss 0.0441 (0.0381)	MaskDICELoss 0.0834 (0.0868)
Epoch: [0][ 43/500]	Time 74.498 (74.498)	Loss 1.1632 (1.1420)	CeLoss 0.1963 (0.1900)	SegCLSLoss 0.1838 (0.1766)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1272 (0.1297)	MaskBCELoss 0.0410 (0.0449)	MaskDICELoss 0.0863 (0.0848)
Epoch: [0][ 44/500]	Time 68.208 (68.208)	Loss 1.1189 (1.1209)	CeLoss 0.1963 (0.1998)	SegCLSLoss 0.1620 (0.1598)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1274 (0.1287)	MaskBCELoss 0.0414 (0.0426)	MaskDICELoss 0.0860 (0.0861)
Epoch: [0][ 45/500]	Time 60.479 (60.479)	Loss 1.0777 (1.1294)	CeLoss 0.1973 (0.2068)	SegCLSLoss 0.1508 (0.1634)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1319 (0.1254)	MaskBCELoss 0.0532 (0.0393)	MaskDICELoss 0.0787 (0.0862)
Epoch: [0][ 46/500]	Time 69.542 (69.542)	Loss 1.0617 (1.1126)	CeLoss 0.1885 (0.1974)	SegCLSLoss 0.1322 (0.1566)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1337 (0.1301)	MaskBCELoss 0.0483 (0.0447)	MaskDICELoss 0.0854 (0.0854)
Epoch: [0][ 47/500]	Time 73.065 (73.065)	Loss 1.1375 (1.1015)	CeLoss 0.2129 (0.2124)	SegCLSLoss 0.1587 (0.1451)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1322 (0.1301)	MaskBCELoss 0.0464 (0.0455)	MaskDICELoss 0.0858 (0.0847)
Epoch: [0][ 48/500]	Time 66.855 (66.855)	Loss 1.0331 (1.1073)	CeLoss 0.2324 (0.2136)	SegCLSLoss 0.1018 (0.1479)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1240 (0.1270)	MaskBCELoss 0.0369 (0.0410)	MaskDICELoss 0.0872 (0.0860)
Epoch: [0][ 49/500]	Time 63.228 (63.228)	Loss 1.2296 (1.1145)	CeLoss 0.1797 (0.2100)	SegCLSLoss 0.2190 (0.1513)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1292 (0.1275)	MaskBCELoss 0.0409 (0.0408)	MaskDICELoss 0.0884 (0.0867)
Epoch: [0][ 50/500]	Time 72.504 (72.504)	Loss 1.0732 (1.1076)	CeLoss 0.1973 (0.2007)	SegCLSLoss 0.1534 (0.1541)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1284 (0.1306)	MaskBCELoss 0.0502 (0.0463)	MaskDICELoss 0.0782 (0.0844)
Epoch: [0][ 51/500]	Time 64.663 (64.663)	Loss 1.0601 (1.0782)	CeLoss 0.1777 (0.1544)	SegCLSLoss 0.1519 (0.1716)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1148 (0.1227)	MaskBCELoss 0.0277 (0.0390)	MaskDICELoss 0.0871 (0.0838)
Epoch: [0][ 52/500]	Time 69.834 (69.834)	Loss 0.9999 (1.0361)	CeLoss 0.1504 (0.1662)	SegCLSLoss 0.1420 (0.1491)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1258 (0.1220)	MaskBCELoss 0.0472 (0.0400)	MaskDICELoss 0.0786 (0.0820)
Epoch: [0][ 53/500]	Time 65.201 (65.201)	Loss 0.9719 (1.0459)	CeLoss 0.1484 (0.1553)	SegCLSLoss 0.1361 (0.1584)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1200 (0.1226)	MaskBCELoss 0.0422 (0.0405)	MaskDICELoss 0.0778 (0.0821)
Epoch: [0][ 54/500]	Time 68.367 (68.367)	Loss 0.9672 (1.0362)	CeLoss 0.2109 (0.1668)	SegCLSLoss 0.0932 (0.1492)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1157 (0.1220)	MaskBCELoss 0.0310 (0.0403)	MaskDICELoss 0.0847 (0.0817)
Epoch: [0][ 55/500]	Time 66.046 (66.046)	Loss 0.9416 (1.0535)	CeLoss 0.2051 (0.1593)	SegCLSLoss 0.0951 (0.1604)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1159 (0.1231)	MaskBCELoss 0.0372 (0.0413)	MaskDICELoss 0.0787 (0.0818)
Epoch: [0][ 56/500]	Time 72.371 (72.371)	Loss 1.0329 (1.0259)	CeLoss 0.1660 (0.1564)	SegCLSLoss 0.1430 (0.1470)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1264 (0.1249)	MaskBCELoss 0.0445 (0.0433)	MaskDICELoss 0.0819 (0.0815)
Epoch: [0][ 57/500]	Time 63.141 (63.141)	Loss 1.0829 (1.0714)	CeLoss 0.1582 (0.1581)	SegCLSLoss 0.1752 (0.1667)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1193 (0.1241)	MaskBCELoss 0.0352 (0.0412)	MaskDICELoss 0.0841 (0.0829)
Epoch: [0][ 58/500]	Time 70.950 (70.950)	Loss 0.9847 (1.0358)	CeLoss 0.1777 (0.1608)	SegCLSLoss 0.1112 (0.1484)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1281 (0.1242)	MaskBCELoss 0.0461 (0.0417)	MaskDICELoss 0.0820 (0.0824)
Epoch: [0][ 59/500]	Time 72.767 (72.767)	Loss 1.0079 (1.0308)	CeLoss 0.1543 (0.1618)	SegCLSLoss 0.1335 (0.1490)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1350 (0.1226)	MaskBCELoss 0.0559 (0.0412)	MaskDICELoss 0.0791 (0.0814)
Epoch: [0][ 60/500]	Time 63.216 (63.216)	Loss 1.1434 (1.0252)	CeLoss 0.1504 (0.1738)	SegCLSLoss 0.2161 (0.1430)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1220 (0.1206)	MaskBCELoss 0.0430 (0.0396)	MaskDICELoss 0.0791 (0.0810)
Epoch: [0][ 61/500]	Time 71.157 (71.157)	Loss 0.9100 (0.8419)	CeLoss 0.1201 (0.1103)	SegCLSLoss 0.1073 (0.0880)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1161 (0.1140)	MaskBCELoss 0.0304 (0.0322)	MaskDICELoss 0.0857 (0.0819)
Epoch: [0][ 62/500]	Time 71.573 (71.573)	Loss 0.8467 (0.8300)	CeLoss 0.1030 (0.1123)	SegCLSLoss 0.0888 (0.0890)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1133 (0.1091)	MaskBCELoss 0.0285 (0.0287)	MaskDICELoss 0.0848 (0.0804)
Epoch: [0][ 63/500]	Time 62.728 (62.728)	Loss 0.8516 (0.8548)	CeLoss 0.1318 (0.1140)	SegCLSLoss 0.0809 (0.0911)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1093 (0.1116)	MaskBCELoss 0.0244 (0.0277)	MaskDICELoss 0.0848 (0.0838)
Epoch: [0][ 64/500]	Time 67.506 (67.506)	Loss 0.9017 (0.8499)	CeLoss 0.1001 (0.1125)	SegCLSLoss 0.1153 (0.0923)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1187 (0.1129)	MaskBCELoss 0.0353 (0.0311)	MaskDICELoss 0.0834 (0.0818)
Epoch: [0][ 65/500]	Time 72.320 (72.320)	Loss 0.8627 (0.8501)	CeLoss 0.1108 (0.1130)	SegCLSLoss 0.0978 (0.0912)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1142 (0.1131)	MaskBCELoss 0.0322 (0.0309)	MaskDICELoss 0.0820 (0.0821)
Epoch: [0][ 66/500]	Time 65.581 (65.581)	Loss 0.8467 (0.8433)	CeLoss 0.1289 (0.1151)	SegCLSLoss 0.0827 (0.0870)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1069 (0.1117)	MaskBCELoss 0.0223 (0.0290)	MaskDICELoss 0.0846 (0.0827)
Epoch: [0][ 67/500]	Time 71.746 (71.746)	Loss 0.8579 (0.8490)	CeLoss 0.1030 (0.1121)	SegCLSLoss 0.0877 (0.0861)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1153 (0.1148)	MaskBCELoss 0.0281 (0.0310)	MaskDICELoss 0.0872 (0.0838)
Epoch: [0][ 68/500]	Time 66.099 (66.099)	Loss 0.8929 (0.8501)	CeLoss 0.0967 (0.1071)	SegCLSLoss 0.1232 (0.0977)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1160 (0.1107)	MaskBCELoss 0.0365 (0.0292)	MaskDICELoss 0.0795 (0.0815)
Epoch: [0][ 69/500]	Time 72.971 (72.971)	Loss 0.8144 (0.8497)	CeLoss 0.1235 (0.1070)	SegCLSLoss 0.0753 (0.0989)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1096 (0.1107)	MaskBCELoss 0.0293 (0.0298)	MaskDICELoss 0.0803 (0.0809)
Epoch: [0][ 70/500]	Time 71.603 (71.603)	Loss 0.8568 (0.8404)	CeLoss 0.1006 (0.1127)	SegCLSLoss 0.0955 (0.0891)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1193 (0.1117)	MaskBCELoss 0.0378 (0.0301)	MaskDICELoss 0.0816 (0.0816)
Epoch: [0][ 71/500]	Time 66.300 (66.300)	Loss 2.7493 (2.6370)	CeLoss 0.0674 (0.0708)	SegCLSLoss 1.0474 (1.0293)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1287 (0.1054)	MaskBCELoss 0.0462 (0.0312)	MaskDICELoss 0.0825 (0.0742)
Epoch: [0][ 72/500]	Time 62.966 (62.966)	Loss 2.5406 (2.5831)	CeLoss 0.0820 (0.0738)	SegCLSLoss 1.0079 (1.0123)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0848 (0.1005)	MaskBCELoss 0.0165 (0.0296)	MaskDICELoss 0.0683 (0.0709)
Epoch: [0][ 73/500]	Time 69.112 (69.112)	Loss 2.6229 (2.5889)	CeLoss 0.0579 (0.0682)	SegCLSLoss 1.0316 (1.0121)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1040 (0.1024)	MaskBCELoss 0.0305 (0.0295)	MaskDICELoss 0.0735 (0.0729)
Epoch: [0][ 74/500]	Time 70.100 (70.100)	Loss 2.6831 (2.6217)	CeLoss 0.0552 (0.0678)	SegCLSLoss 1.0559 (1.0257)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1106 (0.1056)	MaskBCELoss 0.0368 (0.0328)	MaskDICELoss 0.0738 (0.0728)
Epoch: [0][ 75/500]	Time 66.310 (66.310)	Loss 2.5647 (2.6477)	CeLoss 0.0613 (0.0692)	SegCLSLoss 1.0123 (1.0304)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0982 (0.1084)	MaskBCELoss 0.0276 (0.0332)	MaskDICELoss 0.0706 (0.0752)
Epoch: [0][ 76/500]	Time 70.426 (70.426)	Loss 2.5862 (2.6283)	CeLoss 0.0908 (0.0701)	SegCLSLoss 1.0061 (1.0228)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1030 (0.1093)	MaskBCELoss 0.0337 (0.0359)	MaskDICELoss 0.0693 (0.0735)
Epoch: [0][ 77/500]	Time 59.605 (59.605)	Loss 2.5737 (2.5848)	CeLoss 0.0762 (0.0702)	SegCLSLoss 0.9839 (1.0050)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1115 (0.1067)	MaskBCELoss 0.0348 (0.0339)	MaskDICELoss 0.0767 (0.0728)
Epoch: [0][ 78/500]	Time 65.466 (65.466)	Loss 2.5579 (2.6410)	CeLoss 0.0894 (0.0708)	SegCLSLoss 1.0133 (1.0318)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0877 (0.1058)	MaskBCELoss 0.0211 (0.0321)	MaskDICELoss 0.0666 (0.0737)
Epoch: [0][ 79/500]	Time 72.435 (72.435)	Loss 2.6325 (2.6503)	CeLoss 0.0693 (0.0686)	SegCLSLoss 1.0235 (1.0360)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1137 (0.1096)	MaskBCELoss 0.0416 (0.0369)	MaskDICELoss 0.0721 (0.0726)
Epoch: [0][ 80/500]	Time 63.958 (63.958)	Loss 2.6444 (2.6001)	CeLoss 0.0664 (0.0708)	SegCLSLoss 0.9965 (1.0086)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1267 (0.1086)	MaskBCELoss 0.0437 (0.0350)	MaskDICELoss 0.0829 (0.0737)
Epoch: [0][ 81/500]	Time 68.344 (68.344)	Loss 0.9458 (0.9257)	CeLoss 0.0417 (0.0494)	SegCLSLoss 0.2065 (0.1940)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1066 (0.1043)	MaskBCELoss 0.0372 (0.0344)	MaskDICELoss 0.0694 (0.0699)
Epoch: [0][ 82/500]	Time 66.931 (66.931)	Loss 0.9585 (0.9470)	CeLoss 0.0422 (0.0450)	SegCLSLoss 0.1974 (0.2054)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1128 (0.1056)	MaskBCELoss 0.0388 (0.0355)	MaskDICELoss 0.0740 (0.0700)
Epoch: [0][ 83/500]	Time 66.639 (66.639)	Loss 0.9348 (0.9092)	CeLoss 0.0417 (0.0455)	SegCLSLoss 0.2023 (0.1939)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1043 (0.1014)	MaskBCELoss 0.0344 (0.0331)	MaskDICELoss 0.0699 (0.0683)
Epoch: [0][ 84/500]	Time 67.471 (67.471)	Loss 0.8831 (0.9801)	CeLoss 0.0306 (0.0421)	SegCLSLoss 0.2118 (0.2117)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0902 (0.1138)	MaskBCELoss 0.0281 (0.0421)	MaskDICELoss 0.0621 (0.0718)
Epoch: [0][ 85/500]	Time 64.240 (64.240)	Loss 0.9567 (0.8416)	CeLoss 0.0396 (0.0454)	SegCLSLoss 0.2165 (0.1622)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1055 (0.0994)	MaskBCELoss 0.0372 (0.0311)	MaskDICELoss 0.0683 (0.0683)
Epoch: [0][ 86/500]	Time 66.106 (66.106)	Loss 0.8855 (0.8817)	CeLoss 0.0479 (0.0439)	SegCLSLoss 0.1474 (0.1803)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1326 (0.1015)	MaskBCELoss 0.0632 (0.0330)	MaskDICELoss 0.0694 (0.0685)
Epoch: [0][ 87/500]	Time 65.800 (65.800)	Loss 1.0070 (0.9453)	CeLoss 0.0366 (0.0455)	SegCLSLoss 0.2068 (0.2010)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1267 (0.1052)	MaskBCELoss 0.0509 (0.0334)	MaskDICELoss 0.0758 (0.0718)
Epoch: [0][ 88/500]	Time 64.372 (64.372)	Loss 1.0616 (0.9201)	CeLoss 0.0500 (0.0449)	SegCLSLoss 0.2476 (0.1840)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1077 (0.1090)	MaskBCELoss 0.0324 (0.0367)	MaskDICELoss 0.0753 (0.0723)
Epoch: [0][ 89/500]	Time 73.195 (73.195)	Loss 0.9060 (0.9043)	CeLoss 0.0500 (0.0454)	SegCLSLoss 0.1887 (0.1763)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1021 (0.1073)	MaskBCELoss 0.0335 (0.0344)	MaskDICELoss 0.0686 (0.0729)
Epoch: [0][ 90/500]	Time 65.975 (65.975)	Loss 0.9780 (0.9248)	CeLoss 0.0483 (0.0468)	SegCLSLoss 0.1702 (0.1979)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1386 (0.1024)	MaskBCELoss 0.0605 (0.0330)	MaskDICELoss 0.0781 (0.0694)
Epoch: [0][ 91/500]	Time 65.155 (65.155)	Loss 0.9788 (1.0565)	CeLoss 0.0222 (0.0294)	SegCLSLoss 0.2042 (0.2781)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1271 (0.0989)	MaskBCELoss 0.0536 (0.0306)	MaskDICELoss 0.0735 (0.0683)
Epoch: [0][ 92/500]	Time 69.308 (69.308)	Loss 1.0518 (1.0395)	CeLoss 0.0349 (0.0316)	SegCLSLoss 0.2976 (0.2675)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0823 (0.1004)	MaskBCELoss 0.0180 (0.0323)	MaskDICELoss 0.0643 (0.0680)
Epoch: [0][ 93/500]	Time 64.277 (64.277)	Loss 0.9091 (1.0150)	CeLoss 0.0339 (0.0331)	SegCLSLoss 0.2329 (0.2684)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0801 (0.0918)	MaskBCELoss 0.0179 (0.0264)	MaskDICELoss 0.0623 (0.0654)
Epoch: [0][ 94/500]	Time 76.431 (76.431)	Loss 1.2745 (1.0687)	CeLoss 0.0378 (0.0317)	SegCLSLoss 0.3537 (0.2672)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1127 (0.1087)	MaskBCELoss 0.0368 (0.0373)	MaskDICELoss 0.0759 (0.0713)
Epoch: [0][ 95/500]	Time 71.618 (71.618)	Loss 1.2374 (1.0768)	CeLoss 0.0293 (0.0313)	SegCLSLoss 0.3493 (0.2792)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1088 (0.1031)	MaskBCELoss 0.0359 (0.0329)	MaskDICELoss 0.0730 (0.0702)
Epoch: [0][ 96/500]	Time 66.881 (66.881)	Loss 1.0020 (1.1058)	CeLoss 0.0220 (0.0286)	SegCLSLoss 0.2272 (0.2979)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1114 (0.1035)	MaskBCELoss 0.0356 (0.0349)	MaskDICELoss 0.0757 (0.0686)
Epoch: [0][ 97/500]	Time 70.911 (70.911)	Loss 1.0585 (1.0051)	CeLoss 0.0283 (0.0310)	SegCLSLoss 0.2995 (0.2496)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0918 (0.1017)	MaskBCELoss 0.0298 (0.0338)	MaskDICELoss 0.0619 (0.0679)
Epoch: [0][ 98/500]	Time 72.337 (72.337)	Loss 1.0966 (1.0580)	CeLoss 0.0315 (0.0319)	SegCLSLoss 0.2853 (0.2639)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1031 (0.1093)	MaskBCELoss 0.0311 (0.0394)	MaskDICELoss 0.0721 (0.0699)
Epoch: [0][ 99/500]	Time 78.938 (78.938)	Loss 1.0426 (1.1308)	CeLoss 0.0175 (0.0293)	SegCLSLoss 0.2370 (0.3029)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1190 (0.1060)	MaskBCELoss 0.0408 (0.0350)	MaskDICELoss 0.0783 (0.0709)
[2025-04-12 16:52:39,722] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[0.00029991566265060235], mom=[(0.9, 0.95)]
[2025-04-12 16:52:39,737] [INFO] [timer.py:215:stop] epoch=0/micro_step=100/global_step=10, RunningAvgSamplesPerSec=1.142782985380782, CurrSamplesPerSec=1.0433334677723887, MemAllocated=38.87GB, MaxMemAllocated=48.48GB
Epoch: [0][100/500]	Time 80.807 (80.807)	Loss 1.3182 (1.1193)	CeLoss 0.0337 (0.0290)	SegCLSLoss 0.3711 (0.2942)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1195 (0.1091)	MaskBCELoss 0.0436 (0.0382)	MaskDICELoss 0.0759 (0.0709)
Epoch: [0][101/500]	Time 72.385 (72.385)	Loss 1.2947 (1.2351)	CeLoss 0.0303 (0.0282)	SegCLSLoss 0.4405 (0.3810)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0857 (0.0971)	MaskBCELoss 0.0326 (0.0344)	MaskDICELoss 0.0530 (0.0627)
Epoch: [0][102/500]	Time 65.350 (65.350)	Loss 1.2255 (1.2880)	CeLoss 0.0247 (0.0275)	SegCLSLoss 0.3667 (0.4034)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0970 (0.0974)	MaskBCELoss 0.0286 (0.0327)	MaskDICELoss 0.0683 (0.0647)
Epoch: [0][103/500]	Time 72.281 (72.281)	Loss 0.9952 (1.1218)	CeLoss 0.0232 (0.0252)	SegCLSLoss 0.2367 (0.3221)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1235 (0.1001)	MaskBCELoss 0.0606 (0.0370)	MaskDICELoss 0.0629 (0.0631)
Epoch: [0][104/500]	Time 60.666 (60.666)	Loss 1.1107 (1.0765)	CeLoss 0.0247 (0.0250)	SegCLSLoss 0.2902 (0.3093)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1143 (0.0954)	MaskBCELoss 0.0450 (0.0348)	MaskDICELoss 0.0693 (0.0606)
Epoch: [0][105/500]	Time 75.788 (75.788)	Loss 1.0102 (1.1066)	CeLoss 0.0173 (0.0241)	SegCLSLoss 0.2730 (0.3069)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0956 (0.1054)	MaskBCELoss 0.0317 (0.0410)	MaskDICELoss 0.0639 (0.0645)
Epoch: [0][106/500]	Time 73.130 (73.130)	Loss 0.9803 (1.1722)	CeLoss 0.0270 (0.0254)	SegCLSLoss 0.2690 (0.3323)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0958 (0.1090)	MaskBCELoss 0.0398 (0.0430)	MaskDICELoss 0.0559 (0.0660)
Epoch: [0][107/500]	Time 61.286 (61.286)	Loss 0.9162 (1.0387)	CeLoss 0.0293 (0.0238)	SegCLSLoss 0.2813 (0.2877)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0588 (0.0953)	MaskBCELoss 0.0071 (0.0330)	MaskDICELoss 0.0517 (0.0623)
Epoch: [0][108/500]	Time 68.382 (68.382)	Loss 1.1807 (1.1298)	CeLoss 0.0189 (0.0272)	SegCLSLoss 0.3139 (0.3201)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1198 (0.1043)	MaskBCELoss 0.0462 (0.0408)	MaskDICELoss 0.0736 (0.0635)
Epoch: [0][109/500]	Time 61.735 (61.735)	Loss 1.3427 (1.1716)	CeLoss 0.0208 (0.0253)	SegCLSLoss 0.4053 (0.3413)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1112 (0.1003)	MaskBCELoss 0.0389 (0.0345)	MaskDICELoss 0.0722 (0.0658)
Epoch: [0][110/500]	Time 65.518 (65.518)	Loss 1.2174 (1.1676)	CeLoss 0.0247 (0.0253)	SegCLSLoss 0.3679 (0.3538)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1025 (0.0961)	MaskBCELoss 0.0396 (0.0355)	MaskDICELoss 0.0630 (0.0606)
Epoch: [0][111/500]	Time 72.635 (72.635)	Loss 1.1314 (1.1780)	CeLoss 0.0271 (0.0224)	SegCLSLoss 0.3175 (0.3401)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1067 (0.1093)	MaskBCELoss 0.0428 (0.0451)	MaskDICELoss 0.0640 (0.0642)
Epoch: [0][112/500]	Time 62.704 (62.704)	Loss 1.4349 (1.1947)	CeLoss 0.0239 (0.0243)	SegCLSLoss 0.4515 (0.3769)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1185 (0.0937)	MaskBCELoss 0.0508 (0.0365)	MaskDICELoss 0.0677 (0.0573)
Epoch: [0][113/500]	Time 66.443 (66.443)	Loss 1.0988 (1.1036)	CeLoss 0.0183 (0.0186)	SegCLSLoss 0.3678 (0.3341)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0750 (0.0915)	MaskBCELoss 0.0262 (0.0330)	MaskDICELoss 0.0488 (0.0585)
Epoch: [0][114/500]	Time 71.228 (71.228)	Loss 1.3041 (1.1036)	CeLoss 0.0192 (0.0169)	SegCLSLoss 0.3931 (0.3196)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1154 (0.1036)	MaskBCELoss 0.0484 (0.0436)	MaskDICELoss 0.0670 (0.0601)
Epoch: [0][115/500]	Time 71.136 (71.136)	Loss 0.8251 (1.1174)	CeLoss 0.0123 (0.0213)	SegCLSLoss 0.1949 (0.3243)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0931 (0.1023)	MaskBCELoss 0.0339 (0.0416)	MaskDICELoss 0.0592 (0.0607)
Epoch: [0][116/500]	Time 66.296 (66.296)	Loss 0.9310 (1.0607)	CeLoss 0.0173 (0.0167)	SegCLSLoss 0.2438 (0.3025)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1014 (0.0988)	MaskBCELoss 0.0456 (0.0384)	MaskDICELoss 0.0558 (0.0604)
Epoch: [0][117/500]	Time 72.012 (72.012)	Loss 1.4931 (1.1727)	CeLoss 0.0270 (0.0214)	SegCLSLoss 0.5025 (0.3565)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1032 (0.0965)	MaskBCELoss 0.0395 (0.0351)	MaskDICELoss 0.0637 (0.0614)
Epoch: [0][118/500]	Time 62.219 (62.219)	Loss 1.2861 (1.2650)	CeLoss 0.0309 (0.0239)	SegCLSLoss 0.3757 (0.3979)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1134 (0.1003)	MaskBCELoss 0.0442 (0.0391)	MaskDICELoss 0.0693 (0.0612)
Epoch: [0][119/500]	Time 69.104 (69.104)	Loss 1.0046 (1.1382)	CeLoss 0.0134 (0.0204)	SegCLSLoss 0.2922 (0.3405)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0887 (0.0981)	MaskBCELoss 0.0313 (0.0379)	MaskDICELoss 0.0574 (0.0602)
Epoch: [0][120/500]	Time 74.322 (74.322)	Loss 1.1625 (1.0765)	CeLoss 0.0247 (0.0196)	SegCLSLoss 0.3576 (0.3063)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0965 (0.1020)	MaskBCELoss 0.0392 (0.0419)	MaskDICELoss 0.0574 (0.0601)
Epoch: [0][121/500]	Time 65.925 (65.925)	Loss 1.1570 (1.0626)	CeLoss 0.0212 (0.0194)	SegCLSLoss 0.3720 (0.3221)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0841 (0.0872)	MaskBCELoss 0.0283 (0.0310)	MaskDICELoss 0.0559 (0.0562)
Epoch: [0][122/500]	Time 71.448 (71.448)	Loss 0.8997 (1.0432)	CeLoss 0.0176 (0.0167)	SegCLSLoss 0.2102 (0.3137)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1118 (0.0875)	MaskBCELoss 0.0522 (0.0315)	MaskDICELoss 0.0595 (0.0560)
Epoch: [0][123/500]	Time 71.141 (71.141)	Loss 1.0869 (1.0770)	CeLoss 0.0197 (0.0181)	SegCLSLoss 0.2694 (0.3207)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1144 (0.0888)	MaskBCELoss 0.0395 (0.0289)	MaskDICELoss 0.0749 (0.0600)
Epoch: [0][124/500]	Time 68.262 (68.262)	Loss 1.3309 (1.0504)	CeLoss 0.0270 (0.0185)	SegCLSLoss 0.4633 (0.3171)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0815 (0.0897)	MaskBCELoss 0.0279 (0.0351)	MaskDICELoss 0.0536 (0.0546)
Epoch: [0][125/500]	Time 63.779 (63.779)	Loss 0.9530 (1.0000)	CeLoss 0.0162 (0.0167)	SegCLSLoss 0.2578 (0.2912)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0917 (0.0851)	MaskBCELoss 0.0322 (0.0274)	MaskDICELoss 0.0595 (0.0577)
Epoch: [0][126/500]	Time 64.849 (64.849)	Loss 1.1711 (0.9986)	CeLoss 0.0164 (0.0151)	SegCLSLoss 0.3815 (0.2885)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0822 (0.0898)	MaskBCELoss 0.0253 (0.0331)	MaskDICELoss 0.0569 (0.0567)
Epoch: [0][127/500]	Time 71.776 (71.776)	Loss 0.7813 (1.0125)	CeLoss 0.0146 (0.0165)	SegCLSLoss 0.2066 (0.2836)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0733 (0.0939)	MaskBCELoss 0.0216 (0.0337)	MaskDICELoss 0.0517 (0.0602)
Epoch: [0][128/500]	Time 61.770 (61.770)	Loss 1.0081 (1.0385)	CeLoss 0.0175 (0.0188)	SegCLSLoss 0.2825 (0.3204)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1034 (0.0815)	MaskBCELoss 0.0488 (0.0276)	MaskDICELoss 0.0547 (0.0540)
Epoch: [0][129/500]	Time 65.421 (65.421)	Loss 0.8665 (1.0392)	CeLoss 0.0085 (0.0164)	SegCLSLoss 0.2034 (0.2990)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1015 (0.0927)	MaskBCELoss 0.0394 (0.0328)	MaskDICELoss 0.0621 (0.0599)
Epoch: [0][130/500]	Time 64.206 (64.206)	Loss 0.8641 (0.9828)	CeLoss 0.0140 (0.0166)	SegCLSLoss 0.2038 (0.2808)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1019 (0.0902)	MaskBCELoss 0.0422 (0.0341)	MaskDICELoss 0.0597 (0.0561)
Epoch: [0][131/500]	Time 69.161 (69.161)	Loss 0.6930 (0.8648)	CeLoss 0.0143 (0.0145)	SegCLSLoss 0.1636 (0.2132)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0773 (0.0928)	MaskBCELoss 0.0280 (0.0332)	MaskDICELoss 0.0493 (0.0596)
Epoch: [0][132/500]	Time 70.871 (70.871)	Loss 0.8606 (0.8084)	CeLoss 0.0140 (0.0110)	SegCLSLoss 0.1889 (0.1906)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1071 (0.0892)	MaskBCELoss 0.0434 (0.0297)	MaskDICELoss 0.0637 (0.0595)
Epoch: [0][133/500]	Time 69.899 (69.899)	Loss 0.7938 (0.8557)	CeLoss 0.0118 (0.0143)	SegCLSLoss 0.2170 (0.2198)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0857 (0.0873)	MaskBCELoss 0.0415 (0.0306)	MaskDICELoss 0.0442 (0.0568)
Epoch: [0][134/500]	Time 63.538 (63.538)	Loss 0.8080 (0.8372)	CeLoss 0.0155 (0.0145)	SegCLSLoss 0.2110 (0.2049)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0783 (0.0896)	MaskBCELoss 0.0249 (0.0311)	MaskDICELoss 0.0534 (0.0585)
Epoch: [0][135/500]	Time 67.551 (67.551)	Loss 0.9428 (0.8540)	CeLoss 0.0104 (0.0147)	SegCLSLoss 0.2174 (0.2043)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1095 (0.0955)	MaskBCELoss 0.0398 (0.0355)	MaskDICELoss 0.0697 (0.0599)
Epoch: [0][136/500]	Time 66.180 (66.180)	Loss 0.6514 (0.8357)	CeLoss 0.0126 (0.0180)	SegCLSLoss 0.1660 (0.2065)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0628 (0.0882)	MaskBCELoss 0.0175 (0.0311)	MaskDICELoss 0.0453 (0.0571)
Epoch: [0][137/500]	Time 60.857 (60.857)	Loss 0.7737 (0.7988)	CeLoss 0.0159 (0.0138)	SegCLSLoss 0.1598 (0.2039)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0917 (0.0793)	MaskBCELoss 0.0281 (0.0246)	MaskDICELoss 0.0637 (0.0547)
Epoch: [0][138/500]	Time 57.644 (57.644)	Loss 0.8148 (0.9134)	CeLoss 0.0120 (0.0166)	SegCLSLoss 0.2032 (0.2577)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0859 (0.0813)	MaskBCELoss 0.0298 (0.0266)	MaskDICELoss 0.0561 (0.0547)
Epoch: [0][139/500]	Time 69.264 (69.264)	Loss 0.7383 (0.8956)	CeLoss 0.0098 (0.0155)	SegCLSLoss 0.1554 (0.2269)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0991 (0.0918)	MaskBCELoss 0.0441 (0.0311)	MaskDICELoss 0.0549 (0.0607)
Epoch: [0][140/500]	Time 64.868 (64.868)	Loss 0.6472 (0.8263)	CeLoss 0.0045 (0.0142)	SegCLSLoss 0.1285 (0.2094)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0851 (0.0844)	MaskBCELoss 0.0312 (0.0282)	MaskDICELoss 0.0539 (0.0562)
Epoch: [0][141/500]	Time 62.520 (62.520)	Loss 0.6331 (0.6599)	CeLoss 0.0117 (0.0122)	SegCLSLoss 0.1758 (0.1239)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0547 (0.0852)	MaskBCELoss 0.0145 (0.0277)	MaskDICELoss 0.0401 (0.0574)
Epoch: [0][142/500]	Time 59.637 (59.637)	Loss 0.6279 (0.6152)	CeLoss 0.0145 (0.0125)	SegCLSLoss 0.1183 (0.1103)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0791 (0.0792)	MaskBCELoss 0.0244 (0.0233)	MaskDICELoss 0.0546 (0.0559)
Epoch: [0][143/500]	Time 55.920 (55.920)	Loss 0.6621 (0.6261)	CeLoss 0.0138 (0.0084)	SegCLSLoss 0.1279 (0.0968)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0787 (0.0934)	MaskBCELoss 0.0199 (0.0340)	MaskDICELoss 0.0588 (0.0594)
Epoch: [0][144/500]	Time 64.412 (64.412)	Loss 0.7833 (0.6345)	CeLoss 0.0126 (0.0117)	SegCLSLoss 0.1482 (0.1259)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1020 (0.0778)	MaskBCELoss 0.0345 (0.0239)	MaskDICELoss 0.0676 (0.0539)
Epoch: [0][145/500]	Time 70.508 (70.508)	Loss 0.8336 (0.6852)	CeLoss 0.0183 (0.0137)	SegCLSLoss 0.1732 (0.1304)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1060 (0.0883)	MaskBCELoss 0.0418 (0.0298)	MaskDICELoss 0.0642 (0.0585)
Epoch: [0][146/500]	Time 98.975 (98.975)	Loss 0.4944 (0.6634)	CeLoss 0.0075 (0.0134)	SegCLSLoss 0.0982 (0.1190)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0578 (0.0878)	MaskBCELoss 0.0140 (0.0286)	MaskDICELoss 0.0437 (0.0591)
Epoch: [0][147/500]	Time 69.728 (69.728)	Loss 0.6541 (0.6639)	CeLoss 0.0199 (0.0138)	SegCLSLoss 0.1228 (0.1139)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0888 (0.0913)	MaskBCELoss 0.0361 (0.0315)	MaskDICELoss 0.0527 (0.0599)
Epoch: [0][148/500]	Time 59.679 (59.679)	Loss 0.7058 (0.6200)	CeLoss 0.0190 (0.0144)	SegCLSLoss 0.1645 (0.1089)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0697 (0.0828)	MaskBCELoss 0.0151 (0.0271)	MaskDICELoss 0.0546 (0.0556)
Epoch: [0][149/500]	Time 70.643 (70.643)	Loss 0.5580 (0.6560)	CeLoss 0.0115 (0.0135)	SegCLSLoss 0.1096 (0.1122)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0684 (0.0901)	MaskBCELoss 0.0208 (0.0306)	MaskDICELoss 0.0476 (0.0595)
Epoch: [0][150/500]	Time 71.902 (71.902)	Loss 0.6911 (0.6718)	CeLoss 0.0119 (0.0141)	SegCLSLoss 0.1020 (0.1266)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1072 (0.0848)	MaskBCELoss 0.0420 (0.0261)	MaskDICELoss 0.0652 (0.0587)
Epoch: [0][151/500]	Time 71.087 (71.087)	Loss 0.5956 (0.5289)	CeLoss 0.0122 (0.0119)	SegCLSLoss 0.0400 (0.0422)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1137 (0.0934)	MaskBCELoss 0.0448 (0.0320)	MaskDICELoss 0.0690 (0.0614)
Epoch: [0][152/500]	Time 71.318 (71.318)	Loss 0.5300 (0.5280)	CeLoss 0.0114 (0.0115)	SegCLSLoss 0.0368 (0.0395)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0944 (0.0954)	MaskBCELoss 0.0304 (0.0337)	MaskDICELoss 0.0641 (0.0617)
Epoch: [0][153/500]	Time 71.005 (71.005)	Loss 0.5177 (0.5094)	CeLoss 0.0124 (0.0109)	SegCLSLoss 0.0327 (0.0388)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0911 (0.0919)	MaskBCELoss 0.0267 (0.0327)	MaskDICELoss 0.0644 (0.0592)
Epoch: [0][154/500]	Time 62.534 (62.534)	Loss 0.5475 (0.4910)	CeLoss 0.0066 (0.0107)	SegCLSLoss 0.0351 (0.0421)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1100 (0.0857)	MaskBCELoss 0.0472 (0.0295)	MaskDICELoss 0.0627 (0.0562)
Epoch: [0][155/500]	Time 99.130 (99.130)	Loss 0.5361 (0.5254)	CeLoss 0.0092 (0.0117)	SegCLSLoss 0.0321 (0.0431)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1033 (0.0932)	MaskBCELoss 0.0392 (0.0328)	MaskDICELoss 0.0641 (0.0603)
Epoch: [0][156/500]	Time 70.555 (70.555)	Loss 0.4174 (0.5003)	CeLoss 0.0089 (0.0109)	SegCLSLoss 0.0313 (0.0411)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0717 (0.0884)	MaskBCELoss 0.0211 (0.0308)	MaskDICELoss 0.0506 (0.0576)
Epoch: [0][157/500]	Time 66.970 (66.970)	Loss 0.5399 (0.5252)	CeLoss 0.0130 (0.0136)	SegCLSLoss 0.0465 (0.0454)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0901 (0.0914)	MaskBCELoss 0.0266 (0.0319)	MaskDICELoss 0.0634 (0.0595)
Epoch: [0][158/500]	Time 67.092 (67.092)	Loss 0.5878 (0.4905)	CeLoss 0.0217 (0.0119)	SegCLSLoss 0.0498 (0.0394)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1048 (0.0872)	MaskBCELoss 0.0406 (0.0309)	MaskDICELoss 0.0642 (0.0563)
Epoch: [0][159/500]	Time 75.139 (75.139)	Loss 0.5010 (0.5437)	CeLoss 0.0078 (0.0112)	SegCLSLoss 0.0440 (0.0382)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0909 (0.1014)	MaskBCELoss 0.0351 (0.0381)	MaskDICELoss 0.0558 (0.0633)
Epoch: [0][160/500]	Time 59.013 (59.013)	Loss 0.5319 (0.4732)	CeLoss 0.0173 (0.0108)	SegCLSLoss 0.0383 (0.0358)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0951 (0.0875)	MaskBCELoss 0.0331 (0.0336)	MaskDICELoss 0.0620 (0.0539)
Epoch: [0][161/500]	Time 64.758 (64.758)	Loss 0.5652 (0.5343)	CeLoss 0.0062 (0.0082)	SegCLSLoss 0.0688 (0.0675)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0942 (0.0870)	MaskBCELoss 0.0360 (0.0327)	MaskDICELoss 0.0582 (0.0543)
Epoch: [0][162/500]	Time 74.504 (74.504)	Loss 0.5532 (0.5730)	CeLoss 0.0109 (0.0102)	SegCLSLoss 0.0769 (0.0724)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0826 (0.0915)	MaskBCELoss 0.0267 (0.0327)	MaskDICELoss 0.0558 (0.0588)
Epoch: [0][163/500]	Time 68.224 (68.224)	Loss 0.5918 (0.5768)	CeLoss 0.0093 (0.0099)	SegCLSLoss 0.0696 (0.0696)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0985 (0.0937)	MaskBCELoss 0.0369 (0.0337)	MaskDICELoss 0.0616 (0.0601)
Epoch: [0][164/500]	Time 71.282 (71.282)	Loss 0.5888 (0.5549)	CeLoss 0.0050 (0.0074)	SegCLSLoss 0.0711 (0.0683)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0987 (0.0896)	MaskBCELoss 0.0376 (0.0317)	MaskDICELoss 0.0611 (0.0579)
Epoch: [0][165/500]	Time 66.831 (66.831)	Loss 0.5705 (0.5530)	CeLoss 0.0106 (0.0084)	SegCLSLoss 0.0649 (0.0671)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0977 (0.0920)	MaskBCELoss 0.0390 (0.0354)	MaskDICELoss 0.0587 (0.0566)
Epoch: [0][166/500]	Time 60.740 (60.740)	Loss 0.5060 (0.5016)	CeLoss 0.0103 (0.0081)	SegCLSLoss 0.0717 (0.0677)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0742 (0.0802)	MaskBCELoss 0.0232 (0.0308)	MaskDICELoss 0.0510 (0.0494)
Epoch: [0][167/500]	Time 60.598 (60.598)	Loss 0.5433 (0.5440)	CeLoss 0.0123 (0.0097)	SegCLSLoss 0.0673 (0.0725)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0884 (0.0856)	MaskBCELoss 0.0335 (0.0310)	MaskDICELoss 0.0549 (0.0546)
Epoch: [0][168/500]	Time 82.859 (82.859)	Loss 0.5913 (0.5501)	CeLoss 0.0123 (0.0080)	SegCLSLoss 0.0812 (0.0699)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0908 (0.0895)	MaskBCELoss 0.0321 (0.0336)	MaskDICELoss 0.0588 (0.0559)
Epoch: [0][169/500]	Time 98.556 (98.556)	Loss 0.4716 (0.5334)	CeLoss 0.0079 (0.0088)	SegCLSLoss 0.0652 (0.0662)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0712 (0.0872)	MaskBCELoss 0.0235 (0.0328)	MaskDICELoss 0.0477 (0.0544)
Epoch: [0][170/500]	Time 132.664 (132.664)	Loss 0.5351 (0.5660)	CeLoss 0.0087 (0.0109)	SegCLSLoss 0.0704 (0.0700)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0889 (0.0938)	MaskBCELoss 0.0370 (0.0369)	MaskDICELoss 0.0520 (0.0569)
Epoch: [0][171/500]	Time 118.072 (118.072)	Loss 0.6528 (0.6376)	CeLoss 0.0101 (0.0079)	SegCLSLoss 0.1058 (0.1088)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0928 (0.0920)	MaskBCELoss 0.0315 (0.0349)	MaskDICELoss 0.0614 (0.0570)
Epoch: [0][172/500]	Time 127.926 (127.926)	Loss 0.6573 (0.6300)	CeLoss 0.0033 (0.0066)	SegCLSLoss 0.0930 (0.1038)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0989 (0.0901)	MaskBCELoss 0.0313 (0.0312)	MaskDICELoss 0.0676 (0.0589)
Epoch: [0][173/500]	Time 135.129 (135.129)	Loss 0.6975 (0.6178)	CeLoss 0.0117 (0.0080)	SegCLSLoss 0.1110 (0.1076)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1053 (0.0848)	MaskBCELoss 0.0420 (0.0285)	MaskDICELoss 0.0633 (0.0563)
Epoch: [0][174/500]	Time 156.352 (156.352)	Loss 0.6383 (0.6258)	CeLoss 0.0095 (0.0072)	SegCLSLoss 0.0903 (0.1006)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0997 (0.0929)	MaskBCELoss 0.0375 (0.0349)	MaskDICELoss 0.0622 (0.0579)
Epoch: [0][175/500]	Time 141.465 (141.465)	Loss 0.5656 (0.5911)	CeLoss 0.0077 (0.0079)	SegCLSLoss 0.0650 (0.1018)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0962 (0.0830)	MaskBCELoss 0.0373 (0.0296)	MaskDICELoss 0.0589 (0.0534)
Epoch: [0][176/500]	Time 188.435 (188.435)	Loss 0.6035 (0.6276)	CeLoss 0.0034 (0.0053)	SegCLSLoss 0.1125 (0.1023)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0762 (0.0905)	MaskBCELoss 0.0205 (0.0313)	MaskDICELoss 0.0557 (0.0592)
Epoch: [0][177/500]	Time 188.924 (188.924)	Loss 0.7373 (0.6008)	CeLoss 0.0094 (0.0067)	SegCLSLoss 0.1024 (0.0951)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1193 (0.0883)	MaskBCELoss 0.0482 (0.0315)	MaskDICELoss 0.0711 (0.0568)
Epoch: [0][178/500]	Time 186.914 (186.914)	Loss 0.6802 (0.6413)	CeLoss 0.0046 (0.0073)	SegCLSLoss 0.0993 (0.1020)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1026 (0.0930)	MaskBCELoss 0.0346 (0.0320)	MaskDICELoss 0.0679 (0.0610)
Epoch: [0][179/500]	Time 160.064 (160.064)	Loss 0.6917 (0.5956)	CeLoss 0.0051 (0.0071)	SegCLSLoss 0.1341 (0.1007)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0918 (0.0842)	MaskBCELoss 0.0331 (0.0296)	MaskDICELoss 0.0587 (0.0547)
Epoch: [0][180/500]	Time 170.582 (170.582)	Loss 0.5415 (0.5929)	CeLoss 0.0013 (0.0075)	SegCLSLoss 0.0982 (0.1091)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0740 (0.0787)	MaskBCELoss 0.0250 (0.0263)	MaskDICELoss 0.0490 (0.0524)
Epoch: [0][181/500]	Time 154.468 (154.468)	Loss 0.6507 (0.6258)	CeLoss 0.0077 (0.0065)	SegCLSLoss 0.1367 (0.1182)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0811 (0.0828)	MaskBCELoss 0.0292 (0.0285)	MaskDICELoss 0.0519 (0.0543)
Epoch: [0][182/500]	Time 98.250 (98.250)	Loss 0.6934 (0.5892)	CeLoss 0.0046 (0.0041)	SegCLSLoss 0.1592 (0.1152)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0778 (0.0763)	MaskBCELoss 0.0241 (0.0258)	MaskDICELoss 0.0537 (0.0505)
Epoch: [0][183/500]	Time 115.055 (115.055)	Loss 0.6817 (0.6323)	CeLoss 0.0046 (0.0044)	SegCLSLoss 0.0884 (0.1098)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1221 (0.0907)	MaskBCELoss 0.0581 (0.0339)	MaskDICELoss 0.0640 (0.0567)
Epoch: [0][184/500]	Time 147.155 (147.155)	Loss 0.4718 (0.5852)	CeLoss 0.0025 (0.0036)	SegCLSLoss 0.1170 (0.1258)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0487 (0.0700)	MaskBCELoss 0.0142 (0.0225)	MaskDICELoss 0.0345 (0.0475)
Epoch: [0][185/500]	Time 192.101 (192.101)	Loss 0.6054 (0.6272)	CeLoss 0.0084 (0.0066)	SegCLSLoss 0.1065 (0.1249)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0847 (0.0808)	MaskBCELoss 0.0311 (0.0285)	MaskDICELoss 0.0536 (0.0523)
Epoch: [0][186/500]	Time 189.453 (189.453)	Loss 0.7766 (0.6527)	CeLoss 0.0060 (0.0061)	SegCLSLoss 0.1397 (0.1194)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1132 (0.0888)	MaskBCELoss 0.0470 (0.0312)	MaskDICELoss 0.0662 (0.0576)
Epoch: [0][187/500]	Time 191.612 (191.612)	Loss 0.6802 (0.6111)	CeLoss 0.0114 (0.0068)	SegCLSLoss 0.1474 (0.1265)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0779 (0.0756)	MaskBCELoss 0.0234 (0.0256)	MaskDICELoss 0.0546 (0.0500)
Epoch: [0][188/500]	Time 188.335 (188.335)	Loss 0.5814 (0.6368)	CeLoss 0.0036 (0.0062)	SegCLSLoss 0.1016 (0.1219)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0768 (0.0850)	MaskBCELoss 0.0216 (0.0308)	MaskDICELoss 0.0552 (0.0542)
Epoch: [0][189/500]	Time 141.917 (141.917)	Loss 0.4709 (0.5909)	CeLoss 0.0029 (0.0051)	SegCLSLoss 0.0901 (0.1237)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0606 (0.0744)	MaskBCELoss 0.0190 (0.0270)	MaskDICELoss 0.0416 (0.0474)
Epoch: [0][190/500]	Time 134.062 (134.062)	Loss 0.5138 (0.6368)	CeLoss 0.0092 (0.0074)	SegCLSLoss 0.1276 (0.1322)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0481 (0.0773)	MaskBCELoss 0.0098 (0.0247)	MaskDICELoss 0.0383 (0.0526)
Epoch: [0][191/500]	Time 124.715 (124.715)	Loss 0.6051 (0.5953)	CeLoss 0.0067 (0.0050)	SegCLSLoss 0.0827 (0.1032)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0970 (0.0838)	MaskBCELoss 0.0372 (0.0297)	MaskDICELoss 0.0598 (0.0541)
Epoch: [0][192/500]	Time 119.491 (119.491)	Loss 0.5455 (0.5999)	CeLoss 0.0016 (0.0053)	SegCLSLoss 0.0777 (0.0991)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0807 (0.0856)	MaskBCELoss 0.0239 (0.0292)	MaskDICELoss 0.0568 (0.0563)
Epoch: [0][193/500]	Time 144.073 (144.073)	Loss 0.4707 (0.5770)	CeLoss 0.0014 (0.0056)	SegCLSLoss 0.0839 (0.0897)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0605 (0.0851)	MaskBCELoss 0.0154 (0.0297)	MaskDICELoss 0.0451 (0.0554)
Epoch: [0][194/500]	Time 118.941 (118.941)	Loss 0.6230 (0.6447)	CeLoss 0.0060 (0.0049)	SegCLSLoss 0.1069 (0.1203)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0865 (0.0852)	MaskBCELoss 0.0290 (0.0280)	MaskDICELoss 0.0575 (0.0572)
Epoch: [0][195/500]	Time 132.672 (132.672)	Loss 0.6390 (0.5857)	CeLoss 0.0041 (0.0053)	SegCLSLoss 0.0822 (0.0930)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1145 (0.0887)	MaskBCELoss 0.0541 (0.0344)	MaskDICELoss 0.0604 (0.0542)
Epoch: [0][196/500]	Time 163.962 (163.962)	Loss 0.5665 (0.5524)	CeLoss 0.0100 (0.0061)	SegCLSLoss 0.1151 (0.0910)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0742 (0.0816)	MaskBCELoss 0.0298 (0.0314)	MaskDICELoss 0.0445 (0.0502)
Epoch: [0][197/500]	Time 183.521 (183.521)	Loss 0.6467 (0.5413)	CeLoss 0.0062 (0.0057)	SegCLSLoss 0.0762 (0.0907)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1090 (0.0771)	MaskBCELoss 0.0415 (0.0271)	MaskDICELoss 0.0675 (0.0500)
Epoch: [0][198/500]	Time 143.909 (143.909)	Loss 0.7008 (0.5840)	CeLoss 0.0039 (0.0048)	SegCLSLoss 0.1530 (0.0955)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0856 (0.0860)	MaskBCELoss 0.0307 (0.0320)	MaskDICELoss 0.0549 (0.0540)
Epoch: [0][199/500]	Time 107.689 (107.689)	Loss 0.5855 (0.5394)	CeLoss 0.0075 (0.0040)	SegCLSLoss 0.0741 (0.0831)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1016 (0.0828)	MaskBCELoss 0.0449 (0.0319)	MaskDICELoss 0.0567 (0.0509)
[2025-04-12 19:28:46,436] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[0.0002985903614457831], mom=[(0.9, 0.95)]
[2025-04-12 19:28:46,450] [INFO] [timer.py:215:stop] epoch=0/micro_step=200/global_step=20, RunningAvgSamplesPerSec=0.9784518838447466, CurrSamplesPerSec=0.5935057342291483, MemAllocated=36.02GB, MaxMemAllocated=48.48GB
Epoch: [0][200/500]	Time 129.657 (129.657)	Loss 0.6321 (0.6239)	CeLoss 0.0039 (0.0049)	SegCLSLoss 0.1057 (0.1048)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0977 (0.0898)	MaskBCELoss 0.0424 (0.0323)	MaskDICELoss 0.0553 (0.0575)
Epoch: [0][201/500]	Time 127.693 (127.693)	Loss 0.3868 (0.4987)	CeLoss 0.0022 (0.0041)	SegCLSLoss 0.0474 (0.0541)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0630 (0.0849)	MaskBCELoss 0.0220 (0.0308)	MaskDICELoss 0.0410 (0.0541)
Epoch: [0][202/500]	Time 135.436 (135.436)	Loss 0.6202 (0.5368)	CeLoss 0.0078 (0.0041)	SegCLSLoss 0.1111 (0.0674)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0814 (0.0882)	MaskBCELoss 0.0246 (0.0329)	MaskDICELoss 0.0569 (0.0553)
Epoch: [0][203/500]	Time 131.273 (131.273)	Loss 0.4342 (0.4698)	CeLoss 0.0015 (0.0039)	SegCLSLoss 0.0505 (0.0638)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0758 (0.0750)	MaskBCELoss 0.0308 (0.0279)	MaskDICELoss 0.0450 (0.0471)
Epoch: [0][204/500]	Time 139.366 (139.366)	Loss 0.5792 (0.5056)	CeLoss 0.0040 (0.0032)	SegCLSLoss 0.0469 (0.0563)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1131 (0.0867)	MaskBCELoss 0.0492 (0.0326)	MaskDICELoss 0.0638 (0.0541)
Epoch: [0][205/500]	Time 125.070 (125.070)	Loss 0.6287 (0.4866)	CeLoss 0.0051 (0.0049)	SegCLSLoss 0.0636 (0.0586)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1212 (0.0805)	MaskBCELoss 0.0578 (0.0296)	MaskDICELoss 0.0635 (0.0509)
Epoch: [0][206/500]	Time 136.628 (136.628)	Loss 0.5339 (0.4968)	CeLoss 0.0019 (0.0032)	SegCLSLoss 0.0644 (0.0584)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0905 (0.0835)	MaskBCELoss 0.0349 (0.0310)	MaskDICELoss 0.0556 (0.0525)
Epoch: [0][207/500]	Time 119.591 (119.591)	Loss 0.5317 (0.4883)	CeLoss 0.0037 (0.0035)	SegCLSLoss 0.0605 (0.0608)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0968 (0.0796)	MaskBCELoss 0.0434 (0.0286)	MaskDICELoss 0.0534 (0.0510)
Epoch: [0][208/500]	Time 116.956 (116.956)	Loss 0.4061 (0.4973)	CeLoss 0.0037 (0.0038)	SegCLSLoss 0.0473 (0.0615)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0692 (0.0806)	MaskBCELoss 0.0269 (0.0283)	MaskDICELoss 0.0423 (0.0523)
Epoch: [0][209/500]	Time 161.323 (161.323)	Loss 0.5710 (0.4884)	CeLoss 0.0084 (0.0033)	SegCLSLoss 0.0840 (0.0617)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0845 (0.0789)	MaskBCELoss 0.0282 (0.0279)	MaskDICELoss 0.0564 (0.0510)
Epoch: [0][210/500]	Time 181.837 (181.837)	Loss 0.5131 (0.4806)	CeLoss 0.0010 (0.0036)	SegCLSLoss 0.0411 (0.0583)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1048 (0.0807)	MaskBCELoss 0.0497 (0.0310)	MaskDICELoss 0.0551 (0.0497)
Epoch: [0][211/500]	Time 182.058 (182.058)	Loss 0.1968 (0.4376)	CeLoss 0.0002 (0.0025)	SegCLSLoss 0.0326 (0.0347)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0260 (0.0812)	MaskBCELoss 0.0062 (0.0303)	MaskDICELoss 0.0199 (0.0508)
Epoch: [0][212/500]	Time 185.795 (185.795)	Loss 0.3847 (0.4270)	CeLoss 0.0006 (0.0031)	SegCLSLoss 0.0311 (0.0390)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0760 (0.0781)	MaskBCELoss 0.0335 (0.0307)	MaskDICELoss 0.0425 (0.0474)
Epoch: [0][213/500]	Time 188.237 (188.237)	Loss 0.5760 (0.4292)	CeLoss 0.0033 (0.0022)	SegCLSLoss 0.0366 (0.0340)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1155 (0.0800)	MaskBCELoss 0.0485 (0.0302)	MaskDICELoss 0.0671 (0.0498)
Epoch: [0][214/500]	Time 176.850 (176.850)	Loss 0.3827 (0.4518)	CeLoss 0.0037 (0.0032)	SegCLSLoss 0.0297 (0.0366)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0753 (0.0852)	MaskBCELoss 0.0331 (0.0340)	MaskDICELoss 0.0422 (0.0512)
Epoch: [0][215/500]	Time 167.427 (167.427)	Loss 0.3664 (0.4149)	CeLoss 0.0046 (0.0024)	SegCLSLoss 0.0395 (0.0367)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0619 (0.0755)	MaskBCELoss 0.0222 (0.0285)	MaskDICELoss 0.0397 (0.0470)
Epoch: [0][216/500]	Time 175.756 (175.756)	Loss 0.4360 (0.4005)	CeLoss 0.0041 (0.0029)	SegCLSLoss 0.0353 (0.0380)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0858 (0.0707)	MaskBCELoss 0.0384 (0.0257)	MaskDICELoss 0.0474 (0.0451)
Epoch: [0][217/500]	Time 184.363 (184.363)	Loss 0.3516 (0.4277)	CeLoss 0.0012 (0.0034)	SegCLSLoss 0.0443 (0.0414)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0550 (0.0749)	MaskBCELoss 0.0171 (0.0270)	MaskDICELoss 0.0379 (0.0479)
Epoch: [0][218/500]	Time 173.313 (173.313)	Loss 0.3876 (0.3989)	CeLoss 0.0016 (0.0020)	SegCLSLoss 0.0305 (0.0380)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0706 (0.0718)	MaskBCELoss 0.0246 (0.0275)	MaskDICELoss 0.0460 (0.0444)
Epoch: [0][219/500]	Time 189.516 (189.516)	Loss 0.4181 (0.4420)	CeLoss 0.0008 (0.0036)	SegCLSLoss 0.0331 (0.0371)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0786 (0.0797)	MaskBCELoss 0.0302 (0.0285)	MaskDICELoss 0.0485 (0.0512)
Epoch: [0][220/500]	Time 173.257 (173.257)	Loss 0.3859 (0.4171)	CeLoss 0.0036 (0.0032)	SegCLSLoss 0.0303 (0.0374)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0733 (0.0760)	MaskBCELoss 0.0296 (0.0292)	MaskDICELoss 0.0437 (0.0468)
Epoch: [0][221/500]	Time 191.103 (191.103)	Loss 0.5008 (0.4551)	CeLoss 0.0030 (0.0028)	SegCLSLoss 0.0436 (0.0395)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0876 (0.0837)	MaskBCELoss 0.0287 (0.0323)	MaskDICELoss 0.0589 (0.0515)
Epoch: [0][222/500]	Time 170.653 (170.653)	Loss 0.4364 (0.4415)	CeLoss 0.0031 (0.0027)	SegCLSLoss 0.0333 (0.0352)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0946 (0.0846)	MaskBCELoss 0.0503 (0.0348)	MaskDICELoss 0.0444 (0.0498)
Epoch: [0][223/500]	Time 211.199 (211.199)	Loss 0.4152 (0.4820)	CeLoss 0.0016 (0.0024)	SegCLSLoss 0.0361 (0.0346)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0763 (0.0943)	MaskBCELoss 0.0291 (0.0389)	MaskDICELoss 0.0472 (0.0554)
Epoch: [0][224/500]	Time 181.167 (181.167)	Loss 0.4596 (0.4197)	CeLoss 0.0030 (0.0026)	SegCLSLoss 0.0373 (0.0354)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0821 (0.0786)	MaskBCELoss 0.0276 (0.0313)	MaskDICELoss 0.0545 (0.0473)
Epoch: [0][225/500]	Time 168.930 (168.930)	Loss 0.4315 (0.4606)	CeLoss 0.0010 (0.0021)	SegCLSLoss 0.0319 (0.0359)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0785 (0.0868)	MaskBCELoss 0.0260 (0.0335)	MaskDICELoss 0.0524 (0.0533)
Epoch: [0][226/500]	Time 113.871 (113.871)	Loss 0.3009 (0.4161)	CeLoss 0.0013 (0.0021)	SegCLSLoss 0.0298 (0.0313)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0528 (0.0794)	MaskBCELoss 0.0193 (0.0313)	MaskDICELoss 0.0336 (0.0481)
Epoch: [0][227/500]	Time 110.292 (110.292)	Loss 0.3908 (0.3632)	CeLoss 0.0026 (0.0018)	SegCLSLoss 0.0278 (0.0327)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0758 (0.0662)	MaskBCELoss 0.0306 (0.0253)	MaskDICELoss 0.0452 (0.0409)
Epoch: [0][228/500]	Time 103.258 (103.258)	Loss 0.3769 (0.4157)	CeLoss 0.0035 (0.0033)	SegCLSLoss 0.0390 (0.0369)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0610 (0.0760)	MaskBCELoss 0.0176 (0.0293)	MaskDICELoss 0.0434 (0.0467)
Epoch: [0][229/500]	Time 122.511 (122.511)	Loss 0.4148 (0.3979)	CeLoss 0.0035 (0.0035)	SegCLSLoss 0.0438 (0.0373)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0709 (0.0708)	MaskBCELoss 0.0254 (0.0263)	MaskDICELoss 0.0455 (0.0446)
Epoch: [0][230/500]	Time 130.943 (130.943)	Loss 0.5197 (0.4789)	CeLoss 0.0021 (0.0028)	SegCLSLoss 0.0431 (0.0345)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0908 (0.0934)	MaskBCELoss 0.0284 (0.0383)	MaskDICELoss 0.0624 (0.0551)
Epoch: [0][231/500]	Time 107.637 (107.637)	Loss 0.4959 (0.3898)	CeLoss 0.0023 (0.0017)	SegCLSLoss 0.0397 (0.0364)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0926 (0.0702)	MaskBCELoss 0.0354 (0.0265)	MaskDICELoss 0.0572 (0.0437)
Epoch: [0][232/500]	Time 173.562 (173.562)	Loss 0.2297 (0.4194)	CeLoss 0.0005 (0.0016)	SegCLSLoss 0.0291 (0.0355)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0403 (0.0768)	MaskBCELoss 0.0177 (0.0285)	MaskDICELoss 0.0226 (0.0483)
Epoch: [0][233/500]	Time 169.115 (169.115)	Loss 0.3764 (0.4196)	CeLoss 0.0004 (0.0020)	SegCLSLoss 0.0376 (0.0373)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0634 (0.0762)	MaskBCELoss 0.0199 (0.0286)	MaskDICELoss 0.0435 (0.0476)
Epoch: [0][234/500]	Time 168.292 (168.292)	Loss 0.5577 (0.4286)	CeLoss 0.0016 (0.0024)	SegCLSLoss 0.0400 (0.0391)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1034 (0.0761)	MaskBCELoss 0.0361 (0.0272)	MaskDICELoss 0.0673 (0.0489)
Epoch: [0][235/500]	Time 177.504 (177.504)	Loss 0.4441 (0.4156)	CeLoss 0.0021 (0.0021)	SegCLSLoss 0.0334 (0.0378)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0847 (0.0735)	MaskBCELoss 0.0332 (0.0257)	MaskDICELoss 0.0515 (0.0477)
Epoch: [0][236/500]	Time 159.488 (159.488)	Loss 0.4720 (0.4587)	CeLoss 0.0033 (0.0022)	SegCLSLoss 0.0374 (0.0385)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0865 (0.0836)	MaskBCELoss 0.0314 (0.0305)	MaskDICELoss 0.0552 (0.0531)
Epoch: [0][237/500]	Time 111.113 (111.113)	Loss 0.4976 (0.4356)	CeLoss 0.0019 (0.0022)	SegCLSLoss 0.0396 (0.0348)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0931 (0.0802)	MaskBCELoss 0.0355 (0.0293)	MaskDICELoss 0.0576 (0.0509)
Epoch: [0][238/500]	Time 141.699 (141.699)	Loss 0.3444 (0.4551)	CeLoss 0.0010 (0.0019)	SegCLSLoss 0.0359 (0.0391)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0599 (0.0831)	MaskBCELoss 0.0220 (0.0308)	MaskDICELoss 0.0379 (0.0522)
Epoch: [0][239/500]	Time 122.146 (122.146)	Loss 0.4032 (0.4689)	CeLoss 0.0016 (0.0025)	SegCLSLoss 0.0359 (0.0375)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0694 (0.0895)	MaskBCELoss 0.0216 (0.0363)	MaskDICELoss 0.0478 (0.0531)
Epoch: [0][240/500]	Time 115.817 (115.817)	Loss 0.4839 (0.4196)	CeLoss 0.0013 (0.0018)	SegCLSLoss 0.0460 (0.0391)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0835 (0.0734)	MaskBCELoss 0.0275 (0.0253)	MaskDICELoss 0.0559 (0.0482)
Epoch: [0][241/500]	Time 136.299 (136.299)	Loss 0.4510 (0.4301)	CeLoss 0.0023 (0.0014)	SegCLSLoss 0.0290 (0.0308)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0838 (0.0806)	MaskBCELoss 0.0280 (0.0292)	MaskDICELoss 0.0558 (0.0514)
Epoch: [0][242/500]	Time 195.681 (195.681)	Loss 0.5355 (0.4215)	CeLoss 0.0011 (0.0013)	SegCLSLoss 0.0357 (0.0319)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1029 (0.0775)	MaskBCELoss 0.0387 (0.0272)	MaskDICELoss 0.0643 (0.0503)
Epoch: [0][243/500]	Time 183.156 (183.156)	Loss 0.4369 (0.4204)	CeLoss 0.0012 (0.0014)	SegCLSLoss 0.0350 (0.0321)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0786 (0.0779)	MaskBCELoss 0.0264 (0.0281)	MaskDICELoss 0.0522 (0.0498)
Epoch: [0][244/500]	Time 127.851 (127.851)	Loss 0.4620 (0.4321)	CeLoss 0.0014 (0.0014)	SegCLSLoss 0.0309 (0.0309)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0936 (0.0827)	MaskBCELoss 0.0406 (0.0318)	MaskDICELoss 0.0529 (0.0509)
Epoch: [0][245/500]	Time 113.829 (113.829)	Loss 0.4613 (0.4347)	CeLoss 0.0014 (0.0020)	SegCLSLoss 0.0366 (0.0295)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0833 (0.0822)	MaskBCELoss 0.0283 (0.0298)	MaskDICELoss 0.0550 (0.0523)
Epoch: [0][246/500]	Time 134.160 (134.160)	Loss 0.4109 (0.3990)	CeLoss 0.0013 (0.0015)	SegCLSLoss 0.0294 (0.0306)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0848 (0.0739)	MaskBCELoss 0.0394 (0.0268)	MaskDICELoss 0.0453 (0.0471)
Epoch: [0][247/500]	Time 137.601 (137.601)	Loss 0.5883 (0.4467)	CeLoss 0.0015 (0.0013)	SegCLSLoss 0.0342 (0.0306)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1119 (0.0850)	MaskBCELoss 0.0382 (0.0315)	MaskDICELoss 0.0737 (0.0535)
Epoch: [0][248/500]	Time 101.759 (101.759)	Loss 0.4606 (0.4348)	CeLoss 0.0023 (0.0016)	SegCLSLoss 0.0372 (0.0308)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0821 (0.0830)	MaskBCELoss 0.0272 (0.0317)	MaskDICELoss 0.0549 (0.0514)
Epoch: [0][249/500]	Time 123.441 (123.441)	Loss 0.3284 (0.4321)	CeLoss 0.0016 (0.0014)	SegCLSLoss 0.0240 (0.0296)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0639 (0.0824)	MaskBCELoss 0.0261 (0.0307)	MaskDICELoss 0.0377 (0.0517)
Epoch: [0][250/500]	Time 116.403 (116.403)	Loss 0.2574 (0.4122)	CeLoss 0.0022 (0.0023)	SegCLSLoss 0.0257 (0.0314)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0432 (0.0757)	MaskBCELoss 0.0138 (0.0268)	MaskDICELoss 0.0294 (0.0489)
Epoch: [0][251/500]	Time 125.842 (125.842)	Loss 0.4067 (0.3854)	CeLoss 0.0023 (0.0011)	SegCLSLoss 0.0224 (0.0190)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0774 (0.0760)	MaskBCELoss 0.0262 (0.0274)	MaskDICELoss 0.0512 (0.0486)
Epoch: [0][252/500]	Time 120.337 (120.337)	Loss 0.5124 (0.3785)	CeLoss 0.0020 (0.0018)	SegCLSLoss 0.0233 (0.0199)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0971 (0.0734)	MaskBCELoss 0.0297 (0.0258)	MaskDICELoss 0.0674 (0.0476)
Epoch: [0][253/500]	Time 129.017 (129.017)	Loss 0.4340 (0.3838)	CeLoss 0.0009 (0.0013)	SegCLSLoss 0.0230 (0.0215)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0837 (0.0753)	MaskBCELoss 0.0288 (0.0280)	MaskDICELoss 0.0549 (0.0472)
Epoch: [0][254/500]	Time 109.150 (109.150)	Loss 0.4297 (0.4033)	CeLoss 0.0008 (0.0011)	SegCLSLoss 0.0184 (0.0216)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0867 (0.0787)	MaskBCELoss 0.0321 (0.0283)	MaskDICELoss 0.0547 (0.0504)
Epoch: [0][255/500]	Time 132.771 (132.771)	Loss 0.3552 (0.4090)	CeLoss 0.0008 (0.0012)	SegCLSLoss 0.0171 (0.0210)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0723 (0.0800)	MaskBCELoss 0.0284 (0.0286)	MaskDICELoss 0.0439 (0.0515)
Epoch: [0][256/500]	Time 113.048 (113.048)	Loss 0.3834 (0.3983)	CeLoss 0.0034 (0.0017)	SegCLSLoss 0.0235 (0.0209)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0725 (0.0789)	MaskBCELoss 0.0255 (0.0297)	MaskDICELoss 0.0470 (0.0492)
Epoch: [0][257/500]	Time 116.945 (116.945)	Loss 0.3699 (0.3806)	CeLoss 0.0009 (0.0013)	SegCLSLoss 0.0260 (0.0199)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0713 (0.0755)	MaskBCELoss 0.0277 (0.0284)	MaskDICELoss 0.0436 (0.0471)
Epoch: [0][258/500]	Time 123.584 (123.584)	Loss 0.2613 (0.3301)	CeLoss 0.0010 (0.0012)	SegCLSLoss 0.0191 (0.0182)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0455 (0.0640)	MaskBCELoss 0.0127 (0.0229)	MaskDICELoss 0.0328 (0.0411)
Epoch: [0][259/500]	Time 135.947 (135.947)	Loss 0.2392 (0.3632)	CeLoss 0.0005 (0.0011)	SegCLSLoss 0.0137 (0.0206)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0453 (0.0696)	MaskBCELoss 0.0152 (0.0242)	MaskDICELoss 0.0302 (0.0454)
Epoch: [0][260/500]	Time 98.800 (98.800)	Loss 0.3693 (0.3743)	CeLoss 0.0034 (0.0016)	SegCLSLoss 0.0160 (0.0197)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0734 (0.0736)	MaskBCELoss 0.0266 (0.0272)	MaskDICELoss 0.0468 (0.0465)
Epoch: [0][261/500]	Time 123.540 (123.540)	Loss 0.4320 (0.3814)	CeLoss 0.0020 (0.0017)	SegCLSLoss 0.0234 (0.0208)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0806 (0.0763)	MaskBCELoss 0.0251 (0.0299)	MaskDICELoss 0.0555 (0.0464)
Epoch: [0][262/500]	Time 117.586 (117.586)	Loss 0.4232 (0.4304)	CeLoss 0.0012 (0.0014)	SegCLSLoss 0.0265 (0.0220)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0913 (0.0884)	MaskBCELoss 0.0448 (0.0364)	MaskDICELoss 0.0466 (0.0521)
Epoch: [0][263/500]	Time 128.105 (128.105)	Loss 0.4824 (0.3627)	CeLoss 0.0009 (0.0012)	SegCLSLoss 0.0209 (0.0207)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0946 (0.0693)	MaskBCELoss 0.0320 (0.0240)	MaskDICELoss 0.0626 (0.0453)
Epoch: [0][264/500]	Time 105.769 (105.769)	Loss 0.3889 (0.3954)	CeLoss 0.0005 (0.0016)	SegCLSLoss 0.0152 (0.0194)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0764 (0.0779)	MaskBCELoss 0.0251 (0.0281)	MaskDICELoss 0.0513 (0.0498)
Epoch: [0][265/500]	Time 121.803 (121.803)	Loss 0.4355 (0.4186)	CeLoss 0.0010 (0.0022)	SegCLSLoss 0.0197 (0.0217)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0836 (0.0804)	MaskBCELoss 0.0266 (0.0273)	MaskDICELoss 0.0570 (0.0531)
Epoch: [0][266/500]	Time 120.689 (120.689)	Loss 0.3932 (0.3719)	CeLoss 0.0014 (0.0013)	SegCLSLoss 0.0164 (0.0215)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0771 (0.0736)	MaskBCELoss 0.0260 (0.0285)	MaskDICELoss 0.0512 (0.0451)
Epoch: [0][267/500]	Time 121.590 (121.590)	Loss 0.3933 (0.3634)	CeLoss 0.0018 (0.0014)	SegCLSLoss 0.0208 (0.0216)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0734 (0.0712)	MaskBCELoss 0.0227 (0.0271)	MaskDICELoss 0.0508 (0.0441)
Epoch: [0][268/500]	Time 112.432 (112.432)	Loss 0.3408 (0.3875)	CeLoss 0.0012 (0.0014)	SegCLSLoss 0.0226 (0.0234)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0669 (0.0765)	MaskBCELoss 0.0267 (0.0299)	MaskDICELoss 0.0402 (0.0466)
Epoch: [0][269/500]	Time 110.438 (110.438)	Loss 0.3100 (0.4033)	CeLoss 0.0017 (0.0015)	SegCLSLoss 0.0176 (0.0230)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0575 (0.0792)	MaskBCELoss 0.0179 (0.0299)	MaskDICELoss 0.0395 (0.0493)
Epoch: [0][270/500]	Time 151.393 (151.393)	Loss 0.3434 (0.3625)	CeLoss 0.0007 (0.0014)	SegCLSLoss 0.0233 (0.0226)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0626 (0.0690)	MaskBCELoss 0.0199 (0.0245)	MaskDICELoss 0.0427 (0.0445)
Epoch: [0][271/500]	Time 191.594 (191.594)	Loss 0.5089 (0.4210)	CeLoss 0.0022 (0.0018)	SegCLSLoss 0.0298 (0.0174)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1017 (0.0834)	MaskBCELoss 0.0408 (0.0289)	MaskDICELoss 0.0609 (0.0544)
Epoch: [0][272/500]	Time 200.841 (200.841)	Loss 0.4673 (0.4294)	CeLoss 0.0010 (0.0013)	SegCLSLoss 0.0152 (0.0168)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1030 (0.0870)	MaskBCELoss 0.0455 (0.0319)	MaskDICELoss 0.0575 (0.0551)
Epoch: [0][273/500]	Time 175.294 (175.294)	Loss 0.3450 (0.3525)	CeLoss 0.0008 (0.0013)	SegCLSLoss 0.0169 (0.0165)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0655 (0.0691)	MaskBCELoss 0.0207 (0.0241)	MaskDICELoss 0.0448 (0.0450)
Epoch: [0][274/500]	Time 183.813 (183.813)	Loss 0.3304 (0.4048)	CeLoss 0.0007 (0.0013)	SegCLSLoss 0.0224 (0.0193)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0612 (0.0800)	MaskBCELoss 0.0205 (0.0287)	MaskDICELoss 0.0406 (0.0512)
Epoch: [0][275/500]	Time 169.352 (169.352)	Loss 0.4157 (0.3299)	CeLoss 0.0009 (0.0014)	SegCLSLoss 0.0299 (0.0166)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0775 (0.0645)	MaskBCELoss 0.0275 (0.0229)	MaskDICELoss 0.0500 (0.0416)
Epoch: [0][276/500]	Time 180.838 (180.838)	Loss 0.4372 (0.3892)	CeLoss 0.0022 (0.0012)	SegCLSLoss 0.0238 (0.0186)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0836 (0.0756)	MaskBCELoss 0.0286 (0.0257)	MaskDICELoss 0.0550 (0.0499)
Epoch: [0][277/500]	Time 168.909 (168.909)	Loss 0.1206 (0.3544)	CeLoss 0.0008 (0.0012)	SegCLSLoss 0.0141 (0.0166)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0182 (0.0706)	MaskBCELoss 0.0045 (0.0260)	MaskDICELoss 0.0138 (0.0447)
Epoch: [0][278/500]	Time 194.810 (194.810)	Loss 0.5733 (0.3883)	CeLoss 0.0010 (0.0011)	SegCLSLoss 0.0171 (0.0153)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1230 (0.0773)	MaskBCELoss 0.0499 (0.0268)	MaskDICELoss 0.0730 (0.0505)
Epoch: [0][279/500]	Time 185.524 (185.524)	Loss 0.3780 (0.3751)	CeLoss 0.0003 (0.0014)	SegCLSLoss 0.0164 (0.0197)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0781 (0.0706)	MaskBCELoss 0.0310 (0.0223)	MaskDICELoss 0.0471 (0.0483)
Epoch: [0][280/500]	Time 176.973 (176.973)	Loss 0.4258 (0.4126)	CeLoss 0.0011 (0.0014)	SegCLSLoss 0.0165 (0.0179)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0852 (0.0823)	MaskBCELoss 0.0298 (0.0296)	MaskDICELoss 0.0554 (0.0527)
Epoch: [0][281/500]	Time 182.306 (182.306)	Loss 0.4053 (0.3888)	CeLoss 0.0009 (0.0012)	SegCLSLoss 0.0262 (0.0157)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0766 (0.0802)	MaskBCELoss 0.0269 (0.0312)	MaskDICELoss 0.0497 (0.0490)
Epoch: [0][282/500]	Time 187.452 (187.452)	Loss 0.3540 (0.3507)	CeLoss 0.0008 (0.0013)	SegCLSLoss 0.0200 (0.0143)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0662 (0.0694)	MaskBCELoss 0.0210 (0.0239)	MaskDICELoss 0.0452 (0.0455)
Epoch: [0][283/500]	Time 121.587 (121.587)	Loss 0.5235 (0.3469)	CeLoss 0.0016 (0.0013)	SegCLSLoss 0.0128 (0.0132)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1091 (0.0698)	MaskBCELoss 0.0395 (0.0249)	MaskDICELoss 0.0695 (0.0449)
Epoch: [0][284/500]	Time 172.732 (172.732)	Loss 0.4304 (0.3812)	CeLoss 0.0004 (0.0009)	SegCLSLoss 0.0138 (0.0159)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0873 (0.0778)	MaskBCELoss 0.0304 (0.0295)	MaskDICELoss 0.0570 (0.0482)
Epoch: [0][285/500]	Time 142.599 (142.599)	Loss 0.4425 (0.3386)	CeLoss 0.0018 (0.0012)	SegCLSLoss 0.0167 (0.0140)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0904 (0.0666)	MaskBCELoss 0.0338 (0.0225)	MaskDICELoss 0.0566 (0.0441)
Epoch: [0][286/500]	Time 134.373 (134.373)	Loss 0.3515 (0.3719)	CeLoss 0.0014 (0.0011)	SegCLSLoss 0.0124 (0.0147)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0688 (0.0749)	MaskBCELoss 0.0219 (0.0270)	MaskDICELoss 0.0469 (0.0479)
Epoch: [0][287/500]	Time 135.613 (135.613)	Loss 0.3538 (0.4161)	CeLoss 0.0007 (0.0013)	SegCLSLoss 0.0155 (0.0150)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0731 (0.0836)	MaskBCELoss 0.0291 (0.0291)	MaskDICELoss 0.0440 (0.0544)
Epoch: [0][288/500]	Time 146.222 (146.222)	Loss 0.3820 (0.3892)	CeLoss 0.0011 (0.0009)	SegCLSLoss 0.0150 (0.0146)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0834 (0.0798)	MaskBCELoss 0.0374 (0.0299)	MaskDICELoss 0.0460 (0.0499)
Epoch: [0][289/500]	Time 126.693 (126.693)	Loss 0.2546 (0.3796)	CeLoss 0.0005 (0.0011)	SegCLSLoss 0.0100 (0.0134)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0519 (0.0765)	MaskBCELoss 0.0193 (0.0269)	MaskDICELoss 0.0326 (0.0497)
Epoch: [0][290/500]	Time 124.172 (124.172)	Loss 0.3008 (0.3568)	CeLoss 0.0005 (0.0016)	SegCLSLoss 0.0134 (0.0150)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0591 (0.0696)	MaskBCELoss 0.0202 (0.0231)	MaskDICELoss 0.0388 (0.0465)
Epoch: [0][291/500]	Time 166.617 (166.617)	Loss 0.3073 (0.3338)	CeLoss 0.0004 (0.0008)	SegCLSLoss 0.0118 (0.0111)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0616 (0.0693)	MaskBCELoss 0.0216 (0.0262)	MaskDICELoss 0.0400 (0.0430)
Epoch: [0][292/500]	Time 154.044 (154.044)	Loss 0.2973 (0.3156)	CeLoss 0.0010 (0.0011)	SegCLSLoss 0.0157 (0.0140)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0575 (0.0622)	MaskBCELoss 0.0200 (0.0217)	MaskDICELoss 0.0375 (0.0405)
Epoch: [0][293/500]	Time 120.402 (120.402)	Loss 0.4131 (0.3834)	CeLoss 0.0011 (0.0012)	SegCLSLoss 0.0191 (0.0138)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0783 (0.0778)	MaskBCELoss 0.0239 (0.0281)	MaskDICELoss 0.0543 (0.0497)
Epoch: [0][294/500]	Time 116.449 (116.449)	Loss 0.3986 (0.4225)	CeLoss 0.0009 (0.0007)	SegCLSLoss 0.0106 (0.0138)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0819 (0.0866)	MaskBCELoss 0.0287 (0.0313)	MaskDICELoss 0.0532 (0.0553)
Epoch: [0][295/500]	Time 100.938 (100.938)	Loss 0.1759 (0.3245)	CeLoss 0.0018 (0.0006)	SegCLSLoss 0.0148 (0.0136)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0315 (0.0657)	MaskBCELoss 0.0111 (0.0244)	MaskDICELoss 0.0204 (0.0413)
Epoch: [0][296/500]	Time 148.103 (148.103)	Loss 0.2708 (0.3001)	CeLoss 0.0008 (0.0010)	SegCLSLoss 0.0120 (0.0144)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0539 (0.0593)	MaskBCELoss 0.0194 (0.0214)	MaskDICELoss 0.0345 (0.0379)
Epoch: [0][297/500]	Time 101.206 (101.206)	Loss 0.2450 (0.3341)	CeLoss 0.0003 (0.0007)	SegCLSLoss 0.0119 (0.0118)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0511 (0.0711)	MaskBCELoss 0.0214 (0.0292)	MaskDICELoss 0.0297 (0.0419)
Epoch: [0][298/500]	Time 108.024 (108.024)	Loss 0.2882 (0.3642)	CeLoss 0.0008 (0.0007)	SegCLSLoss 0.0097 (0.0130)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0566 (0.0759)	MaskBCELoss 0.0180 (0.0295)	MaskDICELoss 0.0387 (0.0464)
Epoch: [0][299/500]	Time 108.285 (108.285)	Loss 0.4366 (0.3354)	CeLoss 0.0013 (0.0012)	SegCLSLoss 0.0183 (0.0128)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0871 (0.0661)	MaskBCELoss 0.0310 (0.0219)	MaskDICELoss 0.0561 (0.0441)
[2025-04-12 23:29:23,817] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[0.0002972650602409638], mom=[(0.9, 0.95)]
[2025-04-12 23:29:23,833] [INFO] [timer.py:215:stop] epoch=0/micro_step=300/global_step=30, RunningAvgSamplesPerSec=0.7614317144495052, CurrSamplesPerSec=0.6785106426544638, MemAllocated=35.37GB, MaxMemAllocated=48.48GB
Epoch: [0][300/500]	Time 100.609 (100.609)	Loss 0.3744 (0.3518)	CeLoss 0.0022 (0.0011)	SegCLSLoss 0.0096 (0.0122)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0741 (0.0724)	MaskBCELoss 0.0229 (0.0270)	MaskDICELoss 0.0512 (0.0454)
Epoch: [0][301/500]	Time 128.339 (128.339)	Loss 0.3737 (0.3671)	CeLoss 0.0005 (0.0009)	SegCLSLoss 0.0126 (0.0110)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0738 (0.0762)	MaskBCELoss 0.0236 (0.0283)	MaskDICELoss 0.0501 (0.0479)
Epoch: [0][302/500]	Time 149.515 (149.515)	Loss 0.4000 (0.4018)	CeLoss 0.0007 (0.0010)	SegCLSLoss 0.0131 (0.0107)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0833 (0.0837)	MaskBCELoss 0.0317 (0.0307)	MaskDICELoss 0.0516 (0.0530)
Epoch: [0][303/500]	Time 170.548 (170.548)	Loss 0.3469 (0.3520)	CeLoss 0.0006 (0.0009)	SegCLSLoss 0.0125 (0.0115)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0693 (0.0719)	MaskBCELoss 0.0236 (0.0257)	MaskDICELoss 0.0457 (0.0461)
Epoch: [0][304/500]	Time 164.622 (164.622)	Loss 0.2906 (0.3337)	CeLoss 0.0008 (0.0006)	SegCLSLoss 0.0090 (0.0113)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0611 (0.0685)	MaskBCELoss 0.0237 (0.0251)	MaskDICELoss 0.0374 (0.0434)
Epoch: [0][305/500]	Time 146.223 (146.223)	Loss 0.3347 (0.3536)	CeLoss 0.0001 (0.0009)	SegCLSLoss 0.0161 (0.0115)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0673 (0.0723)	MaskBCELoss 0.0253 (0.0261)	MaskDICELoss 0.0420 (0.0463)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 844, in <module>
[rank0]:     main(sys.argv[1:])
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 529, in main
[rank0]:     train_iter = train(
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 634, in train
[rank0]:     output_dict = model(**input_dict)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1735, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/peft/peft_model.py", line 922, in forward
[rank0]:     return self.base_model(
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py", line 532, in forward
[rank0]:     return self.model_forward(**kwargs)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py", line 550, in model_forward
[rank0]:     image_embeddings = self.get_visual_embs(images)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py", line 521, in get_visual_embs
[rank0]:     image_embeddings = self.model.visual_model.image_encoder(
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/segment_anything/modeling/image_encoder.py", line 116, in forward
[rank0]:     x = blk(x)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/segment_anything/modeling/image_encoder.py", line 185, in forward
[rank0]:     x = self.attn(x)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/segment_anything/modeling/image_encoder.py", line 247, in forward
[rank0]:     attn = add_decomposed_rel_pos(
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/segment_anything/modeling/image_encoder.py", line 387, in add_decomposed_rel_pos
[rank0]:     attn.view(B, q_h, q_w, k_h, k_w)
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU