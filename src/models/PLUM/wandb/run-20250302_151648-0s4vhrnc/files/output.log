You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565



Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.83s/it]
trainable params: 6,553,600 || all params: 14,025,701,683 || trainable%: 0.0467256480147682
n:  base_model.model.model.embed_tokens.weight p.shape:  torch.Size([32002, 5120])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_token.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.mask_decoder.mask_tokens.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.weight p.shape:  torch.Size([256, 64, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.weight p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.weight p.shape:  torch.Size([64, 32, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.bias p.shape:  torch.Size([4])
n:  base_model.model.model.text_hidden_fcs.0.0.weight p.shape:  torch.Size([5120, 5120])
n:  base_model.model.model.text_hidden_fcs.0.0.bias p.shape:  torch.Size([5120])
n:  base_model.model.model.text_hidden_fcs.0.2.weight p.shape:  torch.Size([256, 5120])
n:  base_model.model.model.text_hidden_fcs.0.2.bias p.shape:  torch.Size([256])
n:  base_model.model.lm_head.weight p.shape:  torch.Size([32002, 5120])
ade20k:  20210
cocostuff:  118287
loading annotations into memory...
Done (t=0.53s)
creating index...
index created!
pascal_part:  4366
loading annotations into memory...
Done (t=7.43s)
creating index...
index created!
paco_lvis:  45790
mapillary:  18000
loading dataset refclef into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refclef/refs(unc).p
creating index...
index created.
DONE (t=2.64s)
dataset refclef (refs unc) (train split) has 17978 images and 99523 annotations.
loading dataset refcoco into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco/refs(unc).p
creating index...
index created.
DONE (t=5.12s)
dataset refcoco (refs unc) (train split) has 16994 images and 196771 annotations.
loading dataset refcoco+ into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco+/refs(unc).p
creating index...
index created.
DONE (t=6.02s)
dataset refcoco+ (refs unc) (train split) has 16992 images and 196737 annotations.
loading dataset refcocog into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcocog/refs(umd).p
creating index...
index created.
DONE (t=3.36s)
dataset refcocog (refs umd) (train split) has 21899 images and 208960 annotations.
vqa_data:  157712
number of reason_seg samples:  239
len(self.img_to_explanation):  239
Training with 5000 examples and validating with 200 examples.
[2025-03-02 15:18:14,920] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5, git-hash=unknown, git-branch=unknown
[2025-03-02 15:18:14,920] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2025-03-02 15:18:14,920] [INFO] [comm.py:594:init_distributed] cdb=None
[2025-03-02 15:18:14,920] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Using /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module fused_adam...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/tensor/python_tensor.cpp:78.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
[2025-03-02 15:18:27,742] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Time to load fused_adam op: 0.12407422065734863 seconds
[2025-03-02 15:18:28,048] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-03-02 15:18:28,236] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-03-02 15:18:28,236] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-03-02 15:18:28,236] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-03-02 15:18:28,236] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500000000
[2025-03-02 15:18:28,236] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500000000
[2025-03-02 15:18:28,236] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2025-03-02 15:18:28,236] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(365842916, False)]
[2025-03-02 15:18:32,413] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2025-03-02 15:18:32,413] [INFO] [utils.py:786:see_memory_usage] MA 27.69 GB         Max_MA 28.37 GB         CA 28.51 GB         Max_CA 29 GB
[2025-03-02 15:18:32,414] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 82.57 GB, percent = 8.2%
[2025-03-02 15:18:35,439] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2025-03-02 15:18:35,440] [INFO] [utils.py:786:see_memory_usage] MA 30.41 GB         Max_MA 31.78 GB         CA 32.6 GB         Max_CA 33 GB
[2025-03-02 15:18:35,440] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 82.58 GB, percent = 8.2%
[2025-03-02 15:18:35,440] [INFO] [stage_1_and_2.py:488:__init__] optimizer state initialized
[2025-03-02 15:18:38,353] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2025-03-02 15:18:38,354] [INFO] [utils.py:786:see_memory_usage] MA 30.41 GB         Max_MA 30.41 GB         CA 32.6 GB         Max_CA 33 GB
[2025-03-02 15:18:38,355] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 82.58 GB, percent = 8.2%
[2025-03-02 15:18:38,360] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-03-02 15:18:38,360] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2025-03-02 15:18:38,361] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7fcd67098a30>
[2025-03-02 15:18:38,361] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
[2025-03-02 15:18:38,363] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2025-03-02 15:18:38,364] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-03-02 15:18:38,364] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-03-02 15:18:38,364] [INFO] [config.py:964:print]   amp_enabled .................. False
[2025-03-02 15:18:38,364] [INFO] [config.py:964:print]   amp_params ................... False
[2025-03-02 15:18:38,364] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-02 15:18:38,364] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2025-03-02 15:18:38,364] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2025-03-02 15:18:38,364] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2025-03-02 15:18:38,364] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2025-03-02 15:18:38,364] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fcc7d4350f0>
[2025-03-02 15:18:38,364] [INFO] [config.py:964:print]   communication_data_type ...... None
[2025-03-02 15:18:38,364] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-02 15:18:38,364] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2025-03-02 15:18:38,365] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2025-03-02 15:18:38,365] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-02 15:18:38,365] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2025-03-02 15:18:38,365] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2025-03-02 15:18:38,365] [INFO] [config.py:964:print]   disable_allgather ............ False
[2025-03-02 15:18:38,365] [INFO] [config.py:964:print]   dump_state ................... False
[2025-03-02 15:18:38,365] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2025-03-02 15:18:38,365] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2025-03-02 15:18:38,365] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-02 15:18:38,365] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-02 15:18:38,365] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2025-03-02 15:18:38,365] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2025-03-02 15:18:38,365] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2025-03-02 15:18:38,365] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2025-03-02 15:18:38,365] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2025-03-02 15:18:38,365] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2025-03-02 15:18:38,365] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-03-02 15:18:38,365] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2025-03-02 15:18:38,365] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2025-03-02 15:18:38,365] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2025-03-02 15:18:38,365] [INFO] [config.py:964:print]   global_rank .................. 0
[2025-03-02 15:18:38,366] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2025-03-02 15:18:38,366] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 10
[2025-03-02 15:18:38,366] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0
[2025-03-02 15:18:38,366] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2025-03-02 15:18:38,366] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-02 15:18:38,366] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2025-03-02 15:18:38,366] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2025-03-02 15:18:38,366] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2025-03-02 15:18:38,366] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2025-03-02 15:18:38,366] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2025-03-02 15:18:38,366] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2025-03-02 15:18:38,366] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-03-02 15:18:38,366] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-03-02 15:18:38,366] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2025-03-02 15:18:38,366] [INFO] [config.py:964:print]   optimizer_name ............... adamw
[2025-03-02 15:18:38,366] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.0003, 'weight_decay': 0.0, 'betas': (0.9, 0.95)}
[2025-03-02 15:18:38,366] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2025-03-02 15:18:38,366] [INFO] [config.py:964:print]   pld_enabled .................. False
[2025-03-02 15:18:38,367] [INFO] [config.py:964:print]   pld_params ................... False
[2025-03-02 15:18:38,367] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2025-03-02 15:18:38,367] [INFO] [config.py:964:print]   scheduler_name ............... WarmupDecayLR
[2025-03-02 15:18:38,367] [INFO] [config.py:964:print]   scheduler_params ............. {'total_num_steps': 5000, 'warmup_min_lr': 0, 'warmup_max_lr': 0.0003, 'warmup_num_steps': 100, 'warmup_type': 'linear'}
[2025-03-02 15:18:38,367] [INFO] [config.py:964:print]   sparse_attention ............. None
[2025-03-02 15:18:38,367] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2025-03-02 15:18:38,367] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2025-03-02 15:18:38,367] [INFO] [config.py:964:print]   train_batch_size ............. 10
[2025-03-02 15:18:38,367] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2025-03-02 15:18:38,367] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2025-03-02 15:18:38,367] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2025-03-02 15:18:38,367] [INFO] [config.py:964:print]   world_size ................... 1
[2025-03-02 15:18:38,367] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2025-03-02 15:18:38,367] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2025-03-02 15:18:38,367] [INFO] [config.py:964:print]   zero_enabled ................. True
[2025-03-02 15:18:38,367] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-02 15:18:38,367] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2025-03-02 15:18:38,368] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1,
    "gradient_accumulation_steps": 10,
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 0.0003,
            "weight_decay": 0.0,
            "betas": [0.9, 0.95]
        }
    },
    "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "total_num_steps": 5.000000e+03,
            "warmup_min_lr": 0,
            "warmup_max_lr": 0.0003,
            "warmup_num_steps": 100,
            "warmup_type": "linear"
        }
    },
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "gradient_clipping": 1.0,
    "zero_optimization": {
        "stage": 2,
        "contiguous_gradients": true,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 5.000000e+08,
        "allgather_bucket_size": 5.000000e+08
    }
}
(train) >> AFTER DEEPSPEED
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)
  return F.conv2d(input, weight, bias, self.stride,
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([334, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch: [0][  1/500]	Time 10.711 (10.711)	Loss 6.5589 (6.8902)	CeLoss 3.2344 (2.7008)	SegCLSLoss 1.0859 (0.7711)	KLLoss 0.5195 (0.4033)	MaskLoss 1.3654 (1.8822)	MaskBCELoss 0.5910 (1.2203)	MaskDICELoss 0.7744 (0.6619)
Epoch: [0][  2/500]	Time  9.858 ( 9.858)	Loss 0.7891 (4.1061)	CeLoss 0.7891 (1.9027)	SegCLSLoss 0.0000 (0.6492)	KLLoss 0.0000 (0.3217)	MaskLoss 0.0000 (0.9228)	MaskBCELoss 0.0000 (0.3325)	MaskDICELoss 0.0000 (0.5903)
Epoch: [0][  3/500]	Time 12.189 (12.189)	Loss 5.7668 (5.6504)	CeLoss 2.5625 (2.5305)	SegCLSLoss 1.0781 (0.9844)	KLLoss 0.4941 (0.4818)	MaskLoss 1.3131 (1.2897)	MaskBCELoss 0.3147 (0.3936)	MaskDICELoss 0.9983 (0.8960)
Epoch: [0][  4/500]	Time 10.568 (10.568)	Loss 5.1074 (4.9360)	CeLoss 2.4531 (2.3301)	SegCLSLoss 1.0469 (0.8578)	KLLoss 0.5078 (0.4318)	MaskLoss 1.0459 (1.0670)	MaskBCELoss 0.0601 (0.2713)	MaskDICELoss 0.9859 (0.7957)
Epoch: [0][  5/500]	Time 10.698 (10.698)	Loss 0.9492 (5.8692)	CeLoss 0.9492 (2.7090)	SegCLSLoss 0.0000 (0.8641)	KLLoss 0.0000 (0.5082)	MaskLoss 0.0000 (1.3395)	MaskBCELoss 0.0000 (0.5451)	MaskDICELoss 0.0000 (0.7944)
Epoch: [0][  6/500]	Time 10.169 (10.169)	Loss 5.8323 (4.7143)	CeLoss 2.9844 (2.3203)	SegCLSLoss 1.1016 (0.7500)	KLLoss 0.5508 (0.3865)	MaskLoss 1.1193 (0.9900)	MaskBCELoss 0.1400 (0.3062)	MaskDICELoss 0.9793 (0.6838)
Epoch: [0][  7/500]	Time 12.223 (12.223)	Loss 7.4341 (5.6690)	CeLoss 3.5938 (2.7328)	SegCLSLoss 1.1562 (0.9922)	KLLoss 0.6719 (0.5287)	MaskLoss 1.5920 (1.1908)	MaskBCELoss 0.6209 (0.3115)	MaskDICELoss 0.9711 (0.8792)
Epoch: [0][  8/500]	Time  7.482 ( 7.482)	Loss 5.4470 (4.2243)	CeLoss 2.7188 (2.2598)	SegCLSLoss 1.1094 (0.7578)	KLLoss 0.4727 (0.3641)	MaskLoss 1.0594 (0.7752)	MaskBCELoss 0.0704 (0.0981)	MaskDICELoss 0.9890 (0.6771)
Epoch: [0][  9/500]	Time 10.553 (10.553)	Loss 1.6328 (4.3941)	CeLoss 1.6328 (2.2477)	SegCLSLoss 0.0000 (0.7727)	KLLoss 0.0000 (0.4027)	MaskLoss 0.0000 (0.8584)	MaskBCELoss 0.0000 (0.1970)	MaskDICELoss 0.0000 (0.6614)
[2025-03-02 15:20:25,792] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[5.1e-05], mom=[(0.9, 0.95)]
[2025-03-02 15:20:25,800] [INFO] [timer.py:215:stop] epoch=0/micro_step=100/global_step=10, RunningAvgSamplesPerSec=0.9230607797410019, CurrSamplesPerSec=0.7811128169944719, MemAllocated=31.25GB, MaxMemAllocated=36.09GB
Epoch: [0][ 10/500]	Time 12.804 (12.804)	Loss 5.7598 (5.2163)	CeLoss 2.3594 (2.3898)	SegCLSLoss 1.0312 (0.9867)	KLLoss 0.4492 (0.4965)	MaskLoss 1.4190 (1.1433)	MaskBCELoss 0.6411 (0.3220)	MaskDICELoss 0.7779 (0.8213)
Epoch: [0][ 11/500]	Time 10.098 (10.098)	Loss 5.2701 (4.2168)	CeLoss 2.3125 (1.9859)	SegCLSLoss 1.1406 (0.7727)	KLLoss 0.5664 (0.4234)	MaskLoss 1.1663 (0.9006)	MaskBCELoss 0.1963 (0.2449)	MaskDICELoss 0.9700 (0.6557)
Epoch: [0][ 12/500]	Time 12.013 (12.013)	Loss 5.3330 (4.5232)	CeLoss 2.2969 (1.9125)	SegCLSLoss 1.1328 (0.8594)	KLLoss 0.6602 (0.4541)	MaskLoss 1.2055 (1.0670)	MaskBCELoss 0.2136 (0.3588)	MaskDICELoss 0.9919 (0.7083)
Epoch: [0][ 13/500]	Time 12.536 (12.536)	Loss 4.9298 (4.8627)	CeLoss 1.9062 (2.0836)	SegCLSLoss 1.1016 (0.9984)	KLLoss 0.5078 (0.5156)	MaskLoss 1.2149 (1.1134)	MaskBCELoss 0.2623 (0.2518)	MaskDICELoss 0.9526 (0.8616)
Epoch: [0][ 14/500]	Time 12.071 (12.071)	Loss 4.1725 (4.6758)	CeLoss 1.3516 (1.8687)	SegCLSLoss 1.0547 (0.9766)	KLLoss 0.4961 (0.5115)	MaskLoss 1.1253 (1.1356)	MaskBCELoss 0.1271 (0.2946)	MaskDICELoss 0.9982 (0.8410)
Epoch: [0][ 15/500]	Time 10.195 (10.195)	Loss 1.2188 (3.5981)	CeLoss 1.2188 (1.5781)	SegCLSLoss 0.0000 (0.7711)	KLLoss 0.0000 (0.3570)	MaskLoss 0.0000 (0.7983)	MaskBCELoss 0.0000 (0.1265)	MaskDICELoss 0.0000 (0.6717)
Epoch: [0][ 16/500]	Time  8.919 ( 8.919)	Loss 0.6992 (3.2486)	CeLoss 0.6992 (1.3680)	SegCLSLoss 0.0000 (0.6484)	KLLoss 0.0000 (0.3211)	MaskLoss 0.0000 (0.7622)	MaskBCELoss 0.0000 (0.1866)	MaskDICELoss 0.0000 (0.5756)
Epoch: [0][ 17/500]	Time 11.508 (11.508)	Loss 5.0316 (3.5934)	CeLoss 1.6719 (1.3309)	SegCLSLoss 1.0156 (0.7602)	KLLoss 0.4277 (0.3748)	MaskLoss 1.3986 (0.9233)	MaskBCELoss 0.4765 (0.2722)	MaskDICELoss 0.9221 (0.6511)
Epoch: [0][ 18/500]	Time  9.930 ( 9.930)	Loss 3.5905 (3.2741)	CeLoss 0.9297 (1.2969)	SegCLSLoss 1.0625 (0.7578)	KLLoss 0.4785 (0.3666)	MaskLoss 1.0414 (0.7816)	MaskBCELoss 0.0471 (0.0973)	MaskDICELoss 0.9942 (0.6843)
Epoch: [0][ 19/500]	Time 12.576 (12.576)	Loss 4.6210 (3.2080)	CeLoss 1.3047 (1.2281)	SegCLSLoss 1.0469 (0.6406)	KLLoss 0.6914 (0.3820)	MaskLoss 1.3613 (0.8109)	MaskBCELoss 0.3967 (0.2318)	MaskDICELoss 0.9646 (0.5790)
[2025-03-02 15:22:16,863] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[0.00011099999999999999], mom=[(0.9, 0.95)]
[2025-03-02 15:22:16,868] [INFO] [timer.py:215:stop] epoch=0/micro_step=200/global_step=20, RunningAvgSamplesPerSec=0.9103735022306447, CurrSamplesPerSec=0.891237017015996, MemAllocated=31.55GB, MaxMemAllocated=36.09GB
Epoch: [0][ 20/500]	Time 11.222 (11.222)	Loss 1.0703 (3.1747)	CeLoss 1.0703 (1.0457)	SegCLSLoss 0.0000 (0.7320)	KLLoss 0.0000 (0.3697)	MaskLoss 0.0000 (0.8632)	MaskBCELoss 0.0000 (0.1911)	MaskDICELoss 0.0000 (0.6721)
Epoch: [0][ 21/500]	Time  9.625 ( 9.625)	Loss 4.8611 (3.5301)	CeLoss 0.9609 (1.2516)	SegCLSLoss 1.0469 (0.7445)	KLLoss 0.7070 (0.4131)	MaskLoss 1.6532 (0.9330)	MaskBCELoss 0.7439 (0.2671)	MaskDICELoss 0.9093 (0.6659)
Epoch: [0][ 22/500]	Time  9.706 ( 9.706)	Loss 3.8205 (3.7066)	CeLoss 0.9453 (1.0480)	SegCLSLoss 1.0391 (0.9414)	KLLoss 0.5117 (0.4799)	MaskLoss 1.1485 (1.0689)	MaskBCELoss 0.1797 (0.2179)	MaskDICELoss 0.9688 (0.8510)
Epoch: [0][ 23/500]	Time 10.368 (10.368)	Loss 4.0159 (2.5452)	CeLoss 1.0391 (1.0008)	SegCLSLoss 1.0391 (0.5055)	KLLoss 0.4941 (0.2414)	MaskLoss 1.2033 (0.6330)	MaskBCELoss 0.2843 (0.1951)	MaskDICELoss 0.9190 (0.4378)
Epoch: [0][ 24/500]	Time 10.680 (10.680)	Loss 4.8837 (2.4403)	CeLoss 1.5000 (1.1000)	SegCLSLoss 1.0469 (0.4141)	KLLoss 0.5156 (0.2223)	MaskLoss 1.4028 (0.5555)	MaskBCELoss 0.4151 (0.1830)	MaskDICELoss 0.9877 (0.3725)
Epoch: [0][ 25/500]	Time 10.339 (10.339)	Loss 4.3194 (3.2943)	CeLoss 0.6719 (1.0465)	SegCLSLoss 0.9609 (0.7156)	KLLoss 0.4336 (0.4133)	MaskLoss 1.5581 (0.9231)	MaskBCELoss 0.7065 (0.2951)	MaskDICELoss 0.8517 (0.6280)
Epoch: [0][ 26/500]	Time 11.518 (11.518)	Loss 4.2284 (3.3757)	CeLoss 0.8281 (0.9023)	SegCLSLoss 1.0078 (0.7973)	KLLoss 0.5859 (0.4285)	MaskLoss 1.4189 (1.0154)	MaskBCELoss 0.5103 (0.2918)	MaskDICELoss 0.9085 (0.7236)
Epoch: [0][ 27/500]	Time  9.880 ( 9.880)	Loss 3.9933 (3.2221)	CeLoss 0.7773 (0.9062)	SegCLSLoss 0.9961 (0.7965)	KLLoss 0.6055 (0.4535)	MaskLoss 1.3287 (0.9368)	MaskBCELoss 0.4000 (0.1713)	MaskDICELoss 0.9287 (0.7655)
Epoch: [0][ 28/500]	Time 11.407 (11.407)	Loss 4.1859 (3.2783)	CeLoss 0.8242 (0.8227)	SegCLSLoss 0.9648 (0.7688)	KLLoss 0.6250 (0.4447)	MaskLoss 1.4094 (1.0137)	MaskBCELoss 0.4779 (0.2591)	MaskDICELoss 0.9315 (0.7546)
Epoch: [0][ 29/500]	Time  9.772 ( 9.772)	Loss 1.2422 (2.7346)	CeLoss 1.2422 (0.8357)	SegCLSLoss 0.0000 (0.5566)	KLLoss 0.0000 (0.3395)	MaskLoss 0.0000 (0.7931)	MaskBCELoss 0.0000 (0.2398)	MaskDICELoss 0.0000 (0.5533)
[2025-03-02 15:23:59,803] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[0.00017099999999999998], mom=[(0.9, 0.95)]
[2025-03-02 15:23:59,811] [INFO] [timer.py:215:stop] epoch=0/micro_step=300/global_step=30, RunningAvgSamplesPerSec=0.9313204237025065, CurrSamplesPerSec=1.0366376403625872, MemAllocated=31.26GB, MaxMemAllocated=36.13GB
Epoch: [0][ 30/500]	Time  9.649 ( 9.649)	Loss 3.2630 (3.4335)	CeLoss 0.4629 (0.6582)	SegCLSLoss 0.8906 (0.8277)	KLLoss 0.4219 (0.4938)	MaskLoss 1.1569 (1.1562)	MaskBCELoss 0.1641 (0.3267)	MaskDICELoss 0.9928 (0.8295)
Epoch: [0][ 31/500]	Time  9.929 ( 9.929)	Loss 1.0781 (2.9465)	CeLoss 1.0781 (0.7461)	SegCLSLoss 0.0000 (0.6199)	KLLoss 0.0000 (0.3916)	MaskLoss 0.0000 (0.9252)	MaskBCELoss 0.0000 (0.3021)	MaskDICELoss 0.0000 (0.6231)
Epoch: [0][ 32/500]	Time 12.140 (12.140)	Loss 3.5387 (3.3884)	CeLoss 0.4277 (0.6602)	SegCLSLoss 0.8828 (0.7773)	KLLoss 0.4629 (0.4877)	MaskLoss 1.3123 (1.1455)	MaskBCELoss 0.4200 (0.3392)	MaskDICELoss 0.8923 (0.8063)
Epoch: [0][ 33/500]	Time 10.458 (10.458)	Loss 3.7137 (3.1394)	CeLoss 0.5156 (0.6969)	SegCLSLoss 0.8086 (0.6719)	KLLoss 0.6250 (0.4727)	MaskLoss 1.3647 (1.0291)	MaskBCELoss 0.4589 (0.3149)	MaskDICELoss 0.9057 (0.7142)
Epoch: [0][ 34/500]	Time  9.985 ( 9.985)	Loss 3.2171 (3.1749)	CeLoss 0.4863 (0.5707)	SegCLSLoss 0.7773 (0.7176)	KLLoss 0.6953 (0.5363)	MaskLoss 1.1359 (1.0954)	MaskBCELoss 0.1790 (0.2667)	MaskDICELoss 0.9569 (0.8287)
Epoch: [0][ 35/500]	Time 11.375 (11.375)	Loss 3.2405 (3.4052)	CeLoss 0.4180 (0.5545)	SegCLSLoss 0.7500 (0.6727)	KLLoss 0.4688 (0.5014)	MaskLoss 1.2003 (1.2329)	MaskBCELoss 0.3230 (0.4024)	MaskDICELoss 0.8773 (0.8305)
Epoch: [0][ 36/500]	Time 13.060 (13.060)	Loss 3.3739 (2.8191)	CeLoss 0.4883 (0.5432)	SegCLSLoss 0.6914 (0.5582)	KLLoss 0.5078 (0.4309)	MaskLoss 1.2436 (0.9766)	MaskBCELoss 0.3514 (0.2297)	MaskDICELoss 0.8922 (0.7469)
Epoch: [0][ 37/500]	Time 10.909 (10.909)	Loss 3.7875 (2.7599)	CeLoss 0.4531 (0.5377)	SegCLSLoss 0.6836 (0.5137)	KLLoss 0.5898 (0.4291)	MaskLoss 1.4660 (0.9610)	MaskBCELoss 0.5723 (0.2047)	MaskDICELoss 0.8937 (0.7563)
Epoch: [0][ 38/500]	Time 11.346 (11.346)	Loss 3.6750 (2.8282)	CeLoss 0.4141 (0.4678)	SegCLSLoss 0.5430 (0.4684)	KLLoss 0.6484 (0.4699)	MaskLoss 1.4606 (1.0396)	MaskBCELoss 0.5276 (0.3102)	MaskDICELoss 0.9330 (0.7294)
Epoch: [0][ 39/500]	Time 11.348 (11.348)	Loss 2.8043 (2.3844)	CeLoss 0.3086 (0.4822)	SegCLSLoss 0.4980 (0.3652)	KLLoss 0.4688 (0.3787)	MaskLoss 1.0994 (0.8403)	MaskBCELoss 0.1153 (0.1932)	MaskDICELoss 0.9841 (0.6471)
[2025-03-02 15:25:49,961] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[0.00023099999999999998], mom=[(0.9, 0.95)]
[2025-03-02 15:25:49,969] [INFO] [timer.py:215:stop] epoch=0/micro_step=400/global_step=40, RunningAvgSamplesPerSec=0.9250504163006136, CurrSamplesPerSec=1.040967981411059, MemAllocated=31.24GB, MaxMemAllocated=36.13GB
Epoch: [0][ 40/500]	Time  9.608 ( 9.608)	Loss 3.3514 (2.3310)	CeLoss 0.3281 (0.6553)	SegCLSLoss 0.4375 (0.2795)	KLLoss 0.7188 (0.3367)	MaskLoss 1.3671 (0.7515)	MaskBCELoss 0.4979 (0.1934)	MaskDICELoss 0.8692 (0.5580)
Epoch: [0][ 41/500]	Time 11.419 (11.419)	Loss 3.0819 (2.8686)	CeLoss 0.4492 (0.4637)	SegCLSLoss 0.4180 (0.3791)	KLLoss 0.5156 (0.5062)	MaskLoss 1.1874 (1.0831)	MaskBCELoss 0.2146 (0.2595)	MaskDICELoss 0.9728 (0.8236)
Epoch: [0][ 42/500]	Time  9.848 ( 9.848)	Loss 3.4062 (2.3201)	CeLoss 0.5234 (0.5279)	SegCLSLoss 0.3242 (0.2381)	KLLoss 0.6562 (0.4086)	MaskLoss 1.3261 (0.8159)	MaskBCELoss 0.5529 (0.1552)	MaskDICELoss 0.7732 (0.6607)
Epoch: [0][ 43/500]	Time 10.467 (10.467)	Loss 2.9017 (2.6981)	CeLoss 0.3984 (0.5592)	SegCLSLoss 0.3477 (0.2457)	KLLoss 0.4395 (0.4693)	MaskLoss 1.1442 (0.9848)	MaskBCELoss 0.2293 (0.2421)	MaskDICELoss 0.9149 (0.7427)
Epoch: [0][ 44/500]	Time 11.284 (11.284)	Loss 3.3672 (2.7386)	CeLoss 0.3516 (0.3045)	SegCLSLoss 0.2383 (0.2568)	KLLoss 0.6719 (0.4814)	MaskLoss 1.4141 (1.1291)	MaskBCELoss 0.5117 (0.3228)	MaskDICELoss 0.9023 (0.8063)
Epoch: [0][ 45/500]	Time 10.958 (10.958)	Loss 2.6814 (2.4566)	CeLoss 0.2373 (0.5669)	SegCLSLoss 0.2500 (0.1651)	KLLoss 0.4785 (0.3900)	MaskLoss 1.1356 (0.8842)	MaskBCELoss 0.2138 (0.2487)	MaskDICELoss 0.9218 (0.6354)
Epoch: [0][ 46/500]	Time 10.126 (10.126)	Loss 2.6683 (2.9941)	CeLoss 0.3047 (0.4478)	SegCLSLoss 0.2061 (0.1773)	KLLoss 0.5195 (0.5277)	MaskLoss 1.1037 (1.2024)	MaskBCELoss 0.1212 (0.3959)	MaskDICELoss 0.9825 (0.8065)
Epoch: [0][ 47/500]	Time 11.785 (11.785)	Loss 3.2480 (2.6959)	CeLoss 0.4023 (0.5143)	SegCLSLoss 0.1426 (0.1446)	KLLoss 0.6836 (0.4732)	MaskLoss 1.3545 (1.0310)	MaskBCELoss 0.4611 (0.3122)	MaskDICELoss 0.8933 (0.7187)
Epoch: [0][ 48/500]	Time 11.008 (11.008)	Loss 3.0074 (2.6784)	CeLoss 0.3750 (0.4871)	SegCLSLoss 0.1367 (0.1149)	KLLoss 0.6562 (0.5195)	MaskLoss 1.2498 (1.0410)	MaskBCELoss 0.2504 (0.3474)	MaskDICELoss 0.9994 (0.6936)
Epoch: [0][ 49/500]	Time 11.447 (11.447)	Loss 2.9724 (2.7782)	CeLoss 0.2930 (0.3873)	SegCLSLoss 0.1201 (0.1276)	KLLoss 0.6367 (0.5342)	MaskLoss 1.2772 (1.1368)	MaskBCELoss 0.3838 (0.3264)	MaskDICELoss 0.8934 (0.8103)
[2025-03-02 15:27:38,758] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[0.00029099999999999997], mom=[(0.9, 0.95)]
[2025-03-02 15:27:38,772] [INFO] [timer.py:215:stop] epoch=0/micro_step=500/global_step=50, RunningAvgSamplesPerSec=0.9238297740146463, CurrSamplesPerSec=0.9561126193501208, MemAllocated=31.25GB, MaxMemAllocated=36.13GB
Epoch: [0][ 50/500]	Time 10.461 (10.461)	Loss 2.8080 (2.4298)	CeLoss 0.3359 (0.3977)	SegCLSLoss 0.1084 (0.0915)	KLLoss 0.6719 (0.5107)	MaskLoss 1.1755 (0.9678)	MaskBCELoss 0.2233 (0.2250)	MaskDICELoss 0.9522 (0.7427)
Epoch: [0][ 51/500]	Time 11.569 (11.569)	Loss 2.7037 (2.3289)	CeLoss 0.1855 (0.4404)	SegCLSLoss 0.1709 (0.1037)	KLLoss 0.4648 (0.4602)	MaskLoss 1.1926 (0.8950)	MaskBCELoss 0.2401 (0.1137)	MaskDICELoss 0.9526 (0.7813)
Epoch: [0][ 52/500]	Time 12.513 (12.513)	Loss 3.4201 (2.9940)	CeLoss 0.2490 (0.3149)	SegCLSLoss 0.1777 (0.1165)	KLLoss 0.5195 (0.6148)	MaskLoss 1.5147 (1.2797)	MaskBCELoss 0.5655 (0.3106)	MaskDICELoss 0.9492 (0.9691)
Epoch: [0][ 53/500]	Time 10.973 (10.973)	Loss 3.5210 (2.6138)	CeLoss 0.3633 (0.5020)	SegCLSLoss 0.0718 (0.0857)	KLLoss 0.7617 (0.4934)	MaskLoss 1.5232 (1.0101)	MaskBCELoss 0.6456 (0.2407)	MaskDICELoss 0.8776 (0.7695)
Epoch: [0][ 54/500]	Time 11.660 (11.660)	Loss 2.5545 (2.1778)	CeLoss 0.3926 (0.4132)	SegCLSLoss 0.1328 (0.0882)	KLLoss 0.5547 (0.4016)	MaskLoss 1.0194 (0.8400)	MaskBCELoss 0.0225 (0.1644)	MaskDICELoss 0.9969 (0.6755)
Epoch: [0][ 55/500]	Time 10.716 (10.716)	Loss 2.4980 (2.4731)	CeLoss 0.2949 (0.4004)	SegCLSLoss 0.0942 (0.0806)	KLLoss 0.5703 (0.4961)	MaskLoss 1.0498 (0.9914)	MaskBCELoss 0.0606 (0.2199)	MaskDICELoss 0.9892 (0.7716)
Epoch: [0][ 56/500]	Time 12.351 (12.351)	Loss 2.5638 (2.5508)	CeLoss 0.3027 (0.3392)	SegCLSLoss 0.1089 (0.0819)	KLLoss 0.5625 (0.5578)	MaskLoss 1.0748 (1.0574)	MaskBCELoss 0.0817 (0.1910)	MaskDICELoss 0.9931 (0.8665)
Epoch: [0][ 57/500]	Time 13.077 (13.077)	Loss 2.9127 (2.8041)	CeLoss 0.3926 (0.2578)	SegCLSLoss 0.1338 (0.1138)	KLLoss 0.5391 (0.5895)	MaskLoss 1.2005 (1.2155)	MaskBCELoss 0.2660 (0.2923)	MaskDICELoss 0.9345 (0.9232)
Epoch: [0][ 58/500]	Time  9.673 ( 9.673)	Loss 1.3359 (2.2784)	CeLoss 1.3359 (0.5334)	SegCLSLoss 0.0000 (0.0776)	KLLoss 0.0000 (0.4049)	MaskLoss 0.0000 (0.8330)	MaskBCELoss 0.0000 (0.1781)	MaskDICELoss 0.0000 (0.6548)
Epoch: [0][ 59/500]	Time  9.841 ( 9.841)	Loss 2.6071 (1.9863)	CeLoss 0.2754 (0.5338)	SegCLSLoss 0.0913 (0.0553)	KLLoss 0.5547 (0.3660)	MaskLoss 1.1160 (0.6942)	MaskBCELoss 0.1240 (0.1073)	MaskDICELoss 0.9921 (0.5869)
[2025-03-02 15:29:32,018] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[0.00029895918367346935], mom=[(0.9, 0.95)]
[2025-03-02 15:29:32,027] [INFO] [timer.py:215:stop] epoch=0/micro_step=600/global_step=60, RunningAvgSamplesPerSec=0.9165390657456657, CurrSamplesPerSec=0.9190020825260147, MemAllocated=31.26GB, MaxMemAllocated=36.25GB
Epoch: [0][ 60/500]	Time 10.883 (10.883)	Loss 2.6924 (2.4451)	CeLoss 0.3086 (0.4123)	SegCLSLoss 0.0854 (0.0907)	KLLoss 0.6406 (0.4678)	MaskLoss 1.1382 (0.9701)	MaskBCELoss 0.1601 (0.2365)	MaskDICELoss 0.9781 (0.7336)
Epoch: [0][ 61/500]	Time 11.555 (11.555)	Loss 2.5990 (2.6294)	CeLoss 0.2871 (0.3420)	SegCLSLoss 0.0649 (0.0677)	KLLoss 0.5938 (0.5590)	MaskLoss 1.1101 (1.0987)	MaskBCELoss 0.1269 (0.2574)	MaskDICELoss 0.9832 (0.8413)
Epoch: [0][ 62/500]	Time 10.571 (10.571)	Loss 0.8633 (2.6943)	CeLoss 0.8633 (0.3896)	SegCLSLoss 0.0000 (0.0681)	KLLoss 0.0000 (0.4861)	MaskLoss 0.0000 (1.1110)	MaskBCELoss 0.0000 (0.4293)	MaskDICELoss 0.0000 (0.6817)
Epoch: [0][ 63/500]	Time 12.145 (12.145)	Loss 2.7318 (2.5892)	CeLoss 0.2676 (0.3005)	SegCLSLoss 0.0562 (0.0727)	KLLoss 0.6172 (0.5184)	MaskLoss 1.1872 (1.1002)	MaskBCELoss 0.2507 (0.2535)	MaskDICELoss 0.9364 (0.8467)
Epoch: [0][ 64/500]	Time  9.510 ( 9.510)	Loss 0.8594 (2.1119)	CeLoss 0.8594 (0.5390)	SegCLSLoss 0.0000 (0.0549)	KLLoss 0.0000 (0.3281)	MaskLoss 0.0000 (0.7563)	MaskBCELoss 0.0000 (0.2137)	MaskDICELoss 0.0000 (0.5425)
Epoch: [0][ 65/500]	Time  9.096 ( 9.096)	Loss 0.1777 (2.0630)	CeLoss 0.1777 (0.5717)	SegCLSLoss 0.0000 (0.0405)	KLLoss 0.0000 (0.3607)	MaskLoss 0.0000 (0.7176)	MaskBCELoss 0.0000 (0.1520)	MaskDICELoss 0.0000 (0.5656)
Epoch: [0][ 66/500]	Time  9.742 ( 9.742)	Loss 2.5696 (2.1923)	CeLoss 0.2402 (0.5260)	SegCLSLoss 0.0850 (0.0663)	KLLoss 0.5273 (0.3672)	MaskLoss 1.1168 (0.7983)	MaskBCELoss 0.1399 (0.1246)	MaskDICELoss 0.9770 (0.6737)
Epoch: [0][ 67/500]	Time 10.594 (10.594)	Loss 2.9785 (2.0949)	CeLoss 0.3105 (0.3633)	SegCLSLoss 0.0457 (0.0654)	KLLoss 0.6641 (0.3910)	MaskLoss 1.2890 (0.8296)	MaskBCELoss 0.3770 (0.1728)	MaskDICELoss 0.9120 (0.6568)
Epoch: [0][ 68/500]	Time 11.818 (11.818)	Loss 0.7930 (2.2586)	CeLoss 0.7930 (0.3738)	SegCLSLoss 0.0000 (0.0641)	KLLoss 0.0000 (0.4396)	MaskLoss 0.0000 (0.9043)	MaskBCELoss 0.0000 (0.1417)	MaskDICELoss 0.0000 (0.7626)
Epoch: [0][ 69/500]	Time  9.170 ( 9.170)	Loss 2.6353 (1.9605)	CeLoss 0.1816 (0.7227)	SegCLSLoss 0.1118 (0.0453)	KLLoss 0.4570 (0.2445)	MaskLoss 1.1760 (0.5955)	MaskBCELoss 0.2632 (0.1374)	MaskDICELoss 0.9128 (0.4581)
[2025-03-02 15:31:17,132] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[0.000297734693877551], mom=[(0.9, 0.95)]
[2025-03-02 15:31:17,143] [INFO] [timer.py:215:stop] epoch=0/micro_step=700/global_step=70, RunningAvgSamplesPerSec=0.9215149609909304, CurrSamplesPerSec=0.9163882943878694, MemAllocated=31.3GB, MaxMemAllocated=36.25GB
Epoch: [0][ 70/500]	Time 10.914 (10.914)	Loss 3.0985 (2.5114)	CeLoss 0.2031 (0.4238)	SegCLSLoss 0.1191 (0.0580)	KLLoss 0.4883 (0.4650)	MaskLoss 1.3940 (1.0062)	MaskBCELoss 0.5981 (0.2678)	MaskDICELoss 0.7959 (0.7384)
Epoch: [0][ 71/500]	Time  9.623 ( 9.623)	Loss 2.6873 (2.3935)	CeLoss 0.3066 (0.4894)	SegCLSLoss 0.0962 (0.0547)	KLLoss 0.5078 (0.3979)	MaskLoss 1.1405 (0.9186)	MaskBCELoss 0.1770 (0.2881)	MaskDICELoss 0.9635 (0.6306)
Epoch: [0][ 72/500]	Time  9.726 ( 9.726)	Loss 2.4247 (2.8668)	CeLoss 0.2178 (0.3866)	SegCLSLoss 0.0771 (0.0681)	KLLoss 0.5156 (0.4986)	MaskLoss 1.0590 (1.1981)	MaskBCELoss 0.0640 (0.3955)	MaskDICELoss 0.9950 (0.8026)
Epoch: [0][ 73/500]	Time 11.043 (11.043)	Loss 1.1641 (2.4448)	CeLoss 1.1641 (0.4153)	SegCLSLoss 0.0000 (0.0560)	KLLoss 0.0000 (0.4547)	MaskLoss 0.0000 (0.9779)	MaskBCELoss 0.0000 (0.2559)	MaskDICELoss 0.0000 (0.7220)
Epoch: [0][ 74/500]	Time 10.002 (10.002)	Loss 1.1406 (2.1031)	CeLoss 1.1406 (0.5348)	SegCLSLoss 0.0000 (0.0574)	KLLoss 0.0000 (0.3107)	MaskLoss 0.0000 (0.7544)	MaskBCELoss 0.0000 (0.2185)	MaskDICELoss 0.0000 (0.5359)
Epoch: [0][ 75/500]	Time 11.343 (11.343)	Loss 3.0594 (2.4420)	CeLoss 0.1533 (0.2856)	SegCLSLoss 0.1523 (0.0773)	KLLoss 0.4434 (0.4055)	MaskLoss 1.3930 (1.0387)	MaskBCELoss 0.5025 (0.3221)	MaskDICELoss 0.8905 (0.7166)
Epoch: [0][ 76/500]	Time 11.891 (11.891)	Loss 2.7081 (2.6823)	CeLoss 0.2148 (0.3772)	SegCLSLoss 0.0811 (0.0762)	KLLoss 0.5078 (0.4664)	MaskLoss 1.2007 (1.1102)	MaskBCELoss 0.2282 (0.2746)	MaskDICELoss 0.9725 (0.8356)
Epoch: [0][ 77/500]	Time 10.756 (10.756)	Loss 2.8148 (2.8847)	CeLoss 0.2354 (0.4895)	SegCLSLoss 0.1011 (0.0760)	KLLoss 0.4707 (0.4600)	MaskLoss 1.2414 (1.1556)	MaskBCELoss 0.3017 (0.3339)	MaskDICELoss 0.9397 (0.8217)
Epoch: [0][ 78/500]	Time 11.180 (11.180)	Loss 1.0781 (2.3468)	CeLoss 1.0781 (0.4561)	SegCLSLoss 0.0000 (0.0369)	KLLoss 0.0000 (0.4104)	MaskLoss 0.0000 (0.9158)	MaskBCELoss 0.0000 (0.2822)	MaskDICELoss 0.0000 (0.6336)
Epoch: [0][ 79/500]	Time 10.585 (10.585)	Loss 2.9440 (2.1677)	CeLoss 0.3359 (0.4381)	SegCLSLoss 0.0845 (0.0650)	KLLoss 0.4961 (0.3396)	MaskLoss 1.2582 (0.8316)	MaskBCELoss 0.4204 (0.1869)	MaskDICELoss 0.8377 (0.6447)
[2025-03-02 15:33:04,891] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[0.0002965102040816326], mom=[(0.9, 0.95)]
[2025-03-02 15:33:04,899] [INFO] [timer.py:215:stop] epoch=0/micro_step=800/global_step=80, RunningAvgSamplesPerSec=0.9223617421312003, CurrSamplesPerSec=0.8616608988464524, MemAllocated=31.26GB, MaxMemAllocated=36.32GB
Epoch: [0][ 80/500]	Time 11.608 (11.608)	Loss 2.8529 (2.3837)	CeLoss 0.2275 (0.2530)	SegCLSLoss 0.0967 (0.0749)	KLLoss 0.4316 (0.4422)	MaskLoss 1.2673 (1.0246)	MaskBCELoss 0.4289 (0.1729)	MaskDICELoss 0.8384 (0.8517)
Epoch: [0][ 81/500]	Time  9.603 ( 9.603)	Loss 2.5587 (1.8880)	CeLoss 0.2188 (0.6430)	SegCLSLoss 0.0669 (0.0432)	KLLoss 0.5586 (0.2473)	MaskLoss 1.1250 (0.5994)	MaskBCELoss 0.1811 (0.1387)	MaskDICELoss 0.9439 (0.4607)
Epoch: [0][ 82/500]	Time 11.519 (11.519)	Loss 0.6367 (2.3436)	CeLoss 0.6367 (0.3766)	SegCLSLoss 0.0000 (0.0575)	KLLoss 0.0000 (0.4117)	MaskLoss 0.0000 (0.9486)	MaskBCELoss 0.0000 (0.1828)	MaskDICELoss 0.0000 (0.7658)
Epoch: [0][ 83/500]	Time 11.706 (11.706)	Loss 2.5013 (2.3680)	CeLoss 0.2578 (0.3903)	SegCLSLoss 0.0591 (0.0599)	KLLoss 0.5547 (0.4139)	MaskLoss 1.0798 (0.9531)	MaskBCELoss 0.0897 (0.1856)	MaskDICELoss 0.9900 (0.7675)
Epoch: [0][ 84/500]	Time  9.690 ( 9.690)	Loss 1.2969 (2.0116)	CeLoss 1.2969 (0.4729)	SegCLSLoss 0.0000 (0.0580)	KLLoss 0.0000 (0.2715)	MaskLoss 0.0000 (0.7413)	MaskBCELoss 0.0000 (0.1809)	MaskDICELoss 0.0000 (0.5604)
Epoch: [0][ 85/500]	Time 12.037 (12.037)	Loss 3.0872 (2.6910)	CeLoss 0.2969 (0.3804)	SegCLSLoss 0.0469 (0.0577)	KLLoss 0.5703 (0.4713)	MaskLoss 1.3551 (1.1174)	MaskBCELoss 0.4195 (0.2629)	MaskDICELoss 0.9357 (0.8545)
Epoch: [0][ 86/500]	Time 11.838 (11.838)	Loss 0.6719 (2.3469)	CeLoss 0.6719 (0.4934)	SegCLSLoss 0.0000 (0.0406)	KLLoss 0.0000 (0.3727)	MaskLoss 0.0000 (0.8979)	MaskBCELoss 0.0000 (0.2545)	MaskDICELoss 0.0000 (0.6434)
Epoch: [0][ 87/500]	Time  9.865 ( 9.865)	Loss 2.7179 (2.2224)	CeLoss 0.1787 (0.5177)	SegCLSLoss 0.1328 (0.0552)	KLLoss 0.4375 (0.3365)	MaskLoss 1.2144 (0.8218)	MaskBCELoss 0.3290 (0.1654)	MaskDICELoss 0.8854 (0.6563)
Epoch: [0][ 88/500]	Time 11.519 (11.519)	Loss 1.6016 (2.4242)	CeLoss 1.6016 (0.4385)	SegCLSLoss 0.0000 (0.0464)	KLLoss 0.0000 (0.4260)	MaskLoss 0.0000 (0.9599)	MaskBCELoss 0.0000 (0.2097)	MaskDICELoss 0.0000 (0.7503)
Epoch: [0][ 89/500]	Time 11.499 (11.499)	Loss 0.7266 (2.1597)	CeLoss 0.7266 (0.4196)	SegCLSLoss 0.0000 (0.0496)	KLLoss 0.0000 (0.3535)	MaskLoss 0.0000 (0.8401)	MaskBCELoss 0.0000 (0.1888)	MaskDICELoss 0.0000 (0.6514)
[2025-03-02 15:34:53,890] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[0.0002952857142857143], mom=[(0.9, 0.95)]
[2025-03-02 15:34:53,895] [INFO] [timer.py:215:stop] epoch=0/micro_step=900/global_step=90, RunningAvgSamplesPerSec=0.9218181756121615, CurrSamplesPerSec=1.0289599587935576, MemAllocated=31.59GB, MaxMemAllocated=36.58GB
Epoch: [0][ 90/500]	Time  9.720 ( 9.720)	Loss 2.7401 (2.7076)	CeLoss 0.2910 (0.3623)	SegCLSLoss 0.0728 (0.0696)	KLLoss 0.4941 (0.4451)	MaskLoss 1.1816 (1.1331)	MaskBCELoss 0.2408 (0.3312)	MaskDICELoss 0.9408 (0.8019)
Epoch: [0][ 91/500]	Time  9.519 ( 9.519)	Loss 2.5170 (2.1437)	CeLoss 0.1689 (0.4856)	SegCLSLoss 0.0806 (0.0427)	KLLoss 0.4336 (0.2896)	MaskLoss 1.1325 (0.8039)	MaskBCELoss 0.1342 (0.2883)	MaskDICELoss 0.9983 (0.5156)
Epoch: [0][ 92/500]	Time  7.041 ( 7.041)	Loss 2.6477 (2.4896)	CeLoss 0.2432 (0.3228)	SegCLSLoss 0.0608 (0.0516)	KLLoss 0.4785 (0.4568)	MaskLoss 1.1637 (1.0476)	MaskBCELoss 0.2185 (0.1838)	MaskDICELoss 0.9451 (0.8638)
Epoch: [0][ 93/500]	Time  5.643 ( 5.643)	Loss 1.3203 (1.9733)	CeLoss 1.3203 (0.7610)	SegCLSLoss 0.0000 (0.0292)	KLLoss 0.0000 (0.2477)	MaskLoss 0.0000 (0.5864)	MaskBCELoss 0.0000 (0.1148)	MaskDICELoss 0.0000 (0.4716)
Epoch: [0][ 94/500]	Time  6.923 ( 6.923)	Loss 3.0657 (2.7770)	CeLoss 0.2461 (0.2644)	SegCLSLoss 0.0630 (0.0603)	KLLoss 0.4941 (0.4877)	MaskLoss 1.3698 (1.2168)	MaskBCELoss 0.6098 (0.3047)	MaskDICELoss 0.7600 (0.9122)
Epoch: [0][ 95/500]	Time  6.374 ( 6.374)	Loss 2.7896 (2.5131)	CeLoss 0.2432 (0.2599)	SegCLSLoss 0.0452 (0.0752)	KLLoss 0.5391 (0.4340)	MaskLoss 1.2346 (1.0862)	MaskBCELoss 0.3172 (0.2591)	MaskDICELoss 0.9175 (0.8271)
Epoch: [0][ 96/500]	Time  6.526 ( 6.526)	Loss 1.1953 (2.4174)	CeLoss 1.1953 (0.4071)	SegCLSLoss 0.0000 (0.0559)	KLLoss 0.0000 (0.3846)	MaskLoss 0.0000 (0.9721)	MaskBCELoss 0.0000 (0.2393)	MaskDICELoss 0.0000 (0.7328)
Epoch: [0][ 97/500]	Time  7.275 ( 7.275)	Loss 2.6800 (2.6644)	CeLoss 0.2344 (0.3002)	SegCLSLoss 0.0801 (0.0611)	KLLoss 0.4102 (0.4213)	MaskLoss 1.1828 (1.1459)	MaskBCELoss 0.2262 (0.3362)	MaskDICELoss 0.9565 (0.8097)
Epoch: [0][ 98/500]	Time  5.906 ( 5.906)	Loss 2.4344 (2.2181)	CeLoss 0.2734 (0.3229)	SegCLSLoss 0.0569 (0.0624)	KLLoss 0.4746 (0.3578)	MaskLoss 1.0424 (0.9140)	MaskBCELoss 0.0442 (0.1701)	MaskDICELoss 0.9982 (0.7439)
Epoch: [0][ 99/500]	Time  4.818 ( 4.818)	Loss 1.0000 (1.9784)	CeLoss 1.0000 (0.5507)	SegCLSLoss 0.0000 (0.0399)	KLLoss 0.0000 (0.2908)	MaskLoss 0.0000 (0.6893)	MaskBCELoss 0.0000 (0.1223)	MaskDICELoss 0.0000 (0.5669)
[2025-03-02 15:36:00,448] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[0.00029406122448979587], mom=[(0.9, 0.95)]
[2025-03-02 15:36:00,454] [INFO] [timer.py:215:stop] epoch=0/micro_step=1000/global_step=100, RunningAvgSamplesPerSec=0.9596752709130185, CurrSamplesPerSec=1.5314206372855057, MemAllocated=31.37GB, MaxMemAllocated=36.58GB
Epoch: [0][100/500]	Time  6.532 ( 6.532)	Loss 2.5316 (2.3822)	CeLoss 0.1973 (0.3807)	SegCLSLoss 0.0552 (0.0381)	KLLoss 0.5234 (0.4215)	MaskLoss 1.1271 (0.9703)	MaskBCELoss 0.1392 (0.2512)	MaskDICELoss 0.9879 (0.7191)
Epoch: [0][101/500]	Time  5.530 ( 5.530)	Loss 1.6875 (2.5391)	CeLoss 1.6875 (0.4508)	SegCLSLoss 0.0000 (0.0382)	KLLoss 0.0000 (0.4055)	MaskLoss 0.0000 (1.0144)	MaskBCELoss 0.0000 (0.3457)	MaskDICELoss 0.0000 (0.6687)
Epoch: [0][102/500]	Time  5.573 ( 5.573)	Loss 1.6250 (2.4197)	CeLoss 1.6250 (0.4640)	SegCLSLoss 0.0000 (0.0623)	KLLoss 0.0000 (0.3623)	MaskLoss 0.0000 (0.9443)	MaskBCELoss 0.0000 (0.2293)	MaskDICELoss 0.0000 (0.7150)
Epoch: [0][103/500]	Time  6.302 ( 6.302)	Loss 0.9727 (2.2355)	CeLoss 0.9727 (0.5839)	SegCLSLoss 0.0000 (0.0351)	KLLoss 0.0000 (0.3563)	MaskLoss 0.0000 (0.7992)	MaskBCELoss 0.0000 (0.1549)	MaskDICELoss 0.0000 (0.6443)
Epoch: [0][104/500]	Time  5.888 ( 5.888)	Loss 5.1374 (2.5357)	CeLoss 0.1650 (0.3584)	SegCLSLoss 0.0630 (0.0392)	KLLoss 0.4551 (0.3953)	MaskLoss 2.4476 (1.0590)	MaskBCELoss 1.4916 (0.3233)	MaskDICELoss 0.9560 (0.7356)
Epoch: [0][105/500]	Time  5.221 ( 5.221)	Loss 0.9336 (2.0597)	CeLoss 0.9336 (0.5775)	SegCLSLoss 0.0000 (0.0290)	KLLoss 0.0000 (0.3020)	MaskLoss 0.0000 (0.7186)	MaskBCELoss 0.0000 (0.2139)	MaskDICELoss 0.0000 (0.5048)
Epoch: [0][106/500]	Time  5.418 ( 5.418)	Loss 2.6795 (1.9758)	CeLoss 0.1475 (0.2935)	SegCLSLoss 0.1660 (0.0447)	KLLoss 0.3789 (0.3250)	MaskLoss 1.2060 (0.8137)	MaskBCELoss 0.2583 (0.1856)	MaskDICELoss 0.9477 (0.6281)
Epoch: [0][107/500]	Time  5.983 ( 5.983)	Loss 2.6651 (2.3353)	CeLoss 0.2617 (0.4602)	SegCLSLoss 0.0483 (0.0511)	KLLoss 0.4746 (0.3635)	MaskLoss 1.1666 (0.9067)	MaskBCELoss 0.2748 (0.2301)	MaskDICELoss 0.8917 (0.6766)
Epoch: [0][108/500]	Time  4.576 ( 4.576)	Loss 1.5312 (2.0871)	CeLoss 1.5312 (0.6235)	SegCLSLoss 0.0000 (0.0291)	KLLoss 0.0000 (0.2904)	MaskLoss 0.0000 (0.7101)	MaskBCELoss 0.0000 (0.1616)	MaskDICELoss 0.0000 (0.5485)
Epoch: [0][109/500]	Time  5.645 ( 5.645)	Loss 2.5499 (2.4211)	CeLoss 0.3691 (0.2797)	SegCLSLoss 0.0688 (0.0435)	KLLoss 0.4570 (0.3789)	MaskLoss 1.0504 (1.0408)	MaskBCELoss 0.0768 (0.3309)	MaskDICELoss 0.9735 (0.7098)
[2025-03-02 15:36:57,209] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[0.00029283673469387755], mom=[(0.9, 0.95)]
[2025-03-02 15:36:57,214] [INFO] [timer.py:215:stop] epoch=0/micro_step=1100/global_step=110, RunningAvgSamplesPerSec=1.0019258739483572, CurrSamplesPerSec=1.5100775066531842, MemAllocated=31.25GB, MaxMemAllocated=36.58GB
Epoch: [0][110/500]	Time  6.624 ( 6.624)	Loss 2.3563 (2.3874)	CeLoss 0.2090 (0.2773)	SegCLSLoss 0.0688 (0.0516)	KLLoss 0.4062 (0.4205)	MaskLoss 1.0365 (1.0213)	MaskBCELoss 0.0477 (0.2257)	MaskDICELoss 0.9888 (0.7955)
Epoch: [0][111/500]	Time  5.086 ( 5.086)	Loss 2.8429 (2.4659)	CeLoss 0.1904 (0.3861)	SegCLSLoss 0.0874 (0.0483)	KLLoss 0.4219 (0.3822)	MaskLoss 1.2828 (1.0084)	MaskBCELoss 0.3859 (0.2974)	MaskDICELoss 0.8968 (0.7110)
Epoch: [0][112/500]	Time  5.950 ( 5.950)	Loss 0.6602 (1.9127)	CeLoss 0.6602 (0.6091)	SegCLSLoss 0.0000 (0.0236)	KLLoss 0.0000 (0.2396)	MaskLoss 0.0000 (0.6340)	MaskBCELoss 0.0000 (0.1880)	MaskDICELoss 0.0000 (0.4460)
Epoch: [0][113/500]	Time  5.520 ( 5.520)	Loss 2.5167 (2.3371)	CeLoss 0.2441 (0.4592)	SegCLSLoss 0.0381 (0.0411)	KLLoss 0.5430 (0.3852)	MaskLoss 1.0992 (0.9095)	MaskBCELoss 0.2165 (0.1601)	MaskDICELoss 0.8827 (0.7494)
Epoch: [0][114/500]	Time  4.769 ( 4.769)	Loss 2.4277 (2.3137)	CeLoss 0.2539 (0.6320)	SegCLSLoss 0.0493 (0.0419)	KLLoss 0.4492 (0.3379)	MaskLoss 1.0517 (0.8134)	MaskBCELoss 0.0556 (0.1955)	MaskDICELoss 0.9961 (0.6179)
Epoch: [0][115/500]	Time  7.433 ( 7.433)	Loss 0.1289 (2.3053)	CeLoss 0.1289 (0.3646)	SegCLSLoss 0.0000 (0.0431)	KLLoss 0.0000 (0.3682)	MaskLoss 0.0000 (0.9414)	MaskBCELoss 0.0000 (0.2204)	MaskDICELoss 0.0000 (0.7210)
Epoch: [0][116/500]	Time  5.925 ( 5.925)	Loss 2.7429 (2.3214)	CeLoss 0.3242 (0.4136)	SegCLSLoss 0.0396 (0.0385)	KLLoss 0.5039 (0.3803)	MaskLoss 1.1742 (0.9254)	MaskBCELoss 0.2986 (0.2155)	MaskDICELoss 0.8756 (0.7099)
Epoch: [0][117/500]	Time  4.770 ( 4.770)	Loss 3.0828 (2.1084)	CeLoss 0.2490 (0.4872)	SegCLSLoss 0.0317 (0.0422)	KLLoss 0.5742 (0.3342)	MaskLoss 1.3803 (0.7833)	MaskBCELoss 0.6590 (0.1371)	MaskDICELoss 0.7213 (0.6462)
Epoch: [0][118/500]	Time  5.200 ( 5.200)	Loss 1.5312 (1.5430)	CeLoss 1.5312 (0.5813)	SegCLSLoss 0.0000 (0.0222)	KLLoss 0.0000 (0.1863)	MaskLoss 0.0000 (0.4659)	MaskBCELoss 0.0000 (0.0937)	MaskDICELoss 0.0000 (0.3721)
Epoch: [0][119/500]	Time  4.695 ( 4.695)	Loss 0.8125 (2.0603)	CeLoss 0.8125 (0.7044)	SegCLSLoss 0.0000 (0.0258)	KLLoss 0.0000 (0.2287)	MaskLoss 0.0000 (0.6601)	MaskBCELoss 0.0000 (0.2147)	MaskDICELoss 0.0000 (0.4454)
[2025-03-02 15:37:53,358] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[0.0002916122448979592], mom=[(0.9, 0.95)]
[2025-03-02 15:37:53,363] [INFO] [timer.py:215:stop] epoch=0/micro_step=1200/global_step=120, RunningAvgSamplesPerSec=1.0405095139229226, CurrSamplesPerSec=1.4708978837052984, MemAllocated=31.25GB, MaxMemAllocated=36.58GB
Epoch: [0][120/500]	Time  6.800 ( 6.800)	Loss 2.4281 (2.1774)	CeLoss 0.2422 (0.2264)	SegCLSLoss 0.0491 (0.0358)	KLLoss 0.4434 (0.3836)	MaskLoss 1.0578 (0.9474)	MaskBCELoss 0.0695 (0.2127)	MaskDICELoss 0.9883 (0.7347)
Epoch: [0][121/500]	Time  5.665 ( 5.665)	Loss 3.1999 (2.5541)	CeLoss 0.4512 (0.4056)	SegCLSLoss 0.0371 (0.0327)	KLLoss 0.4746 (0.3893)	MaskLoss 1.3421 (1.0468)	MaskBCELoss 0.4537 (0.3386)	MaskDICELoss 0.8884 (0.7082)
Epoch: [0][122/500]	Time  5.668 ( 5.668)	Loss 1.3984 (2.1240)	CeLoss 1.3984 (0.4254)	SegCLSLoss 0.0000 (0.0378)	KLLoss 0.0000 (0.3191)	MaskLoss 0.0000 (0.8241)	MaskBCELoss 0.0000 (0.1516)	MaskDICELoss 0.0000 (0.6725)
Epoch: [0][123/500]	Time  6.052 ( 6.052)	Loss 2.7546 (1.9447)	CeLoss 0.2031 (0.5352)	SegCLSLoss 0.0339 (0.0289)	KLLoss 0.4805 (0.2691)	MaskLoss 1.2435 (0.6842)	MaskBCELoss 0.3467 (0.1157)	MaskDICELoss 0.8968 (0.5685)
Epoch: [0][124/500]	Time  6.690 ( 6.690)	Loss 2.4669 (1.7211)	CeLoss 0.2773 (0.4021)	SegCLSLoss 0.0591 (0.0247)	KLLoss 0.4121 (0.2900)	MaskLoss 1.0596 (0.6387)	MaskBCELoss 0.0764 (0.0785)	MaskDICELoss 0.9832 (0.5603)
Epoch: [0][125/500]	Time  5.161 ( 5.161)	Loss 2.9417 (1.9969)	CeLoss 0.2207 (0.5240)	SegCLSLoss 0.0413 (0.0210)	KLLoss 0.4688 (0.2889)	MaskLoss 1.3263 (0.7166)	MaskBCELoss 0.5571 (0.1860)	MaskDICELoss 0.7692 (0.5307)
Epoch: [0][126/500]	Time  6.400 ( 6.400)	Loss 2.4488 (2.3178)	CeLoss 0.2676 (0.4476)	SegCLSLoss 0.0520 (0.0355)	KLLoss 0.4277 (0.3805)	MaskLoss 1.0564 (0.9071)	MaskBCELoss 0.1074 (0.2450)	MaskDICELoss 0.9490 (0.6621)
Epoch: [0][127/500]	Time  5.992 ( 5.992)	Loss 2.3305 (2.1602)	CeLoss 0.2178 (0.3687)	SegCLSLoss 0.0405 (0.0383)	KLLoss 0.4902 (0.3701)	MaskLoss 1.0217 (0.8677)	MaskBCELoss 0.0297 (0.0868)	MaskDICELoss 0.9920 (0.7809)
Epoch: [0][128/500]	Time  6.903 ( 6.903)	Loss 4.2626 (2.6265)	CeLoss 0.2207 (0.2661)	SegCLSLoss 0.0574 (0.0355)	KLLoss 0.4258 (0.4377)	MaskLoss 1.9858 (1.1496)	MaskBCELoss 1.0705 (0.3105)	MaskDICELoss 0.9153 (0.8391)
Epoch: [0][129/500]	Time  4.728 ( 4.728)	Loss 2.3649 (1.7004)	CeLoss 0.2734 (0.6435)	SegCLSLoss 0.0330 (0.0159)	KLLoss 0.5078 (0.2615)	MaskLoss 1.0125 (0.5115)	MaskBCELoss 0.1238 (0.0281)	MaskDICELoss 0.8887 (0.4833)
[2025-03-02 15:38:53,435] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[0.00029038775510204076], mom=[(0.9, 0.95)]
[2025-03-02 15:38:53,441] [INFO] [timer.py:215:stop] epoch=0/micro_step=1300/global_step=130, RunningAvgSamplesPerSec=1.0719154851778807, CurrSamplesPerSec=1.4667734668930663, MemAllocated=31.68GB, MaxMemAllocated=36.58GB
Epoch: [0][130/500]	Time  6.819 ( 6.819)	Loss 0.5586 (2.2430)	CeLoss 0.5586 (0.4216)	SegCLSLoss 0.0000 (0.0475)	KLLoss 0.0000 (0.3377)	MaskLoss 0.0000 (0.8818)	MaskBCELoss 0.0000 (0.1106)	MaskDICELoss 0.0000 (0.7713)
Epoch: [0][131/500]	Time  5.466 ( 5.466)	Loss 1.8877 (2.2461)	CeLoss 0.1953 (0.4939)	SegCLSLoss 0.0339 (0.0321)	KLLoss 0.4961 (0.3305)	MaskLoss 0.8130 (0.8516)	MaskBCELoss 0.3767 (0.2708)	MaskDICELoss 0.4363 (0.5808)
Epoch: [0][132/500]	Time  5.714 ( 5.714)	Loss 2.3065 (2.1961)	CeLoss 0.2754 (0.3212)	SegCLSLoss 0.0306 (0.0369)	KLLoss 0.4727 (0.3613)	MaskLoss 0.9843 (0.9102)	MaskBCELoss 0.2267 (0.1896)	MaskDICELoss 0.7576 (0.7206)
Epoch: [0][133/500]	Time  5.429 ( 5.429)	Loss 0.2773 (1.8889)	CeLoss 0.2773 (0.4431)	SegCLSLoss 0.0000 (0.0220)	KLLoss 0.0000 (0.2814)	MaskLoss 0.0000 (0.7033)	MaskBCELoss 0.0000 (0.1945)	MaskDICELoss 0.0000 (0.5088)
Epoch: [0][134/500]	Time  6.852 ( 6.852)	Loss 2.4442 (2.6061)	CeLoss 0.2139 (0.2337)	SegCLSLoss 0.0491 (0.0576)	KLLoss 0.4414 (0.4496)	MaskLoss 1.0805 (1.1492)	MaskBCELoss 0.1060 (0.2487)	MaskDICELoss 0.9745 (0.9006)
Epoch: [0][135/500]	Time  5.514 ( 5.514)	Loss 2.7926 (2.1488)	CeLoss 0.2490 (0.5062)	SegCLSLoss 0.0454 (0.0297)	KLLoss 0.4434 (0.3279)	MaskLoss 1.2381 (0.7976)	MaskBCELoss 0.3530 (0.1892)	MaskDICELoss 0.8851 (0.6084)
Epoch: [0][136/500]	Time  6.454 ( 6.454)	Loss 2.3260 (2.3165)	CeLoss 0.2432 (0.3247)	SegCLSLoss 0.0354 (0.0375)	KLLoss 0.4492 (0.4006)	MaskLoss 1.0097 (0.9664)	MaskBCELoss 0.0968 (0.1458)	MaskDICELoss 0.9129 (0.8206)
Epoch: [0][137/500]	Time  5.988 ( 5.988)	Loss 2.7396 (2.0096)	CeLoss 0.3691 (0.6478)	SegCLSLoss 0.0349 (0.0198)	KLLoss 0.4414 (0.2826)	MaskLoss 1.1540 (0.6618)	MaskBCELoss 0.2374 (0.1221)	MaskDICELoss 0.9165 (0.5397)
Epoch: [0][138/500]	Time  5.896 ( 5.896)	Loss 0.6328 (2.6431)	CeLoss 0.6328 (0.5075)	SegCLSLoss 0.0000 (0.0356)	KLLoss 0.0000 (0.3176)	MaskLoss 0.0000 (1.0429)	MaskBCELoss 0.0000 (0.3967)	MaskDICELoss 0.0000 (0.6462)
Epoch: [0][139/500]	Time  6.585 ( 6.585)	Loss 2.5545 (2.3601)	CeLoss 0.2559 (0.2417)	SegCLSLoss 0.0287 (0.0334)	KLLoss 0.4902 (0.4215)	MaskLoss 1.1180 (1.0299)	MaskBCELoss 0.2438 (0.2540)	MaskDICELoss 0.8742 (0.7759)
[2025-03-02 15:39:52,752] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[0.00028916326530612244], mom=[(0.9, 0.95)]
[2025-03-02 15:39:52,757] [INFO] [timer.py:215:stop] epoch=0/micro_step=1400/global_step=140, RunningAvgSamplesPerSec=1.1009826553195428, CurrSamplesPerSec=1.846397630724964, MemAllocated=30.73GB, MaxMemAllocated=36.58GB
Epoch: [0][140/500]	Time  5.418 ( 5.418)	Loss 0.8164 (2.3402)	CeLoss 0.8164 (0.4146)	SegCLSLoss 0.0000 (0.0359)	KLLoss 0.0000 (0.3617)	MaskLoss 0.0000 (0.9356)	MaskBCELoss 0.0000 (0.2666)	MaskDICELoss 0.0000 (0.6690)
Epoch: [0][141/500]	Time  5.836 ( 5.836)	Loss 1.7344 (2.4714)	CeLoss 1.7344 (0.4653)	SegCLSLoss 0.0000 (0.0453)	KLLoss 0.0000 (0.3354)	MaskLoss 0.0000 (0.9750)	MaskBCELoss 0.0000 (0.2658)	MaskDICELoss 0.0000 (0.7093)
Epoch: [0][142/500]	Time  5.426 ( 5.426)	Loss 1.1875 (1.9861)	CeLoss 1.1875 (0.5833)	SegCLSLoss 0.0000 (0.0198)	KLLoss 0.0000 (0.2838)	MaskLoss 0.0000 (0.6823)	MaskBCELoss 0.0000 (0.1823)	MaskDICELoss 0.0000 (0.5000)
Epoch: [0][143/500]	Time  5.326 ( 5.326)	Loss 3.1342 (2.4788)	CeLoss 0.1787 (0.2667)	SegCLSLoss 0.0630 (0.0557)	KLLoss 0.3828 (0.3816)	MaskLoss 1.4431 (1.0731)	MaskBCELoss 0.8599 (0.3063)	MaskDICELoss 0.5832 (0.7668)
Epoch: [0][144/500]	Time  6.086 ( 6.086)	Loss 2.4099 (2.2270)	CeLoss 0.2812 (0.3885)	SegCLSLoss 0.0317 (0.0267)	KLLoss 0.4883 (0.3869)	MaskLoss 1.0321 (0.8933)	MaskBCELoss 0.0394 (0.1712)	MaskDICELoss 0.9927 (0.7221)
Epoch: [0][145/500]	Time  6.636 ( 6.636)	Loss 2.3143 (2.5548)	CeLoss 0.1973 (0.2698)	SegCLSLoss 0.0459 (0.0397)	KLLoss 0.4121 (0.4592)	MaskLoss 1.0263 (1.1096)	MaskBCELoss 0.0306 (0.2034)	MaskDICELoss 0.9957 (0.9063)
Epoch: [0][146/500]	Time  7.322 ( 7.322)	Loss 2.7370 (2.1772)	CeLoss 0.2578 (0.2755)	SegCLSLoss 0.0280 (0.0309)	KLLoss 0.4746 (0.3529)	MaskLoss 1.2093 (0.9256)	MaskBCELoss 0.2680 (0.1754)	MaskDICELoss 0.9413 (0.7502)
Epoch: [0][147/500]	Time  5.719 ( 5.719)	Loss 2.3942 (2.3344)	CeLoss 0.2871 (0.4450)	SegCLSLoss 0.0232 (0.0229)	KLLoss 0.5391 (0.3836)	MaskLoss 1.0203 (0.9196)	MaskBCELoss 0.3055 (0.2044)	MaskDICELoss 0.7148 (0.7153)
Epoch: [0][148/500]	Time  6.791 ( 6.791)	Loss 2.4662 (2.1438)	CeLoss 0.1963 (0.4667)	SegCLSLoss 0.0347 (0.0221)	KLLoss 0.4414 (0.3270)	MaskLoss 1.1042 (0.8166)	MaskBCELoss 0.1478 (0.1963)	MaskDICELoss 0.9564 (0.6204)
Epoch: [0][149/500]	Time  5.933 ( 5.933)	Loss 2.3024 (2.3546)	CeLoss 0.1631 (0.5027)	SegCLSLoss 0.0540 (0.0342)	KLLoss 0.4336 (0.3059)	MaskLoss 1.0340 (0.9021)	MaskBCELoss 0.0438 (0.2883)	MaskDICELoss 0.9902 (0.6138)
[2025-03-02 15:40:53,570] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[0.000287938775510204], mom=[(0.9, 0.95)]
[2025-03-02 15:40:53,576] [INFO] [timer.py:215:stop] epoch=0/micro_step=1500/global_step=150, RunningAvgSamplesPerSec=1.1261348057565141, CurrSamplesPerSec=1.7417526783320203, MemAllocated=31.23GB, MaxMemAllocated=36.58GB
Epoch: [0][150/500]	Time  5.743 ( 5.743)	Loss 2.7740 (2.2877)	CeLoss 0.2344 (0.3052)	SegCLSLoss 0.0256 (0.0313)	KLLoss 0.5078 (0.3680)	MaskLoss 1.2386 (0.9652)	MaskBCELoss 0.4066 (0.2500)	MaskDICELoss 0.8320 (0.7152)
Epoch: [0][151/500]	Time  5.650 ( 5.650)	Loss 2.5620 (2.1012)	CeLoss 0.2441 (0.6289)	SegCLSLoss 0.0261 (0.0209)	KLLoss 0.4883 (0.2744)	MaskLoss 1.1277 (0.7172)	MaskBCELoss 0.1884 (0.1619)	MaskDICELoss 0.9393 (0.5553)
Epoch: [0][152/500]	Time  6.352 ( 6.352)	Loss 2.5043 (2.2087)	CeLoss 0.2188 (0.2725)	SegCLSLoss 0.0386 (0.0341)	KLLoss 0.4414 (0.3598)	MaskLoss 1.1106 (0.9415)	MaskBCELoss 0.1882 (0.2771)	MaskDICELoss 0.9223 (0.6644)
Epoch: [0][153/500]	Time  6.492 ( 6.492)	Loss 3.2797 (2.4613)	CeLoss 0.3633 (0.3842)	SegCLSLoss 0.0232 (0.0274)	KLLoss 0.5273 (0.3734)	MaskLoss 1.4260 (1.0130)	MaskBCELoss 0.6671 (0.3336)	MaskDICELoss 0.7589 (0.6795)
Epoch: [0][154/500]	Time  5.466 ( 5.466)	Loss 2.7208 (2.3709)	CeLoss 0.2168 (0.4283)	SegCLSLoss 0.0442 (0.0279)	KLLoss 0.4121 (0.3545)	MaskLoss 1.2198 (0.9465)	MaskBCELoss 0.3377 (0.2409)	MaskDICELoss 0.8821 (0.7055)
Epoch: [0][155/500]	Time  5.684 ( 5.684)	Loss 2.6265 (2.3324)	CeLoss 0.2432 (0.4098)	SegCLSLoss 0.0605 (0.0303)	KLLoss 0.4121 (0.3617)	MaskLoss 1.1560 (0.9357)	MaskBCELoss 0.2054 (0.2864)	MaskDICELoss 0.9506 (0.6493)
Epoch: [0][156/500]	Time  6.103 ( 6.103)	Loss 2.3083 (2.5220)	CeLoss 0.2100 (0.2221)	SegCLSLoss 0.0356 (0.0441)	KLLoss 0.4570 (0.4359)	MaskLoss 1.0174 (1.1172)	MaskBCELoss 0.0225 (0.2115)	MaskDICELoss 0.9949 (0.9058)
Epoch: [0][157/500]	Time  5.118 ( 5.118)	Loss 1.5938 (1.6479)	CeLoss 1.5938 (0.7460)	SegCLSLoss 0.0000 (0.0130)	KLLoss 0.0000 (0.1818)	MaskLoss 0.0000 (0.4386)	MaskBCELoss 0.0000 (0.1053)	MaskDICELoss 0.0000 (0.3332)
Epoch: [0][158/500]	Time  5.338 ( 5.338)	Loss 2.7866 (2.2773)	CeLoss 0.1562 (0.6954)	SegCLSLoss 0.0864 (0.0250)	KLLoss 0.3711 (0.2633)	MaskLoss 1.2751 (0.7715)	MaskBCELoss 0.3710 (0.2450)	MaskDICELoss 0.9041 (0.5265)
Epoch: [0][159/500]	Time  5.761 ( 5.761)	Loss 0.8555 (2.3592)	CeLoss 0.8555 (0.6889)	SegCLSLoss 0.0000 (0.0202)	KLLoss 0.0000 (0.2586)	MaskLoss 0.0000 (0.8173)	MaskBCELoss 0.0000 (0.2585)	MaskDICELoss 0.0000 (0.5588)
[2025-03-02 15:41:52,125] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[0.0002867142857142857], mom=[(0.9, 0.95)]
[2025-03-02 15:41:52,131] [INFO] [timer.py:215:stop] epoch=0/micro_step=1600/global_step=160, RunningAvgSamplesPerSec=1.1509567418200568, CurrSamplesPerSec=1.517663743017724, MemAllocated=31.25GB, MaxMemAllocated=36.58GB
Epoch: [0][160/500]	Time  6.591 ( 6.591)	Loss 2.3690 (2.5006)	CeLoss 0.1934 (0.3340)	SegCLSLoss 0.0454 (0.0311)	KLLoss 0.4062 (0.3959)	MaskLoss 1.0566 (1.0560)	MaskBCELoss 0.0910 (0.2041)	MaskDICELoss 0.9656 (0.8519)
Epoch: [0][161/500]	Time  5.381 ( 5.381)	Loss 2.6888 (1.8194)	CeLoss 0.2715 (0.6649)	SegCLSLoss 0.0208 (0.0138)	KLLoss 0.4570 (0.2277)	MaskLoss 1.1813 (0.5625)	MaskBCELoss 0.2047 (0.1082)	MaskDICELoss 0.9766 (0.4543)
Epoch: [0][162/500]	Time  5.527 ( 5.527)	Loss 2.5885 (2.4440)	CeLoss 0.2451 (0.4615)	SegCLSLoss 0.0199 (0.0297)	KLLoss 0.4648 (0.3514)	MaskLoss 1.1429 (0.9659)	MaskBCELoss 0.2058 (0.2543)	MaskDICELoss 0.9371 (0.7116)
Epoch: [0][163/500]	Time  6.719 ( 6.719)	Loss 2.4980 (2.4588)	CeLoss 0.2637 (0.3130)	SegCLSLoss 0.0347 (0.0348)	KLLoss 0.4023 (0.3945)	MaskLoss 1.0879 (1.0442)	MaskBCELoss 0.1411 (0.2646)	MaskDICELoss 0.9468 (0.7796)
Epoch: [0][164/500]	Time  5.884 ( 5.884)	Loss 2.8510 (2.0902)	CeLoss 0.1572 (0.4162)	SegCLSLoss 0.0728 (0.0308)	KLLoss 0.3750 (0.2887)	MaskLoss 1.3103 (0.8148)	MaskBCELoss 0.4085 (0.1732)	MaskDICELoss 0.9018 (0.6416)
Epoch: [0][165/500]	Time  6.287 ( 6.287)	Loss 0.6211 (2.0050)	CeLoss 0.6211 (0.3758)	SegCLSLoss 0.0000 (0.0229)	KLLoss 0.0000 (0.3074)	MaskLoss 0.0000 (0.7936)	MaskBCELoss 0.0000 (0.1921)	MaskDICELoss 0.0000 (0.6015)
Epoch: [0][166/500]	Time  6.232 ( 6.232)	Loss 2.4163 (2.4192)	CeLoss 0.2617 (0.4715)	SegCLSLoss 0.0293 (0.0325)	KLLoss 0.4531 (0.3459)	MaskLoss 1.0470 (0.9485)	MaskBCELoss 0.0665 (0.2668)	MaskDICELoss 0.9805 (0.6817)
Epoch: [0][167/500]	Time  6.506 ( 6.506)	Loss 2.2631 (2.6612)	CeLoss 0.3223 (0.2303)	SegCLSLoss 0.0522 (0.0394)	KLLoss 0.3809 (0.4338)	MaskLoss 0.9382 (1.1838)	MaskBCELoss 0.0614 (0.3167)	MaskDICELoss 0.8768 (0.8671)
Epoch: [0][168/500]	Time  5.806 ( 5.806)	Loss 0.8633 (2.3577)	CeLoss 0.8633 (0.3062)	SegCLSLoss 0.0000 (0.0301)	KLLoss 0.0000 (0.3889)	MaskLoss 0.0000 (0.9988)	MaskBCELoss 0.0000 (0.2761)	MaskDICELoss 0.0000 (0.7227)
Epoch: [0][169/500]	Time  6.219 ( 6.219)	Loss 2.2327 (2.3431)	CeLoss 0.2402 (0.3025)	SegCLSLoss 0.0334 (0.0312)	KLLoss 0.4805 (0.3918)	MaskLoss 0.9630 (0.9927)	MaskBCELoss 0.1271 (0.1868)	MaskDICELoss 0.8359 (0.8059)
[2025-03-02 15:42:52,515] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[0.00028548979591836734], mom=[(0.9, 0.95)]
[2025-03-02 15:42:52,520] [INFO] [timer.py:215:stop] epoch=0/micro_step=1700/global_step=170, RunningAvgSamplesPerSec=1.1722462887665535, CurrSamplesPerSec=1.7159312009735506, MemAllocated=30.8GB, MaxMemAllocated=36.58GB
Epoch: [0][170/500]	Time  5.830 ( 5.830)	Loss 2.4733 (2.1532)	CeLoss 0.1953 (0.4266)	SegCLSLoss 0.0415 (0.0297)	KLLoss 0.4297 (0.3467)	MaskLoss 1.1078 (0.8386)	MaskBCELoss 0.1421 (0.1624)	MaskDICELoss 0.9657 (0.6762)
Epoch: [0][171/500]	Time  5.590 ( 5.590)	Loss 2.3364 (2.1960)	CeLoss 0.2002 (0.4935)	SegCLSLoss 0.0312 (0.0180)	KLLoss 0.4414 (0.3158)	MaskLoss 1.0383 (0.8311)	MaskBCELoss 0.0393 (0.2676)	MaskDICELoss 0.9990 (0.5635)
Epoch: [0][172/500]	Time  6.432 ( 6.432)	Loss 3.5432 (2.3053)	CeLoss 0.2383 (0.3178)	SegCLSLoss 0.0215 (0.0248)	KLLoss 0.4629 (0.3363)	MaskLoss 1.6241 (0.9709)	MaskBCELoss 0.7518 (0.2436)	MaskDICELoss 0.8723 (0.7273)
Epoch: [0][173/500]	Time  6.389 ( 6.389)	Loss 0.7500 (1.8560)	CeLoss 0.7500 (0.5592)	SegCLSLoss 0.0000 (0.0151)	KLLoss 0.0000 (0.2676)	MaskLoss 0.0000 (0.6313)	MaskBCELoss 0.0000 (0.1135)	MaskDICELoss 0.0000 (0.5178)
Epoch: [0][174/500]	Time  6.152 ( 6.152)	Loss 2.5361 (2.5416)	CeLoss 0.1641 (0.3351)	SegCLSLoss 0.0408 (0.0281)	KLLoss 0.4004 (0.3816)	MaskLoss 1.1557 (1.0774)	MaskBCELoss 0.1800 (0.2534)	MaskDICELoss 0.9757 (0.8241)
Epoch: [0][175/500]	Time  4.636 ( 4.636)	Loss 1.5703 (2.0707)	CeLoss 1.5703 (0.6268)	SegCLSLoss 0.0000 (0.0185)	KLLoss 0.0000 (0.2488)	MaskLoss 0.0000 (0.7049)	MaskBCELoss 0.0000 (0.2419)	MaskDICELoss 0.0000 (0.4629)
Epoch: [0][176/500]	Time  6.056 ( 6.056)	Loss 2.6927 (2.2578)	CeLoss 0.2461 (0.4221)	SegCLSLoss 0.0293 (0.0241)	KLLoss 0.4004 (0.3377)	MaskLoss 1.1960 (0.8949)	MaskBCELoss 0.3677 (0.2027)	MaskDICELoss 0.8283 (0.6923)
Epoch: [0][177/500]	Time  6.347 ( 6.347)	Loss 2.5432 (2.3556)	CeLoss 0.1895 (0.4469)	SegCLSLoss 0.0454 (0.0232)	KLLoss 0.3867 (0.3338)	MaskLoss 1.1466 (0.9320)	MaskBCELoss 0.2551 (0.2412)	MaskDICELoss 0.8915 (0.6908)
Epoch: [0][178/500]	Time  5.939 ( 5.939)	Loss 0.2969 (1.8984)	CeLoss 0.2969 (0.5017)	SegCLSLoss 0.0000 (0.0217)	KLLoss 0.0000 (0.2469)	MaskLoss 0.0000 (0.6805)	MaskBCELoss 0.0000 (0.1107)	MaskDICELoss 0.0000 (0.5698)
Epoch: [0][179/500]	Time  4.379 ( 4.379)	Loss 1.1016 (1.9034)	CeLoss 1.1016 (0.7416)	SegCLSLoss 0.0000 (0.0207)	KLLoss 0.0000 (0.2115)	MaskLoss 0.0000 (0.5653)	MaskBCELoss 0.0000 (0.1743)	MaskDICELoss 0.0000 (0.3910)
[2025-03-02 15:43:50,529] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[0.00028426530612244897], mom=[(0.9, 0.95)]
[2025-03-02 15:43:50,534] [INFO] [timer.py:215:stop] epoch=0/micro_step=1800/global_step=180, RunningAvgSamplesPerSec=1.193713639303425, CurrSamplesPerSec=1.6415868587384994, MemAllocated=31.25GB, MaxMemAllocated=36.81GB
Epoch: [0][180/500]	Time  6.094 ( 6.094)	Loss 2.2431 (2.1343)	CeLoss 0.2617 (0.5338)	SegCLSLoss 0.0327 (0.0219)	KLLoss 0.4004 (0.2869)	MaskLoss 0.9634 (0.7806)	MaskBCELoss 0.0447 (0.1531)	MaskDICELoss 0.9186 (0.6275)
Epoch: [0][181/500]	Time  6.526 ( 6.526)	Loss 2.5779 (2.5303)	CeLoss 0.2754 (0.2844)	SegCLSLoss 0.0173 (0.0219)	KLLoss 0.4570 (0.4504)	MaskLoss 1.1249 (1.0949)	MaskBCELoss 0.2670 (0.2867)	MaskDICELoss 0.8579 (0.8082)
Epoch: [0][182/500]	Time  5.353 ( 5.353)	Loss 2.3432 (2.0890)	CeLoss 0.2246 (0.4097)	SegCLSLoss 0.0383 (0.0260)	KLLoss 0.4180 (0.2912)	MaskLoss 1.0290 (0.8186)	MaskBCELoss 0.0498 (0.2069)	MaskDICELoss 0.9792 (0.6117)
Epoch: [0][183/500]	Time  6.276 ( 6.276)	Loss 1.9800 (1.8715)	CeLoss 0.2393 (0.3497)	SegCLSLoss 0.0325 (0.0218)	KLLoss 0.4160 (0.2963)	MaskLoss 0.8416 (0.7406)	MaskBCELoss 0.1588 (0.1825)	MaskDICELoss 0.6828 (0.5581)
Epoch: [0][184/500]	Time  6.955 ( 6.955)	Loss 2.4602 (2.3345)	CeLoss 0.2832 (0.3367)	SegCLSLoss 0.0278 (0.0356)	KLLoss 0.4160 (0.3615)	MaskLoss 1.0612 (0.9721)	MaskBCELoss 0.0847 (0.1816)	MaskDICELoss 0.9764 (0.7905)
Epoch: [0][185/500]	Time  5.858 ( 5.858)	Loss 2.6699 (2.2163)	CeLoss 0.2812 (0.4038)	SegCLSLoss 0.0311 (0.0316)	KLLoss 0.4258 (0.3197)	MaskLoss 1.1650 (0.8823)	MaskBCELoss 0.2118 (0.1345)	MaskDICELoss 0.9532 (0.7478)
Epoch: [0][186/500]	Time  6.538 ( 6.538)	Loss 2.2682 (2.3220)	CeLoss 0.1855 (0.2954)	SegCLSLoss 0.0447 (0.0300)	KLLoss 0.3867 (0.3639)	MaskLoss 1.0111 (0.9878)	MaskBCELoss 0.0349 (0.1844)	MaskDICELoss 0.9762 (0.8033)
Epoch: [0][187/500]	Time  5.804 ( 5.804)	Loss 2.3547 (2.2366)	CeLoss 0.1865 (0.3961)	SegCLSLoss 0.0435 (0.0293)	KLLoss 0.3789 (0.3270)	MaskLoss 1.0543 (0.8966)	MaskBCELoss 0.0663 (0.1883)	MaskDICELoss 0.9880 (0.7083)
Epoch: [0][188/500]	Time  4.980 ( 4.980)	Loss 1.2656 (1.6581)	CeLoss 1.2656 (0.6451)	SegCLSLoss 0.0000 (0.0167)	KLLoss 0.0000 (0.1584)	MaskLoss 0.0000 (0.4943)	MaskBCELoss 0.0000 (0.1651)	MaskDICELoss 0.0000 (0.3292)
Epoch: [0][189/500]	Time  6.395 ( 6.395)	Loss 2.5147 (2.4376)	CeLoss 0.2422 (0.2099)	SegCLSLoss 0.0288 (0.0350)	KLLoss 0.4258 (0.3572)	MaskLoss 1.1079 (1.0874)	MaskBCELoss 0.1689 (0.2790)	MaskDICELoss 0.9391 (0.8084)
[2025-03-02 15:44:50,097] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[0.0002830408163265306], mom=[(0.9, 0.95)]
[2025-03-02 15:44:50,103] [INFO] [timer.py:215:stop] epoch=0/micro_step=1900/global_step=190, RunningAvgSamplesPerSec=1.2123573498503535, CurrSamplesPerSec=2.0478337135026585, MemAllocated=30.7GB, MaxMemAllocated=36.81GB
Epoch: [0][190/500]	Time  4.885 ( 4.885)	Loss 1.0781 (1.8577)	CeLoss 1.0781 (0.4649)	SegCLSLoss 0.0000 (0.0238)	KLLoss 0.0000 (0.2408)	MaskLoss 0.0000 (0.6784)	MaskBCELoss 0.0000 (0.1291)	MaskDICELoss 0.0000 (0.5494)
Epoch: [0][191/500]	Time  5.477 ( 5.477)	Loss 0.9180 (1.5649)	CeLoss 0.9180 (0.6104)	SegCLSLoss 0.0000 (0.0117)	KLLoss 0.0000 (0.1635)	MaskLoss 0.0000 (0.4660)	MaskBCELoss 0.0000 (0.1101)	MaskDICELoss 0.0000 (0.3559)
Epoch: [0][192/500]	Time  6.126 ( 6.126)	Loss 1.2969 (1.9995)	CeLoss 1.2969 (0.4623)	SegCLSLoss 0.0000 (0.0186)	KLLoss 0.0000 (0.2883)	MaskLoss 0.0000 (0.7496)	MaskBCELoss 0.0000 (0.1793)	MaskDICELoss 0.0000 (0.5703)
Epoch: [0][193/500]	Time  7.161 ( 7.161)	Loss 0.7227 (2.0096)	CeLoss 0.7227 (0.3534)	SegCLSLoss 0.0000 (0.0225)	KLLoss 0.0000 (0.2871)	MaskLoss 0.0000 (0.8081)	MaskBCELoss 0.0000 (0.2087)	MaskDICELoss 0.0000 (0.5994)
Epoch: [0][194/500]	Time  5.156 ( 5.156)	Loss 2.3484 (1.7999)	CeLoss 0.1982 (0.4275)	SegCLSLoss 0.0261 (0.0165)	KLLoss 0.4160 (0.2557)	MaskLoss 1.0482 (0.6693)	MaskBCELoss 0.0812 (0.1667)	MaskDICELoss 0.9670 (0.5025)
Epoch: [0][195/500]	Time  7.370 ( 7.370)	Loss 2.7992 (2.1725)	CeLoss 0.2344 (0.2923)	SegCLSLoss 0.0188 (0.0181)	KLLoss 0.4551 (0.3500)	MaskLoss 1.2551 (0.9181)	MaskBCELoss 0.4871 (0.2079)	MaskDICELoss 0.7679 (0.7102)
Epoch: [0][196/500]	Time  5.958 ( 5.958)	Loss 2.4462 (2.2565)	CeLoss 0.2441 (0.3555)	SegCLSLoss 0.0349 (0.0231)	KLLoss 0.4160 (0.3346)	MaskLoss 1.0717 (0.9279)	MaskBCELoss 0.2558 (0.2370)	MaskDICELoss 0.8160 (0.6909)
Epoch: [0][197/500]	Time  6.094 ( 6.094)	Loss 2.1460 (2.2020)	CeLoss 0.2559 (0.4355)	SegCLSLoss 0.0299 (0.0197)	KLLoss 0.4258 (0.3355)	MaskLoss 0.9158 (0.8614)	MaskBCELoss 0.0445 (0.1638)	MaskDICELoss 0.8712 (0.6977)
Epoch: [0][198/500]	Time  6.667 ( 6.667)	Loss 2.5988 (2.3947)	CeLoss 0.2217 (0.3110)	SegCLSLoss 0.0193 (0.0234)	KLLoss 0.4277 (0.3678)	MaskLoss 1.1627 (1.0178)	MaskBCELoss 0.1882 (0.1834)	MaskDICELoss 0.9745 (0.8344)
Epoch: [0][199/500]	Time  5.265 ( 5.265)	Loss 1.3516 (1.9789)	CeLoss 1.3516 (0.5037)	SegCLSLoss 0.0000 (0.0221)	KLLoss 0.0000 (0.2809)	MaskLoss 0.0000 (0.7179)	MaskBCELoss 0.0000 (0.1420)	MaskDICELoss 0.0000 (0.5759)
[2025-03-02 15:45:51,959] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[0.00028181632653061223], mom=[(0.9, 0.95)]
[2025-03-02 15:45:51,965] [INFO] [timer.py:215:stop] epoch=0/micro_step=2000/global_step=200, RunningAvgSamplesPerSec=1.2278725082310717, CurrSamplesPerSec=1.518430084602718, MemAllocated=31.46GB, MaxMemAllocated=36.81GB
Epoch: [0][200/500]	Time  6.587 ( 6.587)	Loss 2.4973 (2.3281)	CeLoss 0.2002 (0.3088)	SegCLSLoss 0.0344 (0.0345)	KLLoss 0.3906 (0.3623)	MaskLoss 1.1197 (0.9828)	MaskBCELoss 0.1581 (0.1752)	MaskDICELoss 0.9617 (0.8076)
Epoch: [0][201/500]	Time  4.030 ( 4.030)	Loss 0.7930 (1.6183)	CeLoss 0.7930 (0.7479)	SegCLSLoss 0.0000 (0.0092)	KLLoss 0.0000 (0.1635)	MaskLoss 0.0000 (0.4247)	MaskBCELoss 0.0000 (0.0351)	MaskDICELoss 0.0000 (0.3896)
Epoch: [0][202/500]	Time  6.436 ( 6.436)	Loss 2.3054 (2.9294)	CeLoss 0.1836 (0.2260)	SegCLSLoss 0.0444 (0.0338)	KLLoss 0.3711 (0.4041)	MaskLoss 1.0311 (1.3230)	MaskBCELoss 0.0795 (0.4781)	MaskDICELoss 0.9516 (0.8450)
Epoch: [0][203/500]	Time  4.037 ( 4.037)	Loss 2.4065 (1.8429)	CeLoss 0.2314 (0.4879)	SegCLSLoss 0.0221 (0.0166)	KLLoss 0.4316 (0.2488)	MaskLoss 1.0607 (0.6609)	MaskBCELoss 0.0721 (0.1281)	MaskDICELoss 0.9886 (0.5328)
Epoch: [0][204/500]	Time  5.549 ( 5.549)	Loss 2.3053 (2.0031)	CeLoss 0.2490 (0.4564)	SegCLSLoss 0.0250 (0.0274)	KLLoss 0.4160 (0.2789)	MaskLoss 1.0013 (0.7526)	MaskBCELoss 0.2329 (0.1289)	MaskDICELoss 0.7684 (0.6237)
Epoch: [0][205/500]	Time  6.434 ( 6.434)	Loss 2.5955 (2.1041)	CeLoss 0.2422 (0.2983)	SegCLSLoss 0.0187 (0.0203)	KLLoss 0.4277 (0.3350)	MaskLoss 1.1503 (0.8811)	MaskBCELoss 0.3262 (0.2453)	MaskDICELoss 0.8241 (0.6358)
Epoch: [0][206/500]	Time  6.426 ( 6.426)	Loss 2.6018 (2.1074)	CeLoss 0.3320 (0.5737)	SegCLSLoss 0.0256 (0.0189)	KLLoss 0.3906 (0.2803)	MaskLoss 1.1085 (0.7481)	MaskBCELoss 0.1525 (0.1288)	MaskDICELoss 0.9560 (0.6193)
Epoch: [0][207/500]	Time  7.163 ( 7.163)	Loss 2.3818 (2.2566)	CeLoss 0.2949 (0.2759)	SegCLSLoss 0.0231 (0.0270)	KLLoss 0.4102 (0.3607)	MaskLoss 1.0171 (0.9656)	MaskBCELoss 0.0178 (0.1953)	MaskDICELoss 0.9993 (0.7702)
Epoch: [0][208/500]	Time  5.520 ( 5.520)	Loss 2.7239 (2.4364)	CeLoss 0.2988 (0.4272)	SegCLSLoss 0.0206 (0.0179)	KLLoss 0.4258 (0.3336)	MaskLoss 1.1862 (0.9834)	MaskBCELoss 0.3485 (0.2854)	MaskDICELoss 0.8377 (0.6980)
Epoch: [0][209/500]	Time  5.941 ( 5.941)	Loss 2.5393 (2.1127)	CeLoss 0.1318 (0.4411)	SegCLSLoss 0.0659 (0.0248)	KLLoss 0.3633 (0.2795)	MaskLoss 1.1691 (0.8155)	MaskBCELoss 0.2549 (0.2034)	MaskDICELoss 0.9141 (0.6121)
[2025-03-02 15:46:48,939] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=0, lr=[0.00028059183673469386], mom=[(0.9, 0.95)]
[2025-03-02 15:46:48,945] [INFO] [timer.py:215:stop] epoch=0/micro_step=2100/global_step=210, RunningAvgSamplesPerSec=1.2458735361771, CurrSamplesPerSec=1.8375599113268553, MemAllocated=31.1GB, MaxMemAllocated=36.81GB
Epoch: [0][210/500]	Time  5.444 ( 5.444)	Loss 2.3005 (2.1742)	CeLoss 0.2617 (0.4804)	SegCLSLoss 0.0415 (0.0207)	KLLoss 0.3711 (0.3352)	MaskLoss 0.9901 (0.8249)	MaskBCELoss 0.1060 (0.2139)	MaskDICELoss 0.8841 (0.6110)
Epoch: [0][211/500]	Time  6.236 ( 6.236)	Loss 2.6360 (2.3976)	CeLoss 0.2520 (0.3835)	SegCLSLoss 0.0349 (0.0267)	KLLoss 0.4160 (0.3291)	MaskLoss 1.1627 (0.9838)	MaskBCELoss 0.4432 (0.2755)	MaskDICELoss 0.7196 (0.7084)
Epoch: [0][212/500]	Time  6.346 ( 6.346)	Loss 2.6573 (2.2917)	CeLoss 0.2188 (0.3666)	SegCLSLoss 0.0183 (0.0229)	KLLoss 0.4570 (0.3256)	MaskLoss 1.1919 (0.9406)	MaskBCELoss 0.2627 (0.2383)	MaskDICELoss 0.9292 (0.7022)
Epoch: [0][213/500]	Time  5.376 ( 5.376)	Loss 2.7498 (2.0904)	CeLoss 0.3008 (0.4401)	SegCLSLoss 0.0189 (0.0276)	KLLoss 0.4336 (0.2777)	MaskLoss 1.1982 (0.8045)	MaskBCELoss 0.3360 (0.2349)	MaskDICELoss 0.8621 (0.5696)
Epoch: [0][214/500]	Time  6.113 ( 6.113)	Loss 0.7305 (1.7573)	CeLoss 0.7305 (0.4664)	SegCLSLoss 0.0000 (0.0185)	KLLoss 0.0000 (0.2436)	MaskLoss 0.0000 (0.6286)	MaskBCELoss 0.0000 (0.1481)	MaskDICELoss 0.0000 (0.4805)
Epoch: [0][215/500]	Time  6.515 ( 6.515)	Loss 2.6464 (2.2575)	CeLoss 0.2988 (0.3078)	SegCLSLoss 0.0203 (0.0241)	KLLoss 0.4434 (0.3781)	MaskLoss 1.1465 (0.9500)	MaskBCELoss 0.4693 (0.1719)	MaskDICELoss 0.6772 (0.7780)
Epoch: [0][216/500]	Time  5.717 ( 5.717)	Loss 2.3481 (1.9890)	CeLoss 0.2295 (0.3994)	SegCLSLoss 0.0175 (0.0154)	KLLoss 0.4785 (0.3063)	MaskLoss 1.0315 (0.7758)	MaskBCELoss 0.0412 (0.2117)	MaskDICELoss 0.9903 (0.5641)
Epoch: [0][217/500]	Time  7.405 ( 7.405)	Loss 2.0883 (2.6423)	CeLoss 0.2070 (0.2446)	SegCLSLoss 0.0352 (0.0311)	KLLoss 0.4160 (0.4115)	MaskLoss 0.9113 (1.1705)	MaskBCELoss 0.3137 (0.3197)	MaskDICELoss 0.5976 (0.8508)
Epoch: [0][218/500]	Time  5.255 ( 5.255)	Loss 2.5201 (1.9590)	CeLoss 0.2812 (0.5666)	SegCLSLoss 0.0167 (0.0154)	KLLoss 0.4473 (0.2596)	MaskLoss 1.0930 (0.6794)	MaskBCELoss 0.2142 (0.2187)	MaskDICELoss 0.8788 (0.4607)
Epoch: [0][219/500]	Time  5.103 ( 5.103)	Loss 1.2500 (2.0584)	CeLoss 1.2500 (0.6289)	SegCLSLoss 0.0000 (0.0118)	KLLoss 0.0000 (0.2666)	MaskLoss 0.0000 (0.6983)	MaskBCELoss 0.0000 (0.2415)	MaskDICELoss 0.0000 (0.4568)
[2025-03-02 15:47:48,237] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=0, lr=[0.0002793673469387755], mom=[(0.9, 0.95)]
[2025-03-02 15:47:48,242] [INFO] [timer.py:215:stop] epoch=0/micro_step=2200/global_step=220, RunningAvgSamplesPerSec=1.2609940407144047, CurrSamplesPerSec=1.9117128868231772, MemAllocated=30.91GB, MaxMemAllocated=36.81GB
Epoch: [0][220/500]	Time  5.232 ( 5.232)	Loss 0.7109 (1.9133)	CeLoss 0.7109 (0.6204)	SegCLSLoss 0.0000 (0.0196)	KLLoss 0.0000 (0.2496)	MaskLoss 0.0000 (0.6290)	MaskBCELoss 0.0000 (0.1337)	MaskDICELoss 0.0000 (0.4953)
Epoch: [0][221/500]	Time  6.393 ( 6.393)	Loss 2.4961 (2.1954)	CeLoss 0.2207 (0.4526)	SegCLSLoss 0.0245 (0.0240)	KLLoss 0.4473 (0.3283)	MaskLoss 1.1094 (0.8491)	MaskBCELoss 0.4814 (0.1164)	MaskDICELoss 0.6280 (0.7327)
Epoch: [0][222/500]	Time  6.125 ( 6.125)	Loss 2.4242 (2.2223)	CeLoss 0.2471 (0.2254)	SegCLSLoss 0.0260 (0.0276)	KLLoss 0.4121 (0.3738)	MaskLoss 1.0617 (0.9729)	MaskBCELoss 0.0916 (0.2345)	MaskDICELoss 0.9701 (0.7384)
Epoch: [0][223/500]	Time  6.699 ( 6.699)	Loss 2.7967 (2.2588)	CeLoss 0.3828 (0.3257)	SegCLSLoss 0.0167 (0.0262)	KLLoss 0.4473 (0.3680)	MaskLoss 1.1806 (0.9415)	MaskBCELoss 0.4663 (0.1780)	MaskDICELoss 0.7143 (0.7635)
Epoch: [0][224/500]	Time  5.895 ( 5.895)	Loss 2.5889 (1.7401)	CeLoss 0.2500 (0.4546)	SegCLSLoss 0.0369 (0.0194)	KLLoss 0.3848 (0.2420)	MaskLoss 1.1411 (0.6259)	MaskBCELoss 0.3098 (0.1473)	MaskDICELoss 0.8314 (0.4786)
Epoch: [0][225/500]	Time  6.277 ( 6.277)	Loss 2.0155 (2.2741)	CeLoss 0.2637 (0.3754)	SegCLSLoss 0.0219 (0.0240)	KLLoss 0.4160 (0.3652)	MaskLoss 0.8496 (0.9251)	MaskBCELoss 0.1926 (0.1108)	MaskDICELoss 0.6569 (0.8142)
Epoch: [0][226/500]	Time  7.321 ( 7.321)	Loss 2.5688 (2.1098)	CeLoss 0.2832 (0.2564)	SegCLSLoss 0.0172 (0.0288)	KLLoss 0.4414 (0.3244)	MaskLoss 1.1165 (0.9031)	MaskBCELoss 0.3339 (0.2190)	MaskDICELoss 0.7826 (0.6841)
Epoch: [0][227/500]	Time  7.237 ( 7.237)	Loss 2.1212 (2.1958)	CeLoss 0.4082 (0.2718)	SegCLSLoss 0.0154 (0.0226)	KLLoss 0.4316 (0.3703)	MaskLoss 0.8311 (0.9377)	MaskBCELoss 0.1029 (0.1485)	MaskDICELoss 0.7282 (0.7892)
Epoch: [0][228/500]	Time  6.117 ( 6.117)	Loss 2.5006 (2.5027)	CeLoss 0.2178 (0.2463)	SegCLSLoss 0.0306 (0.0297)	KLLoss 0.3984 (0.4021)	MaskLoss 1.1136 (1.1007)	MaskBCELoss 0.1452 (0.2760)	MaskDICELoss 0.9684 (0.8247)
Epoch: [0][229/500]	Time  5.637 ( 5.637)	Loss 0.8633 (1.9890)	CeLoss 0.8633 (0.6401)	SegCLSLoss 0.0000 (0.0216)	KLLoss 0.0000 (0.2430)	MaskLoss 0.0000 (0.6569)	MaskBCELoss 0.0000 (0.1587)	MaskDICELoss 0.0000 (0.4982)
[2025-03-02 15:48:52,241] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=0, lr=[0.0002781428571428571], mom=[(0.9, 0.95)]
[2025-03-02 15:48:52,247] [INFO] [timer.py:215:stop] epoch=0/micro_step=2300/global_step=230, RunningAvgSamplesPerSec=1.2717641135626536, CurrSamplesPerSec=1.5869164626601482, MemAllocated=31.31GB, MaxMemAllocated=36.81GB
Epoch: [0][230/500]	Time  6.303 ( 6.303)	Loss 2.1843 (2.4793)	CeLoss 0.2393 (0.2229)	SegCLSLoss 0.0197 (0.0344)	KLLoss 0.4258 (0.3924)	MaskLoss 0.9466 (1.1000)	MaskBCELoss 0.0969 (0.1854)	MaskDICELoss 0.8498 (0.9145)
Epoch: [0][231/500]	Time  4.492 ( 4.492)	Loss 1.3750 (1.7053)	CeLoss 1.3750 (0.7632)	SegCLSLoss 0.0000 (0.0138)	KLLoss 0.0000 (0.1635)	MaskLoss 0.0000 (0.4595)	MaskBCELoss 0.0000 (0.1018)	MaskDICELoss 0.0000 (0.3577)
Epoch: [0][232/500]	Time  6.444 ( 6.444)	Loss 2.4982 (2.1193)	CeLoss 0.1807 (0.3064)	SegCLSLoss 0.0664 (0.0281)	KLLoss 0.3613 (0.3279)	MaskLoss 1.1241 (0.8831)	MaskBCELoss 0.2483 (0.1975)	MaskDICELoss 0.8758 (0.6857)
Epoch: [0][233/500]	Time  6.408 ( 6.408)	Loss 2.3388 (2.0684)	CeLoss 0.1875 (0.3620)	SegCLSLoss 0.0444 (0.0225)	KLLoss 0.3926 (0.3297)	MaskLoss 1.0449 (0.8311)	MaskBCELoss 0.1655 (0.1520)	MaskDICELoss 0.8794 (0.6792)
Epoch: [0][234/500]	Time  5.735 ( 5.735)	Loss 2.4071 (2.0239)	CeLoss 0.2930 (0.5618)	SegCLSLoss 0.0190 (0.0190)	KLLoss 0.4629 (0.2943)	MaskLoss 1.0288 (0.7117)	MaskBCELoss 0.2509 (0.1249)	MaskDICELoss 0.7779 (0.5867)
Epoch: [0][235/500]	Time  6.999 ( 6.999)	Loss 2.5483 (2.6023)	CeLoss 0.1885 (0.3380)	SegCLSLoss 0.0327 (0.0297)	KLLoss 0.3887 (0.3576)	MaskLoss 1.1521 (1.1066)	MaskBCELoss 0.2573 (0.2869)	MaskDICELoss 0.8947 (0.8197)
Epoch: [0][236/500]	Time  6.077 ( 6.077)	Loss 2.5208 (2.0136)	CeLoss 0.2246 (0.5002)	SegCLSLoss 0.0354 (0.0201)	KLLoss 0.4102 (0.2855)	MaskLoss 1.1188 (0.7373)	MaskBCELoss 0.1539 (0.1009)	MaskDICELoss 0.9649 (0.6364)
Epoch: [0][237/500]	Time  6.825 ( 6.825)	Loss 2.4794 (2.4383)	CeLoss 0.2559 (0.3253)	SegCLSLoss 0.0175 (0.0291)	KLLoss 0.4531 (0.3660)	MaskLoss 1.0854 (1.0310)	MaskBCELoss 0.3842 (0.2263)	MaskDICELoss 0.7012 (0.8047)
Epoch: [0][238/500]	Time  6.583 ( 6.583)	Loss 2.4527 (1.8519)	CeLoss 0.2256 (0.5381)	SegCLSLoss 0.0223 (0.0164)	KLLoss 0.4258 (0.2422)	MaskLoss 1.0867 (0.6407)	MaskBCELoss 0.1987 (0.0874)	MaskDICELoss 0.8880 (0.5534)
Epoch: [0][239/500]	Time  5.270 ( 5.270)	Loss 0.6562 (1.9187)	CeLoss 0.6562 (0.5479)	SegCLSLoss 0.0000 (0.0170)	KLLoss 0.0000 (0.2520)	MaskLoss 0.0000 (0.6686)	MaskBCELoss 0.0000 (0.1695)	MaskDICELoss 0.0000 (0.4990)
[2025-03-02 15:49:53,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=0, lr=[0.00027691836734693875], mom=[(0.9, 0.95)]
[2025-03-02 15:49:53,921] [INFO] [timer.py:215:stop] epoch=0/micro_step=2400/global_step=240, RunningAvgSamplesPerSec=1.2834023418380751, CurrSamplesPerSec=1.461754556159805, MemAllocated=31.25GB, MaxMemAllocated=36.81GB
Epoch: [0][240/500]	Time  6.843 ( 6.843)	Loss 2.3279 (2.2933)	CeLoss 0.2305 (0.3502)	SegCLSLoss 0.0264 (0.0232)	KLLoss 0.3848 (0.3211)	MaskLoss 1.0223 (0.9496)	MaskBCELoss 0.1573 (0.2569)	MaskDICELoss 0.8651 (0.6927)
Epoch: [0][241/500]	Time  6.624 ( 6.624)	Loss 2.5251 (1.9289)	CeLoss 0.1787 (0.4664)	SegCLSLoss 0.0359 (0.0248)	KLLoss 0.3867 (0.2314)	MaskLoss 1.1454 (0.7136)	MaskBCELoss 0.2624 (0.1668)	MaskDICELoss 0.8830 (0.5467)
Epoch: [0][242/500]	Time  5.820 ( 5.820)	Loss 1.0000 (2.3703)	CeLoss 1.0000 (0.3107)	SegCLSLoss 0.0000 (0.0305)	KLLoss 0.0000 (0.3744)	MaskLoss 0.0000 (1.0034)	MaskBCELoss 0.0000 (0.3028)	MaskDICELoss 0.0000 (0.7007)
Epoch: [0][243/500]	Time  7.273 ( 7.273)	Loss 2.6819 (2.3189)	CeLoss 0.2852 (0.3406)	SegCLSLoss 0.0195 (0.0277)	KLLoss 0.4219 (0.3596)	MaskLoss 1.1720 (0.9642)	MaskBCELoss 0.3105 (0.1990)	MaskDICELoss 0.8615 (0.7652)
Epoch: [0][244/500]	Time  6.560 ( 6.560)	Loss 0.6875 (2.1190)	CeLoss 0.6875 (0.3631)	SegCLSLoss 0.0000 (0.0271)	KLLoss 0.0000 (0.3299)	MaskLoss 0.0000 (0.8546)	MaskBCELoss 0.0000 (0.1823)	MaskDICELoss 0.0000 (0.6723)
Epoch: [0][245/500]	Time  6.847 ( 6.847)	Loss 1.0469 (1.7064)	CeLoss 1.0469 (0.4267)	SegCLSLoss 0.0000 (0.0139)	KLLoss 0.0000 (0.2521)	MaskLoss 0.0000 (0.6236)	MaskBCELoss 0.0000 (0.1160)	MaskDICELoss 0.0000 (0.5077)
Epoch: [0][246/500]	Time  5.010 ( 5.010)	Loss 2.2981 (2.3047)	CeLoss 0.1807 (0.6102)	SegCLSLoss 0.0284 (0.0195)	KLLoss 0.3926 (0.3010)	MaskLoss 1.0318 (0.8274)	MaskBCELoss 0.0864 (0.2255)	MaskDICELoss 0.9455 (0.6019)
Epoch: [0][247/500]	Time  6.717 ( 6.717)	Loss 2.3569 (2.4916)	CeLoss 0.2139 (0.2613)	SegCLSLoss 0.0342 (0.0321)	KLLoss 0.3906 (0.4105)	MaskLoss 1.0437 (1.0864)	MaskBCELoss 0.1176 (0.2814)	MaskDICELoss 0.9260 (0.8050)
Epoch: [0][248/500]	Time  6.835 ( 6.835)	Loss 2.1830 (2.0750)	CeLoss 0.2012 (0.4275)	SegCLSLoss 0.0339 (0.0219)	KLLoss 0.3965 (0.2889)	MaskLoss 0.9626 (0.8038)	MaskBCELoss 0.0651 (0.1660)	MaskDICELoss 0.8975 (0.6378)
Epoch: [0][249/500]	Time  5.260 ( 5.260)	Loss 2.8059 (1.9342)	CeLoss 0.1309 (0.4808)	SegCLSLoss 0.0410 (0.0226)	KLLoss 0.3750 (0.2348)	MaskLoss 1.3082 (0.7092)	MaskBCELoss 0.3086 (0.1506)	MaskDICELoss 0.9996 (0.5586)
[2025-03-02 15:50:56,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=0, lr=[0.0002756938775510204], mom=[(0.9, 0.95)]
[2025-03-02 15:50:56,784] [INFO] [timer.py:215:stop] epoch=0/micro_step=2500/global_step=250, RunningAvgSamplesPerSec=1.293488754996432, CurrSamplesPerSec=1.6907071097707362, MemAllocated=30.8GB, MaxMemAllocated=36.81GB
Epoch: [0][250/500]	Time  5.916 ( 5.916)	Loss 2.9581 (1.8214)	CeLoss 0.2227 (0.3102)	SegCLSLoss 0.0408 (0.0230)	KLLoss 0.3828 (0.2943)	MaskLoss 1.3384 (0.7350)	MaskBCELoss 0.3938 (0.1717)	MaskDICELoss 0.9446 (0.5633)
Epoch: [0][251/500]	Time  6.234 ( 6.234)	Loss 1.8984 (2.1358)	CeLoss 1.8984 (0.5703)	SegCLSLoss 0.0000 (0.0204)	KLLoss 0.0000 (0.2939)	MaskLoss 0.0000 (0.7631)	MaskBCELoss 0.0000 (0.1731)	MaskDICELoss 0.0000 (0.5900)
Epoch: [0][252/500]	Time  5.758 ( 5.758)	Loss 2.7310 (2.0876)	CeLoss 0.2480 (0.4985)	SegCLSLoss 0.0408 (0.0233)	KLLoss 0.3828 (0.2900)	MaskLoss 1.2122 (0.7742)	MaskBCELoss 0.3875 (0.2032)	MaskDICELoss 0.8247 (0.5710)
Epoch: [0][253/500]	Time  6.424 ( 6.424)	Loss 2.7120 (2.3752)	CeLoss 0.2441 (0.2926)	SegCLSLoss 0.0325 (0.0263)	KLLoss 0.3945 (0.3848)	MaskLoss 1.2066 (1.0155)	MaskBCELoss 0.3422 (0.3364)	MaskDICELoss 0.8644 (0.6791)
Epoch: [0][254/500]	Time  6.368 ( 6.368)	Loss 2.3672 (2.0781)	CeLoss 0.2412 (0.4326)	SegCLSLoss 0.0304 (0.0273)	KLLoss 0.4062 (0.3418)	MaskLoss 1.0351 (0.7989)	MaskBCELoss 0.0532 (0.1855)	MaskDICELoss 0.9820 (0.6134)
Epoch: [0][255/500]	Time  6.540 ( 6.540)	Loss 3.5133 (2.3965)	CeLoss 0.2119 (0.4748)	SegCLSLoss 0.0327 (0.0276)	KLLoss 0.3945 (0.3299)	MaskLoss 1.6228 (0.9374)	MaskBCELoss 0.8458 (0.2970)	MaskDICELoss 0.7770 (0.6404)
Epoch: [0][256/500]	Time  6.897 ( 6.897)	Loss 2.4101 (2.1894)	CeLoss 0.2695 (0.2958)	SegCLSLoss 0.0264 (0.0266)	KLLoss 0.4434 (0.3852)	MaskLoss 1.0410 (0.9209)	MaskBCELoss 0.2099 (0.1943)	MaskDICELoss 0.8311 (0.7266)
Epoch: [0][257/500]	Time  5.808 ( 5.808)	Loss 2.5330 (2.5327)	CeLoss 0.2061 (0.2247)	SegCLSLoss 0.0708 (0.0406)	KLLoss 0.3672 (0.4033)	MaskLoss 1.1278 (1.1238)	MaskBCELoss 0.1751 (0.2470)	MaskDICELoss 0.9527 (0.8768)
Epoch: [0][258/500]	Time  6.012 ( 6.012)	Loss 2.5510 (1.9273)	CeLoss 0.2617 (0.4906)	SegCLSLoss 0.0211 (0.0214)	KLLoss 0.4375 (0.2992)	MaskLoss 1.1182 (0.6982)	MaskBCELoss 0.1457 (0.0934)	MaskDICELoss 0.9726 (0.6048)
Epoch: [0][259/500]	Time  8.555 ( 8.555)	Loss 2.2743 (2.3478)	CeLoss 0.1973 (0.2320)	SegCLSLoss 0.0383 (0.0360)	KLLoss 0.4062 (0.3611)	MaskLoss 1.0082 (1.0308)	MaskBCELoss 0.0240 (0.2523)	MaskDICELoss 0.9842 (0.7785)
[2025-03-02 15:52:01,816] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=0, lr=[0.000274469387755102], mom=[(0.9, 0.95)]
[2025-03-02 15:52:01,822] [INFO] [timer.py:215:stop] epoch=0/micro_step=2600/global_step=260, RunningAvgSamplesPerSec=1.3015054667794976, CurrSamplesPerSec=1.5530325316738838, MemAllocated=31.25GB, MaxMemAllocated=36.81GB
Epoch: [0][260/500]	Time  6.441 ( 6.441)	Loss 2.0905 (2.0700)	CeLoss 0.2012 (0.4441)	SegCLSLoss 0.0205 (0.0192)	KLLoss 0.4473 (0.3436)	MaskLoss 0.9173 (0.7911)	MaskBCELoss 0.1435 (0.1670)	MaskDICELoss 0.7738 (0.6240)
Epoch: [0][261/500]	Time  6.495 ( 6.495)	Loss 0.9844 (1.6399)	CeLoss 0.9844 (0.3872)	SegCLSLoss 0.0000 (0.0189)	KLLoss 0.0000 (0.2408)	MaskLoss 0.0000 (0.6097)	MaskBCELoss 0.0000 (0.0562)	MaskDICELoss 0.0000 (0.5535)
Epoch: [0][262/500]	Time  6.384 ( 6.384)	Loss 2.1283 (2.1673)	CeLoss 0.2412 (0.2852)	SegCLSLoss 0.0214 (0.0279)	KLLoss 0.4375 (0.3736)	MaskLoss 0.9167 (0.9155)	MaskBCELoss 0.1725 (0.1714)	MaskDICELoss 0.7442 (0.7441)
Epoch: [0][263/500]	Time  6.155 ( 6.155)	Loss 3.0627 (2.0613)	CeLoss 0.3164 (0.4002)	SegCLSLoss 0.0276 (0.0219)	KLLoss 0.3984 (0.3279)	MaskLoss 1.3468 (0.8087)	MaskBCELoss 0.6379 (0.2108)	MaskDICELoss 0.7089 (0.5979)
Epoch: [0][264/500]	Time  5.277 ( 5.277)	Loss 2.2684 (1.6907)	CeLoss 0.2324 (0.5231)	SegCLSLoss 0.0243 (0.0149)	KLLoss 0.4121 (0.2533)	MaskLoss 0.9916 (0.5675)	MaskBCELoss 0.0323 (0.1341)	MaskDICELoss 0.9594 (0.4334)
Epoch: [0][265/500]	Time  5.580 ( 5.580)	Loss 0.9258 (1.6618)	CeLoss 0.9258 (0.4348)	SegCLSLoss 0.0000 (0.0172)	KLLoss 0.0000 (0.2439)	MaskLoss 0.0000 (0.5971)	MaskBCELoss 0.0000 (0.1304)	MaskDICELoss 0.0000 (0.4667)
Epoch: [0][266/500]	Time  4.987 ( 4.987)	Loss 1.9653 (1.9164)	CeLoss 0.2344 (0.4452)	SegCLSLoss 0.0175 (0.0220)	KLLoss 0.4258 (0.2787)	MaskLoss 0.8401 (0.7163)	MaskBCELoss 0.1775 (0.1867)	MaskDICELoss 0.6626 (0.5296)
Epoch: [0][267/500]	Time  7.017 ( 7.017)	Loss 2.3341 (2.1632)	CeLoss 0.2676 (0.3554)	SegCLSLoss 0.0270 (0.0222)	KLLoss 0.3965 (0.3840)	MaskLoss 1.0069 (0.8792)	MaskBCELoss 0.0206 (0.1906)	MaskDICELoss 0.9863 (0.6886)
Epoch: [0][268/500]	Time  5.862 ( 5.862)	Loss 2.7026 (2.4154)	CeLoss 0.1699 (0.4519)	SegCLSLoss 0.0559 (0.0253)	KLLoss 0.3613 (0.3195)	MaskLoss 1.2341 (0.9593)	MaskBCELoss 0.4026 (0.2877)	MaskDICELoss 0.8315 (0.6716)
Epoch: [0][269/500]	Time  6.297 ( 6.297)	Loss 1.7344 (1.7173)	CeLoss 1.7344 (0.4284)	SegCLSLoss 0.0000 (0.0171)	KLLoss 0.0000 (0.2928)	MaskLoss 0.0000 (0.6256)	MaskBCELoss 0.0000 (0.0950)	MaskDICELoss 0.0000 (0.5306)
[2025-03-02 15:53:03,803] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=0, lr=[0.00027324489795918364], mom=[(0.9, 0.95)]
[2025-03-02 15:53:03,809] [INFO] [timer.py:215:stop] epoch=0/micro_step=2700/global_step=270, RunningAvgSamplesPerSec=1.3109659076097013, CurrSamplesPerSec=1.2605915905640408, MemAllocated=31.25GB, MaxMemAllocated=36.81GB
Epoch: [0][270/500]	Time  7.934 ( 7.934)	Loss 2.3771 (2.0470)	CeLoss 0.2773 (0.2135)	SegCLSLoss 0.0273 (0.0247)	KLLoss 0.3887 (0.3234)	MaskLoss 1.0235 (0.8943)	MaskBCELoss 0.0920 (0.2439)	MaskDICELoss 0.9315 (0.6503)
Epoch: [0][271/500]	Time  5.679 ( 5.679)	Loss 1.8037 (1.7202)	CeLoss 0.2578 (0.3075)	SegCLSLoss 0.0181 (0.0167)	KLLoss 0.4570 (0.3541)	MaskLoss 0.7456 (0.6842)	MaskBCELoss 0.1317 (0.1692)	MaskDICELoss 0.6139 (0.5150)
Epoch: [0][272/500]	Time  5.880 ( 5.880)	Loss 1.9846 (1.9401)	CeLoss 0.3262 (0.3452)	SegCLSLoss 0.0155 (0.0173)	KLLoss 0.4590 (0.3473)	MaskLoss 0.8029 (0.7758)	MaskBCELoss 0.2063 (0.1298)	MaskDICELoss 0.5965 (0.6461)
Epoch: [0][273/500]	Time  6.196 ( 6.196)	Loss 2.7462 (2.1373)	CeLoss 0.2109 (0.4494)	SegCLSLoss 0.0452 (0.0191)	KLLoss 0.3770 (0.3479)	MaskLoss 1.2374 (0.8218)	MaskBCELoss 0.3768 (0.1793)	MaskDICELoss 0.8605 (0.6425)
Epoch: [0][274/500]	Time  6.326 ( 6.326)	Loss 2.4104 (2.1146)	CeLoss 0.2617 (0.3290)	SegCLSLoss 0.0221 (0.0239)	KLLoss 0.4121 (0.3795)	MaskLoss 1.0480 (0.8679)	MaskBCELoss 0.0618 (0.1618)	MaskDICELoss 0.9862 (0.7061)
Epoch: [0][275/500]	Time  6.563 ( 6.563)	Loss 2.2352 (2.0442)	CeLoss 0.2969 (0.4871)	SegCLSLoss 0.0183 (0.0131)	KLLoss 0.4277 (0.3045)	MaskLoss 0.9428 (0.7601)	MaskBCELoss 0.1057 (0.2026)	MaskDICELoss 0.8371 (0.5575)
Epoch: [0][276/500]	Time  5.440 ( 5.440)	Loss 2.5361 (1.8847)	CeLoss 0.2256 (0.4601)	SegCLSLoss 0.0266 (0.0272)	KLLoss 0.4180 (0.2861)	MaskLoss 1.1274 (0.6912)	MaskBCELoss 0.1639 (0.1696)	MaskDICELoss 0.9635 (0.5217)
Epoch: [0][277/500]	Time  6.831 ( 6.831)	Loss 2.8389 (2.1442)	CeLoss 0.3281 (0.3557)	SegCLSLoss 0.0277 (0.0186)	KLLoss 0.3828 (0.2895)	MaskLoss 1.2290 (0.8751)	MaskBCELoss 0.4074 (0.3280)	MaskDICELoss 0.8216 (0.5471)
Epoch: [0][278/500]	Time  6.528 ( 6.528)	Loss 2.8103 (1.9827)	CeLoss 0.2031 (0.2850)	SegCLSLoss 0.0286 (0.0173)	KLLoss 0.3965 (0.3947)	MaskLoss 1.2763 (0.8249)	MaskBCELoss 0.3316 (0.2233)	MaskDICELoss 0.9447 (0.6016)
Epoch: [0][279/500]	Time  6.685 ( 6.685)	Loss 1.1641 (1.9452)	CeLoss 1.1641 (0.4633)	SegCLSLoss 0.0000 (0.0168)	KLLoss 0.0000 (0.2918)	MaskLoss 0.0000 (0.7222)	MaskBCELoss 0.0000 (0.0932)	MaskDICELoss 0.0000 (0.6290)
[2025-03-02 15:54:06,154] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=0, lr=[0.00027202040816326527], mom=[(0.9, 0.95)]
[2025-03-02 15:54:06,159] [INFO] [timer.py:215:stop] epoch=0/micro_step=2800/global_step=280, RunningAvgSamplesPerSec=1.3196427732148786, CurrSamplesPerSec=1.6075397578547577, MemAllocated=31.09GB, MaxMemAllocated=36.81GB
Epoch: [0][280/500]	Time  6.222 ( 6.222)	Loss 3.0069 (2.0037)	CeLoss 0.2637 (0.4225)	SegCLSLoss 0.0161 (0.0129)	KLLoss 0.4238 (0.3047)	MaskLoss 1.3462 (0.7723)	MaskBCELoss 0.5412 (0.2679)	MaskDICELoss 0.8050 (0.5044)
Epoch: [0][281/500]	Time  6.390 ( 6.390)	Loss 2.6151 (1.9363)	CeLoss 0.1855 (0.3960)	SegCLSLoss 0.0544 (0.0225)	KLLoss 0.3750 (0.2828)	MaskLoss 1.1826 (0.7503)	MaskBCELoss 0.3149 (0.1955)	MaskDICELoss 0.8677 (0.5548)
Epoch: [0][282/500]	Time  5.730 ( 5.730)	Loss 1.4314 (1.9986)	CeLoss 0.2695 (0.2228)	SegCLSLoss 0.0204 (0.0188)	KLLoss 0.4062 (0.3801)	MaskLoss 0.5555 (0.8641)	MaskBCELoss 0.3286 (0.1779)	MaskDICELoss 0.2269 (0.6862)
Epoch: [0][283/500]	Time  5.814 ( 5.814)	Loss 2.4404 (1.7072)	CeLoss 0.2148 (0.4676)	SegCLSLoss 0.0297 (0.0143)	KLLoss 0.3887 (0.2480)	MaskLoss 1.0864 (0.6038)	MaskBCELoss 0.2271 (0.1147)	MaskDICELoss 0.8593 (0.4891)
Epoch: [0][284/500]	Time  5.365 ( 5.365)	Loss 2.0361 (1.7121)	CeLoss 0.2012 (0.5339)	SegCLSLoss 0.0292 (0.0150)	KLLoss 0.4004 (0.2443)	MaskLoss 0.8901 (0.5731)	MaskBCELoss 0.0501 (0.0826)	MaskDICELoss 0.8401 (0.4905)
Epoch: [0][285/500]	Time  5.740 ( 5.740)	Loss 1.1641 (2.0472)	CeLoss 1.1641 (0.5524)	SegCLSLoss 0.0000 (0.0210)	KLLoss 0.0000 (0.2828)	MaskLoss 0.0000 (0.7280)	MaskBCELoss 0.0000 (0.1248)	MaskDICELoss 0.0000 (0.6032)
Epoch: [0][286/500]	Time  5.045 ( 5.045)	Loss 2.2597 (1.7308)	CeLoss 0.1904 (0.6831)	SegCLSLoss 0.0288 (0.0119)	KLLoss 0.4023 (0.2088)	MaskLoss 1.0073 (0.5104)	MaskBCELoss 0.0804 (0.1065)	MaskDICELoss 0.9269 (0.4039)
Epoch: [0][287/500]	Time  5.473 ( 5.473)	Loss 2.2889 (2.1817)	CeLoss 0.2041 (0.4124)	SegCLSLoss 0.0225 (0.0239)	KLLoss 0.4219 (0.3209)	MaskLoss 1.0155 (0.8627)	MaskBCELoss 0.0172 (0.2015)	MaskDICELoss 0.9984 (0.6612)
Epoch: [0][288/500]	Time  6.356 ( 6.356)	Loss 1.7296 (2.0050)	CeLoss 0.2275 (0.3296)	SegCLSLoss 0.0194 (0.0218)	KLLoss 0.4277 (0.3264)	MaskLoss 0.7252 (0.8160)	MaskBCELoss 0.1509 (0.1630)	MaskDICELoss 0.5742 (0.6530)
Epoch: [0][289/500]	Time  5.843 ( 5.843)	Loss 2.0271 (1.9213)	CeLoss 0.1641 (0.3697)	SegCLSLoss 0.0187 (0.0187)	KLLoss 0.4141 (0.2775)	MaskLoss 0.9061 (0.7573)	MaskBCELoss 0.3949 (0.2141)	MaskDICELoss 0.5112 (0.5432)
[2025-03-02 15:55:04,392] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=0, lr=[0.00027079591836734696], mom=[(0.9, 0.95)]
[2025-03-02 15:55:04,396] [INFO] [timer.py:215:stop] epoch=0/micro_step=2900/global_step=290, RunningAvgSamplesPerSec=1.3303431258086185, CurrSamplesPerSec=1.543298055302068, MemAllocated=31.1GB, MaxMemAllocated=36.81GB
Epoch: [0][290/500]	Time  6.482 ( 6.482)	Loss 2.4445 (2.2034)	CeLoss 0.2070 (0.3181)	SegCLSLoss 0.0157 (0.0246)	KLLoss 0.4434 (0.3658)	MaskLoss 1.0924 (0.9182)	MaskBCELoss 0.1774 (0.2472)	MaskDICELoss 0.9150 (0.6711)
Epoch: [0][291/500]	Time  5.827 ( 5.827)	Loss 2.7747 (2.0636)	CeLoss 0.2324 (0.4494)	SegCLSLoss 0.0190 (0.0167)	KLLoss 0.4160 (0.2898)	MaskLoss 1.2457 (0.7884)	MaskBCELoss 0.5176 (0.2697)	MaskDICELoss 0.7281 (0.5187)
Epoch: [0][292/500]	Time  5.435 ( 5.435)	Loss 2.3919 (1.8603)	CeLoss 0.2852 (0.7217)	SegCLSLoss 0.0192 (0.0126)	KLLoss 0.4160 (0.2480)	MaskLoss 1.0280 (0.5538)	MaskBCELoss 0.0834 (0.0757)	MaskDICELoss 0.9446 (0.4781)
Epoch: [0][293/500]	Time  6.254 ( 6.254)	Loss 0.3418 (1.8951)	CeLoss 0.3418 (0.3016)	SegCLSLoss 0.0000 (0.0145)	KLLoss 0.0000 (0.3414)	MaskLoss 0.0000 (0.7761)	MaskBCELoss 0.0000 (0.2023)	MaskDICELoss 0.0000 (0.5738)
Epoch: [0][294/500]	Time  5.616 ( 5.616)	Loss 1.8698 (1.5611)	CeLoss 0.2812 (0.6070)	SegCLSLoss 0.0300 (0.0096)	KLLoss 0.4121 (0.2143)	MaskLoss 0.7659 (0.4640)	MaskBCELoss 0.0277 (0.0978)	MaskDICELoss 0.7382 (0.3661)
Epoch: [0][295/500]	Time  5.919 ( 5.919)	Loss 0.3320 (1.6445)	CeLoss 0.3320 (0.4902)	SegCLSLoss 0.0000 (0.0101)	KLLoss 0.0000 (0.2051)	MaskLoss 0.0000 (0.5644)	MaskBCELoss 0.0000 (0.1421)	MaskDICELoss 0.0000 (0.4223)
Epoch: [0][296/500]	Time  6.450 ( 6.450)	Loss 0.7500 (1.7247)	CeLoss 0.7500 (0.2683)	SegCLSLoss 0.0000 (0.0182)	KLLoss 0.0000 (0.2807)	MaskLoss 0.0000 (0.7097)	MaskBCELoss 0.0000 (0.0832)	MaskDICELoss 0.0000 (0.6265)
Epoch: [0][297/500]	Time  6.467 ( 6.467)	Loss 2.1244 (2.3553)	CeLoss 0.2383 (0.3437)	SegCLSLoss 0.0178 (0.0251)	KLLoss 0.4336 (0.3574)	MaskLoss 0.9167 (0.9815)	MaskBCELoss 0.2849 (0.2388)	MaskDICELoss 0.6318 (0.7427)
Epoch: [0][298/500]	Time  5.635 ( 5.635)	Loss 2.4435 (2.0008)	CeLoss 0.1572 (0.4136)	SegCLSLoss 0.0640 (0.0273)	KLLoss 0.3691 (0.3197)	MaskLoss 1.1085 (0.7707)	MaskBCELoss 0.1883 (0.1431)	MaskDICELoss 0.9202 (0.6277)
Epoch: [0][299/500]	Time  6.246 ( 6.246)	Loss 2.3501 (2.1997)	CeLoss 0.1436 (0.4436)	SegCLSLoss 0.0347 (0.0196)	KLLoss 0.3867 (0.3254)	MaskLoss 1.0754 (0.8570)	MaskBCELoss 0.0859 (0.1930)	MaskDICELoss 0.9895 (0.6640)
[2025-03-02 15:56:03,966] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=0, lr=[0.00026957142857142853], mom=[(0.9, 0.95)]
[2025-03-02 15:56:03,971] [INFO] [timer.py:215:stop] epoch=0/micro_step=3000/global_step=300, RunningAvgSamplesPerSec=1.3396799455340662, CurrSamplesPerSec=1.747173056861779, MemAllocated=31.25GB, MaxMemAllocated=36.81GB
Epoch: [0][300/500]	Time  5.726 ( 5.726)	Loss 2.3022 (1.8290)	CeLoss 0.1553 (0.6349)	SegCLSLoss 0.0325 (0.0161)	KLLoss 0.3789 (0.2377)	MaskLoss 1.0461 (0.5811)	MaskBCELoss 0.1805 (0.0905)	MaskDICELoss 0.8656 (0.4906)
Epoch: [0][301/500]	Time  5.653 ( 5.653)	Loss 2.2258 (2.2076)	CeLoss 0.2988 (0.3380)	SegCLSLoss 0.0223 (0.0203)	KLLoss 0.4121 (0.3713)	MaskLoss 0.9371 (0.9110)	MaskBCELoss 0.2484 (0.2818)	MaskDICELoss 0.6888 (0.6292)
Epoch: [0][302/500]	Time  6.879 ( 6.879)	Loss 2.0080 (2.0853)	CeLoss 0.2119 (0.2254)	SegCLSLoss 0.0386 (0.0270)	KLLoss 0.3984 (0.4031)	MaskLoss 0.8683 (0.9030)	MaskBCELoss 0.4008 (0.1859)	MaskDICELoss 0.4675 (0.7171)
Epoch: [0][303/500]	Time  7.280 ( 7.280)	Loss 2.3134 (2.6216)	CeLoss 0.1904 (0.2246)	SegCLSLoss 0.0283 (0.0307)	KLLoss 0.3789 (0.3900)	MaskLoss 1.0351 (1.1713)	MaskBCELoss 0.2540 (0.3283)	MaskDICELoss 0.7811 (0.8430)
Epoch: [0][304/500]	Time  4.938 ( 4.938)	Loss 1.4453 (1.7750)	CeLoss 1.4453 (0.6145)	SegCLSLoss 0.0000 (0.0198)	KLLoss 0.0000 (0.1910)	MaskLoss 0.0000 (0.5657)	MaskBCELoss 0.0000 (0.1939)	MaskDICELoss 0.0000 (0.3718)
Epoch: [0][305/500]	Time  6.426 ( 6.426)	Loss 1.5198 (1.7213)	CeLoss 0.2344 (0.3533)	SegCLSLoss 0.0165 (0.0177)	KLLoss 0.4531 (0.2814)	MaskLoss 0.6164 (0.6655)	MaskBCELoss 0.2184 (0.1347)	MaskDICELoss 0.3980 (0.5308)
Epoch: [0][306/500]	Time  6.278 ( 6.278)	Loss 1.6803 (1.9764)	CeLoss 0.2461 (0.4179)	SegCLSLoss 0.0168 (0.0207)	KLLoss 0.4121 (0.3133)	MaskLoss 0.6927 (0.7586)	MaskBCELoss 0.1014 (0.1208)	MaskDICELoss 0.5913 (0.6378)
Epoch: [0][307/500]	Time  6.628 ( 6.628)	Loss 2.0610 (2.3918)	CeLoss 0.2305 (0.2963)	SegCLSLoss 0.0272 (0.0283)	KLLoss 0.3906 (0.3451)	MaskLoss 0.8889 (1.0235)	MaskBCELoss 0.1000 (0.2436)	MaskDICELoss 0.7889 (0.7798)
Epoch: [0][308/500]	Time  6.159 ( 6.159)	Loss 3.1452 (1.9997)	CeLoss 0.3809 (0.4523)	SegCLSLoss 0.0190 (0.0194)	KLLoss 0.3926 (0.3139)	MaskLoss 1.3578 (0.7532)	MaskBCELoss 0.5763 (0.2004)	MaskDICELoss 0.7814 (0.5527)
Epoch: [0][309/500]	Time  6.327 ( 6.327)	Loss 2.4136 (2.1240)	CeLoss 0.2188 (0.2217)	SegCLSLoss 0.0249 (0.0320)	KLLoss 0.3789 (0.3475)	MaskLoss 1.0720 (0.9259)	MaskBCELoss 0.1479 (0.1791)	MaskDICELoss 0.9242 (0.7468)
[2025-03-02 15:57:07,700] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=0, lr=[0.00026834693877551016], mom=[(0.9, 0.95)]
[2025-03-02 15:57:07,704] [INFO] [timer.py:215:stop] epoch=0/micro_step=3100/global_step=310, RunningAvgSamplesPerSec=1.3460783289769949, CurrSamplesPerSec=1.3960442156467707, MemAllocated=31.24GB, MaxMemAllocated=36.81GB
Epoch: [0][310/500]	Time  7.165 ( 7.165)	Loss 1.5458 (2.3296)	CeLoss 0.2119 (0.2653)	SegCLSLoss 0.0173 (0.0232)	KLLoss 0.4219 (0.4014)	MaskLoss 0.6411 (1.0059)	MaskBCELoss 0.1337 (0.2780)	MaskDICELoss 0.5073 (0.7279)
Epoch: [0][311/500]	Time  5.747 ( 5.747)	Loss 2.0618 (1.7661)	CeLoss 0.3496 (0.4634)	SegCLSLoss 0.0198 (0.0148)	KLLoss 0.3965 (0.2836)	MaskLoss 0.8317 (0.6336)	MaskBCELoss 0.2475 (0.1246)	MaskDICELoss 0.5842 (0.5090)
Epoch: [0][312/500]	Time  5.608 ( 5.608)	Loss 0.9570 (1.6223)	CeLoss 0.9570 (0.5827)	SegCLSLoss 0.0000 (0.0145)	KLLoss 0.0000 (0.2453)	MaskLoss 0.0000 (0.5040)	MaskBCELoss 0.0000 (0.1091)	MaskDICELoss 0.0000 (0.3948)
Epoch: [0][313/500]	Time  7.732 ( 7.732)	Loss 1.8160 (1.5848)	CeLoss 0.2754 (0.4118)	SegCLSLoss 0.0143 (0.0145)	KLLoss 0.4375 (0.1988)	MaskLoss 0.7449 (0.5728)	MaskBCELoss 0.1700 (0.1881)	MaskDICELoss 0.5749 (0.3847)
Epoch: [0][314/500]	Time  5.963 ( 5.963)	Loss 2.2197 (2.1719)	CeLoss 0.2139 (0.4824)	SegCLSLoss 0.0277 (0.0168)	KLLoss 0.3887 (0.3250)	MaskLoss 0.9770 (0.8243)	MaskBCELoss 0.0694 (0.1893)	MaskDICELoss 0.9077 (0.6350)
Epoch: [0][315/500]	Time  6.215 ( 6.215)	Loss 2.9476 (2.2061)	CeLoss 0.2061 (0.4131)	SegCLSLoss 0.0131 (0.0278)	KLLoss 0.4316 (0.2705)	MaskLoss 1.3459 (0.8760)	MaskBCELoss 0.5239 (0.2767)	MaskDICELoss 0.8220 (0.5993)
Epoch: [0][316/500]	Time  4.916 ( 4.916)	Loss 2.4155 (1.7717)	CeLoss 0.2080 (0.5770)	SegCLSLoss 0.0297 (0.0157)	KLLoss 0.4121 (0.2850)	MaskLoss 1.0759 (0.5793)	MaskBCELoss 0.5468 (0.1570)	MaskDICELoss 0.5291 (0.4223)
Epoch: [0][317/500]	Time  6.003 ( 6.003)	Loss 2.6028 (2.2902)	CeLoss 0.2559 (0.3229)	SegCLSLoss 0.0310 (0.0223)	KLLoss 0.3809 (0.3635)	MaskLoss 1.1471 (0.9600)	MaskBCELoss 0.2048 (0.2381)	MaskDICELoss 0.9423 (0.7220)
Epoch: [0][318/500]	Time  6.234 ( 6.234)	Loss 2.4847 (2.2862)	CeLoss 0.2871 (0.2538)	SegCLSLoss 0.0189 (0.0225)	KLLoss 0.4121 (0.4146)	MaskLoss 1.0734 (0.9897)	MaskBCELoss 0.0929 (0.2756)	MaskDICELoss 0.9805 (0.7141)
Epoch: [0][319/500]	Time  5.568 ( 5.568)	Loss 1.1719 (1.7723)	CeLoss 1.1719 (0.5436)	SegCLSLoss 0.0000 (0.0151)	KLLoss 0.0000 (0.2428)	MaskLoss 0.0000 (0.5984)	MaskBCELoss 0.0000 (0.0925)	MaskDICELoss 0.0000 (0.5059)
[2025-03-02 15:58:07,369] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=0, lr=[0.0002671224489795918], mom=[(0.9, 0.95)]
[2025-03-02 15:58:07,375] [INFO] [timer.py:215:stop] epoch=0/micro_step=3200/global_step=320, RunningAvgSamplesPerSec=1.3544677162408072, CurrSamplesPerSec=1.759413786017181, MemAllocated=30.69GB, MaxMemAllocated=36.81GB
Epoch: [0][320/500]	Time  5.685 ( 5.685)	Loss 1.4453 (2.0151)	CeLoss 1.4453 (0.5122)	SegCLSLoss 0.0000 (0.0149)	KLLoss 0.0000 (0.2906)	MaskLoss 0.0000 (0.7332)	MaskBCELoss 0.0000 (0.1380)	MaskDICELoss 0.0000 (0.5952)
Epoch: [0][321/500]	Time  6.641 ( 6.641)	Loss 1.3777 (2.0946)	CeLoss 0.2451 (0.2979)	SegCLSLoss 0.0157 (0.0239)	KLLoss 0.4414 (0.3703)	MaskLoss 0.5395 (0.8737)	MaskBCELoss 0.0960 (0.1685)	MaskDICELoss 0.4435 (0.7052)
Epoch: [0][322/500]	Time  6.772 ( 6.772)	Loss 1.4702 (1.9608)	CeLoss 0.1992 (0.2231)	SegCLSLoss 0.0137 (0.0227)	KLLoss 0.4531 (0.4225)	MaskLoss 0.6091 (0.8422)	MaskBCELoss 0.2641 (0.1188)	MaskDICELoss 0.3450 (0.7233)
Epoch: [0][323/500]	Time  5.546 ( 5.546)	Loss 1.2891 (1.6636)	CeLoss 1.2891 (0.6660)	SegCLSLoss 0.0000 (0.0135)	KLLoss 0.0000 (0.2574)	MaskLoss 0.0000 (0.4825)	MaskBCELoss 0.0000 (0.0845)	MaskDICELoss 0.0000 (0.3980)
Epoch: [0][324/500]	Time  6.760 ( 6.760)	Loss 2.5683 (2.1283)	CeLoss 0.3027 (0.3374)	SegCLSLoss 0.0153 (0.0219)	KLLoss 0.4316 (0.3771)	MaskLoss 1.1074 (0.8710)	MaskBCELoss 0.2216 (0.1993)	MaskDICELoss 0.8858 (0.6717)
Epoch: [0][325/500]	Time  6.283 ( 6.283)	Loss 2.8304 (2.2343)	CeLoss 0.3125 (0.3191)	SegCLSLoss 0.0349 (0.0237)	KLLoss 0.3867 (0.3740)	MaskLoss 1.2306 (0.9328)	MaskBCELoss 0.2826 (0.1958)	MaskDICELoss 0.9480 (0.7369)
Epoch: [0][326/500]	Time  6.191 ( 6.191)	Loss 2.4081 (1.8948)	CeLoss 0.2949 (0.4552)	SegCLSLoss 0.0221 (0.0194)	KLLoss 0.4121 (0.2906)	MaskLoss 1.0302 (0.7004)	MaskBCELoss 0.0346 (0.1351)	MaskDICELoss 0.9957 (0.5653)
Epoch: [0][327/500]	Time  5.418 ( 5.418)	Loss 1.0234 (1.9921)	CeLoss 1.0234 (0.4145)	SegCLSLoss 0.0000 (0.0226)	KLLoss 0.0000 (0.3246)	MaskLoss 0.0000 (0.7669)	MaskBCELoss 0.0000 (0.1883)	MaskDICELoss 0.0000 (0.5787)
Epoch: [0][328/500]	Time  5.992 ( 5.992)	Loss 2.7706 (1.7931)	CeLoss 0.1523 (0.3674)	SegCLSLoss 0.0698 (0.0315)	KLLoss 0.3672 (0.2809)	MaskLoss 1.2730 (0.6908)	MaskBCELoss 0.4886 (0.1587)	MaskDICELoss 0.7844 (0.5321)
Epoch: [0][329/500]	Time  5.897 ( 5.897)	Loss 1.9990 (2.0525)	CeLoss 0.2539 (0.4726)	SegCLSLoss 0.0182 (0.0151)	KLLoss 0.4160 (0.2852)	MaskLoss 0.8471 (0.7720)	MaskBCELoss 0.2433 (0.1712)	MaskDICELoss 0.6038 (0.6008)
[2025-03-02 15:59:09,007] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=0, lr=[0.0002658979591836734], mom=[(0.9, 0.95)]
[2025-03-02 15:59:09,012] [INFO] [timer.py:215:stop] epoch=0/micro_step=3300/global_step=330, RunningAvgSamplesPerSec=1.3613290736586594, CurrSamplesPerSec=1.6298777765757297, MemAllocated=30.92GB, MaxMemAllocated=36.81GB
Epoch: [0][330/500]	Time  6.137 ( 6.137)	Loss 0.8359 (1.8967)	CeLoss 0.8359 (0.5515)	SegCLSLoss 0.0000 (0.0141)	KLLoss 0.0000 (0.2910)	MaskLoss 0.0000 (0.6547)	MaskBCELoss 0.0000 (0.1466)	MaskDICELoss 0.0000 (0.5081)
Epoch: [0][331/500]	Time  5.774 ( 5.774)	Loss 1.5156 (1.9770)	CeLoss 1.5156 (0.5158)	SegCLSLoss 0.0000 (0.0213)	KLLoss 0.0000 (0.2838)	MaskLoss 0.0000 (0.7111)	MaskBCELoss 0.0000 (0.1088)	MaskDICELoss 0.0000 (0.6023)
Epoch: [0][332/500]	Time  6.140 ( 6.140)	Loss 1.1797 (2.1219)	CeLoss 1.1797 (0.5165)	SegCLSLoss 0.0000 (0.0196)	KLLoss 0.0000 (0.2756)	MaskLoss 0.0000 (0.7841)	MaskBCELoss 0.0000 (0.1745)	MaskDICELoss 0.0000 (0.6096)
Epoch: [0][333/500]	Time  5.816 ( 5.816)	Loss 2.5893 (1.8472)	CeLoss 0.1338 (0.4167)	SegCLSLoss 0.0532 (0.0206)	KLLoss 0.3691 (0.2832)	MaskLoss 1.1960 (0.6959)	MaskBCELoss 0.2888 (0.1260)	MaskDICELoss 0.9073 (0.5699)
Epoch: [0][334/500]	Time  5.299 ( 5.299)	Loss 1.0312 (1.5592)	CeLoss 1.0312 (0.6399)	SegCLSLoss 0.0000 (0.0116)	KLLoss 0.0000 (0.2051)	MaskLoss 0.0000 (0.4465)	MaskBCELoss 0.0000 (0.0612)	MaskDICELoss 0.0000 (0.3853)
Epoch: [0][335/500]	Time  6.056 ( 6.056)	Loss 0.7852 (1.9902)	CeLoss 0.7852 (0.6533)	SegCLSLoss 0.0000 (0.0169)	KLLoss 0.0000 (0.2365)	MaskLoss 0.0000 (0.6525)	MaskBCELoss 0.0000 (0.1848)	MaskDICELoss 0.0000 (0.4677)
Epoch: [0][336/500]	Time  5.983 ( 5.983)	Loss 1.4766 (1.8274)	CeLoss 1.4766 (0.6258)	SegCLSLoss 0.0000 (0.0152)	KLLoss 0.0000 (0.2371)	MaskLoss 0.0000 (0.5853)	MaskBCELoss 0.0000 (0.0509)	MaskDICELoss 0.0000 (0.5344)
Epoch: [0][337/500]	Time  6.034 ( 6.034)	Loss 2.6566 (2.0958)	CeLoss 0.1299 (0.3971)	SegCLSLoss 0.0869 (0.0292)	KLLoss 0.3633 (0.3180)	MaskLoss 1.2233 (0.8262)	MaskBCELoss 0.3088 (0.1484)	MaskDICELoss 0.9145 (0.6779)
Epoch: [0][338/500]	Time  6.313 ( 6.313)	Loss 2.3382 (2.1442)	CeLoss 0.2275 (0.4573)	SegCLSLoss 0.0275 (0.0204)	KLLoss 0.4062 (0.2711)	MaskLoss 1.0285 (0.8249)	MaskBCELoss 0.0351 (0.1712)	MaskDICELoss 0.9934 (0.6538)
Epoch: [0][339/500]	Time  5.952 ( 5.952)	Loss 1.5077 (1.7682)	CeLoss 0.3242 (0.4896)	SegCLSLoss 0.0157 (0.0158)	KLLoss 0.4375 (0.2869)	MaskLoss 0.5664 (0.6211)	MaskBCELoss 0.1076 (0.0869)	MaskDICELoss 0.4588 (0.5342)
[2025-03-02 16:00:08,986] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=0, lr=[0.0002646734693877551], mom=[(0.9, 0.95)]
[2025-03-02 16:00:08,992] [INFO] [timer.py:215:stop] epoch=0/micro_step=3400/global_step=340, RunningAvgSamplesPerSec=1.3687662584457096, CurrSamplesPerSec=1.5125878469814897, MemAllocated=31.24GB, MaxMemAllocated=36.81GB
Epoch: [0][340/500]	Time  6.613 ( 6.613)	Loss 3.0481 (2.2334)	CeLoss 0.2812 (0.3181)	SegCLSLoss 0.0155 (0.0199)	KLLoss 0.4102 (0.3621)	MaskLoss 1.3590 (0.9347)	MaskBCELoss 0.5013 (0.2182)	MaskDICELoss 0.8577 (0.7165)
Epoch: [0][341/500]	Time  5.256 ( 5.256)	Loss 2.5384 (1.8915)	CeLoss 0.1602 (0.6530)	SegCLSLoss 0.0552 (0.0194)	KLLoss 0.3672 (0.2385)	MaskLoss 1.1569 (0.6022)	MaskBCELoss 0.2735 (0.0947)	MaskDICELoss 0.8834 (0.5074)
Epoch: [0][342/500]	Time  6.179 ( 6.179)	Loss 1.5397 (1.9870)	CeLoss 0.1689 (0.3221)	SegCLSLoss 0.0376 (0.0252)	KLLoss 0.3770 (0.3650)	MaskLoss 0.6575 (0.8081)	MaskBCELoss 0.0506 (0.1818)	MaskDICELoss 0.6069 (0.6262)
Epoch: [0][343/500]	Time  6.013 ( 6.013)	Loss 0.1436 (1.7635)	CeLoss 0.1436 (0.3949)	SegCLSLoss 0.0000 (0.0154)	KLLoss 0.0000 (0.2859)	MaskLoss 0.0000 (0.6661)	MaskBCELoss 0.0000 (0.1447)	MaskDICELoss 0.0000 (0.5214)
Epoch: [0][344/500]	Time  5.957 ( 5.957)	Loss 1.2751 (1.7879)	CeLoss 0.2158 (0.3408)	SegCLSLoss 0.0170 (0.0256)	KLLoss 0.4277 (0.3242)	MaskLoss 0.5038 (0.7009)	MaskBCELoss 0.1571 (0.1828)	MaskDICELoss 0.3466 (0.5181)
Epoch: [0][345/500]	Time  5.523 ( 5.523)	Loss 2.6459 (1.8753)	CeLoss 0.2441 (0.5000)	SegCLSLoss 0.0204 (0.0152)	KLLoss 0.4082 (0.2912)	MaskLoss 1.1755 (0.6693)	MaskBCELoss 0.4384 (0.2117)	MaskDICELoss 0.7371 (0.4576)
Epoch: [0][346/500]	Time  5.757 ( 5.757)	Loss 2.0261 (2.0192)	CeLoss 0.2129 (0.5393)	SegCLSLoss 0.0276 (0.0199)	KLLoss 0.3848 (0.2777)	MaskLoss 0.8802 (0.7211)	MaskBCELoss 0.0262 (0.1231)	MaskDICELoss 0.8540 (0.5980)
Epoch: [0][347/500]	Time  6.595 ( 6.595)	Loss 2.4139 (1.9632)	CeLoss 0.1689 (0.4470)	SegCLSLoss 0.0413 (0.0190)	KLLoss 0.3691 (0.3250)	MaskLoss 1.0937 (0.7370)	MaskBCELoss 0.1414 (0.1138)	MaskDICELoss 0.9523 (0.6232)
Epoch: [0][348/500]	Time  6.045 ( 6.045)	Loss 2.1097 (1.9481)	CeLoss 0.2734 (0.2386)	SegCLSLoss 0.0178 (0.0204)	KLLoss 0.4160 (0.3697)	MaskLoss 0.8927 (0.8314)	MaskBCELoss 0.0129 (0.2132)	MaskDICELoss 0.8798 (0.6182)
Epoch: [0][349/500]	Time  6.563 ( 6.563)	Loss 2.5489 (2.1307)	CeLoss 0.2852 (0.3542)	SegCLSLoss 0.0170 (0.0216)	KLLoss 0.4062 (0.3639)	MaskLoss 1.1075 (0.8645)	MaskBCELoss 0.1893 (0.1626)	MaskDICELoss 0.9182 (0.7019)
[2025-03-02 16:01:09,129] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=0, lr=[0.0002634489795918367], mom=[(0.9, 0.95)]
[2025-03-02 16:01:09,135] [INFO] [timer.py:215:stop] epoch=0/micro_step=3500/global_step=350, RunningAvgSamplesPerSec=1.3757626988251066, CurrSamplesPerSec=1.5991642551309473, MemAllocated=31.25GB, MaxMemAllocated=36.81GB
Epoch: [0][350/500]	Time  6.255 ( 6.255)	Loss 2.7065 (2.0107)	CeLoss 0.2559 (0.3124)	SegCLSLoss 0.0284 (0.0252)	KLLoss 0.3730 (0.3084)	MaskLoss 1.2000 (0.8275)	MaskBCELoss 0.3057 (0.1342)	MaskDICELoss 0.8942 (0.6933)
Epoch: [0][351/500]	Time  4.670 ( 4.670)	Loss 6.0339 (3.6753)	CeLoss 0.2266 (0.5187)	SegCLSLoss 0.0260 (0.0213)	KLLoss 0.4082 (0.3266)	MaskLoss 2.8773 (1.5567)	MaskBCELoss 2.0008 (0.9502)	MaskDICELoss 0.8765 (0.6065)
Epoch: [0][352/500]	Time  6.198 ( 6.198)	Loss 2.6903 (2.1130)	CeLoss 0.2461 (0.3635)	SegCLSLoss 0.0398 (0.0233)	KLLoss 0.3730 (0.3154)	MaskLoss 1.1938 (0.8531)	MaskBCELoss 0.2521 (0.1346)	MaskDICELoss 0.9417 (0.7185)
Epoch: [0][353/500]	Time  4.998 ( 4.998)	Loss 2.3771 (1.9101)	CeLoss 0.3594 (0.5785)	SegCLSLoss 0.0208 (0.0150)	KLLoss 0.4004 (0.2916)	MaskLoss 0.9835 (0.6474)	MaskBCELoss 0.0314 (0.1644)	MaskDICELoss 0.9521 (0.4830)
Epoch: [0][354/500]	Time  6.497 ( 6.497)	Loss 0.0569 (1.7159)	CeLoss 0.0569 (0.2690)	SegCLSLoss 0.0000 (0.0157)	KLLoss 0.0000 (0.2918)	MaskLoss 0.0000 (0.7049)	MaskBCELoss 0.0000 (0.1338)	MaskDICELoss 0.0000 (0.5711)
Epoch: [0][355/500]	Time  4.939 ( 4.939)	Loss 2.3506 (1.5043)	CeLoss 0.1914 (0.5395)	SegCLSLoss 0.0250 (0.0108)	KLLoss 0.3789 (0.2025)	MaskLoss 1.0542 (0.4694)	MaskBCELoss 0.0592 (0.0815)	MaskDICELoss 0.9950 (0.3879)
Epoch: [0][356/500]	Time  5.569 ( 5.569)	Loss 2.2889 (1.6664)	CeLoss 0.2490 (0.5997)	SegCLSLoss 0.0306 (0.0107)	KLLoss 0.4004 (0.2053)	MaskLoss 0.9921 (0.5204)	MaskBCELoss 0.0348 (0.0975)	MaskDICELoss 0.9573 (0.4230)
Epoch: [0][357/500]	Time  6.339 ( 6.339)	Loss 1.3203 (1.7454)	CeLoss 1.3203 (0.5370)	SegCLSLoss 0.0000 (0.0169)	KLLoss 0.0000 (0.2428)	MaskLoss 0.0000 (0.5877)	MaskBCELoss 0.0000 (0.0864)	MaskDICELoss 0.0000 (0.5013)
Epoch: [0][358/500]	Time  5.776 ( 5.776)	Loss 3.1295 (2.2049)	CeLoss 0.1758 (0.4073)	SegCLSLoss 0.0471 (0.0202)	KLLoss 0.3926 (0.3217)	MaskLoss 1.4456 (0.8776)	MaskBCELoss 0.6111 (0.2099)	MaskDICELoss 0.8345 (0.6678)
Epoch: [0][359/500]	Time  5.815 ( 5.815)	Loss 2.7918 (2.3100)	CeLoss 0.3027 (0.4973)	SegCLSLoss 0.0264 (0.0194)	KLLoss 0.3848 (0.3133)	MaskLoss 1.2182 (0.8859)	MaskBCELoss 0.4239 (0.1915)	MaskDICELoss 0.7943 (0.6943)
[2025-03-02 16:02:04,956] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=0, lr=[0.00026222448979591837], mom=[(0.9, 0.95)]
[2025-03-02 16:02:04,962] [INFO] [timer.py:215:stop] epoch=0/micro_step=3600/global_step=360, RunningAvgSamplesPerSec=1.384741339861038, CurrSamplesPerSec=1.9904122611897883, MemAllocated=30.94GB, MaxMemAllocated=36.81GB
Epoch: [0][360/500]	Time  5.026 ( 5.026)	Loss 2.0475 (1.8065)	CeLoss 0.2773 (0.5005)	SegCLSLoss 0.0156 (0.0136)	KLLoss 0.4277 (0.2418)	MaskLoss 0.8597 (0.6375)	MaskBCELoss 0.3041 (0.2020)	MaskDICELoss 0.5556 (0.4355)
Epoch: [0][361/500]	Time  6.650 ( 6.650)	Loss 1.4141 (1.8207)	CeLoss 1.4141 (0.4322)	SegCLSLoss 0.0000 (0.0145)	KLLoss 0.0000 (0.2852)	MaskLoss 0.0000 (0.6764)	MaskBCELoss 0.0000 (0.1115)	MaskDICELoss 0.0000 (0.5649)
Epoch: [0][362/500]	Time  5.942 ( 5.942)	Loss 2.1574 (2.3080)	CeLoss 0.2578 (0.3580)	SegCLSLoss 0.0182 (0.0241)	KLLoss 0.4102 (0.3553)	MaskLoss 0.9244 (0.9512)	MaskBCELoss 0.1846 (0.2694)	MaskDICELoss 0.7398 (0.6819)
Epoch: [0][363/500]	Time  6.297 ( 6.297)	Loss 2.1027 (1.9883)	CeLoss 0.2656 (0.5576)	SegCLSLoss 0.0244 (0.0173)	KLLoss 0.4023 (0.2842)	MaskLoss 0.8922 (0.6966)	MaskBCELoss 0.0448 (0.1403)	MaskDICELoss 0.8474 (0.5562)
Epoch: [0][364/500]	Time  5.713 ( 5.713)	Loss 3.0589 (2.1283)	CeLoss 0.1514 (0.3639)	SegCLSLoss 0.0513 (0.0243)	KLLoss 0.3750 (0.3131)	MaskLoss 1.4220 (0.8605)	MaskBCELoss 0.5903 (0.2008)	MaskDICELoss 0.8317 (0.6597)
Epoch: [0][365/500]	Time  7.018 ( 7.018)	Loss 2.1443 (1.9585)	CeLoss 0.2373 (0.3411)	SegCLSLoss 0.0219 (0.0177)	KLLoss 0.4004 (0.3193)	MaskLoss 0.9276 (0.7883)	MaskBCELoss 0.0824 (0.1390)	MaskDICELoss 0.8452 (0.6494)
Epoch: [0][366/500]	Time  6.155 ( 6.155)	Loss 2.1196 (2.0310)	CeLoss 0.3262 (0.3930)	SegCLSLoss 0.0166 (0.0193)	KLLoss 0.4258 (0.3242)	MaskLoss 0.8713 (0.7980)	MaskBCELoss 0.1451 (0.1524)	MaskDICELoss 0.7262 (0.6457)
Epoch: [0][367/500]	Time  5.709 ( 5.709)	Loss 0.1045 (2.0233)	CeLoss 0.1045 (0.4412)	SegCLSLoss 0.0000 (0.0217)	KLLoss 0.0000 (0.2787)	MaskLoss 0.0000 (0.7716)	MaskBCELoss 0.0000 (0.1559)	MaskDICELoss 0.0000 (0.6157)
Epoch: [0][368/500]	Time  6.945 ( 6.945)	Loss 2.6570 (2.0727)	CeLoss 0.1953 (0.3137)	SegCLSLoss 0.0325 (0.0230)	KLLoss 0.3867 (0.3145)	MaskLoss 1.2035 (0.8580)	MaskBCELoss 0.2915 (0.1541)	MaskDICELoss 0.9120 (0.7039)
Epoch: [0][369/500]	Time  6.545 ( 6.545)	Loss 0.0571 (1.5803)	CeLoss 0.0571 (0.2529)	SegCLSLoss 0.0000 (0.0172)	KLLoss 0.0000 (0.2914)	MaskLoss 0.0000 (0.6449)	MaskBCELoss 0.0000 (0.1371)	MaskDICELoss 0.0000 (0.5078)
[2025-03-02 16:03:08,908] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=0, lr=[0.000261], mom=[(0.9, 0.95)]
[2025-03-02 16:03:08,914] [INFO] [timer.py:215:stop] epoch=0/micro_step=3700/global_step=370, RunningAvgSamplesPerSec=1.389067674769343, CurrSamplesPerSec=1.4335887720879374, MemAllocated=31.26GB, MaxMemAllocated=36.81GB
Epoch: [0][370/500]	Time  6.977 ( 6.977)	Loss 2.3990 (2.2913)	CeLoss 0.2656 (0.2491)	SegCLSLoss 0.0337 (0.0253)	KLLoss 0.3789 (0.4090)	MaskLoss 1.0393 (0.9944)	MaskBCELoss 0.0405 (0.2042)	MaskDICELoss 0.9989 (0.7902)
Epoch: [0][371/500]	Time  5.650 ( 5.650)	Loss 2.3254 (1.6955)	CeLoss 0.2461 (0.3261)	SegCLSLoss 0.0229 (0.0170)	KLLoss 0.4023 (0.3357)	MaskLoss 1.0133 (0.6635)	MaskBCELoss 0.0359 (0.1279)	MaskDICELoss 0.9774 (0.5356)
Epoch: [0][372/500]	Time  6.072 ( 6.072)	Loss 2.2745 (1.3811)	CeLoss 0.2109 (0.5236)	SegCLSLoss 0.0359 (0.0127)	KLLoss 0.3711 (0.1551)	MaskLoss 1.0044 (0.4179)	MaskBCELoss 0.0192 (0.0734)	MaskDICELoss 0.9852 (0.3444)
Epoch: [0][373/500]	Time  6.184 ( 6.184)	Loss 1.9999 (1.7390)	CeLoss 0.2598 (0.4885)	SegCLSLoss 0.0197 (0.0166)	KLLoss 0.4160 (0.2385)	MaskLoss 0.8447 (0.6093)	MaskBCELoss 0.0430 (0.1063)	MaskDICELoss 0.8017 (0.5030)
Epoch: [0][374/500]	Time  6.425 ( 6.425)	Loss 2.3918 (1.5452)	CeLoss 0.2383 (0.3884)	SegCLSLoss 0.0200 (0.0176)	KLLoss 0.4277 (0.2477)	MaskLoss 1.0504 (0.5617)	MaskBCELoss 0.4309 (0.1600)	MaskDICELoss 0.6195 (0.4017)
Epoch: [0][375/500]	Time  5.180 ( 5.180)	Loss 2.6270 (1.9518)	CeLoss 0.2080 (0.4003)	SegCLSLoss 0.0171 (0.0231)	KLLoss 0.4160 (0.3260)	MaskLoss 1.1846 (0.7537)	MaskBCELoss 0.1974 (0.2157)	MaskDICELoss 0.9872 (0.5380)
Epoch: [0][376/500]	Time  6.911 ( 6.911)	Loss 2.2257 (2.1206)	CeLoss 0.1494 (0.2122)	SegCLSLoss 0.0317 (0.0250)	KLLoss 0.3926 (0.3563)	MaskLoss 1.0108 (0.9302)	MaskBCELoss 0.0154 (0.1804)	MaskDICELoss 0.9954 (0.7498)
Epoch: [0][377/500]	Time  5.240 ( 5.240)	Loss 2.6075 (1.9414)	CeLoss 0.1641 (0.6250)	SegCLSLoss 0.0447 (0.0161)	KLLoss 0.3652 (0.2855)	MaskLoss 1.1924 (0.6398)	MaskBCELoss 0.5267 (0.2164)	MaskDICELoss 0.6657 (0.4234)
Epoch: [0][378/500]	Time  6.580 ( 6.580)	Loss 2.5878 (1.8172)	CeLoss 0.2617 (0.3087)	SegCLSLoss 0.0242 (0.0200)	KLLoss 0.4160 (0.3285)	MaskLoss 1.1367 (0.7327)	MaskBCELoss 0.3215 (0.1826)	MaskDICELoss 0.8152 (0.5500)
Epoch: [0][379/500]	Time  7.204 ( 7.204)	Loss 1.8314 (2.1453)	CeLoss 0.1914 (0.3977)	SegCLSLoss 0.0261 (0.0215)	KLLoss 0.4062 (0.3207)	MaskLoss 0.7936 (0.8525)	MaskBCELoss 0.0863 (0.1976)	MaskDICELoss 0.7073 (0.6550)
[2025-03-02 16:04:10,401] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=0, lr=[0.0002597755102040816], mom=[(0.9, 0.95)]
[2025-03-02 16:04:10,406] [INFO] [timer.py:215:stop] epoch=0/micro_step=3800/global_step=380, RunningAvgSamplesPerSec=1.3944547776475678, CurrSamplesPerSec=1.6544726851131601, MemAllocated=30.65GB, MaxMemAllocated=36.81GB
Epoch: [0][380/500]	Time  6.046 ( 6.046)	Loss 0.5625 (1.8900)	CeLoss 0.5625 (0.5362)	SegCLSLoss 0.0000 (0.0158)	KLLoss 0.0000 (0.2381)	MaskLoss 0.0000 (0.6609)	MaskBCELoss 0.0000 (0.1695)	MaskDICELoss 0.0000 (0.4915)
Epoch: [0][381/500]	Time  6.105 ( 6.105)	Loss 0.8945 (2.0059)	CeLoss 0.8945 (0.3719)	SegCLSLoss 0.0000 (0.0202)	KLLoss 0.0000 (0.3164)	MaskLoss 0.0000 (0.7963)	MaskBCELoss 0.0000 (0.1511)	MaskDICELoss 0.0000 (0.6452)
Epoch: [0][382/500]	Time  6.279 ( 6.279)	Loss 1.7533 (1.9116)	CeLoss 0.3105 (0.3049)	SegCLSLoss 0.0160 (0.0203)	KLLoss 0.4219 (0.3207)	MaskLoss 0.6960 (0.7823)	MaskBCELoss 0.0495 (0.0919)	MaskDICELoss 0.6465 (0.6904)
Epoch: [0][383/500]	Time  6.325 ( 6.325)	Loss 2.4365 (2.2335)	CeLoss 0.2793 (0.2573)	SegCLSLoss 0.0204 (0.0284)	KLLoss 0.4160 (0.3986)	MaskLoss 1.0532 (0.9611)	MaskBCELoss 0.0651 (0.1190)	MaskDICELoss 0.9882 (0.8421)
Epoch: [0][384/500]	Time  6.063 ( 6.063)	Loss 0.9023 (1.8437)	CeLoss 0.9023 (0.4243)	SegCLSLoss 0.0000 (0.0185)	KLLoss 0.0000 (0.2750)	MaskLoss 0.0000 (0.6914)	MaskBCELoss 0.0000 (0.0699)	MaskDICELoss 0.0000 (0.6215)
Epoch: [0][385/500]	Time  5.314 ( 5.314)	Loss 2.2065 (1.7917)	CeLoss 0.1660 (0.5812)	SegCLSLoss 0.0300 (0.0155)	KLLoss 0.3828 (0.2387)	MaskLoss 0.9939 (0.5893)	MaskBCELoss 0.0169 (0.1235)	MaskDICELoss 0.9769 (0.4658)
Epoch: [0][386/500]	Time  5.469 ( 5.469)	Loss 2.2153 (2.0953)	CeLoss 0.2188 (0.5919)	SegCLSLoss 0.0334 (0.0187)	KLLoss 0.3867 (0.2801)	MaskLoss 0.9709 (0.7329)	MaskBCELoss 0.0214 (0.1333)	MaskDICELoss 0.9495 (0.5996)
Epoch: [0][387/500]	Time  6.369 ( 6.369)	Loss 1.0737 (2.0640)	CeLoss 0.1924 (0.4311)	SegCLSLoss 0.0148 (0.0200)	KLLoss 0.4199 (0.3193)	MaskLoss 0.4158 (0.7956)	MaskBCELoss 0.2158 (0.1617)	MaskDICELoss 0.2000 (0.6339)
Epoch: [0][388/500]	Time  6.163 ( 6.163)	Loss 2.4934 (2.3973)	CeLoss 0.2656 (0.4485)	SegCLSLoss 0.0273 (0.0181)	KLLoss 0.3809 (0.3277)	MaskLoss 1.0885 (0.9534)	MaskBCELoss 0.0953 (0.2915)	MaskDICELoss 0.9932 (0.6619)
Epoch: [0][389/500]	Time  6.112 ( 6.112)	Loss 2.5006 (2.1233)	CeLoss 0.0884 (0.3337)	SegCLSLoss 0.0732 (0.0235)	KLLoss 0.3613 (0.3203)	MaskLoss 1.1697 (0.8728)	MaskBCELoss 0.3324 (0.2397)	MaskDICELoss 0.8373 (0.6330)
[2025-03-02 16:05:10,486] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=0, lr=[0.00025855102040816326], mom=[(0.9, 0.95)]
[2025-03-02 16:05:10,492] [INFO] [timer.py:215:stop] epoch=0/micro_step=3900/global_step=390, RunningAvgSamplesPerSec=1.4003127418621952, CurrSamplesPerSec=1.6989905146976738, MemAllocated=30.97GB, MaxMemAllocated=36.81GB
Epoch: [0][390/500]	Time  5.888 ( 5.888)	Loss 1.1875 (1.9656)	CeLoss 1.1875 (0.3184)	SegCLSLoss 0.0000 (0.0299)	KLLoss 0.0000 (0.3598)	MaskLoss 0.0000 (0.7982)	MaskBCELoss 0.0000 (0.1661)	MaskDICELoss 0.0000 (0.6321)
Exception in thread Thread-5 (_pin_memory_loop):
Traceback (most recent call last):
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 54, in _pin_memory_loop
    do_one_step()
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 31, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/multiprocessing/reductions.py", line 495, in rebuild_storage_fd
    fd = df.detach()
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/connection.py", line 502, in Client
    c = SocketClient(address)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/connection.py", line 630, in SocketClient
    s.connect(address)
FileNotFoundError: [Errno 2] No such file or directory
Epoch: [0][391/500]	Time  5.962 ( 5.962)	Loss 1.6172 (1.8075)	CeLoss 1.6172 (0.6395)	SegCLSLoss 0.0000 (0.0172)	KLLoss 0.0000 (0.2469)	MaskLoss 0.0000 (0.5674)	MaskBCELoss 0.0000 (0.1038)	MaskDICELoss 0.0000 (0.4635)
Epoch: [0][392/500]	Time  5.218 ( 5.218)	Loss 2.5144 (2.2778)	CeLoss 0.1865 (0.4953)	SegCLSLoss 0.0303 (0.0277)	KLLoss 0.3848 (0.3213)	MaskLoss 1.1371 (0.8684)	MaskBCELoss 0.2476 (0.2819)	MaskDICELoss 0.8895 (0.5865)
Epoch: [0][393/500]	Time  6.409 ( 6.409)	Loss 2.4120 (2.1824)	CeLoss 0.2334 (0.2426)	SegCLSLoss 0.0315 (0.0248)	KLLoss 0.4004 (0.4205)	MaskLoss 1.0615 (0.9428)	MaskBCELoss 0.0640 (0.2119)	MaskDICELoss 0.9975 (0.7309)
Epoch: [0][394/500]	Time  4.926 ( 4.926)	Loss 2.2903 (2.0425)	CeLoss 0.3008 (0.5771)	SegCLSLoss 0.0170 (0.0192)	KLLoss 0.4375 (0.2934)	MaskLoss 0.9694 (0.7133)	MaskBCELoss 0.1994 (0.1712)	MaskDICELoss 0.7700 (0.5421)
Epoch: [0][395/500]	Time  6.271 ( 6.271)	Loss 2.1678 (2.0298)	CeLoss 0.2637 (0.4514)	SegCLSLoss 0.0284 (0.0225)	KLLoss 0.3867 (0.3209)	MaskLoss 0.9257 (0.7677)	MaskBCELoss 0.0425 (0.1292)	MaskDICELoss 0.8832 (0.6386)
Epoch: [0][396/500]	Time  5.607 ( 5.607)	Loss 2.3164 (1.8020)	CeLoss 0.2871 (0.6465)	SegCLSLoss 0.0178 (0.0129)	KLLoss 0.4219 (0.2562)	MaskLoss 0.9883 (0.5617)	MaskBCELoss 0.2951 (0.1145)	MaskDICELoss 0.6931 (0.4472)
Epoch: [0][397/500]	Time  5.717 ( 5.717)	Loss 2.5465 (1.8807)	CeLoss 0.0908 (0.2945)	SegCLSLoss 0.0894 (0.0316)	KLLoss 0.3652 (0.3246)	MaskLoss 1.1873 (0.7690)	MaskBCELoss 0.2808 (0.1114)	MaskDICELoss 0.9065 (0.6576)
Epoch: [0][398/500]	Time  5.587 ( 5.587)	Loss 1.9466 (1.7446)	CeLoss 0.2119 (0.4123)	SegCLSLoss 0.0170 (0.0141)	KLLoss 0.4316 (0.2984)	MaskLoss 0.8415 (0.6476)	MaskBCELoss 0.0894 (0.1088)	MaskDICELoss 0.7520 (0.5388)
Epoch: [0][399/500]	Time  5.532 ( 5.532)	Loss 1.5938 (1.8467)	CeLoss 1.5938 (0.5567)	SegCLSLoss 0.0000 (0.0153)	KLLoss 0.0000 (0.2416)	MaskLoss 0.0000 (0.6292)	MaskBCELoss 0.0000 (0.1131)	MaskDICELoss 0.0000 (0.5162)
[2025-03-02 16:06:07,801] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=0, lr=[0.00025732653061224484], mom=[(0.9, 0.95)]
[2025-03-02 16:06:07,806] [INFO] [timer.py:215:stop] epoch=0/micro_step=4000/global_step=400, RunningAvgSamplesPerSec=1.4073780762958732, CurrSamplesPerSec=1.6438505847337483, MemAllocated=31.26GB, MaxMemAllocated=36.81GB
Epoch: [0][400/500]	Time  6.085 ( 6.085)	Loss 2.3758 (2.1877)	CeLoss 0.2695 (0.4104)	SegCLSLoss 0.0227 (0.0219)	KLLoss 0.4121 (0.3309)	MaskLoss 1.0267 (0.8666)	MaskBCELoss 0.0424 (0.1854)	MaskDICELoss 0.9843 (0.6812)
Epoch: [0][401/500]	Time  6.786 ( 6.786)	Loss 2.7402 (2.0254)	CeLoss 0.3594 (0.3953)	SegCLSLoss 0.0184 (0.0203)	KLLoss 0.4258 (0.3297)	MaskLoss 1.1640 (0.7935)	MaskBCELoss 0.2907 (0.1253)	MaskDICELoss 0.8734 (0.6682)
Epoch: [0][402/500]	Time  6.938 ( 6.938)	Loss 2.7127 (1.9636)	CeLoss 0.1729 (0.2020)	SegCLSLoss 0.0282 (0.0244)	KLLoss 0.3887 (0.3643)	MaskLoss 1.2435 (0.8566)	MaskBCELoss 0.3495 (0.1252)	MaskDICELoss 0.8940 (0.7314)
Epoch: [0][403/500]	Time  5.915 ( 5.915)	Loss 1.9184 (1.9012)	CeLoss 0.2480 (0.4547)	SegCLSLoss 0.0167 (0.0188)	KLLoss 0.4492 (0.2855)	MaskLoss 0.8088 (0.7044)	MaskBCELoss 0.2827 (0.1597)	MaskDICELoss 0.5261 (0.5447)
Epoch: [0][404/500]	Time  6.410 ( 6.410)	Loss 1.7691 (1.7541)	CeLoss 0.2354 (0.3527)	SegCLSLoss 0.0150 (0.0216)	KLLoss 0.4590 (0.2873)	MaskLoss 0.7400 (0.6808)	MaskBCELoss 0.1696 (0.2046)	MaskDICELoss 0.5704 (0.4763)
Epoch: [0][405/500]	Time  6.593 ( 6.593)	Loss 1.9620 (1.9165)	CeLoss 0.3262 (0.2551)	SegCLSLoss 0.0138 (0.0227)	KLLoss 0.4434 (0.4172)	MaskLoss 0.7916 (0.8041)	MaskBCELoss 0.1625 (0.1404)	MaskDICELoss 0.6290 (0.6637)
Epoch: [0][406/500]	Time  6.357 ( 6.357)	Loss 1.5196 (1.9847)	CeLoss 0.3867 (0.3301)	SegCLSLoss 0.0157 (0.0232)	KLLoss 0.4375 (0.3721)	MaskLoss 0.5411 (0.8030)	MaskBCELoss 0.0716 (0.1428)	MaskDICELoss 0.4694 (0.6602)
Epoch: [0][407/500]	Time  5.162 ( 5.162)	Loss 0.1001 (1.4383)	CeLoss 0.1001 (0.7023)	SegCLSLoss 0.0000 (0.0098)	KLLoss 0.0000 (0.1639)	MaskLoss 0.0000 (0.3574)	MaskBCELoss 0.0000 (0.0486)	MaskDICELoss 0.0000 (0.3088)
Epoch: [0][408/500]	Time  5.987 ( 5.987)	Loss 2.8680 (1.9286)	CeLoss 0.3867 (0.4383)	SegCLSLoss 0.0284 (0.0199)	KLLoss 0.3730 (0.3312)	MaskLoss 1.2152 (0.7239)	MaskBCELoss 0.3392 (0.1623)	MaskDICELoss 0.8761 (0.5615)
Epoch: [0][409/500]	Time  5.854 ( 5.854)	Loss 2.3028 (1.6733)	CeLoss 0.2285 (0.4289)	SegCLSLoss 0.0238 (0.0116)	KLLoss 0.4023 (0.2574)	MaskLoss 1.0108 (0.6064)	MaskBCELoss 0.0134 (0.1260)	MaskDICELoss 0.9974 (0.4804)
[2025-03-02 16:07:08,986] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=0, lr=[0.0002561020408163265], mom=[(0.9, 0.95)]
[2025-03-02 16:07:08,992] [INFO] [timer.py:215:stop] epoch=0/micro_step=4100/global_step=410, RunningAvgSamplesPerSec=1.4121912281009994, CurrSamplesPerSec=1.92961252425585, MemAllocated=30.72GB, MaxMemAllocated=36.81GB
Epoch: [0][410/500]	Time  5.184 ( 5.184)	Loss 1.4453 (1.7608)	CeLoss 1.4453 (0.6520)	SegCLSLoss 0.0000 (0.0157)	KLLoss 0.0000 (0.2469)	MaskLoss 0.0000 (0.5382)	MaskBCELoss 0.0000 (0.0908)	MaskDICELoss 0.0000 (0.4474)
Epoch: [0][411/500]	Time  6.439 ( 6.439)	Loss 2.1923 (1.9126)	CeLoss 0.2559 (0.4319)	SegCLSLoss 0.0188 (0.0186)	KLLoss 0.4160 (0.2836)	MaskLoss 0.9428 (0.7215)	MaskBCELoss 0.3749 (0.1849)	MaskDICELoss 0.5679 (0.5366)
Epoch: [0][412/500]	Time  6.113 ( 6.113)	Loss 2.5195 (2.1952)	CeLoss 0.5391 (0.3334)	SegCLSLoss 0.0144 (0.0236)	KLLoss 0.4297 (0.3619)	MaskLoss 0.9648 (0.9069)	MaskBCELoss 0.4036 (0.2307)	MaskDICELoss 0.5612 (0.6761)
Epoch: [0][413/500]	Time  7.031 ( 7.031)	Loss 2.1301 (1.8497)	CeLoss 0.1895 (0.2764)	SegCLSLoss 0.0242 (0.0176)	KLLoss 0.4121 (0.3334)	MaskLoss 0.9440 (0.7658)	MaskBCELoss 0.0293 (0.1525)	MaskDICELoss 0.9147 (0.6133)
Epoch: [0][414/500]	Time  5.551 ( 5.551)	Loss 2.1085 (1.8535)	CeLoss 0.2754 (0.5098)	SegCLSLoss 0.0146 (0.0187)	KLLoss 0.4434 (0.2957)	MaskLoss 0.8902 (0.6524)	MaskBCELoss 0.1517 (0.1425)	MaskDICELoss 0.7385 (0.5098)
Epoch: [0][415/500]	Time  5.577 ( 5.577)	Loss 2.2280 (1.9159)	CeLoss 0.1992 (0.4459)	SegCLSLoss 0.0243 (0.0146)	KLLoss 0.4258 (0.2936)	MaskLoss 0.9870 (0.7167)	MaskBCELoss 0.0540 (0.1339)	MaskDICELoss 0.9330 (0.5828)
Epoch: [0][416/500]	Time  6.343 ( 6.343)	Loss 2.5319 (1.7552)	CeLoss 0.2559 (0.2024)	SegCLSLoss 0.0134 (0.0210)	KLLoss 0.4336 (0.3225)	MaskLoss 1.1136 (0.7551)	MaskBCELoss 0.4547 (0.1879)	MaskDICELoss 0.6589 (0.5672)
Epoch: [0][417/500]	Time  5.640 ( 5.640)	Loss 2.7144 (2.1148)	CeLoss 0.1357 (0.4263)	SegCLSLoss 0.0747 (0.0293)	KLLoss 0.3672 (0.3178)	MaskLoss 1.2527 (0.8211)	MaskBCELoss 0.4271 (0.2135)	MaskDICELoss 0.8256 (0.6076)
Epoch: [0][418/500]	Time  6.596 ( 6.596)	Loss 2.8328 (2.0239)	CeLoss 0.2871 (0.3084)	SegCLSLoss 0.0286 (0.0178)	KLLoss 0.3730 (0.3299)	MaskLoss 1.2474 (0.8368)	MaskBCELoss 0.4073 (0.2404)	MaskDICELoss 0.8401 (0.5963)
Epoch: [0][419/500]	Time  5.244 ( 5.244)	Loss 2.3997 (1.7681)	CeLoss 0.1797 (0.5383)	SegCLSLoss 0.0299 (0.0135)	KLLoss 0.3789 (0.2527)	MaskLoss 1.0837 (0.5990)	MaskBCELoss 0.0964 (0.1901)	MaskDICELoss 0.9873 (0.4089)
[2025-03-02 16:08:09,009] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=0, lr=[0.00025487755102040815], mom=[(0.9, 0.95)]
[2025-03-02 16:08:09,015] [INFO] [timer.py:215:stop] epoch=0/micro_step=4200/global_step=420, RunningAvgSamplesPerSec=1.417363862282167, CurrSamplesPerSec=1.822243039451396, MemAllocated=31.26GB, MaxMemAllocated=36.81GB
Epoch: [0][420/500]	Time  5.489 ( 5.489)	Loss 1.9102 (1.7556)	CeLoss 0.1768 (0.4055)	SegCLSLoss 0.0339 (0.0206)	KLLoss 0.3945 (0.3258)	MaskLoss 0.8389 (0.6536)	MaskBCELoss 0.1957 (0.1162)	MaskDICELoss 0.6432 (0.5374)
Epoch: [0][421/500]	Time  6.523 ( 6.523)	Loss 2.3738 (2.1435)	CeLoss 0.2598 (0.4519)	SegCLSLoss 0.0223 (0.0200)	KLLoss 0.4102 (0.3291)	MaskLoss 1.0306 (0.8242)	MaskBCELoss 0.0603 (0.1544)	MaskDICELoss 0.9704 (0.6698)
Epoch: [0][422/500]	Time  6.911 ( 6.911)	Loss 1.4297 (2.1381)	CeLoss 1.4297 (0.3552)	SegCLSLoss 0.0000 (0.0254)	KLLoss 0.0000 (0.3598)	MaskLoss 0.0000 (0.8670)	MaskBCELoss 0.0000 (0.1085)	MaskDICELoss 0.0000 (0.7585)
Epoch: [0][423/500]	Time  5.461 ( 5.461)	Loss 1.0469 (2.0133)	CeLoss 1.0469 (0.5603)	SegCLSLoss 0.0000 (0.0148)	KLLoss 0.0000 (0.2875)	MaskLoss 0.0000 (0.7086)	MaskBCELoss 0.0000 (0.1666)	MaskDICELoss 0.0000 (0.5420)
Epoch: [0][424/500]	Time  6.358 ( 6.358)	Loss 1.1797 (2.0992)	CeLoss 1.1797 (0.3068)	SegCLSLoss 0.0000 (0.0181)	KLLoss 0.0000 (0.3254)	MaskLoss 0.0000 (0.8752)	MaskBCELoss 0.0000 (0.2351)	MaskDICELoss 0.0000 (0.6401)
Epoch: [0][425/500]	Time  6.144 ( 6.144)	Loss 2.1981 (2.0701)	CeLoss 0.2910 (0.4711)	SegCLSLoss 0.0247 (0.0220)	KLLoss 0.4062 (0.3219)	MaskLoss 0.9272 (0.7781)	MaskBCELoss 0.1004 (0.1328)	MaskDICELoss 0.8268 (0.6453)
Epoch: [0][426/500]	Time  5.956 ( 5.956)	Loss 1.9947 (1.8215)	CeLoss 0.2539 (0.3076)	SegCLSLoss 0.0151 (0.0185)	KLLoss 0.4414 (0.2789)	MaskLoss 0.8440 (0.7383)	MaskBCELoss 0.0728 (0.1830)	MaskDICELoss 0.7712 (0.5552)
Epoch: [0][427/500]	Time  6.187 ( 6.187)	Loss 2.7213 (2.0402)	CeLoss 0.1943 (0.4892)	SegCLSLoss 0.0420 (0.0216)	KLLoss 0.3633 (0.3197)	MaskLoss 1.2347 (0.7542)	MaskBCELoss 0.3743 (0.0884)	MaskDICELoss 0.8603 (0.6658)
Epoch: [0][428/500]	Time  5.661 ( 5.661)	Loss 2.6009 (2.0701)	CeLoss 0.1533 (0.3383)	SegCLSLoss 0.0688 (0.0251)	KLLoss 0.3652 (0.3656)	MaskLoss 1.1882 (0.8414)	MaskBCELoss 0.2827 (0.1880)	MaskDICELoss 0.9054 (0.6534)
Epoch: [0][429/500]	Time  5.248 ( 5.248)	Loss 2.3029 (1.6160)	CeLoss 0.2363 (0.5692)	SegCLSLoss 0.0215 (0.0109)	KLLoss 0.3984 (0.2059)	MaskLoss 1.0079 (0.5104)	MaskBCELoss 0.0096 (0.0674)	MaskDICELoss 0.9983 (0.4430)
[2025-03-02 16:09:09,325] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=0, lr=[0.0002536530612244898], mom=[(0.9, 0.95)]
[2025-03-02 16:09:09,331] [INFO] [timer.py:215:stop] epoch=0/micro_step=4300/global_step=430, RunningAvgSamplesPerSec=1.4221911727872407, CurrSamplesPerSec=1.7051952973845892, MemAllocated=31.09GB, MaxMemAllocated=36.81GB
Epoch: [0][430/500]	Time  5.866 ( 5.866)	Loss 1.8674 (1.9484)	CeLoss 0.2480 (0.3405)	SegCLSLoss 0.0160 (0.0183)	KLLoss 0.4297 (0.3285)	MaskLoss 0.7843 (0.7831)	MaskBCELoss 0.2983 (0.2913)	MaskDICELoss 0.4860 (0.4918)
Epoch: [0][431/500]	Time  6.962 ( 6.962)	Loss 1.6650 (1.9315)	CeLoss 0.2637 (0.2402)	SegCLSLoss 0.0161 (0.0203)	KLLoss 0.4277 (0.3680)	MaskLoss 0.6753 (0.8222)	MaskBCELoss 0.2535 (0.1935)	MaskDICELoss 0.4218 (0.6288)
Epoch: [0][432/500]	Time  6.009 ( 6.009)	Loss 2.6700 (2.2498)	CeLoss 0.1719 (0.2768)	SegCLSLoss 0.0491 (0.0289)	KLLoss 0.3770 (0.3547)	MaskLoss 1.2178 (0.9614)	MaskBCELoss 0.5073 (0.2252)	MaskDICELoss 0.7105 (0.7362)
Epoch: [0][433/500]	Time  5.795 ( 5.795)	Loss 1.9062 (1.6802)	CeLoss 0.2139 (0.4165)	SegCLSLoss 0.0437 (0.0187)	KLLoss 0.3691 (0.3324)	MaskLoss 0.8174 (0.6108)	MaskBCELoss 0.1280 (0.1249)	MaskDICELoss 0.6894 (0.4859)
Epoch: [0][434/500]	Time  5.720 ( 5.720)	Loss 2.3649 (1.9981)	CeLoss 0.2871 (0.3170)	SegCLSLoss 0.0186 (0.0205)	KLLoss 0.4004 (0.3770)	MaskLoss 1.0145 (0.8165)	MaskBCELoss 0.1451 (0.1529)	MaskDICELoss 0.8694 (0.6637)
Epoch: [0][435/500]	Time  5.137 ( 5.137)	Loss 2.0288 (1.8672)	CeLoss 0.2334 (0.5123)	SegCLSLoss 0.0208 (0.0179)	KLLoss 0.4121 (0.2902)	MaskLoss 0.8718 (0.6583)	MaskBCELoss 0.1889 (0.1394)	MaskDICELoss 0.6829 (0.5189)
Epoch: [0][436/500]	Time  6.853 ( 6.853)	Loss 2.3726 (2.1164)	CeLoss 0.2383 (0.3563)	SegCLSLoss 0.0295 (0.0233)	KLLoss 0.3887 (0.3664)	MaskLoss 1.0398 (0.8559)	MaskBCELoss 0.0510 (0.1843)	MaskDICELoss 0.9888 (0.6716)
Epoch: [0][437/500]	Time  6.480 ( 6.480)	Loss 1.8668 (1.7577)	CeLoss 0.2656 (0.3271)	SegCLSLoss 0.0161 (0.0187)	KLLoss 0.4434 (0.3287)	MaskLoss 0.7742 (0.6942)	MaskBCELoss 0.2088 (0.1276)	MaskDICELoss 0.5654 (0.5666)
Epoch: [0][438/500]	Time  4.588 ( 4.588)	Loss 2.1581 (1.7080)	CeLoss 0.2852 (0.6125)	SegCLSLoss 0.0179 (0.0139)	KLLoss 0.4121 (0.2465)	MaskLoss 0.9111 (0.5320)	MaskBCELoss 0.2844 (0.1102)	MaskDICELoss 0.6267 (0.4218)
Epoch: [0][439/500]	Time  6.322 ( 6.322)	Loss 2.2910 (2.3282)	CeLoss 0.2148 (0.3720)	SegCLSLoss 0.0211 (0.0204)	KLLoss 0.4121 (0.3748)	MaskLoss 1.0117 (0.9545)	MaskBCELoss 0.0210 (0.2546)	MaskDICELoss 0.9907 (0.6999)
[2025-03-02 16:10:08,397] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=0, lr=[0.0002524285714285714], mom=[(0.9, 0.95)]
[2025-03-02 16:10:08,403] [INFO] [timer.py:215:stop] epoch=0/micro_step=4400/global_step=440, RunningAvgSamplesPerSec=1.4274076165016898, CurrSamplesPerSec=1.921855962249677, MemAllocated=31.47GB, MaxMemAllocated=36.81GB
Epoch: [0][440/500]	Time  5.205 ( 5.205)	Loss 2.4701 (2.1309)	CeLoss 0.2754 (0.4305)	SegCLSLoss 0.0366 (0.0242)	KLLoss 0.3984 (0.3187)	MaskLoss 1.0690 (0.8282)	MaskBCELoss 0.2114 (0.1719)	MaskDICELoss 0.8576 (0.6564)
Epoch: [0][441/500]	Time  6.365 ( 6.365)	Loss 2.3854 (1.9625)	CeLoss 0.2041 (0.4453)	SegCLSLoss 0.0317 (0.0189)	KLLoss 0.3789 (0.3279)	MaskLoss 1.0638 (0.7376)	MaskBCELoss 0.1560 (0.1250)	MaskDICELoss 0.9078 (0.6126)
Epoch: [0][442/500]	Time  5.524 ( 5.524)	Loss 1.2031 (1.7937)	CeLoss 1.2031 (0.5085)	SegCLSLoss 0.0000 (0.0150)	KLLoss 0.0000 (0.2436)	MaskLoss 0.0000 (0.6267)	MaskBCELoss 0.0000 (0.2176)	MaskDICELoss 0.0000 (0.4091)
Epoch: [0][443/500]	Time  5.896 ( 5.896)	Loss 1.1719 (1.9000)	CeLoss 1.1719 (0.4490)	SegCLSLoss 0.0000 (0.0162)	KLLoss 0.0000 (0.2844)	MaskLoss 0.0000 (0.7072)	MaskBCELoss 0.0000 (0.1406)	MaskDICELoss 0.0000 (0.5666)
Epoch: [0][444/500]	Time  7.277 ( 7.277)	Loss 1.5755 (1.9846)	CeLoss 0.2656 (0.2093)	SegCLSLoss 0.0192 (0.0225)	KLLoss 0.4258 (0.3686)	MaskLoss 0.6286 (0.8635)	MaskBCELoss 0.1148 (0.2426)	MaskDICELoss 0.5138 (0.6209)
Epoch: [0][445/500]	Time  6.000 ( 6.000)	Loss 1.8496 (1.7713)	CeLoss 0.2129 (0.2678)	SegCLSLoss 0.0248 (0.0189)	KLLoss 0.4004 (0.3250)	MaskLoss 0.7920 (0.7308)	MaskBCELoss 0.0387 (0.1375)	MaskDICELoss 0.7533 (0.5934)
Epoch: [0][446/500]	Time  4.934 ( 4.934)	Loss 1.1641 (1.7263)	CeLoss 1.1641 (0.5062)	SegCLSLoss 0.0000 (0.0167)	KLLoss 0.0000 (0.2816)	MaskLoss 0.0000 (0.5918)	MaskBCELoss 0.0000 (0.0994)	MaskDICELoss 0.0000 (0.4923)
Epoch: [0][447/500]	Time  5.946 ( 5.946)	Loss 1.8935 (1.8674)	CeLoss 0.2051 (0.4381)	SegCLSLoss 0.0282 (0.0193)	KLLoss 0.3926 (0.2791)	MaskLoss 0.8179 (0.6959)	MaskBCELoss 0.2497 (0.1583)	MaskDICELoss 0.5682 (0.5376)
Epoch: [0][448/500]	Time  6.264 ( 6.264)	Loss 3.8721 (1.8961)	CeLoss 0.1973 (0.3377)	SegCLSLoss 0.0186 (0.0149)	KLLoss 0.4199 (0.3373)	MaskLoss 1.8115 (0.7585)	MaskBCELoss 1.2963 (0.2494)	MaskDICELoss 0.5152 (0.5091)
Epoch: [0][449/500]	Time  6.479 ( 6.479)	Loss 2.5649 (1.8222)	CeLoss 0.1680 (0.2990)	SegCLSLoss 0.0422 (0.0166)	KLLoss 0.3750 (0.2807)	MaskLoss 1.1692 (0.7433)	MaskBCELoss 0.2371 (0.2175)	MaskDICELoss 0.9320 (0.5258)
[2025-03-02 16:11:09,534] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=0, lr=[0.00025120408163265305], mom=[(0.9, 0.95)]
[2025-03-02 16:11:09,541] [INFO] [timer.py:215:stop] epoch=0/micro_step=4500/global_step=450, RunningAvgSamplesPerSec=1.4314813635689336, CurrSamplesPerSec=1.5500163711846455, MemAllocated=30.8GB, MaxMemAllocated=36.81GB
Epoch: [0][450/500]	Time  6.453 ( 6.453)	Loss 2.5851 (2.2169)	CeLoss 0.1758 (0.2426)	SegCLSLoss 0.0430 (0.0213)	KLLoss 0.3652 (0.4121)	MaskLoss 1.1753 (0.9612)	MaskBCELoss 0.1940 (0.2169)	MaskDICELoss 0.9813 (0.7443)
Epoch: [0][451/500]	Time  5.556 ( 5.556)	Loss 1.0312 (2.0949)	CeLoss 1.0312 (0.4739)	SegCLSLoss 0.0000 (0.0229)	KLLoss 0.0000 (0.3187)	MaskLoss 0.0000 (0.7888)	MaskBCELoss 0.0000 (0.2263)	MaskDICELoss 0.0000 (0.5625)
Epoch: [0][452/500]	Time  6.012 ( 6.012)	Loss 2.3603 (1.6738)	CeLoss 0.2100 (0.4307)	SegCLSLoss 0.0144 (0.0146)	KLLoss 0.4258 (0.2893)	MaskLoss 1.0503 (0.6035)	MaskBCELoss 0.0756 (0.1356)	MaskDICELoss 0.9747 (0.4679)
Epoch: [0][453/500]	Time  6.048 ( 6.048)	Loss 0.1172 (1.8768)	CeLoss 0.1172 (0.3935)	SegCLSLoss 0.0000 (0.0171)	KLLoss 0.0000 (0.2818)	MaskLoss 0.0000 (0.7234)	MaskBCELoss 0.0000 (0.1182)	MaskDICELoss 0.0000 (0.6052)
Epoch: [0][454/500]	Time  4.463 ( 4.463)	Loss 2.6558 (1.7425)	CeLoss 0.2080 (0.6163)	SegCLSLoss 0.0302 (0.0123)	KLLoss 0.3789 (0.2027)	MaskLoss 1.1980 (0.5499)	MaskBCELoss 0.2709 (0.1537)	MaskDICELoss 0.9271 (0.3962)
Epoch: [0][455/500]	Time  6.862 ( 6.862)	Loss 2.6823 (2.0138)	CeLoss 0.2754 (0.2786)	SegCLSLoss 0.0143 (0.0204)	KLLoss 0.4414 (0.3697)	MaskLoss 1.1771 (0.8439)	MaskBCELoss 0.3614 (0.1341)	MaskDICELoss 0.8157 (0.7098)
Epoch: [0][456/500]	Time  5.309 ( 5.309)	Loss 2.8602 (1.7621)	CeLoss 0.4473 (0.4754)	SegCLSLoss 0.0173 (0.0141)	KLLoss 0.3945 (0.2445)	MaskLoss 1.1830 (0.6277)	MaskBCELoss 0.3225 (0.1946)	MaskDICELoss 0.8605 (0.4331)
Epoch: [0][457/500]	Time  6.997 ( 6.997)	Loss 0.0535 (1.9617)	CeLoss 0.0535 (0.2384)	SegCLSLoss 0.0000 (0.0181)	KLLoss 0.0000 (0.3703)	MaskLoss 0.0000 (0.8386)	MaskBCELoss 0.0000 (0.0898)	MaskDICELoss 0.0000 (0.7488)
Epoch: [0][458/500]	Time  6.939 ( 6.939)	Loss 2.1978 (2.0226)	CeLoss 0.2676 (0.2689)	SegCLSLoss 0.0211 (0.0231)	KLLoss 0.4062 (0.3689)	MaskLoss 0.9397 (0.8526)	MaskBCELoss 0.0771 (0.1759)	MaskDICELoss 0.8627 (0.6766)
Epoch: [0][459/500]	Time  5.644 ( 5.644)	Loss 1.5009 (1.8275)	CeLoss 0.2002 (0.4856)	SegCLSLoss 0.0294 (0.0188)	KLLoss 0.4219 (0.3301)	MaskLoss 0.6215 (0.6496)	MaskBCELoss 0.1956 (0.1572)	MaskDICELoss 0.4259 (0.4925)
[2025-03-02 16:12:09,772] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=0, lr=[0.0002499795918367347], mom=[(0.9, 0.95)]
[2025-03-02 16:12:09,778] [INFO] [timer.py:215:stop] epoch=0/micro_step=4600/global_step=460, RunningAvgSamplesPerSec=1.4358046603269627, CurrSamplesPerSec=1.5614567775118868, MemAllocated=30.96GB, MaxMemAllocated=36.81GB
Epoch: [0][460/500]	Time  6.406 ( 6.406)	Loss 0.9844 (1.8346)	CeLoss 0.9844 (0.3892)	SegCLSLoss 0.0000 (0.0181)	KLLoss 0.0000 (0.3752)	MaskLoss 0.0000 (0.6992)	MaskBCELoss 0.0000 (0.1645)	MaskDICELoss 0.0000 (0.5347)
Epoch: [0][461/500]	Time  6.085 ( 6.085)	Loss 1.6097 (2.0382)	CeLoss 0.2988 (0.3086)	SegCLSLoss 0.0127 (0.0235)	KLLoss 0.4434 (0.3711)	MaskLoss 0.6300 (0.8404)	MaskBCELoss 0.2584 (0.2467)	MaskDICELoss 0.3716 (0.5937)
Epoch: [0][462/500]	Time  5.510 ( 5.510)	Loss 1.1797 (1.9047)	CeLoss 1.1797 (0.2955)	SegCLSLoss 0.0000 (0.0260)	KLLoss 0.0000 (0.3219)	MaskLoss 0.0000 (0.7820)	MaskBCELoss 0.0000 (0.2124)	MaskDICELoss 0.0000 (0.5696)
Epoch: [0][463/500]	Time  5.734 ( 5.734)	Loss 2.2665 (1.7819)	CeLoss 0.2021 (0.4113)	SegCLSLoss 0.0245 (0.0216)	KLLoss 0.4062 (0.2789)	MaskLoss 1.0053 (0.6658)	MaskBCELoss 0.0695 (0.1343)	MaskDICELoss 0.9358 (0.5315)
Epoch: [0][464/500]	Time  6.454 ( 6.454)	Loss 2.6192 (1.9214)	CeLoss 0.2793 (0.3132)	SegCLSLoss 0.0317 (0.0181)	KLLoss 0.3828 (0.3346)	MaskLoss 1.1426 (0.7829)	MaskBCELoss 0.3499 (0.2038)	MaskDICELoss 0.7927 (0.5791)
Epoch: [0][465/500]	Time  7.469 ( 7.469)	Loss 1.7106 (2.4888)	CeLoss 0.2070 (0.2162)	SegCLSLoss 0.0359 (0.0283)	KLLoss 0.3945 (0.4020)	MaskLoss 0.7235 (1.1092)	MaskBCELoss 0.0198 (0.2853)	MaskDICELoss 0.7037 (0.8239)
Epoch: [0][466/500]	Time  6.701 ( 6.701)	Loss 1.6342 (1.8968)	CeLoss 0.2246 (0.3447)	SegCLSLoss 0.0228 (0.0195)	KLLoss 0.4219 (0.3291)	MaskLoss 0.6774 (0.7545)	MaskBCELoss 0.0454 (0.0767)	MaskDICELoss 0.6320 (0.6779)
Epoch: [0][467/500]	Time  5.941 ( 5.941)	Loss 2.6225 (1.7644)	CeLoss 0.1553 (0.3923)	SegCLSLoss 0.0437 (0.0196)	KLLoss 0.3730 (0.2807)	MaskLoss 1.2043 (0.6672)	MaskBCELoss 0.3494 (0.1108)	MaskDICELoss 0.8549 (0.5564)
Epoch: [0][468/500]	Time  6.120 ( 6.120)	Loss 1.5291 (1.5733)	CeLoss 0.2129 (0.4581)	SegCLSLoss 0.0330 (0.0187)	KLLoss 0.3926 (0.2883)	MaskLoss 0.6298 (0.5384)	MaskBCELoss 0.2021 (0.0947)	MaskDICELoss 0.4276 (0.4438)
Epoch: [0][469/500]	Time  5.723 ( 5.723)	Loss 1.5503 (1.6622)	CeLoss 0.2637 (0.4593)	SegCLSLoss 0.0190 (0.0127)	KLLoss 0.4023 (0.2486)	MaskLoss 0.6179 (0.5858)	MaskBCELoss 0.1051 (0.1337)	MaskDICELoss 0.5128 (0.4521)
[2025-03-02 16:13:10,956] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=0, lr=[0.0002487551020408163], mom=[(0.9, 0.95)]
[2025-03-02 16:13:10,962] [INFO] [timer.py:215:stop] epoch=0/micro_step=4700/global_step=470, RunningAvgSamplesPerSec=1.4395487638208313, CurrSamplesPerSec=1.8365195703889032, MemAllocated=31.33GB, MaxMemAllocated=36.81GB
Epoch: [0][470/500]	Time  5.447 ( 5.447)	Loss 1.9954 (1.7794)	CeLoss 0.2002 (0.4303)	SegCLSLoss 0.0275 (0.0182)	KLLoss 0.3965 (0.3283)	MaskLoss 0.8708 (0.6536)	MaskBCELoss 0.0597 (0.1394)	MaskDICELoss 0.8110 (0.5142)
Epoch: [0][471/500]	Time  4.357 ( 4.357)	Loss 1.0312 (1.6280)	CeLoss 1.0312 (0.8125)	SegCLSLoss 0.0000 (0.0122)	KLLoss 0.0000 (0.1553)	MaskLoss 0.0000 (0.3970)	MaskBCELoss 0.0000 (0.0790)	MaskDICELoss 0.0000 (0.3180)
Epoch: [0][472/500]	Time  5.676 ( 5.676)	Loss 1.1016 (1.7715)	CeLoss 1.1016 (0.4137)	SegCLSLoss 0.0000 (0.0156)	KLLoss 0.0000 (0.2338)	MaskLoss 0.0000 (0.6634)	MaskBCELoss 0.0000 (0.1786)	MaskDICELoss 0.0000 (0.4847)
Epoch: [0][473/500]	Time  5.880 ( 5.880)	Loss 2.3694 (2.1260)	CeLoss 0.3066 (0.3066)	SegCLSLoss 0.0175 (0.0250)	KLLoss 0.4277 (0.3584)	MaskLoss 1.0060 (0.8857)	MaskBCELoss 0.5065 (0.1690)	MaskDICELoss 0.4994 (0.7167)
Epoch: [0][474/500]	Time  5.646 ( 5.646)	Loss 2.4733 (2.2002)	CeLoss 0.1660 (0.3332)	SegCLSLoss 0.0286 (0.0228)	KLLoss 0.4062 (0.3498)	MaskLoss 1.1263 (0.9103)	MaskBCELoss 0.1321 (0.2134)	MaskDICELoss 0.9942 (0.6969)
Epoch: [0][475/500]	Time  5.700 ( 5.700)	Loss 1.8896 (1.9668)	CeLoss 0.2314 (0.3771)	SegCLSLoss 0.0298 (0.0200)	KLLoss 0.3809 (0.3609)	MaskLoss 0.8022 (0.7720)	MaskBCELoss 0.0347 (0.1790)	MaskDICELoss 0.7675 (0.5930)
Epoch: [0][476/500]	Time  6.130 ( 6.130)	Loss 1.1406 (1.9118)	CeLoss 1.1406 (0.3230)	SegCLSLoss 0.0000 (0.0212)	KLLoss 0.0000 (0.3609)	MaskLoss 0.0000 (0.7711)	MaskBCELoss 0.0000 (0.1854)	MaskDICELoss 0.0000 (0.5857)
Epoch: [0][477/500]	Time  6.654 ( 6.654)	Loss 1.2266 (1.9926)	CeLoss 1.2266 (0.4213)	SegCLSLoss 0.0000 (0.0169)	KLLoss 0.0000 (0.3191)	MaskLoss 0.0000 (0.7655)	MaskBCELoss 0.0000 (0.2320)	MaskDICELoss 0.0000 (0.5335)
Epoch: [0][478/500]	Time  4.828 ( 4.828)	Loss 1.5283 (1.4536)	CeLoss 0.1904 (0.5140)	SegCLSLoss 0.0231 (0.0184)	KLLoss 0.4023 (0.1928)	MaskLoss 0.6431 (0.4557)	MaskBCELoss 0.0563 (0.0822)	MaskDICELoss 0.5867 (0.3735)
Epoch: [0][479/500]	Time  6.158 ( 6.158)	Loss 2.3463 (2.0238)	CeLoss 0.2773 (0.2802)	SegCLSLoss 0.0151 (0.0192)	KLLoss 0.4062 (0.4037)	MaskLoss 1.0101 (0.8468)	MaskBCELoss 0.5345 (0.1665)	MaskDICELoss 0.4756 (0.6803)
[2025-03-02 16:14:07,559] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=0, lr=[0.00024753061224489794], mom=[(0.9, 0.95)]
[2025-03-02 16:14:07,565] [INFO] [timer.py:215:stop] epoch=0/micro_step=4800/global_step=480, RunningAvgSamplesPerSec=1.4451523876967736, CurrSamplesPerSec=1.7948772592353521, MemAllocated=30.95GB, MaxMemAllocated=36.81GB
Epoch: [0][480/500]	Time  5.573 ( 5.573)	Loss 2.1931 (1.8635)	CeLoss 0.2197 (0.2606)	SegCLSLoss 0.0260 (0.0198)	KLLoss 0.3965 (0.3615)	MaskLoss 0.9608 (0.7784)	MaskBCELoss 0.0441 (0.1374)	MaskDICELoss 0.9168 (0.6410)
Epoch: [0][481/500]	Time  6.324 ( 6.324)	Loss 2.5575 (2.1364)	CeLoss 0.3184 (0.2488)	SegCLSLoss 0.0195 (0.0232)	KLLoss 0.3789 (0.4037)	MaskLoss 1.0961 (0.9177)	MaskBCELoss 0.4041 (0.2036)	MaskDICELoss 0.6920 (0.7141)
Epoch: [0][482/500]	Time  6.049 ( 6.049)	Loss 1.0312 (1.9997)	CeLoss 1.0312 (0.6138)	SegCLSLoss 0.0000 (0.0140)	KLLoss 0.0000 (0.2850)	MaskLoss 0.0000 (0.6752)	MaskBCELoss 0.0000 (0.0782)	MaskDICELoss 0.0000 (0.5970)
Epoch: [0][483/500]	Time  6.409 ( 6.409)	Loss 4.2469 (2.0547)	CeLoss 0.2471 (0.3350)	SegCLSLoss 0.0220 (0.0226)	KLLoss 0.3984 (0.3609)	MaskLoss 1.9750 (0.8362)	MaskBCELoss 1.4046 (0.2695)	MaskDICELoss 0.5704 (0.5667)
Epoch: [0][484/500]	Time  6.226 ( 6.226)	Loss 2.1842 (2.0435)	CeLoss 0.2100 (0.3351)	SegCLSLoss 0.0232 (0.0222)	KLLoss 0.4180 (0.3670)	MaskLoss 0.9603 (0.8304)	MaskBCELoss 0.0300 (0.1845)	MaskDICELoss 0.9302 (0.6459)
Epoch: [0][485/500]	Time  5.349 ( 5.349)	Loss 0.9978 (1.7515)	CeLoss 0.2422 (0.4967)	SegCLSLoss 0.0187 (0.0174)	KLLoss 0.4238 (0.2875)	MaskLoss 0.3515 (0.6088)	MaskBCELoss 0.1853 (0.0939)	MaskDICELoss 0.1661 (0.5149)
Epoch: [0][486/500]	Time  6.658 ( 6.658)	Loss 1.1016 (2.0267)	CeLoss 1.1016 (0.3456)	SegCLSLoss 0.0000 (0.0163)	KLLoss 0.0000 (0.3314)	MaskLoss 0.0000 (0.8199)	MaskBCELoss 0.0000 (0.2030)	MaskDICELoss 0.0000 (0.6169)
Epoch: [0][487/500]	Time  6.434 ( 6.434)	Loss 2.0638 (1.9616)	CeLoss 0.2480 (0.3854)	SegCLSLoss 0.0237 (0.0216)	KLLoss 0.4180 (0.3275)	MaskLoss 0.8815 (0.7664)	MaskBCELoss 0.0119 (0.1570)	MaskDICELoss 0.8696 (0.6094)
Epoch: [0][488/500]	Time  5.821 ( 5.821)	Loss 0.0889 (1.7800)	CeLoss 0.0889 (0.4190)	SegCLSLoss 0.0000 (0.0171)	KLLoss 0.0000 (0.2439)	MaskLoss 0.0000 (0.6640)	MaskBCELoss 0.0000 (0.1401)	MaskDICELoss 0.0000 (0.5238)
Epoch: [0][489/500]	Time  6.258 ( 6.258)	Loss 2.3898 (1.6021)	CeLoss 0.3262 (0.4525)	SegCLSLoss 0.0226 (0.0139)	KLLoss 0.3945 (0.2453)	MaskLoss 1.0064 (0.5592)	MaskBCELoss 0.0205 (0.1022)	MaskDICELoss 0.9859 (0.4570)
[2025-03-02 16:15:08,513] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=0, lr=[0.00024630612244897957], mom=[(0.9, 0.95)]
[2025-03-02 16:15:08,519] [INFO] [timer.py:215:stop] epoch=0/micro_step=4900/global_step=490, RunningAvgSamplesPerSec=1.4486942532378047, CurrSamplesPerSec=1.8437839159228655, MemAllocated=30.72GB, MaxMemAllocated=36.81GB
Epoch: [0][490/500]	Time  5.425 ( 5.425)	Loss 1.2500 (1.4848)	CeLoss 1.2500 (0.4806)	SegCLSLoss 0.0000 (0.0153)	KLLoss 0.0000 (0.2902)	MaskLoss 0.0000 (0.4837)	MaskBCELoss 0.0000 (0.1078)	MaskDICELoss 0.0000 (0.3759)
Epoch: [0][491/500]	Time  5.359 ( 5.359)	Loss 2.1874 (2.2212)	CeLoss 0.2617 (0.4893)	SegCLSLoss 0.0247 (0.0211)	KLLoss 0.4102 (0.3223)	MaskLoss 0.9365 (0.8448)	MaskBCELoss 0.0313 (0.1983)	MaskDICELoss 0.9052 (0.6465)
Epoch: [0][492/500]	Time  6.813 ( 6.813)	Loss 2.1894 (2.3849)	CeLoss 0.2441 (0.2311)	SegCLSLoss 0.0159 (0.0233)	KLLoss 0.4434 (0.4105)	MaskLoss 0.9463 (1.0507)	MaskBCELoss 0.3062 (0.2431)	MaskDICELoss 0.6401 (0.8076)
Epoch: [0][493/500]	Time  6.289 ( 6.289)	Loss 2.4502 (1.6100)	CeLoss 0.2578 (0.3694)	SegCLSLoss 0.0212 (0.0120)	KLLoss 0.3945 (0.2900)	MaskLoss 1.0718 (0.6027)	MaskBCELoss 0.2399 (0.1153)	MaskDICELoss 0.8319 (0.4874)
Epoch: [0][494/500]	Time  6.667 ( 6.667)	Loss 1.8508 (1.9720)	CeLoss 0.2754 (0.3206)	SegCLSLoss 0.0150 (0.0245)	KLLoss 0.4316 (0.3154)	MaskLoss 0.7623 (0.8038)	MaskBCELoss 0.1066 (0.1460)	MaskDICELoss 0.6557 (0.6578)
Epoch: [0][495/500]	Time  5.324 ( 5.324)	Loss 0.9297 (1.7558)	CeLoss 0.9297 (0.5763)	SegCLSLoss 0.0000 (0.0132)	KLLoss 0.0000 (0.2451)	MaskLoss 0.0000 (0.5742)	MaskBCELoss 0.0000 (0.0948)	MaskDICELoss 0.0000 (0.4794)
Epoch: [0][496/500]	Time  5.719 ( 5.719)	Loss 2.2874 (2.4902)	CeLoss 0.2139 (0.2620)	SegCLSLoss 0.0303 (0.0299)	KLLoss 0.3809 (0.3879)	MaskLoss 1.0109 (1.0872)	MaskBCELoss 0.1349 (0.2285)	MaskDICELoss 0.8760 (0.8587)
Epoch: [0][497/500]	Time  6.531 ( 6.531)	Loss 0.7461 (1.8801)	CeLoss 0.7461 (0.3504)	SegCLSLoss 0.0000 (0.0194)	KLLoss 0.0000 (0.3250)	MaskLoss 0.0000 (0.7437)	MaskBCELoss 0.0000 (0.1762)	MaskDICELoss 0.0000 (0.5675)
Epoch: [0][498/500]	Time  6.427 ( 6.427)	Loss 0.5273 (1.1420)	CeLoss 0.5273 (0.3928)	SegCLSLoss 0.0000 (0.0086)	KLLoss 0.0000 (0.2119)	MaskLoss 0.0000 (0.3618)	MaskBCELoss 0.0000 (0.0755)	MaskDICELoss 0.0000 (0.2862)
Epoch: [0][499/500]	Time  5.153 ( 5.153)	Loss 0.5078 (2.1411)	CeLoss 0.5078 (0.4886)	SegCLSLoss 0.0000 (0.0197)	KLLoss 0.0000 (0.2752)	MaskLoss 0.0000 (0.8077)	MaskBCELoss 0.0000 (0.2358)	MaskDICELoss 0.0000 (0.5719)
  0%|                                                                                                                                                              | 0/200 [00:00<?, ?it/s]
[2025-03-02 16:16:08,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=0, lr=[0.0002450816326530612], mom=[(0.9, 0.95)]
[2025-03-02 16:16:08,294] [INFO] [timer.py:215:stop] epoch=0/micro_step=5000/global_step=500, RunningAvgSamplesPerSec=1.4526096315810222, CurrSamplesPerSec=1.821024896130086, MemAllocated=30.78GB, MaxMemAllocated=36.81GB














100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:29<00:00,  6.84it/s]
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
giou: 0.2034, ciou: 0.2263
[2025-03-02 16:16:37,883] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step500 is about to be saved!
[34m[1mwandb[39m[22m: [33mWARNING[39m Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 499. Dropping entry: {'val/giou': 0.20342881977558136, 'val/ciou': 0.22633934020996094, '_timestamp': 1740953797.8169997}).
[2025-03-02 16:16:50,986] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./runs/plum-13b_kld_0.1_dice_2.0/ckpt_model/global_step500/mp_rank_00_model_states.pt
[2025-03-02 16:16:50,987] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./runs/plum-13b_kld_0.1_dice_2.0/ckpt_model/global_step500/mp_rank_00_model_states.pt...
[2025-03-02 16:19:47,376] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./runs/plum-13b_kld_0.1_dice_2.0/ckpt_model/global_step500/mp_rank_00_model_states.pt.
[2025-03-02 16:19:48,228] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./runs/plum-13b_kld_0.1_dice_2.0/ckpt_model/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([334, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[2025-03-02 16:19:59,395] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./runs/plum-13b_kld_0.1_dice_2.0/ckpt_model/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-02 16:19:59,398] [INFO] [engine.py:3244:_save_zero_checkpoint] zero checkpoint saved ./runs/plum-13b_kld_0.1_dice_2.0/ckpt_model/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-02 16:19:59,398] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step500 is ready now!
Epoch: [1][  1/500]	Time  5.794 ( 5.794)	Loss 3.1546 (1.9180)	CeLoss 0.2051 (0.4553)	SegCLSLoss 0.0203 (0.0199)	KLLoss 0.3984 (0.3219)	MaskLoss 1.4503 (0.7102)	MaskBCELoss 0.5501 (0.1159)	MaskDICELoss 0.9002 (0.5942)
Epoch: [1][  2/500]	Time  6.274 ( 6.274)	Loss 2.3851 (2.2751)	CeLoss 0.2490 (0.3434)	SegCLSLoss 0.0161 (0.0203)	KLLoss 0.4160 (0.3602)	MaskLoss 1.0431 (0.9428)	MaskBCELoss 0.0870 (0.1445)	MaskDICELoss 0.9562 (0.7983)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 499. Dropping entry: {'train/loss': 1.9180007219314574, 'train/ce_loss': 0.4552734375, 'train/seg_cls_loss': 0.0199462890625, 'train/kl_loss': 0.321875, 'train/mask_bce_loss': 0.11593315601348878, 'train/mask_dice_loss': 0.5942390739917756, 'train/mask_loss': 0.7101722240447998, 'metrics/total_secs_per_batch': 5.793555498123169, 'metrics/data_secs_per_batch': 2.3975141286849975, '_timestamp': 1740954005.2004821}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 1 is less than current step: 499. Dropping entry: {'train/loss': 2.27511146068573, 'train/ce_loss': 0.343359375, 'train/seg_cls_loss': 0.02027587890625, 'train/kl_loss': 0.36015625, 'train/mask_bce_loss': 0.14450268130749463, 'train/mask_dice_loss': 0.7983264863491059, 'train/mask_loss': 0.942829167842865, 'metrics/total_secs_per_batch': 6.273920774459839, 'metrics/data_secs_per_batch': 2.8971706867218017, '_timestamp': 1740954011.474536}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 1 is less than current step: 499. Dropping entry: {'train/lr': 0.0002448367346938775, '_timestamp': 1740954011.4748986}).
Epoch: [1][  3/500]	Time  7.128 ( 7.128)	Loss 2.4544 (1.5745)	CeLoss 0.2168 (0.3327)	SegCLSLoss 0.0288 (0.0176)	KLLoss 0.3906 (0.2777)	MaskLoss 1.0924 (0.6027)	MaskBCELoss 0.4073 (0.1566)	MaskDICELoss 0.6852 (0.4462)
Epoch: [1][  4/500]	Time  6.993 ( 6.993)	Loss 1.8798 (1.9222)	CeLoss 0.2012 (0.4805)	SegCLSLoss 0.0234 (0.0187)	KLLoss 0.4004 (0.3187)	MaskLoss 0.8129 (0.7002)	MaskBCELoss 0.0225 (0.0838)	MaskDICELoss 0.7905 (0.6164)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 2 is less than current step: 499. Dropping entry: {'train/loss': 1.5744688034057617, 'train/ce_loss': 0.332666015625, 'train/seg_cls_loss': 0.017596435546875, 'train/kl_loss': 0.277734375, 'train/mask_bce_loss': 0.15656949430704117, 'train/mask_dice_loss': 0.4461678355932236, 'train/mask_loss': 0.6027373254299164, 'metrics/total_secs_per_batch': 7.127601861953735, 'metrics/data_secs_per_batch': 3.2428119659423826, '_timestamp': 1740954018.6019442}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 2 is less than current step: 499. Dropping entry: {'train/lr': 0.0002447142857142857, '_timestamp': 1740954018.602158}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 3 is less than current step: 499. Dropping entry: {'train/loss': 1.9221667885780334, 'train/ce_loss': 0.48046875, 'train/seg_cls_loss': 0.018682861328125, 'train/kl_loss': 0.31875, 'train/mask_bce_loss': 0.08380860285833477, 'train/mask_dice_loss': 0.6164349615573883, 'train/mask_loss': 0.7002435624599457, 'metrics/total_secs_per_batch': 6.9932897090911865, 'metrics/data_secs_per_batch': 3.2614054441452027, '_timestamp': 1740954025.5955613}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 3 is less than current step: 499. Dropping entry: {'train/lr': 0.00024459183673469385, '_timestamp': 1740954025.595991}).
Epoch: [1][  5/500]	Time  7.290 ( 7.290)	Loss 2.4249 (1.9402)	CeLoss 0.2812 (0.2456)	SegCLSLoss 0.0156 (0.0207)	KLLoss 0.3984 (0.3596)	MaskLoss 1.0484 (0.8241)	MaskBCELoss 0.1425 (0.1918)	MaskDICELoss 0.9059 (0.6324)
Epoch: [1][  6/500]	Time  6.373 ( 6.373)	Loss 2.3502 (1.9252)	CeLoss 0.4062 (0.4310)	SegCLSLoss 0.0146 (0.0152)	KLLoss 0.4160 (0.2846)	MaskLoss 0.9475 (0.7292)	MaskBCELoss 0.2714 (0.2055)	MaskDICELoss 0.6761 (0.5237)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 4 is less than current step: 499. Dropping entry: {'train/loss': 1.9402058005332947, 'train/ce_loss': 0.24560546875, 'train/seg_cls_loss': 0.020654296875, 'train/kl_loss': 0.3595703125, 'train/mask_bce_loss': 0.19175024749711156, 'train/mask_dice_loss': 0.6323565542697906, 'train/mask_loss': 0.8241068124771118, 'metrics/total_secs_per_batch': 7.2903923988342285, 'metrics/data_secs_per_batch': 3.3342939138412477, '_timestamp': 1740954032.8863661}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 4 is less than current step: 499. Dropping entry: {'train/lr': 0.00024446938775510204, '_timestamp': 1740954032.8870242}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 5 is less than current step: 499. Dropping entry: {'train/loss': 1.9251954197883605, 'train/ce_loss': 0.43095703125, 'train/seg_cls_loss': 0.01522216796875, 'train/kl_loss': 0.2845703125, 'train/mask_bce_loss': 0.2055195514112711, 'train/mask_dice_loss': 0.5236797273159027, 'train/mask_loss': 0.7291992843151093, 'metrics/total_secs_per_batch': 6.372676372528076, 'metrics/data_secs_per_batch': 2.738143801689148, '_timestamp': 1740954039.2582958}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 5 is less than current step: 499. Dropping entry: {'train/lr': 0.0002443469387755102, '_timestamp': 1740954039.2586071}).
Epoch: [1][  7/500]	Time  6.030 ( 6.030)	Loss 2.2439 (1.8374)	CeLoss 0.1982 (0.3570)	SegCLSLoss 0.0310 (0.0192)	KLLoss 0.3848 (0.3684)	MaskLoss 0.9960 (0.7171)	MaskBCELoss 0.0189 (0.1221)	MaskDICELoss 0.9770 (0.5950)
Epoch: [1][  8/500]	Time  5.633 ( 5.633)	Loss 1.9410 (1.7341)	CeLoss 0.3105 (0.5429)	SegCLSLoss 0.0216 (0.0131)	KLLoss 0.4141 (0.2443)	MaskLoss 0.7888 (0.5801)	MaskBCELoss 0.0351 (0.1159)	MaskDICELoss 0.7538 (0.4641)
Epoch: [1][  9/500]	Time  5.487 ( 5.487)	Loss 2.6230 (1.7641)	CeLoss 0.2168 (0.4338)	SegCLSLoss 0.0317 (0.0216)	KLLoss 0.3711 (0.3199)	MaskLoss 1.1767 (0.6437)	MaskBCELoss 0.2709 (0.1071)	MaskDICELoss 0.9058 (0.5366)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 6 is less than current step: 499. Dropping entry: {'train/loss': 1.8374098658561706, 'train/ce_loss': 0.35703125, 'train/seg_cls_loss': 0.01923828125, 'train/kl_loss': 0.368359375, 'train/mask_bce_loss': 0.12205398473888636, 'train/mask_dice_loss': 0.5950396209955215, 'train/mask_loss': 0.7170936048030854, 'metrics/total_secs_per_batch': 6.02967381477356, 'metrics/data_secs_per_batch': 2.8057776212692263, '_timestamp': 1740954045.2880025}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 6 is less than current step: 499. Dropping entry: {'train/lr': 0.00024422448979591837, '_timestamp': 1740954045.2882235}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 7 is less than current step: 499. Dropping entry: {'train/loss': 1.7341063737869262, 'train/ce_loss': 0.54287109375, 'train/seg_cls_loss': 0.0131103515625, 'train/kl_loss': 0.2443359375, 'train/mask_bce_loss': 0.11594146378338337, 'train/mask_dice_loss': 0.46414881348609927, 'train/mask_loss': 0.5800902962684631, 'metrics/total_secs_per_batch': 5.63320255279541, 'metrics/data_secs_per_batch': 2.370444822311401, '_timestamp': 1740954050.9211924}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 7 is less than current step: 499. Dropping entry: {'train/lr': 0.0002441020408163265, '_timestamp': 1740954050.9215024}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 8 is less than current step: 499. Dropping entry: {'train/loss': 1.7641260266304015, 'train/ce_loss': 0.4337890625, 'train/seg_cls_loss': 0.021588134765625, 'train/kl_loss': 0.319921875, 'train/mask_bce_loss': 0.10714148171246052, 'train/mask_dice_loss': 0.5365914434194565, 'train/mask_loss': 0.6437329381704331, 'metrics/total_secs_per_batch': 5.486629009246826, 'metrics/data_secs_per_batch': 2.2665153980255126, '_timestamp': 1740954056.4077966}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 8 is less than current step: 499. Dropping entry: {'train/lr': 0.00024397959183673467, '_timestamp': 1740954056.4080882}).
[2025-03-02 16:21:01,244] [INFO] [logging.py:96:log_dist] [Rank 0] step=510, skipped=0, lr=[0.00024391836734693876], mom=[(0.9, 0.95)]
[2025-03-02 16:21:01,250] [INFO] [timer.py:215:stop] epoch=0/micro_step=5100/global_step=510, RunningAvgSamplesPerSec=1.455528037267509, CurrSamplesPerSec=2.065510852354945, MemAllocated=30.95GB, MaxMemAllocated=36.81GB
Epoch: [1][ 10/500]	Time  4.843 ( 4.843)	Loss 2.3941 (1.7186)	CeLoss 0.2793 (0.5892)	SegCLSLoss 0.0153 (0.0110)	KLLoss 0.4141 (0.2486)	MaskLoss 1.0330 (0.5497)	MaskBCELoss 0.4626 (0.1424)	MaskDICELoss 0.5704 (0.4073)
Epoch: [1][ 11/500]	Time  5.903 ( 5.903)	Loss 2.3013 (1.9698)	CeLoss 0.2734 (0.5288)	SegCLSLoss 0.0205 (0.0175)	KLLoss 0.3984 (0.2779)	MaskLoss 0.9895 (0.7025)	MaskBCELoss 0.1854 (0.1019)	MaskDICELoss 0.8041 (0.6006)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 9 is less than current step: 499. Dropping entry: {'train/loss': 1.71864994764328, 'train/ce_loss': 0.58916015625, 'train/seg_cls_loss': 0.011029052734375, 'train/kl_loss': 0.2486328125, 'train/mask_bce_loss': 0.14237553533166647, 'train/mask_dice_loss': 0.4072814643383026, 'train/mask_loss': 0.5496570050716401, 'metrics/total_secs_per_batch': 4.8431007862091064, 'metrics/data_secs_per_batch': 2.2348371505737306, '_timestamp': 1740954061.2506845}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 9 is less than current step: 499. Dropping entry: {'train/lr': 0.00024385714285714283, '_timestamp': 1740954061.2509592}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 10 is less than current step: 499. Dropping entry: {'train/loss': 1.969796371459961, 'train/ce_loss': 0.52880859375, 'train/seg_cls_loss': 0.0174560546875, 'train/kl_loss': 0.2779296875, 'train/mask_bce_loss': 0.10187525264918804, 'train/mask_dice_loss': 0.600601053237915, 'train/mask_loss': 0.702476316690445, 'metrics/total_secs_per_batch': 5.903428077697754, 'metrics/data_secs_per_batch': 2.9545873641967773, '_timestamp': 1740954067.1544392}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 10 is less than current step: 499. Dropping entry: {'train/lr': 0.000243734693877551, '_timestamp': 1740954067.1547606}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 11 is less than current step: 499. Dropping entry: {'train/loss': 1.7547373533248902, 'train/ce_loss': 0.458544921875, 'train/seg_cls_loss': 0.012371826171875, 'train/kl_loss': 0.280859375, 'train/mask_bce_loss': 0.11168110384605826, 'train/mask_dice_loss': 0.5193252563476562, 'train/mask_loss': 0.6310063540935517, 'metrics/total_secs_per_batch': 6.522017002105713, 'metrics/data_secs_per_batch': 2.90430543422699, '_timestamp': 1740954073.6765006}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 11 is less than current step: 499. Dropping entry: {'train/lr': 0.00024361224489795916, '_timestamp': 1740954073.67682}).
Epoch: [1][ 12/500]	Time  6.522 ( 6.522)	Loss 2.3028 (1.7547)	CeLoss 0.2422 (0.4585)	SegCLSLoss 0.0210 (0.0124)	KLLoss 0.3965 (0.2809)	MaskLoss 1.0059 (0.6310)	MaskBCELoss 0.0065 (0.1117)	MaskDICELoss 0.9995 (0.5193)
Epoch: [1][ 13/500]	Time  5.664 ( 5.664)	Loss 2.3651 (1.5594)	CeLoss 0.2432 (0.4327)	SegCLSLoss 0.0312 (0.0147)	KLLoss 0.3965 (0.2365)	MaskLoss 1.0341 (0.5479)	MaskBCELoss 0.0387 (0.0391)	MaskDICELoss 0.9954 (0.5089)
Epoch: [1][ 14/500]	Time  7.064 ( 7.064)	Loss 0.5352 (2.0977)	CeLoss 0.5352 (0.2710)	SegCLSLoss 0.0000 (0.0251)	KLLoss 0.0000 (0.3465)	MaskLoss 0.0000 (0.8898)	MaskBCELoss 0.0000 (0.1606)	MaskDICELoss 0.0000 (0.7292)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 12 is less than current step: 499. Dropping entry: {'train/loss': 1.5594326496124267, 'train/ce_loss': 0.4327392578125, 'train/seg_cls_loss': 0.01468505859375, 'train/kl_loss': 0.2365234375, 'train/mask_bce_loss': 0.0390520740300417, 'train/mask_dice_loss': 0.5088649451732635, 'train/mask_loss': 0.5479170143604278, 'metrics/total_secs_per_batch': 5.663645029067993, 'metrics/data_secs_per_batch': 2.466316294670105, '_timestamp': 1740954079.3400514}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 12 is less than current step: 499. Dropping entry: {'train/lr': 0.00024348979591836732, '_timestamp': 1740954079.3403316}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 13 is less than current step: 499. Dropping entry: {'train/loss': 2.097663950920105, 'train/ce_loss': 0.27099609375, 'train/seg_cls_loss': 0.0251220703125, 'train/kl_loss': 0.346484375, 'train/mask_bce_loss': 0.1606449351646006, 'train/mask_dice_loss': 0.729202663898468, 'train/mask_loss': 0.8898476004600525, 'metrics/total_secs_per_batch': 7.064053058624268, 'metrics/data_secs_per_batch': 2.9532915115356446, '_timestamp': 1740954086.4042635}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 13 is less than current step: 499. Dropping entry: {'train/lr': 0.00024336734693877548, '_timestamp': 1740954086.4046159}).
Epoch: [1][ 15/500]	Time  6.665 ( 6.665)	Loss 2.3119 (2.0792)	CeLoss 0.2100 (0.3763)	SegCLSLoss 0.0255 (0.0208)	KLLoss 0.3867 (0.3131)	MaskLoss 1.0251 (0.8306)	MaskBCELoss 0.0260 (0.1688)	MaskDICELoss 0.9991 (0.6618)
Epoch: [1][ 16/500]	Time  5.927 ( 5.927)	Loss 2.2297 (1.9189)	CeLoss 0.2422 (0.5355)	SegCLSLoss 0.0192 (0.0160)	KLLoss 0.4160 (0.2758)	MaskLoss 0.9683 (0.6740)	MaskBCELoss 0.0108 (0.1331)	MaskDICELoss 0.9575 (0.5409)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 14 is less than current step: 499. Dropping entry: {'train/loss': 2.0791671514511108, 'train/ce_loss': 0.37626953125, 'train/seg_cls_loss': 0.02080078125, 'train/kl_loss': 0.3130859375, 'train/mask_bce_loss': 0.16881994158029556, 'train/mask_dice_loss': 0.661779272556305, 'train/mask_loss': 0.8305992066860199, 'metrics/total_secs_per_batch': 6.664903402328491, 'metrics/data_secs_per_batch': 3.1991225242614747, '_timestamp': 1740954093.06911}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 14 is less than current step: 499. Dropping entry: {'train/lr': 0.00024324489795918364, '_timestamp': 1740954093.0694268}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 15 is less than current step: 499. Dropping entry: {'train/loss': 1.9188977599143981, 'train/ce_loss': 0.535546875, 'train/seg_cls_loss': 0.0159912109375, 'train/kl_loss': 0.27578125, 'train/mask_bce_loss': 0.13311229003593325, 'train/mask_dice_loss': 0.5409362018108368, 'train/mask_loss': 0.6740484833717346, 'metrics/total_secs_per_batch': 5.927497148513794, 'metrics/data_secs_per_batch': 2.6174607276916504, '_timestamp': 1740954098.9965985}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 15 is less than current step: 499. Dropping entry: {'train/lr': 0.0002431224489795918, '_timestamp': 1740954098.99692}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 16 is less than current step: 499. Dropping entry: {'train/loss': 2.056428074836731, 'train/ce_loss': 0.52802734375, 'train/seg_cls_loss': 0.0173828125, 'train/kl_loss': 0.2720703125, 'train/mask_bce_loss': 0.20071784555912017, 'train/mask_dice_loss': 0.545562607049942, 'train/mask_loss': 0.74628044962883, 'metrics/total_secs_per_batch': 5.076385259628296, 'metrics/data_secs_per_batch': 2.0109259843826295, '_timestamp': 1740954104.0728505}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 16 is less than current step: 499. Dropping entry: {'train/lr': 0.000243, '_timestamp': 1740954104.0731962}).
Epoch: [1][ 17/500]	Time  5.076 ( 5.076)	Loss 2.6633 (2.0564)	CeLoss 0.2471 (0.5280)	SegCLSLoss 0.0237 (0.0174)	KLLoss 0.3750 (0.2721)	MaskLoss 1.1832 (0.7463)	MaskBCELoss 0.4567 (0.2007)	MaskDICELoss 0.7265 (0.5456)
Epoch: [1][ 18/500]	Time  5.933 ( 5.933)	Loss 1.4531 (1.8641)	CeLoss 1.4531 (0.4361)	SegCLSLoss 0.0000 (0.0218)	KLLoss 0.0000 (0.3080)	MaskLoss 0.0000 (0.6931)	MaskBCELoss 0.0000 (0.1065)	MaskDICELoss 0.0000 (0.5866)
Epoch: [1][ 19/500]	Time  7.107 ( 7.107)	Loss 2.7921 (2.4971)	CeLoss 0.1670 (0.2172)	SegCLSLoss 0.0479 (0.0312)	KLLoss 0.3750 (0.3791)	MaskLoss 1.2818 (1.1132)	MaskBCELoss 0.5543 (0.3448)	MaskDICELoss 0.7275 (0.7684)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 17 is less than current step: 499. Dropping entry: {'train/loss': 1.8640562176704407, 'train/ce_loss': 0.4361328125, 'train/seg_cls_loss': 0.02177734375, 'train/kl_loss': 0.3080078125, 'train/mask_bce_loss': 0.1064569152891636, 'train/mask_dice_loss': 0.5866063505411148, 'train/mask_loss': 0.6930632531642914, 'metrics/total_secs_per_batch': 5.933154582977295, 'metrics/data_secs_per_batch': 2.8436487197875975, '_timestamp': 1740954110.0062487}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 17 is less than current step: 499. Dropping entry: {'train/lr': 0.00024287755102040816, '_timestamp': 1740954110.0066006}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 18 is less than current step: 499. Dropping entry: {'train/loss': 2.4970895528793333, 'train/ce_loss': 0.2171875, 'train/seg_cls_loss': 0.03116455078125, 'train/kl_loss': 0.3791015625, 'train/mask_bce_loss': 0.3448018550872803, 'train/mask_dice_loss': 0.7683913558721542, 'train/mask_loss': 1.1131932139396667, 'metrics/total_secs_per_batch': 7.107266187667847, 'metrics/data_secs_per_batch': 3.148946475982666, '_timestamp': 1740954117.1133163}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 18 is less than current step: 499. Dropping entry: {'train/lr': 0.00024275510204081632, '_timestamp': 1740954117.1136153}).
[2025-03-02 16:22:03,141] [INFO] [logging.py:96:log_dist] [Rank 0] step=520, skipped=0, lr=[0.0002426938775510204], mom=[(0.9, 0.95)]
[2025-03-02 16:22:03,147] [INFO] [timer.py:215:stop] epoch=0/micro_step=5200/global_step=520, RunningAvgSamplesPerSec=1.4583227661429294, CurrSamplesPerSec=1.6575977663809396, MemAllocated=31.24GB, MaxMemAllocated=36.81GB
Epoch: [1][ 20/500]	Time  6.035 ( 6.035)	Loss 2.3681 (1.6581)	CeLoss 0.2617 (0.4246)	SegCLSLoss 0.0258 (0.0149)	KLLoss 0.3789 (0.2734)	MaskLoss 1.0278 (0.5994)	MaskBCELoss 0.1024 (0.0964)	MaskDICELoss 0.9254 (0.5030)
Epoch: [1][ 21/500]	Time  5.904 ( 5.904)	Loss 0.9219 (1.9416)	CeLoss 0.9219 (0.4552)	SegCLSLoss 0.0000 (0.0185)	KLLoss 0.0000 (0.3152)	MaskLoss 0.0000 (0.7228)	MaskBCELoss 0.0000 (0.2036)	MaskDICELoss 0.0000 (0.5192)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 19 is less than current step: 499. Dropping entry: {'train/loss': 1.6580984354019166, 'train/ce_loss': 0.424560546875, 'train/seg_cls_loss': 0.014874267578125, 'train/kl_loss': 0.2734375, 'train/mask_bce_loss': 0.09644108526408672, 'train/mask_dice_loss': 0.5029938846826554, 'train/mask_loss': 0.5994349658489228, 'metrics/total_secs_per_batch': 6.034560918807983, 'metrics/data_secs_per_batch': 2.300596904754639, '_timestamp': 1740954123.1477368}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 19 is less than current step: 499. Dropping entry: {'train/lr': 0.0002426326530612245, '_timestamp': 1740954123.148034}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 20 is less than current step: 499. Dropping entry: {'train/loss': 1.9415665149688721, 'train/ce_loss': 0.45517578125, 'train/seg_cls_loss': 0.0185302734375, 'train/kl_loss': 0.315234375, 'train/mask_bce_loss': 0.20364148691296577, 'train/mask_dice_loss': 0.5191925495862961, 'train/mask_loss': 0.7228340387344361, 'metrics/total_secs_per_batch': 5.903759002685547, 'metrics/data_secs_per_batch': 2.5291845560073853, '_timestamp': 1740954129.0516236}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 20 is less than current step: 499. Dropping entry: {'train/lr': 0.00024251020408163265, '_timestamp': 1740954129.0519147}).
Epoch: [1][ 22/500]	Time  6.767 ( 6.767)	Loss 2.3249 (1.9505)	CeLoss 0.2295 (0.4042)	SegCLSLoss 0.0183 (0.0175)	KLLoss 0.4023 (0.3592)	MaskLoss 1.0228 (0.7507)	MaskBCELoss 0.0232 (0.1402)	MaskDICELoss 0.9996 (0.6105)
Epoch: [1][ 23/500]	Time  6.312 ( 6.312)	Loss 2.0100 (2.1834)	CeLoss 0.2061 (0.2628)	SegCLSLoss 0.0287 (0.0269)	KLLoss 0.3848 (0.3932)	MaskLoss 0.8751 (0.9339)	MaskBCELoss 0.0294 (0.2378)	MaskDICELoss 0.8458 (0.6962)
Epoch: [1][ 24/500]	Time  6.046 ( 6.046)	Loss 2.1864 (1.9368)	CeLoss 0.3711 (0.3529)	SegCLSLoss 0.0136 (0.0159)	KLLoss 0.4004 (0.2801)	MaskLoss 0.8842 (0.7740)	MaskBCELoss 0.1517 (0.2440)	MaskDICELoss 0.7325 (0.5300)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 21 is less than current step: 499. Dropping entry: {'train/loss': 1.9504912972450257, 'train/ce_loss': 0.40419921875, 'train/seg_cls_loss': 0.017529296875, 'train/kl_loss': 0.3591796875, 'train/mask_bce_loss': 0.1401853334158659, 'train/mask_dice_loss': 0.610548597574234, 'train/mask_loss': 0.7507339358329773, 'metrics/total_secs_per_batch': 6.767092227935791, 'metrics/data_secs_per_batch': 3.1145777702331543, '_timestamp': 1740954135.8188102}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 21 is less than current step: 499. Dropping entry: {'train/lr': 0.0002423877551020408, '_timestamp': 1740954135.8191848}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 22 is less than current step: 499. Dropping entry: {'train/loss': 2.183432674407959, 'train/ce_loss': 0.26279296875, 'train/seg_cls_loss': 0.026898193359375, 'train/kl_loss': 0.3931640625, 'train/mask_bce_loss': 0.23775093369185923, 'train/mask_dice_loss': 0.6961529165506363, 'train/mask_loss': 0.9339038610458374, 'metrics/total_secs_per_batch': 6.312223196029663, 'metrics/data_secs_per_batch': 2.9867207765579225, '_timestamp': 1740954142.1309514}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 22 is less than current step: 499. Dropping entry: {'train/lr': 0.00024226530612244898, '_timestamp': 1740954142.1312366}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 23 is less than current step: 499. Dropping entry: {'train/loss': 1.9368436694145204, 'train/ce_loss': 0.3529296875, 'train/seg_cls_loss': 0.015911865234375, 'train/kl_loss': 0.280078125, 'train/mask_bce_loss': 0.2440296284854412, 'train/mask_dice_loss': 0.5299586206674576, 'train/mask_loss': 0.7739882469177246, 'metrics/total_secs_per_batch': 6.045827150344849, 'metrics/data_secs_per_batch': 2.9271011352539062, '_timestamp': 1740954148.1770227}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 23 is less than current step: 499. Dropping entry: {'train/lr': 0.00024214285714285714, '_timestamp': 1740954148.1773808}).
Epoch: [1][ 25/500]	Time  6.240 ( 6.240)	Loss 0.6778 (1.5104)	CeLoss 0.1963 (0.5060)	SegCLSLoss 0.0162 (0.0112)	KLLoss 0.4082 (0.2443)	MaskLoss 0.2163 (0.4871)	MaskBCELoss 0.0690 (0.1306)	MaskDICELoss 0.1473 (0.3565)
Epoch: [1][ 26/500]	Time  5.948 ( 5.948)	Loss 1.3203 (1.9043)	CeLoss 1.3203 (0.4396)	SegCLSLoss 0.0000 (0.0194)	KLLoss 0.0000 (0.2686)	MaskLoss 0.0000 (0.7142)	MaskBCELoss 0.0000 (0.1457)	MaskDICELoss 0.0000 (0.5685)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 24 is less than current step: 499. Dropping entry: {'train/loss': 1.5103847861289978, 'train/ce_loss': 0.50595703125, 'train/seg_cls_loss': 0.011175537109375, 'train/kl_loss': 0.2443359375, 'train/mask_bce_loss': 0.13063487308099866, 'train/mask_dice_loss': 0.35649110823869706, 'train/mask_loss': 0.48712599128484724, 'metrics/total_secs_per_batch': 6.239964723587036, 'metrics/data_secs_per_batch': 2.988768768310547, '_timestamp': 1740954154.416769}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 24 is less than current step: 499. Dropping entry: {'train/lr': 0.0002420204081632653, '_timestamp': 1740954154.4169717}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 25 is less than current step: 499. Dropping entry: {'train/loss': 1.9043057441711426, 'train/ce_loss': 0.43955078125, 'train/seg_cls_loss': 0.019384765625, 'train/kl_loss': 0.2685546875, 'train/mask_bce_loss': 0.145663489587605, 'train/mask_dice_loss': 0.5685011088848114, 'train/mask_loss': 0.7141646027565003, 'metrics/total_secs_per_batch': 5.948476791381836, 'metrics/data_secs_per_batch': 3.149402213096619, '_timestamp': 1740954160.3656843}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 25 is less than current step: 499. Dropping entry: {'train/lr': 0.00024189795918367344, '_timestamp': 1740954160.3661454}).
Epoch: [1][ 27/500]	Time  6.688 ( 6.688)	Loss 1.1413 (1.9342)	CeLoss 0.2539 (0.3196)	SegCLSLoss 0.0130 (0.0203)	KLLoss 0.4141 (0.3650)	MaskLoss 0.4203 (0.7840)	MaskBCELoss 0.2171 (0.1757)	MaskDICELoss 0.2032 (0.6084)
Epoch: [1][ 28/500]	Time  5.963 ( 5.963)	Loss 1.2266 (2.0999)	CeLoss 1.2266 (0.4286)	SegCLSLoss 0.0000 (0.0249)	KLLoss 0.0000 (0.3135)	MaskLoss 0.0000 (0.8137)	MaskBCELoss 0.0000 (0.1146)	MaskDICELoss 0.0000 (0.6991)
Epoch: [1][ 29/500]	Time  5.599 ( 5.599)	Loss 2.7404 (1.8060)	CeLoss 0.2168 (0.4773)	SegCLSLoss 0.0308 (0.0198)	KLLoss 0.3711 (0.2768)	MaskLoss 1.2354 (0.6455)	MaskBCELoss 0.3138 (0.1293)	MaskDICELoss 0.9216 (0.5162)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 26 is less than current step: 499. Dropping entry: {'train/loss': 1.934162437915802, 'train/ce_loss': 0.31962890625, 'train/seg_cls_loss': 0.0202880859375, 'train/kl_loss': 0.3650390625, 'train/mask_bce_loss': 0.17567124888300895, 'train/mask_dice_loss': 0.6083533346652985, 'train/mask_loss': 0.7840245842933655, 'metrics/total_secs_per_batch': 6.688360929489136, 'metrics/data_secs_per_batch': 3.2470064640045164, '_timestamp': 1740954167.0544305}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 26 is less than current step: 499. Dropping entry: {'train/lr': 0.0002417755102040816, '_timestamp': 1740954167.0549822}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 27 is less than current step: 499. Dropping entry: {'train/loss': 2.099893093109131, 'train/ce_loss': 0.42861328125, 'train/seg_cls_loss': 0.0248779296875, 'train/kl_loss': 0.3134765625, 'train/mask_bce_loss': 0.11461418503895401, 'train/mask_dice_loss': 0.6991018712520599, 'train/mask_loss': 0.813716059923172, 'metrics/total_secs_per_batch': 5.963012456893921, 'metrics/data_secs_per_batch': 2.8602074861526487, '_timestamp': 1740954173.0167732}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 27 is less than current step: 499. Dropping entry: {'train/lr': 0.00024165306122448976, '_timestamp': 1740954173.0170875}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 28 is less than current step: 499. Dropping entry: {'train/loss': 1.8060454368591308, 'train/ce_loss': 0.47734375, 'train/seg_cls_loss': 0.019842529296875, 'train/kl_loss': 0.2767578125, 'train/mask_bce_loss': 0.12926921136677266, 'train/mask_dice_loss': 0.5162339776754379, 'train/mask_loss': 0.6455031871795655, 'metrics/total_secs_per_batch': 5.599447250366211, 'metrics/data_secs_per_batch': 2.528916621208191, '_timestamp': 1740954178.616296}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 28 is less than current step: 499. Dropping entry: {'train/lr': 0.00024153061224489793, '_timestamp': 1740954178.6166573}).
[2025-03-02 16:23:05,363] [INFO] [logging.py:96:log_dist] [Rank 0] step=530, skipped=0, lr=[0.00024146938775510202], mom=[(0.9, 0.95)]
[2025-03-02 16:23:05,369] [INFO] [timer.py:215:stop] epoch=0/micro_step=5300/global_step=530, RunningAvgSamplesPerSec=1.4608911029754468, CurrSamplesPerSec=1.4810823278723235, MemAllocated=31.66GB, MaxMemAllocated=36.81GB
Epoch: [1][ 30/500]	Time  6.754 ( 6.754)	Loss 2.0334 (1.8432)	CeLoss 0.2217 (0.3220)	SegCLSLoss 0.0166 (0.0227)	KLLoss 0.4023 (0.3209)	MaskLoss 0.8810 (0.7389)	MaskBCELoss 0.0485 (0.1921)	MaskDICELoss 0.8324 (0.5468)
Epoch: [1][ 31/500]	Time  7.053 ( 7.053)	Loss 2.5589 (1.9130)	CeLoss 0.1748 (0.3230)	SegCLSLoss 0.0344 (0.0183)	KLLoss 0.3809 (0.3176)	MaskLoss 1.1642 (0.7745)	MaskBCELoss 0.3255 (0.1746)	MaskDICELoss 0.8387 (0.5999)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 29 is less than current step: 499. Dropping entry: {'train/loss': 1.8431985259056092, 'train/ce_loss': 0.32197265625, 'train/seg_cls_loss': 0.022747802734375, 'train/kl_loss': 0.3208984375, 'train/mask_bce_loss': 0.19207218212541194, 'train/mask_dice_loss': 0.5468122154474259, 'train/mask_loss': 0.7388844102621078, 'metrics/total_secs_per_batch': 6.753739595413208, 'metrics/data_secs_per_batch': 2.897567629814148, '_timestamp': 1740954185.3696513}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 29 is less than current step: 499. Dropping entry: {'train/lr': 0.0002414081632653061, '_timestamp': 1740954185.3699725}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 30 is less than current step: 499. Dropping entry: {'train/loss': 1.913046646118164, 'train/ce_loss': 0.323046875, 'train/seg_cls_loss': 0.018328857421875, 'train/kl_loss': 0.317578125, 'train/mask_bce_loss': 0.17463776599615813, 'train/mask_dice_loss': 0.5999031335115432, 'train/mask_loss': 0.7745408952236176, 'metrics/total_secs_per_batch': 7.053039789199829, 'metrics/data_secs_per_batch': 3.1705960273742675, '_timestamp': 1740954192.4233825}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 30 is less than current step: 499. Dropping entry: {'train/lr': 0.00024128571428571425, '_timestamp': 1740954192.4238966}).
Epoch: [1][ 32/500]	Time  6.459 ( 6.459)	Loss 2.7911 (1.9409)	CeLoss 0.2002 (0.3818)	SegCLSLoss 0.0181 (0.0166)	KLLoss 0.3906 (0.2799)	MaskLoss 1.2715 (0.7614)	MaskBCELoss 0.4047 (0.2071)	MaskDICELoss 0.8669 (0.5544)
Epoch: [1][ 33/500]	Time  6.764 ( 6.764)	Loss 1.8215 (1.9070)	CeLoss 0.2578 (0.3588)	SegCLSLoss 0.0137 (0.0202)	KLLoss 0.4277 (0.3660)	MaskLoss 0.7565 (0.7508)	MaskBCELoss 0.0981 (0.1362)	MaskDICELoss 0.6584 (0.6146)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 31 is less than current step: 499. Dropping entry: {'train/loss': 1.9409005403518678, 'train/ce_loss': 0.381787109375, 'train/seg_cls_loss': 0.016607666015625, 'train/kl_loss': 0.2798828125, 'train/mask_bce_loss': 0.20706106908619404, 'train/mask_dice_loss': 0.5543804168701172, 'train/mask_loss': 0.7614414870738984, 'metrics/total_secs_per_batch': 6.4592156410217285, 'metrics/data_secs_per_batch': 2.8224246501922607, '_timestamp': 1740954198.882427}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 31 is less than current step: 499. Dropping entry: {'train/lr': 0.00024116326530612242, '_timestamp': 1740954198.882983}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 32 is less than current step: 499. Dropping entry: {'train/loss': 1.9070461511611938, 'train/ce_loss': 0.3587890625, 'train/seg_cls_loss': 0.0202392578125, 'train/kl_loss': 0.366015625, 'train/mask_bce_loss': 0.13615793203935028, 'train/mask_dice_loss': 0.6146307736635208, 'train/mask_loss': 0.7507886946201324, 'metrics/total_secs_per_batch': 6.764271974563599, 'metrics/data_secs_per_batch': 2.8826859474182127, '_timestamp': 1740954205.6467528}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 32 is less than current step: 499. Dropping entry: {'train/lr': 0.00024104081632653058, '_timestamp': 1740954205.6471915}).
Epoch: [1][ 34/500]	Time  6.607 ( 6.607)	Loss 2.2104 (2.5930)	CeLoss 0.3770 (0.2714)	SegCLSLoss 0.0124 (0.0194)	KLLoss 0.4219 (0.4072)	MaskLoss 0.8923 (1.1356)	MaskBCELoss 0.4249 (0.4347)	MaskDICELoss 0.4675 (0.7008)
Epoch: [1][ 35/500]	Time  7.087 ( 7.087)	Loss 1.8365 (1.7910)	CeLoss 0.2559 (0.2181)	SegCLSLoss 0.0154 (0.0212)	KLLoss 0.4258 (0.3238)	MaskLoss 0.7649 (0.7648)	MaskBCELoss 0.1331 (0.0637)	MaskDICELoss 0.6318 (0.7011)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 33 is less than current step: 499. Dropping entry: {'train/loss': 2.5930320739746096, 'train/ce_loss': 0.27138671875, 'train/seg_cls_loss': 0.019366455078125, 'train/kl_loss': 0.4072265625, 'train/mask_bce_loss': 0.43474199399352076, 'train/mask_dice_loss': 0.7008365333080292, 'train/mask_loss': 1.1355785310268403, 'metrics/total_secs_per_batch': 6.606500864028931, 'metrics/data_secs_per_batch': 2.873429036140442, '_timestamp': 1740954212.2529898}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 33 is less than current step: 499. Dropping entry: {'train/lr': 0.00024091836734693874, '_timestamp': 1740954212.2533336}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 34 is less than current step: 499. Dropping entry: {'train/loss': 1.7909838676452636, 'train/ce_loss': 0.218115234375, 'train/seg_cls_loss': 0.0211669921875, 'train/kl_loss': 0.323828125, 'train/mask_bce_loss': 0.06368661485612392, 'train/mask_dice_loss': 0.701116818189621, 'train/mask_loss': 0.7648034274578095, 'metrics/total_secs_per_batch': 7.087302923202515, 'metrics/data_secs_per_batch': 3.2694096326828004, '_timestamp': 1740954219.3402014}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 34 is less than current step: 499. Dropping entry: {'train/lr': 0.0002407959183673469, '_timestamp': 1740954219.340494}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 35 is less than current step: 499. Dropping entry: {'train/loss': 1.6218737244606019, 'train/ce_loss': 0.75966796875, 'train/seg_cls_loss': 0.0084228515625, 'train/kl_loss': 0.21171875, 'train/mask_bce_loss': 0.12501681819558144, 'train/mask_dice_loss': 0.29334190785884856, 'train/mask_loss': 0.41835872530937196, 'metrics/total_secs_per_batch': 5.044126510620117, 'metrics/data_secs_per_batch': 2.431603479385376, '_timestamp': 1740954224.384307}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 35 is less than current step: 499. Dropping entry: {'train/lr': 0.00024067346938775507, '_timestamp': 1740954224.3845992}).
Epoch: [1][ 36/500]	Time  5.044 ( 5.044)	Loss 1.6016 (1.6219)	CeLoss 1.6016 (0.7597)	SegCLSLoss 0.0000 (0.0084)	KLLoss 0.0000 (0.2117)	MaskLoss 0.0000 (0.4184)	MaskBCELoss 0.0000 (0.1250)	MaskDICELoss 0.0000 (0.2933)
Epoch: [1][ 37/500]	Time  6.170 ( 6.170)	Loss 2.4926 (1.8430)	CeLoss 0.1826 (0.4779)	SegCLSLoss 0.0284 (0.0179)	KLLoss 0.3750 (0.2789)	MaskLoss 1.1291 (0.6641)	MaskBCELoss 0.2221 (0.1427)	MaskDICELoss 0.9070 (0.5213)
Epoch: [1][ 38/500]	Time  7.160 ( 7.160)	Loss 2.2947 (2.0395)	CeLoss 0.2217 (0.4496)	SegCLSLoss 0.0278 (0.0196)	KLLoss 0.3965 (0.3250)	MaskLoss 1.0096 (0.7738)	MaskBCELoss 0.0468 (0.1188)	MaskDICELoss 0.9628 (0.6550)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 36 is less than current step: 499. Dropping entry: {'train/loss': 1.8429917573928833, 'train/ce_loss': 0.4779296875, 'train/seg_cls_loss': 0.017926025390625, 'train/kl_loss': 0.27890625, 'train/mask_bce_loss': 0.14272460490465164, 'train/mask_dice_loss': 0.5213494002819061, 'train/mask_loss': 0.6640740156173706, 'metrics/total_secs_per_batch': 6.169791221618652, 'metrics/data_secs_per_batch': 2.7485172510147096, '_timestamp': 1740954230.5543315}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 36 is less than current step: 499. Dropping entry: {'train/lr': 0.00024055102040816323, '_timestamp': 1740954230.5545661}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 37 is less than current step: 499. Dropping entry: {'train/loss': 2.0395219326019287, 'train/ce_loss': 0.449609375, 'train/seg_cls_loss': 0.0195556640625, 'train/kl_loss': 0.325, 'train/mask_bce_loss': 0.11877881158143282, 'train/mask_dice_loss': 0.6549860775470734, 'train/mask_loss': 0.7737648844718933, 'metrics/total_secs_per_batch': 7.160128116607666, 'metrics/data_secs_per_batch': 3.4015318632125853, '_timestamp': 1740954237.7142315}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 37 is less than current step: 499. Dropping entry: {'train/lr': 0.0002404285714285714, '_timestamp': 1740954237.7145214}).
Epoch: [1][ 39/500]	Time  5.331 ( 5.331)	Loss 2.5077 (1.8174)	CeLoss 0.2471 (0.4713)	SegCLSLoss 0.0183 (0.0171)	KLLoss 0.4277 (0.2408)	MaskLoss 1.1044 (0.6567)	MaskBCELoss 0.1392 (0.1265)	MaskDICELoss 0.9652 (0.5302)
[2025-03-02 16:24:10,136] [INFO] [logging.py:96:log_dist] [Rank 0] step=540, skipped=0, lr=[0.00024024489795918368], mom=[(0.9, 0.95)]
[2025-03-02 16:24:10,143] [INFO] [timer.py:215:stop] epoch=0/micro_step=5400/global_step=540, RunningAvgSamplesPerSec=1.462357295785908, CurrSamplesPerSec=1.4091374630879947, MemAllocated=31.25GB, MaxMemAllocated=36.81GB
Epoch: [1][ 40/500]	Time  7.098 ( 7.098)	Loss 2.5600 (2.0627)	CeLoss 0.2080 (0.3244)	SegCLSLoss 0.0306 (0.0201)	KLLoss 0.3906 (0.3727)	MaskLoss 1.1491 (0.8454)	MaskBCELoss 0.2600 (0.1508)	MaskDICELoss 0.8891 (0.6946)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 38 is less than current step: 499. Dropping entry: {'train/loss': 1.817383885383606, 'train/ce_loss': 0.4712890625, 'train/seg_cls_loss': 0.01707763671875, 'train/kl_loss': 0.2408203125, 'train/mask_bce_loss': 0.12651805505156516, 'train/mask_dice_loss': 0.530171924829483, 'train/mask_loss': 0.656689989566803, 'metrics/total_secs_per_batch': 5.331357955932617, 'metrics/data_secs_per_batch': 2.707603120803833, '_timestamp': 1740954243.0456433}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 38 is less than current step: 499. Dropping entry: {'train/lr': 0.00024030612244897956, '_timestamp': 1740954243.0458417}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 39 is less than current step: 499. Dropping entry: {'train/loss': 2.062711572647095, 'train/ce_loss': 0.3244140625, 'train/seg_cls_loss': 0.02010498046875, 'train/kl_loss': 0.37265625, 'train/mask_bce_loss': 0.15076903477311135, 'train/mask_dice_loss': 0.6946492373943329, 'train/mask_loss': 0.8454182803630829, 'metrics/total_secs_per_batch': 7.098133325576782, 'metrics/data_secs_per_batch': 3.1681862831115724, '_timestamp': 1740954250.1435351}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 39 is less than current step: 499. Dropping entry: {'train/lr': 0.00024018367346938775, '_timestamp': 1740954250.143814}).
Epoch: [1][ 41/500]	Time  5.721 ( 5.721)	Loss 2.8412 (1.7559)	CeLoss 0.5430 (0.3433)	SegCLSLoss 0.0177 (0.0159)	KLLoss 0.4238 (0.2873)	MaskLoss 1.1237 (0.6880)	MaskBCELoss 0.2765 (0.1635)	MaskDICELoss 0.8473 (0.5245)
Epoch: [1][ 42/500]	Time  6.209 ( 6.209)	Loss 2.5123 (2.1283)	CeLoss 0.1484 (0.3135)	SegCLSLoss 0.0732 (0.0298)	KLLoss 0.3730 (0.3576)	MaskLoss 1.1448 (0.8819)	MaskBCELoss 0.3223 (0.2174)	MaskDICELoss 0.8225 (0.6645)
Epoch: [1][ 43/500]	Time  5.603 ( 5.603)	Loss 1.6250 (2.0226)	CeLoss 1.6250 (0.4288)	SegCLSLoss 0.0000 (0.0160)	KLLoss 0.0000 (0.3322)	MaskLoss 0.0000 (0.7765)	MaskBCELoss 0.0000 (0.1176)	MaskDICELoss 0.0000 (0.6588)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 40 is less than current step: 499. Dropping entry: {'train/loss': 1.7559150099754333, 'train/ce_loss': 0.343310546875, 'train/seg_cls_loss': 0.015869140625, 'train/kl_loss': 0.2873046875, 'train/mask_bce_loss': 0.16346017569303511, 'train/mask_dice_loss': 0.5245315194129944, 'train/mask_loss': 0.6879916846752167, 'metrics/total_secs_per_batch': 5.721435070037842, 'metrics/data_secs_per_batch': 2.5062729835510256, '_timestamp': 1740954255.8653746}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 40 is less than current step: 499. Dropping entry: {'train/lr': 0.0002400612244897959, '_timestamp': 1740954255.8657176}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 41 is less than current step: 499. Dropping entry: {'train/loss': 2.128273630142212, 'train/ce_loss': 0.3134765625, 'train/seg_cls_loss': 0.02979736328125, 'train/kl_loss': 0.3576171875, 'train/mask_bce_loss': 0.21737318877130746, 'train/mask_dice_loss': 0.6645370632410049, 'train/mask_loss': 0.881910252571106, 'metrics/total_secs_per_batch': 6.2089831829071045, 'metrics/data_secs_per_batch': 2.6983594417572023, '_timestamp': 1740954262.0743303}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 41 is less than current step: 499. Dropping entry: {'train/lr': 0.00023993877551020407, '_timestamp': 1740954262.0746489}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 42 is less than current step: 499. Dropping entry: {'train/loss': 2.0226463794708254, 'train/ce_loss': 0.42880859375, 'train/seg_cls_loss': 0.0160400390625, 'train/kl_loss': 0.3322265625, 'train/mask_bce_loss': 0.11762765310704708, 'train/mask_dice_loss': 0.6588322579860687, 'train/mask_loss': 0.7764599025249481, 'metrics/total_secs_per_batch': 5.602668285369873, 'metrics/data_secs_per_batch': 2.798364353179932, '_timestamp': 1740954267.6768446}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 42 is less than current step: 499. Dropping entry: {'train/lr': 0.00023981632653061224, '_timestamp': 1740954267.6771212}).
Epoch: [1][ 44/500]	Time  6.963 ( 6.963)	Loss 2.1476 (1.9756)	CeLoss 0.1875 (0.2230)	SegCLSLoss 0.0271 (0.0258)	KLLoss 0.4102 (0.4117)	MaskLoss 0.9527 (0.8491)	MaskBCELoss 0.1566 (0.2000)	MaskDICELoss 0.7961 (0.6491)
Epoch: [1][ 45/500]	Time  5.935 ( 5.935)	Loss 2.4952 (1.9582)	CeLoss 0.1689 (0.5912)	SegCLSLoss 0.0369 (0.0133)	KLLoss 0.3809 (0.2906)	MaskLoss 1.1348 (0.6656)	MaskBCELoss 0.2935 (0.1375)	MaskDICELoss 0.8413 (0.5280)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 43 is less than current step: 499. Dropping entry: {'train/loss': 1.97556734085083, 'train/ce_loss': 0.223046875, 'train/seg_cls_loss': 0.025775146484375, 'train/kl_loss': 0.41171875, 'train/mask_bce_loss': 0.20003863014280795, 'train/mask_dice_loss': 0.6490731745958328, 'train/mask_loss': 0.849111795425415, 'metrics/total_secs_per_batch': 6.963143348693848, 'metrics/data_secs_per_batch': 2.8017812252044676, '_timestamp': 1740954274.6399791}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 43 is less than current step: 499. Dropping entry: {'train/lr': 0.0002396938775510204, '_timestamp': 1740954274.640286}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 44 is less than current step: 499. Dropping entry: {'train/loss': 1.9582093954086304, 'train/ce_loss': 0.5912109375, 'train/seg_cls_loss': 0.01328125, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.13753918334841728, 'train/mask_dice_loss': 0.5280401349067688, 'train/mask_loss': 0.6655793190002441, 'metrics/total_secs_per_batch': 5.935327053070068, 'metrics/data_secs_per_batch': 2.389571499824524, '_timestamp': 1740954280.5753038}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 44 is less than current step: 499. Dropping entry: {'train/lr': 0.00023957142857142856, '_timestamp': 1740954280.5755734}).
Epoch: [1][ 46/500]	Time  6.751 ( 6.751)	Loss 1.7168 (2.0704)	CeLoss 0.2832 (0.2757)	SegCLSLoss 0.0223 (0.0298)	KLLoss 0.4219 (0.3979)	MaskLoss 0.6895 (0.8700)	MaskBCELoss 0.1117 (0.1816)	MaskDICELoss 0.5778 (0.6884)
Epoch: [1][ 47/500]	Time  6.582 ( 6.582)	Loss 2.6121 (1.7982)	CeLoss 0.1699 (0.3941)	SegCLSLoss 0.0325 (0.0159)	KLLoss 0.3711 (0.2865)	MaskLoss 1.1942 (0.6837)	MaskBCELoss 0.3226 (0.1849)	MaskDICELoss 0.8717 (0.4988)
Epoch: [1][ 48/500]	Time  5.144 ( 5.144)	Loss 2.2539 (1.9354)	CeLoss 0.2617 (0.2702)	SegCLSLoss 0.0253 (0.0317)	KLLoss 0.4062 (0.3121)	MaskLoss 0.9697 (0.8090)	MaskBCELoss 0.0544 (0.2148)	MaskDICELoss 0.9153 (0.5942)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 45 is less than current step: 499. Dropping entry: {'train/loss': 2.0703508853912354, 'train/ce_loss': 0.275732421875, 'train/seg_cls_loss': 0.0298095703125, 'train/kl_loss': 0.3978515625, 'train/mask_bce_loss': 0.18155857995152475, 'train/mask_dice_loss': 0.6884313315153122, 'train/mask_loss': 0.8699899107217789, 'metrics/total_secs_per_batch': 6.750546932220459, 'metrics/data_secs_per_batch': 2.7521740198135376, '_timestamp': 1740954287.3258767}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 45 is less than current step: 499. Dropping entry: {'train/lr': 0.00023944897959183673, '_timestamp': 1740954287.3261662}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 46 is less than current step: 499. Dropping entry: {'train/loss': 1.7981793522834777, 'train/ce_loss': 0.394140625, 'train/seg_cls_loss': 0.015863037109375, 'train/kl_loss': 0.2865234375, 'train/mask_bce_loss': 0.18489091843366623, 'train/mask_dice_loss': 0.4988178968429565, 'train/mask_loss': 0.6837088167667389, 'metrics/total_secs_per_batch': 6.581998348236084, 'metrics/data_secs_per_batch': 2.814798855781555, '_timestamp': 1740954293.9081774}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 46 is less than current step: 499. Dropping entry: {'train/lr': 0.0002393265306122449, '_timestamp': 1740954293.9085555}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 47 is less than current step: 499. Dropping entry: {'train/loss': 1.935409963130951, 'train/ce_loss': 0.2701904296875, 'train/seg_cls_loss': 0.03167724609375, 'train/kl_loss': 0.312109375, 'train/mask_bce_loss': 0.21482594385743142, 'train/mask_dice_loss': 0.5941998511552811, 'train/mask_loss': 0.8090257823467255, 'metrics/total_secs_per_batch': 5.14431357383728, 'metrics/data_secs_per_batch': 2.003354215621948, '_timestamp': 1740954299.052172}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 47 is less than current step: 499. Dropping entry: {'train/lr': 0.00023920408163265305, '_timestamp': 1740954299.0523713}).
Epoch: [1][ 49/500]	Time  7.077 ( 7.077)	Loss 2.1088 (1.7624)	CeLoss 0.2432 (0.3912)	SegCLSLoss 0.0278 (0.0233)	KLLoss 0.3867 (0.3160)	MaskLoss 0.9060 (0.6640)	MaskBCELoss 0.0696 (0.1673)	MaskDICELoss 0.8364 (0.4967)
[2025-03-02 16:25:11,497] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=0, lr=[0.00023902040816326528], mom=[(0.9, 0.95)]
[2025-03-02 16:25:11,503] [INFO] [timer.py:215:stop] epoch=0/micro_step=5500/global_step=550, RunningAvgSamplesPerSec=1.4651076587908594, CurrSamplesPerSec=1.8607360012833445, MemAllocated=31.25GB, MaxMemAllocated=36.81GB
Epoch: [1][ 50/500]	Time  5.376 ( 5.376)	Loss 2.3254 (2.0102)	CeLoss 0.2109 (0.5087)	SegCLSLoss 0.0261 (0.0188)	KLLoss 0.3848 (0.2756)	MaskLoss 1.0318 (0.7323)	MaskBCELoss 0.0617 (0.1592)	MaskDICELoss 0.9701 (0.5731)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 48 is less than current step: 499. Dropping entry: {'train/loss': 1.762397539615631, 'train/ce_loss': 0.3912109375, 'train/seg_cls_loss': 0.0233154296875, 'train/kl_loss': 0.316015625, 'train/mask_bce_loss': 0.16731001771986484, 'train/mask_dice_loss': 0.49670123755931855, 'train/mask_loss': 0.6640112608671188, 'metrics/total_secs_per_batch': 7.076573133468628, 'metrics/data_secs_per_batch': 3.221007800102234, '_timestamp': 1740954306.1287584}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 48 is less than current step: 499. Dropping entry: {'train/lr': 0.00023908163265306122, '_timestamp': 1740954306.1289577}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 49 is less than current step: 499. Dropping entry: {'train/loss': 2.0101694107055663, 'train/ce_loss': 0.50869140625, 'train/seg_cls_loss': 0.01883544921875, 'train/kl_loss': 0.2755859375, 'train/mask_bce_loss': 0.15921126045286654, 'train/mask_dice_loss': 0.5731195449829102, 'train/mask_loss': 0.7323308110237121, 'metrics/total_secs_per_batch': 5.375750541687012, 'metrics/data_secs_per_batch': 2.2277783393859862, '_timestamp': 1740954311.5043187}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 49 is less than current step: 499. Dropping entry: {'train/lr': 0.00023895918367346938, '_timestamp': 1740954311.5045943}).
Epoch: [1][ 51/500]	Time  6.205 ( 6.205)	Loss 1.4688 (1.9821)	CeLoss 1.4688 (0.5578)	SegCLSLoss 0.0000 (0.0208)	KLLoss 0.0000 (0.2816)	MaskLoss 0.0000 (0.6929)	MaskBCELoss 0.0000 (0.1273)	MaskDICELoss 0.0000 (0.5656)
Epoch: [1][ 52/500]	Time  6.513 ( 6.513)	Loss 2.0970 (1.7772)	CeLoss 0.2461 (0.3789)	SegCLSLoss 0.0552 (0.0281)	KLLoss 0.3730 (0.3209)	MaskLoss 0.8932 (0.6762)	MaskBCELoss 0.2398 (0.1355)	MaskDICELoss 0.6534 (0.5407)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 50 is less than current step: 499. Dropping entry: {'train/loss': 1.9821025848388671, 'train/ce_loss': 0.5578125, 'train/seg_cls_loss': 0.02081298828125, 'train/kl_loss': 0.281640625, 'train/mask_bce_loss': 0.12727322783321143, 'train/mask_dice_loss': 0.5656335055828094, 'train/mask_loss': 0.6929067313671112, 'metrics/total_secs_per_batch': 6.204767942428589, 'metrics/data_secs_per_batch': 3.028395485877991, '_timestamp': 1740954317.709246}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 50 is less than current step: 499. Dropping entry: {'train/lr': 0.00023883673469387754, '_timestamp': 1740954317.709527}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 51 is less than current step: 499. Dropping entry: {'train/loss': 1.7772189617156982, 'train/ce_loss': 0.37890625, 'train/seg_cls_loss': 0.028070068359375, 'train/kl_loss': 0.3208984375, 'train/mask_bce_loss': 0.13547177352011203, 'train/mask_dice_loss': 0.5407353639602661, 'train/mask_loss': 0.6762071311473846, 'metrics/total_secs_per_batch': 6.512983798980713, 'metrics/data_secs_per_batch': 2.9134530067443847, '_timestamp': 1740954324.2223125}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 51 is less than current step: 499. Dropping entry: {'train/lr': 0.0002387142857142857, '_timestamp': 1740954324.2225945}).
Epoch: [1][ 53/500]	Time  6.610 ( 6.610)	Loss 2.4578 (1.3235)	CeLoss 0.1855 (0.4081)	SegCLSLoss 0.0498 (0.0142)	KLLoss 0.3711 (0.2432)	MaskLoss 1.1049 (0.4420)	MaskBCELoss 0.3364 (0.0987)	MaskDICELoss 0.7685 (0.3433)
Epoch: [1][ 54/500]	Time  5.350 ( 5.350)	Loss 2.6878 (2.0289)	CeLoss 0.2578 (0.5197)	SegCLSLoss 0.0303 (0.0151)	KLLoss 0.3828 (0.2822)	MaskLoss 1.1877 (0.7367)	MaskBCELoss 0.4124 (0.2799)	MaskDICELoss 0.7753 (0.4568)
Epoch: [1][ 55/500]	Time  5.436 ( 5.436)	Loss 2.2268 (1.7924)	CeLoss 0.2539 (0.4898)	SegCLSLoss 0.0250 (0.0180)	KLLoss 0.3945 (0.2828)	MaskLoss 0.9611 (0.6327)	MaskBCELoss 0.0700 (0.1154)	MaskDICELoss 0.8911 (0.5173)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 52 is less than current step: 499. Dropping entry: {'train/loss': 1.323462474346161, 'train/ce_loss': 0.40810546875, 'train/seg_cls_loss': 0.014166259765625, 'train/kl_loss': 0.2431640625, 'train/mask_bce_loss': 0.09874167442321777, 'train/mask_dice_loss': 0.34326300024986267, 'train/mask_loss': 0.44200467467308047, 'metrics/total_secs_per_batch': 6.610088586807251, 'metrics/data_secs_per_batch': 3.172960305213928, '_timestamp': 1740954330.8323362}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 52 is less than current step: 499. Dropping entry: {'train/lr': 0.00023859183673469384, '_timestamp': 1740954330.8325326}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 53 is less than current step: 499. Dropping entry: {'train/loss': 2.028901469707489, 'train/ce_loss': 0.5197265625, 'train/seg_cls_loss': 0.015069580078125, 'train/kl_loss': 0.2822265625, 'train/mask_bce_loss': 0.2798813238739967, 'train/mask_dice_loss': 0.4568350285291672, 'train/mask_loss': 0.7367163598537445, 'metrics/total_secs_per_batch': 5.350369453430176, 'metrics/data_secs_per_batch': 2.1989063262939452, '_timestamp': 1740954336.1827242}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 53 is less than current step: 499. Dropping entry: {'train/lr': 0.000238469387755102, '_timestamp': 1740954336.183006}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 54 is less than current step: 499. Dropping entry: {'train/loss': 1.7923670172691346, 'train/ce_loss': 0.48984375, 'train/seg_cls_loss': 0.018017578125, 'train/kl_loss': 0.2828125, 'train/mask_bce_loss': 0.11537104323506356, 'train/mask_dice_loss': 0.5173358857631684, 'train/mask_loss': 0.6327069222927093, 'metrics/total_secs_per_batch': 5.436099290847778, 'metrics/data_secs_per_batch': 2.2184431314468385, '_timestamp': 1740954341.618823}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 54 is less than current step: 499. Dropping entry: {'train/lr': 0.00023834693877551017, '_timestamp': 1740954341.6191027}).
Epoch: [1][ 56/500]	Time  4.916 ( 4.916)	Loss 0.8750 (1.9829)	CeLoss 0.8750 (0.3813)	SegCLSLoss 0.0000 (0.0259)	KLLoss 0.0000 (0.3146)	MaskLoss 0.0000 (0.7786)	MaskBCELoss 0.0000 (0.1720)	MaskDICELoss 0.0000 (0.6066)
Epoch: [1][ 57/500]	Time  6.068 ( 6.068)	Loss 2.2267 (2.1636)	CeLoss 0.2793 (0.5001)	SegCLSLoss 0.0256 (0.0258)	KLLoss 0.3945 (0.3154)	MaskLoss 0.9474 (0.8095)	MaskBCELoss 0.0238 (0.2308)	MaskDICELoss 0.9236 (0.5788)
Epoch: [1][ 58/500]	Time  6.574 ( 6.574)	Loss 1.9608 (2.1342)	CeLoss 0.2598 (0.3781)	SegCLSLoss 0.0227 (0.0245)	KLLoss 0.3984 (0.3607)	MaskLoss 0.8251 (0.8539)	MaskBCELoss 0.0576 (0.1441)	MaskDICELoss 0.7676 (0.7098)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 55 is less than current step: 499. Dropping entry: {'train/loss': 1.9829475641250611, 'train/ce_loss': 0.38134765625, 'train/seg_cls_loss': 0.025927734375, 'train/kl_loss': 0.3146484375, 'train/mask_bce_loss': 0.17200291361659764, 'train/mask_dice_loss': 0.6066290676593781, 'train/mask_loss': 0.778631991147995, 'metrics/total_secs_per_batch': 4.916388273239136, 'metrics/data_secs_per_batch': 2.3306021451950074, '_timestamp': 1740954346.5354097}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 55 is less than current step: 499. Dropping entry: {'train/lr': 0.00023822448979591833, '_timestamp': 1740954346.5357478}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 56 is less than current step: 499. Dropping entry: {'train/loss': 2.163622498512268, 'train/ce_loss': 0.50009765625, 'train/seg_cls_loss': 0.02578125, 'train/kl_loss': 0.3154296875, 'train/mask_bce_loss': 0.23078966811299323, 'train/mask_dice_loss': 0.5787559598684311, 'train/mask_loss': 0.8095456123352051, 'metrics/total_secs_per_batch': 6.068238973617554, 'metrics/data_secs_per_batch': 2.665115737915039, '_timestamp': 1740954352.603505}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 56 is less than current step: 499. Dropping entry: {'train/lr': 0.0002381020408163265, '_timestamp': 1740954352.6037927}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 57 is less than current step: 499. Dropping entry: {'train/loss': 2.1341817498207094, 'train/ce_loss': 0.378125, 'train/seg_cls_loss': 0.024493408203125, 'train/kl_loss': 0.3607421875, 'train/mask_bce_loss': 0.14414400979876518, 'train/mask_dice_loss': 0.7097632646560669, 'train/mask_loss': 0.8539072751998902, 'metrics/total_secs_per_batch': 6.574410676956177, 'metrics/data_secs_per_batch': 2.9745906352996827, '_timestamp': 1740954359.1778767}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 57 is less than current step: 499. Dropping entry: {'train/lr': 0.00023797959183673466, '_timestamp': 1740954359.1781545}).
Epoch: [1][ 59/500]	Time  6.336 ( 6.336)	Loss 2.9111 (2.0771)	CeLoss 0.1572 (0.3291)	SegCLSLoss 0.0461 (0.0223)	KLLoss 0.3770 (0.3625)	MaskLoss 1.3462 (0.8503)	MaskBCELoss 0.3669 (0.1478)	MaskDICELoss 0.9793 (0.7025)
[2025-03-02 16:26:11,140] [INFO] [logging.py:96:log_dist] [Rank 0] step=560, skipped=0, lr=[0.00023779591836734691], mom=[(0.9, 0.95)]
[2025-03-02 16:26:11,145] [INFO] [timer.py:215:stop] epoch=0/micro_step=5600/global_step=560, RunningAvgSamplesPerSec=1.4684330603836477, CurrSamplesPerSec=1.7759871840097199, MemAllocated=31.08GB, MaxMemAllocated=36.81GB
Epoch: [1][ 60/500]	Time  5.632 ( 5.632)	Loss 1.1359 (1.8290)	CeLoss 0.3516 (0.5098)	SegCLSLoss 0.0205 (0.0208)	KLLoss 0.4043 (0.3148)	MaskLoss 0.3668 (0.6386)	MaskBCELoss 0.1983 (0.1250)	MaskDICELoss 0.1685 (0.5137)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 58 is less than current step: 499. Dropping entry: {'train/loss': 2.077110838890076, 'train/ce_loss': 0.3291015625, 'train/seg_cls_loss': 0.022283935546875, 'train/kl_loss': 0.3625, 'train/mask_bce_loss': 0.14777911342680455, 'train/mask_dice_loss': 0.7024950534105301, 'train/mask_loss': 0.8502741754055023, 'metrics/total_secs_per_batch': 6.335808753967285, 'metrics/data_secs_per_batch': 2.931700873374939, '_timestamp': 1740954365.5137343}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 58 is less than current step: 499. Dropping entry: {'train/lr': 0.00023785714285714282, '_timestamp': 1740954365.514032}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 59 is less than current step: 499. Dropping entry: {'train/loss': 1.8290214657783508, 'train/ce_loss': 0.509765625, 'train/seg_cls_loss': 0.02083740234375, 'train/kl_loss': 0.31484375, 'train/mask_bce_loss': 0.12495459876954555, 'train/mask_dice_loss': 0.5136772379279136, 'train/mask_loss': 0.6386318325996398, 'metrics/total_secs_per_batch': 5.63235330581665, 'metrics/data_secs_per_batch': 2.5644540309906008, '_timestamp': 1740954371.145867}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 59 is less than current step: 499. Dropping entry: {'train/lr': 0.00023773469387755098, '_timestamp': 1740954371.1461587}).
Epoch: [1][ 61/500]	Time  5.616 ( 5.616)	Loss 1.6805 (1.6192)	CeLoss 0.2832 (0.6593)	SegCLSLoss 0.0219 (0.0141)	KLLoss 0.3945 (0.2445)	MaskLoss 0.6733 (0.4642)	MaskBCELoss 0.0463 (0.1384)	MaskDICELoss 0.6269 (0.3258)
Epoch: [1][ 62/500]	Time  6.421 ( 6.421)	Loss 2.5551 (2.3593)	CeLoss 0.1108 (0.3300)	SegCLSLoss 0.0596 (0.0298)	KLLoss 0.3691 (0.3520)	MaskLoss 1.1887 (0.9895)	MaskBCELoss 0.3851 (0.2299)	MaskDICELoss 0.8036 (0.7596)
Epoch: [1][ 63/500]	Time  4.033 ( 4.033)	Loss 2.4004 (1.6476)	CeLoss 0.2578 (0.6554)	SegCLSLoss 0.0212 (0.0114)	KLLoss 0.4316 (0.2070)	MaskLoss 1.0449 (0.4829)	MaskBCELoss 0.1170 (0.1125)	MaskDICELoss 0.9279 (0.3704)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 60 is less than current step: 499. Dropping entry: {'train/loss': 1.6191804528236389, 'train/ce_loss': 0.65927734375, 'train/seg_cls_loss': 0.01409912109375, 'train/kl_loss': 0.24453125, 'train/mask_bce_loss': 0.138424040004611, 'train/mask_dice_loss': 0.32575602233409884, 'train/mask_loss': 0.46418007016181945, 'metrics/total_secs_per_batch': 5.615929365158081, 'metrics/data_secs_per_batch': 2.366543436050415, '_timestamp': 1740954376.761968}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 60 is less than current step: 499. Dropping entry: {'train/lr': 0.00023761224489795914, '_timestamp': 1740954376.7622411}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 61 is less than current step: 499. Dropping entry: {'train/loss': 2.359253931045532, 'train/ce_loss': 0.330029296875, 'train/seg_cls_loss': 0.029833984375, 'train/kl_loss': 0.351953125, 'train/mask_bce_loss': 0.22991811577230692, 'train/mask_dice_loss': 0.7596209406852722, 'train/mask_loss': 0.9895390570163727, 'metrics/total_secs_per_batch': 6.421102285385132, 'metrics/data_secs_per_batch': 2.7109670877456664, '_timestamp': 1740954383.1831055}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 61 is less than current step: 499. Dropping entry: {'train/lr': 0.00023748979591836734, '_timestamp': 1740954383.1833844}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 62 is less than current step: 499. Dropping entry: {'train/loss': 1.6476046681404113, 'train/ce_loss': 0.65537109375, 'train/seg_cls_loss': 0.01142578125, 'train/kl_loss': 0.20703125, 'train/mask_bce_loss': 0.11251607201993466, 'train/mask_dice_loss': 0.37036829590797427, 'train/mask_loss': 0.4828843653202057, 'metrics/total_secs_per_batch': 4.032867431640625, 'metrics/data_secs_per_batch': 1.87225501537323, '_timestamp': 1740954387.2159307}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 62 is less than current step: 499. Dropping entry: {'train/lr': 0.0002373673469387755, '_timestamp': 1740954387.216201}).
Epoch: [1][ 64/500]	Time  6.040 ( 6.040)	Loss 1.7351 (1.7570)	CeLoss 0.2832 (0.6021)	SegCLSLoss 0.0194 (0.0143)	KLLoss 0.4023 (0.2865)	MaskLoss 0.7006 (0.5594)	MaskBCELoss 0.1445 (0.0799)	MaskDICELoss 0.5560 (0.4795)
Epoch: [1][ 65/500]	Time  5.335 ( 5.335)	Loss 1.9396 (2.0784)	CeLoss 0.2012 (0.5683)	SegCLSLoss 0.0299 (0.0321)	KLLoss 0.3770 (0.2648)	MaskLoss 0.8428 (0.7339)	MaskBCELoss 0.1039 (0.1732)	MaskDICELoss 0.7389 (0.5607)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 63 is less than current step: 499. Dropping entry: {'train/loss': 1.7569623589515686, 'train/ce_loss': 0.6021484375, 'train/seg_cls_loss': 0.01427001953125, 'train/kl_loss': 0.2865234375, 'train/mask_bce_loss': 0.07991582844406367, 'train/mask_dice_loss': 0.47952239513397216, 'train/mask_loss': 0.5594382226467133, 'metrics/total_secs_per_batch': 6.040034294128418, 'metrics/data_secs_per_batch': 2.4988065004348754, '_timestamp': 1740954393.2559946}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 63 is less than current step: 499. Dropping entry: {'train/lr': 0.00023724489795918366, '_timestamp': 1740954393.2562664}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 64 is less than current step: 499. Dropping entry: {'train/loss': 2.0784477591514587, 'train/ce_loss': 0.56826171875, 'train/seg_cls_loss': 0.0321044921875, 'train/kl_loss': 0.26484375, 'train/mask_bce_loss': 0.1731828324496746, 'train/mask_dice_loss': 0.5606699526309967, 'train/mask_loss': 0.7338527858257293, 'metrics/total_secs_per_batch': 5.334658622741699, 'metrics/data_secs_per_batch': 2.405711126327515, '_timestamp': 1740954398.5906742}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 64 is less than current step: 499. Dropping entry: {'train/lr': 0.00023712244897959182, '_timestamp': 1740954398.5909548}).
Epoch: [1][ 66/500]	Time  6.843 ( 6.843)	Loss 2.2312 (1.8031)	CeLoss 0.1973 (0.3182)	SegCLSLoss 0.0327 (0.0184)	KLLoss 0.3711 (0.3223)	MaskLoss 0.9906 (0.7216)	MaskBCELoss 0.0595 (0.0973)	MaskDICELoss 0.9311 (0.6243)
Epoch: [1][ 67/500]	Time  5.158 ( 5.158)	Loss 2.2317 (1.7075)	CeLoss 0.2500 (0.6678)	SegCLSLoss 0.0160 (0.0156)	KLLoss 0.4219 (0.1975)	MaskLoss 0.9654 (0.5061)	MaskBCELoss 0.0806 (0.0777)	MaskDICELoss 0.8848 (0.4284)
Epoch: [1][ 68/500]	Time  6.117 ( 6.117)	Loss 2.4376 (2.2024)	CeLoss 0.2598 (0.4827)	SegCLSLoss 0.0139 (0.0211)	KLLoss 0.4375 (0.3209)	MaskLoss 1.0635 (0.8386)	MaskBCELoss 0.4685 (0.2107)	MaskDICELoss 0.5950 (0.6278)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 65 is less than current step: 499. Dropping entry: {'train/loss': 1.8030563116073608, 'train/ce_loss': 0.3181640625, 'train/seg_cls_loss': 0.0183837890625, 'train/kl_loss': 0.322265625, 'train/mask_bce_loss': 0.09728224687278271, 'train/mask_dice_loss': 0.6243142664432526, 'train/mask_loss': 0.7215965211391449, 'metrics/total_secs_per_batch': 6.843390703201294, 'metrics/data_secs_per_batch': 2.900144171714783, '_timestamp': 1740954405.4341476}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 65 is less than current step: 499. Dropping entry: {'train/lr': 0.000237, '_timestamp': 1740954405.4344769}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 66 is less than current step: 499. Dropping entry: {'train/loss': 1.7074893474578858, 'train/ce_loss': 0.6677734375, 'train/seg_cls_loss': 0.0155517578125, 'train/kl_loss': 0.1974609375, 'train/mask_bce_loss': 0.07765875905752181, 'train/mask_dice_loss': 0.4284296572208405, 'train/mask_loss': 0.5060884177684783, 'metrics/total_secs_per_batch': 5.157699823379517, 'metrics/data_secs_per_batch': 2.011499118804932, '_timestamp': 1740954410.5918047}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 66 is less than current step: 499. Dropping entry: {'train/lr': 0.00023687755102040815, '_timestamp': 1740954410.5920942}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 67 is less than current step: 499. Dropping entry: {'train/loss': 2.202407956123352, 'train/ce_loss': 0.48271484375, 'train/seg_cls_loss': 0.02113037109375, 'train/kl_loss': 0.3208984375, 'train/mask_bce_loss': 0.210742625920102, 'train/mask_dice_loss': 0.6278148621320725, 'train/mask_loss': 0.8385574877262115, 'metrics/total_secs_per_batch': 6.116841554641724, 'metrics/data_secs_per_batch': 3.113506245613098, '_timestamp': 1740954416.7086148}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 67 is less than current step: 499. Dropping entry: {'train/lr': 0.0002367551020408163, '_timestamp': 1740954416.708899}).
Epoch: [1][ 69/500]	Time  6.422 ( 6.422)	Loss 1.7620 (1.4206)	CeLoss 0.1973 (0.5032)	SegCLSLoss 0.0300 (0.0127)	KLLoss 0.3750 (0.1949)	MaskLoss 0.7560 (0.4458)	MaskBCELoss 0.1192 (0.0250)	MaskDICELoss 0.6367 (0.4208)
[2025-03-02 16:27:07,464] [INFO] [logging.py:96:log_dist] [Rank 0] step=570, skipped=0, lr=[0.00023657142857142854], mom=[(0.9, 0.95)]
[2025-03-02 16:27:07,470] [INFO] [timer.py:215:stop] epoch=0/micro_step=5700/global_step=570, RunningAvgSamplesPerSec=1.4729212139575483, CurrSamplesPerSec=2.3048382048315226, MemAllocated=30.7GB, MaxMemAllocated=36.81GB
Epoch: [1][ 70/500]	Time  4.340 ( 4.340)	Loss 1.5078 (1.6069)	CeLoss 1.5078 (0.8425)	SegCLSLoss 0.0000 (0.0105)	KLLoss 0.0000 (0.1580)	MaskLoss 0.0000 (0.3717)	MaskBCELoss 0.0000 (0.0355)	MaskDICELoss 0.0000 (0.3362)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 68 is less than current step: 499. Dropping entry: {'train/loss': 1.4205702662467956, 'train/ce_loss': 0.503173828125, 'train/seg_cls_loss': 0.0126708984375, 'train/kl_loss': 0.194921875, 'train/mask_bce_loss': 0.024992842599749564, 'train/mask_dice_loss': 0.4208147585391998, 'train/mask_loss': 0.4458075940608978, 'metrics/total_secs_per_batch': 6.422497034072876, 'metrics/data_secs_per_batch': 3.0403814554214477, '_timestamp': 1740954423.131164}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 68 is less than current step: 499. Dropping entry: {'train/lr': 0.00023663265306122448, '_timestamp': 1740954423.1314516}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 69 is less than current step: 499. Dropping entry: {'train/loss': 1.6068648815155029, 'train/ce_loss': 0.84248046875, 'train/seg_cls_loss': 0.01048583984375, 'train/kl_loss': 0.1580078125, 'train/mask_bce_loss': 0.0355215473100543, 'train/mask_dice_loss': 0.33617262840270995, 'train/mask_loss': 0.37169417142868044, 'metrics/total_secs_per_batch': 4.340332508087158, 'metrics/data_secs_per_batch': 2.039658260345459, '_timestamp': 1740954427.4712622}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 69 is less than current step: 499. Dropping entry: {'train/lr': 0.00023651020408163264, '_timestamp': 1740954427.471552}).
Epoch: [1][ 71/500]	Time  7.572 ( 7.572)	Loss 2.2505 (1.9971)	CeLoss 0.2090 (0.2401)	SegCLSLoss 0.0260 (0.0221)	KLLoss 0.3828 (0.4076)	MaskLoss 0.9953 (0.8526)	MaskBCELoss 0.0400 (0.1228)	MaskDICELoss 0.9553 (0.7298)
Epoch: [1][ 72/500]	Time  6.098 ( 6.098)	Loss 2.0096 (2.0132)	CeLoss 0.1924 (0.4303)	SegCLSLoss 0.0247 (0.0206)	KLLoss 0.4023 (0.3244)	MaskLoss 0.8822 (0.7701)	MaskBCELoss 0.0128 (0.1674)	MaskDICELoss 0.8694 (0.6027)
Epoch: [1][ 73/500]	Time  5.828 ( 5.828)	Loss 1.4402 (1.1816)	CeLoss 0.2197 (0.5381)	SegCLSLoss 0.0154 (0.0091)	KLLoss 0.4082 (0.1613)	MaskLoss 0.5863 (0.3115)	MaskBCELoss 0.2576 (0.0563)	MaskDICELoss 0.3287 (0.2552)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 70 is less than current step: 499. Dropping entry: {'train/loss': 1.9971420168876648, 'train/ce_loss': 0.24013671875, 'train/seg_cls_loss': 0.022064208984375, 'train/kl_loss': 0.4076171875, 'train/mask_bce_loss': 0.12278130827471614, 'train/mask_dice_loss': 0.729793593287468, 'train/mask_loss': 0.8525749087333679, 'metrics/total_secs_per_batch': 7.572101593017578, 'metrics/data_secs_per_batch': 3.4704864740371706, '_timestamp': 1740954435.043557}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 70 is less than current step: 499. Dropping entry: {'train/lr': 0.0002363877551020408, '_timestamp': 1740954435.0437496}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 71 is less than current step: 499. Dropping entry: {'train/loss': 2.0131580591201783, 'train/ce_loss': 0.4302734375, 'train/seg_cls_loss': 0.020587158203125, 'train/kl_loss': 0.3244140625, 'train/mask_bce_loss': 0.1673972338438034, 'train/mask_dice_loss': 0.6026583671569824, 'train/mask_loss': 0.770055603981018, 'metrics/total_secs_per_batch': 6.097615957260132, 'metrics/data_secs_per_batch': 2.459718632698059, '_timestamp': 1740954441.1411734}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 71 is less than current step: 499. Dropping entry: {'train/lr': 0.00023626530612244897, '_timestamp': 1740954441.1414514}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 72 is less than current step: 499. Dropping entry: {'train/loss': 1.181645894050598, 'train/ce_loss': 0.5380859375, 'train/seg_cls_loss': 0.009124755859375, 'train/kl_loss': 0.161328125, 'train/mask_bce_loss': 0.05626366175711155, 'train/mask_dice_loss': 0.2552135944366455, 'train/mask_loss': 0.3114772439002991, 'metrics/total_secs_per_batch': 5.827748537063599, 'metrics/data_secs_per_batch': 2.7834606647491453, '_timestamp': 1740954446.9689598}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 72 is less than current step: 499. Dropping entry: {'train/lr': 0.00023614285714285713, '_timestamp': 1740954446.969237}).
Epoch: [1][ 74/500]	Time  5.608 ( 5.608)	Loss 1.9116 (1.9435)	CeLoss 0.2021 (0.4939)	SegCLSLoss 0.0344 (0.0170)	KLLoss 0.3926 (0.2814)	MaskLoss 0.8259 (0.7063)	MaskBCELoss 0.0395 (0.1610)	MaskDICELoss 0.7865 (0.5453)
Epoch: [1][ 75/500]	Time  6.113 ( 6.113)	Loss 2.3025 (1.6889)	CeLoss 0.1206 (0.4547)	SegCLSLoss 0.0654 (0.0193)	KLLoss 0.3828 (0.2900)	MaskLoss 1.0555 (0.5977)	MaskBCELoss 0.1710 (0.0816)	MaskDICELoss 0.8845 (0.5161)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 73 is less than current step: 499. Dropping entry: {'train/loss': 1.9434918761253357, 'train/ce_loss': 0.4939453125, 'train/seg_cls_loss': 0.017034912109375, 'train/kl_loss': 0.2814453125, 'train/mask_bce_loss': 0.16097704209387304, 'train/mask_dice_loss': 0.5453391939401626, 'train/mask_loss': 0.7063162386417389, 'metrics/total_secs_per_batch': 5.607539415359497, 'metrics/data_secs_per_batch': 2.2425994634628297, '_timestamp': 1740954452.5764482}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 73 is less than current step: 499. Dropping entry: {'train/lr': 0.0002360204081632653, '_timestamp': 1740954452.5767221}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 74 is less than current step: 499. Dropping entry: {'train/loss': 1.6889427661895753, 'train/ce_loss': 0.454736328125, 'train/seg_cls_loss': 0.01934814453125, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.08160555437207222, 'train/mask_dice_loss': 0.5161372870206833, 'train/mask_loss': 0.5977428436279297, 'metrics/total_secs_per_batch': 6.1128528118133545, 'metrics/data_secs_per_batch': 2.4919748306274414, '_timestamp': 1740954458.689299}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 74 is less than current step: 499. Dropping entry: {'train/lr': 0.00023589795918367345, '_timestamp': 1740954458.6895747}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 75 is less than current step: 499. Dropping entry: {'train/loss': 2.3585177600383758, 'train/ce_loss': 0.293798828125, 'train/seg_cls_loss': 0.024884033203125, 'train/kl_loss': 0.362109375, 'train/mask_bce_loss': 0.31130301412194966, 'train/mask_dice_loss': 0.6969597801566124, 'train/mask_loss': 1.0082628011703492, 'metrics/total_secs_per_batch': 5.542198896408081, 'metrics/data_secs_per_batch': 2.4847482442855835, '_timestamp': 1740954464.231633}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 75 is less than current step: 499. Dropping entry: {'train/lr': 0.00023577551020408162, '_timestamp': 1740954464.2319431}).
Epoch: [1][ 76/500]	Time  5.542 ( 5.542)	Loss 2.3657 (2.3585)	CeLoss 0.1660 (0.2938)	SegCLSLoss 0.0262 (0.0249)	KLLoss 0.3965 (0.3621)	MaskLoss 1.0735 (1.0083)	MaskBCELoss 0.1065 (0.3113)	MaskDICELoss 0.9670 (0.6970)
Epoch: [1][ 77/500]	Time  5.864 ( 5.864)	Loss 2.4646 (1.6048)	CeLoss 0.3926 (0.3479)	SegCLSLoss 0.0150 (0.0145)	KLLoss 0.4336 (0.2938)	MaskLoss 1.0106 (0.6102)	MaskBCELoss 0.0134 (0.1330)	MaskDICELoss 0.9972 (0.4771)
Epoch: [1][ 78/500]	Time  6.088 ( 6.088)	Loss 2.3488 (1.5727)	CeLoss 0.2578 (0.3219)	SegCLSLoss 0.0255 (0.0161)	KLLoss 0.4004 (0.2865)	MaskLoss 1.0182 (0.6070)	MaskBCELoss 0.0365 (0.0745)	MaskDICELoss 0.9817 (0.5325)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 76 is less than current step: 499. Dropping entry: {'train/loss': 1.6048293590545655, 'train/ce_loss': 0.347900390625, 'train/seg_cls_loss': 0.014520263671875, 'train/kl_loss': 0.29375, 'train/mask_bce_loss': 0.13301797080785036, 'train/mask_dice_loss': 0.47713596522808077, 'train/mask_loss': 0.6101539373397827, 'metrics/total_secs_per_batch': 5.863727569580078, 'metrics/data_secs_per_batch': 2.8440807342529295, '_timestamp': 1740954470.0954208}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 76 is less than current step: 499. Dropping entry: {'train/lr': 0.00023565306122448978, '_timestamp': 1740954470.0957344}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 77 is less than current step: 499. Dropping entry: {'train/loss': 1.5726934552192688, 'train/ce_loss': 0.321923828125, 'train/seg_cls_loss': 0.016131591796875, 'train/kl_loss': 0.2865234375, 'train/mask_bce_loss': 0.07447081413120031, 'train/mask_dice_loss': 0.5325057804584503, 'train/mask_loss': 0.6069765985012054, 'metrics/total_secs_per_batch': 6.087862491607666, 'metrics/data_secs_per_batch': 2.5924034118652344, '_timestamp': 1740954476.1831684}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 77 is less than current step: 499. Dropping entry: {'train/lr': 0.00023553061224489794, '_timestamp': 1740954476.1835265}).
Epoch: [1][ 79/500]	Time  7.789 ( 7.789)	Loss 2.3389 (2.6252)	CeLoss 0.2891 (0.2390)	SegCLSLoss 0.0151 (0.0239)	KLLoss 0.4316 (0.4020)	MaskLoss 0.9995 (1.1670)	MaskBCELoss 0.3309 (0.3452)	MaskDICELoss 0.6686 (0.8218)
[2025-03-02 16:28:10,336] [INFO] [logging.py:96:log_dist] [Rank 0] step=580, skipped=0, lr=[0.00023534693877551018], mom=[(0.9, 0.95)]
[2025-03-02 16:28:10,343] [INFO] [timer.py:215:stop] epoch=0/micro_step=5800/global_step=580, RunningAvgSamplesPerSec=1.4748125115579045, CurrSamplesPerSec=1.5699091562338656, MemAllocated=31.46GB, MaxMemAllocated=36.81GB
Epoch: [1][ 80/500]	Time  6.371 ( 6.371)	Loss 2.1643 (1.8934)	CeLoss 0.2070 (0.4825)	SegCLSLoss 0.0256 (0.0155)	KLLoss 0.4062 (0.3303)	MaskLoss 0.9523 (0.6852)	MaskBCELoss 0.0384 (0.0490)	MaskDICELoss 0.9139 (0.6362)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 78 is less than current step: 499. Dropping entry: {'train/loss': 2.625213289260864, 'train/ce_loss': 0.23896484375, 'train/seg_cls_loss': 0.02392578125, 'train/kl_loss': 0.401953125, 'train/mask_bce_loss': 0.34523279666900636, 'train/mask_dice_loss': 0.8217683881521225, 'train/mask_loss': 1.1670011699199676, 'metrics/total_secs_per_batch': 7.789155721664429, 'metrics/data_secs_per_batch': 3.588483381271362, '_timestamp': 1740954483.9723208}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 78 is less than current step: 499. Dropping entry: {'train/lr': 0.00023540816326530608, '_timestamp': 1740954483.9726026}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 79 is less than current step: 499. Dropping entry: {'train/loss': 1.89342702627182, 'train/ce_loss': 0.48251953125, 'train/seg_cls_loss': 0.01552734375, 'train/kl_loss': 0.3302734375, 'train/mask_bce_loss': 0.04903751965612173, 'train/mask_dice_loss': 0.6361525475978851, 'train/mask_loss': 0.6851900756359101, 'metrics/total_secs_per_batch': 6.371407508850098, 'metrics/data_secs_per_batch': 2.9570813179016113, '_timestamp': 1740954490.3435066}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 79 is less than current step: 499. Dropping entry: {'train/lr': 0.00023528571428571424, '_timestamp': 1740954490.3437772}).
Epoch: [1][ 81/500]	Time  5.668 ( 5.668)	Loss 1.0078 (2.0228)	CeLoss 1.0078 (0.5883)	SegCLSLoss 0.0000 (0.0177)	KLLoss 0.0000 (0.2742)	MaskLoss 0.0000 (0.6991)	MaskBCELoss 0.0000 (0.2027)	MaskDICELoss 0.0000 (0.4963)
Epoch: [1][ 82/500]	Time  6.516 ( 6.516)	Loss 2.4804 (2.0327)	CeLoss 0.3359 (0.3123)	SegCLSLoss 0.0237 (0.0189)	KLLoss 0.3984 (0.3697)	MaskLoss 1.0468 (0.8370)	MaskBCELoss 0.0488 (0.1190)	MaskDICELoss 0.9980 (0.7180)
Epoch: [1][ 83/500]	Time  5.153 ( 5.153)	Loss 2.1232 (2.2020)	CeLoss 0.2285 (0.4967)	SegCLSLoss 0.0178 (0.0135)	KLLoss 0.4160 (0.2381)	MaskLoss 0.9220 (0.8375)	MaskBCELoss 0.1988 (0.3904)	MaskDICELoss 0.7232 (0.4471)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 80 is less than current step: 499. Dropping entry: {'train/loss': 2.022753381729126, 'train/ce_loss': 0.58828125, 'train/seg_cls_loss': 0.01767578125, 'train/kl_loss': 0.27421875, 'train/mask_bce_loss': 0.20274758003652096, 'train/mask_dice_loss': 0.4963244140148163, 'train/mask_loss': 0.699072003364563, 'metrics/total_secs_per_batch': 5.668111324310303, 'metrics/data_secs_per_batch': 2.8960660696029663, '_timestamp': 1740954496.0118153}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 80 is less than current step: 499. Dropping entry: {'train/lr': 0.0002351632653061224, '_timestamp': 1740954496.012106}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 81 is less than current step: 499. Dropping entry: {'train/loss': 2.032679009437561, 'train/ce_loss': 0.3123046875, 'train/seg_cls_loss': 0.01888427734375, 'train/kl_loss': 0.3697265625, 'train/mask_bce_loss': 0.11897869855165481, 'train/mask_dice_loss': 0.7180150836706162, 'train/mask_loss': 0.836993795633316, 'metrics/total_secs_per_batch': 6.515650510787964, 'metrics/data_secs_per_batch': 2.7162302494049073, '_timestamp': 1740954502.5274763}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 81 is less than current step: 499. Dropping entry: {'train/lr': 0.00023504081632653057, '_timestamp': 1740954502.5277498}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 82 is less than current step: 499. Dropping entry: {'train/loss': 2.2020158052444456, 'train/ce_loss': 0.4967041015625, 'train/seg_cls_loss': 0.0135498046875, 'train/kl_loss': 0.2380859375, 'train/mask_bce_loss': 0.3903623756021261, 'train/mask_dice_loss': 0.4471079409122467, 'train/mask_loss': 0.8374703228473663, 'metrics/total_secs_per_batch': 5.152626276016235, 'metrics/data_secs_per_batch': 2.269724631309509, '_timestamp': 1740954507.6800854}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 82 is less than current step: 499. Dropping entry: {'train/lr': 0.00023491836734693873, '_timestamp': 1740954507.6804156}).
Epoch: [1][ 84/500]	Time  6.193 ( 6.193)	Loss 2.1210 (1.7892)	CeLoss 0.1865 (0.3057)	SegCLSLoss 0.0354 (0.0175)	KLLoss 0.3965 (0.3277)	MaskLoss 0.9384 (0.7210)	MaskBCELoss 0.0251 (0.1316)	MaskDICELoss 0.9133 (0.5894)
Epoch: [1][ 85/500]	Time  5.792 ( 5.792)	Loss 2.3457 (1.7716)	CeLoss 0.2324 (0.5059)	SegCLSLoss 0.0311 (0.0155)	KLLoss 0.4180 (0.2867)	MaskLoss 1.0283 (0.6146)	MaskBCELoss 0.1822 (0.1073)	MaskDICELoss 0.8461 (0.5073)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 83 is less than current step: 499. Dropping entry: {'train/loss': 1.7891942739486695, 'train/ce_loss': 0.3056640625, 'train/seg_cls_loss': 0.017510986328125, 'train/kl_loss': 0.327734375, 'train/mask_bce_loss': 0.1315724689513445, 'train/mask_dice_loss': 0.5894407004117965, 'train/mask_loss': 0.7210131764411927, 'metrics/total_secs_per_batch': 6.19250226020813, 'metrics/data_secs_per_batch': 2.551633858680725, '_timestamp': 1740954513.8725913}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 83 is less than current step: 499. Dropping entry: {'train/lr': 0.0002347959183673469, '_timestamp': 1740954513.8727887}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 84 is less than current step: 499. Dropping entry: {'train/loss': 1.7715991497039796, 'train/ce_loss': 0.505859375, 'train/seg_cls_loss': 0.015509033203125, 'train/kl_loss': 0.28671875, 'train/mask_bce_loss': 0.10728478841483594, 'train/mask_dice_loss': 0.5073233842849731, 'train/mask_loss': 0.6146081626415253, 'metrics/total_secs_per_batch': 5.7916412353515625, 'metrics/data_secs_per_batch': 2.5406840801239015, '_timestamp': 1740954519.664255}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 84 is less than current step: 499. Dropping entry: {'train/lr': 0.00023467346938775509, '_timestamp': 1740954519.6645372}).
Epoch: [1][ 86/500]	Time  5.855 ( 5.855)	Loss 1.9173 (2.1546)	CeLoss 0.1729 (0.2397)	SegCLSLoss 0.0233 (0.0265)	KLLoss 0.4023 (0.3551)	MaskLoss 0.8464 (0.9332)	MaskBCELoss 0.0895 (0.1556)	MaskDICELoss 0.7569 (0.7776)
Epoch: [1][ 87/500]	Time  5.691 ( 5.691)	Loss 2.3074 (1.7160)	CeLoss 0.2891 (0.3778)	SegCLSLoss 0.0148 (0.0147)	KLLoss 0.4121 (0.2770)	MaskLoss 0.9848 (0.6516)	MaskBCELoss 0.1200 (0.0995)	MaskDICELoss 0.8647 (0.5521)
Epoch: [1][ 88/500]	Time  5.890 ( 5.890)	Loss 1.5000 (2.0645)	CeLoss 1.5000 (0.5173)	SegCLSLoss 0.0000 (0.0171)	KLLoss 0.0000 (0.3148)	MaskLoss 0.0000 (0.7537)	MaskBCELoss 0.0000 (0.2066)	MaskDICELoss 0.0000 (0.5471)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 85 is less than current step: 499. Dropping entry: {'train/loss': 2.1546067118644716, 'train/ce_loss': 0.23974609375, 'train/seg_cls_loss': 0.0264892578125, 'train/kl_loss': 0.355078125, 'train/mask_bce_loss': 0.1555607158690691, 'train/mask_dice_loss': 0.7776020169258118, 'train/mask_loss': 0.9331627249717712, 'metrics/total_secs_per_batch': 5.85514235496521, 'metrics/data_secs_per_batch': 2.720919632911682, '_timestamp': 1740954525.5194817}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 85 is less than current step: 499. Dropping entry: {'train/lr': 0.00023455102040816325, '_timestamp': 1740954525.5196881}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 86 is less than current step: 499. Dropping entry: {'train/loss': 1.7160495400428772, 'train/ce_loss': 0.37783203125, 'train/seg_cls_loss': 0.01468505859375, 'train/kl_loss': 0.276953125, 'train/mask_bce_loss': 0.09949065037071705, 'train/mask_dice_loss': 0.55213762819767, 'train/mask_loss': 0.6516282737255097, 'metrics/total_secs_per_batch': 5.690720558166504, 'metrics/data_secs_per_batch': 2.516712784767151, '_timestamp': 1740954531.210204}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 86 is less than current step: 499. Dropping entry: {'train/lr': 0.0002344285714285714, '_timestamp': 1740954531.210576}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 87 is less than current step: 499. Dropping entry: {'train/loss': 2.064481794834137, 'train/ce_loss': 0.51728515625, 'train/seg_cls_loss': 0.017144775390625, 'train/kl_loss': 0.31484375, 'train/mask_bce_loss': 0.20658698491752148, 'train/mask_dice_loss': 0.5471382886171341, 'train/mask_loss': 0.7537252724170684, 'metrics/total_secs_per_batch': 5.890340328216553, 'metrics/data_secs_per_batch': 3.016722083091736, '_timestamp': 1740954537.100467}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 87 is less than current step: 499. Dropping entry: {'train/lr': 0.00023430612244897957, '_timestamp': 1740954537.1007302}).
Epoch: [1][ 89/500]	Time  6.501 ( 6.501)	Loss 1.9724 (1.8944)	CeLoss 0.2598 (0.3089)	SegCLSLoss 0.0114 (0.0212)	KLLoss 0.4160 (0.3590)	MaskLoss 0.8329 (0.7696)	MaskBCELoss 0.1910 (0.1716)	MaskDICELoss 0.6419 (0.5980)
[2025-03-02 16:29:10,724] [INFO] [logging.py:96:log_dist] [Rank 0] step=590, skipped=0, lr=[0.00023412244897959183], mom=[(0.9, 0.95)]
[2025-03-02 16:29:10,730] [INFO] [timer.py:215:stop] epoch=0/micro_step=5900/global_step=590, RunningAvgSamplesPerSec=1.4775659886230237, CurrSamplesPerSec=1.4029615663221577, MemAllocated=31.24GB, MaxMemAllocated=36.81GB
Epoch: [1][ 90/500]	Time  7.129 ( 7.129)	Loss 1.8838 (1.7152)	CeLoss 0.2148 (0.3255)	SegCLSLoss 0.0136 (0.0151)	KLLoss 0.4180 (0.3215)	MaskLoss 0.8101 (0.6750)	MaskBCELoss 0.1483 (0.0955)	MaskDICELoss 0.6618 (0.5795)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 88 is less than current step: 499. Dropping entry: {'train/loss': 1.8944023251533508, 'train/ce_loss': 0.30888671875, 'train/seg_cls_loss': 0.02119140625, 'train/kl_loss': 0.358984375, 'train/mask_bce_loss': 0.1715608535334468, 'train/mask_dice_loss': 0.5980035871267319, 'train/mask_loss': 0.7695644408464432, 'metrics/total_secs_per_batch': 6.50145697593689, 'metrics/data_secs_per_batch': 2.920963430404663, '_timestamp': 1740954543.6019773}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 88 is less than current step: 499. Dropping entry: {'train/lr': 0.00023418367346938774, '_timestamp': 1740954543.602255}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 89 is less than current step: 499. Dropping entry: {'train/loss': 1.715201997756958, 'train/ce_loss': 0.32548828125, 'train/seg_cls_loss': 0.01513671875, 'train/kl_loss': 0.321484375, 'train/mask_bce_loss': 0.09547973908483982, 'train/mask_dice_loss': 0.5795040547847747, 'train/mask_loss': 0.6749838054180145, 'metrics/total_secs_per_batch': 7.129424095153809, 'metrics/data_secs_per_batch': 2.96825487613678, '_timestamp': 1740954550.7311692}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 89 is less than current step: 499. Dropping entry: {'train/lr': 0.0002340612244897959, '_timestamp': 1740954550.7314456}).
Epoch: [1][ 91/500]	Time  6.347 ( 6.347)	Loss 2.8469 (1.8490)	CeLoss 0.2090 (0.3005)	SegCLSLoss 0.0273 (0.0195)	KLLoss 0.3750 (0.3180)	MaskLoss 1.2936 (0.7536)	MaskBCELoss 0.4639 (0.1693)	MaskDICELoss 0.8297 (0.5843)
Epoch: [1][ 92/500]	Time  6.697 ( 6.697)	Loss 2.7201 (2.3756)	CeLoss 0.2051 (0.3934)	SegCLSLoss 0.0287 (0.0260)	KLLoss 0.3750 (0.3047)	MaskLoss 1.2311 (0.9694)	MaskBCELoss 0.3002 (0.2498)	MaskDICELoss 0.9310 (0.7196)
Epoch: [1][ 93/500]	Time  5.969 ( 5.969)	Loss 0.8125 (1.6447)	CeLoss 0.8125 (0.5062)	SegCLSLoss 0.0000 (0.0131)	KLLoss 0.0000 (0.2818)	MaskLoss 0.0000 (0.5520)	MaskBCELoss 0.0000 (0.0894)	MaskDICELoss 0.0000 (0.4626)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 90 is less than current step: 499. Dropping entry: {'train/loss': 1.848962676525116, 'train/ce_loss': 0.30048828125, 'train/seg_cls_loss': 0.019464111328125, 'train/kl_loss': 0.31796875, 'train/mask_bce_loss': 0.16932007214054465, 'train/mask_dice_loss': 0.5842628248035908, 'train/mask_loss': 0.753582888841629, 'metrics/total_secs_per_batch': 6.34726095199585, 'metrics/data_secs_per_batch': 2.5593347549438477, '_timestamp': 1740954557.0786173}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 90 is less than current step: 499. Dropping entry: {'train/lr': 0.00023393877551020406, '_timestamp': 1740954557.0788925}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 91 is less than current step: 499. Dropping entry: {'train/loss': 2.375564527511597, 'train/ce_loss': 0.393359375, 'train/seg_cls_loss': 0.02598876953125, 'train/kl_loss': 0.3046875, 'train/mask_bce_loss': 0.24982555028982462, 'train/mask_dice_loss': 0.7195973336696625, 'train/mask_loss': 0.9694228708744049, 'metrics/total_secs_per_batch': 6.697335958480835, 'metrics/data_secs_per_batch': 2.61913058757782, '_timestamp': 1740954563.7759616}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 91 is less than current step: 499. Dropping entry: {'train/lr': 0.00023381632653061223, '_timestamp': 1740954563.776236}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 92 is less than current step: 499. Dropping entry: {'train/loss': 1.6446970462799073, 'train/ce_loss': 0.50625, 'train/seg_cls_loss': 0.013055419921875, 'train/kl_loss': 0.2818359375, 'train/mask_bce_loss': 0.08944132812321186, 'train/mask_dice_loss': 0.46259468197822573, 'train/mask_loss': 0.5520360112190247, 'metrics/total_secs_per_batch': 5.968607187271118, 'metrics/data_secs_per_batch': 2.672669553756714, '_timestamp': 1740954569.7446456}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 92 is less than current step: 499. Dropping entry: {'train/lr': 0.0002336938775510204, '_timestamp': 1740954569.7449272}).
Epoch: [1][ 94/500]	Time  5.308 ( 5.308)	Loss 1.6743 (1.9363)	CeLoss 0.2275 (0.6104)	SegCLSLoss 0.0166 (0.0130)	KLLoss 0.4062 (0.2375)	MaskLoss 0.6985 (0.6477)	MaskBCELoss 0.0355 (0.1957)	MaskDICELoss 0.6630 (0.4520)
Epoch: [1][ 95/500]	Time  6.731 ( 6.731)	Loss 2.4719 (1.9481)	CeLoss 0.2070 (0.2784)	SegCLSLoss 0.0262 (0.0205)	KLLoss 0.3711 (0.3500)	MaskLoss 1.1070 (0.8121)	MaskBCELoss 0.2418 (0.1812)	MaskDICELoss 0.8653 (0.6308)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 93 is less than current step: 499. Dropping entry: {'train/loss': 1.9362630248069763, 'train/ce_loss': 0.61044921875, 'train/seg_cls_loss': 0.0129638671875, 'train/kl_loss': 0.2375, 'train/mask_bce_loss': 0.19571400210261344, 'train/mask_dice_loss': 0.4519585132598877, 'train/mask_loss': 0.6476725161075592, 'metrics/total_secs_per_batch': 5.308091640472412, 'metrics/data_secs_per_batch': 2.258960652351379, '_timestamp': 1740954575.052641}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 93 is less than current step: 499. Dropping entry: {'train/lr': 0.00023357142857142855, '_timestamp': 1740954575.0529864}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 94 is less than current step: 499. Dropping entry: {'train/loss': 1.9481330394744873, 'train/ce_loss': 0.27841796875, 'train/seg_cls_loss': 0.020526123046875, 'train/kl_loss': 0.35, 'train/mask_bce_loss': 0.1812252951785922, 'train/mask_dice_loss': 0.6308295041322708, 'train/mask_loss': 0.8120548009872437, 'metrics/total_secs_per_batch': 6.731131076812744, 'metrics/data_secs_per_batch': 3.061053204536438, '_timestamp': 1740954581.7837863}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 94 is less than current step: 499. Dropping entry: {'train/lr': 0.00023344897959183672, '_timestamp': 1740954581.7840688}).
Epoch: [1][ 96/500]	Time  6.032 ( 6.032)	Loss 0.0825 (1.8875)	CeLoss 0.0825 (0.3999)	SegCLSLoss 0.0000 (0.0165)	KLLoss 0.0000 (0.2756)	MaskLoss 0.0000 (0.7259)	MaskBCELoss 0.0000 (0.1661)	MaskDICELoss 0.0000 (0.5598)
Epoch: [1][ 97/500]	Time  4.429 ( 4.429)	Loss 2.1807 (1.6463)	CeLoss 0.2773 (0.7063)	SegCLSLoss 0.0126 (0.0117)	KLLoss 0.4199 (0.1967)	MaskLoss 0.9282 (0.4573)	MaskBCELoss 0.6141 (0.1219)	MaskDICELoss 0.3142 (0.3353)
Epoch: [1][ 98/500]	Time  5.664 ( 5.664)	Loss 2.7985 (1.6473)	CeLoss 0.2480 (0.5695)	SegCLSLoss 0.0244 (0.0124)	KLLoss 0.3711 (0.2375)	MaskLoss 1.2508 (0.5241)	MaskBCELoss 0.5461 (0.1192)	MaskDICELoss 0.7047 (0.4049)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 95 is less than current step: 499. Dropping entry: {'train/loss': 1.8874992251396179, 'train/ce_loss': 0.399853515625, 'train/seg_cls_loss': 0.016497802734375, 'train/kl_loss': 0.2755859375, 'train/mask_bce_loss': 0.1660657599568367, 'train/mask_dice_loss': 0.5597883343696595, 'train/mask_loss': 0.72585409283638, 'metrics/total_secs_per_batch': 6.032427072525024, 'metrics/data_secs_per_batch': 2.5321492910385133, '_timestamp': 1740954587.816341}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 95 is less than current step: 499. Dropping entry: {'train/lr': 0.00023332653061224488, '_timestamp': 1740954587.8166528}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 96 is less than current step: 499. Dropping entry: {'train/loss': 1.6462729096412658, 'train/ce_loss': 0.70634765625, 'train/seg_cls_loss': 0.011669921875, 'train/kl_loss': 0.1966796875, 'train/mask_bce_loss': 0.12194957165047526, 'train/mask_dice_loss': 0.33531773686408994, 'train/mask_loss': 0.4572673082351685, 'metrics/total_secs_per_batch': 4.428891897201538, 'metrics/data_secs_per_batch': 2.3890798330307006, '_timestamp': 1740954592.2453218}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 96 is less than current step: 499. Dropping entry: {'train/lr': 0.00023320408163265304, '_timestamp': 1740954592.2456322}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 97 is less than current step: 499. Dropping entry: {'train/loss': 1.6473393201828004, 'train/ce_loss': 0.56953125, 'train/seg_cls_loss': 0.0124267578125, 'train/kl_loss': 0.2375, 'train/mask_bce_loss': 0.1192050602287054, 'train/mask_dice_loss': 0.404855215549469, 'train/mask_loss': 0.5240602731704712, 'metrics/total_secs_per_batch': 5.663632869720459, 'metrics/data_secs_per_batch': 2.4590257883071898, '_timestamp': 1740954597.9088628}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 97 is less than current step: 499. Dropping entry: {'train/lr': 0.0002330816326530612, '_timestamp': 1740954597.9091518}).
Epoch: [1][ 99/500]	Time  6.957 ( 6.957)	Loss 6.1114 (2.8543)	CeLoss 0.3730 (0.2658)	SegCLSLoss 0.0332 (0.0246)	KLLoss 0.3770 (0.3893)	MaskLoss 2.8418 (1.2688)	MaskBCELoss 2.0410 (0.5055)	MaskDICELoss 0.8008 (0.7633)
[2025-03-02 16:30:11,005] [INFO] [logging.py:96:log_dist] [Rank 0] step=600, skipped=0, lr=[0.00023289795918367344], mom=[(0.9, 0.95)]
[2025-03-02 16:30:11,011] [INFO] [timer.py:215:stop] epoch=0/micro_step=6000/global_step=600, RunningAvgSamplesPerSec=1.480276366966834, CurrSamplesPerSec=1.6274746229947359, MemAllocated=31.26GB, MaxMemAllocated=36.81GB
Epoch: [1][100/500]	Time  6.146 ( 6.146)	Loss 2.1328 (2.1720)	CeLoss 0.2207 (0.3104)	SegCLSLoss 0.0245 (0.0258)	KLLoss 0.3848 (0.3461)	MaskLoss 0.9307 (0.9070)	MaskBCELoss 0.0708 (0.1670)	MaskDICELoss 0.8598 (0.7400)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 98 is less than current step: 499. Dropping entry: {'train/loss': 2.854302942752838, 'train/ce_loss': 0.2658203125, 'train/seg_cls_loss': 0.024566650390625, 'train/kl_loss': 0.3892578125, 'train/mask_bce_loss': 0.5055305637419224, 'train/mask_dice_loss': 0.763271278142929, 'train/mask_loss': 1.2688018381595612, 'metrics/total_secs_per_batch': 6.957482814788818, 'metrics/data_secs_per_batch': 3.0425616264343263, '_timestamp': 1740954604.8662627}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 98 is less than current step: 499. Dropping entry: {'train/lr': 0.00023295918367346937, '_timestamp': 1740954604.8665433}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 99 is less than current step: 499. Dropping entry: {'train/loss': 2.1719511270523073, 'train/ce_loss': 0.31044921875, 'train/seg_cls_loss': 0.025762939453125, 'train/kl_loss': 0.34609375, 'train/mask_bce_loss': 0.16695753782987593, 'train/mask_dice_loss': 0.7400141298770905, 'train/mask_loss': 0.9069716572761536, 'metrics/total_secs_per_batch': 6.146054029464722, 'metrics/data_secs_per_batch': 2.5269596338272096, '_timestamp': 1740954611.0121045}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 99 is less than current step: 499. Dropping entry: {'train/lr': 0.00023283673469387753, '_timestamp': 1740954611.0123708}).
Epoch: [1][101/500]	Time  6.520 ( 6.520)	Loss 2.1182 (1.8936)	CeLoss 0.2266 (0.4911)	SegCLSLoss 0.0273 (0.0184)	KLLoss 0.3887 (0.3133)	MaskLoss 0.9195 (0.6810)	MaskBCELoss 0.0464 (0.0811)	MaskDICELoss 0.8731 (0.6000)
Epoch: [1][102/500]	Time  5.482 ( 5.482)	Loss 1.6468 (1.9563)	CeLoss 0.2598 (0.4785)	SegCLSLoss 0.0146 (0.0153)	KLLoss 0.4102 (0.2758)	MaskLoss 0.6691 (0.7213)	MaskBCELoss 0.1250 (0.1625)	MaskDICELoss 0.5441 (0.5588)
Epoch: [1][103/500]	Time  5.754 ( 5.754)	Loss 1.9380 (2.1922)	CeLoss 0.2988 (0.3275)	SegCLSLoss 0.0165 (0.0318)	KLLoss 0.4023 (0.3447)	MaskLoss 0.7952 (0.9072)	MaskBCELoss 0.2754 (0.2864)	MaskDICELoss 0.5198 (0.6208)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 100 is less than current step: 499. Dropping entry: {'train/loss': 1.8935683488845825, 'train/ce_loss': 0.49111328125, 'train/seg_cls_loss': 0.01839599609375, 'train/kl_loss': 0.31328125, 'train/mask_bce_loss': 0.08105238077696413, 'train/mask_dice_loss': 0.5999603152275086, 'train/mask_loss': 0.681012699007988, 'metrics/total_secs_per_batch': 6.520039319992065, 'metrics/data_secs_per_batch': 2.667584490776062, '_timestamp': 1740954617.532352}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 100 is less than current step: 499. Dropping entry: {'train/lr': 0.0002327142857142857, '_timestamp': 1740954617.5326958}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 101 is less than current step: 499. Dropping entry: {'train/loss': 1.956291174888611, 'train/ce_loss': 0.478515625, 'train/seg_cls_loss': 0.015313720703125, 'train/kl_loss': 0.27578125, 'train/mask_bce_loss': 0.16254973206669093, 'train/mask_dice_loss': 0.5587599217891693, 'train/mask_loss': 0.7213096499443055, 'metrics/total_secs_per_batch': 5.481734037399292, 'metrics/data_secs_per_batch': 2.360616660118103, '_timestamp': 1740954623.0140798}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 101 is less than current step: 499. Dropping entry: {'train/lr': 0.00023259183673469386, '_timestamp': 1740954623.0143495}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 102 is less than current step: 499. Dropping entry: {'train/loss': 2.192154788970947, 'train/ce_loss': 0.3275390625, 'train/seg_cls_loss': 0.03175048828125, 'train/kl_loss': 0.3447265625, 'train/mask_bce_loss': 0.2864440340548754, 'train/mask_dice_loss': 0.6207661807537079, 'train/mask_loss': 0.9072102129459381, 'metrics/total_secs_per_batch': 5.7537360191345215, 'metrics/data_secs_per_batch': 2.4379958868026734, '_timestamp': 1740954628.7678208}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 102 is less than current step: 499. Dropping entry: {'train/lr': 0.00023246938775510202, '_timestamp': 1740954628.7680893}).
Epoch: [1][104/500]	Time  5.449 ( 5.449)	Loss 0.6016 (1.3456)	CeLoss 0.6016 (0.5252)	SegCLSLoss 0.0000 (0.0080)	KLLoss 0.0000 (0.1619)	MaskLoss 0.0000 (0.4000)	MaskBCELoss 0.0000 (0.0705)	MaskDICELoss 0.0000 (0.3295)
Epoch: [1][105/500]	Time  6.287 ( 6.287)	Loss 1.9022 (2.0038)	CeLoss 0.3066 (0.3593)	SegCLSLoss 0.0156 (0.0192)	KLLoss 0.4102 (0.3559)	MaskLoss 0.7734 (0.7996)	MaskBCELoss 0.1859 (0.2292)	MaskDICELoss 0.5875 (0.5704)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 103 is less than current step: 499. Dropping entry: {'train/loss': 1.3455987334251405, 'train/ce_loss': 0.525244140625, 'train/seg_cls_loss': 0.0079833984375, 'train/kl_loss': 0.1619140625, 'train/mask_bce_loss': 0.07052710391581059, 'train/mask_dice_loss': 0.32949394583702085, 'train/mask_loss': 0.4000210464000702, 'metrics/total_secs_per_batch': 5.4491636753082275, 'metrics/data_secs_per_batch': 2.499893546104431, '_timestamp': 1740954634.2170405}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 103 is less than current step: 499. Dropping entry: {'train/lr': 0.00023234693877551018, '_timestamp': 1740954634.2173214}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 104 is less than current step: 499. Dropping entry: {'train/loss': 2.00383642911911, 'train/ce_loss': 0.35927734375, 'train/seg_cls_loss': 0.019244384765625, 'train/kl_loss': 0.355859375, 'train/mask_bce_loss': 0.2292142391204834, 'train/mask_dice_loss': 0.5704090505838394, 'train/mask_loss': 0.7996232867240906, 'metrics/total_secs_per_batch': 6.2870330810546875, 'metrics/data_secs_per_batch': 2.774215030670166, '_timestamp': 1740954640.5040472}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 104 is less than current step: 499. Dropping entry: {'train/lr': 0.00023222448979591835, '_timestamp': 1740954640.5043206}).
Epoch: [1][106/500]	Time  6.794 ( 6.794)	Loss 2.0619 (1.7088)	CeLoss 0.2373 (0.1983)	SegCLSLoss 0.0247 (0.0195)	KLLoss 0.3809 (0.3156)	MaskLoss 0.8864 (0.7345)	MaskBCELoss 0.1087 (0.1236)	MaskDICELoss 0.7777 (0.6109)
Epoch: [1][107/500]	Time  5.537 ( 5.537)	Loss 1.2477 (1.4463)	CeLoss 0.2656 (0.4205)	SegCLSLoss 0.0159 (0.0122)	KLLoss 0.4141 (0.2410)	MaskLoss 0.4666 (0.4978)	MaskBCELoss 0.0961 (0.0842)	MaskDICELoss 0.3705 (0.4135)
Epoch: [1][108/500]	Time  5.098 ( 5.098)	Loss 1.6271 (1.2939)	CeLoss 0.2637 (0.6188)	SegCLSLoss 0.0188 (0.0083)	KLLoss 0.4102 (0.1584)	MaskLoss 0.6563 (0.3276)	MaskBCELoss 0.2283 (0.0787)	MaskDICELoss 0.4281 (0.2488)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 105 is less than current step: 499. Dropping entry: {'train/loss': 1.7087614417076111, 'train/ce_loss': 0.198291015625, 'train/seg_cls_loss': 0.019482421875, 'train/kl_loss': 0.315625, 'train/mask_bce_loss': 0.12355440258979797, 'train/mask_dice_loss': 0.6109288692474365, 'train/mask_loss': 0.7344832718372345, 'metrics/total_secs_per_batch': 6.7938830852508545, 'metrics/data_secs_per_batch': 3.0065556287765505, '_timestamp': 1740954647.297896}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 105 is less than current step: 499. Dropping entry: {'train/lr': 0.00023210204081632648, '_timestamp': 1740954647.298158}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 106 is less than current step: 499. Dropping entry: {'train/loss': 1.4463164687156678, 'train/ce_loss': 0.4205078125, 'train/seg_cls_loss': 0.012213134765625, 'train/kl_loss': 0.241015625, 'train/mask_bce_loss': 0.08422132655978203, 'train/mask_dice_loss': 0.413546285033226, 'train/mask_loss': 0.49776761829853056, 'metrics/total_secs_per_batch': 5.5372350215911865, 'metrics/data_secs_per_batch': 2.396095085144043, '_timestamp': 1740954652.8354092}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 106 is less than current step: 499. Dropping entry: {'train/lr': 0.00023197959183673465, '_timestamp': 1740954652.8357685}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 107 is less than current step: 499. Dropping entry: {'train/loss': 1.293903648853302, 'train/ce_loss': 0.6187744140625, 'train/seg_cls_loss': 0.008343505859375, 'train/kl_loss': 0.1583984375, 'train/mask_bce_loss': 0.07874042149633169, 'train/mask_dice_loss': 0.24881443083286287, 'train/mask_loss': 0.32755484580993655, 'metrics/total_secs_per_batch': 5.098146200180054, 'metrics/data_secs_per_batch': 2.314901113510132, '_timestamp': 1740954657.9333367}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 107 is less than current step: 499. Dropping entry: {'train/lr': 0.00023185714285714286, '_timestamp': 1740954657.933704}).
Epoch: [1][109/500]	Time  5.837 ( 5.837)	Loss 1.5413 (1.3933)	CeLoss 0.2539 (0.4927)	SegCLSLoss 0.0176 (0.0108)	KLLoss 0.4023 (0.1949)	MaskLoss 0.6193 (0.4378)	MaskBCELoss 0.1630 (0.1124)	MaskDICELoss 0.4563 (0.3254)
[2025-03-02 16:31:08,944] [INFO] [logging.py:96:log_dist] [Rank 0] step=610, skipped=0, lr=[0.0002316734693877551], mom=[(0.9, 0.95)]
[2025-03-02 16:31:08,950] [INFO] [timer.py:215:stop] epoch=0/micro_step=6100/global_step=610, RunningAvgSamplesPerSec=1.4837545747006105, CurrSamplesPerSec=1.9308660121226027, MemAllocated=30.7GB, MaxMemAllocated=36.81GB
Epoch: [1][110/500]	Time  5.181 ( 5.181)	Loss 1.6641 (1.6307)	CeLoss 1.6641 (0.4811)	SegCLSLoss 0.0000 (0.0190)	KLLoss 0.0000 (0.2307)	MaskLoss 0.0000 (0.5585)	MaskBCELoss 0.0000 (0.1162)	MaskDICELoss 0.0000 (0.4423)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 108 is less than current step: 499. Dropping entry: {'train/loss': 1.393328022956848, 'train/ce_loss': 0.492724609375, 'train/seg_cls_loss': 0.010791015625, 'train/kl_loss': 0.194921875, 'train/mask_bce_loss': 0.11238330490887165, 'train/mask_dice_loss': 0.32541840374469755, 'train/mask_loss': 0.43780170679092406, 'metrics/total_secs_per_batch': 5.837206602096558, 'metrics/data_secs_per_batch': 2.4199110746383665, '_timestamp': 1740954663.7704992}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 108 is less than current step: 499. Dropping entry: {'train/lr': 0.00023173469387755103, '_timestamp': 1740954663.7708266}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 109 is less than current step: 499. Dropping entry: {'train/loss': 1.6306991219520568, 'train/ce_loss': 0.481103515625, 'train/seg_cls_loss': 0.01903076171875, 'train/kl_loss': 0.2306640625, 'train/mask_bce_loss': 0.11620326787233352, 'train/mask_dice_loss': 0.4422859251499176, 'train/mask_loss': 0.5584891974925995, 'metrics/total_secs_per_batch': 5.180610179901123, 'metrics/data_secs_per_batch': 2.369283127784729, '_timestamp': 1740954668.9509337}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 109 is less than current step: 499. Dropping entry: {'train/lr': 0.0002316122448979592, '_timestamp': 1740954668.951217}).
Epoch: [1][111/500]	Time  6.295 ( 6.295)	Loss 2.3738 (1.8260)	CeLoss 0.2090 (0.4057)	SegCLSLoss 0.0312 (0.0164)	KLLoss 0.3770 (0.2715)	MaskLoss 1.0560 (0.6926)	MaskBCELoss 0.3207 (0.1292)	MaskDICELoss 0.7353 (0.5634)
Epoch: [1][112/500]	Time  5.410 ( 5.410)	Loss 2.5687 (1.8944)	CeLoss 0.1992 (0.5170)	SegCLSLoss 0.0352 (0.0175)	KLLoss 0.3711 (0.3164)	MaskLoss 1.1574 (0.6685)	MaskBCELoss 0.2994 (0.1523)	MaskDICELoss 0.8581 (0.5162)
Epoch: [1][113/500]	Time  6.652 ( 6.652)	Loss 1.5615 (1.6336)	CeLoss 0.2598 (0.5224)	SegCLSLoss 0.0130 (0.0120)	KLLoss 0.4180 (0.2848)	MaskLoss 0.6274 (0.5383)	MaskBCELoss 0.1940 (0.1181)	MaskDICELoss 0.4334 (0.4202)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 110 is less than current step: 499. Dropping entry: {'train/loss': 1.8259826064109803, 'train/ce_loss': 0.405712890625, 'train/seg_cls_loss': 0.016351318359375, 'train/kl_loss': 0.271484375, 'train/mask_bce_loss': 0.12924268105998635, 'train/mask_dice_loss': 0.5633628904819489, 'train/mask_loss': 0.6926055788993836, 'metrics/total_secs_per_batch': 6.294822454452515, 'metrics/data_secs_per_batch': 2.8574702024459837, '_timestamp': 1740954675.2459571}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 110 is less than current step: 499. Dropping entry: {'train/lr': 0.00023148979591836732, '_timestamp': 1740954675.2463}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 111 is less than current step: 499. Dropping entry: {'train/loss': 1.8944167375564576, 'train/ce_loss': 0.5169921875, 'train/seg_cls_loss': 0.017498779296875, 'train/kl_loss': 0.31640625, 'train/mask_bce_loss': 0.1523081112653017, 'train/mask_dice_loss': 0.5161893188953399, 'train/mask_loss': 0.6684974372386933, 'metrics/total_secs_per_batch': 5.4100329875946045, 'metrics/data_secs_per_batch': 2.293195343017578, '_timestamp': 1740954680.6560225}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 111 is less than current step: 499. Dropping entry: {'train/lr': 0.0002313673469387755, '_timestamp': 1740954680.656296}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 112 is less than current step: 499. Dropping entry: {'train/loss': 1.633611512184143, 'train/ce_loss': 0.52236328125, 'train/seg_cls_loss': 0.01201171875, 'train/kl_loss': 0.284765625, 'train/mask_bce_loss': 0.11809277050197124, 'train/mask_dice_loss': 0.42024617195129393, 'train/mask_loss': 0.5383389472961426, 'metrics/total_secs_per_batch': 6.652464866638184, 'metrics/data_secs_per_batch': 3.065725541114807, '_timestamp': 1740954687.3084521}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 112 is less than current step: 499. Dropping entry: {'train/lr': 0.00023124489795918365, '_timestamp': 1740954687.308795}).
Epoch: [1][114/500]	Time  6.027 ( 6.027)	Loss 2.3781 (1.8493)	CeLoss 0.1216 (0.4492)	SegCLSLoss 0.0486 (0.0176)	KLLoss 0.3691 (0.2732)	MaskLoss 1.0977 (0.6818)	MaskBCELoss 0.1444 (0.1205)	MaskDICELoss 0.9533 (0.5613)
Epoch: [1][115/500]	Time  5.742 ( 5.742)	Loss 1.5469 (2.1118)	CeLoss 1.5469 (0.4685)	SegCLSLoss 0.0000 (0.0195)	KLLoss 0.0000 (0.3197)	MaskLoss 0.0000 (0.8009)	MaskBCELoss 0.0000 (0.1567)	MaskDICELoss 0.0000 (0.6442)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 113 is less than current step: 499. Dropping entry: {'train/loss': 1.849296247959137, 'train/ce_loss': 0.449169921875, 'train/seg_cls_loss': 0.01759033203125, 'train/kl_loss': 0.2732421875, 'train/mask_bce_loss': 0.12054210342466831, 'train/mask_dice_loss': 0.5612837493419647, 'train/mask_loss': 0.6818258583545684, 'metrics/total_secs_per_batch': 6.026753187179565, 'metrics/data_secs_per_batch': 2.723659634590149, '_timestamp': 1740954693.3352451}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 113 is less than current step: 499. Dropping entry: {'train/lr': 0.00023112244897959181, '_timestamp': 1740954693.3355262}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 114 is less than current step: 499. Dropping entry: {'train/loss': 2.1118040561676024, 'train/ce_loss': 0.46845703125, 'train/seg_cls_loss': 0.019476318359375, 'train/kl_loss': 0.3197265625, 'train/mask_bce_loss': 0.1566760517656803, 'train/mask_dice_loss': 0.6441966980695725, 'train/mask_loss': 0.8008727431297302, 'metrics/total_secs_per_batch': 5.742431163787842, 'metrics/data_secs_per_batch': 2.414242458343506, '_timestamp': 1740954699.0776148}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 114 is less than current step: 499. Dropping entry: {'train/lr': 0.00023099999999999998, '_timestamp': 1740954699.0779114}).
Epoch: [1][116/500]	Time  5.904 ( 5.904)	Loss 1.9235 (1.7982)	CeLoss 0.2969 (0.3979)	SegCLSLoss 0.0142 (0.0203)	KLLoss 0.4043 (0.2736)	MaskLoss 0.7889 (0.6813)	MaskBCELoss 0.3008 (0.1330)	MaskDICELoss 0.4881 (0.5483)
Epoch: [1][117/500]	Time  6.693 ( 6.693)	Loss 1.9366 (2.1708)	CeLoss 0.2305 (0.2388)	SegCLSLoss 0.0320 (0.0268)	KLLoss 0.4141 (0.3971)	MaskLoss 0.8247 (0.9394)	MaskBCELoss 0.2227 (0.1249)	MaskDICELoss 0.6021 (0.8145)
Epoch: [1][118/500]	Time  5.494 ( 5.494)	Loss 3.1582 (1.9965)	CeLoss 0.1553 (0.2835)	SegCLSLoss 0.0469 (0.0249)	KLLoss 0.3789 (0.3090)	MaskLoss 1.4707 (0.8348)	MaskBCELoss 0.7326 (0.1443)	MaskDICELoss 0.7381 (0.6905)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 115 is less than current step: 499. Dropping entry: {'train/loss': 1.7981919288635253, 'train/ce_loss': 0.39794921875, 'train/seg_cls_loss': 0.020306396484375, 'train/kl_loss': 0.2736328125, 'train/mask_bce_loss': 0.1330106308683753, 'train/mask_dice_loss': 0.5483118951320648, 'train/mask_loss': 0.6813225269317627, 'metrics/total_secs_per_batch': 5.903982162475586, 'metrics/data_secs_per_batch': 2.6956833839416503, '_timestamp': 1740954704.981555}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 115 is less than current step: 499. Dropping entry: {'train/lr': 0.00023087755102040814, '_timestamp': 1740954704.981815}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 116 is less than current step: 499. Dropping entry: {'train/loss': 2.1708150029182436, 'train/ce_loss': 0.23876953125, 'train/seg_cls_loss': 0.026812744140625, 'train/kl_loss': 0.3970703125, 'train/mask_bce_loss': 0.12489906288683414, 'train/mask_dice_loss': 0.8145123422145844, 'train/mask_loss': 0.9394113957881928, 'metrics/total_secs_per_batch': 6.692733526229858, 'metrics/data_secs_per_batch': 3.1134918928146362, '_timestamp': 1740954711.6743631}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 116 is less than current step: 499. Dropping entry: {'train/lr': 0.0002307551020408163, '_timestamp': 1740954711.6746364}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 117 is less than current step: 499. Dropping entry: {'train/loss': 1.9965367197990418, 'train/ce_loss': 0.283544921875, 'train/seg_cls_loss': 0.0248779296875, 'train/kl_loss': 0.308984375, 'train/mask_bce_loss': 0.1442849848419428, 'train/mask_dice_loss': 0.690506786108017, 'train/mask_loss': 0.8347917795181274, 'metrics/total_secs_per_batch': 5.4940245151519775, 'metrics/data_secs_per_batch': 2.4459454298019407, '_timestamp': 1740954717.1686063}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 117 is less than current step: 499. Dropping entry: {'train/lr': 0.00023063265306122447, '_timestamp': 1740954717.168945}).
Epoch: [1][119/500]	Time  6.334 ( 6.334)	Loss 1.3638 (1.8315)	CeLoss 0.2188 (0.3935)	SegCLSLoss 0.0143 (0.0147)	KLLoss 0.4375 (0.3240)	MaskLoss 0.5471 (0.6992)	MaskBCELoss 0.2668 (0.1604)	MaskDICELoss 0.2803 (0.5389)
[2025-03-02 16:32:10,299] [INFO] [logging.py:96:log_dist] [Rank 0] step=620, skipped=0, lr=[0.00023044897959183672], mom=[(0.9, 0.95)]
[2025-03-02 16:32:10,305] [INFO] [timer.py:215:stop] epoch=0/micro_step=6200/global_step=620, RunningAvgSamplesPerSec=1.4859143586447003, CurrSamplesPerSec=1.470226994096208, MemAllocated=31.25GB, MaxMemAllocated=36.81GB
Epoch: [1][120/500]	Time  6.803 ( 6.803)	Loss 2.0452 (1.9388)	CeLoss 0.2637 (0.2657)	SegCLSLoss 0.0237 (0.0193)	KLLoss 0.3887 (0.3658)	MaskLoss 0.8654 (0.8135)	MaskBCELoss 0.1214 (0.1664)	MaskDICELoss 0.7439 (0.6471)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 118 is less than current step: 499. Dropping entry: {'train/loss': 1.8314714670181274, 'train/ce_loss': 0.39345703125, 'train/seg_cls_loss': 0.014715576171875, 'train/kl_loss': 0.3240234375, 'train/mask_bce_loss': 0.16035172045230867, 'train/mask_dice_loss': 0.5388801217079162, 'train/mask_loss': 0.6992318391799927, 'metrics/total_secs_per_batch': 6.334168910980225, 'metrics/data_secs_per_batch': 3.1369154930114744, '_timestamp': 1740954723.502582}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 118 is less than current step: 499. Dropping entry: {'train/lr': 0.00023051020408163263, '_timestamp': 1740954723.502778}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 119 is less than current step: 499. Dropping entry: {'train/loss': 1.9388170003890992, 'train/ce_loss': 0.26572265625, 'train/seg_cls_loss': 0.01927490234375, 'train/kl_loss': 0.3658203125, 'train/mask_bce_loss': 0.16637541204690934, 'train/mask_dice_loss': 0.6471248745918274, 'train/mask_loss': 0.8135002970695495, 'metrics/total_secs_per_batch': 6.80319356918335, 'metrics/data_secs_per_batch': 3.1189778327941893, '_timestamp': 1740954730.3055735}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 119 is less than current step: 499. Dropping entry: {'train/lr': 0.0002303877551020408, '_timestamp': 1740954730.305851}).
Epoch: [1][121/500]	Time  5.328 ( 5.328)	Loss 1.7758 (1.4885)	CeLoss 0.2168 (0.4666)	SegCLSLoss 0.0159 (0.0117)	KLLoss 0.4180 (0.2434)	MaskLoss 0.7551 (0.4958)	MaskBCELoss 0.1274 (0.0338)	MaskDICELoss 0.6277 (0.4620)
Epoch: [1][122/500]	Time  6.724 ( 6.724)	Loss 1.8125 (1.9083)	CeLoss 1.8125 (0.4137)	SegCLSLoss 0.0000 (0.0169)	KLLoss 0.0000 (0.2775)	MaskLoss 0.0000 (0.7292)	MaskBCELoss 0.0000 (0.2019)	MaskDICELoss 0.0000 (0.5273)
Epoch: [1][123/500]	Time  5.964 ( 5.964)	Loss 0.0635 (2.0801)	CeLoss 0.0635 (0.3255)	SegCLSLoss 0.0000 (0.0188)	KLLoss 0.0000 (0.3342)	MaskLoss 0.0000 (0.8559)	MaskBCELoss 0.0000 (0.1691)	MaskDICELoss 0.0000 (0.6868)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 120 is less than current step: 499. Dropping entry: {'train/loss': 1.4885276913642884, 'train/ce_loss': 0.4666015625, 'train/seg_cls_loss': 0.01171875, 'train/kl_loss': 0.243359375, 'train/mask_bce_loss': 0.033844263106584546, 'train/mask_dice_loss': 0.4619820863008499, 'train/mask_loss': 0.49582635164260863, 'metrics/total_secs_per_batch': 5.327990531921387, 'metrics/data_secs_per_batch': 2.467483639717102, '_timestamp': 1740954735.6338325}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 120 is less than current step: 499. Dropping entry: {'train/lr': 0.00023026530612244896, '_timestamp': 1740954735.6340308}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 121 is less than current step: 499. Dropping entry: {'train/loss': 1.9083250761032104, 'train/ce_loss': 0.413671875, 'train/seg_cls_loss': 0.016851806640625, 'train/kl_loss': 0.2775390625, 'train/mask_bce_loss': 0.20188123695552349, 'train/mask_dice_loss': 0.5273301362991333, 'train/mask_loss': 0.7292113661766052, 'metrics/total_secs_per_batch': 6.723884582519531, 'metrics/data_secs_per_batch': 3.12346293926239, '_timestamp': 1740954742.3576436}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 121 is less than current step: 499. Dropping entry: {'train/lr': 0.00023014285714285712, '_timestamp': 1740954742.3579204}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 122 is less than current step: 499. Dropping entry: {'train/loss': 2.08012056350708, 'train/ce_loss': 0.32548828125, 'train/seg_cls_loss': 0.01876220703125, 'train/kl_loss': 0.3341796875, 'train/mask_bce_loss': 0.1691459539346397, 'train/mask_dice_loss': 0.6867834746837616, 'train/mask_loss': 0.855929434299469, 'metrics/total_secs_per_batch': 5.96416449546814, 'metrics/data_secs_per_batch': 2.8320716857910155, '_timestamp': 1740954748.3218508}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 122 is less than current step: 499. Dropping entry: {'train/lr': 0.00023002040816326528, '_timestamp': 1740954748.3221493}).
Epoch: [1][124/500]	Time  7.515 ( 7.515)	Loss 2.2529 (2.1342)	CeLoss 0.2168 (0.3281)	SegCLSLoss 0.0219 (0.0225)	KLLoss 0.3867 (0.3590)	MaskLoss 0.9937 (0.8796)	MaskBCELoss 0.0201 (0.1237)	MaskDICELoss 0.9736 (0.7558)
Epoch: [1][125/500]	Time  5.950 ( 5.950)	Loss 1.8496 (1.9887)	CeLoss 0.3652 (0.4965)	SegCLSLoss 0.0141 (0.0198)	KLLoss 0.4258 (0.3301)	MaskLoss 0.7168 (0.7243)	MaskBCELoss 0.1071 (0.1728)	MaskDICELoss 0.6097 (0.5515)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 123 is less than current step: 499. Dropping entry: {'train/loss': 2.1342293977737428, 'train/ce_loss': 0.328125, 'train/seg_cls_loss': 0.02249755859375, 'train/kl_loss': 0.358984375, 'train/mask_bce_loss': 0.12373608308844268, 'train/mask_dice_loss': 0.755829781293869, 'train/mask_loss': 0.8795658648014069, 'metrics/total_secs_per_batch': 7.515271425247192, 'metrics/data_secs_per_batch': 3.4891114234924316, '_timestamp': 1740954755.83713}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 123 is less than current step: 499. Dropping entry: {'train/lr': 0.00022989795918367344, '_timestamp': 1740954755.8374097}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 124 is less than current step: 499. Dropping entry: {'train/loss': 1.9886659860610962, 'train/ce_loss': 0.496533203125, 'train/seg_cls_loss': 0.0197998046875, 'train/kl_loss': 0.330078125, 'train/mask_bce_loss': 0.17277200082316996, 'train/mask_dice_loss': 0.5515414565801621, 'train/mask_loss': 0.7243134558200837, 'metrics/total_secs_per_batch': 5.950390100479126, 'metrics/data_secs_per_batch': 2.4174564599990847, '_timestamp': 1740954761.7877007}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 124 is less than current step: 499. Dropping entry: {'train/lr': 0.0002297755102040816, '_timestamp': 1740954761.788049}).
Epoch: [1][126/500]	Time  6.953 ( 6.953)	Loss 0.2832 (1.6812)	CeLoss 0.2832 (0.5830)	SegCLSLoss 0.0000 (0.0133)	KLLoss 0.0000 (0.2420)	MaskLoss 0.0000 (0.5337)	MaskBCELoss 0.0000 (0.0570)	MaskDICELoss 0.0000 (0.4767)
Epoch: [1][127/500]	Time  5.164 ( 5.164)	Loss 2.8626 (1.5667)	CeLoss 0.2773 (0.4813)	SegCLSLoss 0.0120 (0.0108)	KLLoss 0.4102 (0.2475)	MaskLoss 1.2692 (0.5277)	MaskBCELoss 0.5968 (0.1579)	MaskDICELoss 0.6724 (0.3698)
Epoch: [1][128/500]	Time  5.419 ( 5.419)	Loss 1.0312 (1.5641)	CeLoss 1.0312 (0.6576)	SegCLSLoss 0.0000 (0.0120)	KLLoss 0.0000 (0.2076)	MaskLoss 0.0000 (0.4399)	MaskBCELoss 0.0000 (0.0653)	MaskDICELoss 0.0000 (0.3746)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 125 is less than current step: 499. Dropping entry: {'train/loss': 1.6812205195426941, 'train/ce_loss': 0.5830078125, 'train/seg_cls_loss': 0.013348388671875, 'train/kl_loss': 0.2419921875, 'train/mask_bce_loss': 0.056962536787614225, 'train/mask_dice_loss': 0.47671413868665696, 'train/mask_loss': 0.5336766809225082, 'metrics/total_secs_per_batch': 6.952784538269043, 'metrics/data_secs_per_batch': 3.3076678276062013, '_timestamp': 1740954768.7402706}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 125 is less than current step: 499. Dropping entry: {'train/lr': 0.00022965306122448977, '_timestamp': 1740954768.7405794}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 126 is less than current step: 499. Dropping entry: {'train/loss': 1.5666906356811523, 'train/ce_loss': 0.48125, 'train/seg_cls_loss': 0.01075439453125, 'train/kl_loss': 0.2474609375, 'train/mask_bce_loss': 0.15785569548606873, 'train/mask_dice_loss': 0.36982554495334624, 'train/mask_loss': 0.5276812434196472, 'metrics/total_secs_per_batch': 5.164093971252441, 'metrics/data_secs_per_batch': 2.3982353687286375, '_timestamp': 1740954773.9044805}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 126 is less than current step: 499. Dropping entry: {'train/lr': 0.00022953061224489793, '_timestamp': 1740954773.9048676}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 127 is less than current step: 499. Dropping entry: {'train/loss': 1.5641055226325988, 'train/ce_loss': 0.657568359375, 'train/seg_cls_loss': 0.012005615234375, 'train/kl_loss': 0.2076171875, 'train/mask_bce_loss': 0.06532403863966466, 'train/mask_dice_loss': 0.37461445927619935, 'train/mask_loss': 0.4399385035037994, 'metrics/total_secs_per_batch': 5.419226408004761, 'metrics/data_secs_per_batch': 2.5384356498718263, '_timestamp': 1740954779.3237684}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 127 is less than current step: 499. Dropping entry: {'train/lr': 0.0002294081632653061, '_timestamp': 1740954779.3241062}).
Epoch: [1][129/500]	Time  6.833 ( 6.833)	Loss 2.4568 (2.3064)	CeLoss 0.2451 (0.2718)	SegCLSLoss 0.0226 (0.0245)	KLLoss 0.3906 (0.3555)	MaskLoss 1.0809 (0.9936)	MaskBCELoss 0.1757 (0.1902)	MaskDICELoss 0.9052 (0.8034)
[2025-03-02 16:33:11,688] [INFO] [logging.py:96:log_dist] [Rank 0] step=630, skipped=0, lr=[0.00022922448979591833], mom=[(0.9, 0.95)]
[2025-03-02 16:33:11,694] [INFO] [timer.py:215:stop] epoch=0/micro_step=6300/global_step=630, RunningAvgSamplesPerSec=1.48799925890915, CurrSamplesPerSec=1.8060578298795305, MemAllocated=31.14GB, MaxMemAllocated=36.81GB
Epoch: [1][130/500]	Time  5.538 ( 5.538)	Loss 2.8629 (1.9631)	CeLoss 0.1206 (0.5792)	SegCLSLoss 0.0500 (0.0190)	KLLoss 0.3789 (0.2812)	MaskLoss 1.3396 (0.6730)	MaskBCELoss 0.5467 (0.0811)	MaskDICELoss 0.7929 (0.5919)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 128 is less than current step: 499. Dropping entry: {'train/loss': 2.3063655853271485, 'train/ce_loss': 0.27177734375, 'train/seg_cls_loss': 0.024517822265625, 'train/kl_loss': 0.35546875, 'train/mask_bce_loss': 0.19015356730669736, 'train/mask_dice_loss': 0.8034100651741027, 'train/mask_loss': 0.9935636401176453, 'metrics/total_secs_per_batch': 6.83319616317749, 'metrics/data_secs_per_batch': 2.878465747833252, '_timestamp': 1740954786.1568055}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 128 is less than current step: 499. Dropping entry: {'train/lr': 0.00022928571428571426, '_timestamp': 1740954786.157105}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 129 is less than current step: 499. Dropping entry: {'train/loss': 1.9630918145179748, 'train/ce_loss': 0.579248046875, 'train/seg_cls_loss': 0.019024658203125, 'train/kl_loss': 0.28125, 'train/mask_bce_loss': 0.08111163582652807, 'train/mask_dice_loss': 0.591938179731369, 'train/mask_loss': 0.6730498194694519, 'metrics/total_secs_per_batch': 5.538493394851685, 'metrics/data_secs_per_batch': 2.3617660999298096, '_timestamp': 1740954791.6951182}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 129 is less than current step: 499. Dropping entry: {'train/lr': 0.00022916326530612245, '_timestamp': 1740954791.6954}).
Epoch: [1][131/500]	Time  5.906 ( 5.906)	Loss 2.4036 (2.3467)	CeLoss 0.1211 (0.2193)	SegCLSLoss 0.0383 (0.0229)	KLLoss 0.3809 (0.3705)	MaskLoss 1.1124 (1.0393)	MaskBCELoss 0.2203 (0.4680)	MaskDICELoss 0.8921 (0.5713)
Epoch: [1][132/500]	Time  5.867 ( 5.867)	Loss 2.0750 (1.9493)	CeLoss 0.2852 (0.4022)	SegCLSLoss 0.0139 (0.0173)	KLLoss 0.4238 (0.2779)	MaskLoss 0.8695 (0.7552)	MaskBCELoss 0.2774 (0.2094)	MaskDICELoss 0.5921 (0.5458)
Epoch: [1][133/500]	Time  5.210 ( 5.210)	Loss 2.2636 (1.9485)	CeLoss 0.2812 (0.5162)	SegCLSLoss 0.0140 (0.0174)	KLLoss 0.4277 (0.2799)	MaskLoss 0.9658 (0.6977)	MaskBCELoss 0.1841 (0.1021)	MaskDICELoss 0.7817 (0.5957)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 130 is less than current step: 499. Dropping entry: {'train/loss': 2.34667763710022, 'train/ce_loss': 0.219287109375, 'train/seg_cls_loss': 0.022943115234375, 'train/kl_loss': 0.3705078125, 'train/mask_bce_loss': 0.46798123717308043, 'train/mask_dice_loss': 0.5712999880313874, 'train/mask_loss': 1.0392812013626098, 'metrics/total_secs_per_batch': 5.905836582183838, 'metrics/data_secs_per_batch': 2.7322859287261965, '_timestamp': 1740954797.601323}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 130 is less than current step: 499. Dropping entry: {'train/lr': 0.0002290408163265306, '_timestamp': 1740954797.6016734}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 131 is less than current step: 499. Dropping entry: {'train/loss': 1.949300467967987, 'train/ce_loss': 0.402197265625, 'train/seg_cls_loss': 0.017340087890625, 'train/kl_loss': 0.2779296875, 'train/mask_bce_loss': 0.20935884937644006, 'train/mask_dice_loss': 0.5458089709281921, 'train/mask_loss': 0.755167818069458, 'metrics/total_secs_per_batch': 5.867250204086304, 'metrics/data_secs_per_batch': 2.7799127101898193, '_timestamp': 1740954803.4686646}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 131 is less than current step: 499. Dropping entry: {'train/lr': 0.00022891836734693878, '_timestamp': 1740954803.4690244}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 132 is less than current step: 499. Dropping entry: {'train/loss': 1.9485056400299072, 'train/ce_loss': 0.5162109375, 'train/seg_cls_loss': 0.017376708984375, 'train/kl_loss': 0.2798828125, 'train/mask_bce_loss': 0.10206990912556649, 'train/mask_dice_loss': 0.5956692457199096, 'train/mask_loss': 0.6977391541004181, 'metrics/total_secs_per_batch': 5.210058212280273, 'metrics/data_secs_per_batch': 2.2675667285919188, '_timestamp': 1740954808.6784654}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 132 is less than current step: 499. Dropping entry: {'train/lr': 0.00022879591836734694, '_timestamp': 1740954808.6787505}).
Epoch: [1][134/500]	Time  6.793 ( 6.793)	Loss 1.8716 (1.8204)	CeLoss 0.2578 (0.3990)	SegCLSLoss 0.0199 (0.0174)	KLLoss 0.4141 (0.3271)	MaskLoss 0.7815 (0.6899)	MaskBCELoss 0.2245 (0.1142)	MaskDICELoss 0.5570 (0.5757)
Epoch: [1][135/500]	Time  6.001 ( 6.001)	Loss 2.2613 (1.9223)	CeLoss 0.3184 (0.3857)	SegCLSLoss 0.0128 (0.0248)	KLLoss 0.4277 (0.3211)	MaskLoss 0.9471 (0.7460)	MaskBCELoss 0.2292 (0.1547)	MaskDICELoss 0.7179 (0.5912)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 133 is less than current step: 499. Dropping entry: {'train/loss': 1.8203502416610717, 'train/ce_loss': 0.3990234375, 'train/seg_cls_loss': 0.017425537109375, 'train/kl_loss': 0.3271484375, 'train/mask_bce_loss': 0.11419827900826932, 'train/mask_dice_loss': 0.575713163614273, 'train/mask_loss': 0.6899114429950715, 'metrics/total_secs_per_batch': 6.792882919311523, 'metrics/data_secs_per_batch': 2.7091745376586913, '_timestamp': 1740954815.47132}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 133 is less than current step: 499. Dropping entry: {'train/lr': 0.0002286734693877551, '_timestamp': 1740954815.4716032}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 134 is less than current step: 499. Dropping entry: {'train/loss': 1.9222757160663604, 'train/ce_loss': 0.3857421875, 'train/seg_cls_loss': 0.024835205078125, 'train/kl_loss': 0.32109375, 'train/mask_bce_loss': 0.15471706744283437, 'train/mask_dice_loss': 0.5912352487444877, 'train/mask_loss': 0.745952308177948, 'metrics/total_secs_per_batch': 6.0005128383636475, 'metrics/data_secs_per_batch': 2.5962815284729004, '_timestamp': 1740954821.4718752}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 134 is less than current step: 499. Dropping entry: {'train/lr': 0.00022855102040816327, '_timestamp': 1740954821.4721644}).
Epoch: [1][136/500]	Time  6.247 ( 6.247)	Loss 2.3480 (1.7404)	CeLoss 0.2422 (0.4306)	SegCLSLoss 0.0204 (0.0165)	KLLoss 0.4102 (0.2816)	MaskLoss 1.0275 (0.6366)	MaskBCELoss 0.0636 (0.1047)	MaskDICELoss 0.9639 (0.5319)
Epoch: [1][137/500]	Time  5.464 ( 5.464)	Loss 2.5711 (2.0452)	CeLoss 0.2539 (0.5473)	SegCLSLoss 0.0347 (0.0177)	KLLoss 0.3711 (0.2809)	MaskLoss 1.1312 (0.7305)	MaskBCELoss 0.2118 (0.1809)	MaskDICELoss 0.9194 (0.5496)
Epoch: [1][138/500]	Time  5.482 ( 5.482)	Loss 0.6055 (1.8051)	CeLoss 0.6055 (0.4732)	SegCLSLoss 0.0000 (0.0154)	KLLoss 0.0000 (0.2414)	MaskLoss 0.0000 (0.6500)	MaskBCELoss 0.0000 (0.1844)	MaskDICELoss 0.0000 (0.4656)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 135 is less than current step: 499. Dropping entry: {'train/loss': 1.7403608798980712, 'train/ce_loss': 0.43056640625, 'train/seg_cls_loss': 0.016473388671875, 'train/kl_loss': 0.281640625, 'train/mask_bce_loss': 0.10468729212880135, 'train/mask_dice_loss': 0.5319482266902924, 'train/mask_loss': 0.6366355180740356, 'metrics/total_secs_per_batch': 6.246759653091431, 'metrics/data_secs_per_batch': 2.700356674194336, '_timestamp': 1740954827.7186756}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 135 is less than current step: 499. Dropping entry: {'train/lr': 0.00022842857142857143, '_timestamp': 1740954827.718955}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 136 is less than current step: 499. Dropping entry: {'train/loss': 2.045162355899811, 'train/ce_loss': 0.547265625, 'train/seg_cls_loss': 0.01767578125, 'train/kl_loss': 0.280859375, 'train/mask_bce_loss': 0.18089732863008975, 'train/mask_dice_loss': 0.5495940238237381, 'train/mask_loss': 0.7304913461208343, 'metrics/total_secs_per_batch': 5.463756322860718, 'metrics/data_secs_per_batch': 2.212361526489258, '_timestamp': 1740954833.1823907}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 136 is less than current step: 499. Dropping entry: {'train/lr': 0.0002283061224489796, '_timestamp': 1740954833.1826737}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 137 is less than current step: 499. Dropping entry: {'train/loss': 1.805061364173889, 'train/ce_loss': 0.4732421875, 'train/seg_cls_loss': 0.0154296875, 'train/kl_loss': 0.24140625, 'train/mask_bce_loss': 0.18440848886966704, 'train/mask_dice_loss': 0.46558313965797427, 'train/mask_loss': 0.6499916136264801, 'metrics/total_secs_per_batch': 5.482315540313721, 'metrics/data_secs_per_batch': 2.316970109939575, '_timestamp': 1740954838.66485}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 137 is less than current step: 499. Dropping entry: {'train/lr': 0.00022818367346938773, '_timestamp': 1740954838.6651886}).
Epoch: [1][139/500]	Time  6.348 ( 6.348)	Loss 1.9434 (2.0557)	CeLoss 0.3340 (0.2486)	SegCLSLoss 0.0146 (0.0207)	KLLoss 0.4180 (0.3613)	MaskLoss 0.7803 (0.8804)	MaskBCELoss 0.1084 (0.1632)	MaskDICELoss 0.6719 (0.7173)
[2025-03-02 16:34:11,804] [INFO] [logging.py:96:log_dist] [Rank 0] step=640, skipped=0, lr=[0.00022799999999999999], mom=[(0.9, 0.95)]
[2025-03-02 16:34:11,811] [INFO] [timer.py:215:stop] epoch=0/micro_step=6400/global_step=640, RunningAvgSamplesPerSec=1.4904676038760616, CurrSamplesPerSec=1.471267464334698, MemAllocated=30.94GB, MaxMemAllocated=36.81GB
Epoch: [1][140/500]	Time  6.799 ( 6.799)	Loss 1.5514 (1.7543)	CeLoss 0.2637 (0.3390)	SegCLSLoss 0.0156 (0.0187)	KLLoss 0.4277 (0.3717)	MaskLoss 0.6185 (0.6845)	MaskBCELoss 0.2114 (0.1534)	MaskDICELoss 0.4071 (0.5311)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 138 is less than current step: 499. Dropping entry: {'train/loss': 2.0557149171829225, 'train/ce_loss': 0.2486328125, 'train/seg_cls_loss': 0.020709228515625, 'train/kl_loss': 0.361328125, 'train/mask_bce_loss': 0.16316994242370128, 'train/mask_dice_loss': 0.7172754049301148, 'train/mask_loss': 0.8804453432559967, 'metrics/total_secs_per_batch': 6.348455429077148, 'metrics/data_secs_per_batch': 2.7265571117401124, '_timestamp': 1740954845.013366}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 138 is less than current step: 499. Dropping entry: {'train/lr': 0.0002280612244897959, '_timestamp': 1740954845.013698}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 139 is less than current step: 499. Dropping entry: {'train/loss': 1.7543382883071899, 'train/ce_loss': 0.33896484375, 'train/seg_cls_loss': 0.0186767578125, 'train/kl_loss': 0.3716796875, 'train/mask_bce_loss': 0.15335753969848157, 'train/mask_dice_loss': 0.5311358243227005, 'train/mask_loss': 0.6844933658838273, 'metrics/total_secs_per_batch': 6.798667907714844, 'metrics/data_secs_per_batch': 3.0007799863815308, '_timestamp': 1740954851.8116543}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 139 is less than current step: 499. Dropping entry: {'train/lr': 0.00022793877551020405, '_timestamp': 1740954851.8119354}).
Epoch: [1][141/500]	Time  5.018 ( 5.018)	Loss 1.1172 (1.9092)	CeLoss 1.1172 (0.3427)	SegCLSLoss 0.0000 (0.0185)	KLLoss 0.0000 (0.3725)	MaskLoss 0.0000 (0.7602)	MaskBCELoss 0.0000 (0.1448)	MaskDICELoss 0.0000 (0.6155)
Epoch: [1][142/500]	Time  6.358 ( 6.358)	Loss 1.9368 (1.6201)	CeLoss 0.2520 (0.3323)	SegCLSLoss 0.0138 (0.0139)	KLLoss 0.4434 (0.2922)	MaskLoss 0.8161 (0.6257)	MaskBCELoss 0.2362 (0.1469)	MaskDICELoss 0.5798 (0.4788)
Epoch: [1][143/500]	Time  6.371 ( 6.371)	Loss 2.0724 (1.9884)	CeLoss 0.2656 (0.3857)	SegCLSLoss 0.0142 (0.0148)	KLLoss 0.4102 (0.3320)	MaskLoss 0.8790 (0.7809)	MaskBCELoss 0.2506 (0.2750)	MaskDICELoss 0.6284 (0.5059)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 140 is less than current step: 499. Dropping entry: {'train/loss': 1.9091510772705078, 'train/ce_loss': 0.34267578125, 'train/seg_cls_loss': 0.018511962890625, 'train/kl_loss': 0.3724609375, 'train/mask_bce_loss': 0.1447554398328066, 'train/mask_dice_loss': 0.6154841721057892, 'train/mask_loss': 0.7602396070957184, 'metrics/total_secs_per_batch': 5.017521619796753, 'metrics/data_secs_per_batch': 2.323940873146057, '_timestamp': 1740954856.8293772}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 140 is less than current step: 499. Dropping entry: {'train/lr': 0.00022781632653061222, '_timestamp': 1740954856.829665}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 141 is less than current step: 499. Dropping entry: {'train/loss': 1.6201391696929932, 'train/ce_loss': 0.33232421875, 'train/seg_cls_loss': 0.013909912109375, 'train/kl_loss': 0.2921875, 'train/mask_bce_loss': 0.14692991375923156, 'train/mask_dice_loss': 0.478813499212265, 'train/mask_loss': 0.6257434129714966, 'metrics/total_secs_per_batch': 6.357814073562622, 'metrics/data_secs_per_batch': 2.8110637187957765, '_timestamp': 1740954863.187427}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 141 is less than current step: 499. Dropping entry: {'train/lr': 0.00022769387755102038, '_timestamp': 1740954863.1877897}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 142 is less than current step: 499. Dropping entry: {'train/loss': 1.9884286642074585, 'train/ce_loss': 0.3857421875, 'train/seg_cls_loss': 0.014788818359375, 'train/kl_loss': 0.33203125, 'train/mask_bce_loss': 0.27497670501470567, 'train/mask_dice_loss': 0.5059075355529785, 'train/mask_loss': 0.7808842420578003, 'metrics/total_secs_per_batch': 6.371241331100464, 'metrics/data_secs_per_batch': 3.078408885002136, '_timestamp': 1740954869.5584455}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 142 is less than current step: 499. Dropping entry: {'train/lr': 0.00022757142857142854, '_timestamp': 1740954869.5588078}).
Epoch: [1][144/500]	Time  4.607 ( 4.607)	Loss 0.6328 (1.6159)	CeLoss 0.6328 (0.7282)	SegCLSLoss 0.0000 (0.0057)	KLLoss 0.0000 (0.1701)	MaskLoss 0.0000 (0.4339)	MaskBCELoss 0.0000 (0.1787)	MaskDICELoss 0.0000 (0.2552)
Epoch: [1][145/500]	Time  5.781 ( 5.781)	Loss 1.0537 (1.6323)	CeLoss 0.2441 (0.3723)	SegCLSLoss 0.0232 (0.0162)	KLLoss 0.4102 (0.3307)	MaskLoss 0.3784 (0.6095)	MaskBCELoss 0.1519 (0.0787)	MaskDICELoss 0.2265 (0.5308)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 143 is less than current step: 499. Dropping entry: {'train/loss': 1.615884828567505, 'train/ce_loss': 0.72822265625, 'train/seg_cls_loss': 0.005718994140625, 'train/kl_loss': 0.1701171875, 'train/mask_bce_loss': 0.17872500270605088, 'train/mask_dice_loss': 0.25519396662712096, 'train/mask_loss': 0.4339189827442169, 'metrics/total_secs_per_batch': 4.606597423553467, 'metrics/data_secs_per_batch': 2.0758970260620115, '_timestamp': 1740954874.1650622}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 143 is less than current step: 499. Dropping entry: {'train/lr': 0.0002274489795918367, '_timestamp': 1740954874.1653605}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 144 is less than current step: 499. Dropping entry: {'train/loss': 1.6323192298412323, 'train/ce_loss': 0.372265625, 'train/seg_cls_loss': 0.016180419921875, 'train/kl_loss': 0.3306640625, 'train/mask_bce_loss': 0.0787123827263713, 'train/mask_dice_loss': 0.5308066070079803, 'train/mask_loss': 0.6095189958810806, 'metrics/total_secs_per_batch': 5.78084921836853, 'metrics/data_secs_per_batch': 2.4830830335617065, '_timestamp': 1740954879.9461465}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 144 is less than current step: 499. Dropping entry: {'train/lr': 0.00022732653061224487, '_timestamp': 1740954879.9464972}).
Epoch: [1][146/500]	Time  6.303 ( 6.303)	Loss 1.4423 (1.8563)	CeLoss 0.2061 (0.3584)	SegCLSLoss 0.0182 (0.0159)	KLLoss 0.4062 (0.3248)	MaskLoss 0.5932 (0.7287)	MaskBCELoss 0.1765 (0.1670)	MaskDICELoss 0.4168 (0.5618)
Epoch: [1][147/500]	Time  6.484 ( 6.484)	Loss 2.0200 (1.7891)	CeLoss 0.2578 (0.2858)	SegCLSLoss 0.0146 (0.0167)	KLLoss 0.4258 (0.3281)	MaskLoss 0.8557 (0.7308)	MaskBCELoss 0.1768 (0.1515)	MaskDICELoss 0.6788 (0.5793)
Epoch: [1][148/500]	Time  5.805 ( 5.805)	Loss 1.7569 (1.7370)	CeLoss 0.2695 (0.5346)	SegCLSLoss 0.0168 (0.0167)	KLLoss 0.4316 (0.2437)	MaskLoss 0.7183 (0.5848)	MaskBCELoss 0.0896 (0.1160)	MaskDICELoss 0.6287 (0.4688)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 145 is less than current step: 499. Dropping entry: {'train/loss': 1.856313693523407, 'train/ce_loss': 0.3583984375, 'train/seg_cls_loss': 0.015850830078125, 'train/kl_loss': 0.3248046875, 'train/mask_bce_loss': 0.16697746887803078, 'train/mask_dice_loss': 0.5617653280496597, 'train/mask_loss': 0.7287427961826325, 'metrics/total_secs_per_batch': 6.303218364715576, 'metrics/data_secs_per_batch': 2.9151381492614745, '_timestamp': 1740954886.2491543}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 145 is less than current step: 499. Dropping entry: {'train/lr': 0.00022720408163265303, '_timestamp': 1740954886.2494462}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 146 is less than current step: 499. Dropping entry: {'train/loss': 1.7890754461288452, 'train/ce_loss': 0.28583984375, 'train/seg_cls_loss': 0.016741943359375, 'train/kl_loss': 0.328125, 'train/mask_bce_loss': 0.15152867864817382, 'train/mask_dice_loss': 0.5792883604764938, 'train/mask_loss': 0.7308170318603515, 'metrics/total_secs_per_batch': 6.484194755554199, 'metrics/data_secs_per_batch': 2.9310714483261107, '_timestamp': 1740954892.7333472}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 146 is less than current step: 499. Dropping entry: {'train/lr': 0.0002270816326530612, '_timestamp': 1740954892.7336402}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 147 is less than current step: 499. Dropping entry: {'train/loss': 1.7369838237762452, 'train/ce_loss': 0.5345703125, 'train/seg_cls_loss': 0.016668701171875, 'train/kl_loss': 0.24375, 'train/mask_bce_loss': 0.11604987010359764, 'train/mask_dice_loss': 0.46875064373016356, 'train/mask_loss': 0.5848005056381226, 'metrics/total_secs_per_batch': 5.805309295654297, 'metrics/data_secs_per_batch': 2.488319683074951, '_timestamp': 1740954898.5388775}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 147 is less than current step: 499. Dropping entry: {'train/lr': 0.00022695918367346936, '_timestamp': 1740954898.5392463}).
Epoch: [1][149/500]	Time  7.113 ( 7.113)	Loss 2.0596 (1.8321)	CeLoss 0.4141 (0.3668)	SegCLSLoss 0.0153 (0.0174)	KLLoss 0.4336 (0.2902)	MaskLoss 0.7974 (0.7140)	MaskBCELoss 0.1294 (0.1782)	MaskDICELoss 0.6680 (0.5358)
[2025-03-02 16:35:11,823] [INFO] [logging.py:96:log_dist] [Rank 0] step=650, skipped=0, lr=[0.0002267755102040816], mom=[(0.9, 0.95)]
[2025-03-02 16:35:11,829] [INFO] [timer.py:215:stop] epoch=0/micro_step=6500/global_step=650, RunningAvgSamplesPerSec=1.4929015817286617, CurrSamplesPerSec=1.6189390248849638, MemAllocated=30.69GB, MaxMemAllocated=36.81GB
Epoch: [1][150/500]	Time  6.179 ( 6.179)	Loss 1.7109 (2.0824)	CeLoss 1.7109 (0.5642)	SegCLSLoss 0.0000 (0.0159)	KLLoss 0.0000 (0.2781)	MaskLoss 0.0000 (0.7414)	MaskBCELoss 0.0000 (0.1540)	MaskDICELoss 0.0000 (0.5874)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 148 is less than current step: 499. Dropping entry: {'train/loss': 1.8321377277374267, 'train/ce_loss': 0.366796875, 'train/seg_cls_loss': 0.01741943359375, 'train/kl_loss': 0.290234375, 'train/mask_bce_loss': 0.17823981493711472, 'train/mask_dice_loss': 0.5357782542705536, 'train/mask_loss': 0.7140180766582489, 'metrics/total_secs_per_batch': 7.11266016960144, 'metrics/data_secs_per_batch': 3.235536313056946, '_timestamp': 1740954905.651393}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 148 is less than current step: 499. Dropping entry: {'train/lr': 0.00022683673469387752, '_timestamp': 1740954905.651613}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 149 is less than current step: 499. Dropping entry: {'train/loss': 2.0823782444000245, 'train/ce_loss': 0.56416015625, 'train/seg_cls_loss': 0.015875244140625, 'train/kl_loss': 0.278125, 'train/mask_bce_loss': 0.15402431227266788, 'train/mask_dice_loss': 0.5873601317405701, 'train/mask_loss': 0.7413844466209412, 'metrics/total_secs_per_batch': 6.17862606048584, 'metrics/data_secs_per_batch': 2.7826366424560547, '_timestamp': 1740954911.829857}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 149 is less than current step: 499. Dropping entry: {'train/lr': 0.00022671428571428568, '_timestamp': 1740954911.8302355}).
Epoch: [1][151/500]	Time  6.278 ( 6.278)	Loss 1.7335 (1.3829)	CeLoss 0.1670 (0.2468)	SegCLSLoss 0.0248 (0.0177)	KLLoss 0.4004 (0.2846)	MaskLoss 0.7569 (0.5495)	MaskBCELoss 0.0728 (0.1006)	MaskDICELoss 0.6840 (0.4489)
Epoch: [1][152/500]	Time  5.287 ( 5.287)	Loss 1.8391 (1.9509)	CeLoss 0.2148 (0.5217)	SegCLSLoss 0.0130 (0.0169)	KLLoss 0.4316 (0.2879)	MaskLoss 0.7867 (0.6959)	MaskBCELoss 0.1636 (0.1538)	MaskDICELoss 0.6232 (0.5421)
Epoch: [1][153/500]	Time  4.281 ( 4.281)	Loss 1.1094 (1.5593)	CeLoss 1.1094 (0.7316)	SegCLSLoss 0.0000 (0.0107)	KLLoss 0.0000 (0.1592)	MaskLoss 0.0000 (0.4033)	MaskBCELoss 0.0000 (0.0801)	MaskDICELoss 0.0000 (0.3232)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 150 is less than current step: 499. Dropping entry: {'train/loss': 1.382933896780014, 'train/ce_loss': 0.24677734375, 'train/seg_cls_loss': 0.017730712890625, 'train/kl_loss': 0.2845703125, 'train/mask_bce_loss': 0.10060564428567886, 'train/mask_dice_loss': 0.44886912778019905, 'train/mask_loss': 0.5494747728109359, 'metrics/total_secs_per_batch': 6.277995586395264, 'metrics/data_secs_per_batch': 2.8617618560791014, '_timestamp': 1740954918.107959}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 150 is less than current step: 499. Dropping entry: {'train/lr': 0.00022659183673469385, '_timestamp': 1740954918.1082437}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 151 is less than current step: 499. Dropping entry: {'train/loss': 1.950895118713379, 'train/ce_loss': 0.5216796875, 'train/seg_cls_loss': 0.016937255859375, 'train/kl_loss': 0.287890625, 'train/mask_bce_loss': 0.15377078130841254, 'train/mask_dice_loss': 0.542086946964264, 'train/mask_loss': 0.6958577275276184, 'metrics/total_secs_per_batch': 5.287295818328857, 'metrics/data_secs_per_batch': 2.332058238983154, '_timestamp': 1740954923.3955038}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 151 is less than current step: 499. Dropping entry: {'train/lr': 0.000226469387755102, '_timestamp': 1740954923.3958662}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 152 is less than current step: 499. Dropping entry: {'train/loss': 1.5593316674232482, 'train/ce_loss': 0.731640625, 'train/seg_cls_loss': 0.0107177734375, 'train/kl_loss': 0.1591796875, 'train/mask_bce_loss': 0.08013210594654083, 'train/mask_dice_loss': 0.32316653728485106, 'train/mask_loss': 0.40329864621162415, 'metrics/total_secs_per_batch': 4.280837059020996, 'metrics/data_secs_per_batch': 2.0813204765319826, '_timestamp': 1740954927.6761549}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 152 is less than current step: 499. Dropping entry: {'train/lr': 0.0002263469387755102, '_timestamp': 1740954927.676532}).
Epoch: [1][154/500]	Time  6.561 ( 6.561)	Loss 2.2916 (2.0354)	CeLoss 0.2441 (0.3140)	SegCLSLoss 0.0243 (0.0205)	KLLoss 0.3965 (0.3605)	MaskLoss 0.9983 (0.8376)	MaskBCELoss 0.0332 (0.1746)	MaskDICELoss 0.9651 (0.6630)
Epoch: [1][155/500]	Time  6.767 ( 6.767)	Loss 2.5691 (2.2516)	CeLoss 0.1719 (0.2136)	SegCLSLoss 0.0356 (0.0248)	KLLoss 0.3887 (0.4037)	MaskLoss 1.1703 (0.9924)	MaskBCELoss 0.5935 (0.2557)	MaskDICELoss 0.5768 (0.7368)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 153 is less than current step: 499. Dropping entry: {'train/loss': 2.0353864192962647, 'train/ce_loss': 0.31396484375, 'train/seg_cls_loss': 0.020489501953125, 'train/kl_loss': 0.360546875, 'train/mask_bce_loss': 0.17464946988038718, 'train/mask_dice_loss': 0.662965577095747, 'train/mask_loss': 0.8376150667667389, 'metrics/total_secs_per_batch': 6.561337232589722, 'metrics/data_secs_per_batch': 2.739218902587891, '_timestamp': 1740954934.2375054}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 153 is less than current step: 499. Dropping entry: {'train/lr': 0.00022622448979591836, '_timestamp': 1740954934.2378457}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 154 is less than current step: 499. Dropping entry: {'train/loss': 2.2515894174575806, 'train/ce_loss': 0.21357421875, 'train/seg_cls_loss': 0.0247802734375, 'train/kl_loss': 0.4037109375, 'train/mask_bce_loss': 0.25566967241466043, 'train/mask_dice_loss': 0.736775416135788, 'train/mask_loss': 0.9924451112747192, 'metrics/total_secs_per_batch': 6.767264366149902, 'metrics/data_secs_per_batch': 3.198093295097351, '_timestamp': 1740954941.0049725}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 154 is less than current step: 499. Dropping entry: {'train/lr': 0.00022610204081632653, '_timestamp': 1740954941.0053587}).
Epoch: [1][156/500]	Time  5.640 ( 5.640)	Loss 2.3159 (2.0092)	CeLoss 0.1562 (0.4057)	SegCLSLoss 0.0253 (0.0183)	KLLoss 0.3848 (0.3209)	MaskLoss 1.0544 (0.7813)	MaskBCELoss 0.0820 (0.1126)	MaskDICELoss 0.9725 (0.6687)
Epoch: [1][157/500]	Time  7.482 ( 7.482)	Loss 3.0008 (2.3929)	CeLoss 0.2197 (0.2164)	SegCLSLoss 0.0269 (0.0253)	KLLoss 0.3926 (0.4002)	MaskLoss 1.3637 (1.0619)	MaskBCELoss 0.6488 (0.2656)	MaskDICELoss 0.7149 (0.7962)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 155 is less than current step: 499. Dropping entry: {'train/loss': 2.0091957688331603, 'train/ce_loss': 0.4056640625, 'train/seg_cls_loss': 0.018341064453125, 'train/kl_loss': 0.3208984375, 'train/mask_bce_loss': 0.11257617566734553, 'train/mask_dice_loss': 0.6686818540096283, 'train/mask_loss': 0.7812580287456512, 'metrics/total_secs_per_batch': 5.6398091316223145, 'metrics/data_secs_per_batch': 2.7128801584243774, '_timestamp': 1740954946.6445258}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 155 is less than current step: 499. Dropping entry: {'train/lr': 0.0002259795918367347, '_timestamp': 1740954946.6449285}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 156 is less than current step: 499. Dropping entry: {'train/loss': 2.3928820371627806, 'train/ce_loss': 0.216357421875, 'train/seg_cls_loss': 0.025347900390625, 'train/kl_loss': 0.4001953125, 'train/mask_bce_loss': 0.2656299456954002, 'train/mask_dice_loss': 0.796240758895874, 'train/mask_loss': 1.061870700120926, 'metrics/total_secs_per_batch': 7.481617212295532, 'metrics/data_secs_per_batch': 3.567574405670166, '_timestamp': 1740954954.1261573}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 156 is less than current step: 499. Dropping entry: {'train/lr': 0.00022585714285714285, '_timestamp': 1740954954.1265063}).
Epoch: [1][158/500]	Time  6.561 ( 6.561)	Loss 0.7383 (1.6378)	CeLoss 0.7383 (0.3803)	SegCLSLoss 0.0000 (0.0173)	KLLoss 0.0000 (0.3275)	MaskLoss 0.0000 (0.6081)	MaskBCELoss 0.0000 (0.0709)	MaskDICELoss 0.0000 (0.5372)
Epoch: [1][159/500]	Time  5.911 ( 5.911)	Loss 0.8203 (1.3086)	CeLoss 0.8203 (0.5051)	SegCLSLoss 0.0000 (0.0091)	KLLoss 0.0000 (0.2049)	MaskLoss 0.0000 (0.3892)	MaskBCELoss 0.0000 (0.1320)	MaskDICELoss 0.0000 (0.2572)
[2025-03-02 16:36:11,596] [INFO] [logging.py:96:log_dist] [Rank 0] step=660, skipped=0, lr=[0.00022555102040816325], mom=[(0.9, 0.95)]
[2025-03-02 16:36:11,602] [INFO] [timer.py:215:stop] epoch=0/micro_step=6600/global_step=660, RunningAvgSamplesPerSec=1.495352624532727, CurrSamplesPerSec=1.998851101342412, MemAllocated=31.25GB, MaxMemAllocated=36.81GB
Epoch: [1][160/500]	Time  5.005 ( 5.005)	Loss 2.3133 (1.5413)	CeLoss 0.2324 (0.6855)	SegCLSLoss 0.0204 (0.0065)	KLLoss 0.4004 (0.1668)	MaskLoss 1.0160 (0.4179)	MaskBCELoss 0.0386 (0.1031)	MaskDICELoss 0.9774 (0.3148)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 157 is less than current step: 499. Dropping entry: {'train/loss': 1.6377827525138855, 'train/ce_loss': 0.3802734375, 'train/seg_cls_loss': 0.017327880859375, 'train/kl_loss': 0.3275390625, 'train/mask_bce_loss': 0.07093588374555111, 'train/mask_dice_loss': 0.5371644616127014, 'train/mask_loss': 0.6081003531813621, 'metrics/total_secs_per_batch': 6.56109619140625, 'metrics/data_secs_per_batch': 2.920965814590454, '_timestamp': 1740954960.6873178}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 157 is less than current step: 499. Dropping entry: {'train/lr': 0.00022573469387755102, '_timestamp': 1740954960.687658}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 158 is less than current step: 499. Dropping entry: {'train/loss': 1.308634102344513, 'train/ce_loss': 0.505078125, 'train/seg_cls_loss': 0.00914306640625, 'train/kl_loss': 0.2048828125, 'train/mask_bce_loss': 0.13202872648835182, 'train/mask_dice_loss': 0.25715160369873047, 'train/mask_loss': 0.389180326461792, 'metrics/total_secs_per_batch': 5.911339282989502, 'metrics/data_secs_per_batch': 2.8785558938980103, '_timestamp': 1740954966.598637}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 158 is less than current step: 499. Dropping entry: {'train/lr': 0.00022561224489795918, '_timestamp': 1740954966.5989325}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 159 is less than current step: 499. Dropping entry: {'train/loss': 1.5412896633148194, 'train/ce_loss': 0.685546875, 'train/seg_cls_loss': 0.006463623046875, 'train/kl_loss': 0.166796875, 'train/mask_bce_loss': 0.10314520485699177, 'train/mask_dice_loss': 0.31476524472236633, 'train/mask_loss': 0.4179104447364807, 'metrics/total_secs_per_batch': 5.00455379486084, 'metrics/data_secs_per_batch': 2.0676735401153565, '_timestamp': 1740954971.6030273}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 159 is less than current step: 499. Dropping entry: {'train/lr': 0.00022548979591836734, '_timestamp': 1740954971.6033812}).
Epoch: [1][161/500]	Time  6.265 ( 6.265)	Loss 1.2768 (1.7043)	CeLoss 0.2891 (0.3381)	SegCLSLoss 0.0130 (0.0139)	KLLoss 0.4375 (0.3342)	MaskLoss 0.4695 (0.6628)	MaskBCELoss 0.0983 (0.1187)	MaskDICELoss 0.3712 (0.5441)
Epoch: [1][162/500]	Time  6.393 ( 6.393)	Loss 2.3846 (1.8956)	CeLoss 0.2324 (0.3544)	SegCLSLoss 0.0147 (0.0178)	KLLoss 0.4414 (0.3240)	MaskLoss 1.0497 (0.7499)	MaskBCELoss 0.0498 (0.0607)	MaskDICELoss 0.9999 (0.6892)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 160 is less than current step: 499. Dropping entry: {'train/loss': 1.7042632579803467, 'train/ce_loss': 0.3380859375, 'train/seg_cls_loss': 0.01390380859375, 'train/kl_loss': 0.3341796875, 'train/mask_bce_loss': 0.11869034301489592, 'train/mask_dice_loss': 0.5441346496343613, 'train/mask_loss': 0.6628249943256378, 'metrics/total_secs_per_batch': 6.265121698379517, 'metrics/data_secs_per_batch': 2.9120824098587037, '_timestamp': 1740954977.868257}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 160 is less than current step: 499. Dropping entry: {'train/lr': 0.0002253673469387755, '_timestamp': 1740954977.86852}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 161 is less than current step: 499. Dropping entry: {'train/loss': 1.8956170082092285, 'train/ce_loss': 0.35439453125, 'train/seg_cls_loss': 0.017791748046875, 'train/kl_loss': 0.3240234375, 'train/mask_bce_loss': 0.060706041567027566, 'train/mask_dice_loss': 0.6891532480716706, 'train/mask_loss': 0.7498592853546142, 'metrics/total_secs_per_batch': 6.39302921295166, 'metrics/data_secs_per_batch': 3.193853974342346, '_timestamp': 1740954984.2615304}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 161 is less than current step: 499. Dropping entry: {'train/lr': 0.00022524489795918367, '_timestamp': 1740954984.2619016}).
Epoch: [1][163/500]	Time  5.603 ( 5.603)	Loss 1.1719 (1.5906)	CeLoss 1.1719 (0.5787)	SegCLSLoss 0.0000 (0.0088)	KLLoss 0.0000 (0.2062)	MaskLoss 0.0000 (0.4935)	MaskBCELoss 0.0000 (0.1395)	MaskDICELoss 0.0000 (0.3540)
Epoch: [1][164/500]	Time  6.169 ( 6.169)	Loss 1.9252 (1.7701)	CeLoss 0.2969 (0.4578)	SegCLSLoss 0.0153 (0.0158)	KLLoss 0.4160 (0.2775)	MaskLoss 0.7898 (0.6383)	MaskBCELoss 0.0715 (0.1704)	MaskDICELoss 0.7183 (0.4679)
Epoch: [1][165/500]	Time  6.298 ( 6.298)	Loss 1.4453 (1.7427)	CeLoss 1.4453 (0.3446)	SegCLSLoss 0.0000 (0.0166)	KLLoss 0.0000 (0.3264)	MaskLoss 0.0000 (0.6786)	MaskBCELoss 0.0000 (0.1119)	MaskDICELoss 0.0000 (0.5667)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 162 is less than current step: 499. Dropping entry: {'train/loss': 1.590601634979248, 'train/ce_loss': 0.578662109375, 'train/seg_cls_loss': 0.00882568359375, 'train/kl_loss': 0.20625, 'train/mask_bce_loss': 0.13952549966052175, 'train/mask_dice_loss': 0.35399307906627653, 'train/mask_loss': 0.49351857900619506, 'metrics/total_secs_per_batch': 5.602578639984131, 'metrics/data_secs_per_batch': 2.5939919710159303, '_timestamp': 1740954989.8638325}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 162 is less than current step: 499. Dropping entry: {'train/lr': 0.00022512244897959183, '_timestamp': 1740954989.864123}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 163 is less than current step: 499. Dropping entry: {'train/loss': 1.770080465078354, 'train/ce_loss': 0.4578125, 'train/seg_cls_loss': 0.015777587890625, 'train/kl_loss': 0.2775390625, 'train/mask_bce_loss': 0.17036475688219072, 'train/mask_dice_loss': 0.4678981460630894, 'train/mask_loss': 0.638262902200222, 'metrics/total_secs_per_batch': 6.168959856033325, 'metrics/data_secs_per_batch': 2.7221763372421264, '_timestamp': 1740954996.032843}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 163 is less than current step: 499. Dropping entry: {'train/lr': 0.000225, '_timestamp': 1740954996.0331323}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 164 is less than current step: 499. Dropping entry: {'train/loss': 1.7426516532897949, 'train/ce_loss': 0.344580078125, 'train/seg_cls_loss': 0.016552734375, 'train/kl_loss': 0.3263671875, 'train/mask_bce_loss': 0.11191422622650862, 'train/mask_dice_loss': 0.5666625708341598, 'train/mask_loss': 0.6785767912864685, 'metrics/total_secs_per_batch': 6.298351764678955, 'metrics/data_secs_per_batch': 2.856812596321106, '_timestamp': 1740955002.3316388}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 164 is less than current step: 499. Dropping entry: {'train/lr': 0.00022487755102040813, '_timestamp': 1740955002.3320835}).
Epoch: [1][166/500]	Time  5.332 ( 5.332)	Loss 2.1304 (1.3434)	CeLoss 0.2266 (0.5469)	SegCLSLoss 0.0146 (0.0125)	KLLoss 0.4121 (0.2018)	MaskLoss 0.9275 (0.3850)	MaskBCELoss 0.1114 (0.0953)	MaskDICELoss 0.8161 (0.2896)
Epoch: [1][167/500]	Time  6.506 ( 6.506)	Loss 2.1887 (2.1863)	CeLoss 0.2188 (0.4276)	SegCLSLoss 0.0160 (0.0183)	KLLoss 0.4102 (0.3154)	MaskLoss 0.9606 (0.8590)	MaskBCELoss 0.6553 (0.2639)	MaskDICELoss 0.3053 (0.5950)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 165 is less than current step: 499. Dropping entry: {'train/loss': 1.34344242811203, 'train/ce_loss': 0.546875, 'train/seg_cls_loss': 0.012542724609375, 'train/kl_loss': 0.2017578125, 'train/mask_bce_loss': 0.09531467705965042, 'train/mask_dice_loss': 0.28963896334171296, 'train/mask_loss': 0.384953635931015, 'metrics/total_secs_per_batch': 5.33179235458374, 'metrics/data_secs_per_batch': 2.3994430541992187, '_timestamp': 1740955007.6630435}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 165 is less than current step: 499. Dropping entry: {'train/lr': 0.0002247551020408163, '_timestamp': 1740955007.6633291}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 166 is less than current step: 499. Dropping entry: {'train/loss': 2.1863150000572205, 'train/ce_loss': 0.42763671875, 'train/seg_cls_loss': 0.018310546875, 'train/kl_loss': 0.3154296875, 'train/mask_bce_loss': 0.2639287207275629, 'train/mask_dice_loss': 0.5950490951538085, 'train/mask_loss': 0.8589778125286103, 'metrics/total_secs_per_batch': 6.5058770179748535, 'metrics/data_secs_per_batch': 2.6057937145233154, '_timestamp': 1740955014.169093}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 166 is less than current step: 499. Dropping entry: {'train/lr': 0.00022463265306122446, '_timestamp': 1740955014.16945}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 167 is less than current step: 499. Dropping entry: {'train/loss': 2.1998775959014893, 'train/ce_loss': 0.42509765625, 'train/seg_cls_loss': 0.01837158203125, 'train/kl_loss': 0.314453125, 'train/mask_bce_loss': 0.21603996232151984, 'train/mask_dice_loss': 0.6510863363742828, 'train/mask_loss': 0.8671262979507446, 'metrics/total_secs_per_batch': 5.381730318069458, 'metrics/data_secs_per_batch': 2.2400075435638427, '_timestamp': 1740955019.550966}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 167 is less than current step: 499. Dropping entry: {'train/lr': 0.00022451020408163262, '_timestamp': 1740955019.5513608}).
Epoch: [1][168/500]	Time  5.382 ( 5.382)	Loss 2.4072 (2.1999)	CeLoss 0.2354 (0.4251)	SegCLSLoss 0.0220 (0.0184)	KLLoss 0.3867 (0.3145)	MaskLoss 1.0610 (0.8671)	MaskBCELoss 0.1081 (0.2160)	MaskDICELoss 0.9529 (0.6511)
Epoch: [1][169/500]	Time  7.060 ( 7.060)	Loss 2.3945 (2.2812)	CeLoss 0.1934 (0.2878)	SegCLSLoss 0.0190 (0.0173)	KLLoss 0.4121 (0.3645)	MaskLoss 1.0752 (0.9743)	MaskBCELoss 0.0784 (0.2900)	MaskDICELoss 0.9968 (0.6842)
[2025-03-02 16:37:11,782] [INFO] [logging.py:96:log_dist] [Rank 0] step=670, skipped=0, lr=[0.00022432653061224488], mom=[(0.9, 0.95)]
[2025-03-02 16:37:11,789] [INFO] [timer.py:215:stop] epoch=0/micro_step=6700/global_step=670, RunningAvgSamplesPerSec=1.497599159916826, CurrSamplesPerSec=1.93155878897555, MemAllocated=30.7GB, MaxMemAllocated=36.81GB
Epoch: [1][170/500]	Time  5.179 ( 5.179)	Loss 1.1016 (1.5999)	CeLoss 1.1016 (0.5614)	SegCLSLoss 0.0000 (0.0138)	KLLoss 0.0000 (0.2383)	MaskLoss 0.0000 (0.5037)	MaskBCELoss 0.0000 (0.0538)	MaskDICELoss 0.0000 (0.4500)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 168 is less than current step: 499. Dropping entry: {'train/loss': 2.281150162220001, 'train/ce_loss': 0.28779296875, 'train/seg_cls_loss': 0.01732177734375, 'train/kl_loss': 0.364453125, 'train/mask_bce_loss': 0.2900476731359959, 'train/mask_dice_loss': 0.6842188179492951, 'train/mask_loss': 0.974266505241394, 'metrics/total_secs_per_batch': 7.060439109802246, 'metrics/data_secs_per_batch': 3.2855590105056764, '_timestamp': 1740955026.6111112}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 168 is less than current step: 499. Dropping entry: {'train/lr': 0.00022438775510204078, '_timestamp': 1740955026.6114087}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 169 is less than current step: 499. Dropping entry: {'train/loss': 1.5999187529087067, 'train/ce_loss': 0.56142578125, 'train/seg_cls_loss': 0.01383056640625, 'train/kl_loss': 0.23828125, 'train/mask_bce_loss': 0.053753660339862105, 'train/mask_dice_loss': 0.44996548295021055, 'train/mask_loss': 0.5037191420793533, 'metrics/total_secs_per_batch': 5.178826093673706, 'metrics/data_secs_per_batch': 2.6128708839416506, '_timestamp': 1740955031.7897792}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 169 is less than current step: 499. Dropping entry: {'train/lr': 0.00022426530612244895, '_timestamp': 1740955031.790106}).
Epoch: [1][171/500]	Time  6.411 ( 6.411)	Loss 0.5547 (1.9615)	CeLoss 0.5547 (0.4782)	SegCLSLoss 0.0000 (0.0190)	KLLoss 0.0000 (0.2697)	MaskLoss 0.0000 (0.7236)	MaskBCELoss 0.0000 (0.1869)	MaskDICELoss 0.0000 (0.5367)
Epoch: [1][172/500]	Time  5.251 ( 5.251)	Loss 1.5000 (1.7126)	CeLoss 1.5000 (0.5133)	SegCLSLoss 0.0000 (0.0174)	KLLoss 0.0000 (0.2797)	MaskLoss 0.0000 (0.5813)	MaskBCELoss 0.0000 (0.1122)	MaskDICELoss 0.0000 (0.4691)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 170 is less than current step: 499. Dropping entry: {'train/loss': 1.9614999294281006, 'train/ce_loss': 0.47822265625, 'train/seg_cls_loss': 0.019012451171875, 'train/kl_loss': 0.2697265625, 'train/mask_bce_loss': 0.18694315310567616, 'train/mask_dice_loss': 0.5366779059171677, 'train/mask_loss': 0.7236210465431213, 'metrics/total_secs_per_batch': 6.410625457763672, 'metrics/data_secs_per_batch': 2.706882381439209, '_timestamp': 1740955038.2005913}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 170 is less than current step: 499. Dropping entry: {'train/lr': 0.0002241428571428571, '_timestamp': 1740955038.2009082}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 171 is less than current step: 499. Dropping entry: {'train/loss': 1.7125643610954284, 'train/ce_loss': 0.51328125, 'train/seg_cls_loss': 0.017431640625, 'train/kl_loss': 0.2796875, 'train/mask_bce_loss': 0.11217528097331524, 'train/mask_dice_loss': 0.46910690814256667, 'train/mask_loss': 0.5812821924686432, 'metrics/total_secs_per_batch': 5.2509520053863525, 'metrics/data_secs_per_batch': 2.7686037540435793, '_timestamp': 1740955043.4515653}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 171 is less than current step: 499. Dropping entry: {'train/lr': 0.00022402040816326527, '_timestamp': 1740955043.4518967}).
Epoch: [1][173/500]	Time  6.426 ( 6.426)	Loss 2.1199 (1.9434)	CeLoss 0.1943 (0.2704)	SegCLSLoss 0.0253 (0.0258)	KLLoss 0.3867 (0.3541)	MaskLoss 0.9369 (0.8123)	MaskBCELoss 0.0175 (0.1550)	MaskDICELoss 0.9194 (0.6573)
Epoch: [1][174/500]	Time  6.621 ( 6.621)	Loss 1.1016 (1.7180)	CeLoss 1.1016 (0.4736)	SegCLSLoss 0.0000 (0.0129)	KLLoss 0.0000 (0.2902)	MaskLoss 0.0000 (0.6045)	MaskBCELoss 0.0000 (0.0743)	MaskDICELoss 0.0000 (0.5302)
Epoch: [1][175/500]	Time  5.632 ( 5.632)	Loss 2.6345 (2.1020)	CeLoss 0.2715 (0.4398)	SegCLSLoss 0.0203 (0.0165)	KLLoss 0.3770 (0.3262)	MaskLoss 1.1581 (0.8106)	MaskBCELoss 0.2771 (0.2113)	MaskDICELoss 0.8809 (0.5993)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 172 is less than current step: 499. Dropping entry: {'train/loss': 1.9433795809745789, 'train/ce_loss': 0.27041015625, 'train/seg_cls_loss': 0.02576904296875, 'train/kl_loss': 0.3541015625, 'train/mask_bce_loss': 0.15498305950313807, 'train/mask_dice_loss': 0.6572828948497772, 'train/mask_loss': 0.812265956401825, 'metrics/total_secs_per_batch': 6.426443815231323, 'metrics/data_secs_per_batch': 3.027430868148804, '_timestamp': 1740955049.877981}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 172 is less than current step: 499. Dropping entry: {'train/lr': 0.00022389795918367343, '_timestamp': 1740955049.8783028}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 173 is less than current step: 499. Dropping entry: {'train/loss': 1.7179797768592835, 'train/ce_loss': 0.4736328125, 'train/seg_cls_loss': 0.012860107421875, 'train/kl_loss': 0.290234375, 'train/mask_bce_loss': 0.07430561017245055, 'train/mask_dice_loss': 0.5301920950412751, 'train/mask_loss': 0.6044977068901062, 'metrics/total_secs_per_batch': 6.620872974395752, 'metrics/data_secs_per_batch': 3.132530617713928, '_timestamp': 1740955056.4988048}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 173 is less than current step: 499. Dropping entry: {'train/lr': 0.0002237755102040816, '_timestamp': 1740955056.4989955}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 174 is less than current step: 499. Dropping entry: {'train/loss': 2.1019875049591064, 'train/ce_loss': 0.43984375, 'train/seg_cls_loss': 0.0165283203125, 'train/kl_loss': 0.326171875, 'train/mask_bce_loss': 0.21129510067403318, 'train/mask_dice_loss': 0.5992689728736877, 'train/mask_loss': 0.8105640709400177, 'metrics/total_secs_per_batch': 5.631559133529663, 'metrics/data_secs_per_batch': 2.6119176387786864, '_timestamp': 1740955062.1306252}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 174 is less than current step: 499. Dropping entry: {'train/lr': 0.00022365306122448976, '_timestamp': 1740955062.1309817}).
Epoch: [1][176/500]	Time  4.699 ( 4.699)	Loss 2.3183 (1.6099)	CeLoss 0.2949 (0.6384)	SegCLSLoss 0.0198 (0.0116)	KLLoss 0.4004 (0.2029)	MaskLoss 0.9873 (0.4729)	MaskBCELoss 0.0098 (0.0577)	MaskDICELoss 0.9775 (0.4152)
Epoch: [1][177/500]	Time  5.467 ( 5.467)	Loss 2.1074 (1.7372)	CeLoss 0.1924 (0.6149)	SegCLSLoss 0.0157 (0.0137)	KLLoss 0.3965 (0.2844)	MaskLoss 0.9336 (0.5436)	MaskBCELoss 0.2575 (0.0478)	MaskDICELoss 0.6760 (0.4958)
Epoch: [1][178/500]	Time  5.899 ( 5.899)	Loss 1.6601 (1.9252)	CeLoss 0.2812 (0.3092)	SegCLSLoss 0.0123 (0.0155)	KLLoss 0.4277 (0.3703)	MaskLoss 0.6650 (0.7859)	MaskBCELoss 0.1559 (0.2435)	MaskDICELoss 0.5091 (0.5424)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 175 is less than current step: 499. Dropping entry: {'train/loss': 1.6099260210990907, 'train/ce_loss': 0.63837890625, 'train/seg_cls_loss': 0.01163330078125, 'train/kl_loss': 0.2029296875, 'train/mask_bce_loss': 0.057722807349637154, 'train/mask_dice_loss': 0.41520893573760986, 'train/mask_loss': 0.4729317486286163, 'metrics/total_secs_per_batch': 4.698964595794678, 'metrics/data_secs_per_batch': 1.9833589792251587, '_timestamp': 1740955066.829351}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 175 is less than current step: 499. Dropping entry: {'train/lr': 0.00022353061224489795, '_timestamp': 1740955066.8296409}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 176 is less than current step: 499. Dropping entry: {'train/loss': 1.7372005462646485, 'train/ce_loss': 0.61494140625, 'train/seg_cls_loss': 0.01370849609375, 'train/kl_loss': 0.284375, 'train/mask_bce_loss': 0.04781833477318287, 'train/mask_dice_loss': 0.4957819253206253, 'train/mask_loss': 0.543600270152092, 'metrics/total_secs_per_batch': 5.466894149780273, 'metrics/data_secs_per_batch': 2.4757590293884277, '_timestamp': 1740955072.296289}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 176 is less than current step: 499. Dropping entry: {'train/lr': 0.00022340816326530611, '_timestamp': 1740955072.2966685}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 177 is less than current step: 499. Dropping entry: {'train/loss': 1.9252267003059387, 'train/ce_loss': 0.3091796875, 'train/seg_cls_loss': 0.015472412109375, 'train/kl_loss': 0.3703125, 'train/mask_bce_loss': 0.2435040969401598, 'train/mask_dice_loss': 0.5423514485359192, 'train/mask_loss': 0.7858555406332016, 'metrics/total_secs_per_batch': 5.8993980884552, 'metrics/data_secs_per_batch': 2.5687336921691895, '_timestamp': 1740955078.1958637}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 177 is less than current step: 499. Dropping entry: {'train/lr': 0.00022328571428571428, '_timestamp': 1740955078.1962373}).
Epoch: [1][179/500]	Time  6.970 ( 6.970)	Loss 2.8076 (2.2369)	CeLoss 0.1846 (0.4421)	SegCLSLoss 0.0383 (0.0223)	KLLoss 0.3730 (0.3180)	MaskLoss 1.2832 (0.8759)	MaskBCELoss 0.4141 (0.1894)	MaskDICELoss 0.8691 (0.6865)
[2025-03-02 16:38:10,677] [INFO] [logging.py:96:log_dist] [Rank 0] step=680, skipped=0, lr=[0.00022310204081632653], mom=[(0.9, 0.95)]
[2025-03-02 16:38:10,682] [INFO] [timer.py:215:stop] epoch=0/micro_step=6800/global_step=680, RunningAvgSamplesPerSec=1.5002147840701268, CurrSamplesPerSec=1.81278997026704, MemAllocated=30.88GB, MaxMemAllocated=36.81GB
Epoch: [1][180/500]	Time  5.518 ( 5.518)	Loss 1.1328 (2.0140)	CeLoss 1.1328 (0.5452)	SegCLSLoss 0.0000 (0.0154)	KLLoss 0.0000 (0.2832)	MaskLoss 0.0000 (0.7164)	MaskBCELoss 0.0000 (0.2384)	MaskDICELoss 0.0000 (0.4780)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 178 is less than current step: 499. Dropping entry: {'train/loss': 2.2368980765342714, 'train/ce_loss': 0.44208984375, 'train/seg_cls_loss': 0.02225341796875, 'train/kl_loss': 0.31796875, 'train/mask_bce_loss': 0.189401838183403, 'train/mask_dice_loss': 0.6864690363407135, 'train/mask_loss': 0.8758708894252777, 'metrics/total_secs_per_batch': 6.969727993011475, 'metrics/data_secs_per_batch': 2.99297833442688, '_timestamp': 1740955085.1653972}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 178 is less than current step: 499. Dropping entry: {'train/lr': 0.00022316326530612244, '_timestamp': 1740955085.1656904}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 179 is less than current step: 499. Dropping entry: {'train/loss': 2.014025568962097, 'train/ce_loss': 0.54521484375, 'train/seg_cls_loss': 0.01539306640625, 'train/kl_loss': 0.283203125, 'train/mask_bce_loss': 0.2384152501821518, 'train/mask_dice_loss': 0.47797254025936126, 'train/mask_loss': 0.716387790441513, 'metrics/total_secs_per_batch': 5.518111944198608, 'metrics/data_secs_per_batch': 2.4089864253997804, '_timestamp': 1740955090.6836557}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 179 is less than current step: 499. Dropping entry: {'train/lr': 0.0002230408163265306, '_timestamp': 1740955090.6842127}).
Epoch: [1][181/500]	Time  5.478 ( 5.478)	Loss 2.1124 (1.6825)	CeLoss 0.1816 (0.5946)	SegCLSLoss 0.0334 (0.0161)	KLLoss 0.3906 (0.2408)	MaskLoss 0.9375 (0.5279)	MaskBCELoss 0.0176 (0.0976)	MaskDICELoss 0.9200 (0.4303)
Epoch: [1][182/500]	Time  5.546 ( 5.546)	Loss 2.1201 (1.6350)	CeLoss 0.2383 (0.6288)	SegCLSLoss 0.0221 (0.0123)	KLLoss 0.4023 (0.2457)	MaskLoss 0.9155 (0.4878)	MaskBCELoss 0.0284 (0.0942)	MaskDICELoss 0.8871 (0.3935)
Epoch: [1][183/500]	Time  5.805 ( 5.805)	Loss 2.1524 (1.3763)	CeLoss 0.2012 (0.4122)	SegCLSLoss 0.0320 (0.0147)	KLLoss 0.3809 (0.2439)	MaskLoss 0.9493 (0.4664)	MaskBCELoss 0.1566 (0.0608)	MaskDICELoss 0.7927 (0.4056)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 180 is less than current step: 499. Dropping entry: {'train/loss': 1.682478094100952, 'train/ce_loss': 0.59462890625, 'train/seg_cls_loss': 0.016064453125, 'train/kl_loss': 0.2408203125, 'train/mask_bce_loss': 0.0976080073043704, 'train/mask_dice_loss': 0.4302521154284477, 'train/mask_loss': 0.5278601109981537, 'metrics/total_secs_per_batch': 5.47824764251709, 'metrics/data_secs_per_batch': 2.3852041721343995, '_timestamp': 1740955096.161858}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 180 is less than current step: 499. Dropping entry: {'train/lr': 0.00022291836734693877, '_timestamp': 1740955096.1623}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 181 is less than current step: 499. Dropping entry: {'train/loss': 1.6349575877189637, 'train/ce_loss': 0.62880859375, 'train/seg_cls_loss': 0.012261962890625, 'train/kl_loss': 0.245703125, 'train/mask_bce_loss': 0.09424204174429178, 'train/mask_dice_loss': 0.3935492545366287, 'train/mask_loss': 0.4877912998199463, 'metrics/total_secs_per_batch': 5.545743465423584, 'metrics/data_secs_per_batch': 2.2777801275253298, '_timestamp': 1740955101.7077637}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 181 is less than current step: 499. Dropping entry: {'train/lr': 0.00022279591836734693, '_timestamp': 1740955101.7081206}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 182 is less than current step: 499. Dropping entry: {'train/loss': 1.3762640297412871, 'train/ce_loss': 0.412158203125, 'train/seg_cls_loss': 0.01474609375, 'train/kl_loss': 0.2439453125, 'train/mask_bce_loss': 0.060754889994859694, 'train/mask_dice_loss': 0.40562418550252916, 'train/mask_loss': 0.46637907326221467, 'metrics/total_secs_per_batch': 5.805306673049927, 'metrics/data_secs_per_batch': 2.694612503051758, '_timestamp': 1740955107.512906}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 182 is less than current step: 499. Dropping entry: {'train/lr': 0.0002226734693877551, '_timestamp': 1740955107.5132205}).
Epoch: [1][184/500]	Time  5.293 ( 5.293)	Loss 2.2729 (1.5180)	CeLoss 0.2402 (0.4447)	SegCLSLoss 0.0233 (0.0166)	KLLoss 0.3965 (0.2367)	MaskLoss 0.9910 (0.5207)	MaskBCELoss 0.0249 (0.0630)	MaskDICELoss 0.9660 (0.4578)
Epoch: [1][185/500]	Time  6.363 ( 6.363)	Loss 2.0245 (2.1377)	CeLoss 0.2715 (0.2501)	SegCLSLoss 0.0151 (0.0224)	KLLoss 0.4023 (0.3990)	MaskLoss 0.8521 (0.9183)	MaskBCELoss 0.0624 (0.1353)	MaskDICELoss 0.7897 (0.7830)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 183 is less than current step: 499. Dropping entry: {'train/loss': 1.5180458664894103, 'train/ce_loss': 0.4447265625, 'train/seg_cls_loss': 0.01656494140625, 'train/kl_loss': 0.23671875, 'train/mask_bce_loss': 0.06296467613428831, 'train/mask_dice_loss': 0.45777700543403627, 'train/mask_loss': 0.5207416832447052, 'metrics/total_secs_per_batch': 5.293032884597778, 'metrics/data_secs_per_batch': 1.9306029319763183, '_timestamp': 1740955112.806622}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 183 is less than current step: 499. Dropping entry: {'train/lr': 0.00022255102040816325, '_timestamp': 1740955112.8070736}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 184 is less than current step: 499. Dropping entry: {'train/loss': 2.1376845955848696, 'train/ce_loss': 0.25009765625, 'train/seg_cls_loss': 0.022369384765625, 'train/kl_loss': 0.3990234375, 'train/mask_bce_loss': 0.13527081292122603, 'train/mask_dice_loss': 0.7830343514680862, 'train/mask_loss': 0.9183051645755768, 'metrics/total_secs_per_batch': 6.363148927688599, 'metrics/data_secs_per_batch': 2.813854718208313, '_timestamp': 1740955119.1691875}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 184 is less than current step: 499. Dropping entry: {'train/lr': 0.00022242857142857142, '_timestamp': 1740955119.1695402}).
Epoch: [1][186/500]	Time  6.723 ( 6.723)	Loss 1.3089 (1.9659)	CeLoss 0.2559 (0.2427)	SegCLSLoss 0.0139 (0.0187)	KLLoss 0.4316 (0.3682)	MaskLoss 0.5011 (0.8386)	MaskBCELoss 0.1282 (0.1933)	MaskDICELoss 0.3729 (0.6453)
Epoch: [1][187/500]	Time  7.068 ( 7.068)	Loss 2.7329 (2.1575)	CeLoss 0.2432 (0.2510)	SegCLSLoss 0.0219 (0.0198)	KLLoss 0.3789 (0.4023)	MaskLoss 1.2209 (0.9282)	MaskBCELoss 0.4876 (0.1652)	MaskDICELoss 0.7333 (0.7631)
Epoch: [1][188/500]	Time  5.552 ( 5.552)	Loss 2.6320 (2.0168)	CeLoss 0.1699 (0.4914)	SegCLSLoss 0.0317 (0.0179)	KLLoss 0.3789 (0.2721)	MaskLoss 1.2042 (0.7449)	MaskBCELoss 0.3510 (0.1477)	MaskDICELoss 0.8532 (0.5972)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 185 is less than current step: 499. Dropping entry: {'train/loss': 1.9658974409103394, 'train/ce_loss': 0.2426513671875, 'train/seg_cls_loss': 0.01868896484375, 'train/kl_loss': 0.3681640625, 'train/mask_bce_loss': 0.19333127327263355, 'train/mask_dice_loss': 0.6452937126159668, 'train/mask_loss': 0.8386249899864197, 'metrics/total_secs_per_batch': 6.723281621932983, 'metrics/data_secs_per_batch': 3.1960906028747558, '_timestamp': 1740955125.8923216}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 185 is less than current step: 499. Dropping entry: {'train/lr': 0.00022230612244897958, '_timestamp': 1740955125.8926022}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 186 is less than current step: 499. Dropping entry: {'train/loss': 2.157473075389862, 'train/ce_loss': 0.2509765625, 'train/seg_cls_loss': 0.019842529296875, 'train/kl_loss': 0.40234375, 'train/mask_bce_loss': 0.16516011413186787, 'train/mask_dice_loss': 0.7630881547927857, 'train/mask_loss': 0.9282482594251633, 'metrics/total_secs_per_batch': 7.068091630935669, 'metrics/data_secs_per_batch': 2.9474539518356324, '_timestamp': 1740955132.9606588}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 186 is less than current step: 499. Dropping entry: {'train/lr': 0.00022218367346938774, '_timestamp': 1740955132.9610257}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 187 is less than current step: 499. Dropping entry: {'train/loss': 2.016761302947998, 'train/ce_loss': 0.49140625, 'train/seg_cls_loss': 0.01790771484375, 'train/kl_loss': 0.2720703125, 'train/mask_bce_loss': 0.1476753367111087, 'train/mask_dice_loss': 0.5971799075603486, 'train/mask_loss': 0.7448552489280701, 'metrics/total_secs_per_batch': 5.5516955852508545, 'metrics/data_secs_per_batch': 2.3731409549713134, '_timestamp': 1740955138.5123143}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 187 is less than current step: 499. Dropping entry: {'train/lr': 0.0002220612244897959, '_timestamp': 1740955138.5126715}).
Epoch: [1][189/500]	Time  7.110 ( 7.110)	Loss 1.5190 (1.3401)	CeLoss 0.2539 (0.2447)	SegCLSLoss 0.0153 (0.0129)	KLLoss 0.4180 (0.2904)	MaskLoss 0.6082 (0.5300)	MaskBCELoss 0.1373 (0.1154)	MaskDICELoss 0.4708 (0.4146)
[2025-03-02 16:39:11,614] [INFO] [logging.py:96:log_dist] [Rank 0] step=690, skipped=0, lr=[0.00022187755102040814], mom=[(0.9, 0.95)]
[2025-03-02 16:39:11,620] [INFO] [timer.py:215:stop] epoch=0/micro_step=6900/global_step=690, RunningAvgSamplesPerSec=1.5020930466331568, CurrSamplesPerSec=1.6673063374633872, MemAllocated=30.69GB, MaxMemAllocated=36.81GB
Epoch: [1][190/500]	Time  5.999 ( 5.999)	Loss 1.2188 (1.9318)	CeLoss 1.2188 (0.4922)	SegCLSLoss 0.0000 (0.0164)	KLLoss 0.0000 (0.2744)	MaskLoss 0.0000 (0.7019)	MaskBCELoss 0.0000 (0.1030)	MaskDICELoss 0.0000 (0.5989)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 188 is less than current step: 499. Dropping entry: {'train/loss': 1.3401140451431275, 'train/ce_loss': 0.2447265625, 'train/seg_cls_loss': 0.01292724609375, 'train/kl_loss': 0.2904296875, 'train/mask_bce_loss': 0.11540598990395665, 'train/mask_dice_loss': 0.41456313282251356, 'train/mask_loss': 0.5299691200256348, 'metrics/total_secs_per_batch': 7.109996557235718, 'metrics/data_secs_per_batch': 2.7781570196151733, '_timestamp': 1740955145.6221564}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 188 is less than current step: 499. Dropping entry: {'train/lr': 0.00022193877551020407, '_timestamp': 1740955145.6223629}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 189 is less than current step: 499. Dropping entry: {'train/loss': 1.9318382382392882, 'train/ce_loss': 0.4921875, 'train/seg_cls_loss': 0.016375732421875, 'train/kl_loss': 0.2744140625, 'train/mask_bce_loss': 0.10304027097299695, 'train/mask_dice_loss': 0.5988651752471924, 'train/mask_loss': 0.7019054532051087, 'metrics/total_secs_per_batch': 5.999330520629883, 'metrics/data_secs_per_batch': 2.964824748039246, '_timestamp': 1740955151.6213381}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 189 is less than current step: 499. Dropping entry: {'train/lr': 0.00022181632653061223, '_timestamp': 1740955151.6217086}).
Epoch: [1][191/500]	Time  6.069 ( 6.069)	Loss 2.4603 (1.7662)	CeLoss 0.2539 (0.3723)	SegCLSLoss 0.0217 (0.0184)	KLLoss 0.3711 (0.2746)	MaskLoss 1.0788 (0.6787)	MaskBCELoss 0.2534 (0.0986)	MaskDICELoss 0.8254 (0.5801)
Epoch: [1][192/500]	Time  7.255 ( 7.255)	Loss 2.7713 (1.9556)	CeLoss 0.2832 (0.3198)	SegCLSLoss 0.0167 (0.0191)	KLLoss 0.4121 (0.3203)	MaskLoss 1.2196 (0.7972)	MaskBCELoss 0.3335 (0.1134)	MaskDICELoss 0.8861 (0.6838)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 190 is less than current step: 499. Dropping entry: {'train/loss': 1.7662041187286377, 'train/ce_loss': 0.372265625, 'train/seg_cls_loss': 0.018426513671875, 'train/kl_loss': 0.274609375, 'train/mask_bce_loss': 0.09861612534150481, 'train/mask_dice_loss': 0.5800913870334625, 'train/mask_loss': 0.6787075102329254, 'metrics/total_secs_per_batch': 6.0688135623931885, 'metrics/data_secs_per_batch': 2.9052989959716795, '_timestamp': 1740955157.690256}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 190 is less than current step: 499. Dropping entry: {'train/lr': 0.00022169387755102037, '_timestamp': 1740955157.6905994}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 191 is less than current step: 499. Dropping entry: {'train/loss': 1.9555598258972169, 'train/ce_loss': 0.31982421875, 'train/seg_cls_loss': 0.0191162109375, 'train/kl_loss': 0.3203125, 'train/mask_bce_loss': 0.11342477053403854, 'train/mask_dice_loss': 0.6837887346744538, 'train/mask_loss': 0.7972135126590729, 'metrics/total_secs_per_batch': 7.255437135696411, 'metrics/data_secs_per_batch': 3.204827356338501, '_timestamp': 1740955164.945758}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 191 is less than current step: 499. Dropping entry: {'train/lr': 0.00022157142857142853, '_timestamp': 1740955164.9460506}).
Epoch: [1][193/500]	Time  5.699 ( 5.699)	Loss 2.6201 (1.9549)	CeLoss 0.2324 (0.5295)	SegCLSLoss 0.0271 (0.0140)	KLLoss 0.3828 (0.2836)	MaskLoss 1.1674 (0.6948)	MaskBCELoss 0.3179 (0.1509)	MaskDICELoss 0.8495 (0.5439)
Epoch: [1][194/500]	Time  7.008 ( 7.008)	Loss 2.1147 (1.9856)	CeLoss 0.2246 (0.2849)	SegCLSLoss 0.0160 (0.0203)	KLLoss 0.4121 (0.3582)	MaskLoss 0.9206 (0.8273)	MaskBCELoss 0.0239 (0.1124)	MaskDICELoss 0.8967 (0.7149)
Epoch: [1][195/500]	Time  5.524 ( 5.524)	Loss 1.1406 (2.0765)	CeLoss 1.1406 (0.5632)	SegCLSLoss 0.0000 (0.0136)	KLLoss 0.0000 (0.2867)	MaskLoss 0.0000 (0.7391)	MaskBCELoss 0.0000 (0.1972)	MaskDICELoss 0.0000 (0.5419)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 192 is less than current step: 499. Dropping entry: {'train/loss': 1.9548925399780273, 'train/ce_loss': 0.5294921875, 'train/seg_cls_loss': 0.013983154296875, 'train/kl_loss': 0.28359375, 'train/mask_bce_loss': 0.15088181532919406, 'train/mask_dice_loss': 0.5439472496509552, 'train/mask_loss': 0.6948290824890136, 'metrics/total_secs_per_batch': 5.69924259185791, 'metrics/data_secs_per_batch': 2.401961636543274, '_timestamp': 1740955170.6450827}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 192 is less than current step: 499. Dropping entry: {'train/lr': 0.0002214489795918367, '_timestamp': 1740955170.6454382}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 193 is less than current step: 499. Dropping entry: {'train/loss': 1.9856052637100219, 'train/ce_loss': 0.28486328125, 'train/seg_cls_loss': 0.02025146484375, 'train/kl_loss': 0.358203125, 'train/mask_bce_loss': 0.11237010746262968, 'train/mask_dice_loss': 0.7149051874876022, 'train/mask_loss': 0.82727530002594, 'metrics/total_secs_per_batch': 7.008131742477417, 'metrics/data_secs_per_batch': 3.2817321538925173, '_timestamp': 1740955177.6530786}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 193 is less than current step: 499. Dropping entry: {'train/lr': 0.00022132653061224486, '_timestamp': 1740955177.6533556}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 194 is less than current step: 499. Dropping entry: {'train/loss': 2.0765429735183716, 'train/ce_loss': 0.56318359375, 'train/seg_cls_loss': 0.013604736328125, 'train/kl_loss': 0.28671875, 'train/mask_bce_loss': 0.19716176390647888, 'train/mask_dice_loss': 0.5418909698724746, 'train/mask_loss': 0.7390527248382568, 'metrics/total_secs_per_batch': 5.523515939712524, 'metrics/data_secs_per_batch': 2.5919071197509767, '_timestamp': 1740955183.176805}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 194 is less than current step: 499. Dropping entry: {'train/lr': 0.00022120408163265302, '_timestamp': 1740955183.1771693}).
Epoch: [1][196/500]	Time  6.373 ( 6.373)	Loss 2.4467 (2.0777)	CeLoss 0.2754 (0.2585)	SegCLSLoss 0.0165 (0.0222)	KLLoss 0.4121 (0.4021)	MaskLoss 1.0612 (0.8840)	MaskBCELoss 0.3494 (0.2043)	MaskDICELoss 0.7119 (0.6797)
Epoch: [1][197/500]	Time  6.156 ( 6.156)	Loss 1.1200 (1.8243)	CeLoss 0.2598 (0.4884)	SegCLSLoss 0.0126 (0.0127)	KLLoss 0.4375 (0.3359)	MaskLoss 0.4057 (0.6481)	MaskBCELoss 0.0923 (0.1397)	MaskDICELoss 0.3134 (0.5084)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 195 is less than current step: 499. Dropping entry: {'train/loss': 2.0777090787887573, 'train/ce_loss': 0.25849609375, 'train/seg_cls_loss': 0.022186279296875, 'train/kl_loss': 0.4021484375, 'train/mask_bce_loss': 0.20429035732522607, 'train/mask_dice_loss': 0.6797301918268204, 'train/mask_loss': 0.8840205490589141, 'metrics/total_secs_per_batch': 6.372584581375122, 'metrics/data_secs_per_batch': 2.979280161857605, '_timestamp': 1740955189.5491676}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 195 is less than current step: 499. Dropping entry: {'train/lr': 0.00022108163265306118, '_timestamp': 1740955189.5493698}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 196 is less than current step: 499. Dropping entry: {'train/loss': 1.8243191957473754, 'train/ce_loss': 0.48837890625, 'train/seg_cls_loss': 0.0127197265625, 'train/kl_loss': 0.3359375, 'train/mask_bce_loss': 0.13968699248507618, 'train/mask_dice_loss': 0.5084101155400276, 'train/mask_loss': 0.6480971097946167, 'metrics/total_secs_per_batch': 6.156180381774902, 'metrics/data_secs_per_batch': 2.591288614273071, '_timestamp': 1740955195.7053955}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 196 is less than current step: 499. Dropping entry: {'train/lr': 0.00022095918367346935, '_timestamp': 1740955195.7057035}).
Epoch: [1][198/500]	Time  6.862 ( 6.862)	Loss 2.1560 (2.0054)	CeLoss 0.2207 (0.3754)	SegCLSLoss 0.0221 (0.0175)	KLLoss 0.3945 (0.3236)	MaskLoss 0.9422 (0.7945)	MaskBCELoss 0.0314 (0.1509)	MaskDICELoss 0.9108 (0.6436)
Epoch: [1][199/500]	Time  6.413 ( 6.413)	Loss 1.9508 (1.8695)	CeLoss 0.2754 (0.3926)	SegCLSLoss 0.0203 (0.0193)	KLLoss 0.3945 (0.3203)	MaskLoss 0.8133 (0.7177)	MaskBCELoss 0.1191 (0.1513)	MaskDICELoss 0.6942 (0.5664)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 197 is less than current step: 499. Dropping entry: {'train/loss': 2.0054236173629763, 'train/ce_loss': 0.375439453125, 'train/seg_cls_loss': 0.017547607421875, 'train/kl_loss': 0.3236328125, 'train/mask_bce_loss': 0.15094691403210164, 'train/mask_dice_loss': 0.6435617625713348, 'train/mask_loss': 0.7945086777210235, 'metrics/total_secs_per_batch': 6.862026929855347, 'metrics/data_secs_per_batch': 3.273477339744568, '_timestamp': 1740955202.5673993}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 197 is less than current step: 499. Dropping entry: {'train/lr': 0.0002208367346938775, '_timestamp': 1740955202.567748}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 198 is less than current step: 499. Dropping entry: {'train/loss': 1.8694501280784608, 'train/ce_loss': 0.392578125, 'train/seg_cls_loss': 0.019342041015625, 'train/kl_loss': 0.3203125, 'train/mask_bce_loss': 0.15132462345063685, 'train/mask_dice_loss': 0.5664082556962967, 'train/mask_loss': 0.7177328705787659, 'metrics/total_secs_per_batch': 6.412722587585449, 'metrics/data_secs_per_batch': 2.6299600839614867, '_timestamp': 1740955208.9803348}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 198 is less than current step: 499. Dropping entry: {'train/lr': 0.0002207142857142857, '_timestamp': 1740955208.9806821}).
[2025-03-02 16:40:15,786] [INFO] [logging.py:96:log_dist] [Rank 0] step=700, skipped=0, lr=[0.0002206530612244898], mom=[(0.9, 0.95)]
[2025-03-02 16:40:15,792] [INFO] [timer.py:215:stop] epoch=0/micro_step=7000/global_step=700, RunningAvgSamplesPerSec=1.502874298524029, CurrSamplesPerSec=1.4682770475081672, MemAllocated=31.26GB, MaxMemAllocated=36.81GB
Epoch: [1][200/500]	Time  6.813 ( 6.813)	Loss 1.8120 (1.8184)	CeLoss 0.2432 (0.4854)	SegCLSLoss 0.0242 (0.0187)	KLLoss 0.3848 (0.2752)	MaskLoss 0.7586 (0.6480)	MaskBCELoss 0.0574 (0.1182)	MaskDICELoss 0.7012 (0.5298)
Epoch: [1][201/500]	Time  5.144 ( 5.144)	Loss 1.0769 (1.8959)	CeLoss 0.2383 (0.4882)	SegCLSLoss 0.0139 (0.0212)	KLLoss 0.4180 (0.3186)	MaskLoss 0.3949 (0.6826)	MaskBCELoss 0.1097 (0.1317)	MaskDICELoss 0.2852 (0.5509)
Epoch: [1][202/500]	Time  6.227 ( 6.227)	Loss 2.4499 (1.8520)	CeLoss 0.2480 (0.4204)	SegCLSLoss 0.0222 (0.0142)	KLLoss 0.4004 (0.2793)	MaskLoss 1.0745 (0.6982)	MaskBCELoss 0.3542 (0.1630)	MaskDICELoss 0.7204 (0.5353)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 199 is less than current step: 499. Dropping entry: {'train/loss': 1.818392848968506, 'train/ce_loss': 0.48544921875, 'train/seg_cls_loss': 0.018658447265625, 'train/kl_loss': 0.2751953125, 'train/mask_bce_loss': 0.11819779984652996, 'train/mask_dice_loss': 0.5298169761896133, 'train/mask_loss': 0.6480147778987885, 'metrics/total_secs_per_batch': 6.812615871429443, 'metrics/data_secs_per_batch': 3.1156850814819337, '_timestamp': 1740955215.792575}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 199 is less than current step: 499. Dropping entry: {'train/lr': 0.00022059183673469386, '_timestamp': 1740955215.7928634}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 200 is less than current step: 499. Dropping entry: {'train/loss': 1.8958544611930848, 'train/ce_loss': 0.48818359375, 'train/seg_cls_loss': 0.021160888671875, 'train/kl_loss': 0.3185546875, 'train/mask_bce_loss': 0.13172667361795903, 'train/mask_dice_loss': 0.5509173601865769, 'train/mask_loss': 0.6826440304517746, 'metrics/total_secs_per_batch': 5.144405126571655, 'metrics/data_secs_per_batch': 2.445589303970337, '_timestamp': 1740955220.9371753}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 200 is less than current step: 499. Dropping entry: {'train/lr': 0.00022046938775510203, '_timestamp': 1740955220.9375362}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 201 is less than current step: 499. Dropping entry: {'train/loss': 1.851950216293335, 'train/ce_loss': 0.420361328125, 'train/seg_cls_loss': 0.01416015625, 'train/kl_loss': 0.279296875, 'train/mask_bce_loss': 0.16295895017683507, 'train/mask_dice_loss': 0.5352573573589325, 'train/mask_loss': 0.698216313123703, 'metrics/total_secs_per_batch': 6.227221965789795, 'metrics/data_secs_per_batch': 2.7343602895736696, '_timestamp': 1740955227.1646569}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 201 is less than current step: 499. Dropping entry: {'train/lr': 0.0002203469387755102, '_timestamp': 1740955227.1650164}).
Epoch: [1][203/500]	Time  6.171 ( 6.171)	Loss 2.0614 (1.8584)	CeLoss 0.2383 (0.3948)	SegCLSLoss 0.0292 (0.0212)	KLLoss 0.3984 (0.3252)	MaskLoss 0.8852 (0.7104)	MaskBCELoss 0.3454 (0.1840)	MaskDICELoss 0.5399 (0.5264)
Epoch: [1][204/500]	Time  5.836 ( 5.836)	Loss 1.0547 (1.6589)	CeLoss 1.0547 (0.4724)	SegCLSLoss 0.0000 (0.0150)	KLLoss 0.0000 (0.2838)	MaskLoss 0.0000 (0.5752)	MaskBCELoss 0.0000 (0.1270)	MaskDICELoss 0.0000 (0.4482)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 202 is less than current step: 499. Dropping entry: {'train/loss': 1.8584257304668426, 'train/ce_loss': 0.39482421875, 'train/seg_cls_loss': 0.021160888671875, 'train/kl_loss': 0.3251953125, 'train/mask_bce_loss': 0.18396534956991673, 'train/mask_dice_loss': 0.5264486819505692, 'train/mask_loss': 0.7104140251874924, 'metrics/total_secs_per_batch': 6.171071767807007, 'metrics/data_secs_per_batch': 3.0175278902053835, '_timestamp': 1740955233.3355076}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 202 is less than current step: 499. Dropping entry: {'train/lr': 0.00022022448979591835, '_timestamp': 1740955233.3357167}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 203 is less than current step: 499. Dropping entry: {'train/loss': 1.6588762164115907, 'train/ce_loss': 0.47236328125, 'train/seg_cls_loss': 0.0150390625, 'train/kl_loss': 0.2837890625, 'train/mask_bce_loss': 0.12698823697865008, 'train/mask_dice_loss': 0.44820181131362913, 'train/mask_loss': 0.5751900553703309, 'metrics/total_secs_per_batch': 5.83637547492981, 'metrics/data_secs_per_batch': 2.5304725885391237, '_timestamp': 1740955239.1720476}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 203 is less than current step: 499. Dropping entry: {'train/lr': 0.00022010204081632652, '_timestamp': 1740955239.1723804}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 204 is less than current step: 499. Dropping entry: {'train/loss': 1.7710042715072631, 'train/ce_loss': 0.4908203125, 'train/seg_cls_loss': 0.01497802734375, 'train/kl_loss': 0.2859375, 'train/mask_bce_loss': 0.13358603417873383, 'train/mask_dice_loss': 0.4884395360946655, 'train/mask_loss': 0.6220255732536316, 'metrics/total_secs_per_batch': 5.790546894073486, 'metrics/data_secs_per_batch': 2.7359345674514772, '_timestamp': 1740955244.9624271}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 204 is less than current step: 499. Dropping entry: {'train/lr': 0.00021997959183673468, '_timestamp': 1740955244.9627192}).
Epoch: [1][205/500]	Time  5.791 ( 5.791)	Loss 0.8555 (1.7710)	CeLoss 0.8555 (0.4908)	SegCLSLoss 0.0000 (0.0150)	KLLoss 0.0000 (0.2859)	MaskLoss 0.0000 (0.6220)	MaskBCELoss 0.0000 (0.1336)	MaskDICELoss 0.0000 (0.4884)
Epoch: [1][206/500]	Time  6.736 ( 6.736)	Loss 0.7227 (2.0024)	CeLoss 0.7227 (0.2566)	SegCLSLoss 0.0000 (0.0274)	KLLoss 0.0000 (0.3482)	MaskLoss 0.0000 (0.8486)	MaskBCELoss 0.0000 (0.1350)	MaskDICELoss 0.0000 (0.7136)
Epoch: [1][207/500]	Time  5.694 ( 5.694)	Loss 0.1689 (1.9183)	CeLoss 0.1689 (0.3566)	SegCLSLoss 0.0000 (0.0172)	KLLoss 0.0000 (0.2822)	MaskLoss 0.0000 (0.7622)	MaskBCELoss 0.0000 (0.1524)	MaskDICELoss 0.0000 (0.6098)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 205 is less than current step: 499. Dropping entry: {'train/loss': 2.0023632049560547, 'train/ce_loss': 0.256640625, 'train/seg_cls_loss': 0.0274169921875, 'train/kl_loss': 0.3482421875, 'train/mask_bce_loss': 0.13502826672047377, 'train/mask_dice_loss': 0.7136142790317536, 'train/mask_loss': 0.8486425459384919, 'metrics/total_secs_per_batch': 6.736351728439331, 'metrics/data_secs_per_batch': 3.073914098739624, '_timestamp': 1740955251.698759}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 205 is less than current step: 499. Dropping entry: {'train/lr': 0.00021985714285714284, '_timestamp': 1740955251.6990497}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 206 is less than current step: 499. Dropping entry: {'train/loss': 1.9183271408081055, 'train/ce_loss': 0.356640625, 'train/seg_cls_loss': 0.017181396484375, 'train/kl_loss': 0.2822265625, 'train/mask_bce_loss': 0.15244624856859446, 'train/mask_dice_loss': 0.609793508052826, 'train/mask_loss': 0.7622397541999817, 'metrics/total_secs_per_batch': 5.6935811042785645, 'metrics/data_secs_per_batch': 2.297310137748718, '_timestamp': 1740955257.392544}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 206 is less than current step: 499. Dropping entry: {'train/lr': 0.000219734693877551, '_timestamp': 1740955257.3928857}).
Epoch: [1][208/500]	Time  4.643 ( 4.643)	Loss 1.0860 (1.7313)	CeLoss 0.3887 (0.6393)	SegCLSLoss 0.0143 (0.0120)	KLLoss 0.4199 (0.2436)	MaskLoss 0.3242 (0.5308)	MaskBCELoss 0.1332 (0.1599)	MaskDICELoss 0.1910 (0.3708)
Epoch: [1][209/500]	Time  6.185 ( 6.185)	Loss 1.3309 (1.9667)	CeLoss 0.2148 (0.2805)	SegCLSLoss 0.0160 (0.0266)	KLLoss 0.4316 (0.3568)	MaskLoss 0.5326 (0.8187)	MaskBCELoss 0.1233 (0.1526)	MaskDICELoss 0.4093 (0.6661)
[2025-03-02 16:41:14,564] [INFO] [logging.py:96:log_dist] [Rank 0] step=710, skipped=0, lr=[0.0002194285714285714], mom=[(0.9, 0.95)]
[2025-03-02 16:41:14,570] [INFO] [timer.py:215:stop] epoch=0/micro_step=7100/global_step=710, RunningAvgSamplesPerSec=1.5053583681712148, CurrSamplesPerSec=1.5750197886479644, MemAllocated=31.57GB, MaxMemAllocated=36.81GB
Epoch: [1][210/500]	Time  6.351 ( 6.351)	Loss 2.4072 (2.0678)	CeLoss 0.2275 (0.3155)	SegCLSLoss 0.0361 (0.0208)	KLLoss 0.3945 (0.3600)	MaskLoss 1.0610 (0.8531)	MaskBCELoss 0.1090 (0.1490)	MaskDICELoss 0.9520 (0.7041)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 207 is less than current step: 499. Dropping entry: {'train/loss': 1.7312614560127257, 'train/ce_loss': 0.6392578125, 'train/seg_cls_loss': 0.012042236328125, 'train/kl_loss': 0.2435546875, 'train/mask_bce_loss': 0.15992551818490028, 'train/mask_dice_loss': 0.37084191739559175, 'train/mask_loss': 0.530767434835434, 'metrics/total_secs_per_batch': 4.6434326171875, 'metrics/data_secs_per_batch': 1.901824951171875, '_timestamp': 1740955262.0357788}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 207 is less than current step: 499. Dropping entry: {'train/lr': 0.00021961224489795917, '_timestamp': 1740955262.0360646}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 208 is less than current step: 499. Dropping entry: {'train/loss': 1.9667084455490111, 'train/ce_loss': 0.28046875, 'train/seg_cls_loss': 0.02659912109375, 'train/kl_loss': 0.3568359375, 'train/mask_bce_loss': 0.152588374260813, 'train/mask_dice_loss': 0.6661174088716507, 'train/mask_loss': 0.8187057733535766, 'metrics/total_secs_per_batch': 6.184584379196167, 'metrics/data_secs_per_batch': 2.905204176902771, '_timestamp': 1740955268.2203672}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 208 is less than current step: 499. Dropping entry: {'train/lr': 0.00021948979591836733, '_timestamp': 1740955268.220646}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 209 is less than current step: 499. Dropping entry: {'train/loss': 2.067847120761871, 'train/ce_loss': 0.31552734375, 'train/seg_cls_loss': 0.02078857421875, 'train/kl_loss': 0.3599609375, 'train/mask_bce_loss': 0.1489865154027939, 'train/mask_dice_loss': 0.7040776550769806, 'train/mask_loss': 0.8530641794204712, 'metrics/total_secs_per_batch': 6.350690126419067, 'metrics/data_secs_per_batch': 2.70901198387146, '_timestamp': 1740955274.570863}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 209 is less than current step: 499. Dropping entry: {'train/lr': 0.0002193673469387755, '_timestamp': 1740955274.5711477}).
Epoch: [1][211/500]	Time  6.439 ( 6.439)	Loss 2.4225 (2.0950)	CeLoss 0.2773 (0.5271)	SegCLSLoss 0.0221 (0.0170)	KLLoss 0.3867 (0.3209)	MaskLoss 1.0472 (0.7638)	MaskBCELoss 0.0532 (0.1700)	MaskDICELoss 0.9940 (0.5938)
Epoch: [1][212/500]	Time  6.087 ( 6.087)	Loss 0.8828 (1.6682)	CeLoss 0.8828 (0.5008)	SegCLSLoss 0.0000 (0.0118)	KLLoss 0.0000 (0.2412)	MaskLoss 0.0000 (0.5687)	MaskBCELoss 0.0000 (0.1066)	MaskDICELoss 0.0000 (0.4621)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 210 is less than current step: 499. Dropping entry: {'train/loss': 2.09497709274292, 'train/ce_loss': 0.5271484375, 'train/seg_cls_loss': 0.017010498046875, 'train/kl_loss': 0.3208984375, 'train/mask_bce_loss': 0.17001922223716975, 'train/mask_dice_loss': 0.5938267290592194, 'train/mask_loss': 0.76384596824646, 'metrics/total_secs_per_batch': 6.438727855682373, 'metrics/data_secs_per_batch': 3.0936513900756837, '_timestamp': 1740955281.0100133}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 210 is less than current step: 499. Dropping entry: {'train/lr': 0.00021924489795918366, '_timestamp': 1740955281.010366}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 211 is less than current step: 499. Dropping entry: {'train/loss': 1.6682397484779359, 'train/ce_loss': 0.50078125, 'train/seg_cls_loss': 0.011834716796875, 'train/kl_loss': 0.2412109375, 'train/mask_bce_loss': 0.10662522315979003, 'train/mask_dice_loss': 0.4621138036251068, 'train/mask_loss': 0.5687390327453613, 'metrics/total_secs_per_batch': 6.087100028991699, 'metrics/data_secs_per_batch': 2.5369499206542967, '_timestamp': 1740955287.096923}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 211 is less than current step: 499. Dropping entry: {'train/lr': 0.00021912244897959182, '_timestamp': 1740955287.0972166}).
Epoch: [1][213/500]	Time  6.588 ( 6.588)	Loss 2.4326 (1.6346)	CeLoss 0.2656 (0.3105)	SegCLSLoss 0.0132 (0.0150)	KLLoss 0.4102 (0.2799)	MaskLoss 1.0601 (0.6444)	MaskBCELoss 0.3234 (0.1566)	MaskDICELoss 0.7367 (0.4878)
Epoch: [1][214/500]	Time  6.125 ( 6.125)	Loss 0.4180 (1.9501)	CeLoss 0.4180 (0.5314)	SegCLSLoss 0.0000 (0.0195)	KLLoss 0.0000 (0.2748)	MaskLoss 0.0000 (0.6908)	MaskBCELoss 0.0000 (0.1105)	MaskDICELoss 0.0000 (0.5802)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 212 is less than current step: 499. Dropping entry: {'train/loss': 1.634600818157196, 'train/ce_loss': 0.310546875, 'train/seg_cls_loss': 0.01497802734375, 'train/kl_loss': 0.2798828125, 'train/mask_bce_loss': 0.15655009020119906, 'train/mask_dice_loss': 0.4878499358892441, 'train/mask_loss': 0.6444000244140625, 'metrics/total_secs_per_batch': 6.587926387786865, 'metrics/data_secs_per_batch': 2.8040934562683106, '_timestamp': 1740955293.6848514}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 212 is less than current step: 499. Dropping entry: {'train/lr': 0.00021899999999999998, '_timestamp': 1740955293.6850548}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 213 is less than current step: 499. Dropping entry: {'train/loss': 1.9501245379447938, 'train/ce_loss': 0.5314453125, 'train/seg_cls_loss': 0.01953125, 'train/kl_loss': 0.2748046875, 'train/mask_bce_loss': 0.1105425076559186, 'train/mask_dice_loss': 0.5802424252033234, 'train/mask_loss': 0.6907849311828613, 'metrics/total_secs_per_batch': 6.124784469604492, 'metrics/data_secs_per_batch': 2.7631526947021485, '_timestamp': 1740955299.8098207}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 213 is less than current step: 499. Dropping entry: {'train/lr': 0.00021887755102040815, '_timestamp': 1740955299.8101795}).
Epoch: [1][215/500]	Time  6.242 ( 6.242)	Loss 1.5331 (1.9435)	CeLoss 0.2930 (0.3891)	SegCLSLoss 0.0133 (0.0160)	KLLoss 0.4004 (0.3211)	MaskLoss 0.5966 (0.7571)	MaskBCELoss 0.0882 (0.2209)	MaskDICELoss 0.5085 (0.5362)
Epoch: [1][216/500]	Time  6.378 ( 6.378)	Loss 0.2012 (1.2931)	CeLoss 0.2012 (0.3064)	SegCLSLoss 0.0000 (0.0122)	KLLoss 0.0000 (0.2393)	MaskLoss 0.0000 (0.4783)	MaskBCELoss 0.0000 (0.0948)	MaskDICELoss 0.0000 (0.3836)
Epoch: [1][217/500]	Time  6.270 ( 6.270)	Loss 2.5134 (2.0820)	CeLoss 0.1943 (0.2226)	SegCLSLoss 0.0303 (0.0199)	KLLoss 0.3730 (0.3621)	MaskLoss 1.1336 (0.9066)	MaskBCELoss 0.3973 (0.1909)	MaskDICELoss 0.7363 (0.7158)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 214 is less than current step: 499. Dropping entry: {'train/loss': 1.9434709906578065, 'train/ce_loss': 0.3890625, 'train/seg_cls_loss': 0.015972900390625, 'train/kl_loss': 0.32109375, 'train/mask_bce_loss': 0.22094195773825048, 'train/mask_dice_loss': 0.5361939311027527, 'train/mask_loss': 0.7571358919143677, 'metrics/total_secs_per_batch': 6.241708993911743, 'metrics/data_secs_per_batch': 2.6983863592147825, '_timestamp': 1740955306.0513728}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 214 is less than current step: 499. Dropping entry: {'train/lr': 0.0002187551020408163, '_timestamp': 1740955306.0516713}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 215 is less than current step: 499. Dropping entry: {'train/loss': 1.2930590033531189, 'train/ce_loss': 0.3064453125, 'train/seg_cls_loss': 0.012188720703125, 'train/kl_loss': 0.2392578125, 'train/mask_bce_loss': 0.09476655796170234, 'train/mask_dice_loss': 0.3835500627756119, 'train/mask_loss': 0.47831661701202394, 'metrics/total_secs_per_batch': 6.377508878707886, 'metrics/data_secs_per_batch': 2.6615835666656493, '_timestamp': 1740955312.4291244}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 215 is less than current step: 499. Dropping entry: {'train/lr': 0.00021863265306122447, '_timestamp': 1740955312.4295073}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 216 is less than current step: 499. Dropping entry: {'train/loss': 2.0819942235946653, 'train/ce_loss': 0.222607421875, 'train/seg_cls_loss': 0.019879150390625, 'train/kl_loss': 0.362109375, 'train/mask_bce_loss': 0.19088398516178132, 'train/mask_dice_loss': 0.7157625377178192, 'train/mask_loss': 0.9066465139389038, 'metrics/total_secs_per_batch': 6.2700629234313965, 'metrics/data_secs_per_batch': 2.833427405357361, '_timestamp': 1740955318.6989045}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 216 is less than current step: 499. Dropping entry: {'train/lr': 0.00021851020408163264, '_timestamp': 1740955318.6991928}).
Epoch: [1][218/500]	Time  6.166 ( 6.166)	Loss 2.2792 (1.8287)	CeLoss 0.3887 (0.3232)	SegCLSLoss 0.0128 (0.0209)	KLLoss 0.4316 (0.3141)	MaskLoss 0.9208 (0.7319)	MaskBCELoss 0.6435 (0.1467)	MaskDICELoss 0.2773 (0.5852)
Epoch: [1][219/500]	Time  5.370 ( 5.370)	Loss 2.0346 (1.9739)	CeLoss 0.1875 (0.6310)	SegCLSLoss 0.0266 (0.0125)	KLLoss 0.3945 (0.2357)	MaskLoss 0.8972 (0.6565)	MaskBCELoss 0.0604 (0.1820)	MaskDICELoss 0.8368 (0.4745)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 217 is less than current step: 499. Dropping entry: {'train/loss': 1.8287275314331055, 'train/ce_loss': 0.323193359375, 'train/seg_cls_loss': 0.020867919921875, 'train/kl_loss': 0.3140625, 'train/mask_bce_loss': 0.14666986353695394, 'train/mask_dice_loss': 0.5851987779140473, 'train/mask_loss': 0.7318686366081237, 'metrics/total_secs_per_batch': 6.165951490402222, 'metrics/data_secs_per_batch': 2.943451929092407, '_timestamp': 1740955324.8651023}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 217 is less than current step: 499. Dropping entry: {'train/lr': 0.00021838775510204077, '_timestamp': 1740955324.865463}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 218 is less than current step: 499. Dropping entry: {'train/loss': 1.9738857746124268, 'train/ce_loss': 0.63095703125, 'train/seg_cls_loss': 0.012506103515625, 'train/kl_loss': 0.2357421875, 'train/mask_bce_loss': 0.18203043527901172, 'train/mask_dice_loss': 0.47449253797531127, 'train/mask_loss': 0.6565229713916778, 'metrics/total_secs_per_batch': 5.369857311248779, 'metrics/data_secs_per_batch': 2.032549786567688, '_timestamp': 1740955330.2347577}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 218 is less than current step: 499. Dropping entry: {'train/lr': 0.00021826530612244893, '_timestamp': 1740955330.2350473}).
[2025-03-02 16:42:15,985] [INFO] [logging.py:96:log_dist] [Rank 0] step=720, skipped=0, lr=[0.00021820408163265303], mom=[(0.9, 0.95)]
[2025-03-02 16:42:15,991] [INFO] [timer.py:215:stop] epoch=0/micro_step=7200/global_step=720, RunningAvgSamplesPerSec=1.5069448930288172, CurrSamplesPerSec=1.7373419641849344, MemAllocated=30.65GB, MaxMemAllocated=36.81GB
Epoch: [1][220/500]	Time  5.758 ( 5.758)	Loss 1.5771 (1.8687)	CeLoss 0.1807 (0.4575)	SegCLSLoss 0.0131 (0.0170)	KLLoss 0.4180 (0.2756)	MaskLoss 0.6738 (0.6876)	MaskBCELoss 0.5341 (0.2200)	MaskDICELoss 0.1397 (0.4675)
Epoch: [1][221/500]	Time  6.113 ( 6.113)	Loss 2.5409 (1.9545)	CeLoss 0.1299 (0.5101)	SegCLSLoss 0.0601 (0.0190)	KLLoss 0.3711 (0.2723)	MaskLoss 1.1718 (0.7038)	MaskBCELoss 0.2332 (0.1518)	MaskDICELoss 0.9387 (0.5520)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 219 is less than current step: 499. Dropping entry: {'train/loss': 1.8687100648880004, 'train/ce_loss': 0.45751953125, 'train/seg_cls_loss': 0.016986083984375, 'train/kl_loss': 0.2755859375, 'train/mask_bce_loss': 0.22002771086990833, 'train/mask_dice_loss': 0.46754997968673706, 'train/mask_loss': 0.6875776886940003, 'metrics/total_secs_per_batch': 5.757549285888672, 'metrics/data_secs_per_batch': 2.8004736423492433, '_timestamp': 1740955335.9920933}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 219 is less than current step: 499. Dropping entry: {'train/lr': 0.0002181428571428571, '_timestamp': 1740955335.9922838}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 220 is less than current step: 499. Dropping entry: {'train/loss': 1.9545163631439209, 'train/ce_loss': 0.51005859375, 'train/seg_cls_loss': 0.019024658203125, 'train/kl_loss': 0.272265625, 'train/mask_bce_loss': 0.15178703237324953, 'train/mask_dice_loss': 0.5520336240530014, 'train/mask_loss': 0.7038206696510315, 'metrics/total_secs_per_batch': 6.113441228866577, 'metrics/data_secs_per_batch': 2.870169520378113, '_timestamp': 1740955342.105977}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 220 is less than current step: 499. Dropping entry: {'train/lr': 0.00021802040816326532, '_timestamp': 1740955342.1063626}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 221 is less than current step: 499. Dropping entry: {'train/loss': 2.114214324951172, 'train/ce_loss': 0.39462890625, 'train/seg_cls_loss': 0.0203125, 'train/kl_loss': 0.30703125, 'train/mask_bce_loss': 0.14115771893411874, 'train/mask_dice_loss': 0.6980295300483703, 'train/mask_loss': 0.8391872465610504, 'metrics/total_secs_per_batch': 7.165316104888916, 'metrics/data_secs_per_batch': 3.267428922653198, '_timestamp': 1740955349.2712958}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 221 is less than current step: 499. Dropping entry: {'train/lr': 0.00021789795918367348, '_timestamp': 1740955349.2716634}).
Epoch: [1][222/500]	Time  7.165 ( 7.165)	Loss 1.1094 (2.1142)	CeLoss 1.1094 (0.3946)	SegCLSLoss 0.0000 (0.0203)	KLLoss 0.0000 (0.3070)	MaskLoss 0.0000 (0.8392)	MaskBCELoss 0.0000 (0.1412)	MaskDICELoss 0.0000 (0.6980)
Epoch: [1][223/500]	Time  5.072 ( 5.072)	Loss 1.6401 (1.7312)	CeLoss 0.2100 (0.6403)	SegCLSLoss 0.0200 (0.0141)	KLLoss 0.4062 (0.2330)	MaskLoss 0.6902 (0.5303)	MaskBCELoss 0.0100 (0.1052)	MaskDICELoss 0.6801 (0.4251)
Epoch: [1][224/500]	Time  6.109 ( 6.109)	Loss 2.5419 (2.0404)	CeLoss 0.2207 (0.4129)	SegCLSLoss 0.0281 (0.0195)	KLLoss 0.3867 (0.3145)	MaskLoss 1.1342 (0.7931)	MaskBCELoss 0.1995 (0.0952)	MaskDICELoss 0.9348 (0.6979)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 222 is less than current step: 499. Dropping entry: {'train/loss': 1.7311575531959533, 'train/ce_loss': 0.64033203125, 'train/seg_cls_loss': 0.01405029296875, 'train/kl_loss': 0.2330078125, 'train/mask_bce_loss': 0.10522083155810832, 'train/mask_dice_loss': 0.42510404586791994, 'train/mask_loss': 0.5303248703479767, 'metrics/total_secs_per_batch': 5.07190728187561, 'metrics/data_secs_per_batch': 2.295151972770691, '_timestamp': 1740955354.3430429}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 222 is less than current step: 499. Dropping entry: {'train/lr': 0.00021777551020408161, '_timestamp': 1740955354.3432405}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 223 is less than current step: 499. Dropping entry: {'train/loss': 2.04042809009552, 'train/ce_loss': 0.412890625, 'train/seg_cls_loss': 0.01947021484375, 'train/kl_loss': 0.314453125, 'train/mask_bce_loss': 0.09524080418050289, 'train/mask_dice_loss': 0.6978736042976379, 'train/mask_loss': 0.7931144118309021, 'metrics/total_secs_per_batch': 6.109479188919067, 'metrics/data_secs_per_batch': 2.699254870414734, '_timestamp': 1740955360.4525797}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 223 is less than current step: 499. Dropping entry: {'train/lr': 0.00021765306122448978, '_timestamp': 1740955360.452907}).
Epoch: [1][225/500]	Time  7.063 ( 7.063)	Loss 1.0870 (1.9890)	CeLoss 0.2471 (0.3456)	SegCLSLoss 0.0339 (0.0206)	KLLoss 0.3730 (0.3520)	MaskLoss 0.3931 (0.7990)	MaskBCELoss 0.0902 (0.1214)	MaskDICELoss 0.3029 (0.6776)
Epoch: [1][226/500]	Time  6.625 ( 6.625)	Loss 2.2795 (1.8658)	CeLoss 0.2188 (0.4085)	SegCLSLoss 0.0157 (0.0168)	KLLoss 0.3984 (0.3162)	MaskLoss 1.0069 (0.7085)	MaskBCELoss 0.0071 (0.1492)	MaskDICELoss 0.9998 (0.5593)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 224 is less than current step: 499. Dropping entry: {'train/loss': 1.9890179634094238, 'train/ce_loss': 0.34560546875, 'train/seg_cls_loss': 0.02064208984375, 'train/kl_loss': 0.351953125, 'train/mask_bce_loss': 0.12138860896229745, 'train/mask_dice_loss': 0.6776125639677048, 'train/mask_loss': 0.7990011692047119, 'metrics/total_secs_per_batch': 7.063074588775635, 'metrics/data_secs_per_batch': 3.1708413124084474, '_timestamp': 1740955367.5155792}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 224 is less than current step: 499. Dropping entry: {'train/lr': 0.00021753061224489794, '_timestamp': 1740955367.5158832}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 225 is less than current step: 499. Dropping entry: {'train/loss': 1.8658107280731202, 'train/ce_loss': 0.40849609375, 'train/seg_cls_loss': 0.0168212890625, 'train/kl_loss': 0.3162109375, 'train/mask_bce_loss': 0.14922659578733147, 'train/mask_dice_loss': 0.5593135207891464, 'train/mask_loss': 0.7085401311516761, 'metrics/total_secs_per_batch': 6.6246113777160645, 'metrics/data_secs_per_batch': 2.9594633102416994, '_timestamp': 1740955374.14017}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 225 is less than current step: 499. Dropping entry: {'train/lr': 0.0002174081632653061, '_timestamp': 1740955374.1404636}).
Epoch: [1][227/500]	Time  6.085 ( 6.085)	Loss 2.5170 (1.6714)	CeLoss 0.4629 (0.4859)	SegCLSLoss 0.0166 (0.0150)	KLLoss 0.3906 (0.2369)	MaskLoss 1.0046 (0.5774)	MaskBCELoss 0.0105 (0.0809)	MaskDICELoss 0.9941 (0.4965)
Epoch: [1][228/500]	Time  7.031 ( 7.031)	Loss 2.2663 (2.0037)	CeLoss 0.2852 (0.3203)	SegCLSLoss 0.0154 (0.0196)	KLLoss 0.3965 (0.3545)	MaskLoss 0.9671 (0.8190)	MaskBCELoss 0.1336 (0.1213)	MaskDICELoss 0.8335 (0.6977)
Epoch: [1][229/500]	Time  4.997 ( 4.997)	Loss 1.7233 (1.5977)	CeLoss 0.2490 (0.5026)	SegCLSLoss 0.0153 (0.0144)	KLLoss 0.4121 (0.2404)	MaskLoss 0.7132 (0.5318)	MaskBCELoss 0.3026 (0.1249)	MaskDICELoss 0.4106 (0.4069)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 226 is less than current step: 499. Dropping entry: {'train/loss': 1.6714253544807434, 'train/ce_loss': 0.4859375, 'train/seg_cls_loss': 0.014996337890625, 'train/kl_loss': 0.2369140625, 'train/mask_bce_loss': 0.08094097096472978, 'train/mask_dice_loss': 0.49647092819213867, 'train/mask_loss': 0.5774119019508361, 'metrics/total_secs_per_batch': 6.085096597671509, 'metrics/data_secs_per_batch': 2.717500996589661, '_timestamp': 1740955380.2256231}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 226 is less than current step: 499. Dropping entry: {'train/lr': 0.00021728571428571427, '_timestamp': 1740955380.2260053}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 227 is less than current step: 499. Dropping entry: {'train/loss': 2.003697121143341, 'train/ce_loss': 0.3203125, 'train/seg_cls_loss': 0.019622802734375, 'train/kl_loss': 0.3544921875, 'train/mask_bce_loss': 0.12134270668029785, 'train/mask_dice_loss': 0.6976933598518371, 'train/mask_loss': 0.8190360546112061, 'metrics/total_secs_per_batch': 7.031178951263428, 'metrics/data_secs_per_batch': 3.0651276826858522, '_timestamp': 1740955387.2566824}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 227 is less than current step: 499. Dropping entry: {'train/lr': 0.00021716326530612243, '_timestamp': 1740955387.2570372}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 228 is less than current step: 499. Dropping entry: {'train/loss': 1.5976565957069397, 'train/ce_loss': 0.50263671875, 'train/seg_cls_loss': 0.01444091796875, 'train/kl_loss': 0.2404296875, 'train/mask_bce_loss': 0.12493246458470822, 'train/mask_dice_loss': 0.4069036394357681, 'train/mask_loss': 0.5318361103534699, 'metrics/total_secs_per_batch': 4.996510744094849, 'metrics/data_secs_per_batch': 2.2637051343917847, '_timestamp': 1740955392.2529316}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 228 is less than current step: 499. Dropping entry: {'train/lr': 0.0002170408163265306, '_timestamp': 1740955392.2532172}).
[2025-03-02 16:43:18,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=730, skipped=0, lr=[0.0002169795918367347], mom=[(0.9, 0.95)]
[2025-03-02 16:43:18,528] [INFO] [timer.py:215:stop] epoch=0/micro_step=7300/global_step=730, RunningAvgSamplesPerSec=1.508142468034768, CurrSamplesPerSec=1.5936406113786659, MemAllocated=31.56GB, MaxMemAllocated=36.81GB
Epoch: [1][230/500]	Time  6.276 ( 6.276)	Loss 2.0459 (2.2016)	CeLoss 0.2314 (0.3266)	SegCLSLoss 0.0203 (0.0249)	KLLoss 0.4023 (0.3520)	MaskLoss 0.8823 (0.9139)	MaskBCELoss 0.1093 (0.1785)	MaskDICELoss 0.7730 (0.7353)
Epoch: [1][231/500]	Time  5.340 ( 5.340)	Loss 1.9012 (1.5860)	CeLoss 0.2393 (0.5579)	SegCLSLoss 0.0197 (0.0127)	KLLoss 0.3945 (0.1939)	MaskLoss 0.8061 (0.5012)	MaskBCELoss 0.0556 (0.0729)	MaskDICELoss 0.7505 (0.4283)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 229 is less than current step: 499. Dropping entry: {'train/loss': 2.2015681147575377, 'train/ce_loss': 0.3265625, 'train/seg_cls_loss': 0.02486572265625, 'train/kl_loss': 0.351953125, 'train/mask_bce_loss': 0.17854863442480565, 'train/mask_dice_loss': 0.7353213608264924, 'train/mask_loss': 0.913869994878769, 'metrics/total_secs_per_batch': 6.276493072509766, 'metrics/data_secs_per_batch': 2.839909482002258, '_timestamp': 1740955398.5292625}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 229 is less than current step: 499. Dropping entry: {'train/lr': 0.00021691836734693876, '_timestamp': 1740955398.5295901}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 230 is less than current step: 499. Dropping entry: {'train/loss': 1.5860180497169494, 'train/ce_loss': 0.55791015625, 'train/seg_cls_loss': 0.01270751953125, 'train/kl_loss': 0.1939453125, 'train/mask_bce_loss': 0.07285879105329514, 'train/mask_dice_loss': 0.42830452919006345, 'train/mask_loss': 0.5011633217334748, 'metrics/total_secs_per_batch': 5.340342998504639, 'metrics/data_secs_per_batch': 2.144372057914734, '_timestamp': 1740955403.8698027}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 230 is less than current step: 499. Dropping entry: {'train/lr': 0.00021679591836734692, '_timestamp': 1740955403.8702078}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 231 is less than current step: 499. Dropping entry: {'train/loss': 2.0348289966583253, 'train/ce_loss': 0.49990234375, 'train/seg_cls_loss': 0.0165283203125, 'train/kl_loss': 0.234765625, 'train/mask_bce_loss': 0.24219108903780578, 'train/mask_dice_loss': 0.5094030916690826, 'train/mask_loss': 0.7515941977500915, 'metrics/total_secs_per_batch': 6.028999328613281, 'metrics/data_secs_per_batch': 2.7683427810668944, '_timestamp': 1740955409.8990018}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 231 is less than current step: 499. Dropping entry: {'train/lr': 0.00021667346938775508, '_timestamp': 1740955409.8994644}).
Epoch: [1][232/500]	Time  6.029 ( 6.029)	Loss 1.3516 (2.0348)	CeLoss 1.3516 (0.4999)	SegCLSLoss 0.0000 (0.0165)	KLLoss 0.0000 (0.2348)	MaskLoss 0.0000 (0.7516)	MaskBCELoss 0.0000 (0.2422)	MaskDICELoss 0.0000 (0.5094)
Epoch: [1][233/500]	Time  5.407 ( 5.407)	Loss 2.6239 (1.3639)	CeLoss 0.3086 (0.5283)	SegCLSLoss 0.0126 (0.0071)	KLLoss 0.4180 (0.1600)	MaskLoss 1.1342 (0.4080)	MaskBCELoss 0.3610 (0.0715)	MaskDICELoss 0.7732 (0.3364)
Epoch: [1][234/500]	Time  6.188 ( 6.188)	Loss 2.0877 (1.9646)	CeLoss 0.2617 (0.4150)	SegCLSLoss 0.0130 (0.0177)	KLLoss 0.4160 (0.3131)	MaskLoss 0.8896 (0.7549)	MaskBCELoss 0.1859 (0.1026)	MaskDICELoss 0.7037 (0.6523)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 232 is less than current step: 499. Dropping entry: {'train/loss': 1.3638948917388916, 'train/ce_loss': 0.5283203125, 'train/seg_cls_loss': 0.007098388671875, 'train/kl_loss': 0.1599609375, 'train/mask_bce_loss': 0.07154900543391704, 'train/mask_dice_loss': 0.3364238262176514, 'train/mask_loss': 0.40797284841537473, 'metrics/total_secs_per_batch': 5.406785011291504, 'metrics/data_secs_per_batch': 2.446608328819275, '_timestamp': 1740955415.305869}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 232 is less than current step: 499. Dropping entry: {'train/lr': 0.00021655102040816324, '_timestamp': 1740955415.3063295}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 233 is less than current step: 499. Dropping entry: {'train/loss': 1.964585816860199, 'train/ce_loss': 0.4150390625, 'train/seg_cls_loss': 0.017694091796875, 'train/kl_loss': 0.3130859375, 'train/mask_bce_loss': 0.10264187082648277, 'train/mask_dice_loss': 0.6522584676742553, 'train/mask_loss': 0.754900336265564, 'metrics/total_secs_per_batch': 6.187755107879639, 'metrics/data_secs_per_batch': 2.527166175842285, '_timestamp': 1740955421.4936318}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 233 is less than current step: 499. Dropping entry: {'train/lr': 0.0002164285714285714, '_timestamp': 1740955421.4940078}).
Epoch: [1][235/500]	Time  6.579 ( 6.579)	Loss 2.6642 (2.1238)	CeLoss 0.2871 (0.3218)	SegCLSLoss 0.0308 (0.0231)	KLLoss 0.3789 (0.3531)	MaskLoss 1.1622 (0.8775)	MaskBCELoss 0.3855 (0.1751)	MaskDICELoss 0.7766 (0.7024)
Epoch: [1][236/500]	Time  6.132 ( 6.132)	Loss 2.3501 (1.8058)	CeLoss 0.2754 (0.3973)	SegCLSLoss 0.0210 (0.0127)	KLLoss 0.3887 (0.2809)	MaskLoss 1.0129 (0.6872)	MaskBCELoss 0.1017 (0.1491)	MaskDICELoss 0.9113 (0.5381)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 234 is less than current step: 499. Dropping entry: {'train/loss': 2.1237861037254335, 'train/ce_loss': 0.32177734375, 'train/seg_cls_loss': 0.023101806640625, 'train/kl_loss': 0.353125, 'train/mask_bce_loss': 0.1750792648643255, 'train/mask_dice_loss': 0.7023899406194687, 'train/mask_loss': 0.8774692118167877, 'metrics/total_secs_per_batch': 6.579344034194946, 'metrics/data_secs_per_batch': 2.9090670347213745, '_timestamp': 1740955428.072721}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 234 is less than current step: 499. Dropping entry: {'train/lr': 0.00021630612244897957, '_timestamp': 1740955428.0730264}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 235 is less than current step: 499. Dropping entry: {'train/loss': 1.8058488607406615, 'train/ce_loss': 0.397265625, 'train/seg_cls_loss': 0.0126953125, 'train/kl_loss': 0.280859375, 'train/mask_bce_loss': 0.1490912765264511, 'train/mask_dice_loss': 0.5380616605281829, 'train/mask_loss': 0.6871529340744018, 'metrics/total_secs_per_batch': 6.132352828979492, 'metrics/data_secs_per_batch': 2.542182517051697, '_timestamp': 1740955434.2054343}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 235 is less than current step: 499. Dropping entry: {'train/lr': 0.00021618367346938773, '_timestamp': 1740955434.205803}).
Epoch: [1][237/500]	Time  7.415 ( 7.415)	Loss 2.2396 (1.8673)	CeLoss 0.1738 (0.2011)	SegCLSLoss 0.0237 (0.0199)	KLLoss 0.3906 (0.3553)	MaskLoss 1.0075 (0.8104)	MaskBCELoss 0.0240 (0.0894)	MaskDICELoss 0.9835 (0.7210)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 236 is less than current step: 499. Dropping entry: {'train/loss': 1.867327630519867, 'train/ce_loss': 0.20107421875, 'train/seg_cls_loss': 0.019903564453125, 'train/kl_loss': 0.3552734375, 'train/mask_bce_loss': 0.08943185899406672, 'train/mask_dice_loss': 0.7209653615951538, 'train/mask_loss': 0.8103972256183625, 'metrics/total_secs_per_batch': 7.415022134780884, 'metrics/data_secs_per_batch': 3.2684659004211425, '_timestamp': 1740955441.6201131}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 236 is less than current step: 499. Dropping entry: {'train/lr': 0.0002160612244897959, '_timestamp': 1740955441.6203122}).
Epoch: [1][238/500]	Time 32.091 (32.091)	Loss 1.2656 (1.4909)	CeLoss 1.2656 (0.4600)	SegCLSLoss 0.0000 (0.0129)	KLLoss 0.0000 (0.2416)	MaskLoss 0.0000 (0.5001)	MaskBCELoss 0.0000 (0.0899)	MaskDICELoss 0.0000 (0.4102)
Epoch: [1][239/500]	Time  4.910 ( 4.910)	Loss 2.9393 (1.6630)	CeLoss 0.1826 (0.7783)	SegCLSLoss 0.0334 (0.0126)	KLLoss 0.3848 (0.2029)	MaskLoss 1.3505 (0.4291)	MaskBCELoss 0.5475 (0.1280)	MaskDICELoss 0.8030 (0.3010)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 237 is less than current step: 499. Dropping entry: {'train/loss': 1.4908796191215514, 'train/ce_loss': 0.4599853515625, 'train/seg_cls_loss': 0.01285400390625, 'train/kl_loss': 0.2416015625, 'train/mask_bce_loss': 0.08990549594163895, 'train/mask_dice_loss': 0.41016077995300293, 'train/mask_loss': 0.5000662803649902, 'metrics/total_secs_per_batch': 32.09093880653381, 'metrics/data_secs_per_batch': 20.770597171783447, '_timestamp': 1740955473.7111766}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 237 is less than current step: 499. Dropping entry: {'train/lr': 0.00021593877551020406, '_timestamp': 1740955473.7114055}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 238 is less than current step: 499. Dropping entry: {'train/loss': 1.66303870677948, 'train/ce_loss': 0.7783203125, 'train/seg_cls_loss': 0.012567138671875, 'train/kl_loss': 0.2029296875, 'train/mask_bce_loss': 0.12804450392723082, 'train/mask_dice_loss': 0.30103345662355424, 'train/mask_loss': 0.42907795011997224, 'metrics/total_secs_per_batch': 4.909512758255005, 'metrics/data_secs_per_batch': 2.2906433820724486, '_timestamp': 1740955478.6205516}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 238 is less than current step: 499. Dropping entry: {'train/lr': 0.00021581632653061222, '_timestamp': 1740955478.6209233}).
[2025-03-02 16:44:45,686] [INFO] [logging.py:96:log_dist] [Rank 0] step=740, skipped=0, lr=[0.0002157551020408163], mom=[(0.9, 0.95)]
[2025-03-02 16:44:45,692] [INFO] [timer.py:215:stop] epoch=0/micro_step=7400/global_step=740, RunningAvgSamplesPerSec=1.5017461847241282, CurrSamplesPerSec=1.4142861410745624, MemAllocated=31.44GB, MaxMemAllocated=36.81GB
Epoch: [1][240/500]	Time  7.072 ( 7.072)	Loss 1.9914 (1.8215)	CeLoss 0.2236 (0.4469)	SegCLSLoss 0.0186 (0.0178)	KLLoss 0.4102 (0.2805)	MaskLoss 0.8590 (0.6689)	MaskBCELoss 0.0405 (0.0813)	MaskDICELoss 0.8185 (0.5877)
Epoch: [1][241/500]	Time  6.594 ( 6.594)	Loss 1.3607 (1.9406)	CeLoss 0.2451 (0.3116)	SegCLSLoss 0.0184 (0.0195)	KLLoss 0.4102 (0.3668)	MaskLoss 0.5329 (0.7914)	MaskBCELoss 0.0337 (0.1263)	MaskDICELoss 0.4992 (0.6650)
Epoch: [1][242/500]	Time  6.371 ( 6.371)	Loss 2.6293 (2.1575)	CeLoss 0.2793 (0.4792)	SegCLSLoss 0.0208 (0.0165)	KLLoss 0.3965 (0.3195)	MaskLoss 1.1506 (0.8189)	MaskBCELoss 0.3736 (0.1616)	MaskDICELoss 0.7770 (0.6573)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 239 is less than current step: 499. Dropping entry: {'train/loss': 1.8214605808258058, 'train/ce_loss': 0.446875, 'train/seg_cls_loss': 0.017755126953125, 'train/kl_loss': 0.28046875, 'train/mask_bce_loss': 0.08127837143838405, 'train/mask_dice_loss': 0.5876550137996673, 'train/mask_loss': 0.6689333856105805, 'metrics/total_secs_per_batch': 7.072373390197754, 'metrics/data_secs_per_batch': 3.052358102798462, '_timestamp': 1740955485.6927373}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 239 is less than current step: 499. Dropping entry: {'train/lr': 0.00021569387755102039, '_timestamp': 1740955485.6930406}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 240 is less than current step: 499. Dropping entry: {'train/loss': 1.9406166553497315, 'train/ce_loss': 0.31162109375, 'train/seg_cls_loss': 0.019500732421875, 'train/kl_loss': 0.366796875, 'train/mask_bce_loss': 0.12630678452551364, 'train/mask_dice_loss': 0.6650464653968811, 'train/mask_loss': 0.7913532495498657, 'metrics/total_secs_per_batch': 6.593919038772583, 'metrics/data_secs_per_batch': 2.7017927885055544, '_timestamp': 1740955492.2869318}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 240 is less than current step: 499. Dropping entry: {'train/lr': 0.00021557142857142855, '_timestamp': 1740955492.2872374}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 241 is less than current step: 499. Dropping entry: {'train/loss': 2.157473015785217, 'train/ce_loss': 0.47919921875, 'train/seg_cls_loss': 0.01646728515625, 'train/kl_loss': 0.31953125, 'train/mask_bce_loss': 0.16155239385552705, 'train/mask_dice_loss': 0.6573208510875702, 'train/mask_loss': 0.8188732385635376, 'metrics/total_secs_per_batch': 6.371336221694946, 'metrics/data_secs_per_batch': 2.8266823291778564, '_timestamp': 1740955498.658414}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 241 is less than current step: 499. Dropping entry: {'train/lr': 0.0002154489795918367, '_timestamp': 1740955498.6587667}).
Epoch: [1][243/500]	Time  6.472 ( 6.472)	Loss 2.3471 (1.5896)	CeLoss 0.2793 (0.3573)	SegCLSLoss 0.0156 (0.0157)	KLLoss 0.4062 (0.2830)	MaskLoss 1.0095 (0.5981)	MaskBCELoss 0.2852 (0.1017)	MaskDICELoss 0.7243 (0.4964)
Epoch: [1][244/500]	Time  6.569 ( 6.569)	Loss 1.3672 (2.0045)	CeLoss 1.3672 (0.4396)	SegCLSLoss 0.0000 (0.0148)	KLLoss 0.0000 (0.3221)	MaskLoss 0.0000 (0.7628)	MaskBCELoss 0.0000 (0.1928)	MaskDICELoss 0.0000 (0.5701)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 242 is less than current step: 499. Dropping entry: {'train/loss': 1.5896499037742615, 'train/ce_loss': 0.357275390625, 'train/seg_cls_loss': 0.015673828125, 'train/kl_loss': 0.2830078125, 'train/mask_bce_loss': 0.10168629884719849, 'train/mask_dice_loss': 0.4963857173919678, 'train/mask_loss': 0.5980720162391663, 'metrics/total_secs_per_batch': 6.472100734710693, 'metrics/data_secs_per_batch': 2.6782283544540406, '_timestamp': 1740955505.1303313}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 242 is less than current step: 499. Dropping entry: {'train/lr': 0.00021532653061224488, '_timestamp': 1740955505.1306171}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 243 is less than current step: 499. Dropping entry: {'train/loss': 2.004542648792267, 'train/ce_loss': 0.43955078125, 'train/seg_cls_loss': 0.0148193359375, 'train/kl_loss': 0.3220703125, 'train/mask_bce_loss': 0.19275181088596582, 'train/mask_dice_loss': 0.5700663775205612, 'train/mask_loss': 0.76281818151474, 'metrics/total_secs_per_batch': 6.569112539291382, 'metrics/data_secs_per_batch': 3.1780920028686523, '_timestamp': 1740955511.6997278}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 243 is less than current step: 499. Dropping entry: {'train/lr': 0.00021520408163265307, '_timestamp': 1740955511.7001143}).
Epoch: [1][245/500]	Time  6.298 ( 6.298)	Loss 1.0167 (1.8810)	CeLoss 0.2598 (0.3903)	SegCLSLoss 0.0145 (0.0141)	KLLoss 0.4277 (0.3275)	MaskLoss 0.3531 (0.7253)	MaskBCELoss 0.0838 (0.1998)	MaskDICELoss 0.2692 (0.5255)
Epoch: [1][246/500]	Time  7.541 ( 7.541)	Loss 2.6392 (2.0444)	CeLoss 0.1426 (0.2412)	SegCLSLoss 0.0549 (0.0252)	KLLoss 0.3848 (0.3613)	MaskLoss 1.2151 (0.8772)	MaskBCELoss 0.3694 (0.2086)	MaskDICELoss 0.8457 (0.6686)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 244 is less than current step: 499. Dropping entry: {'train/loss': 1.8809765696525573, 'train/ce_loss': 0.3902587890625, 'train/seg_cls_loss': 0.014141845703125, 'train/kl_loss': 0.3275390625, 'train/mask_bce_loss': 0.1998251186683774, 'train/mask_dice_loss': 0.5255142629146576, 'train/mask_loss': 0.7253393828868866, 'metrics/total_secs_per_batch': 6.298499584197998, 'metrics/data_secs_per_batch': 2.9398912668228148, '_timestamp': 1740955517.9979215}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 244 is less than current step: 499. Dropping entry: {'train/lr': 0.00021508163265306123, '_timestamp': 1740955517.9982214}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 245 is less than current step: 499. Dropping entry: {'train/loss': 2.0444082975387574, 'train/ce_loss': 0.2412109375, 'train/seg_cls_loss': 0.02515869140625, 'train/kl_loss': 0.361328125, 'train/mask_bce_loss': 0.20859686471521854, 'train/mask_dice_loss': 0.6686365813016891, 'train/mask_loss': 0.8772334575653076, 'metrics/total_secs_per_batch': 7.541440010070801, 'metrics/data_secs_per_batch': 3.3224966287612916, '_timestamp': 1740955525.5393205}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 245 is less than current step: 499. Dropping entry: {'train/lr': 0.0002149591836734694, '_timestamp': 1740955525.5395906}).
Epoch: [1][247/500]	Time  5.862 ( 5.862)	Loss 1.3984 (2.0740)	CeLoss 1.3984 (0.4525)	SegCLSLoss 0.0000 (0.0209)	KLLoss 0.0000 (0.3229)	MaskLoss 0.0000 (0.7896)	MaskBCELoss 0.0000 (0.2100)	MaskDICELoss 0.0000 (0.5795)
Epoch: [1][248/500]	Time  5.339 ( 5.339)	Loss 2.3390 (1.8492)	CeLoss 0.2910 (0.4688)	SegCLSLoss 0.0157 (0.0199)	KLLoss 0.4219 (0.2795)	MaskLoss 0.9986 (0.6712)	MaskBCELoss 0.1905 (0.1854)	MaskDICELoss 0.8081 (0.4858)
Epoch: [1][249/500]	Time  6.442 ( 6.442)	Loss 2.1433 (1.4173)	CeLoss 0.1963 (0.4856)	SegCLSLoss 0.0320 (0.0141)	KLLoss 0.3984 (0.2021)	MaskLoss 0.9457 (0.4522)	MaskBCELoss 0.0555 (0.0637)	MaskDICELoss 0.8901 (0.3884)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 246 is less than current step: 499. Dropping entry: {'train/loss': 2.0740397930145265, 'train/ce_loss': 0.452490234375, 'train/seg_cls_loss': 0.0208740234375, 'train/kl_loss': 0.3228515625, 'train/mask_bce_loss': 0.21002659983932973, 'train/mask_dice_loss': 0.5795323580503464, 'train/mask_loss': 0.7895589649677277, 'metrics/total_secs_per_batch': 5.861782073974609, 'metrics/data_secs_per_batch': 2.573781681060791, '_timestamp': 1740955531.4011347}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 246 is less than current step: 499. Dropping entry: {'train/lr': 0.00021483673469387755, '_timestamp': 1740955531.4014127}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 247 is less than current step: 499. Dropping entry: {'train/loss': 1.8492204189300536, 'train/ce_loss': 0.468798828125, 'train/seg_cls_loss': 0.019921875, 'train/kl_loss': 0.2794921875, 'train/mask_bce_loss': 0.18542473763227463, 'train/mask_dice_loss': 0.4857674866914749, 'train/mask_loss': 0.6711922258138656, 'metrics/total_secs_per_batch': 5.339124441146851, 'metrics/data_secs_per_batch': 2.517747902870178, '_timestamp': 1740955536.7405105}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 247 is less than current step: 499. Dropping entry: {'train/lr': 0.00021471428571428572, '_timestamp': 1740955536.7409303}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 248 is less than current step: 499. Dropping entry: {'train/loss': 1.4173180103302, 'train/ce_loss': 0.48564453125, 'train/seg_cls_loss': 0.01405029296875, 'train/kl_loss': 0.2021484375, 'train/mask_bce_loss': 0.06373224947601557, 'train/mask_dice_loss': 0.38843261897563935, 'train/mask_loss': 0.4521648705005646, 'metrics/total_secs_per_batch': 6.441982984542847, 'metrics/data_secs_per_batch': 3.26724636554718, '_timestamp': 1740955543.1822891}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 248 is less than current step: 499. Dropping entry: {'train/lr': 0.00021459183673469388, '_timestamp': 1740955543.182654}).
[2025-03-02 16:45:50,185] [INFO] [logging.py:96:log_dist] [Rank 0] step=750, skipped=0, lr=[0.00021453061224489795], mom=[(0.9, 0.95)]
[2025-03-02 16:45:50,191] [INFO] [timer.py:215:stop] epoch=0/micro_step=7500/global_step=750, RunningAvgSamplesPerSec=1.5023806855311432, CurrSamplesPerSec=1.426968040614133, MemAllocated=31.31GB, MaxMemAllocated=36.81GB
Epoch: [1][250/500]	Time  7.010 ( 7.010)	Loss 2.5866 (1.6126)	CeLoss 0.1328 (0.2172)	SegCLSLoss 0.0527 (0.0222)	KLLoss 0.3887 (0.3168)	MaskLoss 1.1942 (0.6763)	MaskBCELoss 0.3492 (0.1493)	MaskDICELoss 0.8450 (0.5270)
Epoch: [1][251/500]	Time  6.801 ( 6.801)	Loss 2.3574 (1.8131)	CeLoss 0.2275 (0.3672)	SegCLSLoss 0.0177 (0.0180)	KLLoss 0.4160 (0.3248)	MaskLoss 1.0400 (0.7022)	MaskBCELoss 0.0430 (0.0936)	MaskDICELoss 0.9970 (0.6086)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 249 is less than current step: 499. Dropping entry: {'train/loss': 1.612588381767273, 'train/ce_loss': 0.217236328125, 'train/seg_cls_loss': 0.0221923828125, 'train/kl_loss': 0.316796875, 'train/mask_bce_loss': 0.1493396621197462, 'train/mask_dice_loss': 0.5269984573125839, 'train/mask_loss': 0.6763381212949753, 'metrics/total_secs_per_batch': 7.009547710418701, 'metrics/data_secs_per_batch': 3.2828706026077272, '_timestamp': 1740955550.191714}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 249 is less than current step: 499. Dropping entry: {'train/lr': 0.00021446938775510202, '_timestamp': 1740955550.19206}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 250 is less than current step: 499. Dropping entry: {'train/loss': 1.8130663514137269, 'train/ce_loss': 0.3671875, 'train/seg_cls_loss': 0.017974853515625, 'train/kl_loss': 0.3248046875, 'train/mask_bce_loss': 0.09364200150594115, 'train/mask_dice_loss': 0.6085942894220352, 'train/mask_loss': 0.7022362977266312, 'metrics/total_secs_per_batch': 6.800936460494995, 'metrics/data_secs_per_batch': 2.942637014389038, '_timestamp': 1740955556.9927442}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 250 is less than current step: 499. Dropping entry: {'train/lr': 0.00021434693877551018, '_timestamp': 1740955556.9930317}).
Epoch: [1][252/500]	Time  6.583 ( 6.583)	Loss 1.2109 (1.5974)	CeLoss 1.2109 (0.4924)	SegCLSLoss 0.0000 (0.0153)	KLLoss 0.0000 (0.2910)	MaskLoss 0.0000 (0.5341)	MaskBCELoss 0.0000 (0.1435)	MaskDICELoss 0.0000 (0.3907)
Epoch: [1][253/500]	Time  6.333 ( 6.333)	Loss 2.5050 (2.0698)	CeLoss 0.2090 (0.3643)	SegCLSLoss 0.0265 (0.0194)	KLLoss 0.4062 (0.3641)	MaskLoss 1.1206 (0.8296)	MaskBCELoss 0.1965 (0.1678)	MaskDICELoss 0.9241 (0.6618)
Epoch: [1][254/500]	Time  5.113 ( 5.113)	Loss 0.8242 (1.5232)	CeLoss 0.8242 (0.6503)	SegCLSLoss 0.0000 (0.0091)	KLLoss 0.0000 (0.2041)	MaskLoss 0.0000 (0.4240)	MaskBCELoss 0.0000 (0.0871)	MaskDICELoss 0.0000 (0.3369)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 251 is less than current step: 499. Dropping entry: {'train/loss': 1.597386085987091, 'train/ce_loss': 0.4923828125, 'train/seg_cls_loss': 0.015277099609375, 'train/kl_loss': 0.291015625, 'train/mask_bce_loss': 0.14347341135144234, 'train/mask_dice_loss': 0.3906688570976257, 'train/mask_loss': 0.5341422706842422, 'metrics/total_secs_per_batch': 6.582539081573486, 'metrics/data_secs_per_batch': 3.136106181144714, '_timestamp': 1740955563.5754006}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 251 is less than current step: 499. Dropping entry: {'train/lr': 0.00021422448979591834, '_timestamp': 1740955563.575747}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 252 is less than current step: 499. Dropping entry: {'train/loss': 2.069772493839264, 'train/ce_loss': 0.3642578125, 'train/seg_cls_loss': 0.019403076171875, 'train/kl_loss': 0.3640625, 'train/mask_bce_loss': 0.16784116439521313, 'train/mask_dice_loss': 0.661771634221077, 'train/mask_loss': 0.8296128034591674, 'metrics/total_secs_per_batch': 6.333249092102051, 'metrics/data_secs_per_batch': 2.9864994764328, '_timestamp': 1740955569.9085753}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 252 is less than current step: 499. Dropping entry: {'train/lr': 0.0002141020408163265, '_timestamp': 1740955569.908858}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 253 is less than current step: 499. Dropping entry: {'train/loss': 1.5232287883758544, 'train/ce_loss': 0.65029296875, 'train/seg_cls_loss': 0.009112548828125, 'train/kl_loss': 0.2041015625, 'train/mask_bce_loss': 0.08707458656281233, 'train/mask_dice_loss': 0.3369421511888504, 'train/mask_loss': 0.4240167438983917, 'metrics/total_secs_per_batch': 5.1134326457977295, 'metrics/data_secs_per_batch': 2.4441589593887327, '_timestamp': 1740955575.022089}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 253 is less than current step: 499. Dropping entry: {'train/lr': 0.00021397959183673467, '_timestamp': 1740955575.02241}).
Epoch: [1][255/500]	Time  6.755 ( 6.755)	Loss 1.9022 (1.8606)	CeLoss 0.2891 (0.3549)	SegCLSLoss 0.0153 (0.0219)	KLLoss 0.4160 (0.3219)	MaskLoss 0.7822 (0.7315)	MaskBCELoss 0.0644 (0.1151)	MaskDICELoss 0.7178 (0.6164)
Epoch: [1][256/500]	Time  6.712 ( 6.712)	Loss 2.3027 (1.3795)	CeLoss 0.1904 (0.4159)	SegCLSLoss 0.0222 (0.0117)	KLLoss 0.3867 (0.2469)	MaskLoss 1.0312 (0.4664)	MaskBCELoss 0.0450 (0.0723)	MaskDICELoss 0.9863 (0.3941)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 254 is less than current step: 499. Dropping entry: {'train/loss': 1.8605841636657714, 'train/ce_loss': 0.3548828125, 'train/seg_cls_loss': 0.021929931640625, 'train/kl_loss': 0.321875, 'train/mask_bce_loss': 0.11505373744294048, 'train/mask_dice_loss': 0.6164102256298065, 'train/mask_loss': 0.7314639568328858, 'metrics/total_secs_per_batch': 6.754965305328369, 'metrics/data_secs_per_batch': 2.6210097789764406, '_timestamp': 1740955581.7771213}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 254 is less than current step: 499. Dropping entry: {'train/lr': 0.00021385714285714283, '_timestamp': 1740955581.777444}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 255 is less than current step: 499. Dropping entry: {'train/loss': 1.379505705833435, 'train/ce_loss': 0.41591796875, 'train/seg_cls_loss': 0.011724853515625, 'train/kl_loss': 0.246875, 'train/mask_bce_loss': 0.07227797918021679, 'train/mask_dice_loss': 0.39408619701862335, 'train/mask_loss': 0.4663641780614853, 'metrics/total_secs_per_batch': 6.712263107299805, 'metrics/data_secs_per_batch': 2.9831369876861573, '_timestamp': 1740955588.4894686}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 255 is less than current step: 499. Dropping entry: {'train/lr': 0.000213734693877551, '_timestamp': 1740955588.4898343}).
Epoch: [1][257/500]	Time  6.730 ( 6.730)	Loss 1.2756 (1.5533)	CeLoss 0.2773 (0.2334)	SegCLSLoss 0.0172 (0.0189)	KLLoss 0.4141 (0.3199)	MaskLoss 0.4747 (0.6393)	MaskBCELoss 0.1809 (0.0799)	MaskDICELoss 0.2938 (0.5594)
Epoch: [1][258/500]	Time  5.614 ( 5.614)	Loss 1.1562 (1.3840)	CeLoss 1.1562 (0.5220)	SegCLSLoss 0.0000 (0.0118)	KLLoss 0.0000 (0.2053)	MaskLoss 0.0000 (0.4178)	MaskBCELoss 0.0000 (0.0373)	MaskDICELoss 0.0000 (0.3804)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 256 is less than current step: 499. Dropping entry: {'train/loss': 1.5533020913600921, 'train/ce_loss': 0.2333740234375, 'train/seg_cls_loss': 0.01893310546875, 'train/kl_loss': 0.319921875, 'train/mask_bce_loss': 0.07988453507423401, 'train/mask_dice_loss': 0.5593763872981071, 'train/mask_loss': 0.6392609238624573, 'metrics/total_secs_per_batch': 6.7300615310668945, 'metrics/data_secs_per_batch': 3.072562336921692, '_timestamp': 1740955595.2193666}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 256 is less than current step: 499. Dropping entry: {'train/lr': 0.00021361224489795916, '_timestamp': 1740955595.2196653}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 257 is less than current step: 499. Dropping entry: {'train/loss': 1.3839722871780396, 'train/ce_loss': 0.52197265625, 'train/seg_cls_loss': 0.011798095703125, 'train/kl_loss': 0.2052734375, 'train/mask_bce_loss': 0.03733815604355186, 'train/mask_dice_loss': 0.38042923510074617, 'train/mask_loss': 0.41776739358901976, 'metrics/total_secs_per_batch': 5.613656759262085, 'metrics/data_secs_per_batch': 2.7473706007003784, '_timestamp': 1740955600.8331923}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 257 is less than current step: 499. Dropping entry: {'train/lr': 0.00021348979591836732, '_timestamp': 1740955600.833563}).
Epoch: [1][259/500]	Time  7.816 ( 7.816)	Loss 0.8113 (1.7854)	CeLoss 0.1953 (0.2072)	SegCLSLoss 0.0162 (0.0236)	KLLoss 0.4062 (0.3580)	MaskLoss 0.2836 (0.7653)	MaskBCELoss 0.1604 (0.1563)	MaskDICELoss 0.1232 (0.6090)
[2025-03-02 16:46:55,670] [INFO] [logging.py:96:log_dist] [Rank 0] step=760, skipped=0, lr=[0.00021330612244897958], mom=[(0.9, 0.95)]
[2025-03-02 16:46:55,676] [INFO] [timer.py:215:stop] epoch=0/micro_step=7600/global_step=760, RunningAvgSamplesPerSec=1.5027051322814455, CurrSamplesPerSec=1.4231566223713845, MemAllocated=31.24GB, MaxMemAllocated=36.81GB
Epoch: [1][260/500]	Time  7.028 ( 7.028)	Loss 3.1940 (1.8546)	CeLoss 0.3242 (0.3513)	SegCLSLoss 0.0264 (0.0144)	KLLoss 0.3809 (0.3217)	MaskLoss 1.4095 (0.7320)	MaskBCELoss 0.5308 (0.1768)	MaskDICELoss 0.8787 (0.5552)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 258 is less than current step: 499. Dropping entry: {'train/loss': 1.785433715581894, 'train/ce_loss': 0.207177734375, 'train/seg_cls_loss': 0.02364501953125, 'train/kl_loss': 0.3580078125, 'train/mask_bce_loss': 0.1562900581397116, 'train/mask_dice_loss': 0.6090098157525062, 'train/mask_loss': 0.765299865603447, 'metrics/total_secs_per_batch': 7.815747261047363, 'metrics/data_secs_per_batch': 3.7542110443115235, '_timestamp': 1740955608.6487436}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 258 is less than current step: 499. Dropping entry: {'train/lr': 0.00021336734693877548, '_timestamp': 1740955608.6489508}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 259 is less than current step: 499. Dropping entry: {'train/loss': 1.8546328663825988, 'train/ce_loss': 0.35126953125, 'train/seg_cls_loss': 0.014422607421875, 'train/kl_loss': 0.3216796875, 'train/mask_bce_loss': 0.17677576132118702, 'train/mask_dice_loss': 0.5552281528711319, 'train/mask_loss': 0.7320039212703705, 'metrics/total_secs_per_batch': 7.028321027755737, 'metrics/data_secs_per_batch': 3.233931374549866, '_timestamp': 1740955615.677146}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 259 is less than current step: 499. Dropping entry: {'train/lr': 0.00021324489795918365, '_timestamp': 1740955615.6776059}).
Epoch: [1][261/500]	Time  6.856 ( 6.856)	Loss 2.6202 (2.1007)	CeLoss 0.1982 (0.3708)	SegCLSLoss 0.0283 (0.0206)	KLLoss 0.3867 (0.3166)	MaskLoss 1.1841 (0.8438)	MaskBCELoss 0.3155 (0.2577)	MaskDICELoss 0.8687 (0.5861)
Epoch: [1][262/500]	Time  6.101 ( 6.101)	Loss 2.5477 (1.9092)	CeLoss 0.1904 (0.3976)	SegCLSLoss 0.0344 (0.0145)	KLLoss 0.3770 (0.2783)	MaskLoss 1.1508 (0.7383)	MaskBCELoss 0.3246 (0.2365)	MaskDICELoss 0.8262 (0.5018)
Epoch: [1][263/500]	Time  6.000 ( 6.000)	Loss 2.5516 (1.9228)	CeLoss 0.2969 (0.4763)	SegCLSLoss 0.0154 (0.0128)	KLLoss 0.4004 (0.2805)	MaskLoss 1.1039 (0.7061)	MaskBCELoss 0.3297 (0.2339)	MaskDICELoss 0.7742 (0.4722)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 260 is less than current step: 499. Dropping entry: {'train/loss': 2.1007102727890015, 'train/ce_loss': 0.37080078125, 'train/seg_cls_loss': 0.020599365234375, 'train/kl_loss': 0.3166015625, 'train/mask_bce_loss': 0.2577233139425516, 'train/mask_dice_loss': 0.5860888600349426, 'train/mask_loss': 0.8438121795654296, 'metrics/total_secs_per_batch': 6.856329917907715, 'metrics/data_secs_per_batch': 3.0068243503570558, '_timestamp': 1740955622.5334713}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 260 is less than current step: 499. Dropping entry: {'train/lr': 0.0002131224489795918, '_timestamp': 1740955622.5338147}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 261 is less than current step: 499. Dropping entry: {'train/loss': 1.9092459559440613, 'train/ce_loss': 0.39755859375, 'train/seg_cls_loss': 0.0145263671875, 'train/kl_loss': 0.2783203125, 'train/mask_bce_loss': 0.23650443255901338, 'train/mask_dice_loss': 0.5018099397420883, 'train/mask_loss': 0.7383143782615662, 'metrics/total_secs_per_batch': 6.101139545440674, 'metrics/data_secs_per_batch': 2.519353151321411, '_timestamp': 1740955628.6348004}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 261 is less than current step: 499. Dropping entry: {'train/lr': 0.00021299999999999997, '_timestamp': 1740955628.6351748}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 262 is less than current step: 499. Dropping entry: {'train/loss': 1.922849678993225, 'train/ce_loss': 0.4762939453125, 'train/seg_cls_loss': 0.012750244140625, 'train/kl_loss': 0.28046875, 'train/mask_bce_loss': 0.23385738097131253, 'train/mask_dice_loss': 0.4722329765558243, 'train/mask_loss': 0.7060903489589692, 'metrics/total_secs_per_batch': 6.000483989715576, 'metrics/data_secs_per_batch': 2.733621430397034, '_timestamp': 1740955634.6360114}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 262 is less than current step: 499. Dropping entry: {'train/lr': 0.00021287755102040814, '_timestamp': 1740955634.6365318}).
Epoch: [1][264/500]	Time  5.925 ( 5.925)	Loss 2.4698 (1.9083)	CeLoss 0.2041 (0.3833)	SegCLSLoss 0.0254 (0.0207)	KLLoss 0.3770 (0.3129)	MaskLoss 1.1080 (0.7416)	MaskBCELoss 0.1089 (0.1473)	MaskDICELoss 0.9990 (0.5944)
Epoch: [1][265/500]	Time  5.802 ( 5.802)	Loss 1.6641 (1.8174)	CeLoss 1.6641 (0.5441)	SegCLSLoss 0.0000 (0.0131)	KLLoss 0.0000 (0.2785)	MaskLoss 0.0000 (0.6195)	MaskBCELoss 0.0000 (0.1409)	MaskDICELoss 0.0000 (0.4786)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 263 is less than current step: 499. Dropping entry: {'train/loss': 1.9082887768745422, 'train/ce_loss': 0.38330078125, 'train/seg_cls_loss': 0.020721435546875, 'train/kl_loss': 0.312890625, 'train/mask_bce_loss': 0.14725073240697384, 'train/mask_dice_loss': 0.5943936809897423, 'train/mask_loss': 0.741644412279129, 'metrics/total_secs_per_batch': 5.925177574157715, 'metrics/data_secs_per_batch': 2.218021273612976, '_timestamp': 1740955640.5602562}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 263 is less than current step: 499. Dropping entry: {'train/lr': 0.0002127551020408163, '_timestamp': 1740955640.56057}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 264 is less than current step: 499. Dropping entry: {'train/loss': 1.8174106240272523, 'train/ce_loss': 0.544140625, 'train/seg_cls_loss': 0.013140869140625, 'train/kl_loss': 0.278515625, 'train/mask_bce_loss': 0.14091766402125358, 'train/mask_dice_loss': 0.47862749695777895, 'train/mask_loss': 0.6195451736450195, 'metrics/total_secs_per_batch': 5.80234432220459, 'metrics/data_secs_per_batch': 2.7119011640548707, '_timestamp': 1740955646.3629296}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 264 is less than current step: 499. Dropping entry: {'train/lr': 0.00021263265306122446, '_timestamp': 1740955646.3633335}).
Epoch: [1][266/500]	Time  5.676 ( 5.676)	Loss 0.9180 (1.4103)	CeLoss 0.9180 (0.4728)	SegCLSLoss 0.0000 (0.0091)	KLLoss 0.0000 (0.1984)	MaskLoss 0.0000 (0.4566)	MaskBCELoss 0.0000 (0.1551)	MaskDICELoss 0.0000 (0.3015)
Epoch: [1][267/500]	Time  6.658 ( 6.658)	Loss 3.2376 (2.0585)	CeLoss 0.2197 (0.3664)	SegCLSLoss 0.0311 (0.0192)	KLLoss 0.3848 (0.3537)	MaskLoss 1.4821 (0.8238)	MaskBCELoss 0.7910 (0.2610)	MaskDICELoss 0.6911 (0.5628)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 265 is less than current step: 499. Dropping entry: {'train/loss': 1.410334324836731, 'train/ce_loss': 0.4728271484375, 'train/seg_cls_loss': 0.009051513671875, 'train/kl_loss': 0.1984375, 'train/mask_bce_loss': 0.15510092228651046, 'train/mask_dice_loss': 0.30154328644275663, 'train/mask_loss': 0.45664421916007997, 'metrics/total_secs_per_batch': 5.676406145095825, 'metrics/data_secs_per_batch': 2.5998831272125242, '_timestamp': 1740955652.0393615}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 265 is less than current step: 499. Dropping entry: {'train/lr': 0.00021251020408163263, '_timestamp': 1740955652.039809}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 266 is less than current step: 499. Dropping entry: {'train/loss': 2.058538591861725, 'train/ce_loss': 0.36640625, 'train/seg_cls_loss': 0.019207763671875, 'train/kl_loss': 0.3537109375, 'train/mask_bce_loss': 0.2609961085021496, 'train/mask_dice_loss': 0.5627556085586548, 'train/mask_loss': 0.8237517103552818, 'metrics/total_secs_per_batch': 6.657941102981567, 'metrics/data_secs_per_batch': 2.8981827020645143, '_timestamp': 1740955658.6972513}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 266 is less than current step: 499. Dropping entry: {'train/lr': 0.00021238775510204082, '_timestamp': 1740955658.6976373}).
Epoch: [1][268/500]	Time  6.799 ( 6.799)	Loss 2.7505 (1.9358)	CeLoss 0.2383 (0.4926)	SegCLSLoss 0.0145 (0.0151)	KLLoss 0.4023 (0.3180)	MaskLoss 1.2327 (0.7020)	MaskBCELoss 0.3336 (0.1235)	MaskDICELoss 0.8990 (0.5785)
Epoch: [1][269/500]	Time  6.869 ( 6.869)	Loss 1.6250 (2.0434)	CeLoss 1.6250 (0.4916)	SegCLSLoss 0.0000 (0.0175)	KLLoss 0.0000 (0.2701)	MaskLoss 0.0000 (0.7581)	MaskBCELoss 0.0000 (0.1488)	MaskDICELoss 0.0000 (0.6092)
[2025-03-02 16:47:59,134] [INFO] [logging.py:96:log_dist] [Rank 0] step=770, skipped=0, lr=[0.0002120816326530612], mom=[(0.9, 0.95)]
[2025-03-02 16:47:59,139] [INFO] [timer.py:215:stop] epoch=0/micro_step=7700/global_step=770, RunningAvgSamplesPerSec=1.503617227589879, CurrSamplesPerSec=1.4762981392212968, MemAllocated=31.25GB, MaxMemAllocated=36.81GB
Epoch: [1][270/500]	Time  6.776 ( 6.776)	Loss 2.2811 (1.4773)	CeLoss 0.1914 (0.3325)	SegCLSLoss 0.0225 (0.0171)	KLLoss 0.3887 (0.2729)	MaskLoss 1.0194 (0.5545)	MaskBCELoss 0.0560 (0.0745)	MaskDICELoss 0.9634 (0.4799)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 267 is less than current step: 499. Dropping entry: {'train/loss': 1.9358417093753815, 'train/ce_loss': 0.492578125, 'train/seg_cls_loss': 0.015093994140625, 'train/kl_loss': 0.31796875, 'train/mask_bce_loss': 0.12348311925306917, 'train/mask_dice_loss': 0.5785197466611862, 'train/mask_loss': 0.7020028740167618, 'metrics/total_secs_per_batch': 6.798775911331177, 'metrics/data_secs_per_batch': 3.1909059047698975, '_timestamp': 1740955665.4957356}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 267 is less than current step: 499. Dropping entry: {'train/lr': 0.00021226530612244898, '_timestamp': 1740955665.496055}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 268 is less than current step: 499. Dropping entry: {'train/loss': 2.043353724479675, 'train/ce_loss': 0.4916015625, 'train/seg_cls_loss': 0.01748046875, 'train/kl_loss': 0.2701171875, 'train/mask_bce_loss': 0.14881615713238716, 'train/mask_dice_loss': 0.6092376291751862, 'train/mask_loss': 0.7580537974834443, 'metrics/total_secs_per_batch': 6.868634223937988, 'metrics/data_secs_per_batch': 3.354132890701294, '_timestamp': 1740955672.3645995}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 268 is less than current step: 499. Dropping entry: {'train/lr': 0.00021214285714285714, '_timestamp': 1740955672.364976}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 269 is less than current step: 499. Dropping entry: {'train/loss': 1.4773298859596253, 'train/ce_loss': 0.33251953125, 'train/seg_cls_loss': 0.01708984375, 'train/kl_loss': 0.2728515625, 'train/mask_bce_loss': 0.07454052306711674, 'train/mask_dice_loss': 0.4799447387456894, 'train/mask_loss': 0.5544852495193482, 'metrics/total_secs_per_batch': 6.775647401809692, 'metrics/data_secs_per_batch': 3.2562879085540772, '_timestamp': 1740955679.1400328}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 269 is less than current step: 499. Dropping entry: {'train/lr': 0.0002120204081632653, '_timestamp': 1740955679.1404011}).
Epoch: [1][271/500]	Time  6.309 ( 6.309)	Loss 1.8003 (1.6706)	CeLoss 0.2949 (0.3237)	SegCLSLoss 0.0154 (0.0171)	KLLoss 0.3965 (0.3133)	MaskLoss 0.7293 (0.6536)	MaskBCELoss 0.0626 (0.0706)	MaskDICELoss 0.6667 (0.5830)
Epoch: [1][272/500]	Time  7.367 ( 7.367)	Loss 1.4388 (1.8614)	CeLoss 0.1777 (0.2591)	SegCLSLoss 0.0317 (0.0193)	KLLoss 0.3730 (0.3504)	MaskLoss 0.6042 (0.7787)	MaskBCELoss 0.0150 (0.1262)	MaskDICELoss 0.5892 (0.6525)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 270 is less than current step: 499. Dropping entry: {'train/loss': 1.6706419944763184, 'train/ce_loss': 0.32373046875, 'train/seg_cls_loss': 0.01708984375, 'train/kl_loss': 0.31328125, 'train/mask_bce_loss': 0.07061600852757692, 'train/mask_dice_loss': 0.5829667121171951, 'train/mask_loss': 0.6535827219486237, 'metrics/total_secs_per_batch': 6.308878183364868, 'metrics/data_secs_per_batch': 2.4434499502182008, '_timestamp': 1740955685.4489794}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 270 is less than current step: 499. Dropping entry: {'train/lr': 0.00021189795918367347, '_timestamp': 1740955685.4492943}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 271 is less than current step: 499. Dropping entry: {'train/loss': 1.8614239692687988, 'train/ce_loss': 0.25908203125, 'train/seg_cls_loss': 0.01934814453125, 'train/kl_loss': 0.350390625, 'train/mask_bce_loss': 0.1261652881745249, 'train/mask_dice_loss': 0.6525447458028794, 'train/mask_loss': 0.7787100315093994, 'metrics/total_secs_per_batch': 7.366686582565308, 'metrics/data_secs_per_batch': 3.292112445831299, '_timestamp': 1740955692.8158484}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 271 is less than current step: 499. Dropping entry: {'train/lr': 0.00021177551020408163, '_timestamp': 1740955692.816213}).
Epoch: [1][273/500]	Time  5.786 ( 5.786)	Loss 2.0810 (1.7601)	CeLoss 0.1992 (0.4447)	SegCLSLoss 0.0344 (0.0163)	KLLoss 0.3848 (0.2736)	MaskLoss 0.9126 (0.6400)	MaskBCELoss 0.2295 (0.1296)	MaskDICELoss 0.6830 (0.5104)
Epoch: [1][274/500]	Time  4.878 ( 4.878)	Loss 2.5543 (2.0776)	CeLoss 0.2256 (0.4394)	SegCLSLoss 0.0167 (0.0231)	KLLoss 0.3965 (0.3137)	MaskLoss 1.1404 (0.7976)	MaskBCELoss 0.3441 (0.1855)	MaskDICELoss 0.7964 (0.6121)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 272 is less than current step: 499. Dropping entry: {'train/loss': 1.7600607037544251, 'train/ce_loss': 0.4447265625, 'train/seg_cls_loss': 0.016278076171875, 'train/kl_loss': 0.2736328125, 'train/mask_bce_loss': 0.12963706739246844, 'train/mask_dice_loss': 0.5103542119264602, 'train/mask_loss': 0.6399912774562836, 'metrics/total_secs_per_batch': 5.7863523960113525, 'metrics/data_secs_per_batch': 2.8194088459014894, '_timestamp': 1740955698.601989}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 272 is less than current step: 499. Dropping entry: {'train/lr': 0.0002116530612244898, '_timestamp': 1740955698.6022844}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 273 is less than current step: 499. Dropping entry: {'train/loss': 2.077578091621399, 'train/ce_loss': 0.43935546875, 'train/seg_cls_loss': 0.02314453125, 'train/kl_loss': 0.313671875, 'train/mask_bce_loss': 0.18552144467830659, 'train/mask_dice_loss': 0.6121054828166962, 'train/mask_loss': 0.7976269364356995, 'metrics/total_secs_per_batch': 4.877666473388672, 'metrics/data_secs_per_batch': 2.133335018157959, '_timestamp': 1740955703.4798844}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 273 is less than current step: 499. Dropping entry: {'train/lr': 0.00021153061224489796, '_timestamp': 1740955703.480261}).
Epoch: [1][275/500]	Time  7.479 ( 7.479)	Loss 1.3557 (2.2445)	CeLoss 0.2334 (0.2113)	SegCLSLoss 0.0167 (0.0243)	KLLoss 0.4121 (0.3936)	MaskLoss 0.5363 (0.9907)	MaskBCELoss 0.0960 (0.2712)	MaskDICELoss 0.4403 (0.7195)
Epoch: [1][276/500]	Time  5.292 ( 5.292)	Loss 0.0688 (1.5582)	CeLoss 0.0688 (0.6648)	SegCLSLoss 0.0000 (0.0101)	KLLoss 0.0000 (0.2014)	MaskLoss 0.0000 (0.4341)	MaskBCELoss 0.0000 (0.0493)	MaskDICELoss 0.0000 (0.3848)
Epoch: [1][277/500]	Time  5.608 ( 5.608)	Loss 1.0312 (1.4688)	CeLoss 1.0312 (0.5996)	SegCLSLoss 0.0000 (0.0113)	KLLoss 0.0000 (0.1977)	MaskLoss 0.0000 (0.4218)	MaskBCELoss 0.0000 (0.1183)	MaskDICELoss 0.0000 (0.3035)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 274 is less than current step: 499. Dropping entry: {'train/loss': 2.244457173347473, 'train/ce_loss': 0.211328125, 'train/seg_cls_loss': 0.02425537109375, 'train/kl_loss': 0.3935546875, 'train/mask_bce_loss': 0.2712458634749055, 'train/mask_dice_loss': 0.719488587975502, 'train/mask_loss': 0.9907344460487366, 'metrics/total_secs_per_batch': 7.478678226470947, 'metrics/data_secs_per_batch': 3.1881744861602783, '_timestamp': 1740955710.958369}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 274 is less than current step: 499. Dropping entry: {'train/lr': 0.00021140816326530612, '_timestamp': 1740955710.9586673}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 275 is less than current step: 499. Dropping entry: {'train/loss': 1.558150041103363, 'train/ce_loss': 0.664794921875, 'train/seg_cls_loss': 0.01009521484375, 'train/kl_loss': 0.2013671875, 'train/mask_bce_loss': 0.04931057412177324, 'train/mask_dice_loss': 0.38481815457344054, 'train/mask_loss': 0.4341287314891815, 'metrics/total_secs_per_batch': 5.291943073272705, 'metrics/data_secs_per_batch': 2.4132567644119263, '_timestamp': 1740955716.25056}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 275 is less than current step: 499. Dropping entry: {'train/lr': 0.00021128571428571426, '_timestamp': 1740955716.2509053}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 276 is less than current step: 499. Dropping entry: {'train/loss': 1.4687974810600282, 'train/ce_loss': 0.5995849609375, 'train/seg_cls_loss': 0.0113037109375, 'train/kl_loss': 0.19765625, 'train/mask_bce_loss': 0.11831550002098083, 'train/mask_dice_loss': 0.3034977912902832, 'train/mask_loss': 0.4218132883310318, 'metrics/total_secs_per_batch': 5.607535362243652, 'metrics/data_secs_per_batch': 2.898369646072388, '_timestamp': 1740955721.8578796}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 276 is less than current step: 499. Dropping entry: {'train/lr': 0.00021116326530612242, '_timestamp': 1740955721.8581827}).
Epoch: [1][278/500]	Time  5.762 ( 5.762)	Loss 1.0000 (1.5060)	CeLoss 1.0000 (0.3656)	SegCLSLoss 0.0000 (0.0153)	KLLoss 0.0000 (0.2379)	MaskLoss 0.0000 (0.5545)	MaskBCELoss 0.0000 (0.1419)	MaskDICELoss 0.0000 (0.4125)
Epoch: [1][279/500]	Time  5.708 ( 5.708)	Loss 0.0791 (1.7504)	CeLoss 0.0791 (0.6222)	SegCLSLoss 0.0000 (0.0113)	KLLoss 0.0000 (0.2426)	MaskLoss 0.0000 (0.5492)	MaskBCELoss 0.0000 (0.0936)	MaskDICELoss 0.0000 (0.4555)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 277 is less than current step: 499. Dropping entry: {'train/loss': 1.5060324907302856, 'train/ce_loss': 0.3656494140625, 'train/seg_cls_loss': 0.015252685546875, 'train/kl_loss': 0.237890625, 'train/mask_bce_loss': 0.141934148222208, 'train/mask_dice_loss': 0.41253472566604615, 'train/mask_loss': 0.5544688820838928, 'metrics/total_secs_per_batch': 5.76195764541626, 'metrics/data_secs_per_batch': 2.740034890174866, '_timestamp': 1740955727.6198497}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 277 is less than current step: 499. Dropping entry: {'train/lr': 0.00021104081632653058, '_timestamp': 1740955727.6200664}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 278 is less than current step: 499. Dropping entry: {'train/loss': 1.7503579139709473, 'train/ce_loss': 0.62216796875, 'train/seg_cls_loss': 0.011273193359375, 'train/kl_loss': 0.242578125, 'train/mask_bce_loss': 0.09363497849553823, 'train/mask_dice_loss': 0.4555185973644257, 'train/mask_loss': 0.5491535663604736, 'metrics/total_secs_per_batch': 5.70792818069458, 'metrics/data_secs_per_batch': 2.7071253776550295, '_timestamp': 1740955733.3279788}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 278 is less than current step: 499. Dropping entry: {'train/lr': 0.00021091836734693875, '_timestamp': 1740955733.3283527}).
[2025-03-02 16:49:00,441] [INFO] [logging.py:96:log_dist] [Rank 0] step=780, skipped=0, lr=[0.00021085714285714284], mom=[(0.9, 0.95)]
[2025-03-02 16:49:00,446] [INFO] [timer.py:215:stop] epoch=0/micro_step=7800/global_step=780, RunningAvgSamplesPerSec=1.5051336914244835, CurrSamplesPerSec=1.4049727271146817, MemAllocated=31.25GB, MaxMemAllocated=36.81GB
Epoch: [1][280/500]	Time  7.120 ( 7.120)	Loss 1.7451 (1.5554)	CeLoss 0.2500 (0.2900)	SegCLSLoss 0.0153 (0.0165)	KLLoss 0.3945 (0.3176)	MaskLoss 0.7241 (0.6125)	MaskBCELoss 0.1162 (0.0692)	MaskDICELoss 0.6079 (0.5433)
Epoch: [1][281/500]	Time  6.395 ( 6.395)	Loss 2.1365 (1.8230)	CeLoss 0.2090 (0.4504)	SegCLSLoss 0.0273 (0.0162)	KLLoss 0.3730 (0.2717)	MaskLoss 0.9384 (0.6685)	MaskBCELoss 0.0200 (0.1340)	MaskDICELoss 0.9183 (0.5345)
Epoch: [1][282/500]	Time  5.695 ( 5.695)	Loss 9.0992 (2.3575)	CeLoss 0.1699 (0.4713)	SegCLSLoss 0.0520 (0.0175)	KLLoss 0.3906 (0.2322)	MaskLoss 4.4319 (0.9271)	MaskBCELoss 3.6814 (0.5062)	MaskDICELoss 0.7505 (0.4209)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 279 is less than current step: 499. Dropping entry: {'train/loss': 1.5553514242172242, 'train/ce_loss': 0.2900390625, 'train/seg_cls_loss': 0.016485595703125, 'train/kl_loss': 0.317578125, 'train/mask_bce_loss': 0.06923823207616805, 'train/mask_dice_loss': 0.5433007538318634, 'train/mask_loss': 0.6125389829277992, 'metrics/total_secs_per_batch': 7.11958646774292, 'metrics/data_secs_per_batch': 3.5000113964080812, '_timestamp': 1740955740.4472163}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 279 is less than current step: 499. Dropping entry: {'train/lr': 0.0002107959183673469, '_timestamp': 1740955740.447555}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 280 is less than current step: 499. Dropping entry: {'train/loss': 1.8230491995811462, 'train/ce_loss': 0.450439453125, 'train/seg_cls_loss': 0.0161865234375, 'train/kl_loss': 0.2716796875, 'train/mask_bce_loss': 0.13403601050376893, 'train/mask_dice_loss': 0.5344954162836075, 'train/mask_loss': 0.6685314238071441, 'metrics/total_secs_per_batch': 6.3951356410980225, 'metrics/data_secs_per_batch': 3.2134581089019774, '_timestamp': 1740955746.8424983}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 280 is less than current step: 499. Dropping entry: {'train/lr': 0.00021067346938775507, '_timestamp': 1740955746.8428006}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 281 is less than current step: 499. Dropping entry: {'train/loss': 2.3575210332870484, 'train/ce_loss': 0.4712890625, 'train/seg_cls_loss': 0.017523193359375, 'train/kl_loss': 0.2322265625, 'train/mask_bce_loss': 0.5062391996383667, 'train/mask_dice_loss': 0.42086113691329957, 'train/mask_loss': 0.9271003484725953, 'metrics/total_secs_per_batch': 5.695244550704956, 'metrics/data_secs_per_batch': 2.796646809577942, '_timestamp': 1740955752.537952}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 281 is less than current step: 499. Dropping entry: {'train/lr': 0.00021055102040816323, '_timestamp': 1740955752.538318}).
Epoch: [1][283/500]	Time  7.035 ( 7.035)	Loss 2.2110 (1.9684)	CeLoss 0.2051 (0.3792)	SegCLSLoss 0.0262 (0.0187)	KLLoss 0.3711 (0.3072)	MaskLoss 0.9786 (0.7746)	MaskBCELoss 0.1128 (0.1810)	MaskDICELoss 0.8658 (0.5936)
Epoch: [1][284/500]	Time  7.643 ( 7.643)	Loss 2.2888 (1.9447)	CeLoss 0.1963 (0.2389)	SegCLSLoss 0.0234 (0.0225)	KLLoss 0.3965 (0.3934)	MaskLoss 1.0204 (0.8277)	MaskBCELoss 0.2117 (0.1664)	MaskDICELoss 0.8086 (0.6613)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 282 is less than current step: 499. Dropping entry: {'train/loss': 1.9683965682983398, 'train/ce_loss': 0.37919921875, 'train/seg_cls_loss': 0.018695068359375, 'train/kl_loss': 0.3072265625, 'train/mask_bce_loss': 0.18099445402622222, 'train/mask_dice_loss': 0.5936334937810898, 'train/mask_loss': 0.77462797164917, 'metrics/total_secs_per_batch': 7.034778118133545, 'metrics/data_secs_per_batch': 3.3805176258087157, '_timestamp': 1740955759.572478}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 282 is less than current step: 499. Dropping entry: {'train/lr': 0.0002104285714285714, '_timestamp': 1740955759.5728385}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 283 is less than current step: 499. Dropping entry: {'train/loss': 1.9446618556976318, 'train/ce_loss': 0.2388671875, 'train/seg_cls_loss': 0.0224609375, 'train/kl_loss': 0.393359375, 'train/mask_bce_loss': 0.166360891982913, 'train/mask_dice_loss': 0.6613411270081997, 'train/mask_loss': 0.8277020215988159, 'metrics/total_secs_per_batch': 7.643013000488281, 'metrics/data_secs_per_batch': 3.4772249937057493, '_timestamp': 1740955767.215795}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 283 is less than current step: 499. Dropping entry: {'train/lr': 0.00021030612244897956, '_timestamp': 1740955767.2161746}).
Epoch: [1][285/500]	Time  6.210 ( 6.210)	Loss 2.0599 (2.2062)	CeLoss 0.2236 (0.2019)	SegCLSLoss 0.0170 (0.0228)	KLLoss 0.3965 (0.3877)	MaskLoss 0.8942 (0.9772)	MaskBCELoss 0.1843 (0.1906)	MaskDICELoss 0.7099 (0.7867)
Epoch: [1][286/500]	Time  6.455 ( 6.455)	Loss 2.3650 (2.1126)	CeLoss 0.1289 (0.3505)	SegCLSLoss 0.0540 (0.0208)	KLLoss 0.3672 (0.3484)	MaskLoss 1.0858 (0.8584)	MaskBCELoss 0.1897 (0.2249)	MaskDICELoss 0.8962 (0.6335)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 284 is less than current step: 499. Dropping entry: {'train/loss': 2.2062265634536744, 'train/ce_loss': 0.20185546875, 'train/seg_cls_loss': 0.0228271484375, 'train/kl_loss': 0.3876953125, 'train/mask_bce_loss': 0.19058096325024962, 'train/mask_dice_loss': 0.786653408408165, 'train/mask_loss': 0.9772343635559082, 'metrics/total_secs_per_batch': 6.209694147109985, 'metrics/data_secs_per_batch': 3.01590256690979, '_timestamp': 1740955773.425233}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 284 is less than current step: 499. Dropping entry: {'train/lr': 0.00021018367346938772, '_timestamp': 1740955773.4255388}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 285 is less than current step: 499. Dropping entry: {'train/loss': 2.112601613998413, 'train/ce_loss': 0.35048828125, 'train/seg_cls_loss': 0.020751953125, 'train/kl_loss': 0.3484375, 'train/mask_bce_loss': 0.2248614564538002, 'train/mask_dice_loss': 0.6334901243448258, 'train/mask_loss': 0.8583515882492065, 'metrics/total_secs_per_batch': 6.454815149307251, 'metrics/data_secs_per_batch': 2.833781909942627, '_timestamp': 1740955779.8802626}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 285 is less than current step: 499. Dropping entry: {'train/lr': 0.0002100612244897959, '_timestamp': 1740955779.880629}).
Epoch: [1][287/500]	Time  6.207 ( 6.207)	Loss 1.2926 (1.8286)	CeLoss 0.2891 (0.2954)	SegCLSLoss 0.0150 (0.0198)	KLLoss 0.3867 (0.3486)	MaskLoss 0.4784 (0.7443)	MaskBCELoss 0.0943 (0.1383)	MaskDICELoss 0.3840 (0.6060)
Epoch: [1][288/500]	Time  6.482 ( 6.482)	Loss 2.1870 (1.9203)	CeLoss 0.2324 (0.3213)	SegCLSLoss 0.0208 (0.0182)	KLLoss 0.3789 (0.3035)	MaskLoss 0.9529 (0.7798)	MaskBCELoss 0.0394 (0.1856)	MaskDICELoss 0.9134 (0.5942)
Epoch: [1][289/500]	Time  5.629 ( 5.629)	Loss 2.2046 (1.7608)	CeLoss 0.2051 (0.4966)	SegCLSLoss 0.0283 (0.0209)	KLLoss 0.3945 (0.2691)	MaskLoss 0.9734 (0.6134)	MaskBCELoss 0.0380 (0.0792)	MaskDICELoss 0.9354 (0.5342)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 286 is less than current step: 499. Dropping entry: {'train/loss': 1.8286416292190553, 'train/ce_loss': 0.29541015625, 'train/seg_cls_loss': 0.019818115234375, 'train/kl_loss': 0.3486328125, 'train/mask_bce_loss': 0.1382977284491062, 'train/mask_dice_loss': 0.605954709649086, 'train/mask_loss': 0.7442524552345275, 'metrics/total_secs_per_batch': 6.207073211669922, 'metrics/data_secs_per_batch': 2.5221439599990845, '_timestamp': 1740955786.087112}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 286 is less than current step: 499. Dropping entry: {'train/lr': 0.00020993877551020405, '_timestamp': 1740955786.0874057}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 287 is less than current step: 499. Dropping entry: {'train/loss': 1.9203487515449524, 'train/ce_loss': 0.3212890625, 'train/seg_cls_loss': 0.01815185546875, 'train/kl_loss': 0.303515625, 'train/mask_bce_loss': 0.18562302328646182, 'train/mask_dice_loss': 0.5941802397370338, 'train/mask_loss': 0.7798032641410828, 'metrics/total_secs_per_batch': 6.482331037521362, 'metrics/data_secs_per_batch': 3.0050286531448362, '_timestamp': 1740955792.569672}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 287 is less than current step: 499. Dropping entry: {'train/lr': 0.0002098163265306122, '_timestamp': 1740955792.5700336}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 288 is less than current step: 499. Dropping entry: {'train/loss': 1.7607914090156556, 'train/ce_loss': 0.496630859375, 'train/seg_cls_loss': 0.020941162109375, 'train/kl_loss': 0.269140625, 'train/mask_bce_loss': 0.07916147476062178, 'train/mask_dice_loss': 0.5341931998729705, 'train/mask_loss': 0.6133546739816665, 'metrics/total_secs_per_batch': 5.628963470458984, 'metrics/data_secs_per_batch': 2.5958994388580323, '_timestamp': 1740955798.1984622}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 288 is less than current step: 499. Dropping entry: {'train/lr': 0.0002096938775510204, '_timestamp': 1740955798.1987598}).
[2025-03-02 16:50:06,327] [INFO] [logging.py:96:log_dist] [Rank 0] step=790, skipped=0, lr=[0.0002096326530612245], mom=[(0.9, 0.95)]
[2025-03-02 16:50:06,332] [INFO] [timer.py:215:stop] epoch=0/micro_step=7900/global_step=790, RunningAvgSamplesPerSec=1.5052966739171307, CurrSamplesPerSec=1.2294797798505448, MemAllocated=31.24GB, MaxMemAllocated=36.81GB
Epoch: [1][290/500]	Time  8.135 ( 8.135)	Loss 2.3698 (2.0705)	CeLoss 0.2490 (0.2308)	SegCLSLoss 0.0216 (0.0230)	KLLoss 0.3711 (0.3846)	MaskLoss 1.0365 (0.8948)	MaskBCELoss 0.0568 (0.1587)	MaskDICELoss 0.9796 (0.7361)
Epoch: [1][291/500]	Time  6.697 ( 6.697)	Loss 2.4241 (1.8781)	CeLoss 0.1562 (0.2318)	SegCLSLoss 0.0483 (0.0239)	KLLoss 0.3711 (0.3441)	MaskLoss 1.1031 (0.7999)	MaskBCELoss 0.1136 (0.1499)	MaskDICELoss 0.9896 (0.6500)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 289 is less than current step: 499. Dropping entry: {'train/loss': 2.07052104473114, 'train/ce_loss': 0.23076171875, 'train/seg_cls_loss': 0.022979736328125, 'train/kl_loss': 0.3845703125, 'train/mask_bce_loss': 0.15869494751095772, 'train/mask_dice_loss': 0.7361359089612961, 'train/mask_loss': 0.894830858707428, 'metrics/total_secs_per_batch': 8.13523244857788, 'metrics/data_secs_per_batch': 3.7121415615081785, '_timestamp': 1740955806.333632}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 289 is less than current step: 499. Dropping entry: {'train/lr': 0.00020957142857142857, '_timestamp': 1740955806.334029}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 290 is less than current step: 499. Dropping entry: {'train/loss': 1.8781312823295593, 'train/ce_loss': 0.2318359375, 'train/seg_cls_loss': 0.02388916015625, 'train/kl_loss': 0.344140625, 'train/mask_bce_loss': 0.14990579411387445, 'train/mask_dice_loss': 0.6499996811151505, 'train/mask_loss': 0.7999054729938507, 'metrics/total_secs_per_batch': 6.697324275970459, 'metrics/data_secs_per_batch': 3.209310293197632, '_timestamp': 1740955813.030998}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 290 is less than current step: 499. Dropping entry: {'train/lr': 0.00020944897959183673, '_timestamp': 1740955813.0312786}).
Epoch: [1][292/500]	Time  4.950 ( 4.950)	Loss 1.2812 (1.6816)	CeLoss 1.2812 (0.5887)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.2295)	MaskLoss 0.0000 (0.5315)	MaskBCELoss 0.0000 (0.0758)	MaskDICELoss 0.0000 (0.4557)
Epoch: [1][293/500]	Time  7.231 ( 7.231)	Loss 2.4001 (1.6031)	CeLoss 0.1309 (0.2252)	SegCLSLoss 0.0554 (0.0187)	KLLoss 0.3652 (0.3051)	MaskLoss 1.1024 (0.6690)	MaskBCELoss 0.1814 (0.1317)	MaskDICELoss 0.9210 (0.5374)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 291 is less than current step: 499. Dropping entry: {'train/loss': 1.6816148281097412, 'train/ce_loss': 0.588671875, 'train/seg_cls_loss': 0.01417236328125, 'train/kl_loss': 0.2294921875, 'train/mask_bce_loss': 0.07582653816789389, 'train/mask_dice_loss': 0.4556546986103058, 'train/mask_loss': 0.5314812421798706, 'metrics/total_secs_per_batch': 4.950375556945801, 'metrics/data_secs_per_batch': 2.5328109502792358, '_timestamp': 1740955817.981354}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 291 is less than current step: 499. Dropping entry: {'train/lr': 0.0002093265306122449, '_timestamp': 1740955817.9816313}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 292 is less than current step: 499. Dropping entry: {'train/loss': 1.603054928779602, 'train/ce_loss': 0.225244140625, 'train/seg_cls_loss': 0.018719482421875, 'train/kl_loss': 0.305078125, 'train/mask_bce_loss': 0.13166686575859785, 'train/mask_dice_loss': 0.5373654633760452, 'train/mask_loss': 0.6690323352813721, 'metrics/total_secs_per_batch': 7.231108665466309, 'metrics/data_secs_per_batch': 3.1511502265930176, '_timestamp': 1740955825.2127264}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 292 is less than current step: 499. Dropping entry: {'train/lr': 0.00020920408163265306, '_timestamp': 1740955825.2130857}).
Epoch: [1][294/500]	Time  6.172 ( 6.172)	Loss 1.9467 (1.8853)	CeLoss 0.3770 (0.5457)	SegCLSLoss 0.0176 (0.0144)	KLLoss 0.3926 (0.2695)	MaskLoss 0.7605 (0.6528)	MaskBCELoss 0.2920 (0.1977)	MaskDICELoss 0.4684 (0.4551)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 293 is less than current step: 499. Dropping entry: {'train/loss': 1.8853333711624145, 'train/ce_loss': 0.545703125, 'train/seg_cls_loss': 0.0143798828125, 'train/kl_loss': 0.26953125, 'train/mask_bce_loss': 0.19771043360233306, 'train/mask_dice_loss': 0.45511249005794524, 'train/mask_loss': 0.6528229236602783, 'metrics/total_secs_per_batch': 6.172110319137573, 'metrics/data_secs_per_batch': 2.6407505750656126, '_timestamp': 1740955831.384597}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 293 is less than current step: 499. Dropping entry: {'train/lr': 0.00020908163265306122, '_timestamp': 1740955831.384904}).
Epoch: [1][295/500]	Time 21.526 (21.526)	Loss 1.5329 (2.0491)	CeLoss 0.2285 (0.3474)	SegCLSLoss 0.0142 (0.0214)	KLLoss 0.4102 (0.3477)	MaskLoss 0.6278 (0.8282)	MaskBCELoss 0.0470 (0.1313)	MaskDICELoss 0.5808 (0.6970)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 294 is less than current step: 499. Dropping entry: {'train/loss': 2.0490576148033144, 'train/ce_loss': 0.34736328125, 'train/seg_cls_loss': 0.021356201171875, 'train/kl_loss': 0.34765625, 'train/mask_bce_loss': 0.13126975018531084, 'train/mask_dice_loss': 0.6969699919223785, 'train/mask_loss': 0.8282397449016571, 'metrics/total_secs_per_batch': 21.52581548690796, 'metrics/data_secs_per_batch': 5.709672307968139, '_timestamp': 1740955852.910545}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 294 is less than current step: 499. Dropping entry: {'train/lr': 0.00020895918367346938, '_timestamp': 1740955852.9109027}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 295 is less than current step: 499. Dropping entry: {'train/loss': 1.5201630353927613, 'train/ce_loss': 0.393310546875, 'train/seg_cls_loss': 0.013653564453125, 'train/kl_loss': 0.23203125, 'train/mask_bce_loss': 0.11457517147064208, 'train/mask_dice_loss': 0.4336166977882385, 'train/mask_loss': 0.5481918692588806, 'metrics/total_secs_per_batch': 6.65112042427063, 'metrics/data_secs_per_batch': 3.0965636491775514, '_timestamp': 1740955859.5616477}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 295 is less than current step: 499. Dropping entry: {'train/lr': 0.00020883673469387754, '_timestamp': 1740955859.562048}).
Epoch: [1][296/500]	Time  6.651 ( 6.651)	Loss 1.7647 (1.5202)	CeLoss 0.2539 (0.3933)	SegCLSLoss 0.0157 (0.0137)	KLLoss 0.3828 (0.2320)	MaskLoss 0.7320 (0.5482)	MaskBCELoss 0.0705 (0.1146)	MaskDICELoss 0.6614 (0.4336)
Epoch: [1][297/500]	Time  7.259 ( 7.259)	Loss 2.2704 (1.6070)	CeLoss 0.2715 (0.5372)	SegCLSLoss 0.0123 (0.0111)	KLLoss 0.3965 (0.1889)	MaskLoss 0.9770 (0.5228)	MaskBCELoss 0.0103 (0.0619)	MaskDICELoss 0.9667 (0.4609)
Epoch: [1][298/500]	Time  7.110 ( 7.110)	Loss 2.2876 (1.9466)	CeLoss 0.1367 (0.1936)	SegCLSLoss 0.0383 (0.0229)	KLLoss 0.3887 (0.3861)	MaskLoss 1.0461 (0.8516)	MaskBCELoss 0.1860 (0.1395)	MaskDICELoss 0.8602 (0.7120)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 296 is less than current step: 499. Dropping entry: {'train/loss': 1.606981873512268, 'train/ce_loss': 0.53720703125, 'train/seg_cls_loss': 0.01109619140625, 'train/kl_loss': 0.1888671875, 'train/mask_bce_loss': 0.061879954393953086, 'train/mask_dice_loss': 0.46094692349433897, 'train/mask_loss': 0.522826874256134, 'metrics/total_secs_per_batch': 7.258587121963501, 'metrics/data_secs_per_batch': 3.235502815246582, '_timestamp': 1740955866.8202868}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 296 is less than current step: 499. Dropping entry: {'train/lr': 0.0002087142857142857, '_timestamp': 1740955866.8205256}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 297 is less than current step: 499. Dropping entry: {'train/loss': 1.946619176864624, 'train/ce_loss': 0.1935546875, 'train/seg_cls_loss': 0.0229248046875, 'train/kl_loss': 0.3861328125, 'train/mask_bce_loss': 0.13953472087159752, 'train/mask_dice_loss': 0.712046355009079, 'train/mask_loss': 0.8515810757875443, 'metrics/total_secs_per_batch': 7.11026668548584, 'metrics/data_secs_per_batch': 3.2562010288238525, '_timestamp': 1740955873.9306767}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 297 is less than current step: 499. Dropping entry: {'train/lr': 0.00020859183673469387, '_timestamp': 1740955873.9310603}).
Epoch: [1][299/500]	Time  5.777 ( 5.777)	Loss 2.1414 (1.5997)	CeLoss 0.1797 (0.5852)	SegCLSLoss 0.0334 (0.0146)	KLLoss 0.3828 (0.2707)	MaskLoss 0.9535 (0.4900)	MaskBCELoss 0.0260 (0.0459)	MaskDICELoss 0.9276 (0.4441)
[2025-03-02 16:51:26,188] [INFO] [logging.py:96:log_dist] [Rank 0] step=800, skipped=0, lr=[0.0002084081632653061], mom=[(0.9, 0.95)]
[2025-03-02 16:51:26,193] [INFO] [timer.py:215:stop] epoch=0/micro_step=8000/global_step=800, RunningAvgSamplesPerSec=1.5014971505096262, CurrSamplesPerSec=1.5418147608910162, MemAllocated=31.49GB, MaxMemAllocated=36.81GB
Epoch: [1][300/500]	Time  6.488 ( 6.488)	Loss 1.6945 (1.9423)	CeLoss 0.1729 (0.4564)	SegCLSLoss 0.0237 (0.0173)	KLLoss 0.3887 (0.3064)	MaskLoss 0.7354 (0.7233)	MaskBCELoss 0.0287 (0.1761)	MaskDICELoss 0.7067 (0.5472)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 298 is less than current step: 499. Dropping entry: {'train/loss': 1.599699854850769, 'train/ce_loss': 0.58515625, 'train/seg_cls_loss': 0.014630126953125, 'train/kl_loss': 0.270703125, 'train/mask_bce_loss': 0.04589019445702434, 'train/mask_dice_loss': 0.4440964639186859, 'train/mask_loss': 0.4899866461753845, 'metrics/total_secs_per_batch': 5.77663779258728, 'metrics/data_secs_per_batch': 2.409732460975647, '_timestamp': 1740955879.7070434}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 298 is less than current step: 499. Dropping entry: {'train/lr': 0.00020846938775510203, '_timestamp': 1740955879.7073457}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 299 is less than current step: 499. Dropping entry: {'train/loss': 1.9423495769500732, 'train/ce_loss': 0.4564453125, 'train/seg_cls_loss': 0.01728515625, 'train/kl_loss': 0.3064453125, 'train/mask_bce_loss': 0.17608669605106114, 'train/mask_dice_loss': 0.5472365140914917, 'train/mask_loss': 0.7233232021331787, 'metrics/total_secs_per_batch': 6.487502336502075, 'metrics/data_secs_per_batch': 2.8138710498809814, '_timestamp': 1740955886.1944482}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 299 is less than current step: 499. Dropping entry: {'train/lr': 0.0002083469387755102, '_timestamp': 1740955886.1947997}).
Epoch: [1][301/500]	Time  5.414 ( 5.414)	Loss 3.1191 (2.1036)	CeLoss 0.2295 (0.5896)	SegCLSLoss 0.0309 (0.0164)	KLLoss 0.3945 (0.2686)	MaskLoss 1.4169 (0.7394)	MaskBCELoss 0.6261 (0.2052)	MaskDICELoss 0.7908 (0.5341)
Epoch: [1][302/500]	Time  7.113 ( 7.113)	Loss 2.3056 (2.0223)	CeLoss 0.2275 (0.3351)	SegCLSLoss 0.0255 (0.0172)	KLLoss 0.3809 (0.3451)	MaskLoss 1.0141 (0.8222)	MaskBCELoss 0.0930 (0.1396)	MaskDICELoss 0.9211 (0.6826)
Epoch: [1][303/500]	Time  5.953 ( 5.953)	Loss 2.6822 (1.8506)	CeLoss 0.1797 (0.5141)	SegCLSLoss 0.0383 (0.0194)	KLLoss 0.3672 (0.2219)	MaskLoss 1.2229 (0.6524)	MaskBCELoss 0.3364 (0.1401)	MaskDICELoss 0.8866 (0.5123)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 300 is less than current step: 499. Dropping entry: {'train/loss': 2.1035724520683288, 'train/ce_loss': 0.5896484375, 'train/seg_cls_loss': 0.016436767578125, 'train/kl_loss': 0.2685546875, 'train/mask_bce_loss': 0.20524005200713874, 'train/mask_dice_loss': 0.5341438353061676, 'train/mask_loss': 0.7393838942050934, 'metrics/total_secs_per_batch': 5.413956880569458, 'metrics/data_secs_per_batch': 2.3774524211883543, '_timestamp': 1740955891.6085346}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 300 is less than current step: 499. Dropping entry: {'train/lr': 0.00020822448979591836, '_timestamp': 1740955891.6088297}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 301 is less than current step: 499. Dropping entry: {'train/loss': 2.0223142743110656, 'train/ce_loss': 0.33505859375, 'train/seg_cls_loss': 0.017181396484375, 'train/kl_loss': 0.3451171875, 'train/mask_bce_loss': 0.13957645762711762, 'train/mask_dice_loss': 0.6826158463954926, 'train/mask_loss': 0.8221922874450683, 'metrics/total_secs_per_batch': 7.112634181976318, 'metrics/data_secs_per_batch': 3.0825359582901, '_timestamp': 1740955898.7214067}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 301 is less than current step: 499. Dropping entry: {'train/lr': 0.00020810204081632652, '_timestamp': 1740955898.7217798}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 302 is less than current step: 499. Dropping entry: {'train/loss': 1.8505810737609862, 'train/ce_loss': 0.5140625, 'train/seg_cls_loss': 0.01942138671875, 'train/kl_loss': 0.221875, 'train/mask_bce_loss': 0.14013030137866736, 'train/mask_dice_loss': 0.5123086631298065, 'train/mask_loss': 0.6524389684200287, 'metrics/total_secs_per_batch': 5.952807188034058, 'metrics/data_secs_per_batch': 2.5899969577789306, '_timestamp': 1740955904.674124}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 302 is less than current step: 499. Dropping entry: {'train/lr': 0.00020797959183673466, '_timestamp': 1740955904.674561}).
Epoch: [1][304/500]	Time  6.386 ( 6.386)	Loss 2.4570 (1.5599)	CeLoss 0.1846 (0.4065)	SegCLSLoss 0.0513 (0.0191)	KLLoss 0.3652 (0.3080)	MaskLoss 1.1055 (0.5565)	MaskBCELoss 0.3492 (0.1599)	MaskDICELoss 0.7563 (0.3966)
Epoch: [1][305/500]	Time  6.333 ( 6.333)	Loss 1.2705 (2.1106)	CeLoss 0.2422 (0.2195)	SegCLSLoss 0.0139 (0.0207)	KLLoss 0.3828 (0.3830)	MaskLoss 0.4917 (0.9213)	MaskBCELoss 0.1692 (0.2795)	MaskDICELoss 0.3225 (0.6418)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 303 is less than current step: 499. Dropping entry: {'train/loss': 1.5599372327327727, 'train/ce_loss': 0.40654296875, 'train/seg_cls_loss': 0.019134521484375, 'train/kl_loss': 0.3080078125, 'train/mask_bce_loss': 0.1598708663135767, 'train/mask_dice_loss': 0.3966114118695259, 'train/mask_loss': 0.5564822852611542, 'metrics/total_secs_per_batch': 6.386340618133545, 'metrics/data_secs_per_batch': 2.8238844871520996, '_timestamp': 1740955911.0603857}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 303 is less than current step: 499. Dropping entry: {'train/lr': 0.00020785714285714282, '_timestamp': 1740955911.0607016}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 304 is less than current step: 499. Dropping entry: {'train/loss': 2.1106436371803285, 'train/ce_loss': 0.21953125, 'train/seg_cls_loss': 0.02066650390625, 'train/kl_loss': 0.3830078125, 'train/mask_bce_loss': 0.2794987238943577, 'train/mask_dice_loss': 0.6417898908257484, 'train/mask_loss': 0.921288612484932, 'metrics/total_secs_per_batch': 6.3332343101501465, 'metrics/data_secs_per_batch': 2.997129774093628, '_timestamp': 1740955917.393599}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 304 is less than current step: 499. Dropping entry: {'train/lr': 0.00020773469387755098, '_timestamp': 1740955917.393884}).
Epoch: [1][306/500]	Time  5.660 ( 5.660)	Loss 1.2188 (1.4170)	CeLoss 1.2188 (0.5163)	SegCLSLoss 0.0000 (0.0122)	KLLoss 0.0000 (0.2270)	MaskLoss 0.0000 (0.4360)	MaskBCELoss 0.0000 (0.0515)	MaskDICELoss 0.0000 (0.3845)
Epoch: [1][307/500]	Time  6.036 ( 6.036)	Loss 1.4453 (1.6142)	CeLoss 1.4453 (0.5662)	SegCLSLoss 0.0000 (0.0161)	KLLoss 0.0000 (0.2689)	MaskLoss 0.0000 (0.5065)	MaskBCELoss 0.0000 (0.1433)	MaskDICELoss 0.0000 (0.3632)
Epoch: [1][308/500]	Time  6.357 ( 6.357)	Loss 2.6832 (1.8987)	CeLoss 0.2217 (0.5973)	SegCLSLoss 0.0292 (0.0210)	KLLoss 0.3789 (0.2271)	MaskLoss 1.2049 (0.6342)	MaskBCELoss 0.2070 (0.1633)	MaskDICELoss 0.9979 (0.4709)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 305 is less than current step: 499. Dropping entry: {'train/loss': 1.4170072555541993, 'train/ce_loss': 0.51630859375, 'train/seg_cls_loss': 0.01219482421875, 'train/kl_loss': 0.226953125, 'train/mask_bce_loss': 0.051473290845751765, 'train/mask_dice_loss': 0.3845205754041672, 'train/mask_loss': 0.4359938681125641, 'metrics/total_secs_per_batch': 5.660367250442505, 'metrics/data_secs_per_batch': 2.4663952589035034, '_timestamp': 1740955923.0539458}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 305 is less than current step: 499. Dropping entry: {'train/lr': 0.00020761224489795915, '_timestamp': 1740955923.05424}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 306 is less than current step: 499. Dropping entry: {'train/loss': 1.6141682863235474, 'train/ce_loss': 0.5662109375, 'train/seg_cls_loss': 0.016058349609375, 'train/kl_loss': 0.2689453125, 'train/mask_bce_loss': 0.14331329092383385, 'train/mask_dice_loss': 0.3631849229335785, 'train/mask_loss': 0.5064982205629349, 'metrics/total_secs_per_batch': 6.036134719848633, 'metrics/data_secs_per_batch': 2.7938846826553343, '_timestamp': 1740955929.090269}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 306 is less than current step: 499. Dropping entry: {'train/lr': 0.0002074897959183673, '_timestamp': 1740955929.0906138}).
Epoch: [1][309/500]	Time  5.979 ( 5.979)	Loss 2.0618 (1.4228)	CeLoss 0.2656 (0.3521)	SegCLSLoss 0.0151 (0.0185)	KLLoss 0.3867 (0.2672)	MaskLoss 0.8746 (0.5171)	MaskBCELoss 0.1200 (0.0999)	MaskDICELoss 0.7546 (0.4172)
[2025-03-02 16:52:27,069] [INFO] [logging.py:96:log_dist] [Rank 0] step=810, skipped=0, lr=[0.00020718367346938773], mom=[(0.9, 0.95)]
[2025-03-02 16:52:27,075] [INFO] [timer.py:215:stop] epoch=0/micro_step=8100/global_step=810, RunningAvgSamplesPerSec=1.5030981292842704, CurrSamplesPerSec=1.7702685489298586, MemAllocated=31.25GB, MaxMemAllocated=36.81GB
Epoch: [1][310/500]	Time  5.650 ( 5.650)	Loss 1.8196 (1.2406)	CeLoss 0.2520 (0.5120)	SegCLSLoss 0.0134 (0.0085)	KLLoss 0.3848 (0.1930)	MaskLoss 0.7613 (0.3525)	MaskBCELoss 0.1978 (0.0789)	MaskDICELoss 0.5635 (0.2736)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 307 is less than current step: 499. Dropping entry: {'train/loss': 1.8987117290496827, 'train/ce_loss': 0.597265625, 'train/seg_cls_loss': 0.020977783203125, 'train/kl_loss': 0.2271484375, 'train/mask_bce_loss': 0.1632724866271019, 'train/mask_dice_loss': 0.4708978354930878, 'train/mask_loss': 0.6341703176498413, 'metrics/total_secs_per_batch': 6.356664180755615, 'metrics/data_secs_per_batch': 2.9839860677719114, '_timestamp': 1740955935.446737}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 307 is less than current step: 499. Dropping entry: {'train/lr': 0.00020736734693877547, '_timestamp': 1740955935.4470203}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 308 is less than current step: 499. Dropping entry: {'train/loss': 1.4227502703666688, 'train/ce_loss': 0.3521484375, 'train/seg_cls_loss': 0.01849365234375, 'train/kl_loss': 0.2671875, 'train/mask_bce_loss': 0.09993155337870122, 'train/mask_dice_loss': 0.4171564787626266, 'train/mask_loss': 0.5170880347490311, 'metrics/total_secs_per_batch': 5.979137420654297, 'metrics/data_secs_per_batch': 2.5574523210525513, '_timestamp': 1740955941.4258502}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 308 is less than current step: 499. Dropping entry: {'train/lr': 0.00020724489795918364, '_timestamp': 1740955941.4261363}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 309 is less than current step: 499. Dropping entry: {'train/loss': 1.2405623197555542, 'train/ce_loss': 0.51201171875, 'train/seg_cls_loss': 0.0085205078125, 'train/kl_loss': 0.19296875, 'train/mask_bce_loss': 0.07891216091811656, 'train/mask_dice_loss': 0.27359557151794434, 'train/mask_loss': 0.3525077313184738, 'metrics/total_secs_per_batch': 5.650477409362793, 'metrics/data_secs_per_batch': 2.3631769895553587, '_timestamp': 1740955947.0761685}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 309 is less than current step: 499. Dropping entry: {'train/lr': 0.0002071224489795918, '_timestamp': 1740955947.0764616}).
Epoch: [1][311/500]	Time  6.413 ( 6.413)	Loss 2.6692 (2.0369)	CeLoss 0.2617 (0.2971)	SegCLSLoss 0.0272 (0.0207)	KLLoss 0.3789 (0.3410)	MaskLoss 1.1783 (0.8476)	MaskBCELoss 0.3785 (0.2843)	MaskDICELoss 0.7998 (0.5634)
Epoch: [1][312/500]	Time  6.233 ( 6.233)	Loss 1.1641 (1.9397)	CeLoss 1.1641 (0.4493)	SegCLSLoss 0.0000 (0.0167)	KLLoss 0.0000 (0.2686)	MaskLoss 0.0000 (0.7276)	MaskBCELoss 0.0000 (0.1355)	MaskDICELoss 0.0000 (0.5920)
Epoch: [1][313/500]	Time  5.156 ( 5.156)	Loss 2.1619 (1.5386)	CeLoss 0.1416 (0.4541)	SegCLSLoss 0.0417 (0.0164)	KLLoss 0.3691 (0.3135)	MaskLoss 0.9813 (0.5226)	MaskBCELoss 0.1538 (0.1037)	MaskDICELoss 0.8275 (0.4189)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 310 is less than current step: 499. Dropping entry: {'train/loss': 2.0368639826774597, 'train/ce_loss': 0.2970703125, 'train/seg_cls_loss': 0.020733642578125, 'train/kl_loss': 0.341015625, 'train/mask_bce_loss': 0.28426820077002046, 'train/mask_dice_loss': 0.5633630067110061, 'train/mask_loss': 0.8476312041282654, 'metrics/total_secs_per_batch': 6.412863731384277, 'metrics/data_secs_per_batch': 3.0949814319610596, '_timestamp': 1740955953.4892306}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 310 is less than current step: 499. Dropping entry: {'train/lr': 0.00020699999999999996, '_timestamp': 1740955953.4895043}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 311 is less than current step: 499. Dropping entry: {'train/loss': 1.9396963357925414, 'train/ce_loss': 0.449267578125, 'train/seg_cls_loss': 0.01669921875, 'train/kl_loss': 0.2685546875, 'train/mask_bce_loss': 0.13551735170185567, 'train/mask_dice_loss': 0.5920456707477569, 'train/mask_loss': 0.7275630176067353, 'metrics/total_secs_per_batch': 6.232580900192261, 'metrics/data_secs_per_batch': 2.999743437767029, '_timestamp': 1740955959.7220824}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 311 is less than current step: 499. Dropping entry: {'train/lr': 0.00020687755102040815, '_timestamp': 1740955959.72242}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 312 is less than current step: 499. Dropping entry: {'train/loss': 1.5386029601097106, 'train/ce_loss': 0.4541015625, 'train/seg_cls_loss': 0.016400146484375, 'train/kl_loss': 0.3134765625, 'train/mask_bce_loss': 0.1036605566740036, 'train/mask_dice_loss': 0.4189124211668968, 'train/mask_loss': 0.5225729733705521, 'metrics/total_secs_per_batch': 5.155969858169556, 'metrics/data_secs_per_batch': 2.5320313215255736, '_timestamp': 1740955964.8778286}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 312 is less than current step: 499. Dropping entry: {'train/lr': 0.00020675510204081632, '_timestamp': 1740955964.8781352}).
Epoch: [1][314/500]	Time  6.850 ( 6.850)	Loss 1.5485 (2.0412)	CeLoss 0.2393 (0.2355)	SegCLSLoss 0.0134 (0.0239)	KLLoss 0.3965 (0.3869)	MaskLoss 0.6317 (0.8775)	MaskBCELoss 0.0732 (0.2115)	MaskDICELoss 0.5585 (0.6660)
Epoch: [1][315/500]	Time  6.141 ( 6.141)	Loss 1.7008 (1.4572)	CeLoss 0.2930 (0.3789)	SegCLSLoss 0.0168 (0.0135)	KLLoss 0.3906 (0.3098)	MaskLoss 0.6805 (0.5203)	MaskBCELoss 0.0541 (0.0991)	MaskDICELoss 0.6264 (0.4212)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 313 is less than current step: 499. Dropping entry: {'train/loss': 2.041165328025818, 'train/ce_loss': 0.235546875, 'train/seg_cls_loss': 0.02386474609375, 'train/kl_loss': 0.3869140625, 'train/mask_bce_loss': 0.21147693656384944, 'train/mask_dice_loss': 0.6659904837608337, 'train/mask_loss': 0.8774674355983734, 'metrics/total_secs_per_batch': 6.8504958152771, 'metrics/data_secs_per_batch': 3.2899020433425905, '_timestamp': 1740955971.7282727}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 313 is less than current step: 499. Dropping entry: {'train/lr': 0.00020663265306122448, '_timestamp': 1740955971.728559}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 314 is less than current step: 499. Dropping entry: {'train/loss': 1.4571654438972472, 'train/ce_loss': 0.37890625, 'train/seg_cls_loss': 0.0135009765625, 'train/kl_loss': 0.309765625, 'train/mask_bce_loss': 0.0991493046283722, 'train/mask_dice_loss': 0.4211814448237419, 'train/mask_loss': 0.5203307554125786, 'metrics/total_secs_per_batch': 6.140726804733276, 'metrics/data_secs_per_batch': 2.9600542068481444, '_timestamp': 1740955977.8690174}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 314 is less than current step: 499. Dropping entry: {'train/lr': 0.00020651020408163264, '_timestamp': 1740955977.8693626}).
Epoch: [1][316/500]	Time  6.509 ( 6.509)	Loss 2.4988 (1.6556)	CeLoss 0.2676 (0.2873)	SegCLSLoss 0.0238 (0.0192)	KLLoss 0.3750 (0.3105)	MaskLoss 1.0912 (0.6639)	MaskBCELoss 0.1952 (0.1605)	MaskDICELoss 0.8960 (0.5034)
Epoch: [1][317/500]	Time  5.669 ( 5.669)	Loss 0.8828 (1.7397)	CeLoss 0.8828 (0.4871)	SegCLSLoss 0.0000 (0.0137)	KLLoss 0.0000 (0.2766)	MaskLoss 0.0000 (0.6090)	MaskBCELoss 0.0000 (0.0896)	MaskDICELoss 0.0000 (0.5195)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 315 is less than current step: 499. Dropping entry: {'train/loss': 1.6555784583091735, 'train/ce_loss': 0.2873046875, 'train/seg_cls_loss': 0.019232177734375, 'train/kl_loss': 0.310546875, 'train/mask_bce_loss': 0.1605206698179245, 'train/mask_dice_loss': 0.5033769588917494, 'train/mask_loss': 0.6638976275920868, 'metrics/total_secs_per_batch': 6.508908271789551, 'metrics/data_secs_per_batch': 2.8066709280014037, '_timestamp': 1740955984.3778987}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 315 is less than current step: 499. Dropping entry: {'train/lr': 0.0002063877551020408, '_timestamp': 1740955984.3780956}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 316 is less than current step: 499. Dropping entry: {'train/loss': 1.7397286534309386, 'train/ce_loss': 0.487109375, 'train/seg_cls_loss': 0.013671875, 'train/kl_loss': 0.2765625, 'train/mask_bce_loss': 0.08956161104142665, 'train/mask_dice_loss': 0.5194628745317459, 'train/mask_loss': 0.6090244740247727, 'metrics/total_secs_per_batch': 5.668868780136108, 'metrics/data_secs_per_batch': 2.789786529541016, '_timestamp': 1740955990.0469239}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 316 is less than current step: 499. Dropping entry: {'train/lr': 0.00020626530612244897, '_timestamp': 1740955990.0471969}).
Epoch: [1][318/500]	Time  5.886 ( 5.886)	Loss 1.6891 (2.0726)	CeLoss 0.2090 (0.4738)	SegCLSLoss 0.0272 (0.0192)	KLLoss 0.3906 (0.3176)	MaskLoss 0.7137 (0.7789)	MaskBCELoss 0.0943 (0.1395)	MaskDICELoss 0.6194 (0.6394)
Epoch: [1][319/500]	Time  6.779 ( 6.779)	Loss 1.3269 (2.0026)	CeLoss 0.2969 (0.3719)	SegCLSLoss 0.0124 (0.0186)	KLLoss 0.4121 (0.3557)	MaskLoss 0.4916 (0.7930)	MaskBCELoss 0.1505 (0.1641)	MaskDICELoss 0.3411 (0.6289)
[2025-03-02 16:53:29,564] [INFO] [logging.py:96:log_dist] [Rank 0] step=820, skipped=0, lr=[0.00020595918367346936], mom=[(0.9, 0.95)]
[2025-03-02 16:53:29,570] [INFO] [timer.py:215:stop] epoch=0/micro_step=8200/global_step=820, RunningAvgSamplesPerSec=1.5042168182093643, CurrSamplesPerSec=1.4581421340793492, MemAllocated=31.27GB, MaxMemAllocated=36.81GB
Epoch: [1][320/500]	Time  6.860 ( 6.860)	Loss 2.8321 (1.9368)	CeLoss 0.1846 (0.4074)	SegCLSLoss 0.0282 (0.0156)	KLLoss 0.3848 (0.3189)	MaskLoss 1.2979 (0.7449)	MaskBCELoss 0.5181 (0.1549)	MaskDICELoss 0.7798 (0.5901)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 317 is less than current step: 499. Dropping entry: {'train/loss': 2.072565019130707, 'train/ce_loss': 0.473828125, 'train/seg_cls_loss': 0.01920166015625, 'train/kl_loss': 0.317578125, 'train/mask_bce_loss': 0.13948061112314464, 'train/mask_dice_loss': 0.6393800050020217, 'train/mask_loss': 0.7788606345653534, 'metrics/total_secs_per_batch': 5.885553359985352, 'metrics/data_secs_per_batch': 2.264464855194092, '_timestamp': 1740955995.932365}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 317 is less than current step: 499. Dropping entry: {'train/lr': 0.00020614285714285713, '_timestamp': 1740955995.9326544}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 318 is less than current step: 499. Dropping entry: {'train/loss': 2.002614712715149, 'train/ce_loss': 0.371875, 'train/seg_cls_loss': 0.01856689453125, 'train/kl_loss': 0.3556640625, 'train/mask_bce_loss': 0.16411423832178115, 'train/mask_dice_loss': 0.6288923382759094, 'train/mask_loss': 0.7930065780878067, 'metrics/total_secs_per_batch': 6.7788379192352295, 'metrics/data_secs_per_batch': 3.323400115966797, '_timestamp': 1740956002.711202}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 318 is less than current step: 499. Dropping entry: {'train/lr': 0.0002060204081632653, '_timestamp': 1740956002.7114878}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 319 is less than current step: 499. Dropping entry: {'train/loss': 1.936794686317444, 'train/ce_loss': 0.407421875, 'train/seg_cls_loss': 0.0156005859375, 'train/kl_loss': 0.3189453125, 'train/mask_bce_loss': 0.15485298559069632, 'train/mask_dice_loss': 0.590058034658432, 'train/mask_loss': 0.7449110090732575, 'metrics/total_secs_per_batch': 6.859619379043579, 'metrics/data_secs_per_batch': 2.775445795059204, '_timestamp': 1740956009.5706627}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 319 is less than current step: 499. Dropping entry: {'train/lr': 0.00020589795918367346, '_timestamp': 1740956009.5709524}).
Epoch: [1][321/500]	Time  6.393 ( 6.393)	Loss 0.2891 (1.5519)	CeLoss 0.2891 (0.3846)	SegCLSLoss 0.0000 (0.0153)	KLLoss 0.0000 (0.2766)	MaskLoss 0.0000 (0.5661)	MaskBCELoss 0.0000 (0.0855)	MaskDICELoss 0.0000 (0.4806)
Epoch: [1][322/500]	Time  5.683 ( 5.683)	Loss 2.1733 (1.5799)	CeLoss 0.1523 (0.6182)	SegCLSLoss 0.0444 (0.0112)	KLLoss 0.3770 (0.1961)	MaskLoss 0.9802 (0.4682)	MaskBCELoss 0.1962 (0.0952)	MaskDICELoss 0.7841 (0.3730)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 320 is less than current step: 499. Dropping entry: {'train/loss': 1.551902461051941, 'train/ce_loss': 0.3845703125, 'train/seg_cls_loss': 0.015252685546875, 'train/kl_loss': 0.2765625, 'train/mask_bce_loss': 0.08546699564903974, 'train/mask_dice_loss': 0.48062095046043396, 'train/mask_loss': 0.5660879611968994, 'metrics/total_secs_per_batch': 6.393309593200684, 'metrics/data_secs_per_batch': 3.023813486099243, '_timestamp': 1740956015.9643712}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 320 is less than current step: 499. Dropping entry: {'train/lr': 0.00020577551020408162, '_timestamp': 1740956015.9647367}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 321 is less than current step: 499. Dropping entry: {'train/loss': 1.5799372792243958, 'train/ce_loss': 0.6181640625, 'train/seg_cls_loss': 0.01119384765625, 'train/kl_loss': 0.19609375, 'train/mask_bce_loss': 0.09517142772674561, 'train/mask_dice_loss': 0.3730198562145233, 'train/mask_loss': 0.4681912899017334, 'metrics/total_secs_per_batch': 5.683447599411011, 'metrics/data_secs_per_batch': 2.5417367458343505, '_timestamp': 1740956021.6476455}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 321 is less than current step: 499. Dropping entry: {'train/lr': 0.00020565306122448978, '_timestamp': 1740956021.6480556}).
Epoch: [1][323/500]	Time  6.501 ( 6.501)	Loss 1.5298 (1.9208)	CeLoss 0.1982 (0.1956)	SegCLSLoss 0.0162 (0.0251)	KLLoss 0.4121 (0.3512)	MaskLoss 0.6414 (0.8387)	MaskBCELoss 0.1968 (0.2783)	MaskDICELoss 0.4446 (0.5605)
Epoch: [1][324/500]	Time  6.528 ( 6.528)	Loss 1.8046 (1.9390)	CeLoss 0.2637 (0.3507)	SegCLSLoss 0.0132 (0.0189)	KLLoss 0.4277 (0.3168)	MaskLoss 0.7461 (0.7737)	MaskBCELoss 0.2384 (0.1700)	MaskDICELoss 0.5076 (0.6037)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 322 is less than current step: 499. Dropping entry: {'train/loss': 1.920828253030777, 'train/ce_loss': 0.19560546875, 'train/seg_cls_loss': 0.02513427734375, 'train/kl_loss': 0.351171875, 'train/mask_bce_loss': 0.2782748453319073, 'train/mask_dice_loss': 0.5604595959186554, 'train/mask_loss': 0.8387344390153885, 'metrics/total_secs_per_batch': 6.500681638717651, 'metrics/data_secs_per_batch': 3.053579831123352, '_timestamp': 1740956028.1484501}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 322 is less than current step: 499. Dropping entry: {'train/lr': 0.00020553061224489795, '_timestamp': 1740956028.1488016}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 323 is less than current step: 499. Dropping entry: {'train/loss': 1.938969075679779, 'train/ce_loss': 0.350732421875, 'train/seg_cls_loss': 0.01890869140625, 'train/kl_loss': 0.316796875, 'train/mask_bce_loss': 0.16999514438211918, 'train/mask_dice_loss': 0.603664168715477, 'train/mask_loss': 0.7736593157052993, 'metrics/total_secs_per_batch': 6.528140544891357, 'metrics/data_secs_per_batch': 2.9000632762908936, '_timestamp': 1740956034.6764383}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 323 is less than current step: 499. Dropping entry: {'train/lr': 0.0002054081632653061, '_timestamp': 1740956034.6767597}).
Epoch: [1][325/500]	Time  5.745 ( 5.745)	Loss 2.1333 (1.4504)	CeLoss 0.1611 (0.3563)	SegCLSLoss 0.0234 (0.0160)	KLLoss 0.3906 (0.3223)	MaskLoss 0.9607 (0.5270)	MaskBCELoss 0.0222 (0.0682)	MaskDICELoss 0.9385 (0.4588)
Epoch: [1][326/500]	Time  5.868 ( 5.868)	Loss 1.2109 (1.5387)	CeLoss 1.2109 (0.5168)	SegCLSLoss 0.0000 (0.0116)	KLLoss 0.0000 (0.2437)	MaskLoss 0.0000 (0.4959)	MaskBCELoss 0.0000 (0.0618)	MaskDICELoss 0.0000 (0.4341)
Epoch: [1][327/500]	Time  5.274 ( 5.274)	Loss 2.6537 (1.7212)	CeLoss 0.1807 (0.3861)	SegCLSLoss 0.0474 (0.0192)	KLLoss 0.3828 (0.3156)	MaskLoss 1.2057 (0.6471)	MaskBCELoss 0.3886 (0.1454)	MaskDICELoss 0.8172 (0.5017)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 324 is less than current step: 499. Dropping entry: {'train/loss': 1.4504398941993712, 'train/ce_loss': 0.35625, 'train/seg_cls_loss': 0.015985107421875, 'train/kl_loss': 0.322265625, 'train/mask_bce_loss': 0.06822706647217273, 'train/mask_dice_loss': 0.4587506830692291, 'train/mask_loss': 0.5269777499139309, 'metrics/total_secs_per_batch': 5.744685173034668, 'metrics/data_secs_per_batch': 2.5192163944244386, '_timestamp': 1740956040.4211671}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 324 is less than current step: 499. Dropping entry: {'train/lr': 0.00020528571428571427, '_timestamp': 1740956040.4214292}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 325 is less than current step: 499. Dropping entry: {'train/loss': 1.5386777520179749, 'train/ce_loss': 0.516796875, 'train/seg_cls_loss': 0.011627197265625, 'train/kl_loss': 0.24375, 'train/mask_bce_loss': 0.06179947126656771, 'train/mask_dice_loss': 0.43405306339263916, 'train/mask_loss': 0.495852530002594, 'metrics/total_secs_per_batch': 5.868488788604736, 'metrics/data_secs_per_batch': 2.909352254867554, '_timestamp': 1740956046.2897792}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 325 is less than current step: 499. Dropping entry: {'train/lr': 0.00020516326530612244, '_timestamp': 1740956046.2901053}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 326 is less than current step: 499. Dropping entry: {'train/loss': 1.7212104558944703, 'train/ce_loss': 0.3861328125, 'train/seg_cls_loss': 0.019183349609375, 'train/kl_loss': 0.315625, 'train/mask_bce_loss': 0.1453612320125103, 'train/mask_dice_loss': 0.5017185896635056, 'train/mask_loss': 0.6470798373222351, 'metrics/total_secs_per_batch': 5.274277210235596, 'metrics/data_secs_per_batch': 2.2494708061218263, '_timestamp': 1740956051.5638993}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 326 is less than current step: 499. Dropping entry: {'train/lr': 0.0002050408163265306, '_timestamp': 1740956051.5642445}).
Epoch: [1][328/500]	Time  6.702 ( 6.702)	Loss 2.3249 (1.8800)	CeLoss 0.2637 (0.4295)	SegCLSLoss 0.0192 (0.0194)	KLLoss 0.3984 (0.3223)	MaskLoss 1.0062 (0.7042)	MaskBCELoss 0.0066 (0.0684)	MaskDICELoss 0.9996 (0.6358)
Epoch: [1][329/500]	Time  6.133 ( 6.133)	Loss 1.6016 (1.5965)	CeLoss 1.6016 (0.4734)	SegCLSLoss 0.0000 (0.0156)	KLLoss 0.0000 (0.2422)	MaskLoss 0.0000 (0.5455)	MaskBCELoss 0.0000 (0.1703)	MaskDICELoss 0.0000 (0.3752)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 327 is less than current step: 499. Dropping entry: {'train/loss': 1.879976999759674, 'train/ce_loss': 0.4294921875, 'train/seg_cls_loss': 0.019415283203125, 'train/kl_loss': 0.322265625, 'train/mask_bce_loss': 0.0684478900860995, 'train/mask_dice_loss': 0.6357984095811844, 'train/mask_loss': 0.704246312379837, 'metrics/total_secs_per_batch': 6.702181100845337, 'metrics/data_secs_per_batch': 2.991672396659851, '_timestamp': 1740956058.2662323}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 327 is less than current step: 499. Dropping entry: {'train/lr': 0.00020491836734693876, '_timestamp': 1740956058.2665758}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 328 is less than current step: 499. Dropping entry: {'train/loss': 1.596508502960205, 'train/ce_loss': 0.473388671875, 'train/seg_cls_loss': 0.01563720703125, 'train/kl_loss': 0.2421875, 'train/mask_bce_loss': 0.1703358992934227, 'train/mask_dice_loss': 0.3751595765352249, 'train/mask_loss': 0.5454954624176025, 'metrics/total_secs_per_batch': 6.133320331573486, 'metrics/data_secs_per_batch': 3.079470896720886, '_timestamp': 1740956064.399373}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 328 is less than current step: 499. Dropping entry: {'train/lr': 0.00020479591836734692, '_timestamp': 1740956064.399661}).
[2025-03-02 16:54:31,123] [INFO] [logging.py:96:log_dist] [Rank 0] step=830, skipped=0, lr=[0.000204734693877551], mom=[(0.9, 0.95)]
[2025-03-02 16:54:31,129] [INFO] [timer.py:215:stop] epoch=0/micro_step=8300/global_step=830, RunningAvgSamplesPerSec=1.5055662361838842, CurrSamplesPerSec=1.4860013337613633, MemAllocated=31.26GB, MaxMemAllocated=36.81GB
Epoch: [1][330/500]	Time  6.731 ( 6.731)	Loss 2.2268 (1.9357)	CeLoss 0.2109 (0.4409)	SegCLSLoss 0.0322 (0.0183)	KLLoss 0.3867 (0.3150)	MaskLoss 0.9806 (0.7271)	MaskBCELoss 0.0771 (0.1509)	MaskDICELoss 0.9035 (0.5763)
Epoch: [1][331/500]	Time  6.120 ( 6.120)	Loss 2.4533 (1.9089)	CeLoss 0.1172 (0.4231)	SegCLSLoss 0.0425 (0.0205)	KLLoss 0.3789 (0.3207)	MaskLoss 1.1383 (0.7216)	MaskBCELoss 0.3006 (0.0943)	MaskDICELoss 0.8377 (0.6273)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 329 is less than current step: 499. Dropping entry: {'train/loss': 1.9356924414634704, 'train/ce_loss': 0.44091796875, 'train/seg_cls_loss': 0.01834716796875, 'train/kl_loss': 0.3150390625, 'train/mask_bce_loss': 0.1508616853505373, 'train/mask_dice_loss': 0.5762618780136108, 'train/mask_loss': 0.7271235585212708, 'metrics/total_secs_per_batch': 6.7310826778411865, 'metrics/data_secs_per_batch': 3.0269974946975706, '_timestamp': 1740956071.130347}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 329 is less than current step: 499. Dropping entry: {'train/lr': 0.00020467346938775506, '_timestamp': 1740956071.130661}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 330 is less than current step: 499. Dropping entry: {'train/loss': 1.908910071849823, 'train/ce_loss': 0.42314453125, 'train/seg_cls_loss': 0.020526123046875, 'train/kl_loss': 0.320703125, 'train/mask_bce_loss': 0.09431357588618994, 'train/mask_dice_loss': 0.6273289561271668, 'train/mask_loss': 0.721642541885376, 'metrics/total_secs_per_batch': 6.119507074356079, 'metrics/data_secs_per_batch': 2.6980767488479613, '_timestamp': 1740956077.2501974}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 330 is less than current step: 499. Dropping entry: {'train/lr': 0.00020455102040816322, '_timestamp': 1740956077.2505386}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 331 is less than current step: 499. Dropping entry: {'train/loss': 1.8384405255317688, 'train/ce_loss': 0.26962890625, 'train/seg_cls_loss': 0.01553955078125, 'train/kl_loss': 0.3677734375, 'train/mask_bce_loss': 0.21177224405109882, 'train/mask_dice_loss': 0.550416761636734, 'train/mask_loss': 0.7621890157461166, 'metrics/total_secs_per_batch': 7.014009475708008, 'metrics/data_secs_per_batch': 3.1562933921813965, '_timestamp': 1740956084.2640095}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 331 is less than current step: 499. Dropping entry: {'train/lr': 0.0002044285714285714, '_timestamp': 1740956084.2643669}).
Epoch: [1][332/500]	Time  7.014 ( 7.014)	Loss 1.2683 (1.8384)	CeLoss 0.2285 (0.2696)	SegCLSLoss 0.0140 (0.0155)	KLLoss 0.4219 (0.3678)	MaskLoss 0.4955 (0.7622)	MaskBCELoss 0.2303 (0.2118)	MaskDICELoss 0.2652 (0.5504)
Epoch: [1][333/500]	Time  6.134 ( 6.134)	Loss 1.7578 (1.7382)	CeLoss 1.7578 (0.4189)	SegCLSLoss 0.0000 (0.0178)	KLLoss 0.0000 (0.3645)	MaskLoss 0.0000 (0.6369)	MaskBCELoss 0.0000 (0.0798)	MaskDICELoss 0.0000 (0.5570)
Epoch: [1][334/500]	Time  7.127 ( 7.127)	Loss 1.0938 (2.0771)	CeLoss 1.0938 (0.2965)	SegCLSLoss 0.0000 (0.0273)	KLLoss 0.0000 (0.3525)	MaskLoss 0.0000 (0.8658)	MaskBCELoss 0.0000 (0.2106)	MaskDICELoss 0.0000 (0.6552)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 332 is less than current step: 499. Dropping entry: {'train/loss': 1.7381726622581481, 'train/ce_loss': 0.4189453125, 'train/seg_cls_loss': 0.017791748046875, 'train/kl_loss': 0.364453125, 'train/mask_bce_loss': 0.07982746809720993, 'train/mask_dice_loss': 0.557032297551632, 'train/mask_loss': 0.6368597716093063, 'metrics/total_secs_per_batch': 6.133791208267212, 'metrics/data_secs_per_batch': 2.8389010429382324, '_timestamp': 1740956090.397831}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 332 is less than current step: 499. Dropping entry: {'train/lr': 0.00020430612244897955, '_timestamp': 1740956090.3980339}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 333 is less than current step: 499. Dropping entry: {'train/loss': 2.077139937877655, 'train/ce_loss': 0.296533203125, 'train/seg_cls_loss': 0.027349853515625, 'train/kl_loss': 0.3525390625, 'train/mask_bce_loss': 0.21058780122548343, 'train/mask_dice_loss': 0.6551794171333313, 'train/mask_loss': 0.8657672286033631, 'metrics/total_secs_per_batch': 7.127157211303711, 'metrics/data_secs_per_batch': 3.233059597015381, '_timestamp': 1740956097.5249882}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 333 is less than current step: 499. Dropping entry: {'train/lr': 0.0002041836734693877, '_timestamp': 1740956097.5252726}).
Epoch: [1][335/500]	Time  6.240 ( 6.240)	Loss 2.8910 (1.9124)	CeLoss 0.1602 (0.4537)	SegCLSLoss 0.0422 (0.0179)	KLLoss 0.3789 (0.3164)	MaskLoss 1.3357 (0.7090)	MaskBCELoss 0.6449 (0.1403)	MaskDICELoss 0.6907 (0.5687)
Epoch: [1][336/500]	Time  6.614 ( 6.614)	Loss 0.6406 (2.0775)	CeLoss 0.6406 (0.3703)	SegCLSLoss 0.0000 (0.0190)	KLLoss 0.0000 (0.3164)	MaskLoss 0.0000 (0.8333)	MaskBCELoss 0.0000 (0.1803)	MaskDICELoss 0.0000 (0.6530)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 334 is less than current step: 499. Dropping entry: {'train/loss': 1.912366223335266, 'train/ce_loss': 0.4537109375, 'train/seg_cls_loss': 0.01785888671875, 'train/kl_loss': 0.31640625, 'train/mask_bce_loss': 0.14030499681830405, 'train/mask_dice_loss': 0.5686613112688065, 'train/mask_loss': 0.7089663147926331, 'metrics/total_secs_per_batch': 6.2403998374938965, 'metrics/data_secs_per_batch': 2.716139888763428, '_timestamp': 1740956103.765507}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 334 is less than current step: 499. Dropping entry: {'train/lr': 0.0002040612244897959, '_timestamp': 1740956103.7658787}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 335 is less than current step: 499. Dropping entry: {'train/loss': 2.0775156021118164, 'train/ce_loss': 0.3703125, 'train/seg_cls_loss': 0.019012451171875, 'train/kl_loss': 0.31640625, 'train/mask_bce_loss': 0.18026727065443993, 'train/mask_dice_loss': 0.6530217587947845, 'train/mask_loss': 0.8332890450954438, 'metrics/total_secs_per_batch': 6.613556385040283, 'metrics/data_secs_per_batch': 3.0591919898986815, '_timestamp': 1740956110.3789532}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 335 is less than current step: 499. Dropping entry: {'train/lr': 0.00020393877551020407, '_timestamp': 1740956110.379251}).
Epoch: [1][337/500]	Time  6.775 ( 6.775)	Loss 0.1387 (1.5405)	CeLoss 0.1387 (0.2913)	SegCLSLoss 0.0000 (0.0126)	KLLoss 0.0000 (0.2834)	MaskLoss 0.0000 (0.6074)	MaskBCELoss 0.0000 (0.1748)	MaskDICELoss 0.0000 (0.4326)
Epoch: [1][338/500]	Time  5.868 ( 5.868)	Loss 2.5387 (1.8436)	CeLoss 0.2227 (0.5126)	SegCLSLoss 0.0266 (0.0131)	KLLoss 0.3809 (0.2805)	MaskLoss 1.1326 (0.6483)	MaskBCELoss 0.3241 (0.1629)	MaskDICELoss 0.8085 (0.4853)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 336 is less than current step: 499. Dropping entry: {'train/loss': 1.5405080318450928, 'train/ce_loss': 0.29130859375, 'train/seg_cls_loss': 0.012591552734375, 'train/kl_loss': 0.2833984375, 'train/mask_bce_loss': 0.17476646937429904, 'train/mask_dice_loss': 0.43259693384170533, 'train/mask_loss': 0.6073633968830109, 'metrics/total_secs_per_batch': 6.775001287460327, 'metrics/data_secs_per_batch': 2.856596255302429, '_timestamp': 1740956117.1539533}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 336 is less than current step: 499. Dropping entry: {'train/lr': 0.00020381632653061223, '_timestamp': 1740956117.154238}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 337 is less than current step: 499. Dropping entry: {'train/loss': 1.8436126708984375, 'train/ce_loss': 0.51259765625, 'train/seg_cls_loss': 0.013134765625, 'train/kl_loss': 0.28046875, 'train/mask_bce_loss': 0.16293775700032712, 'train/mask_dice_loss': 0.48533342480659486, 'train/mask_loss': 0.6482711791992187, 'metrics/total_secs_per_batch': 5.867831230163574, 'metrics/data_secs_per_batch': 2.409654664993286, '_timestamp': 1740956123.0220487}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 337 is less than current step: 499. Dropping entry: {'train/lr': 0.0002036938775510204, '_timestamp': 1740956123.022409}).
Epoch: [1][339/500]	Time  7.487 ( 7.487)	Loss 1.8704 (2.0334)	CeLoss 0.2773 (0.2593)	SegCLSLoss 0.0148 (0.0216)	KLLoss 0.4258 (0.3541)	MaskLoss 0.7711 (0.8640)	MaskBCELoss 0.2457 (0.2143)	MaskDICELoss 0.5254 (0.6497)
[2025-03-02 16:55:37,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=840, skipped=0, lr=[0.00020351020408163265], mom=[(0.9, 0.95)]
[2025-03-02 16:55:37,109] [INFO] [timer.py:215:stop] epoch=0/micro_step=8400/global_step=840, RunningAvgSamplesPerSec=1.505689044465398, CurrSamplesPerSec=1.515248749757502, MemAllocated=31.25GB, MaxMemAllocated=36.86GB
Epoch: [1][340/500]	Time  6.601 ( 6.601)	Loss 2.5159 (1.6955)	CeLoss 0.2012 (0.3539)	SegCLSLoss 0.0183 (0.0135)	KLLoss 0.3926 (0.2742)	MaskLoss 1.1334 (0.6538)	MaskBCELoss 0.3932 (0.1543)	MaskDICELoss 0.7402 (0.4995)
Epoch: [1][341/500]	Time  4.845 ( 4.845)	Loss 1.0234 (1.3276)	CeLoss 1.0234 (0.7392)	SegCLSLoss 0.0000 (0.0072)	KLLoss 0.0000 (0.1187)	MaskLoss 0.0000 (0.2865)	MaskBCELoss 0.0000 (0.0321)	MaskDICELoss 0.0000 (0.2544)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 338 is less than current step: 499. Dropping entry: {'train/loss': 2.0333810567855837, 'train/ce_loss': 0.25927734375, 'train/seg_cls_loss': 0.02164306640625, 'train/kl_loss': 0.3541015625, 'train/mask_bce_loss': 0.21434191539883612, 'train/mask_dice_loss': 0.6496630311012268, 'train/mask_loss': 0.8640049457550049, 'metrics/total_secs_per_batch': 7.48672342300415, 'metrics/data_secs_per_batch': 3.4483638763427735, '_timestamp': 1740956130.5085459}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 338 is less than current step: 499. Dropping entry: {'train/lr': 0.00020357142857142856, '_timestamp': 1740956130.5087538}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 339 is less than current step: 499. Dropping entry: {'train/loss': 1.6954803943634034, 'train/ce_loss': 0.353857421875, 'train/seg_cls_loss': 0.013470458984375, 'train/kl_loss': 0.27421875, 'train/mask_bce_loss': 0.15431262832134962, 'train/mask_dice_loss': 0.4994578421115875, 'train/mask_loss': 0.6537704706192017, 'metrics/total_secs_per_batch': 6.601156949996948, 'metrics/data_secs_per_batch': 2.8038514852523804, '_timestamp': 1740956137.1094878}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 339 is less than current step: 499. Dropping entry: {'train/lr': 0.00020344897959183672, '_timestamp': 1740956137.1097775}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 340 is less than current step: 499. Dropping entry: {'train/loss': 1.3276182055473327, 'train/ce_loss': 0.73916015625, 'train/seg_cls_loss': 0.0072265625, 'train/kl_loss': 0.11875, 'train/mask_bce_loss': 0.032084790803492066, 'train/mask_dice_loss': 0.25442939400672915, 'train/mask_loss': 0.28651418089866637, 'metrics/total_secs_per_batch': 4.844583988189697, 'metrics/data_secs_per_batch': 2.1291656255722047, '_timestamp': 1740956141.954252}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 340 is less than current step: 499. Dropping entry: {'train/lr': 0.00020332653061224488, '_timestamp': 1740956141.954522}).
Epoch: [1][342/500]	Time  6.218 ( 6.218)	Loss 1.7690 (1.8249)	CeLoss 0.3340 (0.4524)	SegCLSLoss 0.0124 (0.0149)	KLLoss 0.4121 (0.3195)	MaskLoss 0.6941 (0.6667)	MaskBCELoss 0.1354 (0.1257)	MaskDICELoss 0.5587 (0.5409)
Epoch: [1][343/500]	Time  5.641 ( 5.641)	Loss 2.1432 (1.5425)	CeLoss 0.2100 (0.4812)	SegCLSLoss 0.0206 (0.0144)	KLLoss 0.3867 (0.2359)	MaskLoss 0.9417 (0.5152)	MaskBCELoss 0.2093 (0.1318)	MaskDICELoss 0.7325 (0.3834)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 341 is less than current step: 499. Dropping entry: {'train/loss': 1.8249196887016297, 'train/ce_loss': 0.45244140625, 'train/seg_cls_loss': 0.014898681640625, 'train/kl_loss': 0.31953125, 'train/mask_bce_loss': 0.12573987022042274, 'train/mask_dice_loss': 0.5409191951155663, 'train/mask_loss': 0.6666590631008148, 'metrics/total_secs_per_batch': 6.21820068359375, 'metrics/data_secs_per_batch': 2.4149347066879274, '_timestamp': 1740956148.172471}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 341 is less than current step: 499. Dropping entry: {'train/lr': 0.00020320408163265304, '_timestamp': 1740956148.1727521}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 342 is less than current step: 499. Dropping entry: {'train/loss': 1.5424774765968323, 'train/ce_loss': 0.48115234375, 'train/seg_cls_loss': 0.014385986328125, 'train/kl_loss': 0.2359375, 'train/mask_bce_loss': 0.13182590678334236, 'train/mask_dice_loss': 0.38335814625024794, 'train/mask_loss': 0.5151840567588806, 'metrics/total_secs_per_batch': 5.640809774398804, 'metrics/data_secs_per_batch': 2.467049264907837, '_timestamp': 1740956153.8133516}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 342 is less than current step: 499. Dropping entry: {'train/lr': 0.0002030816326530612, '_timestamp': 1740956153.813646}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 343 is less than current step: 499. Dropping entry: {'train/loss': 1.6572755336761475, 'train/ce_loss': 0.39853515625, 'train/seg_cls_loss': 0.01708984375, 'train/kl_loss': 0.275390625, 'train/mask_bce_loss': 0.13077786017674953, 'train/mask_dice_loss': 0.48042825907468795, 'train/mask_loss': 0.6112061262130737, 'metrics/total_secs_per_batch': 6.361177444458008, 'metrics/data_secs_per_batch': 2.705528664588928, '_timestamp': 1740956160.1746862}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 343 is less than current step: 499. Dropping entry: {'train/lr': 0.00020295918367346937, '_timestamp': 1740956160.1750462}).
Epoch: [1][344/500]	Time  6.361 ( 6.361)	Loss 0.1709 (1.6573)	CeLoss 0.1709 (0.3985)	SegCLSLoss 0.0000 (0.0171)	KLLoss 0.0000 (0.2754)	MaskLoss 0.0000 (0.6112)	MaskBCELoss 0.0000 (0.1308)	MaskDICELoss 0.0000 (0.4804)
Epoch: [1][345/500]	Time  5.664 ( 5.664)	Loss 0.8979 (1.7266)	CeLoss 0.2500 (0.3846)	SegCLSLoss 0.0194 (0.0190)	KLLoss 0.3848 (0.2697)	MaskLoss 0.2996 (0.6528)	MaskBCELoss 0.1871 (0.1746)	MaskDICELoss 0.1124 (0.4782)
Epoch: [1][346/500]	Time  7.266 ( 7.266)	Loss 1.7915 (1.8581)	CeLoss 0.2432 (0.2538)	SegCLSLoss 0.0240 (0.0205)	KLLoss 0.3730 (0.3547)	MaskLoss 0.7493 (0.7793)	MaskBCELoss 0.1089 (0.0966)	MaskDICELoss 0.6403 (0.6826)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 344 is less than current step: 499. Dropping entry: {'train/loss': 1.726603877544403, 'train/ce_loss': 0.3845703125, 'train/seg_cls_loss': 0.01900634765625, 'train/kl_loss': 0.2697265625, 'train/mask_bce_loss': 0.17460685651749372, 'train/mask_dice_loss': 0.4781970351934433, 'train/mask_loss': 0.6528038859367371, 'metrics/total_secs_per_batch': 5.664054870605469, 'metrics/data_secs_per_batch': 2.581070327758789, '_timestamp': 1740956165.8385682}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 344 is less than current step: 499. Dropping entry: {'train/lr': 0.00020283673469387753, '_timestamp': 1740956165.8389003}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 345 is less than current step: 499. Dropping entry: {'train/loss': 1.8580921173095704, 'train/ce_loss': 0.25380859375, 'train/seg_cls_loss': 0.0205078125, 'train/kl_loss': 0.3546875, 'train/mask_bce_loss': 0.09664026889950036, 'train/mask_dice_loss': 0.6826499342918396, 'train/mask_loss': 0.7792901933193207, 'metrics/total_secs_per_batch': 7.265953302383423, 'metrics/data_secs_per_batch': 3.2975316762924196, '_timestamp': 1740956173.1044621}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 345 is less than current step: 499. Dropping entry: {'train/lr': 0.0002027142857142857, '_timestamp': 1740956173.1047366}).
Epoch: [1][347/500]	Time  6.960 ( 6.960)	Loss 0.2041 (1.8373)	CeLoss 0.2041 (0.2271)	SegCLSLoss 0.0000 (0.0194)	KLLoss 0.0000 (0.3652)	MaskLoss 0.0000 (0.7821)	MaskBCELoss 0.0000 (0.1493)	MaskDICELoss 0.0000 (0.6328)
Epoch: [1][348/500]	Time  5.686 ( 5.686)	Loss 2.5502 (1.8437)	CeLoss 0.2969 (0.5016)	SegCLSLoss 0.0152 (0.0156)	KLLoss 0.3867 (0.2773)	MaskLoss 1.1032 (0.6532)	MaskBCELoss 0.3225 (0.1905)	MaskDICELoss 0.7807 (0.4627)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 346 is less than current step: 499. Dropping entry: {'train/loss': 1.837283766269684, 'train/ce_loss': 0.22705078125, 'train/seg_cls_loss': 0.0194091796875, 'train/kl_loss': 0.365234375, 'train/mask_bce_loss': 0.14931125678122042, 'train/mask_dice_loss': 0.6328071907162667, 'train/mask_loss': 0.7821184396743774, 'metrics/total_secs_per_batch': 6.959691047668457, 'metrics/data_secs_per_batch': 3.101984739303589, '_timestamp': 1740956180.0641844}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 346 is less than current step: 499. Dropping entry: {'train/lr': 0.00020259183673469386, '_timestamp': 1740956180.0644684}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 347 is less than current step: 499. Dropping entry: {'train/loss': 1.8436853647232057, 'train/ce_loss': 0.5015625, 'train/seg_cls_loss': 0.01563720703125, 'train/kl_loss': 0.27734375, 'train/mask_bce_loss': 0.1905160129070282, 'train/mask_dice_loss': 0.46267431378364565, 'train/mask_loss': 0.6531903266906738, 'metrics/total_secs_per_batch': 5.686178207397461, 'metrics/data_secs_per_batch': 2.2750060081481935, '_timestamp': 1740956185.7503974}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 347 is less than current step: 499. Dropping entry: {'train/lr': 0.00020246938775510202, '_timestamp': 1740956185.750675}).
Epoch: [1][349/500]	Time  6.568 ( 6.568)	Loss 2.2090 (1.9605)	CeLoss 0.2490 (0.4570)	SegCLSLoss 0.0140 (0.0156)	KLLoss 0.4277 (0.3266)	MaskLoss 0.9551 (0.7316)	MaskBCELoss 0.2644 (0.1720)	MaskDICELoss 0.6907 (0.5596)
[2025-03-02 16:56:38,152] [INFO] [logging.py:96:log_dist] [Rank 0] step=850, skipped=0, lr=[0.00020228571428571425], mom=[(0.9, 0.95)]
[2025-03-02 16:56:38,157] [INFO] [timer.py:215:stop] epoch=0/micro_step=8500/global_step=850, RunningAvgSamplesPerSec=1.5071283547540317, CurrSamplesPerSec=1.7129237591401894, MemAllocated=30.71GB, MaxMemAllocated=37.19GB
Epoch: [1][350/500]	Time  5.840 ( 5.840)	Loss 1.0859 (1.6445)	CeLoss 1.0859 (0.7331)	SegCLSLoss 0.0000 (0.0090)	KLLoss 0.0000 (0.2000)	MaskLoss 0.0000 (0.4434)	MaskBCELoss 0.0000 (0.1064)	MaskDICELoss 0.0000 (0.3369)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 348 is less than current step: 499. Dropping entry: {'train/loss': 1.9605410814285278, 'train/ce_loss': 0.45703125, 'train/seg_cls_loss': 0.01558837890625, 'train/kl_loss': 0.3265625, 'train/mask_bce_loss': 0.17202136293053627, 'train/mask_dice_loss': 0.5596163719892502, 'train/mask_loss': 0.7316377341747284, 'metrics/total_secs_per_batch': 6.568380355834961, 'metrics/data_secs_per_batch': 2.6150328397750853, '_timestamp': 1740956192.3187332}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 348 is less than current step: 499. Dropping entry: {'train/lr': 0.00020234693877551019, '_timestamp': 1740956192.3189216}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 349 is less than current step: 499. Dropping entry: {'train/loss': 1.6445218086242677, 'train/ce_loss': 0.73310546875, 'train/seg_cls_loss': 0.009027099609375, 'train/kl_loss': 0.2, 'train/mask_bce_loss': 0.10643096938729286, 'train/mask_dice_loss': 0.336923685669899, 'train/mask_loss': 0.4433546602725983, 'metrics/total_secs_per_batch': 5.83953595161438, 'metrics/data_secs_per_batch': 2.894307327270508, '_timestamp': 1740956198.1581821}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 349 is less than current step: 499. Dropping entry: {'train/lr': 0.00020222448979591835, '_timestamp': 1740956198.158534}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 350 is less than current step: 499. Dropping entry: {'train/loss': 2.1468547582626343, 'train/ce_loss': 0.34697265625, 'train/seg_cls_loss': 0.019677734375, 'train/kl_loss': 0.3703125, 'train/mask_bce_loss': 0.2581133238971233, 'train/mask_dice_loss': 0.6182925760746002, 'train/mask_loss': 0.8764059126377106, 'metrics/total_secs_per_batch': 6.3279032707214355, 'metrics/data_secs_per_batch': 2.5854116678237915, '_timestamp': 1740956204.4862425}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 350 is less than current step: 499. Dropping entry: {'train/lr': 0.0002021020408163265, '_timestamp': 1740956204.4865503}).
Epoch: [1][351/500]	Time  6.328 ( 6.328)	Loss 2.4575 (2.1469)	CeLoss 0.2217 (0.3470)	SegCLSLoss 0.0168 (0.0197)	KLLoss 0.4121 (0.3703)	MaskLoss 1.0930 (0.8764)	MaskBCELoss 0.1211 (0.2581)	MaskDICELoss 0.9719 (0.6183)
Epoch: [1][352/500]	Time  5.598 ( 5.598)	Loss 2.2970 (1.9917)	CeLoss 0.1875 (0.4759)	SegCLSLoss 0.0248 (0.0165)	KLLoss 0.4141 (0.3275)	MaskLoss 1.0274 (0.7373)	MaskBCELoss 0.0328 (0.1036)	MaskDICELoss 0.9946 (0.6337)
Epoch: [1][353/500]	Time  5.968 ( 5.968)	Loss 1.2792 (1.6805)	CeLoss 0.2148 (0.3548)	SegCLSLoss 0.0247 (0.0162)	KLLoss 0.4258 (0.3301)	MaskLoss 0.5048 (0.6423)	MaskBCELoss 0.0386 (0.1434)	MaskDICELoss 0.4662 (0.4989)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 351 is less than current step: 499. Dropping entry: {'train/loss': 1.9917462825775147, 'train/ce_loss': 0.47587890625, 'train/seg_cls_loss': 0.01650390625, 'train/kl_loss': 0.3275390625, 'train/mask_bce_loss': 0.10355002246797085, 'train/mask_dice_loss': 0.6337293684482574, 'train/mask_loss': 0.7372793972492218, 'metrics/total_secs_per_batch': 5.598155975341797, 'metrics/data_secs_per_batch': 2.815688967704773, '_timestamp': 1740956210.084367}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 351 is less than current step: 499. Dropping entry: {'train/lr': 0.00020197959183673468, '_timestamp': 1740956210.0846586}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 352 is less than current step: 499. Dropping entry: {'train/loss': 1.680478435754776, 'train/ce_loss': 0.354833984375, 'train/seg_cls_loss': 0.016241455078125, 'train/kl_loss': 0.330078125, 'train/mask_bce_loss': 0.14336005300283433, 'train/mask_dice_loss': 0.49892994463443757, 'train/mask_loss': 0.6422899983823299, 'metrics/total_secs_per_batch': 5.967896938323975, 'metrics/data_secs_per_batch': 2.6743648290634154, '_timestamp': 1740956216.0525956}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 352 is less than current step: 499. Dropping entry: {'train/lr': 0.00020185714285714284, '_timestamp': 1740956216.0529714}).
Epoch: [1][354/500]	Time  6.033 ( 6.033)	Loss 1.9901 (1.9538)	CeLoss 0.2432 (0.2898)	SegCLSLoss 0.0212 (0.0213)	KLLoss 0.4258 (0.3689)	MaskLoss 0.8466 (0.8084)	MaskBCELoss 0.0852 (0.1714)	MaskDICELoss 0.7614 (0.6370)
Epoch: [1][355/500]	Time  5.818 ( 5.818)	Loss 0.5039 (1.6478)	CeLoss 0.5039 (0.4783)	SegCLSLoss 0.0000 (0.0122)	KLLoss 0.0000 (0.2424)	MaskLoss 0.0000 (0.5697)	MaskBCELoss 0.0000 (0.1348)	MaskDICELoss 0.0000 (0.4348)
Epoch: [1][356/500]	Time  4.997 ( 4.997)	Loss 1.6626 (1.5783)	CeLoss 0.2891 (0.4940)	SegCLSLoss 0.0131 (0.0130)	KLLoss 0.4180 (0.2441)	MaskLoss 0.6633 (0.5267)	MaskBCELoss 0.0768 (0.0876)	MaskDICELoss 0.5866 (0.4390)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 353 is less than current step: 499. Dropping entry: {'train/loss': 1.9538396358489991, 'train/ce_loss': 0.28984375, 'train/seg_cls_loss': 0.02125244140625, 'train/kl_loss': 0.3689453125, 'train/mask_bce_loss': 0.17138871792703866, 'train/mask_dice_loss': 0.6369764015078545, 'train/mask_loss': 0.8083651155233383, 'metrics/total_secs_per_batch': 6.032575368881226, 'metrics/data_secs_per_batch': 2.786147379875183, '_timestamp': 1740956222.084864}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 353 is less than current step: 499. Dropping entry: {'train/lr': 0.000201734693877551, '_timestamp': 1740956222.085171}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 354 is less than current step: 499. Dropping entry: {'train/loss': 1.6477978825569153, 'train/ce_loss': 0.4783203125, 'train/seg_cls_loss': 0.012237548828125, 'train/kl_loss': 0.2423828125, 'train/mask_bce_loss': 0.13484986908733845, 'train/mask_dice_loss': 0.43484984934329984, 'train/mask_loss': 0.5696997165679931, 'metrics/total_secs_per_batch': 5.81791877746582, 'metrics/data_secs_per_batch': 3.0052419662475587, '_timestamp': 1740956227.902774}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 354 is less than current step: 499. Dropping entry: {'train/lr': 0.00020161224489795916, '_timestamp': 1740956227.90307}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 355 is less than current step: 499. Dropping entry: {'train/loss': 1.578274142742157, 'train/ce_loss': 0.49404296875, 'train/seg_cls_loss': 0.0130126953125, 'train/kl_loss': 0.244140625, 'train/mask_bce_loss': 0.08764873668551446, 'train/mask_dice_loss': 0.4390371680259705, 'train/mask_loss': 0.5266859114170075, 'metrics/total_secs_per_batch': 4.997093439102173, 'metrics/data_secs_per_batch': 2.077796149253845, '_timestamp': 1740956232.9001935}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 355 is less than current step: 499. Dropping entry: {'train/lr': 0.0002014897959183673, '_timestamp': 1740956232.9005625}).
Epoch: [1][357/500]	Time  6.712 ( 6.712)	Loss 2.1837 (1.9300)	CeLoss 0.2393 (0.2699)	SegCLSLoss 0.0209 (0.0182)	KLLoss 0.3965 (0.3701)	MaskLoss 0.9473 (0.8070)	MaskBCELoss 0.1738 (0.1578)	MaskDICELoss 0.7735 (0.6492)
Epoch: [1][358/500]	Time  5.499 ( 5.499)	Loss 0.7539 (2.0827)	CeLoss 0.7539 (0.5400)	SegCLSLoss 0.0000 (0.0131)	KLLoss 0.0000 (0.2818)	MaskLoss 0.0000 (0.7539)	MaskBCELoss 0.0000 (0.3016)	MaskDICELoss 0.0000 (0.4523)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 356 is less than current step: 499. Dropping entry: {'train/loss': 1.9299714684486389, 'train/ce_loss': 0.269921875, 'train/seg_cls_loss': 0.01820068359375, 'train/kl_loss': 0.3701171875, 'train/mask_bce_loss': 0.15778539124876262, 'train/mask_dice_loss': 0.6491925358772278, 'train/mask_loss': 0.8069779336452484, 'metrics/total_secs_per_batch': 6.711635589599609, 'metrics/data_secs_per_batch': 3.2503994941711425, '_timestamp': 1740956239.6115217}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 356 is less than current step: 499. Dropping entry: {'train/lr': 0.00020136734693877552, '_timestamp': 1740956239.611886}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 357 is less than current step: 499. Dropping entry: {'train/loss': 2.0826639890670777, 'train/ce_loss': 0.5400390625, 'train/seg_cls_loss': 0.013134765625, 'train/kl_loss': 0.2818359375, 'train/mask_bce_loss': 0.30158560872077944, 'train/mask_dice_loss': 0.45229520797729494, 'train/mask_loss': 0.7538808166980744, 'metrics/total_secs_per_batch': 5.499122142791748, 'metrics/data_secs_per_batch': 2.5146753072738646, '_timestamp': 1740956245.1107497}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 357 is less than current step: 499. Dropping entry: {'train/lr': 0.00020124489795918368, '_timestamp': 1740956245.1110978}).
Epoch: [1][359/500]	Time  6.645 ( 6.645)	Loss 2.2503 (2.0005)	CeLoss 0.3203 (0.3219)	SegCLSLoss 0.0151 (0.0175)	KLLoss 0.4121 (0.3635)	MaskLoss 0.9406 (0.8167)	MaskBCELoss 0.0982 (0.0990)	MaskDICELoss 0.8423 (0.7178)
[2025-03-02 16:57:39,652] [INFO] [logging.py:96:log_dist] [Rank 0] step=860, skipped=0, lr=[0.0002010612244897959], mom=[(0.9, 0.95)]
[2025-03-02 16:57:39,657] [INFO] [timer.py:215:stop] epoch=0/micro_step=8600/global_step=860, RunningAvgSamplesPerSec=1.5084174780870774, CurrSamplesPerSec=1.265758746244871, MemAllocated=31.27GB, MaxMemAllocated=37.19GB
Epoch: [1][360/500]	Time  7.902 ( 7.902)	Loss 2.1115 (1.8958)	CeLoss 0.1582 (0.1925)	SegCLSLoss 0.0330 (0.0225)	KLLoss 0.3984 (0.3605)	MaskLoss 0.9483 (0.8280)	MaskBCELoss 0.0134 (0.1198)	MaskDICELoss 0.9349 (0.7082)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 358 is less than current step: 499. Dropping entry: {'train/loss': 2.00053733587265, 'train/ce_loss': 0.321875, 'train/seg_cls_loss': 0.017486572265625, 'train/kl_loss': 0.3634765625, 'train/mask_bce_loss': 0.09895583111792802, 'train/mask_dice_loss': 0.7177679151296615, 'train/mask_loss': 0.816723746061325, 'metrics/total_secs_per_batch': 6.645243883132935, 'metrics/data_secs_per_batch': 3.048555874824524, '_timestamp': 1740956251.7560124}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 358 is less than current step: 499. Dropping entry: {'train/lr': 0.00020112244897959184, '_timestamp': 1740956251.7563374}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 359 is less than current step: 499. Dropping entry: {'train/loss': 1.89575297832489, 'train/ce_loss': 0.192529296875, 'train/seg_cls_loss': 0.022454833984375, 'train/kl_loss': 0.360546875, 'train/mask_bce_loss': 0.11984838098287583, 'train/mask_dice_loss': 0.7081550478935241, 'train/mask_loss': 0.8280034303665161, 'metrics/total_secs_per_batch': 7.902212858200073, 'metrics/data_secs_per_batch': 3.2708082675933836, '_timestamp': 1740956259.657977}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 359 is less than current step: 499. Dropping entry: {'train/lr': 0.000201, '_timestamp': 1740956259.658303}).
Epoch: [1][361/500]	Time  5.689 ( 5.689)	Loss 1.9609 (1.9128)	CeLoss 1.9609 (0.6560)	SegCLSLoss 0.0000 (0.0189)	KLLoss 0.0000 (0.2391)	MaskLoss 0.0000 (0.6117)	MaskBCELoss 0.0000 (0.1653)	MaskDICELoss 0.0000 (0.4464)
Epoch: [1][362/500]	Time  6.956 ( 6.956)	Loss 0.0569 (1.8784)	CeLoss 0.0569 (0.3307)	SegCLSLoss 0.0000 (0.0173)	KLLoss 0.0000 (0.3129)	MaskLoss 0.0000 (0.7537)	MaskBCELoss 0.0000 (0.1848)	MaskDICELoss 0.0000 (0.5689)
Epoch: [1][363/500]	Time  4.675 ( 4.675)	Loss 0.8047 (1.4592)	CeLoss 0.8047 (0.7103)	SegCLSLoss 0.0000 (0.0089)	KLLoss 0.0000 (0.1584)	MaskLoss 0.0000 (0.3643)	MaskBCELoss 0.0000 (0.1045)	MaskDICELoss 0.0000 (0.2597)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 360 is less than current step: 499. Dropping entry: {'train/loss': 1.9127575874328613, 'train/ce_loss': 0.65595703125, 'train/seg_cls_loss': 0.018902587890625, 'train/kl_loss': 0.2390625, 'train/mask_bce_loss': 0.16525243036448956, 'train/mask_dice_loss': 0.44639979004859925, 'train/mask_loss': 0.6116522312164306, 'metrics/total_secs_per_batch': 5.689423322677612, 'metrics/data_secs_per_batch': 2.5913525581359864, '_timestamp': 1740956265.3475285}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 360 is less than current step: 499. Dropping entry: {'train/lr': 0.00020087755102040814, '_timestamp': 1740956265.3478205}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 361 is less than current step: 499. Dropping entry: {'train/loss': 1.8783761739730835, 'train/ce_loss': 0.3306884765625, 'train/seg_cls_loss': 0.017315673828125, 'train/kl_loss': 0.312890625, 'train/mask_bce_loss': 0.18481064029037952, 'train/mask_dice_loss': 0.5688671827316284, 'train/mask_loss': 0.7536778330802918, 'metrics/total_secs_per_batch': 6.95606803894043, 'metrics/data_secs_per_batch': 2.891841411590576, '_timestamp': 1740956272.3036513}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 361 is less than current step: 499. Dropping entry: {'train/lr': 0.0002007551020408163, '_timestamp': 1740956272.3038619}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 362 is less than current step: 499. Dropping entry: {'train/loss': 1.4591910004615785, 'train/ce_loss': 0.71025390625, 'train/seg_cls_loss': 0.00889892578125, 'train/kl_loss': 0.1583984375, 'train/mask_bce_loss': 0.10451887100934983, 'train/mask_dice_loss': 0.2597445845603943, 'train/mask_loss': 0.3642634630203247, 'metrics/total_secs_per_batch': 4.674618482589722, 'metrics/data_secs_per_batch': 1.9437789440155029, '_timestamp': 1740956276.9783626}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 362 is less than current step: 499. Dropping entry: {'train/lr': 0.00020063265306122447, '_timestamp': 1740956276.978675}).
Epoch: [1][364/500]	Time  6.273 ( 6.273)	Loss 1.1427 (1.8384)	CeLoss 0.2383 (0.3647)	SegCLSLoss 0.0134 (0.0183)	KLLoss 0.4121 (0.3600)	MaskLoss 0.4278 (0.7142)	MaskBCELoss 0.1169 (0.1247)	MaskDICELoss 0.3109 (0.5894)
Epoch: [1][365/500]	Time  6.070 ( 6.070)	Loss 1.8597 (1.8769)	CeLoss 0.2266 (0.4152)	SegCLSLoss 0.0186 (0.0155)	KLLoss 0.3984 (0.3191)	MaskLoss 0.7922 (0.7111)	MaskBCELoss 0.0479 (0.0789)	MaskDICELoss 0.7443 (0.6322)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 363 is less than current step: 499. Dropping entry: {'train/loss': 1.838375198841095, 'train/ce_loss': 0.36474609375, 'train/seg_cls_loss': 0.018292236328125, 'train/kl_loss': 0.3599609375, 'train/mask_bce_loss': 0.12473126854747534, 'train/mask_dice_loss': 0.5894270539283752, 'train/mask_loss': 0.7141583144664765, 'metrics/total_secs_per_batch': 6.273012161254883, 'metrics/data_secs_per_batch': 2.5630104780197143, '_timestamp': 1740956283.2513468}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 363 is less than current step: 499. Dropping entry: {'train/lr': 0.00020051020408163263, '_timestamp': 1740956283.2517183}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 364 is less than current step: 499. Dropping entry: {'train/loss': 1.8768644034862518, 'train/ce_loss': 0.415234375, 'train/seg_cls_loss': 0.015509033203125, 'train/kl_loss': 0.319140625, 'train/mask_bce_loss': 0.07890408635139465, 'train/mask_dice_loss': 0.6321843519806862, 'train/mask_loss': 0.7110884398221969, 'metrics/total_secs_per_batch': 6.070115566253662, 'metrics/data_secs_per_batch': 2.341577744483948, '_timestamp': 1740956289.3213766}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 364 is less than current step: 499. Dropping entry: {'train/lr': 0.0002003877551020408, '_timestamp': 1740956289.321663}).
Epoch: [1][366/500]	Time  6.353 ( 6.353)	Loss 3.4133 (2.0465)	CeLoss 0.4043 (0.4813)	SegCLSLoss 0.0146 (0.0181)	KLLoss 0.4062 (0.3150)	MaskLoss 1.4801 (0.7623)	MaskBCELoss 0.9957 (0.1674)	MaskDICELoss 0.4844 (0.5949)
Epoch: [1][367/500]	Time  6.060 ( 6.060)	Loss 0.3750 (1.7971)	CeLoss 0.3750 (0.3844)	SegCLSLoss 0.0000 (0.0188)	KLLoss 0.0000 (0.2674)	MaskLoss 0.0000 (0.6883)	MaskBCELoss 0.0000 (0.1562)	MaskDICELoss 0.0000 (0.5322)
Epoch: [1][368/500]	Time  6.954 ( 6.954)	Loss 2.2568 (2.2011)	CeLoss 0.2578 (0.3700)	SegCLSLoss 0.0173 (0.0193)	KLLoss 0.3906 (0.3512)	MaskLoss 0.9761 (0.8933)	MaskBCELoss 0.2196 (0.1975)	MaskDICELoss 0.7565 (0.6959)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 365 is less than current step: 499. Dropping entry: {'train/loss': 2.046515429019928, 'train/ce_loss': 0.48134765625, 'train/seg_cls_loss': 0.018060302734375, 'train/kl_loss': 0.3150390625, 'train/mask_bce_loss': 0.16739053782075644, 'train/mask_dice_loss': 0.5949296891689301, 'train/mask_loss': 0.7623202204704285, 'metrics/total_secs_per_batch': 6.353483438491821, 'metrics/data_secs_per_batch': 2.541786861419678, '_timestamp': 1740956295.6749685}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 365 is less than current step: 499. Dropping entry: {'train/lr': 0.00020026530612244896, '_timestamp': 1740956295.6753325}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 366 is less than current step: 499. Dropping entry: {'train/loss': 1.797073495388031, 'train/ce_loss': 0.384375, 'train/seg_cls_loss': 0.0187744140625, 'train/kl_loss': 0.2673828125, 'train/mask_bce_loss': 0.1561515523120761, 'train/mask_dice_loss': 0.5321801185607911, 'train/mask_loss': 0.6883316695690155, 'metrics/total_secs_per_batch': 6.059753894805908, 'metrics/data_secs_per_batch': 2.79670045375824, '_timestamp': 1740956301.734625}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 366 is less than current step: 499. Dropping entry: {'train/lr': 0.00020014285714285712, '_timestamp': 1740956301.7349105}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 367 is less than current step: 499. Dropping entry: {'train/loss': 2.2010763764381407, 'train/ce_loss': 0.37001953125, 'train/seg_cls_loss': 0.019317626953125, 'train/kl_loss': 0.351171875, 'train/mask_bce_loss': 0.19745556749403476, 'train/mask_dice_loss': 0.6958560466766357, 'train/mask_loss': 0.893311619758606, 'metrics/total_secs_per_batch': 6.9539430141448975, 'metrics/data_secs_per_batch': 3.0856099128723145, '_timestamp': 1740956308.6885731}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 367 is less than current step: 499. Dropping entry: {'train/lr': 0.00020002040816326528, '_timestamp': 1740956308.688877}).
Epoch: [1][369/500]	Time  5.881 ( 5.881)	Loss 2.4277 (1.8800)	CeLoss 0.1807 (0.3566)	SegCLSLoss 0.0245 (0.0173)	KLLoss 0.3867 (0.3111)	MaskLoss 1.0976 (0.7417)	MaskBCELoss 0.3010 (0.1638)	MaskDICELoss 0.7967 (0.5779)
[2025-03-02 16:58:41,136] [INFO] [logging.py:96:log_dist] [Rank 0] step=870, skipped=0, lr=[0.00019983673469387754], mom=[(0.9, 0.95)]
[2025-03-02 16:58:41,141] [INFO] [timer.py:215:stop] epoch=0/micro_step=8700/global_step=870, RunningAvgSamplesPerSec=1.509682937459016, CurrSamplesPerSec=1.5217308016044861, MemAllocated=31.45GB, MaxMemAllocated=37.19GB
Epoch: [1][370/500]	Time  6.573 ( 6.573)	Loss 1.9607 (2.0192)	CeLoss 0.2559 (0.3897)	SegCLSLoss 0.0187 (0.0156)	KLLoss 0.4004 (0.3174)	MaskLoss 0.8280 (0.7948)	MaskBCELoss 0.0286 (0.1534)	MaskDICELoss 0.7994 (0.6414)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 368 is less than current step: 499. Dropping entry: {'train/loss': 1.8799691796302795, 'train/ce_loss': 0.356640625, 'train/seg_cls_loss': 0.017279052734375, 'train/kl_loss': 0.3111328125, 'train/mask_bce_loss': 0.16380463019013405, 'train/mask_dice_loss': 0.5778889566659927, 'train/mask_loss': 0.7416935801506043, 'metrics/total_secs_per_batch': 5.880540370941162, 'metrics/data_secs_per_batch': 2.8979448556900023, '_timestamp': 1740956314.5691493}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 368 is less than current step: 499. Dropping entry: {'train/lr': 0.00019989795918367345, '_timestamp': 1740956314.569353}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 369 is less than current step: 499. Dropping entry: {'train/loss': 2.019151473045349, 'train/ce_loss': 0.38974609375, 'train/seg_cls_loss': 0.01556396484375, 'train/kl_loss': 0.3173828125, 'train/mask_bce_loss': 0.15340948048979045, 'train/mask_dice_loss': 0.6414201498031616, 'train/mask_loss': 0.7948296368122101, 'metrics/total_secs_per_batch': 6.5731425285339355, 'metrics/data_secs_per_batch': 2.9668744087219237, '_timestamp': 1740956321.142118}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 369 is less than current step: 499. Dropping entry: {'train/lr': 0.0001997755102040816, '_timestamp': 1740956321.1424549}).
Epoch: [1][371/500]	Time  6.776 ( 6.776)	Loss 1.6511 (2.0473)	CeLoss 0.3887 (0.3043)	SegCLSLoss 0.0141 (0.0279)	KLLoss 0.4004 (0.3537)	MaskLoss 0.6068 (0.8467)	MaskBCELoss 0.2006 (0.1327)	MaskDICELoss 0.4062 (0.7140)
Epoch: [1][372/500]	Time  7.182 ( 7.182)	Loss 1.7146 (1.8759)	CeLoss 0.2969 (0.3522)	SegCLSLoss 0.0231 (0.0170)	KLLoss 0.3984 (0.3586)	MaskLoss 0.6835 (0.7397)	MaskBCELoss 0.2873 (0.1596)	MaskDICELoss 0.3962 (0.5800)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 370 is less than current step: 499. Dropping entry: {'train/loss': 2.0473044395446776, 'train/ce_loss': 0.304345703125, 'train/seg_cls_loss': 0.027947998046875, 'train/kl_loss': 0.3537109375, 'train/mask_bce_loss': 0.13265966633334755, 'train/mask_dice_loss': 0.714039421081543, 'train/mask_loss': 0.8466990947723388, 'metrics/total_secs_per_batch': 6.776265859603882, 'metrics/data_secs_per_batch': 3.0015942573547365, '_timestamp': 1740956327.9185514}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 370 is less than current step: 499. Dropping entry: {'train/lr': 0.00019965306122448977, '_timestamp': 1740956327.9188626}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 371 is less than current step: 499. Dropping entry: {'train/loss': 1.8759028911590576, 'train/ce_loss': 0.35224609375, 'train/seg_cls_loss': 0.017034912109375, 'train/kl_loss': 0.35859375, 'train/mask_bce_loss': 0.159644478559494, 'train/mask_dice_loss': 0.5800159454345704, 'train/mask_loss': 0.7396604239940643, 'metrics/total_secs_per_batch': 7.181520700454712, 'metrics/data_secs_per_batch': 3.217113494873047, '_timestamp': 1740956335.100052}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 371 is less than current step: 499. Dropping entry: {'train/lr': 0.00019953061224489794, '_timestamp': 1740956335.1003706}).
Epoch: [1][373/500]	Time  6.375 ( 6.375)	Loss 2.1113 (1.8160)	CeLoss 0.2256 (0.4556)	SegCLSLoss 0.0139 (0.0130)	KLLoss 0.4062 (0.3240)	MaskLoss 0.9189 (0.6608)	MaskBCELoss 0.1400 (0.1211)	MaskDICELoss 0.7789 (0.5396)
Epoch: [1][374/500]	Time  7.377 ( 7.377)	Loss 1.8357 (2.2651)	CeLoss 0.2969 (0.2229)	SegCLSLoss 0.0128 (0.0207)	KLLoss 0.4121 (0.3988)	MaskLoss 0.7460 (0.9961)	MaskBCELoss 0.2847 (0.2097)	MaskDICELoss 0.4612 (0.7864)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 372 is less than current step: 499. Dropping entry: {'train/loss': 1.8159668922424317, 'train/ce_loss': 0.45556640625, 'train/seg_cls_loss': 0.01298828125, 'train/kl_loss': 0.3240234375, 'train/mask_bce_loss': 0.12113780118525028, 'train/mask_dice_loss': 0.5396288514137269, 'train/mask_loss': 0.6607666462659836, 'metrics/total_secs_per_batch': 6.37526273727417, 'metrics/data_secs_per_batch': 2.855451726913452, '_timestamp': 1740956341.4753418}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 372 is less than current step: 499. Dropping entry: {'train/lr': 0.0001994081632653061, '_timestamp': 1740956341.4756365}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 373 is less than current step: 499. Dropping entry: {'train/loss': 2.2651358127593992, 'train/ce_loss': 0.22294921875, 'train/seg_cls_loss': 0.020697021484375, 'train/kl_loss': 0.398828125, 'train/mask_bce_loss': 0.20972972828894854, 'train/mask_dice_loss': 0.78636354804039, 'train/mask_loss': 0.9960932791233063, 'metrics/total_secs_per_batch': 7.377321004867554, 'metrics/data_secs_per_batch': 3.2843379020690917, '_timestamp': 1740956348.8529096}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 373 is less than current step: 499. Dropping entry: {'train/lr': 0.00019928571428571426, '_timestamp': 1740956348.8532994}).
Epoch: [1][375/500]	Time  6.953 ( 6.953)	Loss 2.1724 (1.8193)	CeLoss 0.2500 (0.2283)	SegCLSLoss 0.0154 (0.0201)	KLLoss 0.4160 (0.4010)	MaskLoss 0.9368 (0.7704)	MaskBCELoss 0.4086 (0.1866)	MaskDICELoss 0.5282 (0.5838)
Epoch: [1][376/500]	Time  6.805 ( 6.805)	Loss 1.4609 (1.8223)	CeLoss 1.4609 (0.4694)	SegCLSLoss 0.0000 (0.0148)	KLLoss 0.0000 (0.2771)	MaskLoss 0.0000 (0.6588)	MaskBCELoss 0.0000 (0.0847)	MaskDICELoss 0.0000 (0.5741)
Epoch: [1][377/500]	Time  5.712 ( 5.712)	Loss 2.3394 (1.9985)	CeLoss 0.2402 (0.4415)	SegCLSLoss 0.0262 (0.0173)	KLLoss 0.4062 (0.3217)	MaskLoss 1.0222 (0.7580)	MaskBCELoss 0.0553 (0.1226)	MaskDICELoss 0.9670 (0.6354)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 374 is less than current step: 499. Dropping entry: {'train/loss': 1.819299304485321, 'train/ce_loss': 0.2283203125, 'train/seg_cls_loss': 0.020098876953125, 'train/kl_loss': 0.4009765625, 'train/mask_bce_loss': 0.18663544356822967, 'train/mask_dice_loss': 0.5838052205741405, 'train/mask_loss': 0.7704406559467316, 'metrics/total_secs_per_batch': 6.9526894092559814, 'metrics/data_secs_per_batch': 3.254921054840088, '_timestamp': 1740956355.805305}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 374 is less than current step: 499. Dropping entry: {'train/lr': 0.00019916326530612243, '_timestamp': 1740956355.8056037}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 375 is less than current step: 499. Dropping entry: {'train/loss': 1.8223398447036743, 'train/ce_loss': 0.46943359375, 'train/seg_cls_loss': 0.014825439453125, 'train/kl_loss': 0.2771484375, 'train/mask_bce_loss': 0.0846808503381908, 'train/mask_dice_loss': 0.5741453111171723, 'train/mask_loss': 0.6588261723518372, 'metrics/total_secs_per_batch': 6.804692268371582, 'metrics/data_secs_per_batch': 3.1675192594528196, '_timestamp': 1740956362.6100302}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 375 is less than current step: 499. Dropping entry: {'train/lr': 0.0001990408163265306, '_timestamp': 1740956362.6103246}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 376 is less than current step: 499. Dropping entry: {'train/loss': 1.9984825015068055, 'train/ce_loss': 0.44150390625, 'train/seg_cls_loss': 0.017291259765625, 'train/kl_loss': 0.3216796875, 'train/mask_bce_loss': 0.12258218694478273, 'train/mask_dice_loss': 0.6353993058204651, 'train/mask_loss': 0.7579815089702606, 'metrics/total_secs_per_batch': 5.711645841598511, 'metrics/data_secs_per_batch': 2.562563991546631, '_timestamp': 1740956368.3217368}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 376 is less than current step: 499. Dropping entry: {'train/lr': 0.00019891836734693875, '_timestamp': 1740956368.322118}).
Epoch: [1][378/500]	Time  6.069 ( 6.069)	Loss 0.0771 (1.5728)	CeLoss 0.0771 (0.3245)	SegCLSLoss 0.0000 (0.0147)	KLLoss 0.0000 (0.2785)	MaskLoss 0.0000 (0.6065)	MaskBCELoss 0.0000 (0.2061)	MaskDICELoss 0.0000 (0.4005)
Epoch: [1][379/500]	Time  5.643 ( 5.643)	Loss 1.8733 (2.0954)	CeLoss 0.1953 (0.3280)	SegCLSLoss 0.0248 (0.0198)	KLLoss 0.3848 (0.3566)	MaskLoss 0.8136 (0.8610)	MaskBCELoss 0.0316 (0.2270)	MaskDICELoss 0.7820 (0.6340)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 377 is less than current step: 499. Dropping entry: {'train/loss': 1.572766160964966, 'train/ce_loss': 0.32451171875, 'train/seg_cls_loss': 0.01474609375, 'train/kl_loss': 0.278515625, 'train/mask_bce_loss': 0.20609825365245343, 'train/mask_dice_loss': 0.40045082867145537, 'train/mask_loss': 0.6065490961074829, 'metrics/total_secs_per_batch': 6.068964958190918, 'metrics/data_secs_per_batch': 2.7396544218063354, '_timestamp': 1740956374.3906975}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 377 is less than current step: 499. Dropping entry: {'train/lr': 0.00019879591836734691, '_timestamp': 1740956374.3909845}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 378 is less than current step: 499. Dropping entry: {'train/loss': 2.095417356491089, 'train/ce_loss': 0.32802734375, 'train/seg_cls_loss': 0.019775390625, 'train/kl_loss': 0.356640625, 'train/mask_bce_loss': 0.22702302932739257, 'train/mask_dice_loss': 0.6339669108390809, 'train/mask_loss': 0.8609899282455444, 'metrics/total_secs_per_batch': 5.64325213432312, 'metrics/data_secs_per_batch': 2.7080846786499024, '_timestamp': 1740956380.0339193}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 378 is less than current step: 499. Dropping entry: {'train/lr': 0.00019867346938775508, '_timestamp': 1740956380.034273}).
[2025-03-02 16:59:46,797] [INFO] [logging.py:96:log_dist] [Rank 0] step=880, skipped=0, lr=[0.00019861224489795915], mom=[(0.9, 0.95)]
[2025-03-02 16:59:46,802] [INFO] [timer.py:215:stop] epoch=0/micro_step=8800/global_step=880, RunningAvgSamplesPerSec=1.5098363017339673, CurrSamplesPerSec=1.4775413436760776, MemAllocated=31.11GB, MaxMemAllocated=37.19GB
Epoch: [1][380/500]	Time  6.770 ( 6.770)	Loss 1.1989 (1.9747)	CeLoss 0.2734 (0.4350)	SegCLSLoss 0.0255 (0.0164)	KLLoss 0.3984 (0.3176)	MaskLoss 0.4364 (0.7499)	MaskBCELoss 0.1362 (0.1150)	MaskDICELoss 0.3002 (0.6349)
Epoch: [1][381/500]	Time  6.844 ( 6.844)	Loss 2.3994 (1.7581)	CeLoss 0.2188 (0.3165)	SegCLSLoss 0.0248 (0.0177)	KLLoss 0.3711 (0.3131)	MaskLoss 1.0649 (0.7007)	MaskBCELoss 0.1052 (0.1468)	MaskDICELoss 0.9598 (0.5540)
Epoch: [1][382/500]	Time  5.383 ( 5.383)	Loss 0.0830 (1.7127)	CeLoss 0.0830 (0.4809)	SegCLSLoss 0.0000 (0.0141)	KLLoss 0.0000 (0.2363)	MaskLoss 0.0000 (0.6005)	MaskBCELoss 0.0000 (0.1778)	MaskDICELoss 0.0000 (0.4227)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 379 is less than current step: 499. Dropping entry: {'train/loss': 1.974688673019409, 'train/ce_loss': 0.4349609375, 'train/seg_cls_loss': 0.01640625, 'train/kl_loss': 0.317578125, 'train/mask_bce_loss': 0.11500307014212012, 'train/mask_dice_loss': 0.63493891954422, 'train/mask_loss': 0.7499419867992401, 'metrics/total_secs_per_batch': 6.769684553146362, 'metrics/data_secs_per_batch': 2.9487950563430787, '_timestamp': 1740956386.8033829}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 379 is less than current step: 499. Dropping entry: {'train/lr': 0.00019855102040816327, '_timestamp': 1740956386.803659}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 380 is less than current step: 499. Dropping entry: {'train/loss': 1.7580832660198211, 'train/ce_loss': 0.316455078125, 'train/seg_cls_loss': 0.017706298828125, 'train/kl_loss': 0.3130859375, 'train/mask_bce_loss': 0.14675314370542764, 'train/mask_dice_loss': 0.5539925873279572, 'train/mask_loss': 0.7007457219064236, 'metrics/total_secs_per_batch': 6.843533992767334, 'metrics/data_secs_per_batch': 3.473258447647095, '_timestamp': 1740956393.6471252}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 380 is less than current step: 499. Dropping entry: {'train/lr': 0.00019842857142857143, '_timestamp': 1740956393.647493}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 381 is less than current step: 499. Dropping entry: {'train/loss': 1.7127034068107605, 'train/ce_loss': 0.480859375, 'train/seg_cls_loss': 0.01405029296875, 'train/kl_loss': 0.236328125, 'train/mask_bce_loss': 0.17779952399432658, 'train/mask_dice_loss': 0.4227416217327118, 'train/mask_loss': 0.6005411446094513, 'metrics/total_secs_per_batch': 5.38316011428833, 'metrics/data_secs_per_batch': 2.3662203550338745, '_timestamp': 1740956399.0302932}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 381 is less than current step: 499. Dropping entry: {'train/lr': 0.0001983061224489796, '_timestamp': 1740956399.0305736}).
Epoch: [1][383/500]	Time  5.988 ( 5.988)	Loss 2.0252 (1.5690)	CeLoss 0.2695 (0.4658)	SegCLSLoss 0.0193 (0.0121)	KLLoss 0.4199 (0.2428)	MaskLoss 0.8524 (0.5365)	MaskBCELoss 0.0557 (0.1331)	MaskDICELoss 0.7968 (0.4034)
Epoch: [1][384/500]	Time  6.574 ( 6.574)	Loss 1.5547 (2.1815)	CeLoss 1.5547 (0.4387)	SegCLSLoss 0.0000 (0.0185)	KLLoss 0.0000 (0.3098)	MaskLoss 0.0000 (0.8513)	MaskBCELoss 0.0000 (0.1602)	MaskDICELoss 0.0000 (0.6911)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 382 is less than current step: 499. Dropping entry: {'train/loss': 1.5689553022384644, 'train/ce_loss': 0.465771484375, 'train/seg_cls_loss': 0.0120849609375, 'train/kl_loss': 0.2427734375, 'train/mask_bce_loss': 0.13307325839996337, 'train/mask_dice_loss': 0.40343076586723325, 'train/mask_loss': 0.5365040212869644, 'metrics/total_secs_per_batch': 5.98802924156189, 'metrics/data_secs_per_batch': 2.904035496711731, '_timestamp': 1740956405.0186517}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 382 is less than current step: 499. Dropping entry: {'train/lr': 0.00019818367346938776, '_timestamp': 1740956405.0191183}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 383 is less than current step: 499. Dropping entry: {'train/loss': 2.1814876317977907, 'train/ce_loss': 0.438671875, 'train/seg_cls_loss': 0.0184814453125, 'train/kl_loss': 0.309765625, 'train/mask_bce_loss': 0.16020007710903883, 'train/mask_dice_loss': 0.6910906195640564, 'train/mask_loss': 0.8512906968593598, 'metrics/total_secs_per_batch': 6.5738184452056885, 'metrics/data_secs_per_batch': 3.234958553314209, '_timestamp': 1740956411.5921733}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 383 is less than current step: 499. Dropping entry: {'train/lr': 0.00019806122448979592, '_timestamp': 1740956411.59245}).
Epoch: [1][385/500]	Time  5.712 ( 5.712)	Loss 2.6636 (1.5468)	CeLoss 0.2793 (0.2682)	SegCLSLoss 0.0221 (0.0142)	KLLoss 0.3965 (0.2365)	MaskLoss 1.1667 (0.6240)	MaskBCELoss 0.1928 (0.1199)	MaskDICELoss 0.9739 (0.5041)
Epoch: [1][386/500]	Time  6.109 ( 6.109)	Loss 1.9128 (1.6133)	CeLoss 0.2314 (0.6288)	SegCLSLoss 0.0159 (0.0109)	KLLoss 0.4121 (0.2436)	MaskLoss 0.8158 (0.4773)	MaskBCELoss 0.2159 (0.1308)	MaskDICELoss 0.5999 (0.3466)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 384 is less than current step: 499. Dropping entry: {'train/loss': 1.5468316555023194, 'train/ce_loss': 0.268212890625, 'train/seg_cls_loss': 0.01419677734375, 'train/kl_loss': 0.2365234375, 'train/mask_bce_loss': 0.11987903937697411, 'train/mask_dice_loss': 0.5040983080863952, 'train/mask_loss': 0.6239773392677307, 'metrics/total_secs_per_batch': 5.711581230163574, 'metrics/data_secs_per_batch': 2.485592484474182, '_timestamp': 1740956417.3037271}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 384 is less than current step: 499. Dropping entry: {'train/lr': 0.00019793877551020408, '_timestamp': 1740956417.3039265}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 385 is less than current step: 499. Dropping entry: {'train/loss': 1.613280725479126, 'train/ce_loss': 0.62880859375, 'train/seg_cls_loss': 0.010894775390625, 'train/kl_loss': 0.2435546875, 'train/mask_bce_loss': 0.13078709356486798, 'train/mask_dice_loss': 0.3465563923120499, 'train/mask_loss': 0.477343487739563, 'metrics/total_secs_per_batch': 6.108616828918457, 'metrics/data_secs_per_batch': 2.8377962350845336, '_timestamp': 1740956423.4123662}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 385 is less than current step: 499. Dropping entry: {'train/lr': 0.00019781632653061225, '_timestamp': 1740956423.4126542}).
Epoch: [1][387/500]	Time  7.221 ( 7.221)	Loss 2.5404 (2.3451)	CeLoss 0.2891 (0.2412)	SegCLSLoss 0.0110 (0.0234)	KLLoss 0.3965 (0.3906)	MaskLoss 1.1032 (1.0266)	MaskBCELoss 0.1042 (0.2646)	MaskDICELoss 0.9990 (0.7620)
Epoch: [1][388/500]	Time  6.408 ( 6.408)	Loss 2.5888 (1.8574)	CeLoss 0.1973 (0.3437)	SegCLSLoss 0.0262 (0.0163)	KLLoss 0.3848 (0.3162)	MaskLoss 1.1704 (0.7370)	MaskBCELoss 0.3258 (0.1319)	MaskDICELoss 0.8446 (0.6051)
Epoch: [1][389/500]	Time  6.863 ( 6.863)	Loss 1.4222 (1.7648)	CeLoss 0.2393 (0.2962)	SegCLSLoss 0.0117 (0.0159)	KLLoss 0.4160 (0.3203)	MaskLoss 0.5675 (0.7144)	MaskBCELoss 0.1915 (0.1395)	MaskDICELoss 0.3760 (0.5749)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 386 is less than current step: 499. Dropping entry: {'train/loss': 2.3450676679611204, 'train/ce_loss': 0.241162109375, 'train/seg_cls_loss': 0.0234375, 'train/kl_loss': 0.390625, 'train/mask_bce_loss': 0.2645630843937397, 'train/mask_dice_loss': 0.7620234817266465, 'train/mask_loss': 1.0265865564346313, 'metrics/total_secs_per_batch': 7.220848798751831, 'metrics/data_secs_per_batch': 3.420259165763855, '_timestamp': 1740956430.6331928}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 386 is less than current step: 499. Dropping entry: {'train/lr': 0.0001976938775510204, '_timestamp': 1740956430.6334782}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 387 is less than current step: 499. Dropping entry: {'train/loss': 1.8573883414268493, 'train/ce_loss': 0.34365234375, 'train/seg_cls_loss': 0.016339111328125, 'train/kl_loss': 0.3162109375, 'train/mask_bce_loss': 0.1318780280649662, 'train/mask_dice_loss': 0.6051169335842133, 'train/mask_loss': 0.7369949638843536, 'metrics/total_secs_per_batch': 6.407741546630859, 'metrics/data_secs_per_batch': 3.067160129547119, '_timestamp': 1740956437.0409753}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 387 is less than current step: 499. Dropping entry: {'train/lr': 0.00019757142857142855, '_timestamp': 1740956437.0413015}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 388 is less than current step: 499. Dropping entry: {'train/loss': 1.7648013472557067, 'train/ce_loss': 0.296240234375, 'train/seg_cls_loss': 0.015869140625, 'train/kl_loss': 0.3203125, 'train/mask_bce_loss': 0.13945067934691907, 'train/mask_dice_loss': 0.5749080061912537, 'train/mask_loss': 0.7143586874008179, 'metrics/total_secs_per_batch': 6.8630030155181885, 'metrics/data_secs_per_batch': 3.06301326751709, '_timestamp': 1740956443.9039872}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 388 is less than current step: 499. Dropping entry: {'train/lr': 0.0001974489795918367, '_timestamp': 1740956443.904284}).
[2025-03-02 17:00:49,626] [INFO] [logging.py:96:log_dist] [Rank 0] step=890, skipped=0, lr=[0.0001973877551020408], mom=[(0.9, 0.95)]
[2025-03-02 17:00:49,632] [INFO] [timer.py:215:stop] epoch=0/micro_step=8900/global_step=890, RunningAvgSamplesPerSec=1.5107136188257406, CurrSamplesPerSec=1.7459637816616158, MemAllocated=31.34GB, MaxMemAllocated=37.19GB
Epoch: [1][390/500]	Time  5.729 ( 5.729)	Loss 1.3672 (1.6158)	CeLoss 1.3672 (0.4833)	SegCLSLoss 0.0000 (0.0131)	KLLoss 0.0000 (0.2398)	MaskLoss 0.0000 (0.5509)	MaskBCELoss 0.0000 (0.1370)	MaskDICELoss 0.0000 (0.4139)
Epoch: [1][391/500]	Time  6.385 ( 6.385)	Loss 1.0938 (1.8486)	CeLoss 1.0938 (0.5027)	SegCLSLoss 0.0000 (0.0180)	KLLoss 0.0000 (0.2766)	MaskLoss 0.0000 (0.6545)	MaskBCELoss 0.0000 (0.0897)	MaskDICELoss 0.0000 (0.5649)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 389 is less than current step: 499. Dropping entry: {'train/loss': 1.615801215171814, 'train/ce_loss': 0.483251953125, 'train/seg_cls_loss': 0.01307373046875, 'train/kl_loss': 0.23984375, 'train/mask_bce_loss': 0.13702068850398064, 'train/mask_dice_loss': 0.4138730764389038, 'train/mask_loss': 0.550893771648407, 'metrics/total_secs_per_batch': 5.729201316833496, 'metrics/data_secs_per_batch': 2.419818305969238, '_timestamp': 1740956449.6329708}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 389 is less than current step: 499. Dropping entry: {'train/lr': 0.00019732653061224487, '_timestamp': 1740956449.6332524}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 390 is less than current step: 499. Dropping entry: {'train/loss': 1.8485970139503478, 'train/ce_loss': 0.502734375, 'train/seg_cls_loss': 0.01802978515625, 'train/kl_loss': 0.2765625, 'train/mask_bce_loss': 0.08967090025544167, 'train/mask_dice_loss': 0.5648522257804871, 'train/mask_loss': 0.6545231163501739, 'metrics/total_secs_per_batch': 6.384825706481934, 'metrics/data_secs_per_batch': 3.1091328382492067, '_timestamp': 1740956456.018386}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 390 is less than current step: 499. Dropping entry: {'train/lr': 0.00019720408163265303, '_timestamp': 1740956456.0188546}).
Epoch: [1][392/500]	Time  6.907 ( 6.907)	Loss 1.2606 (1.9476)	CeLoss 0.2656 (0.5050)	SegCLSLoss 0.0117 (0.0172)	KLLoss 0.4102 (0.3203)	MaskLoss 0.4741 (0.7012)	MaskBCELoss 0.0803 (0.0767)	MaskDICELoss 0.3938 (0.6245)
Epoch: [1][393/500]	Time  4.508 ( 4.508)	Loss 1.0625 (1.6232)	CeLoss 1.0625 (0.6859)	SegCLSLoss 0.0000 (0.0117)	KLLoss 0.0000 (0.1514)	MaskLoss 0.0000 (0.4581)	MaskBCELoss 0.0000 (0.1171)	MaskDICELoss 0.0000 (0.3410)
Epoch: [1][394/500]	Time  5.557 ( 5.557)	Loss 1.5625 (2.0129)	CeLoss 1.5625 (0.6221)	SegCLSLoss 0.0000 (0.0134)	KLLoss 0.0000 (0.2760)	MaskLoss 0.0000 (0.6783)	MaskBCELoss 0.0000 (0.1726)	MaskDICELoss 0.0000 (0.5057)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 391 is less than current step: 499. Dropping entry: {'train/loss': 1.9476026892662048, 'train/ce_loss': 0.50498046875, 'train/seg_cls_loss': 0.0171630859375, 'train/kl_loss': 0.3203125, 'train/mask_bce_loss': 0.07670289902016521, 'train/mask_dice_loss': 0.6244910150766373, 'train/mask_loss': 0.7011939078569412, 'metrics/total_secs_per_batch': 6.906832218170166, 'metrics/data_secs_per_batch': 3.2102522611618043, '_timestamp': 1740956462.9248524}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 391 is less than current step: 499. Dropping entry: {'train/lr': 0.0001970816326530612, '_timestamp': 1740956462.9251368}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 392 is less than current step: 499. Dropping entry: {'train/loss': 1.6231652975082398, 'train/ce_loss': 0.685888671875, 'train/seg_cls_loss': 0.01173095703125, 'train/kl_loss': 0.1513671875, 'train/mask_bce_loss': 0.11712794676423073, 'train/mask_dice_loss': 0.34096348881721494, 'train/mask_loss': 0.4580914258956909, 'metrics/total_secs_per_batch': 4.508303642272949, 'metrics/data_secs_per_batch': 2.0817975282669066, '_timestamp': 1740956467.4333422}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 392 is less than current step: 499. Dropping entry: {'train/lr': 0.00019695918367346936, '_timestamp': 1740956467.4336898}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 393 is less than current step: 499. Dropping entry: {'train/loss': 2.0128769159317015, 'train/ce_loss': 0.6220703125, 'train/seg_cls_loss': 0.01343994140625, 'train/kl_loss': 0.2759765625, 'train/mask_bce_loss': 0.17260753810405732, 'train/mask_dice_loss': 0.5057059079408646, 'train/mask_loss': 0.6783134460449218, 'metrics/total_secs_per_batch': 5.556870222091675, 'metrics/data_secs_per_batch': 2.5384509801864623, '_timestamp': 1740956472.990029}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 393 is less than current step: 499. Dropping entry: {'train/lr': 0.00019683673469387752, '_timestamp': 1740956472.9903235}).
Epoch: [1][395/500]	Time  6.109 ( 6.109)	Loss 2.3452 (1.7706)	CeLoss 0.1768 (0.4650)	SegCLSLoss 0.0254 (0.0172)	KLLoss 0.3828 (0.2715)	MaskLoss 1.0588 (0.6350)	MaskBCELoss 0.2909 (0.1242)	MaskDICELoss 0.7680 (0.5108)
Epoch: [1][396/500]	Time  6.218 ( 6.218)	Loss 1.8007 (1.9529)	CeLoss 0.3047 (0.4444)	SegCLSLoss 0.0127 (0.0157)	KLLoss 0.4121 (0.3221)	MaskLoss 0.7246 (0.7347)	MaskBCELoss 0.2388 (0.1248)	MaskDICELoss 0.4858 (0.6098)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 394 is less than current step: 499. Dropping entry: {'train/loss': 1.7705575585365296, 'train/ce_loss': 0.4650390625, 'train/seg_cls_loss': 0.01715087890625, 'train/kl_loss': 0.271484375, 'train/mask_bce_loss': 0.12420959211885929, 'train/mask_dice_loss': 0.5108250349760055, 'train/mask_loss': 0.635034641623497, 'metrics/total_secs_per_batch': 6.10939621925354, 'metrics/data_secs_per_batch': 3.009040427207947, '_timestamp': 1740956479.099432}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 394 is less than current step: 499. Dropping entry: {'train/lr': 0.0001967142857142857, '_timestamp': 1740956479.0998027}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 395 is less than current step: 499. Dropping entry: {'train/loss': 1.9528991818428039, 'train/ce_loss': 0.44443359375, 'train/seg_cls_loss': 0.015740966796875, 'train/kl_loss': 0.3220703125, 'train/mask_bce_loss': 0.12481959410943091, 'train/mask_dice_loss': 0.6098331212997437, 'train/mask_loss': 0.734652704000473, 'metrics/total_secs_per_batch': 6.2177441120147705, 'metrics/data_secs_per_batch': 3.0029965162277223, '_timestamp': 1740956485.3173475}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 395 is less than current step: 499. Dropping entry: {'train/lr': 0.00019659183673469385, '_timestamp': 1740956485.3176947}).
Epoch: [1][397/500]	Time  5.879 ( 5.879)	Loss 1.5688 (1.7319)	CeLoss 0.2988 (0.4896)	SegCLSLoss 0.0176 (0.0155)	KLLoss 0.4180 (0.2789)	MaskLoss 0.6096 (0.6032)	MaskBCELoss 0.1697 (0.1498)	MaskDICELoss 0.4399 (0.4534)
Epoch: [1][398/500]	Time  5.937 ( 5.937)	Loss 2.5388 (2.0252)	CeLoss 0.2441 (0.4709)	SegCLSLoss 0.0229 (0.0166)	KLLoss 0.4102 (0.3197)	MaskLoss 1.1210 (0.7570)	MaskBCELoss 0.2421 (0.1614)	MaskDICELoss 0.8788 (0.5956)
Epoch: [1][399/500]	Time  6.720 ( 6.720)	Loss 2.0857 (1.4522)	CeLoss 0.2363 (0.4242)	SegCLSLoss 0.0225 (0.0117)	KLLoss 0.3906 (0.1924)	MaskLoss 0.8993 (0.5015)	MaskBCELoss 0.0528 (0.0819)	MaskDICELoss 0.8465 (0.4195)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 396 is less than current step: 499. Dropping entry: {'train/loss': 1.7319005131721497, 'train/ce_loss': 0.4896484375, 'train/seg_cls_loss': 0.01551513671875, 'train/kl_loss': 0.27890625, 'train/mask_bce_loss': 0.14976748153567315, 'train/mask_dice_loss': 0.4533897861838341, 'train/mask_loss': 0.6031572759151459, 'metrics/total_secs_per_batch': 5.878875970840454, 'metrics/data_secs_per_batch': 2.8291244506835938, '_timestamp': 1740956491.1960526}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 396 is less than current step: 499. Dropping entry: {'train/lr': 0.000196469387755102, '_timestamp': 1740956491.1964247}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 397 is less than current step: 499. Dropping entry: {'train/loss': 2.025182831287384, 'train/ce_loss': 0.4708984375, 'train/seg_cls_loss': 0.016644287109375, 'train/kl_loss': 0.3197265625, 'train/mask_bce_loss': 0.16141419876366853, 'train/mask_dice_loss': 0.5956108063459397, 'train/mask_loss': 0.7570250004529953, 'metrics/total_secs_per_batch': 5.937096357345581, 'metrics/data_secs_per_batch': 2.6620615482330323, '_timestamp': 1740956497.1331472}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 397 is less than current step: 499. Dropping entry: {'train/lr': 0.00019634693877551018, '_timestamp': 1740956497.1334255}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 398 is less than current step: 499. Dropping entry: {'train/loss': 1.4522226572036743, 'train/ce_loss': 0.4241943359375, 'train/seg_cls_loss': 0.011700439453125, 'train/kl_loss': 0.1923828125, 'train/mask_bce_loss': 0.08192114382982255, 'train/mask_dice_loss': 0.4195441961288452, 'train/mask_loss': 0.5014653444290161, 'metrics/total_secs_per_batch': 6.71976375579834, 'metrics/data_secs_per_batch': 3.113294315338135, '_timestamp': 1740956503.8529878}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 398 is less than current step: 499. Dropping entry: {'train/lr': 0.00019622448979591834, '_timestamp': 1740956503.8532846}).
[2025-03-02 17:01:49,886] [INFO] [logging.py:96:log_dist] [Rank 0] step=900, skipped=0, lr=[0.0001961632653061224], mom=[(0.9, 0.95)]
[2025-03-02 17:01:49,892] [INFO] [timer.py:215:stop] epoch=0/micro_step=9000/global_step=900, RunningAvgSamplesPerSec=1.5123222512473524, CurrSamplesPerSec=1.6559513376681054, MemAllocated=30.75GB, MaxMemAllocated=37.19GB
Epoch: [1][400/500]	Time  6.041 ( 6.041)	Loss 0.9180 (1.7270)	CeLoss 0.9180 (0.3757)	SegCLSLoss 0.0000 (0.0172)	KLLoss 0.0000 (0.2764)	MaskLoss 0.0000 (0.6576)	MaskBCELoss 0.0000 (0.1298)	MaskDICELoss 0.0000 (0.5278)
Epoch: [1][401/500]	Time  7.162 ( 7.162)	Loss 2.3703 (2.0320)	CeLoss 0.3301 (0.2417)	SegCLSLoss 0.0156 (0.0206)	KLLoss 0.4062 (0.3557)	MaskLoss 0.9957 (0.8723)	MaskBCELoss 0.0319 (0.1810)	MaskDICELoss 0.9639 (0.6913)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 399 is less than current step: 499. Dropping entry: {'train/loss': 1.7270030975341797, 'train/ce_loss': 0.3756591796875, 'train/seg_cls_loss': 0.0171875, 'train/kl_loss': 0.2763671875, 'train/mask_bce_loss': 0.12978441510349512, 'train/mask_dice_loss': 0.5277723073959351, 'train/mask_loss': 0.6575567185878753, 'metrics/total_secs_per_batch': 6.040543556213379, 'metrics/data_secs_per_batch': 2.9278075218200685, '_timestamp': 1740956509.8933423}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 399 is less than current step: 499. Dropping entry: {'train/lr': 0.0001961020408163265, '_timestamp': 1740956509.8935642}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 400 is less than current step: 499. Dropping entry: {'train/loss': 2.0320062398910523, 'train/ce_loss': 0.24169921875, 'train/seg_cls_loss': 0.020556640625, 'train/kl_loss': 0.3556640625, 'train/mask_bce_loss': 0.1810195915400982, 'train/mask_dice_loss': 0.6912823379039764, 'train/mask_loss': 0.8723019242286683, 'metrics/total_secs_per_batch': 7.161884069442749, 'metrics/data_secs_per_batch': 3.2059354066848753, '_timestamp': 1740956517.0553977}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 400 is less than current step: 499. Dropping entry: {'train/lr': 0.00019597959183673466, '_timestamp': 1740956517.055713}).
Epoch: [1][402/500]	Time  7.144 ( 7.144)	Loss 2.1260 (2.0905)	CeLoss 0.2295 (0.4766)	SegCLSLoss 0.0271 (0.0186)	KLLoss 0.3750 (0.3086)	MaskLoss 0.9224 (0.7869)	MaskBCELoss 0.0894 (0.1933)	MaskDICELoss 0.8330 (0.5936)
Epoch: [1][403/500]	Time  7.110 ( 7.110)	Loss 1.7127 (1.6376)	CeLoss 0.2217 (0.3060)	SegCLSLoss 0.0205 (0.0160)	KLLoss 0.3945 (0.3158)	MaskLoss 0.7206 (0.6460)	MaskBCELoss 0.1107 (0.1014)	MaskDICELoss 0.6100 (0.5446)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 401 is less than current step: 499. Dropping entry: {'train/loss': 2.0905306577682494, 'train/ce_loss': 0.4765625, 'train/seg_cls_loss': 0.018560791015625, 'train/kl_loss': 0.30859375, 'train/mask_bce_loss': 0.19329832941293718, 'train/mask_dice_loss': 0.5936173915863037, 'train/mask_loss': 0.7869157284498215, 'metrics/total_secs_per_batch': 7.144363880157471, 'metrics/data_secs_per_batch': 2.8905453205108644, '_timestamp': 1740956524.1997776}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 401 is less than current step: 499. Dropping entry: {'train/lr': 0.00019585714285714283, '_timestamp': 1740956524.200081}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 402 is less than current step: 499. Dropping entry: {'train/loss': 1.6376327514648437, 'train/ce_loss': 0.306005859375, 'train/seg_cls_loss': 0.015985107421875, 'train/kl_loss': 0.3158203125, 'train/mask_bce_loss': 0.10137381628155709, 'train/mask_dice_loss': 0.5446154087781906, 'train/mask_loss': 0.6459892272949219, 'metrics/total_secs_per_batch': 7.110278844833374, 'metrics/data_secs_per_batch': 2.997681212425232, '_timestamp': 1740956531.3102067}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 402 is less than current step: 499. Dropping entry: {'train/lr': 0.00019573469387755102, '_timestamp': 1740956531.31056}).
Epoch: [1][404/500]	Time  6.879 ( 6.879)	Loss 2.6321 (1.9032)	CeLoss 0.2158 (0.3788)	SegCLSLoss 0.0304 (0.0159)	KLLoss 0.3730 (0.3178)	MaskLoss 1.1813 (0.7423)	MaskBCELoss 0.3096 (0.1394)	MaskDICELoss 0.8717 (0.6029)
Epoch: [1][405/500]	Time  5.773 ( 5.773)	Loss 1.0312 (1.8991)	CeLoss 1.0312 (0.5434)	SegCLSLoss 0.0000 (0.0167)	KLLoss 0.0000 (0.2766)	MaskLoss 0.0000 (0.6599)	MaskBCELoss 0.0000 (0.1127)	MaskDICELoss 0.0000 (0.5471)
Epoch: [1][406/500]	Time  4.800 ( 4.800)	Loss 2.4070 (1.2795)	CeLoss 0.2871 (0.5793)	SegCLSLoss 0.0210 (0.0068)	KLLoss 0.3867 (0.1586)	MaskLoss 1.0355 (0.3405)	MaskBCELoss 0.0484 (0.0824)	MaskDICELoss 0.9871 (0.2580)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 403 is less than current step: 499. Dropping entry: {'train/loss': 1.903194212913513, 'train/ce_loss': 0.37880859375, 'train/seg_cls_loss': 0.015948486328125, 'train/kl_loss': 0.3177734375, 'train/mask_bce_loss': 0.13943933881819248, 'train/mask_dice_loss': 0.6028804033994675, 'train/mask_loss': 0.7423197507858277, 'metrics/total_secs_per_batch': 6.879091739654541, 'metrics/data_secs_per_batch': 3.191605043411255, '_timestamp': 1740956538.1890454}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 403 is less than current step: 499. Dropping entry: {'train/lr': 0.00019561224489795918, '_timestamp': 1740956538.1892357}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 404 is less than current step: 499. Dropping entry: {'train/loss': 1.8990562200546264, 'train/ce_loss': 0.543359375, 'train/seg_cls_loss': 0.01668701171875, 'train/kl_loss': 0.2765625, 'train/mask_bce_loss': 0.11274942234158516, 'train/mask_dice_loss': 0.5471302568912506, 'train/mask_loss': 0.6598796665668487, 'metrics/total_secs_per_batch': 5.772687196731567, 'metrics/data_secs_per_batch': 2.661520314216614, '_timestamp': 1740956543.961947}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 404 is less than current step: 499. Dropping entry: {'train/lr': 0.00019548979591836734, '_timestamp': 1740956543.9622912}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 405 is less than current step: 499. Dropping entry: {'train/loss': 1.2794967532157897, 'train/ce_loss': 0.579345703125, 'train/seg_cls_loss': 0.0067626953125, 'train/kl_loss': 0.15859375, 'train/mask_bce_loss': 0.08244059085845948, 'train/mask_dice_loss': 0.2580157905817032, 'train/mask_loss': 0.3404563844203949, 'metrics/total_secs_per_batch': 4.799826145172119, 'metrics/data_secs_per_batch': 1.8754620552062988, '_timestamp': 1740956548.7617898}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 405 is less than current step: 499. Dropping entry: {'train/lr': 0.0001953673469387755, '_timestamp': 1740956548.7621858}).
Epoch: [1][407/500]	Time  5.655 ( 5.655)	Loss 2.2159 (1.6965)	CeLoss 0.2910 (0.5425)	SegCLSLoss 0.0148 (0.0120)	KLLoss 0.4004 (0.2387)	MaskLoss 0.9380 (0.5620)	MaskBCELoss 0.4146 (0.1308)	MaskDICELoss 0.5234 (0.4312)
Epoch: [1][408/500]	Time  6.463 ( 6.463)	Loss 0.0742 (1.6821)	CeLoss 0.0742 (0.4449)	SegCLSLoss 0.0000 (0.0163)	KLLoss 0.0000 (0.2771)	MaskLoss 0.0000 (0.6008)	MaskBCELoss 0.0000 (0.1361)	MaskDICELoss 0.0000 (0.4647)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 406 is less than current step: 499. Dropping entry: {'train/loss': 1.6965197920799255, 'train/ce_loss': 0.542529296875, 'train/seg_cls_loss': 0.011968994140625, 'train/kl_loss': 0.238671875, 'train/mask_bce_loss': 0.13084624372422696, 'train/mask_dice_loss': 0.4311587572097778, 'train/mask_loss': 0.5620050013065339, 'metrics/total_secs_per_batch': 5.654739141464233, 'metrics/data_secs_per_batch': 2.31413254737854, '_timestamp': 1740956554.4165635}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 406 is less than current step: 499. Dropping entry: {'train/lr': 0.00019524489795918367, '_timestamp': 1740956554.416959}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 407 is less than current step: 499. Dropping entry: {'train/loss': 1.6820727944374085, 'train/ce_loss': 0.444921875, 'train/seg_cls_loss': 0.01634521484375, 'train/kl_loss': 0.2771484375, 'train/mask_bce_loss': 0.1361012440174818, 'train/mask_dice_loss': 0.46470077633857726, 'train/mask_loss': 0.6008020102977752, 'metrics/total_secs_per_batch': 6.462936162948608, 'metrics/data_secs_per_batch': 2.7356160402297975, '_timestamp': 1740956560.8793359}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 407 is less than current step: 499. Dropping entry: {'train/lr': 0.00019512244897959183, '_timestamp': 1740956560.879619}).
Epoch: [1][409/500]	Time  6.250 ( 6.250)	Loss 1.5234 (1.7382)	CeLoss 1.5234 (0.4793)	SegCLSLoss 0.0000 (0.0165)	KLLoss 0.0000 (0.3211)	MaskLoss 0.0000 (0.6092)	MaskBCELoss 0.0000 (0.0965)	MaskDICELoss 0.0000 (0.5128)
[2025-03-02 17:02:52,605] [INFO] [logging.py:96:log_dist] [Rank 0] step=910, skipped=0, lr=[0.00019493877551020406], mom=[(0.9, 0.95)]
[2025-03-02 17:02:52,611] [INFO] [timer.py:215:stop] epoch=0/micro_step=9100/global_step=910, RunningAvgSamplesPerSec=1.5131837772864787, CurrSamplesPerSec=1.8245674600799306, MemAllocated=30.71GB, MaxMemAllocated=37.19GB
Epoch: [1][410/500]	Time  5.482 ( 5.482)	Loss 1.3203 (1.4985)	CeLoss 1.3203 (0.5297)	SegCLSLoss 0.0000 (0.0127)	KLLoss 0.0000 (0.2408)	MaskLoss 0.0000 (0.4693)	MaskBCELoss 0.0000 (0.0689)	MaskDICELoss 0.0000 (0.4004)
Epoch: [1][411/500]	Time  6.221 ( 6.221)	Loss 1.6781 (2.0581)	CeLoss 0.2793 (0.3257)	SegCLSLoss 0.0193 (0.0185)	KLLoss 0.4004 (0.3596)	MaskLoss 0.6750 (0.8436)	MaskBCELoss 0.2844 (0.1391)	MaskDICELoss 0.3906 (0.7044)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 408 is less than current step: 499. Dropping entry: {'train/loss': 1.7382063269615173, 'train/ce_loss': 0.479296875, 'train/seg_cls_loss': 0.016510009765625, 'train/kl_loss': 0.32109375, 'train/mask_bce_loss': 0.09646372348070145, 'train/mask_dice_loss': 0.5127761512994766, 'train/mask_loss': 0.6092398792505265, 'metrics/total_secs_per_batch': 6.250253200531006, 'metrics/data_secs_per_batch': 2.8326317071914673, '_timestamp': 1740956567.1295953}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 408 is less than current step: 499. Dropping entry: {'train/lr': 0.000195, '_timestamp': 1740956567.1299207}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 409 is less than current step: 499. Dropping entry: {'train/loss': 1.49854177236557, 'train/ce_loss': 0.529736328125, 'train/seg_cls_loss': 0.01268310546875, 'train/kl_loss': 0.2408203125, 'train/mask_bce_loss': 0.06886856416240335, 'train/mask_dice_loss': 0.40044626146554946, 'train/mask_loss': 0.469314831495285, 'metrics/total_secs_per_batch': 5.482432842254639, 'metrics/data_secs_per_batch': 2.8932504892349242, '_timestamp': 1740956572.6119}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 409 is less than current step: 499. Dropping entry: {'train/lr': 0.00019487755102040816, '_timestamp': 1740956572.612244}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 410 is less than current step: 499. Dropping entry: {'train/loss': 2.0581355810165407, 'train/ce_loss': 0.32568359375, 'train/seg_cls_loss': 0.01854248046875, 'train/kl_loss': 0.3595703125, 'train/mask_bce_loss': 0.1391339350491762, 'train/mask_dice_loss': 0.7044357925653457, 'train/mask_loss': 0.8435697257518768, 'metrics/total_secs_per_batch': 6.220881223678589, 'metrics/data_secs_per_batch': 2.6151096343994142, '_timestamp': 1740956578.8329256}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 410 is less than current step: 499. Dropping entry: {'train/lr': 0.00019475510204081632, '_timestamp': 1740956578.8332226}).
Epoch: [1][412/500]	Time  6.093 ( 6.093)	Loss 1.8179 (2.0114)	CeLoss 0.2188 (0.3145)	SegCLSLoss 0.0240 (0.0183)	KLLoss 0.3848 (0.3590)	MaskLoss 0.7742 (0.8260)	MaskBCELoss 0.2191 (0.2385)	MaskDICELoss 0.5551 (0.5875)
Epoch: [1][413/500]	Time  6.892 ( 6.892)	Loss 0.5156 (1.5028)	CeLoss 0.5156 (0.4007)	SegCLSLoss 0.0000 (0.0132)	KLLoss 0.0000 (0.2371)	MaskLoss 0.0000 (0.5360)	MaskBCELoss 0.0000 (0.0532)	MaskDICELoss 0.0000 (0.4829)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 411 is less than current step: 499. Dropping entry: {'train/loss': 2.0113621294498443, 'train/ce_loss': 0.314453125, 'train/seg_cls_loss': 0.018280029296875, 'train/kl_loss': 0.358984375, 'train/mask_bce_loss': 0.23850929997861386, 'train/mask_dice_loss': 0.5874842539429664, 'train/mask_loss': 0.8259935706853867, 'metrics/total_secs_per_batch': 6.093254089355469, 'metrics/data_secs_per_batch': 2.6277314901351927, '_timestamp': 1740956584.9261782}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 411 is less than current step: 499. Dropping entry: {'train/lr': 0.00019463265306122449, '_timestamp': 1740956584.9264824}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 412 is less than current step: 499. Dropping entry: {'train/loss': 1.5027753114700317, 'train/ce_loss': 0.400732421875, 'train/seg_cls_loss': 0.01317138671875, 'train/kl_loss': 0.237109375, 'train/mask_bce_loss': 0.05316940415650606, 'train/mask_dice_loss': 0.4828617930412292, 'train/mask_loss': 0.5360311925411224, 'metrics/total_secs_per_batch': 6.89228892326355, 'metrics/data_secs_per_batch': 3.4972134828567505, '_timestamp': 1740956591.8184543}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 412 is less than current step: 499. Dropping entry: {'train/lr': 0.00019451020408163265, '_timestamp': 1740956591.818724}).
Epoch: [1][414/500]	Time  6.548 ( 6.548)	Loss 1.7875 (1.7532)	CeLoss 0.3594 (0.4459)	SegCLSLoss 0.0128 (0.0151)	KLLoss 0.4121 (0.2773)	MaskLoss 0.6906 (0.6359)	MaskBCELoss 0.0435 (0.0619)	MaskDICELoss 0.6471 (0.5740)
Epoch: [1][415/500]	Time  5.142 ( 5.142)	Loss 1.6906 (1.8692)	CeLoss 0.2324 (0.4970)	SegCLSLoss 0.0144 (0.0144)	KLLoss 0.4121 (0.2771)	MaskLoss 0.7047 (0.6687)	MaskBCELoss 0.2466 (0.1861)	MaskDICELoss 0.4581 (0.4826)
Epoch: [1][416/500]	Time  6.085 ( 6.085)	Loss 1.3438 (1.5032)	CeLoss 1.3438 (0.6081)	SegCLSLoss 0.0000 (0.0135)	KLLoss 0.0000 (0.1943)	MaskLoss 0.0000 (0.4345)	MaskBCELoss 0.0000 (0.0250)	MaskDICELoss 0.0000 (0.4095)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 413 is less than current step: 499. Dropping entry: {'train/loss': 1.7532071828842164, 'train/ce_loss': 0.445947265625, 'train/seg_cls_loss': 0.01513671875, 'train/kl_loss': 0.27734375, 'train/mask_bce_loss': 0.06186055755242705, 'train/mask_dice_loss': 0.5740448057651519, 'train/mask_loss': 0.6359053611755371, 'metrics/total_secs_per_batch': 6.548140287399292, 'metrics/data_secs_per_batch': 2.90127809047699, '_timestamp': 1740956598.3665874}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 413 is less than current step: 499. Dropping entry: {'train/lr': 0.0001943877551020408, '_timestamp': 1740956598.3668766}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 414 is less than current step: 499. Dropping entry: {'train/loss': 1.8691843152046204, 'train/ce_loss': 0.49697265625, 'train/seg_cls_loss': 0.014447021484375, 'train/kl_loss': 0.2771484375, 'train/mask_bce_loss': 0.1860918658785522, 'train/mask_dice_loss': 0.48258231580257416, 'train/mask_loss': 0.6686741769313812, 'metrics/total_secs_per_batch': 5.142139911651611, 'metrics/data_secs_per_batch': 2.0127725839614867, '_timestamp': 1740956603.508774}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 414 is less than current step: 499. Dropping entry: {'train/lr': 0.00019426530612244895, '_timestamp': 1740956603.509093}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 415 is less than current step: 499. Dropping entry: {'train/loss': 1.5032130837440492, 'train/ce_loss': 0.60810546875, 'train/seg_cls_loss': 0.0134521484375, 'train/kl_loss': 0.1943359375, 'train/mask_bce_loss': 0.025035343319177627, 'train/mask_dice_loss': 0.40948134660720825, 'train/mask_loss': 0.43451668620109557, 'metrics/total_secs_per_batch': 6.084769010543823, 'metrics/data_secs_per_batch': 2.9207555532455443, '_timestamp': 1740956609.5936358}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 415 is less than current step: 499. Dropping entry: {'train/lr': 0.0001941428571428571, '_timestamp': 1740956609.593976}).
Epoch: [1][417/500]	Time  5.999 ( 5.999)	Loss 1.4375 (1.9099)	CeLoss 1.4375 (0.3364)	SegCLSLoss 0.0000 (0.0175)	KLLoss 0.0000 (0.3139)	MaskLoss 0.0000 (0.7667)	MaskBCELoss 0.0000 (0.0989)	MaskDICELoss 0.0000 (0.6678)
Epoch: [1][418/500]	Time  6.260 ( 6.260)	Loss 1.3047 (2.1128)	CeLoss 1.3047 (0.4510)	SegCLSLoss 0.0000 (0.0176)	KLLoss 0.0000 (0.3113)	MaskLoss 0.0000 (0.8109)	MaskBCELoss 0.0000 (0.1595)	MaskDICELoss 0.0000 (0.6515)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 416 is less than current step: 499. Dropping entry: {'train/loss': 1.909853422641754, 'train/ce_loss': 0.336376953125, 'train/seg_cls_loss': 0.01754150390625, 'train/kl_loss': 0.3138671875, 'train/mask_bce_loss': 0.09885393660515547, 'train/mask_dice_loss': 0.6678159475326538, 'train/mask_loss': 0.7666698634624481, 'metrics/total_secs_per_batch': 5.99927020072937, 'metrics/data_secs_per_batch': 2.677421760559082, '_timestamp': 1740956615.5927484}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 416 is less than current step: 499. Dropping entry: {'train/lr': 0.00019402040816326527, '_timestamp': 1740956615.5929377}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 417 is less than current step: 499. Dropping entry: {'train/loss': 2.1127631664276123, 'train/ce_loss': 0.4509765625, 'train/seg_cls_loss': 0.017596435546875, 'train/kl_loss': 0.311328125, 'train/mask_bce_loss': 0.15946848811581732, 'train/mask_dice_loss': 0.651454108953476, 'train/mask_loss': 0.8109225928783417, 'metrics/total_secs_per_batch': 6.25976300239563, 'metrics/data_secs_per_batch': 2.994690275192261, '_timestamp': 1740956621.8525798}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 417 is less than current step: 499. Dropping entry: {'train/lr': 0.00019389795918367344, '_timestamp': 1740956621.8528645}).
Epoch: [1][419/500]	Time  6.357 ( 6.357)	Loss 2.6087 (1.6963)	CeLoss 0.2236 (0.4448)	SegCLSLoss 0.0245 (0.0137)	KLLoss 0.3691 (0.2756)	MaskLoss 1.1677 (0.6084)	MaskBCELoss 0.2796 (0.0726)	MaskDICELoss 0.8880 (0.5359)
[2025-03-02 17:03:54,553] [INFO] [logging.py:96:log_dist] [Rank 0] step=920, skipped=0, lr=[0.0001937142857142857], mom=[(0.9, 0.95)]
[2025-03-02 17:03:54,559] [INFO] [timer.py:215:stop] epoch=0/micro_step=9200/global_step=920, RunningAvgSamplesPerSec=1.5142195473180367, CurrSamplesPerSec=1.5748764361680019, MemAllocated=31.25GB, MaxMemAllocated=37.19GB
Epoch: [1][420/500]	Time  6.351 ( 6.351)	Loss 2.0716 (1.8228)	CeLoss 0.2207 (0.3591)	SegCLSLoss 0.0155 (0.0190)	KLLoss 0.4102 (0.3584)	MaskLoss 0.9010 (0.7092)	MaskBCELoss 0.2496 (0.1486)	MaskDICELoss 0.6514 (0.5607)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 418 is less than current step: 499. Dropping entry: {'train/loss': 1.696306872367859, 'train/ce_loss': 0.4448486328125, 'train/seg_cls_loss': 0.013714599609375, 'train/kl_loss': 0.2755859375, 'train/mask_bce_loss': 0.07255112510174513, 'train/mask_dice_loss': 0.5358928084373474, 'train/mask_loss': 0.6084439337253571, 'metrics/total_secs_per_batch': 6.356662273406982, 'metrics/data_secs_per_batch': 2.498809242248535, '_timestamp': 1740956628.209188}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 418 is less than current step: 499. Dropping entry: {'train/lr': 0.0001937755102040816, '_timestamp': 1740956628.2093863}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 419 is less than current step: 499. Dropping entry: {'train/loss': 1.822812032699585, 'train/ce_loss': 0.35908203125, 'train/seg_cls_loss': 0.018988037109375, 'train/kl_loss': 0.3583984375, 'train/mask_bce_loss': 0.14855665662325918, 'train/mask_dice_loss': 0.560652095079422, 'train/mask_loss': 0.7092087507247925, 'metrics/total_secs_per_batch': 6.35121750831604, 'metrics/data_secs_per_batch': 2.5916435718536377, '_timestamp': 1740956634.5602257}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 419 is less than current step: 499. Dropping entry: {'train/lr': 0.00019365306122448976, '_timestamp': 1740956634.5604975}).
Epoch: [1][421/500]	Time  6.275 ( 6.275)	Loss 2.3041 (1.9593)	CeLoss 0.1904 (0.5133)	SegCLSLoss 0.0223 (0.0155)	KLLoss 0.3770 (0.2750)	MaskLoss 1.0329 (0.7053)	MaskBCELoss 0.0469 (0.2006)	MaskDICELoss 0.9860 (0.5047)
Epoch: [1][422/500]	Time  6.314 ( 6.314)	Loss 2.7884 (2.2868)	CeLoss 0.2021 (0.2298)	SegCLSLoss 0.0208 (0.0228)	KLLoss 0.3789 (0.3902)	MaskLoss 1.2692 (1.0034)	MaskBCELoss 0.4278 (0.1793)	MaskDICELoss 0.8414 (0.8241)
Epoch: [1][423/500]	Time  7.112 ( 7.112)	Loss 2.1995 (1.7193)	CeLoss 0.2637 (0.4105)	SegCLSLoss 0.0187 (0.0154)	KLLoss 0.3945 (0.2797)	MaskLoss 0.9435 (0.6366)	MaskBCELoss 0.0786 (0.0986)	MaskDICELoss 0.8649 (0.5380)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 420 is less than current step: 499. Dropping entry: {'train/loss': 1.9593194603919983, 'train/ce_loss': 0.51328125, 'train/seg_cls_loss': 0.015484619140625, 'train/kl_loss': 0.275, 'train/mask_bce_loss': 0.20055443793535233, 'train/mask_dice_loss': 0.5047400578856468, 'train/mask_loss': 0.705294492840767, 'metrics/total_secs_per_batch': 6.275485515594482, 'metrics/data_secs_per_batch': 2.599274826049805, '_timestamp': 1740956640.836022}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 420 is less than current step: 499. Dropping entry: {'train/lr': 0.00019353061224489793, '_timestamp': 1740956640.836444}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 421 is less than current step: 499. Dropping entry: {'train/loss': 2.2868302941322325, 'train/ce_loss': 0.22978515625, 'train/seg_cls_loss': 0.022802734375, 'train/kl_loss': 0.390234375, 'train/mask_bce_loss': 0.17932060733437538, 'train/mask_dice_loss': 0.8241043150424957, 'train/mask_loss': 1.003424918651581, 'metrics/total_secs_per_batch': 6.314233779907227, 'metrics/data_secs_per_batch': 2.567355489730835, '_timestamp': 1740956647.1501975}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 421 is less than current step: 499. Dropping entry: {'train/lr': 0.0001934081632653061, '_timestamp': 1740956647.1505573}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 422 is less than current step: 499. Dropping entry: {'train/loss': 1.7193188905715941, 'train/ce_loss': 0.410498046875, 'train/seg_cls_loss': 0.01536865234375, 'train/kl_loss': 0.2796875, 'train/mask_bce_loss': 0.09857487548142671, 'train/mask_dice_loss': 0.5380132824182511, 'train/mask_loss': 0.6365881502628327, 'metrics/total_secs_per_batch': 7.111974000930786, 'metrics/data_secs_per_batch': 2.8065473794937135, '_timestamp': 1740956654.2622645}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 422 is less than current step: 499. Dropping entry: {'train/lr': 0.00019328571428571425, '_timestamp': 1740956654.2625902}).
Epoch: [1][424/500]	Time  5.910 ( 5.910)	Loss 0.7578 (1.3993)	CeLoss 0.7578 (0.4389)	SegCLSLoss 0.0000 (0.0131)	KLLoss 0.0000 (0.1977)	MaskLoss 0.0000 (0.4669)	MaskBCELoss 0.0000 (0.1572)	MaskDICELoss 0.0000 (0.3097)
Epoch: [1][425/500]	Time  5.528 ( 5.528)	Loss 1.4375 (2.1033)	CeLoss 1.4375 (0.6287)	SegCLSLoss 0.0000 (0.0129)	KLLoss 0.0000 (0.2816)	MaskLoss 0.0000 (0.7199)	MaskBCELoss 0.0000 (0.2263)	MaskDICELoss 0.0000 (0.4935)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 423 is less than current step: 499. Dropping entry: {'train/loss': 1.399292516708374, 'train/ce_loss': 0.438916015625, 'train/seg_cls_loss': 0.01312255859375, 'train/kl_loss': 0.19765625, 'train/mask_bce_loss': 0.15721629410982133, 'train/mask_dice_loss': 0.3097151190042496, 'train/mask_loss': 0.466931414604187, 'metrics/total_secs_per_batch': 5.909711122512817, 'metrics/data_secs_per_batch': 2.6881053924560545, '_timestamp': 1740956660.1718795}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 423 is less than current step: 499. Dropping entry: {'train/lr': 0.00019316326530612242, '_timestamp': 1740956660.1721818}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 424 is less than current step: 499. Dropping entry: {'train/loss': 2.103257191181183, 'train/ce_loss': 0.6287109375, 'train/seg_cls_loss': 0.012896728515625, 'train/kl_loss': 0.281640625, 'train/mask_bce_loss': 0.2263423591852188, 'train/mask_dice_loss': 0.49354794919490813, 'train/mask_loss': 0.7198902994394303, 'metrics/total_secs_per_batch': 5.527735233306885, 'metrics/data_secs_per_batch': 2.368086099624634, '_timestamp': 1740956665.6996226}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 424 is less than current step: 499. Dropping entry: {'train/lr': 0.0001930408163265306, '_timestamp': 1740956665.6999173}).
Epoch: [1][426/500]	Time  6.196 ( 6.196)	Loss 2.5822 (1.8014)	CeLoss 0.2432 (0.3644)	SegCLSLoss 0.0208 (0.0149)	KLLoss 0.3691 (0.3623)	MaskLoss 1.1456 (0.6967)	MaskBCELoss 0.3264 (0.1781)	MaskDICELoss 0.8192 (0.5186)
Epoch: [1][427/500]	Time  6.628 ( 6.628)	Loss 2.5210 (2.1289)	CeLoss 0.2021 (0.3052)	SegCLSLoss 0.0221 (0.0218)	KLLoss 0.3789 (0.3459)	MaskLoss 1.1355 (0.8894)	MaskBCELoss 0.3135 (0.1304)	MaskDICELoss 0.8221 (0.7590)
Epoch: [1][428/500]	Time  5.344 ( 5.344)	Loss 1.1648 (1.6149)	CeLoss 0.2656 (0.3558)	SegCLSLoss 0.0115 (0.0148)	KLLoss 0.4199 (0.3197)	MaskLoss 0.4261 (0.6099)	MaskBCELoss 0.1852 (0.1050)	MaskDICELoss 0.2409 (0.5049)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 425 is less than current step: 499. Dropping entry: {'train/loss': 1.801404094696045, 'train/ce_loss': 0.36435546875, 'train/seg_cls_loss': 0.014910888671875, 'train/kl_loss': 0.3623046875, 'train/mask_bce_loss': 0.17810707911849022, 'train/mask_dice_loss': 0.5185910671949386, 'train/mask_loss': 0.6966981530189514, 'metrics/total_secs_per_batch': 6.1963417530059814, 'metrics/data_secs_per_batch': 2.6881747245788574, '_timestamp': 1740956671.895936}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 425 is less than current step: 499. Dropping entry: {'train/lr': 0.00019291836734693877, '_timestamp': 1740956671.8962197}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 426 is less than current step: 499. Dropping entry: {'train/loss': 2.1289448142051697, 'train/ce_loss': 0.30517578125, 'train/seg_cls_loss': 0.02181396484375, 'train/kl_loss': 0.3458984375, 'train/mask_bce_loss': 0.13041240777820348, 'train/mask_dice_loss': 0.7589623212814331, 'train/mask_loss': 0.889374727010727, 'metrics/total_secs_per_batch': 6.628356218338013, 'metrics/data_secs_per_batch': 2.902443051338196, '_timestamp': 1740956678.5245092}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 426 is less than current step: 499. Dropping entry: {'train/lr': 0.00019279591836734693, '_timestamp': 1740956678.524866}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 427 is less than current step: 499. Dropping entry: {'train/loss': 1.6148505687713623, 'train/ce_loss': 0.35576171875, 'train/seg_cls_loss': 0.014764404296875, 'train/kl_loss': 0.3197265625, 'train/mask_bce_loss': 0.10496841520071029, 'train/mask_dice_loss': 0.5048982977867127, 'train/mask_loss': 0.6098667114973069, 'metrics/total_secs_per_batch': 5.343633651733398, 'metrics/data_secs_per_batch': 2.683900570869446, '_timestamp': 1740956683.8679066}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 427 is less than current step: 499. Dropping entry: {'train/lr': 0.0001926734693877551, '_timestamp': 1740956683.8682516}).
Epoch: [1][429/500]	Time  5.701 ( 5.701)	Loss 1.4297 (1.8847)	CeLoss 1.4297 (0.6243)	SegCLSLoss 0.0000 (0.0130)	KLLoss 0.0000 (0.2320)	MaskLoss 0.0000 (0.6153)	MaskBCELoss 0.0000 (0.0815)	MaskDICELoss 0.0000 (0.5338)
[2025-03-02 17:04:56,605] [INFO] [logging.py:96:log_dist] [Rank 0] step=930, skipped=0, lr=[0.00019248979591836735], mom=[(0.9, 0.95)]
[2025-03-02 17:04:56,612] [INFO] [timer.py:215:stop] epoch=0/micro_step=9300/global_step=930, RunningAvgSamplesPerSec=1.5152087648382953, CurrSamplesPerSec=1.419980535421156, MemAllocated=30.8GB, MaxMemAllocated=37.19GB
Epoch: [1][430/500]	Time  7.044 ( 7.044)	Loss 2.6925 (1.8642)	CeLoss 0.2021 (0.2396)	SegCLSLoss 0.0320 (0.0150)	KLLoss 0.3730 (0.3600)	MaskLoss 1.2183 (0.7905)	MaskBCELoss 0.3571 (0.1433)	MaskDICELoss 0.8613 (0.6472)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 428 is less than current step: 499. Dropping entry: {'train/loss': 1.884747290611267, 'train/ce_loss': 0.62431640625, 'train/seg_cls_loss': 0.01302490234375, 'train/kl_loss': 0.23203125, 'train/mask_bce_loss': 0.08150206720456481, 'train/mask_dice_loss': 0.5338208019733429, 'train/mask_loss': 0.6153228580951691, 'metrics/total_secs_per_batch': 5.700855731964111, 'metrics/data_secs_per_batch': 2.4274747133255006, '_timestamp': 1740956689.5688016}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 428 is less than current step: 499. Dropping entry: {'train/lr': 0.00019255102040816326, '_timestamp': 1740956689.5690634}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 429 is less than current step: 499. Dropping entry: {'train/loss': 1.8641552209854126, 'train/ce_loss': 0.2396484375, 'train/seg_cls_loss': 0.01502685546875, 'train/kl_loss': 0.3599609375, 'train/mask_bce_loss': 0.1432986579835415, 'train/mask_dice_loss': 0.6471773684024811, 'train/mask_loss': 0.7904760420322419, 'metrics/total_secs_per_batch': 7.0438971519470215, 'metrics/data_secs_per_batch': 3.2528699398040772, '_timestamp': 1740956696.6126018}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 429 is less than current step: 499. Dropping entry: {'train/lr': 0.00019242857142857142, '_timestamp': 1740956696.612942}).
Epoch: [1][431/500]	Time  5.820 ( 5.820)	Loss 1.8447 (1.6129)	CeLoss 0.2393 (0.5189)	SegCLSLoss 0.0217 (0.0116)	KLLoss 0.3887 (0.2367)	MaskLoss 0.7778 (0.5322)	MaskBCELoss 0.0221 (0.1119)	MaskDICELoss 0.7558 (0.4203)
Epoch: [1][432/500]	Time  6.374 ( 6.374)	Loss 2.0227 (1.8494)	CeLoss 0.2051 (0.4870)	SegCLSLoss 0.0266 (0.0126)	KLLoss 0.3848 (0.2805)	MaskLoss 0.8824 (0.6641)	MaskBCELoss 0.0848 (0.1661)	MaskDICELoss 0.7976 (0.4979)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 430 is less than current step: 499. Dropping entry: {'train/loss': 1.612919068336487, 'train/ce_loss': 0.5189453125, 'train/seg_cls_loss': 0.01156005859375, 'train/kl_loss': 0.23671875, 'train/mask_bce_loss': 0.11194206811487675, 'train/mask_dice_loss': 0.42029871642589567, 'train/mask_loss': 0.5322407901287078, 'metrics/total_secs_per_batch': 5.8195013999938965, 'metrics/data_secs_per_batch': 2.3134876489639282, '_timestamp': 1740956702.4321892}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 430 is less than current step: 499. Dropping entry: {'train/lr': 0.00019230612244897958, '_timestamp': 1740956702.4323883}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 431 is less than current step: 499. Dropping entry: {'train/loss': 1.8493912935256958, 'train/ce_loss': 0.48701171875, 'train/seg_cls_loss': 0.012591552734375, 'train/kl_loss': 0.28046875, 'train/mask_bce_loss': 0.1661017592996359, 'train/mask_dice_loss': 0.49794934689998627, 'train/mask_loss': 0.6640511214733124, 'metrics/total_secs_per_batch': 6.374089479446411, 'metrics/data_secs_per_batch': 2.840730333328247, '_timestamp': 1740956708.8063383}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 431 is less than current step: 499. Dropping entry: {'train/lr': 0.00019218367346938775, '_timestamp': 1740956708.8066914}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 432 is less than current step: 499. Dropping entry: {'train/loss': 2.2737899661064147, 'train/ce_loss': 0.23115234375, 'train/seg_cls_loss': 0.022503662109375, 'train/kl_loss': 0.394140625, 'train/mask_bce_loss': 0.23148157075047493, 'train/mask_dice_loss': 0.7644466191530228, 'train/mask_loss': 0.9959281921386719, 'metrics/total_secs_per_batch': 6.130650281906128, 'metrics/data_secs_per_batch': 2.915598654747009, '_timestamp': 1740956714.9370048}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 432 is less than current step: 499. Dropping entry: {'train/lr': 0.0001920612244897959, '_timestamp': 1740956714.9372883}).
Epoch: [1][433/500]	Time  6.131 ( 6.131)	Loss 2.3903 (2.2738)	CeLoss 0.2373 (0.2312)	SegCLSLoss 0.0132 (0.0225)	KLLoss 0.4023 (0.3941)	MaskLoss 1.0526 (0.9959)	MaskBCELoss 0.2828 (0.2315)	MaskDICELoss 0.7698 (0.7644)
Epoch: [1][434/500]	Time  6.880 ( 6.880)	Loss 1.3487 (2.0154)	CeLoss 0.1875 (0.2984)	SegCLSLoss 0.0240 (0.0248)	KLLoss 0.4023 (0.3488)	MaskLoss 0.5547 (0.8349)	MaskBCELoss 0.2094 (0.1522)	MaskDICELoss 0.3454 (0.6827)
Epoch: [1][435/500]	Time  6.166 ( 6.166)	Loss 0.9961 (2.1544)	CeLoss 0.9961 (0.4393)	SegCLSLoss 0.0000 (0.0185)	KLLoss 0.0000 (0.3027)	MaskLoss 0.0000 (0.8378)	MaskBCELoss 0.0000 (0.1664)	MaskDICELoss 0.0000 (0.6714)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 433 is less than current step: 499. Dropping entry: {'train/loss': 2.0153825163841246, 'train/ce_loss': 0.2984375, 'train/seg_cls_loss': 0.024774169921875, 'train/kl_loss': 0.348828125, 'train/mask_bce_loss': 0.15222039204090834, 'train/mask_dice_loss': 0.6826681077480317, 'train/mask_loss': 0.8348885118961334, 'metrics/total_secs_per_batch': 6.879620790481567, 'metrics/data_secs_per_batch': 2.862666630744934, '_timestamp': 1740956721.8165693}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 433 is less than current step: 499. Dropping entry: {'train/lr': 0.00019193877551020407, '_timestamp': 1740956721.816765}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 434 is less than current step: 499. Dropping entry: {'train/loss': 2.154379630088806, 'train/ce_loss': 0.4392578125, 'train/seg_cls_loss': 0.018548583984375, 'train/kl_loss': 0.302734375, 'train/mask_bce_loss': 0.16642752289772034, 'train/mask_dice_loss': 0.6713580071926117, 'train/mask_loss': 0.8377855300903321, 'metrics/total_secs_per_batch': 6.166112899780273, 'metrics/data_secs_per_batch': 2.5809263706207277, '_timestamp': 1740956727.9829752}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 434 is less than current step: 499. Dropping entry: {'train/lr': 0.00019181632653061224, '_timestamp': 1740956727.9833336}).
Epoch: [1][436/500]	Time  6.715 ( 6.715)	Loss 2.0832 (1.8253)	CeLoss 0.1924 (0.3025)	SegCLSLoss 0.0248 (0.0178)	KLLoss 0.3789 (0.3084)	MaskLoss 0.9200 (0.7416)	MaskBCELoss 0.0846 (0.1721)	MaskDICELoss 0.8354 (0.5696)
Epoch: [1][437/500]	Time  6.479 ( 6.479)	Loss 2.4534 (2.0402)	CeLoss 0.2354 (0.3589)	SegCLSLoss 0.0154 (0.0182)	KLLoss 0.3867 (0.3479)	MaskLoss 1.0861 (0.8187)	MaskBCELoss 0.1495 (0.1574)	MaskDICELoss 0.9366 (0.6613)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 435 is less than current step: 499. Dropping entry: {'train/loss': 1.8252938866615296, 'train/ce_loss': 0.302490234375, 'train/seg_cls_loss': 0.017755126953125, 'train/kl_loss': 0.3083984375, 'train/mask_bce_loss': 0.17207494750618935, 'train/mask_dice_loss': 0.5695514887571335, 'train/mask_loss': 0.7416264295578003, 'metrics/total_secs_per_batch': 6.7151408195495605, 'metrics/data_secs_per_batch': 2.8190638542175295, '_timestamp': 1740956734.6978436}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 435 is less than current step: 499. Dropping entry: {'train/lr': 0.0001916938775510204, '_timestamp': 1740956734.6981575}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 436 is less than current step: 499. Dropping entry: {'train/loss': 2.0401817083358766, 'train/ce_loss': 0.35888671875, 'train/seg_cls_loss': 0.018212890625, 'train/kl_loss': 0.3478515625, 'train/mask_bce_loss': 0.15744426669552922, 'train/mask_dice_loss': 0.661279383301735, 'train/mask_loss': 0.8187236547470093, 'metrics/total_secs_per_batch': 6.47936224937439, 'metrics/data_secs_per_batch': 2.9786403656005858, '_timestamp': 1740956741.177173}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 436 is less than current step: 499. Dropping entry: {'train/lr': 0.00019157142857142856, '_timestamp': 1740956741.1774447}).
Epoch: [1][438/500]	Time  6.225 ( 6.225)	Loss 1.2344 (1.7921)	CeLoss 1.2344 (0.4681)	SegCLSLoss 0.0000 (0.0140)	KLLoss 0.0000 (0.2703)	MaskLoss 0.0000 (0.6451)	MaskBCELoss 0.0000 (0.1500)	MaskDICELoss 0.0000 (0.4951)
Epoch: [1][439/500]	Time  6.083 ( 6.083)	Loss 2.2183 (1.6460)	CeLoss 0.3496 (0.3979)	SegCLSLoss 0.0192 (0.0133)	KLLoss 0.3887 (0.2787)	MaskLoss 0.9100 (0.6069)	MaskBCELoss 0.0529 (0.0977)	MaskDICELoss 0.8570 (0.5092)
[2025-03-02 17:05:59,272] [INFO] [logging.py:96:log_dist] [Rank 0] step=940, skipped=0, lr=[0.00019126530612244896], mom=[(0.9, 0.95)]
[2025-03-02 17:05:59,278] [INFO] [timer.py:215:stop] epoch=0/micro_step=9400/global_step=940, RunningAvgSamplesPerSec=1.5160275086721076, CurrSamplesPerSec=1.7263835885423031, MemAllocated=30.8GB, MaxMemAllocated=37.19GB
Epoch: [1][440/500]	Time  5.794 ( 5.794)	Loss 4.1630 (1.8021)	CeLoss 0.1748 (0.5175)	SegCLSLoss 0.0160 (0.0122)	KLLoss 0.3867 (0.2746)	MaskLoss 1.9711 (0.6257)	MaskBCELoss 1.1859 (0.1758)	MaskDICELoss 0.7853 (0.4499)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 437 is less than current step: 499. Dropping entry: {'train/loss': 1.7920833230018616, 'train/ce_loss': 0.46806640625, 'train/seg_cls_loss': 0.013995361328125, 'train/kl_loss': 0.2703125, 'train/mask_bce_loss': 0.14999163704924284, 'train/mask_dice_loss': 0.49512227773666384, 'train/mask_loss': 0.6451139152050018, 'metrics/total_secs_per_batch': 6.224843263626099, 'metrics/data_secs_per_batch': 2.7776044607162476, '_timestamp': 1740956747.4020128}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 437 is less than current step: 499. Dropping entry: {'train/lr': 0.00019144897959183673, '_timestamp': 1740956747.4022799}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 438 is less than current step: 499. Dropping entry: {'train/loss': 1.6460474252700805, 'train/ce_loss': 0.3978515625, 'train/seg_cls_loss': 0.01334228515625, 'train/kl_loss': 0.2787109375, 'train/mask_bce_loss': 0.09770695585757494, 'train/mask_dice_loss': 0.509154635667801, 'train/mask_loss': 0.6068615853786469, 'metrics/total_secs_per_batch': 6.083340883255005, 'metrics/data_secs_per_batch': 2.9448717832565308, '_timestamp': 1740956753.4853964}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 438 is less than current step: 499. Dropping entry: {'train/lr': 0.0001913265306122449, '_timestamp': 1740956753.4857657}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 439 is less than current step: 499. Dropping entry: {'train/loss': 1.8020758509635926, 'train/ce_loss': 0.51748046875, 'train/seg_cls_loss': 0.01220703125, 'train/kl_loss': 0.274609375, 'train/mask_bce_loss': 0.17576322555541993, 'train/mask_dice_loss': 0.4499328821897507, 'train/mask_loss': 0.625696113705635, 'metrics/total_secs_per_batch': 5.79412579536438, 'metrics/data_secs_per_batch': 2.5181072473526003, '_timestamp': 1740956759.2793827}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 439 is less than current step: 499. Dropping entry: {'train/lr': 0.00019120408163265305, '_timestamp': 1740956759.2796872}).
Epoch: [1][441/500]	Time  5.782 ( 5.782)	Loss 1.1875 (2.0222)	CeLoss 1.1875 (0.4547)	SegCLSLoss 0.0000 (0.0171)	KLLoss 0.0000 (0.3104)	MaskLoss 0.0000 (0.7639)	MaskBCELoss 0.0000 (0.1648)	MaskDICELoss 0.0000 (0.5991)
Epoch: [1][442/500]	Time  6.107 ( 6.107)	Loss 1.1875 (2.0210)	CeLoss 1.1875 (0.4666)	SegCLSLoss 0.0000 (0.0202)	KLLoss 0.0000 (0.3111)	MaskLoss 0.0000 (0.7567)	MaskBCELoss 0.0000 (0.2144)	MaskDICELoss 0.0000 (0.5423)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 440 is less than current step: 499. Dropping entry: {'train/loss': 2.022158122062683, 'train/ce_loss': 0.4546875, 'train/seg_cls_loss': 0.01712646484375, 'train/kl_loss': 0.3103515625, 'train/mask_bce_loss': 0.16480305511504412, 'train/mask_dice_loss': 0.5991080403327942, 'train/mask_loss': 0.7639110922813416, 'metrics/total_secs_per_batch': 5.781684398651123, 'metrics/data_secs_per_batch': 2.908011722564697, '_timestamp': 1740956765.061395}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 440 is less than current step: 499. Dropping entry: {'train/lr': 0.00019108163265306121, '_timestamp': 1740956765.061717}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 441 is less than current step: 499. Dropping entry: {'train/loss': 2.0210155487060546, 'train/ce_loss': 0.4666015625, 'train/seg_cls_loss': 0.02015380859375, 'train/kl_loss': 0.3111328125, 'train/mask_bce_loss': 0.21443182341754435, 'train/mask_dice_loss': 0.5422673463821411, 'train/mask_loss': 0.7566991806030273, 'metrics/total_secs_per_batch': 6.106890678405762, 'metrics/data_secs_per_batch': 2.9988059282302855, '_timestamp': 1740956771.1681483}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 441 is less than current step: 499. Dropping entry: {'train/lr': 0.00019095918367346935, '_timestamp': 1740956771.1684372}).
Epoch: [1][443/500]	Time  6.023 ( 6.023)	Loss 0.8672 (1.4334)	CeLoss 0.8672 (0.3648)	SegCLSLoss 0.0000 (0.0157)	KLLoss 0.0000 (0.2328)	MaskLoss 0.0000 (0.5188)	MaskBCELoss 0.0000 (0.0911)	MaskDICELoss 0.0000 (0.4277)
Epoch: [1][444/500]	Time  7.337 ( 7.337)	Loss 0.1475 (1.7266)	CeLoss 0.1475 (0.2218)	SegCLSLoss 0.0000 (0.0152)	KLLoss 0.0000 (0.3113)	MaskLoss 0.0000 (0.7332)	MaskBCELoss 0.0000 (0.1679)	MaskDICELoss 0.0000 (0.5653)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 442 is less than current step: 499. Dropping entry: {'train/loss': 1.4333740293979644, 'train/ce_loss': 0.364794921875, 'train/seg_cls_loss': 0.01572265625, 'train/kl_loss': 0.2328125, 'train/mask_bce_loss': 0.09110575094819069, 'train/mask_dice_loss': 0.42768087685108186, 'train/mask_loss': 0.5187866240739822, 'metrics/total_secs_per_batch': 6.023248195648193, 'metrics/data_secs_per_batch': 2.986492967605591, '_timestamp': 1740956777.191411}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 442 is less than current step: 499. Dropping entry: {'train/lr': 0.0001908367346938775, '_timestamp': 1740956777.191729}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 443 is less than current step: 499. Dropping entry: {'train/loss': 1.7265731573104859, 'train/ce_loss': 0.22177734375, 'train/seg_cls_loss': 0.015234375, 'train/kl_loss': 0.311328125, 'train/mask_bce_loss': 0.16786965895444156, 'train/mask_dice_loss': 0.5653387665748596, 'train/mask_loss': 0.7332084238529205, 'metrics/total_secs_per_batch': 7.336778163909912, 'metrics/data_secs_per_batch': 3.319187617301941, '_timestamp': 1740956784.528199}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 443 is less than current step: 499. Dropping entry: {'train/lr': 0.00019071428571428568, '_timestamp': 1740956784.5284889}).
Epoch: [1][445/500]	Time  7.108 ( 7.108)	Loss 3.5496 (2.1907)	CeLoss 0.2656 (0.2697)	SegCLSLoss 0.0154 (0.0223)	KLLoss 0.4043 (0.3498)	MaskLoss 1.6176 (0.9376)	MaskBCELoss 1.0133 (0.2399)	MaskDICELoss 0.6043 (0.6976)
Epoch: [1][446/500]	Time  5.682 ( 5.682)	Loss 1.7969 (1.8848)	CeLoss 1.7969 (0.5445)	SegCLSLoss 0.0000 (0.0154)	KLLoss 0.0000 (0.2734)	MaskLoss 0.0000 (0.6526)	MaskBCELoss 0.0000 (0.1195)	MaskDICELoss 0.0000 (0.5331)
Epoch: [1][447/500]	Time  6.161 ( 6.161)	Loss 2.5678 (1.8431)	CeLoss 0.2031 (0.3427)	SegCLSLoss 0.0222 (0.0178)	KLLoss 0.3789 (0.2721)	MaskLoss 1.1579 (0.7321)	MaskBCELoss 0.1725 (0.1496)	MaskDICELoss 0.9854 (0.5825)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 444 is less than current step: 499. Dropping entry: {'train/loss': 2.1907472610473633, 'train/ce_loss': 0.2697265625, 'train/seg_cls_loss': 0.02232666015625, 'train/kl_loss': 0.3498046875, 'train/mask_bce_loss': 0.23991253450512887, 'train/mask_dice_loss': 0.6976485908031463, 'train/mask_loss': 0.9375611305236816, 'metrics/total_secs_per_batch': 7.10841178894043, 'metrics/data_secs_per_batch': 3.756438159942627, '_timestamp': 1740956791.6366029}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 444 is less than current step: 499. Dropping entry: {'train/lr': 0.00019059183673469384, '_timestamp': 1740956791.6369674}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 445 is less than current step: 499. Dropping entry: {'train/loss': 1.8848116517066955, 'train/ce_loss': 0.54453125, 'train/seg_cls_loss': 0.015399169921875, 'train/kl_loss': 0.2734375, 'train/mask_bce_loss': 0.11950510293245316, 'train/mask_dice_loss': 0.5330569684505463, 'train/mask_loss': 0.6525620698928833, 'metrics/total_secs_per_batch': 5.681867361068726, 'metrics/data_secs_per_batch': 2.3359615802764893, '_timestamp': 1740956797.3184884}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 445 is less than current step: 499. Dropping entry: {'train/lr': 0.000190469387755102, '_timestamp': 1740956797.3187735}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 446 is less than current step: 499. Dropping entry: {'train/loss': 1.8430965781211852, 'train/ce_loss': 0.34267578125, 'train/seg_cls_loss': 0.01783447265625, 'train/kl_loss': 0.2720703125, 'train/mask_bce_loss': 0.14959722869098185, 'train/mask_dice_loss': 0.5824979186058045, 'train/mask_loss': 0.7320951521396637, 'metrics/total_secs_per_batch': 6.160616636276245, 'metrics/data_secs_per_batch': 2.776651167869568, '_timestamp': 1740956803.4793065}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 446 is less than current step: 499. Dropping entry: {'train/lr': 0.00019034693877551017, '_timestamp': 1740956803.4796634}).
Epoch: [1][448/500]	Time  6.074 ( 6.074)	Loss 1.5156 (2.0506)	CeLoss 1.5156 (0.5034)	SegCLSLoss 0.0000 (0.0143)	KLLoss 0.0000 (0.3146)	MaskLoss 0.0000 (0.7542)	MaskBCELoss 0.0000 (0.1802)	MaskDICELoss 0.0000 (0.5740)
Epoch: [1][449/500]	Time  6.451 ( 6.451)	Loss 2.4275 (1.7498)	CeLoss 0.2793 (0.4836)	SegCLSLoss 0.0184 (0.0145)	KLLoss 0.3926 (0.2760)	MaskLoss 1.0497 (0.6156)	MaskBCELoss 0.0558 (0.0964)	MaskDICELoss 0.9939 (0.5192)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 447 is less than current step: 499. Dropping entry: {'train/loss': 2.050581169128418, 'train/ce_loss': 0.50341796875, 'train/seg_cls_loss': 0.01429443359375, 'train/kl_loss': 0.3146484375, 'train/mask_bce_loss': 0.18018127214163543, 'train/mask_dice_loss': 0.5740155518054962, 'train/mask_loss': 0.7541968345642089, 'metrics/total_secs_per_batch': 6.073689937591553, 'metrics/data_secs_per_batch': 2.9062992334365845, '_timestamp': 1740956809.5527472}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 447 is less than current step: 499. Dropping entry: {'train/lr': 0.00019022448979591836, '_timestamp': 1740956809.553027}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 448 is less than current step: 499. Dropping entry: {'train/loss': 1.7497578859329224, 'train/ce_loss': 0.48359375, 'train/seg_cls_loss': 0.01448974609375, 'train/kl_loss': 0.2759765625, 'train/mask_bce_loss': 0.09635768420994281, 'train/mask_dice_loss': 0.5191950976848603, 'train/mask_loss': 0.6155527830123901, 'metrics/total_secs_per_batch': 6.450855731964111, 'metrics/data_secs_per_batch': 2.845897364616394, '_timestamp': 1740956816.0035732}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 448 is less than current step: 499. Dropping entry: {'train/lr': 0.00019010204081632652, '_timestamp': 1740956816.0038571}).
[2025-03-02 17:07:01,907] [INFO] [logging.py:96:log_dist] [Rank 0] step=950, skipped=0, lr=[0.00019004081632653061], mom=[(0.9, 0.95)]
[2025-03-02 17:07:01,910] [INFO] [timer.py:215:stop] epoch=0/micro_step=9500/global_step=950, RunningAvgSamplesPerSec=1.5168385534620905, CurrSamplesPerSec=1.6933121279595793, MemAllocated=30.81GB, MaxMemAllocated=37.19GB
Epoch: [1][450/500]	Time  5.907 ( 5.907)	Loss 1.1153 (1.8800)	CeLoss 0.1895 (0.4743)	SegCLSLoss 0.0212 (0.0162)	KLLoss 0.3984 (0.3160)	MaskLoss 0.4375 (0.6830)	MaskBCELoss 0.0717 (0.1595)	MaskDICELoss 0.3658 (0.5234)
Epoch: [1][451/500]	Time  5.504 ( 5.504)	Loss 1.2422 (2.2623)	CeLoss 1.2422 (0.4288)	SegCLSLoss 0.0000 (0.0223)	KLLoss 0.0000 (0.3176)	MaskLoss 0.0000 (0.8953)	MaskBCELoss 0.0000 (0.2971)	MaskDICELoss 0.0000 (0.5982)
Epoch: [1][452/500]	Time  5.640 ( 5.640)	Loss 1.0859 (1.7528)	CeLoss 1.0859 (0.4993)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.2809)	MaskLoss 0.0000 (0.6091)	MaskBCELoss 0.0000 (0.1141)	MaskDICELoss 0.0000 (0.4950)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 449 is less than current step: 499. Dropping entry: {'train/loss': 1.8799797415733337, 'train/ce_loss': 0.47431640625, 'train/seg_cls_loss': 0.016156005859375, 'train/kl_loss': 0.316015625, 'train/mask_bce_loss': 0.1595376580953598, 'train/mask_dice_loss': 0.5234209448099136, 'train/mask_loss': 0.6829586118459702, 'metrics/total_secs_per_batch': 5.90713357925415, 'metrics/data_secs_per_batch': 2.4397052764892577, '_timestamp': 1740956821.9105585}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 449 is less than current step: 499. Dropping entry: {'train/lr': 0.00018997959183673468, '_timestamp': 1740956821.9108357}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 450 is less than current step: 499. Dropping entry: {'train/loss': 2.2623183965682983, 'train/ce_loss': 0.42880859375, 'train/seg_cls_loss': 0.0223388671875, 'train/kl_loss': 0.317578125, 'train/mask_bce_loss': 0.2971153374761343, 'train/mask_dice_loss': 0.5981552004814148, 'train/mask_loss': 0.8952705323696136, 'metrics/total_secs_per_batch': 5.504056692123413, 'metrics/data_secs_per_batch': 2.5168516874313354, '_timestamp': 1740956827.414977}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 450 is less than current step: 499. Dropping entry: {'train/lr': 0.00018985714285714284, '_timestamp': 1740956827.4153192}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 451 is less than current step: 499. Dropping entry: {'train/loss': 1.7528149008750915, 'train/ce_loss': 0.49931640625, 'train/seg_cls_loss': 0.01422119140625, 'train/kl_loss': 0.280859375, 'train/mask_bce_loss': 0.11413351185619831, 'train/mask_dice_loss': 0.49498877823352816, 'train/mask_loss': 0.6091223001480103, 'metrics/total_secs_per_batch': 5.639572858810425, 'metrics/data_secs_per_batch': 2.5879382133483886, '_timestamp': 1740956833.0544057}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 451 is less than current step: 499. Dropping entry: {'train/lr': 0.000189734693877551, '_timestamp': 1740956833.0546937}).
Epoch: [1][453/500]	Time  5.534 ( 5.534)	Loss 1.2656 (1.7918)	CeLoss 1.2656 (0.4887)	SegCLSLoss 0.0000 (0.0168)	KLLoss 0.0000 (0.2848)	MaskLoss 0.0000 (0.6330)	MaskBCELoss 0.0000 (0.2051)	MaskDICELoss 0.0000 (0.4280)
Epoch: [1][454/500]	Time  6.468 ( 6.468)	Loss 2.5395 (2.0098)	CeLoss 0.1943 (0.3783)	SegCLSLoss 0.0200 (0.0156)	KLLoss 0.3867 (0.3182)	MaskLoss 1.1486 (0.7961)	MaskBCELoss 0.3418 (0.1716)	MaskDICELoss 0.8068 (0.6244)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 452 is less than current step: 499. Dropping entry: {'train/loss': 1.7918167948722838, 'train/ce_loss': 0.488671875, 'train/seg_cls_loss': 0.016766357421875, 'train/kl_loss': 0.284765625, 'train/mask_bce_loss': 0.20506303068250417, 'train/mask_dice_loss': 0.427954763174057, 'train/mask_loss': 0.6330177783966064, 'metrics/total_secs_per_batch': 5.534201622009277, 'metrics/data_secs_per_batch': 2.7574441909790037, '_timestamp': 1740956838.5885873}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 452 is less than current step: 499. Dropping entry: {'train/lr': 0.00018961224489795917, '_timestamp': 1740956838.588869}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 453 is less than current step: 499. Dropping entry: {'train/loss': 2.009785544872284, 'train/ce_loss': 0.3783203125, 'train/seg_cls_loss': 0.015557861328125, 'train/kl_loss': 0.3181640625, 'train/mask_bce_loss': 0.17164271250367164, 'train/mask_dice_loss': 0.624412140250206, 'train/mask_loss': 0.7960548579692841, 'metrics/total_secs_per_batch': 6.4683754444122314, 'metrics/data_secs_per_batch': 3.052697491645813, '_timestamp': 1740956845.0572987}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 453 is less than current step: 499. Dropping entry: {'train/lr': 0.00018948979591836733, '_timestamp': 1740956845.0576937}).
Epoch: [1][455/500]	Time  5.944 ( 5.944)	Loss 2.1472 (2.0109)	CeLoss 0.2158 (0.6106)	SegCLSLoss 0.0160 (0.0121)	KLLoss 0.4023 (0.2396)	MaskLoss 0.9417 (0.6852)	MaskBCELoss 0.0218 (0.2352)	MaskDICELoss 0.9200 (0.4500)
Epoch: [1][456/500]	Time  5.098 ( 5.098)	Loss 1.7891 (1.6448)	CeLoss 1.7891 (0.4113)	SegCLSLoss 0.0000 (0.0149)	KLLoss 0.0000 (0.2764)	MaskLoss 0.0000 (0.5993)	MaskBCELoss 0.0000 (0.0835)	MaskDICELoss 0.0000 (0.5158)
Epoch: [1][457/500]	Time  5.849 ( 5.849)	Loss 2.5870 (1.9694)	CeLoss 0.2812 (0.4289)	SegCLSLoss 0.0134 (0.0172)	KLLoss 0.3887 (0.2779)	MaskLoss 1.1304 (0.7520)	MaskBCELoss 0.4869 (0.1548)	MaskDICELoss 0.6435 (0.5973)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 454 is less than current step: 499. Dropping entry: {'train/loss': 2.0108903765678408, 'train/ce_loss': 0.61064453125, 'train/seg_cls_loss': 0.012115478515625, 'train/kl_loss': 0.2396484375, 'train/mask_bce_loss': 0.23522489406168462, 'train/mask_dice_loss': 0.450005441904068, 'train/mask_loss': 0.6852303326129914, 'metrics/total_secs_per_batch': 5.943886756896973, 'metrics/data_secs_per_batch': 2.3273617029190063, '_timestamp': 1740956851.000905}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 454 is less than current step: 499. Dropping entry: {'train/lr': 0.0001893673469387755, '_timestamp': 1740956851.001174}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 455 is less than current step: 499. Dropping entry: {'train/loss': 1.644835317134857, 'train/ce_loss': 0.411328125, 'train/seg_cls_loss': 0.014910888671875, 'train/kl_loss': 0.2763671875, 'train/mask_bce_loss': 0.08345219902694226, 'train/mask_dice_loss': 0.5158209323883056, 'train/mask_loss': 0.5992731213569641, 'metrics/total_secs_per_batch': 5.098179578781128, 'metrics/data_secs_per_batch': 2.187192177772522, '_timestamp': 1740956856.09904}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 455 is less than current step: 499. Dropping entry: {'train/lr': 0.00018924489795918366, '_timestamp': 1740956856.0993125}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 456 is less than current step: 499. Dropping entry: {'train/loss': 1.9693660497665406, 'train/ce_loss': 0.428857421875, 'train/seg_cls_loss': 0.017218017578125, 'train/kl_loss': 0.2779296875, 'train/mask_bce_loss': 0.15475396902766078, 'train/mask_dice_loss': 0.5972874462604523, 'train/mask_loss': 0.7520414173603058, 'metrics/total_secs_per_batch': 5.8490025997161865, 'metrics/data_secs_per_batch': 2.3988099098205566, '_timestamp': 1740956861.9481604}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 456 is less than current step: 499. Dropping entry: {'train/lr': 0.00018912244897959182, '_timestamp': 1740956861.948486}).
Epoch: [1][458/500]	Time  6.068 ( 6.068)	Loss 0.1680 (1.5596)	CeLoss 0.1680 (0.3473)	SegCLSLoss 0.0000 (0.0136)	KLLoss 0.0000 (0.2396)	MaskLoss 0.0000 (0.5907)	MaskBCELoss 0.0000 (0.1500)	MaskDICELoss 0.0000 (0.4407)
Epoch: [1][459/500]	Time  6.617 ( 6.617)	Loss 2.0801 (1.9827)	CeLoss 0.3066 (0.3423)	SegCLSLoss 0.0120 (0.0158)	KLLoss 0.4062 (0.3633)	MaskLoss 0.8633 (0.7981)	MaskBCELoss 0.2921 (0.1877)	MaskDICELoss 0.5712 (0.6104)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 457 is less than current step: 499. Dropping entry: {'train/loss': 1.559599232673645, 'train/ce_loss': 0.347314453125, 'train/seg_cls_loss': 0.01363525390625, 'train/kl_loss': 0.2396484375, 'train/mask_bce_loss': 0.1500026471912861, 'train/mask_dice_loss': 0.44071007072925567, 'train/mask_loss': 0.5907127082347869, 'metrics/total_secs_per_batch': 6.068304777145386, 'metrics/data_secs_per_batch': 2.552895522117615, '_timestamp': 1740956868.0163972}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 457 is less than current step: 499. Dropping entry: {'train/lr': 0.00018899999999999999, '_timestamp': 1740956868.0167053}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 458 is less than current step: 499. Dropping entry: {'train/loss': 1.9826522827148438, 'train/ce_loss': 0.34228515625, 'train/seg_cls_loss': 0.0158203125, 'train/kl_loss': 0.36328125, 'train/mask_bce_loss': 0.1876628492027521, 'train/mask_dice_loss': 0.6104015648365021, 'train/mask_loss': 0.7980644106864929, 'metrics/total_secs_per_batch': 6.616555213928223, 'metrics/data_secs_per_batch': 2.8684207677841185, '_timestamp': 1740956874.6329303}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 458 is less than current step: 499. Dropping entry: {'train/lr': 0.00018887755102040815, '_timestamp': 1740956874.6332307}).
[2025-03-02 17:08:00,267] [INFO] [logging.py:96:log_dist] [Rank 0] step=960, skipped=0, lr=[0.00018881632653061222], mom=[(0.9, 0.95)]
[2025-03-02 17:08:00,273] [INFO] [timer.py:215:stop] epoch=0/micro_step=9600/global_step=960, RunningAvgSamplesPerSec=1.5186603345928258, CurrSamplesPerSec=1.773262500985603, MemAllocated=31.25GB, MaxMemAllocated=37.19GB
Epoch: [1][460/500]	Time  5.641 ( 5.641)	Loss 2.6847 (1.4025)	CeLoss 0.2080 (0.5788)	SegCLSLoss 0.0234 (0.0091)	KLLoss 0.3750 (0.2012)	MaskLoss 1.2135 (0.3995)	MaskBCELoss 0.3391 (0.0683)	MaskDICELoss 0.8743 (0.3311)
Epoch: [1][461/500]	Time  5.153 ( 5.153)	Loss 1.8203 (1.8223)	CeLoss 1.8203 (0.5405)	SegCLSLoss 0.0000 (0.0149)	KLLoss 0.0000 (0.2340)	MaskLoss 0.0000 (0.6253)	MaskBCELoss 0.0000 (0.1211)	MaskDICELoss 0.0000 (0.5042)
Epoch: [1][462/500]	Time  5.589 ( 5.589)	Loss 2.4894 (2.0204)	CeLoss 0.2910 (0.4733)	SegCLSLoss 0.0116 (0.0166)	KLLoss 0.4238 (0.3189)	MaskLoss 1.0748 (0.7534)	MaskBCELoss 0.6811 (0.1954)	MaskDICELoss 0.3937 (0.5580)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 459 is less than current step: 499. Dropping entry: {'train/loss': 1.4024667263031005, 'train/ce_loss': 0.57880859375, 'train/seg_cls_loss': 0.0091064453125, 'train/kl_loss': 0.201171875, 'train/mask_bce_loss': 0.06834557782858611, 'train/mask_dice_loss': 0.33112997114658355, 'train/mask_loss': 0.39947554767131804, 'metrics/total_secs_per_batch': 5.641073226928711, 'metrics/data_secs_per_batch': 2.622338080406189, '_timestamp': 1740956880.2741241}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 459 is less than current step: 499. Dropping entry: {'train/lr': 0.0001887551020408163, '_timestamp': 1740956880.2744749}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 460 is less than current step: 499. Dropping entry: {'train/loss': 1.8223458766937255, 'train/ce_loss': 0.54052734375, 'train/seg_cls_loss': 0.01490478515625, 'train/kl_loss': 0.233984375, 'train/mask_bce_loss': 0.12114494522102177, 'train/mask_dice_loss': 0.5041881501674652, 'train/mask_loss': 0.6253330826759338, 'metrics/total_secs_per_batch': 5.15264368057251, 'metrics/data_secs_per_batch': 2.6003921270370483, '_timestamp': 1740956885.4266737}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 460 is less than current step: 499. Dropping entry: {'train/lr': 0.00018863265306122448, '_timestamp': 1740956885.4269536}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 461 is less than current step: 499. Dropping entry: {'train/loss': 2.020445132255554, 'train/ce_loss': 0.47333984375, 'train/seg_cls_loss': 0.01658935546875, 'train/kl_loss': 0.3189453125, 'train/mask_bce_loss': 0.19542357455939055, 'train/mask_dice_loss': 0.5580118715763092, 'train/mask_loss': 0.7534354537725448, 'metrics/total_secs_per_batch': 5.5888471603393555, 'metrics/data_secs_per_batch': 2.572635865211487, '_timestamp': 1740956891.0156875}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 461 is less than current step: 499. Dropping entry: {'train/lr': 0.00018851020408163264, '_timestamp': 1740956891.0160248}).
Epoch: [1][463/500]	Time  5.799 ( 5.799)	Loss 0.0796 (1.6113)	CeLoss 0.0796 (0.3928)	SegCLSLoss 0.0000 (0.0118)	KLLoss 0.0000 (0.2396)	MaskLoss 0.0000 (0.5944)	MaskBCELoss 0.0000 (0.0858)	MaskDICELoss 0.0000 (0.5086)
Epoch: [1][464/500]	Time  5.607 ( 5.607)	Loss 2.1345 (1.8160)	CeLoss 0.1748 (0.3181)	SegCLSLoss 0.0293 (0.0186)	KLLoss 0.3984 (0.3168)	MaskLoss 0.9525 (0.7286)	MaskBCELoss 0.0279 (0.1518)	MaskDICELoss 0.9246 (0.5768)
Epoch: [1][465/500]	Time  5.439 ( 5.439)	Loss 2.2866 (1.7165)	CeLoss 0.2285 (0.4519)	SegCLSLoss 0.0195 (0.0121)	KLLoss 0.3887 (0.2789)	MaskLoss 1.0046 (0.6154)	MaskBCELoss 0.0159 (0.1072)	MaskDICELoss 0.9887 (0.5083)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 462 is less than current step: 499. Dropping entry: {'train/loss': 1.6113392472267152, 'train/ce_loss': 0.392822265625, 'train/seg_cls_loss': 0.011767578125, 'train/kl_loss': 0.2396484375, 'train/mask_bce_loss': 0.08577041672542692, 'train/mask_dice_loss': 0.508595484495163, 'train/mask_loss': 0.594365906715393, 'metrics/total_secs_per_batch': 5.799432277679443, 'metrics/data_secs_per_batch': 2.494611716270447, '_timestamp': 1740956896.814934}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 462 is less than current step: 499. Dropping entry: {'train/lr': 0.0001883877551020408, '_timestamp': 1740956896.8152275}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 463 is less than current step: 499. Dropping entry: {'train/loss': 1.8160104632377625, 'train/ce_loss': 0.31806640625, 'train/seg_cls_loss': 0.0186279296875, 'train/kl_loss': 0.316796875, 'train/mask_bce_loss': 0.151800967566669, 'train/mask_dice_loss': 0.5768097251653671, 'train/mask_loss': 0.7286107003688812, 'metrics/total_secs_per_batch': 5.606807231903076, 'metrics/data_secs_per_batch': 2.2102676391601563, '_timestamp': 1740956902.4217653}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 463 is less than current step: 499. Dropping entry: {'train/lr': 0.00018826530612244896, '_timestamp': 1740956902.4220514}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 464 is less than current step: 499. Dropping entry: {'train/loss': 1.7165108442306518, 'train/ce_loss': 0.45185546875, 'train/seg_cls_loss': 0.012054443359375, 'train/kl_loss': 0.27890625, 'train/mask_bce_loss': 0.10716423243284226, 'train/mask_dice_loss': 0.5082689106464386, 'train/mask_loss': 0.6154331505298615, 'metrics/total_secs_per_batch': 5.438657283782959, 'metrics/data_secs_per_batch': 2.4945977926254272, '_timestamp': 1740956907.860377}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 464 is less than current step: 499. Dropping entry: {'train/lr': 0.00018814285714285713, '_timestamp': 1740956907.8606613}).
Epoch: [1][466/500]	Time  6.708 ( 6.708)	Loss 1.3202 (1.6607)	CeLoss 0.2305 (0.3391)	SegCLSLoss 0.0117 (0.0150)	KLLoss 0.4160 (0.3555)	MaskLoss 0.5214 (0.6395)	MaskBCELoss 0.0623 (0.1155)	MaskDICELoss 0.4591 (0.5240)
Epoch: [1][467/500]	Time  6.553 ( 6.553)	Loss 1.6675 (1.5554)	CeLoss 0.2070 (0.3978)	SegCLSLoss 0.0259 (0.0128)	KLLoss 0.3906 (0.2771)	MaskLoss 0.7048 (0.5619)	MaskBCELoss 0.1508 (0.0807)	MaskDICELoss 0.5540 (0.4812)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 465 is less than current step: 499. Dropping entry: {'train/loss': 1.6607304096221924, 'train/ce_loss': 0.3390625, 'train/seg_cls_loss': 0.015020751953125, 'train/kl_loss': 0.35546875, 'train/mask_bce_loss': 0.11553302556276321, 'train/mask_dice_loss': 0.5239630371332169, 'train/mask_loss': 0.6394960671663285, 'metrics/total_secs_per_batch': 6.70807957649231, 'metrics/data_secs_per_batch': 2.600423574447632, '_timestamp': 1740956914.5684936}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 465 is less than current step: 499. Dropping entry: {'train/lr': 0.0001880204081632653, '_timestamp': 1740956914.5687747}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 466 is less than current step: 499. Dropping entry: {'train/loss': 1.5554418087005615, 'train/ce_loss': 0.39775390625, 'train/seg_cls_loss': 0.01280517578125, 'train/kl_loss': 0.2771484375, 'train/mask_bce_loss': 0.08073426317423582, 'train/mask_dice_loss': 0.48121515214443206, 'train/mask_loss': 0.5619494140148162, 'metrics/total_secs_per_batch': 6.553277015686035, 'metrics/data_secs_per_batch': 3.013862872123718, '_timestamp': 1740956921.1217713}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 466 is less than current step: 499. Dropping entry: {'train/lr': 0.00018789795918367345, '_timestamp': 1740956921.1220834}).
Epoch: [1][468/500]	Time  6.159 ( 6.159)	Loss 1.3646 (1.6286)	CeLoss 0.2734 (0.3541)	SegCLSLoss 0.0145 (0.0166)	KLLoss 0.4668 (0.3180)	MaskLoss 0.5182 (0.6172)	MaskBCELoss 0.0429 (0.0759)	MaskDICELoss 0.4754 (0.5413)
Epoch: [1][469/500]	Time  5.076 ( 5.076)	Loss 2.7411 (1.6264)	CeLoss 0.2471 (0.6422)	SegCLSLoss 0.0221 (0.0115)	KLLoss 0.3848 (0.2354)	MaskLoss 1.2221 (0.4774)	MaskBCELoss 0.2402 (0.0893)	MaskDICELoss 0.9819 (0.3880)
[2025-03-02 17:08:58,056] [INFO] [logging.py:96:log_dist] [Rank 0] step=970, skipped=0, lr=[0.00018759183673469385], mom=[(0.9, 0.95)]
[2025-03-02 17:08:58,062] [INFO] [timer.py:215:stop] epoch=0/micro_step=9700/global_step=970, RunningAvgSamplesPerSec=1.520585860171298, CurrSamplesPerSec=1.7528891319890865, MemAllocated=31.26GB, MaxMemAllocated=37.19GB
Epoch: [1][470/500]	Time  5.707 ( 5.707)	Loss 1.9310 (1.5241)	CeLoss 0.1709 (0.4597)	SegCLSLoss 0.0225 (0.0115)	KLLoss 0.3848 (0.2406)	MaskLoss 0.8552 (0.5172)	MaskBCELoss 0.0147 (0.0712)	MaskDICELoss 0.8404 (0.4460)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 467 is less than current step: 499. Dropping entry: {'train/loss': 1.6285600125789643, 'train/ce_loss': 0.3541015625, 'train/seg_cls_loss': 0.016644287109375, 'train/kl_loss': 0.31796875, 'train/mask_bce_loss': 0.0758732500486076, 'train/mask_dice_loss': 0.5412876099348068, 'train/mask_loss': 0.6171608656644821, 'metrics/total_secs_per_batch': 6.158682584762573, 'metrics/data_secs_per_batch': 2.5340031147003175, '_timestamp': 1740956927.2806032}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 467 is less than current step: 499. Dropping entry: {'train/lr': 0.0001877755102040816, '_timestamp': 1740956927.2809443}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 468 is less than current step: 499. Dropping entry: {'train/loss': 1.626419758796692, 'train/ce_loss': 0.6421875, 'train/seg_cls_loss': 0.011468505859375, 'train/kl_loss': 0.2353515625, 'train/mask_bce_loss': 0.08934615105390549, 'train/mask_dice_loss': 0.3880238741636276, 'train/mask_loss': 0.477370023727417, 'metrics/total_secs_per_batch': 5.0758936405181885, 'metrics/data_secs_per_batch': 2.3811424493789675, '_timestamp': 1740956932.3564444}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 468 is less than current step: 499. Dropping entry: {'train/lr': 0.00018765306122448975, '_timestamp': 1740956932.3567736}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 469 is less than current step: 499. Dropping entry: {'train/loss': 1.524132490158081, 'train/ce_loss': 0.45966796875, 'train/seg_cls_loss': 0.01151123046875, 'train/kl_loss': 0.240625, 'train/mask_bce_loss': 0.07119502881541848, 'train/mask_dice_loss': 0.44604699313640594, 'train/mask_loss': 0.5172420263290405, 'metrics/total_secs_per_batch': 5.706607341766357, 'metrics/data_secs_per_batch': 2.8251081466674806, '_timestamp': 1740956938.0628517}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 469 is less than current step: 499. Dropping entry: {'train/lr': 0.00018753061224489792, '_timestamp': 1740956938.063175}).
Epoch: [1][471/500]	Time  4.525 ( 4.525)	Loss 1.8507 (1.3620)	CeLoss 0.2969 (0.7545)	SegCLSLoss 0.0147 (0.0049)	KLLoss 0.4023 (0.1195)	MaskLoss 0.7525 (0.2964)	MaskBCELoss 0.2369 (0.1104)	MaskDICELoss 0.5156 (0.1860)
Epoch: [1][472/500]	Time  5.118 ( 5.118)	Loss 2.4590 (1.8069)	CeLoss 0.2432 (0.3966)	SegCLSLoss 0.0232 (0.0172)	KLLoss 0.3926 (0.2791)	MaskLoss 1.0830 (0.6870)	MaskBCELoss 0.1494 (0.1309)	MaskDICELoss 0.9337 (0.5561)
Epoch: [1][473/500]	Time  5.780 ( 5.780)	Loss 2.4238 (1.8371)	CeLoss 0.2080 (0.3861)	SegCLSLoss 0.0156 (0.0182)	KLLoss 0.4219 (0.2684)	MaskLoss 1.0830 (0.7075)	MaskBCELoss 0.3126 (0.1417)	MaskDICELoss 0.7704 (0.5658)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 470 is less than current step: 499. Dropping entry: {'train/loss': 1.3620144486427308, 'train/ce_loss': 0.7544921875, 'train/seg_cls_loss': 0.004937744140625, 'train/kl_loss': 0.11953125, 'train/mask_bce_loss': 0.1104141891002655, 'train/mask_dice_loss': 0.18602271676063536, 'train/mask_loss': 0.29643691182136533, 'metrics/total_secs_per_batch': 4.525206565856934, 'metrics/data_secs_per_batch': 1.6776684284210206, '_timestamp': 1740956942.588202}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 470 is less than current step: 499. Dropping entry: {'train/lr': 0.00018740816326530613, '_timestamp': 1740956942.5885599}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 471 is less than current step: 499. Dropping entry: {'train/loss': 1.806867241859436, 'train/ce_loss': 0.39658203125, 'train/seg_cls_loss': 0.017169189453125, 'train/kl_loss': 0.2791015625, 'train/mask_bce_loss': 0.1309118464589119, 'train/mask_dice_loss': 0.5560666918754578, 'train/mask_loss': 0.686978530883789, 'metrics/total_secs_per_batch': 5.117895126342773, 'metrics/data_secs_per_batch': 2.237276887893677, '_timestamp': 1740956947.7062693}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 471 is less than current step: 499. Dropping entry: {'train/lr': 0.0001872857142857143, '_timestamp': 1740956947.706612}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 472 is less than current step: 499. Dropping entry: {'train/loss': 1.8370540142059326, 'train/ce_loss': 0.386083984375, 'train/seg_cls_loss': 0.01817626953125, 'train/kl_loss': 0.268359375, 'train/mask_bce_loss': 0.14173973333090545, 'train/mask_dice_loss': 0.565800952911377, 'train/mask_loss': 0.7075406849384308, 'metrics/total_secs_per_batch': 5.780061721801758, 'metrics/data_secs_per_batch': 2.8643704891204833, '_timestamp': 1740956953.4863825}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 472 is less than current step: 499. Dropping entry: {'train/lr': 0.00018716326530612243, '_timestamp': 1740956953.4867435}).
Epoch: [1][474/500]	Time  6.507 ( 6.507)	Loss 1.5651 (1.9245)	CeLoss 0.2656 (0.3534)	SegCLSLoss 0.0161 (0.0161)	KLLoss 0.4062 (0.3615)	MaskLoss 0.6253 (0.7635)	MaskBCELoss 0.0862 (0.1312)	MaskDICELoss 0.5391 (0.6323)
Epoch: [1][475/500]	Time  6.029 ( 6.029)	Loss 2.5758 (1.7069)	CeLoss 0.1260 (0.3517)	SegCLSLoss 0.0383 (0.0156)	KLLoss 0.3828 (0.2764)	MaskLoss 1.1961 (0.6600)	MaskBCELoss 0.4566 (0.1736)	MaskDICELoss 0.7395 (0.4863)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 473 is less than current step: 499. Dropping entry: {'train/loss': 1.9244855880737304, 'train/ce_loss': 0.35341796875, 'train/seg_cls_loss': 0.01605224609375, 'train/kl_loss': 0.3615234375, 'train/mask_bce_loss': 0.1312481028959155, 'train/mask_dice_loss': 0.6322642236948013, 'train/mask_loss': 0.7635123133659363, 'metrics/total_secs_per_batch': 6.506657123565674, 'metrics/data_secs_per_batch': 2.9085428714752197, '_timestamp': 1740956959.9928486}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 473 is less than current step: 499. Dropping entry: {'train/lr': 0.0001870408163265306, '_timestamp': 1740956959.993214}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 474 is less than current step: 499. Dropping entry: {'train/loss': 1.7069120287895203, 'train/ce_loss': 0.35166015625, 'train/seg_cls_loss': 0.015570068359375, 'train/kl_loss': 0.2763671875, 'train/mask_bce_loss': 0.17364921532571315, 'train/mask_dice_loss': 0.48630093038082123, 'train/mask_loss': 0.6599501430988312, 'metrics/total_secs_per_batch': 6.028891086578369, 'metrics/data_secs_per_batch': 2.827576780319214, '_timestamp': 1740956966.0217285}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 474 is less than current step: 499. Dropping entry: {'train/lr': 0.00018691836734693876, '_timestamp': 1740956966.0220397}).
Epoch: [1][476/500]	Time  5.623 ( 5.623)	Loss 1.4844 (2.0055)	CeLoss 1.4844 (0.6546)	SegCLSLoss 0.0000 (0.0180)	KLLoss 0.0000 (0.2770)	MaskLoss 0.0000 (0.6570)	MaskBCELoss 0.0000 (0.1162)	MaskDICELoss 0.0000 (0.5408)
Epoch: [1][477/500]	Time  6.301 ( 6.301)	Loss 2.3747 (1.6958)	CeLoss 0.1504 (0.4101)	SegCLSLoss 0.0476 (0.0137)	KLLoss 0.3770 (0.2797)	MaskLoss 1.0819 (0.6255)	MaskBCELoss 0.3065 (0.1263)	MaskDICELoss 0.7754 (0.4992)
Epoch: [1][478/500]	Time  5.730 ( 5.730)	Loss 3.7852 (1.8118)	CeLoss 0.3516 (0.6506)	SegCLSLoss 0.0160 (0.0112)	KLLoss 0.3867 (0.2395)	MaskLoss 1.6934 (0.5659)	MaskBCELoss 0.9065 (0.1861)	MaskDICELoss 0.7869 (0.3797)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 475 is less than current step: 499. Dropping entry: {'train/loss': 2.0054780006408692, 'train/ce_loss': 0.654638671875, 'train/seg_cls_loss': 0.01795654296875, 'train/kl_loss': 0.276953125, 'train/mask_bce_loss': 0.1161895826458931, 'train/mask_dice_loss': 0.5408462941646576, 'train/mask_loss': 0.6570358753204346, 'metrics/total_secs_per_batch': 5.622529745101929, 'metrics/data_secs_per_batch': 2.6801084995269777, '_timestamp': 1740956971.644336}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 475 is less than current step: 499. Dropping entry: {'train/lr': 0.00018679591836734692, '_timestamp': 1740956971.6446714}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 476 is less than current step: 499. Dropping entry: {'train/loss': 1.6958281874656678, 'train/ce_loss': 0.41005859375, 'train/seg_cls_loss': 0.013714599609375, 'train/kl_loss': 0.2796875, 'train/mask_bce_loss': 0.1262641130015254, 'train/mask_dice_loss': 0.4991890132427216, 'train/mask_loss': 0.6254531264305114, 'metrics/total_secs_per_batch': 6.30127739906311, 'metrics/data_secs_per_batch': 2.6957294225692747, '_timestamp': 1740956977.9454975}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 476 is less than current step: 499. Dropping entry: {'train/lr': 0.00018667346938775508, '_timestamp': 1740956977.9457731}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 477 is less than current step: 499. Dropping entry: {'train/loss': 1.8118290305137634, 'train/ce_loss': 0.6505859375, 'train/seg_cls_loss': 0.0112060546875, 'train/kl_loss': 0.239453125, 'train/mask_bce_loss': 0.18614648915827275, 'train/mask_dice_loss': 0.3797289803624153, 'train/mask_loss': 0.5658754646778107, 'metrics/total_secs_per_batch': 5.7301506996154785, 'metrics/data_secs_per_batch': 2.819705939292908, '_timestamp': 1740956983.6757076}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 477 is less than current step: 499. Dropping entry: {'train/lr': 0.00018655102040816325, '_timestamp': 1740956983.675983}).
Epoch: [1][479/500]	Time  7.162 ( 7.162)	Loss 2.4890 (2.1968)	CeLoss 0.2832 (0.2329)	SegCLSLoss 0.0216 (0.0213)	KLLoss 0.3770 (0.3932)	MaskLoss 1.0785 (0.9570)	MaskBCELoss 0.2321 (0.1708)	MaskDICELoss 0.8464 (0.7861)
[2025-03-02 17:09:56,533] [INFO] [logging.py:96:log_dist] [Rank 0] step=980, skipped=0, lr=[0.0001863673469387755], mom=[(0.9, 0.95)]
[2025-03-02 17:09:56,539] [INFO] [timer.py:215:stop] epoch=0/micro_step=9800/global_step=980, RunningAvgSamplesPerSec=1.5223135719275098, CurrSamplesPerSec=1.7539212708676517, MemAllocated=30.82GB, MaxMemAllocated=37.19GB
Epoch: [1][480/500]	Time  5.703 ( 5.703)	Loss 0.9961 (1.7653)	CeLoss 0.9961 (0.5780)	SegCLSLoss 0.0000 (0.0167)	KLLoss 0.0000 (0.2818)	MaskLoss 0.0000 (0.5753)	MaskBCELoss 0.0000 (0.1156)	MaskDICELoss 0.0000 (0.4597)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 478 is less than current step: 499. Dropping entry: {'train/loss': 2.1968129158020018, 'train/ce_loss': 0.23291015625, 'train/seg_cls_loss': 0.021270751953125, 'train/kl_loss': 0.3931640625, 'train/mask_bce_loss': 0.17084411960095167, 'train/mask_dice_loss': 0.7861072540283203, 'train/mask_loss': 0.9569513857364654, 'metrics/total_secs_per_batch': 7.161827325820923, 'metrics/data_secs_per_batch': 3.046595001220703, '_timestamp': 1740956990.8375125}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 478 is less than current step: 499. Dropping entry: {'train/lr': 0.0001864285714285714, '_timestamp': 1740956990.8378243}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 479 is less than current step: 499. Dropping entry: {'train/loss': 1.7653103828430177, 'train/ce_loss': 0.57802734375, 'train/seg_cls_loss': 0.01669921875, 'train/kl_loss': 0.2818359375, 'train/mask_bce_loss': 0.11564971152693033, 'train/mask_dice_loss': 0.45968126356601713, 'train/mask_loss': 0.5753309667110443, 'metrics/total_secs_per_batch': 5.703110218048096, 'metrics/data_secs_per_batch': 2.3756441354751585, '_timestamp': 1740956996.540521}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 479 is less than current step: 499. Dropping entry: {'train/lr': 0.00018630612244897957, '_timestamp': 1740956996.540863}).
Epoch: [1][481/500]	Time  7.005 ( 7.005)	Loss 2.2070 (1.8920)	CeLoss 0.1758 (0.2264)	SegCLSLoss 0.0231 (0.0198)	KLLoss 0.3887 (0.3965)	MaskLoss 0.9902 (0.8082)	MaskBCELoss 0.0132 (0.0834)	MaskDICELoss 0.9770 (0.7248)
Epoch: [1][482/500]	Time  6.494 ( 6.494)	Loss 0.0767 (1.5756)	CeLoss 0.0767 (0.3335)	SegCLSLoss 0.0000 (0.0109)	KLLoss 0.0000 (0.2807)	MaskLoss 0.0000 (0.6043)	MaskBCELoss 0.0000 (0.0890)	MaskDICELoss 0.0000 (0.5153)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 480 is less than current step: 499. Dropping entry: {'train/loss': 1.892001736164093, 'train/ce_loss': 0.2263671875, 'train/seg_cls_loss': 0.019757080078125, 'train/kl_loss': 0.396484375, 'train/mask_bce_loss': 0.08337633945047855, 'train/mask_dice_loss': 0.7247827261686325, 'train/mask_loss': 0.8081590592861175, 'metrics/total_secs_per_batch': 7.0046045780181885, 'metrics/data_secs_per_batch': 3.012958812713623, '_timestamp': 1740957003.5453029}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 480 is less than current step: 499. Dropping entry: {'train/lr': 0.00018618367346938774, '_timestamp': 1740957003.5455878}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 481 is less than current step: 499. Dropping entry: {'train/loss': 1.5756208539009093, 'train/ce_loss': 0.33349609375, 'train/seg_cls_loss': 0.0109375, 'train/kl_loss': 0.2806640625, 'train/mask_bce_loss': 0.08896904587745666, 'train/mask_dice_loss': 0.5153452694416046, 'train/mask_loss': 0.6043143153190613, 'metrics/total_secs_per_batch': 6.494058847427368, 'metrics/data_secs_per_batch': 3.091180658340454, '_timestamp': 1740957010.0392947}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 481 is less than current step: 499. Dropping entry: {'train/lr': 0.0001860612244897959, '_timestamp': 1740957010.039569}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 482 is less than current step: 499. Dropping entry: {'train/loss': 1.708127737045288, 'train/ce_loss': 0.62802734375, 'train/seg_cls_loss': 0.012158203125, 'train/kl_loss': 0.239453125, 'train/mask_bce_loss': 0.09463026388548315, 'train/mask_dice_loss': 0.4304297029972076, 'train/mask_loss': 0.5250599622726441, 'metrics/total_secs_per_batch': 5.066035747528076, 'metrics/data_secs_per_batch': 2.365866756439209, '_timestamp': 1740957015.1053822}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 482 is less than current step: 499. Dropping entry: {'train/lr': 0.00018593877551020406, '_timestamp': 1740957015.105741}).
Epoch: [1][483/500]	Time  5.066 ( 5.066)	Loss 2.3267 (1.7081)	CeLoss 0.2500 (0.6280)	SegCLSLoss 0.0148 (0.0122)	KLLoss 0.4004 (0.2395)	MaskLoss 1.0149 (0.5251)	MaskBCELoss 0.0534 (0.0946)	MaskDICELoss 0.9615 (0.4304)
Epoch: [1][484/500]	Time  5.312 ( 5.312)	Loss 1.8733 (1.8220)	CeLoss 0.2559 (0.7476)	SegCLSLoss 0.0208 (0.0115)	KLLoss 0.4062 (0.2404)	MaskLoss 0.7833 (0.5223)	MaskBCELoss 0.0550 (0.1332)	MaskDICELoss 0.7283 (0.3890)
Epoch: [1][485/500]	Time  5.894 ( 5.894)	Loss 2.7103 (2.0785)	CeLoss 0.1582 (0.4402)	SegCLSLoss 0.0388 (0.0173)	KLLoss 0.3828 (0.3189)	MaskLoss 1.2472 (0.7989)	MaskBCELoss 0.4087 (0.1525)	MaskDICELoss 0.8385 (0.6464)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 483 is less than current step: 499. Dropping entry: {'train/loss': 1.821994376182556, 'train/ce_loss': 0.74755859375, 'train/seg_cls_loss': 0.0114501953125, 'train/kl_loss': 0.2404296875, 'train/mask_bce_loss': 0.13323802165687085, 'train/mask_dice_loss': 0.3890384644269943, 'train/mask_loss': 0.5222764849662781, 'metrics/total_secs_per_batch': 5.312480211257935, 'metrics/data_secs_per_batch': 1.9659463882446289, '_timestamp': 1740957020.4178605}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 483 is less than current step: 499. Dropping entry: {'train/lr': 0.00018581632653061223, '_timestamp': 1740957020.4181426}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 484 is less than current step: 499. Dropping entry: {'train/loss': 2.0785014629364014, 'train/ce_loss': 0.440234375, 'train/seg_cls_loss': 0.01729736328125, 'train/kl_loss': 0.3189453125, 'train/mask_bce_loss': 0.1524995842948556, 'train/mask_dice_loss': 0.6463702976703644, 'train/mask_loss': 0.7988698780536652, 'metrics/total_secs_per_batch': 5.89370322227478, 'metrics/data_secs_per_batch': 2.7490238428115843, '_timestamp': 1740957026.3117025}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 484 is less than current step: 499. Dropping entry: {'train/lr': 0.0001856938775510204, '_timestamp': 1740957026.3120413}).
Epoch: [1][486/500]	Time  5.851 ( 5.851)	Loss 2.5995 (1.7893)	CeLoss 0.1914 (0.4660)	SegCLSLoss 0.0222 (0.0125)	KLLoss 0.3789 (0.2799)	MaskLoss 1.1797 (0.6446)	MaskBCELoss 0.5387 (0.1171)	MaskDICELoss 0.6410 (0.5275)
Epoch: [1][487/500]	Time  5.344 ( 5.344)	Loss 1.9915 (1.7998)	CeLoss 0.2520 (0.5677)	SegCLSLoss 0.0125 (0.0123)	KLLoss 0.4121 (0.2395)	MaskLoss 0.8463 (0.6011)	MaskBCELoss 0.2524 (0.1832)	MaskDICELoss 0.5940 (0.4179)
Epoch: [1][488/500]	Time  5.270 ( 5.270)	Loss 1.0818 (1.7758)	CeLoss 0.2324 (0.5625)	SegCLSLoss 0.0126 (0.0147)	KLLoss 0.4102 (0.2342)	MaskLoss 0.4013 (0.5912)	MaskBCELoss 0.0478 (0.1288)	MaskDICELoss 0.3534 (0.4625)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 485 is less than current step: 499. Dropping entry: {'train/loss': 1.7892580866813659, 'train/ce_loss': 0.466015625, 'train/seg_cls_loss': 0.01251220703125, 'train/kl_loss': 0.2798828125, 'train/mask_bce_loss': 0.11712582856416702, 'train/mask_dice_loss': 0.5275032103061676, 'train/mask_loss': 0.6446290373802185, 'metrics/total_secs_per_batch': 5.851040601730347, 'metrics/data_secs_per_batch': 2.5539524793624877, '_timestamp': 1740957032.1626313}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 485 is less than current step: 499. Dropping entry: {'train/lr': 0.00018557142857142855, '_timestamp': 1740957032.1629307}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 486 is less than current step: 499. Dropping entry: {'train/loss': 1.7998212218284606, 'train/ce_loss': 0.56767578125, 'train/seg_cls_loss': 0.012322998046875, 'train/kl_loss': 0.239453125, 'train/mask_bce_loss': 0.18319645626470446, 'train/mask_dice_loss': 0.41793484687805177, 'train/mask_loss': 0.6011313080787659, 'metrics/total_secs_per_batch': 5.344297409057617, 'metrics/data_secs_per_batch': 2.0548948526382445, '_timestamp': 1740957037.5069258}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 486 is less than current step: 499. Dropping entry: {'train/lr': 0.00018544897959183671, '_timestamp': 1740957037.507224}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 487 is less than current step: 499. Dropping entry: {'train/loss': 1.77576984167099, 'train/ce_loss': 0.5625, 'train/seg_cls_loss': 0.014654541015625, 'train/kl_loss': 0.2341796875, 'train/mask_bce_loss': 0.1287518197670579, 'train/mask_dice_loss': 0.46245340406894686, 'train/mask_loss': 0.5912052273750306, 'metrics/total_secs_per_batch': 5.269545078277588, 'metrics/data_secs_per_batch': 2.174179768562317, '_timestamp': 1740957042.7764614}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 487 is less than current step: 499. Dropping entry: {'train/lr': 0.00018532653061224488, '_timestamp': 1740957042.7767472}).
Epoch: [1][489/500]	Time  6.023 ( 6.023)	Loss 1.2734 (1.8552)	CeLoss 1.2734 (0.5776)	SegCLSLoss 0.0000 (0.0152)	KLLoss 0.0000 (0.2336)	MaskLoss 0.0000 (0.6233)	MaskBCELoss 0.0000 (0.1685)	MaskDICELoss 0.0000 (0.4548)
[2025-03-02 17:10:54,787] [INFO] [logging.py:96:log_dist] [Rank 0] step=990, skipped=0, lr=[0.0001851428571428571], mom=[(0.9, 0.95)]
[2025-03-02 17:10:54,793] [INFO] [timer.py:215:stop] epoch=0/micro_step=9900/global_step=990, RunningAvgSamplesPerSec=1.5240626190312134, CurrSamplesPerSec=1.6684550009248633, MemAllocated=30.96GB, MaxMemAllocated=37.19GB
Epoch: [1][490/500]	Time  5.995 ( 5.995)	Loss 2.4452 (1.5836)	CeLoss 0.1816 (0.3853)	SegCLSLoss 0.0364 (0.0152)	KLLoss 0.3730 (0.3213)	MaskLoss 1.1039 (0.5793)	MaskBCELoss 0.1883 (0.1386)	MaskDICELoss 0.9156 (0.4407)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 488 is less than current step: 499. Dropping entry: {'train/loss': 1.855230712890625, 'train/ce_loss': 0.57763671875, 'train/seg_cls_loss': 0.0151611328125, 'train/kl_loss': 0.23359375, 'train/mask_bce_loss': 0.16854508891701697, 'train/mask_dice_loss': 0.45477340221405027, 'train/mask_loss': 0.6233184933662415, 'metrics/total_secs_per_batch': 6.023017644882202, 'metrics/data_secs_per_batch': 2.7383069038391112, '_timestamp': 1740957048.7994554}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 488 is less than current step: 499. Dropping entry: {'train/lr': 0.00018520408163265304, '_timestamp': 1740957048.7997332}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 489 is less than current step: 499. Dropping entry: {'train/loss': 1.583635503053665, 'train/ce_loss': 0.38525390625, 'train/seg_cls_loss': 0.015179443359375, 'train/kl_loss': 0.3212890625, 'train/mask_bce_loss': 0.1385805736295879, 'train/mask_dice_loss': 0.4407371684908867, 'train/mask_loss': 0.5793177455663681, 'metrics/total_secs_per_batch': 5.995128154754639, 'metrics/data_secs_per_batch': 2.7367634773254395, '_timestamp': 1740957054.7943792}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 489 is less than current step: 499. Dropping entry: {'train/lr': 0.0001850816326530612, '_timestamp': 1740957054.7946477}).
Epoch: [1][491/500]	Time  5.902 ( 5.902)	Loss 0.5430 (1.7682)	CeLoss 0.5430 (0.5024)	SegCLSLoss 0.0000 (0.0139)	KLLoss 0.0000 (0.2791)	MaskLoss 0.0000 (0.6156)	MaskBCELoss 0.0000 (0.1245)	MaskDICELoss 0.0000 (0.4911)
Epoch: [1][492/500]	Time  6.979 ( 6.979)	Loss 2.5143 (1.9715)	CeLoss 0.2432 (0.2189)	SegCLSLoss 0.0264 (0.0219)	KLLoss 0.3574 (0.3475)	MaskLoss 1.1117 (0.8534)	MaskBCELoss 0.2325 (0.2154)	MaskDICELoss 0.8791 (0.6380)
Epoch: [1][493/500]	Time  6.191 ( 6.191)	Loss 0.0977 (1.6339)	CeLoss 0.0977 (0.4354)	SegCLSLoss 0.0000 (0.0110)	KLLoss 0.0000 (0.2396)	MaskLoss 0.0000 (0.5846)	MaskBCELoss 0.0000 (0.1817)	MaskDICELoss 0.0000 (0.4029)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 490 is less than current step: 499. Dropping entry: {'train/loss': 1.7681979060173034, 'train/ce_loss': 0.50244140625, 'train/seg_cls_loss': 0.01392822265625, 'train/kl_loss': 0.2791015625, 'train/mask_bce_loss': 0.12446061000227929, 'train/mask_dice_loss': 0.4911324858665466, 'train/mask_loss': 0.6155930876731872, 'metrics/total_secs_per_batch': 5.901796340942383, 'metrics/data_secs_per_batch': 2.282127571105957, '_timestamp': 1740957060.6964874}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 490 is less than current step: 499. Dropping entry: {'train/lr': 0.00018495918367346937, '_timestamp': 1740957060.69683}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 491 is less than current step: 499. Dropping entry: {'train/loss': 1.9714834690093994, 'train/ce_loss': 0.218896484375, 'train/seg_cls_loss': 0.021881103515625, 'train/kl_loss': 0.3474609375, 'train/mask_bce_loss': 0.21542324433103205, 'train/mask_dice_loss': 0.6380186825990677, 'train/mask_loss': 0.8534419417381287, 'metrics/total_secs_per_batch': 6.9789817333221436, 'metrics/data_secs_per_batch': 3.0593604326248167, '_timestamp': 1740957067.6754072}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 491 is less than current step: 499. Dropping entry: {'train/lr': 0.00018483673469387753, '_timestamp': 1740957067.6757162}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 492 is less than current step: 499. Dropping entry: {'train/loss': 1.633919894695282, 'train/ce_loss': 0.4353515625, 'train/seg_cls_loss': 0.01104736328125, 'train/kl_loss': 0.2396484375, 'train/mask_bce_loss': 0.18171605579555034, 'train/mask_dice_loss': 0.40291968286037444, 'train/mask_loss': 0.584635728597641, 'metrics/total_secs_per_batch': 6.1908135414123535, 'metrics/data_secs_per_batch': 2.8073418140411377, '_timestamp': 1740957073.8661604}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 492 is less than current step: 499. Dropping entry: {'train/lr': 0.00018471428571428572, '_timestamp': 1740957073.8664362}).
Epoch: [1][494/500]	Time  6.017 ( 6.017)	Loss 2.0540 (1.7979)	CeLoss 0.2139 (0.4674)	SegCLSLoss 0.0177 (0.0130)	KLLoss 0.3945 (0.2785)	MaskLoss 0.8961 (0.6482)	MaskBCELoss 0.0085 (0.0624)	MaskDICELoss 0.8877 (0.5857)
Epoch: [1][495/500]	Time  5.543 ( 5.543)	Loss 1.7761 (1.9714)	CeLoss 0.2871 (0.5973)	SegCLSLoss 0.0125 (0.0164)	KLLoss 0.4062 (0.2717)	MaskLoss 0.7211 (0.6694)	MaskBCELoss 0.1466 (0.1412)	MaskDICELoss 0.5745 (0.5282)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 493 is less than current step: 499. Dropping entry: {'train/loss': 1.7978832364082336, 'train/ce_loss': 0.4673828125, 'train/seg_cls_loss': 0.012957763671875, 'train/kl_loss': 0.278515625, 'train/mask_bce_loss': 0.062422238569706676, 'train/mask_dice_loss': 0.5857381105422974, 'train/mask_loss': 0.6481603562831879, 'metrics/total_secs_per_batch': 6.017207622528076, 'metrics/data_secs_per_batch': 2.6498292684555054, '_timestamp': 1740957079.8835647}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 493 is less than current step: 499. Dropping entry: {'train/lr': 0.00018459183673469388, '_timestamp': 1740957079.883945}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 494 is less than current step: 499. Dropping entry: {'train/loss': 1.971418273448944, 'train/ce_loss': 0.597265625, 'train/seg_cls_loss': 0.01640625, 'train/kl_loss': 0.2716796875, 'train/mask_bce_loss': 0.1411919130012393, 'train/mask_dice_loss': 0.5282086282968521, 'train/mask_loss': 0.6694005489349365, 'metrics/total_secs_per_batch': 5.542727470397949, 'metrics/data_secs_per_batch': 2.4803772449493406, '_timestamp': 1740957085.4262164}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 494 is less than current step: 499. Dropping entry: {'train/lr': 0.00018446938775510205, '_timestamp': 1740957085.4265056}).
Epoch: [1][496/500]	Time  6.654 ( 6.654)	Loss 2.1677 (2.0969)	CeLoss 0.1699 (0.3482)	SegCLSLoss 0.0228 (0.0218)	KLLoss 0.3887 (0.3492)	MaskLoss 0.9735 (0.8513)	MaskBCELoss 0.0239 (0.1567)	MaskDICELoss 0.9497 (0.6946)
Epoch: [1][497/500]	Time  5.454 ( 5.454)	Loss 2.8110 (1.8745)	CeLoss 0.2305 (0.3566)	SegCLSLoss 0.0151 (0.0175)	KLLoss 0.4004 (0.3555)	MaskLoss 1.2668 (0.7368)	MaskBCELoss 0.6518 (0.2679)	MaskDICELoss 0.6150 (0.4690)
Epoch: [1][498/500]	Time  5.440 ( 5.440)	Loss 1.1484 (1.6617)	CeLoss 1.1484 (0.4912)	SegCLSLoss 0.0000 (0.0201)	KLLoss 0.0000 (0.2674)	MaskLoss 0.0000 (0.5668)	MaskBCELoss 0.0000 (0.1283)	MaskDICELoss 0.0000 (0.4385)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 495 is less than current step: 499. Dropping entry: {'train/loss': 2.096909248828888, 'train/ce_loss': 0.3482421875, 'train/seg_cls_loss': 0.02177734375, 'train/kl_loss': 0.34921875, 'train/mask_bce_loss': 0.15668722596019508, 'train/mask_dice_loss': 0.6945994555950165, 'train/mask_loss': 0.8512866765260696, 'metrics/total_secs_per_batch': 6.654212236404419, 'metrics/data_secs_per_batch': 2.928701376914978, '_timestamp': 1740957092.0803435}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 495 is less than current step: 499. Dropping entry: {'train/lr': 0.0001843469387755102, '_timestamp': 1740957092.0806322}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 496 is less than current step: 499. Dropping entry: {'train/loss': 1.8744523763656615, 'train/ce_loss': 0.356640625, 'train/seg_cls_loss': 0.01748046875, 'train/kl_loss': 0.35546875, 'train/mask_bce_loss': 0.26787424739450216, 'train/mask_dice_loss': 0.4689613074064255, 'train/mask_loss': 0.7368355631828308, 'metrics/total_secs_per_batch': 5.454071521759033, 'metrics/data_secs_per_batch': 2.490773844718933, '_timestamp': 1740957097.5344276}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 496 is less than current step: 499. Dropping entry: {'train/lr': 0.00018422448979591837, '_timestamp': 1740957097.534695}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 497 is less than current step: 499. Dropping entry: {'train/loss': 1.6617194056510924, 'train/ce_loss': 0.4912109375, 'train/seg_cls_loss': 0.020111083984375, 'train/kl_loss': 0.2673828125, 'train/mask_bce_loss': 0.12825478091835976, 'train/mask_dice_loss': 0.4385424107313156, 'train/mask_loss': 0.5667971804738045, 'metrics/total_secs_per_batch': 5.44000506401062, 'metrics/data_secs_per_batch': 2.115570402145386, '_timestamp': 1740957102.9745712}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 497 is less than current step: 499. Dropping entry: {'train/lr': 0.00018410204081632654, '_timestamp': 1740957102.9748948}).
Epoch: [1][499/500]	Time  6.568 ( 6.568)	Loss 1.8478 (1.3618)	CeLoss 0.2598 (0.3786)	SegCLSLoss 0.0249 (0.0170)	KLLoss 0.4180 (0.2740)	MaskLoss 0.7676 (0.4739)	MaskBCELoss 0.0634 (0.0818)	MaskDICELoss 0.7043 (0.3921)
  0%|                                                                                                                                                              | 0/200 [00:00<?, ?it/s]
[2025-03-02 17:11:56,257] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[0.00018391836734693877], mom=[(0.9, 0.95)]
[2025-03-02 17:11:56,262] [INFO] [timer.py:215:stop] epoch=0/micro_step=10000/global_step=1000, RunningAvgSamplesPerSec=1.5250309948658591, CurrSamplesPerSec=1.4882948276490375, MemAllocated=30.69GB, MaxMemAllocated=37.19GB

 10%|██████████████▉                                                                                                                                      | 20/200 [00:03<00:22,  8.03it/s][34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 498 is less than current step: 499. Dropping entry: {'train/loss': 1.3618139147758483, 'train/ce_loss': 0.37861328125, 'train/seg_cls_loss': 0.017047119140625, 'train/kl_loss': 0.2740234375, 'train/mask_bce_loss': 0.08182993941009045, 'train/mask_dice_loss': 0.39209460392594336, 'train/mask_loss': 0.47392454743385315, 'metrics/total_secs_per_batch': 6.568157434463501, 'metrics/data_secs_per_batch': 2.806199789047241, '_timestamp': 1740957109.5427537}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 498 is less than current step: 499. Dropping entry: {'train/lr': 0.0001839795918367347, '_timestamp': 1740957109.5430906}).













100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:29<00:00,  6.82it/s]
giou: 0.2325, ciou: 0.2425
[2025-03-02 17:12:28,607] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1000 is about to be saved!
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 1 is less than current step: 499. Dropping entry: {'val/giou': 0.23251710832118988, 'val/ciou': 0.2425401359796524, '_timestamp': 1740957145.5913594}).
[2025-03-02 17:12:42,643] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./runs/plum-13b_kld_0.1_dice_2.0/ckpt_model/global_step1000/mp_rank_00_model_states.pt
[2025-03-02 17:12:42,643] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./runs/plum-13b_kld_0.1_dice_2.0/ckpt_model/global_step1000/mp_rank_00_model_states.pt...
[2025-03-02 17:14:37,783] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./runs/plum-13b_kld_0.1_dice_2.0/ckpt_model/global_step1000/mp_rank_00_model_states.pt.
[2025-03-02 17:14:38,627] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./runs/plum-13b_kld_0.1_dice_2.0/ckpt_model/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-02 17:14:48,141] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./runs/plum-13b_kld_0.1_dice_2.0/ckpt_model/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-02 17:14:48,143] [INFO] [engine.py:3244:_save_zero_checkpoint] zero checkpoint saved ./runs/plum-13b_kld_0.1_dice_2.0/ckpt_model/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-02 17:14:48,144] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
Epoch: [2][  1/500]	Time  6.559 ( 6.559)	Loss 1.1875 (1.9968)	CeLoss 1.1875 (0.3067)	SegCLSLoss 0.0000 (0.0205)	KLLoss 0.0000 (0.3045)	MaskLoss 0.0000 (0.8246)	MaskBCELoss 0.0000 (0.2006)	MaskDICELoss 0.0000 (0.6239)
Epoch: [2][  2/500]	Time  5.414 ( 5.414)	Loss 3.5092 (2.0201)	CeLoss 0.2002 (0.4333)	SegCLSLoss 0.0272 (0.0178)	KLLoss 0.3711 (0.3172)	MaskLoss 1.6296 (0.7730)	MaskBCELoss 0.7312 (0.2609)	MaskDICELoss 0.8984 (0.5122)
Epoch: [2][  3/500]	Time  5.988 ( 5.988)	Loss 2.1220 (1.7193)	CeLoss 0.1758 (0.5338)	SegCLSLoss 0.0299 (0.0188)	KLLoss 0.3770 (0.2682)	MaskLoss 0.9467 (0.5748)	MaskBCELoss 0.0104 (0.0987)	MaskDICELoss 0.9363 (0.4761)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 499. Dropping entry: {'train/loss': 1.9968044877052307, 'train/ce_loss': 0.30673828125, 'train/seg_cls_loss': 0.02047119140625, 'train/kl_loss': 0.3044921875, 'train/mask_bce_loss': 0.20064473189413548, 'train/mask_dice_loss': 0.6239294052124024, 'train/mask_loss': 0.8245741367340088, 'metrics/total_secs_per_batch': 6.5593178272247314, 'metrics/data_secs_per_batch': 3.156621050834656, '_timestamp': 1740957294.7154245}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 1 is less than current step: 499. Dropping entry: {'train/loss': 2.0200653195381166, 'train/ce_loss': 0.43330078125, 'train/seg_cls_loss': 0.017767333984375, 'train/kl_loss': 0.3171875, 'train/mask_bce_loss': 0.26085073705762624, 'train/mask_dice_loss': 0.5121702000498771, 'train/mask_loss': 0.7730209261178971, 'metrics/total_secs_per_batch': 5.414100170135498, 'metrics/data_secs_per_batch': 2.0489684343338013, '_timestamp': 1740957300.1294155}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 1 is less than current step: 499. Dropping entry: {'train/lr': 0.0001836734693877551, '_timestamp': 1740957300.1297436}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 2 is less than current step: 499. Dropping entry: {'train/loss': 1.7192937135696411, 'train/ce_loss': 0.5337890625, 'train/seg_cls_loss': 0.01876220703125, 'train/kl_loss': 0.2681640625, 'train/mask_bce_loss': 0.09869121527299285, 'train/mask_dice_loss': 0.47609235495328905, 'train/mask_loss': 0.5747835636138916, 'metrics/total_secs_per_batch': 5.9880571365356445, 'metrics/data_secs_per_batch': 2.6485124826431274, '_timestamp': 1740957306.1174552}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 2 is less than current step: 499. Dropping entry: {'train/lr': 0.00018355102040816326, '_timestamp': 1740957306.117818}).
Epoch: [2][  4/500]	Time  6.810 ( 6.810)	Loss 2.3440 (2.0430)	CeLoss 0.1631 (0.4594)	SegCLSLoss 0.0349 (0.0175)	KLLoss 0.3750 (0.3129)	MaskLoss 1.0626 (0.7718)	MaskBCELoss 0.0693 (0.1760)	MaskDICELoss 0.9933 (0.5958)
Epoch: [2][  5/500]	Time  6.043 ( 6.043)	Loss 1.6805 (1.7708)	CeLoss 0.2520 (0.4332)	SegCLSLoss 0.0135 (0.0151)	KLLoss 0.4062 (0.3150)	MaskLoss 0.6908 (0.6492)	MaskBCELoss 0.1183 (0.1069)	MaskDICELoss 0.5725 (0.5424)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 3 is less than current step: 499. Dropping entry: {'train/loss': 2.043010342121124, 'train/ce_loss': 0.459375, 'train/seg_cls_loss': 0.01751708984375, 'train/kl_loss': 0.312890625, 'train/mask_bce_loss': 0.1759635944850743, 'train/mask_dice_loss': 0.5958345353603363, 'train/mask_loss': 0.7717981398105621, 'metrics/total_secs_per_batch': 6.809732675552368, 'metrics/data_secs_per_batch': 3.2981672048568726, '_timestamp': 1740957312.9274247}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 3 is less than current step: 499. Dropping entry: {'train/lr': 0.00018342857142857142, '_timestamp': 1740957312.9277987}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 4 is less than current step: 499. Dropping entry: {'train/loss': 1.7707613348960876, 'train/ce_loss': 0.433203125, 'train/seg_cls_loss': 0.01505126953125, 'train/kl_loss': 0.3150390625, 'train/mask_bce_loss': 0.10685370489954948, 'train/mask_dice_loss': 0.5423941358923912, 'train/mask_loss': 0.6492478549480438, 'metrics/total_secs_per_batch': 6.042536497116089, 'metrics/data_secs_per_batch': 2.5869198083877563, '_timestamp': 1740957318.9701953}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 4 is less than current step: 499. Dropping entry: {'train/lr': 0.00018330612244897958, '_timestamp': 1740957318.9706051}).
Epoch: [2][  6/500]	Time  4.630 ( 4.630)	Loss 2.4768 (3.1197)	CeLoss 0.3086 (0.4209)	SegCLSLoss 0.0143 (0.0139)	KLLoss 0.4121 (0.2789)	MaskLoss 1.0597 (1.3320)	MaskBCELoss 0.1354 (0.8856)	MaskDICELoss 0.9243 (0.4464)
Epoch: [2][  7/500]	Time  5.807 ( 5.807)	Loss 0.9336 (1.9080)	CeLoss 0.9336 (0.3973)	SegCLSLoss 0.0000 (0.0199)	KLLoss 0.0000 (0.3072)	MaskLoss 0.0000 (0.7349)	MaskBCELoss 0.0000 (0.1004)	MaskDICELoss 0.0000 (0.6345)
Epoch: [2][  8/500]	Time  5.711 ( 5.711)	Loss 0.0540 (1.7846)	CeLoss 0.0540 (0.1565)	SegCLSLoss 0.0000 (0.0270)	KLLoss 0.0000 (0.3047)	MaskLoss 0.0000 (0.7921)	MaskBCELoss 0.0000 (0.1346)	MaskDICELoss 0.0000 (0.6575)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 5 is less than current step: 499. Dropping entry: {'train/loss': 3.1197472333908083, 'train/ce_loss': 0.4208984375, 'train/seg_cls_loss': 0.013922119140625, 'train/kl_loss': 0.27890625, 'train/mask_bce_loss': 0.885642262827605, 'train/mask_dice_loss': 0.44639931321144105, 'train/mask_loss': 1.3320415809750557, 'metrics/total_secs_per_batch': 4.630158185958862, 'metrics/data_secs_per_batch': 1.8845050573348998, '_timestamp': 1740957323.5998783}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 5 is less than current step: 499. Dropping entry: {'train/lr': 0.00018318367346938774, '_timestamp': 1740957323.600083}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 6 is less than current step: 499. Dropping entry: {'train/loss': 1.9079559922218323, 'train/ce_loss': 0.397265625, 'train/seg_cls_loss': 0.019940185546875, 'train/kl_loss': 0.3072265625, 'train/mask_bce_loss': 0.10039685191586614, 'train/mask_dice_loss': 0.6344893410801887, 'train/mask_loss': 0.7348861962556839, 'metrics/total_secs_per_batch': 5.8074951171875, 'metrics/data_secs_per_batch': 2.9215138435363768, '_timestamp': 1740957329.4073865}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 6 is less than current step: 499. Dropping entry: {'train/lr': 0.0001830612244897959, '_timestamp': 1740957329.4076967}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 7 is less than current step: 499. Dropping entry: {'train/loss': 1.7845799565315246, 'train/ce_loss': 0.1564697265625, 'train/seg_cls_loss': 0.0269775390625, 'train/kl_loss': 0.3046875, 'train/mask_bce_loss': 0.13460277020931244, 'train/mask_dice_loss': 0.6575040996074677, 'train/mask_loss': 0.7921068668365479, 'metrics/total_secs_per_batch': 5.7107648849487305, 'metrics/data_secs_per_batch': 2.431687116622925, '_timestamp': 1740957335.118171}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 7 is less than current step: 499. Dropping entry: {'train/lr': 0.00018293877551020407, '_timestamp': 1740957335.118465}).
Epoch: [2][  9/500]	Time  7.117 ( 7.117)	Loss 1.0000 (2.2802)	CeLoss 1.0000 (0.3287)	SegCLSLoss 0.0000 (0.0200)	KLLoss 0.0000 (0.3492)	MaskLoss 0.0000 (0.9533)	MaskBCELoss 0.0000 (0.2167)	MaskDICELoss 0.0000 (0.7366)
[2025-03-02 17:15:47,332] [INFO] [logging.py:96:log_dist] [Rank 0] step=1010, skipped=0, lr=[0.0001827551020408163], mom=[(0.9, 0.95)]
[2025-03-02 17:15:47,338] [INFO] [timer.py:215:stop] epoch=0/micro_step=10100/global_step=1010, RunningAvgSamplesPerSec=1.526509800653765, CurrSamplesPerSec=1.9601664534583239, MemAllocated=30.64GB, MaxMemAllocated=37.19GB
Epoch: [2][ 10/500]	Time  5.103 ( 5.103)	Loss 1.5943 (1.7323)	CeLoss 0.2930 (0.5558)	SegCLSLoss 0.0156 (0.0168)	KLLoss 0.3965 (0.2701)	MaskLoss 0.6272 (0.5706)	MaskBCELoss 0.1187 (0.0716)	MaskDICELoss 0.5085 (0.4990)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 8 is less than current step: 499. Dropping entry: {'train/loss': 2.2802345037460325, 'train/ce_loss': 0.3287109375, 'train/seg_cls_loss': 0.019989013671875, 'train/kl_loss': 0.34921875, 'train/mask_bce_loss': 0.21668608859181404, 'train/mask_dice_loss': 0.7365659356117249, 'train/mask_loss': 0.9532520234584808, 'metrics/total_secs_per_batch': 7.117478847503662, 'metrics/data_secs_per_batch': 3.140344166755676, '_timestamp': 1740957342.2356179}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 8 is less than current step: 499. Dropping entry: {'train/lr': 0.00018281632653061223, '_timestamp': 1740957342.235818}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 9 is less than current step: 499. Dropping entry: {'train/loss': 1.732284462451935, 'train/ce_loss': 0.55576171875, 'train/seg_cls_loss': 0.016827392578125, 'train/kl_loss': 0.2701171875, 'train/mask_bce_loss': 0.07158556245267392, 'train/mask_dice_loss': 0.49900002777576447, 'train/mask_loss': 0.5705855906009674, 'metrics/total_secs_per_batch': 5.103355646133423, 'metrics/data_secs_per_batch': 2.0859152555465696, '_timestamp': 1740957347.3387806}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 9 is less than current step: 499. Dropping entry: {'train/lr': 0.0001826938775510204, '_timestamp': 1740957347.339123}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 10 is less than current step: 499. Dropping entry: {'train/loss': 1.4824697732925416, 'train/ce_loss': 0.773974609375, 'train/seg_cls_loss': 0.00869140625, 'train/kl_loss': 0.154296875, 'train/mask_bce_loss': 0.043889114260673524, 'train/mask_dice_loss': 0.3004951775074005, 'train/mask_loss': 0.3443842947483063, 'metrics/total_secs_per_batch': 4.8694868087768555, 'metrics/data_secs_per_batch': 1.8238086700439453, '_timestamp': 1740957352.2085369}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 10 is less than current step: 499. Dropping entry: {'train/lr': 0.00018257142857142853, '_timestamp': 1740957352.2088547}).
Epoch: [2][ 11/500]	Time  4.869 ( 4.869)	Loss 1.9474 (1.4825)	CeLoss 0.1963 (0.7740)	SegCLSLoss 0.0280 (0.0087)	KLLoss 0.3906 (0.1543)	MaskLoss 0.8492 (0.3444)	MaskBCELoss 0.1556 (0.0439)	MaskDICELoss 0.6936 (0.3005)
Epoch: [2][ 12/500]	Time  6.162 ( 6.162)	Loss 1.1094 (1.9580)	CeLoss 1.1094 (0.4335)	SegCLSLoss 0.0000 (0.0177)	KLLoss 0.0000 (0.2600)	MaskLoss 0.0000 (0.7448)	MaskBCELoss 0.0000 (0.1720)	MaskDICELoss 0.0000 (0.5727)
Epoch: [2][ 13/500]	Time  6.167 ( 6.167)	Loss 1.9136 (1.6710)	CeLoss 0.3223 (0.3695)	SegCLSLoss 0.0143 (0.0141)	KLLoss 0.3867 (0.2682)	MaskLoss 0.7722 (0.6337)	MaskBCELoss 0.2434 (0.0898)	MaskDICELoss 0.5288 (0.5440)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 11 is less than current step: 499. Dropping entry: {'train/loss': 1.9580357313156127, 'train/ce_loss': 0.43349609375, 'train/seg_cls_loss': 0.01768798828125, 'train/kl_loss': 0.2599609375, 'train/mask_bce_loss': 0.17204876560717822, 'train/mask_dice_loss': 0.5727405726909638, 'train/mask_loss': 0.7447893440723419, 'metrics/total_secs_per_batch': 6.161836624145508, 'metrics/data_secs_per_batch': 2.8627233505249023, '_timestamp': 1740957358.370369}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 11 is less than current step: 499. Dropping entry: {'train/lr': 0.0001824489795918367, '_timestamp': 1740957358.3705864}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 12 is less than current step: 499. Dropping entry: {'train/loss': 1.6710332751274108, 'train/ce_loss': 0.369482421875, 'train/seg_cls_loss': 0.014141845703125, 'train/kl_loss': 0.2681640625, 'train/mask_bce_loss': 0.08975939797237516, 'train/mask_dice_loss': 0.5439749956130981, 'train/mask_loss': 0.6337343931198121, 'metrics/total_secs_per_batch': 6.167047739028931, 'metrics/data_secs_per_batch': 2.5255937814712524, '_timestamp': 1740957364.53779}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 12 is less than current step: 499. Dropping entry: {'train/lr': 0.00018232653061224486, '_timestamp': 1740957364.5382304}).
Epoch: [2][ 14/500]	Time  6.122 ( 6.122)	Loss 2.1831 (2.0019)	CeLoss 0.1602 (0.4856)	SegCLSLoss 0.0425 (0.0180)	KLLoss 0.3867 (0.3109)	MaskLoss 0.9812 (0.7380)	MaskBCELoss 0.2759 (0.1969)	MaskDICELoss 0.7053 (0.5411)
Epoch: [2][ 15/500]	Time  6.147 ( 6.147)	Loss 1.2266 (1.8808)	CeLoss 1.2266 (0.5009)	SegCLSLoss 0.0000 (0.0194)	KLLoss 0.0000 (0.2648)	MaskLoss 0.0000 (0.6718)	MaskBCELoss 0.0000 (0.1277)	MaskDICELoss 0.0000 (0.5442)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 13 is less than current step: 499. Dropping entry: {'train/loss': 2.001901161670685, 'train/ce_loss': 0.48564453125, 'train/seg_cls_loss': 0.01796875, 'train/kl_loss': 0.3109375, 'train/mask_bce_loss': 0.19688269775360823, 'train/mask_dice_loss': 0.5410796016454696, 'train/mask_loss': 0.737962293624878, 'metrics/total_secs_per_batch': 6.1220479011535645, 'metrics/data_secs_per_batch': 2.8462321758270264, '_timestamp': 1740957370.6594782}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 13 is less than current step: 499. Dropping entry: {'train/lr': 0.00018220408163265302, '_timestamp': 1740957370.6597795}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 14 is less than current step: 499. Dropping entry: {'train/loss': 1.880768084526062, 'train/ce_loss': 0.50087890625, 'train/seg_cls_loss': 0.019390869140625, 'train/kl_loss': 0.26484375, 'train/mask_bce_loss': 0.12767098206095398, 'train/mask_dice_loss': 0.5441583722829819, 'train/mask_loss': 0.671829354763031, 'metrics/total_secs_per_batch': 6.147008657455444, 'metrics/data_secs_per_batch': 3.1825338125228884, '_timestamp': 1740957376.8064945}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 14 is less than current step: 499. Dropping entry: {'train/lr': 0.00018208163265306119, '_timestamp': 1740957376.8068166}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 15 is less than current step: 499. Dropping entry: {'train/loss': 1.4677572965621948, 'train/ce_loss': 0.5124755859375, 'train/seg_cls_loss': 0.010308837890625, 'train/kl_loss': 0.232421875, 'train/mask_bce_loss': 0.07209574477747083, 'train/mask_dice_loss': 0.3912873029708862, 'train/mask_loss': 0.4633830487728119, 'metrics/total_secs_per_batch': 5.7322916984558105, 'metrics/data_secs_per_batch': 2.8031981945037843, '_timestamp': 1740957382.538775}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 15 is less than current step: 499. Dropping entry: {'train/lr': 0.00018195918367346935, '_timestamp': 1740957382.5391037}).
Epoch: [2][ 16/500]	Time  5.732 ( 5.732)	Loss 1.4609 (1.4678)	CeLoss 1.4609 (0.5125)	SegCLSLoss 0.0000 (0.0103)	KLLoss 0.0000 (0.2324)	MaskLoss 0.0000 (0.4634)	MaskBCELoss 0.0000 (0.0721)	MaskDICELoss 0.0000 (0.3913)
Epoch: [2][ 17/500]	Time  4.751 ( 4.751)	Loss 2.2825 (1.6805)	CeLoss 0.2148 (0.6839)	SegCLSLoss 0.0167 (0.0115)	KLLoss 0.3965 (0.2328)	MaskLoss 1.0094 (0.4837)	MaskBCELoss 0.0096 (0.0739)	MaskDICELoss 0.9999 (0.4098)
Epoch: [2][ 18/500]	Time  6.982 ( 6.982)	Loss 1.9822 (1.8164)	CeLoss 0.2266 (0.3300)	SegCLSLoss 0.0147 (0.0149)	KLLoss 0.4102 (0.3131)	MaskLoss 0.8534 (0.7237)	MaskBCELoss 0.1887 (0.1445)	MaskDICELoss 0.6648 (0.5791)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 16 is less than current step: 499. Dropping entry: {'train/loss': 1.6805190920829773, 'train/ce_loss': 0.68388671875, 'train/seg_cls_loss': 0.011480712890625, 'train/kl_loss': 0.2328125, 'train/mask_bce_loss': 0.07386784087866545, 'train/mask_dice_loss': 0.40984873920679094, 'train/mask_loss': 0.4837165862321854, 'metrics/total_secs_per_batch': 4.751080751419067, 'metrics/data_secs_per_batch': 2.1305965185165405, '_timestamp': 1740957387.2898312}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 16 is less than current step: 499. Dropping entry: {'train/lr': 0.00018183673469387754, '_timestamp': 1740957387.2901468}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 17 is less than current step: 499. Dropping entry: {'train/loss': 1.8163607954978942, 'train/ce_loss': 0.32998046875, 'train/seg_cls_loss': 0.014874267578125, 'train/kl_loss': 0.3130859375, 'train/mask_bce_loss': 0.14451618697494267, 'train/mask_dice_loss': 0.579142713546753, 'train/mask_loss': 0.7236589014530181, 'metrics/total_secs_per_batch': 6.9817893505096436, 'metrics/data_secs_per_batch': 3.151891827583313, '_timestamp': 1740957394.271876}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 17 is less than current step: 499. Dropping entry: {'train/lr': 0.0001817142857142857, '_timestamp': 1740957394.2724717}).
Epoch: [2][ 19/500]	Time  6.197 ( 6.197)	Loss 2.7414 (1.8359)	CeLoss 0.2236 (0.4165)	SegCLSLoss 0.0242 (0.0180)	KLLoss 0.3633 (0.3098)	MaskLoss 1.2350 (0.6896)	MaskBCELoss 0.3264 (0.1245)	MaskDICELoss 0.9085 (0.5651)
[2025-03-02 17:16:47,105] [INFO] [logging.py:96:log_dist] [Rank 0] step=1020, skipped=0, lr=[0.00018153061224489796], mom=[(0.9, 0.95)]
[2025-03-02 17:16:47,111] [INFO] [timer.py:215:stop] epoch=0/micro_step=10200/global_step=1020, RunningAvgSamplesPerSec=1.5278272599002263, CurrSamplesPerSec=1.5057336868236313, MemAllocated=31.39GB, MaxMemAllocated=37.19GB
Epoch: [2][ 20/500]	Time  6.643 ( 6.643)	Loss 1.8508 (1.8822)	CeLoss 0.2832 (0.3409)	SegCLSLoss 0.0142 (0.0180)	KLLoss 0.3887 (0.3490)	MaskLoss 0.7604 (0.7488)	MaskBCELoss 0.0726 (0.1442)	MaskDICELoss 0.6877 (0.6046)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 18 is less than current step: 499. Dropping entry: {'train/loss': 1.8359169602394103, 'train/ce_loss': 0.41650390625, 'train/seg_cls_loss': 0.018035888671875, 'train/kl_loss': 0.309765625, 'train/mask_bce_loss': 0.12448824401944876, 'train/mask_dice_loss': 0.5651499092578888, 'train/mask_loss': 0.6896381586790085, 'metrics/total_secs_per_batch': 6.197205305099487, 'metrics/data_secs_per_batch': 2.919423007965088, '_timestamp': 1740957400.4688733}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 18 is less than current step: 499. Dropping entry: {'train/lr': 0.00018159183673469386, '_timestamp': 1740957400.4690752}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 19 is less than current step: 499. Dropping entry: {'train/loss': 1.8821720242500306, 'train/ce_loss': 0.34091796875, 'train/seg_cls_loss': 0.018035888671875, 'train/kl_loss': 0.3490234375, 'train/mask_bce_loss': 0.14421869441866875, 'train/mask_dice_loss': 0.6045821607112885, 'train/mask_loss': 0.7488008558750152, 'metrics/total_secs_per_batch': 6.643051862716675, 'metrics/data_secs_per_batch': 2.8331772804260256, '_timestamp': 1740957407.111757}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 19 is less than current step: 499. Dropping entry: {'train/lr': 0.00018146938775510203, '_timestamp': 1740957407.1121218}).
Epoch: [2][ 21/500]	Time  6.436 ( 6.436)	Loss 2.2926 (1.7589)	CeLoss 0.2344 (0.4117)	SegCLSLoss 0.0122 (0.0131)	KLLoss 0.4023 (0.3160)	MaskLoss 1.0057 (0.6545)	MaskBCELoss 0.1201 (0.1344)	MaskDICELoss 0.8855 (0.5201)
Epoch: [2][ 22/500]	Time  6.091 ( 6.091)	Loss 2.7064 (2.2390)	CeLoss 0.2559 (0.4695)	SegCLSLoss 0.0137 (0.0243)	KLLoss 0.4004 (0.3072)	MaskLoss 1.2028 (0.8634)	MaskBCELoss 0.5880 (0.2320)	MaskDICELoss 0.6148 (0.6313)
Epoch: [2][ 23/500]	Time  4.878 ( 4.878)	Loss 2.4283 (1.6098)	CeLoss 0.1426 (0.5451)	SegCLSLoss 0.0344 (0.0121)	KLLoss 0.3633 (0.1889)	MaskLoss 1.1160 (0.5198)	MaskBCELoss 0.3079 (0.0719)	MaskDICELoss 0.8081 (0.4478)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 20 is less than current step: 499. Dropping entry: {'train/loss': 1.7588591933250428, 'train/ce_loss': 0.41171875, 'train/seg_cls_loss': 0.013134765625, 'train/kl_loss': 0.316015625, 'train/mask_bce_loss': 0.13442569449543953, 'train/mask_dice_loss': 0.5201015651226044, 'train/mask_loss': 0.6545272648334504, 'metrics/total_secs_per_batch': 6.435926198959351, 'metrics/data_secs_per_batch': 2.9513187170028687, '_timestamp': 1740957413.5477715}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 20 is less than current step: 499. Dropping entry: {'train/lr': 0.0001813469387755102, '_timestamp': 1740957413.547964}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 21 is less than current step: 499. Dropping entry: {'train/loss': 2.2390490651130674, 'train/ce_loss': 0.46953125, 'train/seg_cls_loss': 0.024346923828125, 'train/kl_loss': 0.3072265625, 'train/mask_bce_loss': 0.23204579055309296, 'train/mask_dice_loss': 0.6313264012336731, 'train/mask_loss': 0.8633721888065338, 'metrics/total_secs_per_batch': 6.090805530548096, 'metrics/data_secs_per_batch': 2.5116611242294313, '_timestamp': 1740957419.6386838}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 21 is less than current step: 499. Dropping entry: {'train/lr': 0.00018122448979591835, '_timestamp': 1740957419.6390145}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 22 is less than current step: 499. Dropping entry: {'train/loss': 1.609779143333435, 'train/ce_loss': 0.5451171875, 'train/seg_cls_loss': 0.01209716796875, 'train/kl_loss': 0.1888671875, 'train/mask_bce_loss': 0.07194349635392427, 'train/mask_dice_loss': 0.44783865809440615, 'train/mask_loss': 0.519782155752182, 'metrics/total_secs_per_batch': 4.878181219100952, 'metrics/data_secs_per_batch': 2.146654987335205, '_timestamp': 1740957424.5170066}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 22 is less than current step: 499. Dropping entry: {'train/lr': 0.00018110204081632652, '_timestamp': 1740957424.5174332}).
Epoch: [2][ 24/500]	Time  4.660 ( 4.660)	Loss 2.4935 (1.9106)	CeLoss 0.2500 (0.7029)	SegCLSLoss 0.0193 (0.0104)	KLLoss 0.3730 (0.2346)	MaskLoss 1.0983 (0.5896)	MaskBCELoss 0.2367 (0.1428)	MaskDICELoss 0.8616 (0.4468)
Epoch: [2][ 25/500]	Time  5.737 ( 5.737)	Loss 1.7817 (1.7724)	CeLoss 0.2734 (0.4718)	SegCLSLoss 0.0134 (0.0141)	KLLoss 0.4062 (0.2771)	MaskLoss 0.7307 (0.6330)	MaskBCELoss 0.1629 (0.2224)	MaskDICELoss 0.5678 (0.4106)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 23 is less than current step: 499. Dropping entry: {'train/loss': 1.9105663895606995, 'train/ce_loss': 0.7029296875, 'train/seg_cls_loss': 0.010382080078125, 'train/kl_loss': 0.2345703125, 'train/mask_bce_loss': 0.1427510472945869, 'train/mask_dice_loss': 0.44680948853492736, 'train/mask_loss': 0.5895605325698853, 'metrics/total_secs_per_batch': 4.659932374954224, 'metrics/data_secs_per_batch': 2.0910847663879393, '_timestamp': 1740957429.1767437}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 23 is less than current step: 499. Dropping entry: {'train/lr': 0.00018097959183673468, '_timestamp': 1740957429.1771157}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 24 is less than current step: 499. Dropping entry: {'train/loss': 1.772438931465149, 'train/ce_loss': 0.47177734375, 'train/seg_cls_loss': 0.0141357421875, 'train/kl_loss': 0.2771484375, 'train/mask_bce_loss': 0.22243059277534485, 'train/mask_dice_loss': 0.41056619882583617, 'train/mask_loss': 0.632996791601181, 'metrics/total_secs_per_batch': 5.737307786941528, 'metrics/data_secs_per_batch': 2.576549935340881, '_timestamp': 1740957434.9140801}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 24 is less than current step: 499. Dropping entry: {'train/lr': 0.00018085714285714284, '_timestamp': 1740957434.9143863}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 25 is less than current step: 499. Dropping entry: {'train/loss': 2.0046569108963013, 'train/ce_loss': 0.28154296875, 'train/seg_cls_loss': 0.02225341796875, 'train/kl_loss': 0.347265625, 'train/mask_bce_loss': 0.1851617217063904, 'train/mask_dice_loss': 0.6533483684062957, 'train/mask_loss': 0.8385101020336151, 'metrics/total_secs_per_batch': 6.780781030654907, 'metrics/data_secs_per_batch': 3.137246537208557, '_timestamp': 1740957441.695201}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 25 is less than current step: 499. Dropping entry: {'train/lr': 0.000180734693877551, '_timestamp': 1740957441.6955817}).
Epoch: [2][ 26/500]	Time  6.781 ( 6.781)	Loss 2.6453 (2.0047)	CeLoss 0.1348 (0.2815)	SegCLSLoss 0.0378 (0.0223)	KLLoss 0.3770 (0.3473)	MaskLoss 1.2269 (0.8385)	MaskBCELoss 0.3649 (0.1852)	MaskDICELoss 0.8621 (0.6533)
Epoch: [2][ 27/500]	Time  5.805 ( 5.805)	Loss 1.4845 (1.6069)	CeLoss 0.1855 (0.4912)	SegCLSLoss 0.0270 (0.0133)	KLLoss 0.3848 (0.2740)	MaskLoss 0.6231 (0.5408)	MaskBCELoss 0.2145 (0.1002)	MaskDICELoss 0.4086 (0.4406)
Epoch: [2][ 28/500]	Time  5.525 ( 5.525)	Loss 1.8219 (1.3689)	CeLoss 0.3184 (0.6406)	SegCLSLoss 0.0150 (0.0084)	KLLoss 0.3965 (0.1541)	MaskLoss 0.7283 (0.3543)	MaskBCELoss 0.2568 (0.0584)	MaskDICELoss 0.4716 (0.2959)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 26 is less than current step: 499. Dropping entry: {'train/loss': 1.6068736433982849, 'train/ce_loss': 0.4912109375, 'train/seg_cls_loss': 0.013299560546875, 'train/kl_loss': 0.2740234375, 'train/mask_bce_loss': 0.10023851916193963, 'train/mask_dice_loss': 0.4405518174171448, 'train/mask_loss': 0.5407903403043747, 'metrics/total_secs_per_batch': 5.804910182952881, 'metrics/data_secs_per_batch': 2.782341718673706, '_timestamp': 1740957447.4997847}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 26 is less than current step: 499. Dropping entry: {'train/lr': 0.00018061224489795917, '_timestamp': 1740957447.500106}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 27 is less than current step: 499. Dropping entry: {'train/loss': 1.3688647031784058, 'train/ce_loss': 0.640625, 'train/seg_cls_loss': 0.0084228515625, 'train/kl_loss': 0.1541015625, 'train/mask_bce_loss': 0.058405814319849016, 'train/mask_dice_loss': 0.2958507627248764, 'train/mask_loss': 0.35425658226013185, 'metrics/total_secs_per_batch': 5.52524209022522, 'metrics/data_secs_per_batch': 2.1823188781738283, '_timestamp': 1740957453.0251286}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 27 is less than current step: 499. Dropping entry: {'train/lr': 0.00018048979591836733, '_timestamp': 1740957453.025462}).
Epoch: [2][ 29/500]	Time  5.772 ( 5.772)	Loss 1.7723 (1.4739)	CeLoss 0.2344 (0.4949)	SegCLSLoss 0.0135 (0.0112)	KLLoss 0.3984 (0.1938)	MaskLoss 0.7455 (0.4770)	MaskBCELoss 0.1017 (0.1398)	MaskDICELoss 0.6438 (0.3372)
[2025-03-02 17:17:43,504] [INFO] [logging.py:96:log_dist] [Rank 0] step=1030, skipped=0, lr=[0.00018030612244897956], mom=[(0.9, 0.95)]
[2025-03-02 17:17:43,510] [INFO] [timer.py:215:stop] epoch=0/micro_step=10300/global_step=1030, RunningAvgSamplesPerSec=1.529888910075538, CurrSamplesPerSec=2.121842609339316, MemAllocated=31.47GB, MaxMemAllocated=37.19GB
Epoch: [2][ 30/500]	Time  4.715 ( 4.715)	Loss 0.0557 (1.5632)	CeLoss 0.0557 (0.6071)	SegCLSLoss 0.0000 (0.0100)	KLLoss 0.0000 (0.1953)	MaskLoss 0.0000 (0.4658)	MaskBCELoss 0.0000 (0.0565)	MaskDICELoss 0.0000 (0.4093)
Epoch: [2][ 31/500]	Time  6.114 ( 6.114)	Loss 1.2168 (1.6969)	CeLoss 0.3809 (0.3729)	SegCLSLoss 0.0146 (0.0142)	KLLoss 0.4121 (0.3135)	MaskLoss 0.3936 (0.6427)	MaskBCELoss 0.0744 (0.0619)	MaskDICELoss 0.3192 (0.5808)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 28 is less than current step: 499. Dropping entry: {'train/loss': 1.4738893270492555, 'train/ce_loss': 0.494873046875, 'train/seg_cls_loss': 0.01119384765625, 'train/kl_loss': 0.19375, 'train/mask_bce_loss': 0.13979180641472339, 'train/mask_dice_loss': 0.3371675044298172, 'train/mask_loss': 0.4769593119621277, 'metrics/total_secs_per_batch': 5.771732568740845, 'metrics/data_secs_per_batch': 2.7713127613067625, '_timestamp': 1740957458.7968135}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 28 is less than current step: 499. Dropping entry: {'train/lr': 0.0001803673469387755, '_timestamp': 1740957458.7972536}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 29 is less than current step: 499. Dropping entry: {'train/loss': 1.5632241129875184, 'train/ce_loss': 0.60712890625, 'train/seg_cls_loss': 0.009967041015625, 'train/kl_loss': 0.1953125, 'train/mask_bce_loss': 0.056533918157219885, 'train/mask_dice_loss': 0.4093066453933716, 'train/mask_loss': 0.4658405601978302, 'metrics/total_secs_per_batch': 4.714855670928955, 'metrics/data_secs_per_batch': 2.1029573917388915, '_timestamp': 1740957463.5113852}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 29 is less than current step: 499. Dropping entry: {'train/lr': 0.00018024489795918366, '_timestamp': 1740957463.5116825}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 30 is less than current step: 499. Dropping entry: {'train/loss': 1.6968793272972107, 'train/ce_loss': 0.37294921875, 'train/seg_cls_loss': 0.014190673828125, 'train/kl_loss': 0.3134765625, 'train/mask_bce_loss': 0.06190502978861332, 'train/mask_dice_loss': 0.5808217555284501, 'train/mask_loss': 0.6427267849445343, 'metrics/total_secs_per_batch': 6.113600492477417, 'metrics/data_secs_per_batch': 2.5956192016601562, '_timestamp': 1740957469.6252294}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 30 is less than current step: 499. Dropping entry: {'train/lr': 0.00018012244897959182, '_timestamp': 1740957469.6255567}).
Epoch: [2][ 32/500]	Time  5.811 ( 5.811)	Loss 2.0134 (1.8628)	CeLoss 0.2930 (0.4792)	SegCLSLoss 0.0135 (0.0184)	KLLoss 0.4004 (0.3166)	MaskLoss 0.8368 (0.6712)	MaskBCELoss 0.3204 (0.2046)	MaskDICELoss 0.5164 (0.4666)
Epoch: [2][ 33/500]	Time  6.538 ( 6.538)	Loss 1.9756 (2.0177)	CeLoss 0.2441 (0.3489)	SegCLSLoss 0.0267 (0.0221)	KLLoss 0.3789 (0.3484)	MaskLoss 0.8403 (0.8115)	MaskBCELoss 0.1335 (0.2054)	MaskDICELoss 0.7068 (0.6060)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 31 is less than current step: 499. Dropping entry: {'train/loss': 1.8627928614616394, 'train/ce_loss': 0.479248046875, 'train/seg_cls_loss': 0.01842041015625, 'train/kl_loss': 0.3166015625, 'train/mask_bce_loss': 0.20464772284030913, 'train/mask_dice_loss': 0.46659245193004606, 'train/mask_loss': 0.6712401777505874, 'metrics/total_secs_per_batch': 5.811468601226807, 'metrics/data_secs_per_batch': 2.194374752044678, '_timestamp': 1740957475.4366965}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 31 is less than current step: 499. Dropping entry: {'train/lr': 0.00017999999999999998, '_timestamp': 1740957475.4369965}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 32 is less than current step: 499. Dropping entry: {'train/loss': 2.0177314281463623, 'train/ce_loss': 0.34892578125, 'train/seg_cls_loss': 0.0221435546875, 'train/kl_loss': 0.3484375, 'train/mask_bce_loss': 0.2054147358983755, 'train/mask_dice_loss': 0.6060388788580895, 'train/mask_loss': 0.8114536017179489, 'metrics/total_secs_per_batch': 6.5380096435546875, 'metrics/data_secs_per_batch': 3.0892343282699586, '_timestamp': 1740957481.9748156}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 32 is less than current step: 499. Dropping entry: {'train/lr': 0.00017987755102040815, '_timestamp': 1740957481.975153}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 33 is less than current step: 499. Dropping entry: {'train/loss': 2.117695242166519, 'train/ce_loss': 0.4380859375, 'train/seg_cls_loss': 0.016082763671875, 'train/kl_loss': 0.3119140625, 'train/mask_bce_loss': 0.17762248292565347, 'train/mask_dice_loss': 0.6425532504916192, 'train/mask_loss': 0.8201757371425629, 'metrics/total_secs_per_batch': 5.7129247188568115, 'metrics/data_secs_per_batch': 2.3716309785842897, '_timestamp': 1740957487.687731}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 33 is less than current step: 499. Dropping entry: {'train/lr': 0.0001797551020408163, '_timestamp': 1740957487.6880996}).
Epoch: [2][ 34/500]	Time  5.713 ( 5.713)	Loss 3.2983 (2.1177)	CeLoss 0.2393 (0.4381)	SegCLSLoss 0.0120 (0.0161)	KLLoss 0.4180 (0.3119)	MaskLoss 1.5056 (0.8202)	MaskBCELoss 0.5071 (0.1776)	MaskDICELoss 0.9985 (0.6426)
Epoch: [2][ 35/500]	Time  6.292 ( 6.292)	Loss 2.6093 (1.9709)	CeLoss 0.1924 (0.4040)	SegCLSLoss 0.0371 (0.0163)	KLLoss 0.3789 (0.3137)	MaskLoss 1.1802 (0.7637)	MaskBCELoss 0.2660 (0.2103)	MaskDICELoss 0.9142 (0.5534)
Epoch: [2][ 36/500]	Time  6.087 ( 6.087)	Loss 1.5301 (1.5898)	CeLoss 0.2637 (0.2429)	SegCLSLoss 0.0234 (0.0194)	KLLoss 0.3965 (0.3547)	MaskLoss 0.6078 (0.6511)	MaskBCELoss 0.0296 (0.1061)	MaskDICELoss 0.5783 (0.5449)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 34 is less than current step: 499. Dropping entry: {'train/loss': 1.97089763879776, 'train/ce_loss': 0.40400390625, 'train/seg_cls_loss': 0.016290283203125, 'train/kl_loss': 0.313671875, 'train/mask_bce_loss': 0.21032879427075385, 'train/mask_dice_loss': 0.5533915162086487, 'train/mask_loss': 0.7637203007936477, 'metrics/total_secs_per_batch': 6.292430877685547, 'metrics/data_secs_per_batch': 2.8104973793029786, '_timestamp': 1740957493.980066}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 34 is less than current step: 499. Dropping entry: {'train/lr': 0.00017963265306122447, '_timestamp': 1740957493.9804838}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 35 is less than current step: 499. Dropping entry: {'train/loss': 1.5898319840431214, 'train/ce_loss': 0.24287109375, 'train/seg_cls_loss': 0.019378662109375, 'train/kl_loss': 0.3546875, 'train/mask_bce_loss': 0.10613153185695409, 'train/mask_dice_loss': 0.5449368119239807, 'train/mask_loss': 0.6510683357715606, 'metrics/total_secs_per_batch': 6.086895704269409, 'metrics/data_secs_per_batch': 2.5968303442001344, '_timestamp': 1740957500.0669546}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 35 is less than current step: 499. Dropping entry: {'train/lr': 0.00017951020408163264, '_timestamp': 1740957500.067269}).
Epoch: [2][ 37/500]	Time  6.475 ( 6.475)	Loss 1.6554 (1.5148)	CeLoss 0.2178 (0.4031)	SegCLSLoss 0.0151 (0.0184)	KLLoss 0.4004 (0.3545)	MaskLoss 0.6949 (0.5334)	MaskBCELoss 0.2046 (0.0904)	MaskDICELoss 0.4903 (0.4430)
Epoch: [2][ 38/500]	Time  5.983 ( 5.983)	Loss 1.4453 (1.7297)	CeLoss 1.4453 (0.5528)	SegCLSLoss 0.0000 (0.0136)	KLLoss 0.0000 (0.2756)	MaskLoss 0.0000 (0.5714)	MaskBCELoss 0.0000 (0.0860)	MaskDICELoss 0.0000 (0.4854)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 36 is less than current step: 499. Dropping entry: {'train/loss': 1.5147529184818267, 'train/ce_loss': 0.403125, 'train/seg_cls_loss': 0.018377685546875, 'train/kl_loss': 0.3544921875, 'train/mask_bce_loss': 0.0904287563636899, 'train/mask_dice_loss': 0.44297310411930085, 'train/mask_loss': 0.5334018602967262, 'metrics/total_secs_per_batch': 6.4753711223602295, 'metrics/data_secs_per_batch': 2.9843908309936524, '_timestamp': 1740957506.542929}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 36 is less than current step: 499. Dropping entry: {'train/lr': 0.0001793877551020408, '_timestamp': 1740957506.5434356}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 37 is less than current step: 499. Dropping entry: {'train/loss': 1.7297229051589966, 'train/ce_loss': 0.55283203125, 'train/seg_cls_loss': 0.01357421875, 'train/kl_loss': 0.2755859375, 'train/mask_bce_loss': 0.08604116290807724, 'train/mask_dice_loss': 0.48536325097084043, 'train/mask_loss': 0.5714044153690339, 'metrics/total_secs_per_batch': 5.982729434967041, 'metrics/data_secs_per_batch': 3.0926583051681518, '_timestamp': 1740957512.5251594}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 37 is less than current step: 499. Dropping entry: {'train/lr': 0.00017926530612244894, '_timestamp': 1740957512.5255358}).
Epoch: [2][ 39/500]	Time  6.252 ( 6.252)	Loss 2.8374 (2.3004)	CeLoss 0.1680 (0.3517)	SegCLSLoss 0.0339 (0.0192)	KLLoss 0.3828 (0.3531)	MaskLoss 1.3074 (0.9518)	MaskBCELoss 0.6848 (0.3098)	MaskDICELoss 0.6225 (0.6421)
[2025-03-02 17:18:45,389] [INFO] [logging.py:96:log_dist] [Rank 0] step=1040, skipped=0, lr=[0.00017908163265306122], mom=[(0.9, 0.95)]
[2025-03-02 17:18:45,395] [INFO] [timer.py:215:stop] epoch=0/micro_step=10400/global_step=1040, RunningAvgSamplesPerSec=1.5306772768602825, CurrSamplesPerSec=1.5113170670856153, MemAllocated=31.26GB, MaxMemAllocated=37.19GB
Epoch: [2][ 40/500]	Time  6.619 ( 6.619)	Loss 2.4723 (1.9885)	CeLoss 0.2129 (0.3270)	SegCLSLoss 0.0258 (0.0206)	KLLoss 0.3945 (0.3494)	MaskLoss 1.1033 (0.8080)	MaskBCELoss 0.3110 (0.1612)	MaskDICELoss 0.7923 (0.6468)
Epoch: [2][ 41/500]	Time  5.143 ( 5.143)	Loss 0.1973 (1.5894)	CeLoss 0.1973 (0.5310)	SegCLSLoss 0.0000 (0.0078)	KLLoss 0.0000 (0.1957)	MaskLoss 0.0000 (0.5174)	MaskBCELoss 0.0000 (0.1782)	MaskDICELoss 0.0000 (0.3391)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 38 is less than current step: 499. Dropping entry: {'train/loss': 2.300440192222595, 'train/ce_loss': 0.35166015625, 'train/seg_cls_loss': 0.01922607421875, 'train/kl_loss': 0.353125, 'train/mask_bce_loss': 0.30976636400446295, 'train/mask_dice_loss': 0.6420650571584702, 'train/mask_loss': 0.9518314182758332, 'metrics/total_secs_per_batch': 6.252343416213989, 'metrics/data_secs_per_batch': 2.977777433395386, '_timestamp': 1740957518.7777076}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 38 is less than current step: 499. Dropping entry: {'train/lr': 0.00017914285714285715, '_timestamp': 1740957518.7782383}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 39 is less than current step: 499. Dropping entry: {'train/loss': 1.9885448932647705, 'train/ce_loss': 0.326953125, 'train/seg_cls_loss': 0.020648193359375, 'train/kl_loss': 0.3494140625, 'train/mask_bce_loss': 0.16124227158725263, 'train/mask_dice_loss': 0.6467997163534165, 'train/mask_loss': 0.808041974902153, 'metrics/total_secs_per_batch': 6.618994235992432, 'metrics/data_secs_per_batch': 3.142179775238037, '_timestamp': 1740957525.396283}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 39 is less than current step: 499. Dropping entry: {'train/lr': 0.00017902040816326532, '_timestamp': 1740957525.3966084}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 40 is less than current step: 499. Dropping entry: {'train/loss': 1.5893943071365357, 'train/ce_loss': 0.5310302734375, 'train/seg_cls_loss': 0.007830810546875, 'train/kl_loss': 0.195703125, 'train/mask_bce_loss': 0.17822907418012618, 'train/mask_dice_loss': 0.3391365364193916, 'train/mask_loss': 0.5173656135797501, 'metrics/total_secs_per_batch': 5.14302134513855, 'metrics/data_secs_per_batch': 1.9604315519332887, '_timestamp': 1740957530.5395875}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 40 is less than current step: 499. Dropping entry: {'train/lr': 0.00017889795918367348, '_timestamp': 1740957530.5399222}).
Epoch: [2][ 42/500]	Time  5.219 ( 5.219)	Loss 1.2266 (1.7037)	CeLoss 1.2266 (0.5077)	SegCLSLoss 0.0000 (0.0148)	KLLoss 0.0000 (0.2748)	MaskLoss 0.0000 (0.5806)	MaskBCELoss 0.0000 (0.1201)	MaskDICELoss 0.0000 (0.4605)
Epoch: [2][ 43/500]	Time  6.276 ( 6.276)	Loss 1.5036 (1.7724)	CeLoss 0.2256 (0.3375)	SegCLSLoss 0.0178 (0.0172)	KLLoss 0.3965 (0.3586)	MaskLoss 0.6151 (0.6953)	MaskBCELoss 0.0231 (0.0965)	MaskDICELoss 0.5920 (0.5988)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 41 is less than current step: 499. Dropping entry: {'train/loss': 1.7037112355232238, 'train/ce_loss': 0.50771484375, 'train/seg_cls_loss': 0.014813232421875, 'train/kl_loss': 0.2748046875, 'train/mask_bce_loss': 0.12014251118525862, 'train/mask_dice_loss': 0.4604728609323502, 'train/mask_loss': 0.5806153833866119, 'metrics/total_secs_per_batch': 5.2186279296875, 'metrics/data_secs_per_batch': 2.6055434703826905, '_timestamp': 1740957535.7580085}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 41 is less than current step: 499. Dropping entry: {'train/lr': 0.00017877551020408164, '_timestamp': 1740957535.7582774}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 42 is less than current step: 499. Dropping entry: {'train/loss': 1.7724208950996398, 'train/ce_loss': 0.3375, 'train/seg_cls_loss': 0.01715087890625, 'train/kl_loss': 0.35859375, 'train/mask_bce_loss': 0.09646978722885251, 'train/mask_dice_loss': 0.5988226741552353, 'train/mask_loss': 0.6952924609184266, 'metrics/total_secs_per_batch': 6.275971412658691, 'metrics/data_secs_per_batch': 2.9246006488800047, '_timestamp': 1740957542.0343955}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 42 is less than current step: 499. Dropping entry: {'train/lr': 0.00017865306122448978, '_timestamp': 1740957542.034802}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 43 is less than current step: 499. Dropping entry: {'train/loss': 1.6703821301460267, 'train/ce_loss': 0.596875, 'train/seg_cls_loss': 0.012237548828125, 'train/kl_loss': 0.279296875, 'train/mask_bce_loss': 0.11705961134284734, 'train/mask_dice_loss': 0.40279940962791444, 'train/mask_loss': 0.5198590159416199, 'metrics/total_secs_per_batch': 5.717557668685913, 'metrics/data_secs_per_batch': 2.128652477264404, '_timestamp': 1740957547.7515986}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 43 is less than current step: 499. Dropping entry: {'train/lr': 0.00017853061224489794, '_timestamp': 1740957547.7518864}).
Epoch: [2][ 44/500]	Time  5.718 ( 5.718)	Loss 1.7963 (1.6704)	CeLoss 0.2539 (0.5969)	SegCLSLoss 0.0208 (0.0122)	KLLoss 0.3926 (0.2793)	MaskLoss 0.7468 (0.5199)	MaskBCELoss 0.0082 (0.1171)	MaskDICELoss 0.7385 (0.4028)
Epoch: [2][ 45/500]	Time  5.631 ( 5.631)	Loss 2.6494 (2.1701)	CeLoss 0.0952 (0.3127)	SegCLSLoss 0.0481 (0.0227)	KLLoss 0.3789 (0.3494)	MaskLoss 1.2461 (0.9056)	MaskBCELoss 0.3483 (0.2710)	MaskDICELoss 0.8978 (0.6346)
Epoch: [2][ 46/500]	Time  5.977 ( 5.977)	Loss 2.0477 (1.9921)	CeLoss 0.3789 (0.2707)	SegCLSLoss 0.0154 (0.0221)	KLLoss 0.3926 (0.3525)	MaskLoss 0.8109 (0.8376)	MaskBCELoss 0.1228 (0.1602)	MaskDICELoss 0.6882 (0.6773)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 44 is less than current step: 499. Dropping entry: {'train/loss': 2.1701316118240355, 'train/ce_loss': 0.3126953125, 'train/seg_cls_loss': 0.02271728515625, 'train/kl_loss': 0.3494140625, 'train/mask_bce_loss': 0.2709914162755013, 'train/mask_dice_loss': 0.6346310257911683, 'train/mask_loss': 0.9056224524974823, 'metrics/total_secs_per_batch': 5.630562782287598, 'metrics/data_secs_per_batch': 2.5440035581588747, '_timestamp': 1740957553.3821504}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 44 is less than current step: 499. Dropping entry: {'train/lr': 0.0001784081632653061, '_timestamp': 1740957553.3824537}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 45 is less than current step: 499. Dropping entry: {'train/loss': 1.9920560240745544, 'train/ce_loss': 0.270654296875, 'train/seg_cls_loss': 0.0221435546875, 'train/kl_loss': 0.3525390625, 'train/mask_bce_loss': 0.16024828548543155, 'train/mask_dice_loss': 0.6773324608802795, 'train/mask_loss': 0.8375807464122772, 'metrics/total_secs_per_batch': 5.9771997928619385, 'metrics/data_secs_per_batch': 2.657064747810364, '_timestamp': 1740957559.3595617}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 45 is less than current step: 499. Dropping entry: {'train/lr': 0.00017828571428571427, '_timestamp': 1740957559.359941}).
Epoch: [2][ 47/500]	Time  5.422 ( 5.422)	Loss 1.8864 (1.9671)	CeLoss 0.2246 (0.5035)	SegCLSLoss 0.0124 (0.0164)	KLLoss 0.3965 (0.3133)	MaskLoss 0.8084 (0.7121)	MaskBCELoss 0.1562 (0.1047)	MaskDICELoss 0.6522 (0.6074)
Epoch: [2][ 48/500]	Time  6.938 ( 6.938)	Loss 0.9102 (1.4696)	CeLoss 0.9102 (0.4391)	SegCLSLoss 0.0000 (0.0140)	KLLoss 0.0000 (0.2318)	MaskLoss 0.0000 (0.5000)	MaskBCELoss 0.0000 (0.0718)	MaskDICELoss 0.0000 (0.4283)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 46 is less than current step: 499. Dropping entry: {'train/loss': 1.9670501708984376, 'train/ce_loss': 0.503466796875, 'train/seg_cls_loss': 0.016448974609375, 'train/kl_loss': 0.31328125, 'train/mask_bce_loss': 0.10470345709472895, 'train/mask_dice_loss': 0.6074348986148834, 'train/mask_loss': 0.7121383666992187, 'metrics/total_secs_per_batch': 5.421583414077759, 'metrics/data_secs_per_batch': 2.3482905864715575, '_timestamp': 1740957564.7809682}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 46 is less than current step: 499. Dropping entry: {'train/lr': 0.00017816326530612243, '_timestamp': 1740957564.78127}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 47 is less than current step: 499. Dropping entry: {'train/loss': 1.4695601344108582, 'train/ce_loss': 0.439111328125, 'train/seg_cls_loss': 0.013983154296875, 'train/kl_loss': 0.2318359375, 'train/mask_bce_loss': 0.07178106922656298, 'train/mask_dice_loss': 0.4282577812671661, 'train/mask_loss': 0.5000388562679291, 'metrics/total_secs_per_batch': 6.937642812728882, 'metrics/data_secs_per_batch': 3.6720025539398193, '_timestamp': 1740957571.718827}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 47 is less than current step: 499. Dropping entry: {'train/lr': 0.0001780408163265306, '_timestamp': 1740957571.7191956}).
Epoch: [2][ 49/500]	Time  6.548 ( 6.548)	Loss 1.7690 (2.2432)	CeLoss 0.2930 (0.3356)	SegCLSLoss 0.0157 (0.0196)	KLLoss 0.3945 (0.3475)	MaskLoss 0.7146 (0.9316)	MaskBCELoss 0.1681 (0.2411)	MaskDICELoss 0.5465 (0.6905)
[2025-03-02 17:19:44,714] [INFO] [logging.py:96:log_dist] [Rank 0] step=1050, skipped=0, lr=[0.00017785714285714285], mom=[(0.9, 0.95)]
[2025-03-02 17:19:44,720] [INFO] [timer.py:215:stop] epoch=0/micro_step=10500/global_step=1050, RunningAvgSamplesPerSec=1.532024326371384, CurrSamplesPerSec=1.5497861919329092, MemAllocated=31.25GB, MaxMemAllocated=37.19GB
Epoch: [2][ 50/500]	Time  6.454 ( 6.454)	Loss 1.8285 (1.8859)	CeLoss 0.1943 (0.4306)	SegCLSLoss 0.0254 (0.0172)	KLLoss 0.3809 (0.3109)	MaskLoss 0.7917 (0.7080)	MaskBCELoss 0.1631 (0.0848)	MaskDICELoss 0.6285 (0.6232)
Epoch: [2][ 51/500]	Time  5.973 ( 5.973)	Loss 2.8382 (1.7561)	CeLoss 0.1768 (0.5352)	SegCLSLoss 0.0576 (0.0169)	KLLoss 0.3711 (0.2279)	MaskLoss 1.2980 (0.5948)	MaskBCELoss 0.3271 (0.1088)	MaskDICELoss 0.9709 (0.4860)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 48 is less than current step: 499. Dropping entry: {'train/loss': 2.2431852221488953, 'train/ce_loss': 0.335595703125, 'train/seg_cls_loss': 0.019586181640625, 'train/kl_loss': 0.3474609375, 'train/mask_bce_loss': 0.24109949972480535, 'train/mask_dice_loss': 0.6904540479183197, 'train/mask_loss': 0.9315535366535187, 'metrics/total_secs_per_batch': 6.547935485839844, 'metrics/data_secs_per_batch': 2.870677423477173, '_timestamp': 1740957578.2665257}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 48 is less than current step: 499. Dropping entry: {'train/lr': 0.00017791836734693876, '_timestamp': 1740957578.2667387}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 49 is less than current step: 499. Dropping entry: {'train/loss': 1.8859459400177, 'train/ce_loss': 0.43056640625, 'train/seg_cls_loss': 0.0172119140625, 'train/kl_loss': 0.3109375, 'train/mask_bce_loss': 0.08477095356211066, 'train/mask_dice_loss': 0.6231922507286072, 'train/mask_loss': 0.7079631984233856, 'metrics/total_secs_per_batch': 6.454164743423462, 'metrics/data_secs_per_batch': 3.016929817199707, '_timestamp': 1740957584.720486}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 49 is less than current step: 499. Dropping entry: {'train/lr': 0.00017779591836734692, '_timestamp': 1740957584.7207634}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 50 is less than current step: 499. Dropping entry: {'train/loss': 1.7560988306999206, 'train/ce_loss': 0.535205078125, 'train/seg_cls_loss': 0.016925048828125, 'train/kl_loss': 0.2279296875, 'train/mask_bce_loss': 0.10877473503351212, 'train/mask_dice_loss': 0.4860471248626709, 'train/mask_loss': 0.5948218584060669, 'metrics/total_secs_per_batch': 5.9733476638793945, 'metrics/data_secs_per_batch': 2.631748628616333, '_timestamp': 1740957590.6942792}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 50 is less than current step: 499. Dropping entry: {'train/lr': 0.00017767346938775508, '_timestamp': 1740957590.694657}).
Epoch: [2][ 52/500]	Time  6.255 ( 6.255)	Loss 1.8644 (1.7532)	CeLoss 0.2383 (0.3623)	SegCLSLoss 0.0205 (0.0166)	KLLoss 0.3848 (0.3125)	MaskLoss 0.7887 (0.6757)	MaskBCELoss 0.0430 (0.1147)	MaskDICELoss 0.7456 (0.5610)
Epoch: [2][ 53/500]	Time  6.788 ( 6.788)	Loss 2.1945 (1.9376)	CeLoss 0.1631 (0.4067)	SegCLSLoss 0.0212 (0.0171)	KLLoss 0.3926 (0.3486)	MaskLoss 0.9908 (0.7438)	MaskBCELoss 0.0208 (0.0929)	MaskDICELoss 0.9700 (0.6509)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 51 is less than current step: 499. Dropping entry: {'train/loss': 1.7531899333000183, 'train/ce_loss': 0.3623046875, 'train/seg_cls_loss': 0.01656494140625, 'train/kl_loss': 0.3125, 'train/mask_bce_loss': 0.11470560133457183, 'train/mask_dice_loss': 0.5610104471445083, 'train/mask_loss': 0.6757160604000092, 'metrics/total_secs_per_batch': 6.254870176315308, 'metrics/data_secs_per_batch': 2.7432374477386476, '_timestamp': 1740957596.9489555}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 51 is less than current step: 499. Dropping entry: {'train/lr': 0.00017755102040816325, '_timestamp': 1740957596.9491582}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 52 is less than current step: 499. Dropping entry: {'train/loss': 1.9375519156455994, 'train/ce_loss': 0.40673828125, 'train/seg_cls_loss': 0.01712646484375, 'train/kl_loss': 0.3486328125, 'train/mask_bce_loss': 0.09286491675302386, 'train/mask_dice_loss': 0.6509110450744628, 'train/mask_loss': 0.7437759697437286, 'metrics/total_secs_per_batch': 6.787768602371216, 'metrics/data_secs_per_batch': 3.079754662513733, '_timestamp': 1740957603.7368739}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 52 is less than current step: 499. Dropping entry: {'train/lr': 0.0001774285714285714, '_timestamp': 1740957603.7372286}).
Epoch: [2][ 54/500]	Time  5.709 ( 5.709)	Loss 0.8601 (1.6310)	CeLoss 0.3086 (0.3295)	SegCLSLoss 0.0142 (0.0163)	KLLoss 0.3906 (0.3498)	MaskLoss 0.2523 (0.6291)	MaskBCELoss 0.1491 (0.1592)	MaskDICELoss 0.1032 (0.4699)
Epoch: [2][ 55/500]	Time  6.044 ( 6.044)	Loss 1.0280 (2.0017)	CeLoss 0.5391 (0.3752)	SegCLSLoss 0.0137 (0.0167)	KLLoss 0.3965 (0.3107)	MaskLoss 0.2211 (0.7934)	MaskBCELoss 0.1352 (0.1887)	MaskDICELoss 0.0858 (0.6047)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 53 is less than current step: 499. Dropping entry: {'train/loss': 1.63096364736557, 'train/ce_loss': 0.3294921875, 'train/seg_cls_loss': 0.0163330078125, 'train/kl_loss': 0.3498046875, 'train/mask_bce_loss': 0.15917523093521596, 'train/mask_dice_loss': 0.46988081783056257, 'train/mask_loss': 0.6290560394525528, 'metrics/total_secs_per_batch': 5.708676099777222, 'metrics/data_secs_per_batch': 2.427822160720825, '_timestamp': 1740957609.4453518}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 53 is less than current step: 499. Dropping entry: {'train/lr': 0.00017730612244897957, '_timestamp': 1740957609.4456186}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 54 is less than current step: 499. Dropping entry: {'train/loss': 2.0016917824745177, 'train/ce_loss': 0.3751953125, 'train/seg_cls_loss': 0.0167236328125, 'train/kl_loss': 0.3107421875, 'train/mask_bce_loss': 0.18871710523962976, 'train/mask_dice_loss': 0.6046580865979194, 'train/mask_loss': 0.7933751881122589, 'metrics/total_secs_per_batch': 6.0439229011535645, 'metrics/data_secs_per_batch': 2.770304727554321, '_timestamp': 1740957615.4892998}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 54 is less than current step: 499. Dropping entry: {'train/lr': 0.00017718367346938773, '_timestamp': 1740957615.4895709}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 55 is less than current step: 499. Dropping entry: {'train/loss': 2.090527522563934, 'train/ce_loss': 0.37529296875, 'train/seg_cls_loss': 0.016845703125, 'train/kl_loss': 0.3484375, 'train/mask_bce_loss': 0.28636675868183376, 'train/mask_dice_loss': 0.5497173070907593, 'train/mask_loss': 0.8360840737819671, 'metrics/total_secs_per_batch': 6.241799592971802, 'metrics/data_secs_per_batch': 3.2113407373428347, '_timestamp': 1740957621.7312982}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 55 is less than current step: 499. Dropping entry: {'train/lr': 0.0001770612244897959, '_timestamp': 1740957621.7316532}).
Epoch: [2][ 56/500]	Time  6.242 ( 6.242)	Loss 1.3750 (2.0905)	CeLoss 1.3750 (0.3753)	SegCLSLoss 0.0000 (0.0168)	KLLoss 0.0000 (0.3484)	MaskLoss 0.0000 (0.8361)	MaskBCELoss 0.0000 (0.2864)	MaskDICELoss 0.0000 (0.5497)
Epoch: [2][ 57/500]	Time  6.681 ( 6.681)	Loss 2.0785 (1.7210)	CeLoss 0.1689 (0.4623)	SegCLSLoss 0.0237 (0.0147)	KLLoss 0.3848 (0.3123)	MaskLoss 0.9299 (0.6101)	MaskBCELoss 0.0316 (0.1119)	MaskDICELoss 0.8983 (0.4982)
Epoch: [2][ 58/500]	Time  7.097 ( 7.097)	Loss 2.1295 (1.7108)	CeLoss 0.1641 (0.4341)	SegCLSLoss 0.0177 (0.0126)	KLLoss 0.3867 (0.3168)	MaskLoss 0.9593 (0.6194)	MaskBCELoss 0.0208 (0.1356)	MaskDICELoss 0.9384 (0.4838)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 56 is less than current step: 499. Dropping entry: {'train/loss': 1.7210092902183534, 'train/ce_loss': 0.4623046875, 'train/seg_cls_loss': 0.014691162109375, 'train/kl_loss': 0.3123046875, 'train/mask_bce_loss': 0.11193195516243577, 'train/mask_dice_loss': 0.4981820613145828, 'train/mask_loss': 0.6101140201091766, 'metrics/total_secs_per_batch': 6.68079137802124, 'metrics/data_secs_per_batch': 2.7123517036437987, '_timestamp': 1740957628.4118664}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 56 is less than current step: 499. Dropping entry: {'train/lr': 0.00017693877551020406, '_timestamp': 1740957628.412145}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 57 is less than current step: 499. Dropping entry: {'train/loss': 1.7108123064041139, 'train/ce_loss': 0.43408203125, 'train/seg_cls_loss': 0.01260986328125, 'train/kl_loss': 0.316796875, 'train/mask_bce_loss': 0.13560226913541557, 'train/mask_dice_loss': 0.4837687283754349, 'train/mask_loss': 0.6193710029125213, 'metrics/total_secs_per_batch': 7.097327470779419, 'metrics/data_secs_per_batch': 2.870828700065613, '_timestamp': 1740957635.509183}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 57 is less than current step: 499. Dropping entry: {'train/lr': 0.00017681632653061222, '_timestamp': 1740957635.5094543}).
Epoch: [2][ 59/500]	Time  5.505 ( 5.505)	Loss 2.1541 (1.9194)	CeLoss 0.2637 (0.4362)	SegCLSLoss 0.0124 (0.0213)	KLLoss 0.4062 (0.3070)	MaskLoss 0.9218 (0.7210)	MaskBCELoss 0.2248 (0.1593)	MaskDICELoss 0.6969 (0.5617)
[2025-03-02 17:20:47,436] [INFO] [logging.py:96:log_dist] [Rank 0] step=1060, skipped=0, lr=[0.00017663265306122445], mom=[(0.9, 0.95)]
[2025-03-02 17:20:47,442] [INFO] [timer.py:215:stop] epoch=0/micro_step=10600/global_step=1060, RunningAvgSamplesPerSec=1.532593238650014, CurrSamplesPerSec=1.555870202200243, MemAllocated=31.25GB, MaxMemAllocated=37.19GB
Epoch: [2][ 60/500]	Time  6.429 ( 6.429)	Loss 1.7591 (1.9696)	CeLoss 0.2480 (0.4064)	SegCLSLoss 0.0220 (0.0166)	KLLoss 0.3809 (0.3070)	MaskLoss 0.7311 (0.7621)	MaskBCELoss 0.0501 (0.1087)	MaskDICELoss 0.6810 (0.6534)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 58 is less than current step: 499. Dropping entry: {'train/loss': 1.919368314743042, 'train/ce_loss': 0.43623046875, 'train/seg_cls_loss': 0.02125244140625, 'train/kl_loss': 0.30703125, 'train/mask_bce_loss': 0.1592972818762064, 'train/mask_dice_loss': 0.5616661727428436, 'train/mask_loss': 0.7209634631872177, 'metrics/total_secs_per_batch': 5.505154132843018, 'metrics/data_secs_per_batch': 2.522184467315674, '_timestamp': 1740957641.0145917}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 58 is less than current step: 499. Dropping entry: {'train/lr': 0.0001766938775510204, '_timestamp': 1740957641.0149372}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 59 is less than current step: 499. Dropping entry: {'train/loss': 1.9696208119392395, 'train/ce_loss': 0.4064453125, 'train/seg_cls_loss': 0.01656494140625, 'train/kl_loss': 0.30703125, 'train/mask_bce_loss': 0.10866590458899736, 'train/mask_dice_loss': 0.6533905982971191, 'train/mask_loss': 0.7620565056800842, 'metrics/total_secs_per_batch': 6.429127931594849, 'metrics/data_secs_per_batch': 2.5599430084228514, '_timestamp': 1740957647.4433148}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 59 is less than current step: 499. Dropping entry: {'train/lr': 0.00017657142857142855, '_timestamp': 1740957647.4436}).
Epoch: [2][ 61/500]	Time  6.398 ( 6.398)	Loss 2.6218 (1.7211)	CeLoss 0.2197 (0.2106)	SegCLSLoss 0.0176 (0.0176)	KLLoss 0.3828 (0.3104)	MaskLoss 1.1771 (0.7351)	MaskBCELoss 0.1783 (0.1186)	MaskDICELoss 0.9988 (0.6165)
Epoch: [2][ 62/500]	Time  6.613 ( 6.613)	Loss 2.3163 (1.7934)	CeLoss 0.2158 (0.2693)	SegCLSLoss 0.0317 (0.0214)	KLLoss 0.3828 (0.3100)	MaskLoss 1.0234 (0.7412)	MaskBCELoss 0.0293 (0.1790)	MaskDICELoss 0.9941 (0.5622)
Epoch: [2][ 63/500]	Time  5.821 ( 5.821)	Loss 2.2936 (1.6515)	CeLoss 0.2412 (0.4078)	SegCLSLoss 0.0221 (0.0149)	KLLoss 0.3789 (0.2283)	MaskLoss 1.0023 (0.6068)	MaskBCELoss 0.0515 (0.1061)	MaskDICELoss 0.9508 (0.5006)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 60 is less than current step: 499. Dropping entry: {'train/loss': 1.7210822820663452, 'train/ce_loss': 0.2106201171875, 'train/seg_cls_loss': 0.01761474609375, 'train/kl_loss': 0.3103515625, 'train/mask_bce_loss': 0.11862793751060963, 'train/mask_dice_loss': 0.6164859533309937, 'train/mask_loss': 0.735113900899887, 'metrics/total_secs_per_batch': 6.397648334503174, 'metrics/data_secs_per_batch': 2.7193289518356325, '_timestamp': 1740957653.8411965}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 60 is less than current step: 499. Dropping entry: {'train/lr': 0.0001764489795918367, '_timestamp': 1740957653.8415077}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 61 is less than current step: 499. Dropping entry: {'train/loss': 1.7933569312095643, 'train/ce_loss': 0.269287109375, 'train/seg_cls_loss': 0.021392822265625, 'train/kl_loss': 0.3099609375, 'train/mask_bce_loss': 0.17897761892527342, 'train/mask_dice_loss': 0.5621832817792892, 'train/mask_loss': 0.7411608994007111, 'metrics/total_secs_per_batch': 6.613463401794434, 'metrics/data_secs_per_batch': 2.992250156402588, '_timestamp': 1740957660.454802}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 61 is less than current step: 499. Dropping entry: {'train/lr': 0.0001763265306122449, '_timestamp': 1740957660.4551272}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 62 is less than current step: 499. Dropping entry: {'train/loss': 1.6515118956565857, 'train/ce_loss': 0.407763671875, 'train/seg_cls_loss': 0.01492919921875, 'train/kl_loss': 0.2283203125, 'train/mask_bce_loss': 0.10613930895924568, 'train/mask_dice_loss': 0.5006469249725342, 'train/mask_loss': 0.6067862331867218, 'metrics/total_secs_per_batch': 5.820951700210571, 'metrics/data_secs_per_batch': 2.454286241531372, '_timestamp': 1740957666.2756164}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 62 is less than current step: 499. Dropping entry: {'train/lr': 0.00017620408163265307, '_timestamp': 1740957666.2759075}).
Epoch: [2][ 64/500]	Time  5.690 ( 5.690)	Loss 0.1099 (1.8906)	CeLoss 0.1099 (0.4464)	SegCLSLoss 0.0000 (0.0143)	KLLoss 0.0000 (0.2697)	MaskLoss 0.0000 (0.7051)	MaskBCELoss 0.0000 (0.1441)	MaskDICELoss 0.0000 (0.5611)
Epoch: [2][ 65/500]	Time  6.733 ( 6.733)	Loss 2.6185 (1.6459)	CeLoss 0.1689 (0.1901)	SegCLSLoss 0.0287 (0.0187)	KLLoss 0.3730 (0.3096)	MaskLoss 1.1989 (0.7077)	MaskBCELoss 0.2978 (0.1564)	MaskDICELoss 0.9011 (0.5513)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 63 is less than current step: 499. Dropping entry: {'train/loss': 1.890603816509247, 'train/ce_loss': 0.446435546875, 'train/seg_cls_loss': 0.01431884765625, 'train/kl_loss': 0.2697265625, 'train/mask_bce_loss': 0.144089787453413, 'train/mask_dice_loss': 0.5610509872436523, 'train/mask_loss': 0.7051407814025878, 'metrics/total_secs_per_batch': 5.689876079559326, 'metrics/data_secs_per_batch': 2.4431772947311403, '_timestamp': 1740957671.9656308}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 63 is less than current step: 499. Dropping entry: {'train/lr': 0.00017608163265306123, '_timestamp': 1740957671.965996}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 64 is less than current step: 499. Dropping entry: {'train/loss': 1.6459494590759278, 'train/ce_loss': 0.190087890625, 'train/seg_cls_loss': 0.0187255859375, 'train/kl_loss': 0.3095703125, 'train/mask_bce_loss': 0.156431554723531, 'train/mask_dice_loss': 0.5512843817472458, 'train/mask_loss': 0.7077159464359284, 'metrics/total_secs_per_batch': 6.733196020126343, 'metrics/data_secs_per_batch': 2.8292703151702883, '_timestamp': 1740957678.6986935}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 64 is less than current step: 499. Dropping entry: {'train/lr': 0.0001759591836734694, '_timestamp': 1740957678.6989882}).
Epoch: [2][ 66/500]	Time  5.310 ( 5.310)	Loss 1.1406 (1.6598)	CeLoss 1.1406 (0.5114)	SegCLSLoss 0.0000 (0.0137)	KLLoss 0.0000 (0.2750)	MaskLoss 0.0000 (0.5570)	MaskBCELoss 0.0000 (0.0856)	MaskDICELoss 0.0000 (0.4713)
Epoch: [2][ 67/500]	Time  6.679 ( 6.679)	Loss 1.4501 (1.8659)	CeLoss 0.2559 (0.3183)	SegCLSLoss 0.0146 (0.0218)	KLLoss 0.3926 (0.3453)	MaskLoss 0.5737 (0.7511)	MaskBCELoss 0.1028 (0.1548)	MaskDICELoss 0.4708 (0.5963)
Epoch: [2][ 68/500]	Time  6.433 ( 6.433)	Loss 2.2157 (1.9368)	CeLoss 0.2432 (0.4216)	SegCLSLoss 0.0195 (0.0167)	KLLoss 0.3867 (0.3135)	MaskLoss 0.9613 (0.7376)	MaskBCELoss 0.0191 (0.0838)	MaskDICELoss 0.9423 (0.6538)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 65 is less than current step: 499. Dropping entry: {'train/loss': 1.659799760580063, 'train/ce_loss': 0.51142578125, 'train/seg_cls_loss': 0.013702392578125, 'train/kl_loss': 0.275, 'train/mask_bce_loss': 0.08562325863167644, 'train/mask_dice_loss': 0.4713273912668228, 'train/mask_loss': 0.556950643658638, 'metrics/total_secs_per_batch': 5.310147047042847, 'metrics/data_secs_per_batch': 2.4400589942932127, '_timestamp': 1740957684.0087712}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 65 is less than current step: 499. Dropping entry: {'train/lr': 0.00017583673469387756, '_timestamp': 1740957684.0090742}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 66 is less than current step: 499. Dropping entry: {'train/loss': 1.8659108579158783, 'train/ce_loss': 0.31826171875, 'train/seg_cls_loss': 0.021844482421875, 'train/kl_loss': 0.3453125, 'train/mask_bce_loss': 0.15482276417315005, 'train/mask_dice_loss': 0.5962967306375504, 'train/mask_loss': 0.7511194944381714, 'metrics/total_secs_per_batch': 6.6794726848602295, 'metrics/data_secs_per_batch': 3.0021188020706178, '_timestamp': 1740957690.688486}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 66 is less than current step: 499. Dropping entry: {'train/lr': 0.00017571428571428572, '_timestamp': 1740957690.688865}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 67 is less than current step: 499. Dropping entry: {'train/loss': 1.93681640625, 'train/ce_loss': 0.42158203125, 'train/seg_cls_loss': 0.01671142578125, 'train/kl_loss': 0.3134765625, 'train/mask_bce_loss': 0.08383832089602947, 'train/mask_dice_loss': 0.6538081645965577, 'train/mask_loss': 0.7376464903354645, 'metrics/total_secs_per_batch': 6.432640075683594, 'metrics/data_secs_per_batch': 3.088825011253357, '_timestamp': 1740957697.1209204}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 67 is less than current step: 499. Dropping entry: {'train/lr': 0.00017559183673469388, '_timestamp': 1740957697.1212459}).
Epoch: [2][ 69/500]	Time  6.779 ( 6.779)	Loss 0.9731 (1.7345)	CeLoss 0.2812 (0.3206)	SegCLSLoss 0.0134 (0.0154)	KLLoss 0.4023 (0.3105)	MaskLoss 0.3225 (0.6875)	MaskBCELoss 0.0814 (0.0830)	MaskDICELoss 0.2411 (0.6045)
[2025-03-02 17:21:50,183] [INFO] [logging.py:96:log_dist] [Rank 0] step=1070, skipped=0, lr=[0.0001754081632653061], mom=[(0.9, 0.95)]
[2025-03-02 17:21:50,188] [INFO] [timer.py:215:stop] epoch=0/micro_step=10700/global_step=1070, RunningAvgSamplesPerSec=1.5331468831342427, CurrSamplesPerSec=1.5904343633904148, MemAllocated=31.26GB, MaxMemAllocated=37.19GB
Epoch: [2][ 70/500]	Time  6.289 ( 6.289)	Loss 2.2331 (1.8136)	CeLoss 0.1934 (0.2504)	SegCLSLoss 0.0223 (0.0184)	KLLoss 0.3867 (0.3492)	MaskLoss 0.9954 (0.7597)	MaskBCELoss 0.0085 (0.1416)	MaskDICELoss 0.9869 (0.6181)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 68 is less than current step: 499. Dropping entry: {'train/loss': 1.734517127275467, 'train/ce_loss': 0.32060546875, 'train/seg_cls_loss': 0.01541748046875, 'train/kl_loss': 0.310546875, 'train/mask_bce_loss': 0.08297781893052161, 'train/mask_dice_loss': 0.6045444190502167, 'train/mask_loss': 0.6875222295522689, 'metrics/total_secs_per_batch': 6.778839111328125, 'metrics/data_secs_per_batch': 3.001845669746399, '_timestamp': 1740957703.8998096}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 68 is less than current step: 499. Dropping entry: {'train/lr': 0.00017546938775510204, '_timestamp': 1740957703.9001029}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 69 is less than current step: 499. Dropping entry: {'train/loss': 1.8135533571243285, 'train/ce_loss': 0.250390625, 'train/seg_cls_loss': 0.018359375, 'train/kl_loss': 0.34921875, 'train/mask_bce_loss': 0.14157173531129957, 'train/mask_dice_loss': 0.6181346088647842, 'train/mask_loss': 0.7597063511610032, 'metrics/total_secs_per_batch': 6.2892067432403564, 'metrics/data_secs_per_batch': 2.650858163833618, '_timestamp': 1740957710.1887844}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 69 is less than current step: 499. Dropping entry: {'train/lr': 0.00017534693877551018, '_timestamp': 1740957710.1890693}).
Epoch: [2][ 71/500]	Time  6.060 ( 6.060)	Loss 1.4910 (1.8199)	CeLoss 0.1992 (0.4366)	SegCLSLoss 0.0176 (0.0126)	KLLoss 0.3945 (0.2746)	MaskLoss 0.6215 (0.6749)	MaskBCELoss 0.2418 (0.1666)	MaskDICELoss 0.3797 (0.5083)
Epoch: [2][ 72/500]	Time  6.122 ( 6.122)	Loss 2.1857 (1.9998)	CeLoss 0.1924 (0.3851)	SegCLSLoss 0.0267 (0.0164)	KLLoss 0.3711 (0.3482)	MaskLoss 0.9713 (0.7859)	MaskBCELoss 0.0357 (0.0992)	MaskDICELoss 0.9356 (0.6867)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 70 is less than current step: 499. Dropping entry: {'train/loss': 1.8199033737182617, 'train/ce_loss': 0.43662109375, 'train/seg_cls_loss': 0.01256103515625, 'train/kl_loss': 0.274609375, 'train/mask_bce_loss': 0.166559118963778, 'train/mask_dice_loss': 0.5083339720964432, 'train/mask_loss': 0.6748930811882019, 'metrics/total_secs_per_batch': 6.0598437786102295, 'metrics/data_secs_per_batch': 2.692322516441345, '_timestamp': 1740957716.248897}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 70 is less than current step: 499. Dropping entry: {'train/lr': 0.00017522448979591834, '_timestamp': 1740957716.2491994}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 71 is less than current step: 499. Dropping entry: {'train/loss': 1.9997855067253112, 'train/ce_loss': 0.38505859375, 'train/seg_cls_loss': 0.016351318359375, 'train/kl_loss': 0.3482421875, 'train/mask_bce_loss': 0.0991908514406532, 'train/mask_dice_loss': 0.6866882234811783, 'train/mask_loss': 0.7858790725469589, 'metrics/total_secs_per_batch': 6.122430324554443, 'metrics/data_secs_per_batch': 2.8090510606765746, '_timestamp': 1740957722.3713589}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 71 is less than current step: 499. Dropping entry: {'train/lr': 0.0001751020408163265, '_timestamp': 1740957722.3716657}).
Epoch: [2][ 73/500]	Time  5.804 ( 5.804)	Loss 0.9829 (2.0413)	CeLoss 0.2334 (0.4072)	SegCLSLoss 0.0135 (0.0197)	KLLoss 0.3984 (0.3447)	MaskLoss 0.3518 (0.7948)	MaskBCELoss 0.1258 (0.1182)	MaskDICELoss 0.2260 (0.6765)
Epoch: [2][ 74/500]	Time  5.482 ( 5.482)	Loss 1.6869 (1.9503)	CeLoss 0.1699 (0.4002)	SegCLSLoss 0.0262 (0.0203)	KLLoss 0.3828 (0.3096)	MaskLoss 0.7331 (0.7545)	MaskBCELoss 0.1768 (0.1285)	MaskDICELoss 0.5562 (0.6260)
Epoch: [2][ 75/500]	Time  6.270 ( 6.270)	Loss 0.1689 (1.4513)	CeLoss 0.1689 (0.4028)	SegCLSLoss 0.0000 (0.0148)	KLLoss 0.0000 (0.2734)	MaskLoss 0.0000 (0.5069)	MaskBCELoss 0.0000 (0.0862)	MaskDICELoss 0.0000 (0.4208)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 72 is less than current step: 499. Dropping entry: {'train/loss': 2.0412979423999786, 'train/ce_loss': 0.4072265625, 'train/seg_cls_loss': 0.019732666015625, 'train/kl_loss': 0.3447265625, 'train/mask_bce_loss': 0.1182493522297591, 'train/mask_dice_loss': 0.6765207141637802, 'train/mask_loss': 0.7947700709104538, 'metrics/total_secs_per_batch': 5.803867816925049, 'metrics/data_secs_per_batch': 2.6100209236145018, '_timestamp': 1740957728.175082}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 72 is less than current step: 499. Dropping entry: {'train/lr': 0.00017497959183673467, '_timestamp': 1740957728.1753476}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 73 is less than current step: 499. Dropping entry: {'train/loss': 1.9503444671630858, 'train/ce_loss': 0.4001953125, 'train/seg_cls_loss': 0.02034912109375, 'train/kl_loss': 0.3095703125, 'train/mask_bce_loss': 0.12852375227957963, 'train/mask_dice_loss': 0.62599418759346, 'train/mask_loss': 0.7545179426670074, 'metrics/total_secs_per_batch': 5.482163429260254, 'metrics/data_secs_per_batch': 2.366071581840515, '_timestamp': 1740957733.6573162}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 73 is less than current step: 499. Dropping entry: {'train/lr': 0.00017485714285714283, '_timestamp': 1740957733.6576037}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 74 is less than current step: 499. Dropping entry: {'train/loss': 1.4513017892837525, 'train/ce_loss': 0.40283203125, 'train/seg_cls_loss': 0.0147705078125, 'train/kl_loss': 0.2734375, 'train/mask_bce_loss': 0.08616879656910896, 'train/mask_dice_loss': 0.4207809165120125, 'train/mask_loss': 0.5069497108459473, 'metrics/total_secs_per_batch': 6.2698564529418945, 'metrics/data_secs_per_batch': 2.9666481018066406, '_timestamp': 1740957739.927144}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 74 is less than current step: 499. Dropping entry: {'train/lr': 0.000174734693877551, '_timestamp': 1740957739.9274127}).
Epoch: [2][ 76/500]	Time  6.748 ( 6.748)	Loss 1.9141 (1.8596)	CeLoss 1.9141 (0.3826)	SegCLSLoss 0.0000 (0.0159)	KLLoss 0.0000 (0.3055)	MaskLoss 0.0000 (0.7194)	MaskBCELoss 0.0000 (0.1015)	MaskDICELoss 0.0000 (0.6179)
Epoch: [2][ 77/500]	Time  7.296 ( 7.296)	Loss 2.6143 (1.9776)	CeLoss 0.3359 (0.2299)	SegCLSLoss 0.0181 (0.0220)	KLLoss 0.3965 (0.3836)	MaskLoss 1.1148 (0.8491)	MaskBCELoss 0.2947 (0.0885)	MaskDICELoss 0.8200 (0.7606)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 75 is less than current step: 499. Dropping entry: {'train/loss': 1.859575664997101, 'train/ce_loss': 0.382568359375, 'train/seg_cls_loss': 0.01593017578125, 'train/kl_loss': 0.30546875, 'train/mask_bce_loss': 0.10150451380759477, 'train/mask_dice_loss': 0.6178584903478622, 'train/mask_loss': 0.7193629920482636, 'metrics/total_secs_per_batch': 6.747570991516113, 'metrics/data_secs_per_batch': 3.104420566558838, '_timestamp': 1740957746.674927}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 75 is less than current step: 499. Dropping entry: {'train/lr': 0.00017461224489795916, '_timestamp': 1740957746.6751618}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 76 is less than current step: 499. Dropping entry: {'train/loss': 1.977555751800537, 'train/ce_loss': 0.2298828125, 'train/seg_cls_loss': 0.0219970703125, 'train/kl_loss': 0.38359375, 'train/mask_bce_loss': 0.08850892595946788, 'train/mask_dice_loss': 0.7605716824531555, 'train/mask_loss': 0.849080604314804, 'metrics/total_secs_per_batch': 7.296249151229858, 'metrics/data_secs_per_batch': 3.3274040699005125, '_timestamp': 1740957753.9709952}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 76 is less than current step: 499. Dropping entry: {'train/lr': 0.00017448979591836732, '_timestamp': 1740957753.9712892}).
Epoch: [2][ 78/500]	Time  6.830 ( 6.830)	Loss 0.7109 (1.6284)	CeLoss 0.7109 (0.3335)	SegCLSLoss 0.0000 (0.0158)	KLLoss 0.0000 (0.2717)	MaskLoss 0.0000 (0.6299)	MaskBCELoss 0.0000 (0.0810)	MaskDICELoss 0.0000 (0.5490)
Epoch: [2][ 79/500]	Time  6.333 ( 6.333)	Loss 2.1157 (1.9776)	CeLoss 0.2480 (0.6881)	SegCLSLoss 0.0146 (0.0148)	KLLoss 0.4004 (0.2258)	MaskLoss 0.9094 (0.6299)	MaskBCELoss 0.1573 (0.1581)	MaskDICELoss 0.7521 (0.4718)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 77 is less than current step: 499. Dropping entry: {'train/loss': 1.6284206986427308, 'train/ce_loss': 0.33349609375, 'train/seg_cls_loss': 0.015771484375, 'train/kl_loss': 0.2716796875, 'train/mask_bce_loss': 0.08098029363900423, 'train/mask_dice_loss': 0.548952704668045, 'train/mask_loss': 0.6299330055713653, 'metrics/total_secs_per_batch': 6.830155372619629, 'metrics/data_secs_per_batch': 3.053799939155579, '_timestamp': 1740957760.8012292}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 77 is less than current step: 499. Dropping entry: {'train/lr': 0.00017436734693877548, '_timestamp': 1740957760.801573}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 78 is less than current step: 499. Dropping entry: {'train/loss': 1.9775759696960449, 'train/ce_loss': 0.6880859375, 'train/seg_cls_loss': 0.014849853515625, 'train/kl_loss': 0.22578125, 'train/mask_bce_loss': 0.1580890253186226, 'train/mask_dice_loss': 0.471763414144516, 'train/mask_loss': 0.6298524379730225, 'metrics/total_secs_per_batch': 6.333059072494507, 'metrics/data_secs_per_batch': 2.8253458023071287, '_timestamp': 1740957767.1342385}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 78 is less than current step: 499. Dropping entry: {'train/lr': 0.00017424489795918365, '_timestamp': 1740957767.1345284}).
[2025-03-02 17:22:53,654] [INFO] [logging.py:96:log_dist] [Rank 0] step=1080, skipped=0, lr=[0.00017418367346938774], mom=[(0.9, 0.95)]
[2025-03-02 17:22:53,660] [INFO] [timer.py:215:stop] epoch=0/micro_step=10800/global_step=1080, RunningAvgSamplesPerSec=1.5335319575924897, CurrSamplesPerSec=1.5324465273749135, MemAllocated=30.69GB, MaxMemAllocated=37.19GB
Epoch: [2][ 80/500]	Time  6.527 ( 6.527)	Loss 1.5156 (1.6689)	CeLoss 1.5156 (0.4065)	SegCLSLoss 0.0000 (0.0158)	KLLoss 0.0000 (0.2666)	MaskLoss 0.0000 (0.6138)	MaskBCELoss 0.0000 (0.1364)	MaskDICELoss 0.0000 (0.4774)
Epoch: [2][ 81/500]	Time  6.072 ( 6.072)	Loss 1.6987 (1.7576)	CeLoss 0.2656 (0.5128)	SegCLSLoss 0.0136 (0.0126)	KLLoss 0.3867 (0.2684)	MaskLoss 0.6941 (0.6059)	MaskBCELoss 0.1635 (0.1241)	MaskDICELoss 0.5306 (0.4817)
Epoch: [2][ 82/500]	Time  5.534 ( 5.534)	Loss 2.4953 (1.7841)	CeLoss 0.1377 (0.5428)	SegCLSLoss 0.0408 (0.0135)	KLLoss 0.3750 (0.2295)	MaskLoss 1.1500 (0.6057)	MaskBCELoss 0.3293 (0.1594)	MaskDICELoss 0.8207 (0.4463)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 79 is less than current step: 499. Dropping entry: {'train/loss': 1.6689070463180542, 'train/ce_loss': 0.406494140625, 'train/seg_cls_loss': 0.015789794921875, 'train/kl_loss': 0.2666015625, 'train/mask_bce_loss': 0.1363782834261656, 'train/mask_dice_loss': 0.4774209260940552, 'train/mask_loss': 0.6137992084026337, 'metrics/total_secs_per_batch': 6.527127504348755, 'metrics/data_secs_per_batch': 3.2201607465744018, '_timestamp': 1740957773.6611853}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 79 is less than current step: 499. Dropping entry: {'train/lr': 0.0001741224489795918, '_timestamp': 1740957773.661495}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 80 is less than current step: 499. Dropping entry: {'train/loss': 1.757637333869934, 'train/ce_loss': 0.51279296875, 'train/seg_cls_loss': 0.012591552734375, 'train/kl_loss': 0.268359375, 'train/mask_bce_loss': 0.12414380460977555, 'train/mask_dice_loss': 0.4817256510257721, 'train/mask_loss': 0.6058694541454315, 'metrics/total_secs_per_batch': 6.072214126586914, 'metrics/data_secs_per_batch': 2.8644631624221804, '_timestamp': 1740957779.7336679}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 80 is less than current step: 499. Dropping entry: {'train/lr': 0.00017399999999999997, '_timestamp': 1740957779.7340546}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 81 is less than current step: 499. Dropping entry: {'train/loss': 1.7841305494308473, 'train/ce_loss': 0.5427734375, 'train/seg_cls_loss': 0.0134765625, 'train/kl_loss': 0.2294921875, 'train/mask_bce_loss': 0.1594032220542431, 'train/mask_dice_loss': 0.44628508687019347, 'train/mask_loss': 0.6056883037090302, 'metrics/total_secs_per_batch': 5.534430980682373, 'metrics/data_secs_per_batch': 2.425184416770935, '_timestamp': 1740957785.2680273}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 81 is less than current step: 499. Dropping entry: {'train/lr': 0.00017387755102040814, '_timestamp': 1740957785.2683852}).
Epoch: [2][ 83/500]	Time  6.223 ( 6.223)	Loss 2.2676 (1.9693)	CeLoss 0.1953 (0.4508)	SegCLSLoss 0.0168 (0.0154)	KLLoss 0.4082 (0.3088)	MaskLoss 1.0112 (0.7400)	MaskBCELoss 0.0184 (0.1383)	MaskDICELoss 0.9928 (0.6018)
Epoch: [2][ 84/500]	Time  6.373 ( 6.373)	Loss 2.3689 (1.8964)	CeLoss 0.1436 (0.3037)	SegCLSLoss 0.0515 (0.0210)	KLLoss 0.3652 (0.3414)	MaskLoss 1.0819 (0.7740)	MaskBCELoss 0.2106 (0.1347)	MaskDICELoss 0.8713 (0.6393)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 82 is less than current step: 499. Dropping entry: {'train/loss': 1.969302523136139, 'train/ce_loss': 0.45078125, 'train/seg_cls_loss': 0.015380859375, 'train/kl_loss': 0.3087890625, 'train/mask_bce_loss': 0.1382669311016798, 'train/mask_dice_loss': 0.6017554134130478, 'train/mask_loss': 0.7400223553180695, 'metrics/total_secs_per_batch': 6.2234485149383545, 'metrics/data_secs_per_batch': 2.7857818365097047, '_timestamp': 1740957791.4914174}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 82 is less than current step: 499. Dropping entry: {'train/lr': 0.0001737551020408163, '_timestamp': 1740957791.4916117}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 83 is less than current step: 499. Dropping entry: {'train/loss': 1.8963759422302247, 'train/ce_loss': 0.3037109375, 'train/seg_cls_loss': 0.021044921875, 'train/kl_loss': 0.34140625, 'train/mask_bce_loss': 0.13469966128468513, 'train/mask_dice_loss': 0.6393183678388595, 'train/mask_loss': 0.7740180253982544, 'metrics/total_secs_per_batch': 6.372893810272217, 'metrics/data_secs_per_batch': 2.797197961807251, '_timestamp': 1740957797.8643806}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 83 is less than current step: 499. Dropping entry: {'train/lr': 0.00017363265306122446, '_timestamp': 1740957797.8646927}).
Epoch: [2][ 85/500]	Time  5.527 ( 5.527)	Loss 1.8695 (1.6813)	CeLoss 0.1826 (0.3148)	SegCLSLoss 0.0266 (0.0183)	KLLoss 0.3652 (0.3064)	MaskLoss 0.8186 (0.6634)	MaskBCELoss 0.1618 (0.1845)	MaskDICELoss 0.6568 (0.4789)
Epoch: [2][ 86/500]	Time  5.426 ( 5.426)	Loss 0.8125 (1.7542)	CeLoss 0.8125 (0.4990)	SegCLSLoss 0.0000 (0.0163)	KLLoss 0.0000 (0.2641)	MaskLoss 0.0000 (0.6102)	MaskBCELoss 0.0000 (0.0658)	MaskDICELoss 0.0000 (0.5444)
Epoch: [2][ 87/500]	Time  6.464 ( 6.464)	Loss 1.9137 (2.0923)	CeLoss 0.2676 (0.2189)	SegCLSLoss 0.0128 (0.0228)	KLLoss 0.3965 (0.3389)	MaskLoss 0.8006 (0.9141)	MaskBCELoss 0.2184 (0.2580)	MaskDICELoss 0.5822 (0.6560)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 84 is less than current step: 499. Dropping entry: {'train/loss': 1.6812718868255616, 'train/ce_loss': 0.314794921875, 'train/seg_cls_loss': 0.01827392578125, 'train/kl_loss': 0.3064453125, 'train/mask_bce_loss': 0.18448566757142543, 'train/mask_dice_loss': 0.47892857640981673, 'train/mask_loss': 0.663414254784584, 'metrics/total_secs_per_batch': 5.526771068572998, 'metrics/data_secs_per_batch': 2.2580142498016356, '_timestamp': 1740957803.39118}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 84 is less than current step: 499. Dropping entry: {'train/lr': 0.00017351020408163265, '_timestamp': 1740957803.3915079}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 85 is less than current step: 499. Dropping entry: {'train/loss': 1.7541925668716432, 'train/ce_loss': 0.4990234375, 'train/seg_cls_loss': 0.0163330078125, 'train/kl_loss': 0.2640625, 'train/mask_bce_loss': 0.06582318749278784, 'train/mask_dice_loss': 0.5443785786628723, 'train/mask_loss': 0.6102017670869827, 'metrics/total_secs_per_batch': 5.425852537155151, 'metrics/data_secs_per_batch': 2.312237000465393, '_timestamp': 1740957808.8169842}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 85 is less than current step: 499. Dropping entry: {'train/lr': 0.00017338775510204082, '_timestamp': 1740957808.8172648}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 86 is less than current step: 499. Dropping entry: {'train/loss': 2.092311131954193, 'train/ce_loss': 0.2189453125, 'train/seg_cls_loss': 0.0227783203125, 'train/kl_loss': 0.3388671875, 'train/mask_bce_loss': 0.2580289416015148, 'train/mask_dice_loss': 0.6560465395450592, 'train/mask_loss': 0.9140754818916321, 'metrics/total_secs_per_batch': 6.463837146759033, 'metrics/data_secs_per_batch': 2.8542192459106444, '_timestamp': 1740957815.2808325}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 86 is less than current step: 499. Dropping entry: {'train/lr': 0.00017326530612244898, '_timestamp': 1740957815.2811399}).
Epoch: [2][ 88/500]	Time  6.476 ( 6.476)	Loss 1.8450 (2.0623)	CeLoss 0.2002 (0.4039)	SegCLSLoss 0.0228 (0.0219)	KLLoss 0.3848 (0.3379)	MaskLoss 0.7975 (0.8069)	MaskBCELoss 0.0551 (0.1649)	MaskDICELoss 0.7424 (0.6420)
Epoch: [2][ 89/500]	Time  5.027 ( 5.027)	Loss 1.4084 (1.6417)	CeLoss 0.2598 (0.5791)	SegCLSLoss 0.0125 (0.0122)	KLLoss 0.3809 (0.2691)	MaskLoss 0.5518 (0.5147)	MaskBCELoss 0.1506 (0.0708)	MaskDICELoss 0.4012 (0.4440)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 87 is less than current step: 499. Dropping entry: {'train/loss': 2.062335705757141, 'train/ce_loss': 0.40390625, 'train/seg_cls_loss': 0.0218505859375, 'train/kl_loss': 0.337890625, 'train/mask_bce_loss': 0.1648824367672205, 'train/mask_dice_loss': 0.6419690191745758, 'train/mask_loss': 0.8068514466285706, 'metrics/total_secs_per_batch': 6.475737571716309, 'metrics/data_secs_per_batch': 2.7934890270233153, '_timestamp': 1740957821.7567167}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 87 is less than current step: 499. Dropping entry: {'train/lr': 0.00017314285714285714, '_timestamp': 1740957821.7569487}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 88 is less than current step: 499. Dropping entry: {'train/loss': 1.6416982054710387, 'train/ce_loss': 0.5791015625, 'train/seg_cls_loss': 0.012237548828125, 'train/kl_loss': 0.269140625, 'train/mask_bce_loss': 0.07077955063432455, 'train/mask_dice_loss': 0.4439660280942917, 'train/mask_loss': 0.5147455751895904, 'metrics/total_secs_per_batch': 5.027141094207764, 'metrics/data_secs_per_batch': 2.363804268836975, '_timestamp': 1740957826.7837188}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 88 is less than current step: 499. Dropping entry: {'train/lr': 0.0001730204081632653, '_timestamp': 1740957826.7840314}).
[2025-03-02 17:23:53,750] [INFO] [logging.py:96:log_dist] [Rank 0] step=1090, skipped=0, lr=[0.00017295918367346937], mom=[(0.9, 0.95)]
[2025-03-02 17:23:53,756] [INFO] [timer.py:215:stop] epoch=0/micro_step=10900/global_step=1090, RunningAvgSamplesPerSec=1.5346406493664049, CurrSamplesPerSec=1.4342541473493235, MemAllocated=31.45GB, MaxMemAllocated=37.19GB
Epoch: [2][ 90/500]	Time  6.974 ( 6.974)	Loss 1.4568 (1.5812)	CeLoss 0.2480 (0.3167)	SegCLSLoss 0.0195 (0.0173)	KLLoss 0.3848 (0.2656)	MaskLoss 0.5800 (0.6147)	MaskBCELoss 0.0308 (0.1164)	MaskDICELoss 0.5492 (0.4983)
Epoch: [2][ 91/500]	Time  6.010 ( 6.010)	Loss 1.0781 (1.8296)	CeLoss 1.0781 (0.3082)	SegCLSLoss 0.0000 (0.0167)	KLLoss 0.0000 (0.3025)	MaskLoss 0.0000 (0.7415)	MaskBCELoss 0.0000 (0.1956)	MaskDICELoss 0.0000 (0.5459)
Epoch: [2][ 92/500]	Time  6.435 ( 6.435)	Loss 2.3787 (1.9764)	CeLoss 0.1162 (0.2439)	SegCLSLoss 0.0505 (0.0226)	KLLoss 0.3574 (0.3809)	MaskLoss 1.1005 (0.8415)	MaskBCELoss 0.2787 (0.2273)	MaskDICELoss 0.8218 (0.6142)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 89 is less than current step: 499. Dropping entry: {'train/loss': 1.5811882972717286, 'train/ce_loss': 0.31669921875, 'train/seg_cls_loss': 0.017333984375, 'train/kl_loss': 0.265625, 'train/mask_bce_loss': 0.11639855783432722, 'train/mask_dice_loss': 0.49831668436527254, 'train/mask_loss': 0.6147152483463287, 'metrics/total_secs_per_batch': 6.973932504653931, 'metrics/data_secs_per_batch': 3.273018002510071, '_timestamp': 1740957833.7574914}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 89 is less than current step: 499. Dropping entry: {'train/lr': 0.00017289795918367347, '_timestamp': 1740957833.7577944}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 90 is less than current step: 499. Dropping entry: {'train/loss': 1.82961608171463, 'train/ce_loss': 0.308154296875, 'train/seg_cls_loss': 0.01669921875, 'train/kl_loss': 0.3025390625, 'train/mask_bce_loss': 0.19556336775422095, 'train/mask_dice_loss': 0.5459292352199554, 'train/mask_loss': 0.741492611169815, 'metrics/total_secs_per_batch': 6.009900093078613, 'metrics/data_secs_per_batch': 2.8033560276031495, '_timestamp': 1740957839.7677336}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 90 is less than current step: 499. Dropping entry: {'train/lr': 0.00017277551020408163, '_timestamp': 1740957839.7681196}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 91 is less than current step: 499. Dropping entry: {'train/loss': 1.9764222264289857, 'train/ce_loss': 0.2439453125, 'train/seg_cls_loss': 0.02261962890625, 'train/kl_loss': 0.380859375, 'train/mask_bce_loss': 0.22733580749481916, 'train/mask_dice_loss': 0.6141956239938736, 'train/mask_loss': 0.8415314257144928, 'metrics/total_secs_per_batch': 6.434643983840942, 'metrics/data_secs_per_batch': 2.786647629737854, '_timestamp': 1740957846.2022345}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 91 is less than current step: 499. Dropping entry: {'train/lr': 0.0001726530612244898, '_timestamp': 1740957846.2025514}).
Epoch: [2][ 93/500]	Time  5.823 ( 5.823)	Loss 0.6133 (1.5633)	CeLoss 0.6133 (0.2509)	SegCLSLoss 0.0000 (0.0201)	KLLoss 0.0000 (0.3053)	MaskLoss 0.0000 (0.6359)	MaskBCELoss 0.0000 (0.1856)	MaskDICELoss 0.0000 (0.4502)
Epoch: [2][ 94/500]	Time  6.753 ( 6.753)	Loss 2.7327 (1.9807)	CeLoss 0.2090 (0.3354)	SegCLSLoss 0.0248 (0.0162)	KLLoss 0.3652 (0.3045)	MaskLoss 1.2374 (0.8035)	MaskBCELoss 0.3506 (0.1696)	MaskDICELoss 0.8868 (0.6339)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 92 is less than current step: 499. Dropping entry: {'train/loss': 1.5632517337799072, 'train/ce_loss': 0.25087890625, 'train/seg_cls_loss': 0.020135498046875, 'train/kl_loss': 0.3052734375, 'train/mask_bce_loss': 0.18562714755535126, 'train/mask_dice_loss': 0.4502467706799507, 'train/mask_loss': 0.6358739137649536, 'metrics/total_secs_per_batch': 5.823434591293335, 'metrics/data_secs_per_batch': 2.840689992904663, '_timestamp': 1740957852.0256095}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 92 is less than current step: 499. Dropping entry: {'train/lr': 0.00017253061224489796, '_timestamp': 1740957852.0259073}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 93 is less than current step: 499. Dropping entry: {'train/loss': 1.9806771516799926, 'train/ce_loss': 0.3353515625, 'train/seg_cls_loss': 0.01619873046875, 'train/kl_loss': 0.3044921875, 'train/mask_bce_loss': 0.16956115132197738, 'train/mask_dice_loss': 0.633912181854248, 'train/mask_loss': 0.8034733295440674, 'metrics/total_secs_per_batch': 6.7534637451171875, 'metrics/data_secs_per_batch': 3.032844305038452, '_timestamp': 1740957858.7790637}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 93 is less than current step: 499. Dropping entry: {'train/lr': 0.00017240816326530612, '_timestamp': 1740957858.779436}).
Epoch: [2][ 95/500]	Time  5.408 ( 5.408)	Loss 0.2852 (1.4120)	CeLoss 0.2852 (0.3277)	SegCLSLoss 0.0000 (0.0138)	KLLoss 0.0000 (0.2711)	MaskLoss 0.0000 (0.5252)	MaskBCELoss 0.0000 (0.1235)	MaskDICELoss 0.0000 (0.4017)
Epoch: [2][ 96/500]	Time  5.746 ( 5.746)	Loss 2.0940 (1.8553)	CeLoss 0.2969 (0.5386)	SegCLSLoss 0.0275 (0.0160)	KLLoss 0.4043 (0.2670)	MaskLoss 0.8712 (0.6410)	MaskBCELoss 0.1000 (0.0912)	MaskDICELoss 0.7712 (0.5498)
Epoch: [2][ 97/500]	Time  6.315 ( 6.315)	Loss 1.4476 (1.6769)	CeLoss 0.2139 (0.2962)	SegCLSLoss 0.0172 (0.0161)	KLLoss 0.3867 (0.3080)	MaskLoss 0.5929 (0.6708)	MaskBCELoss 0.0361 (0.1299)	MaskDICELoss 0.5569 (0.5409)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 94 is less than current step: 499. Dropping entry: {'train/loss': 1.4120327472686767, 'train/ce_loss': 0.327734375, 'train/seg_cls_loss': 0.01376953125, 'train/kl_loss': 0.27109375, 'train/mask_bce_loss': 0.1235191810876131, 'train/mask_dice_loss': 0.4016866356134415, 'train/mask_loss': 0.5252058267593384, 'metrics/total_secs_per_batch': 5.4080915451049805, 'metrics/data_secs_per_batch': 2.0860378742218018, '_timestamp': 1740957864.187379}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 94 is less than current step: 499. Dropping entry: {'train/lr': 0.00017228571428571428, '_timestamp': 1740957864.1877224}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 95 is less than current step: 499. Dropping entry: {'train/loss': 1.8553228378295898, 'train/ce_loss': 0.53857421875, 'train/seg_cls_loss': 0.01595458984375, 'train/kl_loss': 0.2669921875, 'train/mask_bce_loss': 0.09120307769626379, 'train/mask_dice_loss': 0.5497883886098862, 'train/mask_loss': 0.640991473197937, 'metrics/total_secs_per_batch': 5.745929479598999, 'metrics/data_secs_per_batch': 2.3068445920944214, '_timestamp': 1740957869.933125}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 95 is less than current step: 499. Dropping entry: {'train/lr': 0.00017216326530612242, '_timestamp': 1740957869.9334743}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 96 is less than current step: 499. Dropping entry: {'train/loss': 1.6768654704093933, 'train/ce_loss': 0.29619140625, 'train/seg_cls_loss': 0.01610107421875, 'train/kl_loss': 0.3080078125, 'train/mask_bce_loss': 0.12986616604030132, 'train/mask_dice_loss': 0.5408908009529114, 'train/mask_loss': 0.6707569658756256, 'metrics/total_secs_per_batch': 6.315197706222534, 'metrics/data_secs_per_batch': 2.7730796098709107, '_timestamp': 1740957876.2483032}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 96 is less than current step: 499. Dropping entry: {'train/lr': 0.00017204081632653058, '_timestamp': 1740957876.2485716}).
Epoch: [2][ 98/500]	Time  5.976 ( 5.976)	Loss 1.6646 (1.7444)	CeLoss 0.2119 (0.5988)	SegCLSLoss 0.0184 (0.0134)	KLLoss 0.3848 (0.2314)	MaskLoss 0.7024 (0.5578)	MaskBCELoss 0.0187 (0.1041)	MaskDICELoss 0.6837 (0.4538)
Epoch: [2][ 99/500]	Time  5.991 ( 5.991)	Loss 1.2812 (1.7153)	CeLoss 1.2812 (0.6562)	SegCLSLoss 0.0000 (0.0132)	KLLoss 0.0000 (0.2264)	MaskLoss 0.0000 (0.5150)	MaskBCELoss 0.0000 (0.1155)	MaskDICELoss 0.0000 (0.3994)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 97 is less than current step: 499. Dropping entry: {'train/loss': 1.7444072723388673, 'train/ce_loss': 0.598828125, 'train/seg_cls_loss': 0.01337890625, 'train/kl_loss': 0.2314453125, 'train/mask_bce_loss': 0.10408756462857127, 'train/mask_dice_loss': 0.4537605971097946, 'train/mask_loss': 0.5578481614589691, 'metrics/total_secs_per_batch': 5.9764416217803955, 'metrics/data_secs_per_batch': 2.7590057373046877, '_timestamp': 1740957882.2247787}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 97 is less than current step: 499. Dropping entry: {'train/lr': 0.00017191836734693875, '_timestamp': 1740957882.2250698}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 98 is less than current step: 499. Dropping entry: {'train/loss': 1.7152527451515198, 'train/ce_loss': 0.65615234375, 'train/seg_cls_loss': 0.01317138671875, 'train/kl_loss': 0.2263671875, 'train/mask_bce_loss': 0.11550213992595673, 'train/mask_dice_loss': 0.39944845661520956, 'train/mask_loss': 0.5149505913257599, 'metrics/total_secs_per_batch': 5.990820646286011, 'metrics/data_secs_per_batch': 2.556955075263977, '_timestamp': 1740957888.2155774}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 98 is less than current step: 499. Dropping entry: {'train/lr': 0.0001717959183673469, '_timestamp': 1740957888.2158358}).
[2025-03-02 17:24:53,199] [INFO] [logging.py:96:log_dist] [Rank 0] step=1100, skipped=0, lr=[0.000171734693877551], mom=[(0.9, 0.95)]
[2025-03-02 17:24:53,205] [INFO] [timer.py:215:stop] epoch=0/micro_step=11000/global_step=1100, RunningAvgSamplesPerSec=1.5358699292532205, CurrSamplesPerSec=2.0045422483531947, MemAllocated=31.24GB, MaxMemAllocated=37.19GB
Epoch: [2][100/500]	Time  4.990 ( 4.990)	Loss 1.9125 (1.9021)	CeLoss 0.2773 (0.6357)	SegCLSLoss 0.0145 (0.0160)	KLLoss 0.3867 (0.2250)	MaskLoss 0.7942 (0.6179)	MaskBCELoss 0.2512 (0.1055)	MaskDICELoss 0.5430 (0.5123)
Epoch: [2][101/500]	Time  8.118 ( 8.118)	Loss 2.4195 (2.2392)	CeLoss 0.1924 (0.2748)	SegCLSLoss 0.0317 (0.0244)	KLLoss 0.3848 (0.3455)	MaskLoss 1.0867 (0.9589)	MaskBCELoss 0.1747 (0.1618)	MaskDICELoss 0.9119 (0.7971)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 99 is less than current step: 499. Dropping entry: {'train/loss': 1.9021175622940063, 'train/ce_loss': 0.6357421875, 'train/seg_cls_loss': 0.015966796875, 'train/kl_loss': 0.225, 'train/mask_bce_loss': 0.10552631043829024, 'train/mask_dice_loss': 0.5123293459415436, 'train/mask_loss': 0.6178556501865387, 'metrics/total_secs_per_batch': 4.99023699760437, 'metrics/data_secs_per_batch': 2.2001220703125, '_timestamp': 1740957893.2056208}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 99 is less than current step: 499. Dropping entry: {'train/lr': 0.00017167346938775507, '_timestamp': 1740957893.2059183}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 100 is less than current step: 499. Dropping entry: {'train/loss': 2.239188480377197, 'train/ce_loss': 0.2748046875, 'train/seg_cls_loss': 0.024383544921875, 'train/kl_loss': 0.3455078125, 'train/mask_bce_loss': 0.16178344385698437, 'train/mask_dice_loss': 0.7970686018466949, 'train/mask_loss': 0.9588520348072052, 'metrics/total_secs_per_batch': 8.118269443511963, 'metrics/data_secs_per_batch': 3.5319748878479005, '_timestamp': 1740957901.3241987}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 100 is less than current step: 499. Dropping entry: {'train/lr': 0.00017155102040816324, '_timestamp': 1740957901.3245265}).
Epoch: [2][102/500]	Time  7.811 ( 7.811)	Loss 0.1826 (1.5389)	CeLoss 0.1826 (0.3577)	SegCLSLoss 0.0000 (0.0151)	KLLoss 0.0000 (0.2648)	MaskLoss 0.0000 (0.5737)	MaskBCELoss 0.0000 (0.0634)	MaskDICELoss 0.0000 (0.5103)
Epoch: [2][103/500]	Time  7.853 ( 7.853)	Loss 1.7687 (1.9089)	CeLoss 0.1787 (0.4778)	SegCLSLoss 0.0211 (0.0166)	KLLoss 0.3848 (0.3098)	MaskLoss 0.7701 (0.6959)	MaskBCELoss 0.0527 (0.1961)	MaskDICELoss 0.7174 (0.4997)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 101 is less than current step: 499. Dropping entry: {'train/loss': 1.538899576663971, 'train/ce_loss': 0.357666015625, 'train/seg_cls_loss': 0.01510009765625, 'train/kl_loss': 0.26484375, 'train/mask_bce_loss': 0.0634171076118946, 'train/mask_dice_loss': 0.5102563023567199, 'train/mask_loss': 0.5736734211444855, 'metrics/total_secs_per_batch': 7.8112592697143555, 'metrics/data_secs_per_batch': 3.2959887027740478, '_timestamp': 1740957909.1353605}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 101 is less than current step: 499. Dropping entry: {'train/lr': 0.0001714285714285714, '_timestamp': 1740957909.135581}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 102 is less than current step: 499. Dropping entry: {'train/loss': 1.9089253067970275, 'train/ce_loss': 0.47783203125, 'train/seg_cls_loss': 0.016644287109375, 'train/kl_loss': 0.309765625, 'train/mask_bce_loss': 0.1961360439658165, 'train/mask_dice_loss': 0.49973285794258115, 'train/mask_loss': 0.6958688974380494, 'metrics/total_secs_per_batch': 7.8530659675598145, 'metrics/data_secs_per_batch': 3.5435415506362915, '_timestamp': 1740957916.9884055}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 102 is less than current step: 499. Dropping entry: {'train/lr': 0.00017130612244897956, '_timestamp': 1740957916.9886725}).
Epoch: [2][104/500]	Time  8.994 ( 8.994)	Loss 1.9074 (1.7388)	CeLoss 0.2139 (0.3458)	SegCLSLoss 0.0254 (0.0156)	KLLoss 0.3691 (0.3082)	MaskLoss 0.8218 (0.6774)	MaskBCELoss 0.2501 (0.1377)	MaskDICELoss 0.5717 (0.5397)
Epoch: [2][105/500]	Time  9.236 ( 9.236)	Loss 1.7415 (1.8284)	CeLoss 0.2031 (0.4860)	SegCLSLoss 0.0176 (0.0162)	KLLoss 0.3867 (0.3107)	MaskLoss 0.7458 (0.6516)	MaskBCELoss 0.1579 (0.0795)	MaskDICELoss 0.5879 (0.5721)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 103 is less than current step: 499. Dropping entry: {'train/loss': 1.7388448476791383, 'train/ce_loss': 0.345751953125, 'train/seg_cls_loss': 0.015594482421875, 'train/kl_loss': 0.308203125, 'train/mask_bce_loss': 0.1376578646712005, 'train/mask_dice_loss': 0.539699125289917, 'train/mask_loss': 0.6773569881916046, 'metrics/total_secs_per_batch': 8.994330883026123, 'metrics/data_secs_per_batch': 3.653810405731201, '_timestamp': 1740957925.9828672}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 103 is less than current step: 499. Dropping entry: {'train/lr': 0.00017118367346938772, '_timestamp': 1740957925.983094}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 104 is less than current step: 499. Dropping entry: {'train/loss': 1.8283978819847106, 'train/ce_loss': 0.48603515625, 'train/seg_cls_loss': 0.016162109375, 'train/kl_loss': 0.3107421875, 'train/mask_bce_loss': 0.07948856428265572, 'train/mask_dice_loss': 0.5721127033233643, 'train/mask_loss': 0.6516012579202652, 'metrics/total_secs_per_batch': 9.23646092414856, 'metrics/data_secs_per_batch': 4.388827919960022, '_timestamp': 1740957935.2191904}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 104 is less than current step: 499. Dropping entry: {'train/lr': 0.0001710612244897959, '_timestamp': 1740957935.2194614}).
Epoch: [2][106/500]	Time  5.134 ( 5.134)	Loss 1.9828 (1.6533)	CeLoss 0.3770 (0.5250)	SegCLSLoss 0.0300 (0.0120)	KLLoss 0.3770 (0.2293)	MaskLoss 0.7766 (0.5498)	MaskBCELoss 0.2771 (0.0692)	MaskDICELoss 0.4994 (0.4806)
Epoch: [2][107/500]	Time  6.609 ( 6.609)	Loss 1.0000 (1.4899)	CeLoss 1.0000 (0.5917)	SegCLSLoss 0.0000 (0.0091)	KLLoss 0.0000 (0.1957)	MaskLoss 0.0000 (0.4371)	MaskBCELoss 0.0000 (0.0461)	MaskDICELoss 0.0000 (0.3910)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 105 is less than current step: 499. Dropping entry: {'train/loss': 1.6533160924911499, 'train/ce_loss': 0.525, 'train/seg_cls_loss': 0.011968994140625, 'train/kl_loss': 0.229296875, 'train/mask_bce_loss': 0.06921120788902044, 'train/mask_dice_loss': 0.4805913656949997, 'train/mask_loss': 0.549802565574646, 'metrics/total_secs_per_batch': 5.133603096008301, 'metrics/data_secs_per_batch': 2.2176334142684935, '_timestamp': 1740957940.3528237}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 105 is less than current step: 499. Dropping entry: {'train/lr': 0.00017093877551020405, '_timestamp': 1740957940.353175}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 106 is less than current step: 499. Dropping entry: {'train/loss': 1.4898932218551635, 'train/ce_loss': 0.591650390625, 'train/seg_cls_loss': 0.009136962890625, 'train/kl_loss': 0.195703125, 'train/mask_bce_loss': 0.04609641502611339, 'train/mask_dice_loss': 0.3909644514322281, 'train/mask_loss': 0.43706085681915285, 'metrics/total_secs_per_batch': 6.609186172485352, 'metrics/data_secs_per_batch': 3.74361310005188, '_timestamp': 1740957946.9622126}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 106 is less than current step: 499. Dropping entry: {'train/lr': 0.00017081632653061224, '_timestamp': 1740957946.9625716}).
Epoch: [2][108/500]	Time  7.135 ( 7.135)	Loss 2.5650 (1.8598)	CeLoss 0.1670 (0.2250)	SegCLSLoss 0.0240 (0.0162)	KLLoss 0.3867 (0.3523)	MaskLoss 1.1741 (0.7957)	MaskBCELoss 0.2873 (0.1859)	MaskDICELoss 0.8868 (0.6098)
Epoch: [2][109/500]	Time  6.482 ( 6.482)	Loss 2.4858 (1.7375)	CeLoss 0.4043 (0.3243)	SegCLSLoss 0.0134 (0.0154)	KLLoss 0.3867 (0.3535)	MaskLoss 1.0183 (0.6853)	MaskBCELoss 0.0189 (0.1597)	MaskDICELoss 0.9994 (0.5256)
[2025-03-02 17:26:05,981] [INFO] [logging.py:96:log_dist] [Rank 0] step=1110, skipped=0, lr=[0.00017051020408163266], mom=[(0.9, 0.95)]
[2025-03-02 17:26:05,987] [INFO] [timer.py:215:stop] epoch=0/micro_step=11100/global_step=1110, RunningAvgSamplesPerSec=1.5342409098916387, CurrSamplesPerSec=1.8490839784342075, MemAllocated=31.26GB, MaxMemAllocated=37.19GB
Epoch: [2][110/500]	Time  5.410 ( 5.410)	Loss 2.3281 (1.5478)	CeLoss 0.2129 (0.5264)	SegCLSLoss 0.0242 (0.0103)	KLLoss 0.3730 (0.1895)	MaskLoss 1.0332 (0.4987)	MaskBCELoss 0.0546 (0.0952)	MaskDICELoss 0.9786 (0.4034)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 107 is less than current step: 499. Dropping entry: {'train/loss': 1.8598012924194336, 'train/ce_loss': 0.225, 'train/seg_cls_loss': 0.01617431640625, 'train/kl_loss': 0.35234375, 'train/mask_bce_loss': 0.18586719501763582, 'train/mask_dice_loss': 0.6098049312829972, 'train/mask_loss': 0.7956721246242523, 'metrics/total_secs_per_batch': 7.134841442108154, 'metrics/data_secs_per_batch': 3.1365317344665526, '_timestamp': 1740957954.0969436}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 107 is less than current step: 499. Dropping entry: {'train/lr': 0.0001706938775510204, '_timestamp': 1740957954.0972922}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 108 is less than current step: 499. Dropping entry: {'train/loss': 1.7374948859214783, 'train/ce_loss': 0.32431640625, 'train/seg_cls_loss': 0.015362548828125, 'train/kl_loss': 0.353515625, 'train/mask_bce_loss': 0.1596775773912668, 'train/mask_dice_loss': 0.5255737826228142, 'train/mask_loss': 0.6852513551712036, 'metrics/total_secs_per_batch': 6.481562614440918, 'metrics/data_secs_per_batch': 2.421428847312927, '_timestamp': 1740957960.5784144}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 108 is less than current step: 499. Dropping entry: {'train/lr': 0.00017057142857142857, '_timestamp': 1740957960.578701}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 109 is less than current step: 499. Dropping entry: {'train/loss': 1.5478209733963013, 'train/ce_loss': 0.5264404296875, 'train/seg_cls_loss': 0.01033935546875, 'train/kl_loss': 0.189453125, 'train/mask_bce_loss': 0.09524358622729778, 'train/mask_dice_loss': 0.40343496203422546, 'train/mask_loss': 0.49867855906486513, 'metrics/total_secs_per_batch': 5.4096338748931885, 'metrics/data_secs_per_batch': 2.212959122657776, '_timestamp': 1740957965.98787}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 109 is less than current step: 499. Dropping entry: {'train/lr': 0.00017044897959183673, '_timestamp': 1740957965.9881437}).
Epoch: [2][111/500]	Time  6.687 ( 6.687)	Loss 2.0352 (2.0040)	CeLoss 0.3047 (0.3664)	SegCLSLoss 0.0112 (0.0180)	KLLoss 0.3887 (0.3445)	MaskLoss 0.8428 (0.7971)	MaskBCELoss 0.2239 (0.1820)	MaskDICELoss 0.6188 (0.6151)
Epoch: [2][112/500]	Time  6.127 ( 6.127)	Loss 1.5534 (1.6842)	CeLoss 0.2598 (0.3735)	SegCLSLoss 0.0137 (0.0135)	KLLoss 0.3965 (0.3090)	MaskLoss 0.6244 (0.6366)	MaskBCELoss 0.0278 (0.1668)	MaskDICELoss 0.5965 (0.4698)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 110 is less than current step: 499. Dropping entry: {'train/loss': 2.0040170192718505, 'train/ce_loss': 0.36640625, 'train/seg_cls_loss': 0.018023681640625, 'train/kl_loss': 0.34453125, 'train/mask_bce_loss': 0.18197726691141725, 'train/mask_dice_loss': 0.6151484340429306, 'train/mask_loss': 0.797125694155693, 'metrics/total_secs_per_batch': 6.687196969985962, 'metrics/data_secs_per_batch': 2.8280425786972048, '_timestamp': 1740957972.675501}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 110 is less than current step: 499. Dropping entry: {'train/lr': 0.0001703265306122449, '_timestamp': 1740957972.6758685}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 111 is less than current step: 499. Dropping entry: {'train/loss': 1.684157705307007, 'train/ce_loss': 0.37353515625, 'train/seg_cls_loss': 0.0134765625, 'train/kl_loss': 0.308984375, 'train/mask_bce_loss': 0.16677272506058216, 'train/mask_dice_loss': 0.46983736753463745, 'train/mask_loss': 0.6366100966930389, 'metrics/total_secs_per_batch': 6.1272406578063965, 'metrics/data_secs_per_batch': 2.695456600189209, '_timestamp': 1740957978.802501}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 111 is less than current step: 499. Dropping entry: {'train/lr': 0.00017020408163265306, '_timestamp': 1740957978.802784}).
Epoch: [2][113/500]	Time  4.351 ( 4.351)	Loss 1.3203 (1.6638)	CeLoss 1.3203 (0.6556)	SegCLSLoss 0.0000 (0.0103)	KLLoss 0.0000 (0.1967)	MaskLoss 0.0000 (0.4917)	MaskBCELoss 0.0000 (0.1114)	MaskDICELoss 0.0000 (0.3803)
Epoch: [2][114/500]	Time  4.950 ( 4.950)	Loss 0.7773 (1.5366)	CeLoss 0.7773 (0.6744)	SegCLSLoss 0.0000 (0.0129)	KLLoss 0.0000 (0.1896)	MaskLoss 0.0000 (0.4184)	MaskBCELoss 0.0000 (0.0973)	MaskDICELoss 0.0000 (0.3211)
Epoch: [2][115/500]	Time  6.096 ( 6.096)	Loss 1.8988 (1.4693)	CeLoss 0.2266 (0.2507)	SegCLSLoss 0.0118 (0.0151)	KLLoss 0.4004 (0.3561)	MaskLoss 0.8127 (0.5878)	MaskBCELoss 0.2175 (0.0996)	MaskDICELoss 0.5951 (0.4882)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 112 is less than current step: 499. Dropping entry: {'train/loss': 1.663830542564392, 'train/ce_loss': 0.65556640625, 'train/seg_cls_loss': 0.0102783203125, 'train/kl_loss': 0.1966796875, 'train/mask_bce_loss': 0.11141461990773678, 'train/mask_dice_loss': 0.38031511306762694, 'train/mask_loss': 0.49172973036766054, 'metrics/total_secs_per_batch': 4.350693225860596, 'metrics/data_secs_per_batch': 2.1216518878936768, '_timestamp': 1740957983.1531339}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 112 is less than current step: 499. Dropping entry: {'train/lr': 0.00017008163265306122, '_timestamp': 1740957983.1534007}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 113 is less than current step: 499. Dropping entry: {'train/loss': 1.536619883775711, 'train/ce_loss': 0.6744140625, 'train/seg_cls_loss': 0.01287841796875, 'train/kl_loss': 0.1896484375, 'train/mask_bce_loss': 0.09726158827543259, 'train/mask_dice_loss': 0.3211459994316101, 'train/mask_loss': 0.41840759813785555, 'metrics/total_secs_per_batch': 4.950072288513184, 'metrics/data_secs_per_batch': 2.2397732496261598, '_timestamp': 1740957988.1032448}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 113 is less than current step: 499. Dropping entry: {'train/lr': 0.00016995918367346938, '_timestamp': 1740957988.1034336}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 114 is less than current step: 499. Dropping entry: {'train/loss': 1.4693019330501556, 'train/ce_loss': 0.25068359375, 'train/seg_cls_loss': 0.015142822265625, 'train/kl_loss': 0.3560546875, 'train/mask_bce_loss': 0.09956291206181049, 'train/mask_dice_loss': 0.4882130533456802, 'train/mask_loss': 0.5877759605646133, 'metrics/total_secs_per_batch': 6.0958781242370605, 'metrics/data_secs_per_batch': 2.679156184196472, '_timestamp': 1740957994.199171}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 114 is less than current step: 499. Dropping entry: {'train/lr': 0.00016983673469387754, '_timestamp': 1740957994.1994615}).
Epoch: [2][116/500]	Time  6.521 ( 6.521)	Loss 2.2334 (1.4932)	CeLoss 0.2480 (0.3669)	SegCLSLoss 0.0228 (0.0118)	KLLoss 0.3828 (0.2295)	MaskLoss 0.9673 (0.5487)	MaskBCELoss 0.0845 (0.1194)	MaskDICELoss 0.8828 (0.4293)
Epoch: [2][117/500]	Time  5.891 ( 5.891)	Loss 1.9901 (1.9273)	CeLoss 0.2021 (0.4203)	SegCLSLoss 0.0309 (0.0170)	KLLoss 0.3809 (0.3055)	MaskLoss 0.8671 (0.7340)	MaskBCELoss 0.0345 (0.1603)	MaskDICELoss 0.8327 (0.5737)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 115 is less than current step: 499. Dropping entry: {'train/loss': 1.493224859237671, 'train/ce_loss': 0.36689453125, 'train/seg_cls_loss': 0.01182861328125, 'train/kl_loss': 0.2294921875, 'train/mask_bce_loss': 0.11939449273049832, 'train/mask_dice_loss': 0.4293175294995308, 'train/mask_loss': 0.5487120300531387, 'metrics/total_secs_per_batch': 6.5206804275512695, 'metrics/data_secs_per_batch': 2.6683040142059324, '_timestamp': 1740958000.71991}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 115 is less than current step: 499. Dropping entry: {'train/lr': 0.0001697142857142857, '_timestamp': 1740958000.7202883}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 116 is less than current step: 499. Dropping entry: {'train/loss': 1.9273232966661453, 'train/ce_loss': 0.4203125, 'train/seg_cls_loss': 0.0169921875, 'train/kl_loss': 0.30546875, 'train/mask_bce_loss': 0.16028815172612668, 'train/mask_dice_loss': 0.5737348079681397, 'train/mask_loss': 0.7340229585766792, 'metrics/total_secs_per_batch': 5.891473293304443, 'metrics/data_secs_per_batch': 2.264361095428467, '_timestamp': 1740958006.6113482}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 116 is less than current step: 499. Dropping entry: {'train/lr': 0.00016959183673469387, '_timestamp': 1740958006.6116457}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 117 is less than current step: 499. Dropping entry: {'train/loss': 1.7854567527770997, 'train/ce_loss': 0.5181640625, 'train/seg_cls_loss': 0.014990234375, 'train/kl_loss': 0.2677734375, 'train/mask_bce_loss': 0.10283774603158236, 'train/mask_dice_loss': 0.5136210918426514, 'train/mask_loss': 0.6164588451385498, 'metrics/total_secs_per_batch': 5.607188701629639, 'metrics/data_secs_per_batch': 2.349191975593567, '_timestamp': 1740958012.2186623}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 117 is less than current step: 499. Dropping entry: {'train/lr': 0.00016946938775510203, '_timestamp': 1740958012.219065}).
Epoch: [2][118/500]	Time  5.607 ( 5.607)	Loss 2.2127 (1.7855)	CeLoss 0.2754 (0.5182)	SegCLSLoss 0.0147 (0.0150)	KLLoss 0.3887 (0.2678)	MaskLoss 0.9452 (0.6165)	MaskBCELoss 0.1666 (0.1028)	MaskDICELoss 0.7787 (0.5136)
Epoch: [2][119/500]	Time  5.619 ( 5.619)	Loss 0.9961 (1.7506)	CeLoss 0.9961 (0.5130)	SegCLSLoss 0.0000 (0.0170)	KLLoss 0.0000 (0.2646)	MaskLoss 0.0000 (0.6013)	MaskBCELoss 0.0000 (0.1668)	MaskDICELoss 0.0000 (0.4345)
[2025-03-02 17:27:04,026] [INFO] [logging.py:96:log_dist] [Rank 0] step=1120, skipped=0, lr=[0.00016928571428571427], mom=[(0.9, 0.95)]
[2025-03-02 17:27:04,032] [INFO] [timer.py:215:stop] epoch=0/micro_step=11200/global_step=1120, RunningAvgSamplesPerSec=1.5357471044803914, CurrSamplesPerSec=1.6146688461447993, MemAllocated=31.25GB, MaxMemAllocated=37.19GB
Epoch: [2][120/500]	Time  6.195 ( 6.195)	Loss 2.1491 (2.1035)	CeLoss 0.2061 (0.2225)	SegCLSLoss 0.0212 (0.0224)	KLLoss 0.3730 (0.3801)	MaskLoss 0.9476 (0.9159)	MaskBCELoss 0.0248 (0.1999)	MaskDICELoss 0.9228 (0.7160)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 118 is less than current step: 499. Dropping entry: {'train/loss': 1.7505659699440002, 'train/ce_loss': 0.51298828125, 'train/seg_cls_loss': 0.01702880859375, 'train/kl_loss': 0.2646484375, 'train/mask_bce_loss': 0.16681231977418065, 'train/mask_dice_loss': 0.43449605703353883, 'train/mask_loss': 0.6013083726167678, 'metrics/total_secs_per_batch': 5.619382619857788, 'metrics/data_secs_per_batch': 2.8082895994186403, '_timestamp': 1740958017.8379154}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 118 is less than current step: 499. Dropping entry: {'train/lr': 0.0001693469387755102, '_timestamp': 1740958017.8381946}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 119 is less than current step: 499. Dropping entry: {'train/loss': 2.1035434007644653, 'train/ce_loss': 0.2224609375, 'train/seg_cls_loss': 0.022442626953125, 'train/kl_loss': 0.380078125, 'train/mask_bce_loss': 0.1998885152861476, 'train/mask_dice_loss': 0.716043335199356, 'train/mask_loss': 0.9159318536520005, 'metrics/total_secs_per_batch': 6.1948018074035645, 'metrics/data_secs_per_batch': 2.7076388359069825, '_timestamp': 1740958024.0325048}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 119 is less than current step: 499. Dropping entry: {'train/lr': 0.00016922448979591836, '_timestamp': 1740958024.0327938}).
Epoch: [2][121/500]	Time  6.847 ( 6.847)	Loss 1.8623 (1.8371)	CeLoss 0.1943 (0.3886)	SegCLSLoss 0.0371 (0.0193)	KLLoss 0.3887 (0.3072)	MaskLoss 0.8052 (0.7040)	MaskBCELoss 0.1600 (0.1006)	MaskDICELoss 0.6451 (0.6034)
Epoch: [2][122/500]	Time  9.627 ( 9.627)	Loss 2.4625 (1.7320)	CeLoss 0.1641 (0.5353)	SegCLSLoss 0.0280 (0.0130)	KLLoss 0.3809 (0.2674)	MaskLoss 1.1233 (0.5818)	MaskBCELoss 0.2369 (0.1192)	MaskDICELoss 0.8865 (0.4626)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 120 is less than current step: 499. Dropping entry: {'train/loss': 1.8370594263076783, 'train/ce_loss': 0.38857421875, 'train/seg_cls_loss': 0.01932373046875, 'train/kl_loss': 0.3072265625, 'train/mask_bce_loss': 0.10063193095847964, 'train/mask_dice_loss': 0.6033958107233047, 'train/mask_loss': 0.7040277332067489, 'metrics/total_secs_per_batch': 6.847414493560791, 'metrics/data_secs_per_batch': 3.0633419036865233, '_timestamp': 1740958030.8801255}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 120 is less than current step: 499. Dropping entry: {'train/lr': 0.00016910204081632652, '_timestamp': 1740958030.8804271}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 121 is less than current step: 499. Dropping entry: {'train/loss': 1.7319902300834655, 'train/ce_loss': 0.53525390625, 'train/seg_cls_loss': 0.0129638671875, 'train/kl_loss': 0.2673828125, 'train/mask_bce_loss': 0.11917286850512028, 'train/mask_dice_loss': 0.4626425564289093, 'train/mask_loss': 0.5818154215812683, 'metrics/total_secs_per_batch': 9.62731122970581, 'metrics/data_secs_per_batch': 3.815874457359314, '_timestamp': 1740958040.5074208}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 121 is less than current step: 499. Dropping entry: {'train/lr': 0.00016897959183673469, '_timestamp': 1740958040.5077803}).
Epoch: [2][123/500]	Time  5.156 ( 5.156)	Loss 2.0126 (1.6439)	CeLoss 0.2773 (0.6344)	SegCLSLoss 0.0148 (0.0107)	KLLoss 0.3848 (0.2307)	MaskLoss 0.8442 (0.4906)	MaskBCELoss 0.0734 (0.1027)	MaskDICELoss 0.7708 (0.3879)
Epoch: [2][124/500]	Time  5.629 ( 5.629)	Loss 1.4375 (1.7834)	CeLoss 1.4375 (0.5393)	SegCLSLoss 0.0000 (0.0129)	KLLoss 0.0000 (0.2684)	MaskLoss 0.0000 (0.6054)	MaskBCELoss 0.0000 (0.0845)	MaskDICELoss 0.0000 (0.5209)
Epoch: [2][125/500]	Time  5.483 ( 5.483)	Loss 2.6394 (1.3571)	CeLoss 0.4062 (0.5395)	SegCLSLoss 0.0172 (0.0102)	KLLoss 0.3652 (0.2311)	MaskLoss 1.0941 (0.3948)	MaskBCELoss 0.3569 (0.0897)	MaskDICELoss 0.7372 (0.3051)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 122 is less than current step: 499. Dropping entry: {'train/loss': 1.6438891887664795, 'train/ce_loss': 0.634423828125, 'train/seg_cls_loss': 0.01070556640625, 'train/kl_loss': 0.2306640625, 'train/mask_bce_loss': 0.1026782963424921, 'train/mask_dice_loss': 0.3878942310810089, 'train/mask_loss': 0.49057252407073976, 'metrics/total_secs_per_batch': 5.156059980392456, 'metrics/data_secs_per_batch': 2.446829056739807, '_timestamp': 1740958045.6635275}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 122 is less than current step: 499. Dropping entry: {'train/lr': 0.00016885714285714282, '_timestamp': 1740958045.6638792}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 123 is less than current step: 499. Dropping entry: {'train/loss': 1.7834389448165893, 'train/ce_loss': 0.5392578125, 'train/seg_cls_loss': 0.012921142578125, 'train/kl_loss': 0.268359375, 'train/mask_bce_loss': 0.08445877097547054, 'train/mask_dice_loss': 0.5209325611591339, 'train/mask_loss': 0.6053913354873657, 'metrics/total_secs_per_batch': 5.628607988357544, 'metrics/data_secs_per_batch': 2.4992859840393065, '_timestamp': 1740958051.2920938}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 123 is less than current step: 499. Dropping entry: {'train/lr': 0.00016873469387755099, '_timestamp': 1740958051.292365}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 124 is less than current step: 499. Dropping entry: {'train/loss': 1.3571061134338378, 'train/ce_loss': 0.539453125, 'train/seg_cls_loss': 0.010235595703125, 'train/kl_loss': 0.2310546875, 'train/mask_bce_loss': 0.08965108692646026, 'train/mask_dice_loss': 0.30511289834976196, 'train/mask_loss': 0.39476399421691893, 'metrics/total_secs_per_batch': 5.4831578731536865, 'metrics/data_secs_per_batch': 2.2488287448883058, '_timestamp': 1740958056.775263}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 124 is less than current step: 499. Dropping entry: {'train/lr': 0.00016861224489795915, '_timestamp': 1740958056.7755332}).
Epoch: [2][126/500]	Time  6.381 ( 6.381)	Loss 2.6396 (1.8824)	CeLoss 0.1670 (0.3081)	SegCLSLoss 0.0226 (0.0182)	KLLoss 0.3672 (0.3455)	MaskLoss 1.2124 (0.7653)	MaskBCELoss 0.3352 (0.1532)	MaskDICELoss 0.8772 (0.6121)
Epoch: [2][127/500]	Time  5.220 ( 5.220)	Loss 1.2344 (1.4220)	CeLoss 1.2344 (0.5028)	SegCLSLoss 0.0000 (0.0122)	KLLoss 0.0000 (0.1967)	MaskLoss 0.0000 (0.4468)	MaskBCELoss 0.0000 (0.0802)	MaskDICELoss 0.0000 (0.3666)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 125 is less than current step: 499. Dropping entry: {'train/loss': 1.8823620080947876, 'train/ce_loss': 0.30810546875, 'train/seg_cls_loss': 0.01815185546875, 'train/kl_loss': 0.3455078125, 'train/mask_bce_loss': 0.1531725011765957, 'train/mask_dice_loss': 0.6121295914053917, 'train/mask_loss': 0.7653021037578582, 'metrics/total_secs_per_batch': 6.380548000335693, 'metrics/data_secs_per_batch': 2.5860723495483398, '_timestamp': 1740958063.1557934}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 125 is less than current step: 499. Dropping entry: {'train/lr': 0.0001684897959183673, '_timestamp': 1740958063.1560664}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 126 is less than current step: 499. Dropping entry: {'train/loss': 1.422001349925995, 'train/ce_loss': 0.502783203125, 'train/seg_cls_loss': 0.012152099609375, 'train/kl_loss': 0.1966796875, 'train/mask_bce_loss': 0.08015367425978184, 'train/mask_dice_loss': 0.3666136056184769, 'train/mask_loss': 0.44676727056503296, 'metrics/total_secs_per_batch': 5.219967842102051, 'metrics/data_secs_per_batch': 2.665720582008362, '_timestamp': 1740958068.3757658}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 126 is less than current step: 499. Dropping entry: {'train/lr': 0.00016836734693877547, '_timestamp': 1740958068.3760571}).
Epoch: [2][128/500]	Time  5.639 ( 5.639)	Loss 1.6207 (1.9183)	CeLoss 0.2109 (0.3348)	SegCLSLoss 0.0198 (0.0191)	KLLoss 0.3809 (0.3051)	MaskLoss 0.6815 (0.7718)	MaskBCELoss 0.0149 (0.1455)	MaskDICELoss 0.6666 (0.6262)
Epoch: [2][129/500]	Time  5.384 ( 5.384)	Loss 2.0901 (2.0523)	CeLoss 0.2285 (0.5808)	SegCLSLoss 0.0237 (0.0164)	KLLoss 0.3770 (0.2703)	MaskLoss 0.9064 (0.7183)	MaskBCELoss 0.0131 (0.2298)	MaskDICELoss 0.8933 (0.4885)
[2025-03-02 17:28:05,716] [INFO] [logging.py:96:log_dist] [Rank 0] step=1130, skipped=0, lr=[0.0001680612244897959], mom=[(0.9, 0.95)]
[2025-03-02 17:28:05,721] [INFO] [timer.py:215:stop] epoch=0/micro_step=11300/global_step=1130, RunningAvgSamplesPerSec=1.5364662670951676, CurrSamplesPerSec=1.5818673960178602, MemAllocated=31.24GB, MaxMemAllocated=37.19GB
Epoch: [2][130/500]	Time  6.324 ( 6.324)	Loss 2.1352 (2.0209)	CeLoss 0.2598 (0.4127)	SegCLSLoss 0.0147 (0.0160)	KLLoss 0.3848 (0.3465)	MaskLoss 0.9143 (0.7827)	MaskBCELoss 0.2980 (0.0939)	MaskDICELoss 0.6163 (0.6888)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 127 is less than current step: 499. Dropping entry: {'train/loss': 1.9182512521743775, 'train/ce_loss': 0.334765625, 'train/seg_cls_loss': 0.019110107421875, 'train/kl_loss': 0.305078125, 'train/mask_bce_loss': 0.14553255829960107, 'train/mask_dice_loss': 0.6262395679950714, 'train/mask_loss': 0.7717721343040467, 'metrics/total_secs_per_batch': 5.638724327087402, 'metrics/data_secs_per_batch': 2.1715192556381226, '_timestamp': 1740958074.014471}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 127 is less than current step: 499. Dropping entry: {'train/lr': 0.00016824489795918364, '_timestamp': 1740958074.01474}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 128 is less than current step: 499. Dropping entry: {'train/loss': 2.052347111701965, 'train/ce_loss': 0.58076171875, 'train/seg_cls_loss': 0.0164306640625, 'train/kl_loss': 0.2703125, 'train/mask_bce_loss': 0.22980713844299316, 'train/mask_dice_loss': 0.4885050803422928, 'train/mask_loss': 0.7183122098445892, 'metrics/total_secs_per_batch': 5.3838372230529785, 'metrics/data_secs_per_batch': 2.0260765075683596, '_timestamp': 1740958079.3983884}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 128 is less than current step: 499. Dropping entry: {'train/lr': 0.0001681224489795918, '_timestamp': 1740958079.3986704}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 129 is less than current step: 499. Dropping entry: {'train/loss': 2.0208953738212587, 'train/ce_loss': 0.4126953125, 'train/seg_cls_loss': 0.01600341796875, 'train/kl_loss': 0.346484375, 'train/mask_bce_loss': 0.09387110359966755, 'train/mask_dice_loss': 0.6888422012329102, 'train/mask_loss': 0.7827132999897003, 'metrics/total_secs_per_batch': 6.323552846908569, 'metrics/data_secs_per_batch': 2.776157283782959, '_timestamp': 1740958085.7220533}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 129 is less than current step: 499. Dropping entry: {'train/lr': 0.000168, '_timestamp': 1740958085.722552}).
Epoch: [2][131/500]	Time  5.935 ( 5.935)	Loss 0.8077 (1.7779)	CeLoss 0.2324 (0.4180)	SegCLSLoss 0.0144 (0.0132)	KLLoss 0.3906 (0.3100)	MaskLoss 0.2652 (0.6613)	MaskBCELoss 0.1271 (0.2336)	MaskDICELoss 0.1381 (0.4277)
Epoch: [2][132/500]	Time  6.255 ( 6.255)	Loss 1.4219 (1.7159)	CeLoss 1.4219 (0.5988)	SegCLSLoss 0.0000 (0.0141)	KLLoss 0.0000 (0.2295)	MaskLoss 0.0000 (0.5436)	MaskBCELoss 0.0000 (0.0773)	MaskDICELoss 0.0000 (0.4663)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 130 is less than current step: 499. Dropping entry: {'train/loss': 1.7779490768909454, 'train/ce_loss': 0.41796875, 'train/seg_cls_loss': 0.01317138671875, 'train/kl_loss': 0.3099609375, 'train/mask_bce_loss': 0.23361387252807617, 'train/mask_dice_loss': 0.42772393971681594, 'train/mask_loss': 0.661337810754776, 'metrics/total_secs_per_batch': 5.935171842575073, 'metrics/data_secs_per_batch': 2.7993868350982667, '_timestamp': 1740958091.6571589}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 130 is less than current step: 499. Dropping entry: {'train/lr': 0.00016787755102040815, '_timestamp': 1740958091.6575456}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 131 is less than current step: 499. Dropping entry: {'train/loss': 1.715903067588806, 'train/ce_loss': 0.598779296875, 'train/seg_cls_loss': 0.01405029296875, 'train/kl_loss': 0.2294921875, 'train/mask_bce_loss': 0.07726328633725643, 'train/mask_dice_loss': 0.46633278727531435, 'train/mask_loss': 0.5435960769653321, 'metrics/total_secs_per_batch': 6.255062103271484, 'metrics/data_secs_per_batch': 2.8318507194519045, '_timestamp': 1740958097.9121904}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 131 is less than current step: 499. Dropping entry: {'train/lr': 0.00016775510204081632, '_timestamp': 1740958097.9124882}).
Epoch: [2][133/500]	Time  5.341 ( 5.341)	Loss 1.1484 (1.6215)	CeLoss 1.1484 (0.6457)	SegCLSLoss 0.0000 (0.0104)	KLLoss 0.0000 (0.1902)	MaskLoss 0.0000 (0.4759)	MaskBCELoss 0.0000 (0.0892)	MaskDICELoss 0.0000 (0.3867)
Epoch: [2][134/500]	Time  5.241 ( 5.241)	Loss 1.9237 (1.4478)	CeLoss 0.2354 (0.5630)	SegCLSLoss 0.0135 (0.0090)	KLLoss 0.3848 (0.1898)	MaskLoss 0.8212 (0.4307)	MaskBCELoss 0.0521 (0.0990)	MaskDICELoss 0.7691 (0.3317)
Epoch: [2][135/500]	Time  7.037 ( 7.037)	Loss 2.3545 (1.9128)	CeLoss 0.2539 (0.3124)	SegCLSLoss 0.0179 (0.0189)	KLLoss 0.3652 (0.3352)	MaskLoss 1.0269 (0.7787)	MaskBCELoss 0.0630 (0.1652)	MaskDICELoss 0.9639 (0.6135)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 132 is less than current step: 499. Dropping entry: {'train/loss': 1.6215420603752135, 'train/ce_loss': 0.645703125, 'train/seg_cls_loss': 0.010406494140625, 'train/kl_loss': 0.190234375, 'train/mask_bce_loss': 0.08918647170066833, 'train/mask_dice_loss': 0.38667244613170626, 'train/mask_loss': 0.47585892081260683, 'metrics/total_secs_per_batch': 5.3411030769348145, 'metrics/data_secs_per_batch': 2.356470060348511, '_timestamp': 1740958103.2533185}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 132 is less than current step: 499. Dropping entry: {'train/lr': 0.00016763265306122448, '_timestamp': 1740958103.253591}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 133 is less than current step: 499. Dropping entry: {'train/loss': 1.4478290557861329, 'train/ce_loss': 0.56298828125, 'train/seg_cls_loss': 0.009033203125, 'train/kl_loss': 0.18984375, 'train/mask_bce_loss': 0.0989817064255476, 'train/mask_dice_loss': 0.3316711038351059, 'train/mask_loss': 0.4306528151035309, 'metrics/total_secs_per_batch': 5.24089503288269, 'metrics/data_secs_per_batch': 2.1384757280349733, '_timestamp': 1740958108.4942377}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 133 is less than current step: 499. Dropping entry: {'train/lr': 0.00016751020408163264, '_timestamp': 1740958108.4945621}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 134 is less than current step: 499. Dropping entry: {'train/loss': 1.912793505191803, 'train/ce_loss': 0.31240234375, 'train/seg_cls_loss': 0.01890869140625, 'train/kl_loss': 0.33515625, 'train/mask_bce_loss': 0.1651704916730523, 'train/mask_dice_loss': 0.613540705293417, 'train/mask_loss': 0.7787111878395081, 'metrics/total_secs_per_batch': 7.037020683288574, 'metrics/data_secs_per_batch': 3.0725500106811525, '_timestamp': 1740958115.531403}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 134 is less than current step: 499. Dropping entry: {'train/lr': 0.0001673877551020408, '_timestamp': 1740958115.531752}).
Epoch: [2][136/500]	Time  5.397 ( 5.397)	Loss 1.0703 (1.4544)	CeLoss 1.0703 (0.7094)	SegCLSLoss 0.0000 (0.0103)	KLLoss 0.0000 (0.1525)	MaskLoss 0.0000 (0.3623)	MaskBCELoss 0.0000 (0.0782)	MaskDICELoss 0.0000 (0.2841)
Epoch: [2][137/500]	Time  5.335 ( 5.335)	Loss 1.4898 (1.6078)	CeLoss 0.1963 (0.6390)	SegCLSLoss 0.0386 (0.0109)	KLLoss 0.3906 (0.2332)	MaskLoss 0.6179 (0.4701)	MaskBCELoss 0.2074 (0.0996)	MaskDICELoss 0.4105 (0.3705)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 135 is less than current step: 499. Dropping entry: {'train/loss': 1.4543519377708436, 'train/ce_loss': 0.709375, 'train/seg_cls_loss': 0.01025390625, 'train/kl_loss': 0.1525390625, 'train/mask_bce_loss': 0.07821044363081456, 'train/mask_dice_loss': 0.28407294750213624, 'train/mask_loss': 0.36228339076042176, 'metrics/total_secs_per_batch': 5.3971827030181885, 'metrics/data_secs_per_batch': 2.5121670961380005, '_timestamp': 1740958120.928362}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 135 is less than current step: 499. Dropping entry: {'train/lr': 0.00016726530612244897, '_timestamp': 1740958120.9286299}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 136 is less than current step: 499. Dropping entry: {'train/loss': 1.607781207561493, 'train/ce_loss': 0.63896484375, 'train/seg_cls_loss': 0.01094970703125, 'train/kl_loss': 0.233203125, 'train/mask_bce_loss': 0.09961929135024547, 'train/mask_dice_loss': 0.37048225700855253, 'train/mask_loss': 0.47010154724121095, 'metrics/total_secs_per_batch': 5.335022449493408, 'metrics/data_secs_per_batch': 2.3258253574371337, '_timestamp': 1740958126.2634223}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 136 is less than current step: 499. Dropping entry: {'train/lr': 0.00016714285714285713, '_timestamp': 1740958126.2637804}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 137 is less than current step: 499. Dropping entry: {'train/loss': 1.5929520726203918, 'train/ce_loss': 0.3671875, 'train/seg_cls_loss': 0.0158203125, 'train/kl_loss': 0.347265625, 'train/mask_bce_loss': 0.11735982596874237, 'train/mask_dice_loss': 0.4741357445716858, 'train/mask_loss': 0.5914955735206604, 'metrics/total_secs_per_batch': 6.643570899963379, 'metrics/data_secs_per_batch': 2.797123384475708, '_timestamp': 1740958132.9074438}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 137 is less than current step: 499. Dropping entry: {'train/lr': 0.0001670204081632653, '_timestamp': 1740958132.9079452}).
Epoch: [2][138/500]	Time  6.644 ( 6.644)	Loss 2.0975 (1.5930)	CeLoss 0.2070 (0.3672)	SegCLSLoss 0.0203 (0.0158)	KLLoss 0.3633 (0.3473)	MaskLoss 0.9218 (0.5915)	MaskBCELoss 0.0702 (0.1174)	MaskDICELoss 0.8516 (0.4741)
Epoch: [2][139/500]	Time  6.095 ( 6.095)	Loss 2.6546 (1.8849)	CeLoss 0.1533 (0.2804)	SegCLSLoss 0.0303 (0.0177)	KLLoss 0.3691 (0.3422)	MaskLoss 1.2247 (0.7808)	MaskBCELoss 0.3121 (0.1556)	MaskDICELoss 0.9126 (0.6253)
[2025-03-02 17:29:05,000] [INFO] [logging.py:96:log_dist] [Rank 0] step=1140, skipped=0, lr=[0.00016683673469387753], mom=[(0.9, 0.95)]
[2025-03-02 17:29:05,007] [INFO] [timer.py:215:stop] epoch=0/micro_step=11400/global_step=1140, RunningAvgSamplesPerSec=1.5376730407285106, CurrSamplesPerSec=1.6655021438708484, MemAllocated=30.69GB, MaxMemAllocated=37.19GB
Epoch: [2][140/500]	Time  6.006 ( 6.006)	Loss 0.6094 (1.9828)	CeLoss 0.6094 (0.4696)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.2648)	MaskLoss 0.0000 (0.7398)	MaskBCELoss 0.0000 (0.2016)	MaskDICELoss 0.0000 (0.5382)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 138 is less than current step: 499. Dropping entry: {'train/loss': 1.8849405884742736, 'train/ce_loss': 0.28037109375, 'train/seg_cls_loss': 0.0177490234375, 'train/kl_loss': 0.3421875, 'train/mask_bce_loss': 0.15556627605110407, 'train/mask_dice_loss': 0.6252829253673553, 'train/mask_loss': 0.7808492064476014, 'metrics/total_secs_per_batch': 6.094858884811401, 'metrics/data_secs_per_batch': 2.6989121437072754, '_timestamp': 1740958139.001909}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 138 is less than current step: 499. Dropping entry: {'train/lr': 0.00016689795918367346, '_timestamp': 1740958139.0022044}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 139 is less than current step: 499. Dropping entry: {'train/loss': 1.9827977061271667, 'train/ce_loss': 0.46962890625, 'train/seg_cls_loss': 0.014190673828125, 'train/kl_loss': 0.26484375, 'train/mask_bce_loss': 0.20163757652044295, 'train/mask_dice_loss': 0.5381987720727921, 'train/mask_loss': 0.7398363590240479, 'metrics/total_secs_per_batch': 6.005839824676514, 'metrics/data_secs_per_batch': 2.693427872657776, '_timestamp': 1740958145.007491}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 139 is less than current step: 499. Dropping entry: {'train/lr': 0.00016677551020408162, '_timestamp': 1740958145.0077558}).
Epoch: [2][141/500]	Time  5.615 ( 5.615)	Loss 1.2427 (1.9368)	CeLoss 0.3867 (0.4064)	SegCLSLoss 0.0113 (0.0140)	KLLoss 0.4004 (0.3055)	MaskLoss 0.4046 (0.7464)	MaskBCELoss 0.0871 (0.1965)	MaskDICELoss 0.3175 (0.5499)
Epoch: [2][142/500]	Time  6.358 ( 6.358)	Loss 1.9703 (1.6985)	CeLoss 0.1943 (0.4756)	SegCLSLoss 0.0142 (0.0134)	KLLoss 0.3906 (0.2693)	MaskLoss 0.8651 (0.5946)	MaskBCELoss 0.0449 (0.0766)	MaskDICELoss 0.8202 (0.5180)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 140 is less than current step: 499. Dropping entry: {'train/loss': 1.9368443369865418, 'train/ce_loss': 0.406396484375, 'train/seg_cls_loss': 0.01397705078125, 'train/kl_loss': 0.30546875, 'train/mask_bce_loss': 0.19650446847081185, 'train/mask_dice_loss': 0.5499450474977493, 'train/mask_loss': 0.7464495062828064, 'metrics/total_secs_per_batch': 5.615118741989136, 'metrics/data_secs_per_batch': 2.5648364543914797, '_timestamp': 1740958150.6230276}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 140 is less than current step: 499. Dropping entry: {'train/lr': 0.00016665306122448978, '_timestamp': 1740958150.6234946}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 141 is less than current step: 499. Dropping entry: {'train/loss': 1.6985247373580932, 'train/ce_loss': 0.4755859375, 'train/seg_cls_loss': 0.01337890625, 'train/kl_loss': 0.2693359375, 'train/mask_bce_loss': 0.0765662744641304, 'train/mask_dice_loss': 0.518008577823639, 'train/mask_loss': 0.5945748567581177, 'metrics/total_secs_per_batch': 6.357743740081787, 'metrics/data_secs_per_batch': 2.8775891780853273, '_timestamp': 1740958156.9806821}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 141 is less than current step: 499. Dropping entry: {'train/lr': 0.00016653061224489795, '_timestamp': 1740958156.9809875}).
Epoch: [2][143/500]	Time  6.645 ( 6.645)	Loss 2.2084 (1.8480)	CeLoss 0.3320 (0.2365)	SegCLSLoss 0.0140 (0.0194)	KLLoss 0.3789 (0.3053)	MaskLoss 0.9157 (0.7858)	MaskBCELoss 0.2227 (0.2112)	MaskDICELoss 0.6930 (0.5746)
Epoch: [2][144/500]	Time  5.808 ( 5.808)	Loss 0.4668 (1.5350)	CeLoss 0.4668 (0.5569)	SegCLSLoss 0.0000 (0.0081)	KLLoss 0.0000 (0.1922)	MaskLoss 0.0000 (0.4774)	MaskBCELoss 0.0000 (0.1454)	MaskDICELoss 0.0000 (0.3321)
Epoch: [2][145/500]	Time  6.103 ( 6.103)	Loss 1.8930 (1.9382)	CeLoss 0.1699 (0.2026)	SegCLSLoss 0.0311 (0.0169)	KLLoss 0.3672 (0.3430)	MaskLoss 0.8352 (0.8464)	MaskBCELoss 0.1217 (0.0856)	MaskDICELoss 0.7134 (0.7608)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 142 is less than current step: 499. Dropping entry: {'train/loss': 1.8480144619941712, 'train/ce_loss': 0.2364990234375, 'train/seg_cls_loss': 0.019378662109375, 'train/kl_loss': 0.3052734375, 'train/mask_bce_loss': 0.21121380086988212, 'train/mask_dice_loss': 0.5745732128620148, 'train/mask_loss': 0.7857870042324067, 'metrics/total_secs_per_batch': 6.645272254943848, 'metrics/data_secs_per_batch': 3.0070623636245726, '_timestamp': 1740958163.6257858}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 142 is less than current step: 499. Dropping entry: {'train/lr': 0.0001664081632653061, '_timestamp': 1740958163.6260777}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 143 is less than current step: 499. Dropping entry: {'train/loss': 1.5350012183189392, 'train/ce_loss': 0.55693359375, 'train/seg_cls_loss': 0.008135986328125, 'train/kl_loss': 0.1921875, 'train/mask_bce_loss': 0.1453626722097397, 'train/mask_dice_loss': 0.3320500493049622, 'train/mask_loss': 0.4774127185344696, 'metrics/total_secs_per_batch': 5.80830979347229, 'metrics/data_secs_per_batch': 2.6220511913299562, '_timestamp': 1740958169.4342227}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 143 is less than current step: 499. Dropping entry: {'train/lr': 0.00016628571428571427, '_timestamp': 1740958169.434518}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 144 is less than current step: 499. Dropping entry: {'train/loss': 1.9382357835769652, 'train/ce_loss': 0.20263671875, 'train/seg_cls_loss': 0.016943359375, 'train/kl_loss': 0.34296875, 'train/mask_bce_loss': 0.08556925226002932, 'train/mask_dice_loss': 0.7607947438955307, 'train/mask_loss': 0.8463639795780182, 'metrics/total_secs_per_batch': 6.103219032287598, 'metrics/data_secs_per_batch': 2.7020681858062745, '_timestamp': 1740958175.5376089}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 144 is less than current step: 499. Dropping entry: {'train/lr': 0.00016616326530612244, '_timestamp': 1740958175.5379896}).
Epoch: [2][146/500]	Time  5.304 ( 5.304)	Loss 2.1237 (1.4708)	CeLoss 0.1855 (0.5526)	SegCLSLoss 0.0303 (0.0117)	KLLoss 0.3633 (0.2264)	MaskLoss 0.9432 (0.4447)	MaskBCELoss 0.1541 (0.0906)	MaskDICELoss 0.7891 (0.3541)
Epoch: [2][147/500]	Time  5.188 ( 5.188)	Loss 3.6848 (1.9269)	CeLoss 0.1875 (0.3592)	SegCLSLoss 0.0204 (0.0172)	KLLoss 0.3945 (0.3102)	MaskLoss 1.7242 (0.7641)	MaskBCELoss 1.0630 (0.1834)	MaskDICELoss 0.6612 (0.5807)
Epoch: [2][148/500]	Time  5.383 ( 5.383)	Loss 1.9293 (1.7603)	CeLoss 0.2500 (0.5353)	SegCLSLoss 0.0108 (0.0140)	KLLoss 0.3926 (0.2664)	MaskLoss 0.8172 (0.5957)	MaskBCELoss 0.2435 (0.0621)	MaskDICELoss 0.5737 (0.5335)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 145 is less than current step: 499. Dropping entry: {'train/loss': 1.4708061814308167, 'train/ce_loss': 0.55263671875, 'train/seg_cls_loss': 0.0116943359375, 'train/kl_loss': 0.2263671875, 'train/mask_bce_loss': 0.09064293317496777, 'train/mask_dice_loss': 0.3540863148868084, 'train/mask_loss': 0.44472925662994384, 'metrics/total_secs_per_batch': 5.3039021492004395, 'metrics/data_secs_per_batch': 2.3358574151992797, '_timestamp': 1740958180.841279}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 145 is less than current step: 499. Dropping entry: {'train/lr': 0.0001660408163265306, '_timestamp': 1740958180.8415918}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 146 is less than current step: 499. Dropping entry: {'train/loss': 1.9269169926643372, 'train/ce_loss': 0.3591796875, 'train/seg_cls_loss': 0.0171630859375, 'train/kl_loss': 0.31015625, 'train/mask_bce_loss': 0.1834349835291505, 'train/mask_dice_loss': 0.5806582629680633, 'train/mask_loss': 0.7640932500362396, 'metrics/total_secs_per_batch': 5.187718391418457, 'metrics/data_secs_per_batch': 2.6460211277008057, '_timestamp': 1740958186.0289454}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 146 is less than current step: 499. Dropping entry: {'train/lr': 0.00016591836734693876, '_timestamp': 1740958186.0292122}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 147 is less than current step: 499. Dropping entry: {'train/loss': 1.7602599263191223, 'train/ce_loss': 0.53525390625, 'train/seg_cls_loss': 0.014013671875, 'train/kl_loss': 0.26640625, 'train/mask_bce_loss': 0.06213546982035041, 'train/mask_dice_loss': 0.5335218369960785, 'train/mask_loss': 0.5956573069095612, 'metrics/total_secs_per_batch': 5.38271164894104, 'metrics/data_secs_per_batch': 2.3589706420898438, '_timestamp': 1740958191.4117036}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 147 is less than current step: 499. Dropping entry: {'train/lr': 0.00016579591836734693, '_timestamp': 1740958191.41198}).
Epoch: [2][149/500]	Time  6.399 ( 6.399)	Loss 2.4116 (1.7162)	CeLoss 0.1270 (0.3099)	SegCLSLoss 0.0364 (0.0158)	KLLoss 0.3691 (0.2629)	MaskLoss 1.1145 (0.6861)	MaskBCELoss 0.2411 (0.1414)	MaskDICELoss 0.8734 (0.5447)
[2025-03-02 17:30:02,517] [INFO] [logging.py:96:log_dist] [Rank 0] step=1150, skipped=0, lr=[0.00016561224489795916], mom=[(0.9, 0.95)]
[2025-03-02 17:30:02,523] [INFO] [timer.py:215:stop] epoch=0/micro_step=11500/global_step=1150, RunningAvgSamplesPerSec=1.5392253772790152, CurrSamplesPerSec=2.1222548796180742, MemAllocated=30.71GB, MaxMemAllocated=37.19GB
Epoch: [2][150/500]	Time  4.714 ( 4.714)	Loss 1.5312 (1.7114)	CeLoss 1.5312 (0.7914)	SegCLSLoss 0.0000 (0.0074)	KLLoss 0.0000 (0.1469)	MaskLoss 0.0000 (0.4509)	MaskBCELoss 0.0000 (0.1144)	MaskDICELoss 0.0000 (0.3365)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 148 is less than current step: 499. Dropping entry: {'train/loss': 1.7161644577980042, 'train/ce_loss': 0.30986328125, 'train/seg_cls_loss': 0.01578369140625, 'train/kl_loss': 0.262890625, 'train/mask_bce_loss': 0.14137951135635377, 'train/mask_dice_loss': 0.5447300493717193, 'train/mask_loss': 0.686109584569931, 'metrics/total_secs_per_batch': 6.39918065071106, 'metrics/data_secs_per_batch': 2.8162036180496215, '_timestamp': 1740958197.8108757}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 148 is less than current step: 499. Dropping entry: {'train/lr': 0.0001656734693877551, '_timestamp': 1740958197.8111572}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 149 is less than current step: 499. Dropping entry: {'train/loss': 1.7114423036575317, 'train/ce_loss': 0.79140625, 'train/seg_cls_loss': 0.007354736328125, 'train/kl_loss': 0.146875, 'train/mask_bce_loss': 0.11441305130720139, 'train/mask_dice_loss': 0.3364741086959839, 'train/mask_loss': 0.4508871674537659, 'metrics/total_secs_per_batch': 4.713581800460815, 'metrics/data_secs_per_batch': 2.388699984550476, '_timestamp': 1740958202.5242884}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 149 is less than current step: 499. Dropping entry: {'train/lr': 0.00016555102040816322, '_timestamp': 1740958202.524568}).
Epoch: [2][151/500]	Time  6.070 ( 6.070)	Loss 1.7558 (1.3919)	CeLoss 0.2852 (0.4741)	SegCLSLoss 0.0129 (0.0085)	KLLoss 0.3848 (0.1926)	MaskLoss 0.7128 (0.4472)	MaskBCELoss 0.1338 (0.0786)	MaskDICELoss 0.5790 (0.3686)
Epoch: [2][152/500]	Time  5.776 ( 5.776)	Loss 2.7471 (1.8993)	CeLoss 0.2227 (0.4221)	SegCLSLoss 0.0179 (0.0168)	KLLoss 0.3594 (0.3023)	MaskLoss 1.2398 (0.7193)	MaskBCELoss 0.4731 (0.1393)	MaskDICELoss 0.7666 (0.5800)
Epoch: [2][153/500]	Time  6.454 ( 6.454)	Loss 1.7214 (1.8418)	CeLoss 0.3203 (0.3577)	SegCLSLoss 0.0112 (0.0180)	KLLoss 0.3945 (0.2645)	MaskLoss 0.6781 (0.7243)	MaskBCELoss 0.1139 (0.1317)	MaskDICELoss 0.5642 (0.5926)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 150 is less than current step: 499. Dropping entry: {'train/loss': 1.3919338226318358, 'train/ce_loss': 0.47412109375, 'train/seg_cls_loss': 0.00850830078125, 'train/kl_loss': 0.192578125, 'train/mask_bce_loss': 0.07861823085695505, 'train/mask_dice_loss': 0.3685693919658661, 'train/mask_loss': 0.447187614440918, 'metrics/total_secs_per_batch': 6.069563865661621, 'metrics/data_secs_per_batch': 2.8295933485031126, '_timestamp': 1740958208.5941708}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 150 is less than current step: 499. Dropping entry: {'train/lr': 0.0001654285714285714, '_timestamp': 1740958208.594523}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 151 is less than current step: 499. Dropping entry: {'train/loss': 1.899266791343689, 'train/ce_loss': 0.4220703125, 'train/seg_cls_loss': 0.0167724609375, 'train/kl_loss': 0.30234375, 'train/mask_bce_loss': 0.13926718346774578, 'train/mask_dice_loss': 0.5799951016902923, 'train/mask_loss': 0.719262284040451, 'metrics/total_secs_per_batch': 5.775825262069702, 'metrics/data_secs_per_batch': 2.594240832328796, '_timestamp': 1740958214.3698668}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 151 is less than current step: 499. Dropping entry: {'train/lr': 0.00016530612244897955, '_timestamp': 1740958214.3701334}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 152 is less than current step: 499. Dropping entry: {'train/loss': 1.8418211340904236, 'train/ce_loss': 0.357666015625, 'train/seg_cls_loss': 0.01802978515625, 'train/kl_loss': 0.264453125, 'train/mask_bce_loss': 0.13169180750846862, 'train/mask_dice_loss': 0.5926367223262787, 'train/mask_loss': 0.7243285357952118, 'metrics/total_secs_per_batch': 6.454068899154663, 'metrics/data_secs_per_batch': 2.8358660697937013, '_timestamp': 1740958220.8239393}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 152 is less than current step: 499. Dropping entry: {'train/lr': 0.00016518367346938777, '_timestamp': 1740958220.8242195}).
Epoch: [2][154/500]	Time  5.905 ( 5.905)	Loss 1.8457 (1.7651)	CeLoss 0.2168 (0.6023)	SegCLSLoss 0.0143 (0.0124)	KLLoss 0.3906 (0.2680)	MaskLoss 0.7920 (0.5648)	MaskBCELoss 0.2215 (0.1461)	MaskDICELoss 0.5705 (0.4187)
Epoch: [2][155/500]	Time  6.366 ( 6.366)	Loss 1.9123 (1.9420)	CeLoss 0.3184 (0.3409)	SegCLSLoss 0.0140 (0.0171)	KLLoss 0.3809 (0.3430)	MaskLoss 0.7745 (0.7790)	MaskBCELoss 0.0711 (0.1466)	MaskDICELoss 0.7034 (0.6324)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 153 is less than current step: 499. Dropping entry: {'train/loss': 1.7651400804519652, 'train/ce_loss': 0.60234375, 'train/seg_cls_loss': 0.0124267578125, 'train/kl_loss': 0.26796875, 'train/mask_bce_loss': 0.1461364789865911, 'train/mask_dice_loss': 0.4186601087450981, 'train/mask_loss': 0.5647965908050537, 'metrics/total_secs_per_batch': 5.905179977416992, 'metrics/data_secs_per_batch': 2.2569289207458496, '_timestamp': 1740958226.7290916}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 153 is less than current step: 499. Dropping entry: {'train/lr': 0.00016506122448979593, '_timestamp': 1740958226.7294314}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 154 is less than current step: 499. Dropping entry: {'train/loss': 1.941979855298996, 'train/ce_loss': 0.34091796875, 'train/seg_cls_loss': 0.017144775390625, 'train/kl_loss': 0.34296875, 'train/mask_bce_loss': 0.14664405090734361, 'train/mask_dice_loss': 0.632353675365448, 'train/mask_loss': 0.778997728228569, 'metrics/total_secs_per_batch': 6.3662848472595215, 'metrics/data_secs_per_batch': 2.7818750143051147, '_timestamp': 1740958233.0954258}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 154 is less than current step: 499. Dropping entry: {'train/lr': 0.00016493877551020407, '_timestamp': 1740958233.0956962}).
Epoch: [2][156/500]	Time  5.887 ( 5.887)	Loss 1.8451 (1.6053)	CeLoss 0.2754 (0.4385)	SegCLSLoss 0.0129 (0.0165)	KLLoss 0.3887 (0.3115)	MaskLoss 0.7624 (0.5637)	MaskBCELoss 0.0952 (0.0600)	MaskDICELoss 0.6672 (0.5037)
Epoch: [2][157/500]	Time  6.722 ( 6.722)	Loss 2.5877 (1.7081)	CeLoss 0.1914 (0.2469)	SegCLSLoss 0.0228 (0.0176)	KLLoss 0.3633 (0.3850)	MaskLoss 1.1742 (0.7070)	MaskBCELoss 0.3527 (0.1044)	MaskDICELoss 0.8215 (0.6026)
Epoch: [2][158/500]	Time  6.720 ( 6.720)	Loss 1.8545 (2.0731)	CeLoss 0.2119 (0.2352)	SegCLSLoss 0.0259 (0.0201)	KLLoss 0.3867 (0.3871)	MaskLoss 0.7954 (0.8944)	MaskBCELoss 0.0145 (0.1541)	MaskDICELoss 0.7809 (0.7403)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 155 is less than current step: 499. Dropping entry: {'train/loss': 1.605259609222412, 'train/ce_loss': 0.4384765625, 'train/seg_cls_loss': 0.016461181640625, 'train/kl_loss': 0.3115234375, 'train/mask_bce_loss': 0.05998459998518228, 'train/mask_dice_loss': 0.5036803603172302, 'train/mask_loss': 0.5636649549007415, 'metrics/total_secs_per_batch': 5.88684606552124, 'metrics/data_secs_per_batch': 2.4478408813476564, '_timestamp': 1740958238.9822705}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 155 is less than current step: 499. Dropping entry: {'train/lr': 0.00016481632653061223, '_timestamp': 1740958238.9825473}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 156 is less than current step: 499. Dropping entry: {'train/loss': 1.708089318871498, 'train/ce_loss': 0.246875, 'train/seg_cls_loss': 0.01761474609375, 'train/kl_loss': 0.3849609375, 'train/mask_bce_loss': 0.10442219038959592, 'train/mask_dice_loss': 0.6025521606206894, 'train/mask_loss': 0.706974346935749, 'metrics/total_secs_per_batch': 6.721914529800415, 'metrics/data_secs_per_batch': 3.096653127670288, '_timestamp': 1740958245.7041621}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 156 is less than current step: 499. Dropping entry: {'train/lr': 0.0001646938775510204, '_timestamp': 1740958245.7044318}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 157 is less than current step: 499. Dropping entry: {'train/loss': 2.0731238961219787, 'train/ce_loss': 0.235205078125, 'train/seg_cls_loss': 0.0201416015625, 'train/kl_loss': 0.387109375, 'train/mask_bce_loss': 0.15411138609051706, 'train/mask_dice_loss': 0.7402630597352982, 'train/mask_loss': 0.8943744480609894, 'metrics/total_secs_per_batch': 6.72005558013916, 'metrics/data_secs_per_batch': 2.8823063373565674, '_timestamp': 1740958252.4244304}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 157 is less than current step: 499. Dropping entry: {'train/lr': 0.00016457142857142856, '_timestamp': 1740958252.4247682}).
Epoch: [2][159/500]	Time  5.618 ( 5.618)	Loss 1.6935 (1.7913)	CeLoss 0.2285 (0.4936)	SegCLSLoss 0.0216 (0.0137)	KLLoss 0.3730 (0.2656)	MaskLoss 0.7081 (0.6322)	MaskBCELoss 0.1002 (0.1455)	MaskDICELoss 0.6079 (0.4868)
[2025-03-02 17:31:03,911] [INFO] [logging.py:96:log_dist] [Rank 0] step=1160, skipped=0, lr=[0.00016438775510204081], mom=[(0.9, 0.95)]
[2025-03-02 17:31:03,917] [INFO] [timer.py:215:stop] epoch=0/micro_step=11600/global_step=1160, RunningAvgSamplesPerSec=1.53995943706329, CurrSamplesPerSec=1.7021424418706106, MemAllocated=31.23GB, MaxMemAllocated=37.19GB
Epoch: [2][160/500]	Time  5.877 ( 5.877)	Loss 2.8712 (2.0517)	CeLoss 0.3242 (0.5465)	SegCLSLoss 0.0118 (0.0140)	KLLoss 0.4023 (0.2668)	MaskLoss 1.2501 (0.7356)	MaskBCELoss 0.2524 (0.1348)	MaskDICELoss 0.9977 (0.6008)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 158 is less than current step: 499. Dropping entry: {'train/loss': 1.7913458108901978, 'train/ce_loss': 0.4935546875, 'train/seg_cls_loss': 0.013702392578125, 'train/kl_loss': 0.265625, 'train/mask_bce_loss': 0.145464625954628, 'train/mask_dice_loss': 0.48678055703639983, 'train/mask_loss': 0.6322451770305634, 'metrics/total_secs_per_batch': 5.61765456199646, 'metrics/data_secs_per_batch': 2.3226882934570314, '_timestamp': 1740958258.041974}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 158 is less than current step: 499. Dropping entry: {'train/lr': 0.00016444897959183672, '_timestamp': 1740958258.042263}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 159 is less than current step: 499. Dropping entry: {'train/loss': 2.0517022967338563, 'train/ce_loss': 0.546484375, 'train/seg_cls_loss': 0.0140380859375, 'train/kl_loss': 0.266796875, 'train/mask_bce_loss': 0.13481386369094253, 'train/mask_dice_loss': 0.6008029043674469, 'train/mask_loss': 0.7356167614459992, 'metrics/total_secs_per_batch': 5.876600980758667, 'metrics/data_secs_per_batch': 2.6048102140426637, '_timestamp': 1740958263.9183085}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 159 is less than current step: 499. Dropping entry: {'train/lr': 0.00016432653061224488, '_timestamp': 1740958263.9185612}).
Epoch: [2][161/500]	Time  6.356 ( 6.356)	Loss 1.3872 (1.5879)	CeLoss 0.2324 (0.3622)	SegCLSLoss 0.0152 (0.0157)	KLLoss 0.3945 (0.3449)	MaskLoss 0.5540 (0.5917)	MaskBCELoss 0.2092 (0.1323)	MaskDICELoss 0.3448 (0.4593)
Epoch: [2][162/500]	Time  5.378 ( 5.378)	Loss 0.9570 (1.7937)	CeLoss 0.9570 (0.5289)	SegCLSLoss 0.0000 (0.0153)	KLLoss 0.0000 (0.2617)	MaskLoss 0.0000 (0.6154)	MaskBCELoss 0.0000 (0.1068)	MaskDICELoss 0.0000 (0.5087)
Epoch: [2][163/500]	Time  5.137 ( 5.137)	Loss 1.5391 (1.3384)	CeLoss 1.5391 (0.6281)	SegCLSLoss 0.0000 (0.0091)	KLLoss 0.0000 (0.1918)	MaskLoss 0.0000 (0.3433)	MaskBCELoss 0.0000 (0.0577)	MaskDICELoss 0.0000 (0.2856)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 160 is less than current step: 499. Dropping entry: {'train/loss': 1.5879489839076997, 'train/ce_loss': 0.36220703125, 'train/seg_cls_loss': 0.01572265625, 'train/kl_loss': 0.344921875, 'train/mask_bce_loss': 0.13234472814947368, 'train/mask_dice_loss': 0.45933485180139544, 'train/mask_loss': 0.5916795760393143, 'metrics/total_secs_per_batch': 6.355655193328857, 'metrics/data_secs_per_batch': 2.695468616485596, '_timestamp': 1740958270.2741404}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 160 is less than current step: 499. Dropping entry: {'train/lr': 0.00016420408163265305, '_timestamp': 1740958270.2744083}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 161 is less than current step: 499. Dropping entry: {'train/loss': 1.7936630606651307, 'train/ce_loss': 0.52890625, 'train/seg_cls_loss': 0.015289306640625, 'train/kl_loss': 0.26171875, 'train/mask_bce_loss': 0.1067619975656271, 'train/mask_dice_loss': 0.5086730360984802, 'train/mask_loss': 0.6154350340366364, 'metrics/total_secs_per_batch': 5.378101825714111, 'metrics/data_secs_per_batch': 2.5044646739959715, '_timestamp': 1740958275.6522658}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 161 is less than current step: 499. Dropping entry: {'train/lr': 0.0001640816326530612, '_timestamp': 1740958275.6525428}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 162 is less than current step: 499. Dropping entry: {'train/loss': 1.3384344935417176, 'train/ce_loss': 0.628125, 'train/seg_cls_loss': 0.009100341796875, 'train/kl_loss': 0.191796875, 'train/mask_bce_loss': 0.05771999806165695, 'train/mask_dice_loss': 0.28561833798885344, 'train/mask_loss': 0.34333832561969757, 'metrics/total_secs_per_batch': 5.137264013290405, 'metrics/data_secs_per_batch': 2.4913230419158934, '_timestamp': 1740958280.7895327}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 162 is less than current step: 499. Dropping entry: {'train/lr': 0.00016395918367346937, '_timestamp': 1740958280.7898035}).
Epoch: [2][164/500]	Time  6.728 ( 6.728)	Loss 2.5876 (1.9388)	CeLoss 0.1582 (0.4035)	SegCLSLoss 0.0337 (0.0157)	KLLoss 0.3770 (0.3006)	MaskLoss 1.1874 (0.7486)	MaskBCELoss 0.2104 (0.0970)	MaskDICELoss 0.9769 (0.6516)
Epoch: [2][165/500]	Time  6.863 ( 6.863)	Loss 1.0078 (1.9109)	CeLoss 1.0078 (0.3883)	SegCLSLoss 0.0000 (0.0186)	KLLoss 0.0000 (0.3031)	MaskLoss 0.0000 (0.7415)	MaskBCELoss 0.0000 (0.0955)	MaskDICELoss 0.0000 (0.6460)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 163 is less than current step: 499. Dropping entry: {'train/loss': 1.938836693763733, 'train/ce_loss': 0.403515625, 'train/seg_cls_loss': 0.01573486328125, 'train/kl_loss': 0.3005859375, 'train/mask_bce_loss': 0.09702841844409704, 'train/mask_dice_loss': 0.6515891492366791, 'train/mask_loss': 0.7486175835132599, 'metrics/total_secs_per_batch': 6.728301048278809, 'metrics/data_secs_per_batch': 2.791962146759033, '_timestamp': 1740958287.5178459}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 163 is less than current step: 499. Dropping entry: {'train/lr': 0.00016383673469387753, '_timestamp': 1740958287.518132}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 164 is less than current step: 499. Dropping entry: {'train/loss': 1.9109288096427917, 'train/ce_loss': 0.38828125, 'train/seg_cls_loss': 0.0185791015625, 'train/kl_loss': 0.303125, 'train/mask_bce_loss': 0.09554542377591133, 'train/mask_dice_loss': 0.6460029482841492, 'train/mask_loss': 0.7415483832359314, 'metrics/total_secs_per_batch': 6.862730026245117, 'metrics/data_secs_per_batch': 3.3457017660140993, '_timestamp': 1740958294.3808835}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 164 is less than current step: 499. Dropping entry: {'train/lr': 0.0001637142857142857, '_timestamp': 1740958294.3812559}).
Epoch: [2][166/500]	Time  6.402 ( 6.402)	Loss 1.9270 (1.8665)	CeLoss 0.2051 (0.3176)	SegCLSLoss 0.0189 (0.0202)	KLLoss 0.3770 (0.3363)	MaskLoss 0.8375 (0.7527)	MaskBCELoss 0.0071 (0.0924)	MaskDICELoss 0.8304 (0.6603)
Epoch: [2][167/500]	Time  6.348 ( 6.348)	Loss 1.2457 (1.6881)	CeLoss 0.2129 (0.5285)	SegCLSLoss 0.0146 (0.0130)	KLLoss 0.3867 (0.2691)	MaskLoss 0.4939 (0.5632)	MaskBCELoss 0.0856 (0.1183)	MaskDICELoss 0.4084 (0.4448)
Epoch: [2][168/500]	Time  6.058 ( 6.058)	Loss 0.0757 (1.4028)	CeLoss 0.0757 (0.4043)	SegCLSLoss 0.0000 (0.0128)	KLLoss 0.0000 (0.1811)	MaskLoss 0.0000 (0.4869)	MaskBCELoss 0.0000 (0.0677)	MaskDICELoss 0.0000 (0.4192)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 165 is less than current step: 499. Dropping entry: {'train/loss': 1.8664646744728088, 'train/ce_loss': 0.317578125, 'train/seg_cls_loss': 0.020220947265625, 'train/kl_loss': 0.336328125, 'train/mask_bce_loss': 0.0923853874206543, 'train/mask_dice_loss': 0.6602805435657502, 'train/mask_loss': 0.7526659309864044, 'metrics/total_secs_per_batch': 6.4022393226623535, 'metrics/data_secs_per_batch': 2.8176418781280517, '_timestamp': 1740958300.7828538}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 165 is less than current step: 499. Dropping entry: {'train/lr': 0.00016359183673469386, '_timestamp': 1740958300.7831504}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 166 is less than current step: 499. Dropping entry: {'train/loss': 1.6880601167678833, 'train/ce_loss': 0.528515625, 'train/seg_cls_loss': 0.012994384765625, 'train/kl_loss': 0.269140625, 'train/mask_bce_loss': 0.11833738349378109, 'train/mask_dice_loss': 0.4448332846164703, 'train/mask_loss': 0.5631706804037094, 'metrics/total_secs_per_batch': 6.347814321517944, 'metrics/data_secs_per_batch': 2.8967469692230225, '_timestamp': 1740958307.1306672}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 166 is less than current step: 499. Dropping entry: {'train/lr': 0.00016346938775510202, '_timestamp': 1740958307.1309764}).
Epoch: [2][169/500]	Time  7.333 ( 7.333)	Loss 2.4699 (2.1255)	CeLoss 0.1846 (0.4053)	SegCLSLoss 0.0371 (0.0177)	KLLoss 0.3672 (0.3436)	MaskLoss 1.1148 (0.8386)	MaskBCELoss 0.2286 (0.1925)	MaskDICELoss 0.8863 (0.6461)
[2025-03-02 17:32:05,351] [INFO] [logging.py:96:log_dist] [Rank 0] step=1170, skipped=0, lr=[0.00016316326530612242], mom=[(0.9, 0.95)]
[2025-03-02 17:32:05,357] [INFO] [timer.py:215:stop] epoch=0/micro_step=11700/global_step=1170, RunningAvgSamplesPerSec=1.5406722931879682, CurrSamplesPerSec=2.0682392561426792, MemAllocated=31.57GB, MaxMemAllocated=37.19GB
Epoch: [2][170/500]	Time  4.837 ( 4.837)	Loss 2.1336 (1.6094)	CeLoss 0.2539 (0.7997)	SegCLSLoss 0.0260 (0.0097)	KLLoss 0.3789 (0.1535)	MaskLoss 0.9145 (0.3948)	MaskBCELoss 0.0246 (0.0595)	MaskDICELoss 0.8898 (0.3353)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 167 is less than current step: 499. Dropping entry: {'train/loss': 1.4027757048606873, 'train/ce_loss': 0.404345703125, 'train/seg_cls_loss': 0.01280517578125, 'train/kl_loss': 0.1810546875, 'train/mask_bce_loss': 0.06770269516855479, 'train/mask_dice_loss': 0.4192076146602631, 'train/mask_loss': 0.4869103193283081, 'metrics/total_secs_per_batch': 6.057528018951416, 'metrics/data_secs_per_batch': 2.924048829078674, '_timestamp': 1740958313.1880844}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 167 is less than current step: 499. Dropping entry: {'train/lr': 0.0001633469387755102, '_timestamp': 1740958313.1883485}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 168 is less than current step: 499. Dropping entry: {'train/loss': 2.1255045533180237, 'train/ce_loss': 0.4052734375, 'train/seg_cls_loss': 0.01768798828125, 'train/kl_loss': 0.3435546875, 'train/mask_bce_loss': 0.19251356963068247, 'train/mask_dice_loss': 0.6461176067590714, 'train/mask_loss': 0.8386311769485474, 'metrics/total_secs_per_batch': 7.333423137664795, 'metrics/data_secs_per_batch': 3.2353219270706175, '_timestamp': 1740958320.5215847}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 168 is less than current step: 499. Dropping entry: {'train/lr': 0.00016322448979591835, '_timestamp': 1740958320.5218666}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 169 is less than current step: 499. Dropping entry: {'train/loss': 1.6094218969345093, 'train/ce_loss': 0.79970703125, 'train/seg_cls_loss': 0.00965576171875, 'train/kl_loss': 0.153515625, 'train/mask_bce_loss': 0.059490013681352136, 'train/mask_dice_loss': 0.3352600157260895, 'train/mask_loss': 0.39475002884864807, 'metrics/total_secs_per_batch': 4.83659029006958, 'metrics/data_secs_per_batch': 2.051160192489624, '_timestamp': 1740958325.357989}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 169 is less than current step: 499. Dropping entry: {'train/lr': 0.0001631020408163265, '_timestamp': 1740958325.3582509}).
Epoch: [2][171/500]	Time  5.789 ( 5.789)	Loss 1.1250 (2.0987)	CeLoss 1.1250 (0.3925)	SegCLSLoss 0.0000 (0.0197)	KLLoss 0.0000 (0.3045)	MaskLoss 0.0000 (0.8330)	MaskBCELoss 0.0000 (0.2797)	MaskDICELoss 0.0000 (0.5533)
Epoch: [2][172/500]	Time  6.528 ( 6.528)	Loss 2.3930 (1.7564)	CeLoss 0.1553 (0.2991)	SegCLSLoss 0.0334 (0.0198)	KLLoss 0.3711 (0.3072)	MaskLoss 1.0920 (0.7084)	MaskBCELoss 0.1546 (0.0768)	MaskDICELoss 0.9374 (0.6315)
Epoch: [2][173/500]	Time  6.400 ( 6.400)	Loss 2.2175 (1.4701)	CeLoss 0.2266 (0.3619)	SegCLSLoss 0.0139 (0.0118)	KLLoss 0.3984 (0.2311)	MaskLoss 0.9720 (0.5396)	MaskBCELoss 0.2106 (0.1004)	MaskDICELoss 0.7614 (0.4392)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 170 is less than current step: 499. Dropping entry: {'train/loss': 2.098688304424286, 'train/ce_loss': 0.39248046875, 'train/seg_cls_loss': 0.019659423828125, 'train/kl_loss': 0.3044921875, 'train/mask_bce_loss': 0.2796934273093939, 'train/mask_dice_loss': 0.55329330265522, 'train/mask_loss': 0.8329867243766784, 'metrics/total_secs_per_batch': 5.789190053939819, 'metrics/data_secs_per_batch': 2.609671473503113, '_timestamp': 1740958331.1473806}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 170 is less than current step: 499. Dropping entry: {'train/lr': 0.00016297959183673468, '_timestamp': 1740958331.14767}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 171 is less than current step: 499. Dropping entry: {'train/loss': 1.7563701033592225, 'train/ce_loss': 0.29912109375, 'train/seg_cls_loss': 0.01978759765625, 'train/kl_loss': 0.3072265625, 'train/mask_bce_loss': 0.07684723027050495, 'train/mask_dice_loss': 0.631513586640358, 'train/mask_loss': 0.7083608210086823, 'metrics/total_secs_per_batch': 6.52827787399292, 'metrics/data_secs_per_batch': 2.899086332321167, '_timestamp': 1740958337.6759124}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 171 is less than current step: 499. Dropping entry: {'train/lr': 0.00016285714285714284, '_timestamp': 1740958337.6762965}).
Epoch: [2][174/500]	Time  6.707 ( 6.707)	Loss 3.1654 (1.7609)	CeLoss 0.2197 (0.1978)	SegCLSLoss 0.0119 (0.0175)	KLLoss 0.4004 (0.3523)	MaskLoss 1.4499 (0.7596)	MaskBCELoss 0.4732 (0.1502)	MaskDICELoss 0.9767 (0.6094)
Epoch: [2][175/500]	Time  6.805 ( 6.805)	Loss 2.3140 (1.9550)	CeLoss 0.2559 (0.4010)	SegCLSLoss 0.0168 (0.0146)	KLLoss 0.4102 (0.3117)	MaskLoss 1.0047 (0.7579)	MaskBCELoss 0.4112 (0.1565)	MaskDICELoss 0.5935 (0.6014)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 172 is less than current step: 499. Dropping entry: {'train/loss': 1.470124077796936, 'train/ce_loss': 0.361865234375, 'train/seg_cls_loss': 0.011846923828125, 'train/kl_loss': 0.2310546875, 'train/mask_bce_loss': 0.10042576678097248, 'train/mask_dice_loss': 0.4391528725624084, 'train/mask_loss': 0.5395786345005036, 'metrics/total_secs_per_batch': 6.399569034576416, 'metrics/data_secs_per_batch': 2.906059455871582, '_timestamp': 1740958344.075232}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 172 is less than current step: 499. Dropping entry: {'train/lr': 0.000162734693877551, '_timestamp': 1740958344.0755255}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 173 is less than current step: 499. Dropping entry: {'train/loss': 1.7608773469924928, 'train/ce_loss': 0.19775390625, 'train/seg_cls_loss': 0.017474365234375, 'train/kl_loss': 0.35234375, 'train/mask_bce_loss': 0.1502205455675721, 'train/mask_dice_loss': 0.6093685060739518, 'train/mask_loss': 0.7595890581607818, 'metrics/total_secs_per_batch': 6.707055568695068, 'metrics/data_secs_per_batch': 3.0219045877456665, '_timestamp': 1740958350.782286}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 173 is less than current step: 499. Dropping entry: {'train/lr': 0.00016261224489795917, '_timestamp': 1740958350.782556}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 174 is less than current step: 499. Dropping entry: {'train/loss': 1.9549855232238769, 'train/ce_loss': 0.4009765625, 'train/seg_cls_loss': 0.01455078125, 'train/kl_loss': 0.31171875, 'train/mask_bce_loss': 0.15652498565614223, 'train/mask_dice_loss': 0.601387694478035, 'train/mask_loss': 0.7579126834869385, 'metrics/total_secs_per_batch': 6.804961919784546, 'metrics/data_secs_per_batch': 3.0280853748321532, '_timestamp': 1740958357.5872004}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 174 is less than current step: 499. Dropping entry: {'train/lr': 0.00016248979591836736, '_timestamp': 1740958357.5875344}).
Epoch: [2][176/500]	Time  6.250 ( 6.250)	Loss 2.0191 (1.5704)	CeLoss 0.2100 (0.2659)	SegCLSLoss 0.0231 (0.0170)	KLLoss 0.3789 (0.3535)	MaskLoss 0.8797 (0.6301)	MaskBCELoss 0.0912 (0.0719)	MaskDICELoss 0.7884 (0.5582)
Epoch: [2][177/500]	Time  6.668 ( 6.668)	Loss 3.0325 (1.8024)	CeLoss 0.2100 (0.4210)	SegCLSLoss 0.0206 (0.0153)	KLLoss 0.3887 (0.2713)	MaskLoss 1.3864 (0.6734)	MaskBCELoss 0.5461 (0.1557)	MaskDICELoss 0.8403 (0.5177)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 175 is less than current step: 499. Dropping entry: {'train/loss': 1.570407909154892, 'train/ce_loss': 0.26591796875, 'train/seg_cls_loss': 0.01700439453125, 'train/kl_loss': 0.353515625, 'train/mask_bce_loss': 0.07192504554986953, 'train/mask_dice_loss': 0.5582007750868797, 'train/mask_loss': 0.630125817656517, 'metrics/total_secs_per_batch': 6.249568700790405, 'metrics/data_secs_per_batch': 3.019502878189087, '_timestamp': 1740958363.836766}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 175 is less than current step: 499. Dropping entry: {'train/lr': 0.00016236734693877552, '_timestamp': 1740958363.837035}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 176 is less than current step: 499. Dropping entry: {'train/loss': 1.8023995876312255, 'train/ce_loss': 0.42099609375, 'train/seg_cls_loss': 0.01529541015625, 'train/kl_loss': 0.2712890625, 'train/mask_bce_loss': 0.15567078217864036, 'train/mask_dice_loss': 0.5176969856023789, 'train/mask_loss': 0.6733677566051484, 'metrics/total_secs_per_batch': 6.668240070343018, 'metrics/data_secs_per_batch': 3.080959701538086, '_timestamp': 1740958370.5051038}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 176 is less than current step: 499. Dropping entry: {'train/lr': 0.00016224489795918368, '_timestamp': 1740958370.5053887}).
Epoch: [2][178/500]	Time  6.370 ( 6.370)	Loss 1.9174 (2.1850)	CeLoss 0.1934 (0.3557)	SegCLSLoss 0.0261 (0.0183)	KLLoss 0.3711 (0.3500)	MaskLoss 0.8371 (0.8924)	MaskBCELoss 0.0065 (0.1974)	MaskDICELoss 0.8306 (0.6950)
Epoch: [2][179/500]	Time  5.954 ( 5.954)	Loss 0.7578 (1.7950)	CeLoss 0.7578 (0.4673)	SegCLSLoss 0.0000 (0.0137)	KLLoss 0.0000 (0.2773)	MaskLoss 0.0000 (0.6465)	MaskBCELoss 0.0000 (0.1437)	MaskDICELoss 0.0000 (0.5027)
[2025-03-02 17:33:07,951] [INFO] [logging.py:96:log_dist] [Rank 0] step=1180, skipped=0, lr=[0.00016193877551020408], mom=[(0.9, 0.95)]
[2025-03-02 17:33:07,957] [INFO] [timer.py:215:stop] epoch=0/micro_step=11800/global_step=1180, RunningAvgSamplesPerSec=1.5411398306672874, CurrSamplesPerSec=1.9505424058759435, MemAllocated=31.53GB, MaxMemAllocated=37.19GB
Epoch: [2][180/500]	Time  5.129 ( 5.129)	Loss 0.0801 (1.2855)	CeLoss 0.0801 (0.6510)	SegCLSLoss 0.0000 (0.0074)	KLLoss 0.0000 (0.1576)	MaskLoss 0.0000 (0.3075)	MaskBCELoss 0.0000 (0.0401)	MaskDICELoss 0.0000 (0.2674)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 177 is less than current step: 499. Dropping entry: {'train/loss': 2.184961974620819, 'train/ce_loss': 0.3556640625, 'train/seg_cls_loss': 0.01826171875, 'train/kl_loss': 0.35, 'train/mask_bce_loss': 0.19741980670951306, 'train/mask_dice_loss': 0.6949635118246078, 'train/mask_loss': 0.8923833310604096, 'metrics/total_secs_per_batch': 6.370098829269409, 'metrics/data_secs_per_batch': 2.996184062957764, '_timestamp': 1740958376.8752031}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 177 is less than current step: 499. Dropping entry: {'train/lr': 0.00016212244897959184, '_timestamp': 1740958376.875477}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 178 is less than current step: 499. Dropping entry: {'train/loss': 1.7949626207351685, 'train/ce_loss': 0.46728515625, 'train/seg_cls_loss': 0.013671875, 'train/kl_loss': 0.27734375, 'train/mask_bce_loss': 0.14372722059488297, 'train/mask_dice_loss': 0.5027286887168885, 'train/mask_loss': 0.6464559137821198, 'metrics/total_secs_per_batch': 5.954019784927368, 'metrics/data_secs_per_batch': 2.6398938655853272, '_timestamp': 1740958382.8294299}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 178 is less than current step: 499. Dropping entry: {'train/lr': 0.000162, '_timestamp': 1740958382.8298209}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 179 is less than current step: 499. Dropping entry: {'train/loss': 1.2855257034301757, 'train/ce_loss': 0.651025390625, 'train/seg_cls_loss': 0.007427978515625, 'train/kl_loss': 0.1576171875, 'train/mask_bce_loss': 0.04006494656205177, 'train/mask_dice_loss': 0.2674195826053619, 'train/mask_loss': 0.3074845314025879, 'metrics/total_secs_per_batch': 5.1286821365356445, 'metrics/data_secs_per_batch': 2.1703105926513673, '_timestamp': 1740958387.9576893}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 179 is less than current step: 499. Dropping entry: {'train/lr': 0.00016187755102040817, '_timestamp': 1740958387.957986}).
Epoch: [2][181/500]	Time  5.212 ( 5.212)	Loss 2.3022 (1.5066)	CeLoss 0.1631 (0.4492)	SegCLSLoss 0.0225 (0.0100)	KLLoss 0.3750 (0.1932)	MaskLoss 1.0456 (0.5165)	MaskBCELoss 0.1897 (0.1369)	MaskDICELoss 0.8560 (0.3796)
Epoch: [2][182/500]	Time  6.096 ( 6.096)	Loss 0.5219 (1.8412)	CeLoss 0.3418 (0.3948)	SegCLSLoss 0.0112 (0.0165)	KLLoss 0.4062 (0.3568)	MaskLoss 0.0666 (0.7011)	MaskBCELoss 0.0391 (0.1399)	MaskDICELoss 0.0275 (0.5612)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 180 is less than current step: 499. Dropping entry: {'train/loss': 1.5066175699234008, 'train/ce_loss': 0.449169921875, 'train/seg_cls_loss': 0.0099609375, 'train/kl_loss': 0.1931640625, 'train/mask_bce_loss': 0.1369153406471014, 'train/mask_dice_loss': 0.37960145175457, 'train/mask_loss': 0.5165167868137359, 'metrics/total_secs_per_batch': 5.212268590927124, 'metrics/data_secs_per_batch': 2.0223692655563354, '_timestamp': 1740958393.1701524}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 180 is less than current step: 499. Dropping entry: {'train/lr': 0.00016175510204081633, '_timestamp': 1740958393.1704333}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 181 is less than current step: 499. Dropping entry: {'train/loss': 1.8412198841571807, 'train/ce_loss': 0.39482421875, 'train/seg_cls_loss': 0.016546630859375, 'train/kl_loss': 0.3568359375, 'train/mask_bce_loss': 0.13993682935833932, 'train/mask_dice_loss': 0.5611906901001931, 'train/mask_loss': 0.7011275202035904, 'metrics/total_secs_per_batch': 6.095857858657837, 'metrics/data_secs_per_batch': 2.848766303062439, '_timestamp': 1740958399.266065}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 181 is less than current step: 499. Dropping entry: {'train/lr': 0.00016163265306122447, '_timestamp': 1740958399.2663538}).
Epoch: [2][183/500]	Time  5.681 ( 5.681)	Loss 1.4531 (1.5601)	CeLoss 1.4531 (0.4502)	SegCLSLoss 0.0000 (0.0120)	KLLoss 0.0000 (0.2797)	MaskLoss 0.0000 (0.5380)	MaskBCELoss 0.0000 (0.1649)	MaskDICELoss 0.0000 (0.3731)
Epoch: [2][184/500]	Time  5.645 ( 5.645)	Loss 2.2392 (2.0656)	CeLoss 0.1050 (0.2820)	SegCLSLoss 0.0608 (0.0213)	KLLoss 0.3750 (0.3555)	MaskLoss 1.0327 (0.8685)	MaskBCELoss 0.2620 (0.2674)	MaskDICELoss 0.7707 (0.6011)
Epoch: [2][185/500]	Time  6.160 ( 6.160)	Loss 1.2728 (1.8570)	CeLoss 0.2412 (0.3810)	SegCLSLoss 0.0126 (0.0162)	KLLoss 0.4121 (0.3174)	MaskLoss 0.4919 (0.7182)	MaskBCELoss 0.0648 (0.1224)	MaskDICELoss 0.4271 (0.5959)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 182 is less than current step: 499. Dropping entry: {'train/loss': 1.5601015090942383, 'train/ce_loss': 0.450244140625, 'train/seg_cls_loss': 0.01204833984375, 'train/kl_loss': 0.2796875, 'train/mask_bce_loss': 0.16489204838871957, 'train/mask_dice_loss': 0.3731421113014221, 'train/mask_loss': 0.5380341589450837, 'metrics/total_secs_per_batch': 5.681301832199097, 'metrics/data_secs_per_batch': 2.750984764099121, '_timestamp': 1740958404.9472785}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 182 is less than current step: 499. Dropping entry: {'train/lr': 0.00016151020408163263, '_timestamp': 1740958404.9475477}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 183 is less than current step: 499. Dropping entry: {'train/loss': 2.065574812889099, 'train/ce_loss': 0.28203125, 'train/seg_cls_loss': 0.021295166015625, 'train/kl_loss': 0.35546875, 'train/mask_bce_loss': 0.2674393571913242, 'train/mask_dice_loss': 0.6010902404785157, 'train/mask_loss': 0.8685295939445495, 'metrics/total_secs_per_batch': 5.644709587097168, 'metrics/data_secs_per_batch': 2.73049476146698, '_timestamp': 1740958410.5919993}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 183 is less than current step: 499. Dropping entry: {'train/lr': 0.0001613877551020408, '_timestamp': 1740958410.5922556}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 184 is less than current step: 499. Dropping entry: {'train/loss': 1.856978952884674, 'train/ce_loss': 0.38095703125, 'train/seg_cls_loss': 0.01617431640625, 'train/kl_loss': 0.3173828125, 'train/mask_bce_loss': 0.12237525954842568, 'train/mask_dice_loss': 0.595860293507576, 'train/mask_loss': 0.7182355523109436, 'metrics/total_secs_per_batch': 6.160463333129883, 'metrics/data_secs_per_batch': 2.5157238960266115, '_timestamp': 1740958416.7525373}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 184 is less than current step: 499. Dropping entry: {'train/lr': 0.00016126530612244896, '_timestamp': 1740958416.7528112}).
Epoch: [2][186/500]	Time  6.544 ( 6.544)	Loss 1.7422 (1.7652)	CeLoss 1.7422 (0.5797)	SegCLSLoss 0.0000 (0.0136)	KLLoss 0.0000 (0.2811)	MaskLoss 0.0000 (0.5753)	MaskBCELoss 0.0000 (0.0994)	MaskDICELoss 0.0000 (0.4759)
Epoch: [2][187/500]	Time  6.017 ( 6.017)	Loss 4.3279 (1.8995)	CeLoss 0.1543 (0.3078)	SegCLSLoss 0.0195 (0.0158)	KLLoss 0.4180 (0.3191)	MaskLoss 2.0609 (0.7758)	MaskBCELoss 1.5093 (0.2756)	MaskDICELoss 0.5516 (0.5003)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 185 is less than current step: 499. Dropping entry: {'train/loss': 1.7652137994766235, 'train/ce_loss': 0.5796875, 'train/seg_cls_loss': 0.013629150390625, 'train/kl_loss': 0.2810546875, 'train/mask_bce_loss': 0.09939999566413463, 'train/mask_dice_loss': 0.47593153119087217, 'train/mask_loss': 0.5753315150737762, 'metrics/total_secs_per_batch': 6.544005393981934, 'metrics/data_secs_per_batch': 3.0673561573028563, '_timestamp': 1740958423.2965362}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 185 is less than current step: 499. Dropping entry: {'train/lr': 0.00016114285714285712, '_timestamp': 1740958423.296827}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 186 is less than current step: 499. Dropping entry: {'train/loss': 1.899543833732605, 'train/ce_loss': 0.3078125, 'train/seg_cls_loss': 0.015753173828125, 'train/kl_loss': 0.319140625, 'train/mask_bce_loss': 0.27556499615311625, 'train/mask_dice_loss': 0.5002811595797538, 'train/mask_loss': 0.7758461505174636, 'metrics/total_secs_per_batch': 6.016964912414551, 'metrics/data_secs_per_batch': 2.7760478019714356, '_timestamp': 1740958429.3135133}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 186 is less than current step: 499. Dropping entry: {'train/lr': 0.00016102040816326528, '_timestamp': 1740958429.3137858}).
Epoch: [2][188/500]	Time  5.629 ( 5.629)	Loss 2.5174 (1.7751)	CeLoss 0.2988 (0.3785)	SegCLSLoss 0.0154 (0.0162)	KLLoss 0.3789 (0.3555)	MaskLoss 1.0868 (0.6765)	MaskBCELoss 0.4873 (0.1467)	MaskDICELoss 0.5995 (0.5298)
Epoch: [2][189/500]	Time  5.681 ( 5.681)	Loss 0.9102 (1.8323)	CeLoss 0.1924 (0.4562)	SegCLSLoss 0.0206 (0.0168)	KLLoss 0.3848 (0.2738)	MaskLoss 0.3340 (0.6701)	MaskBCELoss 0.0468 (0.2012)	MaskDICELoss 0.2872 (0.4689)
[2025-03-02 17:34:06,652] [INFO] [logging.py:96:log_dist] [Rank 0] step=1190, skipped=0, lr=[0.0001607142857142857], mom=[(0.9, 0.95)]
[2025-03-02 17:34:06,658] [INFO] [timer.py:215:stop] epoch=0/micro_step=11900/global_step=1190, RunningAvgSamplesPerSec=1.542379867580112, CurrSamplesPerSec=1.6572365651081606, MemAllocated=31.25GB, MaxMemAllocated=37.19GB
Epoch: [2][190/500]	Time  6.036 ( 6.036)	Loss 2.4394 (1.8095)	CeLoss 0.2178 (0.3771)	SegCLSLoss 0.0249 (0.0183)	KLLoss 0.3789 (0.3105)	MaskLoss 1.0859 (0.6960)	MaskBCELoss 0.2828 (0.1596)	MaskDICELoss 0.8031 (0.5364)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 187 is less than current step: 499. Dropping entry: {'train/loss': 1.7751315593719483, 'train/ce_loss': 0.378515625, 'train/seg_cls_loss': 0.016229248046875, 'train/kl_loss': 0.35546875, 'train/mask_bce_loss': 0.14674883745610715, 'train/mask_dice_loss': 0.5297817841172219, 'train/mask_loss': 0.6765306144952774, 'metrics/total_secs_per_batch': 5.628880739212036, 'metrics/data_secs_per_batch': 2.4874680042266846, '_timestamp': 1740958434.9424365}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 187 is less than current step: 499. Dropping entry: {'train/lr': 0.00016089795918367345, '_timestamp': 1740958434.9426663}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 188 is less than current step: 499. Dropping entry: {'train/loss': 1.8323361814022063, 'train/ce_loss': 0.45615234375, 'train/seg_cls_loss': 0.0167724609375, 'train/kl_loss': 0.273828125, 'train/mask_bce_loss': 0.20116805136203766, 'train/mask_dice_loss': 0.46893071234226225, 'train/mask_loss': 0.6700987607240677, 'metrics/total_secs_per_batch': 5.681430816650391, 'metrics/data_secs_per_batch': 2.616093873977661, '_timestamp': 1740958440.623754}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 188 is less than current step: 499. Dropping entry: {'train/lr': 0.0001607755102040816, '_timestamp': 1740958440.6240263}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 189 is less than current step: 499. Dropping entry: {'train/loss': 1.8094795942306519, 'train/ce_loss': 0.377099609375, 'train/seg_cls_loss': 0.018292236328125, 'train/kl_loss': 0.310546875, 'train/mask_bce_loss': 0.15958144571632146, 'train/mask_dice_loss': 0.5364425182342529, 'train/mask_loss': 0.6960239768028259, 'metrics/total_secs_per_batch': 6.0356621742248535, 'metrics/data_secs_per_batch': 2.691027069091797, '_timestamp': 1740958446.6592538}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 189 is less than current step: 499. Dropping entry: {'train/lr': 0.00016065306122448977, '_timestamp': 1740958446.659527}).
Epoch: [2][191/500]	Time  6.798 ( 6.798)	Loss 2.3000 (2.0074)	CeLoss 0.2539 (0.2500)	SegCLSLoss 0.0161 (0.0172)	KLLoss 0.3926 (0.3947)	MaskLoss 0.9996 (0.8547)	MaskBCELoss 0.0450 (0.1916)	MaskDICELoss 0.9546 (0.6631)
Epoch: [2][192/500]	Time  6.113 ( 6.113)	Loss 1.6584 (1.8127)	CeLoss 0.2969 (0.3399)	SegCLSLoss 0.0139 (0.0166)	KLLoss 0.4023 (0.3158)	MaskLoss 0.6564 (0.7163)	MaskBCELoss 0.1609 (0.1697)	MaskDICELoss 0.4955 (0.5466)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 190 is less than current step: 499. Dropping entry: {'train/loss': 2.0074224948883055, 'train/ce_loss': 0.25, 'train/seg_cls_loss': 0.017205810546875, 'train/kl_loss': 0.3947265625, 'train/mask_bce_loss': 0.19161911811679602, 'train/mask_dice_loss': 0.6630686700344086, 'train/mask_loss': 0.8546877890825272, 'metrics/total_secs_per_batch': 6.797605991363525, 'metrics/data_secs_per_batch': 2.9278229236602784, '_timestamp': 1740958453.4570546}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 190 is less than current step: 499. Dropping entry: {'train/lr': 0.00016053061224489794, '_timestamp': 1740958453.4573388}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 191 is less than current step: 499. Dropping entry: {'train/loss': 1.8126802921295166, 'train/ce_loss': 0.33994140625, 'train/seg_cls_loss': 0.016632080078125, 'train/kl_loss': 0.3158203125, 'train/mask_bce_loss': 0.1697103636339307, 'train/mask_dice_loss': 0.5466395452618599, 'train/mask_loss': 0.7163499087095261, 'metrics/total_secs_per_batch': 6.11271858215332, 'metrics/data_secs_per_batch': 2.57009596824646, '_timestamp': 1740958459.5697856}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 191 is less than current step: 499. Dropping entry: {'train/lr': 0.0001604081632653061, '_timestamp': 1740958459.5700707}).
Epoch: [2][193/500]	Time  6.290 ( 6.290)	Loss 1.0847 (2.2085)	CeLoss 0.2207 (0.2157)	SegCLSLoss 0.0126 (0.0213)	KLLoss 0.4102 (0.3949)	MaskLoss 0.4085 (0.9712)	MaskBCELoss 0.0866 (0.2383)	MaskDICELoss 0.3219 (0.7329)
Epoch: [2][194/500]	Time  5.067 ( 5.067)	Loss 2.7681 (1.7874)	CeLoss 0.1914 (0.5789)	SegCLSLoss 0.0261 (0.0117)	KLLoss 0.3789 (0.2359)	MaskLoss 1.2629 (0.5896)	MaskBCELoss 0.3637 (0.1744)	MaskDICELoss 0.8992 (0.4152)
Epoch: [2][195/500]	Time  5.873 ( 5.873)	Loss 1.9115 (1.9854)	CeLoss 0.2715 (0.3415)	SegCLSLoss 0.0121 (0.0160)	KLLoss 0.4102 (0.3092)	MaskLoss 0.7966 (0.8025)	MaskBCELoss 0.1304 (0.1723)	MaskDICELoss 0.6661 (0.6302)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 192 is less than current step: 499. Dropping entry: {'train/loss': 2.2085351228713987, 'train/ce_loss': 0.21572265625, 'train/seg_cls_loss': 0.02125244140625, 'train/kl_loss': 0.394921875, 'train/mask_bce_loss': 0.23834546315483748, 'train/mask_dice_loss': 0.7328654527664185, 'train/mask_loss': 0.9712109178304672, 'metrics/total_secs_per_batch': 6.290000677108765, 'metrics/data_secs_per_batch': 2.8134049654006956, '_timestamp': 1740958465.8597775}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 192 is less than current step: 499. Dropping entry: {'train/lr': 0.00016028571428571426, '_timestamp': 1740958465.859973}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 193 is less than current step: 499. Dropping entry: {'train/loss': 1.787443333864212, 'train/ce_loss': 0.57890625, 'train/seg_cls_loss': 0.011749267578125, 'train/kl_loss': 0.2359375, 'train/mask_bce_loss': 0.17439634464681147, 'train/mask_dice_loss': 0.4152237609028816, 'train/mask_loss': 0.5896201059222221, 'metrics/total_secs_per_batch': 5.067198991775513, 'metrics/data_secs_per_batch': 2.418383812904358, '_timestamp': 1740958470.926993}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 193 is less than current step: 499. Dropping entry: {'train/lr': 0.00016016326530612243, '_timestamp': 1740958470.9272726}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 194 is less than current step: 499. Dropping entry: {'train/loss': 1.98538076877594, 'train/ce_loss': 0.34150390625, 'train/seg_cls_loss': 0.01597900390625, 'train/kl_loss': 0.3091796875, 'train/mask_bce_loss': 0.17226124312728644, 'train/mask_dice_loss': 0.6301947712898255, 'train/mask_loss': 0.8024560213088989, 'metrics/total_secs_per_batch': 5.8730552196502686, 'metrics/data_secs_per_batch': 2.5381559133529663, '_timestamp': 1740958476.8000593}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 194 is less than current step: 499. Dropping entry: {'train/lr': 0.0001600408163265306, '_timestamp': 1740958476.8003557}).
Epoch: [2][196/500]	Time  5.696 ( 5.696)	Loss 2.3617 (2.0047)	CeLoss 0.2695 (0.3893)	SegCLSLoss 0.0272 (0.0155)	KLLoss 0.3867 (0.3125)	MaskLoss 1.0197 (0.7882)	MaskBCELoss 0.0420 (0.1306)	MaskDICELoss 0.9777 (0.6575)
Epoch: [2][197/500]	Time  5.856 ( 5.856)	Loss 3.1717 (1.5965)	CeLoss 0.1670 (0.6002)	SegCLSLoss 0.0259 (0.0111)	KLLoss 0.3711 (0.1926)	MaskLoss 1.4775 (0.4857)	MaskBCELoss 0.5868 (0.1573)	MaskDICELoss 0.8906 (0.3284)
Epoch: [2][198/500]	Time  5.548 ( 5.548)	Loss 2.0678 (2.0722)	CeLoss 0.2441 (0.3199)	SegCLSLoss 0.0232 (0.0189)	KLLoss 0.3867 (0.3492)	MaskLoss 0.8865 (0.8539)	MaskBCELoss 0.0810 (0.1847)	MaskDICELoss 0.8054 (0.6692)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 195 is less than current step: 499. Dropping entry: {'train/loss': 2.004736375808716, 'train/ce_loss': 0.3892578125, 'train/seg_cls_loss': 0.01553955078125, 'train/kl_loss': 0.3125, 'train/mask_bce_loss': 0.130621818988584, 'train/mask_dice_loss': 0.6575373768806457, 'train/mask_loss': 0.7881591975688934, 'metrics/total_secs_per_batch': 5.695602178573608, 'metrics/data_secs_per_batch': 2.4806113481521606, '_timestamp': 1740958482.4959424}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 195 is less than current step: 499. Dropping entry: {'train/lr': 0.00015991836734693875, '_timestamp': 1740958482.4963307}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 196 is less than current step: 499. Dropping entry: {'train/loss': 1.5964595794677734, 'train/ce_loss': 0.6001953125, 'train/seg_cls_loss': 0.011102294921875, 'train/kl_loss': 0.192578125, 'train/mask_bce_loss': 0.15734520480036734, 'train/mask_dice_loss': 0.32838458120822905, 'train/mask_loss': 0.4857297897338867, 'metrics/total_secs_per_batch': 5.855996370315552, 'metrics/data_secs_per_batch': 2.5389726877212526, '_timestamp': 1740958488.35167}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 196 is less than current step: 499. Dropping entry: {'train/lr': 0.00015979591836734692, '_timestamp': 1740958488.3519952}).
Epoch: [2][199/500]	Time  6.694 ( 6.694)	Loss 0.0903 (1.4333)	CeLoss 0.0903 (0.1966)	SegCLSLoss 0.0000 (0.0108)	KLLoss 0.0000 (0.2799)	MaskLoss 0.0000 (0.6016)	MaskBCELoss 0.0000 (0.1574)	MaskDICELoss 0.0000 (0.4441)
[2025-03-02 17:35:06,942] [INFO] [logging.py:96:log_dist] [Rank 0] step=1200, skipped=0, lr=[0.00015948979591836734], mom=[(0.9, 0.95)]
[2025-03-02 17:35:06,948] [INFO] [timer.py:215:stop] epoch=0/micro_step=12000/global_step=1200, RunningAvgSamplesPerSec=1.5432853863908351, CurrSamplesPerSec=1.5737986987196533, MemAllocated=31.26GB, MaxMemAllocated=37.19GB
Epoch: [2][200/500]	Time  6.356 ( 6.356)	Loss 2.3614 (1.7129)	CeLoss 0.2080 (0.4821)	SegCLSLoss 0.0349 (0.0156)	KLLoss 0.3809 (0.2322)	MaskLoss 1.0488 (0.5998)	MaskBCELoss 0.3508 (0.1130)	MaskDICELoss 0.6980 (0.4868)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 197 is less than current step: 499. Dropping entry: {'train/loss': 2.0721908807754517, 'train/ce_loss': 0.319921875, 'train/seg_cls_loss': 0.018927001953125, 'train/kl_loss': 0.34921875, 'train/mask_bce_loss': 0.1846561251208186, 'train/mask_dice_loss': 0.6692127317190171, 'train/mask_loss': 0.8538688600063324, 'metrics/total_secs_per_batch': 5.54827618598938, 'metrics/data_secs_per_batch': 2.3648828268051147, '_timestamp': 1740958493.8999062}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 197 is less than current step: 499. Dropping entry: {'train/lr': 0.0001596734693877551, '_timestamp': 1740958493.9001946}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 198 is less than current step: 499. Dropping entry: {'train/loss': 1.4333417415618896, 'train/ce_loss': 0.196630859375, 'train/seg_cls_loss': 0.010833740234375, 'train/kl_loss': 0.2798828125, 'train/mask_bce_loss': 0.1574160061776638, 'train/mask_dice_loss': 0.44414256811141967, 'train/mask_loss': 0.6015585660934448, 'metrics/total_secs_per_batch': 6.6936423778533936, 'metrics/data_secs_per_batch': 2.833220958709717, '_timestamp': 1740958500.5935643}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 198 is less than current step: 499. Dropping entry: {'train/lr': 0.00015955102040816327, '_timestamp': 1740958500.5938394}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 199 is less than current step: 499. Dropping entry: {'train/loss': 1.712904441356659, 'train/ce_loss': 0.482080078125, 'train/seg_cls_loss': 0.015625, 'train/kl_loss': 0.2322265625, 'train/mask_bce_loss': 0.11301105115562678, 'train/mask_dice_loss': 0.4867761492729187, 'train/mask_loss': 0.5997871875762939, 'metrics/total_secs_per_batch': 6.355602264404297, 'metrics/data_secs_per_batch': 2.916917109489441, '_timestamp': 1740958506.948989}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 199 is less than current step: 499. Dropping entry: {'train/lr': 0.00015942857142857143, '_timestamp': 1740958506.9492626}).
Epoch: [2][201/500]	Time  5.934 ( 5.934)	Loss 1.5584 (1.6351)	CeLoss 0.2070 (0.4521)	SegCLSLoss 0.0188 (0.0167)	KLLoss 0.4023 (0.2687)	MaskLoss 0.6503 (0.5738)	MaskBCELoss 0.0479 (0.0472)	MaskDICELoss 0.6024 (0.5266)
Epoch: [2][202/500]	Time  5.763 ( 5.763)	Loss 1.0391 (1.6805)	CeLoss 1.0391 (0.3850)	SegCLSLoss 0.0000 (0.0121)	KLLoss 0.0000 (0.2764)	MaskLoss 0.0000 (0.6309)	MaskBCELoss 0.0000 (0.2393)	MaskDICELoss 0.0000 (0.3916)
Epoch: [2][203/500]	Time  5.136 ( 5.136)	Loss 0.1206 (1.8432)	CeLoss 0.1206 (0.4863)	SegCLSLoss 0.0000 (0.0090)	KLLoss 0.0000 (0.1979)	MaskLoss 0.0000 (0.6664)	MaskBCELoss 0.0000 (0.3146)	MaskDICELoss 0.0000 (0.3518)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 200 is less than current step: 499. Dropping entry: {'train/loss': 1.6351499557495117, 'train/ce_loss': 0.4521484375, 'train/seg_cls_loss': 0.01666259765625, 'train/kl_loss': 0.26875, 'train/mask_bce_loss': 0.04720973866060376, 'train/mask_dice_loss': 0.5265664130449295, 'train/mask_loss': 0.5737761437892914, 'metrics/total_secs_per_batch': 5.933531045913696, 'metrics/data_secs_per_batch': 2.289037084579468, '_timestamp': 1740958512.8827143}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 200 is less than current step: 499. Dropping entry: {'train/lr': 0.0001593061224489796, '_timestamp': 1740958512.8829904}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 201 is less than current step: 499. Dropping entry: {'train/loss': 1.6804921686649323, 'train/ce_loss': 0.3849609375, 'train/seg_cls_loss': 0.0120849609375, 'train/kl_loss': 0.2763671875, 'train/mask_bce_loss': 0.23934950679540634, 'train/mask_dice_loss': 0.3915704026818275, 'train/mask_loss': 0.6309199124574661, 'metrics/total_secs_per_batch': 5.762501239776611, 'metrics/data_secs_per_batch': 2.8605205535888674, '_timestamp': 1740958518.6452565}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 201 is less than current step: 499. Dropping entry: {'train/lr': 0.00015918367346938776, '_timestamp': 1740958518.6455448}).
Epoch: [2][204/500]	Time  5.764 ( 5.764)	Loss 1.9076 (1.7885)	CeLoss 0.2559 (0.4157)	SegCLSLoss 0.0154 (0.0162)	KLLoss 0.3965 (0.3541)	MaskLoss 0.8024 (0.6646)	MaskBCELoss 0.0729 (0.1338)	MaskDICELoss 0.7295 (0.5307)
Epoch: [2][205/500]	Time  5.121 ( 5.121)	Loss 2.8406 (1.7352)	CeLoss 0.4121 (0.6264)	SegCLSLoss 0.0110 (0.0096)	KLLoss 0.4102 (0.1967)	MaskLoss 1.1908 (0.5421)	MaskBCELoss 0.4445 (0.1401)	MaskDICELoss 0.7463 (0.4020)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 202 is less than current step: 499. Dropping entry: {'train/loss': 1.8431944489479064, 'train/ce_loss': 0.486328125, 'train/seg_cls_loss': 0.008953857421875, 'train/kl_loss': 0.1978515625, 'train/mask_bce_loss': 0.31462750509381293, 'train/mask_dice_loss': 0.35179392993450165, 'train/mask_loss': 0.6664214372634888, 'metrics/total_secs_per_batch': 5.136432886123657, 'metrics/data_secs_per_batch': 2.0920830249786375, '_timestamp': 1740958523.7817783}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 202 is less than current step: 499. Dropping entry: {'train/lr': 0.00015906122448979592, '_timestamp': 1740958523.7821598}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 203 is less than current step: 499. Dropping entry: {'train/loss': 1.7885029733181, 'train/ce_loss': 0.41572265625, 'train/seg_cls_loss': 0.01619873046875, 'train/kl_loss': 0.3541015625, 'train/mask_bce_loss': 0.13384551350027324, 'train/mask_dice_loss': 0.5307184770703316, 'train/mask_loss': 0.6645639859139919, 'metrics/total_secs_per_batch': 5.764092922210693, 'metrics/data_secs_per_batch': 2.2068148612976075, '_timestamp': 1740958529.5457428}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 203 is less than current step: 499. Dropping entry: {'train/lr': 0.00015893877551020408, '_timestamp': 1740958529.5460186}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 204 is less than current step: 499. Dropping entry: {'train/loss': 1.7351642489433288, 'train/ce_loss': 0.6263671875, 'train/seg_cls_loss': 0.00963134765625, 'train/kl_loss': 0.1966796875, 'train/mask_bce_loss': 0.1401391491293907, 'train/mask_dice_loss': 0.4019546926021576, 'train/mask_loss': 0.5420938432216644, 'metrics/total_secs_per_batch': 5.121350526809692, 'metrics/data_secs_per_batch': 2.349682831764221, '_timestamp': 1740958534.6671462}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 204 is less than current step: 499. Dropping entry: {'train/lr': 0.00015881632653061225, '_timestamp': 1740958534.6675096}).
Epoch: [2][206/500]	Time  5.853 ( 5.853)	Loss 0.9766 (1.8979)	CeLoss 0.9766 (0.4327)	SegCLSLoss 0.0000 (0.0152)	KLLoss 0.0000 (0.2703)	MaskLoss 0.0000 (0.7151)	MaskBCELoss 0.0000 (0.1592)	MaskDICELoss 0.0000 (0.5559)
Epoch: [2][207/500]	Time  5.790 ( 5.790)	Loss 1.2457 (1.7299)	CeLoss 0.2520 (0.4170)	SegCLSLoss 0.0135 (0.0210)	KLLoss 0.3945 (0.3094)	MaskLoss 0.4744 (0.6358)	MaskBCELoss 0.0795 (0.1027)	MaskDICELoss 0.3949 (0.5331)
Epoch: [2][208/500]	Time  7.454 ( 7.454)	Loss 2.6485 (2.0600)	CeLoss 0.1758 (0.2495)	SegCLSLoss 0.0175 (0.0178)	KLLoss 0.3965 (0.3918)	MaskLoss 1.2120 (0.8814)	MaskBCELoss 0.4415 (0.2242)	MaskDICELoss 0.7705 (0.6572)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 205 is less than current step: 499. Dropping entry: {'train/loss': 1.8979139924049377, 'train/ce_loss': 0.43271484375, 'train/seg_cls_loss': 0.0152099609375, 'train/kl_loss': 0.2703125, 'train/mask_bce_loss': 0.15920428475365042, 'train/mask_dice_loss': 0.5559148192405701, 'train/mask_loss': 0.7151190936565399, 'metrics/total_secs_per_batch': 5.852859020233154, 'metrics/data_secs_per_batch': 2.521638822555542, '_timestamp': 1740958540.5199833}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 205 is less than current step: 499. Dropping entry: {'train/lr': 0.0001586938775510204, '_timestamp': 1740958540.5202715}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 206 is less than current step: 499. Dropping entry: {'train/loss': 1.729864740371704, 'train/ce_loss': 0.417041015625, 'train/seg_cls_loss': 0.0210205078125, 'train/kl_loss': 0.309375, 'train/mask_bce_loss': 0.10270923748612404, 'train/mask_dice_loss': 0.5331215798854828, 'train/mask_loss': 0.6358308106660843, 'metrics/total_secs_per_batch': 5.7897865772247314, 'metrics/data_secs_per_batch': 2.452948784828186, '_timestamp': 1740958546.3097699}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 206 is less than current step: 499. Dropping entry: {'train/lr': 0.00015857142857142857, '_timestamp': 1740958546.3100662}).
Epoch: [2][209/500]	Time  5.239 ( 5.239)	Loss 1.2969 (1.5414)	CeLoss 1.2969 (0.6051)	SegCLSLoss 0.0000 (0.0112)	KLLoss 0.0000 (0.1932)	MaskLoss 0.0000 (0.4556)	MaskBCELoss 0.0000 (0.0448)	MaskDICELoss 0.0000 (0.4108)
[2025-03-02 17:36:04,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=1210, skipped=0, lr=[0.00015826530612244897], mom=[(0.9, 0.95)]
[2025-03-02 17:36:04,941] [INFO] [timer.py:215:stop] epoch=0/micro_step=12100/global_step=1210, RunningAvgSamplesPerSec=1.5446304992151338, CurrSamplesPerSec=1.6842099176551715, MemAllocated=31.26GB, MaxMemAllocated=37.19GB
Epoch: [2][210/500]	Time  5.939 ( 5.939)	Loss 2.4183 (1.8132)	CeLoss 0.2578 (0.5094)	SegCLSLoss 0.0248 (0.0142)	KLLoss 0.3633 (0.2705)	MaskLoss 1.0558 (0.6349)	MaskBCELoss 0.2134 (0.1172)	MaskDICELoss 0.8425 (0.5177)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 207 is less than current step: 499. Dropping entry: {'train/loss': 2.0600334644317626, 'train/ce_loss': 0.24951171875, 'train/seg_cls_loss': 0.0177978515625, 'train/kl_loss': 0.391796875, 'train/mask_bce_loss': 0.2241754975169897, 'train/mask_dice_loss': 0.6572084277868271, 'train/mask_loss': 0.8813839197158814, 'metrics/total_secs_per_batch': 7.4536402225494385, 'metrics/data_secs_per_batch': 3.1237706899642945, '_timestamp': 1740958553.7634122}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 207 is less than current step: 499. Dropping entry: {'train/lr': 0.0001584489795918367, '_timestamp': 1740958553.7636063}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 208 is less than current step: 499. Dropping entry: {'train/loss': 1.541387414932251, 'train/ce_loss': 0.605078125, 'train/seg_cls_loss': 0.01124267578125, 'train/kl_loss': 0.1931640625, 'train/mask_bce_loss': 0.04483138788491488, 'train/mask_dice_loss': 0.4107744336128235, 'train/mask_loss': 0.45560582280158995, 'metrics/total_secs_per_batch': 5.239388465881348, 'metrics/data_secs_per_batch': 2.4289544820785522, '_timestamp': 1740958559.0028677}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 208 is less than current step: 499. Dropping entry: {'train/lr': 0.00015832653061224487, '_timestamp': 1740958559.0031736}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 209 is less than current step: 499. Dropping entry: {'train/loss': 1.8132308006286622, 'train/ce_loss': 0.509375, 'train/seg_cls_loss': 0.014154052734375, 'train/kl_loss': 0.2705078125, 'train/mask_bce_loss': 0.117196349427104, 'train/mask_dice_loss': 0.5176905453205108, 'train/mask_loss': 0.6348868846893311, 'metrics/total_secs_per_batch': 5.93916654586792, 'metrics/data_secs_per_batch': 2.7210981845855713, '_timestamp': 1740958564.9418592}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 209 is less than current step: 499. Dropping entry: {'train/lr': 0.00015820408163265304, '_timestamp': 1740958564.9422061}).
Epoch: [2][211/500]	Time  4.838 ( 4.838)	Loss 1.0078 (1.4167)	CeLoss 1.0078 (0.5974)	SegCLSLoss 0.0000 (0.0116)	KLLoss 0.0000 (0.1941)	MaskLoss 0.0000 (0.3970)	MaskBCELoss 0.0000 (0.0832)	MaskDICELoss 0.0000 (0.3138)
Epoch: [2][212/500]	Time  5.850 ( 5.850)	Loss 2.6273 (1.8981)	CeLoss 0.2412 (0.5638)	SegCLSLoss 0.0197 (0.0120)	KLLoss 0.3711 (0.2713)	MaskLoss 1.1691 (0.6506)	MaskBCELoss 0.2532 (0.1576)	MaskDICELoss 0.9159 (0.4930)
Epoch: [2][213/500]	Time  5.374 ( 5.374)	Loss 1.8733 (1.3315)	CeLoss 0.2256 (0.6492)	SegCLSLoss 0.0168 (0.0068)	KLLoss 0.3887 (0.1551)	MaskLoss 0.7999 (0.3317)	MaskBCELoss 0.0305 (0.0539)	MaskDICELoss 0.7694 (0.2777)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 210 is less than current step: 499. Dropping entry: {'train/loss': 1.416664707660675, 'train/ce_loss': 0.597412109375, 'train/seg_cls_loss': 0.01156005859375, 'train/kl_loss': 0.194140625, 'train/mask_bce_loss': 0.08319545723497868, 'train/mask_dice_loss': 0.3137843579053879, 'train/mask_loss': 0.3969798132777214, 'metrics/total_secs_per_batch': 4.838278532028198, 'metrics/data_secs_per_batch': 2.08842613697052, '_timestamp': 1740958569.7802505}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 210 is less than current step: 499. Dropping entry: {'train/lr': 0.0001580816326530612, '_timestamp': 1740958569.7805476}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 211 is less than current step: 499. Dropping entry: {'train/loss': 1.8980563402175903, 'train/ce_loss': 0.56376953125, 'train/seg_cls_loss': 0.0119873046875, 'train/kl_loss': 0.2712890625, 'train/mask_bce_loss': 0.15755362398922443, 'train/mask_dice_loss': 0.49303703010082245, 'train/mask_loss': 0.6505906522274018, 'metrics/total_secs_per_batch': 5.8501787185668945, 'metrics/data_secs_per_batch': 2.577555537223816, '_timestamp': 1740958575.6304443}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 211 is less than current step: 499. Dropping entry: {'train/lr': 0.00015795918367346936, '_timestamp': 1740958575.6307373}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 212 is less than current step: 499. Dropping entry: {'train/loss': 1.331501019001007, 'train/ce_loss': 0.64921875, 'train/seg_cls_loss': 0.00675048828125, 'train/kl_loss': 0.155078125, 'train/mask_bce_loss': 0.05393740255385637, 'train/mask_dice_loss': 0.27773107290267945, 'train/mask_loss': 0.33166847825050355, 'metrics/total_secs_per_batch': 5.374415397644043, 'metrics/data_secs_per_batch': 2.290938925743103, '_timestamp': 1740958581.0048668}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 212 is less than current step: 499. Dropping entry: {'train/lr': 0.00015783673469387752, '_timestamp': 1740958581.005217}).
Epoch: [2][214/500]	Time  5.561 ( 5.561)	Loss 2.9887 (1.4555)	CeLoss 0.3242 (0.3925)	SegCLSLoss 0.0111 (0.0124)	KLLoss 0.3965 (0.1867)	MaskLoss 1.3098 (0.5191)	MaskBCELoss 0.4424 (0.1007)	MaskDICELoss 0.8674 (0.4184)
Epoch: [2][215/500]	Time  5.652 ( 5.652)	Loss 3.1733 (1.8756)	CeLoss 0.2637 (0.4188)	SegCLSLoss 0.0400 (0.0142)	KLLoss 0.3574 (0.2656)	MaskLoss 1.4274 (0.7117)	MaskBCELoss 0.4734 (0.1659)	MaskDICELoss 0.9541 (0.5457)
Epoch: [2][216/500]	Time  5.993 ( 5.993)	Loss 1.4408 (1.5517)	CeLoss 0.1631 (0.2852)	SegCLSLoss 0.0177 (0.0187)	KLLoss 0.4004 (0.3047)	MaskLoss 0.6144 (0.6133)	MaskBCELoss 0.1960 (0.1156)	MaskDICELoss 0.4185 (0.4977)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 213 is less than current step: 499. Dropping entry: {'train/loss': 1.4554974436759949, 'train/ce_loss': 0.39248046875, 'train/seg_cls_loss': 0.01236572265625, 'train/kl_loss': 0.18671875, 'train/mask_bce_loss': 0.1006585754454136, 'train/mask_dice_loss': 0.41839871406555174, 'train/mask_loss': 0.519057285785675, 'metrics/total_secs_per_batch': 5.561319828033447, 'metrics/data_secs_per_batch': 2.310582160949707, '_timestamp': 1740958586.5661817}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 213 is less than current step: 499. Dropping entry: {'train/lr': 0.0001577142857142857, '_timestamp': 1740958586.5664525}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 214 is less than current step: 499. Dropping entry: {'train/loss': 1.8755711317062378, 'train/ce_loss': 0.41875, 'train/seg_cls_loss': 0.014239501953125, 'train/kl_loss': 0.265625, 'train/mask_bce_loss': 0.16593711972236633, 'train/mask_dice_loss': 0.5457254022359848, 'train/mask_loss': 0.7116625249385834, 'metrics/total_secs_per_batch': 5.651879787445068, 'metrics/data_secs_per_batch': 2.741239333152771, '_timestamp': 1740958592.2180502}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 214 is less than current step: 499. Dropping entry: {'train/lr': 0.00015759183673469385, '_timestamp': 1740958592.2183182}).
Epoch: [2][217/500]	Time  4.961 ( 4.961)	Loss 2.0630 (1.8743)	CeLoss 0.2090 (0.5567)	SegCLSLoss 0.0184 (0.0157)	KLLoss 0.3809 (0.2672)	MaskLoss 0.9036 (0.6416)	MaskBCELoss 0.0432 (0.1749)	MaskDICELoss 0.8604 (0.4667)
Epoch: [2][218/500]	Time  6.386 ( 6.386)	Loss 2.2799 (1.6513)	CeLoss 0.2305 (0.3419)	SegCLSLoss 0.0157 (0.0130)	KLLoss 0.3809 (0.3111)	MaskLoss 1.0013 (0.6358)	MaskBCELoss 0.1881 (0.1443)	MaskDICELoss 0.8132 (0.4915)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 215 is less than current step: 499. Dropping entry: {'train/loss': 1.551687526702881, 'train/ce_loss': 0.28515625, 'train/seg_cls_loss': 0.01865234375, 'train/kl_loss': 0.3046875, 'train/mask_bce_loss': 0.11561682727187872, 'train/mask_dice_loss': 0.49772693067789076, 'train/mask_loss': 0.6133437603712082, 'metrics/total_secs_per_batch': 5.992754936218262, 'metrics/data_secs_per_batch': 2.8534733772277834, '_timestamp': 1740958598.210817}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 215 is less than current step: 499. Dropping entry: {'train/lr': 0.000157469387755102, '_timestamp': 1740958598.2111175}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 216 is less than current step: 499. Dropping entry: {'train/loss': 1.8743483662605285, 'train/ce_loss': 0.55673828125, 'train/seg_cls_loss': 0.0157470703125, 'train/kl_loss': 0.2671875, 'train/mask_bce_loss': 0.17486471738666295, 'train/mask_dice_loss': 0.4667039901018143, 'train/mask_loss': 0.6415687024593353, 'metrics/total_secs_per_batch': 4.961108446121216, 'metrics/data_secs_per_batch': 2.188333201408386, '_timestamp': 1740958603.171879}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 216 is less than current step: 499. Dropping entry: {'train/lr': 0.00015734693877551018, '_timestamp': 1740958603.1721613}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 217 is less than current step: 499. Dropping entry: {'train/loss': 1.651253890991211, 'train/ce_loss': 0.34189453125, 'train/seg_cls_loss': 0.013006591796875, 'train/kl_loss': 0.3111328125, 'train/mask_bce_loss': 0.14429510459303857, 'train/mask_dice_loss': 0.49153691455721854, 'train/mask_loss': 0.6358320236206054, 'metrics/total_secs_per_batch': 6.38625168800354, 'metrics/data_secs_per_batch': 2.693569850921631, '_timestamp': 1740958609.5584614}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 217 is less than current step: 499. Dropping entry: {'train/lr': 0.00015722448979591834, '_timestamp': 1740958609.5588586}).
Epoch: [2][219/500]	Time  6.227 ( 6.227)	Loss 2.1083 (1.6800)	CeLoss 0.2021 (0.4024)	SegCLSLoss 0.0225 (0.0164)	KLLoss 0.3906 (0.3074)	MaskLoss 0.9282 (0.6194)	MaskBCELoss 0.1775 (0.0991)	MaskDICELoss 0.7507 (0.5203)
[2025-03-02 17:37:02,981] [INFO] [logging.py:96:log_dist] [Rank 0] step=1220, skipped=0, lr=[0.00015704081632653057], mom=[(0.9, 0.95)]
[2025-03-02 17:37:02,986] [INFO] [timer.py:215:stop] epoch=0/micro_step=12200/global_step=1220, RunningAvgSamplesPerSec=1.5459455169306062, CurrSamplesPerSec=1.3887888099680923, MemAllocated=31.28GB, MaxMemAllocated=37.19GB
Epoch: [2][220/500]	Time  7.202 ( 7.202)	Loss 2.6730 (2.0462)	CeLoss 0.1348 (0.2394)	SegCLSLoss 0.0386 (0.0234)	KLLoss 0.3672 (0.3416)	MaskLoss 1.2408 (0.8803)	MaskBCELoss 0.4317 (0.1546)	MaskDICELoss 0.8091 (0.7256)
Epoch: [2][221/500]	Time  5.553 ( 5.553)	Loss 2.3400 (1.7797)	CeLoss 0.2100 (0.4596)	SegCLSLoss 0.0308 (0.0138)	KLLoss 0.3867 (0.2703)	MaskLoss 1.0382 (0.6430)	MaskBCELoss 0.0466 (0.1188)	MaskDICELoss 0.9915 (0.5241)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 218 is less than current step: 499. Dropping entry: {'train/loss': 1.6799742937088014, 'train/ce_loss': 0.40244140625, 'train/seg_cls_loss': 0.0164306640625, 'train/kl_loss': 0.307421875, 'train/mask_bce_loss': 0.09909435547888279, 'train/mask_dice_loss': 0.5202873200178146, 'train/mask_loss': 0.6193816721439361, 'metrics/total_secs_per_batch': 6.227160692214966, 'metrics/data_secs_per_batch': 2.8646486520767214, '_timestamp': 1740958615.7853122}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 218 is less than current step: 499. Dropping entry: {'train/lr': 0.0001571020408163265, '_timestamp': 1740958615.7855904}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 219 is less than current step: 499. Dropping entry: {'train/loss': 2.046179711818695, 'train/ce_loss': 0.23935546875, 'train/seg_cls_loss': 0.023388671875, 'train/kl_loss': 0.3416015625, 'train/mask_bce_loss': 0.1546327494084835, 'train/mask_dice_loss': 0.7256348371505738, 'train/mask_loss': 0.8802675783634186, 'metrics/total_secs_per_batch': 7.202043294906616, 'metrics/data_secs_per_batch': 3.3900930881500244, '_timestamp': 1740958622.9871898}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 219 is less than current step: 499. Dropping entry: {'train/lr': 0.00015697959183673467, '_timestamp': 1740958622.987467}).
Epoch: [2][222/500]	Time  6.458 ( 6.458)	Loss 2.6051 (1.9765)	CeLoss 0.1572 (0.2241)	SegCLSLoss 0.0427 (0.0258)	KLLoss 0.3672 (0.3805)	MaskLoss 1.1951 (0.8506)	MaskBCELoss 0.3876 (0.1707)	MaskDICELoss 0.8075 (0.6799)
Epoch: [2][223/500]	Time  5.828 ( 5.828)	Loss 2.3197 (1.4387)	CeLoss 0.2207 (0.4965)	SegCLSLoss 0.0193 (0.0110)	KLLoss 0.3789 (0.1922)	MaskLoss 1.0261 (0.4589)	MaskBCELoss 0.0279 (0.0311)	MaskDICELoss 0.9982 (0.4278)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 220 is less than current step: 499. Dropping entry: {'train/loss': 1.779650092124939, 'train/ce_loss': 0.4595703125, 'train/seg_cls_loss': 0.013848876953125, 'train/kl_loss': 0.2703125, 'train/mask_bce_loss': 0.11881090216338634, 'train/mask_dice_loss': 0.5241391450166702, 'train/mask_loss': 0.642950040102005, 'metrics/total_secs_per_batch': 5.552900314331055, 'metrics/data_secs_per_batch': 2.564962887763977, '_timestamp': 1740958628.540258}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 220 is less than current step: 499. Dropping entry: {'train/lr': 0.00015685714285714286, '_timestamp': 1740958628.5405977}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 221 is less than current step: 499. Dropping entry: {'train/loss': 1.9764994740486146, 'train/ce_loss': 0.22412109375, 'train/seg_cls_loss': 0.025762939453125, 'train/kl_loss': 0.38046875, 'train/mask_bce_loss': 0.1706857598386705, 'train/mask_dice_loss': 0.6798686742782593, 'train/mask_loss': 0.8505544304847718, 'metrics/total_secs_per_batch': 6.457610607147217, 'metrics/data_secs_per_batch': 2.8234065532684327, '_timestamp': 1740958634.9979239}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 221 is less than current step: 499. Dropping entry: {'train/lr': 0.00015673469387755102, '_timestamp': 1740958634.9982078}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 222 is less than current step: 499. Dropping entry: {'train/loss': 1.4386996030807495, 'train/ce_loss': 0.496484375, 'train/seg_cls_loss': 0.01099853515625, 'train/kl_loss': 0.1921875, 'train/mask_bce_loss': 0.03109376709908247, 'train/mask_dice_loss': 0.42775797843933105, 'train/mask_loss': 0.45885173678398133, 'metrics/total_secs_per_batch': 5.827864170074463, 'metrics/data_secs_per_batch': 2.3880854845046997, '_timestamp': 1740958640.8258126}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 222 is less than current step: 499. Dropping entry: {'train/lr': 0.00015661224489795918, '_timestamp': 1740958640.826182}).
Epoch: [2][224/500]	Time  6.604 ( 6.604)	Loss 1.5391 (2.2147)	CeLoss 1.5391 (0.3457)	SegCLSLoss 0.0000 (0.0204)	KLLoss 0.0000 (0.3426)	MaskLoss 0.0000 (0.9123)	MaskBCELoss 0.0000 (0.2457)	MaskDICELoss 0.0000 (0.6666)
Epoch: [2][225/500]	Time  7.116 ( 7.116)	Loss 2.4940 (2.2506)	CeLoss 0.1250 (0.3666)	SegCLSLoss 0.0371 (0.0186)	KLLoss 0.3711 (0.3418)	MaskLoss 1.1567 (0.9202)	MaskBCELoss 0.3965 (0.2439)	MaskDICELoss 0.7601 (0.6762)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 223 is less than current step: 499. Dropping entry: {'train/loss': 2.214718520641327, 'train/ce_loss': 0.345703125, 'train/seg_cls_loss': 0.02042236328125, 'train/kl_loss': 0.342578125, 'train/mask_bce_loss': 0.24572696946561337, 'train/mask_dice_loss': 0.6666127651929855, 'train/mask_loss': 0.9123397409915924, 'metrics/total_secs_per_batch': 6.60351037979126, 'metrics/data_secs_per_batch': 3.1830121517181396, '_timestamp': 1740958647.4292417}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 223 is less than current step: 499. Dropping entry: {'train/lr': 0.00015648979591836734, '_timestamp': 1740958647.4295087}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 224 is less than current step: 499. Dropping entry: {'train/loss': 2.2505789518356325, 'train/ce_loss': 0.3666015625, 'train/seg_cls_loss': 0.018621826171875, 'train/kl_loss': 0.341796875, 'train/mask_bce_loss': 0.24393286406993867, 'train/mask_dice_loss': 0.6762296438217164, 'train/mask_loss': 0.9201625049114227, 'metrics/total_secs_per_batch': 7.115776538848877, 'metrics/data_secs_per_batch': 3.2202401876449587, '_timestamp': 1740958654.5450675}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 224 is less than current step: 499. Dropping entry: {'train/lr': 0.0001563673469387755, '_timestamp': 1740958654.54534}).
Epoch: [2][226/500]	Time  7.236 ( 7.236)	Loss 1.6990 (1.9684)	CeLoss 0.2334 (0.2300)	SegCLSLoss 0.0157 (0.0204)	KLLoss 0.3809 (0.3824)	MaskLoss 0.7099 (0.8448)	MaskBCELoss 0.0563 (0.1908)	MaskDICELoss 0.6536 (0.6540)
Epoch: [2][227/500]	Time  5.658 ( 5.658)	Loss 2.7949 (2.2151)	CeLoss 0.1826 (0.2067)	SegCLSLoss 0.0250 (0.0278)	KLLoss 0.3750 (0.3791)	MaskLoss 1.2812 (0.9783)	MaskBCELoss 0.4083 (0.2484)	MaskDICELoss 0.8729 (0.7300)
Epoch: [2][228/500]	Time  6.446 ( 6.446)	Loss 2.1694 (1.9861)	CeLoss 0.2129 (0.3357)	SegCLSLoss 0.0136 (0.0185)	KLLoss 0.3984 (0.3418)	MaskLoss 0.9548 (0.8032)	MaskBCELoss 0.6838 (0.1949)	MaskDICELoss 0.2710 (0.6083)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 225 is less than current step: 499. Dropping entry: {'train/loss': 1.9684238135814667, 'train/ce_loss': 0.22998046875, 'train/seg_cls_loss': 0.020379638671875, 'train/kl_loss': 0.382421875, 'train/mask_bce_loss': 0.19076471272855997, 'train/mask_dice_loss': 0.6539940595626831, 'train/mask_loss': 0.8447587817907334, 'metrics/total_secs_per_batch': 7.235952138900757, 'metrics/data_secs_per_batch': 3.2720617771148683, '_timestamp': 1740958661.781164}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 225 is less than current step: 499. Dropping entry: {'train/lr': 0.00015624489795918367, '_timestamp': 1740958661.7815564}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 226 is less than current step: 499. Dropping entry: {'train/loss': 2.215090572834015, 'train/ce_loss': 0.20673828125, 'train/seg_cls_loss': 0.0278076171875, 'train/kl_loss': 0.3791015625, 'train/mask_bce_loss': 0.24838132485747338, 'train/mask_dice_loss': 0.7299647182226181, 'train/mask_loss': 0.9783460557460785, 'metrics/total_secs_per_batch': 5.65805459022522, 'metrics/data_secs_per_batch': 2.5851061582565307, '_timestamp': 1740958667.439131}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 226 is less than current step: 499. Dropping entry: {'train/lr': 0.00015612244897959183, '_timestamp': 1740958667.4394205}).
Epoch: [2][229/500]	Time  5.076 ( 5.076)	Loss 1.2812 (1.8919)	CeLoss 1.2812 (0.5951)	SegCLSLoss 0.0000 (0.0172)	KLLoss 0.0000 (0.2629)	MaskLoss 0.0000 (0.6310)	MaskBCELoss 0.0000 (0.0915)	MaskDICELoss 0.0000 (0.5395)
[2025-03-02 17:38:05,749] [INFO] [logging.py:96:log_dist] [Rank 0] step=1230, skipped=0, lr=[0.00015581632653061223], mom=[(0.9, 0.95)]
[2025-03-02 17:38:05,754] [INFO] [timer.py:215:stop] epoch=0/micro_step=12300/global_step=1230, RunningAvgSamplesPerSec=1.546321116644537, CurrSamplesPerSec=1.4719615164358175, MemAllocated=30.71GB, MaxMemAllocated=37.19GB
Epoch: [2][230/500]	Time  6.795 ( 6.795)	Loss 1.2969 (2.0010)	CeLoss 1.2969 (0.4562)	SegCLSLoss 0.0000 (0.0167)	KLLoss 0.0000 (0.3055)	MaskLoss 0.0000 (0.7530)	MaskBCELoss 0.0000 (0.1692)	MaskDICELoss 0.0000 (0.5838)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 227 is less than current step: 499. Dropping entry: {'train/loss': 1.9860739707946777, 'train/ce_loss': 0.3357421875, 'train/seg_cls_loss': 0.01849365234375, 'train/kl_loss': 0.341796875, 'train/mask_bce_loss': 0.1949115665629506, 'train/mask_dice_loss': 0.6083305209875107, 'train/mask_loss': 0.8032420754432679, 'metrics/total_secs_per_batch': 6.4455726146698, 'metrics/data_secs_per_batch': 3.2159468650817873, '_timestamp': 1740958673.8846557}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 227 is less than current step: 499. Dropping entry: {'train/lr': 0.000156, '_timestamp': 1740958673.8849323}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 228 is less than current step: 499. Dropping entry: {'train/loss': 1.8918829321861268, 'train/ce_loss': 0.5951171875, 'train/seg_cls_loss': 0.017230224609375, 'train/kl_loss': 0.262890625, 'train/mask_bce_loss': 0.09146778210997582, 'train/mask_dice_loss': 0.5395322799682617, 'train/mask_loss': 0.6310000538825988, 'metrics/total_secs_per_batch': 5.075646638870239, 'metrics/data_secs_per_batch': 2.2596997022628784, '_timestamp': 1740958678.960313}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 228 is less than current step: 499. Dropping entry: {'train/lr': 0.00015587755102040816, '_timestamp': 1740958678.9605901}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 229 is less than current step: 499. Dropping entry: {'train/loss': 2.0009898781776427, 'train/ce_loss': 0.45615234375, 'train/seg_cls_loss': 0.016741943359375, 'train/kl_loss': 0.30546875, 'train/mask_bce_loss': 0.169233538210392, 'train/mask_dice_loss': 0.5837516218423844, 'train/mask_loss': 0.752985167503357, 'metrics/total_secs_per_batch': 6.795214653015137, 'metrics/data_secs_per_batch': 3.2103177070617677, '_timestamp': 1740958685.7553298}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 229 is less than current step: 499. Dropping entry: {'train/lr': 0.00015575510204081632, '_timestamp': 1740958685.7556028}).
Epoch: [2][231/500]	Time  6.773 ( 6.773)	Loss 2.4676 (1.5711)	CeLoss 0.2002 (0.2128)	SegCLSLoss 0.0271 (0.0181)	KLLoss 0.3711 (0.3059)	MaskLoss 1.1088 (0.6595)	MaskBCELoss 0.1358 (0.1053)	MaskDICELoss 0.9729 (0.5542)
Epoch: [2][232/500]	Time  5.050 ( 5.050)	Loss 1.6531 (1.7075)	CeLoss 0.2119 (0.4938)	SegCLSLoss 0.0150 (0.0149)	KLLoss 0.3945 (0.2674)	MaskLoss 0.6967 (0.5898)	MaskBCELoss 0.0772 (0.1406)	MaskDICELoss 0.6194 (0.4492)
Epoch: [2][233/500]	Time  5.771 ( 5.771)	Loss 2.3981 (2.0252)	CeLoss 0.1650 (0.3525)	SegCLSLoss 0.0417 (0.0221)	KLLoss 0.3613 (0.3391)	MaskLoss 1.0882 (0.8139)	MaskBCELoss 0.3335 (0.2360)	MaskDICELoss 0.7547 (0.5778)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 230 is less than current step: 499. Dropping entry: {'train/loss': 1.5711150169372559, 'train/ce_loss': 0.21279296875, 'train/seg_cls_loss': 0.018133544921875, 'train/kl_loss': 0.305859375, 'train/mask_bce_loss': 0.10525825377553702, 'train/mask_dice_loss': 0.5542250156402588, 'train/mask_loss': 0.6594832688570023, 'metrics/total_secs_per_batch': 6.773297548294067, 'metrics/data_secs_per_batch': 3.037420463562012, '_timestamp': 1740958692.5288434}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 230 is less than current step: 499. Dropping entry: {'train/lr': 0.00015563265306122449, '_timestamp': 1740958692.5291326}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 231 is less than current step: 499. Dropping entry: {'train/loss': 1.7074867010116577, 'train/ce_loss': 0.49384765625, 'train/seg_cls_loss': 0.014935302734375, 'train/kl_loss': 0.2673828125, 'train/mask_bce_loss': 0.1405591309070587, 'train/mask_dice_loss': 0.4492193579673767, 'train/mask_loss': 0.5897785067558289, 'metrics/total_secs_per_batch': 5.05036735534668, 'metrics/data_secs_per_batch': 2.4039756059646606, '_timestamp': 1740958697.5791504}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 231 is less than current step: 499. Dropping entry: {'train/lr': 0.00015551020408163265, '_timestamp': 1740958697.5794878}).
Epoch: [2][234/500]	Time  6.160 ( 6.160)	Loss 2.1086 (1.9509)	CeLoss 0.2178 (0.2244)	SegCLSLoss 0.0167 (0.0170)	KLLoss 0.3887 (0.3904)	MaskLoss 0.9215 (0.8394)	MaskBCELoss 0.1895 (0.2004)	MaskDICELoss 0.7320 (0.6390)
Epoch: [2][235/500]	Time  5.983 ( 5.983)	Loss 1.8189 (1.7473)	CeLoss 0.2021 (0.5320)	SegCLSLoss 0.0187 (0.0175)	KLLoss 0.3789 (0.2645)	MaskLoss 0.7844 (0.5901)	MaskBCELoss 0.0142 (0.1432)	MaskDICELoss 0.7703 (0.4469)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 232 is less than current step: 499. Dropping entry: {'train/loss': 2.0251854062080383, 'train/ce_loss': 0.3525390625, 'train/seg_cls_loss': 0.022064208984375, 'train/kl_loss': 0.3390625, 'train/mask_bce_loss': 0.2360453724861145, 'train/mask_dice_loss': 0.5778168618679047, 'train/mask_loss': 0.8138622403144836, 'metrics/total_secs_per_batch': 5.771305084228516, 'metrics/data_secs_per_batch': 2.44083456993103, '_timestamp': 1740958703.350447}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 232 is less than current step: 499. Dropping entry: {'train/lr': 0.0001553877551020408, '_timestamp': 1740958703.3506298}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 233 is less than current step: 499. Dropping entry: {'train/loss': 1.9509213268756866, 'train/ce_loss': 0.2244140625, 'train/seg_cls_loss': 0.0170166015625, 'train/kl_loss': 0.3904296875, 'train/mask_bce_loss': 0.20042847134172917, 'train/mask_dice_loss': 0.6389970168471336, 'train/mask_loss': 0.8394254878163337, 'metrics/total_secs_per_batch': 6.160259962081909, 'metrics/data_secs_per_batch': 2.5813717365264894, '_timestamp': 1740958709.5110106}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 233 is less than current step: 499. Dropping entry: {'train/lr': 0.00015526530612244898, '_timestamp': 1740958709.5113795}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 234 is less than current step: 499. Dropping entry: {'train/loss': 1.7473064184188842, 'train/ce_loss': 0.531982421875, 'train/seg_cls_loss': 0.0174560546875, 'train/kl_loss': 0.264453125, 'train/mask_bce_loss': 0.14319061050191523, 'train/mask_dice_loss': 0.4468688488006592, 'train/mask_loss': 0.5900594592094421, 'metrics/total_secs_per_batch': 5.983435153961182, 'metrics/data_secs_per_batch': 2.6144835472106935, '_timestamp': 1740958715.4943142}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 234 is less than current step: 499. Dropping entry: {'train/lr': 0.0001551428571428571, '_timestamp': 1740958715.494625}).
Epoch: [2][236/500]	Time  6.587 ( 6.587)	Loss 0.1094 (1.3111)	CeLoss 0.1094 (0.2082)	SegCLSLoss 0.0000 (0.0141)	KLLoss 0.0000 (0.3090)	MaskLoss 0.0000 (0.5325)	MaskBCELoss 0.0000 (0.1036)	MaskDICELoss 0.0000 (0.4289)
Epoch: [2][237/500]	Time  5.239 ( 5.239)	Loss 1.7523 (0.9937)	CeLoss 0.1904 (0.5429)	SegCLSLoss 0.0178 (0.0062)	KLLoss 0.3965 (0.1588)	MaskLoss 0.7570 (0.2159)	MaskBCELoss 0.0462 (0.0342)	MaskDICELoss 0.7108 (0.1817)
Epoch: [2][238/500]	Time  5.523 ( 5.523)	Loss 2.3269 (1.8843)	CeLoss 0.3027 (0.3861)	SegCLSLoss 0.0179 (0.0185)	KLLoss 0.3867 (0.3061)	MaskLoss 0.9877 (0.7291)	MaskBCELoss 0.0678 (0.1363)	MaskDICELoss 0.9199 (0.5928)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 235 is less than current step: 499. Dropping entry: {'train/loss': 1.3111237585544586, 'train/ce_loss': 0.2082275390625, 'train/seg_cls_loss': 0.01414794921875, 'train/kl_loss': 0.308984375, 'train/mask_bce_loss': 0.10363203827291727, 'train/mask_dice_loss': 0.4288707494735718, 'train/mask_loss': 0.5325027912855148, 'metrics/total_secs_per_batch': 6.587141752243042, 'metrics/data_secs_per_batch': 2.9657575368881224, '_timestamp': 1740958722.0813637}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 235 is less than current step: 499. Dropping entry: {'train/lr': 0.00015502040816326527, '_timestamp': 1740958722.0816503}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 236 is less than current step: 499. Dropping entry: {'train/loss': 0.9936748534440994, 'train/ce_loss': 0.54287109375, 'train/seg_cls_loss': 0.006207275390625, 'train/kl_loss': 0.1587890625, 'train/mask_bce_loss': 0.03421421777456999, 'train/mask_dice_loss': 0.18166617304086685, 'train/mask_loss': 0.2158803954720497, 'metrics/total_secs_per_batch': 5.238775253295898, 'metrics/data_secs_per_batch': 2.3535937786102297, '_timestamp': 1740958727.320146}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 236 is less than current step: 499. Dropping entry: {'train/lr': 0.00015489795918367344, '_timestamp': 1740958727.3204758}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 237 is less than current step: 499. Dropping entry: {'train/loss': 1.8843316555023193, 'train/ce_loss': 0.3861328125, 'train/seg_cls_loss': 0.018505859375, 'train/kl_loss': 0.3060546875, 'train/mask_bce_loss': 0.1362929718568921, 'train/mask_dice_loss': 0.592786917090416, 'train/mask_loss': 0.7290798902511597, 'metrics/total_secs_per_batch': 5.52281928062439, 'metrics/data_secs_per_batch': 2.3959961891174317, '_timestamp': 1740958732.8429615}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 237 is less than current step: 499. Dropping entry: {'train/lr': 0.0001547755102040816, '_timestamp': 1740958732.8432384}).
Epoch: [2][239/500]	Time  5.122 ( 5.122)	Loss 3.3542 (2.0855)	CeLoss 0.3574 (0.3456)	SegCLSLoss 0.0176 (0.0240)	KLLoss 0.3711 (0.3441)	MaskLoss 1.4749 (0.8467)	MaskBCELoss 0.5526 (0.2836)	MaskDICELoss 0.9223 (0.5631)
[2025-03-02 17:39:03,948] [INFO] [logging.py:96:log_dist] [Rank 0] step=1240, skipped=0, lr=[0.00015459183673469386], mom=[(0.9, 0.95)]
[2025-03-02 17:39:03,953] [INFO] [timer.py:215:stop] epoch=0/micro_step=12400/global_step=1240, RunningAvgSamplesPerSec=1.5475743362508514, CurrSamplesPerSec=1.6701893134267356, MemAllocated=31.24GB, MaxMemAllocated=37.19GB
Epoch: [2][240/500]	Time  5.989 ( 5.989)	Loss 1.5888 (1.6781)	CeLoss 0.3340 (0.3723)	SegCLSLoss 0.0105 (0.0129)	KLLoss 0.3965 (0.2748)	MaskLoss 0.6049 (0.6360)	MaskBCELoss 0.3602 (0.1765)	MaskDICELoss 0.2447 (0.4596)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 238 is less than current step: 499. Dropping entry: {'train/loss': 2.08552405834198, 'train/ce_loss': 0.34560546875, 'train/seg_cls_loss': 0.0239501953125, 'train/kl_loss': 0.344140625, 'train/mask_bce_loss': 0.2836445145308971, 'train/mask_dice_loss': 0.5630726113915443, 'train/mask_loss': 0.84671710729599, 'metrics/total_secs_per_batch': 5.1223390102386475, 'metrics/data_secs_per_batch': 2.178692364692688, '_timestamp': 1740958737.9653094}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 238 is less than current step: 499. Dropping entry: {'train/lr': 0.00015465306122448976, '_timestamp': 1740958737.9656432}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 239 is less than current step: 499. Dropping entry: {'train/loss': 1.6781266450881958, 'train/ce_loss': 0.372265625, 'train/seg_cls_loss': 0.012933349609375, 'train/kl_loss': 0.2748046875, 'train/mask_bce_loss': 0.17648250088095666, 'train/mask_dice_loss': 0.4595534667372704, 'train/mask_loss': 0.6360359638929367, 'metrics/total_secs_per_batch': 5.988966226577759, 'metrics/data_secs_per_batch': 2.45884530544281, '_timestamp': 1740958743.9540646}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 239 is less than current step: 499. Dropping entry: {'train/lr': 0.00015453061224489793, '_timestamp': 1740958743.9543347}).
Epoch: [2][241/500]	Time  6.185 ( 6.185)	Loss 1.6370 (1.3267)	CeLoss 0.3203 (0.4395)	SegCLSLoss 0.0141 (0.0113)	KLLoss 0.4004 (0.1965)	MaskLoss 0.6349 (0.4309)	MaskBCELoss 0.1026 (0.0632)	MaskDICELoss 0.5323 (0.3677)
Epoch: [2][242/500]	Time  6.072 ( 6.072)	Loss 1.5122 (1.6036)	CeLoss 0.3496 (0.4165)	SegCLSLoss 0.0120 (0.0140)	KLLoss 0.4180 (0.2770)	MaskLoss 0.5579 (0.5763)	MaskBCELoss 0.1627 (0.1181)	MaskDICELoss 0.3952 (0.4582)
Epoch: [2][243/500]	Time  5.490 ( 5.490)	Loss 0.7496 (1.6104)	CeLoss 0.2178 (0.3013)	SegCLSLoss 0.0176 (0.0152)	KLLoss 0.4180 (0.3621)	MaskLoss 0.2410 (0.6327)	MaskBCELoss 0.1466 (0.1339)	MaskDICELoss 0.0944 (0.4987)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 240 is less than current step: 499. Dropping entry: {'train/loss': 1.3266828060150146, 'train/ce_loss': 0.4394775390625, 'train/seg_cls_loss': 0.011273193359375, 'train/kl_loss': 0.196484375, 'train/mask_bce_loss': 0.06318846885114908, 'train/mask_dice_loss': 0.3677188366651535, 'train/mask_loss': 0.43090730905532837, 'metrics/total_secs_per_batch': 6.185035467147827, 'metrics/data_secs_per_batch': 2.7917632341384886, '_timestamp': 1740958750.1392999}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 240 is less than current step: 499. Dropping entry: {'train/lr': 0.0001544081632653061, '_timestamp': 1740958750.1396503}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 241 is less than current step: 499. Dropping entry: {'train/loss': 1.6036314368247986, 'train/ce_loss': 0.416455078125, 'train/seg_cls_loss': 0.014044189453125, 'train/kl_loss': 0.276953125, 'train/mask_bce_loss': 0.11809486038982868, 'train/mask_dice_loss': 0.4581837296485901, 'train/mask_loss': 0.5762785971164703, 'metrics/total_secs_per_batch': 6.071924686431885, 'metrics/data_secs_per_batch': 2.7064747333526613, '_timestamp': 1740958756.2113733}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 241 is less than current step: 499. Dropping entry: {'train/lr': 0.00015428571428571425, '_timestamp': 1740958756.211775}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 242 is less than current step: 499. Dropping entry: {'train/loss': 1.6103606522083282, 'train/ce_loss': 0.30126953125, 'train/seg_cls_loss': 0.015167236328125, 'train/kl_loss': 0.362109375, 'train/mask_bce_loss': 0.13392936252057552, 'train/mask_dice_loss': 0.498741215467453, 'train/mask_loss': 0.6326705813407898, 'metrics/total_secs_per_batch': 5.490202188491821, 'metrics/data_secs_per_batch': 2.776293158531189, '_timestamp': 1740958761.701581}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 242 is less than current step: 499. Dropping entry: {'train/lr': 0.00015416326530612244, '_timestamp': 1740958761.7019072}).
Epoch: [2][244/500]	Time  6.999 ( 6.999)	Loss 1.4688 (2.1119)	CeLoss 1.4688 (0.3634)	SegCLSLoss 0.0000 (0.0208)	KLLoss 0.0000 (0.3563)	MaskLoss 0.0000 (0.8510)	MaskBCELoss 0.0000 (0.1377)	MaskDICELoss 0.0000 (0.7133)
Epoch: [2][245/500]	Time  5.185 ( 5.185)	Loss 2.3116 (1.8434)	CeLoss 0.2334 (0.6167)	SegCLSLoss 0.0271 (0.0109)	KLLoss 0.3867 (0.2377)	MaskLoss 1.0132 (0.5988)	MaskBCELoss 0.0201 (0.1245)	MaskDICELoss 0.9931 (0.4743)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 243 is less than current step: 499. Dropping entry: {'train/loss': 2.111929881572723, 'train/ce_loss': 0.36337890625, 'train/seg_cls_loss': 0.0207763671875, 'train/kl_loss': 0.35625, 'train/mask_bce_loss': 0.13770276363939046, 'train/mask_dice_loss': 0.7132817000150681, 'train/mask_loss': 0.8509844660758972, 'metrics/total_secs_per_batch': 6.99931263923645, 'metrics/data_secs_per_batch': 3.370104455947876, '_timestamp': 1740958768.7007828}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 243 is less than current step: 499. Dropping entry: {'train/lr': 0.0001540408163265306, '_timestamp': 1740958768.7009923}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 244 is less than current step: 499. Dropping entry: {'train/loss': 1.8434448957443237, 'train/ce_loss': 0.61669921875, 'train/seg_cls_loss': 0.010888671875, 'train/kl_loss': 0.2376953125, 'train/mask_bce_loss': 0.12447207365185023, 'train/mask_dice_loss': 0.474301153421402, 'train/mask_loss': 0.5987732172012329, 'metrics/total_secs_per_batch': 5.185118675231934, 'metrics/data_secs_per_batch': 2.3900745153427123, '_timestamp': 1740958773.8858414}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 244 is less than current step: 499. Dropping entry: {'train/lr': 0.00015391836734693877, '_timestamp': 1740958773.8861191}).
Epoch: [2][246/500]	Time  5.372 ( 5.372)	Loss 1.3906 (1.6778)	CeLoss 1.3906 (0.5485)	SegCLSLoss 0.0000 (0.0145)	KLLoss 0.0000 (0.2750)	MaskLoss 0.0000 (0.5473)	MaskBCELoss 0.0000 (0.1287)	MaskDICELoss 0.0000 (0.4185)
Epoch: [2][247/500]	Time  6.373 ( 6.373)	Loss 1.9210 (1.8123)	CeLoss 0.2949 (0.2826)	SegCLSLoss 0.0156 (0.0180)	KLLoss 0.4062 (0.3549)	MaskLoss 0.7886 (0.7425)	MaskBCELoss 0.2028 (0.1508)	MaskDICELoss 0.5858 (0.5917)
Epoch: [2][248/500]	Time  5.928 ( 5.928)	Loss 1.8433 (1.9831)	CeLoss 0.2090 (0.4819)	SegCLSLoss 0.0298 (0.0177)	KLLoss 0.3809 (0.3162)	MaskLoss 0.7908 (0.7304)	MaskBCELoss 0.1305 (0.1364)	MaskDICELoss 0.6603 (0.5940)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 245 is less than current step: 499. Dropping entry: {'train/loss': 1.6777653992176056, 'train/ce_loss': 0.54853515625, 'train/seg_cls_loss': 0.01448974609375, 'train/kl_loss': 0.275, 'train/mask_bce_loss': 0.12873968556523324, 'train/mask_dice_loss': 0.4185414478182793, 'train/mask_loss': 0.5472811311483383, 'metrics/total_secs_per_batch': 5.37190580368042, 'metrics/data_secs_per_batch': 2.745801854133606, '_timestamp': 1740958779.257777}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 245 is less than current step: 499. Dropping entry: {'train/lr': 0.00015379591836734693, '_timestamp': 1740958779.2580533}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 246 is less than current step: 499. Dropping entry: {'train/loss': 1.8123277962207793, 'train/ce_loss': 0.2826171875, 'train/seg_cls_loss': 0.017999267578125, 'train/kl_loss': 0.3548828125, 'train/mask_bce_loss': 0.15083958394825459, 'train/mask_dice_loss': 0.5917012691497803, 'train/mask_loss': 0.7425408452749253, 'metrics/total_secs_per_batch': 6.372750759124756, 'metrics/data_secs_per_batch': 2.79368793964386, '_timestamp': 1740958785.630583}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 246 is less than current step: 499. Dropping entry: {'train/lr': 0.0001536734693877551, '_timestamp': 1740958785.630865}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 247 is less than current step: 499. Dropping entry: {'train/loss': 1.983118510246277, 'train/ce_loss': 0.48193359375, 'train/seg_cls_loss': 0.01766357421875, 'train/kl_loss': 0.3162109375, 'train/mask_bce_loss': 0.13640845771878957, 'train/mask_dice_loss': 0.5940180033445358, 'train/mask_loss': 0.7304264545440674, 'metrics/total_secs_per_batch': 5.9276816844940186, 'metrics/data_secs_per_batch': 3.014806294441223, '_timestamp': 1740958791.558224}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 247 is less than current step: 499. Dropping entry: {'train/lr': 0.00015355102040816326, '_timestamp': 1740958791.558506}).
Epoch: [2][249/500]	Time  6.246 ( 6.246)	Loss 1.3747 (1.6447)	CeLoss 0.3184 (0.2958)	SegCLSLoss 0.0104 (0.0148)	KLLoss 0.4160 (0.3230)	MaskLoss 0.5047 (0.6547)	MaskBCELoss 0.0190 (0.1277)	MaskDICELoss 0.4858 (0.5270)
[2025-03-02 17:40:03,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=1250, skipped=0, lr=[0.00015336734693877552], mom=[(0.9, 0.95)]
[2025-03-02 17:40:03,618] [INFO] [timer.py:215:stop] epoch=0/micro_step=12500/global_step=1250, RunningAvgSamplesPerSec=1.5485276036814977, CurrSamplesPerSec=1.7199587649885244, MemAllocated=31.3GB, MaxMemAllocated=37.19GB
Epoch: [2][250/500]	Time  5.816 ( 5.816)	Loss 2.6511 (1.7344)	CeLoss 0.1699 (0.4721)	SegCLSLoss 0.0265 (0.0159)	KLLoss 0.3711 (0.2773)	MaskLoss 1.2152 (0.6134)	MaskBCELoss 0.3966 (0.1942)	MaskDICELoss 0.8186 (0.4191)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 248 is less than current step: 499. Dropping entry: {'train/loss': 1.6446543693542481, 'train/ce_loss': 0.295751953125, 'train/seg_cls_loss': 0.014825439453125, 'train/kl_loss': 0.323046875, 'train/mask_bce_loss': 0.1277124272659421, 'train/mask_dice_loss': 0.5270122081041336, 'train/mask_loss': 0.6547246307134629, 'metrics/total_secs_per_batch': 6.245653390884399, 'metrics/data_secs_per_batch': 3.013113236427307, '_timestamp': 1740958797.8038487}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 248 is less than current step: 499. Dropping entry: {'train/lr': 0.00015342857142857142, '_timestamp': 1740958797.804111}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 249 is less than current step: 499. Dropping entry: {'train/loss': 1.7343889713287353, 'train/ce_loss': 0.472119140625, 'train/seg_cls_loss': 0.015948486328125, 'train/kl_loss': 0.27734375, 'train/mask_bce_loss': 0.19424806386232377, 'train/mask_dice_loss': 0.41911340653896334, 'train/mask_loss': 0.6133614778518677, 'metrics/total_secs_per_batch': 5.81558084487915, 'metrics/data_secs_per_batch': 2.362500309944153, '_timestamp': 1740958803.6192672}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 249 is less than current step: 499. Dropping entry: {'train/lr': 0.00015330612244897958, '_timestamp': 1740958803.6195343}).
Epoch: [2][251/500]	Time  5.661 ( 5.661)	Loss 1.1719 (1.5038)	CeLoss 1.1719 (0.4068)	SegCLSLoss 0.0000 (0.0141)	KLLoss 0.0000 (0.2781)	MaskLoss 0.0000 (0.5311)	MaskBCELoss 0.0000 (0.1052)	MaskDICELoss 0.0000 (0.4260)
Epoch: [2][252/500]	Time  6.199 ( 6.199)	Loss 1.4189 (1.4157)	CeLoss 0.2500 (0.5429)	SegCLSLoss 0.0245 (0.0108)	KLLoss 0.3965 (0.2441)	MaskLoss 0.5590 (0.4215)	MaskBCELoss 0.2233 (0.0977)	MaskDICELoss 0.3358 (0.3238)
Epoch: [2][253/500]	Time  5.735 ( 5.735)	Loss 1.9525 (1.8802)	CeLoss 0.1992 (0.5308)	SegCLSLoss 0.0221 (0.0174)	KLLoss 0.4004 (0.2734)	MaskLoss 0.8513 (0.6567)	MaskBCELoss 0.0459 (0.0771)	MaskDICELoss 0.8053 (0.5796)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 250 is less than current step: 499. Dropping entry: {'train/loss': 1.5037951588630676, 'train/ce_loss': 0.4068359375, 'train/seg_cls_loss': 0.014141845703125, 'train/kl_loss': 0.278125, 'train/mask_bce_loss': 0.1051683658733964, 'train/mask_dice_loss': 0.42597725987434387, 'train/mask_loss': 0.5311456233263016, 'metrics/total_secs_per_batch': 5.661261081695557, 'metrics/data_secs_per_batch': 2.6015212297439576, '_timestamp': 1740958809.2807474}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 250 is less than current step: 499. Dropping entry: {'train/lr': 0.00015318367346938775, '_timestamp': 1740958809.2810314}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 251 is less than current step: 499. Dropping entry: {'train/loss': 1.415676200389862, 'train/ce_loss': 0.54287109375, 'train/seg_cls_loss': 0.010797119140625, 'train/kl_loss': 0.244140625, 'train/mask_bce_loss': 0.0977371621876955, 'train/mask_dice_loss': 0.3237728029489517, 'train/mask_loss': 0.42150997519493105, 'metrics/total_secs_per_batch': 6.199391603469849, 'metrics/data_secs_per_batch': 2.4679641008377073, '_timestamp': 1740958815.4800947}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 251 is less than current step: 499. Dropping entry: {'train/lr': 0.0001530612244897959, '_timestamp': 1740958815.4803598}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 252 is less than current step: 499. Dropping entry: {'train/loss': 1.8801844239234924, 'train/ce_loss': 0.53076171875, 'train/seg_cls_loss': 0.017425537109375, 'train/kl_loss': 0.2734375, 'train/mask_bce_loss': 0.07711260076612234, 'train/mask_dice_loss': 0.5795811533927917, 'train/mask_loss': 0.6566937565803528, 'metrics/total_secs_per_batch': 5.734652280807495, 'metrics/data_secs_per_batch': 2.479122018814087, '_timestamp': 1740958821.2150228}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 252 is less than current step: 499. Dropping entry: {'train/lr': 0.00015293877551020407, '_timestamp': 1740958821.215381}).
Epoch: [2][254/500]	Time  6.436 ( 6.436)	Loss 2.7829 (1.9561)	CeLoss 0.1758 (0.2864)	SegCLSLoss 0.0325 (0.0164)	KLLoss 0.3887 (0.3645)	MaskLoss 1.2757 (0.8124)	MaskBCELoss 0.3623 (0.1704)	MaskDICELoss 0.9134 (0.6420)
Epoch: [2][255/500]	Time  5.267 ( 5.267)	Loss 0.8711 (1.9399)	CeLoss 0.8711 (0.4874)	SegCLSLoss 0.0000 (0.0151)	KLLoss 0.0000 (0.2770)	MaskLoss 0.0000 (0.7086)	MaskBCELoss 0.0000 (0.2007)	MaskDICELoss 0.0000 (0.5079)
Epoch: [2][256/500]	Time  5.351 ( 5.351)	Loss 1.3828 (1.3671)	CeLoss 1.3828 (0.3575)	SegCLSLoss 0.0000 (0.0186)	KLLoss 0.0000 (0.3256)	MaskLoss 0.0000 (0.4838)	MaskBCELoss 0.0000 (0.0932)	MaskDICELoss 0.0000 (0.3907)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 253 is less than current step: 499. Dropping entry: {'train/loss': 1.956092119216919, 'train/ce_loss': 0.28642578125, 'train/seg_cls_loss': 0.01636962890625, 'train/kl_loss': 0.364453125, 'train/mask_bce_loss': 0.17042214013636112, 'train/mask_dice_loss': 0.6419989138841629, 'train/mask_loss': 0.8124210357666015, 'metrics/total_secs_per_batch': 6.435909032821655, 'metrics/data_secs_per_batch': 3.1352961540222166, '_timestamp': 1740958827.650756}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 253 is less than current step: 499. Dropping entry: {'train/lr': 0.00015281632653061224, '_timestamp': 1740958827.6511352}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 254 is less than current step: 499. Dropping entry: {'train/loss': 1.9399201989173889, 'train/ce_loss': 0.48740234375, 'train/seg_cls_loss': 0.01513671875, 'train/kl_loss': 0.276953125, 'train/mask_bce_loss': 0.20068422928452492, 'train/mask_dice_loss': 0.5078989088535308, 'train/mask_loss': 0.7085831463336945, 'metrics/total_secs_per_batch': 5.266880750656128, 'metrics/data_secs_per_batch': 2.1542680740356444, '_timestamp': 1740958832.9176}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 254 is less than current step: 499. Dropping entry: {'train/lr': 0.0001526938775510204, '_timestamp': 1740958832.9179125}).
Epoch: [2][257/500]	Time  6.276 ( 6.276)	Loss 2.2577 (1.8026)	CeLoss 0.2070 (0.2669)	SegCLSLoss 0.0204 (0.0190)	KLLoss 0.4219 (0.3619)	MaskLoss 0.9990 (0.7451)	MaskBCELoss 0.3385 (0.1981)	MaskDICELoss 0.6604 (0.5470)
Epoch: [2][258/500]	Time  6.140 ( 6.140)	Loss 1.6350 (1.5990)	CeLoss 0.2363 (0.4867)	SegCLSLoss 0.0126 (0.0139)	KLLoss 0.4277 (0.2828)	MaskLoss 0.6749 (0.5386)	MaskBCELoss 0.1513 (0.0910)	MaskDICELoss 0.5236 (0.4476)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 255 is less than current step: 499. Dropping entry: {'train/loss': 1.367110002040863, 'train/ce_loss': 0.35751953125, 'train/seg_cls_loss': 0.0185791015625, 'train/kl_loss': 0.3255859375, 'train/mask_bce_loss': 0.09317232756875456, 'train/mask_dice_loss': 0.3906756341457367, 'train/mask_loss': 0.48384796530008317, 'metrics/total_secs_per_batch': 5.351128578186035, 'metrics/data_secs_per_batch': 2.2880692958831785, '_timestamp': 1740958838.2686267}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 255 is less than current step: 499. Dropping entry: {'train/lr': 0.00015257142857142856, '_timestamp': 1740958838.268806}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 256 is less than current step: 499. Dropping entry: {'train/loss': 1.8026384472846986, 'train/ce_loss': 0.26689453125, 'train/seg_cls_loss': 0.01898193359375, 'train/kl_loss': 0.3619140625, 'train/mask_bce_loss': 0.198079277202487, 'train/mask_dice_loss': 0.5469899505376816, 'train/mask_loss': 0.7450692236423493, 'metrics/total_secs_per_batch': 6.275557994842529, 'metrics/data_secs_per_batch': 2.839829611778259, '_timestamp': 1740958844.544271}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 256 is less than current step: 499. Dropping entry: {'train/lr': 0.00015244897959183673, '_timestamp': 1740958844.5445526}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 257 is less than current step: 499. Dropping entry: {'train/loss': 1.5989596128463746, 'train/ce_loss': 0.48671875, 'train/seg_cls_loss': 0.0138916015625, 'train/kl_loss': 0.2828125, 'train/mask_bce_loss': 0.09103701747953892, 'train/mask_dice_loss': 0.44755411744117735, 'train/mask_loss': 0.5385911345481873, 'metrics/total_secs_per_batch': 6.140357971191406, 'metrics/data_secs_per_batch': 2.69345223903656, '_timestamp': 1740958850.6848154}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 257 is less than current step: 499. Dropping entry: {'train/lr': 0.0001523265306122449, '_timestamp': 1740958850.6852076}).
Epoch: [2][259/500]	Time  7.058 ( 7.058)	Loss 2.5233 (2.0838)	CeLoss 0.2363 (0.2300)	SegCLSLoss 0.0201 (0.0206)	KLLoss 0.3789 (0.3990)	MaskLoss 1.1200 (0.9020)	MaskBCELoss 0.3572 (0.2214)	MaskDICELoss 0.7628 (0.6806)
[2025-03-02 17:41:03,399] [INFO] [logging.py:96:log_dist] [Rank 0] step=1260, skipped=0, lr=[0.00015214285714285712], mom=[(0.9, 0.95)]
[2025-03-02 17:41:03,405] [INFO] [timer.py:215:stop] epoch=0/micro_step=12600/global_step=1260, RunningAvgSamplesPerSec=1.549443746212698, CurrSamplesPerSec=1.7663218588006777, MemAllocated=31.24GB, MaxMemAllocated=37.19GB
Epoch: [2][260/500]	Time  5.663 ( 5.663)	Loss 1.5268 (1.5262)	CeLoss 0.2275 (0.4635)	SegCLSLoss 0.0139 (0.0163)	KLLoss 0.4336 (0.2410)	MaskLoss 0.6247 (0.5155)	MaskBCELoss 0.0903 (0.0789)	MaskDICELoss 0.5344 (0.4366)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 258 is less than current step: 499. Dropping entry: {'train/loss': 2.0838107585906984, 'train/ce_loss': 0.22998046875, 'train/seg_cls_loss': 0.020556640625, 'train/kl_loss': 0.3990234375, 'train/mask_bce_loss': 0.2214343070052564, 'train/mask_dice_loss': 0.680578488111496, 'train/mask_loss': 0.9020127952098846, 'metrics/total_secs_per_batch': 7.0584557056427, 'metrics/data_secs_per_batch': 3.12074294090271, '_timestamp': 1740958857.7432747}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 258 is less than current step: 499. Dropping entry: {'train/lr': 0.00015220408163265305, '_timestamp': 1740958857.7436392}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 259 is less than current step: 499. Dropping entry: {'train/loss': 1.5262498557567596, 'train/ce_loss': 0.4634765625, 'train/seg_cls_loss': 0.01629638671875, 'train/kl_loss': 0.241015625, 'train/mask_bce_loss': 0.0788559965789318, 'train/mask_dice_loss': 0.43661269545555115, 'train/mask_loss': 0.5154686897993088, 'metrics/total_secs_per_batch': 5.66333794593811, 'metrics/data_secs_per_batch': 2.5565940618515013, '_timestamp': 1740958863.4062426}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 259 is less than current step: 499. Dropping entry: {'train/lr': 0.00015208163265306121, '_timestamp': 1740958863.4065187}).
Epoch: [2][261/500]	Time  5.565 ( 5.565)	Loss 1.9186 (2.0643)	CeLoss 0.5742 (0.3874)	SegCLSLoss 0.0141 (0.0199)	KLLoss 0.4004 (0.3557)	MaskLoss 0.6488 (0.8157)	MaskBCELoss 0.1911 (0.2096)	MaskDICELoss 0.4577 (0.6061)
Epoch: [2][262/500]	Time  5.991 ( 5.991)	Loss 1.2109 (1.7785)	CeLoss 1.2109 (0.3427)	SegCLSLoss 0.0000 (0.0189)	KLLoss 0.0000 (0.3203)	MaskLoss 0.0000 (0.6971)	MaskBCELoss 0.0000 (0.1573)	MaskDICELoss 0.0000 (0.5398)
Epoch: [2][263/500]	Time  5.210 ( 5.210)	Loss 1.7344 (1.6152)	CeLoss 1.7344 (0.6337)	SegCLSLoss 0.0000 (0.0153)	KLLoss 0.0000 (0.2346)	MaskLoss 0.0000 (0.4752)	MaskBCELoss 0.0000 (0.0949)	MaskDICELoss 0.0000 (0.3803)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 260 is less than current step: 499. Dropping entry: {'train/loss': 2.064341962337494, 'train/ce_loss': 0.38740234375, 'train/seg_cls_loss': 0.01986083984375, 'train/kl_loss': 0.3556640625, 'train/mask_bce_loss': 0.20958065753802657, 'train/mask_dice_loss': 0.6061352550983429, 'train/mask_loss': 0.815715903043747, 'metrics/total_secs_per_batch': 5.564743995666504, 'metrics/data_secs_per_batch': 2.380427432060242, '_timestamp': 1740958868.971289}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 260 is less than current step: 499. Dropping entry: {'train/lr': 0.00015195918367346938, '_timestamp': 1740958868.9716017}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 261 is less than current step: 499. Dropping entry: {'train/loss': 1.7784692406654359, 'train/ce_loss': 0.34267578125, 'train/seg_cls_loss': 0.0188720703125, 'train/kl_loss': 0.3203125, 'train/mask_bce_loss': 0.15734187439084052, 'train/mask_dice_loss': 0.5398029059171676, 'train/mask_loss': 0.6971447765827179, 'metrics/total_secs_per_batch': 5.990699052810669, 'metrics/data_secs_per_batch': 3.029043173789978, '_timestamp': 1740958874.962063}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 261 is less than current step: 499. Dropping entry: {'train/lr': 0.00015183673469387751, '_timestamp': 1740958874.9624176}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 262 is less than current step: 499. Dropping entry: {'train/loss': 1.6151508033275603, 'train/ce_loss': 0.63369140625, 'train/seg_cls_loss': 0.015252685546875, 'train/kl_loss': 0.2345703125, 'train/mask_bce_loss': 0.09489402696490287, 'train/mask_dice_loss': 0.3803083211183548, 'train/mask_loss': 0.4752023369073868, 'metrics/total_secs_per_batch': 5.209789037704468, 'metrics/data_secs_per_batch': 2.5893813610076903, '_timestamp': 1740958880.1717558}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 262 is less than current step: 499. Dropping entry: {'train/lr': 0.00015171428571428568, '_timestamp': 1740958880.1720567}).
Epoch: [2][264/500]	Time  5.698 ( 5.698)	Loss 2.3856 (1.5201)	CeLoss 0.2695 (0.3061)	SegCLSLoss 0.0199 (0.0143)	KLLoss 0.4062 (0.3281)	MaskLoss 1.0326 (0.5870)	MaskBCELoss 0.0664 (0.1103)	MaskDICELoss 0.9663 (0.4767)
Epoch: [2][265/500]	Time  5.603 ( 5.603)	Loss 1.5729 (1.3164)	CeLoss 0.2373 (0.3905)	SegCLSLoss 0.0140 (0.0106)	KLLoss 0.4141 (0.1998)	MaskLoss 0.6439 (0.4503)	MaskBCELoss 0.2335 (0.1646)	MaskDICELoss 0.4104 (0.2857)
Epoch: [2][266/500]	Time  6.610 ( 6.610)	Loss 1.5860 (1.6381)	CeLoss 0.2949 (0.3460)	SegCLSLoss 0.0128 (0.0166)	KLLoss 0.4102 (0.3150)	MaskLoss 0.6221 (0.6262)	MaskBCELoss 0.1589 (0.0765)	MaskDICELoss 0.4632 (0.5497)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 263 is less than current step: 499. Dropping entry: {'train/loss': 1.5201053261756896, 'train/ce_loss': 0.306103515625, 'train/seg_cls_loss': 0.014349365234375, 'train/kl_loss': 0.328125, 'train/mask_bce_loss': 0.11033091712743044, 'train/mask_dice_loss': 0.4766504615545273, 'train/mask_loss': 0.5869813710451126, 'metrics/total_secs_per_batch': 5.6982996463775635, 'metrics/data_secs_per_batch': 2.5867023944854735, '_timestamp': 1740958885.8700109}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 263 is less than current step: 499. Dropping entry: {'train/lr': 0.00015159183673469384, '_timestamp': 1740958885.8703551}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 264 is less than current step: 499. Dropping entry: {'train/loss': 1.3164420247077941, 'train/ce_loss': 0.39052734375, 'train/seg_cls_loss': 0.010614013671875, 'train/kl_loss': 0.1998046875, 'train/mask_bce_loss': 0.16462046280503273, 'train/mask_dice_loss': 0.2856903880834579, 'train/mask_loss': 0.45031085014343264, 'metrics/total_secs_per_batch': 5.603363037109375, 'metrics/data_secs_per_batch': 2.5813381910324096, '_timestamp': 1740958891.4733632}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 264 is less than current step: 499. Dropping entry: {'train/lr': 0.000151469387755102, '_timestamp': 1740958891.4736414}).
Epoch: [2][267/500]	Time  5.982 ( 5.982)	Loss 1.3047 (1.8420)	CeLoss 1.3047 (0.4346)	SegCLSLoss 0.0000 (0.0155)	KLLoss 0.0000 (0.2754)	MaskLoss 0.0000 (0.6861)	MaskBCELoss 0.0000 (0.1209)	MaskDICELoss 0.0000 (0.5652)
Epoch: [2][268/500]	Time  5.127 ( 5.127)	Loss 1.8603 (2.0797)	CeLoss 0.2383 (0.3895)	SegCLSLoss 0.0126 (0.0171)	KLLoss 0.4141 (0.3205)	MaskLoss 0.7876 (0.8248)	MaskBCELoss 0.0028 (0.2501)	MaskDICELoss 0.7848 (0.5747)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 265 is less than current step: 499. Dropping entry: {'train/loss': 1.63813014626503, 'train/ce_loss': 0.34599609375, 'train/seg_cls_loss': 0.016595458984375, 'train/kl_loss': 0.3150390625, 'train/mask_bce_loss': 0.07650607191026211, 'train/mask_dice_loss': 0.5496878981590271, 'train/mask_loss': 0.6261939615011215, 'metrics/total_secs_per_batch': 6.610252618789673, 'metrics/data_secs_per_batch': 3.1426404476165772, '_timestamp': 1740958898.0836225}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 265 is less than current step: 499. Dropping entry: {'train/lr': 0.00015134693877551022, '_timestamp': 1740958898.0839431}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 266 is less than current step: 499. Dropping entry: {'train/loss': 1.8419940710067748, 'train/ce_loss': 0.4345703125, 'train/seg_cls_loss': 0.015545654296875, 'train/kl_loss': 0.275390625, 'train/mask_bce_loss': 0.12092242287471891, 'train/mask_dice_loss': 0.5651624917984008, 'train/mask_loss': 0.686084908246994, 'metrics/total_secs_per_batch': 5.981581687927246, 'metrics/data_secs_per_batch': 2.888845157623291, '_timestamp': 1740958904.0651684}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 266 is less than current step: 499. Dropping entry: {'train/lr': 0.00015122448979591836, '_timestamp': 1740958904.0654497}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 267 is less than current step: 499. Dropping entry: {'train/loss': 2.079744303226471, 'train/ce_loss': 0.389501953125, 'train/seg_cls_loss': 0.017132568359375, 'train/kl_loss': 0.3205078125, 'train/mask_bce_loss': 0.2500637393211946, 'train/mask_dice_loss': 0.5747205212712287, 'train/mask_loss': 0.8247842639684677, 'metrics/total_secs_per_batch': 5.126941680908203, 'metrics/data_secs_per_batch': 2.64842689037323, '_timestamp': 1740958909.192395}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 267 is less than current step: 499. Dropping entry: {'train/lr': 0.00015110204081632652, '_timestamp': 1740958909.1927645}).
Epoch: [2][269/500]	Time  7.026 ( 7.026)	Loss 2.3747 (2.0545)	CeLoss 0.1152 (0.3185)	SegCLSLoss 0.0479 (0.0235)	KLLoss 0.3691 (0.3484)	MaskLoss 1.0994 (0.8448)	MaskBCELoss 0.2646 (0.1683)	MaskDICELoss 0.8348 (0.6765)
[2025-03-02 17:42:02,066] [INFO] [logging.py:96:log_dist] [Rank 0] step=1270, skipped=0, lr=[0.00015091836734693878], mom=[(0.9, 0.95)]
[2025-03-02 17:42:02,072] [INFO] [timer.py:215:stop] epoch=0/micro_step=12700/global_step=1270, RunningAvgSamplesPerSec=1.5505589534081707, CurrSamplesPerSec=1.7083980045858649, MemAllocated=31.25GB, MaxMemAllocated=37.19GB
Epoch: [2][270/500]	Time  5.855 ( 5.855)	Loss 1.3374 (1.7784)	CeLoss 0.2871 (0.5467)	SegCLSLoss 0.0126 (0.0122)	KLLoss 0.4121 (0.2807)	MaskLoss 0.5017 (0.5987)	MaskBCELoss 0.0697 (0.0682)	MaskDICELoss 0.4320 (0.5305)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 268 is less than current step: 499. Dropping entry: {'train/loss': 2.054466390609741, 'train/ce_loss': 0.31845703125, 'train/seg_cls_loss': 0.0235107421875, 'train/kl_loss': 0.3484375, 'train/mask_bce_loss': 0.16827276088297366, 'train/mask_dice_loss': 0.6764897406101227, 'train/mask_loss': 0.8447625041007996, 'metrics/total_secs_per_batch': 7.025747537612915, 'metrics/data_secs_per_batch': 2.9421255588531494, '_timestamp': 1740958916.217918}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 268 is less than current step: 499. Dropping entry: {'train/lr': 0.00015097959183673468, '_timestamp': 1740958916.2182264}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 269 is less than current step: 499. Dropping entry: {'train/loss': 1.778404450416565, 'train/ce_loss': 0.5466796875, 'train/seg_cls_loss': 0.012213134765625, 'train/kl_loss': 0.2806640625, 'train/mask_bce_loss': 0.06817945316433907, 'train/mask_dice_loss': 0.530544251203537, 'train/mask_loss': 0.598723703622818, 'metrics/total_secs_per_batch': 5.855053901672363, 'metrics/data_secs_per_batch': 2.328659772872925, '_timestamp': 1740958922.0727441}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 269 is less than current step: 499. Dropping entry: {'train/lr': 0.00015085714285714285, '_timestamp': 1740958922.0730176}).
Epoch: [2][271/500]	Time  7.271 ( 7.271)	Loss 2.6787 (1.8281)	CeLoss 0.2412 (0.1971)	SegCLSLoss 0.0203 (0.0188)	KLLoss 0.3750 (0.3070)	MaskLoss 1.1948 (0.7955)	MaskBCELoss 0.2697 (0.1756)	MaskDICELoss 0.9251 (0.6199)
Epoch: [2][272/500]	Time  6.115 ( 6.115)	Loss 2.6802 (2.0231)	CeLoss 0.3867 (0.3900)	SegCLSLoss 0.0127 (0.0172)	KLLoss 0.4004 (0.3557)	MaskLoss 1.1233 (0.7946)	MaskBCELoss 0.3138 (0.1525)	MaskDICELoss 0.8095 (0.6421)
Epoch: [2][273/500]	Time  5.480 ( 5.480)	Loss 3.2021 (1.7011)	CeLoss 1.0938 (0.4438)	SegCLSLoss 0.0154 (0.0166)	KLLoss 0.3906 (0.2730)	MaskLoss 1.0307 (0.6108)	MaskBCELoss 0.0308 (0.1322)	MaskDICELoss 0.9999 (0.4786)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 270 is less than current step: 499. Dropping entry: {'train/loss': 1.8281484246253967, 'train/ce_loss': 0.197119140625, 'train/seg_cls_loss': 0.01881103515625, 'train/kl_loss': 0.30703125, 'train/mask_bce_loss': 0.1755565796047449, 'train/mask_dice_loss': 0.6199385166168213, 'train/mask_loss': 0.7954951107501984, 'metrics/total_secs_per_batch': 7.270879745483398, 'metrics/data_secs_per_batch': 3.1724729537963867, '_timestamp': 1740958929.3438666}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 270 is less than current step: 499. Dropping entry: {'train/lr': 0.000150734693877551, '_timestamp': 1740958929.3441575}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 271 is less than current step: 499. Dropping entry: {'train/loss': 2.0231305599212646, 'train/ce_loss': 0.3900390625, 'train/seg_cls_loss': 0.017230224609375, 'train/kl_loss': 0.3556640625, 'train/mask_bce_loss': 0.15250350087881087, 'train/mask_dice_loss': 0.6420695900917053, 'train/mask_loss': 0.7945730924606323, 'metrics/total_secs_per_batch': 6.114920616149902, 'metrics/data_secs_per_batch': 2.8446204900741576, '_timestamp': 1740958935.45873}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 271 is less than current step: 499. Dropping entry: {'train/lr': 0.00015061224489795917, '_timestamp': 1740958935.459019}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 272 is less than current step: 499. Dropping entry: {'train/loss': 1.7010587573051452, 'train/ce_loss': 0.443798828125, 'train/seg_cls_loss': 0.0166259765625, 'train/kl_loss': 0.273046875, 'train/mask_bce_loss': 0.13222897052764893, 'train/mask_dice_loss': 0.4786031484603882, 'train/mask_loss': 0.6108321130275727, 'metrics/total_secs_per_batch': 5.479841470718384, 'metrics/data_secs_per_batch': 2.317430019378662, '_timestamp': 1740958940.93888}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 272 is less than current step: 499. Dropping entry: {'train/lr': 0.00015048979591836733, '_timestamp': 1740958940.9392548}).
Epoch: [2][274/500]	Time  5.590 ( 5.590)	Loss 2.1462 (1.7124)	CeLoss 0.2324 (0.5547)	SegCLSLoss 0.0146 (0.0106)	KLLoss 0.4062 (0.2762)	MaskLoss 0.9334 (0.5624)	MaskBCELoss 0.0705 (0.1127)	MaskDICELoss 0.8629 (0.4497)
Epoch: [2][275/500]	Time  5.926 ( 5.926)	Loss 2.4824 (1.6232)	CeLoss 0.2129 (0.3939)	SegCLSLoss 0.0168 (0.0126)	KLLoss 0.3848 (0.2707)	MaskLoss 1.1113 (0.5978)	MaskBCELoss 0.2905 (0.1178)	MaskDICELoss 0.8208 (0.4801)
Epoch: [2][276/500]	Time  6.172 ( 6.172)	Loss 1.7666 (1.7609)	CeLoss 0.2354 (0.3656)	SegCLSLoss 0.0222 (0.0184)	KLLoss 0.3945 (0.2684)	MaskLoss 0.7407 (0.6796)	MaskBCELoss 0.2981 (0.1376)	MaskDICELoss 0.4426 (0.5420)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 273 is less than current step: 499. Dropping entry: {'train/loss': 1.7124155759811401, 'train/ce_loss': 0.5546875, 'train/seg_cls_loss': 0.010595703125, 'train/kl_loss': 0.276171875, 'train/mask_bce_loss': 0.1126794746145606, 'train/mask_dice_loss': 0.4496806561946869, 'train/mask_loss': 0.5623601257801056, 'metrics/total_secs_per_batch': 5.590483665466309, 'metrics/data_secs_per_batch': 2.5433966636657717, '_timestamp': 1740958946.5290952}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 273 is less than current step: 499. Dropping entry: {'train/lr': 0.0001503673469387755, '_timestamp': 1740958946.5294452}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 274 is less than current step: 499. Dropping entry: {'train/loss': 1.623152595758438, 'train/ce_loss': 0.3939453125, 'train/seg_cls_loss': 0.012640380859375, 'train/kl_loss': 0.270703125, 'train/mask_bce_loss': 0.11775570623576641, 'train/mask_dice_loss': 0.48005106300115585, 'train/mask_loss': 0.5978067591786385, 'metrics/total_secs_per_batch': 5.9262073040008545, 'metrics/data_secs_per_batch': 2.7264898300170897, '_timestamp': 1740958952.4552433}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 274 is less than current step: 499. Dropping entry: {'train/lr': 0.00015024489795918366, '_timestamp': 1740958952.4555387}).
Epoch: [2][277/500]	Time  6.532 ( 6.532)	Loss 0.0806 (1.2201)	CeLoss 0.0806 (0.3957)	SegCLSLoss 0.0000 (0.0106)	KLLoss 0.0000 (0.1918)	MaskLoss 0.0000 (0.4001)	MaskBCELoss 0.0000 (0.0202)	MaskDICELoss 0.0000 (0.3799)
Epoch: [2][278/500]	Time  5.833 ( 5.833)	Loss 1.1172 (1.5591)	CeLoss 1.1172 (0.3169)	SegCLSLoss 0.0000 (0.0169)	KLLoss 0.0000 (0.3127)	MaskLoss 0.0000 (0.6010)	MaskBCELoss 0.0000 (0.1312)	MaskDICELoss 0.0000 (0.4698)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 275 is less than current step: 499. Dropping entry: {'train/loss': 1.7608790278434754, 'train/ce_loss': 0.365576171875, 'train/seg_cls_loss': 0.018359375, 'train/kl_loss': 0.268359375, 'train/mask_bce_loss': 0.13761577140539885, 'train/mask_dice_loss': 0.5419692456722259, 'train/mask_loss': 0.6795850038528443, 'metrics/total_secs_per_batch': 6.172357559204102, 'metrics/data_secs_per_batch': 2.858572483062744, '_timestamp': 1740958958.6276531}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 275 is less than current step: 499. Dropping entry: {'train/lr': 0.00015012244897959182, '_timestamp': 1740958958.627929}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 276 is less than current step: 499. Dropping entry: {'train/loss': 1.2200871109962463, 'train/ce_loss': 0.395654296875, 'train/seg_cls_loss': 0.01060791015625, 'train/kl_loss': 0.191796875, 'train/mask_bce_loss': 0.020173530094325542, 'train/mask_dice_loss': 0.379884672164917, 'train/mask_loss': 0.4000582039356232, 'metrics/total_secs_per_batch': 6.532466888427734, 'metrics/data_secs_per_batch': 2.9377481937408447, '_timestamp': 1740958965.160122}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 276 is less than current step: 499. Dropping entry: {'train/lr': 0.00015, '_timestamp': 1740958965.1604002}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 277 is less than current step: 499. Dropping entry: {'train/loss': 1.5590644955635071, 'train/ce_loss': 0.316943359375, 'train/seg_cls_loss': 0.016851806640625, 'train/kl_loss': 0.3126953125, 'train/mask_bce_loss': 0.13123047761619092, 'train/mask_dice_loss': 0.469786137342453, 'train/mask_loss': 0.6010166153311729, 'metrics/total_secs_per_batch': 5.833477973937988, 'metrics/data_secs_per_batch': 2.676611328125, '_timestamp': 1740958970.9935853}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 277 is less than current step: 499. Dropping entry: {'train/lr': 0.00014987755102040815, '_timestamp': 1740958970.993873}).
Epoch: [2][279/500]	Time  6.906 ( 6.906)	Loss 0.9922 (1.3860)	CeLoss 0.9922 (0.3124)	SegCLSLoss 0.0000 (0.0111)	KLLoss 0.0000 (0.2750)	MaskLoss 0.0000 (0.5204)	MaskBCELoss 0.0000 (0.0804)	MaskDICELoss 0.0000 (0.4400)
[2025-03-02 17:43:04,805] [INFO] [logging.py:96:log_dist] [Rank 0] step=1280, skipped=0, lr=[0.00014969387755102038], mom=[(0.9, 0.95)]
[2025-03-02 17:43:04,810] [INFO] [timer.py:215:stop] epoch=0/micro_step=12800/global_step=1280, RunningAvgSamplesPerSec=1.5508914359938657, CurrSamplesPerSec=1.4472466454709052, MemAllocated=31.25GB, MaxMemAllocated=37.19GB
Epoch: [2][280/500]	Time  6.911 ( 6.911)	Loss 2.2937 (1.6399)	CeLoss 0.2148 (0.2448)	SegCLSLoss 0.0221 (0.0151)	KLLoss 0.3867 (0.3545)	MaskLoss 1.0150 (0.6761)	MaskBCELoss 0.1003 (0.1351)	MaskDICELoss 0.9147 (0.5410)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 278 is less than current step: 499. Dropping entry: {'train/loss': 1.3859590113162994, 'train/ce_loss': 0.3124267578125, 'train/seg_cls_loss': 0.011053466796875, 'train/kl_loss': 0.275, 'train/mask_bce_loss': 0.0804046481847763, 'train/mask_dice_loss': 0.4400040566921234, 'train/mask_loss': 0.5204087048768997, 'metrics/total_secs_per_batch': 6.906436443328857, 'metrics/data_secs_per_batch': 3.3649694681167603, '_timestamp': 1740958977.9000778}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 278 is less than current step: 499. Dropping entry: {'train/lr': 0.0001497551020408163, '_timestamp': 1740958977.9003894}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 279 is less than current step: 499. Dropping entry: {'train/loss': 1.6399185299873351, 'train/ce_loss': 0.24482421875, 'train/seg_cls_loss': 0.015142822265625, 'train/kl_loss': 0.3544921875, 'train/mask_bce_loss': 0.1350712537765503, 'train/mask_dice_loss': 0.5410403430461883, 'train/mask_loss': 0.6761115968227387, 'metrics/total_secs_per_batch': 6.911391258239746, 'metrics/data_secs_per_batch': 2.9721244096755983, '_timestamp': 1740958984.8113189}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 279 is less than current step: 499. Dropping entry: {'train/lr': 0.00014963265306122448, '_timestamp': 1740958984.811678}).
Epoch: [2][281/500]	Time  6.135 ( 6.135)	Loss 1.9012 (1.8407)	CeLoss 0.2598 (0.3056)	SegCLSLoss 0.0153 (0.0169)	KLLoss 0.3887 (0.2701)	MaskLoss 0.7973 (0.7498)	MaskBCELoss 0.2109 (0.2563)	MaskDICELoss 0.5864 (0.4935)
Epoch: [2][282/500]	Time  6.863 ( 6.863)	Loss 1.6759 (2.0838)	CeLoss 0.2754 (0.2706)	SegCLSLoss 0.0119 (0.0211)	KLLoss 0.4062 (0.3488)	MaskLoss 0.6768 (0.8839)	MaskBCELoss 0.1222 (0.1853)	MaskDICELoss 0.5546 (0.6986)
Epoch: [2][283/500]	Time  5.973 ( 5.973)	Loss 1.0691 (1.8591)	CeLoss 0.3184 (0.4408)	SegCLSLoss 0.0116 (0.0152)	KLLoss 0.3906 (0.3119)	MaskLoss 0.3529 (0.6897)	MaskBCELoss 0.1756 (0.1186)	MaskDICELoss 0.1774 (0.5710)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 280 is less than current step: 499. Dropping entry: {'train/loss': 1.8406601548194885, 'train/ce_loss': 0.3055908203125, 'train/seg_cls_loss': 0.01685791015625, 'train/kl_loss': 0.2701171875, 'train/mask_bce_loss': 0.2562629014253616, 'train/mask_dice_loss': 0.49354714155197144, 'train/mask_loss': 0.7498100459575653, 'metrics/total_secs_per_batch': 6.135240793228149, 'metrics/data_secs_per_batch': 2.8480380296707155, '_timestamp': 1740958990.946696}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 280 is less than current step: 499. Dropping entry: {'train/lr': 0.00014951020408163264, '_timestamp': 1740958990.9469936}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 281 is less than current step: 499. Dropping entry: {'train/loss': 2.083791661262512, 'train/ce_loss': 0.27060546875, 'train/seg_cls_loss': 0.021099853515625, 'train/kl_loss': 0.348828125, 'train/mask_bce_loss': 0.1852619955316186, 'train/mask_dice_loss': 0.69862602353096, 'train/mask_loss': 0.8838880181312561, 'metrics/total_secs_per_batch': 6.863466501235962, 'metrics/data_secs_per_batch': 3.0335354566574098, '_timestamp': 1740958997.8104265}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 281 is less than current step: 499. Dropping entry: {'train/lr': 0.0001493877551020408, '_timestamp': 1740958997.8107996}).
Epoch: [2][284/500]	Time  6.515 ( 6.515)	Loss 1.4922 (1.6619)	CeLoss 1.4922 (0.6149)	SegCLSLoss 0.0000 (0.0145)	KLLoss 0.0000 (0.2748)	MaskLoss 0.0000 (0.5061)	MaskBCELoss 0.0000 (0.1062)	MaskDICELoss 0.0000 (0.4000)
Epoch: [2][285/500]	Time  5.678 ( 5.678)	Loss 1.4950 (1.6249)	CeLoss 0.2168 (0.3354)	SegCLSLoss 0.0182 (0.0158)	KLLoss 0.4023 (0.3127)	MaskLoss 0.6147 (0.6251)	MaskBCELoss 0.0780 (0.1584)	MaskDICELoss 0.5367 (0.4667)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 282 is less than current step: 499. Dropping entry: {'train/loss': 1.859065043926239, 'train/ce_loss': 0.4408203125, 'train/seg_cls_loss': 0.01524658203125, 'train/kl_loss': 0.3119140625, 'train/mask_bce_loss': 0.11864559901878238, 'train/mask_dice_loss': 0.5710431590676308, 'train/mask_loss': 0.689688766002655, 'metrics/total_secs_per_batch': 5.97319769859314, 'metrics/data_secs_per_batch': 2.9630853176116942, '_timestamp': 1740959003.783333}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 282 is less than current step: 499. Dropping entry: {'train/lr': 0.00014926530612244897, '_timestamp': 1740959003.78352}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 283 is less than current step: 499. Dropping entry: {'train/loss': 1.661880362033844, 'train/ce_loss': 0.61494140625, 'train/seg_cls_loss': 0.014471435546875, 'train/kl_loss': 0.2748046875, 'train/mask_bce_loss': 0.10615467559546232, 'train/mask_dice_loss': 0.3999808102846146, 'train/mask_loss': 0.5061354845762253, 'metrics/total_secs_per_batch': 6.5153727531433105, 'metrics/data_secs_per_batch': 3.264606404304504, '_timestamp': 1740959010.2986865}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 283 is less than current step: 499. Dropping entry: {'train/lr': 0.00014914285714285713, '_timestamp': 1740959010.2989643}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 284 is less than current step: 499. Dropping entry: {'train/loss': 1.6248708248138428, 'train/ce_loss': 0.3353515625, 'train/seg_cls_loss': 0.01575927734375, 'train/kl_loss': 0.3126953125, 'train/mask_bce_loss': 0.15843696780502797, 'train/mask_dice_loss': 0.46669374108314515, 'train/mask_loss': 0.6251307159662247, 'metrics/total_secs_per_batch': 5.67818021774292, 'metrics/data_secs_per_batch': 2.753922700881958, '_timestamp': 1740959015.976948}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 284 is less than current step: 499. Dropping entry: {'train/lr': 0.0001490204081632653, '_timestamp': 1740959015.9773073}).
Epoch: [2][286/500]	Time  5.963 ( 5.963)	Loss 2.7408 (2.4009)	CeLoss 0.2207 (0.3976)	SegCLSLoss 0.0187 (0.0185)	KLLoss 0.3945 (0.3090)	MaskLoss 1.2356 (0.9815)	MaskBCELoss 0.3283 (0.3352)	MaskDICELoss 0.9074 (0.6463)
Epoch: [2][287/500]	Time  6.584 ( 6.584)	Loss 1.7945 (1.5242)	CeLoss 0.3145 (0.5154)	SegCLSLoss 0.0128 (0.0092)	KLLoss 0.4004 (0.2406)	MaskLoss 0.7176 (0.4901)	MaskBCELoss 0.0681 (0.1106)	MaskDICELoss 0.6495 (0.3796)
Epoch: [2][288/500]	Time  6.494 ( 6.494)	Loss 2.3200 (1.9965)	CeLoss 0.2236 (0.3296)	SegCLSLoss 0.0208 (0.0180)	KLLoss 0.3809 (0.3471)	MaskLoss 1.0233 (0.8115)	MaskBCELoss 0.0524 (0.0838)	MaskDICELoss 0.9709 (0.7277)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 285 is less than current step: 499. Dropping entry: {'train/loss': 2.400867187976837, 'train/ce_loss': 0.39755859375, 'train/seg_cls_loss': 0.018548583984375, 'train/kl_loss': 0.308984375, 'train/mask_bce_loss': 0.3352476261556149, 'train/mask_dice_loss': 0.6462894976139069, 'train/mask_loss': 0.9815371334552765, 'metrics/total_secs_per_batch': 5.962804079055786, 'metrics/data_secs_per_batch': 2.7006027460098267, '_timestamp': 1740959021.9400086}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 285 is less than current step: 499. Dropping entry: {'train/lr': 0.00014889795918367345, '_timestamp': 1740959021.9403942}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 286 is less than current step: 499. Dropping entry: {'train/loss': 1.5241876602172852, 'train/ce_loss': 0.515380859375, 'train/seg_cls_loss': 0.009197998046875, 'train/kl_loss': 0.240625, 'train/mask_bce_loss': 0.1105847716331482, 'train/mask_dice_loss': 0.3795608103275299, 'train/mask_loss': 0.4901455849409103, 'metrics/total_secs_per_batch': 6.584295034408569, 'metrics/data_secs_per_batch': 2.3356277465820314, '_timestamp': 1740959028.523981}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 286 is less than current step: 499. Dropping entry: {'train/lr': 0.00014877551020408162, '_timestamp': 1740959028.5242453}).
Epoch: [2][289/500]	Time  5.478 ( 5.478)	Loss 2.5299 (1.6949)	CeLoss 0.1738 (0.3607)	SegCLSLoss 0.0223 (0.0174)	KLLoss 0.3730 (0.3479)	MaskLoss 1.1541 (0.6453)	MaskBCELoss 0.2929 (0.1672)	MaskDICELoss 0.8612 (0.4781)
[2025-03-02 17:44:07,080] [INFO] [logging.py:96:log_dist] [Rank 0] step=1290, skipped=0, lr=[0.00014846938775510204], mom=[(0.9, 0.95)]
[2025-03-02 17:44:07,086] [INFO] [timer.py:215:stop] epoch=0/micro_step=12900/global_step=1290, RunningAvgSamplesPerSec=1.5513053598837572, CurrSamplesPerSec=1.5173404173114473, MemAllocated=31.56GB, MaxMemAllocated=37.19GB
Epoch: [2][290/500]	Time  6.592 ( 6.592)	Loss 1.6347 (1.9775)	CeLoss 0.2266 (0.3127)	SegCLSLoss 0.0157 (0.0178)	KLLoss 0.3867 (0.3072)	MaskLoss 0.6807 (0.8126)	MaskBCELoss 0.0642 (0.2255)	MaskDICELoss 0.6164 (0.5870)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 287 is less than current step: 499. Dropping entry: {'train/loss': 1.9965179920196534, 'train/ce_loss': 0.32958984375, 'train/seg_cls_loss': 0.018035888671875, 'train/kl_loss': 0.3470703125, 'train/mask_bce_loss': 0.08382961517199874, 'train/mask_dice_loss': 0.7277106255292892, 'train/mask_loss': 0.8115402460098267, 'metrics/total_secs_per_batch': 6.493713855743408, 'metrics/data_secs_per_batch': 2.8649866819381713, '_timestamp': 1740959035.0177152}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 287 is less than current step: 499. Dropping entry: {'train/lr': 0.00014865306122448978, '_timestamp': 1740959035.0179157}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 288 is less than current step: 499. Dropping entry: {'train/loss': 1.694866919517517, 'train/ce_loss': 0.3607421875, 'train/seg_cls_loss': 0.017376708984375, 'train/kl_loss': 0.3478515625, 'train/mask_bce_loss': 0.16716621518135072, 'train/mask_dice_loss': 0.4781187981367111, 'train/mask_loss': 0.645285016298294, 'metrics/total_secs_per_batch': 5.477581739425659, 'metrics/data_secs_per_batch': 2.5222160816192627, '_timestamp': 1740959040.495318}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 288 is less than current step: 499. Dropping entry: {'train/lr': 0.00014853061224489794, '_timestamp': 1740959040.4956148}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 289 is less than current step: 499. Dropping entry: {'train/loss': 1.977538514137268, 'train/ce_loss': 0.312744140625, 'train/seg_cls_loss': 0.017755126953125, 'train/kl_loss': 0.3072265625, 'train/mask_bce_loss': 0.22554861567914486, 'train/mask_dice_loss': 0.5870243430137634, 'train/mask_loss': 0.8125729560852051, 'metrics/total_secs_per_batch': 6.59206748008728, 'metrics/data_secs_per_batch': 2.9422847270965575, '_timestamp': 1740959047.0871923}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 289 is less than current step: 499. Dropping entry: {'train/lr': 0.0001484081632653061, '_timestamp': 1740959047.0874639}).
Epoch: [2][291/500]	Time  6.271 ( 6.271)	Loss 1.2344 (1.9579)	CeLoss 1.2344 (0.4061)	SegCLSLoss 0.0000 (0.0170)	KLLoss 0.0000 (0.3082)	MaskLoss 0.0000 (0.7560)	MaskBCELoss 0.0000 (0.1718)	MaskDICELoss 0.0000 (0.5843)
Epoch: [2][292/500]	Time  6.706 ( 6.706)	Loss 2.4381 (1.0877)	CeLoss 0.2969 (0.3402)	SegCLSLoss 0.0132 (0.0112)	KLLoss 0.3945 (0.1936)	MaskLoss 1.0481 (0.3613)	MaskBCELoss 0.4200 (0.0682)	MaskDICELoss 0.6281 (0.2932)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 290 is less than current step: 499. Dropping entry: {'train/loss': 1.9578662037849426, 'train/ce_loss': 0.4060546875, 'train/seg_cls_loss': 0.01700439453125, 'train/kl_loss': 0.308203125, 'train/mask_bce_loss': 0.17175671299919487, 'train/mask_dice_loss': 0.5842760086059571, 'train/mask_loss': 0.7560327053070068, 'metrics/total_secs_per_batch': 6.270751953125, 'metrics/data_secs_per_batch': 3.091706085205078, '_timestamp': 1740959053.3581386}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 290 is less than current step: 499. Dropping entry: {'train/lr': 0.00014828571428571427, '_timestamp': 1740959053.358413}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 291 is less than current step: 499. Dropping entry: {'train/loss': 1.087696635723114, 'train/ce_loss': 0.340234375, 'train/seg_cls_loss': 0.01119384765625, 'train/kl_loss': 0.1935546875, 'train/mask_bce_loss': 0.06817224882543087, 'train/mask_dice_loss': 0.2931565329432487, 'train/mask_loss': 0.361328786611557, 'metrics/total_secs_per_batch': 6.705923080444336, 'metrics/data_secs_per_batch': 2.863532567024231, '_timestamp': 1740959060.064066}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 291 is less than current step: 499. Dropping entry: {'train/lr': 0.00014816326530612243, '_timestamp': 1740959060.064358}).
Epoch: [2][293/500]	Time  5.584 ( 5.584)	Loss 1.2734 (1.9267)	CeLoss 1.2734 (0.5962)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.2297)	MaskLoss 0.0000 (0.6502)	MaskBCELoss 0.0000 (0.1886)	MaskDICELoss 0.0000 (0.4615)
Epoch: [2][294/500]	Time  4.975 ( 4.975)	Loss 2.7255 (1.8629)	CeLoss 0.1650 (0.2995)	SegCLSLoss 0.0305 (0.0177)	KLLoss 0.3809 (0.3082)	MaskLoss 1.2534 (0.7620)	MaskBCELoss 0.2920 (0.1914)	MaskDICELoss 0.9613 (0.5705)
Epoch: [2][295/500]	Time  6.125 ( 6.125)	Loss 2.9670 (2.1822)	CeLoss 0.2119 (0.4271)	SegCLSLoss 0.0172 (0.0182)	KLLoss 0.3848 (0.3068)	MaskLoss 1.3536 (0.8577)	MaskBCELoss 0.9275 (0.3656)	MaskDICELoss 0.4262 (0.4921)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 292 is less than current step: 499. Dropping entry: {'train/loss': 1.9266865849494934, 'train/ce_loss': 0.59619140625, 'train/seg_cls_loss': 0.014208984375, 'train/kl_loss': 0.2296875, 'train/mask_bce_loss': 0.18862370327115058, 'train/mask_dice_loss': 0.4615359902381897, 'train/mask_loss': 0.6501597046852112, 'metrics/total_secs_per_batch': 5.584237098693848, 'metrics/data_secs_per_batch': 2.2996928215026857, '_timestamp': 1740959065.6484075}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 292 is less than current step: 499. Dropping entry: {'train/lr': 0.0001480408163265306, '_timestamp': 1740959065.6486342}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 293 is less than current step: 499. Dropping entry: {'train/loss': 1.8629136323928832, 'train/ce_loss': 0.29951171875, 'train/seg_cls_loss': 0.0177490234375, 'train/kl_loss': 0.308203125, 'train/mask_bce_loss': 0.19143332839012145, 'train/mask_dice_loss': 0.570541051030159, 'train/mask_loss': 0.7619743824005127, 'metrics/total_secs_per_batch': 4.975340127944946, 'metrics/data_secs_per_batch': 2.412947106361389, '_timestamp': 1740959070.6236987}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 293 is less than current step: 499. Dropping entry: {'train/lr': 0.00014791836734693876, '_timestamp': 1740959070.6239862}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 294 is less than current step: 499. Dropping entry: {'train/loss': 2.1821905612945556, 'train/ce_loss': 0.42705078125, 'train/seg_cls_loss': 0.01822509765625, 'train/kl_loss': 0.3068359375, 'train/mask_bce_loss': 0.36555046439170835, 'train/mask_dice_loss': 0.49214637875556944, 'train/mask_loss': 0.8576968550682068, 'metrics/total_secs_per_batch': 6.124861717224121, 'metrics/data_secs_per_batch': 3.1117403507232666, '_timestamp': 1740959076.7484765}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 294 is less than current step: 499. Dropping entry: {'train/lr': 0.00014779591836734692, '_timestamp': 1740959076.7487426}).
Epoch: [2][296/500]	Time  5.942 ( 5.942)	Loss 1.2930 (2.1187)	CeLoss 0.3203 (0.2144)	SegCLSLoss 0.0121 (0.0253)	KLLoss 0.3926 (0.3768)	MaskLoss 0.4639 (0.9271)	MaskBCELoss 0.0643 (0.1399)	MaskDICELoss 0.3996 (0.7872)
Epoch: [2][297/500]	Time  5.488 ( 5.488)	Loss 2.1443 (1.7075)	CeLoss 0.3438 (0.4353)	SegCLSLoss 0.0188 (0.0135)	KLLoss 0.3809 (0.3127)	MaskLoss 0.8768 (0.6171)	MaskBCELoss 0.1573 (0.1200)	MaskDICELoss 0.7196 (0.4971)
Epoch: [2][298/500]	Time  6.970 ( 6.970)	Loss 1.1681 (1.9650)	CeLoss 0.2676 (0.2836)	SegCLSLoss 0.0127 (0.0183)	KLLoss 0.3926 (0.3461)	MaskLoss 0.4278 (0.8189)	MaskBCELoss 0.1081 (0.2284)	MaskDICELoss 0.3197 (0.5905)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 295 is less than current step: 499. Dropping entry: {'train/loss': 2.1187215924263, 'train/ce_loss': 0.21435546875, 'train/seg_cls_loss': 0.025341796875, 'train/kl_loss': 0.3767578125, 'train/mask_bce_loss': 0.13988190451636912, 'train/mask_dice_loss': 0.7872035026550293, 'train/mask_loss': 0.9270854115486145, 'metrics/total_secs_per_batch': 5.942244291305542, 'metrics/data_secs_per_batch': 2.593211460113525, '_timestamp': 1740959082.690762}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 295 is less than current step: 499. Dropping entry: {'train/lr': 0.00014767346938775508, '_timestamp': 1740959082.6910448}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 296 is less than current step: 499. Dropping entry: {'train/loss': 1.7075064182281494, 'train/ce_loss': 0.43525390625, 'train/seg_cls_loss': 0.0135009765625, 'train/kl_loss': 0.3126953125, 'train/mask_bce_loss': 0.1199862502515316, 'train/mask_dice_loss': 0.49709704220294954, 'train/mask_loss': 0.6170832991600037, 'metrics/total_secs_per_batch': 5.487825870513916, 'metrics/data_secs_per_batch': 2.284507703781128, '_timestamp': 1740959088.1786149}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 296 is less than current step: 499. Dropping entry: {'train/lr': 0.00014755102040816325, '_timestamp': 1740959088.1789289}).
Epoch: [2][299/500]	Time  6.109 ( 6.109)	Loss 2.1573 (1.6400)	CeLoss 0.2441 (0.4890)	SegCLSLoss 0.0193 (0.0117)	KLLoss 0.3750 (0.2283)	MaskLoss 0.9331 (0.5612)	MaskBCELoss 0.0399 (0.0921)	MaskDICELoss 0.8933 (0.4691)
[2025-03-02 17:45:06,678] [INFO] [logging.py:96:log_dist] [Rank 0] step=1300, skipped=0, lr=[0.00014724489795918367], mom=[(0.9, 0.95)]
[2025-03-02 17:45:06,684] [INFO] [timer.py:215:stop] epoch=0/micro_step=13000/global_step=1300, RunningAvgSamplesPerSec=1.5522099355149575, CurrSamplesPerSec=1.8430625172554878, MemAllocated=30.64GB, MaxMemAllocated=37.19GB
Epoch: [2][300/500]	Time  5.427 ( 5.427)	Loss 2.4716 (1.9138)	CeLoss 0.3203 (0.4388)	SegCLSLoss 0.0107 (0.0158)	KLLoss 0.3887 (0.3088)	MaskLoss 1.0532 (0.7182)	MaskBCELoss 0.0540 (0.0560)	MaskDICELoss 0.9992 (0.6622)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 297 is less than current step: 499. Dropping entry: {'train/loss': 1.9650428771972657, 'train/ce_loss': 0.283642578125, 'train/seg_cls_loss': 0.01834716796875, 'train/kl_loss': 0.34609375, 'train/mask_bce_loss': 0.22844581119716167, 'train/mask_dice_loss': 0.5904525846242905, 'train/mask_loss': 0.818898394703865, 'metrics/total_secs_per_batch': 6.9701385498046875, 'metrics/data_secs_per_batch': 3.0245426654815675, '_timestamp': 1740959095.148702}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 297 is less than current step: 499. Dropping entry: {'train/lr': 0.0001474285714285714, '_timestamp': 1740959095.1489859}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 298 is less than current step: 499. Dropping entry: {'train/loss': 1.6400168180465697, 'train/ce_loss': 0.4890380859375, 'train/seg_cls_loss': 0.01170654296875, 'train/kl_loss': 0.2283203125, 'train/mask_bce_loss': 0.09206820577383042, 'train/mask_dice_loss': 0.46911450624465945, 'train/mask_loss': 0.5611827135086059, 'metrics/total_secs_per_batch': 6.109130382537842, 'metrics/data_secs_per_batch': 2.858484721183777, '_timestamp': 1740959101.257847}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 298 is less than current step: 499. Dropping entry: {'train/lr': 0.00014730612244897957, '_timestamp': 1740959101.2581944}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 299 is less than current step: 499. Dropping entry: {'train/loss': 1.9137553930282594, 'train/ce_loss': 0.43876953125, 'train/seg_cls_loss': 0.015838623046875, 'train/kl_loss': 0.3087890625, 'train/mask_bce_loss': 0.05603040810674429, 'train/mask_dice_loss': 0.6621754258871079, 'train/mask_loss': 0.7182058393955231, 'metrics/total_secs_per_batch': 5.427350759506226, 'metrics/data_secs_per_batch': 2.4276993036270142, '_timestamp': 1740959106.6850276}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 299 is less than current step: 499. Dropping entry: {'train/lr': 0.00014718367346938774, '_timestamp': 1740959106.6852992}).
Epoch: [2][301/500]	Time  4.711 ( 4.711)	Loss 1.3594 (1.4685)	CeLoss 1.3594 (0.7103)	SegCLSLoss 0.0000 (0.0094)	KLLoss 0.0000 (0.1936)	MaskLoss 0.0000 (0.3673)	MaskBCELoss 0.0000 (0.0435)	MaskDICELoss 0.0000 (0.3238)
Epoch: [2][302/500]	Time  5.766 ( 5.766)	Loss 1.3984 (1.5722)	CeLoss 1.3984 (0.5738)	SegCLSLoss 0.0000 (0.0117)	KLLoss 0.0000 (0.2313)	MaskLoss 0.0000 (0.4847)	MaskBCELoss 0.0000 (0.1054)	MaskDICELoss 0.0000 (0.3793)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 300 is less than current step: 499. Dropping entry: {'train/loss': 1.468536913394928, 'train/ce_loss': 0.71025390625, 'train/seg_cls_loss': 0.0093994140625, 'train/kl_loss': 0.1935546875, 'train/mask_bce_loss': 0.0434661328792572, 'train/mask_dice_loss': 0.3238101303577423, 'train/mask_loss': 0.367276269197464, 'metrics/total_secs_per_batch': 4.711390972137451, 'metrics/data_secs_per_batch': 2.553518867492676, '_timestamp': 1740959111.3966331}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 300 is less than current step: 499. Dropping entry: {'train/lr': 0.0001470612244897959, '_timestamp': 1740959111.396824}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 301 is less than current step: 499. Dropping entry: {'train/loss': 1.5721550464630127, 'train/ce_loss': 0.573828125, 'train/seg_cls_loss': 0.011669921875, 'train/kl_loss': 0.23125, 'train/mask_bce_loss': 0.10541323907673358, 'train/mask_dice_loss': 0.37929709553718566, 'train/mask_loss': 0.48471033573150635, 'metrics/total_secs_per_batch': 5.765713691711426, 'metrics/data_secs_per_batch': 2.666005039215088, '_timestamp': 1740959117.1623235}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 301 is less than current step: 499. Dropping entry: {'train/lr': 0.00014693877551020406, '_timestamp': 1740959117.162612}).
Epoch: [2][303/500]	Time  6.170 ( 6.170)	Loss 1.3594 (1.7706)	CeLoss 1.3594 (0.3845)	SegCLSLoss 0.0000 (0.0190)	KLLoss 0.0000 (0.3035)	MaskLoss 0.0000 (0.6731)	MaskBCELoss 0.0000 (0.0866)	MaskDICELoss 0.0000 (0.5865)
Epoch: [2][304/500]	Time  5.722 ( 5.722)	Loss 2.1133 (1.8439)	CeLoss 0.1885 (0.3416)	SegCLSLoss 0.0225 (0.0171)	KLLoss 0.3867 (0.3076)	MaskLoss 0.9375 (0.7314)	MaskBCELoss 0.1423 (0.1309)	MaskDICELoss 0.7953 (0.6005)
Epoch: [2][305/500]	Time  6.161 ( 6.161)	Loss 2.4979 (1.8990)	CeLoss 0.1963 (0.3809)	SegCLSLoss 0.0219 (0.0167)	KLLoss 0.3691 (0.3439)	MaskLoss 1.1269 (0.7377)	MaskBCELoss 0.5331 (0.1830)	MaskDICELoss 0.5938 (0.5547)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 302 is less than current step: 499. Dropping entry: {'train/loss': 1.7705906867980956, 'train/ce_loss': 0.38447265625, 'train/seg_cls_loss': 0.01900634765625, 'train/kl_loss': 0.303515625, 'train/mask_bce_loss': 0.08658273639157414, 'train/mask_dice_loss': 0.586505588889122, 'train/mask_loss': 0.6730883121490479, 'metrics/total_secs_per_batch': 6.170191764831543, 'metrics/data_secs_per_batch': 3.0539366006851196, '_timestamp': 1740959123.3325136}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 302 is less than current step: 499. Dropping entry: {'train/lr': 0.00014681632653061223, '_timestamp': 1740959123.332705}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 303 is less than current step: 499. Dropping entry: {'train/loss': 1.8438551783561707, 'train/ce_loss': 0.3416015625, 'train/seg_cls_loss': 0.017083740234375, 'train/kl_loss': 0.3076171875, 'train/mask_bce_loss': 0.13093182649463414, 'train/mask_dice_loss': 0.6004684150218964, 'train/mask_loss': 0.7314002335071563, 'metrics/total_secs_per_batch': 5.72233510017395, 'metrics/data_secs_per_batch': 2.3931065082550047, '_timestamp': 1740959129.0549424}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 303 is less than current step: 499. Dropping entry: {'train/lr': 0.0001466938775510204, '_timestamp': 1740959129.0552604}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 304 is less than current step: 499. Dropping entry: {'train/loss': 1.898993444442749, 'train/ce_loss': 0.380859375, 'train/seg_cls_loss': 0.0167236328125, 'train/kl_loss': 0.3439453125, 'train/mask_bce_loss': 0.1830284849740565, 'train/mask_dice_loss': 0.5546518445014954, 'train/mask_loss': 0.737680321931839, 'metrics/total_secs_per_batch': 6.161498069763184, 'metrics/data_secs_per_batch': 2.555148148536682, '_timestamp': 1740959135.2165365}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 304 is less than current step: 499. Dropping entry: {'train/lr': 0.00014657142857142855, '_timestamp': 1740959135.216866}).
Epoch: [2][306/500]	Time  5.968 ( 5.968)	Loss 2.6175 (1.6009)	CeLoss 0.2812 (0.3437)	SegCLSLoss 0.0172 (0.0129)	KLLoss 0.3652 (0.3096)	MaskLoss 1.1457 (0.6099)	MaskBCELoss 0.2558 (0.2041)	MaskDICELoss 0.8899 (0.4058)
Epoch: [2][307/500]	Time  5.317 ( 5.317)	Loss 1.0938 (1.6474)	CeLoss 1.0938 (0.5257)	SegCLSLoss 0.0000 (0.0131)	KLLoss 0.0000 (0.2693)	MaskLoss 0.0000 (0.5442)	MaskBCELoss 0.0000 (0.0683)	MaskDICELoss 0.0000 (0.4759)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 305 is less than current step: 499. Dropping entry: {'train/loss': 1.6009058058261871, 'train/ce_loss': 0.343701171875, 'train/seg_cls_loss': 0.012933349609375, 'train/kl_loss': 0.3095703125, 'train/mask_bce_loss': 0.20407895296812056, 'train/mask_dice_loss': 0.40582219511270523, 'train/mask_loss': 0.6099011570215225, 'metrics/total_secs_per_batch': 5.968145847320557, 'metrics/data_secs_per_batch': 2.4731953144073486, '_timestamp': 1740959141.1845121}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 305 is less than current step: 499. Dropping entry: {'train/lr': 0.00014644897959183674, '_timestamp': 1740959141.1847117}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 306 is less than current step: 499. Dropping entry: {'train/loss': 1.6474387764930725, 'train/ce_loss': 0.525732421875, 'train/seg_cls_loss': 0.013128662109375, 'train/kl_loss': 0.2693359375, 'train/mask_bce_loss': 0.0682596673257649, 'train/mask_dice_loss': 0.4759186923503876, 'train/mask_loss': 0.5441783607006073, 'metrics/total_secs_per_batch': 5.316802978515625, 'metrics/data_secs_per_batch': 2.3031872749328612, '_timestamp': 1740959146.501329}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 306 is less than current step: 499. Dropping entry: {'train/lr': 0.00014632653061224488, '_timestamp': 1740959146.5016263}).
Epoch: [2][308/500]	Time  7.033 ( 7.033)	Loss 0.6602 (1.4516)	CeLoss 0.6602 (0.4008)	SegCLSLoss 0.0000 (0.0128)	KLLoss 0.0000 (0.2291)	MaskLoss 0.0000 (0.5109)	MaskBCELoss 0.0000 (0.0767)	MaskDICELoss 0.0000 (0.4342)
Epoch: [2][309/500]	Time  7.153 ( 7.153)	Loss 0.9844 (1.6902)	CeLoss 0.9844 (0.3157)	SegCLSLoss 0.0000 (0.0148)	KLLoss 0.0000 (0.3537)	MaskLoss 0.0000 (0.6658)	MaskBCELoss 0.0000 (0.1968)	MaskDICELoss 0.0000 (0.4690)
[2025-03-02 17:46:06,556] [INFO] [logging.py:96:log_dist] [Rank 0] step=1310, skipped=0, lr=[0.0001460204081632653], mom=[(0.9, 0.95)]
[2025-03-02 17:46:06,562] [INFO] [timer.py:215:stop] epoch=0/micro_step=13100/global_step=1310, RunningAvgSamplesPerSec=1.5530501883243635, CurrSamplesPerSec=1.702430056972072, MemAllocated=31.23GB, MaxMemAllocated=37.19GB
Epoch: [2][310/500]	Time  5.876 ( 5.876)	Loss 2.0452 (2.1684)	CeLoss 0.3281 (0.3716)	SegCLSLoss 0.0120 (0.0131)	KLLoss 0.3867 (0.3121)	MaskLoss 0.8361 (0.8794)	MaskBCELoss 0.1500 (0.3211)	MaskDICELoss 0.6861 (0.5584)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 307 is less than current step: 499. Dropping entry: {'train/loss': 1.451599133014679, 'train/ce_loss': 0.40078125, 'train/seg_cls_loss': 0.01279296875, 'train/kl_loss': 0.2291015625, 'train/mask_bce_loss': 0.07669885344803333, 'train/mask_dice_loss': 0.4342081218957901, 'train/mask_loss': 0.510906982421875, 'metrics/total_secs_per_batch': 7.0327887535095215, 'metrics/data_secs_per_batch': 3.2238683700561523, '_timestamp': 1740959153.5340884}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 307 is less than current step: 499. Dropping entry: {'train/lr': 0.00014620408163265304, '_timestamp': 1740959153.5344021}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 308 is less than current step: 499. Dropping entry: {'train/loss': 1.6902311801910401, 'train/ce_loss': 0.31572265625, 'train/seg_cls_loss': 0.014825439453125, 'train/kl_loss': 0.3537109375, 'train/mask_bce_loss': 0.19679928719997405, 'train/mask_dice_loss': 0.46897058337926867, 'train/mask_loss': 0.6657698839902878, 'metrics/total_secs_per_batch': 7.153035879135132, 'metrics/data_secs_per_batch': 3.1633769512176513, '_timestamp': 1740959160.687191}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 308 is less than current step: 499. Dropping entry: {'train/lr': 0.0001460816326530612, '_timestamp': 1740959160.6874654}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 309 is less than current step: 499. Dropping entry: {'train/loss': 2.1683528661727904, 'train/ce_loss': 0.37158203125, 'train/seg_cls_loss': 0.013134765625, 'train/kl_loss': 0.312109375, 'train/mask_bce_loss': 0.3210887937806547, 'train/mask_dice_loss': 0.5583513051271438, 'train/mask_loss': 0.8794400960206985, 'metrics/total_secs_per_batch': 5.875571012496948, 'metrics/data_secs_per_batch': 2.7810797452926637, '_timestamp': 1740959166.5625272}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 309 is less than current step: 499. Dropping entry: {'train/lr': 0.00014595918367346937, '_timestamp': 1740959166.5628}).
Epoch: [2][311/500]	Time  4.260 ( 4.260)	Loss 0.6162 (1.4309)	CeLoss 0.2539 (0.7591)	SegCLSLoss 0.0143 (0.0086)	KLLoss 0.3984 (0.1926)	MaskLoss 0.1577 (0.3241)	MaskBCELoss 0.0843 (0.1188)	MaskDICELoss 0.0733 (0.2053)
Epoch: [2][312/500]	Time  6.702 ( 6.702)	Loss 2.2747 (1.6628)	CeLoss 0.2773 (0.3756)	SegCLSLoss 0.0140 (0.0143)	KLLoss 0.3965 (0.2672)	MaskLoss 0.9752 (0.6267)	MaskBCELoss 0.3521 (0.1772)	MaskDICELoss 0.6232 (0.4494)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 310 is less than current step: 499. Dropping entry: {'train/loss': 1.4308630049228668, 'train/ce_loss': 0.75908203125, 'train/seg_cls_loss': 0.008642578125, 'train/kl_loss': 0.192578125, 'train/mask_bce_loss': 0.11880080737173557, 'train/mask_dice_loss': 0.20532209053635597, 'train/mask_loss': 0.3241229087114334, 'metrics/total_secs_per_batch': 4.260289669036865, 'metrics/data_secs_per_batch': 2.008909559249878, '_timestamp': 1740959170.8230503}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 310 is less than current step: 499. Dropping entry: {'train/lr': 0.00014583673469387753, '_timestamp': 1740959170.8234112}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 311 is less than current step: 499. Dropping entry: {'train/loss': 1.6628193736076355, 'train/ce_loss': 0.3755859375, 'train/seg_cls_loss': 0.014324951171875, 'train/kl_loss': 0.2671875, 'train/mask_bce_loss': 0.17724436409771444, 'train/mask_dice_loss': 0.44942899644374845, 'train/mask_loss': 0.6266733586788178, 'metrics/total_secs_per_batch': 6.7016589641571045, 'metrics/data_secs_per_batch': 3.0246652126312257, '_timestamp': 1740959177.5247142}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 311 is less than current step: 499. Dropping entry: {'train/lr': 0.0001457142857142857, '_timestamp': 1740959177.5249865}).
Epoch: [2][313/500]	Time  5.897 ( 5.897)	Loss 2.5897 (1.9690)	CeLoss 0.1729 (0.3331)	SegCLSLoss 0.0282 (0.0156)	KLLoss 0.3750 (0.3041)	MaskLoss 1.1826 (0.7988)	MaskBCELoss 0.3090 (0.1861)	MaskDICELoss 0.8736 (0.6127)
Epoch: [2][314/500]	Time  6.211 ( 6.211)	Loss 1.2109 (1.7432)	CeLoss 1.2109 (0.4560)	SegCLSLoss 0.0000 (0.0132)	KLLoss 0.0000 (0.2691)	MaskLoss 0.0000 (0.6268)	MaskBCELoss 0.0000 (0.1726)	MaskDICELoss 0.0000 (0.4542)
Epoch: [2][315/500]	Time  5.357 ( 5.357)	Loss 2.5122 (1.5748)	CeLoss 0.1465 (0.5667)	SegCLSLoss 0.0383 (0.0132)	KLLoss 0.3750 (0.1873)	MaskLoss 1.1545 (0.4913)	MaskBCELoss 0.3465 (0.1152)	MaskDICELoss 0.8080 (0.3761)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 312 is less than current step: 499. Dropping entry: {'train/loss': 1.9690170884132385, 'train/ce_loss': 0.33310546875, 'train/seg_cls_loss': 0.015631103515625, 'train/kl_loss': 0.3041015625, 'train/mask_bce_loss': 0.18613842241466044, 'train/mask_dice_loss': 0.6126767516136169, 'train/mask_loss': 0.7988151848316193, 'metrics/total_secs_per_batch': 5.897119045257568, 'metrics/data_secs_per_batch': 2.5643898010253907, '_timestamp': 1740959183.4218075}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 312 is less than current step: 499. Dropping entry: {'train/lr': 0.00014559183673469386, '_timestamp': 1740959183.4221084}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 313 is less than current step: 499. Dropping entry: {'train/loss': 1.7432390093803405, 'train/ce_loss': 0.456005859375, 'train/seg_cls_loss': 0.013177490234375, 'train/kl_loss': 0.269140625, 'train/mask_bce_loss': 0.1726463183760643, 'train/mask_dice_loss': 0.4541733801364899, 'train/mask_loss': 0.6268197000026703, 'metrics/total_secs_per_batch': 6.211464166641235, 'metrics/data_secs_per_batch': 2.9349367380142213, '_timestamp': 1740959189.6332734}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 313 is less than current step: 499. Dropping entry: {'train/lr': 0.00014546938775510202, '_timestamp': 1740959189.633561}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 314 is less than current step: 499. Dropping entry: {'train/loss': 1.5747869968414308, 'train/ce_loss': 0.56669921875, 'train/seg_cls_loss': 0.0131591796875, 'train/kl_loss': 0.1873046875, 'train/mask_bce_loss': 0.11523244716227055, 'train/mask_dice_loss': 0.3760673075914383, 'train/mask_loss': 0.4912997543811798, 'metrics/total_secs_per_batch': 5.357264041900635, 'metrics/data_secs_per_batch': 2.445293593406677, '_timestamp': 1740959194.9907768}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 314 is less than current step: 499. Dropping entry: {'train/lr': 0.00014534693877551018, '_timestamp': 1740959194.9911246}).
Epoch: [2][316/500]	Time  5.508 ( 5.508)	Loss 0.6693 (1.6907)	CeLoss 0.2109 (0.2821)	SegCLSLoss 0.0261 (0.0227)	KLLoss 0.4062 (0.3469)	MaskLoss 0.2028 (0.6813)	MaskBCELoss 0.0128 (0.1354)	MaskDICELoss 0.1901 (0.5459)
Epoch: [2][317/500]	Time  5.527 ( 5.527)	Loss 1.2266 (1.5749)	CeLoss 1.2266 (0.5768)	SegCLSLoss 0.0000 (0.0125)	KLLoss 0.0000 (0.2705)	MaskLoss 0.0000 (0.4825)	MaskBCELoss 0.0000 (0.1289)	MaskDICELoss 0.0000 (0.3536)
Epoch: [2][318/500]	Time  5.275 ( 5.275)	Loss 1.1139 (1.6155)	CeLoss 0.2188 (0.4193)	SegCLSLoss 0.0121 (0.0151)	KLLoss 0.3848 (0.3096)	MaskLoss 0.4251 (0.5789)	MaskBCELoss 0.1775 (0.0721)	MaskDICELoss 0.2476 (0.5068)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 315 is less than current step: 499. Dropping entry: {'train/loss': 1.6906855762004853, 'train/ce_loss': 0.28212890625, 'train/seg_cls_loss': 0.022686767578125, 'train/kl_loss': 0.346875, 'train/mask_bce_loss': 0.13542191982269286, 'train/mask_dice_loss': 0.5458583638072014, 'train/mask_loss': 0.6812802821397781, 'metrics/total_secs_per_batch': 5.508066415786743, 'metrics/data_secs_per_batch': 2.601496195793152, '_timestamp': 1740959200.4985676}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 315 is less than current step: 499. Dropping entry: {'train/lr': 0.00014522448979591835, '_timestamp': 1740959200.4988372}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 316 is less than current step: 499. Dropping entry: {'train/loss': 1.5749170422554015, 'train/ce_loss': 0.5767578125, 'train/seg_cls_loss': 0.012457275390625, 'train/kl_loss': 0.2705078125, 'train/mask_bce_loss': 0.1289040356874466, 'train/mask_dice_loss': 0.35357401669025423, 'train/mask_loss': 0.48247804045677184, 'metrics/total_secs_per_batch': 5.526837348937988, 'metrics/data_secs_per_batch': 2.771582245826721, '_timestamp': 1740959206.0255253}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 316 is less than current step: 499. Dropping entry: {'train/lr': 0.00014510204081632654, '_timestamp': 1740959206.0258493}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 317 is less than current step: 499. Dropping entry: {'train/loss': 1.615502417087555, 'train/ce_loss': 0.4193359375, 'train/seg_cls_loss': 0.01512451171875, 'train/kl_loss': 0.3095703125, 'train/mask_bce_loss': 0.07205129750072956, 'train/mask_dice_loss': 0.5068424835801124, 'train/mask_loss': 0.5788937777280807, 'metrics/total_secs_per_batch': 5.274607419967651, 'metrics/data_secs_per_batch': 2.545627546310425, '_timestamp': 1740959211.3002636}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 317 is less than current step: 499. Dropping entry: {'train/lr': 0.0001449795918367347, '_timestamp': 1740959211.300597}).
Epoch: [2][319/500]	Time  5.456 ( 5.456)	Loss 2.0088 (1.6179)	CeLoss 0.2490 (0.5812)	SegCLSLoss 0.0121 (0.0099)	KLLoss 0.3867 (0.2678)	MaskLoss 0.8569 (0.5024)	MaskBCELoss 0.1125 (0.0687)	MaskDICELoss 0.7444 (0.4337)
[2025-03-02 17:47:03,818] [INFO] [logging.py:96:log_dist] [Rank 0] step=1320, skipped=0, lr=[0.00014479591836734693], mom=[(0.9, 0.95)]
[2025-03-02 17:47:03,822] [INFO] [timer.py:215:stop] epoch=0/micro_step=13200/global_step=1320, RunningAvgSamplesPerSec=1.554358247252322, CurrSamplesPerSec=1.4153265175374725, MemAllocated=31.27GB, MaxMemAllocated=37.19GB
Epoch: [2][320/500]	Time  7.067 ( 7.067)	Loss 1.8833 (1.9734)	CeLoss 0.2256 (0.3213)	SegCLSLoss 0.0154 (0.0153)	KLLoss 0.3867 (0.3418)	MaskLoss 0.8059 (0.8052)	MaskBCELoss 0.0218 (0.1703)	MaskDICELoss 0.7841 (0.6349)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 318 is less than current step: 499. Dropping entry: {'train/loss': 1.617932254076004, 'train/ce_loss': 0.58115234375, 'train/seg_cls_loss': 0.009857177734375, 'train/kl_loss': 0.2677734375, 'train/mask_bce_loss': 0.06874666400253773, 'train/mask_dice_loss': 0.4336764872074127, 'train/mask_loss': 0.5024231463670731, 'metrics/total_secs_per_batch': 5.455966472625732, 'metrics/data_secs_per_batch': 2.4309419870376585, '_timestamp': 1740959216.756087}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 318 is less than current step: 499. Dropping entry: {'train/lr': 0.00014485714285714286, '_timestamp': 1740959216.75636}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 319 is less than current step: 499. Dropping entry: {'train/loss': 1.9733930826187134, 'train/ce_loss': 0.3212890625, 'train/seg_cls_loss': 0.015325927734375, 'train/kl_loss': 0.341796875, 'train/mask_bce_loss': 0.17027531955391167, 'train/mask_dice_loss': 0.6348782479763031, 'train/mask_loss': 0.8051535785198212, 'metrics/total_secs_per_batch': 7.0673487186431885, 'metrics/data_secs_per_batch': 2.768733859062195, '_timestamp': 1740959223.8232846}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 319 is less than current step: 499. Dropping entry: {'train/lr': 0.000144734693877551, '_timestamp': 1740959223.8236463}).
Epoch: [2][321/500]	Time  5.668 ( 5.668)	Loss 0.9844 (1.4764)	CeLoss 0.9844 (0.4133)	SegCLSLoss 0.0000 (0.0118)	KLLoss 0.0000 (0.2258)	MaskLoss 0.0000 (0.5173)	MaskBCELoss 0.0000 (0.1255)	MaskDICELoss 0.0000 (0.3917)
Epoch: [2][322/500]	Time  5.934 ( 5.934)	Loss 1.9903 (1.6172)	CeLoss 0.2188 (0.3252)	SegCLSLoss 0.0239 (0.0172)	KLLoss 0.3711 (0.3023)	MaskLoss 0.8613 (0.6267)	MaskBCELoss 0.0355 (0.1080)	MaskDICELoss 0.8258 (0.5187)
Epoch: [2][323/500]	Time  5.581 ( 5.581)	Loss 2.3639 (1.9086)	CeLoss 0.2383 (0.5560)	SegCLSLoss 0.0249 (0.0172)	KLLoss 0.3711 (0.2629)	MaskLoss 1.0384 (0.6589)	MaskBCELoss 0.0965 (0.1017)	MaskDICELoss 0.9419 (0.5573)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 320 is less than current step: 499. Dropping entry: {'train/loss': 1.4764485597610473, 'train/ce_loss': 0.413330078125, 'train/seg_cls_loss': 0.01181640625, 'train/kl_loss': 0.22578125, 'train/mask_bce_loss': 0.12551611959934234, 'train/mask_dice_loss': 0.39173646569252013, 'train/mask_loss': 0.5172526001930237, 'metrics/total_secs_per_batch': 5.667666673660278, 'metrics/data_secs_per_batch': 2.5267025947570803, '_timestamp': 1740959229.4911878}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 320 is less than current step: 499. Dropping entry: {'train/lr': 0.00014461224489795916, '_timestamp': 1740959229.4915152}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 321 is less than current step: 499. Dropping entry: {'train/loss': 1.6171745777130127, 'train/ce_loss': 0.3251953125, 'train/seg_cls_loss': 0.017218017578125, 'train/kl_loss': 0.30234375, 'train/mask_bce_loss': 0.10801522862166166, 'train/mask_dice_loss': 0.5186872899532318, 'train/mask_loss': 0.6267025172710419, 'metrics/total_secs_per_batch': 5.934391498565674, 'metrics/data_secs_per_batch': 2.695874738693237, '_timestamp': 1740959235.4254441}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 321 is less than current step: 499. Dropping entry: {'train/lr': 0.00014448979591836732, '_timestamp': 1740959235.425715}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 322 is less than current step: 499. Dropping entry: {'train/loss': 1.9086419701576234, 'train/ce_loss': 0.55595703125, 'train/seg_cls_loss': 0.017236328125, 'train/kl_loss': 0.262890625, 'train/mask_bce_loss': 0.10165439881384372, 'train/mask_dice_loss': 0.5572564363479614, 'train/mask_loss': 0.6589108288288117, 'metrics/total_secs_per_batch': 5.580815315246582, 'metrics/data_secs_per_batch': 2.4719858169555664, '_timestamp': 1740959241.0062776}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 322 is less than current step: 499. Dropping entry: {'train/lr': 0.0001443673469387755, '_timestamp': 1740959241.0066235}).
Epoch: [2][324/500]	Time  6.537 ( 6.537)	Loss 0.0581 (1.8723)	CeLoss 0.0581 (0.2248)	SegCLSLoss 0.0000 (0.0181)	KLLoss 0.0000 (0.3459)	MaskLoss 0.0000 (0.8020)	MaskBCELoss 0.0000 (0.1880)	MaskDICELoss 0.0000 (0.6139)
Epoch: [2][325/500]	Time  6.124 ( 6.124)	Loss 2.5810 (1.5690)	CeLoss 0.1494 (0.3940)	SegCLSLoss 0.0332 (0.0135)	KLLoss 0.3809 (0.2686)	MaskLoss 1.1885 (0.5707)	MaskBCELoss 0.2904 (0.1475)	MaskDICELoss 0.8981 (0.4232)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 323 is less than current step: 499. Dropping entry: {'train/loss': 1.8723498940467835, 'train/ce_loss': 0.224755859375, 'train/seg_cls_loss': 0.018072509765625, 'train/kl_loss': 0.3458984375, 'train/mask_bce_loss': 0.18804785376414657, 'train/mask_dice_loss': 0.6139229834079742, 'train/mask_loss': 0.801970836520195, 'metrics/total_secs_per_batch': 6.537370920181274, 'metrics/data_secs_per_batch': 2.81881103515625, '_timestamp': 1740959247.543661}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 323 is less than current step: 499. Dropping entry: {'train/lr': 0.00014424489795918365, '_timestamp': 1740959247.5439315}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 324 is less than current step: 499. Dropping entry: {'train/loss': 1.5689733266830443, 'train/ce_loss': 0.39404296875, 'train/seg_cls_loss': 0.01348876953125, 'train/kl_loss': 0.2685546875, 'train/mask_bce_loss': 0.14747912362217902, 'train/mask_dice_loss': 0.4231891632080078, 'train/mask_loss': 0.570668289065361, 'metrics/total_secs_per_batch': 6.124031066894531, 'metrics/data_secs_per_batch': 2.979525852203369, '_timestamp': 1740959253.6679015}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 324 is less than current step: 499. Dropping entry: {'train/lr': 0.0001441224489795918, '_timestamp': 1740959253.6682975}).
Epoch: [2][326/500]	Time  6.882 ( 6.882)	Loss 2.0544 (1.6479)	CeLoss 0.2061 (0.2976)	SegCLSLoss 0.0157 (0.0143)	KLLoss 0.3711 (0.3037)	MaskLoss 0.9022 (0.6563)	MaskBCELoss 0.2612 (0.0988)	MaskDICELoss 0.6410 (0.5576)
Epoch: [2][327/500]	Time  6.506 ( 6.506)	Loss 1.8991 (1.9947)	CeLoss 0.2539 (0.3925)	SegCLSLoss 0.0181 (0.0167)	KLLoss 0.3887 (0.3051)	MaskLoss 0.7982 (0.7818)	MaskBCELoss 0.2720 (0.1911)	MaskDICELoss 0.5262 (0.5908)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 325 is less than current step: 499. Dropping entry: {'train/loss': 1.6478875994682312, 'train/ce_loss': 0.29755859375, 'train/seg_cls_loss': 0.014312744140625, 'train/kl_loss': 0.3037109375, 'train/mask_bce_loss': 0.09875182379037142, 'train/mask_dice_loss': 0.5575650274753571, 'train/mask_loss': 0.6563168525695801, 'metrics/total_secs_per_batch': 6.882448673248291, 'metrics/data_secs_per_batch': 2.825035262107849, '_timestamp': 1740959260.5500786}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 325 is less than current step: 499. Dropping entry: {'train/lr': 0.00014399999999999998, '_timestamp': 1740959260.550357}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 326 is less than current step: 499. Dropping entry: {'train/loss': 1.9947111845016479, 'train/ce_loss': 0.39248046875, 'train/seg_cls_loss': 0.016748046875, 'train/kl_loss': 0.305078125, 'train/mask_bce_loss': 0.19105032570660113, 'train/mask_dice_loss': 0.5907779216766358, 'train/mask_loss': 0.7818282425403595, 'metrics/total_secs_per_batch': 6.506329774856567, 'metrics/data_secs_per_batch': 3.0335745334625246, '_timestamp': 1740959267.0564177}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 326 is less than current step: 499. Dropping entry: {'train/lr': 0.00014387755102040814, '_timestamp': 1740959267.0567014}).
Epoch: [2][328/500]	Time  6.662 ( 6.662)	Loss 2.4487 (2.0128)	CeLoss 0.2168 (0.3907)	SegCLSLoss 0.0197 (0.0184)	KLLoss 0.3750 (0.3420)	MaskLoss 1.0925 (0.7894)	MaskBCELoss 0.3807 (0.1461)	MaskDICELoss 0.7118 (0.6433)
Epoch: [2][329/500]	Time  6.173 ( 6.173)	Loss 2.0164 (1.7055)	CeLoss 0.2617 (0.3315)	SegCLSLoss 0.0198 (0.0201)	KLLoss 0.3672 (0.3016)	MaskLoss 0.8539 (0.6668)	MaskBCELoss 0.2379 (0.1269)	MaskDICELoss 0.6160 (0.5399)
[2025-03-02 17:48:05,531] [INFO] [logging.py:96:log_dist] [Rank 0] step=1330, skipped=0, lr=[0.00014357142857142856], mom=[(0.9, 0.95)]
[2025-03-02 17:48:05,536] [INFO] [timer.py:215:stop] epoch=0/micro_step=13300/global_step=1330, RunningAvgSamplesPerSec=1.5548376979388507, CurrSamplesPerSec=1.7718403414664232, MemAllocated=31.26GB, MaxMemAllocated=37.19GB
Epoch: [2][330/500]	Time  5.646 ( 5.646)	Loss 2.1414 (1.4580)	CeLoss 0.1572 (0.5142)	SegCLSLoss 0.0205 (0.0092)	KLLoss 0.3711 (0.1928)	MaskLoss 0.9682 (0.4599)	MaskBCELoss 0.0773 (0.0867)	MaskDICELoss 0.8908 (0.3733)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 327 is less than current step: 499. Dropping entry: {'train/loss': 2.0128391861915587, 'train/ce_loss': 0.39072265625, 'train/seg_cls_loss': 0.018389892578125, 'train/kl_loss': 0.3419921875, 'train/mask_bce_loss': 0.1461426886729896, 'train/mask_dice_loss': 0.6432847261428833, 'train/mask_loss': 0.7894274055957794, 'metrics/total_secs_per_batch': 6.6617112159729, 'metrics/data_secs_per_batch': 2.9859290599822996, '_timestamp': 1740959273.7182946}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 327 is less than current step: 499. Dropping entry: {'train/lr': 0.0001437551020408163, '_timestamp': 1740959273.7186403}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 328 is less than current step: 499. Dropping entry: {'train/loss': 1.705544912815094, 'train/ce_loss': 0.33154296875, 'train/seg_cls_loss': 0.020050048828125, 'train/kl_loss': 0.3015625, 'train/mask_bce_loss': 0.12690917477011682, 'train/mask_dice_loss': 0.5398769587278366, 'train/mask_loss': 0.666786128282547, 'metrics/total_secs_per_batch': 6.173486948013306, 'metrics/data_secs_per_batch': 2.8549676656723024, '_timestamp': 1740959279.8917985}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 328 is less than current step: 499. Dropping entry: {'train/lr': 0.0001436326530612245, '_timestamp': 1740959279.8921244}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 329 is less than current step: 499. Dropping entry: {'train/loss': 1.4579808473587037, 'train/ce_loss': 0.51416015625, 'train/seg_cls_loss': 0.00916748046875, 'train/kl_loss': 0.1927734375, 'train/mask_bce_loss': 0.08669591601938009, 'train/mask_dice_loss': 0.37325153946876527, 'train/mask_loss': 0.4599474608898163, 'metrics/total_secs_per_batch': 5.645596027374268, 'metrics/data_secs_per_batch': 2.3409146547317503, '_timestamp': 1740959285.5370708}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 329 is less than current step: 499. Dropping entry: {'train/lr': 0.00014351020408163266, '_timestamp': 1740959285.5373333}).
Epoch: [2][331/500]	Time  5.206 ( 5.206)	Loss 2.5027 (1.8546)	CeLoss 0.1631 (0.6188)	SegCLSLoss 0.0334 (0.0141)	KLLoss 0.3652 (0.2248)	MaskLoss 1.1430 (0.6030)	MaskBCELoss 0.2177 (0.1605)	MaskDICELoss 0.9253 (0.4425)
Epoch: [2][332/500]	Time  6.507 ( 6.507)	Loss 1.7582 (1.5212)	CeLoss 0.2637 (0.3504)	SegCLSLoss 0.0161 (0.0136)	KLLoss 0.3867 (0.3059)	MaskLoss 0.7238 (0.5670)	MaskBCELoss 0.1392 (0.0515)	MaskDICELoss 0.5846 (0.5154)
Epoch: [2][333/500]	Time  4.647 ( 4.647)	Loss 0.8398 (1.6208)	CeLoss 0.8398 (0.5980)	SegCLSLoss 0.0000 (0.0095)	KLLoss 0.0000 (0.1885)	MaskLoss 0.0000 (0.4997)	MaskBCELoss 0.0000 (0.1115)	MaskDICELoss 0.0000 (0.3882)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 330 is less than current step: 499. Dropping entry: {'train/loss': 1.8545965313911439, 'train/ce_loss': 0.61884765625, 'train/seg_cls_loss': 0.014080810546875, 'train/kl_loss': 0.2248046875, 'train/mask_bce_loss': 0.16049783825874328, 'train/mask_dice_loss': 0.44253286719322205, 'train/mask_loss': 0.6030306994915009, 'metrics/total_secs_per_batch': 5.205786943435669, 'metrics/data_secs_per_batch': 2.2792437791824343, '_timestamp': 1740959290.7431128}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 330 is less than current step: 499. Dropping entry: {'train/lr': 0.00014338775510204082, '_timestamp': 1740959290.743489}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 331 is less than current step: 499. Dropping entry: {'train/loss': 1.5212137758731843, 'train/ce_loss': 0.350390625, 'train/seg_cls_loss': 0.013604736328125, 'train/kl_loss': 0.305859375, 'train/mask_bce_loss': 0.05151643306016922, 'train/mask_dice_loss': 0.5154381185770035, 'train/mask_loss': 0.5669545561075211, 'metrics/total_secs_per_batch': 6.50676417350769, 'metrics/data_secs_per_batch': 2.8452982664108277, '_timestamp': 1740959297.249994}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 331 is less than current step: 499. Dropping entry: {'train/lr': 0.00014326530612244898, '_timestamp': 1740959297.2503164}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 332 is less than current step: 499. Dropping entry: {'train/loss': 1.6208193898200989, 'train/ce_loss': 0.598046875, 'train/seg_cls_loss': 0.009454345703125, 'train/kl_loss': 0.1884765625, 'train/mask_bce_loss': 0.11146219559013844, 'train/mask_dice_loss': 0.3882053136825562, 'train/mask_loss': 0.4996675133705139, 'metrics/total_secs_per_batch': 4.6468117237091064, 'metrics/data_secs_per_batch': 2.171922969818115, '_timestamp': 1740959301.8966208}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 332 is less than current step: 499. Dropping entry: {'train/lr': 0.00014314285714285712, '_timestamp': 1740959301.896907}).
Epoch: [2][334/500]	Time  6.134 ( 6.134)	Loss 2.4271 (1.7998)	CeLoss 0.2227 (0.4610)	SegCLSLoss 0.0203 (0.0138)	KLLoss 0.3594 (0.3043)	MaskLoss 1.0798 (0.6506)	MaskBCELoss 0.2222 (0.1223)	MaskDICELoss 0.8575 (0.5283)
Epoch: [2][335/500]	Time  6.772 ( 6.772)	Loss 1.2188 (1.3641)	CeLoss 1.2188 (0.3163)	SegCLSLoss 0.0000 (0.0118)	KLLoss 0.0000 (0.2287)	MaskLoss 0.0000 (0.5096)	MaskBCELoss 0.0000 (0.0645)	MaskDICELoss 0.0000 (0.4451)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 333 is less than current step: 499. Dropping entry: {'train/loss': 1.7998287558555603, 'train/ce_loss': 0.46103515625, 'train/seg_cls_loss': 0.013818359375, 'train/kl_loss': 0.304296875, 'train/mask_bce_loss': 0.12229906879365444, 'train/mask_dice_loss': 0.5283477157354355, 'train/mask_loss': 0.6506467998027802, 'metrics/total_secs_per_batch': 6.134234428405762, 'metrics/data_secs_per_batch': 2.760348582267761, '_timestamp': 1740959308.0308332}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 333 is less than current step: 499. Dropping entry: {'train/lr': 0.00014302040816326528, '_timestamp': 1740959308.031029}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 334 is less than current step: 499. Dropping entry: {'train/loss': 1.3641076326370238, 'train/ce_loss': 0.316259765625, 'train/seg_cls_loss': 0.011785888671875, 'train/kl_loss': 0.2287109375, 'train/mask_bce_loss': 0.06446982994675636, 'train/mask_dice_loss': 0.44514747262001036, 'train/mask_loss': 0.5096173048019409, 'metrics/total_secs_per_batch': 6.771962404251099, 'metrics/data_secs_per_batch': 3.1809502840042114, '_timestamp': 1740959314.8028743}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 334 is less than current step: 499. Dropping entry: {'train/lr': 0.00014289795918367344, '_timestamp': 1740959314.8031616}).
Epoch: [2][336/500]	Time  5.470 ( 5.470)	Loss 1.9033 (1.6701)	CeLoss 0.2422 (0.5197)	SegCLSLoss 0.0198 (0.0135)	KLLoss 0.3789 (0.2645)	MaskLoss 0.8071 (0.5587)	MaskBCELoss 0.0273 (0.1134)	MaskDICELoss 0.7798 (0.4453)
Epoch: [2][337/500]	Time  5.595 ( 5.595)	Loss 2.2853 (1.3495)	CeLoss 0.4141 (0.4521)	SegCLSLoss 0.0122 (0.0107)	KLLoss 0.3887 (0.1887)	MaskLoss 0.9132 (0.4367)	MaskBCELoss 0.1783 (0.0890)	MaskDICELoss 0.7349 (0.3477)
Epoch: [2][338/500]	Time  5.994 ( 5.994)	Loss 2.3850 (1.5317)	CeLoss 0.2197 (0.3597)	SegCLSLoss 0.0264 (0.0134)	KLLoss 0.3691 (0.2625)	MaskLoss 1.0577 (0.5697)	MaskBCELoss 0.1068 (0.0950)	MaskDICELoss 0.9510 (0.4747)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 335 is less than current step: 499. Dropping entry: {'train/loss': 1.6700911045074462, 'train/ce_loss': 0.5197265625, 'train/seg_cls_loss': 0.013507080078125, 'train/kl_loss': 0.264453125, 'train/mask_bce_loss': 0.1133818813599646, 'train/mask_dice_loss': 0.44529647529125216, 'train/mask_loss': 0.5586783587932587, 'metrics/total_secs_per_batch': 5.469586372375488, 'metrics/data_secs_per_batch': 2.4389782428741453, '_timestamp': 1740959320.272438}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 335 is less than current step: 499. Dropping entry: {'train/lr': 0.0001427755102040816, '_timestamp': 1740959320.2727175}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 336 is less than current step: 499. Dropping entry: {'train/loss': 1.3495282530784607, 'train/ce_loss': 0.45205078125, 'train/seg_cls_loss': 0.010711669921875, 'train/kl_loss': 0.188671875, 'train/mask_bce_loss': 0.08904107213020325, 'train/mask_dice_loss': 0.3476859599351883, 'train/mask_loss': 0.4367270290851593, 'metrics/total_secs_per_batch': 5.5954039096832275, 'metrics/data_secs_per_batch': 2.1431570529937742, '_timestamp': 1740959325.8678916}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 336 is less than current step: 499. Dropping entry: {'train/lr': 0.00014265306122448977, '_timestamp': 1740959325.86819}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 337 is less than current step: 499. Dropping entry: {'train/loss': 1.5317084670066834, 'train/ce_loss': 0.35966796875, 'train/seg_cls_loss': 0.013372802734375, 'train/kl_loss': 0.2625, 'train/mask_bce_loss': 0.09497497156262398, 'train/mask_dice_loss': 0.47468783259391784, 'train/mask_loss': 0.5696628093719482, 'metrics/total_secs_per_batch': 5.994147539138794, 'metrics/data_secs_per_batch': 2.754705309867859, '_timestamp': 1740959331.8622563}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 337 is less than current step: 499. Dropping entry: {'train/lr': 0.00014253061224489793, '_timestamp': 1740959331.862616}).
Epoch: [2][339/500]	Time  6.462 ( 6.462)	Loss 1.8448 (1.9240)	CeLoss 0.2852 (0.3249)	SegCLSLoss 0.0152 (0.0177)	KLLoss 0.3848 (0.2998)	MaskLoss 0.7564 (0.7801)	MaskBCELoss 0.1078 (0.1412)	MaskDICELoss 0.6486 (0.6389)
[2025-03-02 17:49:03,510] [INFO] [logging.py:96:log_dist] [Rank 0] step=1340, skipped=0, lr=[0.0001423469387755102], mom=[(0.9, 0.95)]
[2025-03-02 17:49:03,516] [INFO] [timer.py:215:stop] epoch=0/micro_step=13400/global_step=1340, RunningAvgSamplesPerSec=1.5559855662777722, CurrSamplesPerSec=1.9261718854763021, MemAllocated=31.09GB, MaxMemAllocated=37.19GB
Epoch: [2][340/500]	Time  5.193 ( 5.193)	Loss 1.6625 (1.5786)	CeLoss 0.2119 (0.5315)	SegCLSLoss 0.0155 (0.0119)	KLLoss 0.3887 (0.2662)	MaskLoss 0.7024 (0.5073)	MaskBCELoss 0.2940 (0.1055)	MaskDICELoss 0.4083 (0.4017)
Epoch: [2][341/500]	Time  4.946 ( 4.946)	Loss 0.7227 (1.6843)	CeLoss 0.7227 (0.6112)	SegCLSLoss 0.0000 (0.0118)	KLLoss 0.0000 (0.2291)	MaskLoss 0.0000 (0.5221)	MaskBCELoss 0.0000 (0.1219)	MaskDICELoss 0.0000 (0.4002)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 338 is less than current step: 499. Dropping entry: {'train/loss': 1.9240206837654115, 'train/ce_loss': 0.32490234375, 'train/seg_cls_loss': 0.017718505859375, 'train/kl_loss': 0.2998046875, 'train/mask_bce_loss': 0.14116840455681084, 'train/mask_dice_loss': 0.6389083445072175, 'train/mask_loss': 0.7800767421722412, 'metrics/total_secs_per_batch': 6.461798667907715, 'metrics/data_secs_per_batch': 3.024499011039734, '_timestamp': 1740959338.3238683}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 338 is less than current step: 499. Dropping entry: {'train/lr': 0.0001424081632653061, '_timestamp': 1740959338.3242536}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 339 is less than current step: 499. Dropping entry: {'train/loss': 1.5786326587200166, 'train/ce_loss': 0.53154296875, 'train/seg_cls_loss': 0.011944580078125, 'train/kl_loss': 0.2662109375, 'train/mask_bce_loss': 0.10554744824767112, 'train/mask_dice_loss': 0.401737629622221, 'train/mask_loss': 0.5072850733995438, 'metrics/total_secs_per_batch': 5.193387031555176, 'metrics/data_secs_per_batch': 2.407842755317688, '_timestamp': 1740959343.5169897}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 339 is less than current step: 499. Dropping entry: {'train/lr': 0.00014228571428571429, '_timestamp': 1740959343.5172546}).
Epoch: [2][342/500]	Time  6.377 ( 6.377)	Loss 1.4469 (1.4215)	CeLoss 0.2383 (0.6364)	SegCLSLoss 0.0133 (0.0082)	KLLoss 0.3867 (0.2320)	MaskLoss 0.5819 (0.3788)	MaskBCELoss 0.1180 (0.0740)	MaskDICELoss 0.4639 (0.3048)
Epoch: [2][343/500]	Time  6.023 ( 6.023)	Loss 1.8820 (1.6757)	CeLoss 0.3027 (0.4442)	SegCLSLoss 0.0170 (0.0138)	KLLoss 0.3848 (0.3115)	MaskLoss 0.7662 (0.5967)	MaskBCELoss 0.3245 (0.0967)	MaskDICELoss 0.4417 (0.5000)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 340 is less than current step: 499. Dropping entry: {'train/loss': 1.684281313419342, 'train/ce_loss': 0.61123046875, 'train/seg_cls_loss': 0.011846923828125, 'train/kl_loss': 0.2291015625, 'train/mask_bce_loss': 0.1219155766069889, 'train/mask_dice_loss': 0.40020554661750796, 'train/mask_loss': 0.5221211314201355, 'metrics/total_secs_per_batch': 4.9455602169036865, 'metrics/data_secs_per_batch': 2.164653205871582, '_timestamp': 1740959348.4628408}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 340 is less than current step: 499. Dropping entry: {'train/lr': 0.00014216326530612245, '_timestamp': 1740959348.4631946}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 341 is less than current step: 499. Dropping entry: {'train/loss': 1.4215197801589965, 'train/ce_loss': 0.63642578125, 'train/seg_cls_loss': 0.008203125, 'train/kl_loss': 0.23203125, 'train/mask_bce_loss': 0.07400595732033252, 'train/mask_dice_loss': 0.3048203304409981, 'train/mask_loss': 0.3788262903690338, 'metrics/total_secs_per_batch': 6.377478361129761, 'metrics/data_secs_per_batch': 2.8441652059555054, '_timestamp': 1740959354.840345}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 341 is less than current step: 499. Dropping entry: {'train/lr': 0.0001420408163265306, '_timestamp': 1740959354.8406606}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 342 is less than current step: 499. Dropping entry: {'train/loss': 1.6756564497947692, 'train/ce_loss': 0.44423828125, 'train/seg_cls_loss': 0.0137939453125, 'train/kl_loss': 0.3115234375, 'train/mask_bce_loss': 0.09669415876269341, 'train/mask_dice_loss': 0.5000207990407943, 'train/mask_loss': 0.5967149585485458, 'metrics/total_secs_per_batch': 6.022829532623291, 'metrics/data_secs_per_batch': 2.715448808670044, '_timestamp': 1740959360.8632793}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 342 is less than current step: 499. Dropping entry: {'train/lr': 0.00014191836734693878, '_timestamp': 1740959360.8636427}).
Epoch: [2][344/500]	Time  6.025 ( 6.025)	Loss 1.1875 (2.0398)	CeLoss 1.1875 (0.4600)	SegCLSLoss 0.0000 (0.0145)	KLLoss 0.0000 (0.3020)	MaskLoss 0.0000 (0.7710)	MaskBCELoss 0.0000 (0.1802)	MaskDICELoss 0.0000 (0.5908)
Epoch: [2][345/500]	Time  4.945 ( 4.945)	Loss 1.1797 (1.4127)	CeLoss 1.1797 (0.5412)	SegCLSLoss 0.0000 (0.0123)	KLLoss 0.0000 (0.2271)	MaskLoss 0.0000 (0.4212)	MaskBCELoss 0.0000 (0.0457)	MaskDICELoss 0.0000 (0.3755)
Epoch: [2][346/500]	Time  5.664 ( 5.664)	Loss 2.2631 (1.3253)	CeLoss 0.2061 (0.5674)	SegCLSLoss 0.0226 (0.0078)	KLLoss 0.3574 (0.1502)	MaskLoss 1.0046 (0.3694)	MaskBCELoss 0.0633 (0.0615)	MaskDICELoss 0.9413 (0.3080)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 343 is less than current step: 499. Dropping entry: {'train/loss': 2.039783477783203, 'train/ce_loss': 0.4599609375, 'train/seg_cls_loss': 0.014544677734375, 'train/kl_loss': 0.301953125, 'train/mask_bce_loss': 0.18024468868970872, 'train/mask_dice_loss': 0.5907701015472412, 'train/mask_loss': 0.7710147857666015, 'metrics/total_secs_per_batch': 6.025140047073364, 'metrics/data_secs_per_batch': 2.940763521194458, '_timestamp': 1740959366.888197}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 343 is less than current step: 499. Dropping entry: {'train/lr': 0.00014179591836734694, '_timestamp': 1740959366.888402}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 344 is less than current step: 499. Dropping entry: {'train/loss': 1.4127309441566467, 'train/ce_loss': 0.5412109375, 'train/seg_cls_loss': 0.01231689453125, 'train/kl_loss': 0.2271484375, 'train/mask_bce_loss': 0.045699473842978476, 'train/mask_dice_loss': 0.3755097329616547, 'train/mask_loss': 0.42120920717716215, 'metrics/total_secs_per_batch': 4.944826126098633, 'metrics/data_secs_per_batch': 2.2912956714630126, '_timestamp': 1740959371.8330486}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 344 is less than current step: 499. Dropping entry: {'train/lr': 0.0001416734693877551, '_timestamp': 1740959371.8333519}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 345 is less than current step: 499. Dropping entry: {'train/loss': 1.3252561450004579, 'train/ce_loss': 0.567431640625, 'train/seg_cls_loss': 0.007757568359375, 'train/kl_loss': 0.1501953125, 'train/mask_bce_loss': 0.0614596139639616, 'train/mask_dice_loss': 0.3079799860715866, 'train/mask_loss': 0.3694396078586578, 'metrics/total_secs_per_batch': 5.663538217544556, 'metrics/data_secs_per_batch': 2.666455888748169, '_timestamp': 1740959377.496852}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 345 is less than current step: 499. Dropping entry: {'train/lr': 0.00014155102040816326, '_timestamp': 1740959377.497233}).
Epoch: [2][347/500]	Time  6.038 ( 6.038)	Loss 1.8055 (1.6180)	CeLoss 0.3027 (0.4650)	SegCLSLoss 0.0116 (0.0113)	KLLoss 0.3945 (0.2666)	MaskLoss 0.7289 (0.5604)	MaskBCELoss 0.1351 (0.1080)	MaskDICELoss 0.5938 (0.4524)
Epoch: [2][348/500]	Time  6.676 ( 6.676)	Loss 0.4395 (1.6434)	CeLoss 0.4395 (0.2807)	SegCLSLoss 0.0000 (0.0156)	KLLoss 0.0000 (0.2695)	MaskLoss 0.0000 (0.6640)	MaskBCELoss 0.0000 (0.1434)	MaskDICELoss 0.0000 (0.5205)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 346 is less than current step: 499. Dropping entry: {'train/loss': 1.6179518580436707, 'train/ce_loss': 0.4650390625, 'train/seg_cls_loss': 0.011309814453125, 'train/kl_loss': 0.2666015625, 'train/mask_bce_loss': 0.10801426209509372, 'train/mask_dice_loss': 0.4523776829242706, 'train/mask_loss': 0.5603919446468353, 'metrics/total_secs_per_batch': 6.037520170211792, 'metrics/data_secs_per_batch': 2.663499855995178, '_timestamp': 1740959383.5340946}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 346 is less than current step: 499. Dropping entry: {'train/lr': 0.0001414285714285714, '_timestamp': 1740959383.5343692}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 347 is less than current step: 499. Dropping entry: {'train/loss': 1.643383002281189, 'train/ce_loss': 0.280712890625, 'train/seg_cls_loss': 0.0156005859375, 'train/kl_loss': 0.26953125, 'train/mask_bce_loss': 0.14341881051659583, 'train/mask_dice_loss': 0.5205334246158599, 'train/mask_loss': 0.663952249288559, 'metrics/total_secs_per_batch': 6.6761085987091064, 'metrics/data_secs_per_batch': 3.0220643281936646, '_timestamp': 1740959390.2101889}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 347 is less than current step: 499. Dropping entry: {'train/lr': 0.00014130612244897956, '_timestamp': 1740959390.210469}).
Epoch: [2][349/500]	Time  5.602 ( 5.602)	Loss 2.1108 (1.7282)	CeLoss 0.2500 (0.5845)	SegCLSLoss 0.0204 (0.0129)	KLLoss 0.3711 (0.2680)	MaskLoss 0.9070 (0.5552)	MaskBCELoss 0.0286 (0.0689)	MaskDICELoss 0.8784 (0.4863)
[2025-03-02 17:50:01,686] [INFO] [logging.py:96:log_dist] [Rank 0] step=1350, skipped=0, lr=[0.00014112244897959182], mom=[(0.9, 0.95)]
[2025-03-02 17:50:01,691] [INFO] [timer.py:215:stop] epoch=0/micro_step=13500/global_step=1350, RunningAvgSamplesPerSec=1.5570830375863138, CurrSamplesPerSec=1.7013347654770086, MemAllocated=31.45GB, MaxMemAllocated=37.19GB
Epoch: [2][350/500]	Time  5.879 ( 5.879)	Loss 1.9916 (1.5866)	CeLoss 0.2246 (0.4825)	SegCLSLoss 0.0259 (0.0139)	KLLoss 0.3711 (0.3090)	MaskLoss 0.8591 (0.5331)	MaskBCELoss 0.0411 (0.1066)	MaskDICELoss 0.8180 (0.4266)
Epoch: [2][351/500]	Time  6.492 ( 6.492)	Loss 0.2949 (1.8774)	CeLoss 0.2949 (0.3340)	SegCLSLoss 0.0000 (0.0146)	KLLoss 0.0000 (0.3055)	MaskLoss 0.0000 (0.7528)	MaskBCELoss 0.0000 (0.1618)	MaskDICELoss 0.0000 (0.5910)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 348 is less than current step: 499. Dropping entry: {'train/loss': 1.7281905174255372, 'train/ce_loss': 0.58447265625, 'train/seg_cls_loss': 0.012890625, 'train/kl_loss': 0.26796875, 'train/mask_bce_loss': 0.06887278808280825, 'train/mask_dice_loss': 0.4863357499241829, 'train/mask_loss': 0.5552085340023041, 'metrics/total_secs_per_batch': 5.602357625961304, 'metrics/data_secs_per_batch': 2.346992754936218, '_timestamp': 1740959395.8125465}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 348 is less than current step: 499. Dropping entry: {'train/lr': 0.00014118367346938773, '_timestamp': 1740959395.8128283}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 349 is less than current step: 499. Dropping entry: {'train/loss': 1.5865626096725465, 'train/ce_loss': 0.48251953125, 'train/seg_cls_loss': 0.013885498046875, 'train/kl_loss': 0.308984375, 'train/mask_bce_loss': 0.10655654482543468, 'train/mask_dice_loss': 0.4265685245394707, 'train/mask_loss': 0.5331250607967377, 'metrics/total_secs_per_batch': 5.879284143447876, 'metrics/data_secs_per_batch': 2.257441282272339, '_timestamp': 1740959401.6917737}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 349 is less than current step: 499. Dropping entry: {'train/lr': 0.0001410612244897959, '_timestamp': 1740959401.6921353}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 350 is less than current step: 499. Dropping entry: {'train/loss': 1.8774295926094056, 'train/ce_loss': 0.333984375, 'train/seg_cls_loss': 0.014599609375, 'train/kl_loss': 0.30546875, 'train/mask_bce_loss': 0.16181001970544456, 'train/mask_dice_loss': 0.5910161077976227, 'train/mask_loss': 0.7528261303901672, 'metrics/total_secs_per_batch': 6.492126703262329, 'metrics/data_secs_per_batch': 2.9075832843780516, '_timestamp': 1740959408.184044}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 350 is less than current step: 499. Dropping entry: {'train/lr': 0.00014093877551020408, '_timestamp': 1740959408.1843553}).
Epoch: [2][352/500]	Time  5.318 ( 5.318)	Loss 1.6161 (1.3113)	CeLoss 0.1709 (0.4659)	SegCLSLoss 0.0199 (0.0103)	KLLoss 0.3672 (0.2313)	MaskLoss 0.6996 (0.4086)	MaskBCELoss 0.4666 (0.0902)	MaskDICELoss 0.2330 (0.3184)
Epoch: [2][353/500]	Time  6.926 ( 6.926)	Loss 2.1680 (1.7996)	CeLoss 0.2852 (0.2082)	SegCLSLoss 0.0136 (0.0166)	KLLoss 0.3867 (0.3428)	MaskLoss 0.9190 (0.7746)	MaskBCELoss 0.0134 (0.1664)	MaskDICELoss 0.9056 (0.6083)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 351 is less than current step: 499. Dropping entry: {'train/loss': 1.3113146901130677, 'train/ce_loss': 0.46591796875, 'train/seg_cls_loss': 0.01033935546875, 'train/kl_loss': 0.23125, 'train/mask_bce_loss': 0.09019727697595954, 'train/mask_dice_loss': 0.3184385895729065, 'train/mask_loss': 0.4086358606815338, 'metrics/total_secs_per_batch': 5.317706346511841, 'metrics/data_secs_per_batch': 2.4650905609130858, '_timestamp': 1740959413.5017223}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 351 is less than current step: 499. Dropping entry: {'train/lr': 0.00014081632653061224, '_timestamp': 1740959413.5020041}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 352 is less than current step: 499. Dropping entry: {'train/loss': 1.7995867609977723, 'train/ce_loss': 0.208203125, 'train/seg_cls_loss': 0.01656494140625, 'train/kl_loss': 0.3427734375, 'train/mask_bce_loss': 0.1663724103011191, 'train/mask_dice_loss': 0.6082744911313057, 'train/mask_loss': 0.7746468961238862, 'metrics/total_secs_per_batch': 6.926391124725342, 'metrics/data_secs_per_batch': 2.962001991271973, '_timestamp': 1740959420.4281175}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 352 is less than current step: 499. Dropping entry: {'train/lr': 0.0001406938775510204, '_timestamp': 1740959420.4284189}).
Epoch: [2][354/500]	Time  6.642 ( 6.642)	Loss 2.5246 (2.0274)	CeLoss 0.2598 (0.3005)	SegCLSLoss 0.0142 (0.0176)	KLLoss 0.3848 (0.3398)	MaskLoss 1.1090 (0.8420)	MaskBCELoss 0.4179 (0.2550)	MaskDICELoss 0.6911 (0.5870)
Epoch: [2][355/500]	Time  7.237 ( 7.237)	Loss 2.7530 (1.6032)	CeLoss 0.2041 (0.2791)	SegCLSLoss 0.0215 (0.0115)	KLLoss 0.3691 (0.2680)	MaskLoss 1.2505 (0.6459)	MaskBCELoss 0.4668 (0.1179)	MaskDICELoss 0.7837 (0.5281)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 353 is less than current step: 499. Dropping entry: {'train/loss': 2.027427542209625, 'train/ce_loss': 0.30048828125, 'train/seg_cls_loss': 0.01761474609375, 'train/kl_loss': 0.33984375, 'train/mask_bce_loss': 0.25501075349748137, 'train/mask_dice_loss': 0.5869745016098022, 'train/mask_loss': 0.8419852495193482, 'metrics/total_secs_per_batch': 6.641846179962158, 'metrics/data_secs_per_batch': 2.86911301612854, '_timestamp': 1740959427.0699198}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 353 is less than current step: 499. Dropping entry: {'train/lr': 0.00014057142857142857, '_timestamp': 1740959427.070195}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 354 is less than current step: 499. Dropping entry: {'train/loss': 1.6032267451286315, 'train/ce_loss': 0.2791015625, 'train/seg_cls_loss': 0.011529541015625, 'train/kl_loss': 0.26796875, 'train/mask_bce_loss': 0.11785354036837817, 'train/mask_dice_loss': 0.5280957698822022, 'train/mask_loss': 0.6459493041038513, 'metrics/total_secs_per_batch': 7.23730206489563, 'metrics/data_secs_per_batch': 2.969628190994263, '_timestamp': 1740959434.3072715}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 354 is less than current step: 499. Dropping entry: {'train/lr': 0.00014044897959183673, '_timestamp': 1740959434.3075635}).
Epoch: [2][356/500]	Time  6.536 ( 6.536)	Loss 1.5151 (1.6996)	CeLoss 0.2432 (0.3298)	SegCLSLoss 0.0177 (0.0175)	KLLoss 0.3867 (0.3422)	MaskLoss 0.6120 (0.6635)	MaskBCELoss 0.0660 (0.0535)	MaskDICELoss 0.5460 (0.6100)
Epoch: [2][357/500]	Time  6.250 ( 6.250)	Loss 0.0869 (1.6944)	CeLoss 0.0869 (0.3925)	SegCLSLoss 0.0000 (0.0156)	KLLoss 0.0000 (0.2650)	MaskLoss 0.0000 (0.6339)	MaskBCELoss 0.0000 (0.1206)	MaskDICELoss 0.0000 (0.5133)
Epoch: [2][358/500]	Time  5.320 ( 5.320)	Loss 1.7735 (1.7675)	CeLoss 0.2559 (0.5813)	SegCLSLoss 0.0210 (0.0115)	KLLoss 0.3789 (0.2701)	MaskLoss 0.7354 (0.5768)	MaskBCELoss 0.0172 (0.1086)	MaskDICELoss 0.7181 (0.4683)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 355 is less than current step: 499. Dropping entry: {'train/loss': 1.6995593428611755, 'train/ce_loss': 0.32978515625, 'train/seg_cls_loss': 0.01748046875, 'train/kl_loss': 0.3421875, 'train/mask_bce_loss': 0.05346128325909376, 'train/mask_dice_loss': 0.6099902540445328, 'train/mask_loss': 0.6634515404701233, 'metrics/total_secs_per_batch': 6.5359742641448975, 'metrics/data_secs_per_batch': 2.66960666179657, '_timestamp': 1740959440.8433108}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 355 is less than current step: 499. Dropping entry: {'train/lr': 0.0001403265306122449, '_timestamp': 1740959440.843634}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 356 is less than current step: 499. Dropping entry: {'train/loss': 1.6944166541099548, 'train/ce_loss': 0.39248046875, 'train/seg_cls_loss': 0.01563720703125, 'train/kl_loss': 0.2650390625, 'train/mask_bce_loss': 0.12055088840425014, 'train/mask_dice_loss': 0.5133273616433144, 'train/mask_loss': 0.6338782370090484, 'metrics/total_secs_per_batch': 6.250491380691528, 'metrics/data_secs_per_batch': 2.6694669008255003, '_timestamp': 1740959447.0939777}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 356 is less than current step: 499. Dropping entry: {'train/lr': 0.00014020408163265306, '_timestamp': 1740959447.0943372}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 357 is less than current step: 499. Dropping entry: {'train/loss': 1.7674685418605804, 'train/ce_loss': 0.58134765625, 'train/seg_cls_loss': 0.01146240234375, 'train/kl_loss': 0.2701171875, 'train/mask_bce_loss': 0.1085570702329278, 'train/mask_dice_loss': 0.4682924419641495, 'train/mask_loss': 0.5768495038151741, 'metrics/total_secs_per_batch': 5.31962513923645, 'metrics/data_secs_per_batch': 2.1079057931900023, '_timestamp': 1740959452.4133308}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 357 is less than current step: 499. Dropping entry: {'train/lr': 0.00014008163265306122, '_timestamp': 1740959452.4136126}).
Epoch: [2][359/500]	Time  5.374 ( 5.374)	Loss 0.1553 (1.5434)	CeLoss 0.1553 (0.4688)	SegCLSLoss 0.0000 (0.0115)	KLLoss 0.0000 (0.2299)	MaskLoss 0.0000 (0.5229)	MaskBCELoss 0.0000 (0.1065)	MaskDICELoss 0.0000 (0.4164)
[2025-03-02 17:51:03,395] [INFO] [logging.py:96:log_dist] [Rank 0] step=1360, skipped=0, lr=[0.00013989795918367345], mom=[(0.9, 0.95)]
[2025-03-02 17:51:03,401] [INFO] [timer.py:215:stop] epoch=0/micro_step=13600/global_step=1360, RunningAvgSamplesPerSec=1.5575340107103302, CurrSamplesPerSec=1.7816153594526514, MemAllocated=31.23GB, MaxMemAllocated=37.19GB
Epoch: [2][360/500]	Time  5.614 ( 5.614)	Loss 1.2371 (1.6893)	CeLoss 0.4199 (0.6307)	SegCLSLoss 0.0121 (0.0097)	KLLoss 0.3926 (0.2291)	MaskLoss 0.3861 (0.5154)	MaskBCELoss 0.0652 (0.1398)	MaskDICELoss 0.3209 (0.3756)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 358 is less than current step: 499. Dropping entry: {'train/loss': 1.5433647871017455, 'train/ce_loss': 0.46884765625, 'train/seg_cls_loss': 0.0115478515625, 'train/kl_loss': 0.2298828125, 'train/mask_bce_loss': 0.10647290982306004, 'train/mask_dice_loss': 0.41638135313987734, 'train/mask_loss': 0.5228542685508728, 'metrics/total_secs_per_batch': 5.374162435531616, 'metrics/data_secs_per_batch': 2.5016160011291504, '_timestamp': 1740959457.7875032}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 358 is less than current step: 499. Dropping entry: {'train/lr': 0.00013995918367346938, '_timestamp': 1740959457.787709}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 359 is less than current step: 499. Dropping entry: {'train/loss': 1.6893060684204102, 'train/ce_loss': 0.6306640625, 'train/seg_cls_loss': 0.009710693359375, 'train/kl_loss': 0.2291015625, 'train/mask_bce_loss': 0.13980130320414902, 'train/mask_dice_loss': 0.3755548506975174, 'train/mask_loss': 0.5153561428189277, 'metrics/total_secs_per_batch': 5.61445164680481, 'metrics/data_secs_per_batch': 2.462424111366272, '_timestamp': 1740959463.401906}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 359 is less than current step: 499. Dropping entry: {'train/lr': 0.00013983673469387752, '_timestamp': 1740959463.4022772}).
Epoch: [2][361/500]	Time  6.954 ( 6.954)	Loss 0.7930 (1.9025)	CeLoss 0.7930 (0.2863)	SegCLSLoss 0.0000 (0.0184)	KLLoss 0.0000 (0.3430)	MaskLoss 0.0000 (0.7865)	MaskBCELoss 0.0000 (0.1458)	MaskDICELoss 0.0000 (0.6407)
Epoch: [2][362/500]	Time  5.570 ( 5.570)	Loss 0.8789 (1.5500)	CeLoss 0.8789 (0.5611)	SegCLSLoss 0.0000 (0.0116)	KLLoss 0.0000 (0.2648)	MaskLoss 0.0000 (0.4782)	MaskBCELoss 0.0000 (0.0780)	MaskDICELoss 0.0000 (0.4002)
Epoch: [2][363/500]	Time  6.181 ( 6.181)	Loss 2.6326 (1.9463)	CeLoss 0.1895 (0.4683)	SegCLSLoss 0.0332 (0.0166)	KLLoss 0.3691 (0.3031)	MaskLoss 1.1947 (0.7198)	MaskBCELoss 0.7447 (0.1705)	MaskDICELoss 0.4500 (0.5493)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 360 is less than current step: 499. Dropping entry: {'train/loss': 1.90252765417099, 'train/ce_loss': 0.286328125, 'train/seg_cls_loss': 0.018353271484375, 'train/kl_loss': 0.34296875, 'train/mask_bce_loss': 0.14575015399605035, 'train/mask_dice_loss': 0.6407187551259994, 'train/mask_loss': 0.7864689111709595, 'metrics/total_secs_per_batch': 6.9535698890686035, 'metrics/data_secs_per_batch': 3.129999303817749, '_timestamp': 1740959470.3555744}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 360 is less than current step: 499. Dropping entry: {'train/lr': 0.00013971428571428568, '_timestamp': 1740959470.355862}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 361 is less than current step: 499. Dropping entry: {'train/loss': 1.5499904453754425, 'train/ce_loss': 0.5611328125, 'train/seg_cls_loss': 0.01162109375, 'train/kl_loss': 0.26484375, 'train/mask_bce_loss': 0.07801750916987657, 'train/mask_dice_loss': 0.4001515477895737, 'train/mask_loss': 0.4781690567731857, 'metrics/total_secs_per_batch': 5.5701987743377686, 'metrics/data_secs_per_batch': 2.730879378318787, '_timestamp': 1740959475.9257843}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 361 is less than current step: 499. Dropping entry: {'train/lr': 0.00013959183673469385, '_timestamp': 1740959475.9260848}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 362 is less than current step: 499. Dropping entry: {'train/loss': 1.9463453769683838, 'train/ce_loss': 0.46826171875, 'train/seg_cls_loss': 0.016583251953125, 'train/kl_loss': 0.303125, 'train/mask_bce_loss': 0.17050066422671079, 'train/mask_dice_loss': 0.5493028819561004, 'train/mask_loss': 0.7198035538196563, 'metrics/total_secs_per_batch': 6.181442737579346, 'metrics/data_secs_per_batch': 2.5883164405822754, '_timestamp': 1740959482.1074796}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 362 is less than current step: 499. Dropping entry: {'train/lr': 0.00013946938775510204, '_timestamp': 1740959482.1078575}).
Epoch: [2][364/500]	Time  6.343 ( 6.343)	Loss 0.0771 (1.4649)	CeLoss 0.0771 (0.4792)	SegCLSLoss 0.0000 (0.0101)	KLLoss 0.0000 (0.2336)	MaskLoss 0.0000 (0.4786)	MaskBCELoss 0.0000 (0.1145)	MaskDICELoss 0.0000 (0.3641)
Epoch: [2][365/500]	Time  6.000 ( 6.000)	Loss 0.6442 (1.9399)	CeLoss 0.2637 (0.2414)	SegCLSLoss 0.0102 (0.0146)	KLLoss 0.3945 (0.3906)	MaskLoss 0.1678 (0.8260)	MaskBCELoss 0.1140 (0.1576)	MaskDICELoss 0.0538 (0.6684)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 363 is less than current step: 499. Dropping entry: {'train/loss': 1.4648857414722443, 'train/ce_loss': 0.4792236328125, 'train/seg_cls_loss': 0.01014404296875, 'train/kl_loss': 0.23359375, 'train/mask_bce_loss': 0.11452789977192879, 'train/mask_dice_loss': 0.36409417986869813, 'train/mask_loss': 0.4786220759153366, 'metrics/total_secs_per_batch': 6.343378305435181, 'metrics/data_secs_per_batch': 3.0404369592666627, '_timestamp': 1740959488.4506128}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 363 is less than current step: 499. Dropping entry: {'train/lr': 0.0001393469387755102, '_timestamp': 1740959488.4509013}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 364 is less than current step: 499. Dropping entry: {'train/loss': 1.9398692667484283, 'train/ce_loss': 0.24140625, 'train/seg_cls_loss': 0.01455078125, 'train/kl_loss': 0.390625, 'train/mask_bce_loss': 0.15757400617003442, 'train/mask_dice_loss': 0.6684153228998184, 'train/mask_loss': 0.8259893268346786, 'metrics/total_secs_per_batch': 6.000315427780151, 'metrics/data_secs_per_batch': 2.679400897026062, '_timestamp': 1740959494.4509208}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 364 is less than current step: 499. Dropping entry: {'train/lr': 0.00013922448979591836, '_timestamp': 1740959494.4512143}).
Epoch: [2][366/500]	Time  6.279 ( 6.279)	Loss 2.1816 (2.0973)	CeLoss 0.2070 (0.3029)	SegCLSLoss 0.0299 (0.0222)	KLLoss 0.3711 (0.3402)	MaskLoss 0.9619 (0.8747)	MaskBCELoss 0.2059 (0.2701)	MaskDICELoss 0.7560 (0.6046)
Epoch: [2][367/500]	Time  5.562 ( 5.562)	Loss 1.0781 (1.6538)	CeLoss 1.0781 (0.7339)	SegCLSLoss 0.0000 (0.0093)	KLLoss 0.0000 (0.1885)	MaskLoss 0.0000 (0.4482)	MaskBCELoss 0.0000 (0.0396)	MaskDICELoss 0.0000 (0.4087)
Epoch: [2][368/500]	Time  5.784 ( 5.784)	Loss 2.2166 (2.0161)	CeLoss 0.2012 (0.4366)	SegCLSLoss 0.0244 (0.0166)	KLLoss 0.3711 (0.2652)	MaskLoss 0.9833 (0.7723)	MaskBCELoss 0.0164 (0.2523)	MaskDICELoss 0.9669 (0.5200)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 365 is less than current step: 499. Dropping entry: {'train/loss': 2.097326922416687, 'train/ce_loss': 0.3029296875, 'train/seg_cls_loss': 0.022198486328125, 'train/kl_loss': 0.340234375, 'train/mask_bce_loss': 0.2700741391628981, 'train/mask_dice_loss': 0.6046146914362908, 'train/mask_loss': 0.8746888399124145, 'metrics/total_secs_per_batch': 6.279423952102661, 'metrics/data_secs_per_batch': 2.591241550445557, '_timestamp': 1740959500.7303455}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 365 is less than current step: 499. Dropping entry: {'train/lr': 0.00013910204081632653, '_timestamp': 1740959500.7306235}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 366 is less than current step: 499. Dropping entry: {'train/loss': 1.6538102507591248, 'train/ce_loss': 0.73388671875, 'train/seg_cls_loss': 0.00927734375, 'train/kl_loss': 0.1884765625, 'train/mask_bce_loss': 0.03956033047288656, 'train/mask_dice_loss': 0.40868268013000486, 'train/mask_loss': 0.4482430160045624, 'metrics/total_secs_per_batch': 5.56172776222229, 'metrics/data_secs_per_batch': 2.6972305536270142, '_timestamp': 1740959506.2922602}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 366 is less than current step: 499. Dropping entry: {'train/lr': 0.0001389795918367347, '_timestamp': 1740959506.2926185}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 367 is less than current step: 499. Dropping entry: {'train/loss': 2.016069746017456, 'train/ce_loss': 0.43662109375, 'train/seg_cls_loss': 0.016571044921875, 'train/kl_loss': 0.265234375, 'train/mask_bce_loss': 0.2522941691800952, 'train/mask_dice_loss': 0.5199985086917878, 'train/mask_loss': 0.772292697429657, 'metrics/total_secs_per_batch': 5.78395676612854, 'metrics/data_secs_per_batch': 2.357036328315735, '_timestamp': 1740959512.0760577}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 367 is less than current step: 499. Dropping entry: {'train/lr': 0.00013885714285714285, '_timestamp': 1740959512.076412}).
Epoch: [2][369/500]	Time  6.378 ( 6.378)	Loss 2.7373 (1.9983)	CeLoss 0.1875 (0.2516)	SegCLSLoss 0.0227 (0.0180)	KLLoss 0.3789 (0.3871)	MaskLoss 1.2500 (0.8494)	MaskBCELoss 0.2874 (0.2503)	MaskDICELoss 0.9625 (0.5991)
[2025-03-02 17:52:03,459] [INFO] [logging.py:96:log_dist] [Rank 0] step=1370, skipped=0, lr=[0.00013867346938775508], mom=[(0.9, 0.95)]
[2025-03-02 17:52:03,465] [INFO] [timer.py:215:stop] epoch=0/micro_step=13700/global_step=1370, RunningAvgSamplesPerSec=1.558270885339404, CurrSamplesPerSec=1.9960603270979291, MemAllocated=31.24GB, MaxMemAllocated=37.19GB
Epoch: [2][370/500]	Time  5.011 ( 5.011)	Loss 1.8631 (1.9119)	CeLoss 0.2539 (0.4348)	SegCLSLoss 0.0107 (0.0163)	KLLoss 0.3926 (0.3027)	MaskLoss 0.7821 (0.7193)	MaskBCELoss 0.2918 (0.1273)	MaskDICELoss 0.4903 (0.5920)
Epoch: [2][371/500]	Time  5.712 ( 5.712)	Loss 1.5625 (1.8066)	CeLoss 1.5625 (0.5271)	SegCLSLoss 0.0000 (0.0147)	KLLoss 0.0000 (0.2664)	MaskLoss 0.0000 (0.6229)	MaskBCELoss 0.0000 (0.0975)	MaskDICELoss 0.0000 (0.5254)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 368 is less than current step: 499. Dropping entry: {'train/loss': 1.9982527494430542, 'train/ce_loss': 0.2515625, 'train/seg_cls_loss': 0.01800537109375, 'train/kl_loss': 0.387109375, 'train/mask_bce_loss': 0.2502692986279726, 'train/mask_dice_loss': 0.5991012126207351, 'train/mask_loss': 0.8493704974651337, 'metrics/total_secs_per_batch': 6.378283262252808, 'metrics/data_secs_per_batch': 2.926406741142273, '_timestamp': 1740959518.4542758}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 368 is less than current step: 499. Dropping entry: {'train/lr': 0.00013873469387755102, '_timestamp': 1740959518.4544742}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 369 is less than current step: 499. Dropping entry: {'train/loss': 1.9118958353996276, 'train/ce_loss': 0.434765625, 'train/seg_cls_loss': 0.016259765625, 'train/kl_loss': 0.302734375, 'train/mask_bce_loss': 0.12734289276413618, 'train/mask_dice_loss': 0.5919839352369308, 'train/mask_loss': 0.7193268299102783, 'metrics/total_secs_per_batch': 5.011379718780518, 'metrics/data_secs_per_batch': 2.0017547845840453, '_timestamp': 1740959523.4654877}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 369 is less than current step: 499. Dropping entry: {'train/lr': 0.00013861224489795918, '_timestamp': 1740959523.465761}).
Epoch: [2][372/500]	Time  5.067 ( 5.067)	Loss 2.5994 (1.5225)	CeLoss 0.1895 (0.4667)	SegCLSLoss 0.0233 (0.0133)	KLLoss 0.3711 (0.2287)	MaskLoss 1.1806 (0.5132)	MaskBCELoss 0.2886 (0.0856)	MaskDICELoss 0.8920 (0.4276)
Epoch: [2][373/500]	Time  5.531 ( 5.531)	Loss 1.6229 (1.3729)	CeLoss 0.2793 (0.5476)	SegCLSLoss 0.0148 (0.0091)	KLLoss 0.3770 (0.1924)	MaskLoss 0.6493 (0.4007)	MaskBCELoss 0.0648 (0.0457)	MaskDICELoss 0.5845 (0.3550)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 370 is less than current step: 499. Dropping entry: {'train/loss': 1.8065774321556092, 'train/ce_loss': 0.5271484375, 'train/seg_cls_loss': 0.01466064453125, 'train/kl_loss': 0.26640625, 'train/mask_bce_loss': 0.0975413789972663, 'train/mask_dice_loss': 0.5253762483596802, 'train/mask_loss': 0.6229176253080368, 'metrics/total_secs_per_batch': 5.7118775844573975, 'metrics/data_secs_per_batch': 2.4882764339447023, '_timestamp': 1740959529.177528}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 370 is less than current step: 499. Dropping entry: {'train/lr': 0.00013848979591836734, '_timestamp': 1740959529.1778097}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 371 is less than current step: 499. Dropping entry: {'train/loss': 1.5224994719028473, 'train/ce_loss': 0.466748046875, 'train/seg_cls_loss': 0.013323974609375, 'train/kl_loss': 0.2287109375, 'train/mask_bce_loss': 0.08555679451674222, 'train/mask_dice_loss': 0.42762164026498795, 'train/mask_loss': 0.5131784364581108, 'metrics/total_secs_per_batch': 5.067192554473877, 'metrics/data_secs_per_batch': 2.078490376472473, '_timestamp': 1740959534.2447958}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 371 is less than current step: 499. Dropping entry: {'train/lr': 0.0001383673469387755, '_timestamp': 1740959534.2451103}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 372 is less than current step: 499. Dropping entry: {'train/loss': 1.3728897213935851, 'train/ce_loss': 0.54755859375, 'train/seg_cls_loss': 0.009088134765625, 'train/kl_loss': 0.1923828125, 'train/mask_bce_loss': 0.04571636281907558, 'train/mask_dice_loss': 0.35498632192611695, 'train/mask_loss': 0.40070268511772156, 'metrics/total_secs_per_batch': 5.530719518661499, 'metrics/data_secs_per_batch': 2.4097404956817625, '_timestamp': 1740959539.775531}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 372 is less than current step: 499. Dropping entry: {'train/lr': 0.00013824489795918364, '_timestamp': 1740959539.775812}).
Epoch: [2][374/500]	Time  6.422 ( 6.422)	Loss 1.8810 (1.7465)	CeLoss 0.2285 (0.3959)	SegCLSLoss 0.0240 (0.0184)	KLLoss 0.3809 (0.2996)	MaskLoss 0.8018 (0.6556)	MaskBCELoss 0.1829 (0.1552)	MaskDICELoss 0.6190 (0.5005)
Epoch: [2][375/500]	Time  5.776 ( 5.776)	Loss 2.0975 (1.5742)	CeLoss 0.1797 (0.6256)	SegCLSLoss 0.0227 (0.0104)	KLLoss 0.3711 (0.2277)	MaskLoss 0.9345 (0.4603)	MaskBCELoss 0.0721 (0.0383)	MaskDICELoss 0.8624 (0.4221)
Epoch: [2][376/500]	Time  5.778 ( 5.778)	Loss 1.8047 (1.7864)	CeLoss 1.8047 (0.5087)	SegCLSLoss 0.0000 (0.0160)	KLLoss 0.0000 (0.2619)	MaskLoss 0.0000 (0.6219)	MaskBCELoss 0.0000 (0.1103)	MaskDICELoss 0.0000 (0.5116)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 373 is less than current step: 499. Dropping entry: {'train/loss': 1.7465464532375337, 'train/ce_loss': 0.3958984375, 'train/seg_cls_loss': 0.018438720703125, 'train/kl_loss': 0.299609375, 'train/mask_bce_loss': 0.15515134260058402, 'train/mask_dice_loss': 0.5004949361085892, 'train/mask_loss': 0.6556462734937668, 'metrics/total_secs_per_batch': 6.421810150146484, 'metrics/data_secs_per_batch': 2.784475040435791, '_timestamp': 1740959546.1973436}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 373 is less than current step: 499. Dropping entry: {'train/lr': 0.00013812244897959183, '_timestamp': 1740959546.197657}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 374 is less than current step: 499. Dropping entry: {'train/loss': 1.5741888999938964, 'train/ce_loss': 0.6255859375, 'train/seg_cls_loss': 0.010400390625, 'train/kl_loss': 0.227734375, 'train/mask_bce_loss': 0.038279170310124754, 'train/mask_dice_loss': 0.42205746173858644, 'train/mask_loss': 0.46033663153648374, 'metrics/total_secs_per_batch': 5.7761945724487305, 'metrics/data_secs_per_batch': 2.2966506242752076, '_timestamp': 1740959551.9735985}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 374 is less than current step: 499. Dropping entry: {'train/lr': 0.000138, '_timestamp': 1740959551.9739134}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 375 is less than current step: 499. Dropping entry: {'train/loss': 1.7863998651504516, 'train/ce_loss': 0.50869140625, 'train/seg_cls_loss': 0.015985107421875, 'train/kl_loss': 0.2619140625, 'train/mask_bce_loss': 0.11033771373331547, 'train/mask_dice_loss': 0.5115731537342072, 'train/mask_loss': 0.6219108700752258, 'metrics/total_secs_per_batch': 5.777932405471802, 'metrics/data_secs_per_batch': 3.0082579612731934, '_timestamp': 1740959557.751432}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 375 is less than current step: 499. Dropping entry: {'train/lr': 0.00013787755102040816, '_timestamp': 1740959557.7517006}).
Epoch: [2][377/500]	Time  5.610 ( 5.610)	Loss 2.9419 (1.8736)	CeLoss 0.3359 (0.2228)	SegCLSLoss 0.0179 (0.0188)	KLLoss 0.3691 (0.3010)	MaskLoss 1.2796 (0.8055)	MaskBCELoss 0.4154 (0.1977)	MaskDICELoss 0.8642 (0.6078)
Epoch: [2][378/500]	Time  6.103 ( 6.103)	Loss 2.2876 (1.4557)	CeLoss 0.2246 (0.3957)	SegCLSLoss 0.0209 (0.0124)	KLLoss 0.3633 (0.2633)	MaskLoss 1.0081 (0.5138)	MaskBCELoss 0.1344 (0.0489)	MaskDICELoss 0.8737 (0.4649)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 376 is less than current step: 499. Dropping entry: {'train/loss': 1.8735652089118957, 'train/ce_loss': 0.2228271484375, 'train/seg_cls_loss': 0.0188232421875, 'train/kl_loss': 0.3009765625, 'train/mask_bce_loss': 0.19768535643815993, 'train/mask_dice_loss': 0.6078106105327606, 'train/mask_loss': 0.8054959714412689, 'metrics/total_secs_per_batch': 5.610411882400513, 'metrics/data_secs_per_batch': 2.703122854232788, '_timestamp': 1740959563.3619158}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 376 is less than current step: 499. Dropping entry: {'train/lr': 0.00013775510204081632, '_timestamp': 1740959563.3622031}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 377 is less than current step: 499. Dropping entry: {'train/loss': 1.45568288564682, 'train/ce_loss': 0.3956787109375, 'train/seg_cls_loss': 0.01243896484375, 'train/kl_loss': 0.26328125, 'train/mask_bce_loss': 0.04890333004295826, 'train/mask_dice_loss': 0.4648878127336502, 'train/mask_loss': 0.5137911379337311, 'metrics/total_secs_per_batch': 6.1029205322265625, 'metrics/data_secs_per_batch': 2.7275883197784423, '_timestamp': 1740959569.4647954}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 377 is less than current step: 499. Dropping entry: {'train/lr': 0.00013763265306122448, '_timestamp': 1740959569.4650817}).
Epoch: [2][379/500]	Time  5.883 ( 5.883)	Loss 1.4199 (1.7164)	CeLoss 0.2656 (0.4451)	SegCLSLoss 0.0104 (0.0111)	KLLoss 0.3906 (0.2639)	MaskLoss 0.5547 (0.6196)	MaskBCELoss 0.1237 (0.1083)	MaskDICELoss 0.4309 (0.5113)
[2025-03-02 17:53:01,616] [INFO] [logging.py:96:log_dist] [Rank 0] step=1380, skipped=0, lr=[0.0001374489795918367], mom=[(0.9, 0.95)]
[2025-03-02 17:53:01,621] [INFO] [timer.py:215:stop] epoch=0/micro_step=13800/global_step=1380, RunningAvgSamplesPerSec=1.5593340646054563, CurrSamplesPerSec=1.5942448965815237, MemAllocated=31.15GB, MaxMemAllocated=37.19GB
Epoch: [2][380/500]	Time  6.274 ( 6.274)	Loss 1.1193 (1.6303)	CeLoss 0.1885 (0.2997)	SegCLSLoss 0.0269 (0.0140)	KLLoss 0.3926 (0.3451)	MaskLoss 0.4390 (0.6446)	MaskBCELoss 0.1895 (0.1148)	MaskDICELoss 0.2495 (0.5298)
Epoch: [2][381/500]	Time  5.891 ( 5.891)	Loss 1.9099 (1.6457)	CeLoss 0.2656 (0.4717)	SegCLSLoss 0.0193 (0.0134)	KLLoss 0.3750 (0.2674)	MaskLoss 0.7987 (0.5703)	MaskBCELoss 0.2701 (0.1557)	MaskDICELoss 0.5286 (0.4146)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 378 is less than current step: 499. Dropping entry: {'train/loss': 1.7163574814796447, 'train/ce_loss': 0.4451171875, 'train/seg_cls_loss': 0.0111328125, 'train/kl_loss': 0.2638671875, 'train/mask_bce_loss': 0.10829878337681294, 'train/mask_dice_loss': 0.5113057315349578, 'train/mask_loss': 0.6196045100688934, 'metrics/total_secs_per_batch': 5.883342742919922, 'metrics/data_secs_per_batch': 2.4589165687561034, '_timestamp': 1740959575.3481472}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 378 is less than current step: 499. Dropping entry: {'train/lr': 0.00013751020408163265, '_timestamp': 1740959575.3484442}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 379 is less than current step: 499. Dropping entry: {'train/loss': 1.6303239226341248, 'train/ce_loss': 0.29970703125, 'train/seg_cls_loss': 0.01396484375, 'train/kl_loss': 0.3451171875, 'train/mask_bce_loss': 0.11477560009807349, 'train/mask_dice_loss': 0.5298297196626663, 'train/mask_loss': 0.6446053206920623, 'metrics/total_secs_per_batch': 6.274250745773315, 'metrics/data_secs_per_batch': 2.9397104024887084, '_timestamp': 1740959581.6223242}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 379 is less than current step: 499. Dropping entry: {'train/lr': 0.0001373877551020408, '_timestamp': 1740959581.622711}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 380 is less than current step: 499. Dropping entry: {'train/loss': 1.6456846237182616, 'train/ce_loss': 0.471728515625, 'train/seg_cls_loss': 0.013409423828125, 'train/kl_loss': 0.2673828125, 'train/mask_bce_loss': 0.15568714458495378, 'train/mask_dice_loss': 0.41456726789474485, 'train/mask_loss': 0.5702544152736664, 'metrics/total_secs_per_batch': 5.890625953674316, 'metrics/data_secs_per_batch': 2.5658979415893555, '_timestamp': 1740959587.513035}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 380 is less than current step: 499. Dropping entry: {'train/lr': 0.00013726530612244897, '_timestamp': 1740959587.5133433}).
Epoch: [2][382/500]	Time  5.824 ( 5.824)	Loss 1.2956 (1.3304)	CeLoss 0.2480 (0.3131)	SegCLSLoss 0.0105 (0.0147)	KLLoss 0.3789 (0.2650)	MaskLoss 0.5023 (0.4917)	MaskBCELoss 0.0946 (0.0611)	MaskDICELoss 0.4077 (0.4306)
Epoch: [2][383/500]	Time  6.384 ( 6.384)	Loss 1.3457 (1.5539)	CeLoss 0.2754 (0.3388)	SegCLSLoss 0.0125 (0.0131)	KLLoss 0.3828 (0.2641)	MaskLoss 0.5127 (0.5912)	MaskBCELoss 0.0574 (0.0867)	MaskDICELoss 0.4553 (0.5045)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 381 is less than current step: 499. Dropping entry: {'train/loss': 1.3303730130195617, 'train/ce_loss': 0.3130859375, 'train/seg_cls_loss': 0.01474609375, 'train/kl_loss': 0.2650390625, 'train/mask_bce_loss': 0.061050707614049314, 'train/mask_dice_loss': 0.43064945936203003, 'train/mask_loss': 0.49170016646385195, 'metrics/total_secs_per_batch': 5.824400901794434, 'metrics/data_secs_per_batch': 2.388859987258911, '_timestamp': 1740959593.3373952}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 381 is less than current step: 499. Dropping entry: {'train/lr': 0.00013714285714285713, '_timestamp': 1740959593.3375845}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 382 is less than current step: 499. Dropping entry: {'train/loss': 1.5539379000663758, 'train/ce_loss': 0.338818359375, 'train/seg_cls_loss': 0.013079833984375, 'train/kl_loss': 0.2640625, 'train/mask_bce_loss': 0.0867365587502718, 'train/mask_dice_loss': 0.5044657915830613, 'train/mask_loss': 0.5912023544311523, 'metrics/total_secs_per_batch': 6.383591413497925, 'metrics/data_secs_per_batch': 2.765507674217224, '_timestamp': 1740959599.7212512}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 382 is less than current step: 499. Dropping entry: {'train/lr': 0.0001370204081632653, '_timestamp': 1740959599.721622}).
Epoch: [2][384/500]	Time  5.408 ( 5.408)	Loss 2.6805 (1.5854)	CeLoss 0.2285 (0.6502)	SegCLSLoss 0.0236 (0.0118)	KLLoss 0.3711 (0.2277)	MaskLoss 1.2016 (0.4532)	MaskBCELoss 0.3564 (0.0645)	MaskDICELoss 0.8452 (0.3888)
Epoch: [2][385/500]	Time  6.539 ( 6.539)	Loss 1.4922 (1.7949)	CeLoss 1.4922 (0.3640)	SegCLSLoss 0.0000 (0.0165)	KLLoss 0.0000 (0.3039)	MaskLoss 0.0000 (0.6961)	MaskBCELoss 0.0000 (0.1393)	MaskDICELoss 0.0000 (0.5569)
Epoch: [2][386/500]	Time  7.630 ( 7.630)	Loss 2.2472 (2.1325)	CeLoss 0.1719 (0.2252)	SegCLSLoss 0.0277 (0.0198)	KLLoss 0.3633 (0.3754)	MaskLoss 1.0128 (0.9299)	MaskBCELoss 0.0270 (0.1873)	MaskDICELoss 0.9858 (0.7426)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 383 is less than current step: 499. Dropping entry: {'train/loss': 1.5853565096855164, 'train/ce_loss': 0.6501953125, 'train/seg_cls_loss': 0.0118408203125, 'train/kl_loss': 0.227734375, 'train/mask_bce_loss': 0.06447177696973086, 'train/mask_dice_loss': 0.3887533456087112, 'train/mask_loss': 0.4532251268625259, 'metrics/total_secs_per_batch': 5.407999515533447, 'metrics/data_secs_per_batch': 2.5058290719985963, '_timestamp': 1740959605.1289947}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 383 is less than current step: 499. Dropping entry: {'train/lr': 0.00013689795918367346, '_timestamp': 1740959605.1292913}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 384 is less than current step: 499. Dropping entry: {'train/loss': 1.7949221730232239, 'train/ce_loss': 0.36396484375, 'train/seg_cls_loss': 0.01651611328125, 'train/kl_loss': 0.30390625, 'train/mask_bce_loss': 0.13928011655807496, 'train/mask_dice_loss': 0.5568626165390015, 'train/mask_loss': 0.696142727136612, 'metrics/total_secs_per_batch': 6.538626670837402, 'metrics/data_secs_per_batch': 3.2546700477600097, '_timestamp': 1740959611.6676357}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 384 is less than current step: 499. Dropping entry: {'train/lr': 0.00013677551020408162, '_timestamp': 1740959611.6679034}).
Epoch: [2][387/500]	Time  5.605 ( 5.605)	Loss 0.9609 (1.4705)	CeLoss 0.9609 (0.5004)	SegCLSLoss 0.0000 (0.0146)	KLLoss 0.0000 (0.2645)	MaskLoss 0.0000 (0.4681)	MaskBCELoss 0.0000 (0.0518)	MaskDICELoss 0.0000 (0.4164)
Epoch: [2][388/500]	Time  6.041 ( 6.041)	Loss 1.0469 (1.7966)	CeLoss 1.0469 (0.4505)	SegCLSLoss 0.0000 (0.0139)	KLLoss 0.0000 (0.2629)	MaskLoss 0.0000 (0.6564)	MaskBCELoss 0.0000 (0.0791)	MaskDICELoss 0.0000 (0.5773)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 385 is less than current step: 499. Dropping entry: {'train/loss': 2.132538676261902, 'train/ce_loss': 0.2251953125, 'train/seg_cls_loss': 0.0197509765625, 'train/kl_loss': 0.375390625, 'train/mask_bce_loss': 0.18728714101016522, 'train/mask_dice_loss': 0.7426052272319794, 'train/mask_loss': 0.9298923671245575, 'metrics/total_secs_per_batch': 7.6301751136779785, 'metrics/data_secs_per_batch': 3.295158576965332, '_timestamp': 1740959619.2979462}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 385 is less than current step: 499. Dropping entry: {'train/lr': 0.0001366530612244898, '_timestamp': 1740959619.2981777}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 386 is less than current step: 499. Dropping entry: {'train/loss': 1.4704668283462525, 'train/ce_loss': 0.500390625, 'train/seg_cls_loss': 0.014599609375, 'train/kl_loss': 0.264453125, 'train/mask_bce_loss': 0.051766563951969144, 'train/mask_dice_loss': 0.41637700498104097, 'train/mask_loss': 0.4681435704231262, 'metrics/total_secs_per_batch': 5.6047868728637695, 'metrics/data_secs_per_batch': 2.408981776237488, '_timestamp': 1740959624.9026666}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 386 is less than current step: 499. Dropping entry: {'train/lr': 0.00013653061224489795, '_timestamp': 1740959624.9029572}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 387 is less than current step: 499. Dropping entry: {'train/loss': 1.7965968608856202, 'train/ce_loss': 0.45048828125, 'train/seg_cls_loss': 0.013885498046875, 'train/kl_loss': 0.262890625, 'train/mask_bce_loss': 0.07905346127226949, 'train/mask_dice_loss': 0.5773015975952148, 'train/mask_loss': 0.6563550472259522, 'metrics/total_secs_per_batch': 6.041261672973633, 'metrics/data_secs_per_batch': 2.6104833602905275, '_timestamp': 1740959630.943842}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 387 is less than current step: 499. Dropping entry: {'train/lr': 0.0001364081632653061, '_timestamp': 1740959630.944116}).
Epoch: [2][389/500]	Time  6.454 ( 6.454)	Loss 2.4113 (1.4089)	CeLoss 0.2949 (0.3341)	SegCLSLoss 0.0197 (0.0150)	KLLoss 0.3652 (0.2217)	MaskLoss 1.0348 (0.5226)	MaskBCELoss 0.0527 (0.0704)	MaskDICELoss 0.9821 (0.4522)
[2025-03-02 17:54:02,810] [INFO] [logging.py:96:log_dist] [Rank 0] step=1390, skipped=0, lr=[0.00013622448979591834], mom=[(0.9, 0.95)]
[2025-03-02 17:54:02,816] [INFO] [timer.py:215:stop] epoch=0/micro_step=13900/global_step=1390, RunningAvgSamplesPerSec=1.559850723234073, CurrSamplesPerSec=1.8459758772118031, MemAllocated=30.72GB, MaxMemAllocated=37.19GB
Epoch: [2][390/500]	Time  5.419 ( 5.419)	Loss 1.1250 (1.3993)	CeLoss 1.1250 (0.5587)	SegCLSLoss 0.0000 (0.0091)	KLLoss 0.0000 (0.2271)	MaskLoss 0.0000 (0.4068)	MaskBCELoss 0.0000 (0.0771)	MaskDICELoss 0.0000 (0.3297)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 388 is less than current step: 499. Dropping entry: {'train/loss': 1.4089025497436523, 'train/ce_loss': 0.3340576171875, 'train/seg_cls_loss': 0.0150390625, 'train/kl_loss': 0.2216796875, 'train/mask_bce_loss': 0.07041715793311595, 'train/mask_dice_loss': 0.45216155648231504, 'train/mask_loss': 0.5225787162780762, 'metrics/total_secs_per_batch': 6.454172372817993, 'metrics/data_secs_per_batch': 2.7390558242797853, '_timestamp': 1740959637.398034}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 388 is less than current step: 499. Dropping entry: {'train/lr': 0.00013628571428571428, '_timestamp': 1740959637.3983173}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 389 is less than current step: 499. Dropping entry: {'train/loss': 1.3993282437324523, 'train/ce_loss': 0.55869140625, 'train/seg_cls_loss': 0.009112548828125, 'train/kl_loss': 0.2271484375, 'train/mask_bce_loss': 0.0770711712539196, 'train/mask_dice_loss': 0.32972186207771303, 'train/mask_loss': 0.40679303333163264, 'metrics/total_secs_per_batch': 5.41875696182251, 'metrics/data_secs_per_batch': 2.729964327812195, '_timestamp': 1740959642.8167124}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 389 is less than current step: 499. Dropping entry: {'train/lr': 0.00013616326530612244, '_timestamp': 1740959642.8170736}).
Epoch: [2][391/500]	Time  8.148 ( 8.148)	Loss 2.1570 (2.0217)	CeLoss 0.1904 (0.3025)	SegCLSLoss 0.0203 (0.0169)	KLLoss 0.3711 (0.2998)	MaskLoss 0.9599 (0.8404)	MaskBCELoss 0.0508 (0.2406)	MaskDICELoss 0.9091 (0.5998)
Epoch: [2][392/500]	Time  4.465 ( 4.465)	Loss 1.0078 (1.6814)	CeLoss 1.0078 (0.9678)	SegCLSLoss 0.0000 (0.0066)	KLLoss 0.0000 (0.1484)	MaskLoss 0.0000 (0.3477)	MaskBCELoss 0.0000 (0.0867)	MaskDICELoss 0.0000 (0.2610)
Epoch: [2][393/500]	Time  5.746 ( 5.746)	Loss 1.4934 (1.9691)	CeLoss 0.3457 (0.4499)	SegCLSLoss 0.0124 (0.0164)	KLLoss 0.3711 (0.2982)	MaskLoss 0.5524 (0.7406)	MaskBCELoss 0.0744 (0.1458)	MaskDICELoss 0.4780 (0.5948)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 390 is less than current step: 499. Dropping entry: {'train/loss': 2.0216889142990113, 'train/ce_loss': 0.3025390625, 'train/seg_cls_loss': 0.01690673828125, 'train/kl_loss': 0.2998046875, 'train/mask_bce_loss': 0.2405968432314694, 'train/mask_dice_loss': 0.5997886478900909, 'train/mask_loss': 0.8403854966163635, 'metrics/total_secs_per_batch': 8.147916555404663, 'metrics/data_secs_per_batch': 2.7880632400512697, '_timestamp': 1740959650.9649005}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 390 is less than current step: 499. Dropping entry: {'train/lr': 0.0001360408163265306, '_timestamp': 1740959650.9654174}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 391 is less than current step: 499. Dropping entry: {'train/loss': 1.6813619494438172, 'train/ce_loss': 0.9677734375, 'train/seg_cls_loss': 0.0065673828125, 'train/kl_loss': 0.1484375, 'train/mask_bce_loss': 0.08670517653226853, 'train/mask_dice_loss': 0.2610070377588272, 'train/mask_loss': 0.34771221280097964, 'metrics/total_secs_per_batch': 4.4647321701049805, 'metrics/data_secs_per_batch': 1.9946114778518678, '_timestamp': 1740959655.4294162}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 391 is less than current step: 499. Dropping entry: {'train/lr': 0.00013591836734693877, '_timestamp': 1740959655.429682}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 392 is less than current step: 499. Dropping entry: {'train/loss': 1.9691064357757568, 'train/ce_loss': 0.44990234375, 'train/seg_cls_loss': 0.016375732421875, 'train/kl_loss': 0.2982421875, 'train/mask_bce_loss': 0.14576626624912023, 'train/mask_dice_loss': 0.5948416411876678, 'train/mask_loss': 0.7406079173088074, 'metrics/total_secs_per_batch': 5.746028900146484, 'metrics/data_secs_per_batch': 2.518754816055298, '_timestamp': 1740959661.1754735}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 392 is less than current step: 499. Dropping entry: {'train/lr': 0.00013579591836734693, '_timestamp': 1740959661.1758199}).
Epoch: [2][394/500]	Time  5.601 ( 5.601)	Loss 1.2422 (1.5833)	CeLoss 1.2422 (0.4899)	SegCLSLoss 0.0000 (0.0128)	KLLoss 0.0000 (0.2207)	MaskLoss 0.0000 (0.5325)	MaskBCELoss 0.0000 (0.0673)	MaskDICELoss 0.0000 (0.4652)
Epoch: [2][395/500]	Time  5.743 ( 5.743)	Loss 0.1357 (1.7855)	CeLoss 0.1357 (0.3227)	SegCLSLoss 0.0000 (0.0216)	KLLoss 0.0000 (0.2947)	MaskLoss 0.0000 (0.7113)	MaskBCELoss 0.0000 (0.1372)	MaskDICELoss 0.0000 (0.5741)
Epoch: [2][396/500]	Time  5.624 ( 5.624)	Loss 2.3308 (1.7451)	CeLoss 0.2129 (0.4729)	SegCLSLoss 0.0192 (0.0115)	KLLoss 0.3867 (0.2623)	MaskLoss 1.0345 (0.6199)	MaskBCELoss 0.3174 (0.1966)	MaskDICELoss 0.7171 (0.4233)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 393 is less than current step: 499. Dropping entry: {'train/loss': 1.5832706928253173, 'train/ce_loss': 0.48994140625, 'train/seg_cls_loss': 0.012786865234375, 'train/kl_loss': 0.220703125, 'train/mask_bce_loss': 0.06725928811356426, 'train/mask_dice_loss': 0.46519635766744616, 'train/mask_loss': 0.532455638051033, 'metrics/total_secs_per_batch': 5.600683927536011, 'metrics/data_secs_per_batch': 2.3992995500564573, '_timestamp': 1740959666.7762349}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 393 is less than current step: 499. Dropping entry: {'train/lr': 0.0001356734693877551, '_timestamp': 1740959666.776522}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 394 is less than current step: 499. Dropping entry: {'train/loss': 1.7855403661727904, 'train/ce_loss': 0.32265625, 'train/seg_cls_loss': 0.0216064453125, 'train/kl_loss': 0.2947265625, 'train/mask_bce_loss': 0.13719908632338046, 'train/mask_dice_loss': 0.5741257905960083, 'train/mask_loss': 0.711324867606163, 'metrics/total_secs_per_batch': 5.742927551269531, 'metrics/data_secs_per_batch': 2.762060308456421, '_timestamp': 1740959672.5191135}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 394 is less than current step: 499. Dropping entry: {'train/lr': 0.00013555102040816325, '_timestamp': 1740959672.5193927}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 395 is less than current step: 499. Dropping entry: {'train/loss': 1.74510115981102, 'train/ce_loss': 0.47294921875, 'train/seg_cls_loss': 0.011517333984375, 'train/kl_loss': 0.2623046875, 'train/mask_bce_loss': 0.19664885252714157, 'train/mask_dice_loss': 0.42326499670743944, 'train/mask_loss': 0.61991386115551, 'metrics/total_secs_per_batch': 5.624460220336914, 'metrics/data_secs_per_batch': 2.2803314447402956, '_timestamp': 1740959678.143535}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 395 is less than current step: 499. Dropping entry: {'train/lr': 0.00013542857142857142, '_timestamp': 1740959678.1438153}).
Epoch: [2][397/500]	Time  5.913 ( 5.913)	Loss 2.0230 (1.4889)	CeLoss 0.2695 (0.5928)	SegCLSLoss 0.0143 (0.0110)	KLLoss 0.3770 (0.1875)	MaskLoss 0.8543 (0.4359)	MaskBCELoss 0.1242 (0.0910)	MaskDICELoss 0.7301 (0.3449)
Epoch: [2][398/500]	Time  5.832 ( 5.832)	Loss 2.7037 (1.7264)	CeLoss 0.2559 (0.4519)	SegCLSLoss 0.0153 (0.0122)	KLLoss 0.3848 (0.2588)	MaskLoss 1.2005 (0.6213)	MaskBCELoss 0.5659 (0.1474)	MaskDICELoss 0.6346 (0.4739)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 396 is less than current step: 499. Dropping entry: {'train/loss': 1.488884949684143, 'train/ce_loss': 0.592822265625, 'train/seg_cls_loss': 0.01097412109375, 'train/kl_loss': 0.1875, 'train/mask_bce_loss': 0.09104508720338345, 'train/mask_dice_loss': 0.34487687349319457, 'train/mask_loss': 0.43592196702957153, 'metrics/total_secs_per_batch': 5.913290739059448, 'metrics/data_secs_per_batch': 2.322313380241394, '_timestamp': 1740959684.057163}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 396 is less than current step: 499. Dropping entry: {'train/lr': 0.00013530612244897958, '_timestamp': 1740959684.0575128}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 397 is less than current step: 499. Dropping entry: {'train/loss': 1.726427137851715, 'train/ce_loss': 0.45185546875, 'train/seg_cls_loss': 0.012188720703125, 'train/kl_loss': 0.2587890625, 'train/mask_bce_loss': 0.14740273859351874, 'train/mask_dice_loss': 0.4739162981510162, 'train/mask_loss': 0.6213190376758575, 'metrics/total_secs_per_batch': 5.831664562225342, 'metrics/data_secs_per_batch': 2.7308000326156616, '_timestamp': 1740959689.8885934}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 397 is less than current step: 499. Dropping entry: {'train/lr': 0.00013518367346938774, '_timestamp': 1740959689.8888795}).
Epoch: [2][399/500]	Time  5.056 ( 5.056)	Loss 1.3651 (1.1857)	CeLoss 0.2539 (0.5455)	SegCLSLoss 0.0122 (0.0082)	KLLoss 0.3789 (0.1873)	MaskLoss 0.5341 (0.3088)	MaskBCELoss 0.0992 (0.0837)	MaskDICELoss 0.4349 (0.2251)
[2025-03-02 17:55:00,642] [INFO] [logging.py:96:log_dist] [Rank 0] step=1400, skipped=0, lr=[0.000135], mom=[(0.9, 0.95)]
[2025-03-02 17:55:00,646] [INFO] [timer.py:215:stop] epoch=0/micro_step=14000/global_step=1400, RunningAvgSamplesPerSec=1.5610282112847627, CurrSamplesPerSec=1.7540290187542045, MemAllocated=31.25GB, MaxMemAllocated=37.19GB
Epoch: [2][400/500]	Time  5.703 ( 5.703)	Loss 2.3702 (1.5055)	CeLoss 0.2285 (0.4425)	SegCLSLoss 0.0325 (0.0156)	KLLoss 0.3711 (0.2615)	MaskLoss 1.0445 (0.5146)	MaskBCELoss 0.1080 (0.0686)	MaskDICELoss 0.9365 (0.4459)
Epoch: [2][401/500]	Time  6.143 ( 6.143)	Loss 2.4599 (1.8489)	CeLoss 0.1138 (0.4742)	SegCLSLoss 0.0608 (0.0188)	KLLoss 0.3613 (0.2590)	MaskLoss 1.1396 (0.6697)	MaskBCELoss 0.3371 (0.1515)	MaskDICELoss 0.8025 (0.5182)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 398 is less than current step: 499. Dropping entry: {'train/loss': 1.185653066635132, 'train/ce_loss': 0.5455078125, 'train/seg_cls_loss': 0.0081787109375, 'train/kl_loss': 0.1873046875, 'train/mask_bce_loss': 0.08368771336972713, 'train/mask_dice_loss': 0.22510560899972915, 'train/mask_loss': 0.30879332423210143, 'metrics/total_secs_per_batch': 5.055540561676025, 'metrics/data_secs_per_batch': 2.2647975206375124, '_timestamp': 1740959694.9441016}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 398 is less than current step: 499. Dropping entry: {'train/lr': 0.0001350612244897959, '_timestamp': 1740959694.9443834}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 399 is less than current step: 499. Dropping entry: {'train/loss': 1.5054696142673492, 'train/ce_loss': 0.4424560546875, 'train/seg_cls_loss': 0.015606689453125, 'train/kl_loss': 0.2615234375, 'train/mask_bce_loss': 0.06864700764417649, 'train/mask_dice_loss': 0.4459164023399353, 'train/mask_loss': 0.5145634025335312, 'metrics/total_secs_per_batch': 5.703009605407715, 'metrics/data_secs_per_batch': 2.376850414276123, '_timestamp': 1740959700.6470017}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 399 is less than current step: 499. Dropping entry: {'train/lr': 0.00013493877551020407, '_timestamp': 1740959700.6473432}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 400 is less than current step: 499. Dropping entry: {'train/loss': 1.84894397854805, 'train/ce_loss': 0.474169921875, 'train/seg_cls_loss': 0.018817138671875, 'train/kl_loss': 0.258984375, 'train/mask_bce_loss': 0.15147617626935245, 'train/mask_dice_loss': 0.5182106703519821, 'train/mask_loss': 0.669686833024025, 'metrics/total_secs_per_batch': 6.143402338027954, 'metrics/data_secs_per_batch': 2.617787742614746, '_timestamp': 1740959706.790789}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 400 is less than current step: 499. Dropping entry: {'train/lr': 0.00013481632653061223, '_timestamp': 1740959706.7911572}).
Epoch: [2][402/500]	Time  5.634 ( 5.634)	Loss 1.7596 (1.7099)	CeLoss 0.2539 (0.5290)	SegCLSLoss 0.0142 (0.0123)	KLLoss 0.3770 (0.2646)	MaskLoss 0.7304 (0.5740)	MaskBCELoss 0.2745 (0.0932)	MaskDICELoss 0.4559 (0.4809)
Epoch: [2][403/500]	Time  6.329 ( 6.329)	Loss 2.3387 (1.8692)	CeLoss 0.2520 (0.3713)	SegCLSLoss 0.0243 (0.0184)	KLLoss 0.3770 (0.2988)	MaskLoss 1.0190 (0.7293)	MaskBCELoss 0.2072 (0.1096)	MaskDICELoss 0.8118 (0.6198)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 401 is less than current step: 499. Dropping entry: {'train/loss': 1.7099461913108827, 'train/ce_loss': 0.52900390625, 'train/seg_cls_loss': 0.01234130859375, 'train/kl_loss': 0.2646484375, 'train/mask_bce_loss': 0.09315280728042126, 'train/mask_dice_loss': 0.4808632403612137, 'train/mask_loss': 0.5740160465240478, 'metrics/total_secs_per_batch': 5.634441614151001, 'metrics/data_secs_per_batch': 2.596276378631592, '_timestamp': 1740959712.4250216}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 401 is less than current step: 499. Dropping entry: {'train/lr': 0.0001346938775510204, '_timestamp': 1740959712.425397}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 402 is less than current step: 499. Dropping entry: {'train/loss': 1.86923485994339, 'train/ce_loss': 0.3712890625, 'train/seg_cls_loss': 0.018426513671875, 'train/kl_loss': 0.298828125, 'train/mask_bce_loss': 0.10958379302173853, 'train/mask_dice_loss': 0.6197602033615113, 'train/mask_loss': 0.7293439745903015, 'metrics/total_secs_per_batch': 6.3290510177612305, 'metrics/data_secs_per_batch': 2.9818607807159423, '_timestamp': 1740959718.7539895}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 402 is less than current step: 499. Dropping entry: {'train/lr': 0.00013457142857142856, '_timestamp': 1740959718.754274}).
Epoch: [2][404/500]	Time  5.390 ( 5.390)	Loss 2.5375 (1.5305)	CeLoss 0.1108 (0.4003)	SegCLSLoss 0.0288 (0.0161)	KLLoss 0.3652 (0.2266)	MaskLoss 1.1877 (0.5497)	MaskBCELoss 0.2640 (0.0874)	MaskDICELoss 0.9237 (0.4623)
Epoch: [2][405/500]	Time  5.900 ( 5.900)	Loss 2.6863 (2.1725)	CeLoss 0.2207 (0.4044)	SegCLSLoss 0.0183 (0.0180)	KLLoss 0.3711 (0.2996)	MaskLoss 1.2103 (0.8645)	MaskBCELoss 0.3136 (0.3252)	MaskDICELoss 0.8967 (0.5393)
Epoch: [2][406/500]	Time  6.423 ( 6.423)	Loss 0.8398 (1.4832)	CeLoss 0.8398 (0.3049)	SegCLSLoss 0.0000 (0.0123)	KLLoss 0.0000 (0.2670)	MaskLoss 0.0000 (0.5727)	MaskBCELoss 0.0000 (0.1759)	MaskDICELoss 0.0000 (0.3968)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 403 is less than current step: 499. Dropping entry: {'train/loss': 1.5304733276367188, 'train/ce_loss': 0.400341796875, 'train/seg_cls_loss': 0.0161376953125, 'train/kl_loss': 0.2265625, 'train/mask_bce_loss': 0.08739275485277176, 'train/mask_dice_loss': 0.4622677117586136, 'train/mask_loss': 0.5496604740619659, 'metrics/total_secs_per_batch': 5.39030647277832, 'metrics/data_secs_per_batch': 2.628303337097168, '_timestamp': 1740959724.1443748}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 403 is less than current step: 499. Dropping entry: {'train/lr': 0.00013444897959183672, '_timestamp': 1740959724.144704}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 404 is less than current step: 499. Dropping entry: {'train/loss': 2.1725441932678224, 'train/ce_loss': 0.40439453125, 'train/seg_cls_loss': 0.018048095703125, 'train/kl_loss': 0.299609375, 'train/mask_bce_loss': 0.32516184747219085, 'train/mask_dice_loss': 0.5393328964710236, 'train/mask_loss': 0.8644947409629822, 'metrics/total_secs_per_batch': 5.899959564208984, 'metrics/data_secs_per_batch': 2.3111537218093874, '_timestamp': 1740959730.044298}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 404 is less than current step: 499. Dropping entry: {'train/lr': 0.00013432653061224488, '_timestamp': 1740959730.0445879}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 405 is less than current step: 499. Dropping entry: {'train/loss': 1.4831671476364137, 'train/ce_loss': 0.304931640625, 'train/seg_cls_loss': 0.012322998046875, 'train/kl_loss': 0.2669921875, 'train/mask_bce_loss': 0.17590683326125145, 'train/mask_dice_loss': 0.39680466502904893, 'train/mask_loss': 0.5727115035057068, 'metrics/total_secs_per_batch': 6.422858476638794, 'metrics/data_secs_per_batch': 2.854230833053589, '_timestamp': 1740959736.4671376}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 405 is less than current step: 499. Dropping entry: {'train/lr': 0.00013420408163265305, '_timestamp': 1740959736.4674265}).
Epoch: [2][407/500]	Time  4.547 ( 4.547)	Loss 1.5859 (1.8324)	CeLoss 1.5859 (0.6626)	SegCLSLoss 0.0000 (0.0093)	KLLoss 0.0000 (0.2279)	MaskLoss 0.0000 (0.5713)	MaskBCELoss 0.0000 (0.1521)	MaskDICELoss 0.0000 (0.4191)
Epoch: [2][408/500]	Time  4.671 ( 4.671)	Loss 2.5785 (1.5159)	CeLoss 0.1777 (0.6182)	SegCLSLoss 0.0271 (0.0089)	KLLoss 0.3691 (0.1867)	MaskLoss 1.1750 (0.4373)	MaskBCELoss 0.2618 (0.1035)	MaskDICELoss 0.9132 (0.3337)
Epoch: [2][409/500]	Time  7.087 ( 7.087)	Loss 1.5234 (1.9302)	CeLoss 1.5234 (0.3537)	SegCLSLoss 0.0000 (0.0203)	KLLoss 0.0000 (0.3398)	MaskLoss 0.0000 (0.7664)	MaskBCELoss 0.0000 (0.1354)	MaskDICELoss 0.0000 (0.6310)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 406 is less than current step: 499. Dropping entry: {'train/loss': 1.832357692718506, 'train/ce_loss': 0.66259765625, 'train/seg_cls_loss': 0.009332275390625, 'train/kl_loss': 0.2279296875, 'train/mask_bce_loss': 0.15212934464216232, 'train/mask_dice_loss': 0.41912763714790346, 'train/mask_loss': 0.5712569713592529, 'metrics/total_secs_per_batch': 4.547449350357056, 'metrics/data_secs_per_batch': 2.02602379322052, '_timestamp': 1740959741.0145802}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 406 is less than current step: 499. Dropping entry: {'train/lr': 0.0001340816326530612, '_timestamp': 1740959741.0147705}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 407 is less than current step: 499. Dropping entry: {'train/loss': 1.5159274816513062, 'train/ce_loss': 0.6181640625, 'train/seg_cls_loss': 0.00885009765625, 'train/kl_loss': 0.18671875, 'train/mask_bce_loss': 0.10354750901460648, 'train/mask_dice_loss': 0.33371310830116274, 'train/mask_loss': 0.43726062327623366, 'metrics/total_secs_per_batch': 4.671294927597046, 'metrics/data_secs_per_batch': 1.8638383865356445, '_timestamp': 1740959745.6859374}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 407 is less than current step: 499. Dropping entry: {'train/lr': 0.00013395918367346937, '_timestamp': 1740959745.6863034}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 408 is less than current step: 499. Dropping entry: {'train/loss': 1.9302345752716064, 'train/ce_loss': 0.3537109375, 'train/seg_cls_loss': 0.02032470703125, 'train/kl_loss': 0.33984375, 'train/mask_bce_loss': 0.13537637507542968, 'train/mask_dice_loss': 0.6310104429721832, 'train/mask_loss': 0.7663868129253387, 'metrics/total_secs_per_batch': 7.087351322174072, 'metrics/data_secs_per_batch': 3.4892659902572634, '_timestamp': 1740959752.7733872}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 408 is less than current step: 499. Dropping entry: {'train/lr': 0.00013383673469387754, '_timestamp': 1740959752.7737322}).
[2025-03-02 17:55:59,153] [INFO] [logging.py:96:log_dist] [Rank 0] step=1410, skipped=0, lr=[0.00013377551020408163], mom=[(0.9, 0.95)]
[2025-03-02 17:55:59,159] [INFO] [timer.py:215:stop] epoch=0/micro_step=14100/global_step=1410, RunningAvgSamplesPerSec=1.5619910654483948, CurrSamplesPerSec=1.566046393207391, MemAllocated=31.25GB, MaxMemAllocated=37.19GB
Epoch: [2][410/500]	Time  6.387 ( 6.387)	Loss 3.0559 (1.9877)	CeLoss 0.2617 (0.2860)	SegCLSLoss 0.0189 (0.0156)	KLLoss 0.3613 (0.3365)	MaskLoss 1.3736 (0.8301)	MaskBCELoss 0.6041 (0.1650)	MaskDICELoss 0.7696 (0.6651)
Epoch: [2][411/500]	Time  6.199 ( 6.199)	Loss 1.3989 (1.6740)	CeLoss 0.2715 (0.3718)	SegCLSLoss 0.0132 (0.0134)	KLLoss 0.3867 (0.3451)	MaskLoss 0.5413 (0.6303)	MaskBCELoss 0.0710 (0.0884)	MaskDICELoss 0.4702 (0.5418)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 409 is less than current step: 499. Dropping entry: {'train/loss': 1.987718105316162, 'train/ce_loss': 0.28603515625, 'train/seg_cls_loss': 0.0156005859375, 'train/kl_loss': 0.3365234375, 'train/mask_bce_loss': 0.16504217614419758, 'train/mask_dice_loss': 0.6650961488485336, 'train/mask_loss': 0.8301383346319199, 'metrics/total_secs_per_batch': 6.38730525970459, 'metrics/data_secs_per_batch': 2.8112442970275877, '_timestamp': 1740959759.1603458}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 409 is less than current step: 499. Dropping entry: {'train/lr': 0.0001337142857142857, '_timestamp': 1740959759.1605458}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 410 is less than current step: 499. Dropping entry: {'train/loss': 1.6739984154701233, 'train/ce_loss': 0.37177734375, 'train/seg_cls_loss': 0.013421630859375, 'train/kl_loss': 0.3451171875, 'train/mask_bce_loss': 0.08843222334980964, 'train/mask_dice_loss': 0.5418287217617035, 'train/mask_loss': 0.6302609354257583, 'metrics/total_secs_per_batch': 6.199227571487427, 'metrics/data_secs_per_batch': 2.8265343427658083, '_timestamp': 1740959765.359779}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 410 is less than current step: 499. Dropping entry: {'train/lr': 0.00013359183673469386, '_timestamp': 1740959765.3600805}).
Epoch: [2][412/500]	Time  6.407 ( 6.407)	Loss 1.4409 (1.7617)	CeLoss 0.2393 (0.2820)	SegCLSLoss 0.0183 (0.0176)	KLLoss 0.3828 (0.2975)	MaskLoss 0.5769 (0.7207)	MaskBCELoss 0.1563 (0.1328)	MaskDICELoss 0.4206 (0.5879)
Epoch: [2][413/500]	Time  5.897 ( 5.897)	Loss 2.3021 (1.7145)	CeLoss 0.2598 (0.4009)	SegCLSLoss 0.0232 (0.0192)	KLLoss 0.3711 (0.2988)	MaskLoss 0.9967 (0.6371)	MaskBCELoss 0.0157 (0.0965)	MaskDICELoss 0.9811 (0.5406)
Epoch: [2][414/500]	Time  5.717 ( 5.717)	Loss 1.5393 (1.6767)	CeLoss 0.3086 (0.4544)	SegCLSLoss 0.0112 (0.0132)	KLLoss 0.3828 (0.2635)	MaskLoss 0.5929 (0.5947)	MaskBCELoss 0.2698 (0.0953)	MaskDICELoss 0.3231 (0.4994)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 411 is less than current step: 499. Dropping entry: {'train/loss': 1.7617036819458007, 'train/ce_loss': 0.28203125, 'train/seg_cls_loss': 0.017608642578125, 'train/kl_loss': 0.2974609375, 'train/mask_bce_loss': 0.13277652822434902, 'train/mask_dice_loss': 0.5879190564155579, 'train/mask_loss': 0.7206955790519715, 'metrics/total_secs_per_batch': 6.4068427085876465, 'metrics/data_secs_per_batch': 3.0599411487579347, '_timestamp': 1740959771.7666676}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 411 is less than current step: 499. Dropping entry: {'train/lr': 0.00013346938775510203, '_timestamp': 1740959771.766954}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 412 is less than current step: 499. Dropping entry: {'train/loss': 1.7144976735115052, 'train/ce_loss': 0.40087890625, 'train/seg_cls_loss': 0.019232177734375, 'train/kl_loss': 0.298828125, 'train/mask_bce_loss': 0.09652350712567567, 'train/mask_dice_loss': 0.5405593186616897, 'train/mask_loss': 0.6370828241109848, 'metrics/total_secs_per_batch': 5.896959066390991, 'metrics/data_secs_per_batch': 2.7073784828186036, '_timestamp': 1740959777.6635556}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 412 is less than current step: 499. Dropping entry: {'train/lr': 0.0001333469387755102, '_timestamp': 1740959777.6638951}).
Epoch: [2][415/500]	Time  5.451 ( 5.451)	Loss 1.4844 (1.7460)	CeLoss 1.4844 (0.5937)	SegCLSLoss 0.0000 (0.0146)	KLLoss 0.0000 (0.2215)	MaskLoss 0.0000 (0.5614)	MaskBCELoss 0.0000 (0.1165)	MaskDICELoss 0.0000 (0.4449)
Epoch: [2][416/500]	Time  5.867 ( 5.867)	Loss 1.9513 (1.6379)	CeLoss 0.2812 (0.3738)	SegCLSLoss 0.0101 (0.0145)	KLLoss 0.3809 (0.2617)	MaskLoss 0.8135 (0.6154)	MaskBCELoss 0.2412 (0.1324)	MaskDICELoss 0.5723 (0.4830)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 413 is less than current step: 499. Dropping entry: {'train/loss': 1.6766820549964905, 'train/ce_loss': 0.45439453125, 'train/seg_cls_loss': 0.013232421875, 'train/kl_loss': 0.2634765625, 'train/mask_bce_loss': 0.09530210457742214, 'train/mask_dice_loss': 0.4993865847587585, 'train/mask_loss': 0.5946886837482452, 'metrics/total_secs_per_batch': 5.7170727252960205, 'metrics/data_secs_per_batch': 2.6659355640411375, '_timestamp': 1740959783.3805804}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 413 is less than current step: 499. Dropping entry: {'train/lr': 0.00013322448979591835, '_timestamp': 1740959783.3808548}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 414 is less than current step: 499. Dropping entry: {'train/loss': 1.7459753632545472, 'train/ce_loss': 0.59365234375, 'train/seg_cls_loss': 0.0146484375, 'train/kl_loss': 0.221484375, 'train/mask_bce_loss': 0.11649187486618758, 'train/mask_dice_loss': 0.44492354393005373, 'train/mask_loss': 0.5614154219627381, 'metrics/total_secs_per_batch': 5.451364278793335, 'metrics/data_secs_per_batch': 2.488935875892639, '_timestamp': 1740959788.8322475}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 414 is less than current step: 499. Dropping entry: {'train/lr': 0.00013310204081632652, '_timestamp': 1740959788.8325987}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 415 is less than current step: 499. Dropping entry: {'train/loss': 1.637921440601349, 'train/ce_loss': 0.373828125, 'train/seg_cls_loss': 0.01446533203125, 'train/kl_loss': 0.26171875, 'train/mask_bce_loss': 0.1323925716802478, 'train/mask_dice_loss': 0.4830036848783493, 'train/mask_loss': 0.6153962671756744, 'metrics/total_secs_per_batch': 5.867124557495117, 'metrics/data_secs_per_batch': 2.5695459604263307, '_timestamp': 1740959794.6991644}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 415 is less than current step: 499. Dropping entry: {'train/lr': 0.00013297959183673468, '_timestamp': 1740959794.699524}).
Epoch: [2][417/500]	Time  6.347 ( 6.347)	Loss 1.9706 (2.1426)	CeLoss 0.2148 (0.2188)	SegCLSLoss 0.0216 (0.0212)	KLLoss 0.3750 (0.3727)	MaskLoss 0.8535 (0.9380)	MaskBCELoss 0.0452 (0.1391)	MaskDICELoss 0.8082 (0.7989)
Epoch: [2][418/500]	Time  6.820 ( 6.820)	Loss 0.0596 (1.5686)	CeLoss 0.0596 (0.1935)	SegCLSLoss 0.0000 (0.0187)	KLLoss 0.0000 (0.2951)	MaskLoss 0.0000 (0.6683)	MaskBCELoss 0.0000 (0.0894)	MaskDICELoss 0.0000 (0.5789)
Epoch: [2][419/500]	Time  5.761 ( 5.761)	Loss 2.3624 (1.7532)	CeLoss 0.1357 (0.3138)	SegCLSLoss 0.0371 (0.0159)	KLLoss 0.3594 (0.2979)	MaskLoss 1.0860 (0.7007)	MaskBCELoss 0.1818 (0.1943)	MaskDICELoss 0.9042 (0.5064)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 416 is less than current step: 499. Dropping entry: {'train/loss': 2.142610216140747, 'train/ce_loss': 0.21875, 'train/seg_cls_loss': 0.02119140625, 'train/kl_loss': 0.37265625, 'train/mask_bce_loss': 0.13913335595279933, 'train/mask_dice_loss': 0.7988709673285485, 'train/mask_loss': 0.9380043029785157, 'metrics/total_secs_per_batch': 6.34731388092041, 'metrics/data_secs_per_batch': 2.7269948005676268, '_timestamp': 1740959801.0465004}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 416 is less than current step: 499. Dropping entry: {'train/lr': 0.00013285714285714284, '_timestamp': 1740959801.0467927}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 417 is less than current step: 499. Dropping entry: {'train/loss': 1.5686330676078797, 'train/ce_loss': 0.19345703125, 'train/seg_cls_loss': 0.018743896484375, 'train/kl_loss': 0.2951171875, 'train/mask_bce_loss': 0.08936714995652437, 'train/mask_dice_loss': 0.5789337486028672, 'train/mask_loss': 0.6683009058237076, 'metrics/total_secs_per_batch': 6.820162296295166, 'metrics/data_secs_per_batch': 3.0910737037658693, '_timestamp': 1740959807.866639}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 417 is less than current step: 499. Dropping entry: {'train/lr': 0.000132734693877551, '_timestamp': 1740959807.8669193}).
[2025-03-02 17:56:58,423] [INFO] [logging.py:96:log_dist] [Rank 0] step=1420, skipped=0, lr=[0.00013255102040816326], mom=[(0.9, 0.95)]
[2025-03-02 17:56:58,428] [INFO] [timer.py:215:stop] epoch=0/micro_step=14200/global_step=1420, RunningAvgSamplesPerSec=1.5628112132338654, CurrSamplesPerSec=2.0829764082027964, MemAllocated=30.7GB, MaxMemAllocated=37.19GB
Epoch: [2][420/500]	Time  4.802 ( 4.802)	Loss 1.1484 (1.7560)	CeLoss 1.1484 (0.8479)	SegCLSLoss 0.0000 (0.0113)	KLLoss 0.0000 (0.1854)	MaskLoss 0.0000 (0.4420)	MaskBCELoss 0.0000 (0.1007)	MaskDICELoss 0.0000 (0.3413)
Epoch: [2][421/500]	Time  6.561 ( 6.561)	Loss 2.0247 (1.5977)	CeLoss 0.2148 (0.3892)	SegCLSLoss 0.0219 (0.0184)	KLLoss 0.3809 (0.2619)	MaskLoss 0.8805 (0.5866)	MaskBCELoss 0.0155 (0.0371)	MaskDICELoss 0.8650 (0.5495)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 418 is less than current step: 499. Dropping entry: {'train/loss': 1.7532060742378235, 'train/ce_loss': 0.313818359375, 'train/seg_cls_loss': 0.0159423828125, 'train/kl_loss': 0.2978515625, 'train/mask_bce_loss': 0.19433865342289208, 'train/mask_dice_loss': 0.5063610523939133, 'train/mask_loss': 0.7006996989250183, 'metrics/total_secs_per_batch': 5.760511159896851, 'metrics/data_secs_per_batch': 2.6700530529022215, '_timestamp': 1740959813.6271536}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 418 is less than current step: 499. Dropping entry: {'train/lr': 0.00013261224489795917, '_timestamp': 1740959813.6274226}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 419 is less than current step: 499. Dropping entry: {'train/loss': 1.7560211896896363, 'train/ce_loss': 0.8478515625, 'train/seg_cls_loss': 0.011260986328125, 'train/kl_loss': 0.1853515625, 'train/mask_bce_loss': 0.10070076771080494, 'train/mask_dice_loss': 0.3412746787071228, 'train/mask_loss': 0.44197543859481814, 'metrics/total_secs_per_batch': 4.802441120147705, 'metrics/data_secs_per_batch': 2.1453760623931886, '_timestamp': 1740959818.4294016}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 419 is less than current step: 499. Dropping entry: {'train/lr': 0.00013248979591836733, '_timestamp': 1740959818.4296994}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 420 is less than current step: 499. Dropping entry: {'train/loss': 1.5977010011672974, 'train/ce_loss': 0.38916015625, 'train/seg_cls_loss': 0.018408203125, 'train/kl_loss': 0.2619140625, 'train/mask_bce_loss': 0.03705265745520592, 'train/mask_dice_loss': 0.5495419859886169, 'train/mask_loss': 0.5865946352481842, 'metrics/total_secs_per_batch': 6.560906410217285, 'metrics/data_secs_per_batch': 2.8376585721969603, '_timestamp': 1740959824.9906454}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 420 is less than current step: 499. Dropping entry: {'train/lr': 0.0001323673469387755, '_timestamp': 1740959824.9909654}).
Epoch: [2][422/500]	Time  5.917 ( 5.917)	Loss 2.4428 (1.5727)	CeLoss 0.2598 (0.4670)	SegCLSLoss 0.0150 (0.0124)	KLLoss 0.3652 (0.2623)	MaskLoss 1.0690 (0.5366)	MaskBCELoss 0.2465 (0.0727)	MaskDICELoss 0.8225 (0.4639)
Epoch: [2][423/500]	Time  5.973 ( 5.973)	Loss 3.0289 (1.8385)	CeLoss 0.2266 (0.4050)	SegCLSLoss 0.0231 (0.0154)	KLLoss 0.3711 (0.2986)	MaskLoss 1.3767 (0.6980)	MaskBCELoss 0.5905 (0.1025)	MaskDICELoss 0.7863 (0.5955)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 421 is less than current step: 499. Dropping entry: {'train/loss': 1.57274432182312, 'train/ce_loss': 0.4669921875, 'train/seg_cls_loss': 0.012359619140625, 'train/kl_loss': 0.2623046875, 'train/mask_bce_loss': 0.07270794762298464, 'train/mask_dice_loss': 0.46385950446128843, 'train/mask_loss': 0.5365674629807472, 'metrics/total_secs_per_batch': 5.917396306991577, 'metrics/data_secs_per_batch': 2.5440323829650877, '_timestamp': 1740959830.9080064}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 421 is less than current step: 499. Dropping entry: {'train/lr': 0.00013224489795918366, '_timestamp': 1740959830.9083288}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 422 is less than current step: 499. Dropping entry: {'train/loss': 1.8385056018829347, 'train/ce_loss': 0.40498046875, 'train/seg_cls_loss': 0.015380859375, 'train/kl_loss': 0.2986328125, 'train/mask_bce_loss': 0.10248932354152203, 'train/mask_dice_loss': 0.5954744160175324, 'train/mask_loss': 0.6979637444019318, 'metrics/total_secs_per_batch': 5.972587585449219, 'metrics/data_secs_per_batch': 2.749147915840149, '_timestamp': 1740959836.8805578}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 422 is less than current step: 499. Dropping entry: {'train/lr': 0.00013212244897959182, '_timestamp': 1740959836.8808439}).
Epoch: [2][424/500]	Time  7.469 ( 7.469)	Loss 1.9002 (1.9953)	CeLoss 0.1836 (0.2414)	SegCLSLoss 0.0248 (0.0199)	KLLoss 0.3867 (0.3754)	MaskLoss 0.8329 (0.8532)	MaskBCELoss 0.2954 (0.1839)	MaskDICELoss 0.5375 (0.6694)
Epoch: [2][425/500]	Time  5.679 ( 5.679)	Loss 2.4816 (1.3830)	CeLoss 0.1953 (0.3628)	SegCLSLoss 0.0208 (0.0075)	KLLoss 0.3633 (0.1881)	MaskLoss 1.1197 (0.4988)	MaskBCELoss 0.3034 (0.1609)	MaskDICELoss 0.8163 (0.3379)
Epoch: [2][426/500]	Time  5.609 ( 5.609)	Loss 1.5999 (1.6831)	CeLoss 0.2520 (0.3792)	SegCLSLoss 0.0159 (0.0125)	KLLoss 0.3770 (0.2650)	MaskLoss 0.6515 (0.6357)	MaskBCELoss 0.0688 (0.1925)	MaskDICELoss 0.5828 (0.4432)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 423 is less than current step: 499. Dropping entry: {'train/loss': 1.9953263759613038, 'train/ce_loss': 0.24140625, 'train/seg_cls_loss': 0.019866943359375, 'train/kl_loss': 0.375390625, 'train/mask_bce_loss': 0.18386251940391957, 'train/mask_dice_loss': 0.6693670660257339, 'train/mask_loss': 0.8532295823097229, 'metrics/total_secs_per_batch': 7.468812465667725, 'metrics/data_secs_per_batch': 3.3283495903015137, '_timestamp': 1740959844.3493257}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 423 is less than current step: 499. Dropping entry: {'train/lr': 0.00013199999999999998, '_timestamp': 1740959844.3496015}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 424 is less than current step: 499. Dropping entry: {'train/loss': 1.3830469489097594, 'train/ce_loss': 0.362841796875, 'train/seg_cls_loss': 0.007525634765625, 'train/kl_loss': 0.1880859375, 'train/mask_bce_loss': 0.16091184355318547, 'train/mask_dice_loss': 0.3379114389419556, 'train/mask_loss': 0.49882328510284424, 'metrics/total_secs_per_batch': 5.678986549377441, 'metrics/data_secs_per_batch': 2.553031158447266, '_timestamp': 1740959850.0283937}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 424 is less than current step: 499. Dropping entry: {'train/lr': 0.00013187755102040815, '_timestamp': 1740959850.0286853}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 425 is less than current step: 499. Dropping entry: {'train/loss': 1.6831256866455078, 'train/ce_loss': 0.379248046875, 'train/seg_cls_loss': 0.0125244140625, 'train/kl_loss': 0.2650390625, 'train/mask_bce_loss': 0.19253293350338935, 'train/mask_dice_loss': 0.4431949555873871, 'train/mask_loss': 0.6357278823852539, 'metrics/total_secs_per_batch': 5.609130859375, 'metrics/data_secs_per_batch': 2.6173904180526733, '_timestamp': 1740959855.637417}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 425 is less than current step: 499. Dropping entry: {'train/lr': 0.0001317551020408163, '_timestamp': 1740959855.6377032}).
Epoch: [2][427/500]	Time  6.848 ( 6.848)	Loss 1.7279 (1.7507)	CeLoss 0.2246 (0.3057)	SegCLSLoss 0.0145 (0.0138)	KLLoss 0.3809 (0.3014)	MaskLoss 0.7292 (0.7041)	MaskBCELoss 0.1262 (0.1395)	MaskDICELoss 0.6030 (0.5646)
Epoch: [2][428/500]	Time  5.506 ( 5.506)	Loss 1.0938 (1.7392)	CeLoss 1.0938 (0.4075)	SegCLSLoss 0.0000 (0.0184)	KLLoss 0.0000 (0.3002)	MaskLoss 0.0000 (0.6462)	MaskBCELoss 0.0000 (0.1296)	MaskDICELoss 0.0000 (0.5166)
Epoch: [2][429/500]	Time  6.063 ( 6.063)	Loss 2.3467 (1.8012)	CeLoss 0.1484 (0.2899)	SegCLSLoss 0.0549 (0.0227)	KLLoss 0.3672 (0.2986)	MaskLoss 1.0669 (0.7350)	MaskBCELoss 0.1940 (0.1274)	MaskDICELoss 0.8729 (0.6076)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 426 is less than current step: 499. Dropping entry: {'train/loss': 1.7507193684577942, 'train/ce_loss': 0.3056640625, 'train/seg_cls_loss': 0.013775634765625, 'train/kl_loss': 0.3013671875, 'train/mask_bce_loss': 0.13945138975977897, 'train/mask_dice_loss': 0.5646192193031311, 'train/mask_loss': 0.7040706038475036, 'metrics/total_secs_per_batch': 6.84776496887207, 'metrics/data_secs_per_batch': 3.0991413831710815, '_timestamp': 1740959862.485209}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 426 is less than current step: 499. Dropping entry: {'train/lr': 0.00013163265306122447, '_timestamp': 1740959862.4855537}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 427 is less than current step: 499. Dropping entry: {'train/loss': 1.7391763567924499, 'train/ce_loss': 0.40751953125, 'train/seg_cls_loss': 0.0184326171875, 'train/kl_loss': 0.3001953125, 'train/mask_bce_loss': 0.1295627871528268, 'train/mask_dice_loss': 0.516636711359024, 'train/mask_loss': 0.6461995065212249, 'metrics/total_secs_per_batch': 5.506359815597534, 'metrics/data_secs_per_batch': 2.5514455795288087, '_timestamp': 1740959867.9915872}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 427 is less than current step: 499. Dropping entry: {'train/lr': 0.00013151020408163264, '_timestamp': 1740959867.9918766}).
[2025-03-02 17:58:00,106] [INFO] [logging.py:96:log_dist] [Rank 0] step=1430, skipped=0, lr=[0.00013132653061224487], mom=[(0.9, 0.95)]
[2025-03-02 17:58:00,112] [INFO] [timer.py:215:stop] epoch=0/micro_step=14300/global_step=1430, RunningAvgSamplesPerSec=1.5632074393673572, CurrSamplesPerSec=1.6509074944390216, MemAllocated=31.26GB, MaxMemAllocated=37.19GB
Epoch: [2][430/500]	Time  6.059 ( 6.059)	Loss 2.9642 (1.9574)	CeLoss 0.1895 (0.4834)	SegCLSLoss 0.0233 (0.0148)	KLLoss 0.3672 (0.3033)	MaskLoss 1.3630 (0.7180)	MaskBCELoss 0.6384 (0.1593)	MaskDICELoss 0.7245 (0.5587)
Epoch: [2][431/500]	Time  5.394 ( 5.394)	Loss 1.4933 (1.7592)	CeLoss 0.2256 (0.3347)	SegCLSLoss 0.0245 (0.0146)	KLLoss 0.3848 (0.3438)	MaskLoss 0.6080 (0.6913)	MaskBCELoss 0.0084 (0.1730)	MaskDICELoss 0.5996 (0.5183)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 428 is less than current step: 499. Dropping entry: {'train/loss': 1.801173347234726, 'train/ce_loss': 0.289892578125, 'train/seg_cls_loss': 0.02269287109375, 'train/kl_loss': 0.2986328125, 'train/mask_bce_loss': 0.12742547551169991, 'train/mask_dice_loss': 0.60756059512496, 'train/mask_loss': 0.7349860817193985, 'metrics/total_secs_per_batch': 6.062836170196533, 'metrics/data_secs_per_batch': 2.8604648113250732, '_timestamp': 1740959874.0544558}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 428 is less than current step: 499. Dropping entry: {'train/lr': 0.0001313877551020408, '_timestamp': 1740959874.0547893}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 429 is less than current step: 499. Dropping entry: {'train/loss': 1.9573851346969604, 'train/ce_loss': 0.4833984375, 'train/seg_cls_loss': 0.014788818359375, 'train/kl_loss': 0.3033203125, 'train/mask_bce_loss': 0.15933628119528292, 'train/mask_dice_loss': 0.5586629152297974, 'train/mask_loss': 0.7179992079734803, 'metrics/total_secs_per_batch': 6.059043645858765, 'metrics/data_secs_per_batch': 2.73196656703949, '_timestamp': 1740959880.1132793}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 429 is less than current step: 499. Dropping entry: {'train/lr': 0.00013126530612244896, '_timestamp': 1740959880.1135612}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 430 is less than current step: 499. Dropping entry: {'train/loss': 1.7591509222984314, 'train/ce_loss': 0.33466796875, 'train/seg_cls_loss': 0.01463623046875, 'train/kl_loss': 0.34375, 'train/mask_bce_loss': 0.1730040714610368, 'train/mask_dice_loss': 0.5182901442050933, 'train/mask_loss': 0.6912942111492157, 'metrics/total_secs_per_batch': 5.393753528594971, 'metrics/data_secs_per_batch': 2.331822967529297, '_timestamp': 1740959885.5073628}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 430 is less than current step: 499. Dropping entry: {'train/lr': 0.00013114285714285715, '_timestamp': 1740959885.5076902}).
Epoch: [2][432/500]	Time  7.274 ( 7.274)	Loss 2.1186 (1.7797)	CeLoss 0.1680 (0.2033)	SegCLSLoss 0.0215 (0.0181)	KLLoss 0.3789 (0.3379)	MaskLoss 0.9509 (0.7669)	MaskBCELoss 0.0591 (0.0947)	MaskDICELoss 0.8918 (0.6722)
Epoch: [2][433/500]	Time  5.526 ( 5.526)	Loss 0.0522 (1.6394)	CeLoss 0.0522 (0.4874)	SegCLSLoss 0.0000 (0.0120)	KLLoss 0.0000 (0.2232)	MaskLoss 0.0000 (0.5617)	MaskBCELoss 0.0000 (0.1047)	MaskDICELoss 0.0000 (0.4571)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 431 is less than current step: 499. Dropping entry: {'train/loss': 1.7796750783920288, 'train/ce_loss': 0.2033203125, 'train/seg_cls_loss': 0.0180908203125, 'train/kl_loss': 0.337890625, 'train/mask_bce_loss': 0.09470230713486671, 'train/mask_dice_loss': 0.6721859946846962, 'train/mask_loss': 0.7668882995843888, 'metrics/total_secs_per_batch': 7.274371385574341, 'metrics/data_secs_per_batch': 3.165770435333252, '_timestamp': 1740959892.7816565}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 431 is less than current step: 499. Dropping entry: {'train/lr': 0.0001310204081632653, '_timestamp': 1740959892.7819672}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 432 is less than current step: 499. Dropping entry: {'train/loss': 1.6393640160560607, 'train/ce_loss': 0.487353515625, 'train/seg_cls_loss': 0.011962890625, 'train/kl_loss': 0.2232421875, 'train/mask_bce_loss': 0.10468269078992307, 'train/mask_dice_loss': 0.457064750790596, 'train/mask_loss': 0.5617474436759948, 'metrics/total_secs_per_batch': 5.526324510574341, 'metrics/data_secs_per_batch': 2.815932559967041, '_timestamp': 1740959898.3079035}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 432 is less than current step: 499. Dropping entry: {'train/lr': 0.00013089795918367345, '_timestamp': 1740959898.3082054}).
Epoch: [2][434/500]	Time  6.488 ( 6.488)	Loss 0.9961 (1.6968)	CeLoss 0.9961 (0.3147)	SegCLSLoss 0.0000 (0.0148)	KLLoss 0.0000 (0.3432)	MaskLoss 0.0000 (0.6702)	MaskBCELoss 0.0000 (0.0936)	MaskDICELoss 0.0000 (0.5766)
Epoch: [2][435/500]	Time  5.728 ( 5.728)	Loss 1.5657 (1.8457)	CeLoss 0.1914 (0.4680)	SegCLSLoss 0.0236 (0.0156)	KLLoss 0.3691 (0.3039)	MaskLoss 0.6627 (0.6697)	MaskBCELoss 0.0231 (0.1041)	MaskDICELoss 0.6396 (0.5655)
Epoch: [2][436/500]	Time  5.887 ( 5.887)	Loss 2.4465 (1.7423)	CeLoss 0.3066 (0.4672)	SegCLSLoss 0.0287 (0.0170)	KLLoss 0.3789 (0.2639)	MaskLoss 1.0445 (0.6202)	MaskBCELoss 0.1062 (0.1199)	MaskDICELoss 0.9383 (0.5003)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 433 is less than current step: 499. Dropping entry: {'train/loss': 1.6967573404312133, 'train/ce_loss': 0.31474609375, 'train/seg_cls_loss': 0.0147705078125, 'train/kl_loss': 0.3431640625, 'train/mask_bce_loss': 0.09360152054578066, 'train/mask_dice_loss': 0.5766033142805099, 'train/mask_loss': 0.67020483314991, 'metrics/total_secs_per_batch': 6.487761735916138, 'metrics/data_secs_per_batch': 3.1028903484344483, '_timestamp': 1740959904.7956715}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 433 is less than current step: 499. Dropping entry: {'train/lr': 0.00013077551020408161, '_timestamp': 1740959904.7959514}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 434 is less than current step: 499. Dropping entry: {'train/loss': 1.845666480064392, 'train/ce_loss': 0.46796875, 'train/seg_cls_loss': 0.015631103515625, 'train/kl_loss': 0.30390625, 'train/mask_bce_loss': 0.10411819238215685, 'train/mask_dice_loss': 0.5655412167310715, 'train/mask_loss': 0.6696594059467316, 'metrics/total_secs_per_batch': 5.728257179260254, 'metrics/data_secs_per_batch': 2.583315539360046, '_timestamp': 1740959910.5241559}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 434 is less than current step: 499. Dropping entry: {'train/lr': 0.00013065306122448978, '_timestamp': 1740959910.524505}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 435 is less than current step: 499. Dropping entry: {'train/loss': 1.7423211812973023, 'train/ce_loss': 0.4671875, 'train/seg_cls_loss': 0.016961669921875, 'train/kl_loss': 0.2638671875, 'train/mask_bce_loss': 0.11987131461501122, 'train/mask_dice_loss': 0.5003127038478852, 'train/mask_loss': 0.6201840162277221, 'metrics/total_secs_per_batch': 5.886725902557373, 'metrics/data_secs_per_batch': 2.417216753959656, '_timestamp': 1740959916.4106896}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 435 is less than current step: 499. Dropping entry: {'train/lr': 0.00013053061224489794, '_timestamp': 1740959916.4109786}).
Epoch: [2][437/500]	Time  6.118 ( 6.118)	Loss 1.1016 (1.4768)	CeLoss 1.1016 (0.3195)	SegCLSLoss 0.0000 (0.0184)	KLLoss 0.0000 (0.3395)	MaskLoss 0.0000 (0.5571)	MaskBCELoss 0.0000 (0.1367)	MaskDICELoss 0.0000 (0.4204)
Epoch: [2][438/500]	Time  6.169 ( 6.169)	Loss 1.4805 (1.5407)	CeLoss 0.2832 (0.3477)	SegCLSLoss 0.0140 (0.0125)	KLLoss 0.3906 (0.3059)	MaskLoss 0.5752 (0.5781)	MaskBCELoss 0.1265 (0.1023)	MaskDICELoss 0.4487 (0.4757)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 436 is less than current step: 499. Dropping entry: {'train/loss': 1.4768262058496475, 'train/ce_loss': 0.31953125, 'train/seg_cls_loss': 0.01844482421875, 'train/kl_loss': 0.339453125, 'train/mask_bce_loss': 0.13667596988379954, 'train/mask_dice_loss': 0.4203894816339016, 'train/mask_loss': 0.5570654481649399, 'metrics/total_secs_per_batch': 6.1181299686431885, 'metrics/data_secs_per_batch': 2.8122012853622436, '_timestamp': 1740959922.5288284}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 436 is less than current step: 499. Dropping entry: {'train/lr': 0.0001304081632653061, '_timestamp': 1740959922.5290177}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 437 is less than current step: 499. Dropping entry: {'train/loss': 1.5407254815101623, 'train/ce_loss': 0.34765625, 'train/seg_cls_loss': 0.012530517578125, 'train/kl_loss': 0.305859375, 'train/mask_bce_loss': 0.10232995934784413, 'train/mask_dice_loss': 0.47574764490127563, 'train/mask_loss': 0.5780775964260101, 'metrics/total_secs_per_batch': 6.169192314147949, 'metrics/data_secs_per_batch': 2.723576235771179, '_timestamp': 1740959928.6981637}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 437 is less than current step: 499. Dropping entry: {'train/lr': 0.00013028571428571427, '_timestamp': 1740959928.6984966}).
Epoch: [2][439/500]	Time  6.553 ( 6.553)	Loss 1.5791 (1.5658)	CeLoss 0.1992 (0.2750)	SegCLSLoss 0.0155 (0.0133)	KLLoss 0.3828 (0.3047)	MaskLoss 0.6670 (0.6267)	MaskBCELoss 0.0650 (0.1620)	MaskDICELoss 0.6019 (0.4647)
[2025-03-02 17:59:00,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=1440, skipped=0, lr=[0.00013010204081632652], mom=[(0.9, 0.95)]
[2025-03-02 17:59:00,383] [INFO] [timer.py:215:stop] epoch=0/micro_step=14400/global_step=1440, RunningAvgSamplesPerSec=1.5638385855420696, CurrSamplesPerSec=1.9487351609044226, MemAllocated=31.26GB, MaxMemAllocated=37.19GB
Epoch: [2][440/500]	Time  5.133 ( 5.133)	Loss 2.5813 (1.7328)	CeLoss 0.1787 (0.4878)	SegCLSLoss 0.0278 (0.0141)	KLLoss 0.3789 (0.2658)	MaskLoss 1.1754 (0.6057)	MaskBCELoss 0.2684 (0.1229)	MaskDICELoss 0.9070 (0.4828)
Epoch: [2][441/500]	Time  7.472 ( 7.472)	Loss 1.3037 (1.6083)	CeLoss 0.2695 (0.5472)	SegCLSLoss 0.0162 (0.0130)	KLLoss 0.3809 (0.3072)	MaskLoss 0.4936 (0.5119)	MaskBCELoss 0.0427 (0.0739)	MaskDICELoss 0.4509 (0.4380)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 438 is less than current step: 499. Dropping entry: {'train/loss': 1.565813374519348, 'train/ce_loss': 0.275, 'train/seg_cls_loss': 0.01334228515625, 'train/kl_loss': 0.3046875, 'train/mask_bce_loss': 0.1620037481188774, 'train/mask_dice_loss': 0.4647017776966095, 'train/mask_loss': 0.6267055183649063, 'metrics/total_secs_per_batch': 6.553443908691406, 'metrics/data_secs_per_batch': 2.895605373382568, '_timestamp': 1740959935.2514257}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 438 is less than current step: 499. Dropping entry: {'train/lr': 0.00013016326530612243, '_timestamp': 1740959935.251703}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 439 is less than current step: 499. Dropping entry: {'train/loss': 1.7327727913856505, 'train/ce_loss': 0.48779296875, 'train/seg_cls_loss': 0.014141845703125, 'train/kl_loss': 0.2658203125, 'train/mask_bce_loss': 0.1228585459291935, 'train/mask_dice_loss': 0.4828345090150833, 'train/mask_loss': 0.6056930452585221, 'metrics/total_secs_per_batch': 5.133153438568115, 'metrics/data_secs_per_batch': 2.2446969509124757, '_timestamp': 1740959940.3844302}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 439 is less than current step: 499. Dropping entry: {'train/lr': 0.0001300408163265306, '_timestamp': 1740959940.3847265}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 440 is less than current step: 499. Dropping entry: {'train/loss': 1.608272337913513, 'train/ce_loss': 0.54716796875, 'train/seg_cls_loss': 0.0130126953125, 'train/kl_loss': 0.3072265625, 'train/mask_bce_loss': 0.07394028003327549, 'train/mask_dice_loss': 0.43800841122865675, 'train/mask_loss': 0.5119486927986145, 'metrics/total_secs_per_batch': 7.471693754196167, 'metrics/data_secs_per_batch': 3.317382645606995, '_timestamp': 1740959947.8565423}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 440 is less than current step: 499. Dropping entry: {'train/lr': 0.00012991836734693875, '_timestamp': 1740959947.8569014}).
Epoch: [2][442/500]	Time  6.522 ( 6.522)	Loss 0.2275 (1.3892)	CeLoss 0.2275 (0.4143)	SegCLSLoss 0.0000 (0.0145)	KLLoss 0.0000 (0.2635)	MaskLoss 0.0000 (0.4707)	MaskBCELoss 0.0000 (0.0577)	MaskDICELoss 0.0000 (0.4130)
Epoch: [2][443/500]	Time  6.467 ( 6.467)	Loss 2.4513 (1.6670)	CeLoss 0.1279 (0.2866)	SegCLSLoss 0.0291 (0.0158)	KLLoss 0.3809 (0.3439)	MaskLoss 1.1353 (0.6690)	MaskBCELoss 0.2138 (0.0933)	MaskDICELoss 0.9215 (0.5758)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 441 is less than current step: 499. Dropping entry: {'train/loss': 1.3891731381416321, 'train/ce_loss': 0.4142578125, 'train/seg_cls_loss': 0.0145263671875, 'train/kl_loss': 0.2634765625, 'train/mask_bce_loss': 0.05774371176958084, 'train/mask_dice_loss': 0.41296590864658356, 'train/mask_loss': 0.47070961594581606, 'metrics/total_secs_per_batch': 6.5219197273254395, 'metrics/data_secs_per_batch': 2.899851679801941, '_timestamp': 1740959954.3782964}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 441 is less than current step: 499. Dropping entry: {'train/lr': 0.00012979591836734695, '_timestamp': 1740959954.3786101}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 442 is less than current step: 499. Dropping entry: {'train/loss': 1.6670380592346192, 'train/ce_loss': 0.28662109375, 'train/seg_cls_loss': 0.015789794921875, 'train/kl_loss': 0.3439453125, 'train/mask_bce_loss': 0.09325597677379846, 'train/mask_dice_loss': 0.5757611006498337, 'train/mask_loss': 0.6690170764923096, 'metrics/total_secs_per_batch': 6.466891050338745, 'metrics/data_secs_per_batch': 3.1796563625335694, '_timestamp': 1740959960.8452694}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 442 is less than current step: 499. Dropping entry: {'train/lr': 0.0001296734693877551, '_timestamp': 1740959960.8456354}).
Epoch: [2][444/500]	Time  5.972 ( 5.972)	Loss 1.2924 (1.1216)	CeLoss 0.4375 (0.3156)	SegCLSLoss 0.0115 (0.0095)	KLLoss 0.3809 (0.2291)	MaskLoss 0.4060 (0.3891)	MaskBCELoss 0.0800 (0.0815)	MaskDICELoss 0.3260 (0.3075)
Epoch: [2][445/500]	Time  4.379 ( 4.379)	Loss 1.5000 (1.8402)	CeLoss 1.5000 (0.7458)	SegCLSLoss 0.0000 (0.0079)	KLLoss 0.0000 (0.1900)	MaskLoss 0.0000 (0.5359)	MaskBCELoss 0.0000 (0.2298)	MaskDICELoss 0.0000 (0.3061)
Epoch: [2][446/500]	Time  5.139 ( 5.139)	Loss 2.0508 (1.8888)	CeLoss 0.2266 (0.5760)	SegCLSLoss 0.0189 (0.0111)	KLLoss 0.3750 (0.2279)	MaskLoss 0.8887 (0.6422)	MaskBCELoss 0.0320 (0.2340)	MaskDICELoss 0.8567 (0.4082)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 443 is less than current step: 499. Dropping entry: {'train/loss': 1.121576076745987, 'train/ce_loss': 0.315576171875, 'train/seg_cls_loss': 0.009503173828125, 'train/kl_loss': 0.2291015625, 'train/mask_bce_loss': 0.08153690621256829, 'train/mask_dice_loss': 0.3075470268726349, 'train/mask_loss': 0.38908393383026124, 'metrics/total_secs_per_batch': 5.971837997436523, 'metrics/data_secs_per_batch': 2.3739078283309936, '_timestamp': 1740959966.8169758}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 443 is less than current step: 499. Dropping entry: {'train/lr': 0.00012955102040816327, '_timestamp': 1740959966.8173175}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 444 is less than current step: 499. Dropping entry: {'train/loss': 1.8401915788650514, 'train/ce_loss': 0.74580078125, 'train/seg_cls_loss': 0.007879638671875, 'train/kl_loss': 0.1900390625, 'train/mask_bce_loss': 0.229787664860487, 'train/mask_dice_loss': 0.30612843930721284, 'train/mask_loss': 0.5359161019325256, 'metrics/total_secs_per_batch': 4.378536939620972, 'metrics/data_secs_per_batch': 2.1737077474594115, '_timestamp': 1740959971.1955984}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 444 is less than current step: 499. Dropping entry: {'train/lr': 0.0001294285714285714, '_timestamp': 1740959971.1959124}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 445 is less than current step: 499. Dropping entry: {'train/loss': 1.8888037264347077, 'train/ce_loss': 0.5759765625, 'train/seg_cls_loss': 0.011114501953125, 'train/kl_loss': 0.2279296875, 'train/mask_bce_loss': 0.23396617732942104, 'train/mask_dice_loss': 0.4082384049892426, 'train/mask_loss': 0.6422045871615409, 'metrics/total_secs_per_batch': 5.138716697692871, 'metrics/data_secs_per_batch': 2.364947271347046, '_timestamp': 1740959976.3359995}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 445 is less than current step: 499. Dropping entry: {'train/lr': 0.00012930612244897957, '_timestamp': 1740959976.3363264}).
Epoch: [2][447/500]	Time  5.095 ( 5.095)	Loss 1.0859 (1.7419)	CeLoss 1.0859 (0.4266)	SegCLSLoss 0.0000 (0.0122)	KLLoss 0.0000 (0.3121)	MaskLoss 0.0000 (0.6390)	MaskBCELoss 0.0000 (0.1331)	MaskDICELoss 0.0000 (0.5060)
Epoch: [2][448/500]	Time  5.884 ( 5.884)	Loss 0.1494 (1.4016)	CeLoss 0.1494 (0.3917)	SegCLSLoss 0.0000 (0.0118)	KLLoss 0.0000 (0.2287)	MaskLoss 0.0000 (0.4906)	MaskBCELoss 0.0000 (0.1048)	MaskDICELoss 0.0000 (0.3859)
Epoch: [2][449/500]	Time  5.887 ( 5.887)	Loss 0.0864 (1.3562)	CeLoss 0.0864 (0.4491)	SegCLSLoss 0.0000 (0.0118)	KLLoss 0.0000 (0.2305)	MaskLoss 0.0000 (0.4390)	MaskBCELoss 0.0000 (0.0594)	MaskDICELoss 0.0000 (0.3796)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 446 is less than current step: 499. Dropping entry: {'train/loss': 1.741942083835602, 'train/ce_loss': 0.4265625, 'train/seg_cls_loss': 0.012158203125, 'train/kl_loss': 0.312109375, 'train/mask_bce_loss': 0.1330749040469527, 'train/mask_dice_loss': 0.5059625476598739, 'train/mask_loss': 0.6390374422073364, 'metrics/total_secs_per_batch': 5.094639778137207, 'metrics/data_secs_per_batch': 2.242673468589783, '_timestamp': 1740959981.4288538}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 446 is less than current step: 499. Dropping entry: {'train/lr': 0.00012918367346938773, '_timestamp': 1740959981.4291453}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 447 is less than current step: 499. Dropping entry: {'train/loss': 1.4016160249710083, 'train/ce_loss': 0.391748046875, 'train/seg_cls_loss': 0.01180419921875, 'train/kl_loss': 0.2287109375, 'train/mask_bce_loss': 0.10476983413100242, 'train/mask_dice_loss': 0.3858575075864792, 'train/mask_loss': 0.4906273424625397, 'metrics/total_secs_per_batch': 5.883715867996216, 'metrics/data_secs_per_batch': 2.701641011238098, '_timestamp': 1740959987.3128746}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 447 is less than current step: 499. Dropping entry: {'train/lr': 0.0001290612244897959, '_timestamp': 1740959987.3132558}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 448 is less than current step: 499. Dropping entry: {'train/loss': 1.3561761915683745, 'train/ce_loss': 0.449072265625, 'train/seg_cls_loss': 0.011767578125, 'train/kl_loss': 0.23046875, 'train/mask_bce_loss': 0.05935517475008965, 'train/mask_dice_loss': 0.3796459995210171, 'train/mask_loss': 0.43900116682052615, 'metrics/total_secs_per_batch': 5.886688947677612, 'metrics/data_secs_per_batch': 2.2587690353393555, '_timestamp': 1740959993.1993136}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 448 is less than current step: 499. Dropping entry: {'train/lr': 0.00012893877551020406, '_timestamp': 1740959993.1996043}).
[2025-03-02 18:00:00,297] [INFO] [logging.py:96:log_dist] [Rank 0] step=1450, skipped=0, lr=[0.00012887755102040815], mom=[(0.9, 0.95)]
[2025-03-02 18:00:00,302] [INFO] [timer.py:215:stop] epoch=0/micro_step=14500/global_step=1450, RunningAvgSamplesPerSec=1.5645215162486494, CurrSamplesPerSec=1.4079794827208558, MemAllocated=31.66GB, MaxMemAllocated=37.19GB
Epoch: [2][450/500]	Time  7.104 ( 7.104)	Loss 0.2002 (1.8690)	CeLoss 0.2002 (0.2119)	SegCLSLoss 0.0000 (0.0152)	KLLoss 0.0000 (0.2980)	MaskLoss 0.0000 (0.8099)	MaskBCELoss 0.0000 (0.2693)	MaskDICELoss 0.0000 (0.5406)
Epoch: [2][451/500]	Time  6.274 ( 6.274)	Loss 2.2945 (1.6293)	CeLoss 0.2422 (0.3537)	SegCLSLoss 0.0208 (0.0136)	KLLoss 0.3633 (0.3027)	MaskLoss 1.0027 (0.6190)	MaskBCELoss 0.0639 (0.2022)	MaskDICELoss 0.9389 (0.4169)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 449 is less than current step: 499. Dropping entry: {'train/loss': 1.869010078907013, 'train/ce_loss': 0.211865234375, 'train/seg_cls_loss': 0.01517333984375, 'train/kl_loss': 0.298046875, 'train/mask_bce_loss': 0.26929351929575207, 'train/mask_dice_loss': 0.5405777066946029, 'train/mask_loss': 0.8098712384700775, 'metrics/total_secs_per_batch': 7.10407018661499, 'metrics/data_secs_per_batch': 3.191021370887756, '_timestamp': 1740960000.3031766}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 449 is less than current step: 499. Dropping entry: {'train/lr': 0.00012881632653061222, '_timestamp': 1740960000.3034627}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 450 is less than current step: 499. Dropping entry: {'train/loss': 1.6292704939842224, 'train/ce_loss': 0.3537109375, 'train/seg_cls_loss': 0.0136474609375, 'train/kl_loss': 0.302734375, 'train/mask_bce_loss': 0.2021517504006624, 'train/mask_dice_loss': 0.4168780207633972, 'train/mask_loss': 0.6190297707915307, 'metrics/total_secs_per_batch': 6.274471044540405, 'metrics/data_secs_per_batch': 2.663143825531006, '_timestamp': 1740960006.5781093}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 450 is less than current step: 499. Dropping entry: {'train/lr': 0.00012869387755102039, '_timestamp': 1740960006.5784714}).
Epoch: [2][452/500]	Time  7.569 ( 7.569)	Loss 2.2629 (1.9184)	CeLoss 0.1787 (0.3144)	SegCLSLoss 0.0334 (0.0190)	KLLoss 0.3770 (0.3430)	MaskLoss 1.0153 (0.7803)	MaskBCELoss 0.0174 (0.1433)	MaskDICELoss 0.9979 (0.6370)
Epoch: [2][453/500]	Time  5.740 ( 5.740)	Loss 2.5982 (1.6761)	CeLoss 0.1357 (0.2038)	SegCLSLoss 0.0391 (0.0175)	KLLoss 0.3633 (0.3045)	MaskLoss 1.2034 (0.7166)	MaskBCELoss 0.3728 (0.1698)	MaskDICELoss 0.8306 (0.5468)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 451 is less than current step: 499. Dropping entry: {'train/loss': 1.9184411764144897, 'train/ce_loss': 0.31435546875, 'train/seg_cls_loss': 0.019012451171875, 'train/kl_loss': 0.34296875, 'train/mask_bce_loss': 0.14327986827120184, 'train/mask_dice_loss': 0.637034472823143, 'train/mask_loss': 0.7803143322467804, 'metrics/total_secs_per_batch': 7.569305896759033, 'metrics/data_secs_per_batch': 3.752990245819092, '_timestamp': 1740960014.1471596}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 451 is less than current step: 499. Dropping entry: {'train/lr': 0.00012857142857142855, '_timestamp': 1740960014.1474414}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 452 is less than current step: 499. Dropping entry: {'train/loss': 1.6760991871356965, 'train/ce_loss': 0.20380859375, 'train/seg_cls_loss': 0.017462158203125, 'train/kl_loss': 0.3044921875, 'train/mask_bce_loss': 0.16979563143104315, 'train/mask_dice_loss': 0.5467940002679825, 'train/mask_loss': 0.7165896385908127, 'metrics/total_secs_per_batch': 5.739922523498535, 'metrics/data_secs_per_batch': 2.3527546644210817, '_timestamp': 1740960019.8870673}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 452 is less than current step: 499. Dropping entry: {'train/lr': 0.00012844897959183674, '_timestamp': 1740960019.8874183}).
Epoch: [2][454/500]	Time  5.443 ( 5.443)	Loss 4.3329 (2.1035)	CeLoss 0.2559 (0.3088)	SegCLSLoss 0.0156 (0.0193)	KLLoss 0.3906 (0.3398)	MaskLoss 2.0151 (0.8756)	MaskBCELoss 1.2867 (0.3153)	MaskDICELoss 0.7284 (0.5602)
Epoch: [2][455/500]	Time  4.830 ( 4.830)	Loss 1.4766 (1.6911)	CeLoss 1.4766 (0.6746)	SegCLSLoss 0.0000 (0.0128)	KLLoss 0.0000 (0.2248)	MaskLoss 0.0000 (0.4937)	MaskBCELoss 0.0000 (0.0861)	MaskDICELoss 0.0000 (0.4077)
Epoch: [2][456/500]	Time  5.378 ( 5.378)	Loss 1.1797 (1.2477)	CeLoss 1.1797 (0.6677)	SegCLSLoss 0.0000 (0.0063)	KLLoss 0.0000 (0.1936)	MaskLoss 0.0000 (0.2787)	MaskBCELoss 0.0000 (0.0534)	MaskDICELoss 0.0000 (0.2253)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 453 is less than current step: 499. Dropping entry: {'train/loss': 2.1034828186035157, 'train/ce_loss': 0.3087890625, 'train/seg_cls_loss': 0.01925048828125, 'train/kl_loss': 0.33984375, 'train/mask_bce_loss': 0.315339445322752, 'train/mask_dice_loss': 0.5602301120758056, 'train/mask_loss': 0.8755695343017578, 'metrics/total_secs_per_batch': 5.442928791046143, 'metrics/data_secs_per_batch': 2.559869718551636, '_timestamp': 1740960025.3300014}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 453 is less than current step: 499. Dropping entry: {'train/lr': 0.0001283265306122449, '_timestamp': 1740960025.3302855}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 454 is less than current step: 499. Dropping entry: {'train/loss': 1.6911425113677978, 'train/ce_loss': 0.674609375, 'train/seg_cls_loss': 0.012811279296875, 'train/kl_loss': 0.2248046875, 'train/mask_bce_loss': 0.0860566407442093, 'train/mask_dice_loss': 0.40765914171934126, 'train/mask_loss': 0.49371577501297, 'metrics/total_secs_per_batch': 4.829549551010132, 'metrics/data_secs_per_batch': 2.3946762561798094, '_timestamp': 1740960030.1599698}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 454 is less than current step: 499. Dropping entry: {'train/lr': 0.00012820408163265306, '_timestamp': 1740960030.160384}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 455 is less than current step: 499. Dropping entry: {'train/loss': 1.2476807057857513, 'train/ce_loss': 0.667724609375, 'train/seg_cls_loss': 0.006268310546875, 'train/kl_loss': 0.1935546875, 'train/mask_bce_loss': 0.05340326093137264, 'train/mask_dice_loss': 0.22534432262182236, 'train/mask_loss': 0.27874757945537565, 'metrics/total_secs_per_batch': 5.378211259841919, 'metrics/data_secs_per_batch': 2.300918626785278, '_timestamp': 1740960035.5377371}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 455 is less than current step: 499. Dropping entry: {'train/lr': 0.00012808163265306123, '_timestamp': 1740960035.5380309}).
Epoch: [2][457/500]	Time  6.302 ( 6.302)	Loss 0.2656 (1.3890)	CeLoss 0.2656 (0.3670)	SegCLSLoss 0.0000 (0.0129)	KLLoss 0.0000 (0.2273)	MaskLoss 0.0000 (0.4963)	MaskBCELoss 0.0000 (0.0618)	MaskDICELoss 0.0000 (0.4344)
Epoch: [2][458/500]	Time  6.001 ( 6.001)	Loss 2.6680 (2.0634)	CeLoss 0.4863 (0.4744)	SegCLSLoss 0.0122 (0.0162)	KLLoss 0.3906 (0.3014)	MaskLoss 1.0684 (0.7752)	MaskBCELoss 0.0690 (0.1892)	MaskDICELoss 0.9993 (0.5861)
Epoch: [2][459/500]	Time  5.759 ( 5.759)	Loss 2.1680 (1.7068)	CeLoss 0.2266 (0.5409)	SegCLSLoss 0.0244 (0.0093)	KLLoss 0.3711 (0.2299)	MaskLoss 0.9463 (0.5691)	MaskBCELoss 0.0373 (0.2505)	MaskDICELoss 0.9090 (0.3186)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 456 is less than current step: 499. Dropping entry: {'train/loss': 1.3889673829078675, 'train/ce_loss': 0.367041015625, 'train/seg_cls_loss': 0.01292724609375, 'train/kl_loss': 0.22734375, 'train/mask_bce_loss': 0.06183868981897831, 'train/mask_dice_loss': 0.43442722856998445, 'train/mask_loss': 0.4962659180164337, 'metrics/total_secs_per_batch': 6.3020970821380615, 'metrics/data_secs_per_batch': 2.959020161628723, '_timestamp': 1740960041.839831}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 456 is less than current step: 499. Dropping entry: {'train/lr': 0.0001279591836734694, '_timestamp': 1740960041.840026}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 457 is less than current step: 499. Dropping entry: {'train/loss': 2.0633818984031675, 'train/ce_loss': 0.4744140625, 'train/seg_cls_loss': 0.016229248046875, 'train/kl_loss': 0.3013671875, 'train/mask_bce_loss': 0.18915906213223935, 'train/mask_dice_loss': 0.5860865831375122, 'train/mask_loss': 0.7752456367015839, 'metrics/total_secs_per_batch': 6.000783205032349, 'metrics/data_secs_per_batch': 2.6922826766967773, '_timestamp': 1740960047.8409107}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 457 is less than current step: 499. Dropping entry: {'train/lr': 0.00012783673469387755, '_timestamp': 1740960047.8413}).
[2025-03-02 18:00:59,283] [INFO] [logging.py:96:log_dist] [Rank 0] step=1460, skipped=0, lr=[0.00012765306122448979], mom=[(0.9, 0.95)]
[2025-03-02 18:00:59,288] [INFO] [timer.py:215:stop] epoch=0/micro_step=14600/global_step=1460, RunningAvgSamplesPerSec=1.5653520433923371, CurrSamplesPerSec=1.7579360091449636, MemAllocated=30.69GB, MaxMemAllocated=37.19GB
Epoch: [2][460/500]	Time  5.690 ( 5.690)	Loss 1.7266 (1.7105)	CeLoss 1.7266 (0.4422)	SegCLSLoss 0.0000 (0.0118)	KLLoss 0.0000 (0.2250)	MaskLoss 0.0000 (0.6200)	MaskBCELoss 0.0000 (0.2278)	MaskDICELoss 0.0000 (0.3922)
Epoch: [2][461/500]	Time  6.324 ( 6.324)	Loss 1.2717 (1.7801)	CeLoss 0.2393 (0.4359)	SegCLSLoss 0.0251 (0.0177)	KLLoss 0.3828 (0.3021)	MaskLoss 0.4904 (0.6526)	MaskBCELoss 0.1478 (0.1156)	MaskDICELoss 0.3426 (0.5370)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 458 is less than current step: 499. Dropping entry: {'train/loss': 1.7068095803260803, 'train/ce_loss': 0.540869140625, 'train/seg_cls_loss': 0.00926513671875, 'train/kl_loss': 0.2298828125, 'train/mask_bce_loss': 0.25048576537519696, 'train/mask_dice_loss': 0.3185684412717819, 'train/mask_loss': 0.5690542042255402, 'metrics/total_secs_per_batch': 5.7588841915130615, 'metrics/data_secs_per_batch': 2.7461253881454466, '_timestamp': 1740960053.5995066}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 458 is less than current step: 499. Dropping entry: {'train/lr': 0.0001277142857142857, '_timestamp': 1740960053.599697}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 459 is less than current step: 499. Dropping entry: {'train/loss': 1.7105294823646546, 'train/ce_loss': 0.442236328125, 'train/seg_cls_loss': 0.011822509765625, 'train/kl_loss': 0.225, 'train/mask_bce_loss': 0.22784597426652908, 'train/mask_dice_loss': 0.39218929708003997, 'train/mask_loss': 0.6200352549552918, 'metrics/total_secs_per_batch': 5.690011262893677, 'metrics/data_secs_per_batch': 2.5034037113189695, '_timestamp': 1740960059.2893593}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 459 is less than current step: 499. Dropping entry: {'train/lr': 0.00012759183673469385, '_timestamp': 1740960059.2896311}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 460 is less than current step: 499. Dropping entry: {'train/loss': 1.7801493287086487, 'train/ce_loss': 0.4359375, 'train/seg_cls_loss': 0.017694091796875, 'train/kl_loss': 0.3021484375, 'train/mask_bce_loss': 0.11557934116572141, 'train/mask_dice_loss': 0.5369953334331512, 'train/mask_loss': 0.6525746762752533, 'metrics/total_secs_per_batch': 6.323898792266846, 'metrics/data_secs_per_batch': 2.719432806968689, '_timestamp': 1740960065.6135616}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 460 is less than current step: 499. Dropping entry: {'train/lr': 0.00012746938775510202, '_timestamp': 1740960065.6138785}).
Epoch: [2][462/500]	Time  5.363 ( 5.363)	Loss 1.7907 (1.5402)	CeLoss 0.2197 (0.6383)	SegCLSLoss 0.0159 (0.0101)	KLLoss 0.3789 (0.2268)	MaskLoss 0.7625 (0.4370)	MaskBCELoss 0.0929 (0.0671)	MaskDICELoss 0.6696 (0.3700)
Epoch: [2][463/500]	Time  6.770 ( 6.770)	Loss 1.1562 (1.4968)	CeLoss 1.1562 (0.3126)	SegCLSLoss 0.0000 (0.0141)	KLLoss 0.0000 (0.2641)	MaskLoss 0.0000 (0.5751)	MaskBCELoss 0.0000 (0.0909)	MaskDICELoss 0.0000 (0.4842)
Epoch: [2][464/500]	Time  5.294 ( 5.294)	Loss 2.1012 (1.9762)	CeLoss 0.2158 (0.3703)	SegCLSLoss 0.0190 (0.0139)	KLLoss 0.3887 (0.3445)	MaskLoss 0.9188 (0.7824)	MaskBCELoss 0.0113 (0.1990)	MaskDICELoss 0.9075 (0.5834)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 461 is less than current step: 499. Dropping entry: {'train/loss': 1.5401566803455353, 'train/ce_loss': 0.63828125, 'train/seg_cls_loss': 0.010107421875, 'train/kl_loss': 0.2267578125, 'train/mask_bce_loss': 0.06706059165298939, 'train/mask_dice_loss': 0.3699611097574234, 'train/mask_loss': 0.43702169954776765, 'metrics/total_secs_per_batch': 5.362694263458252, 'metrics/data_secs_per_batch': 2.3361658096313476, '_timestamp': 1740960070.9761968}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 461 is less than current step: 499. Dropping entry: {'train/lr': 0.00012734693877551018, '_timestamp': 1740960070.976498}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 462 is less than current step: 499. Dropping entry: {'train/loss': 1.4968072056770325, 'train/ce_loss': 0.3125732421875, 'train/seg_cls_loss': 0.0141357421875, 'train/kl_loss': 0.2640625, 'train/mask_bce_loss': 0.09093979578465224, 'train/mask_dice_loss': 0.4841849967837334, 'train/mask_loss': 0.5751247808337212, 'metrics/total_secs_per_batch': 6.769806146621704, 'metrics/data_secs_per_batch': 3.2589885234832763, '_timestamp': 1740960077.7459998}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 462 is less than current step: 499. Dropping entry: {'train/lr': 0.00012722448979591834, '_timestamp': 1740960077.7462814}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 463 is less than current step: 499. Dropping entry: {'train/loss': 1.9761910378932952, 'train/ce_loss': 0.3703125, 'train/seg_cls_loss': 0.013885498046875, 'train/kl_loss': 0.34453125, 'train/mask_bce_loss': 0.19902554359287022, 'train/mask_dice_loss': 0.5833570778369903, 'train/mask_loss': 0.7823826223611832, 'metrics/total_secs_per_batch': 5.293871164321899, 'metrics/data_secs_per_batch': 2.2571594953536986, '_timestamp': 1740960083.0398078}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 463 is less than current step: 499. Dropping entry: {'train/lr': 0.0001271020408163265, '_timestamp': 1740960083.0400903}).
Epoch: [2][465/500]	Time  4.962 ( 4.962)	Loss 1.4126 (1.0521)	CeLoss 0.1953 (0.4675)	SegCLSLoss 0.0168 (0.0070)	KLLoss 0.3906 (0.1568)	MaskLoss 0.5847 (0.2827)	MaskBCELoss 0.2944 (0.0629)	MaskDICELoss 0.2903 (0.2198)
Epoch: [2][466/500]	Time  4.519 ( 4.519)	Loss 0.9102 (1.5120)	CeLoss 0.9102 (0.5328)	SegCLSLoss 0.0000 (0.0115)	KLLoss 0.0000 (0.1893)	MaskLoss 0.0000 (0.4774)	MaskBCELoss 0.0000 (0.0895)	MaskDICELoss 0.0000 (0.3879)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 464 is less than current step: 499. Dropping entry: {'train/loss': 1.0521323323249816, 'train/ce_loss': 0.467529296875, 'train/seg_cls_loss': 0.007000732421875, 'train/kl_loss': 0.1568359375, 'train/mask_bce_loss': 0.0628693699836731, 'train/mask_dice_loss': 0.21981300711631774, 'train/mask_loss': 0.28268237709999083, 'metrics/total_secs_per_batch': 4.961586952209473, 'metrics/data_secs_per_batch': 2.1507482528686523, '_timestamp': 1740960088.0015051}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 464 is less than current step: 499. Dropping entry: {'train/lr': 0.0001269795918367347, '_timestamp': 1740960088.0017986}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 465 is less than current step: 499. Dropping entry: {'train/loss': 1.5120280027389525, 'train/ce_loss': 0.5328125, 'train/seg_cls_loss': 0.01151123046875, 'train/kl_loss': 0.1892578125, 'train/mask_bce_loss': 0.08947646890301257, 'train/mask_dice_loss': 0.38792423605918885, 'train/mask_loss': 0.47740070819854735, 'metrics/total_secs_per_batch': 4.518532752990723, 'metrics/data_secs_per_batch': 2.023145914077759, '_timestamp': 1740960092.519972}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 465 is less than current step: 499. Dropping entry: {'train/lr': 0.00012685714285714286, '_timestamp': 1740960092.5202415}).
Epoch: [2][467/500]	Time  5.921 ( 5.921)	Loss 2.2105 (1.9355)	CeLoss 0.2383 (0.3520)	SegCLSLoss 0.0198 (0.0155)	KLLoss 0.3770 (0.3449)	MaskLoss 0.9627 (0.7706)	MaskBCELoss 0.0118 (0.1884)	MaskDICELoss 0.9508 (0.5822)
Epoch: [2][468/500]	Time  6.005 ( 6.005)	Loss 2.0261 (1.8854)	CeLoss 0.2852 (0.4704)	SegCLSLoss 0.0121 (0.0166)	KLLoss 0.3848 (0.3010)	MaskLoss 0.8480 (0.6882)	MaskBCELoss 0.2875 (0.1154)	MaskDICELoss 0.5605 (0.5728)
Epoch: [2][469/500]	Time  6.613 ( 6.613)	Loss 1.6614 (2.0188)	CeLoss 0.1553 (0.3320)	SegCLSLoss 0.0305 (0.0190)	KLLoss 0.4102 (0.3453)	MaskLoss 0.7247 (0.8214)	MaskBCELoss 0.0841 (0.1589)	MaskDICELoss 0.6406 (0.6625)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 466 is less than current step: 499. Dropping entry: {'train/loss': 1.9355096638202667, 'train/ce_loss': 0.351953125, 'train/seg_cls_loss': 0.015478515625, 'train/kl_loss': 0.344921875, 'train/mask_bce_loss': 0.18839424811303615, 'train/mask_dice_loss': 0.5821926109492779, 'train/mask_loss': 0.7705868571996689, 'metrics/total_secs_per_batch': 5.9205334186553955, 'metrics/data_secs_per_batch': 2.6749483823776243, '_timestamp': 1740960098.4404888}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 466 is less than current step: 499. Dropping entry: {'train/lr': 0.00012673469387755102, '_timestamp': 1740960098.4407828}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 467 is less than current step: 499. Dropping entry: {'train/loss': 1.8854177713394165, 'train/ce_loss': 0.47041015625, 'train/seg_cls_loss': 0.01656494140625, 'train/kl_loss': 0.3009765625, 'train/mask_bce_loss': 0.1154143488034606, 'train/mask_dice_loss': 0.5728023409843445, 'train/mask_loss': 0.6882167011499405, 'metrics/total_secs_per_batch': 6.004809379577637, 'metrics/data_secs_per_batch': 2.7978424310684202, '_timestamp': 1740960104.4455876}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 467 is less than current step: 499. Dropping entry: {'train/lr': 0.00012661224489795918, '_timestamp': 1740960104.4458299}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 468 is less than current step: 499. Dropping entry: {'train/loss': 2.0187761425971984, 'train/ce_loss': 0.33203125, 'train/seg_cls_loss': 0.018988037109375, 'train/kl_loss': 0.3453125, 'train/mask_bce_loss': 0.15887323226779698, 'train/mask_dice_loss': 0.6624777257442475, 'train/mask_loss': 0.8213509559631348, 'metrics/total_secs_per_batch': 6.612769365310669, 'metrics/data_secs_per_batch': 3.051700496673584, '_timestamp': 1740960111.0581012}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 468 is less than current step: 499. Dropping entry: {'train/lr': 0.00012648979591836735, '_timestamp': 1740960111.0583985}).
[2025-03-02 18:01:56,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=1470, skipped=0, lr=[0.00012642857142857142], mom=[(0.9, 0.95)]
[2025-03-02 18:01:56,901] [INFO] [timer.py:215:stop] epoch=0/micro_step=14700/global_step=1470, RunningAvgSamplesPerSec=1.5664015976164323, CurrSamplesPerSec=1.7117295581698961, MemAllocated=30.72GB, MaxMemAllocated=37.19GB
Epoch: [2][470/500]	Time  5.844 ( 5.844)	Loss 0.8633 (1.4094)	CeLoss 0.8633 (0.4752)	SegCLSLoss 0.0000 (0.0127)	KLLoss 0.0000 (0.2684)	MaskLoss 0.0000 (0.4504)	MaskBCELoss 0.0000 (0.0869)	MaskDICELoss 0.0000 (0.3635)
Epoch: [2][471/500]	Time  5.767 ( 5.767)	Loss 0.5928 (1.6077)	CeLoss 0.2363 (0.3400)	SegCLSLoss 0.0149 (0.0158)	KLLoss 0.3965 (0.3057)	MaskLoss 0.1548 (0.6147)	MaskBCELoss 0.0710 (0.0967)	MaskDICELoss 0.0838 (0.5180)
Epoch: [2][472/500]	Time  4.116 ( 4.116)	Loss 2.8600 (1.9511)	CeLoss 0.2490 (0.5055)	SegCLSLoss 0.0217 (0.0202)	KLLoss 0.3789 (0.2623)	MaskLoss 1.2816 (0.7048)	MaskBCELoss 0.3985 (0.1979)	MaskDICELoss 0.8831 (0.5069)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 469 is less than current step: 499. Dropping entry: {'train/loss': 1.4094425797462464, 'train/ce_loss': 0.4751953125, 'train/seg_cls_loss': 0.01273193359375, 'train/kl_loss': 0.268359375, 'train/mask_bce_loss': 0.08694016141816974, 'train/mask_dice_loss': 0.36348425447940824, 'train/mask_loss': 0.45042442381381986, 'metrics/total_secs_per_batch': 5.8436620235443115, 'metrics/data_secs_per_batch': 2.4098324298858644, '_timestamp': 1740960116.90155}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 469 is less than current step: 499. Dropping entry: {'train/lr': 0.0001263673469387755, '_timestamp': 1740960116.9017346}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 470 is less than current step: 499. Dropping entry: {'train/loss': 1.6076870083808898, 'train/ce_loss': 0.3400390625, 'train/seg_cls_loss': 0.01580810546875, 'train/kl_loss': 0.3056640625, 'train/mask_bce_loss': 0.0967274947091937, 'train/mask_dice_loss': 0.5179558500647545, 'train/mask_loss': 0.6146833464503288, 'metrics/total_secs_per_batch': 5.767357110977173, 'metrics/data_secs_per_batch': 2.9371583223342896, '_timestamp': 1740960122.6691213}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 470 is less than current step: 499. Dropping entry: {'train/lr': 0.00012624489795918367, '_timestamp': 1740960122.6693966}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 471 is less than current step: 499. Dropping entry: {'train/loss': 1.951070785522461, 'train/ce_loss': 0.50546875, 'train/seg_cls_loss': 0.02015380859375, 'train/kl_loss': 0.2623046875, 'train/mask_bce_loss': 0.1978622902184725, 'train/mask_dice_loss': 0.5069211423397064, 'train/mask_loss': 0.7047834292054176, 'metrics/total_secs_per_batch': 4.116483211517334, 'metrics/data_secs_per_batch': 1.8354533433914184, '_timestamp': 1740960126.7856472}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 471 is less than current step: 499. Dropping entry: {'train/lr': 0.0001261224489795918, '_timestamp': 1740960126.7859428}).
Epoch: [2][473/500]	Time  5.915 ( 5.915)	Loss 3.1607 (1.9031)	CeLoss 0.2637 (0.3718)	SegCLSLoss 0.0194 (0.0184)	KLLoss 0.3770 (0.3426)	MaskLoss 1.4251 (0.7439)	MaskBCELoss 0.5852 (0.1760)	MaskDICELoss 0.8399 (0.5679)
Epoch: [2][474/500]	Time  5.443 ( 5.443)	Loss 2.5015 (1.6122)	CeLoss 0.2139 (0.5264)	SegCLSLoss 0.0242 (0.0132)	KLLoss 0.3770 (0.2238)	MaskLoss 1.1189 (0.5283)	MaskBCELoss 0.2527 (0.0527)	MaskDICELoss 0.8662 (0.4755)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 472 is less than current step: 499. Dropping entry: {'train/loss': 1.9030632615089416, 'train/ce_loss': 0.37177734375, 'train/seg_cls_loss': 0.01844482421875, 'train/kl_loss': 0.342578125, 'train/mask_bce_loss': 0.17597967498004435, 'train/mask_dice_loss': 0.5678859293460846, 'train/mask_loss': 0.7438656032085419, 'metrics/total_secs_per_batch': 5.9149699211120605, 'metrics/data_secs_per_batch': 2.5797680139541628, '_timestamp': 1740960132.7006102}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 472 is less than current step: 499. Dropping entry: {'train/lr': 0.00012599999999999997, '_timestamp': 1740960132.7008874}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 473 is less than current step: 499. Dropping entry: {'train/loss': 1.612177562713623, 'train/ce_loss': 0.5263671875, 'train/seg_cls_loss': 0.01319580078125, 'train/kl_loss': 0.223828125, 'train/mask_bce_loss': 0.05273245126008987, 'train/mask_dice_loss': 0.47552430629730225, 'train/mask_loss': 0.528256756067276, 'metrics/total_secs_per_batch': 5.442960023880005, 'metrics/data_secs_per_batch': 2.3314215660095217, '_timestamp': 1740960138.1435463}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 473 is less than current step: 499. Dropping entry: {'train/lr': 0.00012587755102040814, '_timestamp': 1740960138.1439044}).
Epoch: [2][475/500]	Time  6.826 ( 6.826)	Loss 2.3080 (1.5684)	CeLoss 0.2158 (0.2252)	SegCLSLoss 0.0136 (0.0164)	KLLoss 0.3906 (0.3457)	MaskLoss 1.0231 (0.6503)	MaskBCELoss 0.4151 (0.1481)	MaskDICELoss 0.6080 (0.5022)
Epoch: [2][476/500]	Time  6.282 ( 6.282)	Loss 5.0602 (2.4372)	CeLoss 0.1826 (0.2448)	SegCLSLoss 0.0264 (0.0190)	KLLoss 0.3672 (0.3801)	MaskLoss 2.4139 (1.0726)	MaskBCELoss 1.6978 (0.3314)	MaskDICELoss 0.7161 (0.7412)
Epoch: [2][477/500]	Time  4.924 ( 4.924)	Loss 2.0207 (1.6078)	CeLoss 0.1621 (0.5451)	SegCLSLoss 0.0204 (0.0125)	KLLoss 0.3711 (0.2680)	MaskLoss 0.9059 (0.5148)	MaskBCELoss 0.1727 (0.0895)	MaskDICELoss 0.7331 (0.4253)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 474 is less than current step: 499. Dropping entry: {'train/loss': 1.5683915138244628, 'train/ce_loss': 0.2251953125, 'train/seg_cls_loss': 0.016448974609375, 'train/kl_loss': 0.345703125, 'train/mask_bce_loss': 0.14807921685278416, 'train/mask_dice_loss': 0.5022298201918602, 'train/mask_loss': 0.650309032201767, 'metrics/total_secs_per_batch': 6.826399087905884, 'metrics/data_secs_per_batch': 3.1996712923049926, '_timestamp': 1740960144.9699266}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 474 is less than current step: 499. Dropping entry: {'train/lr': 0.0001257551020408163, '_timestamp': 1740960144.9702055}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 475 is less than current step: 499. Dropping entry: {'train/loss': 2.4371503591537476, 'train/ce_loss': 0.24482421875, 'train/seg_cls_loss': 0.019036865234375, 'train/kl_loss': 0.380078125, 'train/mask_bce_loss': 0.3314234359189868, 'train/mask_dice_loss': 0.7411556750535965, 'train/mask_loss': 1.0725790977478027, 'metrics/total_secs_per_batch': 6.282347202301025, 'metrics/data_secs_per_batch': 2.8972975969314576, '_timestamp': 1740960151.2524636}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 475 is less than current step: 499. Dropping entry: {'train/lr': 0.0001256326530612245, '_timestamp': 1740960151.2528014}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 476 is less than current step: 499. Dropping entry: {'train/loss': 1.607806146144867, 'train/ce_loss': 0.5451171875, 'train/seg_cls_loss': 0.0125244140625, 'train/kl_loss': 0.26796875, 'train/mask_bce_loss': 0.08953931927680969, 'train/mask_dice_loss': 0.42530124336481095, 'train/mask_loss': 0.5148405686020852, 'metrics/total_secs_per_batch': 4.924178838729858, 'metrics/data_secs_per_batch': 2.159246253967285, '_timestamp': 1740960156.176509}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 476 is less than current step: 499. Dropping entry: {'train/lr': 0.00012551020408163265, '_timestamp': 1740960156.1768692}).
Epoch: [2][478/500]	Time  5.910 ( 5.910)	Loss 0.0913 (1.8525)	CeLoss 0.0913 (0.3207)	SegCLSLoss 0.0000 (0.0162)	KLLoss 0.0000 (0.2996)	MaskLoss 0.0000 (0.7469)	MaskBCELoss 0.0000 (0.1220)	MaskDICELoss 0.0000 (0.6249)
Epoch: [2][479/500]	Time  6.441 ( 6.441)	Loss 2.1457 (1.9497)	CeLoss 0.2207 (0.2495)	SegCLSLoss 0.0149 (0.0175)	KLLoss 0.3828 (0.3826)	MaskLoss 0.9391 (0.8265)	MaskBCELoss 0.2608 (0.1342)	MaskDICELoss 0.6783 (0.6923)
[2025-03-02 18:02:54,159] [INFO] [logging.py:96:log_dist] [Rank 0] step=1480, skipped=0, lr=[0.00012520408163265305], mom=[(0.9, 0.95)]
[2025-03-02 18:02:54,165] [INFO] [timer.py:215:stop] epoch=0/micro_step=14800/global_step=1480, RunningAvgSamplesPerSec=1.5674961504496414, CurrSamplesPerSec=1.77404605621009, MemAllocated=31.54GB, MaxMemAllocated=37.19GB
Epoch: [2][480/500]	Time  5.639 ( 5.639)	Loss 2.0723 (1.8510)	CeLoss 0.2656 (0.6313)	SegCLSLoss 0.0166 (0.0144)	KLLoss 0.3848 (0.2654)	MaskLoss 0.8799 (0.5930)	MaskBCELoss 0.0748 (0.0959)	MaskDICELoss 0.8051 (0.4971)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 477 is less than current step: 499. Dropping entry: {'train/loss': 1.8525423407554626, 'train/ce_loss': 0.320654296875, 'train/seg_cls_loss': 0.01617431640625, 'train/kl_loss': 0.299609375, 'train/mask_bce_loss': 0.12204527352005243, 'train/mask_dice_loss': 0.6249045938253402, 'train/mask_loss': 0.7469498634338378, 'metrics/total_secs_per_batch': 5.909649610519409, 'metrics/data_secs_per_batch': 2.5017717123031615, '_timestamp': 1740960162.0861425}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 477 is less than current step: 499. Dropping entry: {'train/lr': 0.00012538775510204082, '_timestamp': 1740960162.0864289}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 478 is less than current step: 499. Dropping entry: {'train/loss': 1.949737012386322, 'train/ce_loss': 0.24951171875, 'train/seg_cls_loss': 0.017510986328125, 'train/kl_loss': 0.3826171875, 'train/mask_bce_loss': 0.1342019859701395, 'train/mask_dice_loss': 0.6923266500234604, 'train/mask_loss': 0.8265286445617676, 'metrics/total_secs_per_batch': 6.441397666931152, 'metrics/data_secs_per_batch': 2.8757222652435304, '_timestamp': 1740960168.5277262}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 478 is less than current step: 499. Dropping entry: {'train/lr': 0.00012526530612244898, '_timestamp': 1740960168.5280752}).
Epoch: [2][481/500]	Time  6.887 ( 6.887)	Loss 2.4242 (2.0781)	CeLoss 0.2285 (0.2341)	SegCLSLoss 0.0276 (0.0178)	KLLoss 0.3828 (0.3807)	MaskLoss 1.0715 (0.8984)	MaskBCELoss 0.5156 (0.1982)	MaskDICELoss 0.5558 (0.7002)
Epoch: [2][482/500]	Time  6.135 ( 6.135)	Loss 2.3982 (1.8813)	CeLoss 0.2188 (0.3660)	SegCLSLoss 0.0226 (0.0170)	KLLoss 0.3633 (0.3025)	MaskLoss 1.0653 (0.7382)	MaskBCELoss 0.0759 (0.1117)	MaskDICELoss 0.9894 (0.6265)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 479 is less than current step: 499. Dropping entry: {'train/loss': 1.8510018289089203, 'train/ce_loss': 0.63134765625, 'train/seg_cls_loss': 0.014422607421875, 'train/kl_loss': 0.2654296875, 'train/mask_bce_loss': 0.09593592318706215, 'train/mask_dice_loss': 0.4970942884683609, 'train/mask_loss': 0.5930302083492279, 'metrics/total_secs_per_batch': 5.6386730670928955, 'metrics/data_secs_per_batch': 2.513806414604187, '_timestamp': 1740960174.1660814}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 479 is less than current step: 499. Dropping entry: {'train/lr': 0.00012514285714285714, '_timestamp': 1740960174.166305}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 480 is less than current step: 499. Dropping entry: {'train/loss': 2.078139102458954, 'train/ce_loss': 0.23408203125, 'train/seg_cls_loss': 0.0177734375, 'train/kl_loss': 0.3806640625, 'train/mask_bce_loss': 0.19821763802319764, 'train/mask_dice_loss': 0.7001781076192856, 'train/mask_loss': 0.8983957320451736, 'metrics/total_secs_per_batch': 6.887147665023804, 'metrics/data_secs_per_batch': 3.325103449821472, '_timestamp': 1740960181.053322}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 480 is less than current step: 499. Dropping entry: {'train/lr': 0.0001250204081632653, '_timestamp': 1740960181.0536017}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 481 is less than current step: 499. Dropping entry: {'train/loss': 1.8812509059906006, 'train/ce_loss': 0.366015625, 'train/seg_cls_loss': 0.016973876953125, 'train/kl_loss': 0.3025390625, 'train/mask_bce_loss': 0.11171028092503547, 'train/mask_dice_loss': 0.6264737486839295, 'train/mask_loss': 0.7381840348243713, 'metrics/total_secs_per_batch': 6.134774208068848, 'metrics/data_secs_per_batch': 3.0116982460021973, '_timestamp': 1740960187.188343}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 481 is less than current step: 499. Dropping entry: {'train/lr': 0.00012489795918367347, '_timestamp': 1740960187.188703}).
Epoch: [2][483/500]	Time  6.201 ( 6.201)	Loss 1.5757 (2.0292)	CeLoss 0.2090 (0.3208)	SegCLSLoss 0.0151 (0.0179)	KLLoss 0.3887 (0.3381)	MaskLoss 0.6599 (0.8329)	MaskBCELoss 0.2760 (0.1178)	MaskDICELoss 0.3839 (0.7152)
Epoch: [2][484/500]	Time  5.581 ( 5.581)	Loss 2.7762 (1.7434)	CeLoss 0.2539 (0.4293)	SegCLSLoss 0.0349 (0.0148)	KLLoss 0.3809 (0.2684)	MaskLoss 1.2338 (0.6401)	MaskBCELoss 0.6107 (0.1345)	MaskDICELoss 0.6231 (0.5056)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 482 is less than current step: 499. Dropping entry: {'train/loss': 2.0292405009269716, 'train/ce_loss': 0.32080078125, 'train/seg_cls_loss': 0.0179443359375, 'train/kl_loss': 0.3380859375, 'train/mask_bce_loss': 0.11776825180277228, 'train/mask_dice_loss': 0.7151625245809555, 'train/mask_loss': 0.8329307854175567, 'metrics/total_secs_per_batch': 6.2014172077178955, 'metrics/data_secs_per_batch': 2.717794489860535, '_timestamp': 1740960193.3895628}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 482 is less than current step: 499. Dropping entry: {'train/lr': 0.00012477551020408163, '_timestamp': 1740960193.3898468}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 483 is less than current step: 499. Dropping entry: {'train/loss': 1.743369746208191, 'train/ce_loss': 0.429296875, 'train/seg_cls_loss': 0.01483154296875, 'train/kl_loss': 0.268359375, 'train/mask_bce_loss': 0.13450727192685008, 'train/mask_dice_loss': 0.5056346237659455, 'train/mask_loss': 0.640141898393631, 'metrics/total_secs_per_batch': 5.580885887145996, 'metrics/data_secs_per_batch': 2.3921332597732543, '_timestamp': 1740960198.970452}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 483 is less than current step: 499. Dropping entry: {'train/lr': 0.0001246530612244898, '_timestamp': 1740960198.970729}).
Epoch: [2][485/500]	Time  6.702 ( 6.702)	Loss 2.2063 (1.8365)	CeLoss 0.2402 (0.3215)	SegCLSLoss 0.0227 (0.0169)	KLLoss 0.3887 (0.3426)	MaskLoss 0.9576 (0.7362)	MaskBCELoss 0.0096 (0.1321)	MaskDICELoss 0.9480 (0.6041)
Epoch: [2][486/500]	Time  6.009 ( 6.009)	Loss 1.9945 (1.8936)	CeLoss 0.2812 (0.2328)	SegCLSLoss 0.0111 (0.0173)	KLLoss 0.3906 (0.3416)	MaskLoss 0.8341 (0.8089)	MaskBCELoss 0.2556 (0.2271)	MaskDICELoss 0.5786 (0.5817)
Epoch: [2][487/500]	Time  5.852 ( 5.852)	Loss 2.7276 (1.6980)	CeLoss 0.1494 (0.3131)	SegCLSLoss 0.0320 (0.0155)	KLLoss 0.3711 (0.3057)	MaskLoss 1.2627 (0.6734)	MaskBCELoss 0.2935 (0.1047)	MaskDICELoss 0.9692 (0.5686)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 484 is less than current step: 499. Dropping entry: {'train/loss': 1.8365333437919618, 'train/ce_loss': 0.321484375, 'train/seg_cls_loss': 0.01688232421875, 'train/kl_loss': 0.342578125, 'train/mask_bce_loss': 0.13213801197707653, 'train/mask_dice_loss': 0.6040973871946335, 'train/mask_loss': 0.736235411465168, 'metrics/total_secs_per_batch': 6.7021706104278564, 'metrics/data_secs_per_batch': 2.650718021392822, '_timestamp': 1740960205.6727343}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 484 is less than current step: 499. Dropping entry: {'train/lr': 0.00012453061224489793, '_timestamp': 1740960205.6730843}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 485 is less than current step: 499. Dropping entry: {'train/loss': 1.8936267256736756, 'train/ce_loss': 0.2328125, 'train/seg_cls_loss': 0.017303466796875, 'train/kl_loss': 0.3416015625, 'train/mask_bce_loss': 0.22712690383195877, 'train/mask_dice_loss': 0.581747005879879, 'train/mask_loss': 0.8088738977909088, 'metrics/total_secs_per_batch': 6.008999347686768, 'metrics/data_secs_per_batch': 2.82539381980896, '_timestamp': 1740960211.681632}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 485 is less than current step: 499. Dropping entry: {'train/lr': 0.0001244081632653061, '_timestamp': 1740960211.6819177}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 486 is less than current step: 499. Dropping entry: {'train/loss': 1.6980266869068146, 'train/ce_loss': 0.3130859375, 'train/seg_cls_loss': 0.015472412109375, 'train/kl_loss': 0.3056640625, 'train/mask_bce_loss': 0.10473779924213886, 'train/mask_dice_loss': 0.5686407595872879, 'train/mask_loss': 0.6733785659074784, 'metrics/total_secs_per_batch': 5.851701259613037, 'metrics/data_secs_per_batch': 2.908391261100769, '_timestamp': 1740960217.5333943}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 486 is less than current step: 499. Dropping entry: {'train/lr': 0.00012428571428571428, '_timestamp': 1740960217.533706}).
Epoch: [2][488/500]	Time  6.300 ( 6.300)	Loss 1.0141 (1.6973)	CeLoss 0.2236 (0.3176)	SegCLSLoss 0.0193 (0.0149)	KLLoss 0.3828 (0.3084)	MaskLoss 0.3713 (0.6708)	MaskBCELoss 0.0318 (0.1478)	MaskDICELoss 0.3395 (0.5230)
Epoch: [2][489/500]	Time  5.589 ( 5.589)	Loss 1.0953 (1.6944)	CeLoss 0.3066 (0.3880)	SegCLSLoss 0.0109 (0.0142)	KLLoss 0.3828 (0.3092)	MaskLoss 0.3718 (0.6341)	MaskBCELoss 0.0334 (0.1853)	MaskDICELoss 0.3384 (0.4488)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 487 is less than current step: 499. Dropping entry: {'train/loss': 1.6972668766975403, 'train/ce_loss': 0.317578125, 'train/seg_cls_loss': 0.0149169921875, 'train/kl_loss': 0.3083984375, 'train/mask_bce_loss': 0.14779799496755003, 'train/mask_dice_loss': 0.5230034202337265, 'train/mask_loss': 0.6708014130592346, 'metrics/total_secs_per_batch': 6.299535512924194, 'metrics/data_secs_per_batch': 2.8503094911575317, '_timestamp': 1740960223.8329582}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 487 is less than current step: 499. Dropping entry: {'train/lr': 0.00012416326530612245, '_timestamp': 1740960223.833264}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 488 is less than current step: 499. Dropping entry: {'train/loss': 1.6943594098091126, 'train/ce_loss': 0.38798828125, 'train/seg_cls_loss': 0.014154052734375, 'train/kl_loss': 0.3091796875, 'train/mask_bce_loss': 0.18526684828102588, 'train/mask_dice_loss': 0.4488269090652466, 'train/mask_loss': 0.6340937554836273, 'metrics/total_secs_per_batch': 5.588790655136108, 'metrics/data_secs_per_batch': 2.298633337020874, '_timestamp': 1740960229.4216704}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 488 is less than current step: 499. Dropping entry: {'train/lr': 0.0001240408163265306, '_timestamp': 1740960229.4219675}).
[2025-03-02 18:03:55,830] [INFO] [logging.py:96:log_dist] [Rank 0] step=1490, skipped=0, lr=[0.00012397959183673468], mom=[(0.9, 0.95)]
[2025-03-02 18:03:55,836] [INFO] [timer.py:215:stop] epoch=0/micro_step=14900/global_step=1490, RunningAvgSamplesPerSec=1.567849295273182, CurrSamplesPerSec=1.5592144418740295, MemAllocated=31.26GB, MaxMemAllocated=37.19GB
Epoch: [2][490/500]	Time  6.415 ( 6.415)	Loss 2.4714 (1.7708)	CeLoss 0.1953 (0.4027)	SegCLSLoss 0.0278 (0.0141)	KLLoss 0.3730 (0.3070)	MaskLoss 1.1126 (0.6652)	MaskBCELoss 0.2541 (0.1718)	MaskDICELoss 0.8586 (0.4934)
Epoch: [2][491/500]	Time  5.879 ( 5.879)	Loss 2.1122 (1.9672)	CeLoss 0.1592 (0.3257)	SegCLSLoss 0.0320 (0.0202)	KLLoss 0.3750 (0.3031)	MaskLoss 0.9497 (0.8006)	MaskBCELoss 0.0764 (0.2221)	MaskDICELoss 0.8732 (0.5785)
Epoch: [2][492/500]	Time  4.940 ( 4.940)	Loss 1.3984 (1.5492)	CeLoss 1.3984 (0.5934)	SegCLSLoss 0.0000 (0.0109)	KLLoss 0.0000 (0.2299)	MaskLoss 0.0000 (0.4638)	MaskBCELoss 0.0000 (0.0827)	MaskDICELoss 0.0000 (0.3811)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 489 is less than current step: 499. Dropping entry: {'train/loss': 1.7708420634269715, 'train/ce_loss': 0.402734375, 'train/seg_cls_loss': 0.0140625, 'train/kl_loss': 0.30703125, 'train/mask_bce_loss': 0.17184169068932534, 'train/mask_dice_loss': 0.4933644860982895, 'train/mask_loss': 0.665206179022789, 'metrics/total_secs_per_batch': 6.415101051330566, 'metrics/data_secs_per_batch': 2.569181203842163, '_timestamp': 1740960235.8365676}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 489 is less than current step: 499. Dropping entry: {'train/lr': 0.00012391836734693877, '_timestamp': 1740960235.8368506}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 490 is less than current step: 499. Dropping entry: {'train/loss': 1.9671886444091797, 'train/ce_loss': 0.325732421875, 'train/seg_cls_loss': 0.02017822265625, 'train/kl_loss': 0.303125, 'train/mask_bce_loss': 0.22207845076918603, 'train/mask_dice_loss': 0.5785080552101135, 'train/mask_loss': 0.8005864977836609, 'metrics/total_secs_per_batch': 5.87855863571167, 'metrics/data_secs_per_batch': 2.6173943758010862, '_timestamp': 1740960241.7156057}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 490 is less than current step: 499. Dropping entry: {'train/lr': 0.00012379591836734693, '_timestamp': 1740960241.7159686}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 491 is less than current step: 499. Dropping entry: {'train/loss': 1.549229907989502, 'train/ce_loss': 0.593359375, 'train/seg_cls_loss': 0.01085205078125, 'train/kl_loss': 0.2298828125, 'train/mask_bce_loss': 0.0826887870207429, 'train/mask_dice_loss': 0.38113515079021454, 'train/mask_loss': 0.4638239353895187, 'metrics/total_secs_per_batch': 4.939960956573486, 'metrics/data_secs_per_batch': 2.558280825614929, '_timestamp': 1740960246.6552799}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 491 is less than current step: 499. Dropping entry: {'train/lr': 0.0001236734693877551, '_timestamp': 1740960246.6555665}).
Epoch: [2][493/500]	Time  6.577 ( 6.577)	Loss 1.7377 (2.0368)	CeLoss 0.2578 (0.2538)	SegCLSLoss 0.0211 (0.0170)	KLLoss 0.3887 (0.3807)	MaskLoss 0.7155 (0.8682)	MaskBCELoss 0.0822 (0.2471)	MaskDICELoss 0.6333 (0.6211)
Epoch: [2][494/500]	Time  6.077 ( 6.077)	Loss 1.0234 (1.6887)	CeLoss 1.0234 (0.3940)	SegCLSLoss 0.0000 (0.0120)	KLLoss 0.0000 (0.2264)	MaskLoss 0.0000 (0.6329)	MaskBCELoss 0.0000 (0.1255)	MaskDICELoss 0.0000 (0.5074)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 492 is less than current step: 499. Dropping entry: {'train/loss': 2.036794149875641, 'train/ce_loss': 0.25380859375, 'train/seg_cls_loss': 0.017041015625, 'train/kl_loss': 0.3806640625, 'train/mask_bce_loss': 0.24709374979138374, 'train/mask_dice_loss': 0.6211080089211464, 'train/mask_loss': 0.8682017624378204, 'metrics/total_secs_per_batch': 6.5767128467559814, 'metrics/data_secs_per_batch': 3.059891104698181, '_timestamp': 1740960253.2320178}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 492 is less than current step: 499. Dropping entry: {'train/lr': 0.00012355102040816326, '_timestamp': 1740960253.2322087}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 493 is less than current step: 499. Dropping entry: {'train/loss': 1.6887270927429199, 'train/ce_loss': 0.39404296875, 'train/seg_cls_loss': 0.01201171875, 'train/kl_loss': 0.2263671875, 'train/mask_bce_loss': 0.12545585297048092, 'train/mask_dice_loss': 0.5074330925941467, 'train/mask_loss': 0.6328889191150665, 'metrics/total_secs_per_batch': 6.076719760894775, 'metrics/data_secs_per_batch': 2.931973671913147, '_timestamp': 1740960259.308724}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 493 is less than current step: 499. Dropping entry: {'train/lr': 0.00012342857142857142, '_timestamp': 1740960259.3090155}).
Epoch: [2][495/500]	Time  5.665 ( 5.665)	Loss 2.0540 (1.8994)	CeLoss 0.1719 (0.3439)	SegCLSLoss 0.0302 (0.0206)	KLLoss 0.3770 (0.3051)	MaskLoss 0.9147 (0.7572)	MaskBCELoss 0.1262 (0.1787)	MaskDICELoss 0.7885 (0.5785)
Epoch: [2][496/500]	Time  6.402 ( 6.402)	Loss 1.0078 (1.4601)	CeLoss 1.0078 (0.6274)	SegCLSLoss 0.0000 (0.0102)	KLLoss 0.0000 (0.1910)	MaskLoss 0.0000 (0.4042)	MaskBCELoss 0.0000 (0.0803)	MaskDICELoss 0.0000 (0.3239)
Epoch: [2][497/500]	Time  5.613 ( 5.613)	Loss 1.2031 (1.3082)	CeLoss 1.2031 (0.5237)	SegCLSLoss 0.0000 (0.0100)	KLLoss 0.0000 (0.2314)	MaskLoss 0.0000 (0.3783)	MaskBCELoss 0.0000 (0.0864)	MaskDICELoss 0.0000 (0.2919)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 494 is less than current step: 499. Dropping entry: {'train/loss': 1.8994050979614259, 'train/ce_loss': 0.3439453125, 'train/seg_cls_loss': 0.02064208984375, 'train/kl_loss': 0.305078125, 'train/mask_bce_loss': 0.17868222966790198, 'train/mask_dice_loss': 0.5785398542881012, 'train/mask_loss': 0.7572220861911774, 'metrics/total_secs_per_batch': 5.665110111236572, 'metrics/data_secs_per_batch': 2.668564510345459, '_timestamp': 1740960264.9738734}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 494 is less than current step: 499. Dropping entry: {'train/lr': 0.0001233061224489796, '_timestamp': 1740960264.97424}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 495 is less than current step: 499. Dropping entry: {'train/loss': 1.4601414203643799, 'train/ce_loss': 0.62744140625, 'train/seg_cls_loss': 0.010186767578125, 'train/kl_loss': 0.191015625, 'train/mask_bce_loss': 0.08031439073383809, 'train/mask_dice_loss': 0.323877415060997, 'train/mask_loss': 0.404191792011261, 'metrics/total_secs_per_batch': 6.402175664901733, 'metrics/data_secs_per_batch': 2.872756290435791, '_timestamp': 1740960271.3760505}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 495 is less than current step: 499. Dropping entry: {'train/lr': 0.00012318367346938775, '_timestamp': 1740960271.3763423}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 496 is less than current step: 499. Dropping entry: {'train/loss': 1.3081878244876861, 'train/ce_loss': 0.52373046875, 'train/seg_cls_loss': 0.0100341796875, 'train/kl_loss': 0.2314453125, 'train/mask_bce_loss': 0.08638607114553451, 'train/mask_dice_loss': 0.29187776893377304, 'train/mask_loss': 0.3782638289034367, 'metrics/total_secs_per_batch': 5.613014221191406, 'metrics/data_secs_per_batch': 2.9656289339065554, '_timestamp': 1740960276.9891987}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 496 is less than current step: 499. Dropping entry: {'train/lr': 0.0001230612244897959, '_timestamp': 1740960276.9895408}).
Epoch: [2][498/500]	Time  6.509 ( 6.509)	Loss 1.2780 (1.7179)	CeLoss 0.2832 (0.3709)	SegCLSLoss 0.0104 (0.0137)	KLLoss 0.3926 (0.3055)	MaskLoss 0.4749 (0.6549)	MaskBCELoss 0.0983 (0.1291)	MaskDICELoss 0.3767 (0.5259)
Epoch: [2][499/500]	Time  6.742 ( 6.742)	Loss 1.8984 (1.6679)	CeLoss 1.8984 (0.4240)	SegCLSLoss 0.0000 (0.0123)	KLLoss 0.0000 (0.2670)	MaskLoss 0.0000 (0.6055)	MaskBCELoss 0.0000 (0.1940)	MaskDICELoss 0.0000 (0.4114)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 497 is less than current step: 499. Dropping entry: {'train/loss': 1.7179210543632508, 'train/ce_loss': 0.3708984375, 'train/seg_cls_loss': 0.0137451171875, 'train/kl_loss': 0.30546875, 'train/mask_bce_loss': 0.1290525791235268, 'train/mask_dice_loss': 0.5258552074432373, 'train/mask_loss': 0.6549077928066254, 'metrics/total_secs_per_batch': 6.508759021759033, 'metrics/data_secs_per_batch': 2.676158404350281, '_timestamp': 1740960283.497825}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 497 is less than current step: 499. Dropping entry: {'train/lr': 0.00012293877551020408, '_timestamp': 1740960283.4981227}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 498 is less than current step: 499. Dropping entry: {'train/loss': 1.667858624458313, 'train/ce_loss': 0.4240234375, 'train/seg_cls_loss': 0.01231689453125, 'train/kl_loss': 0.2669921875, 'train/mask_bce_loss': 0.19403452537953852, 'train/mask_dice_loss': 0.41142798960208893, 'train/mask_loss': 0.6054625153541565, 'metrics/total_secs_per_batch': 6.742318868637085, 'metrics/data_secs_per_batch': 3.1444170236587525, '_timestamp': 1740960290.240115}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 498 is less than current step: 499. Dropping entry: {'train/lr': 0.00012281632653061224, '_timestamp': 1740960290.2403877}).
  0%|                                                                                                                                                              | 0/200 [00:00<?, ?it/s]
[2025-03-02 18:04:56,869] [INFO] [logging.py:96:log_dist] [Rank 0] step=1500, skipped=0, lr=[0.00012275510204081633], mom=[(0.9, 0.95)]
[2025-03-02 18:04:56,875] [INFO] [timer.py:215:stop] epoch=0/micro_step=15000/global_step=1500, RunningAvgSamplesPerSec=1.5683015292011409, CurrSamplesPerSec=1.5073393733466154, MemAllocated=31.23GB, MaxMemAllocated=37.19GB














 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 197/200 [00:28<00:00,  6.81it/s]

100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:28<00:00,  6.97it/s]
Epoch: [3][  1/500]	Time  5.214 ( 5.214)	Loss 1.4661 (1.6792)	CeLoss 0.2266 (0.5962)	SegCLSLoss 0.0214 (0.0115)	KLLoss 0.3906 (0.2299)	MaskLoss 0.5944 (0.5272)	MaskBCELoss 0.1268 (0.1044)	MaskDICELoss 0.4676 (0.4228)
Epoch: [3][  2/500]	Time  6.882 ( 6.882)	Loss 2.7217 (2.0547)	CeLoss 0.2021 (0.3466)	SegCLSLoss 0.0242 (0.0168)	KLLoss 0.3672 (0.3404)	MaskLoss 1.2358 (0.8330)	MaskBCELoss 0.3454 (0.1073)	MaskDICELoss 0.8904 (0.7257)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 2 is less than current step: 499. Dropping entry: {'val/giou': 0.1844266951084137, 'val/ciou': 0.14699463546276093, '_timestamp': 1740960325.5751445}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 499. Dropping entry: {'train/loss': 1.679235851764679, 'train/ce_loss': 0.59619140625, 'train/seg_cls_loss': 0.011492919921875, 'train/kl_loss': 0.2298828125, 'train/mask_bce_loss': 0.1043768210336566, 'train/mask_dice_loss': 0.4228387773036957, 'train/mask_loss': 0.5272155940532685, 'metrics/total_secs_per_batch': 5.2141242027282715, 'metrics/data_secs_per_batch': 2.3940351247787475, '_timestamp': 1740960330.7960098}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 1 is less than current step: 499. Dropping entry: {'train/loss': 2.0547171354293825, 'train/ce_loss': 0.34658203125, 'train/seg_cls_loss': 0.016815185546875, 'train/kl_loss': 0.3404296875, 'train/mask_bce_loss': 0.10725360792130231, 'train/mask_dice_loss': 0.7257201969623566, 'train/mask_loss': 0.8329737961292267, 'metrics/total_secs_per_batch': 6.881736516952515, 'metrics/data_secs_per_batch': 2.955165457725525, '_timestamp': 1740960337.677795}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 1 is less than current step: 499. Dropping entry: {'train/lr': 0.00012251020408163263, '_timestamp': 1740960337.678118}).
Epoch: [3][  3/500]	Time  4.664 ( 4.664)	Loss 0.6172 (1.3390)	CeLoss 0.6172 (0.5349)	SegCLSLoss 0.0000 (0.0134)	KLLoss 0.0000 (0.2295)	MaskLoss 0.0000 (0.3871)	MaskBCELoss 0.0000 (0.0780)	MaskDICELoss 0.0000 (0.3091)
Epoch: [3][  4/500]	Time  5.716 ( 5.716)	Loss 2.0124 (1.6632)	CeLoss 0.1758 (0.4011)	SegCLSLoss 0.0225 (0.0163)	KLLoss 0.3887 (0.3023)	MaskLoss 0.8929 (0.6117)	MaskBCELoss 0.0113 (0.1200)	MaskDICELoss 0.8816 (0.4916)
Epoch: [3][  5/500]	Time  5.707 ( 5.707)	Loss 1.0469 (1.3468)	CeLoss 1.0469 (0.3065)	SegCLSLoss 0.0000 (0.0115)	KLLoss 0.0000 (0.2305)	MaskLoss 0.0000 (0.5057)	MaskBCELoss 0.0000 (0.0163)	MaskDICELoss 0.0000 (0.4894)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 2 is less than current step: 499. Dropping entry: {'train/loss': 1.3389932096004487, 'train/ce_loss': 0.53486328125, 'train/seg_cls_loss': 0.01336669921875, 'train/kl_loss': 0.2294921875, 'train/mask_bce_loss': 0.07798712439835072, 'train/mask_dice_loss': 0.30908760651946066, 'train/mask_loss': 0.3870747238397598, 'metrics/total_secs_per_batch': 4.6638994216918945, 'metrics/data_secs_per_batch': 2.0712807416915893, '_timestamp': 1740960342.3415709}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 2 is less than current step: 499. Dropping entry: {'train/lr': 0.0001223877551020408, '_timestamp': 1740960342.3418322}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 3 is less than current step: 499. Dropping entry: {'train/loss': 1.6631739914417267, 'train/ce_loss': 0.40107421875, 'train/seg_cls_loss': 0.0162841796875, 'train/kl_loss': 0.30234375, 'train/mask_bce_loss': 0.12003012271597982, 'train/mask_dice_loss': 0.4916350066661835, 'train/mask_loss': 0.6116651266813278, 'metrics/total_secs_per_batch': 5.716318368911743, 'metrics/data_secs_per_batch': 2.573682427406311, '_timestamp': 1740960348.0578704}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 3 is less than current step: 499. Dropping entry: {'train/lr': 0.00012226530612244896, '_timestamp': 1740960348.0581434}).
Epoch: [3][  6/500]	Time  6.758 ( 6.758)	Loss 1.9544 (1.9270)	CeLoss 0.2041 (0.3334)	SegCLSLoss 0.0256 (0.0162)	KLLoss 0.3633 (0.3020)	MaskLoss 0.8503 (0.7776)	MaskBCELoss 0.0272 (0.1928)	MaskDICELoss 0.8231 (0.5847)
Epoch: [3][  7/500]	Time  5.678 ( 5.678)	Loss 1.8372 (1.7994)	CeLoss 0.2070 (0.4677)	SegCLSLoss 0.0253 (0.0130)	KLLoss 0.3555 (0.2613)	MaskLoss 0.7907 (0.6495)	MaskBCELoss 0.0813 (0.1093)	MaskDICELoss 0.7094 (0.5402)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 4 is less than current step: 499. Dropping entry: {'train/loss': 1.3467501163482667, 'train/ce_loss': 0.30654296875, 'train/seg_cls_loss': 0.011456298828125, 'train/kl_loss': 0.23046875, 'train/mask_bce_loss': 0.016341714281588794, 'train/mask_dice_loss': 0.4894063949584961, 'train/mask_loss': 0.5057481050491333, 'metrics/total_secs_per_batch': 5.70701003074646, 'metrics/data_secs_per_batch': 2.6584378719329833, '_timestamp': 1740960353.7649837}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 4 is less than current step: 499. Dropping entry: {'train/lr': 0.00012214285714285712, '_timestamp': 1740960353.7652683}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 5 is less than current step: 499. Dropping entry: {'train/loss': 1.9269988417625428, 'train/ce_loss': 0.3333984375, 'train/seg_cls_loss': 0.01622314453125, 'train/kl_loss': 0.301953125, 'train/mask_bce_loss': 0.19281976111233234, 'train/mask_dice_loss': 0.5847421526908875, 'train/mask_loss': 0.7775619208812714, 'metrics/total_secs_per_batch': 6.758448839187622, 'metrics/data_secs_per_batch': 3.105417346954346, '_timestamp': 1740960360.523384}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 5 is less than current step: 499. Dropping entry: {'train/lr': 0.0001220204081632653, '_timestamp': 1740960360.5236697}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 6 is less than current step: 499. Dropping entry: {'train/loss': 1.7993988752365113, 'train/ce_loss': 0.46767578125, 'train/seg_cls_loss': 0.013006591796875, 'train/kl_loss': 0.261328125, 'train/mask_bce_loss': 0.10929394718259573, 'train/mask_dice_loss': 0.5401613295078278, 'train/mask_loss': 0.6494552791118622, 'metrics/total_secs_per_batch': 5.677994012832642, 'metrics/data_secs_per_batch': 2.6054857969284058, '_timestamp': 1740960366.201357}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 6 is less than current step: 499. Dropping entry: {'train/lr': 0.00012189795918367345, '_timestamp': 1740960366.201696}).
Epoch: [3][  8/500]	Time  6.344 ( 6.344)	Loss 2.0666 (1.6190)	CeLoss 0.2871 (0.2794)	SegCLSLoss 0.0258 (0.0143)	KLLoss 0.3770 (0.2662)	MaskLoss 0.8643 (0.6530)	MaskBCELoss 0.3220 (0.1664)	MaskDICELoss 0.5423 (0.4865)
Epoch: [3][  9/500]	Time  5.255 ( 5.255)	Loss 2.1999 (1.6835)	CeLoss 0.2217 (0.5122)	SegCLSLoss 0.0126 (0.0154)	KLLoss 0.3867 (0.2682)	MaskLoss 0.9671 (0.5684)	MaskBCELoss 0.2764 (0.1282)	MaskDICELoss 0.6907 (0.4402)
[2025-03-02 18:06:24,225] [INFO] [logging.py:96:log_dist] [Rank 0] step=1510, skipped=0, lr=[0.00012159183673469386], mom=[(0.9, 0.95)]
[2025-03-02 18:06:24,231] [INFO] [timer.py:215:stop] epoch=0/micro_step=15100/global_step=1510, RunningAvgSamplesPerSec=1.5691378207433526, CurrSamplesPerSec=1.5552658173419667, MemAllocated=30.69GB, MaxMemAllocated=37.19GB
Epoch: [3][ 10/500]	Time  6.432 ( 6.432)	Loss 1.4766 (1.9094)	CeLoss 1.4766 (0.3291)	SegCLSLoss 0.0000 (0.0158)	KLLoss 0.0000 (0.3066)	MaskLoss 0.0000 (0.7707)	MaskBCELoss 0.0000 (0.1372)	MaskDICELoss 0.0000 (0.6336)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 7 is less than current step: 499. Dropping entry: {'train/loss': 1.6190410494804381, 'train/ce_loss': 0.279443359375, 'train/seg_cls_loss': 0.014349365234375, 'train/kl_loss': 0.2662109375, 'train/mask_bce_loss': 0.16644757241010666, 'train/mask_dice_loss': 0.48650556802749634, 'train/mask_loss': 0.6529531419277191, 'metrics/total_secs_per_batch': 6.344401836395264, 'metrics/data_secs_per_batch': 2.8419308185577394, '_timestamp': 1740960372.5459776}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 7 is less than current step: 499. Dropping entry: {'train/lr': 0.00012177551020408161, '_timestamp': 1740960372.5463243}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 8 is less than current step: 499. Dropping entry: {'train/loss': 1.6834591150283813, 'train/ce_loss': 0.512158203125, 'train/seg_cls_loss': 0.015399169921875, 'train/kl_loss': 0.2681640625, 'train/mask_bce_loss': 0.12822662573307753, 'train/mask_dice_loss': 0.44021191596984866, 'train/mask_loss': 0.5684385448694229, 'metrics/total_secs_per_batch': 5.255000829696655, 'metrics/data_secs_per_batch': 2.645461392402649, '_timestamp': 1740960377.800893}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 8 is less than current step: 499. Dropping entry: {'train/lr': 0.00012165306122448977, '_timestamp': 1740960377.8011947}).
Epoch: [3][ 11/500]	Time  5.235 ( 5.235)	Loss 2.8237 (1.5855)	CeLoss 0.1904 (0.5984)	SegCLSLoss 0.0201 (0.0113)	KLLoss 0.3828 (0.2291)	MaskLoss 1.2927 (0.4793)	MaskBCELoss 0.4263 (0.0942)	MaskDICELoss 0.8664 (0.3851)
Epoch: [3][ 12/500]	Time  6.082 ( 6.082)	Loss 1.9174 (1.9759)	CeLoss 0.2139 (0.3215)	SegCLSLoss 0.0210 (0.0169)	KLLoss 0.3887 (0.3414)	MaskLoss 0.8269 (0.8059)	MaskBCELoss 0.1019 (0.1831)	MaskDICELoss 0.7249 (0.6228)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 9 is less than current step: 499. Dropping entry: {'train/loss': 1.9094122529029847, 'train/ce_loss': 0.329052734375, 'train/seg_cls_loss': 0.01580810546875, 'train/kl_loss': 0.306640625, 'train/mask_bce_loss': 0.1371533763827756, 'train/mask_dice_loss': 0.6335927903652191, 'train/mask_loss': 0.7707461774349212, 'metrics/total_secs_per_batch': 6.431503057479858, 'metrics/data_secs_per_batch': 2.9470890045166014, '_timestamp': 1740960384.2320912}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 9 is less than current step: 499. Dropping entry: {'train/lr': 0.00012153061224489794, '_timestamp': 1740960384.2323625}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 10 is less than current step: 499. Dropping entry: {'train/loss': 1.5855268716812134, 'train/ce_loss': 0.5984375, 'train/seg_cls_loss': 0.011322021484375, 'train/kl_loss': 0.2291015625, 'train/mask_bce_loss': 0.0941932275891304, 'train/mask_dice_loss': 0.3850936383008957, 'train/mask_loss': 0.4792868733406067, 'metrics/total_secs_per_batch': 5.23471212387085, 'metrics/data_secs_per_batch': 2.398301935195923, '_timestamp': 1740960389.4670024}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 10 is less than current step: 499. Dropping entry: {'train/lr': 0.00012140816326530611, '_timestamp': 1740960389.4672966}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 11 is less than current step: 499. Dropping entry: {'train/loss': 1.9758761405944825, 'train/ce_loss': 0.321484375, 'train/seg_cls_loss': 0.0168701171875, 'train/kl_loss': 0.34140625, 'train/mask_bce_loss': 0.18314409255981445, 'train/mask_dice_loss': 0.6227627113461495, 'train/mask_loss': 0.8059068113565445, 'metrics/total_secs_per_batch': 6.082409143447876, 'metrics/data_secs_per_batch': 2.8891801834106445, '_timestamp': 1740960395.5494068}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 11 is less than current step: 499. Dropping entry: {'train/lr': 0.00012128571428571428, '_timestamp': 1740960395.5497499}).
Epoch: [3][ 13/500]	Time  5.570 ( 5.570)	Loss 1.9395 (1.5632)	CeLoss 0.2539 (0.5357)	SegCLSLoss 0.0167 (0.0115)	KLLoss 0.3867 (0.2314)	MaskLoss 0.8193 (0.4993)	MaskBCELoss 0.0717 (0.0725)	MaskDICELoss 0.7476 (0.4268)
Epoch: [3][ 14/500]	Time  6.794 ( 6.794)	Loss 2.7886 (1.8254)	CeLoss 0.1865 (0.2237)	SegCLSLoss 0.0254 (0.0177)	KLLoss 0.3730 (0.3854)	MaskLoss 1.2762 (0.7773)	MaskBCELoss 0.5191 (0.1908)	MaskDICELoss 0.7571 (0.5864)
Epoch: [3][ 15/500]	Time  5.598 ( 5.598)	Loss 0.8945 (1.7504)	CeLoss 0.8945 (0.5119)	SegCLSLoss 0.0000 (0.0120)	KLLoss 0.0000 (0.2252)	MaskLoss 0.0000 (0.6049)	MaskBCELoss 0.0000 (0.1263)	MaskDICELoss 0.0000 (0.4787)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 12 is less than current step: 499. Dropping entry: {'train/loss': 1.563158631324768, 'train/ce_loss': 0.5357421875, 'train/seg_cls_loss': 0.011517333984375, 'train/kl_loss': 0.2314453125, 'train/mask_bce_loss': 0.07245969250798226, 'train/mask_dice_loss': 0.4268442362546921, 'train/mask_loss': 0.499303925037384, 'metrics/total_secs_per_batch': 5.569559812545776, 'metrics/data_secs_per_batch': 1.9730859518051147, '_timestamp': 1740960401.1190069}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 12 is less than current step: 499. Dropping entry: {'train/lr': 0.00012116326530612244, '_timestamp': 1740960401.1192894}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 13 is less than current step: 499. Dropping entry: {'train/loss': 1.8254405379295349, 'train/ce_loss': 0.22373046875, 'train/seg_cls_loss': 0.01767578125, 'train/kl_loss': 0.3853515625, 'train/mask_bce_loss': 0.190844972175546, 'train/mask_dice_loss': 0.5864260852336883, 'train/mask_loss': 0.7772710531949997, 'metrics/total_secs_per_batch': 6.794412851333618, 'metrics/data_secs_per_batch': 2.9429177045822144, '_timestamp': 1740960407.913381}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 13 is less than current step: 499. Dropping entry: {'train/lr': 0.0001210408163265306, '_timestamp': 1740960407.9136674}).
Epoch: [3][ 16/500]	Time  6.367 ( 6.367)	Loss 2.2405 (1.6055)	CeLoss 0.1934 (0.3157)	SegCLSLoss 0.0254 (0.0174)	KLLoss 0.3926 (0.3406)	MaskLoss 0.9977 (0.6236)	MaskBCELoss 0.0597 (0.1123)	MaskDICELoss 0.9380 (0.5113)
Epoch: [3][ 17/500]	Time  5.612 ( 5.612)	Loss 1.4375 (1.6840)	CeLoss 1.4375 (0.5418)	SegCLSLoss 0.0000 (0.0136)	KLLoss 0.0000 (0.2664)	MaskLoss 0.0000 (0.5543)	MaskBCELoss 0.0000 (0.1004)	MaskDICELoss 0.0000 (0.4539)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 14 is less than current step: 499. Dropping entry: {'train/loss': 1.7503672957420349, 'train/ce_loss': 0.5119140625, 'train/seg_cls_loss': 0.011956787109375, 'train/kl_loss': 0.2251953125, 'train/mask_bce_loss': 0.12626573406159877, 'train/mask_dice_loss': 0.47865424752235414, 'train/mask_loss': 0.604919970035553, 'metrics/total_secs_per_batch': 5.5979905128479, 'metrics/data_secs_per_batch': 2.7580891132354735, '_timestamp': 1740960413.5115392}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 14 is less than current step: 499. Dropping entry: {'train/lr': 0.00012091836734693877, '_timestamp': 1740960413.5118122}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 15 is less than current step: 499. Dropping entry: {'train/loss': 1.6054991960525513, 'train/ce_loss': 0.315673828125, 'train/seg_cls_loss': 0.01737060546875, 'train/kl_loss': 0.340625, 'train/mask_bce_loss': 0.11227751057595015, 'train/mask_dice_loss': 0.5113216832280159, 'train/mask_loss': 0.6235992014408112, 'metrics/total_secs_per_batch': 6.367032527923584, 'metrics/data_secs_per_batch': 2.7234383106231688, '_timestamp': 1740960419.878663}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 15 is less than current step: 499. Dropping entry: {'train/lr': 0.00012079591836734693, '_timestamp': 1740960419.8790298}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 16 is less than current step: 499. Dropping entry: {'train/loss': 1.684011209011078, 'train/ce_loss': 0.541796875, 'train/seg_cls_loss': 0.01363525390625, 'train/kl_loss': 0.26640625, 'train/mask_bce_loss': 0.10039840787649154, 'train/mask_dice_loss': 0.4539118967950344, 'train/mask_loss': 0.5543102979660034, 'metrics/total_secs_per_batch': 5.6117260456085205, 'metrics/data_secs_per_batch': 2.4592437028884886, '_timestamp': 1740960425.4901454}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 16 is less than current step: 499. Dropping entry: {'train/lr': 0.00012067346938775509, '_timestamp': 1740960425.4904172}).
Epoch: [3][ 18/500]	Time  5.809 ( 5.809)	Loss 1.4922 (1.9821)	CeLoss 1.4922 (0.5158)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.3010)	MaskLoss 0.0000 (0.7145)	MaskBCELoss 0.0000 (0.1814)	MaskDICELoss 0.0000 (0.5331)
Epoch: [3][ 19/500]	Time  6.311 ( 6.311)	Loss 0.0625 (1.5505)	CeLoss 0.0625 (0.2222)	SegCLSLoss 0.0000 (0.0143)	KLLoss 0.0000 (0.3016)	MaskLoss 0.0000 (0.6454)	MaskBCELoss 0.0000 (0.1349)	MaskDICELoss 0.0000 (0.5105)
[2025-03-02 18:07:24,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=1520, skipped=0, lr=[0.0001203673469387755], mom=[(0.9, 0.95)]
[2025-03-02 18:07:24,294] [INFO] [timer.py:215:stop] epoch=0/micro_step=15200/global_step=1520, RunningAvgSamplesPerSec=1.5697347737474645, CurrSamplesPerSec=1.496131030176709, MemAllocated=31.1GB, MaxMemAllocated=37.19GB
Epoch: [3][ 20/500]	Time  6.686 ( 6.686)	Loss 0.6866 (1.9858)	CeLoss 0.2637 (0.2227)	SegCLSLoss 0.0132 (0.0220)	KLLoss 0.3906 (0.3770)	MaskLoss 0.1890 (0.8572)	MaskBCELoss 0.0768 (0.1453)	MaskDICELoss 0.1122 (0.7119)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 17 is less than current step: 499. Dropping entry: {'train/loss': 1.9821279644966125, 'train/ce_loss': 0.5158203125, 'train/seg_cls_loss': 0.01424560546875, 'train/kl_loss': 0.3009765625, 'train/mask_bce_loss': 0.1814320044592023, 'train/mask_dice_loss': 0.5330694854259491, 'train/mask_loss': 0.7145014822483062, 'metrics/total_secs_per_batch': 5.808640003204346, 'metrics/data_secs_per_batch': 2.799111843109131, '_timestamp': 1740960431.2987814}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 17 is less than current step: 499. Dropping entry: {'train/lr': 0.00012055102040816326, '_timestamp': 1740960431.299046}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 18 is less than current step: 499. Dropping entry: {'train/loss': 1.5504891514778136, 'train/ce_loss': 0.22216796875, 'train/seg_cls_loss': 0.014337158203125, 'train/kl_loss': 0.3015625, 'train/mask_bce_loss': 0.13486277949996292, 'train/mask_dice_loss': 0.5105478048324585, 'train/mask_loss': 0.6454105824232101, 'metrics/total_secs_per_batch': 6.310600519180298, 'metrics/data_secs_per_batch': 2.551415967941284, '_timestamp': 1740960437.609408}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 18 is less than current step: 499. Dropping entry: {'train/lr': 0.00012042857142857142, '_timestamp': 1740960437.6096842}).
Epoch: [3][ 21/500]	Time  5.513 ( 5.513)	Loss 0.8279 (1.6974)	CeLoss 0.1982 (0.3630)	SegCLSLoss 0.0232 (0.0155)	KLLoss 0.3750 (0.3000)	MaskLoss 0.2899 (0.6482)	MaskBCELoss 0.1328 (0.1547)	MaskDICELoss 0.1571 (0.4935)
Epoch: [3][ 22/500]	Time  6.355 ( 6.355)	Loss 1.1484 (1.7949)	CeLoss 0.2656 (0.3699)	SegCLSLoss 0.0121 (0.0166)	KLLoss 0.3848 (0.3389)	MaskLoss 0.4189 (0.6915)	MaskBCELoss 0.0657 (0.1456)	MaskDICELoss 0.3532 (0.5459)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 19 is less than current step: 499. Dropping entry: {'train/loss': 1.9857994735240936, 'train/ce_loss': 0.22265625, 'train/seg_cls_loss': 0.022015380859375, 'train/kl_loss': 0.376953125, 'train/mask_bce_loss': 0.14525998942553997, 'train/mask_dice_loss': 0.7119464173913002, 'train/mask_loss': 0.8572064027190208, 'metrics/total_secs_per_batch': 6.685518503189087, 'metrics/data_secs_per_batch': 3.150969219207764, '_timestamp': 1740960444.294736}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 19 is less than current step: 499. Dropping entry: {'train/lr': 0.00012030612244897957, '_timestamp': 1740960444.29501}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 20 is less than current step: 499. Dropping entry: {'train/loss': 1.6973719358444215, 'train/ce_loss': 0.36298828125, 'train/seg_cls_loss': 0.015509033203125, 'train/kl_loss': 0.3, 'train/mask_bce_loss': 0.15465337745845317, 'train/mask_dice_loss': 0.49354429841041564, 'train/mask_loss': 0.6481976777315139, 'metrics/total_secs_per_batch': 5.512629508972168, 'metrics/data_secs_per_batch': 2.515805196762085, '_timestamp': 1740960449.8075662}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 20 is less than current step: 499. Dropping entry: {'train/lr': 0.00012018367346938773, '_timestamp': 1740960449.8078296}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 21 is less than current step: 499. Dropping entry: {'train/loss': 1.7948899865150452, 'train/ce_loss': 0.369921875, 'train/seg_cls_loss': 0.016619873046875, 'train/kl_loss': 0.3388671875, 'train/mask_bce_loss': 0.14560003951191902, 'train/mask_dice_loss': 0.5458879068493843, 'train/mask_loss': 0.691487942636013, 'metrics/total_secs_per_batch': 6.354527473449707, 'metrics/data_secs_per_batch': 2.6893554449081423, '_timestamp': 1740960456.1621256}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 21 is less than current step: 499. Dropping entry: {'train/lr': 0.00012006122448979592, '_timestamp': 1740960456.1624205}).
Epoch: [3][ 23/500]	Time  6.550 ( 6.550)	Loss 2.1191 (1.9260)	CeLoss 0.2285 (0.3488)	SegCLSLoss 0.0165 (0.0218)	KLLoss 0.3730 (0.3379)	MaskLoss 0.9228 (0.7663)	MaskBCELoss 0.0417 (0.1384)	MaskDICELoss 0.8811 (0.6279)
Epoch: [3][ 24/500]	Time  6.380 ( 6.380)	Loss 1.2760 (1.9433)	CeLoss 0.1865 (0.3876)	SegCLSLoss 0.0195 (0.0173)	KLLoss 0.3828 (0.2984)	MaskLoss 0.5208 (0.7587)	MaskBCELoss 0.0240 (0.1323)	MaskDICELoss 0.4968 (0.6264)
Epoch: [3][ 25/500]	Time  5.421 ( 5.421)	Loss 0.8281 (1.6900)	CeLoss 0.8281 (0.5707)	SegCLSLoss 0.0000 (0.0119)	KLLoss 0.0000 (0.2250)	MaskLoss 0.0000 (0.5455)	MaskBCELoss 0.0000 (0.1678)	MaskDICELoss 0.0000 (0.3777)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 22 is less than current step: 499. Dropping entry: {'train/loss': 1.926041805744171, 'train/ce_loss': 0.348828125, 'train/seg_cls_loss': 0.02178955078125, 'train/kl_loss': 0.337890625, 'train/mask_bce_loss': 0.13837253153324128, 'train/mask_dice_loss': 0.6279198497533798, 'train/mask_loss': 0.76629239320755, 'metrics/total_secs_per_batch': 6.549712419509888, 'metrics/data_secs_per_batch': 3.0819480657577514, '_timestamp': 1740960462.7118883}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 22 is less than current step: 499. Dropping entry: {'train/lr': 0.00011993877551020407, '_timestamp': 1740960462.712302}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 23 is less than current step: 499. Dropping entry: {'train/loss': 1.9433498978614807, 'train/ce_loss': 0.38759765625, 'train/seg_cls_loss': 0.017291259765625, 'train/kl_loss': 0.2984375, 'train/mask_bce_loss': 0.13234143052250147, 'train/mask_dice_loss': 0.6263940572738648, 'train/mask_loss': 0.7587354898452758, 'metrics/total_secs_per_batch': 6.380402326583862, 'metrics/data_secs_per_batch': 2.8114524364471434, '_timestamp': 1740960469.092226}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 23 is less than current step: 499. Dropping entry: {'train/lr': 0.00011981632653061223, '_timestamp': 1740960469.0925148}).
Epoch: [3][ 26/500]	Time  6.431 ( 6.431)	Loss 1.4284 (1.1250)	CeLoss 0.2275 (0.3560)	SegCLSLoss 0.0193 (0.0105)	KLLoss 0.3711 (0.2266)	MaskLoss 0.5775 (0.3707)	MaskBCELoss 0.1288 (0.1291)	MaskDICELoss 0.4487 (0.2417)
Epoch: [3][ 27/500]	Time  5.075 ( 5.075)	Loss 2.4802 (1.9441)	CeLoss 0.2930 (0.5855)	SegCLSLoss 0.0302 (0.0172)	KLLoss 0.3711 (0.2592)	MaskLoss 1.0672 (0.6620)	MaskBCELoss 0.3381 (0.1266)	MaskDICELoss 0.7291 (0.5353)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 24 is less than current step: 499. Dropping entry: {'train/loss': 1.6899630546569824, 'train/ce_loss': 0.570703125, 'train/seg_cls_loss': 0.01185302734375, 'train/kl_loss': 0.225, 'train/mask_bce_loss': 0.1678155843168497, 'train/mask_dice_loss': 0.3776542149484158, 'train/mask_loss': 0.5454698085784913, 'metrics/total_secs_per_batch': 5.420569658279419, 'metrics/data_secs_per_batch': 2.618192458152771, '_timestamp': 1740960474.5128314}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 24 is less than current step: 499. Dropping entry: {'train/lr': 0.0001196938775510204, '_timestamp': 1740960474.5130427}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 25 is less than current step: 499. Dropping entry: {'train/loss': 1.1250394582748413, 'train/ce_loss': 0.35595703125, 'train/seg_cls_loss': 0.010528564453125, 'train/kl_loss': 0.2265625, 'train/mask_bce_loss': 0.12906515449285508, 'train/mask_dice_loss': 0.24165768921375275, 'train/mask_loss': 0.3707228422164917, 'metrics/total_secs_per_batch': 6.430997371673584, 'metrics/data_secs_per_batch': 2.598813462257385, '_timestamp': 1740960480.9438267}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 25 is less than current step: 499. Dropping entry: {'train/lr': 0.00011957142857142856, '_timestamp': 1740960480.9441278}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 26 is less than current step: 499. Dropping entry: {'train/loss': 1.9441326022148133, 'train/ce_loss': 0.585546875, 'train/seg_cls_loss': 0.017236328125, 'train/kl_loss': 0.2591796875, 'train/mask_bce_loss': 0.12663809694349765, 'train/mask_dice_loss': 0.5353207767009736, 'train/mask_loss': 0.6619588732719421, 'metrics/total_secs_per_batch': 5.0754172801971436, 'metrics/data_secs_per_batch': 2.302199196815491, '_timestamp': 1740960486.019363}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 26 is less than current step: 499. Dropping entry: {'train/lr': 0.00011944897959183672, '_timestamp': 1740960486.0198107}).
Epoch: [3][ 28/500]	Time  5.676 ( 5.676)	Loss 0.9492 (1.8016)	CeLoss 0.9492 (0.5017)	SegCLSLoss 0.0000 (0.0150)	KLLoss 0.0000 (0.2615)	MaskLoss 0.0000 (0.6333)	MaskBCELoss 0.0000 (0.1517)	MaskDICELoss 0.0000 (0.4816)
Epoch: [3][ 29/500]	Time  4.840 ( 4.840)	Loss 0.9706 (1.2682)	CeLoss 0.2012 (0.4112)	SegCLSLoss 0.0139 (0.0119)	KLLoss 0.3750 (0.2635)	MaskLoss 0.3622 (0.4122)	MaskBCELoss 0.1548 (0.1076)	MaskDICELoss 0.2075 (0.3046)
[2025-03-02 18:08:22,349] [INFO] [logging.py:96:log_dist] [Rank 0] step=1530, skipped=0, lr=[0.00011914285714285713], mom=[(0.9, 0.95)]
[2025-03-02 18:08:22,355] [INFO] [timer.py:215:stop] epoch=0/micro_step=15300/global_step=1530, RunningAvgSamplesPerSec=1.5706475219785667, CurrSamplesPerSec=1.718374350504442, MemAllocated=31.25GB, MaxMemAllocated=37.19GB
Epoch: [3][ 30/500]	Time  5.821 ( 5.821)	Loss 1.9460 (2.0863)	CeLoss 0.2256 (0.3991)	SegCLSLoss 0.0225 (0.0162)	KLLoss 0.3711 (0.3037)	MaskLoss 0.8363 (0.8245)	MaskBCELoss 0.0240 (0.2248)	MaskDICELoss 0.8123 (0.5997)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 27 is less than current step: 499. Dropping entry: {'train/loss': 1.8015807747840882, 'train/ce_loss': 0.50166015625, 'train/seg_cls_loss': 0.0150146484375, 'train/kl_loss': 0.2615234375, 'train/mask_bce_loss': 0.15173254036344588, 'train/mask_dice_loss': 0.48157737255096433, 'train/mask_loss': 0.6333099186420441, 'metrics/total_secs_per_batch': 5.675881624221802, 'metrics/data_secs_per_batch': 2.6913248777389525, '_timestamp': 1740960491.6951022}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 27 is less than current step: 499. Dropping entry: {'train/lr': 0.00011932653061224489, '_timestamp': 1740960491.6953943}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 28 is less than current step: 499. Dropping entry: {'train/loss': 1.2681587636470795, 'train/ce_loss': 0.411181640625, 'train/seg_cls_loss': 0.01190185546875, 'train/kl_loss': 0.2634765625, 'train/mask_bce_loss': 0.10764815099537373, 'train/mask_dice_loss': 0.3045806437730789, 'train/mask_loss': 0.4122287958860397, 'metrics/total_secs_per_batch': 4.839694023132324, 'metrics/data_secs_per_batch': 2.00356547832489, '_timestamp': 1740960496.5348384}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 28 is less than current step: 499. Dropping entry: {'train/lr': 0.00011920408163265305, '_timestamp': 1740960496.535196}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 29 is less than current step: 499. Dropping entry: {'train/loss': 2.0863240003585815, 'train/ce_loss': 0.39912109375, 'train/seg_cls_loss': 0.01624755859375, 'train/kl_loss': 0.3037109375, 'train/mask_bce_loss': 0.22479115445166825, 'train/mask_dice_loss': 0.5996696561574936, 'train/mask_loss': 0.8244608223438263, 'metrics/total_secs_per_batch': 5.821158409118652, 'metrics/data_secs_per_batch': 2.651377010345459, '_timestamp': 1740960502.3558266}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 29 is less than current step: 499. Dropping entry: {'train/lr': 0.00011908163265306121, '_timestamp': 1740960502.3561735}).
Epoch: [3][ 31/500]	Time  5.642 ( 5.642)	Loss 1.8144 (1.8454)	CeLoss 0.3242 (0.4311)	SegCLSLoss 0.0137 (0.0134)	KLLoss 0.3750 (0.3061)	MaskLoss 0.7226 (0.6885)	MaskBCELoss 0.2087 (0.1249)	MaskDICELoss 0.5139 (0.5636)
Epoch: [3][ 32/500]	Time  6.234 ( 6.234)	Loss 2.2484 (1.5901)	CeLoss 0.3887 (0.3067)	SegCLSLoss 0.0146 (0.0134)	KLLoss 0.3750 (0.3012)	MaskLoss 0.9074 (0.6233)	MaskBCELoss 0.0524 (0.0998)	MaskDICELoss 0.8550 (0.5234)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 30 is less than current step: 499. Dropping entry: {'train/loss': 1.8453983187675476, 'train/ce_loss': 0.431103515625, 'train/seg_cls_loss': 0.013385009765625, 'train/kl_loss': 0.3060546875, 'train/mask_bce_loss': 0.12494641840457917, 'train/mask_dice_loss': 0.5635730594396591, 'train/mask_loss': 0.6885194778442383, 'metrics/total_secs_per_batch': 5.642253875732422, 'metrics/data_secs_per_batch': 2.379250407218933, '_timestamp': 1740960507.9982893}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 30 is less than current step: 499. Dropping entry: {'train/lr': 0.00011895918367346938, '_timestamp': 1740960507.9985893}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 31 is less than current step: 499. Dropping entry: {'train/loss': 1.5900646805763246, 'train/ce_loss': 0.30673828125, 'train/seg_cls_loss': 0.01341552734375, 'train/kl_loss': 0.301171875, 'train/mask_bce_loss': 0.09984286595135927, 'train/mask_dice_loss': 0.5234121263027192, 'train/mask_loss': 0.6232549965381622, 'metrics/total_secs_per_batch': 6.234253406524658, 'metrics/data_secs_per_batch': 2.888942265510559, '_timestamp': 1740960514.232463}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 31 is less than current step: 499. Dropping entry: {'train/lr': 0.00011883673469387754, '_timestamp': 1740960514.2327473}).
Epoch: [3][ 33/500]	Time  6.415 ( 6.415)	Loss 1.1797 (1.8356)	CeLoss 1.1797 (0.3429)	SegCLSLoss 0.0000 (0.0161)	KLLoss 0.0000 (0.3404)	MaskLoss 0.0000 (0.7253)	MaskBCELoss 0.0000 (0.1590)	MaskDICELoss 0.0000 (0.5663)
Epoch: [3][ 34/500]	Time  6.238 ( 6.238)	Loss 2.2315 (1.8824)	CeLoss 0.2578 (0.3378)	SegCLSLoss 0.0178 (0.0173)	KLLoss 0.3984 (0.3451)	MaskLoss 0.9624 (0.7508)	MaskBCELoss 0.0715 (0.1194)	MaskDICELoss 0.8909 (0.6314)
Epoch: [3][ 35/500]	Time  7.232 ( 7.232)	Loss 1.7439 (1.9452)	CeLoss 0.2461 (0.3083)	SegCLSLoss 0.0165 (0.0203)	KLLoss 0.3945 (0.3463)	MaskLoss 0.7255 (0.7960)	MaskBCELoss 0.1111 (0.1607)	MaskDICELoss 0.6144 (0.6353)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 32 is less than current step: 499. Dropping entry: {'train/loss': 1.8355995178222657, 'train/ce_loss': 0.34287109375, 'train/seg_cls_loss': 0.0160888671875, 'train/kl_loss': 0.3404296875, 'train/mask_bce_loss': 0.15903489487245678, 'train/mask_dice_loss': 0.5662844061851502, 'train/mask_loss': 0.7253193080425262, 'metrics/total_secs_per_batch': 6.41476845741272, 'metrics/data_secs_per_batch': 2.944808769226074, '_timestamp': 1740960520.6472452}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 32 is less than current step: 499. Dropping entry: {'train/lr': 0.00011871428571428572, '_timestamp': 1740960520.647435}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 33 is less than current step: 499. Dropping entry: {'train/loss': 1.8823797464370728, 'train/ce_loss': 0.33779296875, 'train/seg_cls_loss': 0.017254638671875, 'train/kl_loss': 0.3451171875, 'train/mask_bce_loss': 0.11939851455390453, 'train/mask_dice_loss': 0.6314105004072189, 'train/mask_loss': 0.7508090198040008, 'metrics/total_secs_per_batch': 6.23828649520874, 'metrics/data_secs_per_batch': 2.5221514225006105, '_timestamp': 1740960526.885753}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 33 is less than current step: 499. Dropping entry: {'train/lr': 0.00011859183673469388, '_timestamp': 1740960526.8861208}).
Epoch: [3][ 36/500]	Time  6.778 ( 6.778)	Loss 2.0696 (1.9088)	CeLoss 0.1475 (0.3642)	SegCLSLoss 0.0420 (0.0216)	KLLoss 0.3730 (0.3398)	MaskLoss 0.9323 (0.7501)	MaskBCELoss 0.0654 (0.1216)	MaskDICELoss 0.8668 (0.6285)
Epoch: [3][ 37/500]	Time  4.617 ( 4.617)	Loss 2.1194 (1.6293)	CeLoss 0.1973 (0.6989)	SegCLSLoss 0.0256 (0.0115)	KLLoss 0.3574 (0.1873)	MaskLoss 0.9366 (0.4529)	MaskBCELoss 0.0317 (0.1189)	MaskDICELoss 0.9050 (0.3340)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 34 is less than current step: 499. Dropping entry: {'train/loss': 1.945157492160797, 'train/ce_loss': 0.30830078125, 'train/seg_cls_loss': 0.0203369140625, 'train/kl_loss': 0.3462890625, 'train/mask_bce_loss': 0.16068511120975018, 'train/mask_dice_loss': 0.6352823108434678, 'train/mask_loss': 0.7959674239158631, 'metrics/total_secs_per_batch': 7.232406854629517, 'metrics/data_secs_per_batch': 3.001347041130066, '_timestamp': 1740960534.1179578}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 34 is less than current step: 499. Dropping entry: {'train/lr': 0.00011846938775510204, '_timestamp': 1740960534.118241}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 35 is less than current step: 499. Dropping entry: {'train/loss': 1.9088046669960022, 'train/ce_loss': 0.36416015625, 'train/seg_cls_loss': 0.02159423828125, 'train/kl_loss': 0.33984375, 'train/mask_bce_loss': 0.12159006148576737, 'train/mask_dice_loss': 0.6284665644168854, 'train/mask_loss': 0.7500566244125366, 'metrics/total_secs_per_batch': 6.778462648391724, 'metrics/data_secs_per_batch': 2.782955503463745, '_timestamp': 1740960540.8963957}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 35 is less than current step: 499. Dropping entry: {'train/lr': 0.00011834693877551019, '_timestamp': 1740960540.896672}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 36 is less than current step: 499. Dropping entry: {'train/loss': 1.6293287873268127, 'train/ce_loss': 0.69892578125, 'train/seg_cls_loss': 0.0114501953125, 'train/kl_loss': 0.1873046875, 'train/mask_bce_loss': 0.11890083700418472, 'train/mask_dice_loss': 0.33399596214294436, 'train/mask_loss': 0.45289679765701296, 'metrics/total_secs_per_batch': 4.616917371749878, 'metrics/data_secs_per_batch': 1.9017634868621827, '_timestamp': 1740960545.513289}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 36 is less than current step: 499. Dropping entry: {'train/lr': 0.00011822448979591835, '_timestamp': 1740960545.5136244}).
Epoch: [3][ 38/500]	Time  7.360 ( 7.360)	Loss 1.7853 (1.8738)	CeLoss 0.2324 (0.2208)	SegCLSLoss 0.0139 (0.0195)	KLLoss 0.3926 (0.3826)	MaskLoss 0.7530 (0.8026)	MaskBCELoss 0.0578 (0.1138)	MaskDICELoss 0.6952 (0.6888)
Epoch: [3][ 39/500]	Time  5.559 ( 5.559)	Loss 1.2656 (1.8590)	CeLoss 1.2656 (0.4358)	SegCLSLoss 0.0000 (0.0203)	KLLoss 0.0000 (0.3016)	MaskLoss 0.0000 (0.6913)	MaskBCELoss 0.0000 (0.1284)	MaskDICELoss 0.0000 (0.5629)
[2025-03-02 18:09:24,651] [INFO] [logging.py:96:log_dist] [Rank 0] step=1540, skipped=0, lr=[0.00011791836734693876], mom=[(0.9, 0.95)]
[2025-03-02 18:09:24,657] [INFO] [timer.py:215:stop] epoch=0/micro_step=15400/global_step=1540, RunningAvgSamplesPerSec=1.570868552782033, CurrSamplesPerSec=1.6064198209105771, MemAllocated=30.86GB, MaxMemAllocated=37.19GB
Epoch: [3][ 40/500]	Time  6.227 ( 6.227)	Loss 0.9727 (1.5483)	CeLoss 0.9727 (0.3404)	SegCLSLoss 0.0000 (0.0127)	KLLoss 0.0000 (0.2621)	MaskLoss 0.0000 (0.5876)	MaskBCELoss 0.0000 (0.1500)	MaskDICELoss 0.0000 (0.4376)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 37 is less than current step: 499. Dropping entry: {'train/loss': 1.873818290233612, 'train/ce_loss': 0.22080078125, 'train/seg_cls_loss': 0.019549560546875, 'train/kl_loss': 0.3826171875, 'train/mask_bce_loss': 0.11382590672001243, 'train/mask_dice_loss': 0.6887570679187774, 'train/mask_loss': 0.802582973241806, 'metrics/total_secs_per_batch': 7.359567403793335, 'metrics/data_secs_per_batch': 3.4151948928833007, '_timestamp': 1740960552.8728974}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 37 is less than current step: 499. Dropping entry: {'train/lr': 0.00011810204081632652, '_timestamp': 1740960552.8730907}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 38 is less than current step: 499. Dropping entry: {'train/loss': 1.858957302570343, 'train/ce_loss': 0.435791015625, 'train/seg_cls_loss': 0.020343017578125, 'train/kl_loss': 0.3015625, 'train/mask_bce_loss': 0.12835388286039234, 'train/mask_dice_loss': 0.5629411652684212, 'train/mask_loss': 0.691295051574707, 'metrics/total_secs_per_batch': 5.559022665023804, 'metrics/data_secs_per_batch': 2.5952631711959837, '_timestamp': 1740960558.4319057}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 38 is less than current step: 499. Dropping entry: {'train/lr': 0.00011797959183673468, '_timestamp': 1740960558.4321854}).
Epoch: [3][ 41/500]	Time  6.476 ( 6.476)	Loss 0.0820 (1.4844)	CeLoss 0.0820 (0.3170)	SegCLSLoss 0.0000 (0.0141)	KLLoss 0.0000 (0.3016)	MaskLoss 0.0000 (0.5653)	MaskBCELoss 0.0000 (0.0908)	MaskDICELoss 0.0000 (0.4745)
Epoch: [3][ 42/500]	Time  5.324 ( 5.324)	Loss 0.2695 (1.4062)	CeLoss 0.2695 (0.6518)	SegCLSLoss 0.0000 (0.0095)	KLLoss 0.0000 (0.1889)	MaskLoss 0.0000 (0.3656)	MaskBCELoss 0.0000 (0.0750)	MaskDICELoss 0.0000 (0.2906)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 39 is less than current step: 499. Dropping entry: {'train/loss': 1.5482866406440734, 'train/ce_loss': 0.3404296875, 'train/seg_cls_loss': 0.01273193359375, 'train/kl_loss': 0.262109375, 'train/mask_bce_loss': 0.15001210644841195, 'train/mask_dice_loss': 0.43760777413845064, 'train/mask_loss': 0.5876198828220367, 'metrics/total_secs_per_batch': 6.226621627807617, 'metrics/data_secs_per_batch': 3.0898552894592286, '_timestamp': 1740960564.658432}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 39 is less than current step: 499. Dropping entry: {'train/lr': 0.00011785714285714284, '_timestamp': 1740960564.658653}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 40 is less than current step: 499. Dropping entry: {'train/loss': 1.4843862652778625, 'train/ce_loss': 0.3169921875, 'train/seg_cls_loss': 0.014117431640625, 'train/kl_loss': 0.3015625, 'train/mask_bce_loss': 0.09083639271557331, 'train/mask_dice_loss': 0.47445243299007417, 'train/mask_loss': 0.5652888387441635, 'metrics/total_secs_per_batch': 6.475801706314087, 'metrics/data_secs_per_batch': 3.007959985733032, '_timestamp': 1740960571.1344345}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 40 is less than current step: 499. Dropping entry: {'train/lr': 0.000117734693877551, '_timestamp': 1740960571.134768}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 41 is less than current step: 499. Dropping entry: {'train/loss': 1.4061639428138732, 'train/ce_loss': 0.6517578125, 'train/seg_cls_loss': 0.009503173828125, 'train/kl_loss': 0.1888671875, 'train/mask_bce_loss': 0.07502843365073204, 'train/mask_dice_loss': 0.29055353403091433, 'train/mask_loss': 0.3655819743871689, 'metrics/total_secs_per_batch': 5.324317216873169, 'metrics/data_secs_per_batch': 2.5354730129241942, '_timestamp': 1740960576.4587224}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 41 is less than current step: 499. Dropping entry: {'train/lr': 0.00011761224489795917, '_timestamp': 1740960576.4590127}).
Epoch: [3][ 43/500]	Time  5.203 ( 5.203)	Loss 2.1921 (1.7999)	CeLoss 0.1855 (0.5568)	SegCLSLoss 0.0322 (0.0146)	KLLoss 0.3672 (0.2605)	MaskLoss 0.9769 (0.6048)	MaskBCELoss 0.3189 (0.0700)	MaskDICELoss 0.6580 (0.5349)
Epoch: [3][ 44/500]	Time  6.016 ( 6.016)	Loss 1.1016 (1.5847)	CeLoss 1.1016 (0.6176)	SegCLSLoss 0.0000 (0.0136)	KLLoss 0.0000 (0.2643)	MaskLoss 0.0000 (0.4670)	MaskBCELoss 0.0000 (0.0600)	MaskDICELoss 0.0000 (0.4071)
Epoch: [3][ 45/500]	Time  5.190 ( 5.190)	Loss 2.0532 (1.8072)	CeLoss 0.2285 (0.5221)	SegCLSLoss 0.0223 (0.0112)	KLLoss 0.3789 (0.2256)	MaskLoss 0.8879 (0.6285)	MaskBCELoss 0.0586 (0.1099)	MaskDICELoss 0.8294 (0.5186)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 42 is less than current step: 499. Dropping entry: {'train/loss': 1.799929916858673, 'train/ce_loss': 0.5568359375, 'train/seg_cls_loss': 0.014605712890625, 'train/kl_loss': 0.260546875, 'train/mask_bce_loss': 0.06995980255305767, 'train/mask_dice_loss': 0.5348879754543304, 'train/mask_loss': 0.6048477768898011, 'metrics/total_secs_per_batch': 5.202872276306152, 'metrics/data_secs_per_batch': 2.2556469202041627, '_timestamp': 1740960581.6615584}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 42 is less than current step: 499. Dropping entry: {'train/lr': 0.00011748979591836733, '_timestamp': 1740960581.661753}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 43 is less than current step: 499. Dropping entry: {'train/loss': 1.584737890958786, 'train/ce_loss': 0.617578125, 'train/seg_cls_loss': 0.01356201171875, 'train/kl_loss': 0.2642578125, 'train/mask_bce_loss': 0.059957855567336085, 'train/mask_dice_loss': 0.4070692926645279, 'train/mask_loss': 0.4670271545648575, 'metrics/total_secs_per_batch': 6.0162672996521, 'metrics/data_secs_per_batch': 2.724316430091858, '_timestamp': 1740960587.6778371}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 43 is less than current step: 499. Dropping entry: {'train/lr': 0.0001173673469387755, '_timestamp': 1740960587.678116}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 44 is less than current step: 499. Dropping entry: {'train/loss': 1.8071927547454834, 'train/ce_loss': 0.5220703125, 'train/seg_cls_loss': 0.01123046875, 'train/kl_loss': 0.2255859375, 'train/mask_bce_loss': 0.1098858017474413, 'train/mask_dice_loss': 0.5186129033565521, 'train/mask_loss': 0.6284987151622772, 'metrics/total_secs_per_batch': 5.18987774848938, 'metrics/data_secs_per_batch': 2.281360936164856, '_timestamp': 1740960592.86773}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 44 is less than current step: 499. Dropping entry: {'train/lr': 0.00011724489795918367, '_timestamp': 1740960592.8680174}).
Epoch: [3][ 46/500]	Time  7.623 ( 7.623)	Loss 1.4006 (1.9938)	CeLoss 0.2217 (0.2865)	SegCLSLoss 0.0179 (0.0163)	KLLoss 0.3770 (0.2986)	MaskLoss 0.5665 (0.8346)	MaskBCELoss 0.2664 (0.2192)	MaskDICELoss 0.3001 (0.6153)
Epoch: [3][ 47/500]	Time  6.640 ( 6.640)	Loss 1.6616 (2.1316)	CeLoss 0.3125 (0.2225)	SegCLSLoss 0.0143 (0.0188)	KLLoss 0.3789 (0.3748)	MaskLoss 0.6521 (0.9312)	MaskBCELoss 0.0253 (0.2022)	MaskDICELoss 0.6268 (0.7290)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 45 is less than current step: 499. Dropping entry: {'train/loss': 1.993758451938629, 'train/ce_loss': 0.2864501953125, 'train/seg_cls_loss': 0.01634521484375, 'train/kl_loss': 0.2986328125, 'train/mask_bce_loss': 0.21924274004995822, 'train/mask_dice_loss': 0.6153195858001709, 'train/mask_loss': 0.8345623254776001, 'metrics/total_secs_per_batch': 7.622796297073364, 'metrics/data_secs_per_batch': 3.5167558908462526, '_timestamp': 1740960600.4905367}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 45 is less than current step: 499. Dropping entry: {'train/lr': 0.00011712244897959183, '_timestamp': 1740960600.490896}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 46 is less than current step: 499. Dropping entry: {'train/loss': 2.131573808193207, 'train/ce_loss': 0.2224609375, 'train/seg_cls_loss': 0.0187744140625, 'train/kl_loss': 0.3748046875, 'train/mask_bce_loss': 0.20219060098752378, 'train/mask_dice_loss': 0.7289771676063538, 'train/mask_loss': 0.9311677604913712, 'metrics/total_secs_per_batch': 6.640249252319336, 'metrics/data_secs_per_batch': 2.9869875431060793, '_timestamp': 1740960607.1307664}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 46 is less than current step: 499. Dropping entry: {'train/lr': 0.000117, '_timestamp': 1740960607.1310558}).
Epoch: [3][ 48/500]	Time  5.240 ( 5.240)	Loss 0.9141 (1.6622)	CeLoss 0.9141 (0.4321)	SegCLSLoss 0.0000 (0.0150)	KLLoss 0.0000 (0.2627)	MaskLoss 0.0000 (0.5981)	MaskBCELoss 0.0000 (0.0884)	MaskDICELoss 0.0000 (0.5097)
Epoch: [3][ 49/500]	Time  6.159 ( 6.159)	Loss 2.2218 (1.9936)	CeLoss 0.2266 (0.3002)	SegCLSLoss 0.0197 (0.0162)	KLLoss 0.3770 (0.3410)	MaskLoss 0.9742 (0.8257)	MaskBCELoss 0.0146 (0.1630)	MaskDICELoss 0.9596 (0.6627)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 47 is less than current step: 499. Dropping entry: {'train/loss': 1.6621744751930236, 'train/ce_loss': 0.43212890625, 'train/seg_cls_loss': 0.015032958984375, 'train/kl_loss': 0.2626953125, 'train/mask_bce_loss': 0.08839114587754011, 'train/mask_dice_loss': 0.5097371101379394, 'train/mask_loss': 0.5981282532215119, 'metrics/total_secs_per_batch': 5.239897012710571, 'metrics/data_secs_per_batch': 2.530528116226196, '_timestamp': 1740960612.3706305}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 47 is less than current step: 499. Dropping entry: {'train/lr': 0.00011687755102040816, '_timestamp': 1740960612.3708992}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 48 is less than current step: 499. Dropping entry: {'train/loss': 1.9935574173927306, 'train/ce_loss': 0.3001953125, 'train/seg_cls_loss': 0.0161865234375, 'train/kl_loss': 0.341015625, 'train/mask_bce_loss': 0.162973658926785, 'train/mask_dice_loss': 0.662711289525032, 'train/mask_loss': 0.8256849408149719, 'metrics/total_secs_per_batch': 6.159035682678223, 'metrics/data_secs_per_batch': 2.818904709815979, '_timestamp': 1740960618.5298045}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 48 is less than current step: 499. Dropping entry: {'train/lr': 0.00011675510204081632, '_timestamp': 1740960618.5301318}).
[2025-03-02 18:10:25,216] [INFO] [logging.py:96:log_dist] [Rank 0] step=1550, skipped=0, lr=[0.00011669387755102039], mom=[(0.9, 0.95)]
[2025-03-02 18:10:25,222] [INFO] [timer.py:215:stop] epoch=0/micro_step=15500/global_step=1550, RunningAvgSamplesPerSec=1.5713639555646488, CurrSamplesPerSec=1.4943568957523614, MemAllocated=31.79GB, MaxMemAllocated=37.19GB
Epoch: [3][ 50/500]	Time  6.694 ( 6.694)	Loss 1.4238 (1.9354)	CeLoss 0.1904 (0.2420)	SegCLSLoss 0.0204 (0.0169)	KLLoss 0.3770 (0.3369)	MaskLoss 0.5928 (0.8259)	MaskBCELoss 0.0336 (0.1397)	MaskDICELoss 0.5592 (0.6861)
Epoch: [3][ 51/500]	Time  6.303 ( 6.303)	Loss 2.4668 (1.8357)	CeLoss 0.2324 (0.4392)	SegCLSLoss 0.0228 (0.0165)	KLLoss 0.3711 (0.3004)	MaskLoss 1.0928 (0.6791)	MaskBCELoss 0.3046 (0.1808)	MaskDICELoss 0.7882 (0.4983)
Epoch: [3][ 52/500]	Time  6.342 ( 6.342)	Loss 1.6540 (1.9230)	CeLoss 0.2676 (0.2369)	SegCLSLoss 0.0159 (0.0187)	KLLoss 0.3750 (0.3377)	MaskLoss 0.6707 (0.8218)	MaskBCELoss 0.0466 (0.2032)	MaskDICELoss 0.6241 (0.6186)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 49 is less than current step: 499. Dropping entry: {'train/loss': 1.9354135990142822, 'train/ce_loss': 0.2419921875, 'train/seg_cls_loss': 0.016888427734375, 'train/kl_loss': 0.3369140625, 'train/mask_bce_loss': 0.13972649751231075, 'train/mask_dice_loss': 0.6861346036195755, 'train/mask_loss': 0.8258610904216767, 'metrics/total_secs_per_batch': 6.693597078323364, 'metrics/data_secs_per_batch': 2.8286725282669067, '_timestamp': 1740960625.2231328}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 49 is less than current step: 499. Dropping entry: {'train/lr': 0.00011663265306122447, '_timestamp': 1740960625.2234414}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 50 is less than current step: 499. Dropping entry: {'train/loss': 1.8356741845607758, 'train/ce_loss': 0.43916015625, 'train/seg_cls_loss': 0.016522216796875, 'train/kl_loss': 0.300390625, 'train/mask_bce_loss': 0.18079776056110858, 'train/mask_dice_loss': 0.49831862449645997, 'train/mask_loss': 0.6791163891553879, 'metrics/total_secs_per_batch': 6.303487062454224, 'metrics/data_secs_per_batch': 2.620788812637329, '_timestamp': 1740960631.5267484}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 50 is less than current step: 499. Dropping entry: {'train/lr': 0.00011651020408163264, '_timestamp': 1740960631.527029}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 51 is less than current step: 499. Dropping entry: {'train/loss': 1.9230114340782165, 'train/ce_loss': 0.2369140625, 'train/seg_cls_loss': 0.018731689453125, 'train/kl_loss': 0.3376953125, 'train/mask_bce_loss': 0.20315999882295727, 'train/mask_dice_loss': 0.6186484664678573, 'train/mask_loss': 0.8218084573745728, 'metrics/total_secs_per_batch': 6.342328071594238, 'metrics/data_secs_per_batch': 2.774977684020996, '_timestamp': 1740960637.869153}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 51 is less than current step: 499. Dropping entry: {'train/lr': 0.0001163877551020408, '_timestamp': 1740960637.8694484}).
Epoch: [3][ 53/500]	Time  5.826 ( 5.826)	Loss 2.0696 (1.5209)	CeLoss 0.2109 (0.4075)	SegCLSLoss 0.0278 (0.0162)	KLLoss 0.3633 (0.2666)	MaskLoss 0.9039 (0.5393)	MaskBCELoss 0.0887 (0.0919)	MaskDICELoss 0.8152 (0.4475)
Epoch: [3][ 54/500]	Time  6.321 ( 6.321)	Loss 2.1975 (1.9011)	CeLoss 0.2119 (0.3204)	SegCLSLoss 0.0209 (0.0187)	KLLoss 0.3633 (0.3361)	MaskLoss 0.9689 (0.7688)	MaskBCELoss 0.0407 (0.1000)	MaskDICELoss 0.9281 (0.6688)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 52 is less than current step: 499. Dropping entry: {'train/loss': 1.520902967453003, 'train/ce_loss': 0.407470703125, 'train/seg_cls_loss': 0.016156005859375, 'train/kl_loss': 0.2666015625, 'train/mask_bce_loss': 0.09186401730403304, 'train/mask_dice_loss': 0.44746931195259093, 'train/mask_loss': 0.5393333315849305, 'metrics/total_secs_per_batch': 5.82631254196167, 'metrics/data_secs_per_batch': 2.3675923109054566, '_timestamp': 1740960643.6956367}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 52 is less than current step: 499. Dropping entry: {'train/lr': 0.00011626530612244896, '_timestamp': 1740960643.6959925}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 53 is less than current step: 499. Dropping entry: {'train/loss': 1.901115345954895, 'train/ce_loss': 0.32041015625, 'train/seg_cls_loss': 0.01866455078125, 'train/kl_loss': 0.3361328125, 'train/mask_bce_loss': 0.09999196454882622, 'train/mask_dice_loss': 0.6688274204730987, 'train/mask_loss': 0.7688193917274475, 'metrics/total_secs_per_batch': 6.3214898109436035, 'metrics/data_secs_per_batch': 2.6689306020736696, '_timestamp': 1740960650.016922}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 53 is less than current step: 499. Dropping entry: {'train/lr': 0.00011614285714285713, '_timestamp': 1740960650.0172029}).
Epoch: [3][ 55/500]	Time  6.310 ( 6.310)	Loss 0.5376 (1.6903)	CeLoss 0.2734 (0.3044)	SegCLSLoss 0.0106 (0.0211)	KLLoss 0.3906 (0.3008)	MaskLoss 0.1096 (0.6725)	MaskBCELoss 0.0439 (0.1708)	MaskDICELoss 0.0657 (0.5017)
Epoch: [3][ 56/500]	Time  6.739 ( 6.739)	Loss 2.5284 (1.4993)	CeLoss 0.2100 (0.2546)	SegCLSLoss 0.0258 (0.0160)	KLLoss 0.3730 (0.2662)	MaskLoss 1.1343 (0.6052)	MaskBCELoss 0.3198 (0.1243)	MaskDICELoss 0.8145 (0.4809)
Epoch: [3][ 57/500]	Time  5.263 ( 5.263)	Loss 1.4688 (1.6516)	CeLoss 0.2656 (0.5729)	SegCLSLoss 0.0124 (0.0146)	KLLoss 0.3809 (0.2662)	MaskLoss 0.5801 (0.5225)	MaskBCELoss 0.0389 (0.1175)	MaskDICELoss 0.5412 (0.4051)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 54 is less than current step: 499. Dropping entry: {'train/loss': 1.6902718126773835, 'train/ce_loss': 0.304443359375, 'train/seg_cls_loss': 0.021142578125, 'train/kl_loss': 0.30078125, 'train/mask_bce_loss': 0.1708217140287161, 'train/mask_dice_loss': 0.5017067827284336, 'train/mask_loss': 0.6725284859538079, 'metrics/total_secs_per_batch': 6.310480833053589, 'metrics/data_secs_per_batch': 2.725944232940674, '_timestamp': 1740960656.3275518}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 54 is less than current step: 499. Dropping entry: {'train/lr': 0.00011602040816326529, '_timestamp': 1740960656.3278747}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 55 is less than current step: 499. Dropping entry: {'train/loss': 1.499336874485016, 'train/ce_loss': 0.25458984375, 'train/seg_cls_loss': 0.0159912109375, 'train/kl_loss': 0.2662109375, 'train/mask_bce_loss': 0.12427378464490176, 'train/mask_dice_loss': 0.4809122234582901, 'train/mask_loss': 0.6051860094070435, 'metrics/total_secs_per_batch': 6.738699197769165, 'metrics/data_secs_per_batch': 2.744955563545227, '_timestamp': 1740960663.0662465}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 55 is less than current step: 499. Dropping entry: {'train/lr': 0.00011589795918367347, '_timestamp': 1740960663.066571}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 56 is less than current step: 499. Dropping entry: {'train/loss': 1.6515774130821228, 'train/ce_loss': 0.57294921875, 'train/seg_cls_loss': 0.01456298828125, 'train/kl_loss': 0.2662109375, 'train/mask_bce_loss': 0.11746246181428432, 'train/mask_dice_loss': 0.40505475699901583, 'train/mask_loss': 0.5225172281265259, 'metrics/total_secs_per_batch': 5.262695550918579, 'metrics/data_secs_per_batch': 2.346087193489075, '_timestamp': 1740960668.32881}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 56 is less than current step: 499. Dropping entry: {'train/lr': 0.00011577551020408163, '_timestamp': 1740960668.329086}).
Epoch: [3][ 58/500]	Time  5.900 ( 5.900)	Loss 1.5321 (1.5145)	CeLoss 0.1914 (0.4342)	SegCLSLoss 0.0221 (0.0132)	KLLoss 0.3691 (0.2656)	MaskLoss 0.6464 (0.5236)	MaskBCELoss 0.0406 (0.1102)	MaskDICELoss 0.6059 (0.4134)
Epoch: [3][ 59/500]	Time  5.066 ( 5.066)	Loss 1.5234 (1.6691)	CeLoss 1.5234 (0.5762)	SegCLSLoss 0.0000 (0.0112)	KLLoss 0.0000 (0.2262)	MaskLoss 0.0000 (0.5324)	MaskBCELoss 0.0000 (0.0744)	MaskDICELoss 0.0000 (0.4580)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 57 is less than current step: 499. Dropping entry: {'train/loss': 1.5144529342651367, 'train/ce_loss': 0.4341796875, 'train/seg_cls_loss': 0.013226318359375, 'train/kl_loss': 0.265625, 'train/mask_bce_loss': 0.11022343821823596, 'train/mask_dice_loss': 0.4133604526519775, 'train/mask_loss': 0.5235838800668716, 'metrics/total_secs_per_batch': 5.900222063064575, 'metrics/data_secs_per_batch': 2.403624701499939, '_timestamp': 1740960674.2290716}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 57 is less than current step: 499. Dropping entry: {'train/lr': 0.00011565306122448979, '_timestamp': 1740960674.229267}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 58 is less than current step: 499. Dropping entry: {'train/loss': 1.6690982937812806, 'train/ce_loss': 0.576171875, 'train/seg_cls_loss': 0.011181640625, 'train/kl_loss': 0.226171875, 'train/mask_bce_loss': 0.07438219273462891, 'train/mask_dice_loss': 0.4579697042703629, 'train/mask_loss': 0.5323518872261047, 'metrics/total_secs_per_batch': 5.065654754638672, 'metrics/data_secs_per_batch': 2.4520912170410156, '_timestamp': 1740960679.2947364}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 58 is less than current step: 499. Dropping entry: {'train/lr': 0.00011553061224489795, '_timestamp': 1740960679.2950144}).
[2025-03-02 18:11:25,319] [INFO] [logging.py:96:log_dist] [Rank 0] step=1560, skipped=0, lr=[0.00011546938775510204], mom=[(0.9, 0.95)]
[2025-03-02 18:11:25,325] [INFO] [timer.py:215:stop] epoch=0/micro_step=15600/global_step=1560, RunningAvgSamplesPerSec=1.5719265487742944, CurrSamplesPerSec=1.6583494909208387, MemAllocated=31.26GB, MaxMemAllocated=37.19GB
Epoch: [3][ 60/500]	Time  6.032 ( 6.032)	Loss 2.2650 (1.7377)	CeLoss 0.2246 (0.4080)	SegCLSLoss 0.0256 (0.0151)	KLLoss 0.3594 (0.3002)	MaskLoss 0.9958 (0.6460)	MaskBCELoss 0.0428 (0.0898)	MaskDICELoss 0.9530 (0.5562)
Epoch: [3][ 61/500]	Time  5.510 ( 5.510)	Loss 2.5056 (1.9227)	CeLoss 0.2207 (0.4871)	SegCLSLoss 0.0142 (0.0147)	KLLoss 0.3770 (0.2637)	MaskLoss 1.1200 (0.7010)	MaskBCELoss 0.6075 (0.2173)	MaskDICELoss 0.5124 (0.4837)
Epoch: [3][ 62/500]	Time  6.054 ( 6.054)	Loss 2.8972 (1.8161)	CeLoss 0.1846 (0.2616)	SegCLSLoss 0.0247 (0.0154)	KLLoss 0.3945 (0.3049)	MaskLoss 1.3305 (0.7583)	MaskBCELoss 0.4950 (0.1916)	MaskDICELoss 0.8354 (0.5667)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 59 is less than current step: 499. Dropping entry: {'train/loss': 1.737701964378357, 'train/ce_loss': 0.4080078125, 'train/seg_cls_loss': 0.015069580078125, 'train/kl_loss': 0.3001953125, 'train/mask_bce_loss': 0.08979501174762845, 'train/mask_dice_loss': 0.5562043979763984, 'train/mask_loss': 0.6459994107484818, 'metrics/total_secs_per_batch': 6.031722068786621, 'metrics/data_secs_per_batch': 2.833627152442932, '_timestamp': 1740960685.3262665}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 59 is less than current step: 499. Dropping entry: {'train/lr': 0.00011540816326530612, '_timestamp': 1740960685.3265781}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 60 is less than current step: 499. Dropping entry: {'train/loss': 1.9226978778839112, 'train/ce_loss': 0.487060546875, 'train/seg_cls_loss': 0.014666748046875, 'train/kl_loss': 0.263671875, 'train/mask_bce_loss': 0.2173385417088866, 'train/mask_dice_loss': 0.4836588382720947, 'train/mask_loss': 0.7009973764419556, 'metrics/total_secs_per_batch': 5.509904146194458, 'metrics/data_secs_per_batch': 2.581423306465149, '_timestamp': 1740960690.8364885}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 60 is less than current step: 499. Dropping entry: {'train/lr': 0.00011528571428571428, '_timestamp': 1740960690.8368104}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 61 is less than current step: 499. Dropping entry: {'train/loss': 1.8160871148109436, 'train/ce_loss': 0.2615966796875, 'train/seg_cls_loss': 0.015374755859375, 'train/kl_loss': 0.3048828125, 'train/mask_bce_loss': 0.19160967729985715, 'train/mask_dice_loss': 0.5667390465736389, 'train/mask_loss': 0.7583487331867218, 'metrics/total_secs_per_batch': 6.053621053695679, 'metrics/data_secs_per_batch': 2.634890103340149, '_timestamp': 1740960696.8899384}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 61 is less than current step: 499. Dropping entry: {'train/lr': 0.00011516326530612244, '_timestamp': 1740960696.8902066}).
Epoch: [3][ 63/500]	Time  5.042 ( 5.042)	Loss 1.6250 (1.9664)	CeLoss 1.6250 (0.5312)	SegCLSLoss 0.0000 (0.0155)	KLLoss 0.0000 (0.2674)	MaskLoss 0.0000 (0.7005)	MaskBCELoss 0.0000 (0.1318)	MaskDICELoss 0.0000 (0.5687)
Epoch: [3][ 64/500]	Time  5.484 ( 5.484)	Loss 1.5859 (1.5522)	CeLoss 0.2275 (0.4574)	SegCLSLoss 0.0135 (0.0122)	KLLoss 0.3887 (0.2678)	MaskLoss 0.6562 (0.5310)	MaskBCELoss 0.1495 (0.1261)	MaskDICELoss 0.5067 (0.4049)
Epoch: [3][ 65/500]	Time  6.417 ( 6.417)	Loss 2.1791 (1.9490)	CeLoss 0.1758 (0.3003)	SegCLSLoss 0.0255 (0.0207)	KLLoss 0.3633 (0.2965)	MaskLoss 0.9772 (0.8045)	MaskBCELoss 0.0580 (0.1680)	MaskDICELoss 0.9192 (0.6365)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 62 is less than current step: 499. Dropping entry: {'train/loss': 1.966446304321289, 'train/ce_loss': 0.53125, 'train/seg_cls_loss': 0.015521240234375, 'train/kl_loss': 0.2673828125, 'train/mask_bce_loss': 0.13176067937165498, 'train/mask_dice_loss': 0.5686988025903702, 'train/mask_loss': 0.700459486246109, 'metrics/total_secs_per_batch': 5.041600465774536, 'metrics/data_secs_per_batch': 2.42446768283844, '_timestamp': 1740960701.9315171}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 62 is less than current step: 499. Dropping entry: {'train/lr': 0.00011504081632653059, '_timestamp': 1740960701.9317837}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 63 is less than current step: 499. Dropping entry: {'train/loss': 1.5522000193595886, 'train/ce_loss': 0.457421875, 'train/seg_cls_loss': 0.0122314453125, 'train/kl_loss': 0.2677734375, 'train/mask_bce_loss': 0.12609390616416932, 'train/mask_dice_loss': 0.4049377292394638, 'train/mask_loss': 0.5310316443443298, 'metrics/total_secs_per_batch': 5.4840192794799805, 'metrics/data_secs_per_batch': 2.3228925466537476, '_timestamp': 1740960707.4155798}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 63 is less than current step: 499. Dropping entry: {'train/lr': 0.00011491836734693876, '_timestamp': 1740960707.4158587}).
Epoch: [3][ 66/500]	Time  5.998 ( 5.998)	Loss 1.9211 (2.0720)	CeLoss 0.2041 (0.2161)	SegCLSLoss 0.0275 (0.0227)	KLLoss 0.3730 (0.3406)	MaskLoss 0.8326 (0.9051)	MaskBCELoss 0.0228 (0.1756)	MaskDICELoss 0.8098 (0.7295)
Epoch: [3][ 67/500]	Time  6.141 ( 6.141)	Loss 0.6562 (1.8002)	CeLoss 0.6562 (0.4001)	SegCLSLoss 0.0000 (0.0136)	KLLoss 0.0000 (0.3039)	MaskLoss 0.0000 (0.6814)	MaskBCELoss 0.0000 (0.1954)	MaskDICELoss 0.0000 (0.4860)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 64 is less than current step: 499. Dropping entry: {'train/loss': 1.9490257501602173, 'train/ce_loss': 0.30029296875, 'train/seg_cls_loss': 0.0207275390625, 'train/kl_loss': 0.296484375, 'train/mask_bce_loss': 0.1679647011682391, 'train/mask_dice_loss': 0.6365286409854889, 'train/mask_loss': 0.8044933497905731, 'metrics/total_secs_per_batch': 6.416701316833496, 'metrics/data_secs_per_batch': 2.933973026275635, '_timestamp': 1740960713.83232}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 64 is less than current step: 499. Dropping entry: {'train/lr': 0.00011479591836734692, '_timestamp': 1740960713.832518}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 65 is less than current step: 499. Dropping entry: {'train/loss': 2.07200003862381, 'train/ce_loss': 0.216064453125, 'train/seg_cls_loss': 0.022674560546875, 'train/kl_loss': 0.340625, 'train/mask_bce_loss': 0.17561437748372555, 'train/mask_dice_loss': 0.7295018613338471, 'train/mask_loss': 0.9051162302494049, 'metrics/total_secs_per_batch': 5.997880935668945, 'metrics/data_secs_per_batch': 2.7205825805664063, '_timestamp': 1740960719.830151}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 65 is less than current step: 499. Dropping entry: {'train/lr': 0.00011467346938775508, '_timestamp': 1740960719.8304334}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 66 is less than current step: 499. Dropping entry: {'train/loss': 1.8002036333084106, 'train/ce_loss': 0.40009765625, 'train/seg_cls_loss': 0.013592529296875, 'train/kl_loss': 0.30390625, 'train/mask_bce_loss': 0.1954487081617117, 'train/mask_dice_loss': 0.4859519362449646, 'train/mask_loss': 0.6814006388187408, 'metrics/total_secs_per_batch': 6.141115427017212, 'metrics/data_secs_per_batch': 2.795324420928955, '_timestamp': 1740960725.9715283}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 66 is less than current step: 499. Dropping entry: {'train/lr': 0.00011455102040816326, '_timestamp': 1740960725.9719234}).
Epoch: [3][ 68/500]	Time  6.219 ( 6.219)	Loss 2.0786 (1.6148)	CeLoss 0.1973 (0.3025)	SegCLSLoss 0.0244 (0.0126)	KLLoss 0.3652 (0.3020)	MaskLoss 0.9163 (0.6378)	MaskBCELoss 0.0402 (0.1382)	MaskDICELoss 0.8761 (0.4996)
Epoch: [3][ 69/500]	Time  6.227 ( 6.227)	Loss 2.5796 (1.4838)	CeLoss 0.2012 (0.2719)	SegCLSLoss 0.0212 (0.0151)	KLLoss 0.3887 (0.3051)	MaskLoss 1.1648 (0.5869)	MaskBCELoss 0.3348 (0.0707)	MaskDICELoss 0.8300 (0.5163)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 67 is less than current step: 499. Dropping entry: {'train/loss': 1.6147786140441895, 'train/ce_loss': 0.3025390625, 'train/seg_cls_loss': 0.012603759765625, 'train/kl_loss': 0.301953125, 'train/mask_bce_loss': 0.13822393845766784, 'train/mask_dice_loss': 0.499585285782814, 'train/mask_loss': 0.6378092169761658, 'metrics/total_secs_per_batch': 6.218892335891724, 'metrics/data_secs_per_batch': 2.7555049419403077, '_timestamp': 1740960732.1902127}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 67 is less than current step: 499. Dropping entry: {'train/lr': 0.00011442857142857142, '_timestamp': 1740960732.190499}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 68 is less than current step: 499. Dropping entry: {'train/loss': 1.4838229775428773, 'train/ce_loss': 0.2718505859375, 'train/seg_cls_loss': 0.015106201171875, 'train/kl_loss': 0.305078125, 'train/mask_bce_loss': 0.07065701968967915, 'train/mask_dice_loss': 0.516286201775074, 'train/mask_loss': 0.5869432181119919, 'metrics/total_secs_per_batch': 6.2270917892456055, 'metrics/data_secs_per_batch': 2.990088152885437, '_timestamp': 1740960738.4172504}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 68 is less than current step: 499. Dropping entry: {'train/lr': 0.00011430612244897959, '_timestamp': 1740960738.4175215}).
[2025-03-02 18:12:25,175] [INFO] [logging.py:96:log_dist] [Rank 0] step=1570, skipped=0, lr=[0.00011424489795918367], mom=[(0.9, 0.95)]
[2025-03-02 18:12:25,181] [INFO] [timer.py:215:stop] epoch=0/micro_step=15700/global_step=1570, RunningAvgSamplesPerSec=1.5725213615354026, CurrSamplesPerSec=1.478603819724654, MemAllocated=31.45GB, MaxMemAllocated=37.19GB
Epoch: [3][ 70/500]	Time  6.765 ( 6.765)	Loss 1.5805 (1.4875)	CeLoss 0.2373 (0.3180)	SegCLSLoss 0.0165 (0.0126)	KLLoss 0.3711 (0.2643)	MaskLoss 0.6496 (0.5684)	MaskBCELoss 0.0388 (0.0371)	MaskDICELoss 0.6108 (0.5313)
Epoch: [3][ 71/500]	Time  6.348 ( 6.348)	Loss 2.1293 (1.3201)	CeLoss 0.2637 (0.3037)	SegCLSLoss 0.0209 (0.0124)	KLLoss 0.3613 (0.2240)	MaskLoss 0.9104 (0.4940)	MaskBCELoss 0.0204 (0.0750)	MaskDICELoss 0.8899 (0.4190)
Epoch: [3][ 72/500]	Time  5.816 ( 5.816)	Loss 2.5068 (1.7337)	CeLoss 0.1582 (0.4278)	SegCLSLoss 0.0383 (0.0168)	KLLoss 0.3730 (0.3008)	MaskLoss 1.1460 (0.6336)	MaskBCELoss 0.3516 (0.1252)	MaskDICELoss 0.7944 (0.5084)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 69 is less than current step: 499. Dropping entry: {'train/loss': 1.4875080108642578, 'train/ce_loss': 0.318017578125, 'train/seg_cls_loss': 0.01259765625, 'train/kl_loss': 0.2642578125, 'train/mask_bce_loss': 0.037128159776329994, 'train/mask_dice_loss': 0.5313084602355957, 'train/mask_loss': 0.5684366226196289, 'metrics/total_secs_per_batch': 6.764659404754639, 'metrics/data_secs_per_batch': 3.1069640636444094, '_timestamp': 1740960745.181747}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 69 is less than current step: 499. Dropping entry: {'train/lr': 0.00011418367346938775, '_timestamp': 1740960745.1820345}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 70 is less than current step: 499. Dropping entry: {'train/loss': 1.3200560748577117, 'train/ce_loss': 0.303662109375, 'train/seg_cls_loss': 0.012371826171875, 'train/kl_loss': 0.2240234375, 'train/mask_bce_loss': 0.07503450606018305, 'train/mask_dice_loss': 0.41895348727703097, 'train/mask_loss': 0.4939879983663559, 'metrics/total_secs_per_batch': 6.3484978675842285, 'metrics/data_secs_per_batch': 2.8734256267547607, '_timestamp': 1740960751.5304453}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 70 is less than current step: 499. Dropping entry: {'train/lr': 0.00011406122448979591, '_timestamp': 1740960751.5307338}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 71 is less than current step: 499. Dropping entry: {'train/loss': 1.733691442012787, 'train/ce_loss': 0.42783203125, 'train/seg_cls_loss': 0.016845703125, 'train/kl_loss': 0.30078125, 'train/mask_bce_loss': 0.12520457995124162, 'train/mask_dice_loss': 0.5084380269050598, 'train/mask_loss': 0.6336425960063934, 'metrics/total_secs_per_batch': 5.815520286560059, 'metrics/data_secs_per_batch': 2.457012939453125, '_timestamp': 1740960757.345957}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 71 is less than current step: 499. Dropping entry: {'train/lr': 0.00011393877551020407, '_timestamp': 1740960757.3462298}).
Epoch: [3][ 73/500]	Time  6.478 ( 6.478)	Loss 1.7561 (2.0968)	CeLoss 0.1729 (0.3791)	SegCLSLoss 0.0303 (0.0197)	KLLoss 0.3848 (0.3387)	MaskLoss 0.7647 (0.8371)	MaskBCELoss 0.0100 (0.1370)	MaskDICELoss 0.7547 (0.7001)
Epoch: [3][ 74/500]	Time  6.564 ( 6.564)	Loss 2.2298 (1.8376)	CeLoss 0.2021 (0.3493)	SegCLSLoss 0.0284 (0.0160)	KLLoss 0.3555 (0.3416)	MaskLoss 0.9889 (0.7231)	MaskBCELoss 0.0232 (0.1083)	MaskDICELoss 0.9657 (0.6148)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 72 is less than current step: 499. Dropping entry: {'train/loss': 2.096830940246582, 'train/ce_loss': 0.3791015625, 'train/seg_cls_loss': 0.019696044921875, 'train/kl_loss': 0.338671875, 'train/mask_bce_loss': 0.13700161399319769, 'train/mask_dice_loss': 0.700134551525116, 'train/mask_loss': 0.8371361494064331, 'metrics/total_secs_per_batch': 6.47751522064209, 'metrics/data_secs_per_batch': 2.8943764209747314, '_timestamp': 1740960763.8235013}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 72 is less than current step: 499. Dropping entry: {'train/lr': 0.00011381632653061224, '_timestamp': 1740960763.8237796}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 73 is less than current step: 499. Dropping entry: {'train/loss': 1.8375527739524842, 'train/ce_loss': 0.34931640625, 'train/seg_cls_loss': 0.015997314453125, 'train/kl_loss': 0.3416015625, 'train/mask_bce_loss': 0.1082528829574585, 'train/mask_dice_loss': 0.6148203879594802, 'train/mask_loss': 0.7230732679367066, 'metrics/total_secs_per_batch': 6.564298152923584, 'metrics/data_secs_per_batch': 2.924873614311218, '_timestamp': 1740960770.3878105}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 73 is less than current step: 499. Dropping entry: {'train/lr': 0.0001136938775510204, '_timestamp': 1740960770.3880937}).
Epoch: [3][ 75/500]	Time  7.038 ( 7.038)	Loss 2.7267 (1.8956)	CeLoss 0.1846 (0.2261)	SegCLSLoss 0.0284 (0.0180)	KLLoss 0.3711 (0.3430)	MaskLoss 1.2452 (0.8133)	MaskBCELoss 0.4112 (0.1594)	MaskDICELoss 0.8340 (0.6539)
Epoch: [3][ 76/500]	Time  5.598 ( 5.598)	Loss 1.6368 (1.7308)	CeLoss 0.2383 (0.5076)	SegCLSLoss 0.0155 (0.0139)	KLLoss 0.3789 (0.2652)	MaskLoss 0.6768 (0.5950)	MaskBCELoss 0.2580 (0.1954)	MaskDICELoss 0.4188 (0.3996)
Epoch: [3][ 77/500]	Time  5.485 ( 5.485)	Loss 0.8125 (1.6088)	CeLoss 0.8125 (0.5154)	SegCLSLoss 0.0000 (0.0116)	KLLoss 0.0000 (0.2281)	MaskLoss 0.0000 (0.5325)	MaskBCELoss 0.0000 (0.1081)	MaskDICELoss 0.0000 (0.4244)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 74 is less than current step: 499. Dropping entry: {'train/loss': 1.8955519556999207, 'train/ce_loss': 0.22607421875, 'train/seg_cls_loss': 0.018011474609375, 'train/kl_loss': 0.34296875, 'train/mask_bce_loss': 0.15936234421096743, 'train/mask_dice_loss': 0.6539409726858139, 'train/mask_loss': 0.8133033156394959, 'metrics/total_secs_per_batch': 7.03832483291626, 'metrics/data_secs_per_batch': 3.1378940105438233, '_timestamp': 1740960777.426195}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 74 is less than current step: 499. Dropping entry: {'train/lr': 0.00011357142857142856, '_timestamp': 1740960777.4263966}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 75 is less than current step: 499. Dropping entry: {'train/loss': 1.7308288753032683, 'train/ce_loss': 0.507568359375, 'train/seg_cls_loss': 0.0139404296875, 'train/kl_loss': 0.265234375, 'train/mask_bce_loss': 0.1953508101403713, 'train/mask_dice_loss': 0.39962905943393706, 'train/mask_loss': 0.5949798673391342, 'metrics/total_secs_per_batch': 5.598188161849976, 'metrics/data_secs_per_batch': 2.374022436141968, '_timestamp': 1740960783.0244448}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 75 is less than current step: 499. Dropping entry: {'train/lr': 0.00011344897959183671, '_timestamp': 1740960783.0247688}).
Epoch: [3][ 78/500]	Time  8.240 ( 8.240)	Loss 1.8526 (1.8064)	CeLoss 0.1836 (0.2621)	SegCLSLoss 0.0214 (0.0187)	KLLoss 0.3770 (0.3406)	MaskLoss 0.8101 (0.7505)	MaskBCELoss 0.0110 (0.0814)	MaskDICELoss 0.7991 (0.6691)
Epoch: [3][ 79/500]	Time  6.866 ( 6.866)	Loss 1.9325 (1.7339)	CeLoss 0.2197 (0.3330)	SegCLSLoss 0.0162 (0.0163)	KLLoss 0.3906 (0.3449)	MaskLoss 0.8334 (0.6791)	MaskBCELoss 0.2280 (0.0840)	MaskDICELoss 0.6054 (0.5951)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 76 is less than current step: 499. Dropping entry: {'train/loss': 1.6087671995162964, 'train/ce_loss': 0.5154296875, 'train/seg_cls_loss': 0.01160888671875, 'train/kl_loss': 0.228125, 'train/mask_bce_loss': 0.1080740824341774, 'train/mask_dice_loss': 0.42438569366931916, 'train/mask_loss': 0.5324597775936126, 'metrics/total_secs_per_batch': 5.48529839515686, 'metrics/data_secs_per_batch': 2.7081500768661497, '_timestamp': 1740960788.5095932}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 76 is less than current step: 499. Dropping entry: {'train/lr': 0.00011332653061224488, '_timestamp': 1740960788.5098672}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 77 is less than current step: 499. Dropping entry: {'train/loss': 1.8064092814922332, 'train/ce_loss': 0.262109375, 'train/seg_cls_loss': 0.01873779296875, 'train/kl_loss': 0.340625, 'train/mask_bce_loss': 0.08143881093710661, 'train/mask_dice_loss': 0.6690802559256553, 'train/mask_loss': 0.750519073009491, 'metrics/total_secs_per_batch': 8.239771842956543, 'metrics/data_secs_per_batch': 3.737812614440918, '_timestamp': 1740960796.7494094}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 77 is less than current step: 499. Dropping entry: {'train/lr': 0.00011320408163265304, '_timestamp': 1740960796.7496953}).
[2025-03-02 18:13:29,282] [INFO] [logging.py:96:log_dist] [Rank 0] step=1580, skipped=0, lr=[0.0001130204081632653], mom=[(0.9, 0.95)]
[2025-03-02 18:13:29,288] [INFO] [timer.py:215:stop] epoch=0/micro_step=15800/global_step=1580, RunningAvgSamplesPerSec=1.5724425299853226, CurrSamplesPerSec=1.7630018054116618, MemAllocated=31.25GB, MaxMemAllocated=37.19GB
Epoch: [3][ 80/500]	Time  5.674 ( 5.674)	Loss 2.4519 (1.8072)	CeLoss 0.2354 (0.3680)	SegCLSLoss 0.0133 (0.0200)	KLLoss 0.3809 (0.3406)	MaskLoss 1.0853 (0.6974)	MaskBCELoss 0.1128 (0.1146)	MaskDICELoss 0.9725 (0.5829)
Epoch: [3][ 81/500]	Time  6.499 ( 6.499)	Loss 1.7271 (1.5248)	CeLoss 0.2422 (0.3301)	SegCLSLoss 0.0136 (0.0119)	KLLoss 0.3867 (0.2656)	MaskLoss 0.7200 (0.5811)	MaskBCELoss 0.3348 (0.0952)	MaskDICELoss 0.3852 (0.4859)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 78 is less than current step: 499. Dropping entry: {'train/loss': 1.7339008569717407, 'train/ce_loss': 0.3330078125, 'train/seg_cls_loss': 0.016302490234375, 'train/kl_loss': 0.344921875, 'train/mask_bce_loss': 0.08397053815424442, 'train/mask_dice_loss': 0.5951381027698517, 'train/mask_loss': 0.6791086375713349, 'metrics/total_secs_per_batch': 6.866271257400513, 'metrics/data_secs_per_batch': 3.1131316423416138, '_timestamp': 1740960803.6156075}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 78 is less than current step: 499. Dropping entry: {'train/lr': 0.00011308163265306122, '_timestamp': 1740960803.615879}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 79 is less than current step: 499. Dropping entry: {'train/loss': 1.8072036683559418, 'train/ce_loss': 0.36796875, 'train/seg_cls_loss': 0.020013427734375, 'train/kl_loss': 0.340625, 'train/mask_bce_loss': 0.1145873743109405, 'train/mask_dice_loss': 0.5828620970249176, 'train/mask_loss': 0.6974494755268097, 'metrics/total_secs_per_batch': 5.673657178878784, 'metrics/data_secs_per_batch': 2.4135060548782348, '_timestamp': 1740960809.2891223}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 79 is less than current step: 499. Dropping entry: {'train/lr': 0.00011295918367346938, '_timestamp': 1740960809.2894123}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 80 is less than current step: 499. Dropping entry: {'train/loss': 1.5247827053070069, 'train/ce_loss': 0.3301025390625, 'train/seg_cls_loss': 0.011944580078125, 'train/kl_loss': 0.265625, 'train/mask_bce_loss': 0.0952399104833603, 'train/mask_dice_loss': 0.48588925004005434, 'train/mask_loss': 0.5811291575431824, 'metrics/total_secs_per_batch': 6.499307870864868, 'metrics/data_secs_per_batch': 2.9751806259155273, '_timestamp': 1740960815.7886217}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 80 is less than current step: 499. Dropping entry: {'train/lr': 0.00011283673469387754, '_timestamp': 1740960815.7888963}).
Epoch: [3][ 82/500]	Time  7.562 ( 7.562)	Loss 1.6131 (1.3702)	CeLoss 0.2930 (0.3952)	SegCLSLoss 0.0121 (0.0127)	KLLoss 0.3867 (0.2648)	MaskLoss 0.6376 (0.4711)	MaskBCELoss 0.1183 (0.1054)	MaskDICELoss 0.5193 (0.3657)
Epoch: [3][ 83/500]	Time  8.219 ( 8.219)	Loss 1.3573 (1.5668)	CeLoss 0.2676 (0.4494)	SegCLSLoss 0.0115 (0.0127)	KLLoss 0.3945 (0.2668)	MaskLoss 0.5224 (0.5423)	MaskBCELoss 0.2516 (0.1291)	MaskDICELoss 0.2708 (0.4132)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 81 is less than current step: 499. Dropping entry: {'train/loss': 1.3702186584472655, 'train/ce_loss': 0.395166015625, 'train/seg_cls_loss': 0.0126953125, 'train/kl_loss': 0.26484375, 'train/mask_bce_loss': 0.10539375841617585, 'train/mask_dice_loss': 0.3657263174653053, 'train/mask_loss': 0.4711200758814812, 'metrics/total_secs_per_batch': 7.562460422515869, 'metrics/data_secs_per_batch': 3.294539189338684, '_timestamp': 1740960823.35106}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 81 is less than current step: 499. Dropping entry: {'train/lr': 0.0001127142857142857, '_timestamp': 1740960823.3513262}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 82 is less than current step: 499. Dropping entry: {'train/loss': 1.5668007731437683, 'train/ce_loss': 0.4494140625, 'train/seg_cls_loss': 0.012664794921875, 'train/kl_loss': 0.266796875, 'train/mask_bce_loss': 0.12905930802226068, 'train/mask_dice_loss': 0.41322779804468157, 'train/mask_loss': 0.5422871083021163, 'metrics/total_secs_per_batch': 8.218586206436157, 'metrics/data_secs_per_batch': 3.8332288026809693, '_timestamp': 1740960831.5696406}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 82 is less than current step: 499. Dropping entry: {'train/lr': 0.00011259183673469387, '_timestamp': 1740960831.5699224}).
Epoch: [3][ 84/500]	Time  8.484 ( 8.484)	Loss 1.8671 (2.0786)	CeLoss 0.2383 (0.2267)	SegCLSLoss 0.0282 (0.0226)	KLLoss 0.3633 (0.3762)	MaskLoss 0.7890 (0.9015)	MaskBCELoss 0.0180 (0.1614)	MaskDICELoss 0.7711 (0.7401)
Epoch: [3][ 85/500]	Time  8.813 ( 8.813)	Loss 1.3125 (1.6150)	CeLoss 1.3125 (0.4372)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.2295)	MaskLoss 0.0000 (0.5739)	MaskBCELoss 0.0000 (0.1265)	MaskDICELoss 0.0000 (0.4475)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 83 is less than current step: 499. Dropping entry: {'train/loss': 2.0786108136177064, 'train/ce_loss': 0.22666015625, 'train/seg_cls_loss': 0.022613525390625, 'train/kl_loss': 0.376171875, 'train/mask_bce_loss': 0.16136146625503897, 'train/mask_dice_loss': 0.7401021391153335, 'train/mask_loss': 0.9014636084437371, 'metrics/total_secs_per_batch': 8.484268426895142, 'metrics/data_secs_per_batch': 3.979352903366089, '_timestamp': 1740960840.0541832}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 83 is less than current step: 499. Dropping entry: {'train/lr': 0.00011246938775510203, '_timestamp': 1740960840.0545442}).
Epoch: [3][ 86/500]	Time  8.363 ( 8.363)	Loss 2.6890 (1.6360)	CeLoss 0.1826 (0.3471)	SegCLSLoss 0.0219 (0.0106)	KLLoss 0.3789 (0.2258)	MaskLoss 1.2288 (0.6305)	MaskBCELoss 0.4823 (0.1668)	MaskDICELoss 0.7464 (0.4637)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 84 is less than current step: 499. Dropping entry: {'train/loss': 1.6149983644485473, 'train/ce_loss': 0.437158203125, 'train/seg_cls_loss': 0.014178466796875, 'train/kl_loss': 0.2294921875, 'train/mask_bce_loss': 0.1264691723510623, 'train/mask_dice_loss': 0.44746065735816953, 'train/mask_loss': 0.5739298343658448, 'metrics/total_secs_per_batch': 8.812580347061157, 'metrics/data_secs_per_batch': 4.3198713779449465, '_timestamp': 1740960848.8664706}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 84 is less than current step: 499. Dropping entry: {'train/lr': 0.0001123469387755102, '_timestamp': 1740960848.8666646}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 85 is less than current step: 499. Dropping entry: {'train/loss': 1.6360337615013123, 'train/ce_loss': 0.347119140625, 'train/seg_cls_loss': 0.0106201171875, 'train/kl_loss': 0.22578125, 'train/mask_bce_loss': 0.1668078288435936, 'train/mask_dice_loss': 0.4637334406375885, 'train/mask_loss': 0.6305412799119949, 'metrics/total_secs_per_batch': 8.36269235610962, 'metrics/data_secs_per_batch': 3.8196435451507567, '_timestamp': 1740960857.2292478}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 85 is less than current step: 499. Dropping entry: {'train/lr': 0.00011222448979591836, '_timestamp': 1740960857.2295933}).
Epoch: [3][ 87/500]	Time  8.831 ( 8.831)	Loss 1.9658 (1.6108)	CeLoss 0.2002 (0.2238)	SegCLSLoss 0.0228 (0.0189)	KLLoss 0.3770 (0.3389)	MaskLoss 0.8579 (0.6716)	MaskBCELoss 0.1306 (0.0849)	MaskDICELoss 0.7273 (0.5866)
Epoch: [3][ 88/500]	Time  8.068 ( 8.068)	Loss 0.0913 (1.5824)	CeLoss 0.0913 (0.3272)	SegCLSLoss 0.0000 (0.0143)	KLLoss 0.0000 (0.2646)	MaskLoss 0.0000 (0.6110)	MaskBCELoss 0.0000 (0.1374)	MaskDICELoss 0.0000 (0.4735)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 86 is less than current step: 499. Dropping entry: {'train/loss': 1.6107822060585022, 'train/ce_loss': 0.223779296875, 'train/seg_cls_loss': 0.01888427734375, 'train/kl_loss': 0.3388671875, 'train/mask_bce_loss': 0.08494479116052389, 'train/mask_dice_loss': 0.586632850766182, 'train/mask_loss': 0.6715776413679123, 'metrics/total_secs_per_batch': 8.830800771713257, 'metrics/data_secs_per_batch': 4.005851769447327, '_timestamp': 1740960866.060086}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 86 is less than current step: 499. Dropping entry: {'train/lr': 0.00011210204081632652, '_timestamp': 1740960866.0603144}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 87 is less than current step: 499. Dropping entry: {'train/loss': 1.582441794872284, 'train/ce_loss': 0.327197265625, 'train/seg_cls_loss': 0.014263916015625, 'train/kl_loss': 0.2646484375, 'train/mask_bce_loss': 0.13743745610117913, 'train/mask_dice_loss': 0.47353442907333376, 'train/mask_loss': 0.6109718799591064, 'metrics/total_secs_per_batch': 8.067606210708618, 'metrics/data_secs_per_batch': 3.211711311340332, '_timestamp': 1740960874.1276639}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 87 is less than current step: 499. Dropping entry: {'train/lr': 0.00011197959183673468, '_timestamp': 1740960874.1279538}).
Epoch: [3][ 89/500]	Time  7.113 ( 7.113)	Loss 1.4688 (1.8131)	CeLoss 1.4688 (0.6162)	SegCLSLoss 0.0000 (0.0120)	KLLoss 0.0000 (0.2248)	MaskLoss 0.0000 (0.5843)	MaskBCELoss 0.0000 (0.1146)	MaskDICELoss 0.0000 (0.4697)
[2025-03-02 18:14:48,678] [INFO] [logging.py:96:log_dist] [Rank 0] step=1590, skipped=0, lr=[0.00011179591836734694], mom=[(0.9, 0.95)]
[2025-03-02 18:14:48,684] [INFO] [timer.py:215:stop] epoch=0/micro_step=15900/global_step=1590, RunningAvgSamplesPerSec=1.5699881123358363, CurrSamplesPerSec=1.3436816875162503, MemAllocated=30.95GB, MaxMemAllocated=37.19GB
Epoch: [3][ 90/500]	Time  7.444 ( 7.444)	Loss 2.4236 (1.8787)	CeLoss 0.1904 (0.2102)	SegCLSLoss 0.0242 (0.0160)	KLLoss 0.3965 (0.3461)	MaskLoss 1.0907 (0.8130)	MaskBCELoss 0.2241 (0.2011)	MaskDICELoss 0.8666 (0.6119)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 88 is less than current step: 499. Dropping entry: {'train/loss': 1.813096523284912, 'train/ce_loss': 0.6162109375, 'train/seg_cls_loss': 0.01197509765625, 'train/kl_loss': 0.2248046875, 'train/mask_bce_loss': 0.11455975328572095, 'train/mask_dice_loss': 0.4697228729724884, 'train/mask_loss': 0.584282636642456, 'metrics/total_secs_per_batch': 7.113408088684082, 'metrics/data_secs_per_batch': 3.1122390031814575, '_timestamp': 1740960881.241157}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 88 is less than current step: 499. Dropping entry: {'train/lr': 0.00011185714285714285, '_timestamp': 1740960881.2414925}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 89 is less than current step: 499. Dropping entry: {'train/loss': 1.8786794304847718, 'train/ce_loss': 0.21015625, 'train/seg_cls_loss': 0.015997314453125, 'train/kl_loss': 0.34609375, 'train/mask_bce_loss': 0.20105206817388535, 'train/mask_dice_loss': 0.6119204699993134, 'train/mask_loss': 0.8129725337028504, 'metrics/total_secs_per_batch': 7.443984746932983, 'metrics/data_secs_per_batch': 3.309014177322388, '_timestamp': 1740960888.6849062}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 89 is less than current step: 499. Dropping entry: {'train/lr': 0.00011173469387755102, '_timestamp': 1740960888.6852279}).
Epoch: [3][ 91/500]	Time  9.545 ( 9.545)	Loss 1.3525 (1.6199)	CeLoss 0.2207 (0.3271)	SegCLSLoss 0.0112 (0.0145)	KLLoss 0.3945 (0.3088)	MaskLoss 0.5434 (0.6274)	MaskBCELoss 0.1391 (0.1012)	MaskDICELoss 0.4044 (0.5262)
Epoch: [3][ 92/500]	Time  8.761 ( 8.761)	Loss 3.0380 (1.8837)	CeLoss 0.2490 (0.3848)	SegCLSLoss 0.0139 (0.0166)	KLLoss 0.3828 (0.3432)	MaskLoss 1.3716 (0.7281)	MaskBCELoss 0.6077 (0.1703)	MaskDICELoss 0.7638 (0.5578)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 90 is less than current step: 499. Dropping entry: {'train/loss': 1.6199127793312074, 'train/ce_loss': 0.327099609375, 'train/seg_cls_loss': 0.014471435546875, 'train/kl_loss': 0.3087890625, 'train/mask_bce_loss': 0.101188905374147, 'train/mask_dice_loss': 0.5261747062206268, 'train/mask_loss': 0.6273636132478714, 'metrics/total_secs_per_batch': 9.545221090316772, 'metrics/data_secs_per_batch': 3.7810698747634888, '_timestamp': 1740960898.2303145}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 90 is less than current step: 499. Dropping entry: {'train/lr': 0.00011161224489795919, '_timestamp': 1740960898.2306068}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 91 is less than current step: 499. Dropping entry: {'train/loss': 1.8836982190608977, 'train/ce_loss': 0.384765625, 'train/seg_cls_loss': 0.016571044921875, 'train/kl_loss': 0.3431640625, 'train/mask_bce_loss': 0.17025995599105953, 'train/mask_dice_loss': 0.5578196287155152, 'train/mask_loss': 0.7280795767903327, 'metrics/total_secs_per_batch': 8.760851383209229, 'metrics/data_secs_per_batch': 4.256567478179932, '_timestamp': 1740960906.9913025}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 91 is less than current step: 499. Dropping entry: {'train/lr': 0.00011148979591836734, '_timestamp': 1740960906.9916894}).
Epoch: [3][ 93/500]	Time  7.104 ( 7.104)	Loss 2.0601 (1.3267)	CeLoss 0.2539 (0.5561)	SegCLSLoss 0.0134 (0.0085)	KLLoss 0.3789 (0.2289)	MaskLoss 0.8816 (0.3720)	MaskBCELoss 0.1428 (0.0631)	MaskDICELoss 0.7388 (0.3089)
Epoch: [3][ 94/500]	Time  8.539 ( 8.539)	Loss 1.3618 (1.7594)	CeLoss 0.2676 (0.3464)	SegCLSLoss 0.0125 (0.0169)	KLLoss 0.3809 (0.3441)	MaskLoss 0.5246 (0.6850)	MaskBCELoss 0.0304 (0.0959)	MaskDICELoss 0.4942 (0.5891)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 92 is less than current step: 499. Dropping entry: {'train/loss': 1.3267142176628113, 'train/ce_loss': 0.5560546875, 'train/seg_cls_loss': 0.008526611328125, 'train/kl_loss': 0.22890625, 'train/mask_bce_loss': 0.06309804944321513, 'train/mask_dice_loss': 0.30890164226293565, 'train/mask_loss': 0.37199969291687013, 'metrics/total_secs_per_batch': 7.1042234897613525, 'metrics/data_secs_per_batch': 3.0707595348358154, '_timestamp': 1740960914.095332}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 92 is less than current step: 499. Dropping entry: {'train/lr': 0.0001113673469387755, '_timestamp': 1740960914.0956128}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 93 is less than current step: 499. Dropping entry: {'train/loss': 1.7594012379646302, 'train/ce_loss': 0.34638671875, 'train/seg_cls_loss': 0.01689453125, 'train/kl_loss': 0.344140625, 'train/mask_bce_loss': 0.09594653202220797, 'train/mask_dice_loss': 0.5890763632953167, 'train/mask_loss': 0.685022895783186, 'metrics/total_secs_per_batch': 8.539352893829346, 'metrics/data_secs_per_batch': 3.6454213857650757, '_timestamp': 1740960922.6348882}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 93 is less than current step: 499. Dropping entry: {'train/lr': 0.00011124489795918366, '_timestamp': 1740960922.63524}).
Epoch: [3][ 95/500]	Time  8.124 ( 8.124)	Loss 2.2696 (1.6455)	CeLoss 0.1914 (0.3895)	SegCLSLoss 0.0267 (0.0137)	KLLoss 0.3789 (0.2646)	MaskLoss 1.0132 (0.6112)	MaskBCELoss 0.0198 (0.1094)	MaskDICELoss 0.9934 (0.5019)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 94 is less than current step: 499. Dropping entry: {'train/loss': 1.6454609334468842, 'train/ce_loss': 0.389501953125, 'train/seg_cls_loss': 0.013702392578125, 'train/kl_loss': 0.2646484375, 'train/mask_bce_loss': 0.1093532731756568, 'train/mask_dice_loss': 0.501878160238266, 'train/mask_loss': 0.6112314254045487, 'metrics/total_secs_per_batch': 8.124014616012573, 'metrics/data_secs_per_batch': 3.1171664953231812, '_timestamp': 1740960930.7587423}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 94 is less than current step: 499. Dropping entry: {'train/lr': 0.00011112244897959182, '_timestamp': 1740960930.7590344}).
Epoch: [3][ 96/500]	Time  8.729 ( 8.729)	Loss 2.5381 (1.8942)	CeLoss 0.1787 (0.3461)	SegCLSLoss 0.0251 (0.0155)	KLLoss 0.3711 (0.3459)	MaskLoss 1.1548 (0.7529)	MaskBCELoss 0.1614 (0.1191)	MaskDICELoss 0.9934 (0.6338)
Epoch: [3][ 97/500]	Time  9.233 ( 9.233)	Loss 1.0547 (1.9871)	CeLoss 1.0547 (0.3292)	SegCLSLoss 0.0000 (0.0146)	KLLoss 0.0000 (0.3447)	MaskLoss 0.0000 (0.8081)	MaskBCELoss 0.0000 (0.1718)	MaskDICELoss 0.0000 (0.6363)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 95 is less than current step: 499. Dropping entry: {'train/loss': 1.894195008277893, 'train/ce_loss': 0.34609375, 'train/seg_cls_loss': 0.015533447265625, 'train/kl_loss': 0.3458984375, 'train/mask_bce_loss': 0.11910070385783911, 'train/mask_dice_loss': 0.6338073432445526, 'train/mask_loss': 0.7529080390930176, 'metrics/total_secs_per_batch': 8.728600025177002, 'metrics/data_secs_per_batch': 4.048764181137085, '_timestamp': 1740960939.487317}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 95 is less than current step: 499. Dropping entry: {'train/lr': 0.00011099999999999999, '_timestamp': 1740960939.4875135}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 96 is less than current step: 499. Dropping entry: {'train/loss': 1.987079131603241, 'train/ce_loss': 0.32919921875, 'train/seg_cls_loss': 0.0146240234375, 'train/kl_loss': 0.3447265625, 'train/mask_bce_loss': 0.17179245129227638, 'train/mask_dice_loss': 0.6363467156887055, 'train/mask_loss': 0.8081391841173172, 'metrics/total_secs_per_batch': 9.232992172241211, 'metrics/data_secs_per_batch': 4.373328900337219, '_timestamp': 1740960948.7202828}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 96 is less than current step: 499. Dropping entry: {'train/lr': 0.00011087755102040815, '_timestamp': 1740960948.7205536}).
Epoch: [3][ 98/500]	Time  7.721 ( 7.721)	Loss 0.9180 (1.7949)	CeLoss 0.9180 (0.4109)	SegCLSLoss 0.0000 (0.0154)	KLLoss 0.0000 (0.3020)	MaskLoss 0.0000 (0.6732)	MaskBCELoss 0.0000 (0.1038)	MaskDICELoss 0.0000 (0.5694)
Epoch: [3][ 99/500]	Time  8.044 ( 8.044)	Loss 2.2575 (1.8099)	CeLoss 0.1895 (0.3307)	SegCLSLoss 0.0248 (0.0181)	KLLoss 0.3867 (0.3041)	MaskLoss 1.0086 (0.7200)	MaskBCELoss 0.0099 (0.1075)	MaskDICELoss 0.9987 (0.6125)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 97 is less than current step: 499. Dropping entry: {'train/loss': 1.7948718160390853, 'train/ce_loss': 0.4109375, 'train/seg_cls_loss': 0.0154052734375, 'train/kl_loss': 0.301953125, 'train/mask_bce_loss': 0.10382394855841995, 'train/mask_dice_loss': 0.5693932056427002, 'train/mask_loss': 0.6732171460986137, 'metrics/total_secs_per_batch': 7.721439361572266, 'metrics/data_secs_per_batch': 3.5753598690032957, '_timestamp': 1740960956.4418511}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 97 is less than current step: 499. Dropping entry: {'train/lr': 0.00011075510204081631, '_timestamp': 1740960956.4421928}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 98 is less than current step: 499. Dropping entry: {'train/loss': 1.809947395324707, 'train/ce_loss': 0.3306640625, 'train/seg_cls_loss': 0.01807861328125, 'train/kl_loss': 0.3041015625, 'train/mask_bce_loss': 0.10749043459072709, 'train/mask_dice_loss': 0.6125223353505135, 'train/mask_loss': 0.7200127691030502, 'metrics/total_secs_per_batch': 8.044488668441772, 'metrics/data_secs_per_batch': 3.205013966560364, '_timestamp': 1740960964.486235}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 98 is less than current step: 499. Dropping entry: {'train/lr': 0.00011063265306122448, '_timestamp': 1740960964.4865103}).
[2025-03-02 18:16:13,408] [INFO] [logging.py:96:log_dist] [Rank 0] step=1600, skipped=0, lr=[0.00011057142857142856], mom=[(0.9, 0.95)]
[2025-03-02 18:16:13,414] [INFO] [timer.py:215:stop] epoch=0/micro_step=16000/global_step=1600, RunningAvgSamplesPerSec=1.56675207184511, CurrSamplesPerSec=1.1201330388723956, MemAllocated=31.24GB, MaxMemAllocated=37.19GB
Epoch: [3][100/500]	Time  8.929 ( 8.929)	Loss 1.9401 (1.8692)	CeLoss 0.3164 (0.3623)	SegCLSLoss 0.0146 (0.0154)	KLLoss 0.3789 (0.3412)	MaskLoss 0.7894 (0.7328)	MaskBCELoss 0.0681 (0.1710)	MaskDICELoss 0.7212 (0.5617)
Epoch: [3][101/500]	Time  6.992 ( 6.992)	Loss 1.1641 (1.4815)	CeLoss 1.1641 (0.5126)	SegCLSLoss 0.0000 (0.0114)	KLLoss 0.0000 (0.2260)	MaskLoss 0.0000 (0.4705)	MaskBCELoss 0.0000 (0.0991)	MaskDICELoss 0.0000 (0.3714)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 99 is less than current step: 499. Dropping entry: {'train/loss': 1.8692303597927094, 'train/ce_loss': 0.3623046875, 'train/seg_cls_loss': 0.015380859375, 'train/kl_loss': 0.3412109375, 'train/mask_bce_loss': 0.1710325201973319, 'train/mask_dice_loss': 0.5617271870374679, 'train/mask_loss': 0.7327597111463546, 'metrics/total_secs_per_batch': 8.929055452346802, 'metrics/data_secs_per_batch': 4.043035507202148, '_timestamp': 1740960973.4151201}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 99 is less than current step: 499. Dropping entry: {'train/lr': 0.00011051020408163264, '_timestamp': 1740960973.4153993}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 100 is less than current step: 499. Dropping entry: {'train/loss': 1.481452739238739, 'train/ce_loss': 0.51259765625, 'train/seg_cls_loss': 0.01142578125, 'train/kl_loss': 0.2259765625, 'train/mask_bce_loss': 0.09913817662745714, 'train/mask_dice_loss': 0.3713733494281769, 'train/mask_loss': 0.4705115258693695, 'metrics/total_secs_per_batch': 6.991928339004517, 'metrics/data_secs_per_batch': 3.2612514734268188, '_timestamp': 1740960980.40722}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 100 is less than current step: 499. Dropping entry: {'train/lr': 0.00011038775510204082, '_timestamp': 1740960980.4074938}).
Epoch: [3][102/500]	Time  8.258 ( 8.258)	Loss 2.2045 (1.7537)	CeLoss 0.2139 (0.3927)	SegCLSLoss 0.0250 (0.0156)	KLLoss 0.3691 (0.3023)	MaskLoss 0.9704 (0.6615)	MaskBCELoss 0.1341 (0.1258)	MaskDICELoss 0.8363 (0.5356)
Epoch: [3][103/500]	Time 10.388 (10.388)	Loss 2.1208 (1.7398)	CeLoss 0.2891 (0.2395)	SegCLSLoss 0.0121 (0.0145)	KLLoss 0.3750 (0.3396)	MaskLoss 0.8944 (0.7298)	MaskBCELoss 0.3194 (0.1049)	MaskDICELoss 0.5750 (0.6249)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 101 is less than current step: 499. Dropping entry: {'train/loss': 1.75369735956192, 'train/ce_loss': 0.39267578125, 'train/seg_cls_loss': 0.015570068359375, 'train/kl_loss': 0.30234375, 'train/mask_bce_loss': 0.12583557586185634, 'train/mask_dice_loss': 0.5356322448700667, 'train/mask_loss': 0.6614678204059601, 'metrics/total_secs_per_batch': 8.258371353149414, 'metrics/data_secs_per_batch': 3.188566207885742, '_timestamp': 1740960988.6656065}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 101 is less than current step: 499. Dropping entry: {'train/lr': 0.00011026530612244898, '_timestamp': 1740960988.665886}).
Epoch: [3][104/500]	Time  7.411 ( 7.411)	Loss 1.2188 (1.3806)	CeLoss 1.2188 (0.4890)	SegCLSLoss 0.0000 (0.0079)	KLLoss 0.0000 (0.2260)	MaskLoss 0.0000 (0.4326)	MaskBCELoss 0.0000 (0.0810)	MaskDICELoss 0.0000 (0.3516)
Epoch: [3][105/500]	Time  8.169 ( 8.169)	Loss 1.7194 (1.8531)	CeLoss 0.2617 (0.4062)	SegCLSLoss 0.0103 (0.0144)	KLLoss 0.3887 (0.3027)	MaskLoss 0.7064 (0.7047)	MaskBCELoss 0.0717 (0.1456)	MaskDICELoss 0.6347 (0.5591)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 102 is less than current step: 499. Dropping entry: {'train/loss': 1.7397806763648986, 'train/ce_loss': 0.239453125, 'train/seg_cls_loss': 0.014495849609375, 'train/kl_loss': 0.3396484375, 'train/mask_bce_loss': 0.10485725738108158, 'train/mask_dice_loss': 0.6248963624238968, 'train/mask_loss': 0.7297536224126816, 'metrics/total_secs_per_batch': 10.388097763061523, 'metrics/data_secs_per_batch': 5.07146806716919, '_timestamp': 1740960999.053732}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 102 is less than current step: 499. Dropping entry: {'train/lr': 0.00011014285714285714, '_timestamp': 1740960999.0540273}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 103 is less than current step: 499. Dropping entry: {'train/loss': 1.3805687367916106, 'train/ce_loss': 0.48896484375, 'train/seg_cls_loss': 0.00794677734375, 'train/kl_loss': 0.2259765625, 'train/mask_bce_loss': 0.08101945035159588, 'train/mask_dice_loss': 0.3515500783920288, 'train/mask_loss': 0.43256953060626985, 'metrics/total_secs_per_batch': 7.411161184310913, 'metrics/data_secs_per_batch': 3.4811628818511964, '_timestamp': 1740961006.4650023}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 103 is less than current step: 499. Dropping entry: {'train/lr': 0.0001100204081632653, '_timestamp': 1740961006.4653158}).
Epoch: [3][106/500]	Time  6.423 ( 6.423)	Loss 1.6094 (1.9517)	CeLoss 1.6094 (0.5031)	SegCLSLoss 0.0000 (0.0176)	KLLoss 0.0000 (0.2635)	MaskLoss 0.0000 (0.7066)	MaskBCELoss 0.0000 (0.1184)	MaskDICELoss 0.0000 (0.5882)
Epoch: [3][107/500]	Time  7.758 ( 7.758)	Loss 1.4582 (1.6838)	CeLoss 0.2617 (0.6227)	SegCLSLoss 0.0096 (0.0118)	KLLoss 0.3867 (0.2277)	MaskLoss 0.5767 (0.5164)	MaskBCELoss 0.2572 (0.0985)	MaskDICELoss 0.3195 (0.4179)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 104 is less than current step: 499. Dropping entry: {'train/loss': 1.8530994296073913, 'train/ce_loss': 0.40615234375, 'train/seg_cls_loss': 0.014410400390625, 'train/kl_loss': 0.302734375, 'train/mask_bce_loss': 0.14557656664401292, 'train/mask_dice_loss': 0.5590981498360634, 'train/mask_loss': 0.7046747207641602, 'metrics/total_secs_per_batch': 8.169434547424316, 'metrics/data_secs_per_batch': 3.520573854446411, '_timestamp': 1740961014.6343334}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 104 is less than current step: 499. Dropping entry: {'train/lr': 0.00010989795918367347, '_timestamp': 1740961014.6345992}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 105 is less than current step: 499. Dropping entry: {'train/loss': 1.9516952276229858, 'train/ce_loss': 0.503125, 'train/seg_cls_loss': 0.01756591796875, 'train/kl_loss': 0.2634765625, 'train/mask_bce_loss': 0.11840019132941962, 'train/mask_dice_loss': 0.5882091283798218, 'train/mask_loss': 0.7066093206405639, 'metrics/total_secs_per_batch': 6.422766923904419, 'metrics/data_secs_per_batch': 3.3204556703567505, '_timestamp': 1740961021.0570693}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 105 is less than current step: 499. Dropping entry: {'train/lr': 0.00010977551020408162, '_timestamp': 1740961021.0573614}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 106 is less than current step: 499. Dropping entry: {'train/loss': 1.6837931632995606, 'train/ce_loss': 0.62265625, 'train/seg_cls_loss': 0.01182861328125, 'train/kl_loss': 0.227734375, 'train/mask_bce_loss': 0.09848603662103414, 'train/mask_dice_loss': 0.4178734302520752, 'train/mask_loss': 0.5163594722747803, 'metrics/total_secs_per_batch': 7.757605791091919, 'metrics/data_secs_per_batch': 3.583296465873718, '_timestamp': 1740961028.8147905}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 106 is less than current step: 499. Dropping entry: {'train/lr': 0.00010965306122448978, '_timestamp': 1740961028.8150964}).
Epoch: [3][108/500]	Time  8.424 ( 8.424)	Loss 2.3160 (1.8049)	CeLoss 0.2578 (0.3920)	SegCLSLoss 0.0168 (0.0114)	KLLoss 0.3750 (0.2629)	MaskLoss 1.0066 (0.6904)	MaskBCELoss 0.0086 (0.1790)	MaskDICELoss 0.9980 (0.5114)
Epoch: [3][109/500]	Time  6.097 ( 6.097)	Loss 1.5585 (1.5309)	CeLoss 0.1680 (0.6165)	SegCLSLoss 0.0254 (0.0121)	KLLoss 0.3613 (0.2623)	MaskLoss 0.6709 (0.4409)	MaskBCELoss 0.0419 (0.0813)	MaskDICELoss 0.6290 (0.3596)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 107 is less than current step: 499. Dropping entry: {'train/loss': 1.8049014925956726, 'train/ce_loss': 0.3919921875, 'train/seg_cls_loss': 0.011358642578125, 'train/kl_loss': 0.262890625, 'train/mask_bce_loss': 0.17900804206728935, 'train/mask_dice_loss': 0.5114309787750244, 'train/mask_loss': 0.6904390260577202, 'metrics/total_secs_per_batch': 8.424365043640137, 'metrics/data_secs_per_batch': 3.907586145401001, '_timestamp': 1740961037.2391899}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 107 is less than current step: 499. Dropping entry: {'train/lr': 0.00010953061224489794, '_timestamp': 1740961037.2395096}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 108 is less than current step: 499. Dropping entry: {'train/loss': 1.5309173345565796, 'train/ce_loss': 0.61650390625, 'train/seg_cls_loss': 0.012109375, 'train/kl_loss': 0.2623046875, 'train/mask_bce_loss': 0.0813020944595337, 'train/mask_dice_loss': 0.35964485257864, 'train/mask_loss': 0.44094694703817366, 'metrics/total_secs_per_batch': 6.096625089645386, 'metrics/data_secs_per_batch': 2.5927497148513794, '_timestamp': 1740961043.3357801}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 108 is less than current step: 499. Dropping entry: {'train/lr': 0.00010940816326530611, '_timestamp': 1740961043.3360987}).
[2025-03-02 18:17:30,811] [INFO] [logging.py:96:log_dist] [Rank 0] step=1610, skipped=0, lr=[0.00010934693877551019], mom=[(0.9, 0.95)]
[2025-03-02 18:17:30,817] [INFO] [timer.py:215:stop] epoch=0/micro_step=16100/global_step=1610, RunningAvgSamplesPerSec=1.5646840841178835, CurrSamplesPerSec=1.3367002736053273, MemAllocated=31.23GB, MaxMemAllocated=37.19GB
Epoch: [3][110/500]	Time  7.483 ( 7.483)	Loss 2.0798 (1.7455)	CeLoss 0.2695 (0.6904)	SegCLSLoss 0.0148 (0.0106)	KLLoss 0.3789 (0.2273)	MaskLoss 0.8827 (0.5135)	MaskBCELoss 0.2785 (0.1138)	MaskDICELoss 0.6042 (0.3996)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 109 is less than current step: 499. Dropping entry: {'train/loss': 1.7455256938934327, 'train/ce_loss': 0.6904296875, 'train/seg_cls_loss': 0.01063232421875, 'train/kl_loss': 0.22734375, 'train/mask_bce_loss': 0.11383848302066327, 'train/mask_dice_loss': 0.3996470123529434, 'train/mask_loss': 0.5134855031967163, 'metrics/total_secs_per_batch': 7.482757806777954, 'metrics/data_secs_per_batch': 3.2898213148117064, '_timestamp': 1740961050.8183074}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 109 is less than current step: 499. Dropping entry: {'train/lr': 0.00010928571428571427, '_timestamp': 1740961050.8186135}).
Epoch: [3][111/500]	Time  8.719 ( 8.719)	Loss 1.8423 (1.5433)	CeLoss 0.2080 (0.2793)	SegCLSLoss 0.0211 (0.0129)	KLLoss 0.3633 (0.2631)	MaskLoss 0.7932 (0.6157)	MaskBCELoss 0.0838 (0.1236)	MaskDICELoss 0.7094 (0.4921)
Epoch: [3][112/500]	Time  9.093 ( 9.093)	Loss 2.0181 (2.0417)	CeLoss 0.2148 (0.2895)	SegCLSLoss 0.0260 (0.0173)	KLLoss 0.3691 (0.3377)	MaskLoss 0.8772 (0.8550)	MaskBCELoss 0.2701 (0.2518)	MaskDICELoss 0.6071 (0.6032)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 110 is less than current step: 499. Dropping entry: {'train/loss': 1.5433103084564208, 'train/ce_loss': 0.279296875, 'train/seg_cls_loss': 0.012933349609375, 'train/kl_loss': 0.2630859375, 'train/mask_bce_loss': 0.12364494502544403, 'train/mask_dice_loss': 0.4920531719923019, 'train/mask_loss': 0.615698117017746, 'metrics/total_secs_per_batch': 8.719285249710083, 'metrics/data_secs_per_batch': 3.691089940071106, '_timestamp': 1740961059.5377386}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 110 is less than current step: 499. Dropping entry: {'train/lr': 0.00010916326530612243, '_timestamp': 1740961059.5381088}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 111 is less than current step: 499. Dropping entry: {'train/loss': 2.041716343164444, 'train/ce_loss': 0.289453125, 'train/seg_cls_loss': 0.017266845703125, 'train/kl_loss': 0.3376953125, 'train/mask_bce_loss': 0.2518369069322944, 'train/mask_dice_loss': 0.6031521178781987, 'train/mask_loss': 0.8549890108406544, 'metrics/total_secs_per_batch': 9.092665195465088, 'metrics/data_secs_per_batch': 3.991576981544495, '_timestamp': 1740961068.6306}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 111 is less than current step: 499. Dropping entry: {'train/lr': 0.0001090408163265306, '_timestamp': 1740961068.6309426}).
Epoch: [3][113/500]	Time  8.513 ( 8.513)	Loss 1.5203 (1.6293)	CeLoss 0.2324 (0.3527)	SegCLSLoss 0.0151 (0.0146)	KLLoss 0.3809 (0.3408)	MaskLoss 0.6215 (0.6177)	MaskBCELoss 0.2111 (0.1188)	MaskDICELoss 0.4104 (0.4990)
Epoch: [3][114/500]	Time  7.401 ( 7.401)	Loss 2.2927 (1.6139)	CeLoss 0.2266 (0.4609)	SegCLSLoss 0.0126 (0.0146)	KLLoss 0.3828 (0.2996)	MaskLoss 1.0106 (0.5578)	MaskBCELoss 0.0108 (0.0538)	MaskDICELoss 0.9998 (0.5040)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 112 is less than current step: 499. Dropping entry: {'train/loss': 1.6293248891830445, 'train/ce_loss': 0.352734375, 'train/seg_cls_loss': 0.014593505859375, 'train/kl_loss': 0.3408203125, 'train/mask_bce_loss': 0.11878115832805633, 'train/mask_dice_loss': 0.4989574566483498, 'train/mask_loss': 0.6177386075258255, 'metrics/total_secs_per_batch': 8.512730598449707, 'metrics/data_secs_per_batch': 4.078070974349975, '_timestamp': 1740961077.1431875}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 112 is less than current step: 499. Dropping entry: {'train/lr': 0.00010891836734693877, '_timestamp': 1740961077.143458}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 113 is less than current step: 499. Dropping entry: {'train/loss': 1.6139382719993591, 'train/ce_loss': 0.4609375, 'train/seg_cls_loss': 0.01458740234375, 'train/kl_loss': 0.299609375, 'train/mask_bce_loss': 0.05375660602003336, 'train/mask_dice_loss': 0.5040426194667816, 'train/mask_loss': 0.5577992260456085, 'metrics/total_secs_per_batch': 7.4005279541015625, 'metrics/data_secs_per_batch': 3.197866940498352, '_timestamp': 1740961084.5436666}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 113 is less than current step: 499. Dropping entry: {'train/lr': 0.00010879591836734694, '_timestamp': 1740961084.5439343}).
Epoch: [3][115/500]	Time  6.292 ( 6.292)	Loss 2.0002 (1.8953)	CeLoss 0.2100 (0.5992)	SegCLSLoss 0.0248 (0.0141)	KLLoss 0.3672 (0.2627)	MaskLoss 0.8702 (0.6313)	MaskBCELoss 0.0667 (0.1704)	MaskDICELoss 0.8035 (0.4608)
Epoch: [3][116/500]	Time  6.384 ( 6.384)	Loss 2.5354 (1.8825)	CeLoss 0.2949 (0.3028)	SegCLSLoss 0.0134 (0.0171)	KLLoss 0.3730 (0.3031)	MaskLoss 1.0988 (0.7705)	MaskBCELoss 0.2161 (0.1545)	MaskDICELoss 0.8827 (0.6160)
Epoch: [3][117/500]	Time  6.378 ( 6.378)	Loss 0.4688 (1.3676)	CeLoss 0.4688 (0.6048)	SegCLSLoss 0.0000 (0.0108)	KLLoss 0.0000 (0.1846)	MaskLoss 0.0000 (0.3694)	MaskBCELoss 0.0000 (0.0680)	MaskDICELoss 0.0000 (0.3015)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 114 is less than current step: 499. Dropping entry: {'train/loss': 1.8953215718269347, 'train/ce_loss': 0.59921875, 'train/seg_cls_loss': 0.0140869140625, 'train/kl_loss': 0.2626953125, 'train/mask_bce_loss': 0.17043633554130794, 'train/mask_dice_loss': 0.46081818640232086, 'train/mask_loss': 0.6312545299530029, 'metrics/total_secs_per_batch': 6.291696310043335, 'metrics/data_secs_per_batch': 2.2039771556854246, '_timestamp': 1740961090.8353841}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 114 is less than current step: 499. Dropping entry: {'train/lr': 0.0001086734693877551, '_timestamp': 1740961090.8356647}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 115 is less than current step: 499. Dropping entry: {'train/loss': 1.8824935615062715, 'train/ce_loss': 0.302783203125, 'train/seg_cls_loss': 0.0171142578125, 'train/kl_loss': 0.303125, 'train/mask_bce_loss': 0.15448087267577648, 'train/mask_dice_loss': 0.6160383552312851, 'train/mask_loss': 0.7705192312598228, 'metrics/total_secs_per_batch': 6.38370418548584, 'metrics/data_secs_per_batch': 3.043159317970276, '_timestamp': 1740961097.2192872}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 115 is less than current step: 499. Dropping entry: {'train/lr': 0.00010855102040816326, '_timestamp': 1740961097.2196305}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 116 is less than current step: 499. Dropping entry: {'train/loss': 1.3676209449768066, 'train/ce_loss': 0.60478515625, 'train/seg_cls_loss': 0.010791015625, 'train/kl_loss': 0.1845703125, 'train/mask_bce_loss': 0.06795590221881867, 'train/mask_dice_loss': 0.30145027935504914, 'train/mask_loss': 0.36940617859363556, 'metrics/total_secs_per_batch': 6.3783674240112305, 'metrics/data_secs_per_batch': 2.7147679805755613, '_timestamp': 1740961103.59745}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 116 is less than current step: 499. Dropping entry: {'train/lr': 0.00010842857142857143, '_timestamp': 1740961103.597722}).
Epoch: [3][118/500]	Time  7.762 ( 7.762)	Loss 1.4880 (1.7135)	CeLoss 0.1875 (0.2768)	SegCLSLoss 0.0154 (0.0165)	KLLoss 0.3691 (0.3389)	MaskLoss 0.6278 (0.6973)	MaskBCELoss 0.1225 (0.2134)	MaskDICELoss 0.5053 (0.4839)
Epoch: [3][119/500]	Time  8.125 ( 8.125)	Loss 1.7272 (1.7275)	CeLoss 0.1914 (0.4614)	SegCLSLoss 0.0198 (0.0128)	KLLoss 0.3789 (0.2619)	MaskLoss 0.7440 (0.6168)	MaskBCELoss 0.0215 (0.1177)	MaskDICELoss 0.7224 (0.4992)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 117 is less than current step: 499. Dropping entry: {'train/loss': 1.713503932952881, 'train/ce_loss': 0.2767578125, 'train/seg_cls_loss': 0.01649169921875, 'train/kl_loss': 0.3388671875, 'train/mask_bce_loss': 0.21342591159045696, 'train/mask_dice_loss': 0.48385338559746743, 'train/mask_loss': 0.6972793102264404, 'metrics/total_secs_per_batch': 7.762185096740723, 'metrics/data_secs_per_batch': 3.806858515739441, '_timestamp': 1740961111.3596408}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 117 is less than current step: 499. Dropping entry: {'train/lr': 0.00010830612244897959, '_timestamp': 1740961111.3599432}).
[2025-03-02 18:18:49,654] [INFO] [logging.py:96:log_dist] [Rank 0] step=1620, skipped=0, lr=[0.00010812244897959182], mom=[(0.9, 0.95)]
[2025-03-02 18:18:49,660] [INFO] [timer.py:215:stop] epoch=0/micro_step=16200/global_step=1620, RunningAvgSamplesPerSec=1.5624297867214898, CurrSamplesPerSec=0.9827826798536514, MemAllocated=31.14GB, MaxMemAllocated=37.19GB
Epoch: [3][120/500]	Time 10.177 (10.177)	Loss 2.9098 (1.5431)	CeLoss 0.1309 (0.2368)	SegCLSLoss 0.0378 (0.0152)	KLLoss 0.3711 (0.2994)	MaskLoss 1.3616 (0.6343)	MaskBCELoss 0.4438 (0.1268)	MaskDICELoss 0.9179 (0.5075)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 118 is less than current step: 499. Dropping entry: {'train/loss': 1.7274947047233582, 'train/ce_loss': 0.46142578125, 'train/seg_cls_loss': 0.01279296875, 'train/kl_loss': 0.2619140625, 'train/mask_bce_loss': 0.11767234746366739, 'train/mask_dice_loss': 0.49915116727352143, 'train/mask_loss': 0.6168235242366791, 'metrics/total_secs_per_batch': 8.124711036682129, 'metrics/data_secs_per_batch': 3.2606157541275023, '_timestamp': 1740961119.4842892}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 118 is less than current step: 499. Dropping entry: {'train/lr': 0.00010818367346938774, '_timestamp': 1740961119.4845362}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 119 is less than current step: 499. Dropping entry: {'train/loss': 1.543093591928482, 'train/ce_loss': 0.23681640625, 'train/seg_cls_loss': 0.015185546875, 'train/kl_loss': 0.2994140625, 'train/mask_bce_loss': 0.1268087294884026, 'train/mask_dice_loss': 0.5075310364365577, 'train/mask_loss': 0.6343397632241249, 'metrics/total_secs_per_batch': 10.176648616790771, 'metrics/data_secs_per_batch': 4.972686219215393, '_timestamp': 1740961129.660831}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 119 is less than current step: 499. Dropping entry: {'train/lr': 0.0001080612244897959, '_timestamp': 1740961129.6611116}).
Epoch: [3][121/500]	Time  9.459 ( 9.459)	Loss 1.9018 (1.5362)	CeLoss 0.2617 (0.2940)	SegCLSLoss 0.0168 (0.0147)	KLLoss 0.3789 (0.3016)	MaskLoss 0.7976 (0.6023)	MaskBCELoss 0.2916 (0.0995)	MaskDICELoss 0.5060 (0.5028)
Epoch: [3][122/500]	Time  8.378 ( 8.378)	Loss 1.6264 (1.3988)	CeLoss 0.2129 (0.2873)	SegCLSLoss 0.0193 (0.0135)	KLLoss 0.3809 (0.2641)	MaskLoss 0.6824 (0.5389)	MaskBCELoss 0.0147 (0.0710)	MaskDICELoss 0.6676 (0.4680)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 120 is less than current step: 499. Dropping entry: {'train/loss': 1.536166524887085, 'train/ce_loss': 0.29404296875, 'train/seg_cls_loss': 0.01468505859375, 'train/kl_loss': 0.3015625, 'train/mask_bce_loss': 0.09946213075891137, 'train/mask_dice_loss': 0.5028496474027634, 'train/mask_loss': 0.6023117810487747, 'metrics/total_secs_per_batch': 9.45923376083374, 'metrics/data_secs_per_batch': 4.438207888603211, '_timestamp': 1740961139.1202424}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 120 is less than current step: 499. Dropping entry: {'train/lr': 0.00010793877551020406, '_timestamp': 1740961139.1205058}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 121 is less than current step: 499. Dropping entry: {'train/loss': 1.3987944364547729, 'train/ce_loss': 0.2873046875, 'train/seg_cls_loss': 0.01348876953125, 'train/kl_loss': 0.2640625, 'train/mask_bce_loss': 0.07098966194316744, 'train/mask_dice_loss': 0.46795834004879, 'train/mask_loss': 0.5389479964971542, 'metrics/total_secs_per_batch': 8.377676248550415, 'metrics/data_secs_per_batch': 3.735616612434387, '_timestamp': 1740961147.498135}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 121 is less than current step: 499. Dropping entry: {'train/lr': 0.00010781632653061223, '_timestamp': 1740961147.4984825}).
Epoch: [3][123/500]	Time  7.244 ( 7.244)	Loss 1.3935 (1.9160)	CeLoss 0.2363 (0.3281)	SegCLSLoss 0.0162 (0.0198)	KLLoss 0.3770 (0.3381)	MaskLoss 0.5561 (0.7720)	MaskBCELoss 0.4167 (0.1868)	MaskDICELoss 0.1395 (0.5852)
Epoch: [3][124/500]	Time  7.429 ( 7.429)	Loss 1.6509 (1.5491)	CeLoss 0.2217 (0.4794)	SegCLSLoss 0.0231 (0.0127)	KLLoss 0.3652 (0.2240)	MaskLoss 0.6907 (0.5205)	MaskBCELoss 0.0202 (0.0925)	MaskDICELoss 0.6705 (0.4280)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 122 is less than current step: 499. Dropping entry: {'train/loss': 1.9160086870193482, 'train/ce_loss': 0.328076171875, 'train/seg_cls_loss': 0.0197998046875, 'train/kl_loss': 0.3380859375, 'train/mask_bce_loss': 0.1867599805817008, 'train/mask_dice_loss': 0.5852091878652572, 'train/mask_loss': 0.7719691693782806, 'metrics/total_secs_per_batch': 7.2444751262664795, 'metrics/data_secs_per_batch': 3.3020954847335817, '_timestamp': 1740961154.7423954}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 122 is less than current step: 499. Dropping entry: {'train/lr': 0.00010769387755102039, '_timestamp': 1740961154.7426565}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 123 is less than current step: 499. Dropping entry: {'train/loss': 1.5490859508514405, 'train/ce_loss': 0.47939453125, 'train/seg_cls_loss': 0.012738037109375, 'train/kl_loss': 0.2240234375, 'train/mask_bce_loss': 0.09252378270030022, 'train/mask_dice_loss': 0.4280152887105942, 'train/mask_loss': 0.5205390632152558, 'metrics/total_secs_per_batch': 7.429479122161865, 'metrics/data_secs_per_batch': 3.4506449937820434, '_timestamp': 1740961162.1718671}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 123 is less than current step: 499. Dropping entry: {'train/lr': 0.00010757142857142857, '_timestamp': 1740961162.1721807}).
Epoch: [3][125/500]	Time  8.327 ( 8.327)	Loss 1.4688 (1.7356)	CeLoss 1.4688 (0.4372)	SegCLSLoss 0.0000 (0.0136)	KLLoss 0.0000 (0.2990)	MaskLoss 0.0000 (0.6308)	MaskBCELoss 0.0000 (0.1221)	MaskDICELoss 0.0000 (0.5087)
Epoch: [3][126/500]	Time  7.793 ( 7.793)	Loss 1.5008 (1.4224)	CeLoss 0.2812 (0.4167)	SegCLSLoss 0.0124 (0.0102)	KLLoss 0.3848 (0.2652)	MaskLoss 0.5873 (0.4871)	MaskBCELoss 0.0691 (0.0986)	MaskDICELoss 0.5182 (0.3885)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 124 is less than current step: 499. Dropping entry: {'train/loss': 1.7356379449367523, 'train/ce_loss': 0.43720703125, 'train/seg_cls_loss': 0.013623046875, 'train/kl_loss': 0.2990234375, 'train/mask_bce_loss': 0.12206438472494482, 'train/mask_dice_loss': 0.5087428659200668, 'train/mask_loss': 0.6308072477579116, 'metrics/total_secs_per_batch': 8.326554298400879, 'metrics/data_secs_per_batch': 4.094697880744934, '_timestamp': 1740961170.4984574}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 124 is less than current step: 499. Dropping entry: {'train/lr': 0.00010744897959183673, '_timestamp': 1740961170.4987335}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 125 is less than current step: 499. Dropping entry: {'train/loss': 1.4224474668502807, 'train/ce_loss': 0.41669921875, 'train/seg_cls_loss': 0.01024169921875, 'train/kl_loss': 0.265234375, 'train/mask_bce_loss': 0.09857430681586266, 'train/mask_dice_loss': 0.3885283231735229, 'train/mask_loss': 0.4871026337146759, 'metrics/total_secs_per_batch': 7.792884588241577, 'metrics/data_secs_per_batch': 3.545425033569336, '_timestamp': 1740961178.291346}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 125 is less than current step: 499. Dropping entry: {'train/lr': 0.00010732653061224489, '_timestamp': 1740961178.2916157}).
Epoch: [3][127/500]	Time  8.063 ( 8.063)	Loss 1.2422 (1.9349)	CeLoss 1.2422 (0.4870)	SegCLSLoss 0.0000 (0.0135)	KLLoss 0.0000 (0.2602)	MaskLoss 0.0000 (0.7075)	MaskBCELoss 0.0000 (0.1768)	MaskDICELoss 0.0000 (0.5307)
Epoch: [3][128/500]	Time  8.142 ( 8.142)	Loss 2.6026 (1.9362)	CeLoss 0.1846 (0.3691)	SegCLSLoss 0.0261 (0.0146)	KLLoss 0.3770 (0.2643)	MaskLoss 1.1841 (0.7667)	MaskBCELoss 0.2974 (0.1672)	MaskDICELoss 0.8867 (0.5995)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 126 is less than current step: 499. Dropping entry: {'train/loss': 1.9349103212356566, 'train/ce_loss': 0.48701171875, 'train/seg_cls_loss': 0.01346435546875, 'train/kl_loss': 0.26015625, 'train/mask_bce_loss': 0.17677178755402564, 'train/mask_dice_loss': 0.5307224303483963, 'train/mask_loss': 0.7074942231178284, 'metrics/total_secs_per_batch': 8.062947034835815, 'metrics/data_secs_per_batch': 3.893600678443909, '_timestamp': 1740961186.354272}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 126 is less than current step: 499. Dropping entry: {'train/lr': 0.00010720408163265306, '_timestamp': 1740961186.354541}).
Epoch: [3][129/500]	Time  8.327 ( 8.327)	Loss 4.2563 (2.0429)	CeLoss 0.2158 (0.3301)	SegCLSLoss 0.0148 (0.0162)	KLLoss 0.3711 (0.3422)	MaskLoss 1.9973 (0.8353)	MaskBCELoss 1.3985 (0.3186)	MaskDICELoss 0.5988 (0.5167)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 127 is less than current step: 499. Dropping entry: {'train/loss': 1.9362382888793945, 'train/ce_loss': 0.369140625, 'train/seg_cls_loss': 0.0146484375, 'train/kl_loss': 0.2642578125, 'train/mask_bce_loss': 0.16716000512242318, 'train/mask_dice_loss': 0.5995431065559387, 'train/mask_loss': 0.7667031228542328, 'metrics/total_secs_per_batch': 8.142498254776001, 'metrics/data_secs_per_batch': 3.9023897886276244, '_timestamp': 1740961194.4966903}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 127 is less than current step: 499. Dropping entry: {'train/lr': 0.00010708163265306122, '_timestamp': 1740961194.4969106}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 128 is less than current step: 499. Dropping entry: {'train/loss': 2.042870819568634, 'train/ce_loss': 0.330078125, 'train/seg_cls_loss': 0.01617431640625, 'train/kl_loss': 0.3421875, 'train/mask_bce_loss': 0.3185517663136125, 'train/mask_dice_loss': 0.5167020231485366, 'train/mask_loss': 0.835253781080246, 'metrics/total_secs_per_batch': 8.326804161071777, 'metrics/data_secs_per_batch': 3.6668912887573244, '_timestamp': 1740961202.8235753}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 128 is less than current step: 499. Dropping entry: {'train/lr': 0.00010695918367346938, '_timestamp': 1740961202.823837}).
[2025-03-02 18:20:10,587] [INFO] [logging.py:96:log_dist] [Rank 0] step=1630, skipped=0, lr=[0.00010689795918367346], mom=[(0.9, 0.95)]
[2025-03-02 18:20:10,593] [INFO] [timer.py:215:stop] epoch=0/micro_step=16300/global_step=1630, RunningAvgSamplesPerSec=1.559896986768146, CurrSamplesPerSec=1.2871976298375705, MemAllocated=31.23GB, MaxMemAllocated=37.19GB
Epoch: [3][130/500]	Time  7.770 ( 7.770)	Loss 2.1348 (1.6204)	CeLoss 0.2412 (0.4141)	SegCLSLoss 0.0125 (0.0112)	KLLoss 0.3750 (0.2662)	MaskLoss 0.9248 (0.5870)	MaskBCELoss 0.4897 (0.2111)	MaskDICELoss 0.4351 (0.3759)
Epoch: [3][131/500]	Time  8.369 ( 8.369)	Loss 1.7912 (1.1835)	CeLoss 0.2871 (0.2363)	SegCLSLoss 0.0117 (0.0108)	KLLoss 0.3906 (0.2273)	MaskLoss 0.7296 (0.4595)	MaskBCELoss 0.0772 (0.0466)	MaskDICELoss 0.6524 (0.4130)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 129 is less than current step: 499. Dropping entry: {'train/loss': 1.6204118371009826, 'train/ce_loss': 0.414111328125, 'train/seg_cls_loss': 0.011187744140625, 'train/kl_loss': 0.2662109375, 'train/mask_bce_loss': 0.21110988445580006, 'train/mask_dice_loss': 0.37592709735035895, 'train/mask_loss': 0.587036969512701, 'metrics/total_secs_per_batch': 7.7703142166137695, 'metrics/data_secs_per_batch': 3.2580776691436766, '_timestamp': 1740961210.5938187}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 129 is less than current step: 499. Dropping entry: {'train/lr': 0.00010683673469387754, '_timestamp': 1740961210.59416}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 130 is less than current step: 499. Dropping entry: {'train/loss': 1.1835420727729797, 'train/ce_loss': 0.236279296875, 'train/seg_cls_loss': 0.01080322265625, 'train/kl_loss': 0.22734375, 'train/mask_bce_loss': 0.04656072147190571, 'train/mask_dice_loss': 0.4129593431949615, 'train/mask_loss': 0.45952006578445437, 'metrics/total_secs_per_batch': 8.36892557144165, 'metrics/data_secs_per_batch': 3.549527621269226, '_timestamp': 1740961218.9628475}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 130 is less than current step: 499. Dropping entry: {'train/lr': 0.00010671428571428571, '_timestamp': 1740961218.9631171}).
Epoch: [3][132/500]	Time  8.217 ( 8.217)	Loss 1.6007 (1.6326)	CeLoss 0.2578 (0.2430)	SegCLSLoss 0.0117 (0.0162)	KLLoss 0.3848 (0.3795)	MaskLoss 0.6490 (0.6717)	MaskBCELoss 0.2434 (0.1390)	MaskDICELoss 0.4056 (0.5327)
Epoch: [3][133/500]	Time  8.020 ( 8.020)	Loss 2.0501 (1.6196)	CeLoss 0.2598 (0.5608)	SegCLSLoss 0.0117 (0.0106)	KLLoss 0.3848 (0.2281)	MaskLoss 0.8727 (0.5154)	MaskBCELoss 0.2105 (0.0686)	MaskDICELoss 0.6622 (0.4469)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 131 is less than current step: 499. Dropping entry: {'train/loss': 1.6326440572738647, 'train/ce_loss': 0.24296875, 'train/seg_cls_loss': 0.016204833984375, 'train/kl_loss': 0.3794921875, 'train/mask_bce_loss': 0.1390296969562769, 'train/mask_dice_loss': 0.5327122539281846, 'train/mask_loss': 0.6717419475317001, 'metrics/total_secs_per_batch': 8.216707468032837, 'metrics/data_secs_per_batch': 3.877928066253662, '_timestamp': 1740961227.1795738}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 131 is less than current step: 499. Dropping entry: {'train/lr': 0.00010659183673469386, '_timestamp': 1740961227.1798453}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 132 is less than current step: 499. Dropping entry: {'train/loss': 1.6196481466293335, 'train/ce_loss': 0.56083984375, 'train/seg_cls_loss': 0.01063232421875, 'train/kl_loss': 0.228125, 'train/mask_bce_loss': 0.06857331916689872, 'train/mask_dice_loss': 0.44686598777770997, 'train/mask_loss': 0.515439310669899, 'metrics/total_secs_per_batch': 8.019901990890503, 'metrics/data_secs_per_batch': 3.6498761892318727, '_timestamp': 1740961235.1996546}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 132 is less than current step: 499. Dropping entry: {'train/lr': 0.00010646938775510202, '_timestamp': 1740961235.2000034}).
Epoch: [3][134/500]	Time  9.483 ( 9.483)	Loss 2.5223 (1.9146)	CeLoss 0.1719 (0.2451)	SegCLSLoss 0.0171 (0.0177)	KLLoss 0.3770 (0.3783)	MaskLoss 1.1518 (0.8114)	MaskBCELoss 0.3593 (0.1675)	MaskDICELoss 0.7924 (0.6439)
Epoch: [3][135/500]	Time  8.198 ( 8.198)	Loss 2.2830 (2.0591)	CeLoss 0.1191 (0.3891)	SegCLSLoss 0.0233 (0.0165)	KLLoss 0.3848 (0.3055)	MaskLoss 1.0565 (0.8156)	MaskBCELoss 0.1966 (0.1890)	MaskDICELoss 0.8599 (0.6266)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 133 is less than current step: 499. Dropping entry: {'train/loss': 1.9146270751953125, 'train/ce_loss': 0.2451171875, 'train/seg_cls_loss': 0.0177490234375, 'train/kl_loss': 0.3783203125, 'train/mask_bce_loss': 0.1675442535430193, 'train/mask_dice_loss': 0.6438708364963531, 'train/mask_loss': 0.8114151000976563, 'metrics/total_secs_per_batch': 9.482793807983398, 'metrics/data_secs_per_batch': 4.199136996269226, '_timestamp': 1740961244.682246}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 133 is less than current step: 499. Dropping entry: {'train/lr': 0.00010634693877551018, '_timestamp': 1740961244.6825151}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 134 is less than current step: 499. Dropping entry: {'train/loss': 2.059127116203308, 'train/ce_loss': 0.3890625, 'train/seg_cls_loss': 0.016461181640625, 'train/kl_loss': 0.30546875, 'train/mask_bce_loss': 0.18903111405670642, 'train/mask_dice_loss': 0.626567593216896, 'train/mask_loss': 0.8155987143516541, 'metrics/total_secs_per_batch': 8.198478698730469, 'metrics/data_secs_per_batch': 3.356185293197632, '_timestamp': 1740961252.8809235}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 134 is less than current step: 499. Dropping entry: {'train/lr': 0.00010622448979591836, '_timestamp': 1740961252.8812537}).
Epoch: [3][136/500]	Time 10.020 (10.020)	Loss 2.4801 (1.7009)	CeLoss 0.1357 (0.1927)	SegCLSLoss 0.0359 (0.0214)	KLLoss 0.3984 (0.3043)	MaskLoss 1.1434 (0.7334)	MaskBCELoss 0.3866 (0.1267)	MaskDICELoss 0.7568 (0.6067)
Epoch: [3][137/500]	Time  6.632 ( 6.632)	Loss 2.0608 (1.7715)	CeLoss 0.1953 (0.5287)	SegCLSLoss 0.0217 (0.0133)	KLLoss 0.3789 (0.2674)	MaskLoss 0.9083 (0.6046)	MaskBCELoss 0.0866 (0.1216)	MaskDICELoss 0.8217 (0.4830)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 135 is less than current step: 499. Dropping entry: {'train/loss': 1.7009163975715638, 'train/ce_loss': 0.192724609375, 'train/seg_cls_loss': 0.02142333984375, 'train/kl_loss': 0.304296875, 'train/mask_bce_loss': 0.12672703340649605, 'train/mask_dice_loss': 0.6067145764827728, 'train/mask_loss': 0.7334416091442109, 'metrics/total_secs_per_batch': 10.019649267196655, 'metrics/data_secs_per_batch': 4.652010154724121, '_timestamp': 1740961262.9004033}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 135 is less than current step: 499. Dropping entry: {'train/lr': 0.00010610204081632652, '_timestamp': 1740961262.900696}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 136 is less than current step: 499. Dropping entry: {'train/loss': 1.7714975953102112, 'train/ce_loss': 0.5287109375, 'train/seg_cls_loss': 0.0132568359375, 'train/kl_loss': 0.2673828125, 'train/mask_bce_loss': 0.12156332097947598, 'train/mask_dice_loss': 0.4830331265926361, 'train/mask_loss': 0.6045964479446411, 'metrics/total_secs_per_batch': 6.631876468658447, 'metrics/data_secs_per_batch': 2.9193745136260985, '_timestamp': 1740961269.5322483}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 136 is less than current step: 499. Dropping entry: {'train/lr': 0.00010597959183673469, '_timestamp': 1740961269.5325212}).
Epoch: [3][138/500]	Time  6.638 ( 6.638)	Loss 2.6629 (1.4319)	CeLoss 0.1270 (0.4949)	SegCLSLoss 0.0315 (0.0121)	KLLoss 0.3945 (0.2297)	MaskLoss 1.2406 (0.4539)	MaskBCELoss 0.4699 (0.1299)	MaskDICELoss 0.7707 (0.3240)
Epoch: [3][139/500]	Time  7.407 ( 7.407)	Loss 2.1208 (1.7187)	CeLoss 0.3125 (0.5145)	SegCLSLoss 0.0140 (0.0113)	KLLoss 0.3809 (0.2281)	MaskLoss 0.8807 (0.5878)	MaskBCELoss 0.0230 (0.1420)	MaskDICELoss 0.8578 (0.4458)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 137 is less than current step: 499. Dropping entry: {'train/loss': 1.4319080471992494, 'train/ce_loss': 0.494921875, 'train/seg_cls_loss': 0.012091064453125, 'train/kl_loss': 0.2296875, 'train/mask_bce_loss': 0.12993059530854226, 'train/mask_dice_loss': 0.3240117013454437, 'train/mask_loss': 0.4539423018693924, 'metrics/total_secs_per_batch': 6.637772083282471, 'metrics/data_secs_per_batch': 2.8713889360427856, '_timestamp': 1740961276.170174}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 137 is less than current step: 499. Dropping entry: {'train/lr': 0.00010585714285714285, '_timestamp': 1740961276.1704986}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 138 is less than current step: 499. Dropping entry: {'train/loss': 1.7187495589256288, 'train/ce_loss': 0.5145263671875, 'train/seg_cls_loss': 0.0112548828125, 'train/kl_loss': 0.228125, 'train/mask_bce_loss': 0.14199723713099957, 'train/mask_dice_loss': 0.4458077162504196, 'train/mask_loss': 0.5878049433231354, 'metrics/total_secs_per_batch': 7.406538009643555, 'metrics/data_secs_per_batch': 3.303777265548706, '_timestamp': 1740961283.576719}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 138 is less than current step: 499. Dropping entry: {'train/lr': 0.00010573469387755101, '_timestamp': 1740961283.5770266}).
[2025-03-02 18:21:32,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=1640, skipped=0, lr=[0.0001056734693877551], mom=[(0.9, 0.95)]
[2025-03-02 18:21:32,938] [INFO] [timer.py:215:stop] epoch=0/micro_step=16400/global_step=1640, RunningAvgSamplesPerSec=1.5571941609659354, CurrSamplesPerSec=1.0682471843641137, MemAllocated=31.27GB, MaxMemAllocated=37.19GB
Epoch: [3][140/500]	Time  9.363 ( 9.363)	Loss 1.2575 (1.6286)	CeLoss 0.1660 (0.3149)	SegCLSLoss 0.0229 (0.0163)	KLLoss 0.4004 (0.3086)	MaskLoss 0.5199 (0.6374)	MaskBCELoss 0.2618 (0.1424)	MaskDICELoss 0.2581 (0.4950)
Epoch: [3][141/500]	Time  8.636 ( 8.636)	Loss 1.7337 (2.0002)	CeLoss 0.2637 (0.3078)	SegCLSLoss 0.0142 (0.0166)	KLLoss 0.3809 (0.3447)	MaskLoss 0.7125 (0.8249)	MaskBCELoss 0.0359 (0.1380)	MaskDICELoss 0.6766 (0.6868)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 139 is less than current step: 499. Dropping entry: {'train/loss': 1.6286226272583009, 'train/ce_loss': 0.31494140625, 'train/seg_cls_loss': 0.0162841796875, 'train/kl_loss': 0.30859375, 'train/mask_bce_loss': 0.14238852672278882, 'train/mask_dice_loss': 0.4950184911489487, 'train/mask_loss': 0.6374070227146149, 'metrics/total_secs_per_batch': 9.362818241119385, 'metrics/data_secs_per_batch': 3.8506318092346192, '_timestamp': 1740961292.939223}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 139 is less than current step: 499. Dropping entry: {'train/lr': 0.00010561224489795918, '_timestamp': 1740961292.9394996}).
Epoch: [3][142/500]	Time  8.745 ( 8.745)	Loss 0.0767 (1.3290)	CeLoss 0.0767 (0.3326)	SegCLSLoss 0.0000 (0.0132)	KLLoss 0.0000 (0.3057)	MaskLoss 0.0000 (0.4798)	MaskBCELoss 0.0000 (0.1160)	MaskDICELoss 0.0000 (0.3638)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 140 is less than current step: 499. Dropping entry: {'train/loss': 2.000198709964752, 'train/ce_loss': 0.3078125, 'train/seg_cls_loss': 0.01661376953125, 'train/kl_loss': 0.3447265625, 'train/mask_bce_loss': 0.13803905863314866, 'train/mask_dice_loss': 0.686816143989563, 'train/mask_loss': 0.8248551905155181, 'metrics/total_secs_per_batch': 8.636252403259277, 'metrics/data_secs_per_batch': 4.127724003791809, '_timestamp': 1740961301.5758169}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 140 is less than current step: 499. Dropping entry: {'train/lr': 0.00010548979591836734, '_timestamp': 1740961301.5761485}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 141 is less than current step: 499. Dropping entry: {'train/loss': 1.3289917826652526, 'train/ce_loss': 0.332568359375, 'train/seg_cls_loss': 0.013189697265625, 'train/kl_loss': 0.3056640625, 'train/mask_bce_loss': 0.11599517799913883, 'train/mask_dice_loss': 0.3637595109641552, 'train/mask_loss': 0.4797546863555908, 'metrics/total_secs_per_batch': 8.744646310806274, 'metrics/data_secs_per_batch': 3.7020614385604858, '_timestamp': 1740961310.3203459}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 141 is less than current step: 499. Dropping entry: {'train/lr': 0.0001053673469387755, '_timestamp': 1740961310.320634}).
Epoch: [3][143/500]	Time  7.567 ( 7.567)	Loss 2.6958 (1.8486)	CeLoss 0.3477 (0.4473)	SegCLSLoss 0.0192 (0.0150)	KLLoss 0.3984 (0.3072)	MaskLoss 1.1496 (0.6817)	MaskBCELoss 0.3471 (0.1426)	MaskDICELoss 0.8025 (0.5391)
Epoch: [3][144/500]	Time  9.652 ( 9.652)	Loss 1.6231 (1.8756)	CeLoss 0.2168 (0.2521)	SegCLSLoss 0.0096 (0.0196)	KLLoss 0.3789 (0.3918)	MaskLoss 0.6817 (0.7872)	MaskBCELoss 0.2133 (0.1831)	MaskDICELoss 0.4684 (0.6041)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 142 is less than current step: 499. Dropping entry: {'train/loss': 1.8485865950584413, 'train/ce_loss': 0.447265625, 'train/seg_cls_loss': 0.0150390625, 'train/kl_loss': 0.3072265625, 'train/mask_bce_loss': 0.14258752148598433, 'train/mask_dice_loss': 0.5391276478767395, 'train/mask_loss': 0.6817151606082916, 'metrics/total_secs_per_batch': 7.567221164703369, 'metrics/data_secs_per_batch': 3.228699231147766, '_timestamp': 1740961317.8875353}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 142 is less than current step: 499. Dropping entry: {'train/lr': 0.00010524489795918366, '_timestamp': 1740961317.8878222}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 143 is less than current step: 499. Dropping entry: {'train/loss': 1.8756121039390563, 'train/ce_loss': 0.25205078125, 'train/seg_cls_loss': 0.019635009765625, 'train/kl_loss': 0.391796875, 'train/mask_bce_loss': 0.18306765044108034, 'train/mask_dice_loss': 0.6041036516427993, 'train/mask_loss': 0.7871712863445282, 'metrics/total_secs_per_batch': 9.652247667312622, 'metrics/data_secs_per_batch': 5.08045084476471, '_timestamp': 1740961327.5398507}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 143 is less than current step: 499. Dropping entry: {'train/lr': 0.00010512244897959183, '_timestamp': 1740961327.5401607}).
Epoch: [3][145/500]	Time  8.574 ( 8.574)	Loss 2.3298 (1.7181)	CeLoss 0.1836 (0.3089)	SegCLSLoss 0.0176 (0.0163)	KLLoss 0.3711 (0.3002)	MaskLoss 1.0502 (0.6857)	MaskBCELoss 0.1887 (0.1013)	MaskDICELoss 0.8614 (0.5843)
Epoch: [3][146/500]	Time  7.554 ( 7.554)	Loss 2.0917 (1.6559)	CeLoss 0.1914 (0.4095)	SegCLSLoss 0.0192 (0.0143)	KLLoss 0.3652 (0.2668)	MaskLoss 0.9267 (0.6063)	MaskBCELoss 0.0769 (0.1005)	MaskDICELoss 0.8498 (0.5058)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 144 is less than current step: 499. Dropping entry: {'train/loss': 1.7181199014186859, 'train/ce_loss': 0.30888671875, 'train/seg_cls_loss': 0.016253662109375, 'train/kl_loss': 0.3001953125, 'train/mask_bce_loss': 0.10133489794097841, 'train/mask_dice_loss': 0.5843363553285599, 'train/mask_loss': 0.685671254992485, 'metrics/total_secs_per_batch': 8.574318885803223, 'metrics/data_secs_per_batch': 3.706695055961609, '_timestamp': 1740961336.1141753}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 144 is less than current step: 499. Dropping entry: {'train/lr': 0.00010499999999999999, '_timestamp': 1740961336.114482}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 145 is less than current step: 499. Dropping entry: {'train/loss': 1.655912733078003, 'train/ce_loss': 0.40947265625, 'train/seg_cls_loss': 0.014306640625, 'train/kl_loss': 0.266796875, 'train/mask_bce_loss': 0.10049931965768337, 'train/mask_dice_loss': 0.5057773530483246, 'train/mask_loss': 0.6062766700983048, 'metrics/total_secs_per_batch': 7.554024934768677, 'metrics/data_secs_per_batch': 3.473447585105896, '_timestamp': 1740961343.668141}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 145 is less than current step: 499. Dropping entry: {'train/lr': 0.00010487755102040814, '_timestamp': 1740961343.668408}).
Epoch: [3][147/500]	Time  7.347 ( 7.347)	Loss 1.9858 (1.4259)	CeLoss 0.2139 (0.6125)	SegCLSLoss 0.0103 (0.0098)	KLLoss 0.3789 (0.2291)	MaskLoss 0.8650 (0.3930)	MaskBCELoss 0.1547 (0.0810)	MaskDICELoss 0.7103 (0.3120)
Epoch: [3][148/500]	Time  7.453 ( 7.453)	Loss 0.8984 (1.7585)	CeLoss 0.8984 (0.3924)	SegCLSLoss 0.0000 (0.0127)	KLLoss 0.0000 (0.2662)	MaskLoss 0.0000 (0.6666)	MaskBCELoss 0.0000 (0.2095)	MaskDICELoss 0.0000 (0.4571)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 146 is less than current step: 499. Dropping entry: {'train/loss': 1.4258843123912812, 'train/ce_loss': 0.6125, 'train/seg_cls_loss': 0.00980224609375, 'train/kl_loss': 0.2291015625, 'train/mask_bce_loss': 0.08098376914858818, 'train/mask_dice_loss': 0.31198767349123957, 'train/mask_loss': 0.39297143667936324, 'metrics/total_secs_per_batch': 7.3469767570495605, 'metrics/data_secs_per_batch': 3.227046775817871, '_timestamp': 1740961351.0151067}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 146 is less than current step: 499. Dropping entry: {'train/lr': 0.00010475510204081633, '_timestamp': 1740961351.015425}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 147 is less than current step: 499. Dropping entry: {'train/loss': 1.7585180759429933, 'train/ce_loss': 0.3923828125, 'train/seg_cls_loss': 0.01273193359375, 'train/kl_loss': 0.2662109375, 'train/mask_bce_loss': 0.20948832631111144, 'train/mask_dice_loss': 0.4571242183446884, 'train/mask_loss': 0.6666125535964966, 'metrics/total_secs_per_batch': 7.452939987182617, 'metrics/data_secs_per_batch': 3.204125714302063, '_timestamp': 1740961358.4680574}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 147 is less than current step: 499. Dropping entry: {'train/lr': 0.00010463265306122448, '_timestamp': 1740961358.4683223}).
Epoch: [3][149/500]	Time  6.503 ( 6.503)	Loss 2.4632 (1.4784)	CeLoss 0.2539 (0.4223)	SegCLSLoss 0.0151 (0.0135)	KLLoss 0.3750 (0.2656)	MaskLoss 1.0822 (0.5116)	MaskBCELoss 0.2902 (0.0776)	MaskDICELoss 0.7920 (0.4339)
[2025-03-02 18:22:53,066] [INFO] [logging.py:96:log_dist] [Rank 0] step=1650, skipped=0, lr=[0.00010444897959183672], mom=[(0.9, 0.95)]
[2025-03-02 18:22:53,073] [INFO] [timer.py:215:stop] epoch=0/micro_step=16500/global_step=1650, RunningAvgSamplesPerSec=1.5548575279120815, CurrSamplesPerSec=1.2344663936321456, MemAllocated=31.26GB, MaxMemAllocated=37.19GB
Epoch: [3][150/500]	Time  8.102 ( 8.102)	Loss 2.5075 (1.8984)	CeLoss 0.1719 (0.5484)	SegCLSLoss 0.0267 (0.0129)	KLLoss 0.3828 (0.2660)	MaskLoss 1.1419 (0.6585)	MaskBCELoss 0.3561 (0.2033)	MaskDICELoss 0.7858 (0.4552)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 148 is less than current step: 499. Dropping entry: {'train/loss': 1.4784377813339233, 'train/ce_loss': 0.422314453125, 'train/seg_cls_loss': 0.0135009765625, 'train/kl_loss': 0.265625, 'train/mask_bce_loss': 0.07764131780713797, 'train/mask_dice_loss': 0.43391644656658174, 'train/mask_loss': 0.5115577578544617, 'metrics/total_secs_per_batch': 6.5034825801849365, 'metrics/data_secs_per_batch': 2.6951865911483766, '_timestamp': 1740961364.9715526}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 148 is less than current step: 499. Dropping entry: {'train/lr': 0.00010451020408163264, '_timestamp': 1740961364.9718218}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 149 is less than current step: 499. Dropping entry: {'train/loss': 1.8983729004859924, 'train/ce_loss': 0.5484375, 'train/seg_cls_loss': 0.012884521484375, 'train/kl_loss': 0.266015625, 'train/mask_bce_loss': 0.20330692306160927, 'train/mask_dice_loss': 0.4551568657159805, 'train/mask_loss': 0.6584637790918351, 'metrics/total_secs_per_batch': 8.10217547416687, 'metrics/data_secs_per_batch': 3.5676374673843383, '_timestamp': 1740961373.0735354}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 149 is less than current step: 499. Dropping entry: {'train/lr': 0.0001043877551020408, '_timestamp': 1740961373.0737991}).
Epoch: [3][151/500]	Time  6.250 ( 6.250)	Loss 5.0544 (2.0178)	CeLoss 0.2656 (0.5203)	SegCLSLoss 0.0120 (0.0125)	KLLoss 0.3828 (0.2297)	MaskLoss 2.3719 (0.7340)	MaskBCELoss 1.6384 (0.2610)	MaskDICELoss 0.7336 (0.4731)
Epoch: [3][152/500]	Time  7.635 ( 7.635)	Loss 1.7827 (1.6437)	CeLoss 0.2275 (0.3937)	SegCLSLoss 0.0240 (0.0217)	KLLoss 0.3848 (0.3031)	MaskLoss 0.7527 (0.6045)	MaskBCELoss 0.1747 (0.1555)	MaskDICELoss 0.5780 (0.4490)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 150 is less than current step: 499. Dropping entry: {'train/loss': 2.0177869319915773, 'train/ce_loss': 0.5203125, 'train/seg_cls_loss': 0.012548828125, 'train/kl_loss': 0.2296875, 'train/mask_bce_loss': 0.26095128767192366, 'train/mask_dice_loss': 0.47308867871761323, 'train/mask_loss': 0.7340399622917175, 'metrics/total_secs_per_batch': 6.250483751296997, 'metrics/data_secs_per_batch': 2.7230075120925905, '_timestamp': 1740961379.324372}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 150 is less than current step: 499. Dropping entry: {'train/lr': 0.00010426530612244897, '_timestamp': 1740961379.3247068}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 151 is less than current step: 499. Dropping entry: {'train/loss': 1.6436858415603637, 'train/ce_loss': 0.39365234375, 'train/seg_cls_loss': 0.0217041015625, 'train/kl_loss': 0.303125, 'train/mask_bce_loss': 0.15550881549715995, 'train/mask_dice_loss': 0.4490001261234283, 'train/mask_loss': 0.6045089364051819, 'metrics/total_secs_per_batch': 7.635099172592163, 'metrics/data_secs_per_batch': 3.463320779800415, '_timestamp': 1740961386.959338}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 151 is less than current step: 499. Dropping entry: {'train/lr': 0.00010414285714285713, '_timestamp': 1740961386.9596603}).
Epoch: [3][153/500]	Time  7.802 ( 7.802)	Loss 0.0581 (1.0874)	CeLoss 0.0581 (0.3932)	SegCLSLoss 0.0000 (0.0077)	KLLoss 0.0000 (0.1900)	MaskLoss 0.0000 (0.3356)	MaskBCELoss 0.0000 (0.0901)	MaskDICELoss 0.0000 (0.2456)
Epoch: [3][154/500]	Time  7.377 ( 7.377)	Loss 2.2417 (1.6713)	CeLoss 0.2139 (0.3826)	SegCLSLoss 0.0262 (0.0174)	KLLoss 0.3594 (0.2621)	MaskLoss 0.9900 (0.6270)	MaskBCELoss 0.0305 (0.1166)	MaskDICELoss 0.9595 (0.5104)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 152 is less than current step: 499. Dropping entry: {'train/loss': 1.0874357640743255, 'train/ce_loss': 0.393212890625, 'train/seg_cls_loss': 0.00772705078125, 'train/kl_loss': 0.1900390625, 'train/mask_bce_loss': 0.09006076343357564, 'train/mask_dice_loss': 0.24557606428861617, 'train/mask_loss': 0.33563682436943054, 'metrics/total_secs_per_batch': 7.801601886749268, 'metrics/data_secs_per_batch': 3.0848769664764406, '_timestamp': 1740961394.760901}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 152 is less than current step: 499. Dropping entry: {'train/lr': 0.0001040204081632653, '_timestamp': 1740961394.7611723}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 153 is less than current step: 499. Dropping entry: {'train/loss': 1.6713046312332154, 'train/ce_loss': 0.382568359375, 'train/seg_cls_loss': 0.0173583984375, 'train/kl_loss': 0.262109375, 'train/mask_bce_loss': 0.11658000946044922, 'train/mask_dice_loss': 0.5104053109884262, 'train/mask_loss': 0.6269853174686432, 'metrics/total_secs_per_batch': 7.37738299369812, 'metrics/data_secs_per_batch': 3.305563282966614, '_timestamp': 1740961402.1382997}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 153 is less than current step: 499. Dropping entry: {'train/lr': 0.00010389795918367346, '_timestamp': 1740961402.1385727}).
Epoch: [3][155/500]	Time  7.947 ( 7.947)	Loss 0.0688 (1.5546)	CeLoss 0.0688 (0.3360)	SegCLSLoss 0.0000 (0.0130)	KLLoss 0.0000 (0.2652)	MaskLoss 0.0000 (0.5928)	MaskBCELoss 0.0000 (0.1473)	MaskDICELoss 0.0000 (0.4456)
Epoch: [3][156/500]	Time  7.952 ( 7.952)	Loss 0.1230 (1.6950)	CeLoss 0.1230 (0.2892)	SegCLSLoss 0.0000 (0.0160)	KLLoss 0.0000 (0.3043)	MaskLoss 0.0000 (0.6837)	MaskBCELoss 0.0000 (0.1306)	MaskDICELoss 0.0000 (0.5531)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 154 is less than current step: 499. Dropping entry: {'train/loss': 1.5545960545539856, 'train/ce_loss': 0.335986328125, 'train/seg_cls_loss': 0.01298828125, 'train/kl_loss': 0.265234375, 'train/mask_bce_loss': 0.1472888644784689, 'train/mask_dice_loss': 0.44556092023849486, 'train/mask_loss': 0.5928497850894928, 'metrics/total_secs_per_batch': 7.947211265563965, 'metrics/data_secs_per_batch': 3.6237109899520874, '_timestamp': 1740961410.085541}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 154 is less than current step: 499. Dropping entry: {'train/lr': 0.00010377551020408162, '_timestamp': 1740961410.085811}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 155 is less than current step: 499. Dropping entry: {'train/loss': 1.6949873328208924, 'train/ce_loss': 0.28916015625, 'train/seg_cls_loss': 0.0159912109375, 'train/kl_loss': 0.304296875, 'train/mask_bce_loss': 0.1305851886048913, 'train/mask_dice_loss': 0.553090101480484, 'train/mask_loss': 0.6836753010749816, 'metrics/total_secs_per_batch': 7.952157735824585, 'metrics/data_secs_per_batch': 3.6945960760116576, '_timestamp': 1740961418.0377207}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 155 is less than current step: 499. Dropping entry: {'train/lr': 0.00010365306122448978, '_timestamp': 1740961418.038006}).
Epoch: [3][157/500]	Time  7.894 ( 7.894)	Loss 1.7569 (1.4406)	CeLoss 0.3066 (0.4128)	SegCLSLoss 0.0151 (0.0143)	KLLoss 0.3711 (0.2295)	MaskLoss 0.7027 (0.4988)	MaskBCELoss 0.0215 (0.1273)	MaskDICELoss 0.6811 (0.3716)
Epoch: [3][158/500]	Time  9.121 ( 9.121)	Loss 1.5105 (1.6478)	CeLoss 0.2812 (0.2609)	SegCLSLoss 0.0104 (0.0136)	KLLoss 0.3750 (0.3404)	MaskLoss 0.5931 (0.6730)	MaskBCELoss 0.0430 (0.1134)	MaskDICELoss 0.5502 (0.5595)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 156 is less than current step: 499. Dropping entry: {'train/loss': 1.4406155586242675, 'train/ce_loss': 0.41279296875, 'train/seg_cls_loss': 0.014263916015625, 'train/kl_loss': 0.2294921875, 'train/mask_bce_loss': 0.12725591426715255, 'train/mask_dice_loss': 0.3715919017791748, 'train/mask_loss': 0.49884781837463377, 'metrics/total_secs_per_batch': 7.8939008712768555, 'metrics/data_secs_per_batch': 3.8317451953887938, '_timestamp': 1740961425.9315856}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 156 is less than current step: 499. Dropping entry: {'train/lr': 0.00010353061224489795, '_timestamp': 1740961425.93185}).
Epoch: [3][159/500]	Time  6.241 ( 6.241)	Loss 2.4006 (1.5366)	CeLoss 0.3281 (0.5872)	SegCLSLoss 0.0216 (0.0112)	KLLoss 0.3770 (0.2291)	MaskLoss 1.0118 (0.4605)	MaskBCELoss 0.0122 (0.0509)	MaskDICELoss 0.9997 (0.4096)
[2025-03-02 18:24:09,357] [INFO] [logging.py:96:log_dist] [Rank 0] step=1660, skipped=0, lr=[0.00010322448979591837], mom=[(0.9, 0.95)]
[2025-03-02 18:24:09,363] [INFO] [timer.py:215:stop] epoch=0/micro_step=16600/global_step=1660, RunningAvgSamplesPerSec=1.5531149068189982, CurrSamplesPerSec=1.239246863158446, MemAllocated=31.25GB, MaxMemAllocated=37.19GB
Epoch: [3][160/500]	Time  8.071 ( 8.071)	Loss 1.4083 (1.5656)	CeLoss 0.1953 (0.4471)	SegCLSLoss 0.0175 (0.0136)	KLLoss 0.3906 (0.2654)	MaskLoss 0.5826 (0.5424)	MaskBCELoss 0.0201 (0.1106)	MaskDICELoss 0.5625 (0.4318)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 157 is less than current step: 499. Dropping entry: {'train/loss': 1.647761058807373, 'train/ce_loss': 0.2609375, 'train/seg_cls_loss': 0.013592529296875, 'train/kl_loss': 0.3404296875, 'train/mask_bce_loss': 0.11341941803693771, 'train/mask_dice_loss': 0.5595333576202393, 'train/mask_loss': 0.6729527831077575, 'metrics/total_secs_per_batch': 9.12131953239441, 'metrics/data_secs_per_batch': 4.1132173299789425, '_timestamp': 1740961435.0528383}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 157 is less than current step: 499. Dropping entry: {'train/lr': 0.00010340816326530612, '_timestamp': 1740961435.0530968}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 158 is less than current step: 499. Dropping entry: {'train/loss': 1.53657968044281, 'train/ce_loss': 0.587158203125, 'train/seg_cls_loss': 0.0111572265625, 'train/kl_loss': 0.2291015625, 'train/mask_bce_loss': 0.0508612816222012, 'train/mask_dice_loss': 0.4096160501241684, 'train/mask_loss': 0.460477340221405, 'metrics/total_secs_per_batch': 6.24056601524353, 'metrics/data_secs_per_batch': 2.6459610939025877, '_timestamp': 1740961441.2936363}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 158 is less than current step: 499. Dropping entry: {'train/lr': 0.00010328571428571429, '_timestamp': 1740961441.2939737}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 159 is less than current step: 499. Dropping entry: {'train/loss': 1.565605103969574, 'train/ce_loss': 0.4470703125, 'train/seg_cls_loss': 0.013629150390625, 'train/kl_loss': 0.2654296875, 'train/mask_bce_loss': 0.11057590413838625, 'train/mask_dice_loss': 0.43184578120708467, 'train/mask_loss': 0.542421692609787, 'metrics/total_secs_per_batch': 8.071136474609375, 'metrics/data_secs_per_batch': 3.3816701412200927, '_timestamp': 1740961449.3644364}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 159 is less than current step: 499. Dropping entry: {'train/lr': 0.00010316326530612245, '_timestamp': 1740961449.3647125}).
Epoch: [3][161/500]	Time  8.487 ( 8.487)	Loss 1.4844 (2.2689)	CeLoss 1.4844 (0.4373)	SegCLSLoss 0.0000 (0.0193)	KLLoss 0.0000 (0.3010)	MaskLoss 0.0000 (0.8959)	MaskBCELoss 0.0000 (0.2641)	MaskDICELoss 0.0000 (0.6319)
Epoch: [3][162/500]	Time  7.463 ( 7.463)	Loss 0.2617 (1.3799)	CeLoss 0.2617 (0.2825)	SegCLSLoss 0.0000 (0.0149)	KLLoss 0.0000 (0.2652)	MaskLoss 0.0000 (0.5317)	MaskBCELoss 0.0000 (0.0711)	MaskDICELoss 0.0000 (0.4606)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 160 is less than current step: 499. Dropping entry: {'train/loss': 2.268912971019745, 'train/ce_loss': 0.4373046875, 'train/seg_cls_loss': 0.019317626953125, 'train/kl_loss': 0.3009765625, 'train/mask_bce_loss': 0.26406282410025594, 'train/mask_dice_loss': 0.6318682491779327, 'train/mask_loss': 0.895931077003479, 'metrics/total_secs_per_batch': 8.486674308776855, 'metrics/data_secs_per_batch': 4.717268633842468, '_timestamp': 1740961457.8512697}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 160 is less than current step: 499. Dropping entry: {'train/lr': 0.0001030408163265306, '_timestamp': 1740961457.851458}).
Epoch: [3][163/500]	Time  8.488 ( 8.488)	Loss 1.5879 (2.0505)	CeLoss 0.3242 (0.3257)	SegCLSLoss 0.0112 (0.0130)	KLLoss 0.3711 (0.3396)	MaskLoss 0.6104 (0.8420)	MaskBCELoss 0.1029 (0.2098)	MaskDICELoss 0.5074 (0.6323)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 161 is less than current step: 499. Dropping entry: {'train/loss': 1.3799227833747865, 'train/ce_loss': 0.28251953125, 'train/seg_cls_loss': 0.01488037109375, 'train/kl_loss': 0.265234375, 'train/mask_bce_loss': 0.07110124845057726, 'train/mask_dice_loss': 0.4605593651533127, 'train/mask_loss': 0.5316606104373932, 'metrics/total_secs_per_batch': 7.462995529174805, 'metrics/data_secs_per_batch': 3.340992474555969, '_timestamp': 1740961465.3142774}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 161 is less than current step: 499. Dropping entry: {'train/lr': 0.00010291836734693876, '_timestamp': 1740961465.314557}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 162 is less than current step: 499. Dropping entry: {'train/loss': 2.050462210178375, 'train/ce_loss': 0.32568359375, 'train/seg_cls_loss': 0.013031005859375, 'train/kl_loss': 0.3396484375, 'train/mask_bce_loss': 0.20976594043895602, 'train/mask_dice_loss': 0.6322620362043381, 'train/mask_loss': 0.8420279830694198, 'metrics/total_secs_per_batch': 8.488439083099365, 'metrics/data_secs_per_batch': 3.681372141838074, '_timestamp': 1740961473.802745}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 162 is less than current step: 499. Dropping entry: {'train/lr': 0.00010279591836734693, '_timestamp': 1740961473.8030126}).
Epoch: [3][164/500]	Time  6.479 ( 6.479)	Loss 1.2109 (1.7492)	CeLoss 1.2109 (0.6121)	SegCLSLoss 0.0000 (0.0112)	KLLoss 0.0000 (0.2301)	MaskLoss 0.0000 (0.5543)	MaskBCELoss 0.0000 (0.1317)	MaskDICELoss 0.0000 (0.4227)
Epoch: [3][165/500]	Time  7.458 ( 7.458)	Loss 2.6725 (1.5366)	CeLoss 0.2715 (0.4184)	SegCLSLoss 0.0151 (0.0097)	KLLoss 0.3770 (0.2631)	MaskLoss 1.1780 (0.5436)	MaskBCELoss 0.2295 (0.1144)	MaskDICELoss 0.9485 (0.4292)
Epoch: [3][166/500]	Time  6.323 ( 6.323)	Loss 1.1057 (1.5390)	CeLoss 0.3047 (0.6070)	SegCLSLoss 0.0095 (0.0120)	KLLoss 0.3809 (0.2262)	MaskLoss 0.3790 (0.4517)	MaskBCELoss 0.1666 (0.1237)	MaskDICELoss 0.2125 (0.3280)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 163 is less than current step: 499. Dropping entry: {'train/loss': 1.7491947770118714, 'train/ce_loss': 0.612109375, 'train/seg_cls_loss': 0.011187744140625, 'train/kl_loss': 0.230078125, 'train/mask_bce_loss': 0.13166613611392677, 'train/mask_dice_loss': 0.4226675808429718, 'train/mask_loss': 0.5543337166309357, 'metrics/total_secs_per_batch': 6.47895359992981, 'metrics/data_secs_per_batch': 3.196244788169861, '_timestamp': 1740961480.2816727}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 163 is less than current step: 499. Dropping entry: {'train/lr': 0.00010267346938775509, '_timestamp': 1740961480.281953}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 164 is less than current step: 499. Dropping entry: {'train/loss': 1.5365733683109284, 'train/ce_loss': 0.418408203125, 'train/seg_cls_loss': 0.00972900390625, 'train/kl_loss': 0.2630859375, 'train/mask_bce_loss': 0.11436017230153084, 'train/mask_dice_loss': 0.42919507026672366, 'train/mask_loss': 0.5435552388429642, 'metrics/total_secs_per_batch': 7.458207845687866, 'metrics/data_secs_per_batch': 3.193375849723816, '_timestamp': 1740961487.7400796}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 164 is less than current step: 499. Dropping entry: {'train/lr': 0.00010255102040816325, '_timestamp': 1740961487.7404127}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 165 is less than current step: 499. Dropping entry: {'train/loss': 1.5389916062355042, 'train/ce_loss': 0.60703125, 'train/seg_cls_loss': 0.01199951171875, 'train/kl_loss': 0.226171875, 'train/mask_bce_loss': 0.12371993232518434, 'train/mask_dice_loss': 0.32800243943929674, 'train/mask_loss': 0.45172237157821654, 'metrics/total_secs_per_batch': 6.322969913482666, 'metrics/data_secs_per_batch': 2.6524372339248656, '_timestamp': 1740961494.06292}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 165 is less than current step: 499. Dropping entry: {'train/lr': 0.00010242857142857141, '_timestamp': 1740961494.0632129}).
Epoch: [3][167/500]	Time  8.344 ( 8.344)	Loss 1.6382 (1.3327)	CeLoss 0.2432 (0.3402)	SegCLSLoss 0.0133 (0.0140)	KLLoss 0.3770 (0.2652)	MaskLoss 0.6755 (0.4795)	MaskBCELoss 0.1553 (0.0394)	MaskDICELoss 0.5202 (0.4402)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 166 is less than current step: 499. Dropping entry: {'train/loss': 1.332665705680847, 'train/ce_loss': 0.340234375, 'train/seg_cls_loss': 0.01396484375, 'train/kl_loss': 0.265234375, 'train/mask_bce_loss': 0.03935195682570338, 'train/mask_dice_loss': 0.4401644945144653, 'train/mask_loss': 0.4795164465904236, 'metrics/total_secs_per_batch': 8.344196081161499, 'metrics/data_secs_per_batch': 3.356614589691162, '_timestamp': 1740961502.4071054}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 166 is less than current step: 499. Dropping entry: {'train/lr': 0.00010230612244897958, '_timestamp': 1740961502.4073863}).
Epoch: [3][168/500]	Time  9.189 ( 9.189)	Loss 2.2829 (1.8430)	CeLoss 0.2969 (0.3554)	SegCLSLoss 0.0183 (0.0161)	KLLoss 0.3652 (0.3387)	MaskLoss 0.9696 (0.7228)	MaskBCELoss 0.0370 (0.1251)	MaskDICELoss 0.9326 (0.5977)
Epoch: [3][169/500]	Time  8.363 ( 8.363)	Loss 1.8335 (1.6323)	CeLoss 0.2334 (0.4232)	SegCLSLoss 0.0166 (0.0157)	KLLoss 0.4082 (0.3033)	MaskLoss 0.7752 (0.5856)	MaskBCELoss 0.0044 (0.1164)	MaskDICELoss 0.7707 (0.4692)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 167 is less than current step: 499. Dropping entry: {'train/loss': 1.843030673265457, 'train/ce_loss': 0.35537109375, 'train/seg_cls_loss': 0.016058349609375, 'train/kl_loss': 0.338671875, 'train/mask_bce_loss': 0.12510005235671998, 'train/mask_dice_loss': 0.5977336436510086, 'train/mask_loss': 0.722833690047264, 'metrics/total_secs_per_batch': 9.189441919326782, 'metrics/data_secs_per_batch': 4.061277580261231, '_timestamp': 1740961511.596496}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 167 is less than current step: 499. Dropping entry: {'train/lr': 0.00010218367346938774, '_timestamp': 1740961511.5967202}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 168 is less than current step: 499. Dropping entry: {'train/loss': 1.632336139678955, 'train/ce_loss': 0.4232421875, 'train/seg_cls_loss': 0.0157470703125, 'train/kl_loss': 0.3033203125, 'train/mask_bce_loss': 0.11642139982432127, 'train/mask_dice_loss': 0.4691802844405174, 'train/mask_loss': 0.585601681470871, 'metrics/total_secs_per_batch': 8.363334894180298, 'metrics/data_secs_per_batch': 3.5171812057495115, '_timestamp': 1740961519.959905}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 168 is less than current step: 499. Dropping entry: {'train/lr': 0.00010206122448979592, '_timestamp': 1740961519.960215}).
[2025-03-02 18:25:26,009] [INFO] [logging.py:96:log_dist] [Rank 0] step=1670, skipped=0, lr=[0.000102], mom=[(0.9, 0.95)]
[2025-03-02 18:25:26,015] [INFO] [timer.py:215:stop] epoch=0/micro_step=16700/global_step=1670, RunningAvgSamplesPerSec=1.5513450097470998, CurrSamplesPerSec=1.6517014875813574, MemAllocated=31.73GB, MaxMemAllocated=37.19GB
Epoch: [3][170/500]	Time  6.056 ( 6.056)	Loss 0.0596 (1.1744)	CeLoss 0.0596 (0.5818)	SegCLSLoss 0.0000 (0.0063)	KLLoss 0.0000 (0.1904)	MaskLoss 0.0000 (0.2851)	MaskBCELoss 0.0000 (0.0732)	MaskDICELoss 0.0000 (0.2118)
Epoch: [3][171/500]	Time  7.731 ( 7.731)	Loss 2.1267 (1.7123)	CeLoss 0.2598 (0.4247)	SegCLSLoss 0.0106 (0.0126)	KLLoss 0.3789 (0.2635)	MaskLoss 0.9120 (0.6276)	MaskBCELoss 0.3903 (0.1279)	MaskDICELoss 0.5217 (0.4996)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 169 is less than current step: 499. Dropping entry: {'train/loss': 1.1743819296360016, 'train/ce_loss': 0.5818359375, 'train/seg_cls_loss': 0.006268310546875, 'train/kl_loss': 0.1904296875, 'train/mask_bce_loss': 0.07324863076210023, 'train/mask_dice_loss': 0.21184272617101668, 'train/mask_loss': 0.28509135395288465, 'metrics/total_secs_per_batch': 6.055972099304199, 'metrics/data_secs_per_batch': 2.369974231719971, '_timestamp': 1740961526.0156379}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 169 is less than current step: 499. Dropping entry: {'train/lr': 0.00010193877551020408, '_timestamp': 1740961526.015902}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 170 is less than current step: 499. Dropping entry: {'train/loss': 1.7122753262519836, 'train/ce_loss': 0.42470703125, 'train/seg_cls_loss': 0.012615966796875, 'train/kl_loss': 0.2634765625, 'train/mask_bce_loss': 0.12794483304023743, 'train/mask_dice_loss': 0.4996283665299416, 'train/mask_loss': 0.6275732114911079, 'metrics/total_secs_per_batch': 7.730762004852295, 'metrics/data_secs_per_batch': 3.2792553663253785, '_timestamp': 1740961533.7466016}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 170 is less than current step: 499. Dropping entry: {'train/lr': 0.00010181632653061224, '_timestamp': 1740961533.7469132}).
Epoch: [3][172/500]	Time  9.063 ( 9.063)	Loss 1.6005 (1.7694)	CeLoss 0.3535 (0.3298)	SegCLSLoss 0.0140 (0.0132)	KLLoss 0.3730 (0.3027)	MaskLoss 0.6010 (0.7011)	MaskBCELoss 0.2339 (0.1567)	MaskDICELoss 0.3672 (0.5444)
Epoch: [3][173/500]	Time  7.984 ( 7.984)	Loss 2.2762 (1.8794)	CeLoss 0.2656 (0.4507)	SegCLSLoss 0.0220 (0.0141)	KLLoss 0.3672 (0.2619)	MaskLoss 0.9809 (0.6977)	MaskBCELoss 0.0465 (0.1018)	MaskDICELoss 0.9344 (0.5959)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 171 is less than current step: 499. Dropping entry: {'train/loss': 1.7693780422210694, 'train/ce_loss': 0.32978515625, 'train/seg_cls_loss': 0.013226318359375, 'train/kl_loss': 0.302734375, 'train/mask_bce_loss': 0.15673559680581092, 'train/mask_dice_loss': 0.5444084852933884, 'train/mask_loss': 0.7011440873146058, 'metrics/total_secs_per_batch': 9.06267523765564, 'metrics/data_secs_per_batch': 4.122816848754883, '_timestamp': 1740961542.8092628}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 171 is less than current step: 499. Dropping entry: {'train/lr': 0.00010169387755102041, '_timestamp': 1740961542.809539}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 172 is less than current step: 499. Dropping entry: {'train/loss': 1.879439377784729, 'train/ce_loss': 0.45068359375, 'train/seg_cls_loss': 0.01412353515625, 'train/kl_loss': 0.2619140625, 'train/mask_bce_loss': 0.10182381607592106, 'train/mask_dice_loss': 0.5959037125110627, 'train/mask_loss': 0.6977275252342224, 'metrics/total_secs_per_batch': 7.983936309814453, 'metrics/data_secs_per_batch': 3.978971552848816, '_timestamp': 1740961550.7934072}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 172 is less than current step: 499. Dropping entry: {'train/lr': 0.00010157142857142857, '_timestamp': 1740961550.7937496}).
Epoch: [3][174/500]	Time  8.035 ( 8.035)	Loss 2.2876 (1.8143)	CeLoss 0.1934 (0.4377)	SegCLSLoss 0.0149 (0.0110)	KLLoss 0.3984 (0.3066)	MaskLoss 1.0232 (0.6703)	MaskBCELoss 0.0233 (0.1593)	MaskDICELoss 0.9999 (0.5110)
Epoch: [3][175/500]	Time  7.514 ( 7.514)	Loss 1.9520 (1.8717)	CeLoss 0.1973 (0.5650)	SegCLSLoss 0.0203 (0.0121)	KLLoss 0.3672 (0.2629)	MaskLoss 0.8539 (0.6372)	MaskBCELoss 0.2841 (0.1340)	MaskDICELoss 0.5698 (0.5032)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 173 is less than current step: 499. Dropping entry: {'train/loss': 1.8143285512924194, 'train/ce_loss': 0.4376953125, 'train/seg_cls_loss': 0.011029052734375, 'train/kl_loss': 0.306640625, 'train/mask_bce_loss': 0.15929421540349722, 'train/mask_dice_loss': 0.5110048174858093, 'train/mask_loss': 0.6702990412712098, 'metrics/total_secs_per_batch': 8.03537917137146, 'metrics/data_secs_per_batch': 3.8780271291732786, '_timestamp': 1740961558.828588}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 173 is less than current step: 499. Dropping entry: {'train/lr': 0.00010144897959183673, '_timestamp': 1740961558.828773}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 174 is less than current step: 499. Dropping entry: {'train/loss': 1.8717139601707458, 'train/ce_loss': 0.5650390625, 'train/seg_cls_loss': 0.01207275390625, 'train/kl_loss': 0.262890625, 'train/mask_bce_loss': 0.13400610461831092, 'train/mask_dice_loss': 0.5031692415475846, 'train/mask_loss': 0.6371753424406051, 'metrics/total_secs_per_batch': 7.513710975646973, 'metrics/data_secs_per_batch': 3.3527671575546263, '_timestamp': 1740961566.342481}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 174 is less than current step: 499. Dropping entry: {'train/lr': 0.00010132653061224488, '_timestamp': 1740961566.3428278}).
Epoch: [3][176/500]	Time  7.731 ( 7.731)	Loss 2.1070 (1.6662)	CeLoss 0.2754 (0.3836)	SegCLSLoss 0.0122 (0.0166)	KLLoss 0.3770 (0.2984)	MaskLoss 0.8943 (0.6224)	MaskBCELoss 0.2548 (0.0548)	MaskDICELoss 0.6395 (0.5677)
Epoch: [3][177/500]	Time  9.558 ( 9.558)	Loss 1.4901 (1.9241)	CeLoss 0.2773 (0.2355)	SegCLSLoss 0.0107 (0.0168)	KLLoss 0.3867 (0.3797)	MaskLoss 0.5839 (0.8210)	MaskBCELoss 0.1067 (0.2710)	MaskDICELoss 0.4772 (0.5500)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 175 is less than current step: 499. Dropping entry: {'train/loss': 1.6662426948547364, 'train/ce_loss': 0.38359375, 'train/seg_cls_loss': 0.0165771484375, 'train/kl_loss': 0.2984375, 'train/mask_bce_loss': 0.05475048730149865, 'train/mask_dice_loss': 0.5676774919033051, 'train/mask_loss': 0.6224279820919036, 'metrics/total_secs_per_batch': 7.731242656707764, 'metrics/data_secs_per_batch': 3.2762778759002686, '_timestamp': 1740961574.0735602}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 175 is less than current step: 499. Dropping entry: {'train/lr': 0.00010120408163265305, '_timestamp': 1740961574.0738323}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 176 is less than current step: 499. Dropping entry: {'train/loss': 1.9240698218345642, 'train/ce_loss': 0.235546875, 'train/seg_cls_loss': 0.016839599609375, 'train/kl_loss': 0.3796875, 'train/mask_bce_loss': 0.27102953642606736, 'train/mask_dice_loss': 0.5499897480010987, 'train/mask_loss': 0.8210192769765854, 'metrics/total_secs_per_batch': 9.557659387588501, 'metrics/data_secs_per_batch': 3.8313496589660643, '_timestamp': 1740961583.6312213}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 176 is less than current step: 499. Dropping entry: {'train/lr': 0.00010108163265306121, '_timestamp': 1740961583.6314995}).
Epoch: [3][178/500]	Time  8.483 ( 8.483)	Loss 1.0547 (1.8495)	CeLoss 1.0547 (0.5106)	SegCLSLoss 0.0000 (0.0126)	KLLoss 0.0000 (0.2598)	MaskLoss 0.0000 (0.6533)	MaskBCELoss 0.0000 (0.1389)	MaskDICELoss 0.0000 (0.5144)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 177 is less than current step: 499. Dropping entry: {'train/loss': 1.8494897842407227, 'train/ce_loss': 0.51064453125, 'train/seg_cls_loss': 0.012554931640625, 'train/kl_loss': 0.259765625, 'train/mask_bce_loss': 0.13889733608812094, 'train/mask_dice_loss': 0.5144120156764984, 'train/mask_loss': 0.6533093392848969, 'metrics/total_secs_per_batch': 8.48330545425415, 'metrics/data_secs_per_batch': 3.959769916534424, '_timestamp': 1740961592.114523}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 177 is less than current step: 499. Dropping entry: {'train/lr': 0.00010095918367346937, '_timestamp': 1740961592.114809}).
Epoch: [3][179/500]	Time  7.998 ( 7.998)	Loss 1.2031 (1.8059)	CeLoss 1.2031 (0.4457)	SegCLSLoss 0.0000 (0.0136)	KLLoss 0.0000 (0.3002)	MaskLoss 0.0000 (0.6619)	MaskBCELoss 0.0000 (0.1066)	MaskDICELoss 0.0000 (0.5552)
[2025-03-02 18:26:48,742] [INFO] [logging.py:96:log_dist] [Rank 0] step=1680, skipped=0, lr=[0.00010077551020408162], mom=[(0.9, 0.95)]
[2025-03-02 18:26:48,747] [INFO] [timer.py:215:stop] epoch=0/micro_step=16800/global_step=1680, RunningAvgSamplesPerSec=1.5487304993787314, CurrSamplesPerSec=1.1583017831279918, MemAllocated=31.23GB, MaxMemAllocated=37.19GB
Epoch: [3][180/500]	Time  8.635 ( 8.635)	Loss 2.0602 (2.1403)	CeLoss 0.3438 (0.3010)	SegCLSLoss 0.0114 (0.0169)	KLLoss 0.3750 (0.3395)	MaskLoss 0.8367 (0.8984)	MaskBCELoss 0.2230 (0.1666)	MaskDICELoss 0.6137 (0.7318)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 178 is less than current step: 499. Dropping entry: {'train/loss': 1.8059358239173888, 'train/ce_loss': 0.445703125, 'train/seg_cls_loss': 0.01357421875, 'train/kl_loss': 0.3001953125, 'train/mask_bce_loss': 0.10662830853834748, 'train/mask_dice_loss': 0.5552263021469116, 'train/mask_loss': 0.6618546068668365, 'metrics/total_secs_per_batch': 7.998499155044556, 'metrics/data_secs_per_batch': 3.6464030027389525, '_timestamp': 1740961600.113111}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 178 is less than current step: 499. Dropping entry: {'train/lr': 0.00010083673469387753, '_timestamp': 1740961600.1134493}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 179 is less than current step: 499. Dropping entry: {'train/loss': 2.1402573227882384, 'train/ce_loss': 0.3009765625, 'train/seg_cls_loss': 0.016864013671875, 'train/kl_loss': 0.339453125, 'train/mask_bce_loss': 0.16658098213374614, 'train/mask_dice_loss': 0.7317703545093537, 'train/mask_loss': 0.8983513236045837, 'metrics/total_secs_per_batch': 8.634998083114624, 'metrics/data_secs_per_batch': 3.795686388015747, '_timestamp': 1740961608.7478528}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 179 is less than current step: 499. Dropping entry: {'train/lr': 0.0001007142857142857, '_timestamp': 1740961608.7481127}).
Epoch: [3][181/500]	Time  7.749 ( 7.749)	Loss 1.3359 (1.7080)	CeLoss 1.3359 (0.5792)	SegCLSLoss 0.0000 (0.0133)	KLLoss 0.0000 (0.2652)	MaskLoss 0.0000 (0.5479)	MaskBCELoss 0.0000 (0.0791)	MaskDICELoss 0.0000 (0.4688)
Epoch: [3][182/500]	Time  7.715 ( 7.715)	Loss 2.7343 (1.7696)	CeLoss 0.1836 (0.4037)	SegCLSLoss 0.0209 (0.0152)	KLLoss 0.3730 (0.2645)	MaskLoss 1.2514 (0.6660)	MaskBCELoss 0.3393 (0.1756)	MaskDICELoss 0.9121 (0.4903)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 180 is less than current step: 499. Dropping entry: {'train/loss': 1.7080035716295243, 'train/ce_loss': 0.57919921875, 'train/seg_cls_loss': 0.0133056640625, 'train/kl_loss': 0.265234375, 'train/mask_bce_loss': 0.07910750792361795, 'train/mask_dice_loss': 0.4688395902514458, 'train/mask_loss': 0.5479470983147621, 'metrics/total_secs_per_batch': 7.749199390411377, 'metrics/data_secs_per_batch': 3.4886373043060304, '_timestamp': 1740961616.4972355}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 180 is less than current step: 499. Dropping entry: {'train/lr': 0.00010059183673469387, '_timestamp': 1740961616.4975202}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 181 is less than current step: 499. Dropping entry: {'train/loss': 1.7696364164352416, 'train/ce_loss': 0.403662109375, 'train/seg_cls_loss': 0.015203857421875, 'train/kl_loss': 0.264453125, 'train/mask_bce_loss': 0.17563968654721976, 'train/mask_dice_loss': 0.4903308480978012, 'train/mask_loss': 0.6659705340862274, 'metrics/total_secs_per_batch': 7.71491551399231, 'metrics/data_secs_per_batch': 3.261740565299988, '_timestamp': 1740961624.2121506}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 181 is less than current step: 499. Dropping entry: {'train/lr': 0.00010046938775510204, '_timestamp': 1740961624.212421}).
Epoch: [3][183/500]	Time  7.994 ( 7.994)	Loss 3.3439 (1.7122)	CeLoss 0.3066 (0.4438)	SegCLSLoss 0.0227 (0.0145)	KLLoss 0.3809 (0.2973)	MaskLoss 1.4942 (0.6157)	MaskBCELoss 0.7840 (0.1798)	MaskDICELoss 0.7102 (0.4358)
Epoch: [3][184/500]	Time  6.676 ( 6.676)	Loss 1.2031 (1.3772)	CeLoss 1.2031 (0.4801)	SegCLSLoss 0.0000 (0.0101)	KLLoss 0.0000 (0.1852)	MaskLoss 0.0000 (0.4367)	MaskBCELoss 0.0000 (0.0770)	MaskDICELoss 0.0000 (0.3597)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 182 is less than current step: 499. Dropping entry: {'train/loss': 1.7121637225151063, 'train/ce_loss': 0.44384765625, 'train/seg_cls_loss': 0.014544677734375, 'train/kl_loss': 0.297265625, 'train/mask_bce_loss': 0.17980224788188934, 'train/mask_dice_loss': 0.4358499348163605, 'train/mask_loss': 0.6156521737575531, 'metrics/total_secs_per_batch': 7.994441509246826, 'metrics/data_secs_per_batch': 3.6221527338027952, '_timestamp': 1740961632.2065892}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 182 is less than current step: 499. Dropping entry: {'train/lr': 0.0001003469387755102, '_timestamp': 1740961632.2069082}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 183 is less than current step: 499. Dropping entry: {'train/loss': 1.37719783782959, 'train/ce_loss': 0.480126953125, 'train/seg_cls_loss': 0.010064697265625, 'train/kl_loss': 0.18515625, 'train/mask_bce_loss': 0.07695654556155204, 'train/mask_dice_loss': 0.3597136601805687, 'train/mask_loss': 0.43667019307613375, 'metrics/total_secs_per_batch': 6.676152467727661, 'metrics/data_secs_per_batch': 3.122800040245056, '_timestamp': 1740961638.882736}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 183 is less than current step: 499. Dropping entry: {'train/lr': 0.00010022448979591836, '_timestamp': 1740961638.883017}).
Epoch: [3][185/500]	Time  9.251 ( 9.251)	Loss 2.0593 (1.8957)	CeLoss 0.1787 (0.4574)	SegCLSLoss 0.0289 (0.0142)	KLLoss 0.3906 (0.3010)	MaskLoss 0.9134 (0.7004)	MaskBCELoss 0.0569 (0.1074)	MaskDICELoss 0.8566 (0.5930)
Epoch: [3][186/500]	Time  7.864 ( 7.864)	Loss 0.1011 (1.7248)	CeLoss 0.1011 (0.2125)	SegCLSLoss 0.0000 (0.0153)	KLLoss 0.0000 (0.3424)	MaskLoss 0.0000 (0.7352)	MaskBCELoss 0.0000 (0.1534)	MaskDICELoss 0.0000 (0.5818)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 184 is less than current step: 499. Dropping entry: {'train/loss': 1.8957427859306335, 'train/ce_loss': 0.457421875, 'train/seg_cls_loss': 0.01417236328125, 'train/kl_loss': 0.3009765625, 'train/mask_bce_loss': 0.10744672063738107, 'train/mask_dice_loss': 0.5929637283086777, 'train/mask_loss': 0.7004104405641556, 'metrics/total_secs_per_batch': 9.251105308532715, 'metrics/data_secs_per_batch': 3.690604877471924, '_timestamp': 1740961648.1340399}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 184 is less than current step: 499. Dropping entry: {'train/lr': 0.00010010204081632653, '_timestamp': 1740961648.1343837}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 185 is less than current step: 499. Dropping entry: {'train/loss': 1.7247962355613708, 'train/ce_loss': 0.212451171875, 'train/seg_cls_loss': 0.015283203125, 'train/kl_loss': 0.3423828125, 'train/mask_bce_loss': 0.15341986659914256, 'train/mask_dice_loss': 0.5818054020404816, 'train/mask_loss': 0.7352252721786499, 'metrics/total_secs_per_batch': 7.863933563232422, 'metrics/data_secs_per_batch': 3.732985591888428, '_timestamp': 1740961655.997806}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 185 is less than current step: 499. Dropping entry: {'train/lr': 9.997959183673469e-05, '_timestamp': 1740961655.9980893}).
Epoch: [3][187/500]	Time  6.943 ( 6.943)	Loss 2.3073 (1.6518)	CeLoss 0.2188 (0.5257)	SegCLSLoss 0.0216 (0.0116)	KLLoss 0.3691 (0.2246)	MaskLoss 1.0208 (0.5489)	MaskBCELoss 0.0316 (0.0644)	MaskDICELoss 0.9893 (0.4845)
Epoch: [3][188/500]	Time  9.657 ( 9.657)	Loss 1.9259 (1.6759)	CeLoss 0.1758 (0.2278)	SegCLSLoss 0.0269 (0.0159)	KLLoss 0.3691 (0.3383)	MaskLoss 0.8497 (0.7032)	MaskBCELoss 0.0965 (0.0769)	MaskDICELoss 0.7531 (0.6263)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 186 is less than current step: 499. Dropping entry: {'train/loss': 1.651819932460785, 'train/ce_loss': 0.52568359375, 'train/seg_cls_loss': 0.0116455078125, 'train/kl_loss': 0.224609375, 'train/mask_bce_loss': 0.06443197969347239, 'train/mask_dice_loss': 0.48447603583335874, 'train/mask_loss': 0.5489080250263214, 'metrics/total_secs_per_batch': 6.942589998245239, 'metrics/data_secs_per_batch': 3.216995692253113, '_timestamp': 1740961662.9403772}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 186 is less than current step: 499. Dropping entry: {'train/lr': 9.985714285714285e-05, '_timestamp': 1740961662.940682}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 187 is less than current step: 499. Dropping entry: {'train/loss': 1.6758846759796142, 'train/ce_loss': 0.22783203125, 'train/seg_cls_loss': 0.015899658203125, 'train/kl_loss': 0.33828125, 'train/mask_bce_loss': 0.076890904083848, 'train/mask_dice_loss': 0.6263346493244171, 'train/mask_loss': 0.7032255470752716, 'metrics/total_secs_per_batch': 9.656649351119995, 'metrics/data_secs_per_batch': 4.003317523002624, '_timestamp': 1740961672.5970361}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 187 is less than current step: 499. Dropping entry: {'train/lr': 9.9734693877551e-05, '_timestamp': 1740961672.5973144}).
Epoch: [3][189/500]	Time  7.413 ( 7.413)	Loss 1.4933 (1.7953)	CeLoss 0.2598 (0.4043)	SegCLSLoss 0.0134 (0.0139)	KLLoss 0.3789 (0.3021)	MaskLoss 0.5953 (0.6771)	MaskBCELoss 0.2441 (0.1356)	MaskDICELoss 0.3512 (0.5415)
[2025-03-02 18:28:07,228] [INFO] [logging.py:96:log_dist] [Rank 0] step=1690, skipped=0, lr=[9.955102040816325e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:28:07,233] [INFO] [timer.py:215:stop] epoch=0/micro_step=16900/global_step=1690, RunningAvgSamplesPerSec=1.546757184726337, CurrSamplesPerSec=1.3846721367706016, MemAllocated=31.26GB, MaxMemAllocated=37.19GB
Epoch: [3][190/500]	Time  7.224 ( 7.224)	Loss 1.8007 (1.6340)	CeLoss 0.1914 (0.5080)	SegCLSLoss 0.0245 (0.0107)	KLLoss 0.3594 (0.2275)	MaskLoss 0.7802 (0.5490)	MaskBCELoss 0.1854 (0.1996)	MaskDICELoss 0.5948 (0.3493)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 188 is less than current step: 499. Dropping entry: {'train/loss': 1.795287024974823, 'train/ce_loss': 0.404296875, 'train/seg_cls_loss': 0.01385498046875, 'train/kl_loss': 0.3021484375, 'train/mask_bce_loss': 0.13561473339796065, 'train/mask_dice_loss': 0.5414721488952636, 'train/mask_loss': 0.6770868718624115, 'metrics/total_secs_per_batch': 7.413312673568726, 'metrics/data_secs_per_batch': 3.237504577636719, '_timestamp': 1740961680.0103524}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 188 is less than current step: 499. Dropping entry: {'train/lr': 9.961224489795917e-05, '_timestamp': 1740961680.0106697}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 189 is less than current step: 499. Dropping entry: {'train/loss': 1.6340364038944244, 'train/ce_loss': 0.5080078125, 'train/seg_cls_loss': 0.0106689453125, 'train/kl_loss': 0.2275390625, 'train/mask_bce_loss': 0.1996241480112076, 'train/mask_dice_loss': 0.3493276447057724, 'train/mask_loss': 0.5489517956972122, 'metrics/total_secs_per_batch': 7.2235107421875, 'metrics/data_secs_per_batch': 3.2519261837005615, '_timestamp': 1740961687.233734}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 189 is less than current step: 499. Dropping entry: {'train/lr': 9.948979591836733e-05, '_timestamp': 1740961687.2340562}).
Epoch: [3][191/500]	Time  7.434 ( 7.434)	Loss 1.0781 (1.7440)	CeLoss 1.0781 (0.3930)	SegCLSLoss 0.0000 (0.0134)	KLLoss 0.0000 (0.2631)	MaskLoss 0.0000 (0.6590)	MaskBCELoss 0.0000 (0.1652)	MaskDICELoss 0.0000 (0.4938)
Epoch: [3][192/500]	Time  7.940 ( 7.940)	Loss 1.9220 (1.4659)	CeLoss 0.2070 (0.3437)	SegCLSLoss 0.0182 (0.0145)	KLLoss 0.3789 (0.2998)	MaskLoss 0.8341 (0.5425)	MaskBCELoss 0.0171 (0.0659)	MaskDICELoss 0.8170 (0.4766)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 190 is less than current step: 499. Dropping entry: {'train/loss': 1.74400475025177, 'train/ce_loss': 0.393017578125, 'train/seg_cls_loss': 0.013433837890625, 'train/kl_loss': 0.2630859375, 'train/mask_bce_loss': 0.1652103118598461, 'train/mask_dice_loss': 0.4937793731689453, 'train/mask_loss': 0.658989679813385, 'metrics/total_secs_per_batch': 7.434106111526489, 'metrics/data_secs_per_batch': 3.778670406341553, '_timestamp': 1740961694.668055}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 190 is less than current step: 499. Dropping entry: {'train/lr': 9.936734693877549e-05, '_timestamp': 1740961694.6683552}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 191 is less than current step: 499. Dropping entry: {'train/loss': 1.4659405171871185, 'train/ce_loss': 0.34365234375, 'train/seg_cls_loss': 0.014520263671875, 'train/kl_loss': 0.2998046875, 'train/mask_bce_loss': 0.06594117023050786, 'train/mask_dice_loss': 0.4765505716204643, 'train/mask_loss': 0.5424917340278625, 'metrics/total_secs_per_batch': 7.940157175064087, 'metrics/data_secs_per_batch': 3.74792857170105, '_timestamp': 1740961702.6081195}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 191 is less than current step: 499. Dropping entry: {'train/lr': 9.924489795918367e-05, '_timestamp': 1740961702.6083798}).
Epoch: [3][193/500]	Time  7.633 ( 7.633)	Loss 1.9650 (1.6659)	CeLoss 0.1768 (0.5485)	SegCLSLoss 0.0228 (0.0135)	KLLoss 0.3672 (0.2641)	MaskLoss 0.8702 (0.5420)	MaskBCELoss 0.0099 (0.0889)	MaskDICELoss 0.8603 (0.4532)
Epoch: [3][194/500]	Time  9.107 ( 9.107)	Loss 2.2633 (1.7939)	CeLoss 0.1758 (0.3168)	SegCLSLoss 0.0247 (0.0154)	KLLoss 0.3867 (0.3014)	MaskLoss 1.0184 (0.7195)	MaskBCELoss 0.0235 (0.0798)	MaskDICELoss 0.9949 (0.6397)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 192 is less than current step: 499. Dropping entry: {'train/loss': 1.6659053146839142, 'train/ce_loss': 0.54853515625, 'train/seg_cls_loss': 0.013525390625, 'train/kl_loss': 0.2640625, 'train/mask_bce_loss': 0.08887578779831529, 'train/mask_dice_loss': 0.45315891206264497, 'train/mask_loss': 0.5420346945524216, 'metrics/total_secs_per_batch': 7.633448123931885, 'metrics/data_secs_per_batch': 3.0972132205963137, '_timestamp': 1740961710.2415931}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 192 is less than current step: 499. Dropping entry: {'train/lr': 9.912244897959183e-05, '_timestamp': 1740961710.24187}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 193 is less than current step: 499. Dropping entry: {'train/loss': 1.7939080238342284, 'train/ce_loss': 0.316845703125, 'train/seg_cls_loss': 0.0153564453125, 'train/kl_loss': 0.3013671875, 'train/mask_bce_loss': 0.07978183636441827, 'train/mask_dice_loss': 0.6397063642740249, 'train/mask_loss': 0.7194881916046143, 'metrics/total_secs_per_batch': 9.106685161590576, 'metrics/data_secs_per_batch': 3.9570335149765015, '_timestamp': 1740961719.3484807}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 193 is less than current step: 499. Dropping entry: {'train/lr': 9.9e-05, '_timestamp': 1740961719.3488314}).
Epoch: [3][195/500]	Time  9.995 ( 9.995)	Loss 1.6144 (1.8652)	CeLoss 0.1943 (0.3445)	SegCLSLoss 0.0099 (0.0131)	KLLoss 0.3711 (0.2969)	MaskLoss 0.6890 (0.7423)	MaskBCELoss 0.1550 (0.1958)	MaskDICELoss 0.5341 (0.5464)
Epoch: [3][196/500]	Time  6.047 ( 6.047)	Loss 2.7636 (1.3608)	CeLoss 0.3809 (0.6176)	SegCLSLoss 0.0110 (0.0056)	KLLoss 0.3848 (0.1506)	MaskLoss 1.1689 (0.3626)	MaskBCELoss 0.8461 (0.1078)	MaskDICELoss 0.3229 (0.2548)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 194 is less than current step: 499. Dropping entry: {'train/loss': 1.8651778817176818, 'train/ce_loss': 0.34453125, 'train/seg_cls_loss': 0.0130615234375, 'train/kl_loss': 0.296875, 'train/mask_bce_loss': 0.19581271782517434, 'train/mask_dice_loss': 0.5464441880583764, 'train/mask_loss': 0.7422569036483765, 'metrics/total_secs_per_batch': 9.995474100112915, 'metrics/data_secs_per_batch': 4.611699652671814, '_timestamp': 1740961729.3437302}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 194 is less than current step: 499. Dropping entry: {'train/lr': 9.887755102040816e-05, '_timestamp': 1740961729.3440368}).
Epoch: [3][197/500]	Time  8.361 ( 8.361)	Loss 2.1803 (1.4726)	CeLoss 0.2148 (0.3422)	SegCLSLoss 0.0193 (0.0116)	KLLoss 0.3574 (0.3002)	MaskLoss 0.9603 (0.5472)	MaskBCELoss 0.0203 (0.1109)	MaskDICELoss 0.9400 (0.4363)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 195 is less than current step: 499. Dropping entry: {'train/loss': 1.3607736468315124, 'train/ce_loss': 0.617578125, 'train/seg_cls_loss': 0.00556640625, 'train/kl_loss': 0.1505859375, 'train/mask_bce_loss': 0.10784891787916422, 'train/mask_dice_loss': 0.254764461517334, 'train/mask_loss': 0.36261337995529175, 'metrics/total_secs_per_batch': 6.046862840652466, 'metrics/data_secs_per_batch': 2.538311171531677, '_timestamp': 1740961735.3906827}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 195 is less than current step: 499. Dropping entry: {'train/lr': 9.875510204081632e-05, '_timestamp': 1740961735.391007}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 196 is less than current step: 499. Dropping entry: {'train/loss': 1.4725842475891113, 'train/ce_loss': 0.342236328125, 'train/seg_cls_loss': 0.0116455078125, 'train/kl_loss': 0.3001953125, 'train/mask_bce_loss': 0.11089762058109046, 'train/mask_dice_loss': 0.43625876754522325, 'train/mask_loss': 0.5471563845872879, 'metrics/total_secs_per_batch': 8.361064434051514, 'metrics/data_secs_per_batch': 3.8913233280181885, '_timestamp': 1740961743.7517116}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 196 is less than current step: 499. Dropping entry: {'train/lr': 9.863265306122448e-05, '_timestamp': 1740961743.7520316}).
Epoch: [3][198/500]	Time  7.976 ( 7.976)	Loss 3.6289 (2.1346)	CeLoss 0.2578 (0.2957)	SegCLSLoss 0.0172 (0.0154)	KLLoss 0.3770 (0.3363)	MaskLoss 1.6631 (0.8988)	MaskBCELoss 1.0323 (0.3211)	MaskDICELoss 0.6307 (0.5778)
Epoch: [3][199/500]	Time  7.937 ( 7.937)	Loss 1.3047 (1.9167)	CeLoss 1.3047 (0.4706)	SegCLSLoss 0.0000 (0.0162)	KLLoss 0.0000 (0.3016)	MaskLoss 0.0000 (0.7040)	MaskBCELoss 0.0000 (0.1291)	MaskDICELoss 0.0000 (0.5749)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 197 is less than current step: 499. Dropping entry: {'train/loss': 2.1346062779426576, 'train/ce_loss': 0.295703125, 'train/seg_cls_loss': 0.015362548828125, 'train/kl_loss': 0.336328125, 'train/mask_bce_loss': 0.32109363973140714, 'train/mask_dice_loss': 0.5777524620294571, 'train/mask_loss': 0.8988461077213288, 'metrics/total_secs_per_batch': 7.976394176483154, 'metrics/data_secs_per_batch': 3.656210732460022, '_timestamp': 1740961751.7280858}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 197 is less than current step: 499. Dropping entry: {'train/lr': 9.851020408163265e-05, '_timestamp': 1740961751.72835}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 198 is less than current step: 499. Dropping entry: {'train/loss': 1.916741919517517, 'train/ce_loss': 0.47060546875, 'train/seg_cls_loss': 0.0162109375, 'train/kl_loss': 0.3015625, 'train/mask_bce_loss': 0.12909412728622555, 'train/mask_dice_loss': 0.5749311149120331, 'train/mask_loss': 0.7040252447128296, 'metrics/total_secs_per_batch': 7.936990022659302, 'metrics/data_secs_per_batch': 3.637520956993103, '_timestamp': 1740961759.6650825}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 198 is less than current step: 499. Dropping entry: {'train/lr': 9.838775510204081e-05, '_timestamp': 1740961759.6653616}).
[2025-03-02 18:29:27,232] [INFO] [logging.py:96:log_dist] [Rank 0] step=1700, skipped=0, lr=[9.832653061224489e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:29:27,238] [INFO] [timer.py:215:stop] epoch=0/micro_step=17000/global_step=1700, RunningAvgSamplesPerSec=1.5445986370602807, CurrSamplesPerSec=1.3206410268237891, MemAllocated=30.83GB, MaxMemAllocated=37.19GB
Epoch: [3][200/500]	Time  7.574 ( 7.574)	Loss 0.9648 (1.6263)	CeLoss 0.9648 (0.5712)	SegCLSLoss 0.0000 (0.0118)	KLLoss 0.0000 (0.2617)	MaskLoss 0.0000 (0.5115)	MaskBCELoss 0.0000 (0.1134)	MaskDICELoss 0.0000 (0.3981)
Epoch: [3][201/500]	Time  8.145 ( 8.145)	Loss 1.7747 (1.4745)	CeLoss 0.2061 (0.3973)	SegCLSLoss 0.0112 (0.0115)	KLLoss 0.3711 (0.2615)	MaskLoss 0.7629 (0.5227)	MaskBCELoss 0.1415 (0.0810)	MaskDICELoss 0.6213 (0.4417)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 199 is less than current step: 499. Dropping entry: {'train/loss': 1.626255178451538, 'train/ce_loss': 0.57119140625, 'train/seg_cls_loss': 0.011822509765625, 'train/kl_loss': 0.26171875, 'train/mask_bce_loss': 0.1133705910295248, 'train/mask_dice_loss': 0.39814566671848295, 'train/mask_loss': 0.5115162611007691, 'metrics/total_secs_per_batch': 7.573644161224365, 'metrics/data_secs_per_batch': 3.630479264259338, '_timestamp': 1740961767.238628}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 199 is less than current step: 499. Dropping entry: {'train/lr': 9.826530612244897e-05, '_timestamp': 1740961767.2389615}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 200 is less than current step: 499. Dropping entry: {'train/loss': 1.4745263576507568, 'train/ce_loss': 0.3972900390625, 'train/seg_cls_loss': 0.01146240234375, 'train/kl_loss': 0.2615234375, 'train/mask_bce_loss': 0.08100995495915413, 'train/mask_dice_loss': 0.44173906147480013, 'train/mask_loss': 0.5227490127086639, 'metrics/total_secs_per_batch': 8.145429372787476, 'metrics/data_secs_per_batch': 3.5247809410095217, '_timestamp': 1740961775.3841443}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 200 is less than current step: 499. Dropping entry: {'train/lr': 9.814285714285714e-05, '_timestamp': 1740961775.3844137}).
Epoch: [3][202/500]	Time  8.922 ( 8.922)	Loss 0.0669 (1.3672)	CeLoss 0.0669 (0.2380)	SegCLSLoss 0.0000 (0.0123)	KLLoss 0.0000 (0.2217)	MaskLoss 0.0000 (0.5505)	MaskBCELoss 0.0000 (0.0973)	MaskDICELoss 0.0000 (0.4532)
Epoch: [3][203/500]	Time  7.526 ( 7.526)	Loss 2.0135 (1.3577)	CeLoss 0.1934 (0.5334)	SegCLSLoss 0.0238 (0.0121)	KLLoss 0.3809 (0.1877)	MaskLoss 0.8852 (0.3998)	MaskBCELoss 0.1253 (0.0434)	MaskDICELoss 0.7599 (0.3564)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 201 is less than current step: 499. Dropping entry: {'train/loss': 1.3672342896461487, 'train/ce_loss': 0.238037109375, 'train/seg_cls_loss': 0.012310791015625, 'train/kl_loss': 0.2216796875, 'train/mask_bce_loss': 0.09734798185527324, 'train/mask_dice_loss': 0.4531881034374237, 'train/mask_loss': 0.5505360901355744, 'metrics/total_secs_per_batch': 8.921989917755127, 'metrics/data_secs_per_batch': 4.034364867210388, '_timestamp': 1740961784.3061585}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 201 is less than current step: 499. Dropping entry: {'train/lr': 9.802040816326528e-05, '_timestamp': 1740961784.3064497}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 202 is less than current step: 499. Dropping entry: {'train/loss': 1.3577053785324096, 'train/ce_loss': 0.5333740234375, 'train/seg_cls_loss': 0.012103271484375, 'train/kl_loss': 0.1876953125, 'train/mask_bce_loss': 0.043409788236022, 'train/mask_dice_loss': 0.3563535362482071, 'train/mask_loss': 0.39976332485675814, 'metrics/total_secs_per_batch': 7.52556586265564, 'metrics/data_secs_per_batch': 3.780103302001953, '_timestamp': 1740961791.831977}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 202 is less than current step: 499. Dropping entry: {'train/lr': 9.789795918367348e-05, '_timestamp': 1740961791.832316}).
Epoch: [3][204/500]	Time  7.819 ( 7.819)	Loss 1.7819 (1.7815)	CeLoss 0.3145 (0.6767)	SegCLSLoss 0.0165 (0.0126)	KLLoss 0.3750 (0.2236)	MaskLoss 0.7112 (0.5381)	MaskBCELoss 0.0439 (0.1114)	MaskDICELoss 0.6674 (0.4266)
Epoch: [3][205/500]	Time  9.006 ( 9.006)	Loss 1.8138 (1.8362)	CeLoss 0.1709 (0.2442)	SegCLSLoss 0.0211 (0.0142)	KLLoss 0.3633 (0.3342)	MaskLoss 0.7980 (0.7758)	MaskBCELoss 0.0233 (0.1536)	MaskDICELoss 0.7747 (0.6222)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 203 is less than current step: 499. Dropping entry: {'train/loss': 1.7815445423126222, 'train/ce_loss': 0.67666015625, 'train/seg_cls_loss': 0.01260986328125, 'train/kl_loss': 0.2236328125, 'train/mask_bce_loss': 0.11144630778580904, 'train/mask_dice_loss': 0.4266404420137405, 'train/mask_loss': 0.5380867481231689, 'metrics/total_secs_per_batch': 7.818850517272949, 'metrics/data_secs_per_batch': 3.4319129943847657, '_timestamp': 1740961799.6506407}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 203 is less than current step: 499. Dropping entry: {'train/lr': 9.777551020408162e-05, '_timestamp': 1740961799.6509273}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 204 is less than current step: 499. Dropping entry: {'train/loss': 1.8362486243247986, 'train/ce_loss': 0.24423828125, 'train/seg_cls_loss': 0.014239501953125, 'train/kl_loss': 0.3341796875, 'train/mask_bce_loss': 0.15358001785352826, 'train/mask_dice_loss': 0.6222103178501129, 'train/mask_loss': 0.7757903397083282, 'metrics/total_secs_per_batch': 9.006098508834839, 'metrics/data_secs_per_batch': 4.245930957794189, '_timestamp': 1740961808.6566973}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 204 is less than current step: 499. Dropping entry: {'train/lr': 9.765306122448979e-05, '_timestamp': 1740961808.6569717}).
Epoch: [3][206/500]	Time  9.210 ( 9.210)	Loss 2.7062 (1.9182)	CeLoss 0.1670 (0.3040)	SegCLSLoss 0.0293 (0.0173)	KLLoss 0.3750 (0.3348)	MaskLoss 1.2437 (0.7861)	MaskBCELoss 0.2883 (0.1476)	MaskDICELoss 0.9555 (0.6385)
Epoch: [3][207/500]	Time  8.027 ( 8.027)	Loss 2.2843 (1.7978)	CeLoss 0.2412 (0.2787)	SegCLSLoss 0.0209 (0.0171)	KLLoss 0.3711 (0.3727)	MaskLoss 0.9976 (0.7365)	MaskBCELoss 0.0093 (0.1019)	MaskDICELoss 0.9883 (0.6346)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 205 is less than current step: 499. Dropping entry: {'train/loss': 1.9182346940040589, 'train/ce_loss': 0.30400390625, 'train/seg_cls_loss': 0.017333984375, 'train/kl_loss': 0.334765625, 'train/mask_bce_loss': 0.14758479027077556, 'train/mask_dice_loss': 0.6385344997048378, 'train/mask_loss': 0.7861192882061004, 'metrics/total_secs_per_batch': 9.209940433502197, 'metrics/data_secs_per_batch': 4.0510965347290036, '_timestamp': 1740961817.8668191}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 205 is less than current step: 499. Dropping entry: {'train/lr': 9.753061224489795e-05, '_timestamp': 1740961817.8671591}).
Epoch: [3][208/500]	Time  7.872 ( 7.872)	Loss 2.4128 (1.7492)	CeLoss 0.2988 (0.3605)	SegCLSLoss 0.0117 (0.0149)	KLLoss 0.3770 (0.2578)	MaskLoss 1.0355 (0.6778)	MaskBCELoss 0.2338 (0.1548)	MaskDICELoss 0.8017 (0.5230)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 206 is less than current step: 499. Dropping entry: {'train/loss': 1.79782737493515, 'train/ce_loss': 0.2787109375, 'train/seg_cls_loss': 0.0170654296875, 'train/kl_loss': 0.37265625, 'train/mask_bce_loss': 0.10188523293472826, 'train/mask_dice_loss': 0.6346261113882065, 'train/mask_loss': 0.7365113437175751, 'metrics/total_secs_per_batch': 8.027196407318115, 'metrics/data_secs_per_batch': 3.715410304069519, '_timestamp': 1740961825.8938084}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 206 is less than current step: 499. Dropping entry: {'train/lr': 9.740816326530611e-05, '_timestamp': 1740961825.8940728}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 207 is less than current step: 499. Dropping entry: {'train/loss': 1.7492016553878784, 'train/ce_loss': 0.360546875, 'train/seg_cls_loss': 0.014874267578125, 'train/kl_loss': 0.2578125, 'train/mask_bce_loss': 0.15480009522289037, 'train/mask_dice_loss': 0.523023396730423, 'train/mask_loss': 0.6778234899044037, 'metrics/total_secs_per_batch': 7.872303247451782, 'metrics/data_secs_per_batch': 3.328452157974243, '_timestamp': 1740961833.7661352}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 207 is less than current step: 499. Dropping entry: {'train/lr': 9.728571428571428e-05, '_timestamp': 1740961833.7663932}).
Epoch: [3][209/500]	Time  6.846 ( 6.846)	Loss 1.7980 (1.7107)	CeLoss 0.2695 (0.3765)	SegCLSLoss 0.0215 (0.0167)	KLLoss 0.3691 (0.3342)	MaskLoss 0.7398 (0.6461)	MaskBCELoss 0.0950 (0.1748)	MaskDICELoss 0.6448 (0.4714)
[2025-03-02 18:30:50,036] [INFO] [logging.py:96:log_dist] [Rank 0] step=1710, skipped=0, lr=[9.710204081632652e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:30:50,042] [INFO] [timer.py:215:stop] epoch=0/micro_step=17100/global_step=1710, RunningAvgSamplesPerSec=1.5420813800681592, CurrSamplesPerSec=1.0604777148821798, MemAllocated=30.69GB, MaxMemAllocated=37.19GB
Epoch: [3][210/500]	Time  9.431 ( 9.431)	Loss 1.3828 (1.6656)	CeLoss 1.3828 (0.4192)	SegCLSLoss 0.0000 (0.0126)	KLLoss 0.0000 (0.2982)	MaskLoss 0.0000 (0.6051)	MaskBCELoss 0.0000 (0.1069)	MaskDICELoss 0.0000 (0.4982)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 208 is less than current step: 499. Dropping entry: {'train/loss': 1.7106728196144103, 'train/ce_loss': 0.37646484375, 'train/seg_cls_loss': 0.016748046875, 'train/kl_loss': 0.3341796875, 'train/mask_bce_loss': 0.17475561816245316, 'train/mask_dice_loss': 0.471352282166481, 'train/mask_loss': 0.6461078852415085, 'metrics/total_secs_per_batch': 6.8460400104522705, 'metrics/data_secs_per_batch': 2.9428737640380858, '_timestamp': 1740961840.6121905}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 208 is less than current step: 499. Dropping entry: {'train/lr': 9.716326530612244e-05, '_timestamp': 1740961840.6123886}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 209 is less than current step: 499. Dropping entry: {'train/loss': 1.6655516982078553, 'train/ce_loss': 0.41923828125, 'train/seg_cls_loss': 0.01258544921875, 'train/kl_loss': 0.2982421875, 'train/mask_bce_loss': 0.10691931061446666, 'train/mask_dice_loss': 0.498219820857048, 'train/mask_loss': 0.6051391333341598, 'metrics/total_secs_per_batch': 9.431226015090942, 'metrics/data_secs_per_batch': 5.03129768371582, '_timestamp': 1740961850.0432856}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 209 is less than current step: 499. Dropping entry: {'train/lr': 9.70408163265306e-05, '_timestamp': 1740961850.0436137}).
Epoch: [3][211/500]	Time  8.191 ( 8.191)	Loss 1.7822 (1.7629)	CeLoss 0.3457 (0.2343)	SegCLSLoss 0.0124 (0.0177)	KLLoss 0.3652 (0.3361)	MaskLoss 0.6968 (0.7430)	MaskBCELoss 0.1846 (0.2102)	MaskDICELoss 0.5121 (0.5328)
Epoch: [3][212/500]	Time  8.585 ( 8.585)	Loss 2.7577 (1.7254)	CeLoss 0.2969 (0.1975)	SegCLSLoss 0.0193 (0.0146)	KLLoss 0.3711 (0.2992)	MaskLoss 1.2070 (0.7453)	MaskBCELoss 0.4730 (0.1424)	MaskDICELoss 0.7339 (0.6029)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 210 is less than current step: 499. Dropping entry: {'train/loss': 1.7628924787044524, 'train/ce_loss': 0.23427734375, 'train/seg_cls_loss': 0.01766357421875, 'train/kl_loss': 0.3361328125, 'train/mask_bce_loss': 0.21021686010062696, 'train/mask_dice_loss': 0.5328016422688961, 'train/mask_loss': 0.7430185094475746, 'metrics/total_secs_per_batch': 8.191418886184692, 'metrics/data_secs_per_batch': 3.3210604667663572, '_timestamp': 1740961858.2348242}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 210 is less than current step: 499. Dropping entry: {'train/lr': 9.691836734693877e-05, '_timestamp': 1740961858.2350912}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 211 is less than current step: 499. Dropping entry: {'train/loss': 1.7254023194313048, 'train/ce_loss': 0.1975341796875, 'train/seg_cls_loss': 0.01463623046875, 'train/kl_loss': 0.29921875, 'train/mask_bce_loss': 0.14244348406791688, 'train/mask_dice_loss': 0.6028870701789856, 'train/mask_loss': 0.745330560207367, 'metrics/total_secs_per_batch': 8.584819555282593, 'metrics/data_secs_per_batch': 3.876146650314331, '_timestamp': 1740961866.8196585}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 211 is less than current step: 499. Dropping entry: {'train/lr': 9.679591836734693e-05, '_timestamp': 1740961866.8199255}).
Epoch: [3][213/500]	Time  8.683 ( 8.683)	Loss 0.8745 (1.6953)	CeLoss 0.2227 (0.2843)	SegCLSLoss 0.0100 (0.0156)	KLLoss 0.3730 (0.3355)	MaskLoss 0.3044 (0.6848)	MaskBCELoss 0.0856 (0.1423)	MaskDICELoss 0.2189 (0.5426)
Epoch: [3][214/500]	Time  8.982 ( 8.982)	Loss 1.7409 (1.7539)	CeLoss 0.2100 (0.2419)	SegCLSLoss 0.0171 (0.0173)	KLLoss 0.3848 (0.3734)	MaskLoss 0.7416 (0.7331)	MaskBCELoss 0.0275 (0.1090)	MaskDICELoss 0.7141 (0.6242)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 212 is less than current step: 499. Dropping entry: {'train/loss': 1.6953040778636932, 'train/ce_loss': 0.28427734375, 'train/seg_cls_loss': 0.015594482421875, 'train/kl_loss': 0.335546875, 'train/mask_bce_loss': 0.14225836042314768, 'train/mask_dice_loss': 0.5425518959760666, 'train/mask_loss': 0.684810248017311, 'metrics/total_secs_per_batch': 8.683229446411133, 'metrics/data_secs_per_batch': 3.9361701011657715, '_timestamp': 1740961875.502892}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 212 is less than current step: 499. Dropping entry: {'train/lr': 9.667346938775509e-05, '_timestamp': 1740961875.5030794}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 213 is less than current step: 499. Dropping entry: {'train/loss': 1.7538531482219697, 'train/ce_loss': 0.24189453125, 'train/seg_cls_loss': 0.01734619140625, 'train/kl_loss': 0.3734375, 'train/mask_bce_loss': 0.10896019018255174, 'train/mask_dice_loss': 0.6241675607860089, 'train/mask_loss': 0.7331277512013912, 'metrics/total_secs_per_batch': 8.981568574905396, 'metrics/data_secs_per_batch': 3.813833999633789, '_timestamp': 1740961884.4846628}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 213 is less than current step: 499. Dropping entry: {'train/lr': 9.655102040816326e-05, '_timestamp': 1740961884.4850166}).
Epoch: [3][215/500]	Time  8.004 ( 8.004)	Loss 1.4096 (1.8169)	CeLoss 0.2344 (0.4442)	SegCLSLoss 0.0201 (0.0141)	KLLoss 0.3711 (0.3000)	MaskLoss 0.5642 (0.6679)	MaskBCELoss 0.0901 (0.2275)	MaskDICELoss 0.4741 (0.4404)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 214 is less than current step: 499. Dropping entry: {'train/loss': 1.8169400930404662, 'train/ce_loss': 0.44423828125, 'train/seg_cls_loss': 0.01412353515625, 'train/kl_loss': 0.3, 'train/mask_bce_loss': 0.22751489132642747, 'train/mask_dice_loss': 0.44042781591415403, 'train/mask_loss': 0.6679427087306976, 'metrics/total_secs_per_batch': 8.004437923431396, 'metrics/data_secs_per_batch': 3.8951612234115602, '_timestamp': 1740961892.4888911}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 214 is less than current step: 499. Dropping entry: {'train/lr': 9.642857142857143e-05, '_timestamp': 1740961892.4891458}).
Epoch: [3][216/500]	Time  8.771 ( 8.771)	Loss 2.4764 (1.8260)	CeLoss 0.2363 (0.2847)	SegCLSLoss 0.0225 (0.0155)	KLLoss 0.3711 (0.3016)	MaskLoss 1.0956 (0.7517)	MaskBCELoss 0.3308 (0.1326)	MaskDICELoss 0.7648 (0.6192)
Epoch: [3][217/500]	Time  8.140 ( 8.140)	Loss 0.9727 (1.9529)	CeLoss 0.9727 (0.3170)	SegCLSLoss 0.0000 (0.0147)	KLLoss 0.0000 (0.3365)	MaskLoss 0.0000 (0.7975)	MaskBCELoss 0.0000 (0.2728)	MaskDICELoss 0.0000 (0.5247)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 215 is less than current step: 499. Dropping entry: {'train/loss': 1.8259528279304504, 'train/ce_loss': 0.28466796875, 'train/seg_cls_loss': 0.015496826171875, 'train/kl_loss': 0.3015625, 'train/mask_bce_loss': 0.13259137719869613, 'train/mask_dice_loss': 0.619154542684555, 'train/mask_loss': 0.7517459332942963, 'metrics/total_secs_per_batch': 8.770986318588257, 'metrics/data_secs_per_batch': 4.149242663383484, '_timestamp': 1740961901.259999}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 215 is less than current step: 499. Dropping entry: {'train/lr': 9.63061224489796e-05, '_timestamp': 1740961901.2603657}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 216 is less than current step: 499. Dropping entry: {'train/loss': 1.952933418750763, 'train/ce_loss': 0.3169921875, 'train/seg_cls_loss': 0.014697265625, 'train/kl_loss': 0.3365234375, 'train/mask_bce_loss': 0.27278688829392195, 'train/mask_dice_loss': 0.5246759086847306, 'train/mask_loss': 0.7974627941846848, 'metrics/total_secs_per_batch': 8.139937162399292, 'metrics/data_secs_per_batch': 3.6586735248565674, '_timestamp': 1740961909.3998473}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 216 is less than current step: 499. Dropping entry: {'train/lr': 9.618367346938774e-05, '_timestamp': 1740961909.4001248}).
Epoch: [3][218/500]	Time  8.360 ( 8.360)	Loss 2.4161 (2.2658)	CeLoss 0.1875 (0.3267)	SegCLSLoss 0.0199 (0.0200)	KLLoss 0.3730 (0.3303)	MaskLoss 1.0909 (0.9481)	MaskBCELoss 0.3553 (0.1751)	MaskDICELoss 0.7356 (0.7729)
Epoch: [3][219/500]	Time  9.311 ( 9.311)	Loss 0.1621 (1.6442)	CeLoss 0.1621 (0.3371)	SegCLSLoss 0.0000 (0.0146)	KLLoss 0.0000 (0.2604)	MaskLoss 0.0000 (0.6368)	MaskBCELoss 0.0000 (0.1143)	MaskDICELoss 0.0000 (0.5225)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 217 is less than current step: 499. Dropping entry: {'train/loss': 2.265814185142517, 'train/ce_loss': 0.326708984375, 'train/seg_cls_loss': 0.0200439453125, 'train/kl_loss': 0.3302734375, 'train/mask_bce_loss': 0.17514470554888248, 'train/mask_dice_loss': 0.7729479491710662, 'train/mask_loss': 0.948092645406723, 'metrics/total_secs_per_batch': 8.359784364700317, 'metrics/data_secs_per_batch': 3.795058274269104, '_timestamp': 1740961917.7595947}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 217 is less than current step: 499. Dropping entry: {'train/lr': 9.606122448979591e-05, '_timestamp': 1740961917.7598543}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 218 is less than current step: 499. Dropping entry: {'train/loss': 1.6441998958587647, 'train/ce_loss': 0.337060546875, 'train/seg_cls_loss': 0.0145751953125, 'train/kl_loss': 0.2603515625, 'train/mask_bce_loss': 0.11432253122329712, 'train/mask_dice_loss': 0.5224502623081207, 'train/mask_loss': 0.6367727935314178, 'metrics/total_secs_per_batch': 9.31102466583252, 'metrics/data_secs_per_batch': 4.244765520095825, '_timestamp': 1740961927.0706463}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 218 is less than current step: 499. Dropping entry: {'train/lr': 9.593877551020407e-05, '_timestamp': 1740961927.0709293}).
[2025-03-02 18:32:14,395] [INFO] [logging.py:96:log_dist] [Rank 0] step=1720, skipped=0, lr=[9.587755102040815e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:32:14,401] [INFO] [timer.py:215:stop] epoch=0/micro_step=17200/global_step=1720, RunningAvgSamplesPerSec=1.5393870761118635, CurrSamplesPerSec=1.364297275678896, MemAllocated=30.96GB, MaxMemAllocated=37.19GB
Epoch: [3][220/500]	Time  7.331 ( 7.331)	Loss 2.2694 (1.5174)	CeLoss 0.1934 (0.3114)	SegCLSLoss 0.0233 (0.0149)	KLLoss 0.3652 (0.2977)	MaskLoss 1.0136 (0.5843)	MaskBCELoss 0.0138 (0.1103)	MaskDICELoss 0.9997 (0.4740)
Epoch: [3][221/500]	Time  7.321 ( 7.321)	Loss 1.4148 (1.4264)	CeLoss 0.2295 (0.4967)	SegCLSLoss 0.0128 (0.0128)	KLLoss 0.3789 (0.3014)	MaskLoss 0.5707 (0.4466)	MaskBCELoss 0.1264 (0.0650)	MaskDICELoss 0.4443 (0.3816)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 219 is less than current step: 499. Dropping entry: {'train/loss': 1.5173728734254837, 'train/ce_loss': 0.31142578125, 'train/seg_cls_loss': 0.0149169921875, 'train/kl_loss': 0.29765625, 'train/mask_bce_loss': 0.11030774461105466, 'train/mask_dice_loss': 0.47401345074176787, 'train/mask_loss': 0.5843211904168129, 'metrics/total_secs_per_batch': 7.331320524215698, 'metrics/data_secs_per_batch': 2.888478231430054, '_timestamp': 1740961934.4018621}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 219 is less than current step: 499. Dropping entry: {'train/lr': 9.581632653061223e-05, '_timestamp': 1740961934.4022021}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 220 is less than current step: 499. Dropping entry: {'train/loss': 1.4263917922973632, 'train/ce_loss': 0.4966796875, 'train/seg_cls_loss': 0.01275634765625, 'train/kl_loss': 0.3013671875, 'train/mask_bce_loss': 0.06503037810325622, 'train/mask_dice_loss': 0.3815639495849609, 'train/mask_loss': 0.44659432768821716, 'metrics/total_secs_per_batch': 7.321359395980835, 'metrics/data_secs_per_batch': 2.935733604431152, '_timestamp': 1740961941.7233243}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 220 is less than current step: 499. Dropping entry: {'train/lr': 9.56938775510204e-05, '_timestamp': 1740961941.723594}).
Epoch: [3][222/500]	Time  7.487 ( 7.487)	Loss 2.6904 (1.4869)	CeLoss 0.2119 (0.3933)	SegCLSLoss 0.0239 (0.0129)	KLLoss 0.3750 (0.2629)	MaskLoss 1.2143 (0.5304)	MaskBCELoss 0.3404 (0.1681)	MaskDICELoss 0.8740 (0.3623)
Epoch: [3][223/500]	Time  6.753 ( 6.753)	Loss 0.9914 (1.3062)	CeLoss 0.2754 (0.5275)	SegCLSLoss 0.0096 (0.0079)	KLLoss 0.3730 (0.2238)	MaskLoss 0.3375 (0.3764)	MaskBCELoss 0.0737 (0.0657)	MaskDICELoss 0.2638 (0.3107)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 221 is less than current step: 499. Dropping entry: {'train/loss': 1.4868778884410858, 'train/ce_loss': 0.39326171875, 'train/seg_cls_loss': 0.012872314453125, 'train/kl_loss': 0.262890625, 'train/mask_bce_loss': 0.16811214722692966, 'train/mask_dice_loss': 0.3622896924614906, 'train/mask_loss': 0.5304018288850785, 'metrics/total_secs_per_batch': 7.487094402313232, 'metrics/data_secs_per_batch': 2.8989230394363403, '_timestamp': 1740961949.210436}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 221 is less than current step: 499. Dropping entry: {'train/lr': 9.557142857142856e-05, '_timestamp': 1740961949.210736}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 222 is less than current step: 499. Dropping entry: {'train/loss': 1.3061814308166504, 'train/ce_loss': 0.5275390625, 'train/seg_cls_loss': 0.007855224609375, 'train/kl_loss': 0.223828125, 'train/mask_bce_loss': 0.06573067232966423, 'train/mask_dice_loss': 0.31065105497837064, 'train/mask_loss': 0.37638172805309295, 'metrics/total_secs_per_batch': 6.752856254577637, 'metrics/data_secs_per_batch': 2.683945322036743, '_timestamp': 1740961955.9632697}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 222 is less than current step: 499. Dropping entry: {'train/lr': 9.544897959183672e-05, '_timestamp': 1740961955.9635856}).
Epoch: [3][224/500]	Time  7.606 ( 7.606)	Loss 2.5474 (1.5480)	CeLoss 0.3652 (0.5717)	SegCLSLoss 0.0112 (0.0093)	KLLoss 0.3633 (0.2223)	MaskLoss 1.0696 (0.4746)	MaskBCELoss 0.0711 (0.0744)	MaskDICELoss 0.9985 (0.4002)
Epoch: [3][225/500]	Time  7.677 ( 7.677)	Loss 2.0144 (1.6009)	CeLoss 0.2383 (0.3330)	SegCLSLoss 0.0225 (0.0148)	KLLoss 0.3633 (0.2197)	MaskLoss 0.8636 (0.6191)	MaskBCELoss 0.0590 (0.1243)	MaskDICELoss 0.8046 (0.4948)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 223 is less than current step: 499. Dropping entry: {'train/loss': 1.5479663252830504, 'train/ce_loss': 0.5716796875, 'train/seg_cls_loss': 0.009295654296875, 'train/kl_loss': 0.222265625, 'train/mask_bce_loss': 0.07437566854059696, 'train/mask_dice_loss': 0.40019344389438627, 'train/mask_loss': 0.47456910610198977, 'metrics/total_secs_per_batch': 7.606073379516602, 'metrics/data_secs_per_batch': 3.474803924560547, '_timestamp': 1740961963.5693593}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 223 is less than current step: 499. Dropping entry: {'train/lr': 9.532653061224489e-05, '_timestamp': 1740961963.569624}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 224 is less than current step: 499. Dropping entry: {'train/loss': 1.600928544998169, 'train/ce_loss': 0.3330078125, 'train/seg_cls_loss': 0.0148193359375, 'train/kl_loss': 0.2197265625, 'train/mask_bce_loss': 0.12431670855730773, 'train/mask_dice_loss': 0.49479991793632505, 'train/mask_loss': 0.6191166281700134, 'metrics/total_secs_per_batch': 7.6774742603302, 'metrics/data_secs_per_batch': 2.985538959503174, '_timestamp': 1740961971.246835}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 224 is less than current step: 499. Dropping entry: {'train/lr': 9.520408163265305e-05, '_timestamp': 1740961971.247106}).
Epoch: [3][226/500]	Time  8.866 ( 8.866)	Loss 1.8372 (1.6970)	CeLoss 0.2617 (0.2878)	SegCLSLoss 0.0156 (0.0188)	KLLoss 0.3711 (0.3350)	MaskLoss 0.7653 (0.6833)	MaskBCELoss 0.0392 (0.0662)	MaskDICELoss 0.7261 (0.6171)
Epoch: [3][227/500]	Time  7.995 ( 7.995)	Loss 3.0890 (1.7453)	CeLoss 0.1582 (0.3923)	SegCLSLoss 0.0264 (0.0146)	KLLoss 0.3672 (0.2979)	MaskLoss 1.4400 (0.6580)	MaskBCELoss 0.7256 (0.1355)	MaskDICELoss 0.7144 (0.5224)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 225 is less than current step: 499. Dropping entry: {'train/loss': 1.6970011234283446, 'train/ce_loss': 0.28779296875, 'train/seg_cls_loss': 0.01878662109375, 'train/kl_loss': 0.3349609375, 'train/mask_bce_loss': 0.06620010696351528, 'train/mask_dice_loss': 0.6170660734176636, 'train/mask_loss': 0.6832661807537079, 'metrics/total_secs_per_batch': 8.866025447845459, 'metrics/data_secs_per_batch': 3.3977607011795046, '_timestamp': 1740961980.1128886}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 225 is less than current step: 499. Dropping entry: {'train/lr': 9.508163265306123e-05, '_timestamp': 1740961980.1131697}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 226 is less than current step: 499. Dropping entry: {'train/loss': 1.7452635526657105, 'train/ce_loss': 0.39228515625, 'train/seg_cls_loss': 0.014556884765625, 'train/kl_loss': 0.2978515625, 'train/mask_bce_loss': 0.13554281555116177, 'train/mask_dice_loss': 0.5224405199289321, 'train/mask_loss': 0.657983335852623, 'metrics/total_secs_per_batch': 7.995081424713135, 'metrics/data_secs_per_batch': 3.608266019821167, '_timestamp': 1740961988.1081316}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 226 is less than current step: 499. Dropping entry: {'train/lr': 9.495918367346939e-05, '_timestamp': 1740961988.1084492}).
Epoch: [3][228/500]	Time  7.060 ( 7.060)	Loss 1.6947 (1.6563)	CeLoss 0.3848 (0.5565)	SegCLSLoss 0.0132 (0.0115)	KLLoss 0.3652 (0.2643)	MaskLoss 0.6335 (0.5338)	MaskBCELoss 0.1349 (0.1319)	MaskDICELoss 0.4986 (0.4019)
Epoch: [3][229/500]	Time  6.551 ( 6.551)	Loss 1.0781 (1.2812)	CeLoss 1.0781 (0.4876)	SegCLSLoss 0.0000 (0.0115)	KLLoss 0.0000 (0.2248)	MaskLoss 0.0000 (0.3828)	MaskBCELoss 0.0000 (0.0572)	MaskDICELoss 0.0000 (0.3256)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 227 is less than current step: 499. Dropping entry: {'train/loss': 1.6562752604484559, 'train/ce_loss': 0.55654296875, 'train/seg_cls_loss': 0.011474609375, 'train/kl_loss': 0.2642578125, 'train/mask_bce_loss': 0.13192334473133088, 'train/mask_dice_loss': 0.40187833905220033, 'train/mask_loss': 0.5338016837835312, 'metrics/total_secs_per_batch': 7.059851169586182, 'metrics/data_secs_per_batch': 2.7585998296737673, '_timestamp': 1740961995.167795}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 227 is less than current step: 499. Dropping entry: {'train/lr': 9.483673469387755e-05, '_timestamp': 1740961995.1680186}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 228 is less than current step: 499. Dropping entry: {'train/loss': 1.2812476098537444, 'train/ce_loss': 0.48759765625, 'train/seg_cls_loss': 0.01146240234375, 'train/kl_loss': 0.2248046875, 'train/mask_bce_loss': 0.057169305719435216, 'train/mask_dice_loss': 0.32564199566841123, 'train/mask_loss': 0.38281130492687226, 'metrics/total_secs_per_batch': 6.551055908203125, 'metrics/data_secs_per_batch': 2.9617171764373778, '_timestamp': 1740962001.7188413}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 228 is less than current step: 499. Dropping entry: {'train/lr': 9.471428571428571e-05, '_timestamp': 1740962001.7190979}).
[2025-03-02 18:33:28,344] [INFO] [logging.py:96:log_dist] [Rank 0] step=1730, skipped=0, lr=[9.46530612244898e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:33:28,350] [INFO] [timer.py:215:stop] epoch=0/micro_step=17300/global_step=1730, RunningAvgSamplesPerSec=1.538157057501305, CurrSamplesPerSec=1.5080917424298401, MemAllocated=31.28GB, MaxMemAllocated=37.19GB
Epoch: [3][230/500]	Time  6.632 ( 6.632)	Loss 2.5382 (1.5089)	CeLoss 0.1729 (0.4117)	SegCLSLoss 0.0264 (0.0163)	KLLoss 0.3633 (0.3002)	MaskLoss 1.1583 (0.5295)	MaskBCELoss 0.3484 (0.1448)	MaskDICELoss 0.8099 (0.3848)
Epoch: [3][231/500]	Time  8.987 ( 8.987)	Loss 2.1442 (1.7845)	CeLoss 0.1787 (0.2183)	SegCLSLoss 0.0251 (0.0192)	KLLoss 0.3711 (0.3365)	MaskLoss 0.9578 (0.7614)	MaskBCELoss 0.0190 (0.1059)	MaskDICELoss 0.9389 (0.6555)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 229 is less than current step: 499. Dropping entry: {'train/loss': 1.5089272201061248, 'train/ce_loss': 0.41171875, 'train/seg_cls_loss': 0.016326904296875, 'train/kl_loss': 0.3001953125, 'train/mask_bce_loss': 0.14475461521651595, 'train/mask_dice_loss': 0.38475782573223116, 'train/mask_loss': 0.5295124337077141, 'metrics/total_secs_per_batch': 6.632345676422119, 'metrics/data_secs_per_batch': 3.100323438644409, '_timestamp': 1740962008.3510187}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 229 is less than current step: 499. Dropping entry: {'train/lr': 9.459183673469388e-05, '_timestamp': 1740962008.35127}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 230 is less than current step: 499. Dropping entry: {'train/loss': 1.7845171332359313, 'train/ce_loss': 0.218310546875, 'train/seg_cls_loss': 0.019219970703125, 'train/kl_loss': 0.3365234375, 'train/mask_bce_loss': 0.10594721732195467, 'train/mask_dice_loss': 0.6555008083581925, 'train/mask_loss': 0.7614480286836625, 'metrics/total_secs_per_batch': 8.987433910369873, 'metrics/data_secs_per_batch': 4.103895378112793, '_timestamp': 1740962017.3386414}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 230 is less than current step: 499. Dropping entry: {'train/lr': 9.446938775510203e-05, '_timestamp': 1740962017.3389077}).
Epoch: [3][232/500]	Time  9.038 ( 9.038)	Loss 0.9793 (1.8416)	CeLoss 0.3242 (0.2591)	SegCLSLoss 0.0114 (0.0182)	KLLoss 0.3809 (0.3758)	MaskLoss 0.3061 (0.7679)	MaskBCELoss 0.0803 (0.1103)	MaskDICELoss 0.2258 (0.6576)
Epoch: [3][233/500]	Time  7.656 ( 7.656)	Loss 1.9105 (1.5708)	CeLoss 0.1943 (0.4057)	SegCLSLoss 0.0201 (0.0144)	KLLoss 0.3789 (0.3006)	MaskLoss 0.8342 (0.5639)	MaskBCELoss 0.0072 (0.1127)	MaskDICELoss 0.8270 (0.4512)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 231 is less than current step: 499. Dropping entry: {'train/loss': 1.8416267156600952, 'train/ce_loss': 0.25908203125, 'train/seg_cls_loss': 0.0181884765625, 'train/kl_loss': 0.37578125, 'train/mask_bce_loss': 0.11028302367776632, 'train/mask_dice_loss': 0.6576494887471199, 'train/mask_loss': 0.7679325073957444, 'metrics/total_secs_per_batch': 9.037660598754883, 'metrics/data_secs_per_batch': 3.990442705154419, '_timestamp': 1740962026.3763144}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 231 is less than current step: 499. Dropping entry: {'train/lr': 9.434693877551019e-05, '_timestamp': 1740962026.3765826}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 232 is less than current step: 499. Dropping entry: {'train/loss': 1.5708474516868591, 'train/ce_loss': 0.4056640625, 'train/seg_cls_loss': 0.01435546875, 'train/kl_loss': 0.3005859375, 'train/mask_bce_loss': 0.11274467566981912, 'train/mask_dice_loss': 0.45119467079639436, 'train/mask_loss': 0.5639393478631973, 'metrics/total_secs_per_batch': 7.655641078948975, 'metrics/data_secs_per_batch': 3.1156380653381346, '_timestamp': 1740962034.0320518}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 232 is less than current step: 499. Dropping entry: {'train/lr': 9.422448979591835e-05, '_timestamp': 1740962034.0323591}).
Epoch: [3][234/500]	Time  6.145 ( 6.145)	Loss 1.3720 (1.5470)	CeLoss 0.5156 (0.5749)	SegCLSLoss 0.0098 (0.0112)	KLLoss 0.3906 (0.2273)	MaskLoss 0.4067 (0.4720)	MaskBCELoss 0.0841 (0.0975)	MaskDICELoss 0.3226 (0.3745)
Epoch: [3][235/500]	Time  9.266 ( 9.266)	Loss 1.0625 (1.8960)	CeLoss 1.0625 (0.4229)	SegCLSLoss 0.0000 (0.0151)	KLLoss 0.0000 (0.2992)	MaskLoss 0.0000 (0.7179)	MaskBCELoss 0.0000 (0.0922)	MaskDICELoss 0.0000 (0.6257)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 233 is less than current step: 499. Dropping entry: {'train/loss': 1.5470369577407836, 'train/ce_loss': 0.57490234375, 'train/seg_cls_loss': 0.0112060546875, 'train/kl_loss': 0.22734375, 'train/mask_bce_loss': 0.09753329893574118, 'train/mask_dice_loss': 0.37447150945663454, 'train/mask_loss': 0.4720048099756241, 'metrics/total_secs_per_batch': 6.1450417041778564, 'metrics/data_secs_per_batch': 3.0040239334106444, '_timestamp': 1740962040.177094}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 233 is less than current step: 499. Dropping entry: {'train/lr': 9.410204081632652e-05, '_timestamp': 1740962040.1773946}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 234 is less than current step: 499. Dropping entry: {'train/loss': 1.8960170984268188, 'train/ce_loss': 0.42294921875, 'train/seg_cls_loss': 0.015142822265625, 'train/kl_loss': 0.29921875, 'train/mask_bce_loss': 0.09221942164003849, 'train/mask_dice_loss': 0.6256621718406677, 'train/mask_loss': 0.7178815960884094, 'metrics/total_secs_per_batch': 9.266454935073853, 'metrics/data_secs_per_batch': 4.45376946926117, '_timestamp': 1740962049.443484}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 234 is less than current step: 499. Dropping entry: {'train/lr': 9.397959183673468e-05, '_timestamp': 1740962049.4437618}).
Epoch: [3][236/500]	Time  8.576 ( 8.576)	Loss 1.3571 (1.3723)	CeLoss 0.3516 (0.2085)	SegCLSLoss 0.0101 (0.0143)	KLLoss 0.3770 (0.2986)	MaskLoss 0.4813 (0.5634)	MaskBCELoss 0.2121 (0.0538)	MaskDICELoss 0.2692 (0.5096)
Epoch: [3][237/500]	Time  8.259 ( 8.259)	Loss 1.6323 (1.4005)	CeLoss 0.2354 (0.2809)	SegCLSLoss 0.0112 (0.0125)	KLLoss 0.3711 (0.2631)	MaskLoss 0.6775 (0.5435)	MaskBCELoss 0.0751 (0.1075)	MaskDICELoss 0.6024 (0.4360)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 235 is less than current step: 499. Dropping entry: {'train/loss': 1.372286081314087, 'train/ce_loss': 0.2085205078125, 'train/seg_cls_loss': 0.01431884765625, 'train/kl_loss': 0.2986328125, 'train/mask_bce_loss': 0.05381745733320713, 'train/mask_dice_loss': 0.5095594823360443, 'train/mask_loss': 0.5633769363164902, 'metrics/total_secs_per_batch': 8.576401472091675, 'metrics/data_secs_per_batch': 3.5562450170516966, '_timestamp': 1740962058.0198598}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 235 is less than current step: 499. Dropping entry: {'train/lr': 9.385714285714284e-05, '_timestamp': 1740962058.0201294}).
Epoch: [3][238/500]	Time  8.001 ( 8.001)	Loss 2.5386 (1.5861)	CeLoss 0.2256 (0.3804)	SegCLSLoss 0.0211 (0.0156)	KLLoss 0.3711 (0.2650)	MaskLoss 1.1326 (0.5858)	MaskBCELoss 0.2432 (0.1224)	MaskDICELoss 0.8894 (0.4633)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 236 is less than current step: 499. Dropping entry: {'train/loss': 1.4005293726921082, 'train/ce_loss': 0.280908203125, 'train/seg_cls_loss': 0.012457275390625, 'train/kl_loss': 0.2630859375, 'train/mask_bce_loss': 0.10747658871114255, 'train/mask_dice_loss': 0.436025407910347, 'train/mask_loss': 0.543501989543438, 'metrics/total_secs_per_batch': 8.259128332138062, 'metrics/data_secs_per_batch': 3.3161767721176147, '_timestamp': 1740962066.2789812}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 236 is less than current step: 499. Dropping entry: {'train/lr': 9.373469387755102e-05, '_timestamp': 1740962066.2792437}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 237 is less than current step: 499. Dropping entry: {'train/loss': 1.5860968768596648, 'train/ce_loss': 0.38037109375, 'train/seg_cls_loss': 0.015643310546875, 'train/kl_loss': 0.2650390625, 'train/mask_bce_loss': 0.1224272521212697, 'train/mask_dice_loss': 0.4633457899093628, 'train/mask_loss': 0.585773041844368, 'metrics/total_secs_per_batch': 8.001251935958862, 'metrics/data_secs_per_batch': 3.379556727409363, '_timestamp': 1740962074.2802398}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 237 is less than current step: 499. Dropping entry: {'train/lr': 9.361224489795918e-05, '_timestamp': 1740962074.2805076}).
Epoch: [3][239/500]	Time  8.358 ( 8.358)	Loss 2.3392 (1.8189)	CeLoss 0.3418 (0.2055)	SegCLSLoss 0.0126 (0.0178)	KLLoss 0.3906 (0.3383)	MaskLoss 0.9763 (0.7854)	MaskBCELoss 0.1239 (0.1674)	MaskDICELoss 0.8523 (0.6180)
[2025-03-02 18:34:50,475] [INFO] [logging.py:96:log_dist] [Rank 0] step=1740, skipped=0, lr=[9.342857142857143e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:34:50,482] [INFO] [timer.py:215:stop] epoch=0/micro_step=17400/global_step=1740, RunningAvgSamplesPerSec=1.5358317965717168, CurrSamplesPerSec=1.2749830614939583, MemAllocated=30.8GB, MaxMemAllocated=37.19GB
Epoch: [3][240/500]	Time  7.845 ( 7.845)	Loss 1.4021 (1.7575)	CeLoss 0.2021 (0.2961)	SegCLSLoss 0.0114 (0.0174)	KLLoss 0.3750 (0.3377)	MaskLoss 0.5780 (0.7096)	MaskBCELoss 0.2117 (0.1070)	MaskDICELoss 0.3664 (0.6025)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 238 is less than current step: 499. Dropping entry: {'train/loss': 1.818858516216278, 'train/ce_loss': 0.205517578125, 'train/seg_cls_loss': 0.017791748046875, 'train/kl_loss': 0.33828125, 'train/mask_bce_loss': 0.16742647755891085, 'train/mask_dice_loss': 0.6179549306631088, 'train/mask_loss': 0.7853813946247101, 'metrics/total_secs_per_batch': 8.357852220535278, 'metrics/data_secs_per_batch': 3.625005030632019, '_timestamp': 1740962082.6380959}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 238 is less than current step: 499. Dropping entry: {'train/lr': 9.348979591836735e-05, '_timestamp': 1740962082.6383603}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 239 is less than current step: 499. Dropping entry: {'train/loss': 1.75749329328537, 'train/ce_loss': 0.29609375, 'train/seg_cls_loss': 0.0173828125, 'train/kl_loss': 0.3376953125, 'train/mask_bce_loss': 0.10703261569142342, 'train/mask_dice_loss': 0.6025245755910873, 'train/mask_loss': 0.709557193517685, 'metrics/total_secs_per_batch': 7.844742774963379, 'metrics/data_secs_per_batch': 3.3880100011825562, '_timestamp': 1740962090.482663}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 239 is less than current step: 499. Dropping entry: {'train/lr': 9.336734693877551e-05, '_timestamp': 1740962090.4829206}).
Epoch: [3][241/500]	Time  7.729 ( 7.729)	Loss 1.4731 (1.4196)	CeLoss 0.2754 (0.4601)	SegCLSLoss 0.0121 (0.0111)	KLLoss 0.3828 (0.2687)	MaskLoss 0.5764 (0.4636)	MaskBCELoss 0.2082 (0.0785)	MaskDICELoss 0.3682 (0.3851)
Epoch: [3][242/500]	Time  7.382 ( 7.382)	Loss 2.0981 (2.1561)	CeLoss 0.2871 (0.3737)	SegCLSLoss 0.0101 (0.0134)	KLLoss 0.3789 (0.3408)	MaskLoss 0.8840 (0.8710)	MaskBCELoss 0.2326 (0.2451)	MaskDICELoss 0.6515 (0.6259)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 240 is less than current step: 499. Dropping entry: {'train/loss': 1.4196353256702423, 'train/ce_loss': 0.46005859375, 'train/seg_cls_loss': 0.011090087890625, 'train/kl_loss': 0.26875, 'train/mask_bce_loss': 0.07853578142821789, 'train/mask_dice_loss': 0.3850904658436775, 'train/mask_loss': 0.46362624913454054, 'metrics/total_secs_per_batch': 7.729069948196411, 'metrics/data_secs_per_batch': 3.555956482887268, '_timestamp': 1740962098.2119157}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 240 is less than current step: 499. Dropping entry: {'train/lr': 9.324489795918367e-05, '_timestamp': 1740962098.212224}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 241 is less than current step: 499. Dropping entry: {'train/loss': 2.1561323404312134, 'train/ce_loss': 0.37373046875, 'train/seg_cls_loss': 0.0134033203125, 'train/kl_loss': 0.3408203125, 'train/mask_bce_loss': 0.2451140558347106, 'train/mask_dice_loss': 0.6258720487356186, 'train/mask_loss': 0.8709860861301422, 'metrics/total_secs_per_batch': 7.38205099105835, 'metrics/data_secs_per_batch': 3.4330498933792115, '_timestamp': 1740962105.5941558}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 241 is less than current step: 499. Dropping entry: {'train/lr': 9.312244897959183e-05, '_timestamp': 1740962105.5944827}).
Epoch: [3][243/500]	Time  8.993 ( 8.993)	Loss 1.3957 (1.7015)	CeLoss 0.2061 (0.4333)	SegCLSLoss 0.0222 (0.0133)	KLLoss 0.3613 (0.2996)	MaskLoss 0.5709 (0.6158)	MaskBCELoss 0.1427 (0.1302)	MaskDICELoss 0.4282 (0.4856)
Epoch: [3][244/500]	Time  6.382 ( 6.382)	Loss 2.7270 (1.7948)	CeLoss 0.1523 (0.4466)	SegCLSLoss 0.0305 (0.0140)	KLLoss 0.3652 (0.2637)	MaskLoss 1.2610 (0.6575)	MaskBCELoss 0.4893 (0.2312)	MaskDICELoss 0.7717 (0.4262)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 242 is less than current step: 499. Dropping entry: {'train/loss': 1.7014690279960631, 'train/ce_loss': 0.43330078125, 'train/seg_cls_loss': 0.01326904296875, 'train/kl_loss': 0.299609375, 'train/mask_bce_loss': 0.1301554936915636, 'train/mask_dice_loss': 0.4856180727481842, 'train/mask_loss': 0.6157735586166382, 'metrics/total_secs_per_batch': 8.99334454536438, 'metrics/data_secs_per_batch': 4.579906439781189, '_timestamp': 1740962114.587362}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 242 is less than current step: 499. Dropping entry: {'train/lr': 9.3e-05, '_timestamp': 1740962114.5876405}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 243 is less than current step: 499. Dropping entry: {'train/loss': 1.7947870522737503, 'train/ce_loss': 0.44658203125, 'train/seg_cls_loss': 0.013970947265625, 'train/kl_loss': 0.263671875, 'train/mask_bce_loss': 0.23124818727374077, 'train/mask_dice_loss': 0.4262039326131344, 'train/mask_loss': 0.6574521191418171, 'metrics/total_secs_per_batch': 6.38191556930542, 'metrics/data_secs_per_batch': 2.921270656585693, '_timestamp': 1740962120.9692712}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 243 is less than current step: 499. Dropping entry: {'train/lr': 9.287755102040815e-05, '_timestamp': 1740962120.9695802}).
Epoch: [3][245/500]	Time  5.956 ( 5.956)	Loss 1.6719 (1.6100)	CeLoss 1.6719 (0.5451)	SegCLSLoss 0.0000 (0.0155)	KLLoss 0.0000 (0.2223)	MaskLoss 0.0000 (0.5175)	MaskBCELoss 0.0000 (0.1192)	MaskDICELoss 0.0000 (0.3983)
Epoch: [3][246/500]	Time  6.764 ( 6.764)	Loss 0.0569 (1.5754)	CeLoss 0.0569 (0.4966)	SegCLSLoss 0.0000 (0.0108)	KLLoss 0.0000 (0.2258)	MaskLoss 0.0000 (0.5254)	MaskBCELoss 0.0000 (0.1027)	MaskDICELoss 0.0000 (0.4227)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 244 is less than current step: 499. Dropping entry: {'train/loss': 1.6100313067436218, 'train/ce_loss': 0.5451171875, 'train/seg_cls_loss': 0.01549072265625, 'train/kl_loss': 0.222265625, 'train/mask_bce_loss': 0.11918583549559117, 'train/mask_dice_loss': 0.39832982420921326, 'train/mask_loss': 0.5175156533718109, 'metrics/total_secs_per_batch': 5.956245422363281, 'metrics/data_secs_per_batch': 2.6666683912277223, '_timestamp': 1740962126.92548}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 244 is less than current step: 499. Dropping entry: {'train/lr': 9.275510204081631e-05, '_timestamp': 1740962126.9257476}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 245 is less than current step: 499. Dropping entry: {'train/loss': 1.5753729939460754, 'train/ce_loss': 0.4966064453125, 'train/seg_cls_loss': 0.010797119140625, 'train/kl_loss': 0.22578125, 'train/mask_bce_loss': 0.10269854906946421, 'train/mask_dice_loss': 0.42271987795829774, 'train/mask_loss': 0.5254184156656265, 'metrics/total_secs_per_batch': 6.763584852218628, 'metrics/data_secs_per_batch': 3.1827990770339967, '_timestamp': 1740962133.6890786}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 245 is less than current step: 499. Dropping entry: {'train/lr': 9.263265306122447e-05, '_timestamp': 1740962133.6893382}).
Epoch: [3][247/500]	Time  8.268 ( 8.268)	Loss 2.2170 (1.7369)	CeLoss 0.2500 (0.4119)	SegCLSLoss 0.0121 (0.0141)	KLLoss 0.3789 (0.3035)	MaskLoss 0.9620 (0.6439)	MaskBCELoss 0.3159 (0.1495)	MaskDICELoss 0.6461 (0.4944)
Epoch: [3][248/500]	Time  7.923 ( 7.923)	Loss 2.8366 (2.0029)	CeLoss 0.1719 (0.4593)	SegCLSLoss 0.0294 (0.0171)	KLLoss 0.3809 (0.3064)	MaskLoss 1.3060 (0.7521)	MaskBCELoss 0.5269 (0.2015)	MaskDICELoss 0.7791 (0.5505)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 246 is less than current step: 499. Dropping entry: {'train/loss': 1.7369191527366639, 'train/ce_loss': 0.4119140625, 'train/seg_cls_loss': 0.01412353515625, 'train/kl_loss': 0.303515625, 'train/mask_bce_loss': 0.14952161964029073, 'train/mask_dice_loss': 0.4944262370467186, 'train/mask_loss': 0.6439478546380997, 'metrics/total_secs_per_batch': 8.267912149429321, 'metrics/data_secs_per_batch': 3.5944360494613647, '_timestamp': 1740962141.9569721}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 246 is less than current step: 499. Dropping entry: {'train/lr': 9.251020408163264e-05, '_timestamp': 1740962141.957162}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 247 is less than current step: 499. Dropping entry: {'train/loss': 2.0029402494430544, 'train/ce_loss': 0.45927734375, 'train/seg_cls_loss': 0.017120361328125, 'train/kl_loss': 0.3064453125, 'train/mask_bce_loss': 0.20153635032474995, 'train/mask_dice_loss': 0.5505197167396545, 'train/mask_loss': 0.7520560622215271, 'metrics/total_secs_per_batch': 7.922509431838989, 'metrics/data_secs_per_batch': 3.5728962898254393, '_timestamp': 1740962149.8795407}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 247 is less than current step: 499. Dropping entry: {'train/lr': 9.23877551020408e-05, '_timestamp': 1740962149.8798215}).
Epoch: [3][249/500]	Time  8.790 ( 8.790)	Loss 0.0723 (1.6770)	CeLoss 0.0723 (0.3241)	SegCLSLoss 0.0000 (0.0156)	KLLoss 0.0000 (0.2648)	MaskLoss 0.0000 (0.6592)	MaskBCELoss 0.0000 (0.1482)	MaskDICELoss 0.0000 (0.5111)
[2025-03-02 18:36:06,276] [INFO] [logging.py:96:log_dist] [Rank 0] step=1750, skipped=0, lr=[9.220408163265306e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:36:06,282] [INFO] [timer.py:215:stop] epoch=0/micro_step=17500/global_step=1750, RunningAvgSamplesPerSec=1.5343922788086548, CurrSamplesPerSec=1.3135567900827825, MemAllocated=31.74GB, MaxMemAllocated=37.19GB
Epoch: [3][250/500]	Time  7.614 ( 7.614)	Loss 0.0991 (1.3301)	CeLoss 0.0991 (0.4905)	SegCLSLoss 0.0000 (0.0092)	KLLoss 0.0000 (0.2254)	MaskLoss 0.0000 (0.4061)	MaskBCELoss 0.0000 (0.0740)	MaskDICELoss 0.0000 (0.3321)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 248 is less than current step: 499. Dropping entry: {'train/loss': 1.6770381331443787, 'train/ce_loss': 0.324072265625, 'train/seg_cls_loss': 0.015594482421875, 'train/kl_loss': 0.26484375, 'train/mask_bce_loss': 0.1481625944375992, 'train/mask_dice_loss': 0.5110840052366257, 'train/mask_loss': 0.6592465937137604, 'metrics/total_secs_per_batch': 8.789660930633545, 'metrics/data_secs_per_batch': 3.8609176158905028, '_timestamp': 1740962158.6691716}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 248 is less than current step: 499. Dropping entry: {'train/lr': 9.226530612244898e-05, '_timestamp': 1740962158.669446}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 249 is less than current step: 499. Dropping entry: {'train/loss': 1.3301265776157378, 'train/ce_loss': 0.490478515625, 'train/seg_cls_loss': 0.009228515625, 'train/kl_loss': 0.225390625, 'train/mask_bce_loss': 0.07403795085847378, 'train/mask_dice_loss': 0.3320653632283211, 'train/mask_loss': 0.40610331594944, 'metrics/total_secs_per_batch': 7.614427804946899, 'metrics/data_secs_per_batch': 3.4581285953521728, '_timestamp': 1740962166.283419}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 249 is less than current step: 499. Dropping entry: {'train/lr': 9.214285714285714e-05, '_timestamp': 1740962166.283679}).
Epoch: [3][251/500]	Time  8.816 ( 8.816)	Loss 1.6158 (1.6067)	CeLoss 0.2266 (0.3939)	SegCLSLoss 0.0168 (0.0128)	KLLoss 0.3711 (0.2975)	MaskLoss 0.6712 (0.5883)	MaskBCELoss 0.0740 (0.0764)	MaskDICELoss 0.5972 (0.5119)
Epoch: [3][252/500]	Time  6.136 ( 6.136)	Loss 1.4492 (1.6384)	CeLoss 0.2520 (0.7896)	SegCLSLoss 0.0117 (0.0098)	KLLoss 0.3789 (0.1902)	MaskLoss 0.5771 (0.4125)	MaskBCELoss 0.2437 (0.0461)	MaskDICELoss 0.3334 (0.3663)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 250 is less than current step: 499. Dropping entry: {'train/loss': 1.606749552488327, 'train/ce_loss': 0.3939453125, 'train/seg_cls_loss': 0.012811279296875, 'train/kl_loss': 0.2974609375, 'train/mask_bce_loss': 0.07643433418124915, 'train/mask_dice_loss': 0.5119013637304306, 'train/mask_loss': 0.5883356958627701, 'metrics/total_secs_per_batch': 8.816363334655762, 'metrics/data_secs_per_batch': 3.415001654624939, '_timestamp': 1740962175.1000144}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 250 is less than current step: 499. Dropping entry: {'train/lr': 9.20204081632653e-05, '_timestamp': 1740962175.1003096}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 251 is less than current step: 499. Dropping entry: {'train/loss': 1.6384137392044067, 'train/ce_loss': 0.7896484375, 'train/seg_cls_loss': 0.009771728515625, 'train/kl_loss': 0.190234375, 'train/mask_bce_loss': 0.0461313871666789, 'train/mask_dice_loss': 0.36633720993995667, 'train/mask_loss': 0.41246858835220335, 'metrics/total_secs_per_batch': 6.136109352111816, 'metrics/data_secs_per_batch': 2.681341314315796, '_timestamp': 1740962181.2360742}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 251 is less than current step: 499. Dropping entry: {'train/lr': 9.189795918367346e-05, '_timestamp': 1740962181.2363427}).
Epoch: [3][253/500]	Time  8.941 ( 8.941)	Loss 2.5713 (1.7328)	CeLoss 0.1855 (0.3913)	SegCLSLoss 0.0266 (0.0140)	KLLoss 0.3867 (0.3045)	MaskLoss 1.1665 (0.6520)	MaskBCELoss 0.3198 (0.1115)	MaskDICELoss 0.8467 (0.5405)
Epoch: [3][254/500]	Time  6.184 ( 6.184)	Loss 1.6797 (1.8860)	CeLoss 1.6797 (0.5577)	SegCLSLoss 0.0000 (0.0146)	KLLoss 0.0000 (0.2273)	MaskLoss 0.0000 (0.6490)	MaskBCELoss 0.0000 (0.1429)	MaskDICELoss 0.0000 (0.5062)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 252 is less than current step: 499. Dropping entry: {'train/loss': 1.7327809095382691, 'train/ce_loss': 0.39130859375, 'train/seg_cls_loss': 0.01402587890625, 'train/kl_loss': 0.3044921875, 'train/mask_bce_loss': 0.11152778696268797, 'train/mask_dice_loss': 0.5405071884393692, 'train/mask_loss': 0.6520349740982055, 'metrics/total_secs_per_batch': 8.941441059112549, 'metrics/data_secs_per_batch': 3.6834022283554075, '_timestamp': 1740962190.1776743}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 252 is less than current step: 499. Dropping entry: {'train/lr': 9.177551020408163e-05, '_timestamp': 1740962190.1779}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 253 is less than current step: 499. Dropping entry: {'train/loss': 1.8859529614448547, 'train/ce_loss': 0.55771484375, 'train/seg_cls_loss': 0.014556884765625, 'train/kl_loss': 0.22734375, 'train/mask_bce_loss': 0.1428689543157816, 'train/mask_dice_loss': 0.5061622083187103, 'train/mask_loss': 0.6490311563014984, 'metrics/total_secs_per_batch': 6.184344053268433, 'metrics/data_secs_per_batch': 3.1021883726119994, '_timestamp': 1740962196.361903}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 253 is less than current step: 499. Dropping entry: {'train/lr': 9.165306122448979e-05, '_timestamp': 1740962196.3621814}).
Epoch: [3][255/500]	Time  8.878 ( 8.878)	Loss 2.1531 (1.7365)	CeLoss 0.2148 (0.2234)	SegCLSLoss 0.0249 (0.0166)	KLLoss 0.3809 (0.3410)	MaskLoss 0.9437 (0.7353)	MaskBCELoss 0.0492 (0.1415)	MaskDICELoss 0.8945 (0.5937)
Epoch: [3][256/500]	Time  8.037 ( 8.037)	Loss 2.1133 (1.4561)	CeLoss 0.2090 (0.3836)	SegCLSLoss 0.0212 (0.0112)	KLLoss 0.3633 (0.2244)	MaskLoss 0.9287 (0.5222)	MaskBCELoss 0.0116 (0.1174)	MaskDICELoss 0.9172 (0.4048)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 254 is less than current step: 499. Dropping entry: {'train/loss': 1.7364689111709595, 'train/ce_loss': 0.223388671875, 'train/seg_cls_loss': 0.0165771484375, 'train/kl_loss': 0.341015625, 'train/mask_bce_loss': 0.14150806665420532, 'train/mask_dice_loss': 0.5937429785728454, 'train/mask_loss': 0.7352510452270508, 'metrics/total_secs_per_batch': 8.878138303756714, 'metrics/data_secs_per_batch': 3.9334561824798584, '_timestamp': 1740962205.2400093}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 254 is less than current step: 499. Dropping entry: {'train/lr': 9.153061224489795e-05, '_timestamp': 1740962205.2402785}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 255 is less than current step: 499. Dropping entry: {'train/loss': 1.456073236465454, 'train/ce_loss': 0.383642578125, 'train/seg_cls_loss': 0.011163330078125, 'train/kl_loss': 0.2244140625, 'train/mask_bce_loss': 0.11736522866412998, 'train/mask_dice_loss': 0.40478759706020356, 'train/mask_loss': 0.5221528232097625, 'metrics/total_secs_per_batch': 8.037440299987793, 'metrics/data_secs_per_batch': 3.2069206714630125, '_timestamp': 1740962213.2774448}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 255 is less than current step: 499. Dropping entry: {'train/lr': 9.140816326530612e-05, '_timestamp': 1740962213.2777529}).
Epoch: [3][257/500]	Time  6.601 ( 6.601)	Loss 1.2628 (1.5227)	CeLoss 0.2217 (0.5196)	SegCLSLoss 0.0131 (0.0108)	KLLoss 0.3711 (0.2244)	MaskLoss 0.4986 (0.4876)	MaskBCELoss 0.0955 (0.0569)	MaskDICELoss 0.4031 (0.4307)
Epoch: [3][258/500]	Time  8.487 ( 8.487)	Loss 1.0801 (1.9170)	CeLoss 0.2656 (0.2205)	SegCLSLoss 0.0129 (0.0193)	KLLoss 0.3926 (0.3762)	MaskLoss 0.3848 (0.8245)	MaskBCELoss 0.1301 (0.1736)	MaskDICELoss 0.2547 (0.6510)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 256 is less than current step: 499. Dropping entry: {'train/loss': 1.5226595044136046, 'train/ce_loss': 0.51962890625, 'train/seg_cls_loss': 0.01082763671875, 'train/kl_loss': 0.2244140625, 'train/mask_bce_loss': 0.056900351867079736, 'train/mask_dice_loss': 0.4307477504014969, 'train/mask_loss': 0.48764810860157015, 'metrics/total_secs_per_batch': 6.601484537124634, 'metrics/data_secs_per_batch': 2.4554341793060304, '_timestamp': 1740962219.8789504}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 256 is less than current step: 499. Dropping entry: {'train/lr': 9.128571428571427e-05, '_timestamp': 1740962219.8792186}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 257 is less than current step: 499. Dropping entry: {'train/loss': 1.9169697046279908, 'train/ce_loss': 0.2205078125, 'train/seg_cls_loss': 0.019317626953125, 'train/kl_loss': 0.376171875, 'train/mask_bce_loss': 0.17355575179681182, 'train/mask_dice_loss': 0.6509935542941093, 'train/mask_loss': 0.8245493054389954, 'metrics/total_secs_per_batch': 8.48672080039978, 'metrics/data_secs_per_batch': 3.8430359601974486, '_timestamp': 1740962228.3656616}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 257 is less than current step: 499. Dropping entry: {'train/lr': 9.116326530612243e-05, '_timestamp': 1740962228.3659375}).
Epoch: [3][259/500]	Time  7.134 ( 7.134)	Loss 2.7670 (1.2658)	CeLoss 0.1680 (0.4936)	SegCLSLoss 0.0243 (0.0086)	KLLoss 0.3711 (0.1492)	MaskLoss 1.2751 (0.3765)	MaskBCELoss 0.5235 (0.1232)	MaskDICELoss 0.7516 (0.2534)
[2025-03-02 18:37:23,131] [INFO] [logging.py:96:log_dist] [Rank 0] step=1760, skipped=0, lr=[9.097959183673467e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:37:23,136] [INFO] [timer.py:215:stop] epoch=0/micro_step=17600/global_step=1760, RunningAvgSamplesPerSec=1.5328310576752886, CurrSamplesPerSec=1.3096121499718831, MemAllocated=30.78GB, MaxMemAllocated=37.19GB
Epoch: [3][260/500]	Time  7.638 ( 7.638)	Loss 0.7617 (1.4939)	CeLoss 0.7617 (0.5267)	SegCLSLoss 0.0000 (0.0101)	KLLoss 0.0000 (0.1877)	MaskLoss 0.0000 (0.4717)	MaskBCELoss 0.0000 (0.1382)	MaskDICELoss 0.0000 (0.3336)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 258 is less than current step: 499. Dropping entry: {'train/loss': 1.2657815098762513, 'train/ce_loss': 0.493603515625, 'train/seg_cls_loss': 0.00855712890625, 'train/kl_loss': 0.14921875, 'train/mask_bce_loss': 0.12316146418452263, 'train/mask_dice_loss': 0.25335723385214803, 'train/mask_loss': 0.3765186846256256, 'metrics/total_secs_per_batch': 7.134174346923828, 'metrics/data_secs_per_batch': 3.480836296081543, '_timestamp': 1740962235.49993}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 258 is less than current step: 499. Dropping entry: {'train/lr': 9.104081632653059e-05, '_timestamp': 1740962235.5002925}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 259 is less than current step: 499. Dropping entry: {'train/loss': 1.493888533115387, 'train/ce_loss': 0.526708984375, 'train/seg_cls_loss': 0.010089111328125, 'train/kl_loss': 0.1876953125, 'train/mask_bce_loss': 0.1381534207612276, 'train/mask_dice_loss': 0.3335711181163788, 'train/mask_loss': 0.471724534034729, 'metrics/total_secs_per_batch': 7.637542724609375, 'metrics/data_secs_per_batch': 3.034886693954468, '_timestamp': 1740962243.137244}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 259 is less than current step: 499. Dropping entry: {'train/lr': 9.091836734693877e-05, '_timestamp': 1740962243.1375349}).
Epoch: [3][261/500]	Time  7.299 ( 7.299)	Loss 1.9490 (1.9411)	CeLoss 0.2139 (0.3452)	SegCLSLoss 0.0223 (0.0199)	KLLoss 0.3750 (0.3000)	MaskLoss 0.8437 (0.7780)	MaskBCELoss 0.1026 (0.1849)	MaskDICELoss 0.7410 (0.5930)
Epoch: [3][262/500]	Time  9.664 ( 9.664)	Loss 1.7542 (2.1103)	CeLoss 0.1787 (0.2306)	SegCLSLoss 0.0295 (0.0200)	KLLoss 0.3789 (0.3740)	MaskLoss 0.7614 (0.9162)	MaskBCELoss 0.1209 (0.1638)	MaskDICELoss 0.6405 (0.7524)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 260 is less than current step: 499. Dropping entry: {'train/loss': 1.941050612926483, 'train/ce_loss': 0.34521484375, 'train/seg_cls_loss': 0.019903564453125, 'train/kl_loss': 0.3, 'train/mask_bce_loss': 0.18494672169908882, 'train/mask_dice_loss': 0.5930492669343949, 'train/mask_loss': 0.7779959857463836, 'metrics/total_secs_per_batch': 7.299067974090576, 'metrics/data_secs_per_batch': 2.8645437002182006, '_timestamp': 1740962250.4365015}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 260 is less than current step: 499. Dropping entry: {'train/lr': 9.079591836734693e-05, '_timestamp': 1740962250.4367044}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 261 is less than current step: 499. Dropping entry: {'train/loss': 2.110255682468414, 'train/ce_loss': 0.23056640625, 'train/seg_cls_loss': 0.020001220703125, 'train/kl_loss': 0.3740234375, 'train/mask_bce_loss': 0.1637897846288979, 'train/mask_dice_loss': 0.7524220407009125, 'train/mask_loss': 0.9162118196487427, 'metrics/total_secs_per_batch': 9.664148807525635, 'metrics/data_secs_per_batch': 4.203717422485352, '_timestamp': 1740962260.100622}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 261 is less than current step: 499. Dropping entry: {'train/lr': 9.06734693877551e-05, '_timestamp': 1740962260.1009014}).
Epoch: [3][263/500]	Time  8.893 ( 8.893)	Loss 1.2010 (2.0638)	CeLoss 0.2559 (0.3198)	SegCLSLoss 0.0139 (0.0173)	KLLoss 0.3848 (0.3350)	MaskLoss 0.4491 (0.8510)	MaskBCELoss 0.0866 (0.2502)	MaskDICELoss 0.3626 (0.6008)
Epoch: [3][264/500]	Time  7.621 ( 7.621)	Loss 1.2687 (1.6105)	CeLoss 0.2676 (0.4193)	SegCLSLoss 0.0111 (0.0139)	KLLoss 0.3809 (0.3014)	MaskLoss 0.4781 (0.5772)	MaskBCELoss 0.1023 (0.0849)	MaskDICELoss 0.3759 (0.4923)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 262 is less than current step: 499. Dropping entry: {'train/loss': 2.063829267024994, 'train/ce_loss': 0.31982421875, 'train/seg_cls_loss': 0.01732177734375, 'train/kl_loss': 0.3349609375, 'train/mask_bce_loss': 0.2501634370535612, 'train/mask_dice_loss': 0.6007941544055939, 'train/mask_loss': 0.8509575992822647, 'metrics/total_secs_per_batch': 8.893189668655396, 'metrics/data_secs_per_batch': 3.7589075326919557, '_timestamp': 1740962268.993793}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 262 is less than current step: 499. Dropping entry: {'train/lr': 9.055102040816326e-05, '_timestamp': 1740962268.9940634}).
Epoch: [3][265/500]	Time  6.545 ( 6.545)	Loss 2.4185 (1.4978)	CeLoss 0.1982 (0.2872)	SegCLSLoss 0.0251 (0.0165)	KLLoss 0.3809 (0.3018)	MaskLoss 1.0848 (0.5861)	MaskBCELoss 0.2834 (0.0790)	MaskDICELoss 0.8013 (0.5071)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 263 is less than current step: 499. Dropping entry: {'train/loss': 1.6105227053165436, 'train/ce_loss': 0.4193359375, 'train/seg_cls_loss': 0.013885498046875, 'train/kl_loss': 0.3013671875, 'train/mask_bce_loss': 0.08488941602408887, 'train/mask_dice_loss': 0.4923445925116539, 'train/mask_loss': 0.577234010398388, 'metrics/total_secs_per_batch': 7.620936393737793, 'metrics/data_secs_per_batch': 3.6001391410827637, '_timestamp': 1740962276.6147335}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 263 is less than current step: 499. Dropping entry: {'train/lr': 9.042857142857142e-05, '_timestamp': 1740962276.614914}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 264 is less than current step: 499. Dropping entry: {'train/loss': 1.4978356689214707, 'train/ce_loss': 0.28720703125, 'train/seg_cls_loss': 0.01650390625, 'train/kl_loss': 0.3017578125, 'train/mask_bce_loss': 0.07901331027969719, 'train/mask_dice_loss': 0.5070627212524415, 'train/mask_loss': 0.5860760286450386, 'metrics/total_secs_per_batch': 6.544751405715942, 'metrics/data_secs_per_batch': 3.2263649702072144, '_timestamp': 1740962283.159538}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 264 is less than current step: 499. Dropping entry: {'train/lr': 9.030612244897958e-05, '_timestamp': 1740962283.1598063}).
Epoch: [3][266/500]	Time  8.611 ( 8.611)	Loss 2.4513 (1.5937)	CeLoss 0.1709 (0.2734)	SegCLSLoss 0.0205 (0.0160)	KLLoss 0.3809 (0.3453)	MaskLoss 1.1158 (0.6387)	MaskBCELoss 0.2101 (0.0764)	MaskDICELoss 0.9057 (0.5623)
Epoch: [3][267/500]	Time  7.549 ( 7.549)	Loss 2.1954 (1.9263)	CeLoss 0.2168 (0.5355)	SegCLSLoss 0.0248 (0.0141)	KLLoss 0.3652 (0.2666)	MaskLoss 0.9649 (0.6786)	MaskBCELoss 0.2672 (0.1291)	MaskDICELoss 0.6977 (0.5495)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 265 is less than current step: 499. Dropping entry: {'train/loss': 1.5936628103256225, 'train/ce_loss': 0.2734375, 'train/seg_cls_loss': 0.016015625, 'train/kl_loss': 0.3453125, 'train/mask_bce_loss': 0.07635161327198148, 'train/mask_dice_loss': 0.5623254880309105, 'train/mask_loss': 0.6386771127581596, 'metrics/total_secs_per_batch': 8.611200332641602, 'metrics/data_secs_per_batch': 4.049207544326782, '_timestamp': 1740962291.7706928}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 265 is less than current step: 499. Dropping entry: {'train/lr': 9.018367346938775e-05, '_timestamp': 1740962291.7709558}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 266 is less than current step: 499. Dropping entry: {'train/loss': 1.926290762424469, 'train/ce_loss': 0.535498046875, 'train/seg_cls_loss': 0.01405029296875, 'train/kl_loss': 0.2666015625, 'train/mask_bce_loss': 0.12912197289988397, 'train/mask_dice_loss': 0.54945307970047, 'train/mask_loss': 0.6785750567913056, 'metrics/total_secs_per_batch': 7.5491578578948975, 'metrics/data_secs_per_batch': 3.706727910041809, '_timestamp': 1740962299.3198612}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 266 is less than current step: 499. Dropping entry: {'train/lr': 9.006122448979591e-05, '_timestamp': 1740962299.3201761}).
Epoch: [3][268/500]	Time 10.064 (10.064)	Loss 1.9149 (1.6806)	CeLoss 0.2637 (0.1983)	SegCLSLoss 0.0147 (0.0174)	KLLoss 0.3809 (0.3393)	MaskLoss 0.8031 (0.7198)	MaskBCELoss 0.0176 (0.0715)	MaskDICELoss 0.7856 (0.6482)
Epoch: [3][269/500]	Time  7.392 ( 7.392)	Loss 2.3129 (1.7002)	CeLoss 0.2090 (0.4113)	SegCLSLoss 0.0186 (0.0167)	KLLoss 0.3672 (0.2986)	MaskLoss 1.0285 (0.6253)	MaskBCELoss 0.0340 (0.1120)	MaskDICELoss 0.9945 (0.5133)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 267 is less than current step: 499. Dropping entry: {'train/loss': 1.6806226015090941, 'train/ce_loss': 0.19833984375, 'train/seg_cls_loss': 0.017425537109375, 'train/kl_loss': 0.3392578125, 'train/mask_bce_loss': 0.07153423340059817, 'train/mask_dice_loss': 0.6482204228639603, 'train/mask_loss': 0.7197546541690827, 'metrics/total_secs_per_batch': 10.06421160697937, 'metrics/data_secs_per_batch': 4.886304903030395, '_timestamp': 1740962309.38407}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 267 is less than current step: 499. Dropping entry: {'train/lr': 8.993877551020407e-05, '_timestamp': 1740962309.3843334}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 268 is less than current step: 499. Dropping entry: {'train/loss': 1.7001832187175752, 'train/ce_loss': 0.411328125, 'train/seg_cls_loss': 0.016693115234375, 'train/kl_loss': 0.2986328125, 'train/mask_bce_loss': 0.11198242101818323, 'train/mask_dice_loss': 0.5133044809103012, 'train/mask_loss': 0.6252869009971619, 'metrics/total_secs_per_batch': 7.391528129577637, 'metrics/data_secs_per_batch': 3.2422842741012574, '_timestamp': 1740962316.775837}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 268 is less than current step: 499. Dropping entry: {'train/lr': 8.981632653061224e-05, '_timestamp': 1740962316.7761886}).
[2025-03-02 18:38:44,394] [INFO] [logging.py:96:log_dist] [Rank 0] step=1770, skipped=0, lr=[8.975510204081632e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:38:44,401] [INFO] [timer.py:215:stop] epoch=0/micro_step=17700/global_step=1770, RunningAvgSamplesPerSec=1.5307058700521707, CurrSamplesPerSec=1.3116065640618957, MemAllocated=30.94GB, MaxMemAllocated=37.19GB
Epoch: [3][270/500]	Time  7.626 ( 7.626)	Loss 1.7998 (1.4789)	CeLoss 0.2119 (0.4310)	SegCLSLoss 0.0278 (0.0115)	KLLoss 0.3809 (0.2264)	MaskLoss 0.7681 (0.5098)	MaskBCELoss 0.2850 (0.1275)	MaskDICELoss 0.4830 (0.3823)
Epoch: [3][271/500]	Time  8.252 ( 8.252)	Loss 1.5689 (1.3895)	CeLoss 0.2969 (0.3810)	SegCLSLoss 0.0117 (0.0107)	KLLoss 0.3770 (0.2232)	MaskLoss 0.6145 (0.4905)	MaskBCELoss 0.1507 (0.0577)	MaskDICELoss 0.4638 (0.4327)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 269 is less than current step: 499. Dropping entry: {'train/loss': 1.4788909077644348, 'train/ce_loss': 0.431005859375, 'train/seg_cls_loss': 0.01146240234375, 'train/kl_loss': 0.2263671875, 'train/mask_bce_loss': 0.1274632155895233, 'train/mask_dice_loss': 0.38231914639472964, 'train/mask_loss': 0.509782361984253, 'metrics/total_secs_per_batch': 7.626071453094482, 'metrics/data_secs_per_batch': 3.1365025997161866, '_timestamp': 1740962324.401516}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 269 is less than current step: 499. Dropping entry: {'train/lr': 8.96938775510204e-05, '_timestamp': 1740962324.4017851}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 270 is less than current step: 499. Dropping entry: {'train/loss': 1.3894839644432069, 'train/ce_loss': 0.381005859375, 'train/seg_cls_loss': 0.0107421875, 'train/kl_loss': 0.2232421875, 'train/mask_bce_loss': 0.0577341565862298, 'train/mask_dice_loss': 0.43273537456989286, 'train/mask_loss': 0.4904695272445679, 'metrics/total_secs_per_batch': 8.251997709274292, 'metrics/data_secs_per_batch': 3.4038946628570557, '_timestamp': 1740962332.6536787}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 270 is less than current step: 499. Dropping entry: {'train/lr': 8.957142857142858e-05, '_timestamp': 1740962332.653948}).
Epoch: [3][272/500]	Time  7.488 ( 7.488)	Loss 2.7245 (1.6153)	CeLoss 0.1406 (0.4372)	SegCLSLoss 0.0369 (0.0134)	KLLoss 0.3789 (0.2666)	MaskLoss 1.2636 (0.5723)	MaskBCELoss 0.5467 (0.1583)	MaskDICELoss 0.7169 (0.4140)
Epoch: [3][273/500]	Time  8.218 ( 8.218)	Loss 1.5679 (1.6341)	CeLoss 0.2695 (0.3735)	SegCLSLoss 0.0115 (0.0121)	KLLoss 0.4004 (0.3049)	MaskLoss 0.6258 (0.6119)	MaskBCELoss 0.0584 (0.1361)	MaskDICELoss 0.5674 (0.4758)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 271 is less than current step: 499. Dropping entry: {'train/loss': 1.61533682346344, 'train/ce_loss': 0.43720703125, 'train/seg_cls_loss': 0.01343994140625, 'train/kl_loss': 0.2666015625, 'train/mask_bce_loss': 0.15832909494638442, 'train/mask_dice_loss': 0.4139877438545227, 'train/mask_loss': 0.5723168343305588, 'metrics/total_secs_per_batch': 7.487787961959839, 'metrics/data_secs_per_batch': 3.0506221771240236, '_timestamp': 1740962340.141475}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 271 is less than current step: 499. Dropping entry: {'train/lr': 8.944897959183674e-05, '_timestamp': 1740962340.141743}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 272 is less than current step: 499. Dropping entry: {'train/loss': 1.6340558648109436, 'train/ce_loss': 0.373486328125, 'train/seg_cls_loss': 0.012109375, 'train/kl_loss': 0.3048828125, 'train/mask_bce_loss': 0.1361284555401653, 'train/mask_dice_loss': 0.4757969409227371, 'train/mask_loss': 0.6119253933429718, 'metrics/total_secs_per_batch': 8.218088388442993, 'metrics/data_secs_per_batch': 3.373459005355835, '_timestamp': 1740962348.3595672}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 272 is less than current step: 499. Dropping entry: {'train/lr': 8.932653061224489e-05, '_timestamp': 1740962348.3598423}).
Epoch: [3][274/500]	Time  7.523 ( 7.523)	Loss 2.2606 (1.7128)	CeLoss 0.2324 (0.4614)	SegCLSLoss 0.0117 (0.0146)	KLLoss 0.3867 (0.2611)	MaskLoss 0.9916 (0.6091)	MaskBCELoss 0.4253 (0.1223)	MaskDICELoss 0.5664 (0.4868)
Epoch: [3][275/500]	Time  8.734 ( 8.734)	Loss 2.0219 (1.5549)	CeLoss 0.2041 (0.3455)	SegCLSLoss 0.0231 (0.0163)	KLLoss 0.3672 (0.3404)	MaskLoss 0.8850 (0.5838)	MaskBCELoss 0.1395 (0.1236)	MaskDICELoss 0.7455 (0.4602)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 273 is less than current step: 499. Dropping entry: {'train/loss': 1.7128185510635376, 'train/ce_loss': 0.46142578125, 'train/seg_cls_loss': 0.014556884765625, 'train/kl_loss': 0.2611328125, 'train/mask_bce_loss': 0.12225025594234466, 'train/mask_dice_loss': 0.48684456944465637, 'train/mask_loss': 0.6090948224067688, 'metrics/total_secs_per_batch': 7.5228822231292725, 'metrics/data_secs_per_batch': 3.041597318649292, '_timestamp': 1740962355.8824515}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 273 is less than current step: 499. Dropping entry: {'train/lr': 8.920408163265305e-05, '_timestamp': 1740962355.8826444}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 274 is less than current step: 499. Dropping entry: {'train/loss': 1.554881465435028, 'train/ce_loss': 0.3455078125, 'train/seg_cls_loss': 0.016265869140625, 'train/kl_loss': 0.3404296875, 'train/mask_bce_loss': 0.12362250201404094, 'train/mask_dice_loss': 0.46016588285565374, 'train/mask_loss': 0.5837883859872818, 'metrics/total_secs_per_batch': 8.734435081481934, 'metrics/data_secs_per_batch': 3.500019383430481, '_timestamp': 1740962364.6168802}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 274 is less than current step: 499. Dropping entry: {'train/lr': 8.908163265306122e-05, '_timestamp': 1740962364.6171422}).
Epoch: [3][276/500]	Time  8.520 ( 8.520)	Loss 2.3355 (1.9071)	CeLoss 0.2207 (0.3231)	SegCLSLoss 0.0188 (0.0157)	KLLoss 0.4082 (0.3410)	MaskLoss 1.0320 (0.7708)	MaskBCELoss 0.0322 (0.1254)	MaskDICELoss 0.9999 (0.6454)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 275 is less than current step: 499. Dropping entry: {'train/loss': 1.9071073412895203, 'train/ce_loss': 0.32314453125, 'train/seg_cls_loss': 0.0157470703125, 'train/kl_loss': 0.341015625, 'train/mask_bce_loss': 0.12539174258708954, 'train/mask_dice_loss': 0.645447063446045, 'train/mask_loss': 0.7708388090133667, 'metrics/total_secs_per_batch': 8.519589185714722, 'metrics/data_secs_per_batch': 3.3381521701812744, '_timestamp': 1740962373.1364825}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 275 is less than current step: 499. Dropping entry: {'train/lr': 8.895918367346938e-05, '_timestamp': 1740962373.13675}).
Epoch: [3][277/500]	Time  9.281 ( 9.281)	Loss 1.5312 (1.5735)	CeLoss 1.5312 (0.4698)	SegCLSLoss 0.0000 (0.0120)	KLLoss 0.0000 (0.3039)	MaskLoss 0.0000 (0.5337)	MaskBCELoss 0.0000 (0.0598)	MaskDICELoss 0.0000 (0.4740)
Epoch: [3][278/500]	Time  6.404 ( 6.404)	Loss 0.1895 (1.6454)	CeLoss 0.1895 (0.6399)	SegCLSLoss 0.0000 (0.0082)	KLLoss 0.0000 (0.1867)	MaskLoss 0.0000 (0.4915)	MaskBCELoss 0.0000 (0.0834)	MaskDICELoss 0.0000 (0.4081)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 276 is less than current step: 499. Dropping entry: {'train/loss': 1.5734989762306213, 'train/ce_loss': 0.46982421875, 'train/seg_cls_loss': 0.01201171875, 'train/kl_loss': 0.30390625, 'train/mask_bce_loss': 0.05976605000905692, 'train/mask_dice_loss': 0.473956099152565, 'train/mask_loss': 0.5337221443653106, 'metrics/total_secs_per_batch': 9.281111240386963, 'metrics/data_secs_per_batch': 4.660493493080139, '_timestamp': 1740962382.4177551}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 276 is less than current step: 499. Dropping entry: {'train/lr': 8.883673469387754e-05, '_timestamp': 1740962382.4180875}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 277 is less than current step: 499. Dropping entry: {'train/loss': 1.6454235196113587, 'train/ce_loss': 0.63994140625, 'train/seg_cls_loss': 0.008160400390625, 'train/kl_loss': 0.18671875, 'train/mask_bce_loss': 0.08341358844190835, 'train/mask_dice_loss': 0.40809699296951296, 'train/mask_loss': 0.4915105879306793, 'metrics/total_secs_per_batch': 6.40418553352356, 'metrics/data_secs_per_batch': 2.717490482330322, '_timestamp': 1740962388.8218052}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 277 is less than current step: 499. Dropping entry: {'train/lr': 8.87142857142857e-05, '_timestamp': 1740962388.8220818}).
Epoch: [3][279/500]	Time  8.505 ( 8.505)	Loss 1.3047 (1.8830)	CeLoss 1.3047 (0.2966)	SegCLSLoss 0.0000 (0.0216)	KLLoss 0.0000 (0.3367)	MaskLoss 0.0000 (0.7710)	MaskBCELoss 0.0000 (0.1349)	MaskDICELoss 0.0000 (0.6361)
[2025-03-02 18:40:04,934] [INFO] [logging.py:96:log_dist] [Rank 0] step=1780, skipped=0, lr=[8.853061224489795e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:40:04,941] [INFO] [timer.py:215:stop] epoch=0/micro_step=17800/global_step=1780, RunningAvgSamplesPerSec=1.5287055179416231, CurrSamplesPerSec=1.313409904528534, MemAllocated=30.81GB, MaxMemAllocated=37.19GB
Epoch: [3][280/500]	Time  7.615 ( 7.615)	Loss 2.6020 (1.6265)	CeLoss 0.1660 (0.4831)	SegCLSLoss 0.0344 (0.0116)	KLLoss 0.3828 (0.2238)	MaskLoss 1.1902 (0.5577)	MaskBCELoss 0.3606 (0.1172)	MaskDICELoss 0.8295 (0.4405)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 278 is less than current step: 499. Dropping entry: {'train/loss': 1.8830030262470245, 'train/ce_loss': 0.29658203125, 'train/seg_cls_loss': 0.021575927734375, 'train/kl_loss': 0.33671875, 'train/mask_bce_loss': 0.13493972420692443, 'train/mask_dice_loss': 0.6361027985811234, 'train/mask_loss': 0.7710425287485123, 'metrics/total_secs_per_batch': 8.504744291305542, 'metrics/data_secs_per_batch': 4.2957984685897825, '_timestamp': 1740962397.3265338}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 278 is less than current step: 499. Dropping entry: {'train/lr': 8.859183673469387e-05, '_timestamp': 1740962397.3267925}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 279 is less than current step: 499. Dropping entry: {'train/loss': 1.626496398448944, 'train/ce_loss': 0.483056640625, 'train/seg_cls_loss': 0.011602783203125, 'train/kl_loss': 0.223828125, 'train/mask_bce_loss': 0.1171665471047163, 'train/mask_dice_loss': 0.4404908210039139, 'train/mask_loss': 0.5576573789119721, 'metrics/total_secs_per_batch': 7.615246295928955, 'metrics/data_secs_per_batch': 3.4610442399978636, '_timestamp': 1740962404.9416}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 279 is less than current step: 499. Dropping entry: {'train/lr': 8.846938775510203e-05, '_timestamp': 1740962404.9418573}).
Epoch: [3][281/500]	Time  8.246 ( 8.246)	Loss 0.7031 (1.7870)	CeLoss 0.7031 (0.4082)	SegCLSLoss 0.0000 (0.0157)	KLLoss 0.0000 (0.2582)	MaskLoss 0.0000 (0.6727)	MaskBCELoss 0.0000 (0.1315)	MaskDICELoss 0.0000 (0.5411)
Epoch: [3][282/500]	Time  7.268 ( 7.268)	Loss 1.8419 (1.5007)	CeLoss 0.2891 (0.4272)	SegCLSLoss 0.0099 (0.0109)	KLLoss 0.3711 (0.2248)	MaskLoss 0.7549 (0.5228)	MaskBCELoss 0.3825 (0.2035)	MaskDICELoss 0.3724 (0.3193)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 280 is less than current step: 499. Dropping entry: {'train/loss': 1.7870024919509888, 'train/ce_loss': 0.408203125, 'train/seg_cls_loss': 0.01572265625, 'train/kl_loss': 0.258203125, 'train/mask_bce_loss': 0.13154451809823514, 'train/mask_dice_loss': 0.5411071240901947, 'train/mask_loss': 0.6726516366004944, 'metrics/total_secs_per_batch': 8.24642539024353, 'metrics/data_secs_per_batch': 3.770542550086975, '_timestamp': 1740962413.1882105}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 280 is less than current step: 499. Dropping entry: {'train/lr': 8.83469387755102e-05, '_timestamp': 1740962413.1884692}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 281 is less than current step: 499. Dropping entry: {'train/loss': 1.5006746351718903, 'train/ce_loss': 0.42724609375, 'train/seg_cls_loss': 0.01085205078125, 'train/kl_loss': 0.2248046875, 'train/mask_bce_loss': 0.2035455320030451, 'train/mask_dice_loss': 0.3192771390080452, 'train/mask_loss': 0.5228226691484451, 'metrics/total_secs_per_batch': 7.267800331115723, 'metrics/data_secs_per_batch': 3.477317786216736, '_timestamp': 1740962420.4560196}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 281 is less than current step: 499. Dropping entry: {'train/lr': 8.822448979591836e-05, '_timestamp': 1740962420.4562953}).
Epoch: [3][283/500]	Time  7.371 ( 7.371)	Loss 1.9839 (2.1208)	CeLoss 0.2520 (0.3330)	SegCLSLoss 0.0153 (0.0172)	KLLoss 0.3711 (0.3387)	MaskLoss 0.8435 (0.8728)	MaskBCELoss 0.1648 (0.2468)	MaskDICELoss 0.6787 (0.6260)
Epoch: [3][284/500]	Time  7.516 ( 7.516)	Loss 0.9727 (1.7105)	CeLoss 0.9727 (0.3146)	SegCLSLoss 0.0000 (0.0158)	KLLoss 0.0000 (0.3363)	MaskLoss 0.0000 (0.6774)	MaskBCELoss 0.0000 (0.1812)	MaskDICELoss 0.0000 (0.4962)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 282 is less than current step: 499. Dropping entry: {'train/loss': 2.120787334442139, 'train/ce_loss': 0.3330078125, 'train/seg_cls_loss': 0.017169189453125, 'train/kl_loss': 0.338671875, 'train/mask_bce_loss': 0.24678092962130904, 'train/mask_dice_loss': 0.6260150760412216, 'train/mask_loss': 0.8727960109710693, 'metrics/total_secs_per_batch': 7.3714728355407715, 'metrics/data_secs_per_batch': 3.070854830741882, '_timestamp': 1740962427.827631}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 282 is less than current step: 499. Dropping entry: {'train/lr': 8.810204081632653e-05, '_timestamp': 1740962427.8279653}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 283 is less than current step: 499. Dropping entry: {'train/loss': 1.7105455994606018, 'train/ce_loss': 0.31455078125, 'train/seg_cls_loss': 0.015771484375, 'train/kl_loss': 0.336328125, 'train/mask_bce_loss': 0.18121555261313915, 'train/mask_dice_loss': 0.4962252199649811, 'train/mask_loss': 0.6774407714605332, 'metrics/total_secs_per_batch': 7.5161895751953125, 'metrics/data_secs_per_batch': 3.4287664175033568, '_timestamp': 1740962435.3437233}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 283 is less than current step: 499. Dropping entry: {'train/lr': 8.79795918367347e-05, '_timestamp': 1740962435.3440037}).
Epoch: [3][285/500]	Time  7.568 ( 7.568)	Loss 2.3995 (1.5526)	CeLoss 0.1289 (0.6003)	SegCLSLoss 0.0269 (0.0108)	KLLoss 0.4023 (0.1904)	MaskLoss 1.1084 (0.4640)	MaskBCELoss 0.2046 (0.0869)	MaskDICELoss 0.9038 (0.3770)
Epoch: [3][286/500]	Time  9.035 ( 9.035)	Loss 2.1428 (1.7158)	CeLoss 0.2500 (0.2809)	SegCLSLoss 0.0179 (0.0127)	KLLoss 0.3770 (0.3406)	MaskLoss 0.9229 (0.6972)	MaskBCELoss 0.2518 (0.1607)	MaskDICELoss 0.6711 (0.5364)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 284 is less than current step: 499. Dropping entry: {'train/loss': 1.5526119589805603, 'train/ce_loss': 0.60029296875, 'train/seg_cls_loss': 0.0107666015625, 'train/kl_loss': 0.1904296875, 'train/mask_bce_loss': 0.08691479023545981, 'train/mask_dice_loss': 0.3770376741886139, 'train/mask_loss': 0.46395245790481565, 'metrics/total_secs_per_batch': 7.567678689956665, 'metrics/data_secs_per_batch': 3.966284155845642, '_timestamp': 1740962442.9113717}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 284 is less than current step: 499. Dropping entry: {'train/lr': 8.785714285714286e-05, '_timestamp': 1740962442.9116387}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 285 is less than current step: 499. Dropping entry: {'train/loss': 1.7157938718795775, 'train/ce_loss': 0.280859375, 'train/seg_cls_loss': 0.012677001953125, 'train/kl_loss': 0.340625, 'train/mask_bce_loss': 0.1607220694422722, 'train/mask_dice_loss': 0.5364326715469361, 'train/mask_loss': 0.6971547484397889, 'metrics/total_secs_per_batch': 9.035141944885254, 'metrics/data_secs_per_batch': 4.383471131324768, '_timestamp': 1740962451.9465065}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 285 is less than current step: 499. Dropping entry: {'train/lr': 8.773469387755102e-05, '_timestamp': 1740962451.946771}).
Epoch: [3][287/500]	Time  8.141 ( 8.141)	Loss 2.1718 (1.5893)	CeLoss 0.2031 (0.3814)	SegCLSLoss 0.0256 (0.0150)	KLLoss 0.3594 (0.2613)	MaskLoss 0.9599 (0.5872)	MaskBCELoss 0.0766 (0.0863)	MaskDICELoss 0.8833 (0.5008)
Epoch: [3][288/500]	Time  6.975 ( 6.975)	Loss 1.6016 (1.5964)	CeLoss 1.6016 (0.5239)	SegCLSLoss 0.0000 (0.0094)	KLLoss 0.0000 (0.2248)	MaskLoss 0.0000 (0.5227)	MaskBCELoss 0.0000 (0.1132)	MaskDICELoss 0.0000 (0.4095)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 286 is less than current step: 499. Dropping entry: {'train/loss': 1.5893163323402404, 'train/ce_loss': 0.381396484375, 'train/seg_cls_loss': 0.014959716796875, 'train/kl_loss': 0.261328125, 'train/mask_bce_loss': 0.08634844720363617, 'train/mask_dice_loss': 0.5008145943284035, 'train/mask_loss': 0.5871630370616913, 'metrics/total_secs_per_batch': 8.140896797180176, 'metrics/data_secs_per_batch': 3.371172642707825, '_timestamp': 1740962460.087406}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 286 is less than current step: 499. Dropping entry: {'train/lr': 8.761224489795917e-05, '_timestamp': 1740962460.087669}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 287 is less than current step: 499. Dropping entry: {'train/loss': 1.5964249849319458, 'train/ce_loss': 0.523876953125, 'train/seg_cls_loss': 0.009375, 'train/kl_loss': 0.2248046875, 'train/mask_bce_loss': 0.1132393728941679, 'train/mask_dice_loss': 0.40946040749549867, 'train/mask_loss': 0.5226997762918473, 'metrics/total_secs_per_batch': 6.974519968032837, 'metrics/data_secs_per_batch': 3.066472363471985, '_timestamp': 1740962467.0619297}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 287 is less than current step: 499. Dropping entry: {'train/lr': 8.748979591836733e-05, '_timestamp': 1740962467.0622125}).
Epoch: [3][289/500]	Time  7.182 ( 7.182)	Loss 2.3854 (1.6632)	CeLoss 0.2314 (0.5625)	SegCLSLoss 0.0200 (0.0104)	KLLoss 0.3652 (0.2656)	MaskLoss 1.0540 (0.5343)	MaskBCELoss 0.3572 (0.0998)	MaskDICELoss 0.6968 (0.4345)
[2025-03-02 18:41:21,961] [INFO] [logging.py:96:log_dist] [Rank 0] step=1790, skipped=0, lr=[8.730612244897958e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:41:21,967] [INFO] [timer.py:215:stop] epoch=0/micro_step=17900/global_step=1790, RunningAvgSamplesPerSec=1.5271909185765895, CurrSamplesPerSec=1.2949830023415758, MemAllocated=31.26GB, MaxMemAllocated=37.19GB
Epoch: [3][290/500]	Time  7.724 ( 7.724)	Loss 1.9461 (1.5596)	CeLoss 0.1943 (0.4385)	SegCLSLoss 0.0265 (0.0127)	KLLoss 0.3613 (0.2256)	MaskLoss 0.8510 (0.5461)	MaskBCELoss 0.1223 (0.1526)	MaskDICELoss 0.7287 (0.3935)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 288 is less than current step: 499. Dropping entry: {'train/loss': 1.6631561636924743, 'train/ce_loss': 0.5625, 'train/seg_cls_loss': 0.01041259765625, 'train/kl_loss': 0.265625, 'train/mask_bce_loss': 0.09977157358080149, 'train/mask_dice_loss': 0.4345408707857132, 'train/mask_loss': 0.5343124389648437, 'metrics/total_secs_per_batch': 7.182412147521973, 'metrics/data_secs_per_batch': 3.0482584953308107, '_timestamp': 1740962474.2443764}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 288 is less than current step: 499. Dropping entry: {'train/lr': 8.73673469387755e-05, '_timestamp': 1740962474.244652}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 289 is less than current step: 499. Dropping entry: {'train/loss': 1.5596101760864258, 'train/ce_loss': 0.438525390625, 'train/seg_cls_loss': 0.01265869140625, 'train/kl_loss': 0.2255859375, 'train/mask_bce_loss': 0.1525700122117996, 'train/mask_dice_loss': 0.3935192584991455, 'train/mask_loss': 0.5460892677307129, 'metrics/total_secs_per_batch': 7.723632335662842, 'metrics/data_secs_per_batch': 3.363796091079712, '_timestamp': 1740962481.9678633}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 289 is less than current step: 499. Dropping entry: {'train/lr': 8.724489795918366e-05, '_timestamp': 1740962481.9681854}).
Epoch: [3][291/500]	Time  5.912 ( 5.912)	Loss 1.2578 (1.7208)	CeLoss 1.2578 (0.6521)	SegCLSLoss 0.0000 (0.0087)	KLLoss 0.0000 (0.1891)	MaskLoss 0.0000 (0.5227)	MaskBCELoss 0.0000 (0.1837)	MaskDICELoss 0.0000 (0.3389)
Epoch: [3][292/500]	Time  7.085 ( 7.085)	Loss 2.1320 (1.5327)	CeLoss 0.3945 (0.5293)	SegCLSLoss 0.0140 (0.0135)	KLLoss 0.3730 (0.2238)	MaskLoss 0.8463 (0.4872)	MaskBCELoss 0.0290 (0.0316)	MaskDICELoss 0.8172 (0.4555)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 290 is less than current step: 499. Dropping entry: {'train/loss': 1.7207943201065063, 'train/ce_loss': 0.65205078125, 'train/seg_cls_loss': 0.008697509765625, 'train/kl_loss': 0.1890625, 'train/mask_bce_loss': 0.18370793312788009, 'train/mask_dice_loss': 0.338945084810257, 'train/mask_loss': 0.5226530194282532, 'metrics/total_secs_per_batch': 5.912075042724609, 'metrics/data_secs_per_batch': 2.819346308708191, '_timestamp': 1740962487.8800495}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 290 is less than current step: 499. Dropping entry: {'train/lr': 8.712244897959182e-05, '_timestamp': 1740962487.8802357}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 291 is less than current step: 499. Dropping entry: {'train/loss': 1.5326984524726868, 'train/ce_loss': 0.529296875, 'train/seg_cls_loss': 0.01346435546875, 'train/kl_loss': 0.223828125, 'train/mask_bce_loss': 0.031624327041208745, 'train/mask_dice_loss': 0.45552568435668944, 'train/mask_loss': 0.4871500074863434, 'metrics/total_secs_per_batch': 7.084650754928589, 'metrics/data_secs_per_batch': 3.2917036533355715, '_timestamp': 1740962494.9647553}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 291 is less than current step: 499. Dropping entry: {'train/lr': 8.699999999999999e-05, '_timestamp': 1740962494.9650652}).
Epoch: [3][293/500]	Time  8.026 ( 8.026)	Loss 1.8409 (2.0468)	CeLoss 0.2266 (0.4366)	SegCLSLoss 0.0216 (0.0168)	KLLoss 0.3652 (0.2977)	MaskLoss 0.7837 (0.7859)	MaskBCELoss 0.0880 (0.2190)	MaskDICELoss 0.6957 (0.5670)
Epoch: [3][294/500]	Time  8.228 ( 8.228)	Loss 0.0776 (1.2930)	CeLoss 0.0776 (0.3455)	SegCLSLoss 0.0000 (0.0121)	KLLoss 0.0000 (0.2215)	MaskLoss 0.0000 (0.4596)	MaskBCELoss 0.0000 (0.0485)	MaskDICELoss 0.0000 (0.4111)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 292 is less than current step: 499. Dropping entry: {'train/loss': 2.046760857105255, 'train/ce_loss': 0.43662109375, 'train/seg_cls_loss': 0.016778564453125, 'train/kl_loss': 0.29765625, 'train/mask_bce_loss': 0.2189756091684103, 'train/mask_dice_loss': 0.5669536590576172, 'train/mask_loss': 0.7859292566776276, 'metrics/total_secs_per_batch': 8.025655269622803, 'metrics/data_secs_per_batch': 3.1529376745224, '_timestamp': 1740962502.9903774}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 292 is less than current step: 499. Dropping entry: {'train/lr': 8.687755102040815e-05, '_timestamp': 1740962502.9905615}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 293 is less than current step: 499. Dropping entry: {'train/loss': 1.2929837703704834, 'train/ce_loss': 0.345458984375, 'train/seg_cls_loss': 0.012115478515625, 'train/kl_loss': 0.221484375, 'train/mask_bce_loss': 0.04846864882856607, 'train/mask_dice_loss': 0.41113359332084654, 'train/mask_loss': 0.4596022367477417, 'metrics/total_secs_per_batch': 8.228221416473389, 'metrics/data_secs_per_batch': 4.242225813865661, '_timestamp': 1740962511.2186186}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 293 is less than current step: 499. Dropping entry: {'train/lr': 8.675510204081633e-05, '_timestamp': 1740962511.2188962}).
Epoch: [3][295/500]	Time  7.746 ( 7.746)	Loss 1.4736 (1.8043)	CeLoss 0.1660 (0.3256)	SegCLSLoss 0.0192 (0.0142)	KLLoss 0.3770 (0.3004)	MaskLoss 0.6303 (0.7207)	MaskBCELoss 0.0942 (0.2143)	MaskDICELoss 0.5362 (0.5064)
Epoch: [3][296/500]	Time  7.869 ( 7.869)	Loss 2.7112 (1.5772)	CeLoss 0.1504 (0.5303)	SegCLSLoss 0.0311 (0.0128)	KLLoss 0.3730 (0.2623)	MaskLoss 1.2540 (0.5071)	MaskBCELoss 0.3688 (0.1137)	MaskDICELoss 0.8853 (0.3934)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 294 is less than current step: 499. Dropping entry: {'train/loss': 1.8043380975723267, 'train/ce_loss': 0.3255859375, 'train/seg_cls_loss': 0.0141845703125, 'train/kl_loss': 0.300390625, 'train/mask_bce_loss': 0.21432397849857807, 'train/mask_dice_loss': 0.5063997507095337, 'train/mask_loss': 0.7207237303256988, 'metrics/total_secs_per_batch': 7.745861530303955, 'metrics/data_secs_per_batch': 3.4887272834777834, '_timestamp': 1740962518.9644501}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 294 is less than current step: 499. Dropping entry: {'train/lr': 8.663265306122449e-05, '_timestamp': 1740962518.9647067}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 295 is less than current step: 499. Dropping entry: {'train/loss': 1.5771718919277191, 'train/ce_loss': 0.5302734375, 'train/seg_cls_loss': 0.01279296875, 'train/kl_loss': 0.2623046875, 'train/mask_bce_loss': 0.11369927823543549, 'train/mask_dice_loss': 0.3934413596987724, 'train/mask_loss': 0.5071406349539757, 'metrics/total_secs_per_batch': 7.86897873878479, 'metrics/data_secs_per_batch': 3.4950459957122804, '_timestamp': 1740962526.8334434}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 295 is less than current step: 499. Dropping entry: {'train/lr': 8.651020408163265e-05, '_timestamp': 1740962526.8337064}).
Epoch: [3][297/500]	Time  8.671 ( 8.671)	Loss 2.2603 (2.1992)	CeLoss 0.2461 (0.2551)	SegCLSLoss 0.0151 (0.0176)	KLLoss 0.3809 (0.3734)	MaskLoss 0.9837 (0.9489)	MaskBCELoss 0.2350 (0.3006)	MaskDICELoss 0.7487 (0.6483)
Epoch: [3][298/500]	Time  7.978 ( 7.978)	Loss 2.4010 (2.2152)	CeLoss 0.2852 (0.3537)	SegCLSLoss 0.0128 (0.0132)	KLLoss 0.3828 (0.3373)	MaskLoss 1.0354 (0.9105)	MaskBCELoss 0.2642 (0.3192)	MaskDICELoss 0.7712 (0.5914)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 296 is less than current step: 499. Dropping entry: {'train/loss': 2.1992222547531126, 'train/ce_loss': 0.255078125, 'train/seg_cls_loss': 0.0176025390625, 'train/kl_loss': 0.3734375, 'train/mask_bce_loss': 0.3006484638899565, 'train/mask_dice_loss': 0.648279070854187, 'train/mask_loss': 0.9489275217056274, 'metrics/total_secs_per_batch': 8.671072006225586, 'metrics/data_secs_per_batch': 3.8837548732757567, '_timestamp': 1740962535.5045283}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 296 is less than current step: 499. Dropping entry: {'train/lr': 8.638775510204082e-05, '_timestamp': 1740962535.5047183}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 297 is less than current step: 499. Dropping entry: {'train/loss': 2.2152063608169557, 'train/ce_loss': 0.3537109375, 'train/seg_cls_loss': 0.013165283203125, 'train/kl_loss': 0.3373046875, 'train/mask_bce_loss': 0.3191702999174595, 'train/mask_dice_loss': 0.5913625821471215, 'train/mask_loss': 0.9105328679084778, 'metrics/total_secs_per_batch': 7.977678298950195, 'metrics/data_secs_per_batch': 2.946057605743408, '_timestamp': 1740962543.482197}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 297 is less than current step: 499. Dropping entry: {'train/lr': 8.626530612244898e-05, '_timestamp': 1740962543.4824636}).
Epoch: [3][299/500]	Time  8.633 ( 8.633)	Loss 1.6627 (1.7211)	CeLoss 0.2041 (0.3078)	SegCLSLoss 0.0151 (0.0127)	KLLoss 0.3691 (0.2953)	MaskLoss 0.7068 (0.6886)	MaskBCELoss 0.0588 (0.1399)	MaskDICELoss 0.6480 (0.5487)
[2025-03-02 18:42:40,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=1800, skipped=0, lr=[8.608163265306121e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:42:40,760] [INFO] [timer.py:215:stop] epoch=0/micro_step=18000/global_step=1800, RunningAvgSamplesPerSec=1.5254673333051292, CurrSamplesPerSec=1.1567378099868493, MemAllocated=30.89GB, MaxMemAllocated=37.19GB
Epoch: [3][300/500]	Time  8.647 ( 8.647)	Loss 0.9336 (1.5743)	CeLoss 0.9336 (0.3017)	SegCLSLoss 0.0000 (0.0133)	KLLoss 0.0000 (0.3008)	MaskLoss 0.0000 (0.6181)	MaskBCELoss 0.0000 (0.0873)	MaskDICELoss 0.0000 (0.5308)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 298 is less than current step: 499. Dropping entry: {'train/loss': 1.7211170315742492, 'train/ce_loss': 0.3078125, 'train/seg_cls_loss': 0.012738037109375, 'train/kl_loss': 0.2953125, 'train/mask_bce_loss': 0.13990702666342258, 'train/mask_dice_loss': 0.5486788302659988, 'train/mask_loss': 0.6885858595371246, 'metrics/total_secs_per_batch': 8.632719278335571, 'metrics/data_secs_per_batch': 3.6306809186935425, '_timestamp': 1740962552.1149392}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 298 is less than current step: 499. Dropping entry: {'train/lr': 8.614285714285714e-05, '_timestamp': 1740962552.1152034}).
Epoch: [3][301/500]	Time  7.393 ( 7.393)	Loss 2.0814 (1.4751)	CeLoss 0.2266 (0.4219)	SegCLSLoss 0.0117 (0.0127)	KLLoss 0.3867 (0.2613)	MaskLoss 0.9050 (0.5102)	MaskBCELoss 0.1386 (0.1136)	MaskDICELoss 0.7663 (0.3967)
Epoch: [3][302/500]	Time  7.671 ( 7.671)	Loss 0.0674 (1.6187)	CeLoss 0.0674 (0.3966)	SegCLSLoss 0.0000 (0.0155)	KLLoss 0.0000 (0.2611)	MaskLoss 0.0000 (0.5942)	MaskBCELoss 0.0000 (0.0809)	MaskDICELoss 0.0000 (0.5133)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 299 is less than current step: 499. Dropping entry: {'train/loss': 1.5743215918540954, 'train/ce_loss': 0.30166015625, 'train/seg_cls_loss': 0.01329345703125, 'train/kl_loss': 0.30078125, 'train/mask_bce_loss': 0.08728320170193911, 'train/mask_dice_loss': 0.5308346211910248, 'train/mask_loss': 0.6181178152561188, 'metrics/total_secs_per_batch': 8.646528244018555, 'metrics/data_secs_per_batch': 3.8107285022735597, '_timestamp': 1740962560.7613287}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 299 is less than current step: 499. Dropping entry: {'train/lr': 8.602040816326529e-05, '_timestamp': 1740962560.7615283}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 300 is less than current step: 499. Dropping entry: {'train/loss': 1.4751384258270264, 'train/ce_loss': 0.421875, 'train/seg_cls_loss': 0.0126708984375, 'train/kl_loss': 0.261328125, 'train/mask_bce_loss': 0.11355204796418547, 'train/mask_dice_loss': 0.3966734170913696, 'train/mask_loss': 0.5102254688739777, 'metrics/total_secs_per_batch': 7.392805814743042, 'metrics/data_secs_per_batch': 2.950256037712097, '_timestamp': 1740962568.1543474}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 300 is less than current step: 499. Dropping entry: {'train/lr': 8.589795918367345e-05, '_timestamp': 1740962568.154617}).
Epoch: [3][303/500]	Time  8.264 ( 8.264)	Loss 1.1838 (1.4709)	CeLoss 0.2715 (0.3031)	SegCLSLoss 0.0120 (0.0130)	KLLoss 0.3770 (0.2982)	MaskLoss 0.4347 (0.5657)	MaskBCELoss 0.2258 (0.1411)	MaskDICELoss 0.2089 (0.4246)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 301 is less than current step: 499. Dropping entry: {'train/loss': 1.618742549419403, 'train/ce_loss': 0.39658203125, 'train/seg_cls_loss': 0.01553955078125, 'train/kl_loss': 0.2611328125, 'train/mask_bce_loss': 0.08088765395805239, 'train/mask_dice_loss': 0.5132980704307556, 'train/mask_loss': 0.5941857278347016, 'metrics/total_secs_per_batch': 7.671082019805908, 'metrics/data_secs_per_batch': 3.561947989463806, '_timestamp': 1740962575.8252847}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 301 is less than current step: 499. Dropping entry: {'train/lr': 8.577551020408162e-05, '_timestamp': 1740962575.8255396}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 302 is less than current step: 499. Dropping entry: {'train/loss': 1.4709304332733155, 'train/ce_loss': 0.303076171875, 'train/seg_cls_loss': 0.01300048828125, 'train/kl_loss': 0.2982421875, 'train/mask_bce_loss': 0.1411267202347517, 'train/mask_dice_loss': 0.4245875120162964, 'train/mask_loss': 0.5657142370939254, 'metrics/total_secs_per_batch': 8.263513326644897, 'metrics/data_secs_per_batch': 3.624169683456421, '_timestamp': 1740962584.088851}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 302 is less than current step: 499. Dropping entry: {'train/lr': 8.565306122448978e-05, '_timestamp': 1740962584.0890412}).
Epoch: [3][304/500]	Time  7.741 ( 7.741)	Loss 2.5353 (1.5334)	CeLoss 0.1211 (0.3613)	SegCLSLoss 0.0342 (0.0151)	KLLoss 0.3750 (0.2645)	MaskLoss 1.1798 (0.5690)	MaskBCELoss 0.2791 (0.0897)	MaskDICELoss 0.9007 (0.4793)
Epoch: [3][305/500]	Time  8.543 ( 8.543)	Loss 2.0224 (1.9099)	CeLoss 0.2793 (0.4104)	SegCLSLoss 0.0121 (0.0158)	KLLoss 0.3848 (0.2967)	MaskLoss 0.8491 (0.7309)	MaskBCELoss 0.1645 (0.1303)	MaskDICELoss 0.6845 (0.6006)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 303 is less than current step: 499. Dropping entry: {'train/loss': 1.5333916604518891, 'train/ce_loss': 0.361279296875, 'train/seg_cls_loss': 0.01507568359375, 'train/kl_loss': 0.264453125, 'train/mask_bce_loss': 0.08965902184136212, 'train/mask_dice_loss': 0.4793073207139969, 'train/mask_loss': 0.5689663380384445, 'metrics/total_secs_per_batch': 7.740880966186523, 'metrics/data_secs_per_batch': 3.410703349113464, '_timestamp': 1740962591.8297505}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 303 is less than current step: 499. Dropping entry: {'train/lr': 8.553061224489794e-05, '_timestamp': 1740962591.8299417}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 304 is less than current step: 499. Dropping entry: {'train/loss': 1.9098880290985107, 'train/ce_loss': 0.41044921875, 'train/seg_cls_loss': 0.015838623046875, 'train/kl_loss': 0.2966796875, 'train/mask_bce_loss': 0.13033145619556308, 'train/mask_dice_loss': 0.600589120388031, 'train/mask_loss': 0.7309205710887909, 'metrics/total_secs_per_batch': 8.54345965385437, 'metrics/data_secs_per_batch': 3.4464588165283203, '_timestamp': 1740962600.3731906}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 304 is less than current step: 499. Dropping entry: {'train/lr': 8.540816326530612e-05, '_timestamp': 1740962600.3734598}).
Epoch: [3][306/500]	Time  8.607 ( 8.607)	Loss 2.6494 (1.9742)	CeLoss 0.2734 (0.3364)	SegCLSLoss 0.0176 (0.0171)	KLLoss 0.3574 (0.3334)	MaskLoss 1.1665 (0.7980)	MaskBCELoss 0.5446 (0.1980)	MaskDICELoss 0.6219 (0.6000)
Epoch: [3][307/500]	Time  8.004 ( 8.004)	Loss 2.5136 (1.6822)	CeLoss 0.2334 (0.2781)	SegCLSLoss 0.0092 (0.0122)	KLLoss 0.3789 (0.2631)	MaskLoss 1.1191 (0.6859)	MaskBCELoss 0.7183 (0.2219)	MaskDICELoss 0.4008 (0.4640)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 305 is less than current step: 499. Dropping entry: {'train/loss': 1.9742252230644226, 'train/ce_loss': 0.33642578125, 'train/seg_cls_loss': 0.017071533203125, 'train/kl_loss': 0.3333984375, 'train/mask_bce_loss': 0.19799840319901704, 'train/mask_dice_loss': 0.6000028789043427, 'train/mask_loss': 0.7980012744665146, 'metrics/total_secs_per_batch': 8.606669664382935, 'metrics/data_secs_per_batch': 3.712311363220215, '_timestamp': 1740962608.9799824}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 305 is less than current step: 499. Dropping entry: {'train/lr': 8.528571428571428e-05, '_timestamp': 1740962608.9802861}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 306 is less than current step: 499. Dropping entry: {'train/loss': 1.6822155714035034, 'train/ce_loss': 0.278125, 'train/seg_cls_loss': 0.01217041015625, 'train/kl_loss': 0.2630859375, 'train/mask_bce_loss': 0.22187539357692004, 'train/mask_dice_loss': 0.4640077829360962, 'train/mask_loss': 0.6858831763267517, 'metrics/total_secs_per_batch': 8.004045248031616, 'metrics/data_secs_per_batch': 3.847738575935364, '_timestamp': 1740962616.9840634}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 306 is less than current step: 499. Dropping entry: {'train/lr': 8.516326530612245e-05, '_timestamp': 1740962616.9843717}).
Epoch: [3][308/500]	Time  6.995 ( 6.995)	Loss 1.4900 (1.8147)	CeLoss 0.2188 (0.5510)	SegCLSLoss 0.0125 (0.0142)	KLLoss 0.3848 (0.2629)	MaskLoss 0.6132 (0.6150)	MaskBCELoss 0.1251 (0.1488)	MaskDICELoss 0.4881 (0.4662)
Epoch: [3][309/500]	Time  9.198 ( 9.198)	Loss 1.7618 (1.7065)	CeLoss 0.2148 (0.2843)	SegCLSLoss 0.0294 (0.0177)	KLLoss 0.3633 (0.2938)	MaskLoss 0.7481 (0.6919)	MaskBCELoss 0.1141 (0.1447)	MaskDICELoss 0.6340 (0.5472)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 307 is less than current step: 499. Dropping entry: {'train/loss': 1.8147325038909912, 'train/ce_loss': 0.5509765625, 'train/seg_cls_loss': 0.01422119140625, 'train/kl_loss': 0.262890625, 'train/mask_bce_loss': 0.14880979135632516, 'train/mask_dice_loss': 0.4662224858999252, 'train/mask_loss': 0.6150322675704956, 'metrics/total_secs_per_batch': 6.994597434997559, 'metrics/data_secs_per_batch': 3.0942963361740112, '_timestamp': 1740962623.978547}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 307 is less than current step: 499. Dropping entry: {'train/lr': 8.504081632653061e-05, '_timestamp': 1740962623.9788551}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 308 is less than current step: 499. Dropping entry: {'train/loss': 1.7065376043319702, 'train/ce_loss': 0.284326171875, 'train/seg_cls_loss': 0.0177490234375, 'train/kl_loss': 0.29375, 'train/mask_bce_loss': 0.14474322265014053, 'train/mask_dice_loss': 0.5471730411052704, 'train/mask_loss': 0.6919162571430206, 'metrics/total_secs_per_batch': 9.198170185089111, 'metrics/data_secs_per_batch': 3.557355451583862, '_timestamp': 1740962633.1767058}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 308 is less than current step: 499. Dropping entry: {'train/lr': 8.491836734693877e-05, '_timestamp': 1740962633.1769612}).
[2025-03-02 18:44:00,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=1810, skipped=0, lr=[8.485714285714285e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:44:00,921] [INFO] [timer.py:215:stop] epoch=0/micro_step=18100/global_step=1810, RunningAvgSamplesPerSec=1.5235910630715361, CurrSamplesPerSec=1.2912683735726385, MemAllocated=31.56GB, MaxMemAllocated=37.19GB
Epoch: [3][310/500]	Time  7.746 ( 7.746)	Loss 1.9368 (1.2774)	CeLoss 0.2314 (0.4554)	SegCLSLoss 0.0204 (0.0088)	KLLoss 0.3711 (0.1840)	MaskLoss 0.8297 (0.3998)	MaskBCELoss 0.2011 (0.0658)	MaskDICELoss 0.6286 (0.3340)
Epoch: [3][311/500]	Time  6.550 ( 6.550)	Loss 1.2218 (1.5311)	CeLoss 0.2266 (0.4938)	SegCLSLoss 0.0204 (0.0115)	KLLoss 0.3730 (0.2600)	MaskLoss 0.4742 (0.5028)	MaskBCELoss 0.0421 (0.0867)	MaskDICELoss 0.4321 (0.4161)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 309 is less than current step: 499. Dropping entry: {'train/loss': 1.277359962463379, 'train/ce_loss': 0.45537109375, 'train/seg_cls_loss': 0.008758544921875, 'train/kl_loss': 0.183984375, 'train/mask_bce_loss': 0.0658316096290946, 'train/mask_dice_loss': 0.33398119211196897, 'train/mask_loss': 0.39981279373168943, 'metrics/total_secs_per_batch': 7.745808362960815, 'metrics/data_secs_per_batch': 3.915782618522644, '_timestamp': 1740962640.9223416}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 309 is less than current step: 499. Dropping entry: {'train/lr': 8.479591836734694e-05, '_timestamp': 1740962640.9226012}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 310 is less than current step: 499. Dropping entry: {'train/loss': 1.531116497516632, 'train/ce_loss': 0.493798828125, 'train/seg_cls_loss': 0.011474609375, 'train/kl_loss': 0.2599609375, 'train/mask_bce_loss': 0.08670946676284075, 'train/mask_dice_loss': 0.4160802200436592, 'train/mask_loss': 0.5027896925806999, 'metrics/total_secs_per_batch': 6.550174713134766, 'metrics/data_secs_per_batch': 2.9111042976379395, '_timestamp': 1740962647.4727027}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 310 is less than current step: 499. Dropping entry: {'train/lr': 8.46734693877551e-05, '_timestamp': 1740962647.4730134}).
Epoch: [3][312/500]	Time  7.729 ( 7.729)	Loss 1.2891 (1.5141)	CeLoss 1.2891 (0.5263)	SegCLSLoss 0.0000 (0.0109)	KLLoss 0.0000 (0.1895)	MaskLoss 0.0000 (0.4816)	MaskBCELoss 0.0000 (0.0493)	MaskDICELoss 0.0000 (0.4324)
Epoch: [3][313/500]	Time  9.718 ( 9.718)	Loss 2.4974 (2.2880)	CeLoss 0.1875 (0.1922)	SegCLSLoss 0.0264 (0.0232)	KLLoss 0.3711 (0.3707)	MaskLoss 1.1296 (1.0236)	MaskBCELoss 0.2467 (0.2470)	MaskDICELoss 0.8828 (0.7765)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 311 is less than current step: 499. Dropping entry: {'train/loss': 1.5140529036521913, 'train/ce_loss': 0.5263427734375, 'train/seg_cls_loss': 0.0108642578125, 'train/kl_loss': 0.189453125, 'train/mask_bce_loss': 0.049252978852018715, 'train/mask_dice_loss': 0.4323950409889221, 'train/mask_loss': 0.4816480100154877, 'metrics/total_secs_per_batch': 7.728504657745361, 'metrics/data_secs_per_batch': 3.8933871746063233, '_timestamp': 1740962655.201202}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 311 is less than current step: 499. Dropping entry: {'train/lr': 8.455102040816326e-05, '_timestamp': 1740962655.2014577}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 312 is less than current step: 499. Dropping entry: {'train/loss': 2.2879611968994142, 'train/ce_loss': 0.1921875, 'train/seg_cls_loss': 0.02322998046875, 'train/kl_loss': 0.370703125, 'train/mask_bce_loss': 0.2470243502408266, 'train/mask_dice_loss': 0.7765460789203644, 'train/mask_loss': 1.023570442199707, 'metrics/total_secs_per_batch': 9.717843055725098, 'metrics/data_secs_per_batch': 4.08153395652771, '_timestamp': 1740962664.919053}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 312 is less than current step: 499. Dropping entry: {'train/lr': 8.442857142857141e-05, '_timestamp': 1740962664.919327}).
Epoch: [3][314/500]	Time  7.942 ( 7.942)	Loss 1.4625 (1.8737)	CeLoss 0.2012 (0.3307)	SegCLSLoss 0.0208 (0.0171)	KLLoss 0.3711 (0.3379)	MaskLoss 0.6067 (0.7504)	MaskBCELoss 0.1821 (0.1624)	MaskDICELoss 0.4246 (0.5880)
Epoch: [3][315/500]	Time  8.273 ( 8.273)	Loss 1.3410 (1.7489)	CeLoss 0.2539 (0.3173)	SegCLSLoss 0.0129 (0.0136)	KLLoss 0.3867 (0.2980)	MaskLoss 0.5211 (0.6974)	MaskBCELoss 0.0856 (0.1289)	MaskDICELoss 0.4355 (0.5685)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 313 is less than current step: 499. Dropping entry: {'train/loss': 1.8737443685531616, 'train/ce_loss': 0.3306640625, 'train/seg_cls_loss': 0.017083740234375, 'train/kl_loss': 0.337890625, 'train/mask_bce_loss': 0.16241670455783605, 'train/mask_dice_loss': 0.5880296871066093, 'train/mask_loss': 0.7504463866353035, 'metrics/total_secs_per_batch': 7.941887855529785, 'metrics/data_secs_per_batch': 3.452771234512329, '_timestamp': 1740962672.8609393}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 313 is less than current step: 499. Dropping entry: {'train/lr': 8.430612244897957e-05, '_timestamp': 1740962672.861203}).
Epoch: [3][316/500]	Time  6.118 ( 6.118)	Loss 1.4688 (1.5201)	CeLoss 1.4688 (0.5163)	SegCLSLoss 0.0000 (0.0122)	KLLoss 0.0000 (0.2211)	MaskLoss 0.0000 (0.4878)	MaskBCELoss 0.0000 (0.1288)	MaskDICELoss 0.0000 (0.3590)
Epoch: [3][317/500]	Time  8.246 ( 8.246)	Loss 1.9749 (1.6572)	CeLoss 0.2490 (0.2791)	SegCLSLoss 0.0186 (0.0155)	KLLoss 0.3809 (0.3449)	MaskLoss 0.8390 (0.6679)	MaskBCELoss 0.0389 (0.0832)	MaskDICELoss 0.8001 (0.5847)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 314 is less than current step: 499. Dropping entry: {'train/loss': 1.7488957047462463, 'train/ce_loss': 0.317333984375, 'train/seg_cls_loss': 0.0135986328125, 'train/kl_loss': 0.298046875, 'train/mask_bce_loss': 0.12893696166574956, 'train/mask_dice_loss': 0.5684845328330994, 'train/mask_loss': 0.6974214941263199, 'metrics/total_secs_per_batch': 8.273195028305054, 'metrics/data_secs_per_batch': 3.7858395099639894, '_timestamp': 1740962681.1342208}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 314 is less than current step: 499. Dropping entry: {'train/lr': 8.418367346938774e-05, '_timestamp': 1740962681.1345382}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 315 is less than current step: 499. Dropping entry: {'train/loss': 1.5200854122638703, 'train/ce_loss': 0.516259765625, 'train/seg_cls_loss': 0.012249755859375, 'train/kl_loss': 0.22109375, 'train/mask_bce_loss': 0.12877431316301227, 'train/mask_dice_loss': 0.35902718305587766, 'train/mask_loss': 0.48780149817466734, 'metrics/total_secs_per_batch': 6.117955923080444, 'metrics/data_secs_per_batch': 2.950076937675476, '_timestamp': 1740962687.2521381}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 315 is less than current step: 499. Dropping entry: {'train/lr': 8.40612244897959e-05, '_timestamp': 1740962687.252422}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 316 is less than current step: 499. Dropping entry: {'train/loss': 1.6571717381477356, 'train/ce_loss': 0.2791015625, 'train/seg_cls_loss': 0.015509033203125, 'train/kl_loss': 0.344921875, 'train/mask_bce_loss': 0.08321745004504918, 'train/mask_dice_loss': 0.5847238928079606, 'train/mask_loss': 0.6679413378238678, 'metrics/total_secs_per_batch': 8.24645209312439, 'metrics/data_secs_per_batch': 3.8031827449798583, '_timestamp': 1740962695.4985905}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 316 is less than current step: 499. Dropping entry: {'train/lr': 8.393877551020408e-05, '_timestamp': 1740962695.4988708}).
Epoch: [3][318/500]	Time  8.416 ( 8.416)	Loss 1.6684 (1.6687)	CeLoss 0.3496 (0.4741)	SegCLSLoss 0.0133 (0.0151)	KLLoss 0.3711 (0.2625)	MaskLoss 0.6379 (0.5805)	MaskBCELoss 0.0315 (0.0551)	MaskDICELoss 0.6064 (0.5253)
Epoch: [3][319/500]	Time  7.359 ( 7.359)	Loss 1.7584 (1.7947)	CeLoss 0.2178 (0.3953)	SegCLSLoss 0.0100 (0.0129)	KLLoss 0.3809 (0.3010)	MaskLoss 0.7483 (0.6815)	MaskBCELoss 0.3698 (0.1872)	MaskDICELoss 0.3786 (0.4943)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 317 is less than current step: 499. Dropping entry: {'train/loss': 1.6687469840049745, 'train/ce_loss': 0.47412109375, 'train/seg_cls_loss': 0.0150634765625, 'train/kl_loss': 0.2625, 'train/mask_bce_loss': 0.05512198256328702, 'train/mask_dice_loss': 0.5253452479839325, 'train/mask_loss': 0.5804672300815582, 'metrics/total_secs_per_batch': 8.416308403015137, 'metrics/data_secs_per_batch': 4.316889309883118, '_timestamp': 1740962703.9150774}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 317 is less than current step: 499. Dropping entry: {'train/lr': 8.381632653061224e-05, '_timestamp': 1740962703.9154627}).
[2025-03-02 18:45:18,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=1820, skipped=0, lr=[8.363265306122448e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:45:18,293] [INFO] [timer.py:215:stop] epoch=0/micro_step=18200/global_step=1820, RunningAvgSamplesPerSec=1.5220953570970466, CurrSamplesPerSec=1.42462413062279, MemAllocated=31.25GB, MaxMemAllocated=37.19GB
Epoch: [3][320/500]	Time  7.021 ( 7.021)	Loss 2.5359 (1.8873)	CeLoss 0.2490 (0.2474)	SegCLSLoss 0.0125 (0.0149)	KLLoss 0.3633 (0.3406)	MaskLoss 1.1215 (0.7991)	MaskBCELoss 0.4615 (0.1979)	MaskDICELoss 0.6600 (0.6012)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 318 is less than current step: 499. Dropping entry: {'train/loss': 1.7946872472763062, 'train/ce_loss': 0.3953125, 'train/seg_cls_loss': 0.012884521484375, 'train/kl_loss': 0.3009765625, 'train/mask_bce_loss': 0.1871638808399439, 'train/mask_dice_loss': 0.49431060552597045, 'train/mask_loss': 0.6814744889736175, 'metrics/total_secs_per_batch': 7.358624219894409, 'metrics/data_secs_per_batch': 3.5475869178771973, '_timestamp': 1740962711.2734575}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 318 is less than current step: 499. Dropping entry: {'train/lr': 8.36938775510204e-05, '_timestamp': 1740962711.273711}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 319 is less than current step: 499. Dropping entry: {'train/loss': 1.8873020887374878, 'train/ce_loss': 0.247412109375, 'train/seg_cls_loss': 0.0149169921875, 'train/kl_loss': 0.340625, 'train/mask_bce_loss': 0.19788414761424064, 'train/mask_dice_loss': 0.601186814904213, 'train/mask_loss': 0.7990709662437439, 'metrics/total_secs_per_batch': 7.020883798599243, 'metrics/data_secs_per_batch': 3.3496337890625, '_timestamp': 1740962718.2943153}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 319 is less than current step: 499. Dropping entry: {'train/lr': 8.357142857142857e-05, '_timestamp': 1740962718.2946534}).
Epoch: [3][321/500]	Time  9.223 ( 9.223)	Loss 1.5746 (1.8563)	CeLoss 0.2197 (0.3338)	SegCLSLoss 0.0203 (0.0170)	KLLoss 0.3711 (0.3328)	MaskLoss 0.6545 (0.7403)	MaskBCELoss 0.0522 (0.1640)	MaskDICELoss 0.6023 (0.5763)
Epoch: [3][322/500]	Time  8.752 ( 8.752)	Loss 2.5002 (2.1922)	CeLoss 0.1660 (0.2379)	SegCLSLoss 0.0251 (0.0201)	KLLoss 0.3672 (0.3752)	MaskLoss 1.1427 (0.9535)	MaskBCELoss 0.3488 (0.1690)	MaskDICELoss 0.7939 (0.7846)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 320 is less than current step: 499. Dropping entry: {'train/loss': 1.8562614440917968, 'train/ce_loss': 0.3337890625, 'train/seg_cls_loss': 0.016986083984375, 'train/kl_loss': 0.3328125, 'train/mask_bce_loss': 0.16400643549859523, 'train/mask_dice_loss': 0.5762824892997742, 'train/mask_loss': 0.7402889132499695, 'metrics/total_secs_per_batch': 9.22297215461731, 'metrics/data_secs_per_batch': 3.947651767730713, '_timestamp': 1740962727.517392}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 320 is less than current step: 499. Dropping entry: {'train/lr': 8.344897959183673e-05, '_timestamp': 1740962727.51767}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 321 is less than current step: 499. Dropping entry: {'train/loss': 2.192228412628174, 'train/ce_loss': 0.237890625, 'train/seg_cls_loss': 0.020062255859375, 'train/kl_loss': 0.3751953125, 'train/mask_bce_loss': 0.1689562164247036, 'train/mask_dice_loss': 0.7845798641443252, 'train/mask_loss': 0.9535360693931579, 'metrics/total_secs_per_batch': 8.751919507980347, 'metrics/data_secs_per_batch': 3.7084126710891723, '_timestamp': 1740962736.269286}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 321 is less than current step: 499. Dropping entry: {'train/lr': 8.332653061224489e-05, '_timestamp': 1740962736.26956}).
Epoch: [3][323/500]	Time  9.922 ( 9.922)	Loss 1.3715 (1.8654)	CeLoss 0.2695 (0.2337)	SegCLSLoss 0.0121 (0.0202)	KLLoss 0.3887 (0.3785)	MaskLoss 0.5285 (0.7919)	MaskBCELoss 0.0660 (0.1070)	MaskDICELoss 0.4625 (0.6849)
Epoch: [3][324/500]	Time  7.872 ( 7.872)	Loss 2.7357 (1.6796)	CeLoss 0.3613 (0.3173)	SegCLSLoss 0.0146 (0.0123)	KLLoss 0.3730 (0.3025)	MaskLoss 1.1647 (0.6629)	MaskBCELoss 0.4347 (0.0963)	MaskDICELoss 0.7300 (0.5666)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 322 is less than current step: 499. Dropping entry: {'train/loss': 1.8653879642486573, 'train/ce_loss': 0.23369140625, 'train/seg_cls_loss': 0.02021484375, 'train/kl_loss': 0.378515625, 'train/mask_bce_loss': 0.10695600230246782, 'train/mask_dice_loss': 0.6849176615476609, 'train/mask_loss': 0.7918736577033997, 'metrics/total_secs_per_batch': 9.921519041061401, 'metrics/data_secs_per_batch': 4.5240614175796505, '_timestamp': 1740962746.1908891}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 322 is less than current step: 499. Dropping entry: {'train/lr': 8.320408163265306e-05, '_timestamp': 1740962746.1911898}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 323 is less than current step: 499. Dropping entry: {'train/loss': 1.6795943319797515, 'train/ce_loss': 0.3173095703125, 'train/seg_cls_loss': 0.0123046875, 'train/kl_loss': 0.3025390625, 'train/mask_bce_loss': 0.09633499048650265, 'train/mask_dice_loss': 0.5665944814682007, 'train/mask_loss': 0.6629294797778129, 'metrics/total_secs_per_batch': 7.872141361236572, 'metrics/data_secs_per_batch': 2.8554278135299684, '_timestamp': 1740962754.0630772}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 323 is less than current step: 499. Dropping entry: {'train/lr': 8.308163265306122e-05, '_timestamp': 1740962754.0633843}).
Epoch: [3][325/500]	Time  6.678 ( 6.678)	Loss 0.3184 (1.0389)	CeLoss 0.3184 (0.5566)	SegCLSLoss 0.0000 (0.0052)	KLLoss 0.0000 (0.1139)	MaskLoss 0.0000 (0.2342)	MaskBCELoss 0.0000 (0.0175)	MaskDICELoss 0.0000 (0.2167)
Epoch: [3][326/500]	Time  9.953 ( 9.953)	Loss 1.5390 (1.7743)	CeLoss 0.1699 (0.2005)	SegCLSLoss 0.0330 (0.0184)	KLLoss 0.3770 (0.3359)	MaskLoss 0.6572 (0.7654)	MaskBCELoss 0.0319 (0.0893)	MaskDICELoss 0.6253 (0.6762)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 324 is less than current step: 499. Dropping entry: {'train/loss': 1.0389394402503966, 'train/ce_loss': 0.556591796875, 'train/seg_cls_loss': 0.005194091796875, 'train/kl_loss': 0.1138671875, 'train/mask_bce_loss': 0.017503177002072334, 'train/mask_dice_loss': 0.2167370468378067, 'train/mask_loss': 0.23424023091793061, 'metrics/total_secs_per_batch': 6.678215026855469, 'metrics/data_secs_per_batch': 2.7088709831237794, '_timestamp': 1740962760.7411623}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 324 is less than current step: 499. Dropping entry: {'train/lr': 8.295918367346938e-05, '_timestamp': 1740962760.7413578}).
Epoch: [3][327/500]	Time  8.222 ( 8.222)	Loss 1.8090 (1.6518)	CeLoss 0.2910 (0.3177)	SegCLSLoss 0.0117 (0.0175)	KLLoss 0.3789 (0.3408)	MaskLoss 0.7375 (0.6456)	MaskBCELoss 0.1410 (0.1359)	MaskDICELoss 0.5965 (0.5097)
Epoch: [3][328/500]	Time  8.178 ( 8.178)	Loss 2.4835 (1.6138)	CeLoss 0.2051 (0.4094)	SegCLSLoss 0.0161 (0.0125)	KLLoss 0.3750 (0.3002)	MaskLoss 1.1167 (0.5842)	MaskBCELoss 0.2976 (0.1351)	MaskDICELoss 0.8191 (0.4491)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 325 is less than current step: 499. Dropping entry: {'train/loss': 1.7743051588535308, 'train/ce_loss': 0.200537109375, 'train/seg_cls_loss': 0.01839599609375, 'train/kl_loss': 0.3359375, 'train/mask_bce_loss': 0.08926278292201459, 'train/mask_dice_loss': 0.6761856839060784, 'train/mask_loss': 0.7654484808444977, 'metrics/total_secs_per_batch': 9.953353881835938, 'metrics/data_secs_per_batch': 4.844749331474304, '_timestamp': 1740962770.6944466}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 325 is less than current step: 499. Dropping entry: {'train/lr': 8.283673469387754e-05, '_timestamp': 1740962770.6946275}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 326 is less than current step: 499. Dropping entry: {'train/loss': 1.6518410325050354, 'train/ce_loss': 0.31767578125, 'train/seg_cls_loss': 0.01748046875, 'train/kl_loss': 0.3408203125, 'train/mask_bce_loss': 0.1359077852219343, 'train/mask_dice_loss': 0.5097392946481705, 'train/mask_loss': 0.64564708173275, 'metrics/total_secs_per_batch': 8.22230339050293, 'metrics/data_secs_per_batch': 3.571217942237854, '_timestamp': 1740962778.9168372}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 326 is less than current step: 499. Dropping entry: {'train/lr': 8.27142857142857e-05, '_timestamp': 1740962778.9171028}).
Epoch: [3][329/500]	Time  7.753 ( 7.753)	Loss 0.8450 (1.6219)	CeLoss 0.2637 (0.4119)	SegCLSLoss 0.0121 (0.0136)	KLLoss 0.3750 (0.2617)	MaskLoss 0.2692 (0.5886)	MaskBCELoss 0.0714 (0.0801)	MaskDICELoss 0.1978 (0.5085)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 327 is less than current step: 499. Dropping entry: {'train/loss': 1.6138498544692994, 'train/ce_loss': 0.409375, 'train/seg_cls_loss': 0.012469482421875, 'train/kl_loss': 0.3001953125, 'train/mask_bce_loss': 0.13507550582289696, 'train/mask_dice_loss': 0.44909551814198495, 'train/mask_loss': 0.5841710314154624, 'metrics/total_secs_per_batch': 8.177667140960693, 'metrics/data_secs_per_batch': 3.426376748085022, '_timestamp': 1740962787.0944889}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 327 is less than current step: 499. Dropping entry: {'train/lr': 8.259183673469388e-05, '_timestamp': 1740962787.0947642}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 328 is less than current step: 499. Dropping entry: {'train/loss': 1.621911245584488, 'train/ce_loss': 0.4119140625, 'train/seg_cls_loss': 0.013555908203125, 'train/kl_loss': 0.26171875, 'train/mask_bce_loss': 0.08009480773471296, 'train/mask_dice_loss': 0.5084975302219391, 'train/mask_loss': 0.5885923355817795, 'metrics/total_secs_per_batch': 7.753216743469238, 'metrics/data_secs_per_batch': 3.2705371141433717, '_timestamp': 1740962794.847938}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 328 is less than current step: 499. Dropping entry: {'train/lr': 8.246938775510203e-05, '_timestamp': 1740962794.8482869}).
[2025-03-02 18:46:43,316] [INFO] [logging.py:96:log_dist] [Rank 0] step=1830, skipped=0, lr=[8.240816326530612e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:46:43,322] [INFO] [timer.py:215:stop] epoch=0/micro_step=18300/global_step=1830, RunningAvgSamplesPerSec=1.5196509675967473, CurrSamplesPerSec=1.1801169379046035, MemAllocated=31.25GB, MaxMemAllocated=37.19GB
Epoch: [3][330/500]	Time  8.476 ( 8.476)	Loss 1.6285 (1.4109)	CeLoss 0.2402 (0.3092)	SegCLSLoss 0.0154 (0.0115)	KLLoss 0.3945 (0.2666)	MaskLoss 0.6707 (0.5348)	MaskBCELoss 0.1992 (0.1234)	MaskDICELoss 0.4715 (0.4115)
Epoch: [3][331/500]	Time  8.315 ( 8.315)	Loss 1.4123 (1.7935)	CeLoss 0.2236 (0.2420)	SegCLSLoss 0.0182 (0.0179)	KLLoss 0.3750 (0.3803)	MaskLoss 0.5714 (0.7524)	MaskBCELoss 0.0820 (0.2038)	MaskDICELoss 0.4894 (0.5486)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 329 is less than current step: 499. Dropping entry: {'train/loss': 1.410942280292511, 'train/ce_loss': 0.3091552734375, 'train/seg_cls_loss': 0.011505126953125, 'train/kl_loss': 0.2666015625, 'train/mask_bce_loss': 0.12336408607661724, 'train/mask_dice_loss': 0.41146496683359146, 'train/mask_loss': 0.53482905626297, 'metrics/total_secs_per_batch': 8.475526094436646, 'metrics/data_secs_per_batch': 3.5962532997131347, '_timestamp': 1740962803.3230655}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 329 is less than current step: 499. Dropping entry: {'train/lr': 8.23469387755102e-05, '_timestamp': 1740962803.3232512}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 330 is less than current step: 499. Dropping entry: {'train/loss': 1.7934791803359986, 'train/ce_loss': 0.2419921875, 'train/seg_cls_loss': 0.017864990234375, 'train/kl_loss': 0.3802734375, 'train/mask_bce_loss': 0.20383210629224777, 'train/mask_dice_loss': 0.5485715389251709, 'train/mask_loss': 0.7524036586284637, 'metrics/total_secs_per_batch': 8.314708709716797, 'metrics/data_secs_per_batch': 4.055265760421753, '_timestamp': 1740962811.6379905}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 330 is less than current step: 499. Dropping entry: {'train/lr': 8.222448979591836e-05, '_timestamp': 1740962811.6382725}).
Epoch: [3][332/500]	Time  6.629 ( 6.629)	Loss 2.6121 (1.4675)	CeLoss 0.2891 (0.7335)	SegCLSLoss 0.0160 (0.0082)	KLLoss 0.3652 (0.1889)	MaskLoss 1.1391 (0.3554)	MaskBCELoss 0.3200 (0.0581)	MaskDICELoss 0.8191 (0.2973)
Epoch: [3][333/500]	Time  6.643 ( 6.643)	Loss 2.2056 (1.7216)	CeLoss 0.1885 (0.4683)	SegCLSLoss 0.0212 (0.0127)	KLLoss 0.3848 (0.2674)	MaskLoss 0.9836 (0.6101)	MaskBCELoss 0.1591 (0.2111)	MaskDICELoss 0.8245 (0.3990)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 331 is less than current step: 499. Dropping entry: {'train/loss': 1.4675403356552124, 'train/ce_loss': 0.73349609375, 'train/seg_cls_loss': 0.008160400390625, 'train/kl_loss': 0.1888671875, 'train/mask_bce_loss': 0.058144602552056314, 'train/mask_dice_loss': 0.2973052591085434, 'train/mask_loss': 0.35544986128807066, 'metrics/total_secs_per_batch': 6.629474401473999, 'metrics/data_secs_per_batch': 2.7804356813430786, '_timestamp': 1740962818.2674434}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 331 is less than current step: 499. Dropping entry: {'train/lr': 8.210204081632652e-05, '_timestamp': 1740962818.2677526}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 332 is less than current step: 499. Dropping entry: {'train/loss': 1.7215873003005981, 'train/ce_loss': 0.46826171875, 'train/seg_cls_loss': 0.0126708984375, 'train/kl_loss': 0.2673828125, 'train/mask_bce_loss': 0.21106626316905022, 'train/mask_dice_loss': 0.3990438014268875, 'train/mask_loss': 0.6101100608706475, 'metrics/total_secs_per_batch': 6.642826080322266, 'metrics/data_secs_per_batch': 2.750999927520752, '_timestamp': 1740962824.9102898}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 332 is less than current step: 499. Dropping entry: {'train/lr': 8.197959183673469e-05, '_timestamp': 1740962824.910623}).
Epoch: [3][334/500]	Time  9.004 ( 9.004)	Loss 2.1085 (1.7225)	CeLoss 0.2520 (0.2895)	SegCLSLoss 0.0239 (0.0150)	KLLoss 0.3789 (0.3416)	MaskLoss 0.9038 (0.6959)	MaskBCELoss 0.0981 (0.1137)	MaskDICELoss 0.8057 (0.5822)
Epoch: [3][335/500]	Time  9.614 ( 9.614)	Loss 1.6820 (1.6193)	CeLoss 0.2656 (0.2466)	SegCLSLoss 0.0117 (0.0148)	KLLoss 0.3906 (0.3371)	MaskLoss 0.6857 (0.6657)	MaskBCELoss 0.2529 (0.1386)	MaskDICELoss 0.4329 (0.5271)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 333 is less than current step: 499. Dropping entry: {'train/loss': 1.7224605083465576, 'train/ce_loss': 0.289453125, 'train/seg_cls_loss': 0.01497802734375, 'train/kl_loss': 0.3416015625, 'train/mask_bce_loss': 0.11371568785980343, 'train/mask_dice_loss': 0.5821825325489044, 'train/mask_loss': 0.6958982348442078, 'metrics/total_secs_per_batch': 9.00379753112793, 'metrics/data_secs_per_batch': 4.119617652893067, '_timestamp': 1740962833.9142156}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 333 is less than current step: 499. Dropping entry: {'train/lr': 8.185714285714285e-05, '_timestamp': 1740962833.9145267}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 334 is less than current step: 499. Dropping entry: {'train/loss': 1.6192616105079651, 'train/ce_loss': 0.246630859375, 'train/seg_cls_loss': 0.0147705078125, 'train/kl_loss': 0.337109375, 'train/mask_bce_loss': 0.13856122381985186, 'train/mask_dice_loss': 0.5271486729383469, 'train/mask_loss': 0.6657098978757858, 'metrics/total_secs_per_batch': 9.613601684570312, 'metrics/data_secs_per_batch': 4.511901903152466, '_timestamp': 1740962843.5278747}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 334 is less than current step: 499. Dropping entry: {'train/lr': 8.173469387755101e-05, '_timestamp': 1740962843.5282173}).
Epoch: [3][336/500]	Time  7.983 ( 7.983)	Loss 0.5820 (1.5526)	CeLoss 0.5820 (0.4383)	SegCLSLoss 0.0000 (0.0127)	KLLoss 0.0000 (0.2270)	MaskLoss 0.0000 (0.5427)	MaskBCELoss 0.0000 (0.0852)	MaskDICELoss 0.0000 (0.4574)
Epoch: [3][337/500]	Time  8.349 ( 8.349)	Loss 2.1957 (1.7753)	CeLoss 0.2520 (0.2928)	SegCLSLoss 0.0142 (0.0160)	KLLoss 0.3809 (0.3021)	MaskLoss 0.9494 (0.7221)	MaskBCELoss 0.0525 (0.1335)	MaskDICELoss 0.8970 (0.5886)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 335 is less than current step: 499. Dropping entry: {'train/loss': 1.5526338458061217, 'train/ce_loss': 0.43828125, 'train/seg_cls_loss': 0.012652587890625, 'train/kl_loss': 0.226953125, 'train/mask_bce_loss': 0.0852429773658514, 'train/mask_dice_loss': 0.45743137001991274, 'train/mask_loss': 0.5426743447780609, 'metrics/total_secs_per_batch': 7.982887268066406, 'metrics/data_secs_per_batch': 3.6970270395278932, '_timestamp': 1740962851.5105822}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 335 is less than current step: 499. Dropping entry: {'train/lr': 8.161224489795917e-05, '_timestamp': 1740962851.5108497}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 336 is less than current step: 499. Dropping entry: {'train/loss': 1.7752758383750915, 'train/ce_loss': 0.2927734375, 'train/seg_cls_loss': 0.01602783203125, 'train/kl_loss': 0.3021484375, 'train/mask_bce_loss': 0.1334781415760517, 'train/mask_dice_loss': 0.5886079937219619, 'train/mask_loss': 0.7220861375331878, 'metrics/total_secs_per_batch': 8.34864330291748, 'metrics/data_secs_per_batch': 3.851544976234436, '_timestamp': 1740962859.8594422}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 336 is less than current step: 499. Dropping entry: {'train/lr': 8.148979591836734e-05, '_timestamp': 1740962859.8597944}).
Epoch: [3][338/500]	Time  8.829 ( 8.829)	Loss 1.5469 (1.9056)	CeLoss 1.5469 (0.3775)	SegCLSLoss 0.0000 (0.0169)	KLLoss 0.0000 (0.2961)	MaskLoss 0.0000 (0.7449)	MaskBCELoss 0.0000 (0.1505)	MaskDICELoss 0.0000 (0.5944)
Epoch: [3][339/500]	Time  6.787 ( 6.787)	Loss 2.1812 (1.6643)	CeLoss 0.2695 (0.5075)	SegCLSLoss 0.0131 (0.0111)	KLLoss 0.3867 (0.2271)	MaskLoss 0.9334 (0.5642)	MaskBCELoss 0.1019 (0.0714)	MaskDICELoss 0.8314 (0.4929)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 337 is less than current step: 499. Dropping entry: {'train/loss': 1.9055821180343628, 'train/ce_loss': 0.3775390625, 'train/seg_cls_loss': 0.016900634765625, 'train/kl_loss': 0.29609375, 'train/mask_bce_loss': 0.15053263753652574, 'train/mask_dice_loss': 0.5943970859050751, 'train/mask_loss': 0.7449297249317169, 'metrics/total_secs_per_batch': 8.82885456085205, 'metrics/data_secs_per_batch': 4.302750945091248, '_timestamp': 1740962868.6881847}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 337 is less than current step: 499. Dropping entry: {'train/lr': 8.13673469387755e-05, '_timestamp': 1740962868.6884909}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 338 is less than current step: 499. Dropping entry: {'train/loss': 1.6643283247947693, 'train/ce_loss': 0.50751953125, 'train/seg_cls_loss': 0.011138916015625, 'train/kl_loss': 0.2271484375, 'train/mask_bce_loss': 0.07137200189754367, 'train/mask_dice_loss': 0.4928722262382507, 'train/mask_loss': 0.5642442405223846, 'metrics/total_secs_per_batch': 6.786814212799072, 'metrics/data_secs_per_batch': 3.3167444705963134, '_timestamp': 1740962875.475016}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 338 is less than current step: 499. Dropping entry: {'train/lr': 8.124489795918368e-05, '_timestamp': 1740962875.4753594}).
[2025-03-02 18:48:04,199] [INFO] [logging.py:96:log_dist] [Rank 0] step=1840, skipped=0, lr=[8.118367346938776e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:48:04,204] [INFO] [timer.py:215:stop] epoch=0/micro_step=18400/global_step=1840, RunningAvgSamplesPerSec=1.517760504716241, CurrSamplesPerSec=1.1456280522943212, MemAllocated=30.78GB, MaxMemAllocated=37.19GB
Epoch: [3][340/500]	Time  8.731 ( 8.731)	Loss 1.0625 (1.4209)	CeLoss 1.0625 (0.3385)	SegCLSLoss 0.0000 (0.0128)	KLLoss 0.0000 (0.3369)	MaskLoss 0.0000 (0.5211)	MaskBCELoss 0.0000 (0.1352)	MaskDICELoss 0.0000 (0.3859)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 339 is less than current step: 499. Dropping entry: {'train/loss': 1.4208532691001892, 'train/ce_loss': 0.3384765625, 'train/seg_cls_loss': 0.012811279296875, 'train/kl_loss': 0.3369140625, 'train/mask_bce_loss': 0.1351913742721081, 'train/mask_dice_loss': 0.38592862263321875, 'train/mask_loss': 0.5211199924349785, 'metrics/total_secs_per_batch': 8.730534315109253, 'metrics/data_secs_per_batch': 4.306911659240723, '_timestamp': 1740962884.2052586}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 339 is less than current step: 499. Dropping entry: {'train/lr': 8.112244897959184e-05, '_timestamp': 1740962884.2055252}).
Epoch: [3][341/500]	Time  7.939 ( 7.939)	Loss 1.8051 (1.8867)	CeLoss 0.1768 (0.4839)	SegCLSLoss 0.0272 (0.0131)	KLLoss 0.3594 (0.2588)	MaskLoss 0.7893 (0.6852)	MaskBCELoss 0.0833 (0.1506)	MaskDICELoss 0.7060 (0.5345)
Epoch: [3][342/500]	Time  8.559 ( 8.559)	Loss 1.2266 (1.3253)	CeLoss 1.2266 (0.3751)	SegCLSLoss 0.0000 (0.0104)	KLLoss 0.0000 (0.2678)	MaskLoss 0.0000 (0.4591)	MaskBCELoss 0.0000 (0.0696)	MaskDICELoss 0.0000 (0.3894)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 340 is less than current step: 499. Dropping entry: {'train/loss': 1.8866512775421143, 'train/ce_loss': 0.48388671875, 'train/seg_cls_loss': 0.013092041015625, 'train/kl_loss': 0.2587890625, 'train/mask_bce_loss': 0.15062471441924571, 'train/mask_dice_loss': 0.5345466375350952, 'train/mask_loss': 0.6851713478565216, 'metrics/total_secs_per_batch': 7.938766002655029, 'metrics/data_secs_per_batch': 3.582602024078369, '_timestamp': 1740962892.1443648}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 340 is less than current step: 499. Dropping entry: {'train/lr': 8.1e-05, '_timestamp': 1740962892.1447196}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 341 is less than current step: 499. Dropping entry: {'train/loss': 1.3253330945968629, 'train/ce_loss': 0.37509765625, 'train/seg_cls_loss': 0.01036376953125, 'train/kl_loss': 0.2677734375, 'train/mask_bce_loss': 0.0696151228621602, 'train/mask_dice_loss': 0.38943814039230346, 'train/mask_loss': 0.4590532690286636, 'metrics/total_secs_per_batch': 8.558741331100464, 'metrics/data_secs_per_batch': 3.834520196914673, '_timestamp': 1740962900.7030666}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 341 is less than current step: 499. Dropping entry: {'train/lr': 8.087755102040817e-05, '_timestamp': 1740962900.7033696}).
Epoch: [3][343/500]	Time  7.076 ( 7.076)	Loss 2.9525 (2.0146)	CeLoss 0.1650 (0.4513)	SegCLSLoss 0.0291 (0.0153)	KLLoss 0.3828 (0.2652)	MaskLoss 1.3674 (0.7646)	MaskBCELoss 0.4059 (0.2161)	MaskDICELoss 0.9614 (0.5485)
Epoch: [3][344/500]	Time  7.046 ( 7.046)	Loss 0.5859 (1.3092)	CeLoss 0.5859 (0.4039)	SegCLSLoss 0.0000 (0.0119)	KLLoss 0.0000 (0.2242)	MaskLoss 0.0000 (0.4385)	MaskBCELoss 0.0000 (0.0950)	MaskDICELoss 0.0000 (0.3435)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 342 is less than current step: 499. Dropping entry: {'train/loss': 2.0145953059196473, 'train/ce_loss': 0.45126953125, 'train/seg_cls_loss': 0.015277099609375, 'train/kl_loss': 0.265234375, 'train/mask_bce_loss': 0.2160832904279232, 'train/mask_dice_loss': 0.5484897285699845, 'train/mask_loss': 0.7645730316638947, 'metrics/total_secs_per_batch': 7.076319694519043, 'metrics/data_secs_per_batch': 2.7996463537216187, '_timestamp': 1740962907.7792642}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 342 is less than current step: 499. Dropping entry: {'train/lr': 8.075510204081632e-05, '_timestamp': 1740962907.7795224}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 343 is less than current step: 499. Dropping entry: {'train/loss': 1.3091945886611938, 'train/ce_loss': 0.403857421875, 'train/seg_cls_loss': 0.011944580078125, 'train/kl_loss': 0.22421875, 'train/mask_bce_loss': 0.09497255254536867, 'train/mask_dice_loss': 0.34348705112934114, 'train/mask_loss': 0.4384596019983292, 'metrics/total_secs_per_batch': 7.04609751701355, 'metrics/data_secs_per_batch': 3.0820887088775635, '_timestamp': 1740962914.825387}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 343 is less than current step: 499. Dropping entry: {'train/lr': 8.063265306122448e-05, '_timestamp': 1740962914.825659}).
Epoch: [3][345/500]	Time  8.148 ( 8.148)	Loss 1.9127 (1.8119)	CeLoss 0.2109 (0.3860)	SegCLSLoss 0.0190 (0.0151)	KLLoss 0.3652 (0.2582)	MaskLoss 0.8275 (0.6962)	MaskBCELoss 0.0434 (0.1478)	MaskDICELoss 0.7840 (0.5484)
Epoch: [3][346/500]	Time  7.825 ( 7.825)	Loss 0.6602 (1.5394)	CeLoss 0.6602 (0.4513)	SegCLSLoss 0.0000 (0.0136)	KLLoss 0.0000 (0.2611)	MaskLoss 0.0000 (0.5277)	MaskBCELoss 0.0000 (0.0644)	MaskDICELoss 0.0000 (0.4633)
Epoch: [3][347/500]	Time  6.357 ( 6.357)	Loss 1.5383 (1.4215)	CeLoss 0.2617 (0.6817)	SegCLSLoss 0.0129 (0.0097)	KLLoss 0.3828 (0.2230)	MaskLoss 0.6158 (0.3564)	MaskBCELoss 0.1904 (0.0810)	MaskDICELoss 0.4255 (0.2754)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 344 is less than current step: 499. Dropping entry: {'train/loss': 1.811878252029419, 'train/ce_loss': 0.385986328125, 'train/seg_cls_loss': 0.01514892578125, 'train/kl_loss': 0.258203125, 'train/mask_bce_loss': 0.14782073423266412, 'train/mask_dice_loss': 0.5484260231256485, 'train/mask_loss': 0.696246761083603, 'metrics/total_secs_per_batch': 8.147908926010132, 'metrics/data_secs_per_batch': 3.4182636976242065, '_timestamp': 1740962922.973279}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 344 is less than current step: 499. Dropping entry: {'train/lr': 8.051020408163264e-05, '_timestamp': 1740962922.9735026}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 345 is less than current step: 499. Dropping entry: {'train/loss': 1.5393939733505249, 'train/ce_loss': 0.45126953125, 'train/seg_cls_loss': 0.01363525390625, 'train/kl_loss': 0.2611328125, 'train/mask_bce_loss': 0.06441214475780725, 'train/mask_dice_loss': 0.4632926434278488, 'train/mask_loss': 0.5277047872543335, 'metrics/total_secs_per_batch': 7.825126886367798, 'metrics/data_secs_per_batch': 3.304488205909729, '_timestamp': 1740962930.7984014}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 345 is less than current step: 499. Dropping entry: {'train/lr': 8.03877551020408e-05, '_timestamp': 1740962930.7986555}).
Epoch: [3][348/500]	Time  7.729 ( 7.729)	Loss 1.6567 (1.9065)	CeLoss 0.2393 (0.3425)	SegCLSLoss 0.0234 (0.0168)	KLLoss 0.3672 (0.3359)	MaskLoss 0.6848 (0.7610)	MaskBCELoss 0.1629 (0.1692)	MaskDICELoss 0.5219 (0.5918)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 346 is less than current step: 499. Dropping entry: {'train/loss': 1.4215049982070922, 'train/ce_loss': 0.68173828125, 'train/seg_cls_loss': 0.009686279296875, 'train/kl_loss': 0.223046875, 'train/mask_bce_loss': 0.08095189649611712, 'train/mask_dice_loss': 0.2754060670733452, 'train/mask_loss': 0.3563579633831978, 'metrics/total_secs_per_batch': 6.357406139373779, 'metrics/data_secs_per_batch': 2.783560013771057, '_timestamp': 1740962937.1558003}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 346 is less than current step: 499. Dropping entry: {'train/lr': 8.026530612244897e-05, '_timestamp': 1740962937.1561046}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 347 is less than current step: 499. Dropping entry: {'train/loss': 1.9064702033996581, 'train/ce_loss': 0.34248046875, 'train/seg_cls_loss': 0.01678466796875, 'train/kl_loss': 0.3359375, 'train/mask_bce_loss': 0.16918576769530774, 'train/mask_dice_loss': 0.5918130099773407, 'train/mask_loss': 0.7609987735748291, 'metrics/total_secs_per_batch': 7.729491710662842, 'metrics/data_secs_per_batch': 3.8441200256347656, '_timestamp': 1740962944.8854868}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 347 is less than current step: 499. Dropping entry: {'train/lr': 8.014285714285713e-05, '_timestamp': 1740962944.8858209}).
Epoch: [3][349/500]	Time  6.756 ( 6.756)	Loss 2.4073 (1.7883)	CeLoss 0.1523 (0.3744)	SegCLSLoss 0.0388 (0.0188)	KLLoss 0.3652 (0.2977)	MaskLoss 1.0991 (0.6873)	MaskBCELoss 0.3947 (0.1375)	MaskDICELoss 0.7044 (0.5498)
[2025-03-02 18:49:20,707] [INFO] [logging.py:96:log_dist] [Rank 0] step=1850, skipped=0, lr=[7.995918367346938e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:49:20,713] [INFO] [timer.py:215:stop] epoch=0/micro_step=18500/global_step=1850, RunningAvgSamplesPerSec=1.5164390662514429, CurrSamplesPerSec=1.1023753046720584, MemAllocated=30.94GB, MaxMemAllocated=37.19GB
Epoch: [3][350/500]	Time  9.073 ( 9.073)	Loss 1.5921 (1.9676)	CeLoss 0.2520 (0.2080)	SegCLSLoss 0.0101 (0.0202)	KLLoss 0.3750 (0.3732)	MaskLoss 0.6486 (0.8559)	MaskBCELoss 0.3423 (0.2097)	MaskDICELoss 0.3063 (0.6462)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 348 is less than current step: 499. Dropping entry: {'train/loss': 1.7882713079452515, 'train/ce_loss': 0.3744140625, 'train/seg_cls_loss': 0.018841552734375, 'train/kl_loss': 0.29765625, 'train/mask_bce_loss': 0.13746225461363792, 'train/mask_dice_loss': 0.5498374551534653, 'train/mask_loss': 0.6872997105121612, 'metrics/total_secs_per_batch': 6.755940198898315, 'metrics/data_secs_per_batch': 2.7155839443206786, '_timestamp': 1740962951.6413028}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 348 is less than current step: 499. Dropping entry: {'train/lr': 8.00204081632653e-05, '_timestamp': 1740962951.641591}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 349 is less than current step: 499. Dropping entry: {'train/loss': 1.967567002773285, 'train/ce_loss': 0.2080078125, 'train/seg_cls_loss': 0.02020263671875, 'train/kl_loss': 0.3732421875, 'train/mask_bce_loss': 0.20968109248206018, 'train/mask_dice_loss': 0.646221536397934, 'train/mask_loss': 0.855902636051178, 'metrics/total_secs_per_batch': 9.072872877120972, 'metrics/data_secs_per_batch': 3.8573733806610107, '_timestamp': 1740962960.713959}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 349 is less than current step: 499. Dropping entry: {'train/lr': 7.989795918367346e-05, '_timestamp': 1740962960.7142184}).
Epoch: [3][351/500]	Time  8.368 ( 8.368)	Loss 1.7221 (1.6249)	CeLoss 0.3242 (0.2913)	SegCLSLoss 0.0114 (0.0146)	KLLoss 0.3789 (0.2975)	MaskLoss 0.6775 (0.6483)	MaskBCELoss 0.2543 (0.1611)	MaskDICELoss 0.4231 (0.4872)
Epoch: [3][352/500]	Time  8.464 ( 8.464)	Loss 2.8399 (1.7045)	CeLoss 0.1943 (0.2262)	SegCLSLoss 0.0223 (0.0187)	KLLoss 0.3574 (0.3326)	MaskLoss 1.2999 (0.7178)	MaskBCELoss 0.4482 (0.1516)	MaskDICELoss 0.8517 (0.5662)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 350 is less than current step: 499. Dropping entry: {'train/loss': 1.6248514533042908, 'train/ce_loss': 0.29130859375, 'train/seg_cls_loss': 0.014605712890625, 'train/kl_loss': 0.2974609375, 'train/mask_bce_loss': 0.16114013441838324, 'train/mask_dice_loss': 0.4871742770075798, 'train/mask_loss': 0.6483144015073776, 'metrics/total_secs_per_batch': 8.367956161499023, 'metrics/data_secs_per_batch': 3.7304511308670043, '_timestamp': 1740962969.0820816}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 350 is less than current step: 499. Dropping entry: {'train/lr': 7.977551020408163e-05, '_timestamp': 1740962969.0823529}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 351 is less than current step: 499. Dropping entry: {'train/loss': 1.7045255422592163, 'train/ce_loss': 0.226220703125, 'train/seg_cls_loss': 0.01875, 'train/kl_loss': 0.3326171875, 'train/mask_bce_loss': 0.1516348786652088, 'train/mask_dice_loss': 0.5661796495318413, 'train/mask_loss': 0.7178145200014114, 'metrics/total_secs_per_batch': 8.463659763336182, 'metrics/data_secs_per_batch': 3.340304398536682, '_timestamp': 1740962977.5457597}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 351 is less than current step: 499. Dropping entry: {'train/lr': 7.96530612244898e-05, '_timestamp': 1740962977.5460255}).
Epoch: [3][353/500]	Time  8.760 ( 8.760)	Loss 0.3926 (1.3456)	CeLoss 0.3926 (0.3763)	SegCLSLoss 0.0000 (0.0085)	KLLoss 0.0000 (0.2244)	MaskLoss 0.0000 (0.4712)	MaskBCELoss 0.0000 (0.1356)	MaskDICELoss 0.0000 (0.3356)
Epoch: [3][354/500]	Time  7.576 ( 7.576)	Loss 1.3124 (1.4920)	CeLoss 0.1963 (0.4102)	SegCLSLoss 0.0204 (0.0144)	KLLoss 0.3633 (0.2584)	MaskLoss 0.5351 (0.5244)	MaskBCELoss 0.0412 (0.0967)	MaskDICELoss 0.4939 (0.4277)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 352 is less than current step: 499. Dropping entry: {'train/loss': 1.345555877685547, 'train/ce_loss': 0.376318359375, 'train/seg_cls_loss': 0.00845947265625, 'train/kl_loss': 0.2244140625, 'train/mask_bce_loss': 0.13561910800635815, 'train/mask_dice_loss': 0.33562074303627015, 'train/mask_loss': 0.4712398529052734, 'metrics/total_secs_per_batch': 8.760430812835693, 'metrics/data_secs_per_batch': 3.7835015535354612, '_timestamp': 1740962986.3061855}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 352 is less than current step: 499. Dropping entry: {'train/lr': 7.953061224489796e-05, '_timestamp': 1740962986.3064623}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 353 is less than current step: 499. Dropping entry: {'train/loss': 1.4920279443264008, 'train/ce_loss': 0.41015625, 'train/seg_cls_loss': 0.01435546875, 'train/kl_loss': 0.2583984375, 'train/mask_bce_loss': 0.09671658910810947, 'train/mask_dice_loss': 0.4276665210723877, 'train/mask_loss': 0.5243831068277359, 'metrics/total_secs_per_batch': 7.575961112976074, 'metrics/data_secs_per_batch': 3.432729887962341, '_timestamp': 1740962993.8822637}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 353 is less than current step: 499. Dropping entry: {'train/lr': 7.940816326530612e-05, '_timestamp': 1740962993.8826072}).
Epoch: [3][355/500]	Time  6.570 ( 6.570)	Loss 2.4640 (1.6215)	CeLoss 0.1973 (0.3926)	SegCLSLoss 0.0243 (0.0151)	KLLoss 0.3770 (0.2611)	MaskLoss 1.1090 (0.5977)	MaskBCELoss 0.1585 (0.1018)	MaskDICELoss 0.9505 (0.4958)
Epoch: [3][356/500]	Time  8.732 ( 8.732)	Loss 1.4844 (1.9594)	CeLoss 1.4844 (0.4696)	SegCLSLoss 0.0000 (0.0175)	KLLoss 0.0000 (0.2955)	MaskLoss 0.0000 (0.7257)	MaskBCELoss 0.0000 (0.1202)	MaskDICELoss 0.0000 (0.6055)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 354 is less than current step: 499. Dropping entry: {'train/loss': 1.6214951276779175, 'train/ce_loss': 0.392578125, 'train/seg_cls_loss': 0.015142822265625, 'train/kl_loss': 0.2611328125, 'train/mask_bce_loss': 0.10181555431336164, 'train/mask_dice_loss': 0.4958460614085197, 'train/mask_loss': 0.5976616084575653, 'metrics/total_secs_per_batch': 6.569884300231934, 'metrics/data_secs_per_batch': 2.844494080543518, '_timestamp': 1740963000.452183}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 354 is less than current step: 499. Dropping entry: {'train/lr': 7.928571428571429e-05, '_timestamp': 1740963000.4524875}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 355 is less than current step: 499. Dropping entry: {'train/loss': 1.959440541267395, 'train/ce_loss': 0.46962890625, 'train/seg_cls_loss': 0.01754150390625, 'train/kl_loss': 0.2955078125, 'train/mask_bce_loss': 0.12024913746863604, 'train/mask_dice_loss': 0.6054672479629517, 'train/mask_loss': 0.7257163882255554, 'metrics/total_secs_per_batch': 8.732288360595703, 'metrics/data_secs_per_batch': 4.218041706085205, '_timestamp': 1740963009.1843522}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 355 is less than current step: 499. Dropping entry: {'train/lr': 7.916326530612244e-05, '_timestamp': 1740963009.1846242}).
Epoch: [3][357/500]	Time  6.760 ( 6.760)	Loss 2.5663 (1.6929)	CeLoss 0.1885 (0.4304)	SegCLSLoss 0.0227 (0.0171)	KLLoss 0.3574 (0.3002)	MaskLoss 1.1650 (0.6117)	MaskBCELoss 0.4446 (0.2094)	MaskDICELoss 0.7203 (0.4023)
Epoch: [3][358/500]	Time  8.104 ( 8.104)	Loss 1.1016 (1.9432)	CeLoss 1.1016 (0.4297)	SegCLSLoss 0.0000 (0.0139)	KLLoss 0.0000 (0.2922)	MaskLoss 0.0000 (0.7386)	MaskBCELoss 0.0000 (0.1050)	MaskDICELoss 0.0000 (0.6336)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 356 is less than current step: 499. Dropping entry: {'train/loss': 1.6928763151168824, 'train/ce_loss': 0.43037109375, 'train/seg_cls_loss': 0.017144775390625, 'train/kl_loss': 0.3001953125, 'train/mask_bce_loss': 0.20939403171651066, 'train/mask_dice_loss': 0.40232732594013215, 'train/mask_loss': 0.6117213547229767, 'metrics/total_secs_per_batch': 6.759877443313599, 'metrics/data_secs_per_batch': 2.6396337509155274, '_timestamp': 1740963015.9442246}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 356 is less than current step: 499. Dropping entry: {'train/lr': 7.90408163265306e-05, '_timestamp': 1740963015.9445028}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 357 is less than current step: 499. Dropping entry: {'train/loss': 1.94321665763855, 'train/ce_loss': 0.4296875, 'train/seg_cls_loss': 0.01385498046875, 'train/kl_loss': 0.2921875, 'train/mask_bce_loss': 0.10501681445166469, 'train/mask_dice_loss': 0.6336325228214263, 'train/mask_loss': 0.7386493384838104, 'metrics/total_secs_per_batch': 8.103610515594482, 'metrics/data_secs_per_batch': 3.610385370254517, '_timestamp': 1740963024.0478492}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 357 is less than current step: 499. Dropping entry: {'train/lr': 7.891836734693876e-05, '_timestamp': 1740963024.0481234}).
Epoch: [3][359/500]	Time  9.468 ( 9.468)	Loss 2.0260 (2.0678)	CeLoss 0.1914 (0.2182)	SegCLSLoss 0.0242 (0.0174)	KLLoss 0.3652 (0.3709)	MaskLoss 0.8929 (0.9018)	MaskBCELoss 0.0813 (0.2465)	MaskDICELoss 0.8116 (0.6553)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 358 is less than current step: 499. Dropping entry: {'train/loss': 2.067848026752472, 'train/ce_loss': 0.2181640625, 'train/seg_cls_loss': 0.017449951171875, 'train/kl_loss': 0.3708984375, 'train/mask_bce_loss': 0.2465334243606776, 'train/mask_dice_loss': 0.6552616626024246, 'train/mask_loss': 0.9017951011657714, 'metrics/total_secs_per_batch': 9.467581033706665, 'metrics/data_secs_per_batch': 4.372138333320618, '_timestamp': 1740963033.5154183}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 358 is less than current step: 499. Dropping entry: {'train/lr': 7.879591836734693e-05, '_timestamp': 1740963033.5156882}).
[2025-03-02 18:50:41,644] [INFO] [logging.py:96:log_dist] [Rank 0] step=1860, skipped=0, lr=[7.8734693877551e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:50:41,650] [INFO] [timer.py:215:stop] epoch=0/micro_step=18600/global_step=1860, RunningAvgSamplesPerSec=1.5145871164586786, CurrSamplesPerSec=1.229346374832666, MemAllocated=30.72GB, MaxMemAllocated=37.19GB
Epoch: [3][360/500]	Time  8.136 ( 8.136)	Loss 1.1797 (1.6897)	CeLoss 1.1797 (0.4213)	SegCLSLoss 0.0000 (0.0167)	KLLoss 0.0000 (0.2963)	MaskLoss 0.0000 (0.6152)	MaskBCELoss 0.0000 (0.1505)	MaskDICELoss 0.0000 (0.4647)
Epoch: [3][361/500]	Time  7.116 ( 7.116)	Loss 0.1475 (1.3609)	CeLoss 0.1475 (0.3787)	SegCLSLoss 0.0000 (0.0130)	KLLoss 0.0000 (0.2576)	MaskLoss 0.0000 (0.4749)	MaskBCELoss 0.0000 (0.1083)	MaskDICELoss 0.0000 (0.3667)
Epoch: [3][362/500]	Time  7.355 ( 7.355)	Loss 1.3070 (1.7613)	CeLoss 0.2441 (0.4618)	SegCLSLoss 0.0130 (0.0172)	KLLoss 0.3770 (0.2949)	MaskLoss 0.5099 (0.6306)	MaskBCELoss 0.1126 (0.0706)	MaskDICELoss 0.3974 (0.5599)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 359 is less than current step: 499. Dropping entry: {'train/loss': 1.6896678388118744, 'train/ce_loss': 0.421337890625, 'train/seg_cls_loss': 0.01666259765625, 'train/kl_loss': 0.2962890625, 'train/mask_bce_loss': 0.15049414820969104, 'train/mask_dice_loss': 0.4647010967135429, 'train/mask_loss': 0.6151952415704727, 'metrics/total_secs_per_batch': 8.135887145996094, 'metrics/data_secs_per_batch': 3.1938556432724, '_timestamp': 1740963041.651107}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 359 is less than current step: 499. Dropping entry: {'train/lr': 7.867346938775509e-05, '_timestamp': 1740963041.651283}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 360 is less than current step: 499. Dropping entry: {'train/loss': 1.3609320282936097, 'train/ce_loss': 0.3787109375, 'train/seg_cls_loss': 0.013006591796875, 'train/kl_loss': 0.2576171875, 'train/mask_bce_loss': 0.10829515196383, 'train/mask_dice_loss': 0.3666532874107361, 'train/mask_loss': 0.47494843453168867, 'metrics/total_secs_per_batch': 7.115508079528809, 'metrics/data_secs_per_batch': 3.0048442840576173, '_timestamp': 1740963048.76682}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 360 is less than current step: 499. Dropping entry: {'train/lr': 7.855102040816325e-05, '_timestamp': 1740963048.767073}).
Epoch: [3][363/500]	Time  8.619 ( 8.619)	Loss 0.8409 (1.6387)	CeLoss 0.2373 (0.3069)	SegCLSLoss 0.0106 (0.0180)	KLLoss 0.3711 (0.3322)	MaskLoss 0.2808 (0.6448)	MaskBCELoss 0.0504 (0.1640)	MaskDICELoss 0.2305 (0.4808)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 361 is less than current step: 499. Dropping entry: {'train/loss': 1.7613028049468995, 'train/ce_loss': 0.46181640625, 'train/seg_cls_loss': 0.01715087890625, 'train/kl_loss': 0.294921875, 'train/mask_bce_loss': 0.07061782826203852, 'train/mask_dice_loss': 0.5599359154701233, 'train/mask_loss': 0.6305537402629853, 'metrics/total_secs_per_batch': 7.354604005813599, 'metrics/data_secs_per_batch': 3.185767650604248, '_timestamp': 1740963056.1213388}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 361 is less than current step: 499. Dropping entry: {'train/lr': 7.842857142857143e-05, '_timestamp': 1740963056.1215162}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 362 is less than current step: 499. Dropping entry: {'train/loss': 1.6387288749217988, 'train/ce_loss': 0.30693359375, 'train/seg_cls_loss': 0.0180419921875, 'train/kl_loss': 0.3322265625, 'train/mask_bce_loss': 0.16395889818668366, 'train/mask_dice_loss': 0.4807961508631706, 'train/mask_loss': 0.6447550654411316, 'metrics/total_secs_per_batch': 8.61861515045166, 'metrics/data_secs_per_batch': 3.8509930849075316, '_timestamp': 1740963064.7402906}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 362 is less than current step: 499. Dropping entry: {'train/lr': 7.830612244897959e-05, '_timestamp': 1740963064.7406452}).
Epoch: [3][364/500]	Time  9.439 ( 9.439)	Loss 1.2243 (1.6918)	CeLoss 0.2734 (0.3908)	SegCLSLoss 0.0134 (0.0162)	KLLoss 0.3652 (0.2959)	MaskLoss 0.4540 (0.6316)	MaskBCELoss 0.0442 (0.1043)	MaskDICELoss 0.4098 (0.5273)
Epoch: [3][365/500]	Time  8.488 ( 8.488)	Loss 0.9291 (1.5605)	CeLoss 0.2354 (0.2702)	SegCLSLoss 0.0156 (0.0192)	KLLoss 0.3711 (0.3361)	MaskLoss 0.3239 (0.6233)	MaskBCELoss 0.1005 (0.1132)	MaskDICELoss 0.2234 (0.5101)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 363 is less than current step: 499. Dropping entry: {'train/loss': 1.6917535066604614, 'train/ce_loss': 0.3908203125, 'train/seg_cls_loss': 0.0162109375, 'train/kl_loss': 0.2958984375, 'train/mask_bce_loss': 0.10426303632557392, 'train/mask_dice_loss': 0.5273070722818375, 'train/mask_loss': 0.6315701097249985, 'metrics/total_secs_per_batch': 9.438856840133667, 'metrics/data_secs_per_batch': 4.289554905891419, '_timestamp': 1740963074.1789389}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 363 is less than current step: 499. Dropping entry: {'train/lr': 7.818367346938775e-05, '_timestamp': 1740963074.1792188}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 364 is less than current step: 499. Dropping entry: {'train/loss': 1.5605462610721588, 'train/ce_loss': 0.270166015625, 'train/seg_cls_loss': 0.019189453125, 'train/kl_loss': 0.3361328125, 'train/mask_bce_loss': 0.11323142442852259, 'train/mask_dice_loss': 0.5101081073284149, 'train/mask_loss': 0.6233395367860795, 'metrics/total_secs_per_batch': 8.488144636154175, 'metrics/data_secs_per_batch': 3.9847892045974733, '_timestamp': 1740963082.6670415}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 364 is less than current step: 499. Dropping entry: {'train/lr': 7.806122448979592e-05, '_timestamp': 1740963082.6673055}).
Epoch: [3][366/500]	Time  9.442 ( 9.442)	Loss 0.9357 (1.9562)	CeLoss 0.2441 (0.2332)	SegCLSLoss 0.0132 (0.0159)	KLLoss 0.3711 (0.3705)	MaskLoss 0.3243 (0.8391)	MaskBCELoss 0.1327 (0.1727)	MaskDICELoss 0.1916 (0.6663)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 365 is less than current step: 499. Dropping entry: {'train/loss': 1.956228494644165, 'train/ce_loss': 0.233203125, 'train/seg_cls_loss': 0.015863037109375, 'train/kl_loss': 0.3705078125, 'train/mask_bce_loss': 0.1727143581956625, 'train/mask_dice_loss': 0.6663373783230782, 'train/mask_loss': 0.8390517443418503, 'metrics/total_secs_per_batch': 9.442442417144775, 'metrics/data_secs_per_batch': 4.403459167480468, '_timestamp': 1740963092.109497}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 365 is less than current step: 499. Dropping entry: {'train/lr': 7.793877551020408e-05, '_timestamp': 1740963092.1097708}).
Epoch: [3][367/500]	Time  9.866 ( 9.866)	Loss 1.7460 (1.7540)	CeLoss 0.2119 (0.2311)	SegCLSLoss 0.0194 (0.0173)	KLLoss 0.3945 (0.3721)	MaskLoss 0.7421 (0.7386)	MaskBCELoss 0.0823 (0.0944)	MaskDICELoss 0.6598 (0.6442)
Epoch: [3][368/500]	Time  6.719 ( 6.719)	Loss 1.9980 (1.3384)	CeLoss 0.2832 (0.6140)	SegCLSLoss 0.0159 (0.0083)	KLLoss 0.3691 (0.1855)	MaskLoss 0.8349 (0.3508)	MaskBCELoss 0.3911 (0.0801)	MaskDICELoss 0.4438 (0.2708)
Epoch: [3][369/500]	Time  5.922 ( 5.922)	Loss 0.2617 (1.2593)	CeLoss 0.2617 (0.6514)	SegCLSLoss 0.0000 (0.0073)	KLLoss 0.0000 (0.1477)	MaskLoss 0.0000 (0.2948)	MaskBCELoss 0.0000 (0.0608)	MaskDICELoss 0.0000 (0.2340)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 366 is less than current step: 499. Dropping entry: {'train/loss': 1.754026997089386, 'train/ce_loss': 0.2310546875, 'train/seg_cls_loss': 0.017254638671875, 'train/kl_loss': 0.3720703125, 'train/mask_bce_loss': 0.09441573070362211, 'train/mask_dice_loss': 0.6442188590764999, 'train/mask_loss': 0.7386345833539962, 'metrics/total_secs_per_batch': 9.865614175796509, 'metrics/data_secs_per_batch': 4.317986154556275, '_timestamp': 1740963101.9751089}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 366 is less than current step: 499. Dropping entry: {'train/lr': 7.781632653061224e-05, '_timestamp': 1740963101.975381}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 367 is less than current step: 499. Dropping entry: {'train/loss': 1.3383602261543275, 'train/ce_loss': 0.61396484375, 'train/seg_cls_loss': 0.00826416015625, 'train/kl_loss': 0.185546875, 'train/mask_bce_loss': 0.08005786128342152, 'train/mask_dice_loss': 0.27076287269592286, 'train/mask_loss': 0.35082073509693146, 'metrics/total_secs_per_batch': 6.7185869216918945, 'metrics/data_secs_per_batch': 2.8792293071746826, '_timestamp': 1740963108.6936896}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 367 is less than current step: 499. Dropping entry: {'train/lr': 7.76938775510204e-05, '_timestamp': 1740963108.6939993}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 368 is less than current step: 499. Dropping entry: {'train/loss': 1.2592967867851257, 'train/ce_loss': 0.6513671875, 'train/seg_cls_loss': 0.00733642578125, 'train/kl_loss': 0.14765625, 'train/mask_bce_loss': 0.060829808562994005, 'train/mask_dice_loss': 0.23400413393974304, 'train/mask_loss': 0.2948339402675629, 'metrics/total_secs_per_batch': 5.922468900680542, 'metrics/data_secs_per_batch': 2.3593788623809813, '_timestamp': 1740963114.6161695}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 368 is less than current step: 499. Dropping entry: {'train/lr': 7.757142857142856e-05, '_timestamp': 1740963114.6164343}).
[2025-03-02 18:52:03,838] [INFO] [logging.py:96:log_dist] [Rank 0] step=1870, skipped=0, lr=[7.751020408163264e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:52:03,844] [INFO] [timer.py:215:stop] epoch=0/micro_step=18700/global_step=1870, RunningAvgSamplesPerSec=1.5126054348697209, CurrSamplesPerSec=1.0836686309169954, MemAllocated=31.25GB, MaxMemAllocated=37.19GB
Epoch: [3][370/500]	Time  9.229 ( 9.229)	Loss 2.7207 (1.7899)	CeLoss 0.2637 (0.3118)	SegCLSLoss 0.0205 (0.0157)	KLLoss 0.3809 (0.2949)	MaskLoss 1.2041 (0.7202)	MaskBCELoss 0.3210 (0.1294)	MaskDICELoss 0.8831 (0.5908)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 369 is less than current step: 499. Dropping entry: {'train/loss': 1.7898857355117799, 'train/ce_loss': 0.31181640625, 'train/seg_cls_loss': 0.015692138671875, 'train/kl_loss': 0.294921875, 'train/mask_bce_loss': 0.1293885787948966, 'train/mask_dice_loss': 0.5908472418785096, 'train/mask_loss': 0.7202358305454254, 'metrics/total_secs_per_batch': 9.22943377494812, 'metrics/data_secs_per_batch': 3.783404326438904, '_timestamp': 1740963123.845424}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 369 is less than current step: 499. Dropping entry: {'train/lr': 7.744897959183672e-05, '_timestamp': 1740963123.8456867}).
Epoch: [3][371/500]	Time  8.142 ( 8.142)	Loss 2.5712 (1.8316)	CeLoss 0.1729 (0.3559)	SegCLSLoss 0.0201 (0.0138)	KLLoss 0.3613 (0.2988)	MaskLoss 1.1762 (0.7194)	MaskBCELoss 0.4610 (0.1177)	MaskDICELoss 0.7152 (0.6017)
Epoch: [3][372/500]	Time  6.729 ( 6.729)	Loss 2.1550 (1.5980)	CeLoss 0.1699 (0.3934)	SegCLSLoss 0.0269 (0.0136)	KLLoss 0.3516 (0.2574)	MaskLoss 0.9681 (0.5861)	MaskBCELoss 0.0563 (0.0741)	MaskDICELoss 0.9118 (0.5120)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 370 is less than current step: 499. Dropping entry: {'train/loss': 1.8316137671470643, 'train/ce_loss': 0.355859375, 'train/seg_cls_loss': 0.0137939453125, 'train/kl_loss': 0.298828125, 'train/mask_bce_loss': 0.11771415341645479, 'train/mask_dice_loss': 0.6017060041427612, 'train/mask_loss': 0.7194201648235321, 'metrics/total_secs_per_batch': 8.14160680770874, 'metrics/data_secs_per_batch': 2.981683874130249, '_timestamp': 1740963131.9872775}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 370 is less than current step: 499. Dropping entry: {'train/lr': 7.732653061224488e-05, '_timestamp': 1740963131.9875667}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 371 is less than current step: 499. Dropping entry: {'train/loss': 1.5979773163795472, 'train/ce_loss': 0.393359375, 'train/seg_cls_loss': 0.01361083984375, 'train/kl_loss': 0.257421875, 'train/mask_bce_loss': 0.07406886778771878, 'train/mask_dice_loss': 0.5120291739702225, 'train/mask_loss': 0.5860980451107025, 'metrics/total_secs_per_batch': 6.728759288787842, 'metrics/data_secs_per_batch': 3.066661500930786, '_timestamp': 1740963138.716087}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 371 is less than current step: 499. Dropping entry: {'train/lr': 7.720408163265304e-05, '_timestamp': 1740963138.716387}).
Epoch: [3][373/500]	Time  9.373 ( 9.373)	Loss 1.4656 (1.7176)	CeLoss 0.2090 (0.2262)	SegCLSLoss 0.0156 (0.0154)	KLLoss 0.3691 (0.3322)	MaskLoss 0.6059 (0.7254)	MaskBCELoss 0.0371 (0.1134)	MaskDICELoss 0.5688 (0.6120)
Epoch: [3][374/500]	Time  8.145 ( 8.145)	Loss 0.6797 (1.4824)	CeLoss 0.6797 (0.3470)	SegCLSLoss 0.0000 (0.0105)	KLLoss 0.0000 (0.2574)	MaskLoss 0.0000 (0.5522)	MaskBCELoss 0.0000 (0.0604)	MaskDICELoss 0.0000 (0.4919)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 372 is less than current step: 499. Dropping entry: {'train/loss': 1.7176306009292603, 'train/ce_loss': 0.226220703125, 'train/seg_cls_loss': 0.01539306640625, 'train/kl_loss': 0.3322265625, 'train/mask_bce_loss': 0.11344474852085114, 'train/mask_dice_loss': 0.6119965255260468, 'train/mask_loss': 0.7254412740468978, 'metrics/total_secs_per_batch': 9.373071670532227, 'metrics/data_secs_per_batch': 4.160937738418579, '_timestamp': 1740963148.0890872}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 372 is less than current step: 499. Dropping entry: {'train/lr': 7.708163265306122e-05, '_timestamp': 1740963148.089368}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 373 is less than current step: 499. Dropping entry: {'train/loss': 1.482377028465271, 'train/ce_loss': 0.34697265625, 'train/seg_cls_loss': 0.010528564453125, 'train/kl_loss': 0.257421875, 'train/mask_bce_loss': 0.06035606022924185, 'train/mask_dice_loss': 0.4918676167726517, 'train/mask_loss': 0.5522236675024033, 'metrics/total_secs_per_batch': 8.145291090011597, 'metrics/data_secs_per_batch': 3.846718740463257, '_timestamp': 1740963156.2343564}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 373 is less than current step: 499. Dropping entry: {'train/lr': 7.695918367346938e-05, '_timestamp': 1740963156.2346227}).
Epoch: [3][375/500]	Time  6.678 ( 6.678)	Loss 1.0312 (1.8417)	CeLoss 1.0312 (0.7967)	SegCLSLoss 0.0000 (0.0114)	KLLoss 0.0000 (0.1842)	MaskLoss 0.0000 (0.5104)	MaskBCELoss 0.0000 (0.0915)	MaskDICELoss 0.0000 (0.4189)
Epoch: [3][376/500]	Time  8.076 ( 8.076)	Loss 1.6811 (1.6494)	CeLoss 0.3184 (0.4274)	SegCLSLoss 0.0126 (0.0124)	KLLoss 0.3730 (0.2982)	MaskLoss 0.6599 (0.5929)	MaskBCELoss 0.1050 (0.1524)	MaskDICELoss 0.5549 (0.4405)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 374 is less than current step: 499. Dropping entry: {'train/loss': 1.8417256355285645, 'train/ce_loss': 0.7966796875, 'train/seg_cls_loss': 0.0114013671875, 'train/kl_loss': 0.1841796875, 'train/mask_bce_loss': 0.09147995114326476, 'train/mask_dice_loss': 0.4189336359500885, 'train/mask_loss': 0.5104135811328888, 'metrics/total_secs_per_batch': 6.678198337554932, 'metrics/data_secs_per_batch': 2.971432995796204, '_timestamp': 1740963162.912564}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 374 is less than current step: 499. Dropping entry: {'train/lr': 7.683673469387755e-05, '_timestamp': 1740963162.9128304}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 375 is less than current step: 499. Dropping entry: {'train/loss': 1.649359792470932, 'train/ce_loss': 0.42744140625, 'train/seg_cls_loss': 0.012371826171875, 'train/kl_loss': 0.2982421875, 'train/mask_bce_loss': 0.15235064141452312, 'train/mask_dice_loss': 0.4405421495437622, 'train/mask_loss': 0.592892786860466, 'metrics/total_secs_per_batch': 8.076354503631592, 'metrics/data_secs_per_batch': 3.516339826583862, '_timestamp': 1740963170.9889224}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 375 is less than current step: 499. Dropping entry: {'train/lr': 7.671428571428571e-05, '_timestamp': 1740963170.989198}).
Epoch: [3][377/500]	Time  7.838 ( 7.838)	Loss 2.0693 (1.2266)	CeLoss 0.2100 (0.3399)	SegCLSLoss 0.0315 (0.0115)	KLLoss 0.3652 (0.2207)	MaskLoss 0.9038 (0.4295)	MaskBCELoss 0.0682 (0.0893)	MaskDICELoss 0.8356 (0.3402)
Epoch: [3][378/500]	Time  7.630 ( 7.630)	Loss 4.1495 (1.9480)	CeLoss 0.2227 (0.4604)	SegCLSLoss 0.0179 (0.0169)	KLLoss 0.3750 (0.2947)	MaskLoss 1.9400 (0.7248)	MaskBCELoss 1.1634 (0.2029)	MaskDICELoss 0.7765 (0.5219)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 376 is less than current step: 499. Dropping entry: {'train/loss': 1.2266381621360778, 'train/ce_loss': 0.339892578125, 'train/seg_cls_loss': 0.0114990234375, 'train/kl_loss': 0.220703125, 'train/mask_bce_loss': 0.08929297849535942, 'train/mask_dice_loss': 0.3401637881994247, 'train/mask_loss': 0.42945677042007446, 'metrics/total_secs_per_batch': 7.837641716003418, 'metrics/data_secs_per_batch': 3.6250826120376587, '_timestamp': 1740963178.8265452}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 376 is less than current step: 499. Dropping entry: {'train/lr': 7.659183673469387e-05, '_timestamp': 1740963178.826849}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 377 is less than current step: 499. Dropping entry: {'train/loss': 1.947963571548462, 'train/ce_loss': 0.4603515625, 'train/seg_cls_loss': 0.016888427734375, 'train/kl_loss': 0.2947265625, 'train/mask_bce_loss': 0.202904006652534, 'train/mask_dice_loss': 0.5218590319156646, 'train/mask_loss': 0.7247630476951599, 'metrics/total_secs_per_batch': 7.630386590957642, 'metrics/data_secs_per_batch': 3.036615014076233, '_timestamp': 1740963186.4569387}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 377 is less than current step: 499. Dropping entry: {'train/lr': 7.646938775510204e-05, '_timestamp': 1740963186.4571998}).
Epoch: [3][379/500]	Time  7.516 ( 7.516)	Loss 1.7342 (1.8410)	CeLoss 0.2637 (0.3629)	SegCLSLoss 0.0132 (0.0147)	KLLoss 0.3789 (0.2957)	MaskLoss 0.7138 (0.7206)	MaskBCELoss 0.1770 (0.1800)	MaskDICELoss 0.5367 (0.5406)
[2025-03-02 18:53:21,433] [INFO] [logging.py:96:log_dist] [Rank 0] step=1880, skipped=0, lr=[7.628571428571428e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:53:21,438] [INFO] [timer.py:215:stop] epoch=0/micro_step=18800/global_step=1880, RunningAvgSamplesPerSec=1.5112091920744832, CurrSamplesPerSec=1.339521384161196, MemAllocated=30.78GB, MaxMemAllocated=37.19GB
Epoch: [3][380/500]	Time  7.467 ( 7.467)	Loss 1.0234 (1.5286)	CeLoss 1.0234 (0.5150)	SegCLSLoss 0.0000 (0.0117)	KLLoss 0.0000 (0.2607)	MaskLoss 0.0000 (0.4910)	MaskBCELoss 0.0000 (0.1130)	MaskDICELoss 0.0000 (0.3780)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 378 is less than current step: 499. Dropping entry: {'train/loss': 1.8409605622291565, 'train/ce_loss': 0.362939453125, 'train/seg_cls_loss': 0.014715576171875, 'train/kl_loss': 0.295703125, 'train/mask_bce_loss': 0.17999700652435421, 'train/mask_dice_loss': 0.5405565083026886, 'train/mask_loss': 0.7205535233020782, 'metrics/total_secs_per_batch': 7.51560115814209, 'metrics/data_secs_per_batch': 3.314993166923523, '_timestamp': 1740963193.9725459}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 378 is less than current step: 499. Dropping entry: {'train/lr': 7.63469387755102e-05, '_timestamp': 1740963193.9728565}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 379 is less than current step: 499. Dropping entry: {'train/loss': 1.528619760274887, 'train/ce_loss': 0.5150390625, 'train/seg_cls_loss': 0.011669921875, 'train/kl_loss': 0.2607421875, 'train/mask_bce_loss': 0.11300541125237942, 'train/mask_dice_loss': 0.3779646188020706, 'train/mask_loss': 0.4909700363874435, 'metrics/total_secs_per_batch': 7.466915845870972, 'metrics/data_secs_per_batch': 3.23450448513031, '_timestamp': 1740963201.4393697}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 379 is less than current step: 499. Dropping entry: {'train/lr': 7.622448979591836e-05, '_timestamp': 1740963201.439696}).
Epoch: [3][381/500]	Time  7.568 ( 7.568)	Loss 1.8744 (1.0423)	CeLoss 0.2139 (0.3348)	SegCLSLoss 0.0227 (0.0081)	KLLoss 0.3613 (0.1840)	MaskLoss 0.8063 (0.3426)	MaskBCELoss 0.1430 (0.1017)	MaskDICELoss 0.6633 (0.2409)
Epoch: [3][382/500]	Time  9.885 ( 9.885)	Loss 1.2857 (1.6530)	CeLoss 0.2119 (0.2446)	SegCLSLoss 0.0181 (0.0167)	KLLoss 0.3711 (0.3689)	MaskLoss 0.5139 (0.6816)	MaskBCELoss 0.0462 (0.1061)	MaskDICELoss 0.4678 (0.5755)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 380 is less than current step: 499. Dropping entry: {'train/loss': 1.042326068878174, 'train/ce_loss': 0.334765625, 'train/seg_cls_loss': 0.008099365234375, 'train/kl_loss': 0.183984375, 'train/mask_bce_loss': 0.10172538440674543, 'train/mask_dice_loss': 0.24087319821119307, 'train/mask_loss': 0.34259858205914495, 'metrics/total_secs_per_batch': 7.568187236785889, 'metrics/data_secs_per_batch': 3.792056369781494, '_timestamp': 1740963209.0077152}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 380 is less than current step: 499. Dropping entry: {'train/lr': 7.610204081632653e-05, '_timestamp': 1740963209.0079887}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 381 is less than current step: 499. Dropping entry: {'train/loss': 1.653015911579132, 'train/ce_loss': 0.24462890625, 'train/seg_cls_loss': 0.016705322265625, 'train/kl_loss': 0.3689453125, 'train/mask_bce_loss': 0.10612252876162528, 'train/mask_dice_loss': 0.5754635542631149, 'train/mask_loss': 0.6815860748291016, 'metrics/total_secs_per_batch': 9.885474920272827, 'metrics/data_secs_per_batch': 4.63242769241333, '_timestamp': 1740963218.8931408}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 381 is less than current step: 499. Dropping entry: {'train/lr': 7.597959183673469e-05, '_timestamp': 1740963218.8934042}).
Epoch: [3][383/500]	Time  7.509 ( 7.509)	Loss 2.1120 (1.9480)	CeLoss 0.6523 (0.5446)	SegCLSLoss 0.0097 (0.0127)	KLLoss 0.3867 (0.2582)	MaskLoss 0.7083 (0.6856)	MaskBCELoss 0.1210 (0.1855)	MaskDICELoss 0.5873 (0.5000)
Epoch: [3][384/500]	Time  7.469 ( 7.469)	Loss 1.2621 (1.7604)	CeLoss 0.1914 (0.3363)	SegCLSLoss 0.0184 (0.0167)	KLLoss 0.3672 (0.2934)	MaskLoss 0.5129 (0.6932)	MaskBCELoss 0.0517 (0.1330)	MaskDICELoss 0.4611 (0.5602)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 382 is less than current step: 499. Dropping entry: {'train/loss': 1.947990345954895, 'train/ce_loss': 0.54462890625, 'train/seg_cls_loss': 0.012738037109375, 'train/kl_loss': 0.258203125, 'train/mask_bce_loss': 0.18552466556429864, 'train/mask_dice_loss': 0.5000427603721619, 'train/mask_loss': 0.685567432641983, 'metrics/total_secs_per_batch': 7.509018659591675, 'metrics/data_secs_per_batch': 2.8519702196121215, '_timestamp': 1740963226.4021525}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 382 is less than current step: 499. Dropping entry: {'train/lr': 7.585714285714284e-05, '_timestamp': 1740963226.4024148}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 383 is less than current step: 499. Dropping entry: {'train/loss': 1.760376524925232, 'train/ce_loss': 0.336328125, 'train/seg_cls_loss': 0.016650390625, 'train/kl_loss': 0.293359375, 'train/mask_bce_loss': 0.13297684169374407, 'train/mask_dice_loss': 0.5602485477924347, 'train/mask_loss': 0.6932253777980805, 'metrics/total_secs_per_batch': 7.469132661819458, 'metrics/data_secs_per_batch': 3.321070599555969, '_timestamp': 1740963233.8712907}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 383 is less than current step: 499. Dropping entry: {'train/lr': 7.5734693877551e-05, '_timestamp': 1740963233.8715541}).
Epoch: [3][385/500]	Time  9.225 ( 9.225)	Loss 1.9581 (1.5796)	CeLoss 0.2021 (0.3621)	SegCLSLoss 0.0114 (0.0109)	KLLoss 0.3672 (0.2951)	MaskLoss 0.8570 (0.5912)	MaskBCELoss 0.1711 (0.1630)	MaskDICELoss 0.6859 (0.4282)
Epoch: [3][386/500]	Time  7.716 ( 7.716)	Loss 1.1562 (1.5646)	CeLoss 1.1562 (0.4014)	SegCLSLoss 0.0000 (0.0125)	KLLoss 0.0000 (0.2178)	MaskLoss 0.0000 (0.5674)	MaskBCELoss 0.0000 (0.0764)	MaskDICELoss 0.0000 (0.4910)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 384 is less than current step: 499. Dropping entry: {'train/loss': 1.5795756578445435, 'train/ce_loss': 0.362109375, 'train/seg_cls_loss': 0.01085205078125, 'train/kl_loss': 0.2951171875, 'train/mask_bce_loss': 0.16302938908338546, 'train/mask_dice_loss': 0.42817442789673804, 'train/mask_loss': 0.5912038236856461, 'metrics/total_secs_per_batch': 9.225471496582031, 'metrics/data_secs_per_batch': 3.781323289871216, '_timestamp': 1740963243.0969903}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 384 is less than current step: 499. Dropping entry: {'train/lr': 7.561224489795918e-05, '_timestamp': 1740963243.097259}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 385 is less than current step: 499. Dropping entry: {'train/loss': 1.5645543575286864, 'train/ce_loss': 0.4014404296875, 'train/seg_cls_loss': 0.012530517578125, 'train/kl_loss': 0.2177734375, 'train/mask_bce_loss': 0.07643127581104636, 'train/mask_dice_loss': 0.4909899473190308, 'train/mask_loss': 0.5674212217330933, 'metrics/total_secs_per_batch': 7.715587615966797, 'metrics/data_secs_per_batch': 3.5454864501953125, '_timestamp': 1740963250.812387}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 385 is less than current step: 499. Dropping entry: {'train/lr': 7.548979591836734e-05, '_timestamp': 1740963250.8126647}).
Epoch: [3][387/500]	Time  8.241 ( 8.241)	Loss 1.4607 (1.7824)	CeLoss 0.2354 (0.3524)	SegCLSLoss 0.0139 (0.0153)	KLLoss 0.3691 (0.3303)	MaskLoss 0.5907 (0.6947)	MaskBCELoss 0.0404 (0.1270)	MaskDICELoss 0.5503 (0.5677)
Epoch: [3][388/500]	Time  7.826 ( 7.826)	Loss 0.9180 (1.5188)	CeLoss 0.9180 (0.3465)	SegCLSLoss 0.0000 (0.0113)	KLLoss 0.0000 (0.2211)	MaskLoss 0.0000 (0.5723)	MaskBCELoss 0.0000 (0.1775)	MaskDICELoss 0.0000 (0.3948)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 386 is less than current step: 499. Dropping entry: {'train/loss': 1.7824165582656861, 'train/ce_loss': 0.35244140625, 'train/seg_cls_loss': 0.015277099609375, 'train/kl_loss': 0.3302734375, 'train/mask_bce_loss': 0.12698617968708276, 'train/mask_dice_loss': 0.5676888883113861, 'train/mask_loss': 0.6946750760078431, 'metrics/total_secs_per_batch': 8.240949153900146, 'metrics/data_secs_per_batch': 3.3206934690475465, '_timestamp': 1740963259.0533054}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 386 is less than current step: 499. Dropping entry: {'train/lr': 7.53673469387755e-05, '_timestamp': 1740963259.0535023}).
Epoch: [3][389/500]	Time  7.867 ( 7.867)	Loss 1.9761 (1.5073)	CeLoss 0.1758 (0.3150)	SegCLSLoss 0.0283 (0.0169)	KLLoss 0.3633 (0.2570)	MaskLoss 0.8753 (0.5791)	MaskBCELoss 0.0408 (0.0925)	MaskDICELoss 0.8344 (0.4866)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 387 is less than current step: 499. Dropping entry: {'train/loss': 1.5188385844230652, 'train/ce_loss': 0.346533203125, 'train/seg_cls_loss': 0.011285400390625, 'train/kl_loss': 0.22109375, 'train/mask_bce_loss': 0.17750595388934015, 'train/mask_dice_loss': 0.39482838213443755, 'train/mask_loss': 0.5723343312740325, 'metrics/total_secs_per_batch': 7.826483964920044, 'metrics/data_secs_per_batch': 3.49155433177948, '_timestamp': 1740963266.8798246}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 387 is less than current step: 499. Dropping entry: {'train/lr': 7.524489795918367e-05, '_timestamp': 1740963266.8801496}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 388 is less than current step: 499. Dropping entry: {'train/loss': 1.5072964191436768, 'train/ce_loss': 0.3150390625, 'train/seg_cls_loss': 0.01688232421875, 'train/kl_loss': 0.25703125, 'train/mask_bce_loss': 0.09251323454082012, 'train/mask_dice_loss': 0.48657441437244414, 'train/mask_loss': 0.5790876567363739, 'metrics/total_secs_per_batch': 7.867113828659058, 'metrics/data_secs_per_batch': 3.51352481842041, '_timestamp': 1740963274.7469199}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 388 is less than current step: 499. Dropping entry: {'train/lr': 7.512244897959183e-05, '_timestamp': 1740963274.7471938}).
[2025-03-02 18:54:42,087] [INFO] [logging.py:96:log_dist] [Rank 0] step=1890, skipped=0, lr=[7.506122448979591e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:54:42,092] [INFO] [timer.py:215:stop] epoch=0/micro_step=18900/global_step=1890, RunningAvgSamplesPerSec=1.5094609367548, CurrSamplesPerSec=1.3614820498486395, MemAllocated=31.64GB, MaxMemAllocated=37.19GB
Epoch: [3][390/500]	Time  7.346 ( 7.346)	Loss 0.1084 (1.3033)	CeLoss 0.1084 (0.4388)	SegCLSLoss 0.0000 (0.0119)	KLLoss 0.0000 (0.2223)	MaskLoss 0.0000 (0.4181)	MaskBCELoss 0.0000 (0.0847)	MaskDICELoss 0.0000 (0.3334)
Epoch: [3][391/500]	Time  8.452 ( 8.452)	Loss 2.5898 (1.5035)	CeLoss 0.2773 (0.4837)	SegCLSLoss 0.0114 (0.0121)	KLLoss 0.3711 (0.2234)	MaskLoss 1.1347 (0.4958)	MaskBCELoss 0.3567 (0.1446)	MaskDICELoss 0.7780 (0.3512)
Epoch: [3][392/500]	Time  5.822 ( 5.822)	Loss 1.2934 (1.4572)	CeLoss 0.3320 (0.6108)	SegCLSLoss 0.0096 (0.0086)	KLLoss 0.3730 (0.2215)	MaskLoss 0.4602 (0.4099)	MaskBCELoss 0.0240 (0.0655)	MaskDICELoss 0.4361 (0.3445)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 389 is less than current step: 499. Dropping entry: {'train/loss': 1.3033480882644652, 'train/ce_loss': 0.438818359375, 'train/seg_cls_loss': 0.011932373046875, 'train/kl_loss': 0.222265625, 'train/mask_bce_loss': 0.08473377898335457, 'train/mask_dice_loss': 0.3333709254860878, 'train/mask_loss': 0.4181046962738037, 'metrics/total_secs_per_batch': 7.346431732177734, 'metrics/data_secs_per_batch': 2.891677641868591, '_timestamp': 1740963282.0931687}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 389 is less than current step: 499. Dropping entry: {'train/lr': 7.5e-05, '_timestamp': 1740963282.0934212}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 390 is less than current step: 499. Dropping entry: {'train/loss': 1.503473737835884, 'train/ce_loss': 0.483740234375, 'train/seg_cls_loss': 0.012103271484375, 'train/kl_loss': 0.2234375, 'train/mask_bce_loss': 0.14456367641687393, 'train/mask_dice_loss': 0.3511917442083359, 'train/mask_loss': 0.49575542360544206, 'metrics/total_secs_per_batch': 8.45204472541809, 'metrics/data_secs_per_batch': 3.764233660697937, '_timestamp': 1740963290.5457807}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 390 is less than current step: 499. Dropping entry: {'train/lr': 7.487755102040816e-05, '_timestamp': 1740963290.5462945}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 391 is less than current step: 499. Dropping entry: {'train/loss': 1.4572009801864625, 'train/ce_loss': 0.61083984375, 'train/seg_cls_loss': 0.00859375, 'train/kl_loss': 0.221484375, 'train/mask_bce_loss': 0.0654718448407948, 'train/mask_dice_loss': 0.3444762870669365, 'train/mask_loss': 0.40994813144207, 'metrics/total_secs_per_batch': 5.822152853012085, 'metrics/data_secs_per_batch': 3.002956986427307, '_timestamp': 1740963296.3676796}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 391 is less than current step: 499. Dropping entry: {'train/lr': 7.475510204081632e-05, '_timestamp': 1740963296.3680253}).
Epoch: [3][393/500]	Time  8.102 ( 8.102)	Loss 1.1797 (1.6614)	CeLoss 1.1797 (0.4011)	SegCLSLoss 0.0000 (0.0145)	KLLoss 0.0000 (0.2906)	MaskLoss 0.0000 (0.6119)	MaskBCELoss 0.0000 (0.0800)	MaskDICELoss 0.0000 (0.5319)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 392 is less than current step: 499. Dropping entry: {'train/loss': 1.6613936007022858, 'train/ce_loss': 0.40107421875, 'train/seg_cls_loss': 0.014459228515625, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.08000506926327944, 'train/mask_dice_loss': 0.5318928956985474, 'train/mask_loss': 0.6118979603052139, 'metrics/total_secs_per_batch': 8.10191035270691, 'metrics/data_secs_per_batch': 3.8445778369903563, '_timestamp': 1740963304.4694667}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 392 is less than current step: 499. Dropping entry: {'train/lr': 7.463265306122448e-05, '_timestamp': 1740963304.469732}).
Epoch: [3][394/500]	Time  7.838 ( 7.838)	Loss 2.8426 (1.7927)	CeLoss 0.2637 (0.4665)	SegCLSLoss 0.0209 (0.0127)	KLLoss 0.3633 (0.2951)	MaskLoss 1.2660 (0.6451)	MaskBCELoss 0.5685 (0.1765)	MaskDICELoss 0.6976 (0.4685)
Epoch: [3][395/500]	Time  7.730 ( 7.730)	Loss 1.3516 (1.7662)	CeLoss 1.3516 (0.5769)	SegCLSLoss 0.0000 (0.0111)	KLLoss 0.0000 (0.2580)	MaskLoss 0.0000 (0.5789)	MaskBCELoss 0.0000 (0.1197)	MaskDICELoss 0.0000 (0.4593)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 393 is less than current step: 499. Dropping entry: {'train/loss': 1.792687338590622, 'train/ce_loss': 0.46650390625, 'train/seg_cls_loss': 0.012701416015625, 'train/kl_loss': 0.2951171875, 'train/mask_bce_loss': 0.17654854683205484, 'train/mask_dice_loss': 0.46852558851242065, 'train/mask_loss': 0.6450741440057755, 'metrics/total_secs_per_batch': 7.8379225730896, 'metrics/data_secs_per_batch': 3.7003458023071287, '_timestamp': 1740963312.3073916}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 393 is less than current step: 499. Dropping entry: {'train/lr': 7.451020408163265e-05, '_timestamp': 1740963312.307665}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 394 is less than current step: 499. Dropping entry: {'train/loss': 1.7661559343338014, 'train/ce_loss': 0.57685546875, 'train/seg_cls_loss': 0.011126708984375, 'train/kl_loss': 0.2580078125, 'train/mask_bce_loss': 0.11966395825147629, 'train/mask_dice_loss': 0.45926362574100493, 'train/mask_loss': 0.5789275735616684, 'metrics/total_secs_per_batch': 7.7295448780059814, 'metrics/data_secs_per_batch': 3.5983722686767576, '_timestamp': 1740963320.0369377}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 394 is less than current step: 499. Dropping entry: {'train/lr': 7.438775510204081e-05, '_timestamp': 1740963320.0372005}).
Epoch: [3][396/500]	Time  8.189 ( 8.189)	Loss 1.2461 (1.8454)	CeLoss 0.2158 (0.4765)	SegCLSLoss 0.0103 (0.0147)	KLLoss 0.3789 (0.2971)	MaskLoss 0.4941 (0.6659)	MaskBCELoss 0.1328 (0.0926)	MaskDICELoss 0.3613 (0.5733)
Epoch: [3][397/500]	Time  8.193 ( 8.193)	Loss 1.7072 (1.5554)	CeLoss 0.1895 (0.4403)	SegCLSLoss 0.0209 (0.0113)	KLLoss 0.3555 (0.2207)	MaskLoss 0.7354 (0.5437)	MaskBCELoss 0.0281 (0.0759)	MaskDICELoss 0.7073 (0.4678)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 395 is less than current step: 499. Dropping entry: {'train/loss': 1.8453518152236938, 'train/ce_loss': 0.47646484375, 'train/seg_cls_loss': 0.0147216796875, 'train/kl_loss': 0.2970703125, 'train/mask_bce_loss': 0.09256215961650013, 'train/mask_dice_loss': 0.5733266294002533, 'train/mask_loss': 0.6658887922763824, 'metrics/total_secs_per_batch': 8.189304113388062, 'metrics/data_secs_per_batch': 3.3540724992752073, '_timestamp': 1740963328.2262414}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 395 is less than current step: 499. Dropping entry: {'train/lr': 7.426530612244897e-05, '_timestamp': 1740963328.2265196}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 396 is less than current step: 499. Dropping entry: {'train/loss': 1.5554041028022767, 'train/ce_loss': 0.440283203125, 'train/seg_cls_loss': 0.011346435546875, 'train/kl_loss': 0.220703125, 'train/mask_bce_loss': 0.07590844295918941, 'train/mask_dice_loss': 0.46778481900691987, 'train/mask_loss': 0.5436932742595673, 'metrics/total_secs_per_batch': 8.192700147628784, 'metrics/data_secs_per_batch': 3.385006332397461, '_timestamp': 1740963336.4189858}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 396 is less than current step: 499. Dropping entry: {'train/lr': 7.414285714285713e-05, '_timestamp': 1740963336.4192677}).
Epoch: [3][398/500]	Time  6.551 ( 6.551)	Loss 1.3578 (1.4545)	CeLoss 0.2891 (0.7846)	SegCLSLoss 0.0110 (0.0056)	KLLoss 0.3672 (0.1848)	MaskLoss 0.5129 (0.3243)	MaskBCELoss 0.1265 (0.0664)	MaskDICELoss 0.3864 (0.2579)
Epoch: [3][399/500]	Time  7.792 ( 7.792)	Loss 1.9397 (1.8771)	CeLoss 0.2432 (0.4279)	SegCLSLoss 0.0152 (0.0122)	KLLoss 0.3652 (0.2947)	MaskLoss 0.8263 (0.7067)	MaskBCELoss 0.0454 (0.1750)	MaskDICELoss 0.7809 (0.5317)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 397 is less than current step: 499. Dropping entry: {'train/loss': 1.4544987201690673, 'train/ce_loss': 0.7845703125, 'train/seg_cls_loss': 0.005584716796875, 'train/kl_loss': 0.184765625, 'train/mask_bce_loss': 0.06643710415810347, 'train/mask_dice_loss': 0.2578825712203979, 'train/mask_loss': 0.3243196725845337, 'metrics/total_secs_per_batch': 6.551259756088257, 'metrics/data_secs_per_batch': 2.8096448183059692, '_timestamp': 1740963342.9703038}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 397 is less than current step: 499. Dropping entry: {'train/lr': 7.40204081632653e-05, '_timestamp': 1740963342.9706242}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 398 is less than current step: 499. Dropping entry: {'train/loss': 1.877065360546112, 'train/ce_loss': 0.4279296875, 'train/seg_cls_loss': 0.012176513671875, 'train/kl_loss': 0.2947265625, 'train/mask_bce_loss': 0.17502344343811274, 'train/mask_dice_loss': 0.5317221194505691, 'train/mask_loss': 0.706745570898056, 'metrics/total_secs_per_batch': 7.791615724563599, 'metrics/data_secs_per_batch': 3.5677552461624145, '_timestamp': 1740963350.7618237}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 398 is less than current step: 499. Dropping entry: {'train/lr': 7.389795918367346e-05, '_timestamp': 1740963350.7620862}).
[2025-03-02 18:55:59,974] [INFO] [logging.py:96:log_dist] [Rank 0] step=1900, skipped=0, lr=[7.383673469387754e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:55:59,979] [INFO] [timer.py:215:stop] epoch=0/micro_step=19000/global_step=1900, RunningAvgSamplesPerSec=1.5081104734016746, CurrSamplesPerSec=1.084933926001281, MemAllocated=31.49GB, MaxMemAllocated=37.19GB
Epoch: [3][400/500]	Time  9.219 ( 9.219)	Loss 0.0664 (1.5360)	CeLoss 0.0664 (0.3233)	SegCLSLoss 0.0000 (0.0133)	KLLoss 0.0000 (0.2631)	MaskLoss 0.0000 (0.5899)	MaskBCELoss 0.0000 (0.1160)	MaskDICELoss 0.0000 (0.4739)
Epoch: [3][401/500]	Time  8.873 ( 8.873)	Loss 1.2141 (1.8118)	CeLoss 0.2793 (0.3342)	SegCLSLoss 0.0145 (0.0150)	KLLoss 0.3809 (0.3342)	MaskLoss 0.4449 (0.7184)	MaskBCELoss 0.0828 (0.1264)	MaskDICELoss 0.3621 (0.5921)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 399 is less than current step: 499. Dropping entry: {'train/loss': 1.5360022187232971, 'train/ce_loss': 0.32333984375, 'train/seg_cls_loss': 0.013336181640625, 'train/kl_loss': 0.2630859375, 'train/mask_bce_loss': 0.11604332886636257, 'train/mask_dice_loss': 0.47388162314891813, 'train/mask_loss': 0.5899249404668808, 'metrics/total_secs_per_batch': 9.21862506866455, 'metrics/data_secs_per_batch': 4.17246630191803, '_timestamp': 1740963359.9802637}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 399 is less than current step: 499. Dropping entry: {'train/lr': 7.377551020408162e-05, '_timestamp': 1740963359.9804432}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 400 is less than current step: 499. Dropping entry: {'train/loss': 1.8117969751358032, 'train/ce_loss': 0.3341796875, 'train/seg_cls_loss': 0.01500244140625, 'train/kl_loss': 0.3341796875, 'train/mask_bce_loss': 0.12636647801846265, 'train/mask_dice_loss': 0.5920808285474777, 'train/mask_loss': 0.7184473127126694, 'metrics/total_secs_per_batch': 8.873490333557129, 'metrics/data_secs_per_batch': 3.6637973546981812, '_timestamp': 1740963368.853957}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 400 is less than current step: 499. Dropping entry: {'train/lr': 7.365306122448979e-05, '_timestamp': 1740963368.8542223}).
Epoch: [3][402/500]	Time  8.044 ( 8.044)	Loss 1.8693 (1.6304)	CeLoss 0.2598 (0.3235)	SegCLSLoss 0.0116 (0.0161)	KLLoss 0.3672 (0.3322)	MaskLoss 0.7833 (0.6329)	MaskBCELoss 0.3508 (0.1399)	MaskDICELoss 0.4325 (0.4929)
Epoch: [3][403/500]	Time  7.212 ( 7.212)	Loss 1.4297 (1.4975)	CeLoss 1.4297 (0.5538)	SegCLSLoss 0.0000 (0.0100)	KLLoss 0.0000 (0.2260)	MaskLoss 0.0000 (0.4581)	MaskBCELoss 0.0000 (0.0501)	MaskDICELoss 0.0000 (0.4080)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 401 is less than current step: 499. Dropping entry: {'train/loss': 1.6303796648979187, 'train/ce_loss': 0.32353515625, 'train/seg_cls_loss': 0.01605224609375, 'train/kl_loss': 0.3322265625, 'train/mask_bce_loss': 0.1399292904417962, 'train/mask_dice_loss': 0.4929363183677197, 'train/mask_loss': 0.6328656122088432, 'metrics/total_secs_per_batch': 8.044042348861694, 'metrics/data_secs_per_batch': 3.4972107887268065, '_timestamp': 1740963376.8980055}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 401 is less than current step: 499. Dropping entry: {'train/lr': 7.353061224489795e-05, '_timestamp': 1740963376.8982828}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 402 is less than current step: 499. Dropping entry: {'train/loss': 1.497537338733673, 'train/ce_loss': 0.55380859375, 'train/seg_cls_loss': 0.010040283203125, 'train/kl_loss': 0.2259765625, 'train/mask_bce_loss': 0.050103881489485504, 'train/mask_dice_loss': 0.40803979635238646, 'train/mask_loss': 0.45814367532730105, 'metrics/total_secs_per_batch': 7.211575269699097, 'metrics/data_secs_per_batch': 3.2125293970108033, '_timestamp': 1740963384.1097817}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 402 is less than current step: 499. Dropping entry: {'train/lr': 7.340816326530611e-05, '_timestamp': 1740963384.1101377}).
Epoch: [3][404/500]	Time  6.693 ( 6.693)	Loss 1.4922 (0.9193)	CeLoss 1.4922 (0.6149)	SegCLSLoss 0.0000 (0.0033)	KLLoss 0.0000 (0.1127)	MaskLoss 0.0000 (0.1458)	MaskBCELoss 0.0000 (0.0511)	MaskDICELoss 0.0000 (0.0946)
Epoch: [3][405/500]	Time  8.395 ( 8.395)	Loss 1.4306 (1.6844)	CeLoss 0.2275 (0.4688)	SegCLSLoss 0.0193 (0.0127)	KLLoss 0.3711 (0.2971)	MaskLoss 0.5786 (0.5898)	MaskBCELoss 0.0365 (0.0352)	MaskDICELoss 0.5421 (0.5546)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 403 is less than current step: 499. Dropping entry: {'train/loss': 0.919340443611145, 'train/ce_loss': 0.61494140625, 'train/seg_cls_loss': 0.0033203125, 'train/kl_loss': 0.1126953125, 'train/mask_bce_loss': 0.05110678821802139, 'train/mask_dice_loss': 0.09464742094278336, 'train/mask_loss': 0.14575421065092087, 'metrics/total_secs_per_batch': 6.693380832672119, 'metrics/data_secs_per_batch': 3.3254994630813597, '_timestamp': 1740963390.8029833}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 403 is less than current step: 499. Dropping entry: {'train/lr': 7.328571428571428e-05, '_timestamp': 1740963390.8032591}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 404 is less than current step: 499. Dropping entry: {'train/loss': 1.6844396710395813, 'train/ce_loss': 0.46884765625, 'train/seg_cls_loss': 0.012677001953125, 'train/kl_loss': 0.2970703125, 'train/mask_bce_loss': 0.03522398937493563, 'train/mask_dice_loss': 0.5545544445514679, 'train/mask_loss': 0.5897784292697906, 'metrics/total_secs_per_batch': 8.394904375076294, 'metrics/data_secs_per_batch': 3.4732544898986815, '_timestamp': 1740963399.1978817}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 404 is less than current step: 499. Dropping entry: {'train/lr': 7.316326530612244e-05, '_timestamp': 1740963399.198158}).
Epoch: [3][406/500]	Time  7.332 ( 7.332)	Loss 2.6172 (1.9312)	CeLoss 0.1992 (0.3318)	SegCLSLoss 0.0159 (0.0128)	KLLoss 0.3555 (0.2992)	MaskLoss 1.1875 (0.7817)	MaskBCELoss 0.3815 (0.1724)	MaskDICELoss 0.8060 (0.6093)
Epoch: [3][407/500]	Time  7.187 ( 7.187)	Loss 1.2976 (1.7154)	CeLoss 0.2471 (0.4184)	SegCLSLoss 0.0118 (0.0142)	KLLoss 0.3750 (0.2967)	MaskLoss 0.5033 (0.6302)	MaskBCELoss 0.1081 (0.1124)	MaskDICELoss 0.3952 (0.5178)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 405 is less than current step: 499. Dropping entry: {'train/loss': 1.931172287464142, 'train/ce_loss': 0.3318359375, 'train/seg_cls_loss': 0.012823486328125, 'train/kl_loss': 0.29921875, 'train/mask_bce_loss': 0.17238112296909094, 'train/mask_dice_loss': 0.6092694610357284, 'train/mask_loss': 0.7816505789756775, 'metrics/total_secs_per_batch': 7.332247257232666, 'metrics/data_secs_per_batch': 3.004089903831482, '_timestamp': 1740963406.530132}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 405 is less than current step: 499. Dropping entry: {'train/lr': 7.30408163265306e-05, '_timestamp': 1740963406.5304062}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 406 is less than current step: 499. Dropping entry: {'train/loss': 1.7154427886009216, 'train/ce_loss': 0.418359375, 'train/seg_cls_loss': 0.0142333984375, 'train/kl_loss': 0.2966796875, 'train/mask_bce_loss': 0.11240436602383852, 'train/mask_dice_loss': 0.5177779763936996, 'train/mask_loss': 0.6301823318004608, 'metrics/total_secs_per_batch': 7.186724424362183, 'metrics/data_secs_per_batch': 3.002432036399841, '_timestamp': 1740963413.716834}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 406 is less than current step: 499. Dropping entry: {'train/lr': 7.291836734693877e-05, '_timestamp': 1740963413.717098}).
Epoch: [3][408/500]	Time  7.720 ( 7.720)	Loss 2.4743 (1.3269)	CeLoss 0.1660 (0.5637)	SegCLSLoss 0.0222 (0.0077)	KLLoss 0.3652 (0.1867)	MaskLoss 1.1307 (0.3704)	MaskBCELoss 0.3612 (0.0935)	MaskDICELoss 0.7695 (0.2769)
Epoch: [3][409/500]	Time  9.580 ( 9.580)	Loss 2.5491 (1.7877)	CeLoss 0.1865 (0.3156)	SegCLSLoss 0.0159 (0.0120)	KLLoss 0.3633 (0.2957)	MaskLoss 1.1593 (0.7183)	MaskBCELoss 0.2637 (0.1959)	MaskDICELoss 0.8956 (0.5224)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 407 is less than current step: 499. Dropping entry: {'train/loss': 1.3269206047058106, 'train/ce_loss': 0.563671875, 'train/seg_cls_loss': 0.00765380859375, 'train/kl_loss': 0.18671875, 'train/mask_bce_loss': 0.09351582638919353, 'train/mask_dice_loss': 0.2768780678510666, 'train/mask_loss': 0.3703938990831375, 'metrics/total_secs_per_batch': 7.719897031784058, 'metrics/data_secs_per_batch': 3.4661066055297853, '_timestamp': 1740963421.436742}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 407 is less than current step: 499. Dropping entry: {'train/lr': 7.279591836734693e-05, '_timestamp': 1740963421.437047}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 408 is less than current step: 499. Dropping entry: {'train/loss': 1.787724232673645, 'train/ce_loss': 0.315576171875, 'train/seg_cls_loss': 0.0119873046875, 'train/kl_loss': 0.295703125, 'train/mask_bce_loss': 0.1958618935197592, 'train/mask_dice_loss': 0.5223898768424988, 'train/mask_loss': 0.7182517677545548, 'metrics/total_secs_per_batch': 9.580157041549683, 'metrics/data_secs_per_batch': 4.474655270576477, '_timestamp': 1740963431.016901}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 408 is less than current step: 499. Dropping entry: {'train/lr': 7.267346938775509e-05, '_timestamp': 1740963431.0171793}).
[2025-03-02 18:57:19,056] [INFO] [logging.py:96:log_dist] [Rank 0] step=1910, skipped=0, lr=[7.261224489795917e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:57:19,062] [INFO] [timer.py:215:stop] epoch=0/micro_step=19100/global_step=1910, RunningAvgSamplesPerSec=1.5065906654181296, CurrSamplesPerSec=1.2431066153996944, MemAllocated=30.77GB, MaxMemAllocated=37.19GB
Epoch: [3][410/500]	Time  8.046 ( 8.046)	Loss 0.9922 (1.8167)	CeLoss 0.9922 (0.3920)	SegCLSLoss 0.0000 (0.0148)	KLLoss 0.0000 (0.2984)	MaskLoss 0.0000 (0.6937)	MaskBCELoss 0.0000 (0.0985)	MaskDICELoss 0.0000 (0.5952)
Epoch: [3][411/500]	Time  6.156 ( 6.156)	Loss 2.3680 (1.4206)	CeLoss 0.2617 (0.6544)	SegCLSLoss 0.0106 (0.0066)	KLLoss 0.3691 (0.1871)	MaskLoss 1.0317 (0.3721)	MaskBCELoss 0.6088 (0.1155)	MaskDICELoss 0.4228 (0.2566)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 409 is less than current step: 499. Dropping entry: {'train/loss': 1.8167357563972473, 'train/ce_loss': 0.3919921875, 'train/seg_cls_loss': 0.014794921875, 'train/kl_loss': 0.2984375, 'train/mask_bce_loss': 0.09848726037889718, 'train/mask_dice_loss': 0.5952322006225585, 'train/mask_loss': 0.6937194466590881, 'metrics/total_secs_per_batch': 8.045875549316406, 'metrics/data_secs_per_batch': 3.700263833999634, '_timestamp': 1740963439.0627031}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 409 is less than current step: 499. Dropping entry: {'train/lr': 7.255102040816327e-05, '_timestamp': 1740963439.0630448}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 410 is less than current step: 499. Dropping entry: {'train/loss': 1.4205744028091432, 'train/ce_loss': 0.654443359375, 'train/seg_cls_loss': 0.0065673828125, 'train/kl_loss': 0.187109375, 'train/mask_bce_loss': 0.1155299830250442, 'train/mask_dice_loss': 0.2565980315208435, 'train/mask_loss': 0.37212801575660703, 'metrics/total_secs_per_batch': 6.155568361282349, 'metrics/data_secs_per_batch': 2.891907501220703, '_timestamp': 1740963445.2183938}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 410 is less than current step: 499. Dropping entry: {'train/lr': 7.242857142857143e-05, '_timestamp': 1740963445.218718}).
Epoch: [3][412/500]	Time  8.640 ( 8.640)	Loss 1.4718 (1.6647)	CeLoss 0.2207 (0.2199)	SegCLSLoss 0.0114 (0.0151)	KLLoss 0.3730 (0.3369)	MaskLoss 0.6041 (0.7017)	MaskBCELoss 0.0732 (0.0728)	MaskDICELoss 0.5308 (0.6289)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 411 is less than current step: 499. Dropping entry: {'train/loss': 1.6647368788719177, 'train/ce_loss': 0.2199462890625, 'train/seg_cls_loss': 0.0151123046875, 'train/kl_loss': 0.3369140625, 'train/mask_bce_loss': 0.07280112295411527, 'train/mask_dice_loss': 0.6289398729801178, 'train/mask_loss': 0.7017409920692443, 'metrics/total_secs_per_batch': 8.639521360397339, 'metrics/data_secs_per_batch': 3.8492223024368286, '_timestamp': 1740963453.8622484}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 411 is less than current step: 499. Dropping entry: {'train/lr': 7.230612244897958e-05, '_timestamp': 1740963453.8626328}).
Epoch: [3][413/500]	Time  8.532 ( 8.532)	Loss 0.5300 (1.2747)	CeLoss 0.2256 (0.2158)	SegCLSLoss 0.0103 (0.0119)	KLLoss 0.3730 (0.2969)	MaskLoss 0.1312 (0.5116)	MaskBCELoss 0.0456 (0.0796)	MaskDICELoss 0.0857 (0.4320)
Epoch: [3][414/500]	Time  7.181 ( 7.181)	Loss 1.4190 (1.7156)	CeLoss 0.2617 (0.6224)	SegCLSLoss 0.0117 (0.0106)	KLLoss 0.3750 (0.2229)	MaskLoss 0.5572 (0.5328)	MaskBCELoss 0.1237 (0.1321)	MaskDICELoss 0.4335 (0.4007)
Epoch: [3][415/500]	Time  8.441 ( 8.441)	Loss 2.1121 (1.5158)	CeLoss 0.3125 (0.2801)	SegCLSLoss 0.0129 (0.0168)	KLLoss 0.3672 (0.3693)	MaskLoss 0.8783 (0.5952)	MaskBCELoss 0.1338 (0.1164)	MaskDICELoss 0.7446 (0.4788)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 412 is less than current step: 499. Dropping entry: {'train/loss': 1.274709975719452, 'train/ce_loss': 0.2158447265625, 'train/seg_cls_loss': 0.0119140625, 'train/kl_loss': 0.296875, 'train/mask_bce_loss': 0.07959665730595589, 'train/mask_dice_loss': 0.4320137001574039, 'train/mask_loss': 0.5116103649139404, 'metrics/total_secs_per_batch': 8.53183388710022, 'metrics/data_secs_per_batch': 4.0316280126571655, '_timestamp': 1740963462.3897905}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 412 is less than current step: 499. Dropping entry: {'train/lr': 7.218367346938774e-05, '_timestamp': 1740963462.3901246}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 413 is less than current step: 499. Dropping entry: {'train/loss': 1.7155817151069641, 'train/ce_loss': 0.62236328125, 'train/seg_cls_loss': 0.0105712890625, 'train/kl_loss': 0.2228515625, 'train/mask_bce_loss': 0.13212492689490318, 'train/mask_dice_loss': 0.4006659060716629, 'train/mask_loss': 0.5327908396720886, 'metrics/total_secs_per_batch': 7.180920124053955, 'metrics/data_secs_per_batch': 3.5329593896865843, '_timestamp': 1740963469.5706112}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 413 is less than current step: 499. Dropping entry: {'train/lr': 7.20612244897959e-05, '_timestamp': 1740963469.5709202}).
Epoch: [3][416/500]	Time  9.141 ( 9.141)	Loss 1.7879 (1.7406)	CeLoss 0.2617 (0.3549)	SegCLSLoss 0.0128 (0.0157)	KLLoss 0.3711 (0.3355)	MaskLoss 0.7416 (0.6722)	MaskBCELoss 0.1021 (0.0866)	MaskDICELoss 0.6395 (0.5856)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 414 is less than current step: 499. Dropping entry: {'train/loss': 1.515815222263336, 'train/ce_loss': 0.280078125, 'train/seg_cls_loss': 0.016802978515625, 'train/kl_loss': 0.3693359375, 'train/mask_bce_loss': 0.1163787093013525, 'train/mask_dice_loss': 0.4787847623229027, 'train/mask_loss': 0.5951634734869004, 'metrics/total_secs_per_batch': 8.44057583808899, 'metrics/data_secs_per_batch': 3.607191491127014, '_timestamp': 1740963478.0113633}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 414 is less than current step: 499. Dropping entry: {'train/lr': 7.193877551020407e-05, '_timestamp': 1740963478.0116959}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 415 is less than current step: 499. Dropping entry: {'train/loss': 1.74060218334198, 'train/ce_loss': 0.3548828125, 'train/seg_cls_loss': 0.015655517578125, 'train/kl_loss': 0.335546875, 'train/mask_bce_loss': 0.08657475598156453, 'train/mask_dice_loss': 0.5855818003416061, 'train/mask_loss': 0.6721565544605255, 'metrics/total_secs_per_batch': 9.140734672546387, 'metrics/data_secs_per_batch': 3.858537125587463, '_timestamp': 1740963487.151967}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 415 is less than current step: 499. Dropping entry: {'train/lr': 7.181632653061225e-05, '_timestamp': 1740963487.1522422}).
Epoch: [3][417/500]	Time  8.902 ( 8.902)	Loss 0.8750 (1.7245)	CeLoss 0.8750 (0.3623)	SegCLSLoss 0.0000 (0.0137)	KLLoss 0.0000 (0.2990)	MaskLoss 0.0000 (0.6626)	MaskBCELoss 0.0000 (0.2018)	MaskDICELoss 0.0000 (0.4609)
Epoch: [3][418/500]	Time  6.914 ( 6.914)	Loss 2.0996 (1.6030)	CeLoss 0.2266 (0.3826)	SegCLSLoss 0.0194 (0.0116)	KLLoss 0.3594 (0.2594)	MaskLoss 0.9140 (0.5942)	MaskBCELoss 0.2150 (0.1484)	MaskDICELoss 0.6991 (0.4458)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 416 is less than current step: 499. Dropping entry: {'train/loss': 1.7244768857955932, 'train/ce_loss': 0.3623046875, 'train/seg_cls_loss': 0.013739013671875, 'train/kl_loss': 0.2990234375, 'train/mask_bce_loss': 0.201766655780375, 'train/mask_dice_loss': 0.4608624130487442, 'train/mask_loss': 0.6626290649175643, 'metrics/total_secs_per_batch': 8.90164566040039, 'metrics/data_secs_per_batch': 3.825400161743164, '_timestamp': 1740963496.0536187}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 416 is less than current step: 499. Dropping entry: {'train/lr': 7.169387755102041e-05, '_timestamp': 1740963496.0538995}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 417 is less than current step: 499. Dropping entry: {'train/loss': 1.603005313873291, 'train/ce_loss': 0.3826171875, 'train/seg_cls_loss': 0.01160888671875, 'train/kl_loss': 0.259375, 'train/mask_bce_loss': 0.14838557727634907, 'train/mask_dice_loss': 0.44579286575317384, 'train/mask_loss': 0.5941784381866455, 'metrics/total_secs_per_batch': 6.913517713546753, 'metrics/data_secs_per_batch': 3.3314103841781617, '_timestamp': 1740963502.9674237}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 417 is less than current step: 499. Dropping entry: {'train/lr': 7.157142857142856e-05, '_timestamp': 1740963502.9678001}).
Epoch: [3][419/500]	Time  8.027 ( 8.027)	Loss 2.6637 (1.7363)	CeLoss 0.2236 (0.3530)	SegCLSLoss 0.0173 (0.0160)	KLLoss 0.3652 (0.2963)	MaskLoss 1.1971 (0.6728)	MaskBCELoss 0.3923 (0.1209)	MaskDICELoss 0.8048 (0.5518)
[2025-03-02 18:58:38,623] [INFO] [logging.py:96:log_dist] [Rank 0] step=1920, skipped=0, lr=[7.13877551020408e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:58:38,629] [INFO] [timer.py:215:stop] epoch=0/micro_step=19200/global_step=1920, RunningAvgSamplesPerSec=1.5050330862555608, CurrSamplesPerSec=1.3099455765494636, MemAllocated=31.25GB, MaxMemAllocated=37.19GB
Epoch: [3][420/500]	Time  7.636 ( 7.636)	Loss 2.0110 (1.4734)	CeLoss 0.1943 (0.4697)	SegCLSLoss 0.0223 (0.0128)	KLLoss 0.3613 (0.2232)	MaskLoss 0.8849 (0.4874)	MaskBCELoss 0.0829 (0.1236)	MaskDICELoss 0.8020 (0.3639)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 418 is less than current step: 499. Dropping entry: {'train/loss': 1.7363305568695069, 'train/ce_loss': 0.35302734375, 'train/seg_cls_loss': 0.015960693359375, 'train/kl_loss': 0.2962890625, 'train/mask_bce_loss': 0.12091089645400643, 'train/mask_dice_loss': 0.5518442153930664, 'train/mask_loss': 0.6727551102638245, 'metrics/total_secs_per_batch': 8.027427673339844, 'metrics/data_secs_per_batch': 3.896374559402466, '_timestamp': 1740963510.9946105}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 418 is less than current step: 499. Dropping entry: {'train/lr': 7.144897959183672e-05, '_timestamp': 1740963510.9949474}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 419 is less than current step: 499. Dropping entry: {'train/loss': 1.4733889698982239, 'train/ce_loss': 0.4697021484375, 'train/seg_cls_loss': 0.01282958984375, 'train/kl_loss': 0.2232421875, 'train/mask_bce_loss': 0.12358400821685792, 'train/mask_dice_loss': 0.36385510563850404, 'train/mask_loss': 0.48743910789489747, 'metrics/total_secs_per_batch': 7.6355140209198, 'metrics/data_secs_per_batch': 3.1747889280319215, '_timestamp': 1740963518.6299024}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 419 is less than current step: 499. Dropping entry: {'train/lr': 7.132653061224489e-05, '_timestamp': 1740963518.6301737}).
Epoch: [3][421/500]	Time  8.884 ( 8.884)	Loss 1.6402 (1.3360)	CeLoss 0.3066 (0.2868)	SegCLSLoss 0.0172 (0.0107)	KLLoss 0.3809 (0.3004)	MaskLoss 0.6443 (0.5071)	MaskBCELoss 0.0761 (0.0797)	MaskDICELoss 0.5682 (0.4274)
Epoch: [3][422/500]	Time  8.424 ( 8.424)	Loss 1.4506 (1.8598)	CeLoss 0.2041 (0.4246)	SegCLSLoss 0.0239 (0.0165)	KLLoss 0.3750 (0.3004)	MaskLoss 0.5983 (0.6985)	MaskBCELoss 0.2917 (0.2120)	MaskDICELoss 0.3067 (0.4865)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 420 is less than current step: 499. Dropping entry: {'train/loss': 1.3359921336174012, 'train/ce_loss': 0.28681640625, 'train/seg_cls_loss': 0.010675048828125, 'train/kl_loss': 0.300390625, 'train/mask_bce_loss': 0.0797155162319541, 'train/mask_dice_loss': 0.42739188820123675, 'train/mask_loss': 0.5071074038743972, 'metrics/total_secs_per_batch': 8.883847713470459, 'metrics/data_secs_per_batch': 3.6928134679794313, '_timestamp': 1740963527.5139277}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 420 is less than current step: 499. Dropping entry: {'train/lr': 7.120408163265305e-05, '_timestamp': 1740963527.5141919}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 421 is less than current step: 499. Dropping entry: {'train/loss': 1.859848153591156, 'train/ce_loss': 0.424609375, 'train/seg_cls_loss': 0.016485595703125, 'train/kl_loss': 0.300390625, 'train/mask_bce_loss': 0.2120169241912663, 'train/mask_dice_loss': 0.48646183907985685, 'train/mask_loss': 0.698478764295578, 'metrics/total_secs_per_batch': 8.423576593399048, 'metrics/data_secs_per_batch': 4.101417970657349, '_timestamp': 1740963535.937504}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 421 is less than current step: 499. Dropping entry: {'train/lr': 7.108163265306122e-05, '_timestamp': 1740963535.9377797}).
Epoch: [3][423/500]	Time  5.984 ( 5.984)	Loss 1.2375 (1.5656)	CeLoss 0.2324 (0.5995)	SegCLSLoss 0.0141 (0.0092)	KLLoss 0.3672 (0.1865)	MaskLoss 0.4810 (0.4714)	MaskBCELoss 0.0453 (0.0917)	MaskDICELoss 0.4358 (0.3797)
Epoch: [3][424/500]	Time  7.194 ( 7.194)	Loss 2.2432 (1.7387)	CeLoss 0.1768 (0.5125)	SegCLSLoss 0.0240 (0.0128)	KLLoss 0.3711 (0.2578)	MaskLoss 1.0088 (0.5971)	MaskBCELoss 0.0234 (0.0545)	MaskDICELoss 0.9855 (0.5425)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 422 is less than current step: 499. Dropping entry: {'train/loss': 1.5655704379081725, 'train/ce_loss': 0.599462890625, 'train/seg_cls_loss': 0.00919189453125, 'train/kl_loss': 0.1865234375, 'train/mask_bce_loss': 0.09171971110627056, 'train/mask_dice_loss': 0.3797129660844803, 'train/mask_loss': 0.47143268287181855, 'metrics/total_secs_per_batch': 5.9836413860321045, 'metrics/data_secs_per_batch': 2.5999717712402344, '_timestamp': 1740963541.9219735}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 422 is less than current step: 499. Dropping entry: {'train/lr': 7.095918367346939e-05, '_timestamp': 1740963541.922608}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 423 is less than current step: 499. Dropping entry: {'train/loss': 1.7387476682662963, 'train/ce_loss': 0.5125, 'train/seg_cls_loss': 0.01279296875, 'train/kl_loss': 0.2578125, 'train/mask_bce_loss': 0.05452872030436993, 'train/mask_dice_loss': 0.5425306558609009, 'train/mask_loss': 0.5970593750476837, 'metrics/total_secs_per_batch': 7.193949937820435, 'metrics/data_secs_per_batch': 3.4184518337249754, '_timestamp': 1740963549.115182}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 423 is less than current step: 499. Dropping entry: {'train/lr': 7.083673469387755e-05, '_timestamp': 1740963549.1154659}).
Epoch: [3][425/500]	Time  7.813 ( 7.813)	Loss 1.7650 (1.7001)	CeLoss 0.3047 (0.3585)	SegCLSLoss 0.0186 (0.0163)	KLLoss 0.3594 (0.3322)	MaskLoss 0.7077 (0.6502)	MaskBCELoss 0.0557 (0.1176)	MaskDICELoss 0.6520 (0.5325)
Epoch: [3][426/500]	Time  8.922 ( 8.922)	Loss 2.1893 (1.2335)	CeLoss 0.2285 (0.3401)	SegCLSLoss 0.0216 (0.0110)	KLLoss 0.3652 (0.2209)	MaskLoss 0.9570 (0.4329)	MaskBCELoss 0.0386 (0.0488)	MaskDICELoss 0.9183 (0.3842)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 424 is less than current step: 499. Dropping entry: {'train/loss': 1.7001187682151795, 'train/ce_loss': 0.35849609375, 'train/seg_cls_loss': 0.01627197265625, 'train/kl_loss': 0.3322265625, 'train/mask_bce_loss': 0.11762318201363087, 'train/mask_dice_loss': 0.5325338512659072, 'train/mask_loss': 0.6501570254564285, 'metrics/total_secs_per_batch': 7.813403606414795, 'metrics/data_secs_per_batch': 3.141275405883789, '_timestamp': 1740963556.928722}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 424 is less than current step: 499. Dropping entry: {'train/lr': 7.07142857142857e-05, '_timestamp': 1740963556.9289548}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 425 is less than current step: 499. Dropping entry: {'train/loss': 1.2334580719470978, 'train/ce_loss': 0.34013671875, 'train/seg_cls_loss': 0.011016845703125, 'train/kl_loss': 0.2208984375, 'train/mask_bce_loss': 0.048756140470504764, 'train/mask_dice_loss': 0.38418384045362475, 'train/mask_loss': 0.4329399734735489, 'metrics/total_secs_per_batch': 8.922010660171509, 'metrics/data_secs_per_batch': 3.9697309494018556, '_timestamp': 1740963565.8505442}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 425 is less than current step: 499. Dropping entry: {'train/lr': 7.059183673469386e-05, '_timestamp': 1740963565.8508718}).
Epoch: [3][427/500]	Time  8.410 ( 8.410)	Loss 2.3065 (1.5871)	CeLoss 0.2002 (0.3442)	SegCLSLoss 0.0205 (0.0126)	KLLoss 0.3750 (0.2994)	MaskLoss 1.0292 (0.6034)	MaskBCELoss 0.0295 (0.0935)	MaskDICELoss 0.9998 (0.5099)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 426 is less than current step: 499. Dropping entry: {'train/loss': 1.5871457159519196, 'train/ce_loss': 0.344189453125, 'train/seg_cls_loss': 0.012603759765625, 'train/kl_loss': 0.2994140625, 'train/mask_bce_loss': 0.09346533734351396, 'train/mask_dice_loss': 0.5099463865160943, 'train/mask_loss': 0.6034117341041565, 'metrics/total_secs_per_batch': 8.41011905670166, 'metrics/data_secs_per_batch': 4.1859050989151, '_timestamp': 1740963574.2609768}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 426 is less than current step: 499. Dropping entry: {'train/lr': 7.046938775510204e-05, '_timestamp': 1740963574.261368}).
Epoch: [3][428/500]	Time  8.122 ( 8.122)	Loss 0.1729 (1.6916)	CeLoss 0.1729 (0.4148)	SegCLSLoss 0.0000 (0.0124)	KLLoss 0.0000 (0.2574)	MaskLoss 0.0000 (0.6224)	MaskBCELoss 0.0000 (0.1111)	MaskDICELoss 0.0000 (0.5112)
Epoch: [3][429/500]	Time  7.030 ( 7.030)	Loss 1.0781 (2.0738)	CeLoss 1.0781 (0.4198)	SegCLSLoss 0.0000 (0.0149)	KLLoss 0.0000 (0.2598)	MaskLoss 0.0000 (0.8102)	MaskBCELoss 0.0000 (0.2468)	MaskDICELoss 0.0000 (0.5634)
[2025-03-02 18:59:58,079] [INFO] [logging.py:96:log_dist] [Rank 0] step=1930, skipped=0, lr=[7.016326530612245e-05], mom=[(0.9, 0.95)]
[2025-03-02 18:59:58,085] [INFO] [timer.py:215:stop] epoch=0/micro_step=19300/global_step=1930, RunningAvgSamplesPerSec=1.503507497930867, CurrSamplesPerSec=1.1532126963419522, MemAllocated=31.25GB, MaxMemAllocated=37.19GB
Epoch: [3][430/500]	Time  8.673 ( 8.673)	Loss 1.8739 (1.7889)	CeLoss 0.1973 (0.2251)	SegCLSLoss 0.0204 (0.0179)	KLLoss 0.3750 (0.3736)	MaskLoss 0.8149 (0.7588)	MaskBCELoss 0.0926 (0.1577)	MaskDICELoss 0.7223 (0.6011)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 427 is less than current step: 499. Dropping entry: {'train/loss': 1.6915960133075714, 'train/ce_loss': 0.41484375, 'train/seg_cls_loss': 0.01236572265625, 'train/kl_loss': 0.257421875, 'train/mask_bce_loss': 0.11114955674856901, 'train/mask_dice_loss': 0.511210960149765, 'train/mask_loss': 0.6223605185747146, 'metrics/total_secs_per_batch': 8.122062683105469, 'metrics/data_secs_per_batch': 3.350385308265686, '_timestamp': 1740963582.3828382}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 427 is less than current step: 499. Dropping entry: {'train/lr': 7.03469387755102e-05, '_timestamp': 1740963582.3832078}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 428 is less than current step: 499. Dropping entry: {'train/loss': 2.0737539529800415, 'train/ce_loss': 0.41982421875, 'train/seg_cls_loss': 0.014874267578125, 'train/kl_loss': 0.259765625, 'train/mask_bce_loss': 0.24681372232735158, 'train/mask_dice_loss': 0.5634030878543854, 'train/mask_loss': 0.8102168023586274, 'metrics/total_secs_per_batch': 7.030346155166626, 'metrics/data_secs_per_batch': 2.728310465812683, '_timestamp': 1740963589.413061}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 428 is less than current step: 499. Dropping entry: {'train/lr': 7.022448979591837e-05, '_timestamp': 1740963589.413318}).
Epoch: [3][431/500]	Time  6.663 ( 6.663)	Loss 1.0763 (1.9013)	CeLoss 0.2451 (0.5208)	SegCLSLoss 0.0100 (0.0138)	KLLoss 0.3789 (0.2633)	MaskLoss 0.3946 (0.6738)	MaskBCELoss 0.1124 (0.1294)	MaskDICELoss 0.2823 (0.5443)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 429 is less than current step: 499. Dropping entry: {'train/loss': 1.7888781547546386, 'train/ce_loss': 0.22509765625, 'train/seg_cls_loss': 0.017852783203125, 'train/kl_loss': 0.3736328125, 'train/mask_bce_loss': 0.15771492645144464, 'train/mask_dice_loss': 0.6010796383023262, 'train/mask_loss': 0.7587945550680161, 'metrics/total_secs_per_batch': 8.672920227050781, 'metrics/data_secs_per_batch': 3.947137475013733, '_timestamp': 1740963598.086004}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 429 is less than current step: 499. Dropping entry: {'train/lr': 7.010204081632653e-05, '_timestamp': 1740963598.0863414}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 430 is less than current step: 499. Dropping entry: {'train/loss': 1.9013219952583313, 'train/ce_loss': 0.52080078125, 'train/seg_cls_loss': 0.013800048828125, 'train/kl_loss': 0.26328125, 'train/mask_bce_loss': 0.12940772883594037, 'train/mask_dice_loss': 0.5443489670753479, 'train/mask_loss': 0.6737567007541656, 'metrics/total_secs_per_batch': 6.662647247314453, 'metrics/data_secs_per_batch': 3.50798499584198, '_timestamp': 1740963604.7486937}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 430 is less than current step: 499. Dropping entry: {'train/lr': 6.997959183673469e-05, '_timestamp': 1740963604.749024}).
Epoch: [3][432/500]	Time  7.855 ( 7.855)	Loss 1.4304 (1.6610)	CeLoss 0.1523 (0.4174)	SegCLSLoss 0.0251 (0.0159)	KLLoss 0.3770 (0.2623)	MaskLoss 0.6136 (0.6045)	MaskBCELoss 0.0160 (0.0963)	MaskDICELoss 0.5976 (0.5083)
Epoch: [3][433/500]	Time  8.031 ( 8.031)	Loss 1.1328 (1.4394)	CeLoss 1.1328 (0.4688)	SegCLSLoss 0.0000 (0.0108)	KLLoss 0.0000 (0.2242)	MaskLoss 0.0000 (0.4714)	MaskBCELoss 0.0000 (0.1315)	MaskDICELoss 0.0000 (0.3398)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 431 is less than current step: 499. Dropping entry: {'train/loss': 1.6610026478767395, 'train/ce_loss': 0.4173828125, 'train/seg_cls_loss': 0.0159423828125, 'train/kl_loss': 0.2623046875, 'train/mask_bce_loss': 0.09625730309635401, 'train/mask_dice_loss': 0.5082674652338028, 'train/mask_loss': 0.6045247673988342, 'metrics/total_secs_per_batch': 7.854802846908569, 'metrics/data_secs_per_batch': 3.572864556312561, '_timestamp': 1740963612.6034796}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 431 is less than current step: 499. Dropping entry: {'train/lr': 6.985714285714284e-05, '_timestamp': 1740963612.6037977}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 432 is less than current step: 499. Dropping entry: {'train/loss': 1.4394071698188782, 'train/ce_loss': 0.46875, 'train/seg_cls_loss': 0.010809326171875, 'train/kl_loss': 0.22421875, 'train/mask_bce_loss': 0.1315309561789036, 'train/mask_dice_loss': 0.3398327797651291, 'train/mask_loss': 0.4713637411594391, 'metrics/total_secs_per_batch': 8.030667066574097, 'metrics/data_secs_per_batch': 4.30312020778656, '_timestamp': 1740963620.634303}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 432 is less than current step: 499. Dropping entry: {'train/lr': 6.973469387755102e-05, '_timestamp': 1740963620.6346092}).
Epoch: [3][434/500]	Time  8.354 ( 8.354)	Loss 1.2771 (1.9227)	CeLoss 0.2275 (0.4209)	SegCLSLoss 0.0141 (0.0151)	KLLoss 0.3711 (0.2986)	MaskLoss 0.5028 (0.7323)	MaskBCELoss 0.0669 (0.1923)	MaskDICELoss 0.4359 (0.5400)
Epoch: [3][435/500]	Time  7.773 ( 7.773)	Loss 2.2716 (1.6242)	CeLoss 0.2129 (0.5089)	SegCLSLoss 0.0122 (0.0102)	KLLoss 0.3652 (0.2615)	MaskLoss 1.0079 (0.5421)	MaskBCELoss 0.0091 (0.0441)	MaskDICELoss 0.9988 (0.4980)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 433 is less than current step: 499. Dropping entry: {'train/loss': 1.9226839780807494, 'train/ce_loss': 0.420947265625, 'train/seg_cls_loss': 0.0151123046875, 'train/kl_loss': 0.2986328125, 'train/mask_bce_loss': 0.1923173001036048, 'train/mask_dice_loss': 0.5399719431996346, 'train/mask_loss': 0.7322892308235168, 'metrics/total_secs_per_batch': 8.354236602783203, 'metrics/data_secs_per_batch': 3.9603298902511597, '_timestamp': 1740963628.9884434}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 433 is less than current step: 499. Dropping entry: {'train/lr': 6.961224489795918e-05, '_timestamp': 1740963628.9887543}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 434 is less than current step: 499. Dropping entry: {'train/loss': 1.62421373128891, 'train/ce_loss': 0.50888671875, 'train/seg_cls_loss': 0.010235595703125, 'train/kl_loss': 0.2615234375, 'train/mask_bce_loss': 0.04405334105249494, 'train/mask_dice_loss': 0.49803400635719297, 'train/mask_loss': 0.5420873433351516, 'metrics/total_secs_per_batch': 7.772524833679199, 'metrics/data_secs_per_batch': 3.9075886249542235, '_timestamp': 1740963636.761047}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 434 is less than current step: 499. Dropping entry: {'train/lr': 6.948979591836734e-05, '_timestamp': 1740963636.7613456}).
Epoch: [3][436/500]	Time  8.419 ( 8.419)	Loss 2.6492 (1.7517)	CeLoss 0.1592 (0.3822)	SegCLSLoss 0.0354 (0.0162)	KLLoss 0.3828 (0.3336)	MaskLoss 1.2172 (0.6642)	MaskBCELoss 0.4953 (0.1691)	MaskDICELoss 0.7219 (0.4951)
Epoch: [3][437/500]	Time  7.455 ( 7.455)	Loss 1.6061 (1.6605)	CeLoss 0.2461 (0.3731)	SegCLSLoss 0.0121 (0.0154)	KLLoss 0.3789 (0.2602)	MaskLoss 0.6585 (0.6267)	MaskBCELoss 0.2733 (0.2276)	MaskDICELoss 0.3852 (0.3991)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 435 is less than current step: 499. Dropping entry: {'train/loss': 1.7516805589199067, 'train/ce_loss': 0.3822265625, 'train/seg_cls_loss': 0.016229248046875, 'train/kl_loss': 0.33359375, 'train/mask_bce_loss': 0.16905037667602302, 'train/mask_dice_loss': 0.4951199650764465, 'train/mask_loss': 0.6641703575849534, 'metrics/total_secs_per_batch': 8.419499635696411, 'metrics/data_secs_per_batch': 4.139911746978759, '_timestamp': 1740963645.1803854}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 435 is less than current step: 499. Dropping entry: {'train/lr': 6.936734693877551e-05, '_timestamp': 1740963645.1806514}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 436 is less than current step: 499. Dropping entry: {'train/loss': 1.6605172753334045, 'train/ce_loss': 0.37314453125, 'train/seg_cls_loss': 0.01536865234375, 'train/kl_loss': 0.26015625, 'train/mask_bce_loss': 0.22763110771775247, 'train/mask_dice_loss': 0.3991119086742401, 'train/mask_loss': 0.6267430067062378, 'metrics/total_secs_per_batch': 7.455190181732178, 'metrics/data_secs_per_batch': 3.344017171859741, '_timestamp': 1740963652.635871}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 436 is less than current step: 499. Dropping entry: {'train/lr': 6.924489795918367e-05, '_timestamp': 1740963652.636211}).
Epoch: [3][438/500]	Time  9.533 ( 9.533)	Loss 2.8672 (1.6371)	CeLoss 0.1836 (0.1914)	SegCLSLoss 0.0192 (0.0130)	KLLoss 0.3809 (0.2982)	MaskLoss 1.3179 (0.7047)	MaskBCELoss 0.4953 (0.2207)	MaskDICELoss 0.8226 (0.4840)
Epoch: [3][439/500]	Time  6.138 ( 6.138)	Loss 1.9486 (1.5359)	CeLoss 0.3262 (0.3820)	SegCLSLoss 0.0143 (0.0143)	KLLoss 0.3711 (0.2617)	MaskLoss 0.7888 (0.5603)	MaskBCELoss 0.0515 (0.0538)	MaskDICELoss 0.7373 (0.5065)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 437 is less than current step: 499. Dropping entry: {'train/loss': 1.6371381759643555, 'train/ce_loss': 0.19140625, 'train/seg_cls_loss': 0.012982177734375, 'train/kl_loss': 0.2982421875, 'train/mask_bce_loss': 0.2206819973886013, 'train/mask_dice_loss': 0.4839710682630539, 'train/mask_loss': 0.7046530723571778, 'metrics/total_secs_per_batch': 9.533493041992188, 'metrics/data_secs_per_batch': 4.185042190551758, '_timestamp': 1740963662.1690698}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 437 is less than current step: 499. Dropping entry: {'train/lr': 6.912244897959182e-05, '_timestamp': 1740963662.1692524}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 438 is less than current step: 499. Dropping entry: {'train/loss': 1.5359423279762268, 'train/ce_loss': 0.38203125, 'train/seg_cls_loss': 0.0142822265625, 'train/kl_loss': 0.26171875, 'train/mask_bce_loss': 0.053800028096884486, 'train/mask_dice_loss': 0.5065051257610321, 'train/mask_loss': 0.5603051483631134, 'metrics/total_secs_per_batch': 6.137763261795044, 'metrics/data_secs_per_batch': 2.3715420722961427, '_timestamp': 1740963668.3071203}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 438 is less than current step: 499. Dropping entry: {'train/lr': 6.9e-05, '_timestamp': 1740963668.3074636}).
[2025-03-02 19:01:16,086] [INFO] [logging.py:96:log_dist] [Rank 0] step=1940, skipped=0, lr=[6.893877551020408e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:01:16,092] [INFO] [timer.py:215:stop] epoch=0/micro_step=19400/global_step=1940, RunningAvgSamplesPerSec=1.502169302295096, CurrSamplesPerSec=1.2846513332559657, MemAllocated=30.92GB, MaxMemAllocated=37.19GB
Epoch: [3][440/500]	Time  7.786 ( 7.786)	Loss 1.0703 (1.6510)	CeLoss 1.0703 (0.4453)	SegCLSLoss 0.0000 (0.0104)	KLLoss 0.0000 (0.2637)	MaskLoss 0.0000 (0.5870)	MaskBCELoss 0.0000 (0.1190)	MaskDICELoss 0.0000 (0.4680)
Epoch: [3][441/500]	Time  8.115 ( 8.115)	Loss 1.5266 (1.5944)	CeLoss 0.2139 (0.2850)	SegCLSLoss 0.0123 (0.0142)	KLLoss 0.3789 (0.2621)	MaskLoss 0.6344 (0.6380)	MaskBCELoss 0.0736 (0.1782)	MaskDICELoss 0.5608 (0.4598)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 439 is less than current step: 499. Dropping entry: {'train/loss': 1.650952661037445, 'train/ce_loss': 0.4453125, 'train/seg_cls_loss': 0.010369873046875, 'train/kl_loss': 0.263671875, 'train/mask_bce_loss': 0.11898765545338393, 'train/mask_dice_loss': 0.46801210641860963, 'train/mask_loss': 0.5869997590780258, 'metrics/total_secs_per_batch': 7.786089897155762, 'metrics/data_secs_per_batch': 3.5614604473114015, '_timestamp': 1740963676.0928369}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 439 is less than current step: 499. Dropping entry: {'train/lr': 6.887755102040816e-05, '_timestamp': 1740963676.093135}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 440 is less than current step: 499. Dropping entry: {'train/loss': 1.5943929076194763, 'train/ce_loss': 0.285009765625, 'train/seg_cls_loss': 0.01424560546875, 'train/kl_loss': 0.262109375, 'train/mask_bce_loss': 0.17819183189421892, 'train/mask_dice_loss': 0.459849351644516, 'train/mask_loss': 0.6380411773920059, 'metrics/total_secs_per_batch': 8.114814043045044, 'metrics/data_secs_per_batch': 3.657448244094849, '_timestamp': 1740963684.2079937}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 440 is less than current step: 499. Dropping entry: {'train/lr': 6.875510204081632e-05, '_timestamp': 1740963684.2083387}).
Epoch: [3][442/500]	Time  6.816 ( 6.816)	Loss 0.2314 (1.0984)	CeLoss 0.2314 (0.5051)	SegCLSLoss 0.0000 (0.0055)	KLLoss 0.0000 (0.1125)	MaskLoss 0.0000 (0.2897)	MaskBCELoss 0.0000 (0.0257)	MaskDICELoss 0.0000 (0.2640)
Epoch: [3][443/500]	Time  7.421 ( 7.421)	Loss 1.8146 (1.3457)	CeLoss 0.2051 (0.4558)	SegCLSLoss 0.0186 (0.0088)	KLLoss 0.3613 (0.2254)	MaskLoss 0.7823 (0.4315)	MaskBCELoss 0.0900 (0.1084)	MaskDICELoss 0.6923 (0.3230)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 441 is less than current step: 499. Dropping entry: {'train/loss': 1.0984274744987488, 'train/ce_loss': 0.5050537109375, 'train/seg_cls_loss': 0.00552978515625, 'train/kl_loss': 0.1125, 'train/mask_bce_loss': 0.025683780387043952, 'train/mask_dice_loss': 0.2639718472957611, 'train/mask_loss': 0.2896556258201599, 'metrics/total_secs_per_batch': 6.815623044967651, 'metrics/data_secs_per_batch': 2.920331072807312, '_timestamp': 1740963691.0234187}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 441 is less than current step: 499. Dropping entry: {'train/lr': 6.863265306122449e-05, '_timestamp': 1740963691.023701}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 442 is less than current step: 499. Dropping entry: {'train/loss': 1.3456538319587708, 'train/ce_loss': 0.45576171875, 'train/seg_cls_loss': 0.00875244140625, 'train/kl_loss': 0.225390625, 'train/mask_bce_loss': 0.1084215946495533, 'train/mask_dice_loss': 0.32304790914058684, 'train/mask_loss': 0.43146950006484985, 'metrics/total_secs_per_batch': 7.421262502670288, 'metrics/data_secs_per_batch': 3.04353346824646, '_timestamp': 1740963698.4448915}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 442 is less than current step: 499. Dropping entry: {'train/lr': 6.851020408163265e-05, '_timestamp': 1740963698.4452417}).
Epoch: [3][444/500]	Time  7.145 ( 7.145)	Loss 0.0571 (1.7093)	CeLoss 0.0571 (0.4310)	SegCLSLoss 0.0000 (0.0113)	KLLoss 0.0000 (0.2574)	MaskLoss 0.0000 (0.6234)	MaskBCELoss 0.0000 (0.1417)	MaskDICELoss 0.0000 (0.4817)
Epoch: [3][445/500]	Time  8.468 ( 8.468)	Loss 1.3864 (1.7216)	CeLoss 0.2373 (0.3089)	SegCLSLoss 0.0114 (0.0140)	KLLoss 0.3711 (0.2932)	MaskLoss 0.5526 (0.6881)	MaskBCELoss 0.0598 (0.1218)	MaskDICELoss 0.4928 (0.5664)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 443 is less than current step: 499. Dropping entry: {'train/loss': 1.7092571020126344, 'train/ce_loss': 0.431005859375, 'train/seg_cls_loss': 0.011285400390625, 'train/kl_loss': 0.257421875, 'train/mask_bce_loss': 0.14166051940992475, 'train/mask_dice_loss': 0.48169361650943754, 'train/mask_loss': 0.6233541429042816, 'metrics/total_secs_per_batch': 7.144550085067749, 'metrics/data_secs_per_batch': 2.8803043603897094, '_timestamp': 1740963705.589232}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 443 is less than current step: 499. Dropping entry: {'train/lr': 6.838775510204081e-05, '_timestamp': 1740963705.5895154}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 444 is less than current step: 499. Dropping entry: {'train/loss': 1.721603786945343, 'train/ce_loss': 0.30888671875, 'train/seg_cls_loss': 0.013970947265625, 'train/kl_loss': 0.2931640625, 'train/mask_bce_loss': 0.12177248382940889, 'train/mask_dice_loss': 0.5663731455802917, 'train/mask_loss': 0.688145637512207, 'metrics/total_secs_per_batch': 8.46847152709961, 'metrics/data_secs_per_batch': 3.6849025011062624, '_timestamp': 1740963714.0577052}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 444 is less than current step: 499. Dropping entry: {'train/lr': 6.826530612244897e-05, '_timestamp': 1740963714.0580204}).
Epoch: [3][446/500]	Time  6.963 ( 6.963)	Loss 0.9883 (1.5785)	CeLoss 0.9883 (0.5019)	SegCLSLoss 0.0000 (0.0113)	KLLoss 0.0000 (0.2223)	MaskLoss 0.0000 (0.5243)	MaskBCELoss 0.0000 (0.0935)	MaskDICELoss 0.0000 (0.4308)
Epoch: [3][447/500]	Time  7.870 ( 7.870)	Loss 1.8116 (1.4669)	CeLoss 0.2197 (0.4560)	SegCLSLoss 0.0211 (0.0110)	KLLoss 0.3555 (0.2211)	MaskLoss 0.7730 (0.4915)	MaskBCELoss 0.0630 (0.0865)	MaskDICELoss 0.7099 (0.4050)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 445 is less than current step: 499. Dropping entry: {'train/loss': 1.578458833694458, 'train/ce_loss': 0.50185546875, 'train/seg_cls_loss': 0.01134033203125, 'train/kl_loss': 0.222265625, 'train/mask_bce_loss': 0.09352344004437327, 'train/mask_dice_loss': 0.4308133900165558, 'train/mask_loss': 0.524336838722229, 'metrics/total_secs_per_batch': 6.962521314620972, 'metrics/data_secs_per_batch': 2.986321520805359, '_timestamp': 1740963721.0204587}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 445 is less than current step: 499. Dropping entry: {'train/lr': 6.814285714285714e-05, '_timestamp': 1740963721.0208077}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 446 is less than current step: 499. Dropping entry: {'train/loss': 1.466947042942047, 'train/ce_loss': 0.45595703125, 'train/seg_cls_loss': 0.01103515625, 'train/kl_loss': 0.22109375, 'train/mask_bce_loss': 0.08652816247195005, 'train/mask_dice_loss': 0.40500198900699613, 'train/mask_loss': 0.49153015613555906, 'metrics/total_secs_per_batch': 7.869880437850952, 'metrics/data_secs_per_batch': 3.5587835788726805, '_timestamp': 1740963728.8901122}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 446 is less than current step: 499. Dropping entry: {'train/lr': 6.80204081632653e-05, '_timestamp': 1740963728.8904207}).
Epoch: [3][448/500]	Time  7.735 ( 7.735)	Loss 0.9451 (2.0709)	CeLoss 0.1777 (0.2526)	SegCLSLoss 0.0195 (0.0191)	KLLoss 0.3809 (0.2967)	MaskLoss 0.3598 (0.8895)	MaskBCELoss 0.1176 (0.3238)	MaskDICELoss 0.2422 (0.5656)
Epoch: [3][449/500]	Time  6.928 ( 6.928)	Loss 2.0190 (1.8280)	CeLoss 0.1992 (0.3991)	SegCLSLoss 0.0188 (0.0193)	KLLoss 0.3750 (0.2961)	MaskLoss 0.8865 (0.6948)	MaskBCELoss 0.0791 (0.0839)	MaskDICELoss 0.8073 (0.6110)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 447 is less than current step: 499. Dropping entry: {'train/loss': 2.070898735523224, 'train/ce_loss': 0.25263671875, 'train/seg_cls_loss': 0.019073486328125, 'train/kl_loss': 0.2966796875, 'train/mask_bce_loss': 0.32384963929653166, 'train/mask_dice_loss': 0.5656036317348481, 'train/mask_loss': 0.8894532740116119, 'metrics/total_secs_per_batch': 7.735006093978882, 'metrics/data_secs_per_batch': 3.1688375234603883, '_timestamp': 1740963736.625223}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 447 is less than current step: 499. Dropping entry: {'train/lr': 6.789795918367346e-05, '_timestamp': 1740963736.6255193}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 448 is less than current step: 499. Dropping entry: {'train/loss': 1.8279794692993163, 'train/ce_loss': 0.39912109375, 'train/seg_cls_loss': 0.0193115234375, 'train/kl_loss': 0.29609375, 'train/mask_bce_loss': 0.08389398623257875, 'train/mask_dice_loss': 0.6109551101922989, 'train/mask_loss': 0.6948491036891937, 'metrics/total_secs_per_batch': 6.9278411865234375, 'metrics/data_secs_per_batch': 3.1277902126312256, '_timestamp': 1740963743.5531096}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 448 is less than current step: 499. Dropping entry: {'train/lr': 6.777551020408163e-05, '_timestamp': 1740963743.553414}).
[2025-03-02 19:02:30,739] [INFO] [logging.py:96:log_dist] [Rank 0] step=1950, skipped=0, lr=[6.771428571428571e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:02:30,746] [INFO] [timer.py:215:stop] epoch=0/micro_step=19500/global_step=1950, RunningAvgSamplesPerSec=1.5012350192156294, CurrSamplesPerSec=1.3904262730014503, MemAllocated=30.79GB, MaxMemAllocated=37.19GB
Epoch: [3][450/500]	Time  7.194 ( 7.194)	Loss 1.9624 (1.5427)	CeLoss 0.2969 (0.4339)	SegCLSLoss 0.0129 (0.0119)	KLLoss 0.3711 (0.2588)	MaskLoss 0.8113 (0.5383)	MaskBCELoss 0.1351 (0.1049)	MaskDICELoss 0.6762 (0.4335)
Epoch: [3][451/500]	Time  8.318 ( 8.318)	Loss 1.5014 (1.6820)	CeLoss 0.1904 (0.4196)	SegCLSLoss 0.0277 (0.0140)	KLLoss 0.3730 (0.2555)	MaskLoss 0.6301 (0.6151)	MaskBCELoss 0.1949 (0.1020)	MaskDICELoss 0.4352 (0.5131)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 449 is less than current step: 499. Dropping entry: {'train/loss': 1.542698186635971, 'train/ce_loss': 0.43388671875, 'train/seg_cls_loss': 0.0119384765625, 'train/kl_loss': 0.2587890625, 'train/mask_bce_loss': 0.10485764257609845, 'train/mask_dice_loss': 0.43348363786935806, 'train/mask_loss': 0.538341274857521, 'metrics/total_secs_per_batch': 7.193755626678467, 'metrics/data_secs_per_batch': 3.4712240934371947, '_timestamp': 1740963750.7465572}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 449 is less than current step: 499. Dropping entry: {'train/lr': 6.765306122448979e-05, '_timestamp': 1740963750.746829}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 450 is less than current step: 499. Dropping entry: {'train/loss': 1.6819756388664246, 'train/ce_loss': 0.4195556640625, 'train/seg_cls_loss': 0.014007568359375, 'train/kl_loss': 0.25546875, 'train/mask_bce_loss': 0.1020221833139658, 'train/mask_dice_loss': 0.513123345375061, 'train/mask_loss': 0.6151455223560334, 'metrics/total_secs_per_batch': 8.317612648010254, 'metrics/data_secs_per_batch': 3.843220567703247, '_timestamp': 1740963759.0645547}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 450 is less than current step: 499. Dropping entry: {'train/lr': 6.753061224489795e-05, '_timestamp': 1740963759.0648983}).
Epoch: [3][452/500]	Time  8.258 ( 8.258)	Loss 2.5988 (1.8772)	CeLoss 0.1943 (0.4934)	SegCLSLoss 0.0189 (0.0133)	KLLoss 0.3730 (0.2590)	MaskLoss 1.1788 (0.6756)	MaskBCELoss 0.3628 (0.1382)	MaskDICELoss 0.8160 (0.5374)
Epoch: [3][453/500]	Time  9.437 ( 9.437)	Loss 1.8151 (1.4650)	CeLoss 0.2275 (0.3224)	SegCLSLoss 0.0144 (0.0123)	KLLoss 0.3789 (0.2934)	MaskLoss 0.7718 (0.5536)	MaskBCELoss 0.0943 (0.0567)	MaskDICELoss 0.6775 (0.4969)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 451 is less than current step: 499. Dropping entry: {'train/loss': 1.87720547914505, 'train/ce_loss': 0.493359375, 'train/seg_cls_loss': 0.013275146484375, 'train/kl_loss': 0.258984375, 'train/mask_bce_loss': 0.13818637868389488, 'train/mask_dice_loss': 0.5373792678117753, 'train/mask_loss': 0.6755656242370606, 'metrics/total_secs_per_batch': 8.257744312286377, 'metrics/data_secs_per_batch': 4.029832768440246, '_timestamp': 1740963767.322182}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 451 is less than current step: 499. Dropping entry: {'train/lr': 6.740816326530612e-05, '_timestamp': 1740963767.322533}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 452 is less than current step: 499. Dropping entry: {'train/loss': 1.464978563785553, 'train/ce_loss': 0.322412109375, 'train/seg_cls_loss': 0.0122802734375, 'train/kl_loss': 0.293359375, 'train/mask_bce_loss': 0.056703990790992975, 'train/mask_dice_loss': 0.4968546241521835, 'train/mask_loss': 0.553558611869812, 'metrics/total_secs_per_batch': 9.437384605407715, 'metrics/data_secs_per_batch': 4.218095898628235, '_timestamp': 1740963776.7599058}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 452 is less than current step: 499. Dropping entry: {'train/lr': 6.728571428571428e-05, '_timestamp': 1740963776.7603483}).
Epoch: [3][454/500]	Time  6.303 ( 6.303)	Loss 1.5661 (1.8919)	CeLoss 0.2676 (0.5475)	SegCLSLoss 0.0131 (0.0129)	KLLoss 0.3652 (0.2635)	MaskLoss 0.6278 (0.6560)	MaskBCELoss 0.1924 (0.1916)	MaskDICELoss 0.4354 (0.4643)
Epoch: [3][455/500]	Time  8.135 ( 8.135)	Loss 4.0576 (1.6269)	CeLoss 0.3262 (0.2914)	SegCLSLoss 0.0177 (0.0127)	KLLoss 0.3691 (0.2564)	MaskLoss 1.8423 (0.6516)	MaskBCELoss 1.3517 (0.1856)	MaskDICELoss 0.4906 (0.4660)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 453 is less than current step: 499. Dropping entry: {'train/loss': 1.891925823688507, 'train/ce_loss': 0.5474609375, 'train/seg_cls_loss': 0.01285400390625, 'train/kl_loss': 0.2634765625, 'train/mask_bce_loss': 0.19164290875196457, 'train/mask_dice_loss': 0.4643297761678696, 'train/mask_loss': 0.655972671508789, 'metrics/total_secs_per_batch': 6.302958250045776, 'metrics/data_secs_per_batch': 2.950612449645996, '_timestamp': 1740963783.062455}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 453 is less than current step: 499. Dropping entry: {'train/lr': 6.716326530612244e-05, '_timestamp': 1740963783.0627933}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 454 is less than current step: 499. Dropping entry: {'train/loss': 1.626923954486847, 'train/ce_loss': 0.2914306640625, 'train/seg_cls_loss': 0.012725830078125, 'train/kl_loss': 0.2564453125, 'train/mask_bce_loss': 0.18558030426502228, 'train/mask_dice_loss': 0.46600421667099, 'train/mask_loss': 0.6515845239162446, 'metrics/total_secs_per_batch': 8.135226964950562, 'metrics/data_secs_per_batch': 4.006196212768555, '_timestamp': 1740963791.1976674}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 454 is less than current step: 499. Dropping entry: {'train/lr': 6.70408163265306e-05, '_timestamp': 1740963791.1979432}).
Epoch: [3][456/500]	Time  6.493 ( 6.493)	Loss 2.2474 (1.4699)	CeLoss 0.3301 (0.6026)	SegCLSLoss 0.0109 (0.0096)	KLLoss 0.3730 (0.2236)	MaskLoss 0.9372 (0.4198)	MaskBCELoss 0.2769 (0.1152)	MaskDICELoss 0.6602 (0.3046)
Epoch: [3][457/500]	Time  7.779 ( 7.779)	Loss 1.7241 (1.7606)	CeLoss 0.2520 (0.4647)	SegCLSLoss 0.0194 (0.0139)	KLLoss 0.3770 (0.2559)	MaskLoss 0.7126 (0.6316)	MaskBCELoss 0.1170 (0.1700)	MaskDICELoss 0.5956 (0.4616)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 455 is less than current step: 499. Dropping entry: {'train/loss': 1.4699172139167787, 'train/ce_loss': 0.60263671875, 'train/seg_cls_loss': 0.00958251953125, 'train/kl_loss': 0.2236328125, 'train/mask_bce_loss': 0.11521405912935734, 'train/mask_dice_loss': 0.3046078413724899, 'train/mask_loss': 0.41982189416885374, 'metrics/total_secs_per_batch': 6.493185758590698, 'metrics/data_secs_per_batch': 3.002921295166016, '_timestamp': 1740963797.6909316}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 455 is less than current step: 499. Dropping entry: {'train/lr': 6.691836734693877e-05, '_timestamp': 1740963797.691128}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 456 is less than current step: 499. Dropping entry: {'train/loss': 1.7605970323085784, 'train/ce_loss': 0.46474609375, 'train/seg_cls_loss': 0.013909912109375, 'train/kl_loss': 0.255859375, 'train/mask_bce_loss': 0.17001864239573478, 'train/mask_dice_loss': 0.46159822642803194, 'train/mask_loss': 0.6316168755292892, 'metrics/total_secs_per_batch': 7.778588533401489, 'metrics/data_secs_per_batch': 3.4495105743408203, '_timestamp': 1740963805.4696264}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 456 is less than current step: 499. Dropping entry: {'train/lr': 6.679591836734693e-05, '_timestamp': 1740963805.4699543}).
Epoch: [3][458/500]	Time  7.600 ( 7.600)	Loss 1.5484 (1.3371)	CeLoss 0.2441 (0.2906)	SegCLSLoss 0.0144 (0.0108)	KLLoss 0.3672 (0.2586)	MaskLoss 0.6297 (0.5077)	MaskBCELoss 0.0177 (0.0600)	MaskDICELoss 0.6120 (0.4477)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 457 is less than current step: 499. Dropping entry: {'train/loss': 1.337134599685669, 'train/ce_loss': 0.290625, 'train/seg_cls_loss': 0.010784912109375, 'train/kl_loss': 0.25859375, 'train/mask_bce_loss': 0.060030149109661576, 'train/mask_dice_loss': 0.44769730865955354, 'train/mask_loss': 0.5077274590730667, 'metrics/total_secs_per_batch': 7.59975790977478, 'metrics/data_secs_per_batch': 3.344744420051575, '_timestamp': 1740963813.0692036}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 457 is less than current step: 499. Dropping entry: {'train/lr': 6.66734693877551e-05, '_timestamp': 1740963813.0695128}).
Epoch: [3][459/500]	Time  9.396 ( 9.396)	Loss 2.7644 (1.9007)	CeLoss 0.3203 (0.4706)	SegCLSLoss 0.0190 (0.0140)	KLLoss 0.3535 (0.2930)	MaskLoss 1.1996 (0.6970)	MaskBCELoss 0.4747 (0.1484)	MaskDICELoss 0.7248 (0.5486)
[2025-03-02 19:03:50,969] [INFO] [logging.py:96:log_dist] [Rank 0] step=1960, skipped=0, lr=[6.648979591836734e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:03:50,975] [INFO] [timer.py:215:stop] epoch=0/micro_step=19600/global_step=1960, RunningAvgSamplesPerSec=1.49967070044739, CurrSamplesPerSec=1.175083951898004, MemAllocated=31.38GB, MaxMemAllocated=37.19GB
Epoch: [3][460/500]	Time  8.511 ( 8.511)	Loss 1.7136 (1.8266)	CeLoss 0.2461 (0.3568)	SegCLSLoss 0.0143 (0.0139)	KLLoss 0.3672 (0.2947)	MaskLoss 0.7113 (0.7167)	MaskBCELoss 0.1961 (0.1518)	MaskDICELoss 0.5152 (0.5649)
Epoch: [3][461/500]	Time  6.554 ( 6.554)	Loss 2.4164 (1.5872)	CeLoss 0.2617 (0.5927)	SegCLSLoss 0.0157 (0.0089)	KLLoss 0.3711 (0.2213)	MaskLoss 1.0549 (0.4839)	MaskBCELoss 0.3024 (0.1246)	MaskDICELoss 0.7525 (0.3594)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 458 is less than current step: 499. Dropping entry: {'train/loss': 1.9007330536842346, 'train/ce_loss': 0.47060546875, 'train/seg_cls_loss': 0.01395263671875, 'train/kl_loss': 0.29296875, 'train/mask_bce_loss': 0.1483516961336136, 'train/mask_dice_loss': 0.5486456900835037, 'train/mask_loss': 0.6969973832368851, 'metrics/total_secs_per_batch': 9.39556622505188, 'metrics/data_secs_per_batch': 3.509764885902405, '_timestamp': 1740963822.464705}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 458 is less than current step: 499. Dropping entry: {'train/lr': 6.655102040816326e-05, '_timestamp': 1740963822.464894}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 459 is less than current step: 499. Dropping entry: {'train/loss': 1.82657732963562, 'train/ce_loss': 0.3568359375, 'train/seg_cls_loss': 0.013873291015625, 'train/kl_loss': 0.2947265625, 'train/mask_bce_loss': 0.15179210361093282, 'train/mask_dice_loss': 0.5648657113313675, 'train/mask_loss': 0.7166578114032746, 'metrics/total_secs_per_batch': 8.511417865753174, 'metrics/data_secs_per_batch': 3.4107062101364134, '_timestamp': 1740963830.9760206}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 459 is less than current step: 499. Dropping entry: {'train/lr': 6.642857142857142e-05, '_timestamp': 1740963830.9762833}).
Epoch: [3][462/500]	Time  7.518 ( 7.518)	Loss 2.0867 (1.8344)	CeLoss 0.2578 (0.2290)	SegCLSLoss 0.0150 (0.0183)	KLLoss 0.3672 (0.3686)	MaskLoss 0.8920 (0.7795)	MaskBCELoss 0.0097 (0.1794)	MaskDICELoss 0.8823 (0.6001)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 460 is less than current step: 499. Dropping entry: {'train/loss': 1.5871624231338501, 'train/ce_loss': 0.59267578125, 'train/seg_cls_loss': 0.008935546875, 'train/kl_loss': 0.2212890625, 'train/mask_bce_loss': 0.12456315215677023, 'train/mask_dice_loss': 0.3593500882387161, 'train/mask_loss': 0.4839132398366928, 'metrics/total_secs_per_batch': 6.554159641265869, 'metrics/data_secs_per_batch': 2.6713040828704835, '_timestamp': 1740963837.5320718}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 460 is less than current step: 499. Dropping entry: {'train/lr': 6.630612244897958e-05, '_timestamp': 1740963837.5325975}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 461 is less than current step: 499. Dropping entry: {'train/loss': 1.8343687176704406, 'train/ce_loss': 0.228955078125, 'train/seg_cls_loss': 0.01827392578125, 'train/kl_loss': 0.3685546875, 'train/mask_bce_loss': 0.1794407275505364, 'train/mask_dice_loss': 0.6000971615314483, 'train/mask_loss': 0.7795378714799881, 'metrics/total_secs_per_batch': 7.517827272415161, 'metrics/data_secs_per_batch': 3.2126776218414306, '_timestamp': 1740963845.0482275}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 461 is less than current step: 499. Dropping entry: {'train/lr': 6.618367346938775e-05, '_timestamp': 1740963845.0485284}).
Epoch: [3][463/500]	Time  7.675 ( 7.675)	Loss 1.5391 (1.5066)	CeLoss 1.5391 (0.6110)	SegCLSLoss 0.0000 (0.0114)	KLLoss 0.0000 (0.2189)	MaskLoss 0.0000 (0.4339)	MaskBCELoss 0.0000 (0.0816)	MaskDICELoss 0.0000 (0.3522)
Epoch: [3][464/500]	Time  8.094 ( 8.094)	Loss 2.0841 (1.3827)	CeLoss 0.1631 (0.2665)	SegCLSLoss 0.0227 (0.0167)	KLLoss 0.3652 (0.2969)	MaskLoss 0.9366 (0.5390)	MaskBCELoss 0.0085 (0.0981)	MaskDICELoss 0.9280 (0.4409)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 462 is less than current step: 499. Dropping entry: {'train/loss': 1.506646990776062, 'train/ce_loss': 0.61103515625, 'train/seg_cls_loss': 0.011444091796875, 'train/kl_loss': 0.2189453125, 'train/mask_bce_loss': 0.08164281509816647, 'train/mask_dice_loss': 0.3522470936179161, 'train/mask_loss': 0.433889901638031, 'metrics/total_secs_per_batch': 7.674667596817017, 'metrics/data_secs_per_batch': 3.8124053955078123, '_timestamp': 1740963852.7228153}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 462 is less than current step: 499. Dropping entry: {'train/lr': 6.606122448979591e-05, '_timestamp': 1740963852.7230635}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 463 is less than current step: 499. Dropping entry: {'train/loss': 1.3827355265617371, 'train/ce_loss': 0.26650390625, 'train/seg_cls_loss': 0.0166748046875, 'train/kl_loss': 0.296875, 'train/mask_bce_loss': 0.0981108914129436, 'train/mask_dice_loss': 0.4408642828464508, 'train/mask_loss': 0.5389751821756363, 'metrics/total_secs_per_batch': 8.093955993652344, 'metrics/data_secs_per_batch': 3.5263591527938845, '_timestamp': 1740963860.8168106}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 463 is less than current step: 499. Dropping entry: {'train/lr': 6.593877551020407e-05, '_timestamp': 1740963860.8170753}).
Epoch: [3][465/500]	Time  8.499 ( 8.499)	Loss 2.4070 (1.8080)	CeLoss 0.3984 (0.2355)	SegCLSLoss 0.0177 (0.0179)	KLLoss 0.3730 (0.3336)	MaskLoss 0.9808 (0.7651)	MaskBCELoss 0.0043 (0.1688)	MaskDICELoss 0.9765 (0.5963)
Epoch: [3][466/500]	Time  7.815 ( 7.815)	Loss 2.2845 (1.3371)	CeLoss 0.1719 (0.4476)	SegCLSLoss 0.0332 (0.0112)	KLLoss 0.3770 (0.2227)	MaskLoss 1.0290 (0.4307)	MaskBCELoss 0.1635 (0.0623)	MaskDICELoss 0.8655 (0.3685)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 464 is less than current step: 499. Dropping entry: {'train/loss': 1.808037781715393, 'train/ce_loss': 0.235498046875, 'train/seg_cls_loss': 0.0179443359375, 'train/kl_loss': 0.33359375, 'train/mask_bce_loss': 0.16880602920427917, 'train/mask_dice_loss': 0.5962724059820175, 'train/mask_loss': 0.7650784492492676, 'metrics/total_secs_per_batch': 8.499095678329468, 'metrics/data_secs_per_batch': 3.640105676651001, '_timestamp': 1740963869.315928}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 464 is less than current step: 499. Dropping entry: {'train/lr': 6.581632653061224e-05, '_timestamp': 1740963869.3161926}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 465 is less than current step: 499. Dropping entry: {'train/loss': 1.3371375441551208, 'train/ce_loss': 0.44755859375, 'train/seg_cls_loss': 0.01124267578125, 'train/kl_loss': 0.22265625, 'train/mask_bce_loss': 0.06225045453757048, 'train/mask_dice_loss': 0.3684765130281448, 'train/mask_loss': 0.4307269752025604, 'metrics/total_secs_per_batch': 7.815032005310059, 'metrics/data_secs_per_batch': 3.352072525024414, '_timestamp': 1740963877.1309464}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 465 is less than current step: 499. Dropping entry: {'train/lr': 6.56938775510204e-05, '_timestamp': 1740963877.1312213}).
Epoch: [3][467/500]	Time  7.621 ( 7.621)	Loss 2.1856 (1.6390)	CeLoss 0.2539 (0.4428)	SegCLSLoss 0.0095 (0.0107)	KLLoss 0.3711 (0.2562)	MaskLoss 0.9453 (0.5826)	MaskBCELoss 0.1199 (0.0973)	MaskDICELoss 0.8254 (0.4853)
Epoch: [3][468/500]	Time  8.871 ( 8.871)	Loss 2.1201 (1.8451)	CeLoss 0.2031 (0.3146)	SegCLSLoss 0.0176 (0.0160)	KLLoss 0.3691 (0.3295)	MaskLoss 0.9355 (0.7448)	MaskBCELoss 0.0557 (0.1390)	MaskDICELoss 0.8799 (0.6058)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 466 is less than current step: 499. Dropping entry: {'train/loss': 1.6389663934707641, 'train/ce_loss': 0.442822265625, 'train/seg_cls_loss': 0.0106689453125, 'train/kl_loss': 0.25625, 'train/mask_bce_loss': 0.09726223973557353, 'train/mask_dice_loss': 0.4853313237428665, 'train/mask_loss': 0.5825935572385788, 'metrics/total_secs_per_batch': 7.621489763259888, 'metrics/data_secs_per_batch': 3.5205894470214845, '_timestamp': 1740963884.7525375}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 466 is less than current step: 499. Dropping entry: {'train/lr': 6.557142857142858e-05, '_timestamp': 1740963884.7529175}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 467 is less than current step: 499. Dropping entry: {'train/loss': 1.8450767517089843, 'train/ce_loss': 0.3146484375, 'train/seg_cls_loss': 0.0160400390625, 'train/kl_loss': 0.3294921875, 'train/mask_bce_loss': 0.13896717596799135, 'train/mask_dice_loss': 0.6057879894971847, 'train/mask_loss': 0.7447551727294922, 'metrics/total_secs_per_batch': 8.870959520339966, 'metrics/data_secs_per_batch': 3.7272591590881348, '_timestamp': 1740963893.6234586}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 467 is less than current step: 499. Dropping entry: {'train/lr': 6.544897959183673e-05, '_timestamp': 1740963893.623753}).
Epoch: [3][469/500]	Time  7.512 ( 7.512)	Loss 2.5094 (1.9908)	CeLoss 0.1758 (0.4415)	SegCLSLoss 0.0242 (0.0173)	KLLoss 0.3789 (0.2969)	MaskLoss 1.1419 (0.7554)	MaskBCELoss 0.2178 (0.1994)	MaskDICELoss 0.9241 (0.5561)
[2025-03-02 19:05:09,362] [INFO] [logging.py:96:log_dist] [Rank 0] step=1970, skipped=0, lr=[6.526530612244897e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:05:09,368] [INFO] [timer.py:215:stop] epoch=0/micro_step=19700/global_step=1970, RunningAvgSamplesPerSec=1.4983350239861748, CurrSamplesPerSec=1.214699415686205, MemAllocated=31.23GB, MaxMemAllocated=37.19GB
Epoch: [3][470/500]	Time  8.234 ( 8.234)	Loss 1.4199 (1.7374)	CeLoss 0.2461 (0.3756)	SegCLSLoss 0.0114 (0.0163)	KLLoss 0.3711 (0.2965)	MaskLoss 0.5654 (0.6620)	MaskBCELoss 0.1335 (0.1156)	MaskDICELoss 0.4319 (0.5464)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 468 is less than current step: 499. Dropping entry: {'train/loss': 1.9907623887062074, 'train/ce_loss': 0.44150390625, 'train/seg_cls_loss': 0.01734619140625, 'train/kl_loss': 0.296875, 'train/mask_bce_loss': 0.19937852174043655, 'train/mask_dice_loss': 0.5560612618923187, 'train/mask_loss': 0.7554397881031036, 'metrics/total_secs_per_batch': 7.51195502281189, 'metrics/data_secs_per_batch': 3.3098189353942873, '_timestamp': 1740963901.1353915}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 468 is less than current step: 499. Dropping entry: {'train/lr': 6.532653061224489e-05, '_timestamp': 1740963901.1356733}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 469 is less than current step: 499. Dropping entry: {'train/loss': 1.737446242570877, 'train/ce_loss': 0.3755859375, 'train/seg_cls_loss': 0.0162841796875, 'train/kl_loss': 0.296484375, 'train/mask_bce_loss': 0.1156087575480342, 'train/mask_dice_loss': 0.5464249044656754, 'train/mask_loss': 0.662033674120903, 'metrics/total_secs_per_batch': 8.234004259109497, 'metrics/data_secs_per_batch': 3.751551628112793, '_timestamp': 1740963909.3692753}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 469 is less than current step: 499. Dropping entry: {'train/lr': 6.520408163265305e-05, '_timestamp': 1740963909.3696115}).
Epoch: [3][471/500]	Time  7.110 ( 7.110)	Loss 1.2882 (1.6712)	CeLoss 0.3086 (0.4659)	SegCLSLoss 0.0119 (0.0107)	KLLoss 0.3711 (0.2584)	MaskLoss 0.4683 (0.5869)	MaskBCELoss 0.0892 (0.2144)	MaskDICELoss 0.3791 (0.3725)
Epoch: [3][472/500]	Time  7.759 ( 7.759)	Loss 1.8292 (1.6145)	CeLoss 0.2217 (0.4063)	SegCLSLoss 0.0234 (0.0146)	KLLoss 0.3672 (0.2561)	MaskLoss 0.7798 (0.5875)	MaskBCELoss 0.0187 (0.0594)	MaskDICELoss 0.7612 (0.5281)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 470 is less than current step: 499. Dropping entry: {'train/loss': 1.6711812973022462, 'train/ce_loss': 0.46591796875, 'train/seg_cls_loss': 0.010723876953125, 'train/kl_loss': 0.2583984375, 'train/mask_bce_loss': 0.21436895206570625, 'train/mask_dice_loss': 0.3724912226200104, 'train/mask_loss': 0.5868601858615875, 'metrics/total_secs_per_batch': 7.10974645614624, 'metrics/data_secs_per_batch': 3.0229603052139282, '_timestamp': 1740963916.479136}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 470 is less than current step: 499. Dropping entry: {'train/lr': 6.508163265306121e-05, '_timestamp': 1740963916.4793305}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 471 is less than current step: 499. Dropping entry: {'train/loss': 1.6144738793373108, 'train/ce_loss': 0.40634765625, 'train/seg_cls_loss': 0.0145751953125, 'train/kl_loss': 0.2560546875, 'train/mask_bce_loss': 0.05944566410034895, 'train/mask_dice_loss': 0.5280647069215775, 'train/mask_loss': 0.5875103622674942, 'metrics/total_secs_per_batch': 7.759054183959961, 'metrics/data_secs_per_batch': 3.5257953882217405, '_timestamp': 1740963924.238272}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 471 is less than current step: 499. Dropping entry: {'train/lr': 6.495918367346938e-05, '_timestamp': 1740963924.2386158}).
Epoch: [3][473/500]	Time  8.135 ( 8.135)	Loss 0.8158 (1.5483)	CeLoss 0.2295 (0.4500)	SegCLSLoss 0.0148 (0.0145)	KLLoss 0.3711 (0.2580)	MaskLoss 0.2712 (0.5327)	MaskBCELoss 0.1639 (0.1235)	MaskDICELoss 0.1073 (0.4092)
Epoch: [3][474/500]	Time  8.650 ( 8.650)	Loss 1.6175 (1.2729)	CeLoss 0.2891 (0.4392)	SegCLSLoss 0.0110 (0.0088)	KLLoss 0.3809 (0.2246)	MaskLoss 0.6428 (0.4036)	MaskBCELoss 0.0661 (0.0422)	MaskDICELoss 0.5767 (0.3614)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 472 is less than current step: 499. Dropping entry: {'train/loss': 1.5483492016792297, 'train/ce_loss': 0.450048828125, 'train/seg_cls_loss': 0.014459228515625, 'train/kl_loss': 0.2580078125, 'train/mask_bce_loss': 0.12347834296524525, 'train/mask_dice_loss': 0.40921675115823747, 'train/mask_loss': 0.5326950907707214, 'metrics/total_secs_per_batch': 8.135462045669556, 'metrics/data_secs_per_batch': 4.507681131362915, '_timestamp': 1740963932.3737154}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 472 is less than current step: 499. Dropping entry: {'train/lr': 6.483673469387755e-05, '_timestamp': 1740963932.3740342}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 473 is less than current step: 499. Dropping entry: {'train/loss': 1.272936511039734, 'train/ce_loss': 0.43916015625, 'train/seg_cls_loss': 0.008831787109375, 'train/kl_loss': 0.224609375, 'train/mask_bce_loss': 0.04219104768708348, 'train/mask_dice_loss': 0.36141587793827057, 'train/mask_loss': 0.4036069273948669, 'metrics/total_secs_per_batch': 8.650479555130005, 'metrics/data_secs_per_batch': 4.199749541282654, '_timestamp': 1740963941.0241582}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 473 is less than current step: 499. Dropping entry: {'train/lr': 6.47142857142857e-05, '_timestamp': 1740963941.0245194}).
Epoch: [3][475/500]	Time  8.806 ( 8.806)	Loss 1.3273 (1.7275)	CeLoss 0.1963 (0.2579)	SegCLSLoss 0.0182 (0.0150)	KLLoss 0.3730 (0.3709)	MaskLoss 0.5425 (0.7126)	MaskBCELoss 0.0569 (0.1821)	MaskDICELoss 0.4856 (0.5305)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 474 is less than current step: 499. Dropping entry: {'train/loss': 1.7275286674499513, 'train/ce_loss': 0.25791015625, 'train/seg_cls_loss': 0.015045166015625, 'train/kl_loss': 0.3708984375, 'train/mask_bce_loss': 0.18212189450860022, 'train/mask_dice_loss': 0.5304705709218979, 'train/mask_loss': 0.71259246468544, 'metrics/total_secs_per_batch': 8.805911540985107, 'metrics/data_secs_per_batch': 3.719368052482605, '_timestamp': 1740963949.830282}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 474 is less than current step: 499. Dropping entry: {'train/lr': 6.459183673469387e-05, '_timestamp': 1740963949.8306274}).
Epoch: [3][476/500]	Time  9.604 ( 9.604)	Loss 1.5991 (1.6280)	CeLoss 0.1816 (0.4253)	SegCLSLoss 0.0244 (0.0155)	KLLoss 0.3809 (0.2957)	MaskLoss 0.6838 (0.5827)	MaskBCELoss 0.0668 (0.1018)	MaskDICELoss 0.6170 (0.4809)
Epoch: [3][477/500]	Time  8.602 ( 8.602)	Loss 2.4070 (1.8501)	CeLoss 0.1279 (0.3223)	SegCLSLoss 0.0304 (0.0202)	KLLoss 0.3711 (0.3355)	MaskLoss 1.1132 (0.7420)	MaskBCELoss 0.2250 (0.1908)	MaskDICELoss 0.8882 (0.5512)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 475 is less than current step: 499. Dropping entry: {'train/loss': 1.628014361858368, 'train/ce_loss': 0.42529296875, 'train/seg_cls_loss': 0.0155029296875, 'train/kl_loss': 0.295703125, 'train/mask_bce_loss': 0.10182554386556149, 'train/mask_dice_loss': 0.48088280856609344, 'train/mask_loss': 0.5827083557844162, 'metrics/total_secs_per_batch': 9.604307651519775, 'metrics/data_secs_per_batch': 4.359608173370361, '_timestamp': 1740963959.4344475}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 475 is less than current step: 499. Dropping entry: {'train/lr': 6.446938775510203e-05, '_timestamp': 1740963959.434783}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 476 is less than current step: 499. Dropping entry: {'train/loss': 1.850143176317215, 'train/ce_loss': 0.322265625, 'train/seg_cls_loss': 0.020196533203125, 'train/kl_loss': 0.335546875, 'train/mask_bce_loss': 0.19082975387573242, 'train/mask_dice_loss': 0.5511851981282234, 'train/mask_loss': 0.7420149445533752, 'metrics/total_secs_per_batch': 8.601831912994385, 'metrics/data_secs_per_batch': 3.401087021827698, '_timestamp': 1740963968.036208}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 476 is less than current step: 499. Dropping entry: {'train/lr': 6.434693877551019e-05, '_timestamp': 1740963968.036482}).
Epoch: [3][478/500]	Time  7.965 ( 7.965)	Loss 1.9958 (1.8118)	CeLoss 0.2285 (0.4206)	SegCLSLoss 0.0179 (0.0148)	KLLoss 0.3652 (0.2930)	MaskLoss 0.8612 (0.6773)	MaskBCELoss 0.0224 (0.1510)	MaskDICELoss 0.8388 (0.5263)
Epoch: [3][479/500]	Time  7.872 ( 7.872)	Loss 2.5708 (1.7077)	CeLoss 0.2754 (0.3509)	SegCLSLoss 0.0101 (0.0166)	KLLoss 0.3711 (0.2980)	MaskLoss 1.1262 (0.6593)	MaskBCELoss 0.4732 (0.1329)	MaskDICELoss 0.6530 (0.5264)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 477 is less than current step: 499. Dropping entry: {'train/loss': 1.811769223213196, 'train/ce_loss': 0.420556640625, 'train/seg_cls_loss': 0.014794921875, 'train/kl_loss': 0.29296875, 'train/mask_bce_loss': 0.1509729778394103, 'train/mask_dice_loss': 0.5262983709573745, 'train/mask_loss': 0.6772713527083397, 'metrics/total_secs_per_batch': 7.964934587478638, 'metrics/data_secs_per_batch': 2.9342686414718626, '_timestamp': 1740963976.0012362}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 477 is less than current step: 499. Dropping entry: {'train/lr': 6.422448979591837e-05, '_timestamp': 1740963976.0015721}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 478 is less than current step: 499. Dropping entry: {'train/loss': 1.7077374458312988, 'train/ce_loss': 0.35087890625, 'train/seg_cls_loss': 0.01663818359375, 'train/kl_loss': 0.298046875, 'train/mask_bce_loss': 0.1328978785313666, 'train/mask_dice_loss': 0.5264395877718926, 'train/mask_loss': 0.6593374669551849, 'metrics/total_secs_per_batch': 7.87237548828125, 'metrics/data_secs_per_batch': 3.2906137704849243, '_timestamp': 1740963983.8735158}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 478 is less than current step: 499. Dropping entry: {'train/lr': 6.410204081632653e-05, '_timestamp': 1740963983.873785}).
[2025-03-02 19:06:30,279] [INFO] [logging.py:96:log_dist] [Rank 0] step=1980, skipped=0, lr=[6.404081632653061e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:06:30,286] [INFO] [timer.py:215:stop] epoch=0/micro_step=19800/global_step=1980, RunningAvgSamplesPerSec=1.4967291164410845, CurrSamplesPerSec=1.5595852621917523, MemAllocated=30.84GB, MaxMemAllocated=37.19GB
Epoch: [3][480/500]	Time  6.413 ( 6.413)	Loss 1.2500 (1.5929)	CeLoss 1.2500 (0.6468)	SegCLSLoss 0.0000 (0.0107)	KLLoss 0.0000 (0.1820)	MaskLoss 0.0000 (0.4612)	MaskBCELoss 0.0000 (0.1093)	MaskDICELoss 0.0000 (0.3519)
Epoch: [3][481/500]	Time  7.470 ( 7.470)	Loss 2.1751 (1.2490)	CeLoss 0.2490 (0.4049)	SegCLSLoss 0.0200 (0.0096)	KLLoss 0.3574 (0.2234)	MaskLoss 0.9401 (0.4084)	MaskBCELoss 0.0433 (0.0484)	MaskDICELoss 0.8968 (0.3600)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 479 is less than current step: 499. Dropping entry: {'train/loss': 1.5929359436035155, 'train/ce_loss': 0.646826171875, 'train/seg_cls_loss': 0.0107177734375, 'train/kl_loss': 0.18203125, 'train/mask_bce_loss': 0.1092925813049078, 'train/mask_dice_loss': 0.35194588601589205, 'train/mask_loss': 0.46123847663402556, 'metrics/total_secs_per_batch': 6.413431406021118, 'metrics/data_secs_per_batch': 3.1134616851806642, '_timestamp': 1740963990.2867439}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 479 is less than current step: 499. Dropping entry: {'train/lr': 6.39795918367347e-05, '_timestamp': 1740963990.2869222}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 480 is less than current step: 499. Dropping entry: {'train/loss': 1.24896040558815, 'train/ce_loss': 0.404931640625, 'train/seg_cls_loss': 0.0096435546875, 'train/kl_loss': 0.2234375, 'train/mask_bce_loss': 0.048428009077906606, 'train/mask_dice_loss': 0.3599633127450943, 'train/mask_loss': 0.40839131623506547, 'metrics/total_secs_per_batch': 7.4701247215271, 'metrics/data_secs_per_batch': 3.2027252197265623, '_timestamp': 1740963997.7571373}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 480 is less than current step: 499. Dropping entry: {'train/lr': 6.385714285714284e-05, '_timestamp': 1740963997.7574914}).
Epoch: [3][482/500]	Time  6.595 ( 6.595)	Loss 1.8059 (1.6279)	CeLoss 0.2383 (0.5202)	SegCLSLoss 0.0134 (0.0114)	KLLoss 0.3691 (0.2576)	MaskLoss 0.7623 (0.5380)	MaskBCELoss 0.4954 (0.1482)	MaskDICELoss 0.2669 (0.3898)
Epoch: [3][483/500]	Time  7.613 ( 7.613)	Loss 1.3016 (1.7481)	CeLoss 0.2852 (0.3763)	SegCLSLoss 0.0214 (0.0166)	KLLoss 0.3594 (0.3336)	MaskLoss 0.4858 (0.6650)	MaskBCELoss 0.0915 (0.2225)	MaskDICELoss 0.3943 (0.4425)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 481 is less than current step: 499. Dropping entry: {'train/loss': 1.627859103679657, 'train/ce_loss': 0.52021484375, 'train/seg_cls_loss': 0.0114013671875, 'train/kl_loss': 0.2576171875, 'train/mask_bce_loss': 0.1481750164180994, 'train/mask_dice_loss': 0.3898268029093742, 'train/mask_loss': 0.538001823425293, 'metrics/total_secs_per_batch': 6.594954967498779, 'metrics/data_secs_per_batch': 3.2668206214904787, '_timestamp': 1740964004.35208}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 481 is less than current step: 499. Dropping entry: {'train/lr': 6.373469387755101e-05, '_timestamp': 1740964004.3523777}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 482 is less than current step: 499. Dropping entry: {'train/loss': 1.7480650961399078, 'train/ce_loss': 0.37626953125, 'train/seg_cls_loss': 0.016619873046875, 'train/kl_loss': 0.33359375, 'train/mask_bce_loss': 0.22251930143684148, 'train/mask_dice_loss': 0.4425288803875446, 'train/mask_loss': 0.6650481790304184, 'metrics/total_secs_per_batch': 7.613236427307129, 'metrics/data_secs_per_batch': 3.5805506467819215, '_timestamp': 1740964011.96525}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 482 is less than current step: 499. Dropping entry: {'train/lr': 6.361224489795917e-05, '_timestamp': 1740964011.9655044}).
Epoch: [3][484/500]	Time  8.367 ( 8.367)	Loss 1.4839 (1.4319)	CeLoss 0.2871 (0.3229)	SegCLSLoss 0.0134 (0.0129)	KLLoss 0.3711 (0.2566)	MaskLoss 0.5769 (0.5384)	MaskBCELoss 0.0209 (0.1136)	MaskDICELoss 0.5560 (0.4248)
Epoch: [3][485/500]	Time  7.231 ( 7.231)	Loss 1.2109 (1.5453)	CeLoss 1.2109 (0.6926)	SegCLSLoss 0.0000 (0.0107)	KLLoss 0.0000 (0.2223)	MaskLoss 0.0000 (0.4126)	MaskBCELoss 0.0000 (0.0741)	MaskDICELoss 0.0000 (0.3385)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 483 is less than current step: 499. Dropping entry: {'train/loss': 1.4319241642951965, 'train/ce_loss': 0.32294921875, 'train/seg_cls_loss': 0.0128662109375, 'train/kl_loss': 0.256640625, 'train/mask_bce_loss': 0.11359568201005459, 'train/mask_dice_loss': 0.4248273178935051, 'train/mask_loss': 0.5384230136871337, 'metrics/total_secs_per_batch': 8.366840124130249, 'metrics/data_secs_per_batch': 3.02579607963562, '_timestamp': 1740964020.3321}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 483 is less than current step: 499. Dropping entry: {'train/lr': 6.348979591836735e-05, '_timestamp': 1740964020.33236}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 484 is less than current step: 499. Dropping entry: {'train/loss': 1.545326542854309, 'train/ce_loss': 0.692578125, 'train/seg_cls_loss': 0.0107177734375, 'train/kl_loss': 0.222265625, 'train/mask_bce_loss': 0.07406753804534674, 'train/mask_dice_loss': 0.3385371401906013, 'train/mask_loss': 0.4126046821475029, 'metrics/total_secs_per_batch': 7.23115611076355, 'metrics/data_secs_per_batch': 3.525188112258911, '_timestamp': 1740964027.5632663}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 484 is less than current step: 499. Dropping entry: {'train/lr': 6.336734693877551e-05, '_timestamp': 1740964027.5635417}).
Epoch: [3][486/500]	Time  6.555 ( 6.555)	Loss 1.3047 (1.7877)	CeLoss 1.3047 (0.5280)	SegCLSLoss 0.0000 (0.0132)	KLLoss 0.0000 (0.2594)	MaskLoss 0.0000 (0.6136)	MaskBCELoss 0.0000 (0.1528)	MaskDICELoss 0.0000 (0.4608)
Epoch: [3][487/500]	Time  6.728 ( 6.728)	Loss 1.1172 (1.7196)	CeLoss 1.1172 (0.4523)	SegCLSLoss 0.0000 (0.0139)	KLLoss 0.0000 (0.2566)	MaskLoss 0.0000 (0.6174)	MaskBCELoss 0.0000 (0.1176)	MaskDICELoss 0.0000 (0.4998)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 485 is less than current step: 499. Dropping entry: {'train/loss': 1.7877005279064178, 'train/ce_loss': 0.52802734375, 'train/seg_cls_loss': 0.013165283203125, 'train/kl_loss': 0.259375, 'train/mask_bce_loss': 0.15284220464527606, 'train/mask_dice_loss': 0.46078345030546186, 'train/mask_loss': 0.6136256426572799, 'metrics/total_secs_per_batch': 6.555454730987549, 'metrics/data_secs_per_batch': 2.6174949407577515, '_timestamp': 1740964034.1187904}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 485 is less than current step: 499. Dropping entry: {'train/lr': 6.324489795918367e-05, '_timestamp': 1740964034.1191065}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 486 is less than current step: 499. Dropping entry: {'train/loss': 1.7196167707443237, 'train/ce_loss': 0.45234375, 'train/seg_cls_loss': 0.013916015625, 'train/kl_loss': 0.256640625, 'train/mask_bce_loss': 0.11759986560791731, 'train/mask_dice_loss': 0.4998257040977478, 'train/mask_loss': 0.6174255728721618, 'metrics/total_secs_per_batch': 6.727797269821167, 'metrics/data_secs_per_batch': 3.365956926345825, '_timestamp': 1740964040.8465817}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 486 is less than current step: 499. Dropping entry: {'train/lr': 6.312244897959184e-05, '_timestamp': 1740964040.8468745}).
Epoch: [3][488/500]	Time  9.324 ( 9.324)	Loss 2.1268 (1.6001)	CeLoss 0.2598 (0.2721)	SegCLSLoss 0.0208 (0.0134)	KLLoss 0.3633 (0.3320)	MaskLoss 0.9101 (0.6439)	MaskBCELoss 0.1092 (0.1107)	MaskDICELoss 0.8009 (0.5332)
Epoch: [3][489/500]	Time  7.279 ( 7.279)	Loss 0.0583 (1.6071)	CeLoss 0.0583 (0.4391)	SegCLSLoss 0.0000 (0.0150)	KLLoss 0.0000 (0.2209)	MaskLoss 0.0000 (0.5692)	MaskBCELoss 0.0000 (0.1146)	MaskDICELoss 0.0000 (0.4546)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 487 is less than current step: 499. Dropping entry: {'train/loss': 1.600050014257431, 'train/ce_loss': 0.2720703125, 'train/seg_cls_loss': 0.0134033203125, 'train/kl_loss': 0.33203125, 'train/mask_bce_loss': 0.1106954539194703, 'train/mask_dice_loss': 0.5332260429859161, 'train/mask_loss': 0.64392149746418, 'metrics/total_secs_per_batch': 9.323599338531494, 'metrics/data_secs_per_batch': 4.4494869947433475, '_timestamp': 1740964050.1701458}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 487 is less than current step: 499. Dropping entry: {'train/lr': 6.299999999999999e-05, '_timestamp': 1740964050.1704552}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 488 is less than current step: 499. Dropping entry: {'train/loss': 1.6071425318717956, 'train/ce_loss': 0.4391357421875, 'train/seg_cls_loss': 0.0150390625, 'train/kl_loss': 0.2208984375, 'train/mask_bce_loss': 0.114576381072402, 'train/mask_dice_loss': 0.4546320915222168, 'train/mask_loss': 0.5692084848880767, 'metrics/total_secs_per_batch': 7.279309034347534, 'metrics/data_secs_per_batch': 2.983712840080261, '_timestamp': 1740964057.4494555}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 488 is less than current step: 499. Dropping entry: {'train/lr': 6.287755102040815e-05, '_timestamp': 1740964057.4497209}).
[2025-03-02 19:07:45,771] [INFO] [logging.py:96:log_dist] [Rank 0] step=1990, skipped=0, lr=[6.281632653061224e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:07:45,777] [INFO] [timer.py:215:stop] epoch=0/micro_step=19900/global_step=1990, RunningAvgSamplesPerSec=1.4957530848022227, CurrSamplesPerSec=1.2008801808003942, MemAllocated=31.09GB, MaxMemAllocated=37.19GB
Epoch: [3][490/500]	Time  8.329 ( 8.329)	Loss 1.0712 (1.6251)	CeLoss 0.2520 (0.4309)	SegCLSLoss 0.0123 (0.0114)	KLLoss 0.3711 (0.2615)	MaskLoss 0.3882 (0.5813)	MaskBCELoss 0.1835 (0.1875)	MaskDICELoss 0.2046 (0.3938)
Epoch: [3][491/500]	Time  7.473 ( 7.473)	Loss 1.0781 (1.5265)	CeLoss 1.0781 (0.4615)	SegCLSLoss 0.0000 (0.0128)	KLLoss 0.0000 (0.2221)	MaskLoss 0.0000 (0.5183)	MaskBCELoss 0.0000 (0.0813)	MaskDICELoss 0.0000 (0.4370)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 489 is less than current step: 499. Dropping entry: {'train/loss': 1.625107479095459, 'train/ce_loss': 0.430859375, 'train/seg_cls_loss': 0.011383056640625, 'train/kl_loss': 0.2615234375, 'train/mask_bce_loss': 0.18747910894453526, 'train/mask_dice_loss': 0.39377579391002654, 'train/mask_loss': 0.5812548995018005, 'metrics/total_secs_per_batch': 8.328739881515503, 'metrics/data_secs_per_batch': 3.70276825428009, '_timestamp': 1740964065.778018}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 489 is less than current step: 499. Dropping entry: {'train/lr': 6.275510204081633e-05, '_timestamp': 1740964065.7782717}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 490 is less than current step: 499. Dropping entry: {'train/loss': 1.5265369176864625, 'train/ce_loss': 0.461474609375, 'train/seg_cls_loss': 0.012762451171875, 'train/kl_loss': 0.2220703125, 'train/mask_bce_loss': 0.08133316002786159, 'train/mask_dice_loss': 0.4369890093803406, 'train/mask_loss': 0.5183221757411957, 'metrics/total_secs_per_batch': 7.47333550453186, 'metrics/data_secs_per_batch': 3.7515167236328124, '_timestamp': 1740964073.2515266}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 490 is less than current step: 499. Dropping entry: {'train/lr': 6.263265306122449e-05, '_timestamp': 1740964073.2517993}).
Epoch: [3][492/500]	Time  8.539 ( 8.539)	Loss 1.3438 (1.8833)	CeLoss 1.3438 (0.4082)	SegCLSLoss 0.0000 (0.0150)	KLLoss 0.0000 (0.2939)	MaskLoss 0.0000 (0.7191)	MaskBCELoss 0.0000 (0.1687)	MaskDICELoss 0.0000 (0.5503)
Epoch: [3][493/500]	Time  8.185 ( 8.185)	Loss 1.6937 (1.7982)	CeLoss 0.2246 (0.4621)	SegCLSLoss 0.0262 (0.0151)	KLLoss 0.3672 (0.2609)	MaskLoss 0.7101 (0.6512)	MaskBCELoss 0.0393 (0.1603)	MaskDICELoss 0.6708 (0.4909)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 491 is less than current step: 499. Dropping entry: {'train/loss': 1.883307898044586, 'train/ce_loss': 0.408203125, 'train/seg_cls_loss': 0.01502685546875, 'train/kl_loss': 0.2939453125, 'train/mask_bce_loss': 0.16874751038849353, 'train/mask_dice_loss': 0.5503478422760963, 'train/mask_loss': 0.7190953552722931, 'metrics/total_secs_per_batch': 8.539103507995605, 'metrics/data_secs_per_batch': 3.7684680938720705, '_timestamp': 1740964081.7906985}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 491 is less than current step: 499. Dropping entry: {'train/lr': 6.251020408163265e-05, '_timestamp': 1740964081.7908971}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 492 is less than current step: 499. Dropping entry: {'train/loss': 1.7982397079467773, 'train/ce_loss': 0.462060546875, 'train/seg_cls_loss': 0.01507568359375, 'train/kl_loss': 0.2609375, 'train/mask_bce_loss': 0.16028128303587436, 'train/mask_dice_loss': 0.4909381866455078, 'train/mask_loss': 0.6512194633483886, 'metrics/total_secs_per_batch': 8.18461799621582, 'metrics/data_secs_per_batch': 3.528044056892395, '_timestamp': 1740964089.9753857}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 492 is less than current step: 499. Dropping entry: {'train/lr': 6.238775510204082e-05, '_timestamp': 1740964089.9756992}).
Epoch: [3][494/500]	Time  7.460 ( 7.460)	Loss 1.5363 (1.7532)	CeLoss 0.2949 (0.5243)	SegCLSLoss 0.0099 (0.0096)	KLLoss 0.3672 (0.2188)	MaskLoss 0.5992 (0.6010)	MaskBCELoss 0.1518 (0.1808)	MaskDICELoss 0.4474 (0.4202)
Epoch: [3][495/500]	Time  5.341 ( 5.341)	Loss 1.1641 (1.7073)	CeLoss 1.1641 (0.7151)	SegCLSLoss 0.0000 (0.0120)	KLLoss 0.0000 (0.1824)	MaskLoss 0.0000 (0.4839)	MaskBCELoss 0.0000 (0.1428)	MaskDICELoss 0.0000 (0.3411)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 493 is less than current step: 499. Dropping entry: {'train/loss': 1.753179383277893, 'train/ce_loss': 0.52431640625, 'train/seg_cls_loss': 0.0095703125, 'train/kl_loss': 0.21875, 'train/mask_bce_loss': 0.18075611293315888, 'train/mask_dice_loss': 0.4201988250017166, 'train/mask_loss': 0.6009549260139465, 'metrics/total_secs_per_batch': 7.459672689437866, 'metrics/data_secs_per_batch': 2.9976619482040405, '_timestamp': 1740964097.4349294}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 493 is less than current step: 499. Dropping entry: {'train/lr': 6.226530612244896e-05, '_timestamp': 1740964097.4352443}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 494 is less than current step: 499. Dropping entry: {'train/loss': 1.7072986602783202, 'train/ce_loss': 0.71513671875, 'train/seg_cls_loss': 0.01201171875, 'train/kl_loss': 0.182421875, 'train/mask_bce_loss': 0.14279082315042615, 'train/mask_dice_loss': 0.34113192856311797, 'train/mask_loss': 0.4839227557182312, 'metrics/total_secs_per_batch': 5.341022968292236, 'metrics/data_secs_per_batch': 2.120712494850159, '_timestamp': 1740964102.7759705}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 494 is less than current step: 499. Dropping entry: {'train/lr': 6.214285714285714e-05, '_timestamp': 1740964102.7762423}).
Epoch: [3][496/500]	Time  9.153 ( 9.153)	Loss 1.4453 (1.7680)	CeLoss 1.4453 (0.4366)	SegCLSLoss 0.0000 (0.0147)	KLLoss 0.0000 (0.2588)	MaskLoss 0.0000 (0.6490)	MaskBCELoss 0.0000 (0.1030)	MaskDICELoss 0.0000 (0.5459)
Epoch: [3][497/500]	Time  8.241 ( 8.241)	Loss 1.5421 (1.8084)	CeLoss 0.2090 (0.3275)	SegCLSLoss 0.0132 (0.0158)	KLLoss 0.3652 (0.3320)	MaskLoss 0.6451 (0.7198)	MaskBCELoss 0.0629 (0.1936)	MaskDICELoss 0.5822 (0.5261)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 495 is less than current step: 499. Dropping entry: {'train/loss': 1.7680192708969116, 'train/ce_loss': 0.43662109375, 'train/seg_cls_loss': 0.014654541015625, 'train/kl_loss': 0.2587890625, 'train/mask_bce_loss': 0.10300516337156296, 'train/mask_dice_loss': 0.5459458768367768, 'train/mask_loss': 0.6489510416984559, 'metrics/total_secs_per_batch': 9.153264999389648, 'metrics/data_secs_per_batch': 4.464237451553345, '_timestamp': 1740964111.9292245}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 495 is less than current step: 499. Dropping entry: {'train/lr': 6.20204081632653e-05, '_timestamp': 1740964111.9294922}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 496 is less than current step: 499. Dropping entry: {'train/loss': 1.8083987832069397, 'train/ce_loss': 0.3275390625, 'train/seg_cls_loss': 0.0158447265625, 'train/kl_loss': 0.33203125, 'train/mask_bce_loss': 0.19364862460643054, 'train/mask_dice_loss': 0.5261269316077233, 'train/mask_loss': 0.7197755724191666, 'metrics/total_secs_per_batch': 8.240946292877197, 'metrics/data_secs_per_batch': 3.525131344795227, '_timestamp': 1740964120.1701815}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 496 is less than current step: 499. Dropping entry: {'train/lr': 6.189795918367347e-05, '_timestamp': 1740964120.1704578}).
Epoch: [3][498/500]	Time  8.195 ( 8.195)	Loss 2.0874 (1.4280)	CeLoss 0.2812 (0.4692)	SegCLSLoss 0.0131 (0.0094)	KLLoss 0.3633 (0.2188)	MaskLoss 0.8816 (0.4661)	MaskBCELoss 0.1249 (0.0383)	MaskDICELoss 0.7567 (0.4278)
Epoch: [3][499/500]	Time  7.950 ( 7.950)	Loss 3.1460 (2.2167)	CeLoss 0.1289 (0.4416)	SegCLSLoss 0.0339 (0.0211)	KLLoss 0.3691 (0.2941)	MaskLoss 1.4817 (0.8676)	MaskBCELoss 0.5532 (0.2192)	MaskDICELoss 0.9285 (0.6483)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 497 is less than current step: 499. Dropping entry: {'train/loss': 1.4280471801757812, 'train/ce_loss': 0.469189453125, 'train/seg_cls_loss': 0.0093994140625, 'train/kl_loss': 0.21875, 'train/mask_bce_loss': 0.03832454541698098, 'train/mask_dice_loss': 0.427774241566658, 'train/mask_loss': 0.4660987913608551, 'metrics/total_secs_per_batch': 8.194596767425537, 'metrics/data_secs_per_batch': 3.12737557888031, '_timestamp': 1740964128.3648074}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 497 is less than current step: 499. Dropping entry: {'train/lr': 6.177551020408163e-05, '_timestamp': 1740964128.3649988}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 498 is less than current step: 499. Dropping entry: {'train/loss': 2.216667985916138, 'train/ce_loss': 0.4416015625, 'train/seg_cls_loss': 0.02105712890625, 'train/kl_loss': 0.294140625, 'train/mask_bce_loss': 0.219215552136302, 'train/mask_dice_loss': 0.6483469665050506, 'train/mask_loss': 0.8675625085830688, 'metrics/total_secs_per_batch': 7.950232744216919, 'metrics/data_secs_per_batch': 3.619351840019226, '_timestamp': 1740964136.315113}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 498 is less than current step: 499. Dropping entry: {'train/lr': 6.16530612244898e-05, '_timestamp': 1740964136.3154132}).
[2025-03-02 19:09:05,549] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=0, lr=[6.159183673469388e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:09:05,554] [INFO] [timer.py:215:stop] epoch=0/micro_step=20000/global_step=2000, RunningAvgSamplesPerSec=1.4943089970801036, CurrSamplesPerSec=1.0824612590057914, MemAllocated=31.25GB, MaxMemAllocated=37.19GB
Epoch: [3][500/500]	Time  9.240 ( 9.240)	Loss 1.3721 (1.7501)	CeLoss 0.2773 (0.2562)	SegCLSLoss 0.0106 (0.0150)	KLLoss 0.3730 (0.3686)	MaskLoss 0.5259 (0.7247)	MaskBCELoss 0.0769 (0.1538)	MaskDICELoss 0.4490 (0.5709)


















 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 197/200 [00:37<00:00,  7.45it/s]

100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:38<00:00,  5.15it/s]
Epoch: [4][  1/500]	Time  7.493 ( 7.493)	Loss 0.2773 (1.6706)	CeLoss 0.2773 (0.5911)	SegCLSLoss 0.0000 (0.0121)	KLLoss 0.0000 (0.2193)	MaskLoss 0.0000 (0.5259)	MaskBCELoss 0.0000 (0.0751)	MaskDICELoss 0.0000 (0.4508)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 3 is less than current step: 499. Dropping entry: {'val/giou': 0.1762383133172989, 'val/ciou': 0.1565922051668167, '_timestamp': 1740964184.3705626}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 499. Dropping entry: {'train/loss': 1.6706409096717834, 'train/ce_loss': 0.59111328125, 'train/seg_cls_loss': 0.01209716796875, 'train/kl_loss': 0.2193359375, 'train/mask_bce_loss': 0.07511391835287214, 'train/mask_dice_loss': 0.45083153545856475, 'train/mask_loss': 0.5259454488754273, 'metrics/total_secs_per_batch': 7.492962598800659, 'metrics/data_secs_per_batch': 3.03531014919281, '_timestamp': 1740964191.8703847}).
Epoch: [4][  2/500]	Time  7.208 ( 7.208)	Loss 2.5761 (1.8032)	CeLoss 0.2988 (0.4194)	SegCLSLoss 0.0156 (0.0135)	KLLoss 0.3711 (0.2936)	MaskLoss 1.1162 (0.6738)	MaskBCELoss 0.3710 (0.1893)	MaskDICELoss 0.7452 (0.4845)
Epoch: [4][  3/500]	Time  7.391 ( 7.391)	Loss 2.1356 (1.4694)	CeLoss 0.1816 (0.3774)	SegCLSLoss 0.0264 (0.0123)	KLLoss 0.3652 (0.2219)	MaskLoss 0.9516 (0.5317)	MaskBCELoss 0.0507 (0.1102)	MaskDICELoss 0.9009 (0.4215)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 1 is less than current step: 499. Dropping entry: {'train/loss': 1.8031816601753234, 'train/ce_loss': 0.41943359375, 'train/seg_cls_loss': 0.0134765625, 'train/kl_loss': 0.2935546875, 'train/mask_bce_loss': 0.18926090747117996, 'train/mask_dice_loss': 0.48454673290252687, 'train/mask_loss': 0.6738076388835907, 'metrics/total_secs_per_batch': 7.2076332569122314, 'metrics/data_secs_per_batch': 2.868319940567017, '_timestamp': 1740964199.07815}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 1 is less than current step: 499. Dropping entry: {'train/lr': 6.13469387755102e-05, '_timestamp': 1740964199.0784738}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 2 is less than current step: 499. Dropping entry: {'train/loss': 1.4694423437118531, 'train/ce_loss': 0.37744140625, 'train/seg_cls_loss': 0.0123046875, 'train/kl_loss': 0.221875, 'train/mask_bce_loss': 0.11021768487989902, 'train/mask_dice_loss': 0.4214761435985565, 'train/mask_loss': 0.531693834066391, 'metrics/total_secs_per_batch': 7.390913486480713, 'metrics/data_secs_per_batch': 3.467100405693054, '_timestamp': 1740964206.46909}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 2 is less than current step: 499. Dropping entry: {'train/lr': 6.122448979591836e-05, '_timestamp': 1740964206.4694116}).
Epoch: [4][  4/500]	Time 10.208 (10.208)	Loss 1.9169 (1.8510)	CeLoss 0.2363 (0.2265)	SegCLSLoss 0.0195 (0.0154)	KLLoss 0.3691 (0.3305)	MaskLoss 0.8169 (0.7919)	MaskBCELoss 0.0863 (0.1989)	MaskDICELoss 0.7306 (0.5929)
Epoch: [4][  5/500]	Time  8.010 ( 8.010)	Loss 3.5466 (1.4102)	CeLoss 0.1777 (0.4052)	SegCLSLoss 0.0187 (0.0077)	KLLoss 0.3691 (0.1848)	MaskLoss 1.6610 (0.4912)	MaskBCELoss 0.9548 (0.1739)	MaskDICELoss 0.7062 (0.3173)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 3 is less than current step: 499. Dropping entry: {'train/loss': 1.8509633779525756, 'train/ce_loss': 0.22646484375, 'train/seg_cls_loss': 0.015435791015625, 'train/kl_loss': 0.33046875, 'train/mask_bce_loss': 0.19894616454839706, 'train/mask_dice_loss': 0.5929417669773102, 'train/mask_loss': 0.7918879389762878, 'metrics/total_secs_per_batch': 10.208065271377563, 'metrics/data_secs_per_batch': 4.305110669136047, '_timestamp': 1740964216.6769888}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 3 is less than current step: 499. Dropping entry: {'train/lr': 6.110204081632653e-05, '_timestamp': 1740964216.6772795}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 4 is less than current step: 499. Dropping entry: {'train/loss': 1.4101744651794434, 'train/ce_loss': 0.4052001953125, 'train/seg_cls_loss': 0.007696533203125, 'train/kl_loss': 0.184765625, 'train/mask_bce_loss': 0.17393758818507193, 'train/mask_dice_loss': 0.3172702372074127, 'train/mask_loss': 0.4912078320980072, 'metrics/total_secs_per_batch': 8.010017156600952, 'metrics/data_secs_per_batch': 4.107033395767212, '_timestamp': 1740964224.6871967}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 4 is less than current step: 499. Dropping entry: {'train/lr': 6.097959183673469e-05, '_timestamp': 1740964224.6875412}).
Epoch: [4][  6/500]	Time  8.549 ( 8.549)	Loss 2.3157 (1.8191)	CeLoss 0.2197 (0.2565)	SegCLSLoss 0.0236 (0.0145)	KLLoss 0.3535 (0.2949)	MaskLoss 1.0240 (0.7629)	MaskBCELoss 0.0561 (0.2175)	MaskDICELoss 0.9680 (0.5454)
Epoch: [4][  7/500]	Time  7.500 ( 7.500)	Loss 0.8183 (1.1765)	CeLoss 0.1943 (0.4640)	SegCLSLoss 0.0134 (0.0077)	KLLoss 0.3652 (0.2207)	MaskLoss 0.2900 (0.3433)	MaskBCELoss 0.0512 (0.0705)	MaskDICELoss 0.2388 (0.2728)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 5 is less than current step: 499. Dropping entry: {'train/loss': 1.8191330790519715, 'train/ce_loss': 0.25654296875, 'train/seg_cls_loss': 0.014532470703125, 'train/kl_loss': 0.294921875, 'train/mask_bce_loss': 0.2175293553620577, 'train/mask_dice_loss': 0.5454063266515732, 'train/mask_loss': 0.7629356682300568, 'metrics/total_secs_per_batch': 8.54873776435852, 'metrics/data_secs_per_batch': 3.9544018030166628, '_timestamp': 1740964233.235734}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 5 is less than current step: 499. Dropping entry: {'train/lr': 6.0857142857142847e-05, '_timestamp': 1740964233.236014}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 6 is less than current step: 499. Dropping entry: {'train/loss': 1.1765009582042694, 'train/ce_loss': 0.46396484375, 'train/seg_cls_loss': 0.007666015625, 'train/kl_loss': 0.220703125, 'train/mask_bce_loss': 0.07050177771598101, 'train/mask_dice_loss': 0.27282683104276656, 'train/mask_loss': 0.34332860708236695, 'metrics/total_secs_per_batch': 7.500306844711304, 'metrics/data_secs_per_batch': 3.2879732608795167, '_timestamp': 1740964240.7360406}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 6 is less than current step: 499. Dropping entry: {'train/lr': 6.0734693877551016e-05, '_timestamp': 1740964240.7363229}).
Epoch: [4][  8/500]	Time  5.791 ( 5.791)	Loss 2.0823 (1.5381)	CeLoss 0.1973 (0.5738)	SegCLSLoss 0.0195 (0.0135)	KLLoss 0.3828 (0.2197)	MaskLoss 0.9186 (0.4678)	MaskBCELoss 0.0245 (0.1064)	MaskDICELoss 0.8941 (0.3614)
Epoch: [4][  9/500]	Time  6.552 ( 6.552)	Loss 0.7328 (1.4878)	CeLoss 0.2383 (0.3291)	SegCLSLoss 0.0155 (0.0167)	KLLoss 0.3652 (0.2971)	MaskLoss 0.2248 (0.5603)	MaskBCELoss 0.0050 (0.0908)	MaskDICELoss 0.2198 (0.4694)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 7 is less than current step: 499. Dropping entry: {'train/loss': 1.538071584701538, 'train/ce_loss': 0.573828125, 'train/seg_cls_loss': 0.01353759765625, 'train/kl_loss': 0.2197265625, 'train/mask_bce_loss': 0.10638324599713087, 'train/mask_dice_loss': 0.36138302832841873, 'train/mask_loss': 0.46776626706123353, 'metrics/total_secs_per_batch': 5.790618181228638, 'metrics/data_secs_per_batch': 2.459037685394287, '_timestamp': 1740964246.5267367}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 7 is less than current step: 499. Dropping entry: {'train/lr': 6.061224489795918e-05, '_timestamp': 1740964246.527037}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 8 is less than current step: 499. Dropping entry: {'train/loss': 1.4877872049808503, 'train/ce_loss': 0.329052734375, 'train/seg_cls_loss': 0.01668701171875, 'train/kl_loss': 0.2970703125, 'train/mask_bce_loss': 0.09083255385048687, 'train/mask_dice_loss': 0.46944289207458495, 'train/mask_loss': 0.5602754399180412, 'metrics/total_secs_per_batch': 6.552376985549927, 'metrics/data_secs_per_batch': 3.001037526130676, '_timestamp': 1740964253.0791738}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 8 is less than current step: 499. Dropping entry: {'train/lr': 6.048979591836734e-05, '_timestamp': 1740964253.0794926}).
[2025-03-02 19:11:01,766] [INFO] [logging.py:96:log_dist] [Rank 0] step=2010, skipped=0, lr=[6.0428571428571424e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:11:01,771] [INFO] [timer.py:215:stop] epoch=0/micro_step=20100/global_step=2010, RunningAvgSamplesPerSec=1.493146419721383, CurrSamplesPerSec=1.1505139206894728, MemAllocated=31.25GB, MaxMemAllocated=37.19GB
Epoch: [4][ 10/500]	Time  8.693 ( 8.693)	Loss 2.2893 (1.7899)	CeLoss 0.1807 (0.2244)	SegCLSLoss 0.0208 (0.0183)	KLLoss 0.3555 (0.3682)	MaskLoss 1.0314 (0.7598)	MaskBCELoss 0.0550 (0.1210)	MaskDICELoss 0.9764 (0.6388)
Epoch: [4][ 11/500]	Time  9.941 ( 9.941)	Loss 2.1634 (1.8589)	CeLoss 0.1846 (0.2322)	SegCLSLoss 0.0295 (0.0171)	KLLoss 0.3555 (0.3270)	MaskLoss 0.9645 (0.7927)	MaskBCELoss 0.0069 (0.1477)	MaskDICELoss 0.9576 (0.6451)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 9 is less than current step: 499. Dropping entry: {'train/loss': 1.7898985505104066, 'train/ce_loss': 0.2244140625, 'train/seg_cls_loss': 0.018280029296875, 'train/kl_loss': 0.3681640625, 'train/mask_bce_loss': 0.1209855267778039, 'train/mask_dice_loss': 0.6388075068593025, 'train/mask_loss': 0.7597930312156678, 'metrics/total_secs_per_batch': 8.693454265594482, 'metrics/data_secs_per_batch': 3.842259979248047, '_timestamp': 1740964261.7723348}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 9 is less than current step: 499. Dropping entry: {'train/lr': 6.0367346938775506e-05, '_timestamp': 1740964261.7726002}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 10 is less than current step: 499. Dropping entry: {'train/loss': 1.858860695362091, 'train/ce_loss': 0.232177734375, 'train/seg_cls_loss': 0.017071533203125, 'train/kl_loss': 0.326953125, 'train/mask_bce_loss': 0.14767512120306492, 'train/mask_dice_loss': 0.6450608909130097, 'train/mask_loss': 0.792736005783081, 'metrics/total_secs_per_batch': 9.940551280975342, 'metrics/data_secs_per_batch': 4.555617928504944, '_timestamp': 1740964271.7132556}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 10 is less than current step: 499. Dropping entry: {'train/lr': 6.024489795918367e-05, '_timestamp': 1740964271.7136056}).
Epoch: [4][ 12/500]	Time  9.305 ( 9.305)	Loss 1.5062 (1.6790)	CeLoss 0.2471 (0.3134)	SegCLSLoss 0.0121 (0.0162)	KLLoss 0.3691 (0.2955)	MaskLoss 0.6076 (0.6638)	MaskBCELoss 0.0387 (0.1137)	MaskDICELoss 0.5689 (0.5501)
Epoch: [4][ 13/500]	Time  6.638 ( 6.638)	Loss 0.0576 (1.0429)	CeLoss 0.0576 (0.6098)	SegCLSLoss 0.0000 (0.0057)	KLLoss 0.0000 (0.1484)	MaskLoss 0.0000 (0.2078)	MaskBCELoss 0.0000 (0.0493)	MaskDICELoss 0.0000 (0.1585)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 11 is less than current step: 499. Dropping entry: {'train/loss': 1.6790413856506348, 'train/ce_loss': 0.31337890625, 'train/seg_cls_loss': 0.016217041015625, 'train/kl_loss': 0.2955078125, 'train/mask_bce_loss': 0.11370892748236656, 'train/mask_dice_loss': 0.5501281648874283, 'train/mask_loss': 0.6638370811939239, 'metrics/total_secs_per_batch': 9.304936170578003, 'metrics/data_secs_per_batch': 4.329430794715881, '_timestamp': 1740964281.0180159}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 11 is less than current step: 499. Dropping entry: {'train/lr': 6.0122448979591825e-05, '_timestamp': 1740964281.0182893}).
Epoch: [4][ 14/500]	Time  7.932 ( 7.932)	Loss 1.0703 (1.5672)	CeLoss 1.0703 (0.4226)	SegCLSLoss 0.0000 (0.0111)	KLLoss 0.0000 (0.2559)	MaskLoss 0.0000 (0.5567)	MaskBCELoss 0.0000 (0.1444)	MaskDICELoss 0.0000 (0.4123)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 12 is less than current step: 499. Dropping entry: {'train/loss': 1.0429213762283325, 'train/ce_loss': 0.609814453125, 'train/seg_cls_loss': 0.005657958984375, 'train/kl_loss': 0.1484375, 'train/mask_bce_loss': 0.04928862079977989, 'train/mask_dice_loss': 0.1585246130824089, 'train/mask_loss': 0.20781323611736296, 'metrics/total_secs_per_batch': 6.637629508972168, 'metrics/data_secs_per_batch': 2.5734634399414062, '_timestamp': 1740964287.6556473}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 12 is less than current step: 499. Dropping entry: {'train/lr': 5.9999999999999995e-05, '_timestamp': 1740964287.655961}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 13 is less than current step: 499. Dropping entry: {'train/loss': 1.5671973824501038, 'train/ce_loss': 0.42255859375, 'train/seg_cls_loss': 0.0111328125, 'train/kl_loss': 0.255859375, 'train/mask_bce_loss': 0.14441327583044766, 'train/mask_dice_loss': 0.41228110790252687, 'train/mask_loss': 0.556694382429123, 'metrics/total_secs_per_batch': 7.932084083557129, 'metrics/data_secs_per_batch': 4.1566380023956295, '_timestamp': 1740964295.587768}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 13 is less than current step: 499. Dropping entry: {'train/lr': 5.987755102040816e-05, '_timestamp': 1740964295.5880473}).
Epoch: [4][ 15/500]	Time  9.085 ( 9.085)	Loss 2.4963 (1.8427)	CeLoss 0.2090 (0.2442)	SegCLSLoss 0.0233 (0.0182)	KLLoss 0.3867 (0.3705)	MaskLoss 1.1183 (0.7760)	MaskBCELoss 0.2583 (0.1460)	MaskDICELoss 0.8600 (0.6300)
Epoch: [4][ 16/500]	Time  9.304 ( 9.304)	Loss 2.2161 (1.5842)	CeLoss 0.2227 (0.2122)	SegCLSLoss 0.0282 (0.0144)	KLLoss 0.3652 (0.2574)	MaskLoss 0.9714 (0.6695)	MaskBCELoss 0.0303 (0.1496)	MaskDICELoss 0.9410 (0.5198)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 14 is less than current step: 499. Dropping entry: {'train/loss': 1.8427236199378967, 'train/ce_loss': 0.24423828125, 'train/seg_cls_loss': 0.0182373046875, 'train/kl_loss': 0.3705078125, 'train/mask_bce_loss': 0.1460052777081728, 'train/mask_dice_loss': 0.6300440311431885, 'train/mask_loss': 0.7760493069887161, 'metrics/total_secs_per_batch': 9.084744930267334, 'metrics/data_secs_per_batch': 4.2693033695220945, '_timestamp': 1740964304.6724916}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 14 is less than current step: 499. Dropping entry: {'train/lr': 5.975510204081632e-05, '_timestamp': 1740964304.672687}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 15 is less than current step: 499. Dropping entry: {'train/loss': 1.5842015504837037, 'train/ce_loss': 0.212158203125, 'train/seg_cls_loss': 0.014373779296875, 'train/kl_loss': 0.257421875, 'train/mask_bce_loss': 0.1496499603614211, 'train/mask_dice_loss': 0.5198189944028855, 'train/mask_loss': 0.6694689452648163, 'metrics/total_secs_per_batch': 9.303681373596191, 'metrics/data_secs_per_batch': 4.226250433921814, '_timestamp': 1740964313.9763486}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 15 is less than current step: 499. Dropping entry: {'train/lr': 5.9632653061224484e-05, '_timestamp': 1740964313.9766912}).
Epoch: [4][ 17/500]	Time  7.604 ( 7.604)	Loss 1.2891 (1.2919)	CeLoss 1.2891 (0.4682)	SegCLSLoss 0.0000 (0.0089)	KLLoss 0.0000 (0.1869)	MaskLoss 0.0000 (0.4004)	MaskBCELoss 0.0000 (0.0811)	MaskDICELoss 0.0000 (0.3193)
Epoch: [4][ 18/500]	Time  8.147 ( 8.147)	Loss 1.6149 (1.7545)	CeLoss 0.2217 (0.3245)	SegCLSLoss 0.0214 (0.0156)	KLLoss 0.3633 (0.3305)	MaskLoss 0.6727 (0.6945)	MaskBCELoss 0.0893 (0.1753)	MaskDICELoss 0.5834 (0.5192)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 16 is less than current step: 499. Dropping entry: {'train/loss': 1.29194815158844, 'train/ce_loss': 0.4681884765625, 'train/seg_cls_loss': 0.008868408203125, 'train/kl_loss': 0.1869140625, 'train/mask_bce_loss': 0.08106766529381275, 'train/mask_dice_loss': 0.31928873658180235, 'train/mask_loss': 0.40035640001296996, 'metrics/total_secs_per_batch': 7.604346990585327, 'metrics/data_secs_per_batch': 3.4457364797592165, '_timestamp': 1740964321.580494}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 16 is less than current step: 499. Dropping entry: {'train/lr': 5.951020408163265e-05, '_timestamp': 1740964321.580759}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 17 is less than current step: 499. Dropping entry: {'train/loss': 1.754466986656189, 'train/ce_loss': 0.32451171875, 'train/seg_cls_loss': 0.01563720703125, 'train/kl_loss': 0.33046875, 'train/mask_bce_loss': 0.1753122877329588, 'train/mask_dice_loss': 0.5192063540220261, 'train/mask_loss': 0.694518631696701, 'metrics/total_secs_per_batch': 8.147426843643188, 'metrics/data_secs_per_batch': 3.5871213912963866, '_timestamp': 1740964329.7279353}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 17 is less than current step: 499. Dropping entry: {'train/lr': 5.938775510204081e-05, '_timestamp': 1740964329.7282069}).
Epoch: [4][ 19/500]	Time  8.749 ( 8.749)	Loss 1.7568 (1.8161)	CeLoss 0.2129 (0.4320)	SegCLSLoss 0.0186 (0.0132)	KLLoss 0.3711 (0.2998)	MaskLoss 0.7485 (0.6737)	MaskBCELoss 0.0555 (0.1675)	MaskDICELoss 0.6930 (0.5062)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 18 is less than current step: 499. Dropping entry: {'train/loss': 1.8161372303962708, 'train/ce_loss': 0.43203125, 'train/seg_cls_loss': 0.013214111328125, 'train/kl_loss': 0.2998046875, 'train/mask_bce_loss': 0.16747824512422085, 'train/mask_dice_loss': 0.5062153607606887, 'train/mask_loss': 0.6736936092376709, 'metrics/total_secs_per_batch': 8.749067544937134, 'metrics/data_secs_per_batch': 4.008731651306152, '_timestamp': 1740964338.477013}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 18 is less than current step: 499. Dropping entry: {'train/lr': 5.926530612244898e-05, '_timestamp': 1740964338.477278}).
[2025-03-02 19:12:27,896] [INFO] [logging.py:96:log_dist] [Rank 0] step=2020, skipped=0, lr=[5.920408163265306e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:12:27,903] [INFO] [timer.py:215:stop] epoch=0/micro_step=20200/global_step=2020, RunningAvgSamplesPerSec=1.4910340809269789, CurrSamplesPerSec=1.060952863919917, MemAllocated=31.23GB, MaxMemAllocated=37.19GB
Epoch: [4][ 20/500]	Time  9.427 ( 9.427)	Loss 1.8678 (1.8719)	CeLoss 0.2480 (0.2840)	SegCLSLoss 0.0118 (0.0185)	KLLoss 0.3809 (0.3422)	MaskLoss 0.7874 (0.7721)	MaskBCELoss 0.3042 (0.1555)	MaskDICELoss 0.4832 (0.6166)
Epoch: [4][ 21/500]	Time  8.953 ( 8.953)	Loss 1.3828 (1.7780)	CeLoss 1.3828 (0.4186)	SegCLSLoss 0.0000 (0.0139)	KLLoss 0.0000 (0.2959)	MaskLoss 0.0000 (0.6615)	MaskBCELoss 0.0000 (0.0634)	MaskDICELoss 0.0000 (0.5981)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 19 is less than current step: 499. Dropping entry: {'train/loss': 1.8719263792037963, 'train/ce_loss': 0.283984375, 'train/seg_cls_loss': 0.018499755859375, 'train/kl_loss': 0.3421875, 'train/mask_bce_loss': 0.15553457904607057, 'train/mask_dice_loss': 0.6165614187717438, 'train/mask_loss': 0.7720960021018982, 'metrics/total_secs_per_batch': 9.426974534988403, 'metrics/data_secs_per_batch': 4.304865097999572, '_timestamp': 1740964347.9037924}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 19 is less than current step: 499. Dropping entry: {'train/lr': 5.9142857142857136e-05, '_timestamp': 1740964347.9040496}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 20 is less than current step: 499. Dropping entry: {'train/loss': 1.7780134201049804, 'train/ce_loss': 0.4185546875, 'train/seg_cls_loss': 0.01392822265625, 'train/kl_loss': 0.2958984375, 'train/mask_bce_loss': 0.06336043011397123, 'train/mask_dice_loss': 0.5981072396039963, 'train/mask_loss': 0.6614676654338837, 'metrics/total_secs_per_batch': 8.953212261199951, 'metrics/data_secs_per_batch': 4.181814956665039, '_timestamp': 1740964356.8573802}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 20 is less than current step: 499. Dropping entry: {'train/lr': 5.90204081632653e-05, '_timestamp': 1740964356.8576996}).
Epoch: [4][ 22/500]	Time  7.465 ( 7.465)	Loss 1.2446 (1.7553)	CeLoss 0.1914 (0.4698)	SegCLSLoss 0.0269 (0.0141)	KLLoss 0.3750 (0.2609)	MaskLoss 0.5012 (0.6262)	MaskBCELoss 0.2220 (0.2070)	MaskDICELoss 0.2792 (0.4191)
Epoch: [4][ 23/500]	Time  7.940 ( 7.940)	Loss 1.9817 (1.7174)	CeLoss 0.2617 (0.4230)	SegCLSLoss 0.0186 (0.0132)	KLLoss 0.3691 (0.2586)	MaskLoss 0.8366 (0.6309)	MaskBCELoss 0.1386 (0.0766)	MaskDICELoss 0.6980 (0.5544)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 21 is less than current step: 499. Dropping entry: {'train/loss': 1.7552912831306458, 'train/ce_loss': 0.46982421875, 'train/seg_cls_loss': 0.0140625, 'train/kl_loss': 0.2609375, 'train/mask_bce_loss': 0.20704585853964091, 'train/mask_dice_loss': 0.41913495659828187, 'train/mask_loss': 0.6261808186769485, 'metrics/total_secs_per_batch': 7.465282917022705, 'metrics/data_secs_per_batch': 3.837582540512085, '_timestamp': 1740964364.3225238}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 21 is less than current step: 499. Dropping entry: {'train/lr': 5.889795918367346e-05, '_timestamp': 1740964364.3228526}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 22 is less than current step: 499. Dropping entry: {'train/loss': 1.7174083471298218, 'train/ce_loss': 0.422998046875, 'train/seg_cls_loss': 0.013201904296875, 'train/kl_loss': 0.25859375, 'train/mask_bce_loss': 0.07656643241643905, 'train/mask_dice_loss': 0.5543789505958557, 'train/mask_loss': 0.6309453785419464, 'metrics/total_secs_per_batch': 7.9399254322052, 'metrics/data_secs_per_batch': 3.6394421100616454, '_timestamp': 1740964372.2626197}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 22 is less than current step: 499. Dropping entry: {'train/lr': 5.8775510204081625e-05, '_timestamp': 1740964372.2630248}).
Epoch: [4][ 24/500]	Time  8.788 ( 8.788)	Loss 3.8514 (1.6800)	CeLoss 0.2207 (0.3466)	SegCLSLoss 0.0181 (0.0112)	KLLoss 0.3809 (0.2602)	MaskLoss 1.7919 (0.6509)	MaskBCELoss 0.9993 (0.1697)	MaskDICELoss 0.7926 (0.4812)
Epoch: [4][ 25/500]	Time  8.005 ( 8.005)	Loss 1.5641 (2.0877)	CeLoss 0.2373 (0.2182)	SegCLSLoss 0.0117 (0.0184)	KLLoss 0.3730 (0.3686)	MaskLoss 0.6414 (0.9119)	MaskBCELoss 0.4173 (0.2326)	MaskDICELoss 0.2241 (0.6793)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 23 is less than current step: 499. Dropping entry: {'train/loss': 1.6800392389297485, 'train/ce_loss': 0.34658203125, 'train/seg_cls_loss': 0.01121826171875, 'train/kl_loss': 0.26015625, 'train/mask_bce_loss': 0.16974275149405002, 'train/mask_dice_loss': 0.4811655431985855, 'train/mask_loss': 0.6509082853794098, 'metrics/total_secs_per_batch': 8.788292407989502, 'metrics/data_secs_per_batch': 3.9877071142196656, '_timestamp': 1740964381.0507078}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 23 is less than current step: 499. Dropping entry: {'train/lr': 5.8653061224489795e-05, '_timestamp': 1740964381.0509753}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 24 is less than current step: 499. Dropping entry: {'train/loss': 2.0877464413642883, 'train/ce_loss': 0.2181640625, 'train/seg_cls_loss': 0.01837158203125, 'train/kl_loss': 0.3685546875, 'train/mask_bce_loss': 0.23263885006308554, 'train/mask_dice_loss': 0.6792519509792327, 'train/mask_loss': 0.9118908077478409, 'metrics/total_secs_per_batch': 8.005491256713867, 'metrics/data_secs_per_batch': 3.7536196947097777, '_timestamp': 1740964389.0562336}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 24 is less than current step: 499. Dropping entry: {'train/lr': 5.853061224489796e-05, '_timestamp': 1740964389.056532}).
Epoch: [4][ 26/500]	Time  9.044 ( 9.044)	Loss 1.6374 (1.7208)	CeLoss 0.2500 (0.2384)	SegCLSLoss 0.0101 (0.0161)	KLLoss 0.3672 (0.3357)	MaskLoss 0.6722 (0.7204)	MaskBCELoss 0.1248 (0.0575)	MaskDICELoss 0.5474 (0.6629)
Epoch: [4][ 27/500]	Time  7.916 ( 7.916)	Loss 1.0047 (1.5117)	CeLoss 0.2617 (0.6365)	SegCLSLoss 0.0126 (0.0081)	KLLoss 0.3672 (0.1838)	MaskLoss 0.3500 (0.4263)	MaskBCELoss 0.0796 (0.1394)	MaskDICELoss 0.2704 (0.2869)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 25 is less than current step: 499. Dropping entry: {'train/loss': 1.7207993507385253, 'train/ce_loss': 0.23837890625, 'train/seg_cls_loss': 0.016119384765625, 'train/kl_loss': 0.3357421875, 'train/mask_bce_loss': 0.05746460449881852, 'train/mask_dice_loss': 0.6628960132598877, 'train/mask_loss': 0.7203606218099594, 'metrics/total_secs_per_batch': 9.044407367706299, 'metrics/data_secs_per_batch': 3.9158562898635862, '_timestamp': 1740964398.1007798}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 25 is less than current step: 499. Dropping entry: {'train/lr': 5.840816326530612e-05, '_timestamp': 1740964398.10116}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 26 is less than current step: 499. Dropping entry: {'train/loss': 1.511668288707733, 'train/ce_loss': 0.6365234375, 'train/seg_cls_loss': 0.008056640625, 'train/kl_loss': 0.1837890625, 'train/mask_bce_loss': 0.1394368290901184, 'train/mask_dice_loss': 0.2868563085794449, 'train/mask_loss': 0.4262931257486343, 'metrics/total_secs_per_batch': 7.915756940841675, 'metrics/data_secs_per_batch': 3.49778208732605, '_timestamp': 1740964406.0164032}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 26 is less than current step: 499. Dropping entry: {'train/lr': 5.828571428571428e-05, '_timestamp': 1740964406.0166774}).
Epoch: [4][ 28/500]	Time  6.444 ( 6.444)	Loss 0.9648 (1.7888)	CeLoss 0.9648 (0.5813)	SegCLSLoss 0.0000 (0.0121)	KLLoss 0.0000 (0.2586)	MaskLoss 0.0000 (0.5879)	MaskBCELoss 0.0000 (0.1945)	MaskDICELoss 0.0000 (0.3935)
Epoch: [4][ 29/500]	Time  8.709 ( 8.709)	Loss 2.5425 (1.5032)	CeLoss 0.2676 (0.4942)	SegCLSLoss 0.0121 (0.0099)	KLLoss 0.3691 (0.2217)	MaskLoss 1.1160 (0.4909)	MaskBCELoss 0.4167 (0.1150)	MaskDICELoss 0.6993 (0.3759)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 27 is less than current step: 499. Dropping entry: {'train/loss': 1.7888376832008361, 'train/ce_loss': 0.58134765625, 'train/seg_cls_loss': 0.012066650390625, 'train/kl_loss': 0.25859375, 'train/mask_bce_loss': 0.1944505913183093, 'train/mask_dice_loss': 0.3934741109609604, 'train/mask_loss': 0.587924700975418, 'metrics/total_secs_per_batch': 6.444070100784302, 'metrics/data_secs_per_batch': 2.471004772186279, '_timestamp': 1740964412.4604344}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 27 is less than current step: 499. Dropping entry: {'train/lr': 5.816326530612244e-05, '_timestamp': 1740964412.4606993}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 28 is less than current step: 499. Dropping entry: {'train/loss': 1.5032272458076477, 'train/ce_loss': 0.49423828125, 'train/seg_cls_loss': 0.009942626953125, 'train/kl_loss': 0.2216796875, 'train/mask_bce_loss': 0.11501587629318237, 'train/mask_dice_loss': 0.375855553150177, 'train/mask_loss': 0.49087143540382383, 'metrics/total_secs_per_batch': 8.709157228469849, 'metrics/data_secs_per_batch': 2.934547686576843, '_timestamp': 1740964421.1699095}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 28 is less than current step: 499. Dropping entry: {'train/lr': 5.8040816326530604e-05, '_timestamp': 1740964421.1702924}).
[2025-03-02 19:13:48,369] [INFO] [logging.py:96:log_dist] [Rank 0] step=2030, skipped=0, lr=[5.797959183673469e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:13:48,374] [INFO] [timer.py:215:stop] epoch=0/micro_step=20300/global_step=2030, RunningAvgSamplesPerSec=1.4895675770384544, CurrSamplesPerSec=1.3882008698232036, MemAllocated=31.11GB, MaxMemAllocated=37.19GB
Epoch: [4][ 30/500]	Time  7.206 ( 7.206)	Loss 1.8533 (1.6314)	CeLoss 0.1904 (0.3841)	SegCLSLoss 0.0179 (0.0124)	KLLoss 0.3691 (0.2953)	MaskLoss 0.8085 (0.6057)	MaskBCELoss 0.0696 (0.1016)	MaskDICELoss 0.7388 (0.5041)
Epoch: [4][ 31/500]	Time  8.423 ( 8.423)	Loss 1.7121 (1.5077)	CeLoss 0.1934 (0.3130)	SegCLSLoss 0.0170 (0.0143)	KLLoss 0.3691 (0.2975)	MaskLoss 0.7364 (0.5788)	MaskBCELoss 0.0307 (0.1196)	MaskDICELoss 0.7057 (0.4592)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 29 is less than current step: 499. Dropping entry: {'train/loss': 1.6313595175743103, 'train/ce_loss': 0.38408203125, 'train/seg_cls_loss': 0.012384033203125, 'train/kl_loss': 0.2953125, 'train/mask_bce_loss': 0.10156924407929183, 'train/mask_dice_loss': 0.5041495710611343, 'train/mask_loss': 0.6057188212871552, 'metrics/total_secs_per_batch': 7.2055275440216064, 'metrics/data_secs_per_batch': 2.95344934463501, '_timestamp': 1740964428.3749712}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 29 is less than current step: 499. Dropping entry: {'train/lr': 5.7918367346938773e-05, '_timestamp': 1740964428.375264}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 30 is less than current step: 499. Dropping entry: {'train/loss': 1.5077186286449433, 'train/ce_loss': 0.313037109375, 'train/seg_cls_loss': 0.014251708984375, 'train/kl_loss': 0.2974609375, 'train/mask_bce_loss': 0.1196001946926117, 'train/mask_dice_loss': 0.45923470705747604, 'train/mask_loss': 0.5788349062204361, 'metrics/total_secs_per_batch': 8.422844886779785, 'metrics/data_secs_per_batch': 3.721796751022339, '_timestamp': 1740964436.7979949}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 30 is less than current step: 499. Dropping entry: {'train/lr': 5.7795918367346937e-05, '_timestamp': 1740964436.7982981}).
Epoch: [4][ 32/500]	Time  8.745 ( 8.745)	Loss 2.4056 (1.7713)	CeLoss 0.1475 (0.2838)	SegCLSLoss 0.0245 (0.0180)	KLLoss 0.3809 (0.3369)	MaskLoss 1.1037 (0.7222)	MaskBCELoss 0.3008 (0.1179)	MaskDICELoss 0.8029 (0.6043)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 31 is less than current step: 499. Dropping entry: {'train/loss': 1.7712575316429138, 'train/ce_loss': 0.2837890625, 'train/seg_cls_loss': 0.01795654296875, 'train/kl_loss': 0.3369140625, 'train/mask_bce_loss': 0.11794837238267064, 'train/mask_dice_loss': 0.6042526513338089, 'train/mask_loss': 0.7222010314464569, 'metrics/total_secs_per_batch': 8.744525909423828, 'metrics/data_secs_per_batch': 3.7315844535827636, '_timestamp': 1740964445.5425498}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 31 is less than current step: 499. Dropping entry: {'train/lr': 5.76734693877551e-05, '_timestamp': 1740964445.5428114}).
Epoch: [4][ 33/500]	Time  7.894 ( 7.894)	Loss 2.4449 (1.8294)	CeLoss 0.2188 (0.3588)	SegCLSLoss 0.0216 (0.0163)	KLLoss 0.3672 (0.3324)	MaskLoss 1.0896 (0.7147)	MaskBCELoss 0.1875 (0.1008)	MaskDICELoss 0.9021 (0.6140)
Epoch: [4][ 34/500]	Time  9.319 ( 9.319)	Loss 3.8312 (2.1097)	CeLoss 0.2910 (0.2172)	SegCLSLoss 0.0175 (0.0190)	KLLoss 0.3789 (0.3678)	MaskLoss 1.7476 (0.9230)	MaskBCELoss 1.3043 (0.2651)	MaskDICELoss 0.4433 (0.6579)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 32 is less than current step: 499. Dropping entry: {'train/loss': 1.8293749332427978, 'train/ce_loss': 0.3587890625, 'train/seg_cls_loss': 0.016290283203125, 'train/kl_loss': 0.332421875, 'train/mask_bce_loss': 0.10076382551342249, 'train/mask_dice_loss': 0.6139724582433701, 'train/mask_loss': 0.7147362768650055, 'metrics/total_secs_per_batch': 7.894330739974976, 'metrics/data_secs_per_batch': 3.7237428426742554, '_timestamp': 1740964453.4369526}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 32 is less than current step: 499. Dropping entry: {'train/lr': 5.7551020408163256e-05, '_timestamp': 1740964453.4371712}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 33 is less than current step: 499. Dropping entry: {'train/loss': 2.109660917520523, 'train/ce_loss': 0.2171875, 'train/seg_cls_loss': 0.0190185546875, 'train/kl_loss': 0.3677734375, 'train/mask_bce_loss': 0.2650844148360193, 'train/mask_dice_loss': 0.6579101130366325, 'train/mask_loss': 0.922994515299797, 'metrics/total_secs_per_batch': 9.318638801574707, 'metrics/data_secs_per_batch': 4.452951097488404, '_timestamp': 1740964462.7555132}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 33 is less than current step: 499. Dropping entry: {'train/lr': 5.742857142857142e-05, '_timestamp': 1740964462.755786}).
Epoch: [4][ 35/500]	Time  7.778 ( 7.778)	Loss 1.8823 (1.9120)	CeLoss 0.2832 (0.2860)	SegCLSLoss 0.0107 (0.0123)	KLLoss 0.3711 (0.2967)	MaskLoss 0.7781 (0.7950)	MaskBCELoss 0.1720 (0.2024)	MaskDICELoss 0.6060 (0.5927)
Epoch: [4][ 36/500]	Time  8.132 ( 8.132)	Loss 0.8867 (1.8082)	CeLoss 0.8867 (0.4919)	SegCLSLoss 0.0000 (0.0123)	KLLoss 0.0000 (0.2568)	MaskLoss 0.0000 (0.6422)	MaskBCELoss 0.0000 (0.1402)	MaskDICELoss 0.0000 (0.5020)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 34 is less than current step: 499. Dropping entry: {'train/loss': 1.9119555354118347, 'train/ce_loss': 0.285986328125, 'train/seg_cls_loss': 0.012274169921875, 'train/kl_loss': 0.2966796875, 'train/mask_bce_loss': 0.2023510282859206, 'train/mask_dice_loss': 0.5926648110151291, 'train/mask_loss': 0.7950158447027207, 'metrics/total_secs_per_batch': 7.778352499008179, 'metrics/data_secs_per_batch': 3.0995747089385985, '_timestamp': 1740964470.5338879}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 34 is less than current step: 499. Dropping entry: {'train/lr': 5.730612244897958e-05, '_timestamp': 1740964470.5341666}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 35 is less than current step: 499. Dropping entry: {'train/loss': 1.8082220554351807, 'train/ce_loss': 0.49189453125, 'train/seg_cls_loss': 0.012261962890625, 'train/kl_loss': 0.2568359375, 'train/mask_bce_loss': 0.1402407508343458, 'train/mask_dice_loss': 0.5020050555467606, 'train/mask_loss': 0.6422458052635193, 'metrics/total_secs_per_batch': 8.13240122795105, 'metrics/data_secs_per_batch': 3.7334142208099363, '_timestamp': 1740964478.6662421}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 35 is less than current step: 499. Dropping entry: {'train/lr': 5.718367346938775e-05, '_timestamp': 1740964478.6665027}).
Epoch: [4][ 37/500]	Time  7.025 ( 7.025)	Loss 2.1953 (1.7408)	CeLoss 0.1128 (0.4351)	SegCLSLoss 0.0310 (0.0155)	KLLoss 0.3965 (0.2609)	MaskLoss 1.0137 (0.6360)	MaskBCELoss 0.1497 (0.1188)	MaskDICELoss 0.8639 (0.5172)
Epoch: [4][ 38/500]	Time  7.941 ( 7.941)	Loss 2.7928 (1.5928)	CeLoss 0.1836 (0.4052)	SegCLSLoss 0.0189 (0.0114)	KLLoss 0.3691 (0.2588)	MaskLoss 1.2812 (0.5780)	MaskBCELoss 0.4568 (0.1786)	MaskDICELoss 0.8243 (0.3994)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 36 is less than current step: 499. Dropping entry: {'train/loss': 1.7408334732055664, 'train/ce_loss': 0.435107421875, 'train/seg_cls_loss': 0.015545654296875, 'train/kl_loss': 0.2609375, 'train/mask_bce_loss': 0.11877628117799759, 'train/mask_dice_loss': 0.5172166258096695, 'train/mask_loss': 0.6359929025173188, 'metrics/total_secs_per_batch': 7.025001049041748, 'metrics/data_secs_per_batch': 2.6956188917160033, '_timestamp': 1740964485.6912832}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 36 is less than current step: 499. Dropping entry: {'train/lr': 5.7061224489795915e-05, '_timestamp': 1740964485.6915739}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 37 is less than current step: 499. Dropping entry: {'train/loss': 1.5928094983100891, 'train/ce_loss': 0.405224609375, 'train/seg_cls_loss': 0.011358642578125, 'train/kl_loss': 0.2587890625, 'train/mask_bce_loss': 0.17856723740696906, 'train/mask_dice_loss': 0.399404901266098, 'train/mask_loss': 0.577972137928009, 'metrics/total_secs_per_batch': 7.941051244735718, 'metrics/data_secs_per_batch': 3.576991653442383, '_timestamp': 1740964493.6324174}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 37 is less than current step: 499. Dropping entry: {'train/lr': 5.693877551020408e-05, '_timestamp': 1740964493.6327279}).
Epoch: [4][ 39/500]	Time  8.622 ( 8.622)	Loss 1.1930 (1.6099)	CeLoss 0.2871 (0.3536)	SegCLSLoss 0.0122 (0.0140)	KLLoss 0.3750 (0.2982)	MaskLoss 0.4315 (0.6098)	MaskBCELoss 0.0768 (0.1359)	MaskDICELoss 0.3546 (0.4739)
[2025-03-02 19:15:08,531] [INFO] [logging.py:96:log_dist] [Rank 0] step=2040, skipped=0, lr=[5.675510204081632e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:15:08,537] [INFO] [timer.py:215:stop] epoch=0/micro_step=20400/global_step=2040, RunningAvgSamplesPerSec=1.4881516871937297, CurrSamplesPerSec=1.591732999880458, MemAllocated=31.24GB, MaxMemAllocated=37.19GB
Epoch: [4][ 40/500]	Time  6.284 ( 6.284)	Loss 2.4608 (1.4975)	CeLoss 0.2773 (0.4302)	SegCLSLoss 0.0125 (0.0138)	KLLoss 0.3691 (0.2939)	MaskLoss 1.0702 (0.5155)	MaskBCELoss 0.2494 (0.1376)	MaskDICELoss 0.8208 (0.3779)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 38 is less than current step: 499. Dropping entry: {'train/loss': 1.6099185824394227, 'train/ce_loss': 0.35361328125, 'train/seg_cls_loss': 0.01402587890625, 'train/kl_loss': 0.2982421875, 'train/mask_bce_loss': 0.1359235730022192, 'train/mask_dice_loss': 0.4738696962594986, 'train/mask_loss': 0.609793271124363, 'metrics/total_secs_per_batch': 8.621819734573364, 'metrics/data_secs_per_batch': 4.152375507354736, '_timestamp': 1740964502.2541773}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 38 is less than current step: 499. Dropping entry: {'train/lr': 5.681632653061224e-05, '_timestamp': 1740964502.2543921}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 39 is less than current step: 499. Dropping entry: {'train/loss': 1.4975459545850753, 'train/ce_loss': 0.43017578125, 'train/seg_cls_loss': 0.01383056640625, 'train/kl_loss': 0.2939453125, 'train/mask_bce_loss': 0.1375619936734438, 'train/mask_dice_loss': 0.3779101952910423, 'train/mask_loss': 0.5154721960425377, 'metrics/total_secs_per_batch': 6.283987522125244, 'metrics/data_secs_per_batch': 2.494220995903015, '_timestamp': 1740964508.5379446}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 39 is less than current step: 499. Dropping entry: {'train/lr': 5.66938775510204e-05, '_timestamp': 1740964508.5381956}).
Epoch: [4][ 41/500]	Time  8.116 ( 8.116)	Loss 2.1071 (1.7605)	CeLoss 0.2891 (0.5822)	SegCLSLoss 0.0156 (0.0124)	KLLoss 0.3711 (0.2637)	MaskLoss 0.8866 (0.5728)	MaskBCELoss 0.0521 (0.0765)	MaskDICELoss 0.8345 (0.4963)
Epoch: [4][ 42/500]	Time  7.776 ( 7.776)	Loss 3.5543 (2.0289)	CeLoss 0.2715 (0.4603)	SegCLSLoss 0.0143 (0.0140)	KLLoss 0.3691 (0.2947)	MaskLoss 1.6189 (0.7661)	MaskBCELoss 0.8249 (0.2050)	MaskDICELoss 0.7940 (0.5611)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 40 is less than current step: 499. Dropping entry: {'train/loss': 1.7605046272277831, 'train/ce_loss': 0.5822265625, 'train/seg_cls_loss': 0.012432861328125, 'train/kl_loss': 0.263671875, 'train/mask_bce_loss': 0.07654235763475299, 'train/mask_dice_loss': 0.4962880790233612, 'train/mask_loss': 0.5728304266929627, 'metrics/total_secs_per_batch': 8.115671873092651, 'metrics/data_secs_per_batch': 3.302179765701294, '_timestamp': 1740964516.6537764}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 40 is less than current step: 499. Dropping entry: {'train/lr': 5.657142857142857e-05, '_timestamp': 1740964516.6540358}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 41 is less than current step: 499. Dropping entry: {'train/loss': 2.0289175391197203, 'train/ce_loss': 0.46025390625, 'train/seg_cls_loss': 0.01396484375, 'train/kl_loss': 0.2947265625, 'train/mask_bce_loss': 0.20495636003324763, 'train/mask_dice_loss': 0.561113715171814, 'train/mask_loss': 0.7660700857639313, 'metrics/total_secs_per_batch': 7.776421070098877, 'metrics/data_secs_per_batch': 2.963644099235535, '_timestamp': 1740964524.4302223}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 41 is less than current step: 499. Dropping entry: {'train/lr': 5.644897959183673e-05, '_timestamp': 1740964524.4304934}).
Epoch: [4][ 43/500]	Time  7.046 ( 7.046)	Loss 1.2109 (1.5534)	CeLoss 1.2109 (0.5455)	SegCLSLoss 0.0000 (0.0154)	KLLoss 0.0000 (0.2631)	MaskLoss 0.0000 (0.4870)	MaskBCELoss 0.0000 (0.0801)	MaskDICELoss 0.0000 (0.4069)
Epoch: [4][ 44/500]	Time  9.660 ( 9.660)	Loss 2.0250 (2.0920)	CeLoss 0.1895 (0.2266)	SegCLSLoss 0.0300 (0.0210)	KLLoss 0.3633 (0.3730)	MaskLoss 0.8924 (0.9089)	MaskBCELoss 0.2234 (0.2476)	MaskDICELoss 0.6689 (0.6613)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 42 is less than current step: 499. Dropping entry: {'train/loss': 1.553418469429016, 'train/ce_loss': 0.5455078125, 'train/seg_cls_loss': 0.01536865234375, 'train/kl_loss': 0.2630859375, 'train/mask_bce_loss': 0.08009008560329675, 'train/mask_dice_loss': 0.4068730562925339, 'train/mask_loss': 0.48696314096450805, 'metrics/total_secs_per_batch': 7.046299457550049, 'metrics/data_secs_per_batch': 3.0110810518264772, '_timestamp': 1740964531.476742}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 42 is less than current step: 499. Dropping entry: {'train/lr': 5.632653061224489e-05, '_timestamp': 1740964531.4770951}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 43 is less than current step: 499. Dropping entry: {'train/loss': 2.091951608657837, 'train/ce_loss': 0.2265625, 'train/seg_cls_loss': 0.02098388671875, 'train/kl_loss': 0.373046875, 'train/mask_bce_loss': 0.2475867435336113, 'train/mask_dice_loss': 0.661279670894146, 'train/mask_loss': 0.908866411447525, 'metrics/total_secs_per_batch': 9.65994644165039, 'metrics/data_secs_per_batch': 4.329458904266358, '_timestamp': 1740964541.1365159}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 43 is less than current step: 499. Dropping entry: {'train/lr': 5.6204081632653056e-05, '_timestamp': 1740964541.1367872}).
Epoch: [4][ 45/500]	Time  8.008 ( 8.008)	Loss 1.6172 (1.7767)	CeLoss 1.6172 (0.4296)	SegCLSLoss 0.0000 (0.0157)	KLLoss 0.0000 (0.3000)	MaskLoss 0.0000 (0.6545)	MaskBCELoss 0.0000 (0.1513)	MaskDICELoss 0.0000 (0.5032)
Epoch: [4][ 46/500]	Time  8.661 ( 8.661)	Loss 1.4196 (1.6539)	CeLoss 0.2217 (0.4597)	SegCLSLoss 0.0192 (0.0140)	KLLoss 0.3711 (0.2949)	MaskLoss 0.5750 (0.5790)	MaskBCELoss 0.0508 (0.1156)	MaskDICELoss 0.5242 (0.4635)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 44 is less than current step: 499. Dropping entry: {'train/loss': 1.7767464995384217, 'train/ce_loss': 0.42958984375, 'train/seg_cls_loss': 0.015728759765625, 'train/kl_loss': 0.3, 'train/mask_bce_loss': 0.15133032761514187, 'train/mask_dice_loss': 0.5032050371170044, 'train/mask_loss': 0.6545353621244431, 'metrics/total_secs_per_batch': 8.008230686187744, 'metrics/data_secs_per_batch': 3.804383420944214, '_timestamp': 1740964549.1447408}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 44 is less than current step: 499. Dropping entry: {'train/lr': 5.608163265306122e-05, '_timestamp': 1740964549.1450248}).
Epoch: [4][ 47/500]	Time  9.418 ( 9.418)	Loss 0.1953 (1.7249)	CeLoss 0.1953 (0.3383)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.2924)	MaskLoss 0.0000 (0.6752)	MaskBCELoss 0.0000 (0.1105)	MaskDICELoss 0.0000 (0.5648)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 45 is less than current step: 499. Dropping entry: {'train/loss': 1.653931212425232, 'train/ce_loss': 0.45966796875, 'train/seg_cls_loss': 0.014007568359375, 'train/kl_loss': 0.294921875, 'train/mask_bce_loss': 0.11555102816782892, 'train/mask_dice_loss': 0.4634653672575951, 'train/mask_loss': 0.579016387462616, 'metrics/total_secs_per_batch': 8.660839796066284, 'metrics/data_secs_per_batch': 4.006091213226318, '_timestamp': 1740964557.8055558}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 45 is less than current step: 499. Dropping entry: {'train/lr': 5.595918367346938e-05, '_timestamp': 1740964557.805766}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 46 is less than current step: 499. Dropping entry: {'train/loss': 1.724897575378418, 'train/ce_loss': 0.33828125, 'train/seg_cls_loss': 0.0141845703125, 'train/kl_loss': 0.2923828125, 'train/mask_bce_loss': 0.11047235107980669, 'train/mask_dice_loss': 0.5647694051265717, 'train/mask_loss': 0.6752417504787445, 'metrics/total_secs_per_batch': 9.41774582862854, 'metrics/data_secs_per_batch': 4.541700530052185, '_timestamp': 1740964567.2233043}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 46 is less than current step: 499. Dropping entry: {'train/lr': 5.583673469387755e-05, '_timestamp': 1740964567.2235677}).
Epoch: [4][ 48/500]	Time  7.953 ( 7.953)	Loss 2.3920 (1.6285)	CeLoss 0.2754 (0.3913)	SegCLSLoss 0.0186 (0.0143)	KLLoss 0.3750 (0.2605)	MaskLoss 1.0349 (0.6020)	MaskBCELoss 0.2223 (0.0960)	MaskDICELoss 0.8126 (0.5060)
Epoch: [4][ 49/500]	Time  7.641 ( 7.641)	Loss 1.1250 (1.5626)	CeLoss 1.1250 (0.4119)	SegCLSLoss 0.0000 (0.0121)	KLLoss 0.0000 (0.2213)	MaskLoss 0.0000 (0.5612)	MaskBCELoss 0.0000 (0.1154)	MaskDICELoss 0.0000 (0.4459)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 47 is less than current step: 499. Dropping entry: {'train/loss': 1.6285131096839904, 'train/ce_loss': 0.39130859375, 'train/seg_cls_loss': 0.014306640625, 'train/kl_loss': 0.260546875, 'train/mask_bce_loss': 0.09597522336989642, 'train/mask_dice_loss': 0.5059766322374344, 'train/mask_loss': 0.6019518494606018, 'metrics/total_secs_per_batch': 7.952977657318115, 'metrics/data_secs_per_batch': 3.3783900260925295, '_timestamp': 1740964575.1762772}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 47 is less than current step: 499. Dropping entry: {'train/lr': 5.571428571428571e-05, '_timestamp': 1740964575.1765563}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 48 is less than current step: 499. Dropping entry: {'train/loss': 1.5625600814819336, 'train/ce_loss': 0.411865234375, 'train/seg_cls_loss': 0.012054443359375, 'train/kl_loss': 0.2212890625, 'train/mask_bce_loss': 0.1153599058277905, 'train/mask_dice_loss': 0.4458761870861053, 'train/mask_loss': 0.5612360835075378, 'metrics/total_secs_per_batch': 7.641488313674927, 'metrics/data_secs_per_batch': 3.8516753196716307, '_timestamp': 1740964582.8179557}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 48 is less than current step: 499. Dropping entry: {'train/lr': 5.559183673469387e-05, '_timestamp': 1740964582.8182878}).
[2025-03-02 19:16:29,175] [INFO] [logging.py:96:log_dist] [Rank 0] step=2050, skipped=0, lr=[5.553061224489795e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:16:29,181] [INFO] [timer.py:215:stop] epoch=0/micro_step=20500/global_step=2050, RunningAvgSamplesPerSec=1.4867003785895487, CurrSamplesPerSec=1.5717213782045625, MemAllocated=31.28GB, MaxMemAllocated=37.19GB
Epoch: [4][ 50/500]	Time  6.364 ( 6.364)	Loss 2.9287 (1.5939)	CeLoss 0.1797 (0.6716)	SegCLSLoss 0.0231 (0.0088)	KLLoss 0.3711 (0.1869)	MaskLoss 1.3501 (0.4497)	MaskBCELoss 0.4702 (0.0994)	MaskDICELoss 0.8799 (0.3502)
Epoch: [4][ 51/500]	Time  8.100 ( 8.100)	Loss 2.4832 (1.7792)	CeLoss 0.1885 (0.2958)	SegCLSLoss 0.0273 (0.0176)	KLLoss 0.3691 (0.2943)	MaskLoss 1.1220 (0.7226)	MaskBCELoss 0.3127 (0.0863)	MaskDICELoss 0.8093 (0.6363)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 49 is less than current step: 499. Dropping entry: {'train/loss': 1.59391530752182, 'train/ce_loss': 0.67158203125, 'train/seg_cls_loss': 0.0087646484375, 'train/kl_loss': 0.1869140625, 'train/mask_bce_loss': 0.09944842364639044, 'train/mask_dice_loss': 0.35024361312389374, 'train/mask_loss': 0.44969203174114225, 'metrics/total_secs_per_batch': 6.364192008972168, 'metrics/data_secs_per_batch': 2.511897921562195, '_timestamp': 1740964589.1817746}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 49 is less than current step: 499. Dropping entry: {'train/lr': 5.5469387755102035e-05, '_timestamp': 1740964589.1819584}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 50 is less than current step: 499. Dropping entry: {'train/loss': 1.7792210102081298, 'train/ce_loss': 0.29580078125, 'train/seg_cls_loss': 0.017620849609375, 'train/kl_loss': 0.2943359375, 'train/mask_bce_loss': 0.08631799910217523, 'train/mask_dice_loss': 0.6363003313541412, 'train/mask_loss': 0.7226183295249939, 'metrics/total_secs_per_batch': 8.100412130355835, 'metrics/data_secs_per_batch': 3.421062684059143, '_timestamp': 1740964597.282396}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 50 is less than current step: 499. Dropping entry: {'train/lr': 5.53469387755102e-05, '_timestamp': 1740964597.2826524}).
Epoch: [4][ 52/500]	Time  8.395 ( 8.395)	Loss 2.3695 (1.8178)	CeLoss 0.1895 (0.2894)	SegCLSLoss 0.0189 (0.0169)	KLLoss 0.3672 (0.2947)	MaskLoss 1.0666 (0.7453)	MaskBCELoss 0.3061 (0.0907)	MaskDICELoss 0.7605 (0.6546)
Epoch: [4][ 53/500]	Time  7.999 ( 7.999)	Loss 1.6504 (1.8326)	CeLoss 0.2217 (0.4106)	SegCLSLoss 0.0112 (0.0123)	KLLoss 0.3828 (0.2959)	MaskLoss 0.6924 (0.6930)	MaskBCELoss 0.1852 (0.1194)	MaskDICELoss 0.5072 (0.5737)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 51 is less than current step: 499. Dropping entry: {'train/loss': 1.8177666783332824, 'train/ce_loss': 0.28935546875, 'train/seg_cls_loss': 0.016943359375, 'train/kl_loss': 0.2947265625, 'train/mask_bce_loss': 0.09074852541089058, 'train/mask_dice_loss': 0.6545605897903443, 'train/mask_loss': 0.7453091204166412, 'metrics/total_secs_per_batch': 8.395285606384277, 'metrics/data_secs_per_batch': 3.9985955953598022, '_timestamp': 1740964605.6777153}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 51 is less than current step: 499. Dropping entry: {'train/lr': 5.522448979591836e-05, '_timestamp': 1740964605.677924}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 52 is less than current step: 499. Dropping entry: {'train/loss': 1.8325607657432557, 'train/ce_loss': 0.41064453125, 'train/seg_cls_loss': 0.012255859375, 'train/kl_loss': 0.2958984375, 'train/mask_bce_loss': 0.11935733314603567, 'train/mask_dice_loss': 0.5736808508634568, 'train/mask_loss': 0.6930381715297699, 'metrics/total_secs_per_batch': 7.99868369102478, 'metrics/data_secs_per_batch': 3.938476729393005, '_timestamp': 1740964613.6766148}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 52 is less than current step: 499. Dropping entry: {'train/lr': 5.510204081632653e-05, '_timestamp': 1740964613.6769745}).
Epoch: [4][ 54/500]	Time  8.710 ( 8.710)	Loss 1.1011 (1.6808)	CeLoss 0.2041 (0.5527)	SegCLSLoss 0.0128 (0.0115)	KLLoss 0.3750 (0.2209)	MaskLoss 0.4265 (0.5501)	MaskBCELoss 0.0951 (0.1164)	MaskDICELoss 0.3314 (0.4337)
Epoch: [4][ 55/500]	Time  9.139 ( 9.139)	Loss 2.1520 (1.8446)	CeLoss 0.1670 (0.2744)	SegCLSLoss 0.0303 (0.0175)	KLLoss 0.3867 (0.3025)	MaskLoss 0.9657 (0.7655)	MaskBCELoss 0.0984 (0.1228)	MaskDICELoss 0.8673 (0.6427)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 53 is less than current step: 499. Dropping entry: {'train/loss': 1.680774748325348, 'train/ce_loss': 0.552685546875, 'train/seg_cls_loss': 0.011480712890625, 'train/kl_loss': 0.2208984375, 'train/mask_bce_loss': 0.11641167448833585, 'train/mask_dice_loss': 0.43371690809726715, 'train/mask_loss': 0.5501285791397095, 'metrics/total_secs_per_batch': 8.709822654724121, 'metrics/data_secs_per_batch': 4.154293060302734, '_timestamp': 1740964622.386181}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 53 is less than current step: 499. Dropping entry: {'train/lr': 5.4979591836734694e-05, '_timestamp': 1740964622.3864753}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 54 is less than current step: 499. Dropping entry: {'train/loss': 1.8445581674575806, 'train/ce_loss': 0.2744140625, 'train/seg_cls_loss': 0.017474365234375, 'train/kl_loss': 0.3025390625, 'train/mask_bce_loss': 0.12281445190310478, 'train/mask_dice_loss': 0.6427263587713241, 'train/mask_loss': 0.7655407965183259, 'metrics/total_secs_per_batch': 9.139268159866333, 'metrics/data_secs_per_batch': 4.187742972373963, '_timestamp': 1740964631.5254347}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 54 is less than current step: 499. Dropping entry: {'train/lr': 5.485714285714285e-05, '_timestamp': 1740964631.5257256}).
Epoch: [4][ 56/500]	Time  5.966 ( 5.966)	Loss 0.9955 (1.6575)	CeLoss 0.3086 (0.5300)	SegCLSLoss 0.0098 (0.0126)	KLLoss 0.3730 (0.2592)	MaskLoss 0.3229 (0.5477)	MaskBCELoss 0.1051 (0.1181)	MaskDICELoss 0.2179 (0.4296)
Epoch: [4][ 57/500]	Time  7.691 ( 7.691)	Loss 2.2018 (1.6467)	CeLoss 0.3105 (0.7786)	SegCLSLoss 0.0098 (0.0073)	KLLoss 0.3711 (0.1859)	MaskLoss 0.9241 (0.4228)	MaskBCELoss 0.5115 (0.1233)	MaskDICELoss 0.4126 (0.2995)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 55 is less than current step: 499. Dropping entry: {'train/loss': 1.6575158059597015, 'train/ce_loss': 0.52998046875, 'train/seg_cls_loss': 0.012646484375, 'train/kl_loss': 0.2591796875, 'train/mask_bce_loss': 0.11807348849251867, 'train/mask_dice_loss': 0.4296297296881676, 'train/mask_loss': 0.5477032124996185, 'metrics/total_secs_per_batch': 5.965946912765503, 'metrics/data_secs_per_batch': 2.709101605415344, '_timestamp': 1740964637.4914064}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 55 is less than current step: 499. Dropping entry: {'train/lr': 5.473469387755101e-05, '_timestamp': 1740964637.4916794}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 56 is less than current step: 499. Dropping entry: {'train/loss': 1.6467107653617858, 'train/ce_loss': 0.77861328125, 'train/seg_cls_loss': 0.007269287109375, 'train/kl_loss': 0.1859375, 'train/mask_bce_loss': 0.12331715673208236, 'train/mask_dice_loss': 0.2995011031627655, 'train/mask_loss': 0.4228182673454285, 'metrics/total_secs_per_batch': 7.6906023025512695, 'metrics/data_secs_per_batch': 3.5355401754379274, '_timestamp': 1740964645.1820574}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 56 is less than current step: 499. Dropping entry: {'train/lr': 5.4612244897959176e-05, '_timestamp': 1740964645.1823215}).
Epoch: [4][ 58/500]	Time  8.456 ( 8.456)	Loss 2.0578 (1.3912)	CeLoss 0.2539 (0.5063)	SegCLSLoss 0.0148 (0.0090)	KLLoss 0.3633 (0.1818)	MaskLoss 0.8795 (0.4311)	MaskBCELoss 0.2127 (0.0439)	MaskDICELoss 0.6669 (0.3871)
Epoch: [4][ 59/500]	Time  7.786 ( 7.786)	Loss 1.6641 (1.3627)	CeLoss 1.6641 (0.4072)	SegCLSLoss 0.0000 (0.0123)	KLLoss 0.0000 (0.2553)	MaskLoss 0.0000 (0.4620)	MaskBCELoss 0.0000 (0.0627)	MaskDICELoss 0.0000 (0.3992)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 57 is less than current step: 499. Dropping entry: {'train/loss': 1.3911903619766235, 'train/ce_loss': 0.5063232421875, 'train/seg_cls_loss': 0.00897216796875, 'train/kl_loss': 0.1818359375, 'train/mask_bce_loss': 0.04392679333686829, 'train/mask_dice_loss': 0.38712982535362245, 'train/mask_loss': 0.43105661273002627, 'metrics/total_secs_per_batch': 8.455880880355835, 'metrics/data_secs_per_batch': 3.60599422454834, '_timestamp': 1740964653.6379015}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 57 is less than current step: 499. Dropping entry: {'train/lr': 5.4489795918367346e-05, '_timestamp': 1740964653.6381745}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 58 is less than current step: 499. Dropping entry: {'train/loss': 1.3626616835594176, 'train/ce_loss': 0.407177734375, 'train/seg_cls_loss': 0.012274169921875, 'train/kl_loss': 0.2552734375, 'train/mask_bce_loss': 0.06274443790316582, 'train/mask_dice_loss': 0.39922605603933337, 'train/mask_loss': 0.46197048723697665, 'metrics/total_secs_per_batch': 7.78614068031311, 'metrics/data_secs_per_batch': 3.5662371635437013, '_timestamp': 1740964661.4242337}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 58 is less than current step: 499. Dropping entry: {'train/lr': 5.436734693877551e-05, '_timestamp': 1740964661.4245768}).
[2025-03-02 19:17:49,809] [INFO] [logging.py:96:log_dist] [Rank 0] step=2060, skipped=0, lr=[5.430612244897959e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:17:49,815] [INFO] [timer.py:215:stop] epoch=0/micro_step=20600/global_step=2060, RunningAvgSamplesPerSec=1.485267005792177, CurrSamplesPerSec=1.191843042295029, MemAllocated=31.24GB, MaxMemAllocated=37.19GB
Epoch: [4][ 60/500]	Time  8.392 ( 8.392)	Loss 2.3820 (1.7325)	CeLoss 0.2266 (0.2832)	SegCLSLoss 0.0140 (0.0146)	KLLoss 0.3789 (0.2969)	MaskLoss 1.0553 (0.7062)	MaskBCELoss 0.1290 (0.1390)	MaskDICELoss 0.9263 (0.5672)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 59 is less than current step: 499. Dropping entry: {'train/loss': 1.732466471195221, 'train/ce_loss': 0.283203125, 'train/seg_cls_loss': 0.014556884765625, 'train/kl_loss': 0.296875, 'train/mask_bce_loss': 0.1389894164633006, 'train/mask_dice_loss': 0.5672340601682663, 'train/mask_loss': 0.7062234818935395, 'metrics/total_secs_per_batch': 8.392123937606812, 'metrics/data_secs_per_batch': 3.8199167490005492, '_timestamp': 1740964669.8159974}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 59 is less than current step: 499. Dropping entry: {'train/lr': 5.424489795918367e-05, '_timestamp': 1740964669.816262}).
Epoch: [4][ 61/500]	Time  8.524 ( 8.524)	Loss 1.0625 (1.3702)	CeLoss 1.0625 (0.4333)	SegCLSLoss 0.0000 (0.0124)	KLLoss 0.0000 (0.2193)	MaskLoss 0.0000 (0.4544)	MaskBCELoss 0.0000 (0.0359)	MaskDICELoss 0.0000 (0.4185)
Epoch: [4][ 62/500]	Time  8.911 ( 8.911)	Loss 3.6832 (1.9279)	CeLoss 0.2002 (0.2205)	SegCLSLoss 0.0201 (0.0173)	KLLoss 0.3613 (0.3703)	MaskLoss 1.7185 (0.8309)	MaskBCELoss 0.8811 (0.1944)	MaskDICELoss 0.8375 (0.6365)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 60 is less than current step: 499. Dropping entry: {'train/loss': 1.3701878666877747, 'train/ce_loss': 0.433349609375, 'train/seg_cls_loss': 0.01240234375, 'train/kl_loss': 0.2193359375, 'train/mask_bce_loss': 0.03594777397811413, 'train/mask_dice_loss': 0.418457692861557, 'train/mask_loss': 0.4544054627418518, 'metrics/total_secs_per_batch': 8.52433967590332, 'metrics/data_secs_per_batch': 3.799940991401672, '_timestamp': 1740964678.340464}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 60 is less than current step: 499. Dropping entry: {'train/lr': 5.412244897959183e-05, '_timestamp': 1740964678.3407245}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 61 is less than current step: 499. Dropping entry: {'train/loss': 1.927853709459305, 'train/ce_loss': 0.2205078125, 'train/seg_cls_loss': 0.017279052734375, 'train/kl_loss': 0.3703125, 'train/mask_bce_loss': 0.1943843157729134, 'train/mask_dice_loss': 0.6364859029650688, 'train/mask_loss': 0.8308702200651169, 'metrics/total_secs_per_batch': 8.910702466964722, 'metrics/data_secs_per_batch': 4.042158889770508, '_timestamp': 1740964687.2511997}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 61 is less than current step: 499. Dropping entry: {'train/lr': 5.399999999999999e-05, '_timestamp': 1740964687.2514713}).
Epoch: [4][ 63/500]	Time  8.315 ( 8.315)	Loss 2.6445 (1.9920)	CeLoss 0.2402 (0.3410)	SegCLSLoss 0.0184 (0.0148)	KLLoss 0.3652 (0.3312)	MaskLoss 1.1797 (0.8050)	MaskBCELoss 0.3238 (0.1449)	MaskDICELoss 0.8559 (0.6601)
Epoch: [4][ 64/500]	Time  8.067 ( 8.067)	Loss 1.3449 (2.0796)	CeLoss 0.2520 (0.4806)	SegCLSLoss 0.0110 (0.0152)	KLLoss 0.3730 (0.2936)	MaskLoss 0.5250 (0.7809)	MaskBCELoss 0.1761 (0.1999)	MaskDICELoss 0.3489 (0.5810)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 62 is less than current step: 499. Dropping entry: {'train/loss': 1.991993236541748, 'train/ce_loss': 0.341015625, 'train/seg_cls_loss': 0.0148193359375, 'train/kl_loss': 0.33125, 'train/mask_bce_loss': 0.1448708612471819, 'train/mask_dice_loss': 0.660110130906105, 'train/mask_loss': 0.8049810022115708, 'metrics/total_secs_per_batch': 8.314861536026001, 'metrics/data_secs_per_batch': 3.614147686958313, '_timestamp': 1740964695.5661206}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 62 is less than current step: 499. Dropping entry: {'train/lr': 5.3877551020408154e-05, '_timestamp': 1740964695.566387}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 63 is less than current step: 499. Dropping entry: {'train/loss': 2.079620695114136, 'train/ce_loss': 0.48056640625, 'train/seg_cls_loss': 0.01524658203125, 'train/kl_loss': 0.2935546875, 'train/mask_bce_loss': 0.19990004543215037, 'train/mask_dice_loss': 0.581023582816124, 'train/mask_loss': 0.7809236407279968, 'metrics/total_secs_per_batch': 8.067494630813599, 'metrics/data_secs_per_batch': 3.5204883098602293, '_timestamp': 1740964703.6335852}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 63 is less than current step: 499. Dropping entry: {'train/lr': 5.3755102040816324e-05, '_timestamp': 1740964703.6338527}).
Epoch: [4][ 65/500]	Time  8.058 ( 8.058)	Loss 1.7080 (1.5067)	CeLoss 0.2578 (0.3371)	SegCLSLoss 0.0131 (0.0096)	KLLoss 0.3711 (0.2975)	MaskLoss 0.7036 (0.5676)	MaskBCELoss 0.2244 (0.1429)	MaskDICELoss 0.4792 (0.4247)
Epoch: [4][ 66/500]	Time  8.603 ( 8.603)	Loss 1.0195 (1.5807)	CeLoss 0.2227 (0.4353)	SegCLSLoss 0.0108 (0.0164)	KLLoss 0.3711 (0.2967)	MaskLoss 0.3770 (0.5538)	MaskBCELoss 0.0966 (0.0823)	MaskDICELoss 0.2803 (0.4715)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 64 is less than current step: 499. Dropping entry: {'train/loss': 1.5066989541053772, 'train/ce_loss': 0.337109375, 'train/seg_cls_loss': 0.009576416015625, 'train/kl_loss': 0.2974609375, 'train/mask_bce_loss': 0.14286986109800637, 'train/mask_dice_loss': 0.4246886044740677, 'train/mask_loss': 0.5675584614276886, 'metrics/total_secs_per_batch': 8.05787992477417, 'metrics/data_secs_per_batch': 3.9308773040771485, '_timestamp': 1740964711.6915412}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 64 is less than current step: 499. Dropping entry: {'train/lr': 5.363265306122449e-05, '_timestamp': 1740964711.6918352}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 65 is less than current step: 499. Dropping entry: {'train/loss': 1.5806809425354005, 'train/ce_loss': 0.43525390625, 'train/seg_cls_loss': 0.01640625, 'train/kl_loss': 0.2966796875, 'train/mask_bce_loss': 0.08227109690196813, 'train/mask_dice_loss': 0.471545934677124, 'train/mask_loss': 0.5538170367479325, 'metrics/total_secs_per_batch': 8.603418111801147, 'metrics/data_secs_per_batch': 4.180554366111755, '_timestamp': 1740964720.2950037}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 65 is less than current step: 499. Dropping entry: {'train/lr': 5.351020408163265e-05, '_timestamp': 1740964720.2953517}).
Epoch: [4][ 67/500]	Time  7.101 ( 7.101)	Loss 2.3643 (1.7977)	CeLoss 0.1396 (0.3415)	SegCLSLoss 0.0371 (0.0183)	KLLoss 0.3848 (0.3344)	MaskLoss 1.0835 (0.7068)	MaskBCELoss 0.2039 (0.1337)	MaskDICELoss 0.8796 (0.5731)
Epoch: [4][ 68/500]	Time  8.000 ( 8.000)	Loss 1.5713 (1.4998)	CeLoss 0.2227 (0.4539)	SegCLSLoss 0.0186 (0.0132)	KLLoss 0.3613 (0.2582)	MaskLoss 0.6509 (0.5066)	MaskBCELoss 0.0624 (0.1153)	MaskDICELoss 0.5885 (0.3914)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 66 is less than current step: 499. Dropping entry: {'train/loss': 1.797677719593048, 'train/ce_loss': 0.34150390625, 'train/seg_cls_loss': 0.018255615234375, 'train/kl_loss': 0.334375, 'train/mask_bce_loss': 0.13367736130021513, 'train/mask_dice_loss': 0.5731204718351364, 'train/mask_loss': 0.706797844171524, 'metrics/total_secs_per_batch': 7.101350784301758, 'metrics/data_secs_per_batch': 3.166727566719055, '_timestamp': 1740964727.3962164}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 66 is less than current step: 499. Dropping entry: {'train/lr': 5.338775510204081e-05, '_timestamp': 1740964727.3963983}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 67 is less than current step: 499. Dropping entry: {'train/loss': 1.4997848391532898, 'train/ce_loss': 0.453857421875, 'train/seg_cls_loss': 0.013201904296875, 'train/kl_loss': 0.258203125, 'train/mask_bce_loss': 0.1152644120156765, 'train/mask_dice_loss': 0.3913662821054459, 'train/mask_loss': 0.5066307008266449, 'metrics/total_secs_per_batch': 7.999508380889893, 'metrics/data_secs_per_batch': 4.132804441452026, '_timestamp': 1740964735.395768}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 67 is less than current step: 499. Dropping entry: {'train/lr': 5.326530612244897e-05, '_timestamp': 1740964735.3960297}).
Epoch: [4][ 69/500]	Time  7.637 ( 7.637)	Loss 2.4535 (2.0403)	CeLoss 0.3164 (0.4475)	SegCLSLoss 0.0096 (0.0136)	KLLoss 0.3613 (0.2943)	MaskLoss 1.0490 (0.7783)	MaskBCELoss 0.4530 (0.1454)	MaskDICELoss 0.5960 (0.6329)
[2025-03-02 19:19:11,481] [INFO] [logging.py:96:log_dist] [Rank 0] step=2070, skipped=0, lr=[5.308163265306122e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:19:11,487] [INFO] [timer.py:215:stop] epoch=0/micro_step=20700/global_step=2070, RunningAvgSamplesPerSec=1.4837396609017965, CurrSamplesPerSec=1.182899702564873, MemAllocated=31.25GB, MaxMemAllocated=37.19GB
Epoch: [4][ 70/500]	Time  8.455 ( 8.455)	Loss 2.3102 (1.4473)	CeLoss 0.2344 (0.4483)	SegCLSLoss 0.0211 (0.0125)	KLLoss 0.3594 (0.2539)	MaskLoss 1.0145 (0.4835)	MaskBCELoss 0.1190 (0.0703)	MaskDICELoss 0.8955 (0.4132)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 68 is less than current step: 499. Dropping entry: {'train/loss': 2.040299391746521, 'train/ce_loss': 0.4474609375, 'train/seg_cls_loss': 0.013568115234375, 'train/kl_loss': 0.2943359375, 'train/mask_bce_loss': 0.14535852940753102, 'train/mask_dice_loss': 0.6328966319561005, 'train/mask_loss': 0.7782551765441894, 'metrics/total_secs_per_batch': 7.637325763702393, 'metrics/data_secs_per_batch': 3.5849167108535767, '_timestamp': 1740964743.033054}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 68 is less than current step: 499. Dropping entry: {'train/lr': 5.314285714285713e-05, '_timestamp': 1740964743.0333133}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 69 is less than current step: 499. Dropping entry: {'train/loss': 1.4472578167915344, 'train/ce_loss': 0.44833984375, 'train/seg_cls_loss': 0.012548828125, 'train/kl_loss': 0.25390625, 'train/mask_bce_loss': 0.07026952542364598, 'train/mask_dice_loss': 0.41322265565395355, 'train/mask_loss': 0.4834921836853027, 'metrics/total_secs_per_batch': 8.4552583694458, 'metrics/data_secs_per_batch': 4.026152491569519, '_timestamp': 1740964751.4881475}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 69 is less than current step: 499. Dropping entry: {'train/lr': 5.30204081632653e-05, '_timestamp': 1740964751.488402}).
Epoch: [4][ 71/500]	Time  6.496 ( 6.496)	Loss 0.9219 (1.7319)	CeLoss 0.9219 (0.7128)	SegCLSLoss 0.0000 (0.0098)	KLLoss 0.0000 (0.1820)	MaskLoss 0.0000 (0.4979)	MaskBCELoss 0.0000 (0.1200)	MaskDICELoss 0.0000 (0.3778)
Epoch: [4][ 72/500]	Time  7.645 ( 7.645)	Loss 2.6271 (1.5548)	CeLoss 0.2227 (0.2963)	SegCLSLoss 0.0110 (0.0139)	KLLoss 0.3672 (0.2973)	MaskLoss 1.1807 (0.6108)	MaskBCELoss 0.7931 (0.2017)	MaskDICELoss 0.3876 (0.4092)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 70 is less than current step: 499. Dropping entry: {'train/loss': 1.731893253326416, 'train/ce_loss': 0.71279296875, 'train/seg_cls_loss': 0.009783935546875, 'train/kl_loss': 0.18203125, 'train/mask_bce_loss': 0.12004084940999746, 'train/mask_dice_loss': 0.37783937752246854, 'train/mask_loss': 0.49788021445274355, 'metrics/total_secs_per_batch': 6.4964516162872314, 'metrics/data_secs_per_batch': 3.0295440912246705, '_timestamp': 1740964757.984874}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 70 is less than current step: 499. Dropping entry: {'train/lr': 5.2897959183673465e-05, '_timestamp': 1740964757.985169}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 71 is less than current step: 499. Dropping entry: {'train/loss': 1.5548247337341308, 'train/ce_loss': 0.296337890625, 'train/seg_cls_loss': 0.013861083984375, 'train/kl_loss': 0.297265625, 'train/mask_bce_loss': 0.201665950845927, 'train/mask_dice_loss': 0.4091692790389061, 'train/mask_loss': 0.6108352333307266, 'metrics/total_secs_per_batch': 7.645063877105713, 'metrics/data_secs_per_batch': 3.5547120809555053, '_timestamp': 1740964765.6299949}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 71 is less than current step: 499. Dropping entry: {'train/lr': 5.277551020408163e-05, '_timestamp': 1740964765.6303008}).
Epoch: [4][ 73/500]	Time  6.487 ( 6.487)	Loss 1.7174 (1.1807)	CeLoss 0.2227 (0.4823)	SegCLSLoss 0.0150 (0.0108)	KLLoss 0.3613 (0.2205)	MaskLoss 0.7259 (0.3356)	MaskBCELoss 0.0088 (0.0460)	MaskDICELoss 0.7171 (0.2895)
Epoch: [4][ 74/500]	Time  6.924 ( 6.924)	Loss 0.0762 (1.3540)	CeLoss 0.0762 (0.5354)	SegCLSLoss 0.0000 (0.0080)	KLLoss 0.0000 (0.1846)	MaskLoss 0.0000 (0.3980)	MaskBCELoss 0.0000 (0.0656)	MaskDICELoss 0.0000 (0.3324)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 72 is less than current step: 499. Dropping entry: {'train/loss': 1.180709719657898, 'train/ce_loss': 0.4822509765625, 'train/seg_cls_loss': 0.0108154296875, 'train/kl_loss': 0.2205078125, 'train/mask_bce_loss': 0.0460467011667788, 'train/mask_dice_loss': 0.28951078951358794, 'train/mask_loss': 0.33555748611688613, 'metrics/total_secs_per_batch': 6.487257242202759, 'metrics/data_secs_per_batch': 2.4654560804367067, '_timestamp': 1740964772.1171088}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 72 is less than current step: 499. Dropping entry: {'train/lr': 5.265306122448979e-05, '_timestamp': 1740964772.117335}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 73 is less than current step: 499. Dropping entry: {'train/loss': 1.3539684534072876, 'train/ce_loss': 0.53544921875, 'train/seg_cls_loss': 0.007977294921875, 'train/kl_loss': 0.1845703125, 'train/mask_bce_loss': 0.06556034684181214, 'train/mask_dice_loss': 0.3324199736118317, 'train/mask_loss': 0.3979803204536438, 'metrics/total_secs_per_batch': 6.923804044723511, 'metrics/data_secs_per_batch': 2.9992095947265627, '_timestamp': 1740964779.040928}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 73 is less than current step: 499. Dropping entry: {'train/lr': 5.2530612244897955e-05, '_timestamp': 1740964779.0411854}).
Epoch: [4][ 75/500]	Time  6.016 ( 6.016)	Loss 0.7707 (1.2446)	CeLoss 0.1846 (0.6818)	SegCLSLoss 0.0437 (0.0110)	KLLoss 0.3633 (0.1832)	MaskLoss 0.2642 (0.2694)	MaskBCELoss 0.1023 (0.0303)	MaskDICELoss 0.1620 (0.2391)
Epoch: [4][ 76/500]	Time  6.923 ( 6.923)	Loss 2.6806 (1.7925)	CeLoss 0.1641 (0.4104)	SegCLSLoss 0.0097 (0.0125)	KLLoss 0.3594 (0.2906)	MaskLoss 1.2377 (0.6732)	MaskBCELoss 0.2401 (0.0833)	MaskDICELoss 0.9976 (0.5899)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 74 is less than current step: 499. Dropping entry: {'train/loss': 1.2445604264736176, 'train/ce_loss': 0.6818359375, 'train/seg_cls_loss': 0.01103515625, 'train/kl_loss': 0.183203125, 'train/mask_bce_loss': 0.030322618223726748, 'train/mask_dice_loss': 0.2391255557537079, 'train/mask_loss': 0.2694481760263443, 'metrics/total_secs_per_batch': 6.0159385204315186, 'metrics/data_secs_per_batch': 2.5195931434631347, '_timestamp': 1740964785.0568492}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 74 is less than current step: 499. Dropping entry: {'train/lr': 5.2408163265306124e-05, '_timestamp': 1740964785.0571167}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 75 is less than current step: 499. Dropping entry: {'train/loss': 1.7924681663513184, 'train/ce_loss': 0.41044921875, 'train/seg_cls_loss': 0.012548828125, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.08330829704646021, 'train/mask_dice_loss': 0.5899277478456497, 'train/mask_loss': 0.6732360422611237, 'metrics/total_secs_per_batch': 6.923389434814453, 'metrics/data_secs_per_batch': 3.5527989625930787, '_timestamp': 1740964791.9802551}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 75 is less than current step: 499. Dropping entry: {'train/lr': 5.228571428571428e-05, '_timestamp': 1740964791.9805248}).
Epoch: [4][ 77/500]	Time  7.415 ( 7.415)	Loss 2.1416 (1.6117)	CeLoss 0.2520 (0.6617)	SegCLSLoss 0.0288 (0.0113)	KLLoss 0.3633 (0.1807)	MaskLoss 0.9194 (0.4633)	MaskBCELoss 0.0408 (0.0499)	MaskDICELoss 0.8786 (0.4134)
Epoch: [4][ 78/500]	Time  7.648 ( 7.648)	Loss 2.0236 (1.6529)	CeLoss 0.1797 (0.4723)	SegCLSLoss 0.0266 (0.0150)	KLLoss 0.3613 (0.2928)	MaskLoss 0.8971 (0.5717)	MaskBCELoss 0.0443 (0.0659)	MaskDICELoss 0.8528 (0.5058)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 76 is less than current step: 499. Dropping entry: {'train/loss': 1.6116691470146178, 'train/ce_loss': 0.66171875, 'train/seg_cls_loss': 0.01126708984375, 'train/kl_loss': 0.1806640625, 'train/mask_bce_loss': 0.04990013353526592, 'train/mask_dice_loss': 0.413356339931488, 'train/mask_loss': 0.4632564663887024, 'metrics/total_secs_per_batch': 7.414839744567871, 'metrics/data_secs_per_batch': 3.701434540748596, '_timestamp': 1740964799.3951402}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 76 is less than current step: 499. Dropping entry: {'train/lr': 5.2163265306122444e-05, '_timestamp': 1740964799.3954742}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 77 is less than current step: 499. Dropping entry: {'train/loss': 1.6528656125068664, 'train/ce_loss': 0.472265625, 'train/seg_cls_loss': 0.015032958984375, 'train/kl_loss': 0.2927734375, 'train/mask_bce_loss': 0.06586633287370205, 'train/mask_dice_loss': 0.5058301359415054, 'train/mask_loss': 0.5716964691877365, 'metrics/total_secs_per_batch': 7.647926330566406, 'metrics/data_secs_per_batch': 3.4579134941101075, '_timestamp': 1740964807.043133}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 77 is less than current step: 499. Dropping entry: {'train/lr': 5.204081632653061e-05, '_timestamp': 1740964807.0434365}).
Epoch: [4][ 79/500]	Time  7.319 ( 7.319)	Loss 1.9437 (1.5322)	CeLoss 0.2949 (0.5142)	SegCLSLoss 0.0131 (0.0096)	KLLoss 0.3652 (0.2572)	MaskLoss 0.8029 (0.4938)	MaskBCELoss 0.0524 (0.1524)	MaskDICELoss 0.7505 (0.3415)
[2025-03-02 19:20:23,981] [INFO] [logging.py:96:log_dist] [Rank 0] step=2080, skipped=0, lr=[5.185714285714285e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:20:23,987] [INFO] [timer.py:215:stop] epoch=0/micro_step=20800/global_step=2080, RunningAvgSamplesPerSec=1.4832005342092414, CurrSamplesPerSec=1.0390229373823985, MemAllocated=31.45GB, MaxMemAllocated=37.19GB
Epoch: [4][ 80/500]	Time  9.626 ( 9.626)	Loss 1.6619 (1.9457)	CeLoss 0.2617 (0.3442)	SegCLSLoss 0.0143 (0.0164)	KLLoss 0.3652 (0.3287)	MaskLoss 0.6776 (0.7800)	MaskBCELoss 0.0423 (0.1971)	MaskDICELoss 0.6354 (0.5829)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 78 is less than current step: 499. Dropping entry: {'train/loss': 1.532203161716461, 'train/ce_loss': 0.51416015625, 'train/seg_cls_loss': 0.009649658203125, 'train/kl_loss': 0.2572265625, 'train/mask_bce_loss': 0.152352955006063, 'train/mask_dice_loss': 0.34148299843072893, 'train/mask_loss': 0.49383595287799836, 'metrics/total_secs_per_batch': 7.319173336029053, 'metrics/data_secs_per_batch': 3.224763035774231, '_timestamp': 1740964814.3622103}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 78 is less than current step: 499. Dropping entry: {'train/lr': 5.191836734693877e-05, '_timestamp': 1740964814.36253}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 79 is less than current step: 499. Dropping entry: {'train/loss': 1.9457239985466004, 'train/ce_loss': 0.34423828125, 'train/seg_cls_loss': 0.016400146484375, 'train/kl_loss': 0.3287109375, 'train/mask_bce_loss': 0.19713113866746426, 'train/mask_dice_loss': 0.5828597545623779, 'train/mask_loss': 0.7799908876419067, 'metrics/total_secs_per_batch': 9.626048564910889, 'metrics/data_secs_per_batch': 3.957785224914551, '_timestamp': 1740964823.9881577}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 79 is less than current step: 499. Dropping entry: {'train/lr': 5.179591836734693e-05, '_timestamp': 1740964823.9885087}).
Epoch: [4][ 81/500]	Time  7.914 ( 7.914)	Loss 1.6554 (1.7970)	CeLoss 0.2129 (0.4177)	SegCLSLoss 0.0200 (0.0145)	KLLoss 0.3555 (0.2922)	MaskLoss 0.6988 (0.6715)	MaskBCELoss 0.0799 (0.1094)	MaskDICELoss 0.6189 (0.5621)
Epoch: [4][ 82/500]	Time  8.058 ( 8.058)	Loss 2.6403 (1.8169)	CeLoss 0.1953 (0.4565)	SegCLSLoss 0.0236 (0.0161)	KLLoss 0.3652 (0.2928)	MaskLoss 1.1981 (0.6616)	MaskBCELoss 0.2906 (0.1189)	MaskDICELoss 0.9075 (0.5426)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 80 is less than current step: 499. Dropping entry: {'train/loss': 1.7970375657081603, 'train/ce_loss': 0.41767578125, 'train/seg_cls_loss': 0.01453857421875, 'train/kl_loss': 0.2921875, 'train/mask_bce_loss': 0.10939612872898578, 'train/mask_dice_loss': 0.5620718479156495, 'train/mask_loss': 0.6714679777622223, 'metrics/total_secs_per_batch': 7.914409637451172, 'metrics/data_secs_per_batch': 3.4203123092651366, '_timestamp': 1740964831.9027092}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 80 is less than current step: 499. Dropping entry: {'train/lr': 5.16734693877551e-05, '_timestamp': 1740964831.9029973}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 81 is less than current step: 499. Dropping entry: {'train/loss': 1.8169321179389955, 'train/ce_loss': 0.45654296875, 'train/seg_cls_loss': 0.0160888671875, 'train/kl_loss': 0.2927734375, 'train/mask_bce_loss': 0.11894236356019974, 'train/mask_dice_loss': 0.5426486819982529, 'train/mask_loss': 0.6615910470485687, 'metrics/total_secs_per_batch': 8.057539224624634, 'metrics/data_secs_per_batch': 3.7507948875427246, '_timestamp': 1740964839.9603934}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 81 is less than current step: 499. Dropping entry: {'train/lr': 5.1551020408163266e-05, '_timestamp': 1740964839.9607272}).
Epoch: [4][ 83/500]	Time  7.382 ( 7.382)	Loss 1.2656 (1.2127)	CeLoss 1.2656 (0.4725)	SegCLSLoss 0.0000 (0.0099)	KLLoss 0.0000 (0.2203)	MaskLoss 0.0000 (0.3563)	MaskBCELoss 0.0000 (0.0469)	MaskDICELoss 0.0000 (0.3094)
Epoch: [4][ 84/500]	Time  8.373 ( 8.373)	Loss 1.9019 (1.6051)	CeLoss 0.2207 (0.2021)	SegCLSLoss 0.0160 (0.0171)	KLLoss 0.3672 (0.3270)	MaskLoss 0.8182 (0.6809)	MaskBCELoss 0.0114 (0.0687)	MaskDICELoss 0.8068 (0.6122)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 82 is less than current step: 499. Dropping entry: {'train/loss': 1.2126924037933349, 'train/ce_loss': 0.4724609375, 'train/seg_cls_loss': 0.009893798828125, 'train/kl_loss': 0.2203125, 'train/mask_bce_loss': 0.04689499288797379, 'train/mask_dice_loss': 0.30940237939357756, 'train/mask_loss': 0.35629737228155134, 'metrics/total_secs_per_batch': 7.381966829299927, 'metrics/data_secs_per_batch': 3.4751450061798095, '_timestamp': 1740964847.3421896}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 82 is less than current step: 499. Dropping entry: {'train/lr': 5.142857142857142e-05, '_timestamp': 1740964847.3424652}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 83 is less than current step: 499. Dropping entry: {'train/loss': 1.6050656676292419, 'train/ce_loss': 0.202099609375, 'train/seg_cls_loss': 0.0170654296875, 'train/kl_loss': 0.326953125, 'train/mask_bce_loss': 0.0686685455031693, 'train/mask_dice_loss': 0.6122090071439743, 'train/mask_loss': 0.680877560377121, 'metrics/total_secs_per_batch': 8.37265419960022, 'metrics/data_secs_per_batch': 3.4021761894226072, '_timestamp': 1740964855.7148473}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 83 is less than current step: 499. Dropping entry: {'train/lr': 5.1306122448979585e-05, '_timestamp': 1740964855.7151208}).
Epoch: [4][ 85/500]	Time  7.254 ( 7.254)	Loss 1.4329 (1.7158)	CeLoss 0.2412 (0.2891)	SegCLSLoss 0.0128 (0.0127)	KLLoss 0.3691 (0.2588)	MaskLoss 0.5739 (0.6971)	MaskBCELoss 0.0723 (0.2046)	MaskDICELoss 0.5016 (0.4925)
Epoch: [4][ 86/500]	Time  9.283 ( 9.283)	Loss 1.3033 (1.9026)	CeLoss 0.2041 (0.2062)	SegCLSLoss 0.0244 (0.0185)	KLLoss 0.3555 (0.3645)	MaskLoss 0.5257 (0.8252)	MaskBCELoss 0.1082 (0.1229)	MaskDICELoss 0.4175 (0.7023)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 84 is less than current step: 499. Dropping entry: {'train/loss': 1.7158124089241027, 'train/ce_loss': 0.2890625, 'train/seg_cls_loss': 0.0126953125, 'train/kl_loss': 0.2587890625, 'train/mask_bce_loss': 0.20463172867894172, 'train/mask_dice_loss': 0.4924834638834, 'train/mask_loss': 0.6971151888370514, 'metrics/total_secs_per_batch': 7.25407075881958, 'metrics/data_secs_per_batch': 3.254218101501465, '_timestamp': 1740964862.9690907}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 84 is less than current step: 499. Dropping entry: {'train/lr': 5.118367346938775e-05, '_timestamp': 1740964862.969429}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 85 is less than current step: 499. Dropping entry: {'train/loss': 1.9025958061218262, 'train/ce_loss': 0.20625, 'train/seg_cls_loss': 0.01851806640625, 'train/kl_loss': 0.364453125, 'train/mask_bce_loss': 0.1229005791246891, 'train/mask_dice_loss': 0.7022742927074432, 'train/mask_loss': 0.8251748591661453, 'metrics/total_secs_per_batch': 9.282763719558716, 'metrics/data_secs_per_batch': 4.221975207328796, '_timestamp': 1740964872.2517147}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 85 is less than current step: 499. Dropping entry: {'train/lr': 5.106122448979591e-05, '_timestamp': 1740964872.2519917}).
Epoch: [4][ 87/500]	Time  7.044 ( 7.044)	Loss 2.3424 (1.4504)	CeLoss 0.2266 (0.4372)	SegCLSLoss 0.0190 (0.0118)	KLLoss 0.3633 (0.2609)	MaskLoss 1.0345 (0.4904)	MaskBCELoss 0.3393 (0.1501)	MaskDICELoss 0.6952 (0.3403)
Epoch: [4][ 88/500]	Time  7.151 ( 7.151)	Loss 2.6085 (1.4329)	CeLoss 0.2314 (0.4597)	SegCLSLoss 0.0210 (0.0129)	KLLoss 0.3750 (0.2594)	MaskLoss 1.1646 (0.4703)	MaskBCELoss 0.3154 (0.1155)	MaskDICELoss 0.8492 (0.3548)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 86 is less than current step: 499. Dropping entry: {'train/loss': 1.4504092454910278, 'train/ce_loss': 0.43720703125, 'train/seg_cls_loss': 0.011846923828125, 'train/kl_loss': 0.2609375, 'train/mask_bce_loss': 0.15010637529194354, 'train/mask_dice_loss': 0.34028380215167997, 'train/mask_loss': 0.4903901740908623, 'metrics/total_secs_per_batch': 7.043569803237915, 'metrics/data_secs_per_batch': 2.64873321056366, '_timestamp': 1740964879.2954702}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 86 is less than current step: 499. Dropping entry: {'train/lr': 5.093877551020408e-05, '_timestamp': 1740964879.2958195}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 87 is less than current step: 499. Dropping entry: {'train/loss': 1.432932847738266, 'train/ce_loss': 0.459716796875, 'train/seg_cls_loss': 0.01287841796875, 'train/kl_loss': 0.259375, 'train/mask_bce_loss': 0.11549177803099156, 'train/mask_dice_loss': 0.3548076495528221, 'train/mask_loss': 0.470299431681633, 'metrics/total_secs_per_batch': 7.15097975730896, 'metrics/data_secs_per_batch': 3.4097534656524657, '_timestamp': 1740964886.4462473}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 87 is less than current step: 499. Dropping entry: {'train/lr': 5.0816326530612244e-05, '_timestamp': 1740964886.4465399}).
Epoch: [4][ 89/500]	Time  8.035 ( 8.035)	Loss 0.6549 (1.4978)	CeLoss 0.2461 (0.2992)	SegCLSLoss 0.0149 (0.0153)	KLLoss 0.3594 (0.3303)	MaskLoss 0.1829 (0.5790)	MaskBCELoss 0.0152 (0.1265)	MaskDICELoss 0.1677 (0.4525)
[2025-03-02 19:21:43,110] [INFO] [logging.py:96:log_dist] [Rank 0] step=2090, skipped=0, lr=[5.063265306122448e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:21:43,116] [INFO] [timer.py:215:stop] epoch=0/micro_step=20900/global_step=2090, RunningAvgSamplesPerSec=1.4819694581088763, CurrSamplesPerSec=1.158258537319387, MemAllocated=31.26GB, MaxMemAllocated=37.19GB
Epoch: [4][ 90/500]	Time  8.635 ( 8.635)	Loss 2.1735 (1.8363)	CeLoss 0.1865 (0.3381)	SegCLSLoss 0.0216 (0.0127)	KLLoss 0.3555 (0.2900)	MaskLoss 0.9705 (0.7314)	MaskBCELoss 0.0279 (0.1847)	MaskDICELoss 0.9427 (0.5467)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 88 is less than current step: 499. Dropping entry: {'train/loss': 1.497781217098236, 'train/ce_loss': 0.29921875, 'train/seg_cls_loss': 0.01529541015625, 'train/kl_loss': 0.3302734375, 'train/mask_bce_loss': 0.1264903897419572, 'train/mask_dice_loss': 0.4524783492088318, 'train/mask_loss': 0.5789687380194664, 'metrics/total_secs_per_batch': 8.035470724105835, 'metrics/data_secs_per_batch': 3.447175621986389, '_timestamp': 1740964894.481716}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 88 is less than current step: 499. Dropping entry: {'train/lr': 5.06938775510204e-05, '_timestamp': 1740964894.4819815}).
Epoch: [4][ 91/500]	Time  7.922 ( 7.922)	Loss 2.6674 (1.7452)	CeLoss 0.2520 (0.4417)	SegCLSLoss 0.0178 (0.0140)	KLLoss 0.3672 (0.2916)	MaskLoss 1.1843 (0.6336)	MaskBCELoss 0.2496 (0.0551)	MaskDICELoss 0.9347 (0.5785)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 89 is less than current step: 499. Dropping entry: {'train/loss': 1.836348283290863, 'train/ce_loss': 0.3380859375, 'train/seg_cls_loss': 0.01265869140625, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.18472289703786374, 'train/mask_dice_loss': 0.546683669090271, 'train/mask_loss': 0.7314065635204315, 'metrics/total_secs_per_batch': 8.635176420211792, 'metrics/data_secs_per_batch': 3.767581081390381, '_timestamp': 1740964903.1167893}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 89 is less than current step: 499. Dropping entry: {'train/lr': 5.0571428571428564e-05, '_timestamp': 1740964903.117103}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 90 is less than current step: 499. Dropping entry: {'train/loss': 1.7451710760593415, 'train/ce_loss': 0.44169921875, 'train/seg_cls_loss': 0.01402587890625, 'train/kl_loss': 0.2916015625, 'train/mask_bce_loss': 0.05512356329709291, 'train/mask_dice_loss': 0.5784970998764039, 'train/mask_loss': 0.6336206696927548, 'metrics/total_secs_per_batch': 7.921831846237183, 'metrics/data_secs_per_batch': 3.618466854095459, '_timestamp': 1740964911.0387318}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 90 is less than current step: 499. Dropping entry: {'train/lr': 5.0448979591836727e-05, '_timestamp': 1740964911.0389967}).
Epoch: [4][ 92/500]	Time  8.273 ( 8.273)	Loss 2.9376 (1.7323)	CeLoss 0.3301 (0.4300)	SegCLSLoss 0.0201 (0.0119)	KLLoss 0.3652 (0.2549)	MaskLoss 1.2803 (0.6355)	MaskBCELoss 0.4897 (0.1189)	MaskDICELoss 0.7906 (0.5165)
Epoch: [4][ 93/500]	Time  9.790 ( 9.790)	Loss 1.3539 (1.6317)	CeLoss 0.2480 (0.2626)	SegCLSLoss 0.0197 (0.0157)	KLLoss 0.3691 (0.3311)	MaskLoss 0.5295 (0.6641)	MaskBCELoss 0.0877 (0.1755)	MaskDICELoss 0.4418 (0.4886)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 91 is less than current step: 499. Dropping entry: {'train/loss': 1.7323387384414672, 'train/ce_loss': 0.430029296875, 'train/seg_cls_loss': 0.011865234375, 'train/kl_loss': 0.2548828125, 'train/mask_bce_loss': 0.11893396135419607, 'train/mask_dice_loss': 0.5165469348430634, 'train/mask_loss': 0.6354808926582336, 'metrics/total_secs_per_batch': 8.27272343635559, 'metrics/data_secs_per_batch': 3.8823358535766603, '_timestamp': 1740964919.311428}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 91 is less than current step: 499. Dropping entry: {'train/lr': 5.0326530612244896e-05, '_timestamp': 1740964919.3116083}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 92 is less than current step: 499. Dropping entry: {'train/loss': 1.6316532075405121, 'train/ce_loss': 0.26259765625, 'train/seg_cls_loss': 0.01571044921875, 'train/kl_loss': 0.3310546875, 'train/mask_bce_loss': 0.17550493739545345, 'train/mask_dice_loss': 0.48856382966041567, 'train/mask_loss': 0.6640687704086303, 'metrics/total_secs_per_batch': 9.790100336074829, 'metrics/data_secs_per_batch': 4.28731632232666, '_timestamp': 1740964929.1017337}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 92 is less than current step: 499. Dropping entry: {'train/lr': 5.020408163265306e-05, '_timestamp': 1740964929.1020708}).
Epoch: [4][ 94/500]	Time  7.934 ( 7.934)	Loss 1.1668 (1.5010)	CeLoss 0.2559 (0.3307)	SegCLSLoss 0.0114 (0.0137)	KLLoss 0.3711 (0.2939)	MaskLoss 0.4340 (0.5670)	MaskBCELoss 0.1387 (0.1334)	MaskDICELoss 0.2953 (0.4336)
Epoch: [4][ 95/500]	Time  9.599 ( 9.599)	Loss 0.9453 (1.7931)	CeLoss 0.9453 (0.3020)	SegCLSLoss 0.0000 (0.0164)	KLLoss 0.0000 (0.3283)	MaskLoss 0.0000 (0.7250)	MaskBCELoss 0.0000 (0.1166)	MaskDICELoss 0.0000 (0.6084)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 93 is less than current step: 499. Dropping entry: {'train/loss': 1.50100280046463, 'train/ce_loss': 0.3306640625, 'train/seg_cls_loss': 0.013739013671875, 'train/kl_loss': 0.2939453125, 'train/mask_bce_loss': 0.1333669644780457, 'train/mask_dice_loss': 0.43363832533359525, 'train/mask_loss': 0.5670053035020828, 'metrics/total_secs_per_batch': 7.93379807472229, 'metrics/data_secs_per_batch': 3.295757842063904, '_timestamp': 1740964937.0353932}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 93 is less than current step: 499. Dropping entry: {'train/lr': 5.008163265306122e-05, '_timestamp': 1740964937.0357473}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 94 is less than current step: 499. Dropping entry: {'train/loss': 1.7931013822555542, 'train/ce_loss': 0.301953125, 'train/seg_cls_loss': 0.016357421875, 'train/kl_loss': 0.3283203125, 'train/mask_bce_loss': 0.11656458228826523, 'train/mask_dice_loss': 0.6084040910005569, 'train/mask_loss': 0.724968671798706, 'metrics/total_secs_per_batch': 9.598726511001587, 'metrics/data_secs_per_batch': 4.279359698295593, '_timestamp': 1740964946.634094}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 94 is less than current step: 499. Dropping entry: {'train/lr': 4.9959183673469386e-05, '_timestamp': 1740964946.634354}).
Epoch: [4][ 96/500]	Time  7.075 ( 7.075)	Loss 2.2048 (1.3972)	CeLoss 0.2539 (0.2590)	SegCLSLoss 0.0145 (0.0146)	KLLoss 0.3633 (0.2955)	MaskLoss 0.9530 (0.5505)	MaskBCELoss 0.0208 (0.1399)	MaskDICELoss 0.9322 (0.4106)
Epoch: [4][ 97/500]	Time  8.827 ( 8.827)	Loss 1.8440 (1.5095)	CeLoss 0.1680 (0.3569)	SegCLSLoss 0.0259 (0.0141)	KLLoss 0.3730 (0.2578)	MaskLoss 0.8131 (0.5599)	MaskBCELoss 0.0099 (0.0545)	MaskDICELoss 0.8032 (0.5053)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 95 is less than current step: 499. Dropping entry: {'train/loss': 1.3971818804740905, 'train/ce_loss': 0.259033203125, 'train/seg_cls_loss': 0.014593505859375, 'train/kl_loss': 0.2955078125, 'train/mask_bce_loss': 0.1399188246577978, 'train/mask_dice_loss': 0.41055200546979903, 'train/mask_loss': 0.5504708230495453, 'metrics/total_secs_per_batch': 7.075266122817993, 'metrics/data_secs_per_batch': 3.20862135887146, '_timestamp': 1740964953.7093806}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 95 is less than current step: 499. Dropping entry: {'train/lr': 4.983673469387754e-05, '_timestamp': 1740964953.7096574}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 96 is less than current step: 499. Dropping entry: {'train/loss': 1.50945041179657, 'train/ce_loss': 0.356884765625, 'train/seg_cls_loss': 0.0140625, 'train/kl_loss': 0.2578125, 'train/mask_bce_loss': 0.054539897758513686, 'train/mask_dice_loss': 0.5053366690874099, 'train/mask_loss': 0.5598765641450882, 'metrics/total_secs_per_batch': 8.82741665840149, 'metrics/data_secs_per_batch': 3.899152684211731, '_timestamp': 1740964962.5367675}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 96 is less than current step: 499. Dropping entry: {'train/lr': 4.9714285714285705e-05, '_timestamp': 1740964962.5370362}).
Epoch: [4][ 98/500]	Time  8.371 ( 8.371)	Loss 2.4292 (1.8428)	CeLoss 0.1924 (0.2716)	SegCLSLoss 0.0182 (0.0165)	KLLoss 0.3770 (0.2938)	MaskLoss 1.0955 (0.7669)	MaskBCELoss 0.2643 (0.1521)	MaskDICELoss 0.8312 (0.6148)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 97 is less than current step: 499. Dropping entry: {'train/loss': 1.842763614654541, 'train/ce_loss': 0.2715576171875, 'train/seg_cls_loss': 0.01654052734375, 'train/kl_loss': 0.29375, 'train/mask_bce_loss': 0.15205510929226876, 'train/mask_dice_loss': 0.6147978842258454, 'train/mask_loss': 0.7668529987335205, 'metrics/total_secs_per_batch': 8.370829343795776, 'metrics/data_secs_per_batch': 3.523784303665161, '_timestamp': 1740964970.9077873}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 97 is less than current step: 499. Dropping entry: {'train/lr': 4.9591836734693875e-05, '_timestamp': 1740964970.908131}).
Epoch: [4][ 99/500]	Time  9.019 ( 9.019)	Loss 2.5314 (1.7680)	CeLoss 0.2051 (0.1982)	SegCLSLoss 0.0182 (0.0160)	KLLoss 0.3770 (0.3320)	MaskLoss 1.1397 (0.7643)	MaskBCELoss 0.3312 (0.1618)	MaskDICELoss 0.8085 (0.6025)
[2025-03-02 19:23:07,343] [INFO] [logging.py:96:log_dist] [Rank 0] step=2100, skipped=0, lr=[4.940816326530612e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:23:07,349] [INFO] [timer.py:215:stop] epoch=0/micro_step=21000/global_step=2100, RunningAvgSamplesPerSec=1.4802187494891554, CurrSamplesPerSec=1.3473210716086188, MemAllocated=31.26GB, MaxMemAllocated=37.19GB
Epoch: [4][100/500]	Time  7.424 ( 7.424)	Loss 1.9938 (1.4397)	CeLoss 0.2236 (0.4977)	SegCLSLoss 0.0193 (0.0092)	KLLoss 0.3613 (0.1826)	MaskLoss 0.8621 (0.4595)	MaskBCELoss 0.0210 (0.1036)	MaskDICELoss 0.8411 (0.3560)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 98 is less than current step: 499. Dropping entry: {'train/loss': 1.7680118799209594, 'train/ce_loss': 0.198193359375, 'train/seg_cls_loss': 0.016046142578125, 'train/kl_loss': 0.33203125, 'train/mask_bce_loss': 0.1618491554632783, 'train/mask_dice_loss': 0.6024546355009079, 'train/mask_loss': 0.764303794503212, 'metrics/total_secs_per_batch': 9.019354820251465, 'metrics/data_secs_per_batch': 3.746882200241089, '_timestamp': 1740964979.9269652}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 98 is less than current step: 499. Dropping entry: {'train/lr': 4.946938775510204e-05, '_timestamp': 1740964979.9272227}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 99 is less than current step: 499. Dropping entry: {'train/loss': 1.4397180438041688, 'train/ce_loss': 0.497705078125, 'train/seg_cls_loss': 0.00916748046875, 'train/kl_loss': 0.1826171875, 'train/mask_bce_loss': 0.10357873439788819, 'train/mask_dice_loss': 0.35595314502716063, 'train/mask_loss': 0.45953187346458435, 'metrics/total_secs_per_batch': 7.423602819442749, 'metrics/data_secs_per_batch': 3.3064218044281004, '_timestamp': 1740964987.3503878}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 99 is less than current step: 499. Dropping entry: {'train/lr': 4.93469387755102e-05, '_timestamp': 1740964987.3506498}).
Epoch: [4][101/500]	Time  8.636 ( 8.636)	Loss 2.4348 (1.6959)	CeLoss 0.2441 (0.1777)	SegCLSLoss 0.0164 (0.0189)	KLLoss 0.3789 (0.3383)	MaskLoss 1.0729 (0.7375)	MaskBCELoss 0.2694 (0.1613)	MaskDICELoss 0.8035 (0.5762)
Epoch: [4][102/500]	Time  7.782 ( 7.782)	Loss 1.0602 (1.7544)	CeLoss 0.1904 (0.2900)	SegCLSLoss 0.0148 (0.0144)	KLLoss 0.3711 (0.2984)	MaskLoss 0.4124 (0.7136)	MaskBCELoss 0.0269 (0.1283)	MaskDICELoss 0.3855 (0.5853)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 100 is less than current step: 499. Dropping entry: {'train/loss': 1.695872950553894, 'train/ce_loss': 0.177685546875, 'train/seg_cls_loss': 0.01890869140625, 'train/kl_loss': 0.33828125, 'train/mask_bce_loss': 0.16132274823030457, 'train/mask_dice_loss': 0.5761889308691025, 'train/mask_loss': 0.7375116735696793, 'metrics/total_secs_per_batch': 8.63597321510315, 'metrics/data_secs_per_batch': 3.6744856595993043, '_timestamp': 1740964995.9865367}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 100 is less than current step: 499. Dropping entry: {'train/lr': 4.9224489795918364e-05, '_timestamp': 1740964995.9867237}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 101 is less than current step: 499. Dropping entry: {'train/loss': 1.7543710947036744, 'train/ce_loss': 0.2899658203125, 'train/seg_cls_loss': 0.014404296875, 'train/kl_loss': 0.2984375, 'train/mask_bce_loss': 0.12831872012466192, 'train/mask_dice_loss': 0.5853292346000671, 'train/mask_loss': 0.7136479586362838, 'metrics/total_secs_per_batch': 7.78220009803772, 'metrics/data_secs_per_batch': 3.3643555879592895, '_timestamp': 1740965003.768733}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 101 is less than current step: 499. Dropping entry: {'train/lr': 4.910204081632653e-05, '_timestamp': 1740965003.769001}).
Epoch: [4][103/500]	Time 10.124 (10.124)	Loss 1.3829 (1.7038)	CeLoss 0.2891 (0.3059)	SegCLSLoss 0.0124 (0.0140)	KLLoss 0.3691 (0.3338)	MaskLoss 0.5254 (0.6789)	MaskBCELoss 0.1248 (0.1371)	MaskDICELoss 0.4007 (0.5418)
Epoch: [4][104/500]	Time  8.302 ( 8.302)	Loss 1.5507 (1.5349)	CeLoss 0.3047 (0.4755)	SegCLSLoss 0.0107 (0.0098)	KLLoss 0.3711 (0.2951)	MaskLoss 0.6015 (0.5124)	MaskBCELoss 0.1088 (0.1283)	MaskDICELoss 0.4927 (0.3841)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 102 is less than current step: 499. Dropping entry: {'train/loss': 1.7038014769554137, 'train/ce_loss': 0.305859375, 'train/seg_cls_loss': 0.01397705078125, 'train/kl_loss': 0.3337890625, 'train/mask_bce_loss': 0.13707095440477132, 'train/mask_dice_loss': 0.5417828917503357, 'train/mask_loss': 0.6788538426160813, 'metrics/total_secs_per_batch': 10.123802661895752, 'metrics/data_secs_per_batch': 4.727177977561951, '_timestamp': 1740965013.8926413}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 102 is less than current step: 499. Dropping entry: {'train/lr': 4.897959183673468e-05, '_timestamp': 1740965013.8929775}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 103 is less than current step: 499. Dropping entry: {'train/loss': 1.5349316954612733, 'train/ce_loss': 0.47548828125, 'train/seg_cls_loss': 0.009832763671875, 'train/kl_loss': 0.2951171875, 'train/mask_bce_loss': 0.12829673215746878, 'train/mask_dice_loss': 0.38409098535776137, 'train/mask_loss': 0.5123877108097077, 'metrics/total_secs_per_batch': 8.302270412445068, 'metrics/data_secs_per_batch': 3.9706172704696656, '_timestamp': 1740965022.195002}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 103 is less than current step: 499. Dropping entry: {'train/lr': 4.885714285714285e-05, '_timestamp': 1740965022.1953905}).
Epoch: [4][105/500]	Time  7.831 ( 7.831)	Loss 2.0821 (1.7419)	CeLoss 0.2910 (0.3308)	SegCLSLoss 0.0098 (0.0115)	KLLoss 0.3652 (0.2584)	MaskLoss 0.8740 (0.6897)	MaskBCELoss 0.3969 (0.1820)	MaskDICELoss 0.4772 (0.5077)
Epoch: [4][106/500]	Time  6.313 ( 6.313)	Loss 1.1228 (1.3967)	CeLoss 0.2520 (0.5790)	SegCLSLoss 0.0109 (0.0077)	KLLoss 0.3711 (0.1844)	MaskLoss 0.4139 (0.3976)	MaskBCELoss 0.1315 (0.0510)	MaskDICELoss 0.2824 (0.3467)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 104 is less than current step: 499. Dropping entry: {'train/loss': 1.7419111251831054, 'train/ce_loss': 0.3307861328125, 'train/seg_cls_loss': 0.0114501953125, 'train/kl_loss': 0.2583984375, 'train/mask_bce_loss': 0.1820246208459139, 'train/mask_dice_loss': 0.5076687335968018, 'train/mask_loss': 0.6896933555603028, 'metrics/total_secs_per_batch': 7.830998659133911, 'metrics/data_secs_per_batch': 3.5404026985168455, '_timestamp': 1740965030.0260096}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 104 is less than current step: 499. Dropping entry: {'train/lr': 4.8734693877551016e-05, '_timestamp': 1740965030.0263755}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 105 is less than current step: 499. Dropping entry: {'train/loss': 1.3966988325119019, 'train/ce_loss': 0.57900390625, 'train/seg_cls_loss': 0.007696533203125, 'train/kl_loss': 0.184375, 'train/mask_bce_loss': 0.050951809342950585, 'train/mask_dice_loss': 0.3466651767492294, 'train/mask_loss': 0.3976169764995575, 'metrics/total_secs_per_batch': 6.3134684562683105, 'metrics/data_secs_per_batch': 2.4086902141571045, '_timestamp': 1740965036.3393273}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 105 is less than current step: 499. Dropping entry: {'train/lr': 4.861224489795918e-05, '_timestamp': 1740965036.3395967}).
Epoch: [4][107/500]	Time  7.969 ( 7.969)	Loss 2.4703 (2.0009)	CeLoss 0.2012 (0.3462)	SegCLSLoss 0.0194 (0.0147)	KLLoss 0.3652 (0.2996)	MaskLoss 1.1111 (0.8086)	MaskBCELoss 0.1507 (0.1835)	MaskDICELoss 0.9604 (0.6251)
Epoch: [4][108/500]	Time  8.121 ( 8.121)	Loss 1.1250 (1.9498)	CeLoss 1.1250 (0.4062)	SegCLSLoss 0.0000 (0.0170)	KLLoss 0.0000 (0.2969)	MaskLoss 0.0000 (0.7526)	MaskBCELoss 0.0000 (0.1673)	MaskDICELoss 0.0000 (0.5853)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 106 is less than current step: 499. Dropping entry: {'train/loss': 2.000884139537811, 'train/ce_loss': 0.34619140625, 'train/seg_cls_loss': 0.014739990234375, 'train/kl_loss': 0.299609375, 'train/mask_bce_loss': 0.18350804410874844, 'train/mask_dice_loss': 0.6251371473073959, 'train/mask_loss': 0.8086451828479767, 'metrics/total_secs_per_batch': 7.9688498973846436, 'metrics/data_secs_per_batch': 3.2211726188659666, '_timestamp': 1740965044.3083587}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 106 is less than current step: 499. Dropping entry: {'train/lr': 4.848979591836734e-05, '_timestamp': 1740965044.308705}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 107 is less than current step: 499. Dropping entry: {'train/loss': 1.9498077869415282, 'train/ce_loss': 0.40625, 'train/seg_cls_loss': 0.01695556640625, 'train/kl_loss': 0.296875, 'train/mask_bce_loss': 0.16729311160743238, 'train/mask_dice_loss': 0.5852963119745255, 'train/mask_loss': 0.7525894284248352, 'metrics/total_secs_per_batch': 8.12132978439331, 'metrics/data_secs_per_batch': 3.724108672142029, '_timestamp': 1740965052.4294896}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 107 is less than current step: 499. Dropping entry: {'train/lr': 4.8367346938775505e-05, '_timestamp': 1740965052.4297566}).
Epoch: [4][109/500]	Time  6.671 ( 6.671)	Loss 0.8398 (1.8936)	CeLoss 0.8398 (0.3652)	SegCLSLoss 0.0000 (0.0181)	KLLoss 0.0000 (0.2967)	MaskLoss 0.0000 (0.7448)	MaskBCELoss 0.0000 (0.1597)	MaskDICELoss 0.0000 (0.5851)
[2025-03-02 19:24:26,309] [INFO] [logging.py:96:log_dist] [Rank 0] step=2110, skipped=0, lr=[4.818367346938776e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:24:26,315] [INFO] [timer.py:215:stop] epoch=0/micro_step=21100/global_step=2110, RunningAvgSamplesPerSec=1.4790352222820413, CurrSamplesPerSec=1.3860279213163311, MemAllocated=31.27GB, MaxMemAllocated=37.19GB
Epoch: [4][110/500]	Time  7.216 ( 7.216)	Loss 2.7304 (1.1848)	CeLoss 0.2100 (0.3378)	SegCLSLoss 0.0244 (0.0089)	KLLoss 0.3789 (0.1865)	MaskLoss 1.2353 (0.4121)	MaskBCELoss 0.3009 (0.0832)	MaskDICELoss 0.9344 (0.3289)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 108 is less than current step: 499. Dropping entry: {'train/loss': 1.8936332762241364, 'train/ce_loss': 0.365234375, 'train/seg_cls_loss': 0.01810302734375, 'train/kl_loss': 0.2966796875, 'train/mask_bce_loss': 0.1596674621105194, 'train/mask_dice_loss': 0.5851472377777099, 'train/mask_loss': 0.7448146849870682, 'metrics/total_secs_per_batch': 6.67057991027832, 'metrics/data_secs_per_batch': 3.185282039642334, '_timestamp': 1740965059.100093}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 108 is less than current step: 499. Dropping entry: {'train/lr': 4.8244897959183675e-05, '_timestamp': 1740965059.100366}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 109 is less than current step: 499. Dropping entry: {'train/loss': 1.1848127722740174, 'train/ce_loss': 0.33779296875, 'train/seg_cls_loss': 0.0089111328125, 'train/kl_loss': 0.1865234375, 'train/mask_bce_loss': 0.08322401326149702, 'train/mask_dice_loss': 0.32886011376976965, 'train/mask_loss': 0.4120841220021248, 'metrics/total_secs_per_batch': 7.216430902481079, 'metrics/data_secs_per_batch': 2.9292160511016845, '_timestamp': 1740965066.3164208}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 109 is less than current step: 499. Dropping entry: {'train/lr': 4.812244897959184e-05, '_timestamp': 1740965066.3167593}).
Epoch: [4][111/500]	Time  7.230 ( 7.230)	Loss 1.1484 (2.0166)	CeLoss 1.1484 (0.4289)	SegCLSLoss 0.0000 (0.0158)	KLLoss 0.0000 (0.2963)	MaskLoss 0.0000 (0.7750)	MaskBCELoss 0.0000 (0.1780)	MaskDICELoss 0.0000 (0.5970)
Epoch: [4][112/500]	Time  9.434 ( 9.434)	Loss 1.2452 (2.0385)	CeLoss 0.2637 (0.3404)	SegCLSLoss 0.0203 (0.0190)	KLLoss 0.3691 (0.3332)	MaskLoss 0.4673 (0.8276)	MaskBCELoss 0.2682 (0.1705)	MaskDICELoss 0.1991 (0.6570)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 110 is less than current step: 499. Dropping entry: {'train/loss': 2.0166293025016784, 'train/ce_loss': 0.428857421875, 'train/seg_cls_loss': 0.015753173828125, 'train/kl_loss': 0.2962890625, 'train/mask_bce_loss': 0.17798081822693348, 'train/mask_dice_loss': 0.5969842076301575, 'train/mask_loss': 0.7749650299549102, 'metrics/total_secs_per_batch': 7.229715347290039, 'metrics/data_secs_per_batch': 3.2977630376815794, '_timestamp': 1740965073.546261}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 110 is less than current step: 499. Dropping entry: {'train/lr': 4.7999999999999994e-05, '_timestamp': 1740965073.546525}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 111 is less than current step: 499. Dropping entry: {'train/loss': 2.0385276794433596, 'train/ce_loss': 0.3404296875, 'train/seg_cls_loss': 0.01903076171875, 'train/kl_loss': 0.333203125, 'train/mask_bce_loss': 0.17053527254611253, 'train/mask_dice_loss': 0.6570293366909027, 'train/mask_loss': 0.8275646150112153, 'metrics/total_secs_per_batch': 9.434426307678223, 'metrics/data_secs_per_batch': 4.486164617538452, '_timestamp': 1740965082.9807024}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 111 is less than current step: 499. Dropping entry: {'train/lr': 4.787755102040816e-05, '_timestamp': 1740965082.9809873}).
Epoch: [4][113/500]	Time  7.427 ( 7.427)	Loss 0.8438 (1.1138)	CeLoss 0.8438 (0.4556)	SegCLSLoss 0.0000 (0.0074)	KLLoss 0.0000 (0.1477)	MaskLoss 0.0000 (0.3198)	MaskBCELoss 0.0000 (0.0424)	MaskDICELoss 0.0000 (0.2774)
Epoch: [4][114/500]	Time  8.052 ( 8.052)	Loss 3.2069 (1.9677)	CeLoss 0.2178 (0.3914)	SegCLSLoss 0.0205 (0.0158)	KLLoss 0.3574 (0.2939)	MaskLoss 1.4716 (0.7694)	MaskBCELoss 0.6911 (0.1746)	MaskDICELoss 0.7804 (0.5948)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 112 is less than current step: 499. Dropping entry: {'train/loss': 1.113816511631012, 'train/ce_loss': 0.4556396484375, 'train/seg_cls_loss': 0.007366943359375, 'train/kl_loss': 0.14765625, 'train/mask_bce_loss': 0.04236867427825928, 'train/mask_dice_loss': 0.2774424046278, 'train/mask_loss': 0.3198110818862915, 'metrics/total_secs_per_batch': 7.427404165267944, 'metrics/data_secs_per_batch': 3.4276288986206054, '_timestamp': 1740965090.408149}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 112 is less than current step: 499. Dropping entry: {'train/lr': 4.775510204081632e-05, '_timestamp': 1740965090.4084513}).
Epoch: [4][115/500]	Time  8.728 ( 8.728)	Loss 1.9338 (1.7977)	CeLoss 0.1846 (0.2515)	SegCLSLoss 0.0292 (0.0149)	KLLoss 0.3711 (0.3689)	MaskLoss 0.8488 (0.7510)	MaskBCELoss 0.3188 (0.2433)	MaskDICELoss 0.5300 (0.5077)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 113 is less than current step: 499. Dropping entry: {'train/loss': 1.967711329460144, 'train/ce_loss': 0.39140625, 'train/seg_cls_loss': 0.015826416015625, 'train/kl_loss': 0.2939453125, 'train/mask_bce_loss': 0.17456457316875457, 'train/mask_dice_loss': 0.5947891294956207, 'train/mask_loss': 0.769353699684143, 'metrics/total_secs_per_batch': 8.051867485046387, 'metrics/data_secs_per_batch': 3.835789203643799, '_timestamp': 1740965098.459953}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 113 is less than current step: 499. Dropping entry: {'train/lr': 4.7632653061224484e-05, '_timestamp': 1740965098.460245}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 114 is less than current step: 499. Dropping entry: {'train/loss': 1.7977036237716675, 'train/ce_loss': 0.25146484375, 'train/seg_cls_loss': 0.0149169921875, 'train/kl_loss': 0.3689453125, 'train/mask_bce_loss': 0.2433348437771201, 'train/mask_dice_loss': 0.5077142387628555, 'train/mask_loss': 0.7510490715503693, 'metrics/total_secs_per_batch': 8.72800326347351, 'metrics/data_secs_per_batch': 3.8291913509368896, '_timestamp': 1740965107.1879575}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 114 is less than current step: 499. Dropping entry: {'train/lr': 4.7510204081632653e-05, '_timestamp': 1740965107.1882267}).
Epoch: [4][116/500]	Time  8.323 ( 8.323)	Loss 0.8438 (1.6053)	CeLoss 0.8438 (0.3570)	SegCLSLoss 0.0000 (0.0148)	KLLoss 0.0000 (0.2607)	MaskLoss 0.0000 (0.6075)	MaskBCELoss 0.0000 (0.0835)	MaskDICELoss 0.0000 (0.5239)
Epoch: [4][117/500]	Time  6.848 ( 6.848)	Loss 1.0391 (1.3304)	CeLoss 1.0391 (0.6185)	SegCLSLoss 0.0000 (0.0082)	KLLoss 0.0000 (0.1848)	MaskLoss 0.0000 (0.3447)	MaskBCELoss 0.0000 (0.0700)	MaskDICELoss 0.0000 (0.2748)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 115 is less than current step: 499. Dropping entry: {'train/loss': 1.6053351879119873, 'train/ce_loss': 0.35703125, 'train/seg_cls_loss': 0.014764404296875, 'train/kl_loss': 0.2607421875, 'train/mask_bce_loss': 0.08354909187182784, 'train/mask_dice_loss': 0.5239036768674851, 'train/mask_loss': 0.6074527621269226, 'metrics/total_secs_per_batch': 8.3228440284729, 'metrics/data_secs_per_batch': 4.512590646743774, '_timestamp': 1740965115.5109181}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 115 is less than current step: 499. Dropping entry: {'train/lr': 4.7387755102040816e-05, '_timestamp': 1740965115.5112734}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 116 is less than current step: 499. Dropping entry: {'train/loss': 1.330411672592163, 'train/ce_loss': 0.61845703125, 'train/seg_cls_loss': 0.008221435546875, 'train/kl_loss': 0.184765625, 'train/mask_bce_loss': 0.06996432431042195, 'train/mask_dice_loss': 0.2747825264930725, 'train/mask_loss': 0.34474685192108157, 'metrics/total_secs_per_batch': 6.847536087036133, 'metrics/data_secs_per_batch': 2.854393148422241, '_timestamp': 1740965122.3583376}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 116 is less than current step: 499. Dropping entry: {'train/lr': 4.726530612244897e-05, '_timestamp': 1740965122.3586004}).
Epoch: [4][118/500]	Time  7.161 ( 7.161)	Loss 1.4141 (1.6296)	CeLoss 1.4141 (0.5085)	SegCLSLoss 0.0000 (0.0121)	KLLoss 0.0000 (0.2557)	MaskLoss 0.0000 (0.5448)	MaskBCELoss 0.0000 (0.0670)	MaskDICELoss 0.0000 (0.4778)
Epoch: [4][119/500]	Time  9.161 ( 9.161)	Loss 1.8396 (1.9393)	CeLoss 0.2930 (0.2060)	SegCLSLoss 0.0121 (0.0159)	KLLoss 0.3652 (0.3322)	MaskLoss 0.7518 (0.8461)	MaskBCELoss 0.0434 (0.1795)	MaskDICELoss 0.7085 (0.6666)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 117 is less than current step: 499. Dropping entry: {'train/loss': 1.6295966744422912, 'train/ce_loss': 0.50849609375, 'train/seg_cls_loss': 0.012054443359375, 'train/kl_loss': 0.2556640625, 'train/mask_bce_loss': 0.06701639853417873, 'train/mask_dice_loss': 0.47776240706443784, 'train/mask_loss': 0.5447788119316102, 'metrics/total_secs_per_batch': 7.161426782608032, 'metrics/data_secs_per_batch': 3.3789713382720947, '_timestamp': 1740965129.5198011}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 117 is less than current step: 499. Dropping entry: {'train/lr': 4.7142857142857136e-05, '_timestamp': 1740965129.5200856}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 118 is less than current step: 499. Dropping entry: {'train/loss': 1.9393418669700622, 'train/ce_loss': 0.20595703125, 'train/seg_cls_loss': 0.015863037109375, 'train/kl_loss': 0.3322265625, 'train/mask_bce_loss': 0.17949133273214102, 'train/mask_dice_loss': 0.6665955871343613, 'train/mask_loss': 0.8460869193077087, 'metrics/total_secs_per_batch': 9.160959482192993, 'metrics/data_secs_per_batch': 4.119419813156128, '_timestamp': 1740965138.6809554}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 118 is less than current step: 499. Dropping entry: {'train/lr': 4.70204081632653e-05, '_timestamp': 1740965138.6813111}).
[2025-03-02 19:25:47,008] [INFO] [logging.py:96:log_dist] [Rank 0] step=2120, skipped=0, lr=[4.695918367346938e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:25:47,015] [INFO] [timer.py:215:stop] epoch=0/micro_step=21200/global_step=2120, RunningAvgSamplesPerSec=1.4776859719896729, CurrSamplesPerSec=1.1999800990544105, MemAllocated=30.7GB, MaxMemAllocated=37.19GB
Epoch: [4][120/500]	Time  8.335 ( 8.335)	Loss 1.0625 (1.7448)	CeLoss 1.0625 (0.6367)	SegCLSLoss 0.0000 (0.0103)	KLLoss 0.0000 (0.2215)	MaskLoss 0.0000 (0.5403)	MaskBCELoss 0.0000 (0.0665)	MaskDICELoss 0.0000 (0.4738)
Epoch: [4][121/500]	Time  7.943 ( 7.943)	Loss 1.1897 (1.4954)	CeLoss 0.2422 (0.4581)	SegCLSLoss 0.0140 (0.0098)	KLLoss 0.3613 (0.2541)	MaskLoss 0.4523 (0.5034)	MaskBCELoss 0.0930 (0.0861)	MaskDICELoss 0.3593 (0.4172)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 119 is less than current step: 499. Dropping entry: {'train/loss': 1.7448142170906067, 'train/ce_loss': 0.63671875, 'train/seg_cls_loss': 0.01029052734375, 'train/kl_loss': 0.221484375, 'train/mask_bce_loss': 0.06648521590977907, 'train/mask_dice_loss': 0.47379298210144044, 'train/mask_loss': 0.54027818441391, 'metrics/total_secs_per_batch': 8.335273265838623, 'metrics/data_secs_per_batch': 4.571165132522583, '_timestamp': 1740965147.015806}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 119 is less than current step: 499. Dropping entry: {'train/lr': 4.689795918367346e-05, '_timestamp': 1740965147.0159893}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 120 is less than current step: 499. Dropping entry: {'train/loss': 1.4953912496566772, 'train/ce_loss': 0.45810546875, 'train/seg_cls_loss': 0.009844970703125, 'train/kl_loss': 0.2541015625, 'train/mask_bce_loss': 0.08613926684483886, 'train/mask_dice_loss': 0.41722041815519334, 'train/mask_loss': 0.5033596843481064, 'metrics/total_secs_per_batch': 7.943110942840576, 'metrics/data_secs_per_batch': 3.0915202379226683, '_timestamp': 1740965154.9593074}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 120 is less than current step: 499. Dropping entry: {'train/lr': 4.677551020408163e-05, '_timestamp': 1740965154.9596467}).
Epoch: [4][122/500]	Time  8.448 ( 8.448)	Loss 1.8218 (1.5696)	CeLoss 0.2217 (0.2834)	SegCLSLoss 0.0102 (0.0162)	KLLoss 0.3828 (0.3670)	MaskLoss 0.7781 (0.6207)	MaskBCELoss 0.2853 (0.0663)	MaskDICELoss 0.4928 (0.5544)
Epoch: [4][123/500]	Time  6.790 ( 6.790)	Loss 1.2734 (1.4676)	CeLoss 1.2734 (0.4585)	SegCLSLoss 0.0000 (0.0113)	KLLoss 0.0000 (0.2617)	MaskLoss 0.0000 (0.4886)	MaskBCELoss 0.0000 (0.1296)	MaskDICELoss 0.0000 (0.3589)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 121 is less than current step: 499. Dropping entry: {'train/loss': 1.5696207463741303, 'train/ce_loss': 0.2833984375, 'train/seg_cls_loss': 0.016204833984375, 'train/kl_loss': 0.3669921875, 'train/mask_bce_loss': 0.06626650979742407, 'train/mask_dice_loss': 0.5543836951255798, 'train/mask_loss': 0.6206502109766007, 'metrics/total_secs_per_batch': 8.448254108428955, 'metrics/data_secs_per_batch': 3.9944594621658327, '_timestamp': 1740965163.4074}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 121 is less than current step: 499. Dropping entry: {'train/lr': 4.6653061224489795e-05, '_timestamp': 1740965163.4076734}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 122 is less than current step: 499. Dropping entry: {'train/loss': 1.467579960823059, 'train/ce_loss': 0.458544921875, 'train/seg_cls_loss': 0.01134033203125, 'train/kl_loss': 0.26171875, 'train/mask_bce_loss': 0.12963516488671303, 'train/mask_dice_loss': 0.35893997102975844, 'train/mask_loss': 0.48857513070106506, 'metrics/total_secs_per_batch': 6.790480852127075, 'metrics/data_secs_per_batch': 3.198084235191345, '_timestamp': 1740965170.1978793}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 122 is less than current step: 499. Dropping entry: {'train/lr': 4.653061224489796e-05, '_timestamp': 1740965170.1981854}).
Epoch: [4][124/500]	Time  8.732 ( 8.732)	Loss 1.4619 (1.9195)	CeLoss 0.2891 (0.3170)	SegCLSLoss 0.0093 (0.0128)	KLLoss 0.3672 (0.3312)	MaskLoss 0.5659 (0.7815)	MaskBCELoss 0.3652 (0.2344)	MaskDICELoss 0.2008 (0.5471)
Epoch: [4][125/500]	Time  8.432 ( 8.432)	Loss 0.8585 (1.5815)	CeLoss 0.2695 (0.2302)	SegCLSLoss 0.0112 (0.0135)	KLLoss 0.3691 (0.3301)	MaskLoss 0.2730 (0.6557)	MaskBCELoss 0.0550 (0.1874)	MaskDICELoss 0.2180 (0.4683)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 123 is less than current step: 499. Dropping entry: {'train/loss': 1.919541311264038, 'train/ce_loss': 0.3169921875, 'train/seg_cls_loss': 0.012811279296875, 'train/kl_loss': 0.33125, 'train/mask_bce_loss': 0.234362231194973, 'train/mask_dice_loss': 0.5470880985260009, 'train/mask_loss': 0.7814503371715545, 'metrics/total_secs_per_batch': 8.731646537780762, 'metrics/data_secs_per_batch': 3.748469614982605, '_timestamp': 1740965178.9295127}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 123 is less than current step: 499. Dropping entry: {'train/lr': 4.6408163265306114e-05, '_timestamp': 1740965178.9297898}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 124 is less than current step: 499. Dropping entry: {'train/loss': 1.5815335929393768, 'train/ce_loss': 0.230224609375, 'train/seg_cls_loss': 0.0135009765625, 'train/kl_loss': 0.330078125, 'train/mask_bce_loss': 0.18738419488072394, 'train/mask_dice_loss': 0.46834842264652254, 'train/mask_loss': 0.6557326167821884, 'metrics/total_secs_per_batch': 8.432476997375488, 'metrics/data_secs_per_batch': 3.6097291469573975, '_timestamp': 1740965187.3621972}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 124 is less than current step: 499. Dropping entry: {'train/lr': 4.628571428571428e-05, '_timestamp': 1740965187.36254}).
Epoch: [4][126/500]	Time  7.637 ( 7.637)	Loss 0.8398 (1.7585)	CeLoss 0.8398 (0.3727)	SegCLSLoss 0.0000 (0.0144)	KLLoss 0.0000 (0.2947)	MaskLoss 0.0000 (0.6745)	MaskBCELoss 0.0000 (0.1026)	MaskDICELoss 0.0000 (0.5719)
Epoch: [4][127/500]	Time  8.924 ( 8.924)	Loss 2.3457 (1.4880)	CeLoss 0.2158 (0.2115)	SegCLSLoss 0.0166 (0.0115)	KLLoss 0.3633 (0.2941)	MaskLoss 1.0420 (0.6204)	MaskBCELoss 0.0685 (0.1144)	MaskDICELoss 0.9734 (0.5060)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 125 is less than current step: 499. Dropping entry: {'train/loss': 1.7584797322750092, 'train/ce_loss': 0.37265625, 'train/seg_cls_loss': 0.01436767578125, 'train/kl_loss': 0.2947265625, 'train/mask_bce_loss': 0.10263078175485134, 'train/mask_dice_loss': 0.5718727603554725, 'train/mask_loss': 0.6745035395026207, 'metrics/total_secs_per_batch': 7.637230634689331, 'metrics/data_secs_per_batch': 3.3404722690582274, '_timestamp': 1740965194.999239}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 125 is less than current step: 499. Dropping entry: {'train/lr': 4.616326530612245e-05, '_timestamp': 1740965194.9995186}).
Epoch: [4][128/500]	Time  8.288 ( 8.288)	Loss 1.8341 (1.6713)	CeLoss 0.1758 (0.4624)	SegCLSLoss 0.0255 (0.0127)	KLLoss 0.3711 (0.2176)	MaskLoss 0.8043 (0.5904)	MaskBCELoss 0.0209 (0.0869)	MaskDICELoss 0.7834 (0.5035)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 126 is less than current step: 499. Dropping entry: {'train/loss': 1.4879828214645385, 'train/ce_loss': 0.2115234375, 'train/seg_cls_loss': 0.0115234375, 'train/kl_loss': 0.294140625, 'train/mask_bce_loss': 0.1144240566296503, 'train/mask_dice_loss': 0.5059833556413651, 'train/mask_loss': 0.6204074040055275, 'metrics/total_secs_per_batch': 8.923558235168457, 'metrics/data_secs_per_batch': 3.531736898422241, '_timestamp': 1740965203.92271}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 126 is less than current step: 499. Dropping entry: {'train/lr': 4.604081632653061e-05, '_timestamp': 1740965203.9229617}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 127 is less than current step: 499. Dropping entry: {'train/loss': 1.6713224530220032, 'train/ce_loss': 0.46240234375, 'train/seg_cls_loss': 0.01265869140625, 'train/kl_loss': 0.217578125, 'train/mask_bce_loss': 0.08688535559922457, 'train/mask_dice_loss': 0.5035122036933899, 'train/mask_loss': 0.5903975546360016, 'metrics/total_secs_per_batch': 8.28845763206482, 'metrics/data_secs_per_batch': 3.9203407764434814, '_timestamp': 1740965212.2112634}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 127 is less than current step: 499. Dropping entry: {'train/lr': 4.591836734693877e-05, '_timestamp': 1740965212.2115269}).
Epoch: [4][129/500]	Time  7.159 ( 7.159)	Loss 1.5156 (1.6186)	CeLoss 1.5156 (0.4019)	SegCLSLoss 0.0000 (0.0152)	KLLoss 0.0000 (0.2611)	MaskLoss 0.0000 (0.5915)	MaskBCELoss 0.0000 (0.1871)	MaskDICELoss 0.0000 (0.4044)
[2025-03-02 19:27:07,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=2130, skipped=0, lr=[4.573469387755102e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:27:07,990] [INFO] [timer.py:215:stop] epoch=0/micro_step=21300/global_step=2130, RunningAvgSamplesPerSec=1.476323589263484, CurrSamplesPerSec=1.1601623910469616, MemAllocated=31.59GB, MaxMemAllocated=37.19GB
Epoch: [4][130/500]	Time  8.621 ( 8.621)	Loss 2.4043 (1.6903)	CeLoss 0.2354 (0.3514)	SegCLSLoss 0.0150 (0.0147)	KLLoss 0.3652 (0.3277)	MaskLoss 1.0625 (0.6495)	MaskBCELoss 0.2540 (0.1137)	MaskDICELoss 0.8085 (0.5358)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 128 is less than current step: 499. Dropping entry: {'train/loss': 1.6185963034629822, 'train/ce_loss': 0.401904296875, 'train/seg_cls_loss': 0.015240478515625, 'train/kl_loss': 0.2611328125, 'train/mask_bce_loss': 0.18709901310503482, 'train/mask_dice_loss': 0.4044256925582886, 'train/mask_loss': 0.5915247082710267, 'metrics/total_secs_per_batch': 7.158509969711304, 'metrics/data_secs_per_batch': 3.421016311645508, '_timestamp': 1740965219.3697395}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 128 is less than current step: 499. Dropping entry: {'train/lr': 4.5795918367346936e-05, '_timestamp': 1740965219.3700154}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 129 is less than current step: 499. Dropping entry: {'train/loss': 1.6903052389621736, 'train/ce_loss': 0.3513671875, 'train/seg_cls_loss': 0.01470947265625, 'train/kl_loss': 0.327734375, 'train/mask_bce_loss': 0.1137097891420126, 'train/mask_dice_loss': 0.5357885241508484, 'train/mask_loss': 0.6494983226060868, 'metrics/total_secs_per_batch': 8.62097454071045, 'metrics/data_secs_per_batch': 3.6860544443130494, '_timestamp': 1740965227.9906058}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 129 is less than current step: 499. Dropping entry: {'train/lr': 4.56734693877551e-05, '_timestamp': 1740965227.990903}).
Epoch: [4][131/500]	Time  6.332 ( 6.332)	Loss 0.1348 (1.2919)	CeLoss 0.1348 (0.6216)	SegCLSLoss 0.0000 (0.0100)	KLLoss 0.0000 (0.1480)	MaskLoss 0.0000 (0.3253)	MaskBCELoss 0.0000 (0.0583)	MaskDICELoss 0.0000 (0.2670)
Epoch: [4][132/500]	Time  7.782 ( 7.782)	Loss 2.7947 (1.7315)	CeLoss 0.3457 (0.4569)	SegCLSLoss 0.0116 (0.0152)	KLLoss 0.3711 (0.2953)	MaskLoss 1.2030 (0.6187)	MaskBCELoss 0.5094 (0.1780)	MaskDICELoss 0.6936 (0.4407)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 130 is less than current step: 499. Dropping entry: {'train/loss': 1.2918977618217469, 'train/ce_loss': 0.62158203125, 'train/seg_cls_loss': 0.00997314453125, 'train/kl_loss': 0.148046875, 'train/mask_bce_loss': 0.058299120515584946, 'train/mask_dice_loss': 0.26699545681476594, 'train/mask_loss': 0.3252945750951767, 'metrics/total_secs_per_batch': 6.332371711730957, 'metrics/data_secs_per_batch': 2.7002980947494506, '_timestamp': 1740965234.3231359}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 130 is less than current step: 499. Dropping entry: {'train/lr': 4.5551020408163256e-05, '_timestamp': 1740965234.3234324}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 131 is less than current step: 499. Dropping entry: {'train/loss': 1.7315336763858795, 'train/ce_loss': 0.45693359375, 'train/seg_cls_loss': 0.01519775390625, 'train/kl_loss': 0.2953125, 'train/mask_bce_loss': 0.17798895984888077, 'train/mask_dice_loss': 0.4407075494527817, 'train/mask_loss': 0.6186965078115463, 'metrics/total_secs_per_batch': 7.781688213348389, 'metrics/data_secs_per_batch': 3.7700421094894407, '_timestamp': 1740965242.1048465}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 131 is less than current step: 499. Dropping entry: {'train/lr': 4.5428571428571425e-05, '_timestamp': 1740965242.1051323}).
Epoch: [4][133/500]	Time  7.672 ( 7.672)	Loss 1.3793 (1.1475)	CeLoss 0.2578 (0.2891)	SegCLSLoss 0.0138 (0.0097)	KLLoss 0.3691 (0.1842)	MaskLoss 0.5383 (0.4175)	MaskBCELoss 0.0727 (0.0610)	MaskDICELoss 0.4656 (0.3565)
Epoch: [4][134/500]	Time  8.595 ( 8.595)	Loss 1.7276 (1.8625)	CeLoss 0.2227 (0.4347)	SegCLSLoss 0.0243 (0.0140)	KLLoss 0.3711 (0.2945)	MaskLoss 0.7281 (0.6955)	MaskBCELoss 0.0375 (0.0853)	MaskDICELoss 0.6906 (0.6102)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 132 is less than current step: 499. Dropping entry: {'train/loss': 1.1475030183792114, 'train/ce_loss': 0.2890625, 'train/seg_cls_loss': 0.00968017578125, 'train/kl_loss': 0.1841796875, 'train/mask_bce_loss': 0.060998188238590954, 'train/mask_dice_loss': 0.3565033286809921, 'train/mask_loss': 0.41750151515007017, 'metrics/total_secs_per_batch': 7.672398090362549, 'metrics/data_secs_per_batch': 3.4243417263031004, '_timestamp': 1740965249.7772064}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 132 is less than current step: 499. Dropping entry: {'train/lr': 4.530612244897959e-05, '_timestamp': 1740965249.7774816}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 133 is less than current step: 499. Dropping entry: {'train/loss': 1.8624755978584289, 'train/ce_loss': 0.43466796875, 'train/seg_cls_loss': 0.01402587890625, 'train/kl_loss': 0.29453125, 'train/mask_bce_loss': 0.08530579681973904, 'train/mask_dice_loss': 0.610238641500473, 'train/mask_loss': 0.695544421672821, 'metrics/total_secs_per_batch': 8.594622611999512, 'metrics/data_secs_per_batch': 3.361632966995239, '_timestamp': 1740965258.3718228}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 133 is less than current step: 499. Dropping entry: {'train/lr': 4.518367346938775e-05, '_timestamp': 1740965258.3720832}).
Epoch: [4][135/500]	Time  7.469 ( 7.469)	Loss 1.1973 (1.7350)	CeLoss 0.2246 (0.6426)	SegCLSLoss 0.0109 (0.0114)	KLLoss 0.3691 (0.2553)	MaskLoss 0.4649 (0.5305)	MaskBCELoss 0.1262 (0.0523)	MaskDICELoss 0.3387 (0.4781)
Epoch: [4][136/500]	Time  9.004 ( 9.004)	Loss 2.3847 (2.0539)	CeLoss 0.2402 (0.3112)	SegCLSLoss 0.0186 (0.0183)	KLLoss 0.3711 (0.3326)	MaskLoss 1.0488 (0.8501)	MaskBCELoss 0.3940 (0.1808)	MaskDICELoss 0.6549 (0.6693)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 134 is less than current step: 499. Dropping entry: {'train/loss': 1.7349816679954528, 'train/ce_loss': 0.642578125, 'train/seg_cls_loss': 0.011407470703125, 'train/kl_loss': 0.2552734375, 'train/mask_bce_loss': 0.052340961433947085, 'train/mask_dice_loss': 0.4781381458044052, 'train/mask_loss': 0.5304791033267975, 'metrics/total_secs_per_batch': 7.469231367111206, 'metrics/data_secs_per_batch': 3.612586426734924, '_timestamp': 1740965265.8410726}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 134 is less than current step: 499. Dropping entry: {'train/lr': 4.5061224489795915e-05, '_timestamp': 1740965265.8413603}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 135 is less than current step: 499. Dropping entry: {'train/loss': 2.053903305530548, 'train/ce_loss': 0.31123046875, 'train/seg_cls_loss': 0.01829833984375, 'train/kl_loss': 0.3326171875, 'train/mask_bce_loss': 0.18083738684654235, 'train/mask_dice_loss': 0.6693076223134995, 'train/mask_loss': 0.8501450061798096, 'metrics/total_secs_per_batch': 9.00385308265686, 'metrics/data_secs_per_batch': 4.148187041282654, '_timestamp': 1740965274.8452141}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 135 is less than current step: 499. Dropping entry: {'train/lr': 4.493877551020408e-05, '_timestamp': 1740965274.8455884}).
Epoch: [4][137/500]	Time  7.609 ( 7.609)	Loss 1.2344 (1.9100)	CeLoss 1.2344 (0.4184)	SegCLSLoss 0.0000 (0.0165)	KLLoss 0.0000 (0.2967)	MaskLoss 0.0000 (0.7270)	MaskBCELoss 0.0000 (0.2048)	MaskDICELoss 0.0000 (0.5222)
Epoch: [4][138/500]	Time  7.590 ( 7.590)	Loss 2.2687 (1.9275)	CeLoss 0.2256 (0.3156)	SegCLSLoss 0.0171 (0.0180)	KLLoss 0.3633 (0.3352)	MaskLoss 0.9986 (0.7846)	MaskBCELoss 0.0415 (0.1358)	MaskDICELoss 0.9571 (0.6488)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 136 is less than current step: 499. Dropping entry: {'train/loss': 1.9099672317504883, 'train/ce_loss': 0.418359375, 'train/seg_cls_loss': 0.016522216796875, 'train/kl_loss': 0.2966796875, 'train/mask_bce_loss': 0.20478251129388808, 'train/mask_dice_loss': 0.522173760831356, 'train/mask_loss': 0.7269562721252442, 'metrics/total_secs_per_batch': 7.609019041061401, 'metrics/data_secs_per_batch': 3.9456886053085327, '_timestamp': 1740965282.4539952}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 136 is less than current step: 499. Dropping entry: {'train/lr': 4.4816326530612234e-05, '_timestamp': 1740965282.4541962}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 137 is less than current step: 499. Dropping entry: {'train/loss': 1.927476978302002, 'train/ce_loss': 0.315625, 'train/seg_cls_loss': 0.017962646484375, 'train/kl_loss': 0.33515625, 'train/mask_bce_loss': 0.13575422056019307, 'train/mask_dice_loss': 0.6488338530063629, 'train/mask_loss': 0.7845880806446075, 'metrics/total_secs_per_batch': 7.590449333190918, 'metrics/data_secs_per_batch': 3.437824177742004, '_timestamp': 1740965290.0443993}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 137 is less than current step: 499. Dropping entry: {'train/lr': 4.469387755102041e-05, '_timestamp': 1740965290.0446818}).
Epoch: [4][139/500]	Time  8.444 ( 8.444)	Loss 1.4381 (1.1837)	CeLoss 0.2598 (0.2683)	SegCLSLoss 0.0242 (0.0148)	KLLoss 0.3672 (0.2557)	MaskLoss 0.5647 (0.4413)	MaskBCELoss 0.0333 (0.0433)	MaskDICELoss 0.5314 (0.3981)
[2025-03-02 19:28:27,044] [INFO] [logging.py:96:log_dist] [Rank 0] step=2140, skipped=0, lr=[4.451020408163265e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:28:27,050] [INFO] [timer.py:215:stop] epoch=0/micro_step=21400/global_step=2140, RunningAvgSamplesPerSec=1.4751713258313959, CurrSamplesPerSec=1.1680765048582082, MemAllocated=31.26GB, MaxMemAllocated=37.19GB
Epoch: [4][140/500]	Time  8.563 ( 8.563)	Loss 1.8627 (1.4130)	CeLoss 0.2324 (0.3416)	SegCLSLoss 0.0118 (0.0143)	KLLoss 0.3848 (0.3336)	MaskLoss 0.7927 (0.5154)	MaskBCELoss 0.0469 (0.0624)	MaskDICELoss 0.7457 (0.4529)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 138 is less than current step: 499. Dropping entry: {'train/loss': 1.18371040225029, 'train/ce_loss': 0.26826171875, 'train/seg_cls_loss': 0.014764404296875, 'train/kl_loss': 0.2556640625, 'train/mask_bce_loss': 0.04326697420328855, 'train/mask_dice_loss': 0.398051118850708, 'train/mask_loss': 0.44131809175014497, 'metrics/total_secs_per_batch': 8.443676710128784, 'metrics/data_secs_per_batch': 3.906935143470764, '_timestamp': 1740965298.4882464}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 138 is less than current step: 499. Dropping entry: {'train/lr': 4.457142857142857e-05, '_timestamp': 1740965298.4886274}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 139 is less than current step: 499. Dropping entry: {'train/loss': 1.4129620790481567, 'train/ce_loss': 0.3416015625, 'train/seg_cls_loss': 0.014288330078125, 'train/kl_loss': 0.33359375, 'train/mask_bce_loss': 0.06242382191121578, 'train/mask_dice_loss': 0.4529439307749271, 'train/mask_loss': 0.5153677582740783, 'metrics/total_secs_per_batch': 8.562862157821655, 'metrics/data_secs_per_batch': 3.5883553504943846, '_timestamp': 1740965307.0507722}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 139 is less than current step: 499. Dropping entry: {'train/lr': 4.444897959183673e-05, '_timestamp': 1740965307.0510452}).
Epoch: [4][141/500]	Time  6.806 ( 6.806)	Loss 2.3043 (1.4207)	CeLoss 0.1504 (0.4035)	SegCLSLoss 0.0222 (0.0106)	KLLoss 0.3867 (0.2605)	MaskLoss 1.0525 (0.4928)	MaskBCELoss 0.2424 (0.1145)	MaskDICELoss 0.8102 (0.3783)
Epoch: [4][142/500]	Time  6.486 ( 6.486)	Loss 1.8663 (1.5637)	CeLoss 0.2363 (0.4636)	SegCLSLoss 0.0172 (0.0111)	KLLoss 0.3594 (0.2584)	MaskLoss 0.7925 (0.5344)	MaskBCELoss 0.0105 (0.0917)	MaskDICELoss 0.7820 (0.4427)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 140 is less than current step: 499. Dropping entry: {'train/loss': 1.4207039952278138, 'train/ce_loss': 0.403515625, 'train/seg_cls_loss': 0.010577392578125, 'train/kl_loss': 0.260546875, 'train/mask_bce_loss': 0.11452641859650611, 'train/mask_dice_loss': 0.37829627841711044, 'train/mask_loss': 0.4928226865828037, 'metrics/total_secs_per_batch': 6.805697202682495, 'metrics/data_secs_per_batch': 3.0038716077804564, '_timestamp': 1740965313.8566504}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 140 is less than current step: 499. Dropping entry: {'train/lr': 4.432653061224489e-05, '_timestamp': 1740965313.856922}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 141 is less than current step: 499. Dropping entry: {'train/loss': 1.5637261271476746, 'train/ce_loss': 0.46357421875, 'train/seg_cls_loss': 0.011090087890625, 'train/kl_loss': 0.2583984375, 'train/mask_bce_loss': 0.0916873780079186, 'train/mask_dice_loss': 0.4427147537469864, 'train/mask_loss': 0.5344021260738373, 'metrics/total_secs_per_batch': 6.486442804336548, 'metrics/data_secs_per_batch': 2.5134806871414184, '_timestamp': 1740965320.3430815}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 141 is less than current step: 499. Dropping entry: {'train/lr': 4.4204081632653056e-05, '_timestamp': 1740965320.3433428}).
Epoch: [4][143/500]	Time  8.289 ( 8.289)	Loss 2.2690 (1.7506)	CeLoss 0.2051 (0.4206)	SegCLSLoss 0.0237 (0.0136)	KLLoss 0.3574 (0.2527)	MaskLoss 1.0085 (0.6489)	MaskBCELoss 0.0816 (0.0955)	MaskDICELoss 0.9269 (0.5534)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 142 is less than current step: 499. Dropping entry: {'train/loss': 1.750585663318634, 'train/ce_loss': 0.42060546875, 'train/seg_cls_loss': 0.01361083984375, 'train/kl_loss': 0.252734375, 'train/mask_bce_loss': 0.09551284527406097, 'train/mask_dice_loss': 0.5533639520406723, 'train/mask_loss': 0.6488767981529235, 'metrics/total_secs_per_batch': 8.288743257522583, 'metrics/data_secs_per_batch': 3.9311296939849854, '_timestamp': 1740965328.6320164}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 142 is less than current step: 499. Dropping entry: {'train/lr': 4.4081632653061226e-05, '_timestamp': 1740965328.6323369}).
Epoch: [4][144/500]	Time  9.907 ( 9.907)	Loss 1.5920 (1.9999)	CeLoss 0.2119 (0.3713)	SegCLSLoss 0.0201 (0.0144)	KLLoss 0.3730 (0.2938)	MaskLoss 0.6661 (0.7959)	MaskBCELoss 0.0279 (0.1641)	MaskDICELoss 0.6382 (0.6318)
Epoch: [4][145/500]	Time  6.865 ( 6.865)	Loss 2.1626 (1.3908)	CeLoss 0.2500 (0.2508)	SegCLSLoss 0.0157 (0.0152)	KLLoss 0.3555 (0.3666)	MaskLoss 0.9348 (0.5478)	MaskBCELoss 0.0090 (0.0870)	MaskDICELoss 0.9258 (0.4608)
Epoch: [4][146/500]	Time  7.359 ( 7.359)	Loss 1.0580 (1.6122)	CeLoss 0.1895 (0.5203)	SegCLSLoss 0.0204 (0.0124)	KLLoss 0.3750 (0.2617)	MaskLoss 0.4108 (0.5297)	MaskBCELoss 0.1195 (0.1140)	MaskDICELoss 0.2913 (0.4158)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 143 is less than current step: 499. Dropping entry: {'train/loss': 1.9999353170394898, 'train/ce_loss': 0.3712890625, 'train/seg_cls_loss': 0.014385986328125, 'train/kl_loss': 0.29375, 'train/mask_bce_loss': 0.16410389244556428, 'train/mask_dice_loss': 0.6318110406398774, 'train/mask_loss': 0.7959149360656739, 'metrics/total_secs_per_batch': 9.907191038131714, 'metrics/data_secs_per_batch': 3.808011841773987, '_timestamp': 1740965338.5389729}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 143 is less than current step: 499. Dropping entry: {'train/lr': 4.395918367346939e-05, '_timestamp': 1740965338.5392237}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 144 is less than current step: 499. Dropping entry: {'train/loss': 1.390804535150528, 'train/ce_loss': 0.25078125, 'train/seg_cls_loss': 0.015228271484375, 'train/kl_loss': 0.3666015625, 'train/mask_bce_loss': 0.08695985041558743, 'train/mask_dice_loss': 0.4608349993824959, 'train/mask_loss': 0.5477948486804962, 'metrics/total_secs_per_batch': 6.864598035812378, 'metrics/data_secs_per_batch': 2.877476954460144, '_timestamp': 1740965345.4036713}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 144 is less than current step: 499. Dropping entry: {'train/lr': 4.3836734693877545e-05, '_timestamp': 1740965345.403951}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 145 is less than current step: 499. Dropping entry: {'train/loss': 1.6122238636016846, 'train/ce_loss': 0.5203125, 'train/seg_cls_loss': 0.0123779296875, 'train/kl_loss': 0.26171875, 'train/mask_bce_loss': 0.11397302225232124, 'train/mask_dice_loss': 0.4157717123627663, 'train/mask_loss': 0.5297447323799134, 'metrics/total_secs_per_batch': 7.359363317489624, 'metrics/data_secs_per_batch': 3.625467562675476, '_timestamp': 1740965352.762991}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 145 is less than current step: 499. Dropping entry: {'train/lr': 4.371428571428571e-05, '_timestamp': 1740965352.7632875}).
Epoch: [4][147/500]	Time  6.180 ( 6.180)	Loss 1.4692 (1.3897)	CeLoss 0.1953 (0.5988)	SegCLSLoss 0.0156 (0.0113)	KLLoss 0.3711 (0.2240)	MaskLoss 0.6145 (0.3814)	MaskBCELoss 0.0347 (0.0625)	MaskDICELoss 0.5798 (0.3190)
Epoch: [4][148/500]	Time  7.959 ( 7.959)	Loss 1.0923 (1.6432)	CeLoss 0.2480 (0.3335)	SegCLSLoss 0.0138 (0.0148)	KLLoss 0.3691 (0.3309)	MaskLoss 0.3997 (0.6345)	MaskBCELoss 0.2036 (0.1615)	MaskDICELoss 0.1961 (0.4730)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 146 is less than current step: 499. Dropping entry: {'train/loss': 1.3896853983402253, 'train/ce_loss': 0.598828125, 'train/seg_cls_loss': 0.01126708984375, 'train/kl_loss': 0.2240234375, 'train/mask_bce_loss': 0.06246447232551873, 'train/mask_dice_loss': 0.3189504951238632, 'train/mask_loss': 0.38141496777534484, 'metrics/total_secs_per_batch': 6.179814338684082, 'metrics/data_secs_per_batch': 2.7103533029556273, '_timestamp': 1740965358.9427977}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 146 is less than current step: 499. Dropping entry: {'train/lr': 4.359183673469387e-05, '_timestamp': 1740965358.9430187}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 147 is less than current step: 499. Dropping entry: {'train/loss': 1.6432372689247132, 'train/ce_loss': 0.33349609375, 'train/seg_cls_loss': 0.01484375, 'train/kl_loss': 0.330859375, 'train/mask_bce_loss': 0.16149449069052935, 'train/mask_dice_loss': 0.4730147644877434, 'train/mask_loss': 0.6345092564821243, 'metrics/total_secs_per_batch': 7.958561658859253, 'metrics/data_secs_per_batch': 3.4417692899703978, '_timestamp': 1740965366.9013948}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 147 is less than current step: 499. Dropping entry: {'train/lr': 4.3469387755102034e-05, '_timestamp': 1740965366.9016721}).
Epoch: [4][149/500]	Time  7.655 ( 7.655)	Loss 1.7738 (1.4313)	CeLoss 0.2148 (0.3992)	SegCLSLoss 0.0299 (0.0138)	KLLoss 0.3770 (0.2588)	MaskLoss 0.7531 (0.4997)	MaskBCELoss 0.0133 (0.1022)	MaskDICELoss 0.7399 (0.3975)
[2025-03-02 19:29:42,007] [INFO] [logging.py:96:log_dist] [Rank 0] step=2150, skipped=0, lr=[4.3285714285714286e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:29:42,013] [INFO] [timer.py:215:stop] epoch=0/micro_step=21500/global_step=2150, RunningAvgSamplesPerSec=1.4744460396701473, CurrSamplesPerSec=1.341117218035114, MemAllocated=31.56GB, MaxMemAllocated=37.19GB
Epoch: [4][150/500]	Time  7.458 ( 7.458)	Loss 1.7721 (1.6382)	CeLoss 0.2188 (0.5279)	SegCLSLoss 0.0197 (0.0106)	KLLoss 0.3691 (0.2230)	MaskLoss 0.7532 (0.5415)	MaskBCELoss 0.0500 (0.1672)	MaskDICELoss 0.7032 (0.3742)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 148 is less than current step: 499. Dropping entry: {'train/loss': 1.4313393115997315, 'train/ce_loss': 0.399169921875, 'train/seg_cls_loss': 0.01380615234375, 'train/kl_loss': 0.2587890625, 'train/mask_bce_loss': 0.10218254132196307, 'train/mask_dice_loss': 0.39754473567008974, 'train/mask_loss': 0.49972727298736574, 'metrics/total_secs_per_batch': 7.654534339904785, 'metrics/data_secs_per_batch': 3.647943043708801, '_timestamp': 1740965374.5561297}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 148 is less than current step: 499. Dropping entry: {'train/lr': 4.3346938775510204e-05, '_timestamp': 1740965374.5564713}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 149 is less than current step: 499. Dropping entry: {'train/loss': 1.638202691078186, 'train/ce_loss': 0.5279296875, 'train/seg_cls_loss': 0.01055908203125, 'train/kl_loss': 0.223046875, 'train/mask_bce_loss': 0.16721596866846083, 'train/mask_dice_loss': 0.37424865663051604, 'train/mask_loss': 0.5414646208286286, 'metrics/total_secs_per_batch': 7.458259582519531, 'metrics/data_secs_per_batch': 3.2103358268737794, '_timestamp': 1740965382.0140069}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 149 is less than current step: 499. Dropping entry: {'train/lr': 4.322448979591837e-05, '_timestamp': 1740965382.014262}).
Epoch: [4][151/500]	Time  6.747 ( 6.747)	Loss 1.6558 (1.5665)	CeLoss 0.2793 (0.7211)	SegCLSLoss 0.0134 (0.0085)	KLLoss 0.3633 (0.2215)	MaskLoss 0.6668 (0.4094)	MaskBCELoss 0.0404 (0.0706)	MaskDICELoss 0.6264 (0.3388)
Epoch: [4][152/500]	Time  7.734 ( 7.734)	Loss 1.5557 (1.5854)	CeLoss 0.2227 (0.2978)	SegCLSLoss 0.0168 (0.0123)	KLLoss 0.3652 (0.2994)	MaskLoss 0.6440 (0.6257)	MaskBCELoss 0.0523 (0.1512)	MaskDICELoss 0.5918 (0.4745)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 150 is less than current step: 499. Dropping entry: {'train/loss': 1.5664999961853028, 'train/ce_loss': 0.72109375, 'train/seg_cls_loss': 0.00849609375, 'train/kl_loss': 0.221484375, 'train/mask_bce_loss': 0.07057620026171207, 'train/mask_dice_loss': 0.33884567618370054, 'train/mask_loss': 0.4094218730926514, 'metrics/total_secs_per_batch': 6.74662446975708, 'metrics/data_secs_per_batch': 2.6247795820236206, '_timestamp': 1740965388.7607913}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 150 is less than current step: 499. Dropping entry: {'train/lr': 4.310204081632653e-05, '_timestamp': 1740965388.7609775}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 151 is less than current step: 499. Dropping entry: {'train/loss': 1.58540221452713, 'train/ce_loss': 0.297802734375, 'train/seg_cls_loss': 0.012261962890625, 'train/kl_loss': 0.2994140625, 'train/mask_bce_loss': 0.15122624076902866, 'train/mask_dice_loss': 0.47445827573537824, 'train/mask_loss': 0.6256845101714135, 'metrics/total_secs_per_batch': 7.734471797943115, 'metrics/data_secs_per_batch': 3.3247800350189207, '_timestamp': 1740965396.4952788}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 151 is less than current step: 499. Dropping entry: {'train/lr': 4.2979591836734686e-05, '_timestamp': 1740965396.4955752}).
Epoch: [4][153/500]	Time  8.761 ( 8.761)	Loss 1.5859 (1.5428)	CeLoss 1.5859 (0.4920)	SegCLSLoss 0.0000 (0.0109)	KLLoss 0.0000 (0.2596)	MaskLoss 0.0000 (0.5096)	MaskBCELoss 0.0000 (0.1063)	MaskDICELoss 0.0000 (0.4033)
Epoch: [4][154/500]	Time  8.335 ( 8.335)	Loss 1.2553 (1.6288)	CeLoss 0.2090 (0.4756)	SegCLSLoss 0.0123 (0.0113)	KLLoss 0.3789 (0.2955)	MaskLoss 0.5017 (0.5591)	MaskBCELoss 0.2443 (0.0764)	MaskDICELoss 0.2574 (0.4826)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 152 is less than current step: 499. Dropping entry: {'train/loss': 1.5427942335605622, 'train/ce_loss': 0.492041015625, 'train/seg_cls_loss': 0.010882568359375, 'train/kl_loss': 0.2595703125, 'train/mask_bce_loss': 0.10627302220091224, 'train/mask_dice_loss': 0.4032832831144333, 'train/mask_loss': 0.5095563024282456, 'metrics/total_secs_per_batch': 8.760843515396118, 'metrics/data_secs_per_batch': 3.738486576080322, '_timestamp': 1740965405.2561297}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 152 is less than current step: 499. Dropping entry: {'train/lr': 4.285714285714285e-05, '_timestamp': 1740965405.2563896}).
Epoch: [4][155/500]	Time  9.375 ( 9.375)	Loss 2.9539 (2.1398)	CeLoss 0.2314 (0.3334)	SegCLSLoss 0.0160 (0.0140)	KLLoss 0.3750 (0.3338)	MaskLoss 1.3383 (0.8829)	MaskBCELoss 0.5045 (0.2613)	MaskDICELoss 0.8338 (0.6216)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 153 is less than current step: 499. Dropping entry: {'train/loss': 1.6288377285003661, 'train/ce_loss': 0.4755859375, 'train/seg_cls_loss': 0.011322021484375, 'train/kl_loss': 0.2955078125, 'train/mask_bce_loss': 0.07644930444657802, 'train/mask_dice_loss': 0.48264728486537933, 'train/mask_loss': 0.5590965956449508, 'metrics/total_secs_per_batch': 8.3351571559906, 'metrics/data_secs_per_batch': 3.282743525505066, '_timestamp': 1740965413.591334}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 153 is less than current step: 499. Dropping entry: {'train/lr': 4.273469387755101e-05, '_timestamp': 1740965413.5916648}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 154 is less than current step: 499. Dropping entry: {'train/loss': 2.1398167967796327, 'train/ce_loss': 0.3333984375, 'train/seg_cls_loss': 0.014031982421875, 'train/kl_loss': 0.3337890625, 'train/mask_bce_loss': 0.26133698858320714, 'train/mask_dice_loss': 0.6216085225343704, 'train/mask_loss': 0.8829455018043518, 'metrics/total_secs_per_batch': 9.375030279159546, 'metrics/data_secs_per_batch': 4.474239301681519, '_timestamp': 1740965422.966394}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 154 is less than current step: 499. Dropping entry: {'train/lr': 4.261224489795918e-05, '_timestamp': 1740965422.9665997}).
Epoch: [4][156/500]	Time  6.912 ( 6.912)	Loss 2.4417 (1.2684)	CeLoss 0.1230 (0.5564)	SegCLSLoss 0.0284 (0.0072)	KLLoss 0.3848 (0.1857)	MaskLoss 1.1329 (0.3449)	MaskBCELoss 0.2802 (0.1255)	MaskDICELoss 0.8527 (0.2194)
Epoch: [4][157/500]	Time  7.456 ( 7.456)	Loss 1.8107 (1.0884)	CeLoss 0.1699 (0.5192)	SegCLSLoss 0.0183 (0.0068)	KLLoss 0.3652 (0.1477)	MaskLoss 0.7979 (0.2755)	MaskBCELoss 0.1233 (0.0273)	MaskDICELoss 0.6747 (0.2483)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 155 is less than current step: 499. Dropping entry: {'train/loss': 1.2683810412883758, 'train/ce_loss': 0.556396484375, 'train/seg_cls_loss': 0.007183837890625, 'train/kl_loss': 0.1857421875, 'train/mask_bce_loss': 0.12546866983175278, 'train/mask_dice_loss': 0.2194396212697029, 'train/mask_loss': 0.3449082911014557, 'metrics/total_secs_per_batch': 6.912236213684082, 'metrics/data_secs_per_batch': 2.884384536743164, '_timestamp': 1740965429.8786232}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 155 is less than current step: 499. Dropping entry: {'train/lr': 4.2489795918367345e-05, '_timestamp': 1740965429.8788195}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 156 is less than current step: 499. Dropping entry: {'train/loss': 1.08844997882843, 'train/ce_loss': 0.5192138671875, 'train/seg_cls_loss': 0.00684814453125, 'train/kl_loss': 0.14765625, 'train/mask_bce_loss': 0.027281386032700538, 'train/mask_dice_loss': 0.2482546389102936, 'train/mask_loss': 0.2755360245704651, 'metrics/total_secs_per_batch': 7.455658912658691, 'metrics/data_secs_per_batch': 3.378143048286438, '_timestamp': 1740965437.334354}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 156 is less than current step: 499. Dropping entry: {'train/lr': 4.236734693877551e-05, '_timestamp': 1740965437.3346527}).
Epoch: [4][158/500]	Time  8.220 ( 8.220)	Loss 1.8539 (1.8741)	CeLoss 0.1963 (0.2156)	SegCLSLoss 0.0153 (0.0169)	KLLoss 0.3691 (0.3637)	MaskLoss 0.8064 (0.8068)	MaskBCELoss 0.0097 (0.1660)	MaskDICELoss 0.7967 (0.6408)
Epoch: [4][159/500]	Time  8.179 ( 8.179)	Loss 1.0633 (1.8053)	CeLoss 0.2773 (0.3296)	SegCLSLoss 0.0139 (0.0164)	KLLoss 0.3691 (0.3322)	MaskLoss 0.3705 (0.7169)	MaskBCELoss 0.0538 (0.0776)	MaskDICELoss 0.3167 (0.6393)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 157 is less than current step: 499. Dropping entry: {'train/loss': 1.8740509748458862, 'train/ce_loss': 0.215625, 'train/seg_cls_loss': 0.016888427734375, 'train/kl_loss': 0.363671875, 'train/mask_bce_loss': 0.16601054966449738, 'train/mask_dice_loss': 0.6407903157174587, 'train/mask_loss': 0.8068008691072464, 'metrics/total_secs_per_batch': 8.220057487487793, 'metrics/data_secs_per_batch': 3.751689147949219, '_timestamp': 1740965445.5542927}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 157 is less than current step: 499. Dropping entry: {'train/lr': 4.224489795918367e-05, '_timestamp': 1740965445.5545583}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 158 is less than current step: 499. Dropping entry: {'train/loss': 1.8053267002105713, 'train/ce_loss': 0.32958984375, 'train/seg_cls_loss': 0.016448974609375, 'train/kl_loss': 0.3322265625, 'train/mask_bce_loss': 0.07759212437085808, 'train/mask_dice_loss': 0.6393290430307388, 'train/mask_loss': 0.7169211685657502, 'metrics/total_secs_per_batch': 8.179171085357666, 'metrics/data_secs_per_batch': 4.088120865821838, '_timestamp': 1740965453.7334628}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 158 is less than current step: 499. Dropping entry: {'train/lr': 4.212244897959183e-05, '_timestamp': 1740965453.733739}).
[2025-03-02 19:31:03,013] [INFO] [logging.py:96:log_dist] [Rank 0] step=2160, skipped=0, lr=[4.206122448979591e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:31:03,020] [INFO] [timer.py:215:stop] epoch=0/micro_step=21600/global_step=2160, RunningAvgSamplesPerSec=1.4731202091435878, CurrSamplesPerSec=1.076923120374512, MemAllocated=31.23GB, MaxMemAllocated=37.19GB
Epoch: [4][160/500]	Time  9.287 ( 9.287)	Loss 1.1052 (2.1830)	CeLoss 0.2676 (0.2285)	SegCLSLoss 0.0114 (0.0180)	KLLoss 0.3770 (0.3699)	MaskLoss 0.3973 (0.9544)	MaskBCELoss 0.1132 (0.2118)	MaskDICELoss 0.2841 (0.7425)
Epoch: [4][161/500]	Time  8.314 ( 8.314)	Loss 0.0640 (1.4219)	CeLoss 0.0640 (0.3000)	SegCLSLoss 0.0000 (0.0108)	KLLoss 0.0000 (0.2945)	MaskLoss 0.0000 (0.5435)	MaskBCELoss 0.0000 (0.1907)	MaskDICELoss 0.0000 (0.3528)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 159 is less than current step: 499. Dropping entry: {'train/loss': 2.183046746253967, 'train/ce_loss': 0.228515625, 'train/seg_cls_loss': 0.01796875, 'train/kl_loss': 0.369921875, 'train/mask_bce_loss': 0.21184384524822236, 'train/mask_dice_loss': 0.7425213277339935, 'train/mask_loss': 0.9543651670217514, 'metrics/total_secs_per_batch': 9.287254333496094, 'metrics/data_secs_per_batch': 3.9955476760864257, '_timestamp': 1740965463.020618}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 159 is less than current step: 499. Dropping entry: {'train/lr': 4.2e-05, '_timestamp': 1740965463.0209394}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 160 is less than current step: 499. Dropping entry: {'train/loss': 1.4219281852245331, 'train/ce_loss': 0.299951171875, 'train/seg_cls_loss': 0.010809326171875, 'train/kl_loss': 0.29453125, 'train/mask_bce_loss': 0.19067845242097975, 'train/mask_dice_loss': 0.35278075039386747, 'train/mask_loss': 0.5434592097997666, 'metrics/total_secs_per_batch': 8.313820838928223, 'metrics/data_secs_per_batch': 3.2910678148269654, '_timestamp': 1740965471.3345442}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 160 is less than current step: 499. Dropping entry: {'train/lr': 4.187755102040816e-05, '_timestamp': 1740965471.334816}).
Epoch: [4][162/500]	Time  7.765 ( 7.765)	Loss 0.7344 (1.9635)	CeLoss 0.7344 (0.3390)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.2961)	MaskLoss 0.0000 (0.7939)	MaskBCELoss 0.0000 (0.2264)	MaskDICELoss 0.0000 (0.5675)
Epoch: [4][163/500]	Time  7.137 ( 7.137)	Loss 1.5770 (1.6589)	CeLoss 0.3008 (0.2956)	SegCLSLoss 0.0102 (0.0161)	KLLoss 0.3711 (0.3332)	MaskLoss 0.6166 (0.6610)	MaskBCELoss 0.3224 (0.1236)	MaskDICELoss 0.2942 (0.5375)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 161 is less than current step: 499. Dropping entry: {'train/loss': 1.9634929537773131, 'train/ce_loss': 0.33896484375, 'train/seg_cls_loss': 0.0141845703125, 'train/kl_loss': 0.29609375, 'train/mask_bce_loss': 0.22640766678377985, 'train/mask_dice_loss': 0.5674970120191574, 'train/mask_loss': 0.7939046800136567, 'metrics/total_secs_per_batch': 7.76472282409668, 'metrics/data_secs_per_batch': 3.5377816915512086, '_timestamp': 1740965479.099463}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 161 is less than current step: 499. Dropping entry: {'train/lr': 4.1755102040816324e-05, '_timestamp': 1740965479.099686}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 162 is less than current step: 499. Dropping entry: {'train/loss': 1.6588967442512512, 'train/ce_loss': 0.29560546875, 'train/seg_cls_loss': 0.016131591796875, 'train/kl_loss': 0.333203125, 'train/mask_bce_loss': 0.12356153484433889, 'train/mask_dice_loss': 0.5374786347150803, 'train/mask_loss': 0.6610401690006256, 'metrics/total_secs_per_batch': 7.137227535247803, 'metrics/data_secs_per_batch': 3.005659246444702, '_timestamp': 1740965486.236526}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 162 is less than current step: 499. Dropping entry: {'train/lr': 4.163265306122449e-05, '_timestamp': 1740965486.2367902}).
Epoch: [4][164/500]	Time  9.681 ( 9.681)	Loss 2.2753 (2.0619)	CeLoss 0.2285 (0.3105)	SegCLSLoss 0.0192 (0.0156)	KLLoss 0.3613 (0.3316)	MaskLoss 1.0009 (0.8553)	MaskBCELoss 0.0203 (0.2195)	MaskDICELoss 0.9806 (0.6358)
Epoch: [4][165/500]	Time  6.293 ( 6.293)	Loss 2.4243 (1.6828)	CeLoss 0.1855 (0.4765)	SegCLSLoss 0.0225 (0.0134)	KLLoss 0.3691 (0.2924)	MaskLoss 1.0950 (0.5851)	MaskBCELoss 0.2212 (0.1741)	MaskDICELoss 0.8738 (0.4110)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 163 is less than current step: 499. Dropping entry: {'train/loss': 2.0619100093841554, 'train/ce_loss': 0.310546875, 'train/seg_cls_loss': 0.015594482421875, 'train/kl_loss': 0.331640625, 'train/mask_bce_loss': 0.21945547461509704, 'train/mask_dice_loss': 0.6358159154653549, 'train/mask_loss': 0.8552713871002198, 'metrics/total_secs_per_batch': 9.680635452270508, 'metrics/data_secs_per_batch': 4.487267637252808, '_timestamp': 1740965495.9171286}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 163 is less than current step: 499. Dropping entry: {'train/lr': 4.151020408163265e-05, '_timestamp': 1740965495.9174008}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 164 is less than current step: 499. Dropping entry: {'train/loss': 1.6828214883804322, 'train/ce_loss': 0.47646484375, 'train/seg_cls_loss': 0.013397216796875, 'train/kl_loss': 0.2923828125, 'train/mask_bce_loss': 0.17411725262645633, 'train/mask_dice_loss': 0.41099465638399124, 'train/mask_loss': 0.585111915320158, 'metrics/total_secs_per_batch': 6.293303966522217, 'metrics/data_secs_per_batch': 2.7531550884246827, '_timestamp': 1740965502.2104769}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 164 is less than current step: 499. Dropping entry: {'train/lr': 4.1387755102040806e-05, '_timestamp': 1740965502.2107584}).
Epoch: [4][166/500]	Time  9.504 ( 9.504)	Loss 2.2290 (1.6585)	CeLoss 0.2217 (0.2972)	SegCLSLoss 0.0198 (0.0125)	KLLoss 0.3672 (0.3324)	MaskLoss 0.9807 (0.6609)	MaskBCELoss 0.0253 (0.1545)	MaskDICELoss 0.9554 (0.5064)
Epoch: [4][167/500]	Time  6.280 ( 6.280)	Loss 1.9854 (1.6372)	CeLoss 0.2598 (0.6765)	SegCLSLoss 0.0198 (0.0104)	KLLoss 0.3691 (0.1855)	MaskLoss 0.8394 (0.4685)	MaskBCELoss 0.0111 (0.0871)	MaskDICELoss 0.8282 (0.3814)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 165 is less than current step: 499. Dropping entry: {'train/loss': 1.6584763407707215, 'train/ce_loss': 0.29716796875, 'train/seg_cls_loss': 0.01248779296875, 'train/kl_loss': 0.332421875, 'train/mask_bce_loss': 0.1545001534745097, 'train/mask_dice_loss': 0.5063786298036576, 'train/mask_loss': 0.6608787924051285, 'metrics/total_secs_per_batch': 9.504094123840332, 'metrics/data_secs_per_batch': 3.7921127319335937, '_timestamp': 1740965511.7147458}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 165 is less than current step: 499. Dropping entry: {'train/lr': 4.126530612244898e-05, '_timestamp': 1740965511.7150886}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 166 is less than current step: 499. Dropping entry: {'train/loss': 1.6371962308883667, 'train/ce_loss': 0.67646484375, 'train/seg_cls_loss': 0.01043701171875, 'train/kl_loss': 0.185546875, 'train/mask_bce_loss': 0.08711903635412455, 'train/mask_dice_loss': 0.3814302384853363, 'train/mask_loss': 0.4685492753982544, 'metrics/total_secs_per_batch': 6.280380964279175, 'metrics/data_secs_per_batch': 2.7029674530029295, '_timestamp': 1740965517.9949234}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 166 is less than current step: 499. Dropping entry: {'train/lr': 4.114285714285714e-05, '_timestamp': 1740965517.9952052}).
Epoch: [4][168/500]	Time  7.937 ( 7.937)	Loss 1.7818 (1.7799)	CeLoss 0.3105 (0.2860)	SegCLSLoss 0.0111 (0.0148)	KLLoss 0.3691 (0.2953)	MaskLoss 0.7141 (0.7284)	MaskBCELoss 0.1825 (0.2007)	MaskDICELoss 0.5316 (0.5277)
Epoch: [4][169/500]	Time  8.094 ( 8.094)	Loss 2.7215 (1.9264)	CeLoss 0.1699 (0.3239)	SegCLSLoss 0.0199 (0.0157)	KLLoss 0.3633 (0.2930)	MaskLoss 1.2528 (0.7826)	MaskBCELoss 0.3332 (0.1559)	MaskDICELoss 0.9196 (0.6267)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 167 is less than current step: 499. Dropping entry: {'train/loss': 1.7799477338790894, 'train/ce_loss': 0.28603515625, 'train/seg_cls_loss': 0.014849853515625, 'train/kl_loss': 0.2953125, 'train/mask_bce_loss': 0.20068413838744165, 'train/mask_dice_loss': 0.5276686251163483, 'train/mask_loss': 0.7283527791500092, 'metrics/total_secs_per_batch': 7.937480688095093, 'metrics/data_secs_per_batch': 2.87172589302063, '_timestamp': 1740965525.9325027}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 167 is less than current step: 499. Dropping entry: {'train/lr': 4.10204081632653e-05, '_timestamp': 1740965525.9328134}).
[2025-03-02 19:32:22,794] [INFO] [logging.py:96:log_dist] [Rank 0] step=2170, skipped=0, lr=[4.083673469387755e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:32:22,801] [INFO] [timer.py:215:stop] epoch=0/micro_step=21700/global_step=2170, RunningAvgSamplesPerSec=1.471931451052902, CurrSamplesPerSec=1.1397821978109235, MemAllocated=31.29GB, MaxMemAllocated=37.23GB
Epoch: [4][170/500]	Time  8.775 ( 8.775)	Loss 2.6118 (1.6272)	CeLoss 0.1738 (0.4126)	SegCLSLoss 0.0306 (0.0151)	KLLoss 0.3730 (0.2961)	MaskLoss 1.1926 (0.5886)	MaskBCELoss 0.2985 (0.1366)	MaskDICELoss 0.8942 (0.4520)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 168 is less than current step: 499. Dropping entry: {'train/loss': 1.9263657331466675, 'train/ce_loss': 0.32392578125, 'train/seg_cls_loss': 0.015679931640625, 'train/kl_loss': 0.29296875, 'train/mask_bce_loss': 0.1558809235692024, 'train/mask_dice_loss': 0.626686692237854, 'train/mask_loss': 0.7825676083564759, 'metrics/total_secs_per_batch': 8.094380378723145, 'metrics/data_secs_per_batch': 3.174090528488159, '_timestamp': 1740965534.0267806}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 168 is less than current step: 499. Dropping entry: {'train/lr': 4.0897959183673465e-05, '_timestamp': 1740965534.0270715}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 169 is less than current step: 499. Dropping entry: {'train/loss': 1.6271983504295349, 'train/ce_loss': 0.41259765625, 'train/seg_cls_loss': 0.01512451171875, 'train/kl_loss': 0.29609375, 'train/mask_bce_loss': 0.13659047279506922, 'train/mask_dice_loss': 0.45200868844985964, 'train/mask_loss': 0.5885991618037224, 'metrics/total_secs_per_batch': 8.775134563446045, 'metrics/data_secs_per_batch': 4.365164303779602, '_timestamp': 1740965542.8017428}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 169 is less than current step: 499. Dropping entry: {'train/lr': 4.077551020408163e-05, '_timestamp': 1740965542.8020093}).
Epoch: [4][171/500]	Time  8.052 ( 8.052)	Loss 0.1260 (1.3214)	CeLoss 0.1260 (0.3145)	SegCLSLoss 0.0000 (0.0113)	KLLoss 0.0000 (0.2549)	MaskLoss 0.0000 (0.4877)	MaskBCELoss 0.0000 (0.0950)	MaskDICELoss 0.0000 (0.3928)
Epoch: [4][172/500]	Time  7.378 ( 7.378)	Loss 0.7188 (1.4948)	CeLoss 0.7188 (0.6116)	SegCLSLoss 0.0000 (0.0107)	KLLoss 0.0000 (0.2213)	MaskLoss 0.0000 (0.4278)	MaskBCELoss 0.0000 (0.0786)	MaskDICELoss 0.0000 (0.3492)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 170 is less than current step: 499. Dropping entry: {'train/loss': 1.3213999390602111, 'train/ce_loss': 0.314501953125, 'train/seg_cls_loss': 0.01131591796875, 'train/kl_loss': 0.2548828125, 'train/mask_bce_loss': 0.09495175145566463, 'train/mask_dice_loss': 0.3927745819091797, 'train/mask_loss': 0.48772633671760557, 'metrics/total_secs_per_batch': 8.052071571350098, 'metrics/data_secs_per_batch': 3.4277992486953734, '_timestamp': 1740965550.8540297}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 170 is less than current step: 499. Dropping entry: {'train/lr': 4.065306122448979e-05, '_timestamp': 1740965550.8543057}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 171 is less than current step: 499. Dropping entry: {'train/loss': 1.494825303554535, 'train/ce_loss': 0.61162109375, 'train/seg_cls_loss': 0.010711669921875, 'train/kl_loss': 0.2212890625, 'train/mask_bce_loss': 0.07860874719917774, 'train/mask_dice_loss': 0.34922382831573484, 'train/mask_loss': 0.4278325766324997, 'metrics/total_secs_per_batch': 7.377602577209473, 'metrics/data_secs_per_batch': 3.0458239555358886, '_timestamp': 1740965558.2318096}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 171 is less than current step: 499. Dropping entry: {'train/lr': 4.053061224489796e-05, '_timestamp': 1740965558.232143}).
Epoch: [4][173/500]	Time  8.617 ( 8.617)	Loss 2.2014 (1.7457)	CeLoss 0.1816 (0.3396)	SegCLSLoss 0.0208 (0.0170)	KLLoss 0.3613 (0.3307)	MaskLoss 0.9865 (0.6822)	MaskBCELoss 0.0430 (0.1231)	MaskDICELoss 0.9434 (0.5591)
Epoch: [4][174/500]	Time  9.888 ( 9.888)	Loss 1.9552 (1.8582)	CeLoss 0.1777 (0.2125)	SegCLSLoss 0.0239 (0.0197)	KLLoss 0.3555 (0.3658)	MaskLoss 0.8653 (0.7995)	MaskBCELoss 0.0296 (0.1436)	MaskDICELoss 0.8356 (0.6558)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 172 is less than current step: 499. Dropping entry: {'train/loss': 1.7456920862197876, 'train/ce_loss': 0.33955078125, 'train/seg_cls_loss': 0.0169921875, 'train/kl_loss': 0.3306640625, 'train/mask_bce_loss': 0.12306771632283926, 'train/mask_dice_loss': 0.5591045096516609, 'train/mask_loss': 0.6821722239255905, 'metrics/total_secs_per_batch': 8.617320537567139, 'metrics/data_secs_per_batch': 3.860707235336304, '_timestamp': 1740965566.8489447}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 172 is less than current step: 499. Dropping entry: {'train/lr': 4.040816326530612e-05, '_timestamp': 1740965566.8492184}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 173 is less than current step: 499. Dropping entry: {'train/loss': 1.8582181930541992, 'train/ce_loss': 0.2125, 'train/seg_cls_loss': 0.01973876953125, 'train/kl_loss': 0.3658203125, 'train/mask_bce_loss': 0.14364129728637637, 'train/mask_dice_loss': 0.6558291271328927, 'train/mask_loss': 0.7994704306125641, 'metrics/total_secs_per_batch': 9.887734174728394, 'metrics/data_secs_per_batch': 4.582080435752869, '_timestamp': 1740965576.7366707}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 173 is less than current step: 499. Dropping entry: {'train/lr': 4.028571428571428e-05, '_timestamp': 1740965576.7369545}).
Epoch: [4][175/500]	Time  7.581 ( 7.581)	Loss 2.5023 (1.7951)	CeLoss 0.1846 (0.2625)	SegCLSLoss 0.0231 (0.0181)	KLLoss 0.3789 (0.3301)	MaskLoss 1.1339 (0.7453)	MaskBCELoss 0.1761 (0.1456)	MaskDICELoss 0.9578 (0.5998)
Epoch: [4][176/500]	Time  7.690 ( 7.690)	Loss 2.1446 (1.7353)	CeLoss 0.2500 (0.3864)	SegCLSLoss 0.0143 (0.0148)	KLLoss 0.3750 (0.3338)	MaskLoss 0.9248 (0.6540)	MaskBCELoss 0.0022 (0.1274)	MaskDICELoss 0.9227 (0.5266)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 174 is less than current step: 499. Dropping entry: {'train/loss': 1.7950790524482727, 'train/ce_loss': 0.262451171875, 'train/seg_cls_loss': 0.0180908203125, 'train/kl_loss': 0.330078125, 'train/mask_bce_loss': 0.14557063914835452, 'train/mask_dice_loss': 0.5997716188430786, 'train/mask_loss': 0.7453422635793686, 'metrics/total_secs_per_batch': 7.580657482147217, 'metrics/data_secs_per_batch': 3.601141905784607, '_timestamp': 1740965584.317339}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 174 is less than current step: 499. Dropping entry: {'train/lr': 4.0163265306122443e-05, '_timestamp': 1740965584.3176067}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 175 is less than current step: 499. Dropping entry: {'train/loss': 1.7352615654468537, 'train/ce_loss': 0.38642578125, 'train/seg_cls_loss': 0.014837646484375, 'train/kl_loss': 0.3337890625, 'train/mask_bce_loss': 0.12737440241035075, 'train/mask_dice_loss': 0.5266333401203156, 'train/mask_loss': 0.6540077418088913, 'metrics/total_secs_per_batch': 7.689571142196655, 'metrics/data_secs_per_batch': 3.3329668045043945, '_timestamp': 1740965592.006898}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 175 is less than current step: 499. Dropping entry: {'train/lr': 4.0040816326530607e-05, '_timestamp': 1740965592.0071657}).
Epoch: [4][177/500]	Time  8.907 ( 8.907)	Loss 1.2151 (1.8569)	CeLoss 0.2031 (0.3497)	SegCLSLoss 0.0139 (0.0174)	KLLoss 0.3711 (0.3283)	MaskLoss 0.4840 (0.7328)	MaskBCELoss 0.0266 (0.1333)	MaskDICELoss 0.4574 (0.5994)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 176 is less than current step: 499. Dropping entry: {'train/loss': 1.8569064497947694, 'train/ce_loss': 0.34970703125, 'train/seg_cls_loss': 0.017364501953125, 'train/kl_loss': 0.3283203125, 'train/mask_bce_loss': 0.13330814763903617, 'train/mask_dice_loss': 0.5994419455528259, 'train/mask_loss': 0.732750090956688, 'metrics/total_secs_per_batch': 8.906741380691528, 'metrics/data_secs_per_batch': 4.155475306510925, '_timestamp': 1740965600.9138348}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 176 is less than current step: 499. Dropping entry: {'train/lr': 3.9918367346938776e-05, '_timestamp': 1740965600.9141624}).
Epoch: [4][178/500]	Time  7.954 ( 7.954)	Loss 2.1511 (1.9382)	CeLoss 0.2412 (0.3170)	SegCLSLoss 0.0188 (0.0176)	KLLoss 0.3574 (0.3295)	MaskLoss 0.9330 (0.7897)	MaskBCELoss 0.0433 (0.1388)	MaskDICELoss 0.8896 (0.6510)
Epoch: [4][179/500]	Time  7.635 ( 7.635)	Loss 1.8602 (1.5328)	CeLoss 0.1973 (0.3932)	SegCLSLoss 0.0245 (0.0131)	KLLoss 0.3691 (0.2941)	MaskLoss 0.8066 (0.5518)	MaskBCELoss 0.0299 (0.0895)	MaskDICELoss 0.7767 (0.4624)
[2025-03-02 19:33:44,478] [INFO] [logging.py:96:log_dist] [Rank 0] step=2180, skipped=0, lr=[3.961224489795918e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:33:44,483] [INFO] [timer.py:215:stop] epoch=0/micro_step=21800/global_step=2180, RunningAvgSamplesPerSec=1.4705666427173087, CurrSamplesPerSec=1.2530849078864117, MemAllocated=31.45GB, MaxMemAllocated=37.23GB
Epoch: [4][180/500]	Time  7.982 ( 7.982)	Loss 0.7576 (1.7429)	CeLoss 0.2285 (0.5827)	SegCLSLoss 0.0130 (0.0131)	KLLoss 0.3672 (0.2537)	MaskLoss 0.2431 (0.5641)	MaskBCELoss 0.0202 (0.0342)	MaskDICELoss 0.2228 (0.5299)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 177 is less than current step: 499. Dropping entry: {'train/loss': 1.9382367610931397, 'train/ce_loss': 0.3169921875, 'train/seg_cls_loss': 0.0175537109375, 'train/kl_loss': 0.3294921875, 'train/mask_bce_loss': 0.13875035364180804, 'train/mask_dice_loss': 0.6509734869003296, 'train/mask_loss': 0.7897238492965698, 'metrics/total_secs_per_batch': 7.953606128692627, 'metrics/data_secs_per_batch': 3.833073306083679, '_timestamp': 1740965608.867211}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 177 is less than current step: 499. Dropping entry: {'train/lr': 3.979591836734694e-05, '_timestamp': 1740965608.867469}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 178 is less than current step: 499. Dropping entry: {'train/loss': 1.5328496813774108, 'train/ce_loss': 0.3931640625, 'train/seg_cls_loss': 0.013067626953125, 'train/kl_loss': 0.294140625, 'train/mask_bce_loss': 0.08945025424472988, 'train/mask_dice_loss': 0.46237497702240943, 'train/mask_loss': 0.5518252313137054, 'metrics/total_secs_per_batch': 7.635362386703491, 'metrics/data_secs_per_batch': 3.7751461982727053, '_timestamp': 1740965616.502632}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 178 is less than current step: 499. Dropping entry: {'train/lr': 3.96734693877551e-05, '_timestamp': 1740965616.502889}).
Epoch: [4][181/500]	Time  8.891 ( 8.891)	Loss 1.9413 (1.8028)	CeLoss 0.2852 (0.2275)	SegCLSLoss 0.0114 (0.0173)	KLLoss 0.3711 (0.3660)	MaskLoss 0.8066 (0.7651)	MaskBCELoss 0.1748 (0.1536)	MaskDICELoss 0.6318 (0.6115)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 179 is less than current step: 499. Dropping entry: {'train/loss': 1.742864626646042, 'train/ce_loss': 0.58271484375, 'train/seg_cls_loss': 0.01307373046875, 'train/kl_loss': 0.2537109375, 'train/mask_bce_loss': 0.03420160906389356, 'train/mask_dice_loss': 0.5298576638102531, 'train/mask_loss': 0.5640592813491822, 'metrics/total_secs_per_batch': 7.981786727905273, 'metrics/data_secs_per_batch': 3.525725746154785, '_timestamp': 1740965624.4842381}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 179 is less than current step: 499. Dropping entry: {'train/lr': 3.955102040816326e-05, '_timestamp': 1740965624.4845028}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 180 is less than current step: 499. Dropping entry: {'train/loss': 1.8027938425540924, 'train/ce_loss': 0.2275390625, 'train/seg_cls_loss': 0.01728515625, 'train/kl_loss': 0.366015625, 'train/mask_bce_loss': 0.15357886098790913, 'train/mask_dice_loss': 0.6114899247884751, 'train/mask_loss': 0.7650687903165817, 'metrics/total_secs_per_batch': 8.890816926956177, 'metrics/data_secs_per_batch': 3.9850386142730714, '_timestamp': 1740965633.3752415}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 180 is less than current step: 499. Dropping entry: {'train/lr': 3.942857142857142e-05, '_timestamp': 1740965633.3755045}).
Epoch: [4][182/500]	Time  5.304 ( 5.304)	Loss 2.4893 (1.2903)	CeLoss 0.2002 (0.7295)	SegCLSLoss 0.0210 (0.0082)	KLLoss 0.3594 (0.1832)	MaskLoss 1.1211 (0.2691)	MaskBCELoss 0.2816 (0.0519)	MaskDICELoss 0.8396 (0.2172)
Epoch: [4][183/500]	Time  7.032 ( 7.032)	Loss 1.2597 (1.5734)	CeLoss 0.2715 (0.6800)	SegCLSLoss 0.0147 (0.0098)	KLLoss 0.3594 (0.1824)	MaskLoss 0.4726 (0.4352)	MaskBCELoss 0.2591 (0.1328)	MaskDICELoss 0.2136 (0.3024)
Epoch: [4][184/500]	Time  7.258 ( 7.258)	Loss 2.0525 (1.6436)	CeLoss 0.1973 (0.1970)	SegCLSLoss 0.0214 (0.0170)	KLLoss 0.3789 (0.3354)	MaskLoss 0.9032 (0.7022)	MaskBCELoss 0.0274 (0.1735)	MaskDICELoss 0.8758 (0.5288)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 181 is less than current step: 499. Dropping entry: {'train/loss': 1.290256491303444, 'train/ce_loss': 0.7294921875, 'train/seg_cls_loss': 0.00816650390625, 'train/kl_loss': 0.183203125, 'train/mask_bce_loss': 0.05191269274801016, 'train/mask_dice_loss': 0.2171901635825634, 'train/mask_loss': 0.26910285726189614, 'metrics/total_secs_per_batch': 5.30422306060791, 'metrics/data_secs_per_batch': 2.416932797431946, '_timestamp': 1740965638.679418}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 181 is less than current step: 499. Dropping entry: {'train/lr': 3.9306122448979585e-05, '_timestamp': 1740965638.6796665}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 182 is less than current step: 499. Dropping entry: {'train/loss': 1.5733976483345031, 'train/ce_loss': 0.67998046875, 'train/seg_cls_loss': 0.00975341796875, 'train/kl_loss': 0.182421875, 'train/mask_bce_loss': 0.13275974579155445, 'train/mask_dice_loss': 0.30242540538311, 'train/mask_loss': 0.43518514931201935, 'metrics/total_secs_per_batch': 7.031574010848999, 'metrics/data_secs_per_batch': 3.7339027881622315, '_timestamp': 1740965645.7110286}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 182 is less than current step: 499. Dropping entry: {'train/lr': 3.9183673469387755e-05, '_timestamp': 1740965645.7112901}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 183 is less than current step: 499. Dropping entry: {'train/loss': 1.6435596466064453, 'train/ce_loss': 0.19697265625, 'train/seg_cls_loss': 0.017034912109375, 'train/kl_loss': 0.3353515625, 'train/mask_bce_loss': 0.1734643008094281, 'train/mask_dice_loss': 0.5287598699331284, 'train/mask_loss': 0.7022241592407227, 'metrics/total_secs_per_batch': 7.257529258728027, 'metrics/data_secs_per_batch': 3.112450194358826, '_timestamp': 1740965652.9685822}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 183 is less than current step: 499. Dropping entry: {'train/lr': 3.906122448979592e-05, '_timestamp': 1740965652.9688427}).
Epoch: [4][185/500]	Time  7.432 ( 7.432)	Loss 2.5968 (1.9519)	CeLoss 0.2314 (0.2829)	SegCLSLoss 0.0195 (0.0178)	KLLoss 0.3828 (0.3309)	MaskLoss 1.1587 (0.8133)	MaskBCELoss 0.3115 (0.1698)	MaskDICELoss 0.8472 (0.6435)
Epoch: [4][186/500]	Time  6.130 ( 6.130)	Loss 1.0312 (1.5371)	CeLoss 1.0312 (0.6819)	SegCLSLoss 0.0000 (0.0069)	KLLoss 0.0000 (0.1830)	MaskLoss 0.0000 (0.4166)	MaskBCELoss 0.0000 (0.0763)	MaskDICELoss 0.0000 (0.3403)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 184 is less than current step: 499. Dropping entry: {'train/loss': 1.9518743336200715, 'train/ce_loss': 0.28291015625, 'train/seg_cls_loss': 0.017828369140625, 'train/kl_loss': 0.330859375, 'train/mask_bce_loss': 0.16981085259467363, 'train/mask_dice_loss': 0.6435286551713943, 'train/mask_loss': 0.8133395165205002, 'metrics/total_secs_per_batch': 7.431671857833862, 'metrics/data_secs_per_batch': 3.1981236696243287, '_timestamp': 1740965660.400422}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 184 is less than current step: 499. Dropping entry: {'train/lr': 3.893877551020408e-05, '_timestamp': 1740965660.4007528}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 185 is less than current step: 499. Dropping entry: {'train/loss': 1.5370967388153076, 'train/ce_loss': 0.68193359375, 'train/seg_cls_loss': 0.006890869140625, 'train/kl_loss': 0.1830078125, 'train/mask_bce_loss': 0.07627223897725344, 'train/mask_dice_loss': 0.340323007106781, 'train/mask_loss': 0.41659524142742155, 'metrics/total_secs_per_batch': 6.130414724349976, 'metrics/data_secs_per_batch': 2.8177910327911375, '_timestamp': 1740965666.5307384}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 185 is less than current step: 499. Dropping entry: {'train/lr': 3.8816326530612244e-05, '_timestamp': 1740965666.5310318}).
Epoch: [4][187/500]	Time 10.105 (10.105)	Loss 2.3064 (1.8509)	CeLoss 0.1904 (0.2300)	SegCLSLoss 0.0275 (0.0173)	KLLoss 0.3613 (0.3668)	MaskLoss 1.0331 (0.7878)	MaskBCELoss 0.2334 (0.1012)	MaskDICELoss 0.7997 (0.6866)
Epoch: [4][188/500]	Time  7.769 ( 7.769)	Loss 1.6953 (1.5846)	CeLoss 1.6953 (0.3822)	SegCLSLoss 0.0000 (0.0167)	KLLoss 0.0000 (0.3307)	MaskLoss 0.0000 (0.5806)	MaskBCELoss 0.0000 (0.1003)	MaskDICELoss 0.0000 (0.4803)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 186 is less than current step: 499. Dropping entry: {'train/loss': 1.8508503556251525, 'train/ce_loss': 0.22998046875, 'train/seg_cls_loss': 0.017327880859375, 'train/kl_loss': 0.366796875, 'train/mask_bce_loss': 0.10121942944824695, 'train/mask_dice_loss': 0.6866080850362778, 'train/mask_loss': 0.7878275066614151, 'metrics/total_secs_per_batch': 10.104886531829834, 'metrics/data_secs_per_batch': 4.960011410713196, '_timestamp': 1740965676.6355531}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 186 is less than current step: 499. Dropping entry: {'train/lr': 3.86938775510204e-05, '_timestamp': 1740965676.6357346}).
Epoch: [4][189/500]	Time  7.240 ( 7.240)	Loss 0.0781 (1.8852)	CeLoss 0.0781 (0.3457)	SegCLSLoss 0.0000 (0.0138)	KLLoss 0.0000 (0.2934)	MaskLoss 0.0000 (0.7517)	MaskBCELoss 0.0000 (0.1823)	MaskDICELoss 0.0000 (0.5694)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 187 is less than current step: 499. Dropping entry: {'train/loss': 1.5845901250839234, 'train/ce_loss': 0.3822265625, 'train/seg_cls_loss': 0.016705322265625, 'train/kl_loss': 0.3306640625, 'train/mask_bce_loss': 0.10031444914638996, 'train/mask_dice_loss': 0.4802618518471718, 'train/mask_loss': 0.5805763125419616, 'metrics/total_secs_per_batch': 7.769390344619751, 'metrics/data_secs_per_batch': 3.7850550413131714, '_timestamp': 1740965684.4049509}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 187 is less than current step: 499. Dropping entry: {'train/lr': 3.857142857142856e-05, '_timestamp': 1740965684.405206}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 188 is less than current step: 499. Dropping entry: {'train/loss': 1.8851521015167236, 'train/ce_loss': 0.345703125, 'train/seg_cls_loss': 0.0137939453125, 'train/kl_loss': 0.293359375, 'train/mask_bce_loss': 0.18230379670858382, 'train/mask_dice_loss': 0.5693542808294296, 'train/mask_loss': 0.7516580820083618, 'metrics/total_secs_per_batch': 7.240058898925781, 'metrics/data_secs_per_batch': 3.44593141078949, '_timestamp': 1740965691.6450055}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 188 is less than current step: 499. Dropping entry: {'train/lr': 3.844897959183673e-05, '_timestamp': 1740965691.6452656}).
[2025-03-02 19:34:59,005] [INFO] [logging.py:96:log_dist] [Rank 0] step=2190, skipped=0, lr=[3.8387755102040815e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:34:59,011] [INFO] [timer.py:215:stop] epoch=0/micro_step=21900/global_step=2190, RunningAvgSamplesPerSec=1.4699229852984559, CurrSamplesPerSec=1.3576620126252767, MemAllocated=31.26GB, MaxMemAllocated=37.23GB
Epoch: [4][190/500]	Time  7.367 ( 7.367)	Loss 1.0479 (1.6638)	CeLoss 0.2080 (0.4882)	SegCLSLoss 0.0159 (0.0133)	KLLoss 0.3711 (0.2594)	MaskLoss 0.3970 (0.5715)	MaskBCELoss 0.1439 (0.1463)	MaskDICELoss 0.2531 (0.4252)
Epoch: [4][191/500]	Time  7.508 ( 7.508)	Loss 1.5356 (1.7742)	CeLoss 0.2734 (0.3231)	SegCLSLoss 0.0110 (0.0153)	KLLoss 0.3672 (0.3365)	MaskLoss 0.6096 (0.7049)	MaskBCELoss 0.0567 (0.1299)	MaskDICELoss 0.5530 (0.5751)
Epoch: [4][192/500]	Time  7.085 ( 7.085)	Loss 1.2969 (1.4230)	CeLoss 1.2969 (0.5051)	SegCLSLoss 0.0000 (0.0106)	KLLoss 0.0000 (0.2236)	MaskLoss 0.0000 (0.4451)	MaskBCELoss 0.0000 (0.0767)	MaskDICELoss 0.0000 (0.3684)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 189 is less than current step: 499. Dropping entry: {'train/loss': 1.6637851119041442, 'train/ce_loss': 0.488232421875, 'train/seg_cls_loss': 0.013262939453125, 'train/kl_loss': 0.259375, 'train/mask_bce_loss': 0.1462998510338366, 'train/mask_dice_loss': 0.4252167120575905, 'train/mask_loss': 0.5715165674686432, 'metrics/total_secs_per_batch': 7.3670830726623535, 'metrics/data_secs_per_batch': 3.452488088607788, '_timestamp': 1740965699.0118968}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 189 is less than current step: 499. Dropping entry: {'train/lr': 3.8326530612244896e-05, '_timestamp': 1740965699.0120747}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 190 is less than current step: 499. Dropping entry: {'train/loss': 1.7742189347743988, 'train/ce_loss': 0.323095703125, 'train/seg_cls_loss': 0.01531982421875, 'train/kl_loss': 0.3365234375, 'train/mask_bce_loss': 0.1298645593225956, 'train/mask_dice_loss': 0.5750671975314617, 'train/mask_loss': 0.7049317494034767, 'metrics/total_secs_per_batch': 7.5077900886535645, 'metrics/data_secs_per_batch': 3.0674276113510133, '_timestamp': 1740965706.519913}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 190 is less than current step: 499. Dropping entry: {'train/lr': 3.820408163265306e-05, '_timestamp': 1740965706.5201886}).
Epoch: [4][193/500]	Time  7.010 ( 7.010)	Loss 0.5524 (1.4474)	CeLoss 0.1699 (0.3979)	SegCLSLoss 0.0291 (0.0158)	KLLoss 0.3711 (0.2943)	MaskLoss 0.1654 (0.5060)	MaskBCELoss 0.1113 (0.0931)	MaskDICELoss 0.0541 (0.4129)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 191 is less than current step: 499. Dropping entry: {'train/loss': 1.4229818105697631, 'train/ce_loss': 0.505078125, 'train/seg_cls_loss': 0.010552978515625, 'train/kl_loss': 0.2236328125, 'train/mask_bce_loss': 0.07674715910106897, 'train/mask_dice_loss': 0.36838632822036743, 'train/mask_loss': 0.44513348639011385, 'metrics/total_secs_per_batch': 7.085318088531494, 'metrics/data_secs_per_batch': 3.3358997821807863, '_timestamp': 1740965713.6051493}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 191 is less than current step: 499. Dropping entry: {'train/lr': 3.808163265306122e-05, '_timestamp': 1740965713.6053543}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 192 is less than current step: 499. Dropping entry: {'train/loss': 1.4474003970623017, 'train/ce_loss': 0.39794921875, 'train/seg_cls_loss': 0.0157958984375, 'train/kl_loss': 0.2943359375, 'train/mask_bce_loss': 0.09314867705106736, 'train/mask_dice_loss': 0.41287574768066404, 'train/mask_loss': 0.5060244172811508, 'metrics/total_secs_per_batch': 7.01012396812439, 'metrics/data_secs_per_batch': 3.276069092750549, '_timestamp': 1740965720.6153195}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 192 is less than current step: 499. Dropping entry: {'train/lr': 3.795918367346938e-05, '_timestamp': 1740965720.6155117}).
Epoch: [4][194/500]	Time  8.052 ( 8.052)	Loss 2.0521 (1.6788)	CeLoss 0.2305 (0.2252)	SegCLSLoss 0.0146 (0.0171)	KLLoss 0.3730 (0.3289)	MaskLoss 0.8884 (0.7060)	MaskBCELoss 0.2195 (0.1187)	MaskDICELoss 0.6689 (0.5873)
Epoch: [4][195/500]	Time 10.002 (10.002)	Loss 1.0156 (1.4403)	CeLoss 1.0156 (0.2788)	SegCLSLoss 0.0000 (0.0118)	KLLoss 0.0000 (0.2570)	MaskLoss 0.0000 (0.5649)	MaskBCELoss 0.0000 (0.1147)	MaskDICELoss 0.0000 (0.4502)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 193 is less than current step: 499. Dropping entry: {'train/loss': 1.6787642598152162, 'train/ce_loss': 0.2251953125, 'train/seg_cls_loss': 0.01705322265625, 'train/kl_loss': 0.32890625, 'train/mask_bce_loss': 0.11873177699744701, 'train/mask_dice_loss': 0.5872519120573998, 'train/mask_loss': 0.7059836938977242, 'metrics/total_secs_per_batch': 8.051962852478027, 'metrics/data_secs_per_batch': 3.722427487373352, '_timestamp': 1740965728.667227}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 193 is less than current step: 499. Dropping entry: {'train/lr': 3.7836734693877555e-05, '_timestamp': 1740965728.6674056}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 194 is less than current step: 499. Dropping entry: {'train/loss': 1.4402557373046876, 'train/ce_loss': 0.2788330078125, 'train/seg_cls_loss': 0.01181640625, 'train/kl_loss': 0.25703125, 'train/mask_bce_loss': 0.11473112739622593, 'train/mask_dice_loss': 0.4501599133014679, 'train/mask_loss': 0.5648910403251648, 'metrics/total_secs_per_batch': 10.001824855804443, 'metrics/data_secs_per_batch': 4.491589593887329, '_timestamp': 1740965738.6691878}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 194 is less than current step: 499. Dropping entry: {'train/lr': 3.771428571428571e-05, '_timestamp': 1740965738.669479}).
Epoch: [4][196/500]	Time  8.026 ( 8.026)	Loss 1.8080 (1.5082)	CeLoss 0.1904 (0.4603)	SegCLSLoss 0.0261 (0.0112)	KLLoss 0.3555 (0.2164)	MaskLoss 0.7848 (0.5103)	MaskBCELoss 0.0234 (0.0596)	MaskDICELoss 0.7614 (0.4507)
Epoch: [4][197/500]	Time  8.132 ( 8.132)	Loss 2.6118 (1.8686)	CeLoss 0.2197 (0.2059)	SegCLSLoss 0.0168 (0.0186)	KLLoss 0.3555 (0.3719)	MaskLoss 1.1741 (0.8079)	MaskBCELoss 0.4522 (0.2627)	MaskDICELoss 0.7218 (0.5452)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 195 is less than current step: 499. Dropping entry: {'train/loss': 1.5082436561584474, 'train/ce_loss': 0.460302734375, 'train/seg_cls_loss': 0.011151123046875, 'train/kl_loss': 0.21640625, 'train/mask_bce_loss': 0.05955713205039501, 'train/mask_dice_loss': 0.4507414668798447, 'train/mask_loss': 0.5102985978126526, 'metrics/total_secs_per_batch': 8.026499271392822, 'metrics/data_secs_per_batch': 3.7844545364379885, '_timestamp': 1740965746.6956558}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 195 is less than current step: 499. Dropping entry: {'train/lr': 3.7591836734693874e-05, '_timestamp': 1740965746.695869}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 196 is less than current step: 499. Dropping entry: {'train/loss': 1.8686136722564697, 'train/ce_loss': 0.205859375, 'train/seg_cls_loss': 0.018621826171875, 'train/kl_loss': 0.371875, 'train/mask_bce_loss': 0.26272937264293433, 'train/mask_dice_loss': 0.5452102646231651, 'train/mask_loss': 0.8079396337270737, 'metrics/total_secs_per_batch': 8.132190704345703, 'metrics/data_secs_per_batch': 3.630242133140564, '_timestamp': 1740965754.8278775}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 196 is less than current step: 499. Dropping entry: {'train/lr': 3.746938775510204e-05, '_timestamp': 1740965754.8281639}).
Epoch: [4][198/500]	Time  8.659 ( 8.659)	Loss 2.5044 (1.7772)	CeLoss 0.1572 (0.4519)	SegCLSLoss 0.0221 (0.0131)	KLLoss 0.3711 (0.2928)	MaskLoss 1.1497 (0.6447)	MaskBCELoss 0.3221 (0.1450)	MaskDICELoss 0.8276 (0.4997)
Epoch: [4][199/500]	Time  6.798 ( 6.798)	Loss 1.2167 (1.8347)	CeLoss 0.2090 (0.4207)	SegCLSLoss 0.0151 (0.0133)	KLLoss 0.3691 (0.2955)	MaskLoss 0.4814 (0.6888)	MaskBCELoss 0.0238 (0.2032)	MaskDICELoss 0.4576 (0.4856)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 197 is less than current step: 499. Dropping entry: {'train/loss': 1.7772143602371215, 'train/ce_loss': 0.45185546875, 'train/seg_cls_loss': 0.01314697265625, 'train/kl_loss': 0.2927734375, 'train/mask_bce_loss': 0.14497794285416604, 'train/mask_dice_loss': 0.4997327566146851, 'train/mask_loss': 0.6447106927633286, 'metrics/total_secs_per_batch': 8.658781051635742, 'metrics/data_secs_per_batch': 4.1655843496322635, '_timestamp': 1740965763.4866254}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 197 is less than current step: 499. Dropping entry: {'train/lr': 3.73469387755102e-05, '_timestamp': 1740965763.486893}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 198 is less than current step: 499. Dropping entry: {'train/loss': 1.834716272354126, 'train/ce_loss': 0.420703125, 'train/seg_cls_loss': 0.01334228515625, 'train/kl_loss': 0.2955078125, 'train/mask_bce_loss': 0.203240581555292, 'train/mask_dice_loss': 0.48555309176445005, 'train/mask_loss': 0.6887936770915986, 'metrics/total_secs_per_batch': 6.798494338989258, 'metrics/data_secs_per_batch': 2.4232067346572874, '_timestamp': 1740965770.2850928}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 198 is less than current step: 499. Dropping entry: {'train/lr': 3.7224489795918364e-05, '_timestamp': 1740965770.2853568}).
[2025-03-02 19:36:18,166] [INFO] [logging.py:96:log_dist] [Rank 0] step=2200, skipped=0, lr=[3.7163265306122445e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:36:18,172] [INFO] [timer.py:215:stop] epoch=0/micro_step=22000/global_step=2200, RunningAvgSamplesPerSec=1.4688307655497401, CurrSamplesPerSec=1.267956311097408, MemAllocated=30.94GB, MaxMemAllocated=37.23GB
Epoch: [4][200/500]	Time  7.888 ( 7.888)	Loss 1.7163 (1.7855)	CeLoss 0.2734 (0.3975)	SegCLSLoss 0.0151 (0.0156)	KLLoss 0.3770 (0.2961)	MaskLoss 0.6990 (0.6754)	MaskBCELoss 0.0020 (0.1168)	MaskDICELoss 0.6969 (0.5586)
Epoch: [4][201/500]	Time  9.424 ( 9.424)	Loss 0.0845 (1.9307)	CeLoss 0.0845 (0.2131)	SegCLSLoss 0.0000 (0.0146)	KLLoss 0.0000 (0.3318)	MaskLoss 0.0000 (0.8385)	MaskBCELoss 0.0000 (0.2445)	MaskDICELoss 0.0000 (0.5941)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 199 is less than current step: 499. Dropping entry: {'train/loss': 1.7854550361633301, 'train/ce_loss': 0.3974609375, 'train/seg_cls_loss': 0.01558837890625, 'train/kl_loss': 0.29609375, 'train/mask_bce_loss': 0.11683794967830181, 'train/mask_dice_loss': 0.558555568754673, 'train/mask_loss': 0.6753935217857361, 'metrics/total_secs_per_batch': 7.888166904449463, 'metrics/data_secs_per_batch': 3.504773736000061, '_timestamp': 1740965778.1730895}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 199 is less than current step: 499. Dropping entry: {'train/lr': 3.710204081632653e-05, '_timestamp': 1740965778.173288}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 200 is less than current step: 499. Dropping entry: {'train/loss': 1.9306872189044952, 'train/ce_loss': 0.213134765625, 'train/seg_cls_loss': 0.01456298828125, 'train/kl_loss': 0.3318359375, 'train/mask_bce_loss': 0.24445532709360124, 'train/mask_dice_loss': 0.5940572261810303, 'train/mask_loss': 0.8385125547647476, 'metrics/total_secs_per_batch': 9.423882961273193, 'metrics/data_secs_per_batch': 4.239369344711304, '_timestamp': 1740965787.5971618}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 200 is less than current step: 499. Dropping entry: {'train/lr': 3.697959183673469e-05, '_timestamp': 1740965787.5974302}).
Epoch: [4][202/500]	Time  7.113 ( 7.113)	Loss 1.8289 (1.7109)	CeLoss 0.1768 (0.2670)	SegCLSLoss 0.0240 (0.0138)	KLLoss 0.3711 (0.2953)	MaskLoss 0.8017 (0.7038)	MaskBCELoss 0.0541 (0.1162)	MaskDICELoss 0.7475 (0.5876)
Epoch: [4][203/500]	Time  8.006 ( 8.006)	Loss 1.9687 (2.0423)	CeLoss 0.1992 (0.3595)	SegCLSLoss 0.0194 (0.0168)	KLLoss 0.3652 (0.3299)	MaskLoss 0.8613 (0.8206)	MaskBCELoss 0.1275 (0.2084)	MaskDICELoss 0.7338 (0.6122)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 201 is less than current step: 499. Dropping entry: {'train/loss': 1.7109085321426392, 'train/ce_loss': 0.2669921875, 'train/seg_cls_loss': 0.01378173828125, 'train/kl_loss': 0.2953125, 'train/mask_bce_loss': 0.11618253365159034, 'train/mask_dice_loss': 0.5876115769147873, 'train/mask_loss': 0.7037941098213196, 'metrics/total_secs_per_batch': 7.113055229187012, 'metrics/data_secs_per_batch': 3.153727626800537, '_timestamp': 1740965794.7102177}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 201 is less than current step: 499. Dropping entry: {'train/lr': 3.685714285714285e-05, '_timestamp': 1740965794.7104049}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 202 is less than current step: 499. Dropping entry: {'train/loss': 2.0422799706459047, 'train/ce_loss': 0.35947265625, 'train/seg_cls_loss': 0.016839599609375, 'train/kl_loss': 0.3298828125, 'train/mask_bce_loss': 0.20844956301152706, 'train/mask_dice_loss': 0.612153323739767, 'train/mask_loss': 0.8206028759479522, 'metrics/total_secs_per_batch': 8.006200551986694, 'metrics/data_secs_per_batch': 3.696906232833862, '_timestamp': 1740965802.7164822}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 202 is less than current step: 499. Dropping entry: {'train/lr': 3.6734693877551016e-05, '_timestamp': 1740965802.7168243}).
Epoch: [4][204/500]	Time  8.676 ( 8.676)	Loss 2.4356 (1.8852)	CeLoss 0.2520 (0.4700)	SegCLSLoss 0.0117 (0.0123)	KLLoss 0.3711 (0.2559)	MaskLoss 1.0703 (0.6917)	MaskBCELoss 0.0752 (0.0611)	MaskDICELoss 0.9951 (0.6307)
Epoch: [4][205/500]	Time  7.780 ( 7.780)	Loss 1.1065 (1.6806)	CeLoss 0.2852 (0.3723)	SegCLSLoss 0.0128 (0.0135)	KLLoss 0.3691 (0.2953)	MaskLoss 0.3892 (0.6361)	MaskBCELoss 0.2216 (0.1245)	MaskDICELoss 0.1676 (0.5116)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 203 is less than current step: 499. Dropping entry: {'train/loss': 1.8852347373962401, 'train/ce_loss': 0.47001953125, 'train/seg_cls_loss': 0.012298583984375, 'train/kl_loss': 0.255859375, 'train/mask_bce_loss': 0.0610770083963871, 'train/mask_dice_loss': 0.6306614577770233, 'train/mask_loss': 0.6917384743690491, 'metrics/total_secs_per_batch': 8.675812005996704, 'metrics/data_secs_per_batch': 4.092176532745361, '_timestamp': 1740965811.3922281}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 203 is less than current step: 499. Dropping entry: {'train/lr': 3.6612244897959186e-05, '_timestamp': 1740965811.3924136}).
Epoch: [4][206/500]	Time  9.657 ( 9.657)	Loss 2.3386 (2.0962)	CeLoss 0.2344 (0.2217)	SegCLSLoss 0.0238 (0.0191)	KLLoss 0.3789 (0.3672)	MaskLoss 1.0277 (0.9140)	MaskBCELoss 0.0324 (0.1504)	MaskDICELoss 0.9953 (0.7636)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 204 is less than current step: 499. Dropping entry: {'train/loss': 1.6806034326553345, 'train/ce_loss': 0.372314453125, 'train/seg_cls_loss': 0.0135009765625, 'train/kl_loss': 0.2953125, 'train/mask_bce_loss': 0.12448007985949516, 'train/mask_dice_loss': 0.5115980044007301, 'train/mask_loss': 0.6360780894756317, 'metrics/total_secs_per_batch': 7.780006170272827, 'metrics/data_secs_per_batch': 3.504694104194641, '_timestamp': 1740965819.172332}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 204 is less than current step: 499. Dropping entry: {'train/lr': 3.648979591836734e-05, '_timestamp': 1740965819.1725314}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 205 is less than current step: 499. Dropping entry: {'train/loss': 2.096200633049011, 'train/ce_loss': 0.2216796875, 'train/seg_cls_loss': 0.0190673828125, 'train/kl_loss': 0.3671875, 'train/mask_bce_loss': 0.15041176611557602, 'train/mask_dice_loss': 0.763606509566307, 'train/mask_loss': 0.9140182793140411, 'metrics/total_secs_per_batch': 9.657376527786255, 'metrics/data_secs_per_batch': 4.077189636230469, '_timestamp': 1740965828.8297665}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 205 is less than current step: 499. Dropping entry: {'train/lr': 3.6367346938775505e-05, '_timestamp': 1740965828.830075}).
Epoch: [4][207/500]	Time  7.534 ( 7.534)	Loss 1.4453 (1.8639)	CeLoss 1.4453 (0.4195)	SegCLSLoss 0.0000 (0.0145)	KLLoss 0.0000 (0.2963)	MaskLoss 0.0000 (0.7037)	MaskBCELoss 0.0000 (0.1794)	MaskDICELoss 0.0000 (0.5243)
Epoch: [4][208/500]	Time  7.510 ( 7.510)	Loss 2.1556 (1.8423)	CeLoss 0.1758 (0.3652)	SegCLSLoss 0.0205 (0.0148)	KLLoss 0.3633 (0.2951)	MaskLoss 0.9670 (0.7200)	MaskBCELoss 0.0657 (0.1764)	MaskDICELoss 0.9013 (0.5436)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 206 is less than current step: 499. Dropping entry: {'train/loss': 1.8638884902000428, 'train/ce_loss': 0.41953125, 'train/seg_cls_loss': 0.014532470703125, 'train/kl_loss': 0.2962890625, 'train/mask_bce_loss': 0.17942074909806252, 'train/mask_dice_loss': 0.524252025783062, 'train/mask_loss': 0.7036727607250214, 'metrics/total_secs_per_batch': 7.533836603164673, 'metrics/data_secs_per_batch': 3.236397051811218, '_timestamp': 1740965836.3634584}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 206 is less than current step: 499. Dropping entry: {'train/lr': 3.6244897959183675e-05, '_timestamp': 1740965836.3636408}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 207 is less than current step: 499. Dropping entry: {'train/loss': 1.8422812342643737, 'train/ce_loss': 0.365234375, 'train/seg_cls_loss': 0.014788818359375, 'train/kl_loss': 0.2951171875, 'train/mask_bce_loss': 0.17639314737170936, 'train/mask_dice_loss': 0.543624421954155, 'train/mask_loss': 0.7200175642967224, 'metrics/total_secs_per_batch': 7.5095298290252686, 'metrics/data_secs_per_batch': 3.3949459314346315, '_timestamp': 1740965843.8730187}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 207 is less than current step: 499. Dropping entry: {'train/lr': 3.612244897959183e-05, '_timestamp': 1740965843.873367}).
Epoch: [4][209/500]	Time  7.780 ( 7.780)	Loss 2.2454 (1.3447)	CeLoss 0.2637 (0.3385)	SegCLSLoss 0.0132 (0.0107)	KLLoss 0.3770 (0.1863)	MaskLoss 0.9694 (0.4913)	MaskBCELoss 0.1832 (0.0959)	MaskDICELoss 0.7862 (0.3953)
[2025-03-02 19:37:37,785] [INFO] [logging.py:96:log_dist] [Rank 0] step=2210, skipped=0, lr=[3.5938775510204076e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:37:37,790] [INFO] [timer.py:215:stop] epoch=0/micro_step=22100/global_step=2210, RunningAvgSamplesPerSec=1.467705538142663, CurrSamplesPerSec=1.6296731638666022, MemAllocated=30.7GB, MaxMemAllocated=37.23GB
Epoch: [4][210/500]	Time  6.138 ( 6.138)	Loss 1.0625 (1.4950)	CeLoss 1.0625 (0.5555)	SegCLSLoss 0.0000 (0.0127)	KLLoss 0.0000 (0.2562)	MaskLoss 0.0000 (0.4536)	MaskBCELoss 0.0000 (0.1189)	MaskDICELoss 0.0000 (0.3346)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 208 is less than current step: 499. Dropping entry: {'train/loss': 1.344718062877655, 'train/ce_loss': 0.3384765625, 'train/seg_cls_loss': 0.010693359375, 'train/kl_loss': 0.186328125, 'train/mask_bce_loss': 0.09591475799679756, 'train/mask_dice_loss': 0.39534075260162355, 'train/mask_loss': 0.49125551581382754, 'metrics/total_secs_per_batch': 7.780412673950195, 'metrics/data_secs_per_batch': 3.653809404373169, '_timestamp': 1740965851.6534846}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 208 is less than current step: 499. Dropping entry: {'train/lr': 3.5999999999999994e-05, '_timestamp': 1740965851.6537569}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 209 is less than current step: 499. Dropping entry: {'train/loss': 1.4949615955352784, 'train/ce_loss': 0.55546875, 'train/seg_cls_loss': 0.012725830078125, 'train/kl_loss': 0.25625, 'train/mask_bce_loss': 0.11894813105463982, 'train/mask_dice_loss': 0.33463617861270906, 'train/mask_loss': 0.4535843074321747, 'metrics/total_secs_per_batch': 6.137889862060547, 'metrics/data_secs_per_batch': 2.641949677467346, '_timestamp': 1740965857.791137}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 209 is less than current step: 499. Dropping entry: {'train/lr': 3.5877551020408164e-05, '_timestamp': 1740965857.7914164}).
Epoch: [4][211/500]	Time  8.796 ( 8.796)	Loss 1.8533 (2.0837)	CeLoss 0.2217 (0.2551)	SegCLSLoss 0.0160 (0.0152)	KLLoss 0.3633 (0.2588)	MaskLoss 0.7938 (0.8976)	MaskBCELoss 0.0171 (0.4369)	MaskDICELoss 0.7767 (0.4607)
Epoch: [4][212/500]	Time  9.157 ( 9.157)	Loss 1.3988 (1.6596)	CeLoss 0.3145 (0.2449)	SegCLSLoss 0.0112 (0.0143)	KLLoss 0.3711 (0.3676)	MaskLoss 0.5207 (0.6854)	MaskBCELoss 0.1137 (0.1473)	MaskDICELoss 0.4070 (0.5381)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 210 is less than current step: 499. Dropping entry: {'train/loss': 2.0836769580841064, 'train/ce_loss': 0.255078125, 'train/seg_cls_loss': 0.015167236328125, 'train/kl_loss': 0.2587890625, 'train/mask_bce_loss': 0.4368543727323413, 'train/mask_dice_loss': 0.46069699674844744, 'train/mask_loss': 0.897551366686821, 'metrics/total_secs_per_batch': 8.796116828918457, 'metrics/data_secs_per_batch': 4.152505016326904, '_timestamp': 1740965866.5874395}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 210 is less than current step: 499. Dropping entry: {'train/lr': 3.575510204081632e-05, '_timestamp': 1740965866.5876322}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 211 is less than current step: 499. Dropping entry: {'train/loss': 1.6596111416816712, 'train/ce_loss': 0.244921875, 'train/seg_cls_loss': 0.01427001953125, 'train/kl_loss': 0.367578125, 'train/mask_bce_loss': 0.14728454854339362, 'train/mask_dice_loss': 0.5381362497806549, 'train/mask_loss': 0.6854208022356033, 'metrics/total_secs_per_batch': 9.15722107887268, 'metrics/data_secs_per_batch': 3.9758424282073976, '_timestamp': 1740965875.744902}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 211 is less than current step: 499. Dropping entry: {'train/lr': 3.563265306122448e-05, '_timestamp': 1740965875.745274}).
Epoch: [4][213/500]	Time  7.287 ( 7.287)	Loss 1.2109 (1.6036)	CeLoss 1.2109 (0.7644)	SegCLSLoss 0.0000 (0.0078)	KLLoss 0.0000 (0.1820)	MaskLoss 0.0000 (0.4085)	MaskBCELoss 0.0000 (0.0593)	MaskDICELoss 0.0000 (0.3492)
Epoch: [4][214/500]	Time  6.913 ( 6.913)	Loss 1.9725 (1.5093)	CeLoss 0.2158 (0.4614)	SegCLSLoss 0.0159 (0.0132)	KLLoss 0.3613 (0.2564)	MaskLoss 0.8564 (0.5078)	MaskBCELoss 0.0673 (0.0556)	MaskDICELoss 0.7890 (0.4523)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 212 is less than current step: 499. Dropping entry: {'train/loss': 1.60357768535614, 'train/ce_loss': 0.76435546875, 'train/seg_cls_loss': 0.00784912109375, 'train/kl_loss': 0.18203125, 'train/mask_bce_loss': 0.05930988416075707, 'train/mask_dice_loss': 0.3491684108972549, 'train/mask_loss': 0.40847829878330233, 'metrics/total_secs_per_batch': 7.287364482879639, 'metrics/data_secs_per_batch': 3.3260941743850707, '_timestamp': 1740965883.0321462}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 212 is less than current step: 499. Dropping entry: {'train/lr': 3.551020408163265e-05, '_timestamp': 1740965883.03237}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 213 is less than current step: 499. Dropping entry: {'train/loss': 1.5093115150928498, 'train/ce_loss': 0.461376953125, 'train/seg_cls_loss': 0.013165283203125, 'train/kl_loss': 0.2564453125, 'train/mask_bce_loss': 0.05556312976405024, 'train/mask_dice_loss': 0.4522664546966553, 'train/mask_loss': 0.5078295812010765, 'metrics/total_secs_per_batch': 6.912988901138306, 'metrics/data_secs_per_batch': 2.7673229455947874, '_timestamp': 1740965889.9450037}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 213 is less than current step: 499. Dropping entry: {'train/lr': 3.5387755102040816e-05, '_timestamp': 1740965889.9452639}).
Epoch: [4][215/500]	Time  8.756 ( 8.756)	Loss 0.1455 (1.9676)	CeLoss 0.1455 (0.3018)	SegCLSLoss 0.0000 (0.0124)	KLLoss 0.0000 (0.2982)	MaskLoss 0.0000 (0.8149)	MaskBCELoss 0.0000 (0.2879)	MaskDICELoss 0.0000 (0.5270)
Epoch: [4][216/500]	Time  7.313 ( 7.313)	Loss 0.8809 (1.4386)	CeLoss 0.2793 (0.4828)	SegCLSLoss 0.0113 (0.0105)	KLLoss 0.3652 (0.2920)	MaskLoss 0.2793 (0.4605)	MaskBCELoss 0.0920 (0.1295)	MaskDICELoss 0.1873 (0.3310)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 214 is less than current step: 499. Dropping entry: {'train/loss': 1.9676042735576629, 'train/ce_loss': 0.3017578125, 'train/seg_cls_loss': 0.012353515625, 'train/kl_loss': 0.2982421875, 'train/mask_bce_loss': 0.2879318557679653, 'train/mask_dice_loss': 0.5269737973809242, 'train/mask_loss': 0.814905658364296, 'metrics/total_secs_per_batch': 8.755676746368408, 'metrics/data_secs_per_batch': 3.9013078689575194, '_timestamp': 1740965898.7006931}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 214 is less than current step: 499. Dropping entry: {'train/lr': 3.526530612244897e-05, '_timestamp': 1740965898.7008832}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 215 is less than current step: 499. Dropping entry: {'train/loss': 1.4386359214782716, 'train/ce_loss': 0.4828125, 'train/seg_cls_loss': 0.01048583984375, 'train/kl_loss': 0.2919921875, 'train/mask_bce_loss': 0.12948948070406913, 'train/mask_dice_loss': 0.3310394212603569, 'train/mask_loss': 0.4605288997292519, 'metrics/total_secs_per_batch': 7.313311815261841, 'metrics/data_secs_per_batch': 3.6235397815704347, '_timestamp': 1740965906.0142612}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 215 is less than current step: 499. Dropping entry: {'train/lr': 3.514285714285714e-05, '_timestamp': 1740965906.0146184}).
Epoch: [4][217/500]	Time  9.823 ( 9.823)	Loss 0.0664 (1.5822)	CeLoss 0.0664 (0.2007)	SegCLSLoss 0.0000 (0.0132)	KLLoss 0.0000 (0.2955)	MaskLoss 0.0000 (0.6727)	MaskBCELoss 0.0000 (0.1070)	MaskDICELoss 0.0000 (0.5657)
Epoch: [4][218/500]	Time  7.300 ( 7.300)	Loss 1.7542 (2.0014)	CeLoss 0.2061 (0.6564)	SegCLSLoss 0.0179 (0.0116)	KLLoss 0.3691 (0.2244)	MaskLoss 0.7511 (0.6583)	MaskBCELoss 0.0398 (0.1800)	MaskDICELoss 0.7113 (0.4783)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 216 is less than current step: 499. Dropping entry: {'train/loss': 1.5821918725967408, 'train/ce_loss': 0.20068359375, 'train/seg_cls_loss': 0.01322021484375, 'train/kl_loss': 0.2955078125, 'train/mask_bce_loss': 0.10704514654353262, 'train/mask_dice_loss': 0.565691402554512, 'train/mask_loss': 0.6727365612983703, 'metrics/total_secs_per_batch': 9.823343992233276, 'metrics/data_secs_per_batch': 4.483640193939209, '_timestamp': 1740965915.837338}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 216 is less than current step: 499. Dropping entry: {'train/lr': 3.5020408163265305e-05, '_timestamp': 1740965915.8375216}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 217 is less than current step: 499. Dropping entry: {'train/loss': 2.0013508439064025, 'train/ce_loss': 0.6564453125, 'train/seg_cls_loss': 0.0115966796875, 'train/kl_loss': 0.2244140625, 'train/mask_bce_loss': 0.18003516420722007, 'train/mask_dice_loss': 0.478257429599762, 'train/mask_loss': 0.6582925975322723, 'metrics/total_secs_per_batch': 7.299818992614746, 'metrics/data_secs_per_batch': 3.062778115272522, '_timestamp': 1740965923.1371727}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 217 is less than current step: 499. Dropping entry: {'train/lr': 3.489795918367346e-05, '_timestamp': 1740965923.1374688}).
Epoch: [4][219/500]	Time  6.573 ( 6.573)	Loss 1.4141 (1.5447)	CeLoss 1.4141 (0.4756)	SegCLSLoss 0.0000 (0.0083)	KLLoss 0.0000 (0.2219)	MaskLoss 0.0000 (0.5212)	MaskBCELoss 0.0000 (0.1206)	MaskDICELoss 0.0000 (0.4006)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 218 is less than current step: 499. Dropping entry: {'train/loss': 1.5446901261806487, 'train/ce_loss': 0.4755859375, 'train/seg_cls_loss': 0.00831298828125, 'train/kl_loss': 0.221875, 'train/mask_bce_loss': 0.12060398785397411, 'train/mask_dice_loss': 0.400618015229702, 'train/mask_loss': 0.5212220162153244, 'metrics/total_secs_per_batch': 6.572648286819458, 'metrics/data_secs_per_batch': 3.655936050415039, '_timestamp': 1740965929.7098129}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 218 is less than current step: 499. Dropping entry: {'train/lr': 3.477551020408163e-05, '_timestamp': 1740965929.710008}).
[2025-03-02 19:38:59,777] [INFO] [logging.py:96:log_dist] [Rank 0] step=2220, skipped=0, lr=[3.471428571428571e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:38:59,784] [INFO] [timer.py:215:stop] epoch=0/micro_step=22200/global_step=2220, RunningAvgSamplesPerSec=1.466361855315944, CurrSamplesPerSec=0.9927107320180846, MemAllocated=31.11GB, MaxMemAllocated=37.23GB
Epoch: [4][220/500]	Time 10.075 (10.075)	Loss 2.2715 (1.7237)	CeLoss 0.1914 (0.2972)	SegCLSLoss 0.0199 (0.0151)	KLLoss 0.3535 (0.2926)	MaskLoss 1.0176 (0.6948)	MaskBCELoss 0.0206 (0.0686)	MaskDICELoss 0.9970 (0.6262)
Epoch: [4][221/500]	Time  7.514 ( 7.514)	Loss 1.4579 (1.3891)	CeLoss 0.2754 (0.4330)	SegCLSLoss 0.0115 (0.0132)	KLLoss 0.3633 (0.2957)	MaskLoss 0.5698 (0.4599)	MaskBCELoss 0.0814 (0.0466)	MaskDICELoss 0.4884 (0.4133)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 219 is less than current step: 499. Dropping entry: {'train/loss': 1.723690676689148, 'train/ce_loss': 0.297216796875, 'train/seg_cls_loss': 0.01513671875, 'train/kl_loss': 0.292578125, 'train/mask_bce_loss': 0.06859746100381017, 'train/mask_dice_loss': 0.6261824429035187, 'train/mask_loss': 0.694779908657074, 'metrics/total_secs_per_batch': 10.07488203048706, 'metrics/data_secs_per_batch': 4.320008754730225, '_timestamp': 1740965939.7845871}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 219 is less than current step: 499. Dropping entry: {'train/lr': 3.4653061224489795e-05, '_timestamp': 1740965939.78479}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 220 is less than current step: 499. Dropping entry: {'train/loss': 1.3891275584697724, 'train/ce_loss': 0.4330078125, 'train/seg_cls_loss': 0.013189697265625, 'train/kl_loss': 0.295703125, 'train/mask_bce_loss': 0.046577386930584906, 'train/mask_dice_loss': 0.41331840604543685, 'train/mask_loss': 0.45989578664302827, 'metrics/total_secs_per_batch': 7.5140814781188965, 'metrics/data_secs_per_batch': 3.090352416038513, '_timestamp': 1740965947.298825}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 220 is less than current step: 499. Dropping entry: {'train/lr': 3.453061224489796e-05, '_timestamp': 1740965947.2990975}).
Epoch: [4][222/500]	Time  7.131 ( 7.131)	Loss 1.7891 (1.9079)	CeLoss 1.7891 (0.3683)	SegCLSLoss 0.0000 (0.0166)	KLLoss 0.0000 (0.3314)	MaskLoss 0.0000 (0.7491)	MaskBCELoss 0.0000 (0.2052)	MaskDICELoss 0.0000 (0.5439)
Epoch: [4][223/500]	Time  7.966 ( 7.966)	Loss 1.7833 (1.8315)	CeLoss 0.4160 (0.3366)	SegCLSLoss 0.0136 (0.0148)	KLLoss 0.3711 (0.3332)	MaskLoss 0.6622 (0.7272)	MaskBCELoss 0.0830 (0.1349)	MaskDICELoss 0.5791 (0.5924)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 221 is less than current step: 499. Dropping entry: {'train/loss': 1.9078670859336853, 'train/ce_loss': 0.36826171875, 'train/seg_cls_loss': 0.01661376953125, 'train/kl_loss': 0.3314453125, 'train/mask_bce_loss': 0.2051962949335575, 'train/mask_dice_loss': 0.543903261423111, 'train/mask_loss': 0.7490995585918426, 'metrics/total_secs_per_batch': 7.131017446517944, 'metrics/data_secs_per_batch': 3.4952392578125, '_timestamp': 1740965954.429873}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 221 is less than current step: 499. Dropping entry: {'train/lr': 3.440816326530612e-05, '_timestamp': 1740965954.4300811}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 222 is less than current step: 499. Dropping entry: {'train/loss': 1.8315251827239991, 'train/ce_loss': 0.33662109375, 'train/seg_cls_loss': 0.014764404296875, 'train/kl_loss': 0.333203125, 'train/mask_bce_loss': 0.13487935066223145, 'train/mask_dice_loss': 0.5923578441143036, 'train/mask_loss': 0.7272372022271156, 'metrics/total_secs_per_batch': 7.966390132904053, 'metrics/data_secs_per_batch': 3.4304858922958372, '_timestamp': 1740965962.396204}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 222 is less than current step: 499. Dropping entry: {'train/lr': 3.4285714285714284e-05, '_timestamp': 1740965962.3964636}).
Epoch: [4][224/500]	Time  8.336 ( 8.336)	Loss 2.5468 (2.1212)	CeLoss 0.2119 (0.2152)	SegCLSLoss 0.0199 (0.0189)	KLLoss 0.3906 (0.3715)	MaskLoss 1.1426 (0.9295)	MaskBCELoss 0.2365 (0.2410)	MaskDICELoss 0.9060 (0.6885)
Epoch: [4][225/500]	Time  7.900 ( 7.900)	Loss 1.7161 (1.3850)	CeLoss 0.2539 (0.3998)	SegCLSLoss 0.0107 (0.0118)	KLLoss 0.3691 (0.2543)	MaskLoss 0.7096 (0.4770)	MaskBCELoss 0.2393 (0.0486)	MaskDICELoss 0.4703 (0.4284)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 223 is less than current step: 499. Dropping entry: {'train/loss': 2.1212258100509644, 'train/ce_loss': 0.215234375, 'train/seg_cls_loss': 0.018890380859375, 'train/kl_loss': 0.371484375, 'train/mask_bce_loss': 0.24095643253531307, 'train/mask_dice_loss': 0.6885041475296021, 'train/mask_loss': 0.9294605672359466, 'metrics/total_secs_per_batch': 8.336218118667603, 'metrics/data_secs_per_batch': 3.944028687477112, '_timestamp': 1740965970.732439}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 223 is less than current step: 499. Dropping entry: {'train/lr': 3.416326530612245e-05, '_timestamp': 1740965970.7326314}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 224 is less than current step: 499. Dropping entry: {'train/loss': 1.3849859476089477, 'train/ce_loss': 0.399755859375, 'train/seg_cls_loss': 0.011822509765625, 'train/kl_loss': 0.254296875, 'train/mask_bce_loss': 0.04858599193394184, 'train/mask_dice_loss': 0.4284040480852127, 'train/mask_loss': 0.4769900321960449, 'metrics/total_secs_per_batch': 7.899544715881348, 'metrics/data_secs_per_batch': 3.4467715501785277, '_timestamp': 1740965978.6319811}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 224 is less than current step: 499. Dropping entry: {'train/lr': 3.404081632653061e-05, '_timestamp': 1740965978.6322389}).
Epoch: [4][226/500]	Time  7.465 ( 7.465)	Loss 1.7734 (1.6027)	CeLoss 1.7734 (0.4729)	SegCLSLoss 0.0000 (0.0140)	KLLoss 0.0000 (0.2932)	MaskLoss 0.0000 (0.5468)	MaskBCELoss 0.0000 (0.1548)	MaskDICELoss 0.0000 (0.3920)
Epoch: [4][227/500]	Time  8.355 ( 8.355)	Loss 1.7261 (1.6855)	CeLoss 0.2178 (0.3073)	SegCLSLoss 0.0215 (0.0145)	KLLoss 0.3691 (0.3330)	MaskLoss 0.7302 (0.6686)	MaskBCELoss 0.0480 (0.1176)	MaskDICELoss 0.6822 (0.5510)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 225 is less than current step: 499. Dropping entry: {'train/loss': 1.6026598811149597, 'train/ce_loss': 0.47294921875, 'train/seg_cls_loss': 0.0139892578125, 'train/kl_loss': 0.2931640625, 'train/mask_bce_loss': 0.15477206371724606, 'train/mask_dice_loss': 0.3920168548822403, 'train/mask_loss': 0.5467889249324799, 'metrics/total_secs_per_batch': 7.4652321338653564, 'metrics/data_secs_per_batch': 3.3922593355178834, '_timestamp': 1740965986.0973215}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 225 is less than current step: 499. Dropping entry: {'train/lr': 3.391836734693877e-05, '_timestamp': 1740965986.0975454}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 226 is less than current step: 499. Dropping entry: {'train/loss': 1.6855291962623595, 'train/ce_loss': 0.30732421875, 'train/seg_cls_loss': 0.01453857421875, 'train/kl_loss': 0.3330078125, 'train/mask_bce_loss': 0.11755286129191518, 'train/mask_dice_loss': 0.5510418236255645, 'train/mask_loss': 0.6685946822166443, 'metrics/total_secs_per_batch': 8.354971170425415, 'metrics/data_secs_per_batch': 3.582311987876892, '_timestamp': 1740965994.4521956}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 226 is less than current step: 499. Dropping entry: {'train/lr': 3.3795918367346936e-05, '_timestamp': 1740965994.4524584}).
Epoch: [4][228/500]	Time  9.245 ( 9.245)	Loss 1.8799 (1.6679)	CeLoss 0.1982 (0.4314)	SegCLSLoss 0.0173 (0.0125)	KLLoss 0.3770 (0.2564)	MaskLoss 0.8179 (0.6024)	MaskBCELoss 0.0286 (0.0768)	MaskDICELoss 0.7893 (0.5255)
Epoch: [4][229/500]	Time  8.701 ( 8.701)	Loss 1.4323 (1.5496)	CeLoss 0.2451 (0.2538)	SegCLSLoss 0.0142 (0.0167)	KLLoss 0.3594 (0.3311)	MaskLoss 0.5726 (0.6271)	MaskBCELoss 0.0793 (0.0990)	MaskDICELoss 0.4933 (0.5282)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 227 is less than current step: 499. Dropping entry: {'train/loss': 1.6679276704788208, 'train/ce_loss': 0.4314453125, 'train/seg_cls_loss': 0.012457275390625, 'train/kl_loss': 0.2564453125, 'train/mask_bce_loss': 0.07684678449295461, 'train/mask_dice_loss': 0.5255252540111541, 'train/mask_loss': 0.6023720383644104, 'metrics/total_secs_per_batch': 9.24507737159729, 'metrics/data_secs_per_batch': 3.361585187911987, '_timestamp': 1740966003.69727}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 227 is less than current step: 499. Dropping entry: {'train/lr': 3.36734693877551e-05, '_timestamp': 1740966003.6974657}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 228 is less than current step: 499. Dropping entry: {'train/loss': 1.5496043801307677, 'train/ce_loss': 0.25380859375, 'train/seg_cls_loss': 0.01666259765625, 'train/kl_loss': 0.3310546875, 'train/mask_bce_loss': 0.09896617978811265, 'train/mask_dice_loss': 0.5281797528266907, 'train/mask_loss': 0.6271459400653839, 'metrics/total_secs_per_batch': 8.700823068618774, 'metrics/data_secs_per_batch': 3.719703483581543, '_timestamp': 1740966012.398126}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 228 is less than current step: 499. Dropping entry: {'train/lr': 3.355102040816326e-05, '_timestamp': 1740966012.3983881}).
[2025-03-02 19:40:18,899] [INFO] [logging.py:96:log_dist] [Rank 0] step=2230, skipped=0, lr=[3.3489795918367344e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:40:18,906] [INFO] [timer.py:215:stop] epoch=0/micro_step=22300/global_step=2230, RunningAvgSamplesPerSec=1.4653092734250288, CurrSamplesPerSec=1.5367878851854235, MemAllocated=30.74GB, MaxMemAllocated=37.23GB
Epoch: [4][230/500]	Time  6.509 ( 6.509)	Loss 0.9648 (1.3553)	CeLoss 0.9648 (0.4886)	SegCLSLoss 0.0000 (0.0096)	KLLoss 0.0000 (0.2240)	MaskLoss 0.0000 (0.4196)	MaskBCELoss 0.0000 (0.0728)	MaskDICELoss 0.0000 (0.3469)
Epoch: [4][231/500]	Time  7.967 ( 7.967)	Loss 1.0234 (1.6128)	CeLoss 1.0234 (0.2951)	SegCLSLoss 0.0000 (0.0166)	KLLoss 0.0000 (0.2926)	MaskLoss 0.0000 (0.6401)	MaskBCELoss 0.0000 (0.1472)	MaskDICELoss 0.0000 (0.4929)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 229 is less than current step: 499. Dropping entry: {'train/loss': 1.355307948589325, 'train/ce_loss': 0.48857421875, 'train/seg_cls_loss': 0.009649658203125, 'train/kl_loss': 0.2240234375, 'train/mask_bce_loss': 0.07276410590857267, 'train/mask_dice_loss': 0.3468820467591286, 'train/mask_loss': 0.41964614689350127, 'metrics/total_secs_per_batch': 6.508656024932861, 'metrics/data_secs_per_batch': 2.972094917297363, '_timestamp': 1740966018.906594}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 229 is less than current step: 499. Dropping entry: {'train/lr': 3.3428571428571425e-05, '_timestamp': 1740966018.9068766}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 230 is less than current step: 499. Dropping entry: {'train/loss': 1.6128270864486693, 'train/ce_loss': 0.2951171875, 'train/seg_cls_loss': 0.0166259765625, 'train/kl_loss': 0.292578125, 'train/mask_bce_loss': 0.14716807901859283, 'train/mask_dice_loss': 0.49293686747550963, 'train/mask_loss': 0.6401049554347992, 'metrics/total_secs_per_batch': 7.966728687286377, 'metrics/data_secs_per_batch': 3.4832458019256594, '_timestamp': 1740966026.8735332}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 230 is less than current step: 499. Dropping entry: {'train/lr': 3.330612244897959e-05, '_timestamp': 1740966026.8738298}).
Epoch: [4][232/500]	Time  7.814 ( 7.814)	Loss 1.1316 (1.3433)	CeLoss 0.2520 (0.4132)	SegCLSLoss 0.0097 (0.0099)	KLLoss 0.3711 (0.2232)	MaskLoss 0.4193 (0.4515)	MaskBCELoss 0.0443 (0.0615)	MaskDICELoss 0.3750 (0.3900)
Epoch: [4][233/500]	Time  9.418 ( 9.418)	Loss 1.2891 (1.9710)	CeLoss 1.2891 (0.4602)	SegCLSLoss 0.0000 (0.0140)	KLLoss 0.0000 (0.2920)	MaskLoss 0.0000 (0.7375)	MaskBCELoss 0.0000 (0.1512)	MaskDICELoss 0.0000 (0.5863)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 231 is less than current step: 499. Dropping entry: {'train/loss': 1.3433496713638307, 'train/ce_loss': 0.41318359375, 'train/seg_cls_loss': 0.009893798828125, 'train/kl_loss': 0.2232421875, 'train/mask_bce_loss': 0.06148390043526888, 'train/mask_dice_loss': 0.39004932940006254, 'train/mask_loss': 0.45153322219848635, 'metrics/total_secs_per_batch': 7.814453840255737, 'metrics/data_secs_per_batch': 3.248989129066467, '_timestamp': 1740966034.688155}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 231 is less than current step: 499. Dropping entry: {'train/lr': 3.318367346938775e-05, '_timestamp': 1740966034.6884992}).
Epoch: [4][234/500]	Time  8.032 ( 8.032)	Loss 1.7109 (1.6395)	CeLoss 1.7109 (0.4718)	SegCLSLoss 0.0000 (0.0145)	KLLoss 0.0000 (0.2559)	MaskLoss 0.0000 (0.5673)	MaskBCELoss 0.0000 (0.1104)	MaskDICELoss 0.0000 (0.4569)
Epoch: [4][235/500]	Time  6.158 ( 6.158)	Loss 1.9513 (1.8403)	CeLoss 0.2344 (0.5403)	SegCLSLoss 0.0156 (0.0143)	KLLoss 0.3652 (0.2574)	MaskLoss 0.8360 (0.6334)	MaskBCELoss 0.0827 (0.1002)	MaskDICELoss 0.7533 (0.5332)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 232 is less than current step: 499. Dropping entry: {'train/loss': 1.9710307359695434, 'train/ce_loss': 0.46015625, 'train/seg_cls_loss': 0.014013671875, 'train/kl_loss': 0.2919921875, 'train/mask_bce_loss': 0.15117413448169828, 'train/mask_dice_loss': 0.5862943649291992, 'train/mask_loss': 0.7374684989452363, 'metrics/total_secs_per_batch': 9.418259382247925, 'metrics/data_secs_per_batch': 4.638163948059082, '_timestamp': 1740966044.1061656}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 232 is less than current step: 499. Dropping entry: {'train/lr': 3.3061224489795914e-05, '_timestamp': 1740966044.106424}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 233 is less than current step: 499. Dropping entry: {'train/loss': 1.6395270705223084, 'train/ce_loss': 0.471826171875, 'train/seg_cls_loss': 0.014544677734375, 'train/kl_loss': 0.255859375, 'train/mask_bce_loss': 0.11036481969058513, 'train/mask_dice_loss': 0.4569084823131561, 'train/mask_loss': 0.5672733038663864, 'metrics/total_secs_per_batch': 8.032183408737183, 'metrics/data_secs_per_batch': 3.4594962120056154, '_timestamp': 1740966052.13857}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 233 is less than current step: 499. Dropping entry: {'train/lr': 3.293877551020408e-05, '_timestamp': 1740966052.1389067}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 234 is less than current step: 499. Dropping entry: {'train/loss': 1.8402670860290526, 'train/ce_loss': 0.54033203125, 'train/seg_cls_loss': 0.01429443359375, 'train/kl_loss': 0.257421875, 'train/mask_bce_loss': 0.10018737241625786, 'train/mask_dice_loss': 0.5332274168729783, 'train/mask_loss': 0.6334147751331329, 'metrics/total_secs_per_batch': 6.15827488899231, 'metrics/data_secs_per_batch': 2.2332756757736205, '_timestamp': 1740966058.2966902}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 234 is less than current step: 499. Dropping entry: {'train/lr': 3.281632653061224e-05, '_timestamp': 1740966058.296967}).
Epoch: [4][236/500]	Time  9.270 ( 9.270)	Loss 2.3697 (1.4233)	CeLoss 0.1973 (0.5043)	SegCLSLoss 0.0204 (0.0104)	KLLoss 0.3633 (0.2193)	MaskLoss 1.0633 (0.4459)	MaskBCELoss 0.0669 (0.0746)	MaskDICELoss 0.9964 (0.3713)
Epoch: [4][237/500]	Time  7.034 ( 7.034)	Loss 1.8516 (1.6543)	CeLoss 0.2305 (0.6768)	SegCLSLoss 0.0186 (0.0113)	KLLoss 0.3789 (0.2186)	MaskLoss 0.7871 (0.4751)	MaskBCELoss 0.0167 (0.0396)	MaskDICELoss 0.7704 (0.4354)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 235 is less than current step: 499. Dropping entry: {'train/loss': 1.4233346581459045, 'train/ce_loss': 0.504296875, 'train/seg_cls_loss': 0.010357666015625, 'train/kl_loss': 0.2193359375, 'train/mask_bce_loss': 0.07460175231099128, 'train/mask_dice_loss': 0.3713429167866707, 'train/mask_loss': 0.44594466984272, 'metrics/total_secs_per_batch': 9.270344495773315, 'metrics/data_secs_per_batch': 3.8757437229156495, '_timestamp': 1740966067.5670116}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 235 is less than current step: 499. Dropping entry: {'train/lr': 3.2693877551020403e-05, '_timestamp': 1740966067.5673165}).
Epoch: [4][238/500]	Time  7.697 ( 7.697)	Loss 2.5333 (1.6375)	CeLoss 0.2178 (0.4863)	SegCLSLoss 0.0200 (0.0116)	KLLoss 0.3633 (0.2553)	MaskLoss 1.1348 (0.5597)	MaskBCELoss 0.4021 (0.1605)	MaskDICELoss 0.7327 (0.3992)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 236 is less than current step: 499. Dropping entry: {'train/loss': 1.6542507767677308, 'train/ce_loss': 0.6767578125, 'train/seg_cls_loss': 0.011297607421875, 'train/kl_loss': 0.2185546875, 'train/mask_bce_loss': 0.03964755944907665, 'train/mask_dice_loss': 0.4354270428419113, 'train/mask_loss': 0.4750746011734009, 'metrics/total_secs_per_batch': 7.0339133739471436, 'metrics/data_secs_per_batch': 2.9948822021484376, '_timestamp': 1740966074.6010227}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 236 is less than current step: 499. Dropping entry: {'train/lr': 3.2571428571428566e-05, '_timestamp': 1740966074.601312}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 237 is less than current step: 499. Dropping entry: {'train/loss': 1.637458121776581, 'train/ce_loss': 0.486328125, 'train/seg_cls_loss': 0.01162109375, 'train/kl_loss': 0.2552734375, 'train/mask_bce_loss': 0.16050715800374746, 'train/mask_dice_loss': 0.39923752546310426, 'train/mask_loss': 0.5597446858882904, 'metrics/total_secs_per_batch': 7.697091817855835, 'metrics/data_secs_per_batch': 3.4538779497146606, '_timestamp': 1740966082.298202}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 237 is less than current step: 499. Dropping entry: {'train/lr': 3.2448979591836736e-05, '_timestamp': 1740966082.2985258}).
Epoch: [4][239/500]	Time  8.323 ( 8.323)	Loss 1.6698 (1.7546)	CeLoss 0.2139 (0.4068)	SegCLSLoss 0.0128 (0.0125)	KLLoss 0.3691 (0.2928)	MaskLoss 0.7060 (0.6560)	MaskBCELoss 0.1288 (0.0883)	MaskDICELoss 0.5772 (0.5677)
[2025-03-02 19:41:38,464] [INFO] [logging.py:96:log_dist] [Rank 0] step=2240, skipped=0, lr=[3.2265306122448974e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:41:38,471] [INFO] [timer.py:215:stop] epoch=0/micro_step=22400/global_step=2240, RunningAvgSamplesPerSec=1.4642251905958177, CurrSamplesPerSec=1.2739734694359195, MemAllocated=30.67GB, MaxMemAllocated=37.23GB
Epoch: [4][240/500]	Time  7.851 ( 7.851)	Loss 1.2891 (1.7120)	CeLoss 1.2891 (0.5779)	SegCLSLoss 0.0000 (0.0122)	KLLoss 0.0000 (0.2564)	MaskLoss 0.0000 (0.5510)	MaskBCELoss 0.0000 (0.1156)	MaskDICELoss 0.0000 (0.4354)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 238 is less than current step: 499. Dropping entry: {'train/loss': 1.7546300053596497, 'train/ce_loss': 0.4068359375, 'train/seg_cls_loss': 0.012506103515625, 'train/kl_loss': 0.2927734375, 'train/mask_bce_loss': 0.08833406772464514, 'train/mask_dice_loss': 0.5676918536424637, 'train/mask_loss': 0.6560259282588958, 'metrics/total_secs_per_batch': 8.323059558868408, 'metrics/data_secs_per_batch': 3.572909688949585, '_timestamp': 1740966090.6211088}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 238 is less than current step: 499. Dropping entry: {'train/lr': 3.232653061224489e-05, '_timestamp': 1740966090.6213827}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 239 is less than current step: 499. Dropping entry: {'train/loss': 1.711953580379486, 'train/ce_loss': 0.5779296875, 'train/seg_cls_loss': 0.012249755859375, 'train/kl_loss': 0.2564453125, 'train/mask_bce_loss': 0.11562164761126041, 'train/mask_dice_loss': 0.43537466824054716, 'train/mask_loss': 0.5509963184595108, 'metrics/total_secs_per_batch': 7.850960969924927, 'metrics/data_secs_per_batch': 3.832749581336975, '_timestamp': 1740966098.4718733}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 239 is less than current step: 499. Dropping entry: {'train/lr': 3.2204081632653056e-05, '_timestamp': 1740966098.4721303}).
Epoch: [4][241/500]	Time  7.603 ( 7.603)	Loss 2.1974 (1.5173)	CeLoss 0.2031 (0.3573)	SegCLSLoss 0.0187 (0.0138)	KLLoss 0.3652 (0.2938)	MaskLoss 0.9737 (0.5618)	MaskBCELoss 0.0325 (0.0664)	MaskDICELoss 0.9412 (0.4954)
Epoch: [4][242/500]	Time  6.987 ( 6.987)	Loss 2.0364 (1.7689)	CeLoss 0.1895 (0.4115)	SegCLSLoss 0.0320 (0.0135)	KLLoss 0.3691 (0.2939)	MaskLoss 0.8971 (0.6606)	MaskBCELoss 0.3751 (0.2382)	MaskDICELoss 0.5220 (0.4225)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 240 is less than current step: 499. Dropping entry: {'train/loss': 1.5172640264034272, 'train/ce_loss': 0.35732421875, 'train/seg_cls_loss': 0.013751220703125, 'train/kl_loss': 0.29375, 'train/mask_bce_loss': 0.06639078157022596, 'train/mask_dice_loss': 0.4953662306070328, 'train/mask_loss': 0.5617570102214813, 'metrics/total_secs_per_batch': 7.603074073791504, 'metrics/data_secs_per_batch': 3.14989857673645, '_timestamp': 1740966106.0751302}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 240 is less than current step: 499. Dropping entry: {'train/lr': 3.2081632653061225e-05, '_timestamp': 1740966106.075325}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 241 is less than current step: 499. Dropping entry: {'train/loss': 1.7689148426055907, 'train/ce_loss': 0.4115234375, 'train/seg_cls_loss': 0.0134765625, 'train/kl_loss': 0.2939453125, 'train/mask_bce_loss': 0.23815065082162618, 'train/mask_dice_loss': 0.4224786549806595, 'train/mask_loss': 0.6606292963027954, 'metrics/total_secs_per_batch': 6.986715078353882, 'metrics/data_secs_per_batch': 2.9744956731796264, '_timestamp': 1740966113.0618427}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 241 is less than current step: 499. Dropping entry: {'train/lr': 3.195918367346939e-05, '_timestamp': 1740966113.0621026}).
Epoch: [4][243/500]	Time  7.602 ( 7.602)	Loss 2.2804 (2.0502)	CeLoss 0.1895 (0.4383)	SegCLSLoss 0.0199 (0.0171)	KLLoss 0.3633 (0.2922)	MaskLoss 1.0225 (0.7871)	MaskBCELoss 0.0274 (0.1731)	MaskDICELoss 0.9952 (0.6140)
Epoch: [4][244/500]	Time  9.141 ( 9.141)	Loss 0.5008 (1.7587)	CeLoss 0.3555 (0.3359)	SegCLSLoss 0.0107 (0.0168)	KLLoss 0.3691 (0.3297)	MaskLoss 0.0512 (0.6907)	MaskBCELoss 0.0155 (0.1359)	MaskDICELoss 0.0357 (0.5548)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 242 is less than current step: 499. Dropping entry: {'train/loss': 2.050188958644867, 'train/ce_loss': 0.43828125, 'train/seg_cls_loss': 0.017071533203125, 'train/kl_loss': 0.2921875, 'train/mask_bce_loss': 0.17307696817442775, 'train/mask_dice_loss': 0.6139804065227509, 'train/mask_loss': 0.787057363986969, 'metrics/total_secs_per_batch': 7.602026700973511, 'metrics/data_secs_per_batch': 3.2846380710601806, '_timestamp': 1740966120.663874}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 242 is less than current step: 499. Dropping entry: {'train/lr': 3.1836734693877545e-05, '_timestamp': 1740966120.6640625}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 243 is less than current step: 499. Dropping entry: {'train/loss': 1.7587381958961488, 'train/ce_loss': 0.3359375, 'train/seg_cls_loss': 0.016839599609375, 'train/kl_loss': 0.3296875, 'train/mask_bce_loss': 0.13594033047556878, 'train/mask_dice_loss': 0.5547568760812283, 'train/mask_loss': 0.6906972050666809, 'metrics/total_secs_per_batch': 9.141172647476196, 'metrics/data_secs_per_batch': 4.2009340763092045, '_timestamp': 1740966129.805098}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 243 is less than current step: 499. Dropping entry: {'train/lr': 3.1714285714285715e-05, '_timestamp': 1740966129.8053787}).
Epoch: [4][245/500]	Time  8.649 ( 8.649)	Loss 0.9266 (1.8726)	CeLoss 0.2773 (0.3134)	SegCLSLoss 0.0095 (0.0158)	KLLoss 0.3730 (0.3363)	MaskLoss 0.3041 (0.7590)	MaskBCELoss 0.1114 (0.1364)	MaskDICELoss 0.1927 (0.6226)
Epoch: [4][246/500]	Time  9.055 ( 9.055)	Loss 2.2445 (1.6550)	CeLoss 0.2656 (0.3600)	SegCLSLoss 0.0144 (0.0143)	KLLoss 0.3652 (0.2908)	MaskLoss 0.9670 (0.6293)	MaskBCELoss 0.0179 (0.0711)	MaskDICELoss 0.9491 (0.5582)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 244 is less than current step: 499. Dropping entry: {'train/loss': 1.8726498663425446, 'train/ce_loss': 0.31337890625, 'train/seg_cls_loss': 0.015814208984375, 'train/kl_loss': 0.336328125, 'train/mask_bce_loss': 0.13636028598994016, 'train/mask_dice_loss': 0.622620877623558, 'train/mask_loss': 0.7589811712503434, 'metrics/total_secs_per_batch': 8.64909839630127, 'metrics/data_secs_per_batch': 4.127336239814758, '_timestamp': 1740966138.4542408}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 244 is less than current step: 499. Dropping entry: {'train/lr': 3.159183673469388e-05, '_timestamp': 1740966138.4545603}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 245 is less than current step: 499. Dropping entry: {'train/loss': 1.655036634206772, 'train/ce_loss': 0.3599609375, 'train/seg_cls_loss': 0.014300537109375, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.07107989015057684, 'train/mask_dice_loss': 0.558245088160038, 'train/mask_loss': 0.6293249785900116, 'metrics/total_secs_per_batch': 9.055346250534058, 'metrics/data_secs_per_batch': 3.693822479248047, '_timestamp': 1740966147.5095482}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 245 is less than current step: 499. Dropping entry: {'train/lr': 3.1469387755102034e-05, '_timestamp': 1740966147.5098279}).
Epoch: [4][247/500]	Time  9.204 ( 9.204)	Loss 2.1098 (1.6424)	CeLoss 0.3281 (0.2788)	SegCLSLoss 0.0143 (0.0150)	KLLoss 0.3691 (0.3303)	MaskLoss 0.8684 (0.6616)	MaskBCELoss 0.1644 (0.0869)	MaskDICELoss 0.7039 (0.5747)
Epoch: [4][248/500]	Time  8.416 ( 8.416)	Loss 1.7341 (1.4222)	CeLoss 0.2969 (0.2990)	SegCLSLoss 0.0108 (0.0129)	KLLoss 0.3672 (0.2961)	MaskLoss 0.6971 (0.5437)	MaskBCELoss 0.1424 (0.1265)	MaskDICELoss 0.5547 (0.4172)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 246 is less than current step: 499. Dropping entry: {'train/loss': 1.642449277639389, 'train/ce_loss': 0.27880859375, 'train/seg_cls_loss': 0.014959716796875, 'train/kl_loss': 0.3302734375, 'train/mask_bce_loss': 0.08686263523995877, 'train/mask_dice_loss': 0.5746940411627293, 'train/mask_loss': 0.6615566715598107, 'metrics/total_secs_per_batch': 9.203614950180054, 'metrics/data_secs_per_batch': 4.171724939346314, '_timestamp': 1740966156.7131705}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 246 is less than current step: 499. Dropping entry: {'train/lr': 3.1346938775510204e-05, '_timestamp': 1740966156.7134557}).
Epoch: [4][249/500]	Time  7.906 ( 7.906)	Loss 0.7673 (1.3704)	CeLoss 0.2832 (0.5555)	SegCLSLoss 0.0128 (0.0113)	KLLoss 0.3691 (0.2592)	MaskLoss 0.2205 (0.3917)	MaskBCELoss 0.0987 (0.1194)	MaskDICELoss 0.1219 (0.2723)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 247 is less than current step: 499. Dropping entry: {'train/loss': 1.4221780717372894, 'train/ce_loss': 0.2989990234375, 'train/seg_cls_loss': 0.012884521484375, 'train/kl_loss': 0.29609375, 'train/mask_bce_loss': 0.1264869921375066, 'train/mask_dice_loss': 0.41718260645866395, 'train/mask_loss': 0.5436696022748947, 'metrics/total_secs_per_batch': 8.415897607803345, 'metrics/data_secs_per_batch': 3.8422035455703734, '_timestamp': 1740966165.129029}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 247 is less than current step: 499. Dropping entry: {'train/lr': 3.122448979591837e-05, '_timestamp': 1740966165.129289}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 248 is less than current step: 499. Dropping entry: {'train/loss': 1.3703867435455321, 'train/ce_loss': 0.55546875, 'train/seg_cls_loss': 0.011279296875, 'train/kl_loss': 0.2591796875, 'train/mask_bce_loss': 0.11938916333019733, 'train/mask_dice_loss': 0.27234716787934304, 'train/mask_loss': 0.3917363300919533, 'metrics/total_secs_per_batch': 7.905715227127075, 'metrics/data_secs_per_batch': 3.710333967208862, '_timestamp': 1740966173.0349379}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 248 is less than current step: 499. Dropping entry: {'train/lr': 3.110204081632652e-05, '_timestamp': 1740966173.0352662}).
[2025-03-02 19:43:02,549] [INFO] [logging.py:96:log_dist] [Rank 0] step=2250, skipped=0, lr=[3.104081632653061e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:43:02,555] [INFO] [timer.py:215:stop] epoch=0/micro_step=22500/global_step=2250, RunningAvgSamplesPerSec=1.462722079251839, CurrSamplesPerSec=1.050453107179875, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [4][250/500]	Time  9.521 ( 9.521)	Loss 2.0228 (2.0034)	CeLoss 0.2080 (0.3134)	SegCLSLoss 0.0237 (0.0179)	KLLoss 0.3555 (0.3277)	MaskLoss 0.8835 (0.8241)	MaskBCELoss 0.0755 (0.1470)	MaskDICELoss 0.8079 (0.6771)
Epoch: [4][251/500]	Time  7.930 ( 7.930)	Loss 3.0050 (1.7156)	CeLoss 0.2441 (0.4300)	SegCLSLoss 0.0145 (0.0104)	KLLoss 0.3672 (0.2549)	MaskLoss 1.3580 (0.6274)	MaskBCELoss 0.7924 (0.1602)	MaskDICELoss 0.5656 (0.4672)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 249 is less than current step: 499. Dropping entry: {'train/loss': 2.003371465206146, 'train/ce_loss': 0.31337890625, 'train/seg_cls_loss': 0.01788330078125, 'train/kl_loss': 0.327734375, 'train/mask_bce_loss': 0.14702377934008837, 'train/mask_dice_loss': 0.677122887969017, 'train/mask_loss': 0.8241466641426086, 'metrics/total_secs_per_batch': 9.52144980430603, 'metrics/data_secs_per_batch': 3.964629364013672, '_timestamp': 1740966182.5560145}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 249 is less than current step: 499. Dropping entry: {'train/lr': 3.097959183673469e-05, '_timestamp': 1740966182.5561986}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 250 is less than current step: 499. Dropping entry: {'train/loss': 1.715583038330078, 'train/ce_loss': 0.42998046875, 'train/seg_cls_loss': 0.010418701171875, 'train/kl_loss': 0.2548828125, 'train/mask_bce_loss': 0.16023567207157613, 'train/mask_dice_loss': 0.46718475222587585, 'train/mask_loss': 0.6274204254150391, 'metrics/total_secs_per_batch': 7.930370807647705, 'metrics/data_secs_per_batch': 3.457174563407898, '_timestamp': 1740966190.4867654}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 250 is less than current step: 499. Dropping entry: {'train/lr': 3.0857142857142856e-05, '_timestamp': 1740966190.4871452}).
Epoch: [4][252/500]	Time  8.478 ( 8.478)	Loss 2.4842 (1.7209)	CeLoss 0.1328 (0.3019)	SegCLSLoss 0.0282 (0.0181)	KLLoss 0.3672 (0.2941)	MaskLoss 1.1503 (0.6902)	MaskBCELoss 0.3900 (0.1128)	MaskDICELoss 0.7603 (0.5775)
Epoch: [4][253/500]	Time  6.837 ( 6.837)	Loss 1.1734 (1.5874)	CeLoss 0.2656 (0.4799)	SegCLSLoss 0.0126 (0.0174)	KLLoss 0.3652 (0.2600)	MaskLoss 0.4324 (0.5364)	MaskBCELoss 0.0877 (0.1092)	MaskDICELoss 0.3447 (0.4272)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 251 is less than current step: 499. Dropping entry: {'train/loss': 1.7208799600601197, 'train/ce_loss': 0.30185546875, 'train/seg_cls_loss': 0.01812744140625, 'train/kl_loss': 0.294140625, 'train/mask_bce_loss': 0.11276742238551378, 'train/mask_dice_loss': 0.577457720041275, 'train/mask_loss': 0.6902251362800598, 'metrics/total_secs_per_batch': 8.477715253829956, 'metrics/data_secs_per_batch': 3.1358551740646363, '_timestamp': 1740966198.9642782}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 251 is less than current step: 499. Dropping entry: {'train/lr': 3.073469387755102e-05, '_timestamp': 1740966198.9644642}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 252 is less than current step: 499. Dropping entry: {'train/loss': 1.5873861253261565, 'train/ce_loss': 0.4798828125, 'train/seg_cls_loss': 0.0173828125, 'train/kl_loss': 0.2599609375, 'train/mask_bce_loss': 0.10917884111404419, 'train/mask_dice_loss': 0.42718999832868576, 'train/mask_loss': 0.5363688439130783, 'metrics/total_secs_per_batch': 6.837292432785034, 'metrics/data_secs_per_batch': 3.0676251888275146, '_timestamp': 1740966205.8015878}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 252 is less than current step: 499. Dropping entry: {'train/lr': 3.061224489795918e-05, '_timestamp': 1740966205.8018472}).
Epoch: [4][254/500]	Time  7.644 ( 7.644)	Loss 1.9987 (1.4354)	CeLoss 0.2236 (0.5014)	SegCLSLoss 0.0193 (0.0109)	KLLoss 0.3672 (0.2225)	MaskLoss 0.8646 (0.4534)	MaskBCELoss 0.0093 (0.0757)	MaskDICELoss 0.8553 (0.3777)
Epoch: [4][255/500]	Time  9.744 ( 9.744)	Loss 1.8198 (2.1799)	CeLoss 0.2363 (0.2169)	SegCLSLoss 0.0126 (0.0180)	KLLoss 0.3789 (0.3689)	MaskLoss 0.7703 (0.9585)	MaskBCELoss 0.2659 (0.2625)	MaskDICELoss 0.5044 (0.6960)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 253 is less than current step: 499. Dropping entry: {'train/loss': 1.4354189932346344, 'train/ce_loss': 0.5013671875, 'train/seg_cls_loss': 0.0108642578125, 'train/kl_loss': 0.2224609375, 'train/mask_bce_loss': 0.07568738607224076, 'train/mask_dice_loss': 0.377715477347374, 'train/mask_loss': 0.45340285301208494, 'metrics/total_secs_per_batch': 7.64377498626709, 'metrics/data_secs_per_batch': 3.517504906654358, '_timestamp': 1740966213.4454238}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 253 is less than current step: 499. Dropping entry: {'train/lr': 3.0489795918367345e-05, '_timestamp': 1740966213.4457598}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 254 is less than current step: 499. Dropping entry: {'train/loss': 2.179914379119873, 'train/ce_loss': 0.21689453125, 'train/seg_cls_loss': 0.01800537109375, 'train/kl_loss': 0.3689453125, 'train/mask_bce_loss': 0.26248202063143256, 'train/mask_dice_loss': 0.6960298597812653, 'train/mask_loss': 0.9585118800401687, 'metrics/total_secs_per_batch': 9.743891716003418, 'metrics/data_secs_per_batch': 4.648114347457886, '_timestamp': 1740966223.189396}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 254 is less than current step: 499. Dropping entry: {'train/lr': 3.0367346938775508e-05, '_timestamp': 1740966223.189704}).
Epoch: [4][256/500]	Time  6.922 ( 6.922)	Loss 2.3115 (1.5882)	CeLoss 0.2383 (0.4162)	SegCLSLoss 0.0143 (0.0111)	KLLoss 0.3633 (0.2248)	MaskLoss 1.0151 (0.5721)	MaskBCELoss 0.0410 (0.1274)	MaskDICELoss 0.9741 (0.4447)
Epoch: [4][257/500]	Time  7.546 ( 7.546)	Loss 0.6805 (1.6862)	CeLoss 0.2178 (0.4414)	SegCLSLoss 0.0123 (0.0126)	KLLoss 0.3613 (0.2945)	MaskLoss 0.2104 (0.6046)	MaskBCELoss 0.0832 (0.1029)	MaskDICELoss 0.1272 (0.5017)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 255 is less than current step: 499. Dropping entry: {'train/loss': 1.588230347633362, 'train/ce_loss': 0.416162109375, 'train/seg_cls_loss': 0.01107177734375, 'train/kl_loss': 0.2248046875, 'train/mask_bce_loss': 0.1273910727351904, 'train/mask_dice_loss': 0.44467819929122926, 'train/mask_loss': 0.5720692634582519, 'metrics/total_secs_per_batch': 6.922154664993286, 'metrics/data_secs_per_batch': 2.7900150299072264, '_timestamp': 1740966230.1114206}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 255 is less than current step: 499. Dropping entry: {'train/lr': 3.024489795918367e-05, '_timestamp': 1740966230.1116896}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 256 is less than current step: 499. Dropping entry: {'train/loss': 1.686194860935211, 'train/ce_loss': 0.44140625, 'train/seg_cls_loss': 0.012554931640625, 'train/kl_loss': 0.29453125, 'train/mask_bce_loss': 0.10291602164506912, 'train/mask_dice_loss': 0.5016560181975365, 'train/mask_loss': 0.6045720428228378, 'metrics/total_secs_per_batch': 7.545665502548218, 'metrics/data_secs_per_batch': 3.4298704147338865, '_timestamp': 1740966237.6570706}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 256 is less than current step: 499. Dropping entry: {'train/lr': 3.0122448979591834e-05, '_timestamp': 1740966237.6573353}).
Epoch: [4][258/500]	Time  7.463 ( 7.463)	Loss 1.9864 (1.8380)	CeLoss 0.2734 (0.4319)	SegCLSLoss 0.0130 (0.0101)	KLLoss 0.3730 (0.2588)	MaskLoss 0.8350 (0.6875)	MaskBCELoss 0.0295 (0.2656)	MaskDICELoss 0.8055 (0.4220)
Epoch: [4][259/500]	Time  8.283 ( 8.283)	Loss 1.8602 (1.6281)	CeLoss 0.2314 (0.2975)	SegCLSLoss 0.0111 (0.0132)	KLLoss 0.3633 (0.2922)	MaskLoss 0.7934 (0.6474)	MaskBCELoss 0.0731 (0.1085)	MaskDICELoss 0.7203 (0.5389)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 257 is less than current step: 499. Dropping entry: {'train/loss': 1.8380388498306275, 'train/ce_loss': 0.43193359375, 'train/seg_cls_loss': 0.01014404296875, 'train/kl_loss': 0.2587890625, 'train/mask_bce_loss': 0.26557415053248407, 'train/mask_dice_loss': 0.421951150894165, 'train/mask_loss': 0.687525287270546, 'metrics/total_secs_per_batch': 7.463192939758301, 'metrics/data_secs_per_batch': 2.927392315864563, '_timestamp': 1740966245.1202843}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 257 is less than current step: 499. Dropping entry: {'train/lr': 2.9999999999999997e-05, '_timestamp': 1740966245.1205504}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 258 is less than current step: 499. Dropping entry: {'train/loss': 1.6281237095594405, 'train/ce_loss': 0.2974609375, 'train/seg_cls_loss': 0.013214111328125, 'train/kl_loss': 0.2921875, 'train/mask_bce_loss': 0.10853821709752083, 'train/mask_dice_loss': 0.5388732414692641, 'train/mask_loss': 0.6474114641547203, 'metrics/total_secs_per_batch': 8.282949447631836, 'metrics/data_secs_per_batch': 3.765471076965332, '_timestamp': 1740966253.4032273}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 258 is less than current step: 499. Dropping entry: {'train/lr': 2.987755102040816e-05, '_timestamp': 1740966253.4035003}).
[2025-03-02 19:44:19,668] [INFO] [logging.py:96:log_dist] [Rank 0] step=2260, skipped=0, lr=[2.9816326530612242e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:44:19,674] [INFO] [timer.py:215:stop] epoch=0/micro_step=22600/global_step=2260, RunningAvgSamplesPerSec=1.4618942740132541, CurrSamplesPerSec=1.594829931548579, MemAllocated=30.69GB, MaxMemAllocated=37.23GB
Epoch: [4][260/500]	Time  6.272 ( 6.272)	Loss 1.2969 (1.9369)	CeLoss 1.2969 (0.3922)	SegCLSLoss 0.0000 (0.0161)	KLLoss 0.0000 (0.2936)	MaskLoss 0.0000 (0.7535)	MaskBCELoss 0.0000 (0.1277)	MaskDICELoss 0.0000 (0.6259)
Epoch: [4][261/500]	Time  8.015 ( 8.015)	Loss 2.5857 (1.7332)	CeLoss 0.2148 (0.3213)	SegCLSLoss 0.0117 (0.0131)	KLLoss 0.3633 (0.2916)	MaskLoss 1.1640 (0.6879)	MaskBCELoss 0.4004 (0.1493)	MaskDICELoss 0.7636 (0.5387)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 259 is less than current step: 499. Dropping entry: {'train/loss': 1.936941647529602, 'train/ce_loss': 0.3921875, 'train/seg_cls_loss': 0.01612548828125, 'train/kl_loss': 0.2935546875, 'train/mask_bce_loss': 0.12766585582867265, 'train/mask_dice_loss': 0.6258635520935059, 'train/mask_loss': 0.7535294175148011, 'metrics/total_secs_per_batch': 6.271749019622803, 'metrics/data_secs_per_batch': 2.5890791177749635, '_timestamp': 1740966259.674859}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 259 is less than current step: 499. Dropping entry: {'train/lr': 2.9755102040816323e-05, '_timestamp': 1740966259.6751542}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 260 is less than current step: 499. Dropping entry: {'train/loss': 1.7331585168838501, 'train/ce_loss': 0.321337890625, 'train/seg_cls_loss': 0.01314697265625, 'train/kl_loss': 0.2916015625, 'train/mask_bce_loss': 0.14925193451344967, 'train/mask_dice_loss': 0.5386896371841431, 'train/mask_loss': 0.687941563129425, 'metrics/total_secs_per_batch': 8.01512861251831, 'metrics/data_secs_per_batch': 3.3357749700546266, '_timestamp': 1740966267.6902063}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 260 is less than current step: 499. Dropping entry: {'train/lr': 2.963265306122449e-05, '_timestamp': 1740966267.6904995}).
Epoch: [4][262/500]	Time  8.095 ( 8.095)	Loss 2.3550 (1.8130)	CeLoss 0.1924 (0.3591)	SegCLSLoss 0.0143 (0.0145)	KLLoss 0.3730 (0.2938)	MaskLoss 1.0593 (0.7087)	MaskBCELoss 0.2033 (0.1047)	MaskDICELoss 0.8560 (0.6040)
Epoch: [4][263/500]	Time  8.512 ( 8.512)	Loss 1.8328 (1.3069)	CeLoss 0.2412 (0.3677)	SegCLSLoss 0.0110 (0.0116)	KLLoss 0.3672 (0.2574)	MaskLoss 0.7748 (0.4538)	MaskBCELoss 0.0947 (0.0729)	MaskDICELoss 0.6801 (0.3809)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 261 is less than current step: 499. Dropping entry: {'train/loss': 1.8130431056022644, 'train/ce_loss': 0.35908203125, 'train/seg_cls_loss': 0.014495849609375, 'train/kl_loss': 0.29375, 'train/mask_bce_loss': 0.1047160193324089, 'train/mask_dice_loss': 0.6040027916431427, 'train/mask_loss': 0.7087188065052032, 'metrics/total_secs_per_batch': 8.09468698501587, 'metrics/data_secs_per_batch': 3.686871600151062, '_timestamp': 1740966275.7848322}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 261 is less than current step: 499. Dropping entry: {'train/lr': 2.951020408163265e-05, '_timestamp': 1740966275.7851384}).
Epoch: [4][264/500]	Time  8.210 ( 8.210)	Loss 2.1089 (1.3077)	CeLoss 0.2197 (0.3124)	SegCLSLoss 0.0219 (0.0105)	KLLoss 0.3516 (0.2557)	MaskLoss 0.9216 (0.4823)	MaskBCELoss 0.0417 (0.0883)	MaskDICELoss 0.8799 (0.3940)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 262 is less than current step: 499. Dropping entry: {'train/loss': 1.3069049954414367, 'train/ce_loss': 0.36767578125, 'train/seg_cls_loss': 0.011614990234375, 'train/kl_loss': 0.257421875, 'train/mask_bce_loss': 0.07290229350328445, 'train/mask_dice_loss': 0.38089199662208556, 'train/mask_loss': 0.4537942886352539, 'metrics/total_secs_per_batch': 8.512081623077393, 'metrics/data_secs_per_batch': 3.9521742343902586, '_timestamp': 1740966284.2968304}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 262 is less than current step: 499. Dropping entry: {'train/lr': 2.9387755102040813e-05, '_timestamp': 1740966284.29707}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 263 is less than current step: 499. Dropping entry: {'train/loss': 1.3076611131429672, 'train/ce_loss': 0.312353515625, 'train/seg_cls_loss': 0.01048583984375, 'train/kl_loss': 0.2556640625, 'train/mask_bce_loss': 0.08828917415812612, 'train/mask_dice_loss': 0.39403258115053175, 'train/mask_loss': 0.4823217555880547, 'metrics/total_secs_per_batch': 8.209820747375488, 'metrics/data_secs_per_batch': 3.9963943719863892, '_timestamp': 1740966292.5067306}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 263 is less than current step: 499. Dropping entry: {'train/lr': 2.926530612244898e-05, '_timestamp': 1740966292.5070677}).
Epoch: [4][265/500]	Time  7.825 ( 7.825)	Loss 2.2803 (1.5341)	CeLoss 0.3047 (0.3533)	SegCLSLoss 0.0116 (0.0133)	KLLoss 0.3613 (0.2939)	MaskLoss 0.9663 (0.5724)	MaskBCELoss 0.3555 (0.0997)	MaskDICELoss 0.6108 (0.4727)
Epoch: [4][266/500]	Time  6.395 ( 6.395)	Loss 1.5627 (1.8646)	CeLoss 0.3359 (0.4800)	SegCLSLoss 0.0099 (0.0131)	KLLoss 0.3652 (0.2609)	MaskLoss 0.5919 (0.6759)	MaskBCELoss 0.1159 (0.1526)	MaskDICELoss 0.4760 (0.5233)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 264 is less than current step: 499. Dropping entry: {'train/loss': 1.5340908408164977, 'train/ce_loss': 0.3533203125, 'train/seg_cls_loss': 0.013250732421875, 'train/kl_loss': 0.2939453125, 'train/mask_bce_loss': 0.09967967215925455, 'train/mask_dice_loss': 0.4726880192756653, 'train/mask_loss': 0.5723676919937134, 'metrics/total_secs_per_batch': 7.8246002197265625, 'metrics/data_secs_per_batch': 3.4342767000198364, '_timestamp': 1740966300.3313143}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 264 is less than current step: 499. Dropping entry: {'train/lr': 2.914285714285714e-05, '_timestamp': 1740966300.3315787}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 265 is less than current step: 499. Dropping entry: {'train/loss': 1.8646293520927428, 'train/ce_loss': 0.47998046875, 'train/seg_cls_loss': 0.013055419921875, 'train/kl_loss': 0.2609375, 'train/mask_bce_loss': 0.15263571897521616, 'train/mask_dice_loss': 0.5232824623584748, 'train/mask_loss': 0.6759181678295135, 'metrics/total_secs_per_batch': 6.395251274108887, 'metrics/data_secs_per_batch': 2.680756449699402, '_timestamp': 1740966306.7265859}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 265 is less than current step: 499. Dropping entry: {'train/lr': 2.9020408163265302e-05, '_timestamp': 1740966306.7268424}).
Epoch: [4][267/500]	Time  9.254 ( 9.254)	Loss 1.7711 (1.8355)	CeLoss 0.1904 (0.3729)	SegCLSLoss 0.0203 (0.0149)	KLLoss 0.3652 (0.3320)	MaskLoss 0.7674 (0.7110)	MaskBCELoss 0.0430 (0.2042)	MaskDICELoss 0.7243 (0.5068)
Epoch: [4][268/500]	Time  8.226 ( 8.226)	Loss 2.2970 (1.7343)	CeLoss 0.2754 (0.3206)	SegCLSLoss 0.0161 (0.0156)	KLLoss 0.3672 (0.3291)	MaskLoss 0.9883 (0.6864)	MaskBCELoss 0.4893 (0.1153)	MaskDICELoss 0.4990 (0.5711)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 266 is less than current step: 499. Dropping entry: {'train/loss': 1.8354859590530395, 'train/ce_loss': 0.3728515625, 'train/seg_cls_loss': 0.0149169921875, 'train/kl_loss': 0.33203125, 'train/mask_bce_loss': 0.2042461033910513, 'train/mask_dice_loss': 0.5067585974931716, 'train/mask_loss': 0.7110046952962875, 'metrics/total_secs_per_batch': 9.253556966781616, 'metrics/data_secs_per_batch': 4.292998456954956, '_timestamp': 1740966315.9801357}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 266 is less than current step: 499. Dropping entry: {'train/lr': 2.8897959183673468e-05, '_timestamp': 1740966315.9803228}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 267 is less than current step: 499. Dropping entry: {'train/loss': 1.7343327045440673, 'train/ce_loss': 0.32060546875, 'train/seg_cls_loss': 0.015606689453125, 'train/kl_loss': 0.3291015625, 'train/mask_bce_loss': 0.11526575870811939, 'train/mask_dice_loss': 0.5711388632655143, 'train/mask_loss': 0.6864046305418015, 'metrics/total_secs_per_batch': 8.225940704345703, 'metrics/data_secs_per_batch': 3.665584659576416, '_timestamp': 1740966324.2060769}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 267 is less than current step: 499. Dropping entry: {'train/lr': 2.8775510204081628e-05, '_timestamp': 1740966324.2063413}).
Epoch: [4][269/500]	Time  8.005 ( 8.005)	Loss 1.3011 (1.2817)	CeLoss 0.2178 (0.2859)	SegCLSLoss 0.0117 (0.0127)	KLLoss 0.3672 (0.2920)	MaskLoss 0.5207 (0.4801)	MaskBCELoss 0.0640 (0.1132)	MaskDICELoss 0.4566 (0.3669)
[2025-03-02 19:45:40,311] [INFO] [logging.py:96:log_dist] [Rank 0] step=2270, skipped=0, lr=[2.8591836734693876e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:45:40,318] [INFO] [timer.py:215:stop] epoch=0/micro_step=22700/global_step=2270, RunningAvgSamplesPerSec=1.4607429462971593, CurrSamplesPerSec=1.2336784392257543, MemAllocated=31.44GB, MaxMemAllocated=37.23GB
Epoch: [4][270/500]	Time  8.107 ( 8.107)	Loss 2.0689 (1.4264)	CeLoss 0.2324 (0.2320)	SegCLSLoss 0.0146 (0.0120)	KLLoss 0.3613 (0.2584)	MaskLoss 0.8958 (0.5812)	MaskBCELoss 0.0268 (0.1074)	MaskDICELoss 0.8689 (0.4738)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 268 is less than current step: 499. Dropping entry: {'train/loss': 1.2816876947879792, 'train/ce_loss': 0.2858642578125, 'train/seg_cls_loss': 0.012725830078125, 'train/kl_loss': 0.2919921875, 'train/mask_bce_loss': 0.11320896204560996, 'train/mask_dice_loss': 0.3668804854154587, 'train/mask_loss': 0.48008944988250735, 'metrics/total_secs_per_batch': 8.005351543426514, 'metrics/data_secs_per_batch': 3.466576361656189, '_timestamp': 1740966332.2114751}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 268 is less than current step: 499. Dropping entry: {'train/lr': 2.865306122448979e-05, '_timestamp': 1740966332.2117798}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 269 is less than current step: 499. Dropping entry: {'train/loss': 1.4263563990592956, 'train/ce_loss': 0.231982421875, 'train/seg_cls_loss': 0.012042236328125, 'train/kl_loss': 0.2583984375, 'train/mask_bce_loss': 0.10742184584960342, 'train/mask_dice_loss': 0.473798343539238, 'train/mask_loss': 0.5812201917171478, 'metrics/total_secs_per_batch': 8.10740876197815, 'metrics/data_secs_per_batch': 3.3508177042007445, '_timestamp': 1740966340.3187041}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 269 is less than current step: 499. Dropping entry: {'train/lr': 2.8530612244897957e-05, '_timestamp': 1740966340.319004}).
Epoch: [4][271/500]	Time  9.090 ( 9.090)	Loss 1.8269 (2.0237)	CeLoss 0.2080 (0.3397)	SegCLSLoss 0.0234 (0.0188)	KLLoss 0.3555 (0.3281)	MaskLoss 0.7855 (0.8209)	MaskBCELoss 0.1030 (0.1685)	MaskDICELoss 0.6825 (0.6524)
Epoch: [4][272/500]	Time  8.786 ( 8.786)	Loss 1.4795 (1.5181)	CeLoss 0.3105 (0.3405)	SegCLSLoss 0.0160 (0.0129)	KLLoss 0.3652 (0.2939)	MaskLoss 0.5620 (0.5707)	MaskBCELoss 0.0710 (0.0911)	MaskDICELoss 0.4910 (0.4796)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 270 is less than current step: 499. Dropping entry: {'train/loss': 2.0236857295036317, 'train/ce_loss': 0.339697265625, 'train/seg_cls_loss': 0.018792724609375, 'train/kl_loss': 0.328125, 'train/mask_bce_loss': 0.1684710955247283, 'train/mask_dice_loss': 0.6524049580097199, 'train/mask_loss': 0.8208760499954224, 'metrics/total_secs_per_batch': 9.09035611152649, 'metrics/data_secs_per_batch': 4.2555042743682865, '_timestamp': 1740966349.4091778}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 270 is less than current step: 499. Dropping entry: {'train/lr': 2.840816326530612e-05, '_timestamp': 1740966349.4094415}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 271 is less than current step: 499. Dropping entry: {'train/loss': 1.5180626034736633, 'train/ce_loss': 0.34052734375, 'train/seg_cls_loss': 0.012884521484375, 'train/kl_loss': 0.2939453125, 'train/mask_bce_loss': 0.09113475270569324, 'train/mask_dice_loss': 0.4795664638280869, 'train/mask_loss': 0.5707012295722962, 'metrics/total_secs_per_batch': 8.785733699798584, 'metrics/data_secs_per_batch': 4.203403568267822, '_timestamp': 1740966358.1949568}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 271 is less than current step: 499. Dropping entry: {'train/lr': 2.8285714285714284e-05, '_timestamp': 1740966358.1952195}).
Epoch: [4][273/500]	Time  6.982 ( 6.982)	Loss 2.3685 (1.6141)	CeLoss 0.2236 (0.5608)	SegCLSLoss 0.0205 (0.0118)	KLLoss 0.3555 (0.2188)	MaskLoss 1.0495 (0.5128)	MaskBCELoss 0.2151 (0.1024)	MaskDICELoss 0.8344 (0.4104)
Epoch: [4][274/500]	Time  8.528 ( 8.528)	Loss 1.2266 (1.6935)	CeLoss 1.2266 (0.5491)	SegCLSLoss 0.0000 (0.0115)	KLLoss 0.0000 (0.2551)	MaskLoss 0.0000 (0.5565)	MaskBCELoss 0.0000 (0.0580)	MaskDICELoss 0.0000 (0.4985)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 272 is less than current step: 499. Dropping entry: {'train/loss': 1.6140966415405273, 'train/ce_loss': 0.5608154296875, 'train/seg_cls_loss': 0.011834716796875, 'train/kl_loss': 0.21875, 'train/mask_bce_loss': 0.10235239826142788, 'train/mask_dice_loss': 0.41042101979255674, 'train/mask_loss': 0.5127734065055847, 'metrics/total_secs_per_batch': 6.982221841812134, 'metrics/data_secs_per_batch': 3.2132885217666627, '_timestamp': 1740966365.1771626}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 272 is less than current step: 499. Dropping entry: {'train/lr': 2.8163265306122447e-05, '_timestamp': 1740966365.1774657}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 273 is less than current step: 499. Dropping entry: {'train/loss': 1.6934879064559936, 'train/ce_loss': 0.54912109375, 'train/seg_cls_loss': 0.011517333984375, 'train/kl_loss': 0.255078125, 'train/mask_bce_loss': 0.05801951633766293, 'train/mask_dice_loss': 0.4984900563955307, 'train/mask_loss': 0.5565095752477646, 'metrics/total_secs_per_batch': 8.528457641601562, 'metrics/data_secs_per_batch': 4.6683388471603395, '_timestamp': 1740966373.705636}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 273 is less than current step: 499. Dropping entry: {'train/lr': 2.804081632653061e-05, '_timestamp': 1740966373.7059221}).
Epoch: [4][275/500]	Time  7.969 ( 7.969)	Loss 1.2193 (1.5502)	CeLoss 0.2334 (0.4060)	SegCLSLoss 0.0115 (0.0145)	KLLoss 0.3672 (0.2957)	MaskLoss 0.4720 (0.5537)	MaskBCELoss 0.2632 (0.1479)	MaskDICELoss 0.2087 (0.4057)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 274 is less than current step: 499. Dropping entry: {'train/loss': 1.5501725614070891, 'train/ce_loss': 0.40595703125, 'train/seg_cls_loss': 0.0144775390625, 'train/kl_loss': 0.295703125, 'train/mask_bce_loss': 0.14794043712317945, 'train/mask_dice_loss': 0.40571028292179107, 'train/mask_loss': 0.5536507099866868, 'metrics/total_secs_per_batch': 7.968634605407715, 'metrics/data_secs_per_batch': 3.7549561977386476, '_timestamp': 1740966381.6745076}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 274 is less than current step: 499. Dropping entry: {'train/lr': 2.7918367346938776e-05, '_timestamp': 1740966381.6748602}).
Epoch: [4][276/500]	Time  8.891 ( 8.891)	Loss 1.4378 (1.7273)	CeLoss 0.3789 (0.3333)	SegCLSLoss 0.0112 (0.0133)	KLLoss 0.3633 (0.2902)	MaskLoss 0.5079 (0.6790)	MaskBCELoss 0.0905 (0.0959)	MaskDICELoss 0.4175 (0.5831)
Epoch: [4][277/500]	Time  7.687 ( 7.687)	Loss 2.1304 (1.6974)	CeLoss 0.1738 (0.2505)	SegCLSLoss 0.0251 (0.0175)	KLLoss 0.3711 (0.3330)	MaskLoss 0.9534 (0.7025)	MaskBCELoss 0.0397 (0.1452)	MaskDICELoss 0.9137 (0.5573)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 275 is less than current step: 499. Dropping entry: {'train/loss': 1.727268123626709, 'train/ce_loss': 0.333251953125, 'train/seg_cls_loss': 0.013330078125, 'train/kl_loss': 0.290234375, 'train/mask_bce_loss': 0.09591152649372817, 'train/mask_dice_loss': 0.5830789595842362, 'train/mask_loss': 0.678990489244461, 'metrics/total_secs_per_batch': 8.89073133468628, 'metrics/data_secs_per_batch': 4.158600068092346, '_timestamp': 1740966390.5650098}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 275 is less than current step: 499. Dropping entry: {'train/lr': 2.7795918367346936e-05, '_timestamp': 1740966390.5652041}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 276 is less than current step: 499. Dropping entry: {'train/loss': 1.697395646572113, 'train/ce_loss': 0.25048828125, 'train/seg_cls_loss': 0.01748046875, 'train/kl_loss': 0.3330078125, 'train/mask_bce_loss': 0.14515780434012412, 'train/mask_dice_loss': 0.5573486179113388, 'train/mask_loss': 0.7025064170360565, 'metrics/total_secs_per_batch': 7.686972141265869, 'metrics/data_secs_per_batch': 3.714062547683716, '_timestamp': 1740966398.251975}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 276 is less than current step: 499. Dropping entry: {'train/lr': 2.76734693877551e-05, '_timestamp': 1740966398.252234}).
Epoch: [4][278/500]	Time  8.602 ( 8.602)	Loss 1.5797 (1.8998)	CeLoss 0.3340 (0.4534)	SegCLSLoss 0.0102 (0.0132)	KLLoss 0.3711 (0.2568)	MaskLoss 0.6014 (0.7069)	MaskBCELoss 0.1420 (0.1596)	MaskDICELoss 0.4593 (0.5473)
Epoch: [4][279/500]	Time  9.213 ( 9.213)	Loss 2.8256 (1.7225)	CeLoss 0.2480 (0.2079)	SegCLSLoss 0.0145 (0.0124)	KLLoss 0.3594 (0.2920)	MaskLoss 1.2673 (0.7397)	MaskBCELoss 0.4952 (0.2436)	MaskDICELoss 0.7721 (0.4961)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 277 is less than current step: 499. Dropping entry: {'train/loss': 1.8998326182365417, 'train/ce_loss': 0.45341796875, 'train/seg_cls_loss': 0.013232421875, 'train/kl_loss': 0.2568359375, 'train/mask_bce_loss': 0.15960435718297958, 'train/mask_dice_loss': 0.5473432093858719, 'train/mask_loss': 0.7069475769996643, 'metrics/total_secs_per_batch': 8.601885318756104, 'metrics/data_secs_per_batch': 3.5945388078689575, '_timestamp': 1740966406.853862}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 277 is less than current step: 499. Dropping entry: {'train/lr': 2.7551020408163265e-05, '_timestamp': 1740966406.8541212}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 278 is less than current step: 499. Dropping entry: {'train/loss': 1.7225451350212098, 'train/ce_loss': 0.20791015625, 'train/seg_cls_loss': 0.01236572265625, 'train/kl_loss': 0.2919921875, 'train/mask_bce_loss': 0.24356896989047527, 'train/mask_dice_loss': 0.4961215663701296, 'train/mask_loss': 0.7396905317902565, 'metrics/total_secs_per_batch': 9.213116645812988, 'metrics/data_secs_per_batch': 4.183143877983094, '_timestamp': 1740966416.0669906}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 278 is less than current step: 499. Dropping entry: {'train/lr': 2.7428571428571425e-05, '_timestamp': 1740966416.067248}).
[2025-03-02 19:47:03,496] [INFO] [logging.py:96:log_dist] [Rank 0] step=2280, skipped=0, lr=[2.7367346938775506e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:47:03,501] [INFO] [timer.py:215:stop] epoch=0/micro_step=22800/global_step=2280, RunningAvgSamplesPerSec=1.4593660536807318, CurrSamplesPerSec=1.345182654802977, MemAllocated=31.1GB, MaxMemAllocated=37.23GB
Epoch: [4][280/500]	Time  7.435 ( 7.435)	Loss 2.0771 (1.7784)	CeLoss 0.2285 (0.4390)	SegCLSLoss 0.0148 (0.0133)	KLLoss 0.3555 (0.2893)	MaskLoss 0.9028 (0.6520)	MaskBCELoss 0.2240 (0.1145)	MaskDICELoss 0.6788 (0.5374)
Epoch: [4][281/500]	Time  7.364 ( 7.364)	Loss 2.1778 (1.7488)	CeLoss 0.2148 (0.3280)	SegCLSLoss 0.0186 (0.0197)	KLLoss 0.3594 (0.3285)	MaskLoss 0.9590 (0.6890)	MaskBCELoss 0.0461 (0.0941)	MaskDICELoss 0.9129 (0.5949)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 279 is less than current step: 499. Dropping entry: {'train/loss': 1.7783671021461487, 'train/ce_loss': 0.43896484375, 'train/seg_cls_loss': 0.013250732421875, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.11452650409191847, 'train/mask_dice_loss': 0.5374499917030334, 'train/mask_loss': 0.6519765079021453, 'metrics/total_secs_per_batch': 7.435428619384766, 'metrics/data_secs_per_batch': 3.1456177949905397, '_timestamp': 1740966423.5022292}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 279 is less than current step: 499. Dropping entry: {'train/lr': 2.7306122448979588e-05, '_timestamp': 1740966423.5024884}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 280 is less than current step: 499. Dropping entry: {'train/loss': 1.7487550616264342, 'train/ce_loss': 0.32802734375, 'train/seg_cls_loss': 0.0197265625, 'train/kl_loss': 0.328515625, 'train/mask_bce_loss': 0.0940805371850729, 'train/mask_dice_loss': 0.5948965936899185, 'train/mask_loss': 0.6889771342277526, 'metrics/total_secs_per_batch': 7.3639137744903564, 'metrics/data_secs_per_batch': 3.464309740066528, '_timestamp': 1740966430.8663628}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 280 is less than current step: 499. Dropping entry: {'train/lr': 2.7183673469387754e-05, '_timestamp': 1740966430.8667026}).
Epoch: [4][282/500]	Time 12.216 (12.216)	Loss 2.5953 (1.8101)	CeLoss 0.3457 (0.3441)	SegCLSLoss 0.0134 (0.0143)	KLLoss 0.3809 (0.2939)	MaskLoss 1.1023 (0.7146)	MaskBCELoss 0.3299 (0.0991)	MaskDICELoss 0.7724 (0.6155)
Epoch: [4][283/500]	Time  7.643 ( 7.643)	Loss 1.7491 (1.2612)	CeLoss 0.2852 (0.5006)	SegCLSLoss 0.0101 (0.0081)	KLLoss 0.3711 (0.2188)	MaskLoss 0.7105 (0.3672)	MaskBCELoss 0.1461 (0.0749)	MaskDICELoss 0.5643 (0.2923)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 281 is less than current step: 499. Dropping entry: {'train/loss': 1.810056710243225, 'train/ce_loss': 0.344140625, 'train/seg_cls_loss': 0.014288330078125, 'train/kl_loss': 0.2939453125, 'train/mask_bce_loss': 0.09911886043846607, 'train/mask_dice_loss': 0.6155286252498626, 'train/mask_loss': 0.7146474957466126, 'metrics/total_secs_per_batch': 12.215638399124146, 'metrics/data_secs_per_batch': 6.665837597846985, '_timestamp': 1740966443.0819743}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 281 is less than current step: 499. Dropping entry: {'train/lr': 2.7061224489795914e-05, '_timestamp': 1740966443.0822365}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 282 is less than current step: 499. Dropping entry: {'train/loss': 1.261158686876297, 'train/ce_loss': 0.5005859375, 'train/seg_cls_loss': 0.008148193359375, 'train/kl_loss': 0.21875, 'train/mask_bce_loss': 0.07491618767380714, 'train/mask_dice_loss': 0.29233306646347046, 'train/mask_loss': 0.36724926233291627, 'metrics/total_secs_per_batch': 7.64275050163269, 'metrics/data_secs_per_batch': 3.149581527709961, '_timestamp': 1740966450.7247133}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 282 is less than current step: 499. Dropping entry: {'train/lr': 2.6938775510204077e-05, '_timestamp': 1740966450.7250192}).
Epoch: [4][284/500]	Time  7.257 ( 7.257)	Loss 0.0527 (1.7312)	CeLoss 0.0527 (0.5025)	SegCLSLoss 0.0000 (0.0101)	KLLoss 0.0000 (0.2221)	MaskLoss 0.0000 (0.6008)	MaskBCELoss 0.0000 (0.2000)	MaskDICELoss 0.0000 (0.4008)
Epoch: [4][285/500]	Time  8.200 ( 8.200)	Loss 2.4336 (1.5533)	CeLoss 0.2080 (0.3280)	SegCLSLoss 0.0134 (0.0128)	KLLoss 0.3633 (0.2602)	MaskLoss 1.0908 (0.5965)	MaskBCELoss 0.3048 (0.1447)	MaskDICELoss 0.7860 (0.4517)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 283 is less than current step: 499. Dropping entry: {'train/loss': 1.7311977386474608, 'train/ce_loss': 0.5025390625, 'train/seg_cls_loss': 0.010113525390625, 'train/kl_loss': 0.2220703125, 'train/mask_bce_loss': 0.19998367326334118, 'train/mask_dice_loss': 0.40077145099639894, 'train/mask_loss': 0.6007551193237305, 'metrics/total_secs_per_batch': 7.257410287857056, 'metrics/data_secs_per_batch': 3.0952423572540284, '_timestamp': 1740966457.9821258}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 283 is less than current step: 499. Dropping entry: {'train/lr': 2.6816326530612244e-05, '_timestamp': 1740966457.9823859}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 284 is less than current step: 499. Dropping entry: {'train/loss': 1.5533110976219178, 'train/ce_loss': 0.327978515625, 'train/seg_cls_loss': 0.012847900390625, 'train/kl_loss': 0.26015625, 'train/mask_bce_loss': 0.1447318870574236, 'train/mask_dice_loss': 0.4517234593629837, 'train/mask_loss': 0.5964553534984589, 'metrics/total_secs_per_batch': 8.199767112731934, 'metrics/data_secs_per_batch': 3.558125066757202, '_timestamp': 1740966466.181907}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 284 is less than current step: 499. Dropping entry: {'train/lr': 2.6693877551020407e-05, '_timestamp': 1740966466.182172}).
Epoch: [4][286/500]	Time  9.177 ( 9.177)	Loss 2.4650 (1.9732)	CeLoss 0.2295 (0.1887)	SegCLSLoss 0.0181 (0.0180)	KLLoss 0.3750 (0.3260)	MaskLoss 1.0948 (0.8717)	MaskBCELoss 0.2880 (0.1453)	MaskDICELoss 0.8067 (0.7264)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 285 is less than current step: 499. Dropping entry: {'train/loss': 1.9732191503047942, 'train/ce_loss': 0.188671875, 'train/seg_cls_loss': 0.017962646484375, 'train/kl_loss': 0.3259765625, 'train/mask_bce_loss': 0.14528074096888305, 'train/mask_dice_loss': 0.726387420296669, 'train/mask_loss': 0.8716681629419327, 'metrics/total_secs_per_batch': 9.177441358566284, 'metrics/data_secs_per_batch': 3.5012553691864015, '_timestamp': 1740966475.3593445}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 285 is less than current step: 499. Dropping entry: {'train/lr': 2.6571428571428566e-05, '_timestamp': 1740966475.3596113}).
Epoch: [4][287/500]	Time  8.605 ( 8.605)	Loss 2.1479 (1.6193)	CeLoss 0.1621 (0.2363)	SegCLSLoss 0.0187 (0.0174)	KLLoss 0.3633 (0.3664)	MaskLoss 0.9699 (0.6688)	MaskBCELoss 0.0038 (0.1128)	MaskDICELoss 0.9662 (0.5561)
Epoch: [4][288/500]	Time  7.351 ( 7.351)	Loss 2.2071 (1.5527)	CeLoss 0.2402 (0.3570)	SegCLSLoss 0.0116 (0.0113)	KLLoss 0.3691 (0.2936)	MaskLoss 0.9620 (0.5801)	MaskBCELoss 0.4811 (0.1788)	MaskDICELoss 0.4808 (0.4013)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 286 is less than current step: 499. Dropping entry: {'train/loss': 1.6193094372749328, 'train/ce_loss': 0.236328125, 'train/seg_cls_loss': 0.017425537109375, 'train/kl_loss': 0.36640625, 'train/mask_bce_loss': 0.11278354020323604, 'train/mask_dice_loss': 0.5560508593916893, 'train/mask_loss': 0.6688344001770019, 'metrics/total_secs_per_batch': 8.604825019836426, 'metrics/data_secs_per_batch': 3.5786373615264893, '_timestamp': 1740966483.964394}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 286 is less than current step: 499. Dropping entry: {'train/lr': 2.6448979591836733e-05, '_timestamp': 1740966483.9647589}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 287 is less than current step: 499. Dropping entry: {'train/loss': 1.5526952743530273, 'train/ce_loss': 0.35703125, 'train/seg_cls_loss': 0.011309814453125, 'train/kl_loss': 0.2935546875, 'train/mask_bce_loss': 0.17877776734530926, 'train/mask_dice_loss': 0.40128080546855927, 'train/mask_loss': 0.5800585716962814, 'metrics/total_secs_per_batch': 7.351022243499756, 'metrics/data_secs_per_batch': 3.418154549598694, '_timestamp': 1740966491.3152575}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 287 is less than current step: 499. Dropping entry: {'train/lr': 2.6326530612244896e-05, '_timestamp': 1740966491.3155541}).
Epoch: [4][289/500]	Time  8.091 ( 8.091)	Loss 2.0213 (1.6671)	CeLoss 0.2471 (0.5205)	SegCLSLoss 0.0205 (0.0152)	KLLoss 0.3594 (0.2549)	MaskLoss 0.8642 (0.5568)	MaskBCELoss 0.0081 (0.0740)	MaskDICELoss 0.8560 (0.4828)
[2025-03-02 19:48:28,102] [INFO] [logging.py:96:log_dist] [Rank 0] step=2290, skipped=0, lr=[2.614285714285714e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:48:28,109] [INFO] [timer.py:215:stop] epoch=0/micro_step=22900/global_step=2290, RunningAvgSamplesPerSec=1.457871473718847, CurrSamplesPerSec=1.149169342844427, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [4][290/500]	Time  8.703 ( 8.703)	Loss 2.5086 (1.8147)	CeLoss 0.2148 (0.2219)	SegCLSLoss 0.0176 (0.0163)	KLLoss 0.3672 (0.3664)	MaskLoss 1.1244 (0.7738)	MaskBCELoss 0.2157 (0.1362)	MaskDICELoss 0.9087 (0.6376)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 288 is less than current step: 499. Dropping entry: {'train/loss': 1.6670504570007325, 'train/ce_loss': 0.5205078125, 'train/seg_cls_loss': 0.01522216796875, 'train/kl_loss': 0.2548828125, 'train/mask_bce_loss': 0.07404740126803518, 'train/mask_dice_loss': 0.48276883363723755, 'train/mask_loss': 0.5568162322044372, 'metrics/total_secs_per_batch': 8.091357469558716, 'metrics/data_secs_per_batch': 3.27723650932312, '_timestamp': 1740966499.4065428}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 288 is less than current step: 499. Dropping entry: {'train/lr': 2.6204081632653062e-05, '_timestamp': 1740966499.4068124}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 289 is less than current step: 499. Dropping entry: {'train/loss': 1.8146655440330506, 'train/ce_loss': 0.221875, 'train/seg_cls_loss': 0.016278076171875, 'train/kl_loss': 0.36640625, 'train/mask_bce_loss': 0.13624054975807667, 'train/mask_dice_loss': 0.637596133351326, 'train/mask_loss': 0.7738366723060608, 'metrics/total_secs_per_batch': 8.70339322090149, 'metrics/data_secs_per_batch': 4.017110443115234, '_timestamp': 1740966508.1097925}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 289 is less than current step: 499. Dropping entry: {'train/lr': 2.6081632653061222e-05, '_timestamp': 1740966508.110083}).
Epoch: [4][291/500]	Time  9.756 ( 9.756)	Loss 2.3282 (1.9067)	CeLoss 0.1953 (0.2136)	SegCLSLoss 0.0197 (0.0188)	KLLoss 0.3496 (0.3633)	MaskLoss 1.0440 (0.8239)	MaskBCELoss 0.0793 (0.1584)	MaskDICELoss 0.9647 (0.6655)
Epoch: [4][292/500]	Time  7.798 ( 7.798)	Loss 1.7550 (1.6363)	CeLoss 0.1875 (0.4140)	SegCLSLoss 0.0195 (0.0105)	KLLoss 0.3730 (0.2562)	MaskLoss 0.7603 (0.5959)	MaskBCELoss 0.1452 (0.1370)	MaskDICELoss 0.6151 (0.4588)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 290 is less than current step: 499. Dropping entry: {'train/loss': 1.9067097902297974, 'train/ce_loss': 0.21357421875, 'train/seg_cls_loss': 0.0188232421875, 'train/kl_loss': 0.36328125, 'train/mask_bce_loss': 0.15841135755181313, 'train/mask_dice_loss': 0.6654513359069825, 'train/mask_loss': 0.8238627135753631, 'metrics/total_secs_per_batch': 9.756199836730957, 'metrics/data_secs_per_batch': 4.3299381017684935, '_timestamp': 1740966517.8661566}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 290 is less than current step: 499. Dropping entry: {'train/lr': 2.5959183673469385e-05, '_timestamp': 1740966517.8664289}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 291 is less than current step: 499. Dropping entry: {'train/loss': 1.6363438487052917, 'train/ce_loss': 0.41396484375, 'train/seg_cls_loss': 0.010516357421875, 'train/kl_loss': 0.25625, 'train/mask_bce_loss': 0.13702141009271146, 'train/mask_dice_loss': 0.45883606672286986, 'train/mask_loss': 0.5958574712276459, 'metrics/total_secs_per_batch': 7.7981274127960205, 'metrics/data_secs_per_batch': 3.2623847007751463, '_timestamp': 1740966525.664305}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 291 is less than current step: 499. Dropping entry: {'train/lr': 2.583673469387755e-05, '_timestamp': 1740966525.6645756}).
Epoch: [4][293/500]	Time  8.747 ( 8.747)	Loss 2.2586 (2.1438)	CeLoss 0.1924 (0.2207)	SegCLSLoss 0.0223 (0.0182)	KLLoss 0.3555 (0.3678)	MaskLoss 1.0102 (0.9387)	MaskBCELoss 0.0303 (0.1809)	MaskDICELoss 0.9798 (0.7578)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 292 is less than current step: 499. Dropping entry: {'train/loss': 2.143828046321869, 'train/ce_loss': 0.220703125, 'train/seg_cls_loss': 0.01815185546875, 'train/kl_loss': 0.3677734375, 'train/mask_bce_loss': 0.18086396306753158, 'train/mask_dice_loss': 0.7578469276428222, 'train/mask_loss': 0.9387108951807022, 'metrics/total_secs_per_batch': 8.746650695800781, 'metrics/data_secs_per_batch': 4.067839050292969, '_timestamp': 1740966534.4109576}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 292 is less than current step: 499. Dropping entry: {'train/lr': 2.571428571428571e-05, '_timestamp': 1740966534.4112403}).
Epoch: [4][294/500]	Time  8.314 ( 8.314)	Loss 2.1078 (1.9203)	CeLoss 0.3086 (0.4903)	SegCLSLoss 0.0098 (0.0125)	KLLoss 0.3730 (0.2578)	MaskLoss 0.8791 (0.6991)	MaskBCELoss 0.0180 (0.1913)	MaskDICELoss 0.8611 (0.5078)
Epoch: [4][295/500]	Time  7.367 ( 7.367)	Loss 1.8782 (2.0776)	CeLoss 0.1934 (0.4442)	SegCLSLoss 0.0193 (0.0158)	KLLoss 0.3633 (0.2975)	MaskLoss 0.8195 (0.7977)	MaskBCELoss 0.0153 (0.2663)	MaskDICELoss 0.8042 (0.5314)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 293 is less than current step: 499. Dropping entry: {'train/loss': 1.9202694296836853, 'train/ce_loss': 0.49033203125, 'train/seg_cls_loss': 0.012481689453125, 'train/kl_loss': 0.2578125, 'train/mask_bce_loss': 0.19133410640060902, 'train/mask_dice_loss': 0.5078142821788788, 'train/mask_loss': 0.6991483807563782, 'metrics/total_secs_per_batch': 8.314035415649414, 'metrics/data_secs_per_batch': 3.8049427270889282, '_timestamp': 1740966542.7250266}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 293 is less than current step: 499. Dropping entry: {'train/lr': 2.5591836734693874e-05, '_timestamp': 1740966542.7253418}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 294 is less than current step: 499. Dropping entry: {'train/loss': 2.077557420730591, 'train/ce_loss': 0.444189453125, 'train/seg_cls_loss': 0.0158447265625, 'train/kl_loss': 0.2974609375, 'train/mask_bce_loss': 0.26629863660782577, 'train/mask_dice_loss': 0.5313667774200439, 'train/mask_loss': 0.7976654231548309, 'metrics/total_secs_per_batch': 7.367150068283081, 'metrics/data_secs_per_batch': 3.2678166389465333, '_timestamp': 1740966550.0923297}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 294 is less than current step: 499. Dropping entry: {'train/lr': 2.546938775510204e-05, '_timestamp': 1740966550.0926518}).
Epoch: [4][296/500]	Time  8.003 ( 8.003)	Loss 0.9950 (1.5301)	CeLoss 0.2695 (0.4577)	SegCLSLoss 0.0110 (0.0136)	KLLoss 0.3633 (0.2199)	MaskLoss 0.3412 (0.5218)	MaskBCELoss 0.0835 (0.0756)	MaskDICELoss 0.2578 (0.4462)
Epoch: [4][297/500]	Time  8.066 ( 8.066)	Loss 1.9432 (1.8096)	CeLoss 0.2168 (0.3152)	SegCLSLoss 0.0172 (0.0162)	KLLoss 0.3594 (0.3279)	MaskLoss 0.8408 (0.7266)	MaskBCELoss 0.0603 (0.1635)	MaskDICELoss 0.7804 (0.5631)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 295 is less than current step: 499. Dropping entry: {'train/loss': 1.530111014842987, 'train/ce_loss': 0.45771484375, 'train/seg_cls_loss': 0.013623046875, 'train/kl_loss': 0.219921875, 'train/mask_bce_loss': 0.07555056959390641, 'train/mask_dice_loss': 0.4462188154459, 'train/mask_loss': 0.5217693746089935, 'metrics/total_secs_per_batch': 8.002936840057373, 'metrics/data_secs_per_batch': 3.038877487182617, '_timestamp': 1740966558.0950599}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 295 is less than current step: 499. Dropping entry: {'train/lr': 2.53469387755102e-05, '_timestamp': 1740966558.0953188}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 296 is less than current step: 499. Dropping entry: {'train/loss': 1.8095873475074769, 'train/ce_loss': 0.315234375, 'train/seg_cls_loss': 0.016162109375, 'train/kl_loss': 0.3279296875, 'train/mask_bce_loss': 0.16350644696503877, 'train/mask_dice_loss': 0.5631133854389191, 'train/mask_loss': 0.726619827747345, 'metrics/total_secs_per_batch': 8.066470623016357, 'metrics/data_secs_per_batch': 3.562902903556824, '_timestamp': 1740966566.1616185}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 296 is less than current step: 499. Dropping entry: {'train/lr': 2.5224489795918363e-05, '_timestamp': 1740966566.1619081}).
Epoch: [4][298/500]	Time  6.405 ( 6.405)	Loss 0.7341 (1.4567)	CeLoss 0.2305 (0.5915)	SegCLSLoss 0.0164 (0.0102)	KLLoss 0.3652 (0.2188)	MaskLoss 0.2293 (0.4190)	MaskBCELoss 0.0973 (0.0448)	MaskDICELoss 0.1321 (0.3742)
Epoch: [4][299/500]	Time  9.041 ( 9.041)	Loss 1.8371 (1.4655)	CeLoss 0.1914 (0.3044)	SegCLSLoss 0.0108 (0.0128)	KLLoss 0.3711 (0.2576)	MaskLoss 0.8014 (0.5644)	MaskBCELoss 0.1577 (0.1129)	MaskDICELoss 0.6437 (0.4515)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 297 is less than current step: 499. Dropping entry: {'train/loss': 1.4566624462604523, 'train/ce_loss': 0.59150390625, 'train/seg_cls_loss': 0.010186767578125, 'train/kl_loss': 0.21875, 'train/mask_bce_loss': 0.044769198447465894, 'train/mask_dice_loss': 0.3742358461022377, 'train/mask_loss': 0.4190050408244133, 'metrics/total_secs_per_batch': 6.404587745666504, 'metrics/data_secs_per_batch': 3.0126111030578615, '_timestamp': 1740966572.5660913}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 297 is less than current step: 499. Dropping entry: {'train/lr': 2.510204081632653e-05, '_timestamp': 1740966572.5663846}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 298 is less than current step: 499. Dropping entry: {'train/loss': 1.465467894077301, 'train/ce_loss': 0.3044189453125, 'train/seg_cls_loss': 0.012799072265625, 'train/kl_loss': 0.2576171875, 'train/mask_bce_loss': 0.11286162994801999, 'train/mask_dice_loss': 0.4515007458627224, 'train/mask_loss': 0.5643623650074006, 'metrics/total_secs_per_batch': 9.041247367858887, 'metrics/data_secs_per_batch': 4.078866124153137, '_timestamp': 1740966581.6073835}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 298 is less than current step: 499. Dropping entry: {'train/lr': 2.4979591836734693e-05, '_timestamp': 1740966581.6076891}).
[2025-03-02 19:49:48,547] [INFO] [logging.py:96:log_dist] [Rank 0] step=2300, skipped=0, lr=[2.491836734693877e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:49:48,553] [INFO] [timer.py:215:stop] epoch=0/micro_step=23000/global_step=2300, RunningAvgSamplesPerSec=1.4567773253268175, CurrSamplesPerSec=1.4398897166987157, MemAllocated=30.7GB, MaxMemAllocated=37.23GB
Epoch: [4][300/500]	Time  6.947 ( 6.947)	Loss 1.4531 (1.6609)	CeLoss 1.4531 (0.5636)	SegCLSLoss 0.0000 (0.0103)	KLLoss 0.0000 (0.2551)	MaskLoss 0.0000 (0.5334)	MaskBCELoss 0.0000 (0.0863)	MaskDICELoss 0.0000 (0.4471)
Epoch: [4][301/500]	Time  5.829 ( 5.829)	Loss 1.9237 (1.0974)	CeLoss 0.2891 (0.6222)	SegCLSLoss 0.0116 (0.0055)	KLLoss 0.3672 (0.1100)	MaskLoss 0.7958 (0.2307)	MaskBCELoss 0.3323 (0.0477)	MaskDICELoss 0.4636 (0.1830)
Epoch: [4][302/500]	Time  7.492 ( 7.492)	Loss 1.5861 (1.7230)	CeLoss 0.3301 (0.5160)	SegCLSLoss 0.0109 (0.0119)	KLLoss 0.3633 (0.2555)	MaskLoss 0.6065 (0.5877)	MaskBCELoss 0.0230 (0.1613)	MaskDICELoss 0.5835 (0.4264)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 299 is less than current step: 499. Dropping entry: {'train/loss': 1.6608559370040894, 'train/ce_loss': 0.56357421875, 'train/seg_cls_loss': 0.010272216796875, 'train/kl_loss': 0.255078125, 'train/mask_bce_loss': 0.08634422854520381, 'train/mask_dice_loss': 0.44706224352121354, 'train/mask_loss': 0.5334064722061157, 'metrics/total_secs_per_batch': 6.9465672969818115, 'metrics/data_secs_per_batch': 2.772360992431641, '_timestamp': 1740966588.553792}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 299 is less than current step: 499. Dropping entry: {'train/lr': 2.4857142857142852e-05, '_timestamp': 1740966588.5541003}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 300 is less than current step: 499. Dropping entry: {'train/loss': 1.097413384914398, 'train/ce_loss': 0.62216796875, 'train/seg_cls_loss': 0.005462646484375, 'train/kl_loss': 0.1099609375, 'train/mask_bce_loss': 0.047735970467329025, 'train/mask_dice_loss': 0.18300197422504424, 'train/mask_loss': 0.23073794841766357, 'metrics/total_secs_per_batch': 5.82876443862915, 'metrics/data_secs_per_batch': 2.7534724473953247, '_timestamp': 1740966594.3827415}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 300 is less than current step: 499. Dropping entry: {'train/lr': 2.473469387755102e-05, '_timestamp': 1740966594.3830147}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 301 is less than current step: 499. Dropping entry: {'train/loss': 1.722995972633362, 'train/ce_loss': 0.515966796875, 'train/seg_cls_loss': 0.011865234375, 'train/kl_loss': 0.25546875, 'train/mask_bce_loss': 0.16131862048059703, 'train/mask_dice_loss': 0.4263756603002548, 'train/mask_loss': 0.587694275379181, 'metrics/total_secs_per_batch': 7.492007493972778, 'metrics/data_secs_per_batch': 3.048945951461792, '_timestamp': 1740966601.8749459}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 301 is less than current step: 499. Dropping entry: {'train/lr': 2.4612244897959182e-05, '_timestamp': 1740966601.8752985}).
Epoch: [4][303/500]	Time 10.478 (10.478)	Loss 1.2879 (1.6211)	CeLoss 0.2734 (0.2434)	SegCLSLoss 0.0156 (0.0147)	KLLoss 0.3691 (0.3652)	MaskLoss 0.4848 (0.6669)	MaskBCELoss 0.1550 (0.1113)	MaskDICELoss 0.3298 (0.5556)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 302 is less than current step: 499. Dropping entry: {'train/loss': 1.6211138010025024, 'train/ce_loss': 0.243359375, 'train/seg_cls_loss': 0.014691162109375, 'train/kl_loss': 0.365234375, 'train/mask_bce_loss': 0.11127145402133465, 'train/mask_dice_loss': 0.5556331068277359, 'train/mask_loss': 0.666904553771019, 'metrics/total_secs_per_batch': 10.478227853775024, 'metrics/data_secs_per_batch': 4.656612396240234, '_timestamp': 1740966612.353022}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 302 is less than current step: 499. Dropping entry: {'train/lr': 2.448979591836734e-05, '_timestamp': 1740966612.353308}).
Epoch: [4][304/500]	Time  8.090 ( 8.090)	Loss 1.0075 (1.7673)	CeLoss 0.2793 (0.3749)	SegCLSLoss 0.0133 (0.0142)	KLLoss 0.3672 (0.2918)	MaskLoss 0.3426 (0.6780)	MaskBCELoss 0.0568 (0.1467)	MaskDICELoss 0.2859 (0.5312)
Epoch: [4][305/500]	Time  8.790 ( 8.790)	Loss 2.2462 (1.5222)	CeLoss 0.2393 (0.2785)	SegCLSLoss 0.0166 (0.0120)	KLLoss 0.3613 (0.2893)	MaskLoss 0.9805 (0.6043)	MaskBCELoss 0.1543 (0.1524)	MaskDICELoss 0.8262 (0.4518)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 303 is less than current step: 499. Dropping entry: {'train/loss': 1.7673186659812927, 'train/ce_loss': 0.37490234375, 'train/seg_cls_loss': 0.01424560546875, 'train/kl_loss': 0.291796875, 'train/mask_bce_loss': 0.1467471462674439, 'train/mask_dice_loss': 0.5312481194734573, 'train/mask_loss': 0.6779952675104142, 'metrics/total_secs_per_batch': 8.090317726135254, 'metrics/data_secs_per_batch': 3.4655406713485717, '_timestamp': 1740966620.443299}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 303 is less than current step: 499. Dropping entry: {'train/lr': 2.4367346938775508e-05, '_timestamp': 1740966620.4435604}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 304 is less than current step: 499. Dropping entry: {'train/loss': 1.5222314476966858, 'train/ce_loss': 0.278515625, 'train/seg_cls_loss': 0.011981201171875, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.15244160033762455, 'train/mask_dice_loss': 0.45183818638324735, 'train/mask_loss': 0.6042797863483429, 'metrics/total_secs_per_batch': 8.790143251419067, 'metrics/data_secs_per_batch': 4.080155754089356, '_timestamp': 1740966629.2334316}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 304 is less than current step: 499. Dropping entry: {'train/lr': 2.424489795918367e-05, '_timestamp': 1740966629.2337062}).
Epoch: [4][306/500]	Time  7.746 ( 7.746)	Loss 2.0910 (1.6325)	CeLoss 0.2373 (0.3134)	SegCLSLoss 0.0147 (0.0147)	KLLoss 0.3633 (0.2924)	MaskLoss 0.9049 (0.6412)	MaskBCELoss 0.0273 (0.1346)	MaskDICELoss 0.8776 (0.5066)
Epoch: [4][307/500]	Time  5.916 ( 5.916)	Loss 2.1589 (1.7617)	CeLoss 0.2002 (0.5568)	SegCLSLoss 0.0259 (0.0139)	KLLoss 0.3613 (0.2150)	MaskLoss 0.9549 (0.5883)	MaskBCELoss 0.0294 (0.1069)	MaskDICELoss 0.9255 (0.4813)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 305 is less than current step: 499. Dropping entry: {'train/loss': 1.63254075050354, 'train/ce_loss': 0.31337890625, 'train/seg_cls_loss': 0.01466064453125, 'train/kl_loss': 0.2923828125, 'train/mask_bce_loss': 0.13457730379886926, 'train/mask_dice_loss': 0.5066442459821701, 'train/mask_loss': 0.64122154712677, 'metrics/total_secs_per_batch': 7.746170282363892, 'metrics/data_secs_per_batch': 3.459312105178833, '_timestamp': 1740966636.9796026}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 305 is less than current step: 499. Dropping entry: {'train/lr': 2.4122448979591838e-05, '_timestamp': 1740966636.979879}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 306 is less than current step: 499. Dropping entry: {'train/loss': 1.761675977706909, 'train/ce_loss': 0.5568359375, 'train/seg_cls_loss': 0.01387939453125, 'train/kl_loss': 0.2150390625, 'train/mask_bce_loss': 0.10693763606250287, 'train/mask_dice_loss': 0.48132222294807436, 'train/mask_loss': 0.5882598578929901, 'metrics/total_secs_per_batch': 5.916063070297241, 'metrics/data_secs_per_batch': 2.6884671449661255, '_timestamp': 1740966642.8956666}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 306 is less than current step: 499. Dropping entry: {'train/lr': 2.3999999999999997e-05, '_timestamp': 1740966642.895934}).
Epoch: [4][308/500]	Time  7.133 ( 7.133)	Loss 2.2642 (1.4550)	CeLoss 0.2324 (0.5592)	SegCLSLoss 0.0273 (0.0103)	KLLoss 0.3516 (0.1783)	MaskLoss 0.9915 (0.4364)	MaskBCELoss 0.0803 (0.0716)	MaskDICELoss 0.9112 (0.3648)
Epoch: [4][309/500]	Time  6.522 ( 6.522)	Loss 1.1328 (1.2648)	CeLoss 1.1328 (0.7116)	SegCLSLoss 0.0000 (0.0056)	KLLoss 0.0000 (0.1092)	MaskLoss 0.0000 (0.2698)	MaskBCELoss 0.0000 (0.0178)	MaskDICELoss 0.0000 (0.2520)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 307 is less than current step: 499. Dropping entry: {'train/loss': 1.4550290942192077, 'train/ce_loss': 0.5591552734375, 'train/seg_cls_loss': 0.010302734375, 'train/kl_loss': 0.1783203125, 'train/mask_bce_loss': 0.07162668220698834, 'train/mask_dice_loss': 0.36478679776191714, 'train/mask_loss': 0.43641347885131837, 'metrics/total_secs_per_batch': 7.133367300033569, 'metrics/data_secs_per_batch': 2.924552869796753, '_timestamp': 1740966650.029014}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 307 is less than current step: 499. Dropping entry: {'train/lr': 2.387755102040816e-05, '_timestamp': 1740966650.0291986}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 308 is less than current step: 499. Dropping entry: {'train/loss': 1.2647696375846862, 'train/ce_loss': 0.71162109375, 'train/seg_cls_loss': 0.005633544921875, 'train/kl_loss': 0.1091796875, 'train/mask_bce_loss': 0.01780643064994365, 'train/mask_dice_loss': 0.25198073387145997, 'train/mask_loss': 0.26978716254234314, 'metrics/total_secs_per_batch': 6.521741628646851, 'metrics/data_secs_per_batch': 3.509044647216797, '_timestamp': 1740966656.551071}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 308 is less than current step: 499. Dropping entry: {'train/lr': 2.3755102040816327e-05, '_timestamp': 1740966656.5514452}).
[2025-03-02 19:51:04,923] [INFO] [logging.py:96:log_dist] [Rank 0] step=2310, skipped=0, lr=[2.3693877551020408e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:51:04,929] [INFO] [timer.py:215:stop] epoch=0/micro_step=23100/global_step=2310, RunningAvgSamplesPerSec=1.4560678522112727, CurrSamplesPerSec=1.1936419976715138, MemAllocated=30.93GB, MaxMemAllocated=37.23GB
Epoch: [4][310/500]	Time  8.380 ( 8.380)	Loss 0.9295 (1.7782)	CeLoss 0.3750 (0.3727)	SegCLSLoss 0.0096 (0.0167)	KLLoss 0.3613 (0.3279)	MaskLoss 0.2577 (0.6822)	MaskBCELoss 0.0577 (0.1067)	MaskDICELoss 0.2000 (0.5756)
Epoch: [4][311/500]	Time  8.479 ( 8.479)	Loss 1.6968 (2.1386)	CeLoss 0.1768 (0.3680)	SegCLSLoss 0.0195 (0.0170)	KLLoss 0.3555 (0.3258)	MaskLoss 0.7371 (0.8647)	MaskBCELoss 0.0681 (0.2019)	MaskDICELoss 0.6690 (0.6628)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 309 is less than current step: 499. Dropping entry: {'train/loss': 1.7781688094139099, 'train/ce_loss': 0.37265625, 'train/seg_cls_loss': 0.016693115234375, 'train/kl_loss': 0.3279296875, 'train/mask_bce_loss': 0.1066739372909069, 'train/mask_dice_loss': 0.57557452917099, 'train/mask_loss': 0.682248467206955, 'metrics/total_secs_per_batch': 8.379615545272827, 'metrics/data_secs_per_batch': 3.3349773645401, '_timestamp': 1740966664.930206}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 309 is less than current step: 499. Dropping entry: {'train/lr': 2.3632653061224486e-05, '_timestamp': 1740966664.930386}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 310 is less than current step: 499. Dropping entry: {'train/loss': 2.138589286804199, 'train/ce_loss': 0.36796875, 'train/seg_cls_loss': 0.017034912109375, 'train/kl_loss': 0.32578125, 'train/mask_bce_loss': 0.2019227333366871, 'train/mask_dice_loss': 0.6627820640802383, 'train/mask_loss': 0.8647048115730286, 'metrics/total_secs_per_batch': 8.479153394699097, 'metrics/data_secs_per_batch': 3.8006017208099365, '_timestamp': 1740966673.4095576}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 310 is less than current step: 499. Dropping entry: {'train/lr': 2.351020408163265e-05, '_timestamp': 1740966673.4098558}).
Epoch: [4][312/500]	Time  8.515 ( 8.515)	Loss 0.9258 (1.8286)	CeLoss 0.9258 (0.4142)	SegCLSLoss 0.0000 (0.0140)	KLLoss 0.0000 (0.2906)	MaskLoss 0.0000 (0.6892)	MaskBCELoss 0.0000 (0.2596)	MaskDICELoss 0.0000 (0.4296)
Epoch: [4][313/500]	Time  7.979 ( 7.979)	Loss 1.9011 (1.5981)	CeLoss 0.2812 (0.3030)	SegCLSLoss 0.0187 (0.0116)	KLLoss 0.3555 (0.2908)	MaskLoss 0.7875 (0.6301)	MaskBCELoss 0.0297 (0.1020)	MaskDICELoss 0.7577 (0.5280)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 311 is less than current step: 499. Dropping entry: {'train/loss': 1.8286411821842194, 'train/ce_loss': 0.41416015625, 'train/seg_cls_loss': 0.014013671875, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.259628102555871, 'train/mask_dice_loss': 0.42959484457969666, 'train/mask_loss': 0.6892229318618774, 'metrics/total_secs_per_batch': 8.515031814575195, 'metrics/data_secs_per_batch': 4.208688521385193, '_timestamp': 1740966681.9245894}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 311 is less than current step: 499. Dropping entry: {'train/lr': 2.3387755102040816e-05, '_timestamp': 1740966681.9247725}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 312 is less than current step: 499. Dropping entry: {'train/loss': 1.598146414756775, 'train/ce_loss': 0.30302734375, 'train/seg_cls_loss': 0.01162109375, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.10203319359570742, 'train/mask_dice_loss': 0.528045865893364, 'train/mask_loss': 0.6300790548324585, 'metrics/total_secs_per_batch': 7.978938102722168, 'metrics/data_secs_per_batch': 3.466505789756775, '_timestamp': 1740966689.903531}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 312 is less than current step: 499. Dropping entry: {'train/lr': 2.326530612244898e-05, '_timestamp': 1740966689.9038448}).
Epoch: [4][314/500]	Time  8.189 ( 8.189)	Loss 2.2595 (2.0386)	CeLoss 0.2422 (0.3491)	SegCLSLoss 0.0278 (0.0167)	KLLoss 0.3535 (0.3285)	MaskLoss 0.9842 (0.8242)	MaskBCELoss 0.0085 (0.0819)	MaskDICELoss 0.9757 (0.7422)
Epoch: [4][315/500]	Time  8.250 ( 8.250)	Loss 2.2796 (2.0603)	CeLoss 0.2148 (0.1882)	SegCLSLoss 0.0198 (0.0230)	KLLoss 0.3652 (0.3676)	MaskLoss 1.0090 (0.9119)	MaskBCELoss 0.1327 (0.2244)	MaskDICELoss 0.8762 (0.6874)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 313 is less than current step: 499. Dropping entry: {'train/loss': 2.038553762435913, 'train/ce_loss': 0.34912109375, 'train/seg_cls_loss': 0.01666259765625, 'train/kl_loss': 0.328515625, 'train/mask_bce_loss': 0.08191598979756236, 'train/mask_dice_loss': 0.7422437191009521, 'train/mask_loss': 0.82415971159935, 'metrics/total_secs_per_batch': 8.189064979553223, 'metrics/data_secs_per_batch': 3.504968047142029, '_timestamp': 1740966698.0926046}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 313 is less than current step: 499. Dropping entry: {'train/lr': 2.314285714285714e-05, '_timestamp': 1740966698.0928795}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 314 is less than current step: 499. Dropping entry: {'train/loss': 2.0603028416633604, 'train/ce_loss': 0.18818359375, 'train/seg_cls_loss': 0.023046875, 'train/kl_loss': 0.367578125, 'train/mask_bce_loss': 0.22444735020399093, 'train/mask_dice_loss': 0.6874423652887345, 'train/mask_loss': 0.9118897080421448, 'metrics/total_secs_per_batch': 8.250165700912476, 'metrics/data_secs_per_batch': 3.8052699327468873, '_timestamp': 1740966706.3427963}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 314 is less than current step: 499. Dropping entry: {'train/lr': 2.3020408163265305e-05, '_timestamp': 1740966706.3430612}).
Epoch: [4][316/500]	Time  6.824 ( 6.824)	Loss 1.0947 (1.6167)	CeLoss 0.2520 (0.6117)	SegCLSLoss 0.0126 (0.0108)	KLLoss 0.3633 (0.2168)	MaskLoss 0.3999 (0.4889)	MaskBCELoss 0.1720 (0.1268)	MaskDICELoss 0.2279 (0.3621)
Epoch: [4][317/500]	Time  7.805 ( 7.805)	Loss 1.0078 (1.4459)	CeLoss 1.0078 (0.3099)	SegCLSLoss 0.0000 (0.0122)	KLLoss 0.0000 (0.3303)	MaskLoss 0.0000 (0.5485)	MaskBCELoss 0.0000 (0.1299)	MaskDICELoss 0.0000 (0.4186)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 315 is less than current step: 499. Dropping entry: {'train/loss': 1.6167465925216675, 'train/ce_loss': 0.61171875, 'train/seg_cls_loss': 0.0108154296875, 'train/kl_loss': 0.216796875, 'train/mask_bce_loss': 0.12682995665818453, 'train/mask_dice_loss': 0.36210974156856535, 'train/mask_loss': 0.4889396965503693, 'metrics/total_secs_per_batch': 6.824019193649292, 'metrics/data_secs_per_batch': 2.6586567878723146, '_timestamp': 1740966713.16689}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 315 is less than current step: 499. Dropping entry: {'train/lr': 2.2897959183673468e-05, '_timestamp': 1740966713.1672401}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 316 is less than current step: 499. Dropping entry: {'train/loss': 1.445940613746643, 'train/ce_loss': 0.30986328125, 'train/seg_cls_loss': 0.012213134765625, 'train/kl_loss': 0.3302734375, 'train/mask_bce_loss': 0.12985989712178708, 'train/mask_dice_loss': 0.4186475336551666, 'train/mask_loss': 0.5485074147582054, 'metrics/total_secs_per_batch': 7.804632902145386, 'metrics/data_secs_per_batch': 3.276120591163635, '_timestamp': 1740966720.9717207}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 316 is less than current step: 499. Dropping entry: {'train/lr': 2.2775510204081628e-05, '_timestamp': 1740966720.9720802}).
Epoch: [4][318/500]	Time  7.870 ( 7.870)	Loss 3.0777 (1.9423)	CeLoss 0.1816 (0.3184)	SegCLSLoss 0.0266 (0.0141)	KLLoss 0.3633 (0.2938)	MaskLoss 1.4231 (0.7939)	MaskBCELoss 0.6081 (0.2140)	MaskDICELoss 0.8150 (0.5798)
Epoch: [4][319/500]	Time  7.436 ( 7.436)	Loss 1.3831 (1.6992)	CeLoss 0.2432 (0.4590)	SegCLSLoss 0.0131 (0.0140)	KLLoss 0.3633 (0.2566)	MaskLoss 0.5480 (0.6037)	MaskBCELoss 0.0702 (0.1541)	MaskDICELoss 0.4778 (0.4497)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 317 is less than current step: 499. Dropping entry: {'train/loss': 1.9423383951187134, 'train/ce_loss': 0.318359375, 'train/seg_cls_loss': 0.01412353515625, 'train/kl_loss': 0.29375, 'train/mask_bce_loss': 0.2140480915084481, 'train/mask_dice_loss': 0.5798261761665344, 'train/mask_loss': 0.7938742756843566, 'metrics/total_secs_per_batch': 7.8698203563690186, 'metrics/data_secs_per_batch': 3.7522091388702394, '_timestamp': 1740966728.8412457}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 317 is less than current step: 499. Dropping entry: {'train/lr': 2.2653061224489794e-05, '_timestamp': 1740966728.8414981}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 318 is less than current step: 499. Dropping entry: {'train/loss': 1.6991566240787506, 'train/ce_loss': 0.458984375, 'train/seg_cls_loss': 0.01396484375, 'train/kl_loss': 0.256640625, 'train/mask_bce_loss': 0.1540640376508236, 'train/mask_dice_loss': 0.44966465011239054, 'train/mask_loss': 0.6037287086248397, 'metrics/total_secs_per_batch': 7.436487913131714, 'metrics/data_secs_per_batch': 3.2069879531860352, '_timestamp': 1740966736.2777202}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 318 is less than current step: 499. Dropping entry: {'train/lr': 2.2530612244897957e-05, '_timestamp': 1740966736.2779932}).
[2025-03-02 19:52:24,321] [INFO] [logging.py:96:log_dist] [Rank 0] step=2320, skipped=0, lr=[2.246938775510204e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:52:24,327] [INFO] [timer.py:215:stop] epoch=0/micro_step=23200/global_step=2320, RunningAvgSamplesPerSec=1.455089142204892, CurrSamplesPerSec=1.2424088294563795, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [4][320/500]	Time  8.050 ( 8.050)	Loss 1.9876 (1.6487)	CeLoss 0.2412 (0.4871)	SegCLSLoss 0.0118 (0.0125)	KLLoss 0.3691 (0.2178)	MaskLoss 0.8522 (0.5669)	MaskBCELoss 0.0936 (0.1656)	MaskDICELoss 0.7587 (0.4012)
Epoch: [4][321/500]	Time  8.442 ( 8.442)	Loss 2.1536 (1.9714)	CeLoss 0.2656 (0.3203)	SegCLSLoss 0.0113 (0.0148)	KLLoss 0.3711 (0.3322)	MaskLoss 0.9225 (0.8051)	MaskBCELoss 0.1435 (0.1464)	MaskDICELoss 0.7791 (0.6587)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 319 is less than current step: 499. Dropping entry: {'train/loss': 1.6487454056739808, 'train/ce_loss': 0.487109375, 'train/seg_cls_loss': 0.012518310546875, 'train/kl_loss': 0.2177734375, 'train/mask_bce_loss': 0.16560686118900775, 'train/mask_dice_loss': 0.40124629735946654, 'train/mask_loss': 0.5668531715869903, 'metrics/total_secs_per_batch': 8.050350666046143, 'metrics/data_secs_per_batch': 4.08769588470459, '_timestamp': 1740966744.3279104}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 319 is less than current step: 499. Dropping entry: {'train/lr': 2.2408163265306117e-05, '_timestamp': 1740966744.328181}).
Epoch: [4][322/500]	Time  7.602 ( 7.602)	Loss 1.1017 (1.7203)	CeLoss 0.2041 (0.3010)	SegCLSLoss 0.0175 (0.0146)	KLLoss 0.3633 (0.2918)	MaskLoss 0.4263 (0.6913)	MaskBCELoss 0.0020 (0.1196)	MaskDICELoss 0.4244 (0.5717)
Epoch: [4][323/500]	Time  7.170 ( 7.170)	Loss 2.4944 (1.7543)	CeLoss 0.1895 (0.3837)	SegCLSLoss 0.0198 (0.0137)	KLLoss 0.3809 (0.2969)	MaskLoss 1.1286 (0.6670)	MaskBCELoss 0.2393 (0.1368)	MaskDICELoss 0.8892 (0.5302)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 320 is less than current step: 499. Dropping entry: {'train/loss': 1.9714272141456604, 'train/ce_loss': 0.3203125, 'train/seg_cls_loss': 0.014837646484375, 'train/kl_loss': 0.3322265625, 'train/mask_bce_loss': 0.14643341060727835, 'train/mask_dice_loss': 0.6587138116359711, 'train/mask_loss': 0.8051472127437591, 'metrics/total_secs_per_batch': 8.442123651504517, 'metrics/data_secs_per_batch': 3.7415347337722777, '_timestamp': 1740966752.7702482}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 320 is less than current step: 499. Dropping entry: {'train/lr': 2.2285714285714283e-05, '_timestamp': 1740966752.77052}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 321 is less than current step: 499. Dropping entry: {'train/loss': 1.7203131794929505, 'train/ce_loss': 0.301025390625, 'train/seg_cls_loss': 0.0146484375, 'train/kl_loss': 0.291796875, 'train/mask_bce_loss': 0.11958720639813691, 'train/mask_dice_loss': 0.5716973125934601, 'train/mask_loss': 0.691284516453743, 'metrics/total_secs_per_batch': 7.601938724517822, 'metrics/data_secs_per_batch': 3.64109046459198, '_timestamp': 1740966760.3721497}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 321 is less than current step: 499. Dropping entry: {'train/lr': 2.2163265306122446e-05, '_timestamp': 1740966760.3724108}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 322 is less than current step: 499. Dropping entry: {'train/loss': 1.7543348312377929, 'train/ce_loss': 0.38369140625, 'train/seg_cls_loss': 0.0136962890625, 'train/kl_loss': 0.296875, 'train/mask_bce_loss': 0.13676779121160507, 'train/mask_dice_loss': 0.5301945298910141, 'train/mask_loss': 0.6669623136520386, 'metrics/total_secs_per_batch': 7.169513702392578, 'metrics/data_secs_per_batch': 3.233396792411804, '_timestamp': 1740966767.541693}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 322 is less than current step: 499. Dropping entry: {'train/lr': 2.2040816326530613e-05, '_timestamp': 1740966767.5419734}).
Epoch: [4][324/500]	Time  7.343 ( 7.343)	Loss 1.4219 (1.2207)	CeLoss 1.4219 (0.5116)	SegCLSLoss 0.0000 (0.0076)	KLLoss 0.0000 (0.1430)	MaskLoss 0.0000 (0.3455)	MaskBCELoss 0.0000 (0.0217)	MaskDICELoss 0.0000 (0.3238)
Epoch: [4][325/500]	Time  8.463 ( 8.463)	Loss 2.1391 (1.8478)	CeLoss 0.2012 (0.2600)	SegCLSLoss 0.0211 (0.0177)	KLLoss 0.3555 (0.2926)	MaskLoss 0.9455 (0.7749)	MaskBCELoss 0.0881 (0.2211)	MaskDICELoss 0.8574 (0.5538)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 323 is less than current step: 499. Dropping entry: {'train/loss': 1.2207024455070496, 'train/ce_loss': 0.51162109375, 'train/seg_cls_loss': 0.00762939453125, 'train/kl_loss': 0.14296875, 'train/mask_bce_loss': 0.021666854061186312, 'train/mask_dice_loss': 0.3237917900085449, 'train/mask_loss': 0.34545864462852477, 'metrics/total_secs_per_batch': 7.3433144092559814, 'metrics/data_secs_per_batch': 3.3042165279388427, '_timestamp': 1740966774.8852806}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 323 is less than current step: 499. Dropping entry: {'train/lr': 2.1918367346938773e-05, '_timestamp': 1740966774.885663}).
Epoch: [4][326/500]	Time  5.841 ( 5.841)	Loss 1.2266 (1.6128)	CeLoss 1.2266 (0.6654)	SegCLSLoss 0.0000 (0.0084)	KLLoss 0.0000 (0.1846)	MaskLoss 0.0000 (0.4623)	MaskBCELoss 0.0000 (0.0869)	MaskDICELoss 0.0000 (0.3754)
Epoch: [4][327/500]	Time  8.458 ( 8.458)	Loss 2.3016 (2.0271)	CeLoss 0.2852 (0.2439)	SegCLSLoss 0.0240 (0.0162)	KLLoss 0.3516 (0.3645)	MaskLoss 0.9848 (0.8693)	MaskBCELoss 0.6677 (0.2514)	MaskDICELoss 0.3171 (0.6179)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 324 is less than current step: 499. Dropping entry: {'train/loss': 1.8477871656417846, 'train/ce_loss': 0.260009765625, 'train/seg_cls_loss': 0.01766357421875, 'train/kl_loss': 0.292578125, 'train/mask_bce_loss': 0.2211139366030693, 'train/mask_dice_loss': 0.5538294434547424, 'train/mask_loss': 0.7749433845281601, 'metrics/total_secs_per_batch': 8.462565660476685, 'metrics/data_secs_per_batch': 3.5369573831558228, '_timestamp': 1740966783.3475935}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 324 is less than current step: 499. Dropping entry: {'train/lr': 2.1795918367346936e-05, '_timestamp': 1740966783.3479111}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 325 is less than current step: 499. Dropping entry: {'train/loss': 1.6128247380256653, 'train/ce_loss': 0.6654296875, 'train/seg_cls_loss': 0.0084228515625, 'train/kl_loss': 0.1845703125, 'train/mask_bce_loss': 0.08685579942539334, 'train/mask_dice_loss': 0.3754159390926361, 'train/mask_loss': 0.46227173805236815, 'metrics/total_secs_per_batch': 5.8408238887786865, 'metrics/data_secs_per_batch': 2.6519379138946535, '_timestamp': 1740966789.1885803}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 325 is less than current step: 499. Dropping entry: {'train/lr': 2.1673469387755102e-05, '_timestamp': 1740966789.1889048}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 326 is less than current step: 499. Dropping entry: {'train/loss': 2.027066206932068, 'train/ce_loss': 0.2439453125, 'train/seg_cls_loss': 0.016204833984375, 'train/kl_loss': 0.364453125, 'train/mask_bce_loss': 0.25143151767551897, 'train/mask_dice_loss': 0.6179121404886245, 'train/mask_loss': 0.8693436443805694, 'metrics/total_secs_per_batch': 8.457833766937256, 'metrics/data_secs_per_batch': 3.8453779220581055, '_timestamp': 1740966797.6462529}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 326 is less than current step: 499. Dropping entry: {'train/lr': 2.1551020408163265e-05, '_timestamp': 1740966797.6465378}).
Epoch: [4][328/500]	Time  9.772 ( 9.772)	Loss 1.4254 (1.6059)	CeLoss 0.2109 (0.2288)	SegCLSLoss 0.0151 (0.0136)	KLLoss 0.3711 (0.3260)	MaskLoss 0.5848 (0.6688)	MaskBCELoss 0.1239 (0.1103)	MaskDICELoss 0.4608 (0.5585)
Epoch: [4][329/500]	Time  6.074 ( 6.074)	Loss 1.3828 (1.3491)	CeLoss 1.3828 (0.7264)	SegCLSLoss 0.0000 (0.0077)	KLLoss 0.0000 (0.1482)	MaskLoss 0.0000 (0.3020)	MaskBCELoss 0.0000 (0.0447)	MaskDICELoss 0.0000 (0.2573)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 327 is less than current step: 499. Dropping entry: {'train/loss': 1.6059066772460937, 'train/ce_loss': 0.22880859375, 'train/seg_cls_loss': 0.01356201171875, 'train/kl_loss': 0.3259765625, 'train/mask_bce_loss': 0.11028125314041973, 'train/mask_dice_loss': 0.5584923982620239, 'train/mask_loss': 0.6687736555933952, 'metrics/total_secs_per_batch': 9.772009134292603, 'metrics/data_secs_per_batch': 4.2877370595932005, '_timestamp': 1740966807.4185386}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 327 is less than current step: 499. Dropping entry: {'train/lr': 2.1428571428571425e-05, '_timestamp': 1740966807.418914}).
[2025-03-02 19:53:41,576] [INFO] [logging.py:96:log_dist] [Rank 0] step=2330, skipped=0, lr=[2.1244897959183673e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:53:41,583] [INFO] [timer.py:215:stop] epoch=0/micro_step=23300/global_step=2330, RunningAvgSamplesPerSec=1.4543147682882858, CurrSamplesPerSec=1.2360941019686038, MemAllocated=30.8GB, MaxMemAllocated=37.23GB
Epoch: [4][330/500]	Time  8.091 ( 8.091)	Loss 1.4778 (1.4727)	CeLoss 0.3105 (0.3142)	SegCLSLoss 0.0199 (0.0134)	KLLoss 0.3594 (0.3283)	MaskLoss 0.5611 (0.5594)	MaskBCELoss 0.1027 (0.1475)	MaskDICELoss 0.4584 (0.4120)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 328 is less than current step: 499. Dropping entry: {'train/loss': 1.349133837223053, 'train/ce_loss': 0.7263671875, 'train/seg_cls_loss': 0.0077392578125, 'train/kl_loss': 0.1482421875, 'train/mask_bce_loss': 0.04472280517220497, 'train/mask_dice_loss': 0.25728552639484403, 'train/mask_loss': 0.3020083248615265, 'metrics/total_secs_per_batch': 6.074164152145386, 'metrics/data_secs_per_batch': 2.7077900171279907, '_timestamp': 1740966813.492445}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 328 is less than current step: 499. Dropping entry: {'train/lr': 2.130612244897959e-05, '_timestamp': 1740966813.4926336}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 329 is less than current step: 499. Dropping entry: {'train/loss': 1.472674262523651, 'train/ce_loss': 0.31416015625, 'train/seg_cls_loss': 0.013427734375, 'train/kl_loss': 0.3283203125, 'train/mask_bce_loss': 0.14745167791843414, 'train/mask_dice_loss': 0.41198116838932036, 'train/mask_loss': 0.5594328343868256, 'metrics/total_secs_per_batch': 8.091496467590332, 'metrics/data_secs_per_batch': 4.0443048000335695, '_timestamp': 1740966821.5837317}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 329 is less than current step: 499. Dropping entry: {'train/lr': 2.1183673469387754e-05, '_timestamp': 1740966821.5839956}).
Epoch: [4][331/500]	Time  8.114 ( 8.114)	Loss 1.0135 (1.9103)	CeLoss 0.2324 (0.2189)	SegCLSLoss 0.0129 (0.0171)	KLLoss 0.3652 (0.3645)	MaskLoss 0.3690 (0.8232)	MaskBCELoss 0.0424 (0.1691)	MaskDICELoss 0.3266 (0.6542)
Epoch: [4][332/500]	Time  7.025 ( 7.025)	Loss 1.7814 (1.3839)	CeLoss 0.1885 (0.4343)	SegCLSLoss 0.0238 (0.0093)	KLLoss 0.3535 (0.2193)	MaskLoss 0.7730 (0.4616)	MaskBCELoss 0.0756 (0.0565)	MaskDICELoss 0.6974 (0.4051)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 330 is less than current step: 499. Dropping entry: {'train/loss': 1.9103439688682555, 'train/ce_loss': 0.2189453125, 'train/seg_cls_loss': 0.017083740234375, 'train/kl_loss': 0.364453125, 'train/mask_bce_loss': 0.1690760124474764, 'train/mask_dice_loss': 0.6541623756289482, 'train/mask_loss': 0.8232383996248245, 'metrics/total_secs_per_batch': 8.113876104354858, 'metrics/data_secs_per_batch': 3.4966551780700685, '_timestamp': 1740966829.6980557}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 330 is less than current step: 499. Dropping entry: {'train/lr': 2.1061224489795914e-05, '_timestamp': 1740966829.698425}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 331 is less than current step: 499. Dropping entry: {'train/loss': 1.3839244604110719, 'train/ce_loss': 0.43427734375, 'train/seg_cls_loss': 0.00931396484375, 'train/kl_loss': 0.2193359375, 'train/mask_bce_loss': 0.05649924520403147, 'train/mask_dice_loss': 0.40514073371887205, 'train/mask_loss': 0.4616399765014648, 'metrics/total_secs_per_batch': 7.024744510650635, 'metrics/data_secs_per_batch': 2.946588373184204, '_timestamp': 1740966836.7226017}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 331 is less than current step: 499. Dropping entry: {'train/lr': 2.093877551020408e-05, '_timestamp': 1740966836.722885}).
Epoch: [4][333/500]	Time  9.198 ( 9.198)	Loss 0.1235 (1.7963)	CeLoss 0.1235 (0.2383)	SegCLSLoss 0.0000 (0.0119)	KLLoss 0.0000 (0.2932)	MaskLoss 0.0000 (0.7614)	MaskBCELoss 0.0000 (0.1873)	MaskDICELoss 0.0000 (0.5741)
Epoch: [4][334/500]	Time  9.377 ( 9.377)	Loss 1.7204 (1.5488)	CeLoss 0.2520 (0.2537)	SegCLSLoss 0.0219 (0.0135)	KLLoss 0.3613 (0.3254)	MaskLoss 0.7098 (0.6278)	MaskBCELoss 0.0432 (0.1131)	MaskDICELoss 0.6667 (0.5146)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 332 is less than current step: 499. Dropping entry: {'train/loss': 1.7962869644165038, 'train/ce_loss': 0.238330078125, 'train/seg_cls_loss': 0.011920166015625, 'train/kl_loss': 0.2931640625, 'train/mask_bce_loss': 0.18733159340918065, 'train/mask_dice_loss': 0.5741175413131714, 'train/mask_loss': 0.761449134349823, 'metrics/total_secs_per_batch': 9.198347091674805, 'metrics/data_secs_per_batch': 4.028960156440735, '_timestamp': 1740966845.9208744}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 332 is less than current step: 499. Dropping entry: {'train/lr': 2.0816326530612243e-05, '_timestamp': 1740966845.9211428}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 333 is less than current step: 499. Dropping entry: {'train/loss': 1.54880530834198, 'train/ce_loss': 0.2537109375, 'train/seg_cls_loss': 0.0135009765625, 'train/kl_loss': 0.325390625, 'train/mask_bce_loss': 0.11314813159406185, 'train/mask_dice_loss': 0.5146236717700958, 'train/mask_loss': 0.6277718007564544, 'metrics/total_secs_per_batch': 9.376821994781494, 'metrics/data_secs_per_batch': 3.893796110153198, '_timestamp': 1740966855.2977026}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 333 is less than current step: 499. Dropping entry: {'train/lr': 2.0693877551020403e-05, '_timestamp': 1740966855.2979708}).
Epoch: [4][335/500]	Time  6.620 ( 6.620)	Loss 1.1641 (1.7055)	CeLoss 1.1641 (0.6442)	SegCLSLoss 0.0000 (0.0103)	KLLoss 0.0000 (0.2176)	MaskLoss 0.0000 (0.5172)	MaskBCELoss 0.0000 (0.0877)	MaskDICELoss 0.0000 (0.4295)
Epoch: [4][336/500]	Time  7.957 ( 7.957)	Loss 1.6005 (1.9214)	CeLoss 0.1533 (0.3562)	SegCLSLoss 0.0327 (0.0199)	KLLoss 0.3594 (0.3287)	MaskLoss 0.6972 (0.7611)	MaskBCELoss 0.0379 (0.1806)	MaskDICELoss 0.6593 (0.5806)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 334 is less than current step: 499. Dropping entry: {'train/loss': 1.7055156230926514, 'train/ce_loss': 0.64423828125, 'train/seg_cls_loss': 0.010260009765625, 'train/kl_loss': 0.217578125, 'train/mask_bce_loss': 0.08768003396689891, 'train/mask_dice_loss': 0.42948207855224607, 'train/mask_loss': 0.5171621084213257, 'metrics/total_secs_per_batch': 6.620151519775391, 'metrics/data_secs_per_batch': 3.043010377883911, '_timestamp': 1740966861.9181585}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 334 is less than current step: 499. Dropping entry: {'train/lr': 2.057142857142857e-05, '_timestamp': 1740966861.9184003}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 335 is less than current step: 499. Dropping entry: {'train/loss': 1.9213727951049804, 'train/ce_loss': 0.356201171875, 'train/seg_cls_loss': 0.01986083984375, 'train/kl_loss': 0.3287109375, 'train/mask_bce_loss': 0.18057181346230208, 'train/mask_dice_loss': 0.5805540233850479, 'train/mask_loss': 0.7611258387565613, 'metrics/total_secs_per_batch': 7.95654296875, 'metrics/data_secs_per_batch': 3.2088441133499144, '_timestamp': 1740966869.874496}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 335 is less than current step: 499. Dropping entry: {'train/lr': 2.0448979591836733e-05, '_timestamp': 1740966869.8748038}).
Epoch: [4][337/500]	Time  8.058 ( 8.058)	Loss 2.1232 (1.7033)	CeLoss 0.3398 (0.4296)	SegCLSLoss 0.0115 (0.0128)	KLLoss 0.3652 (0.2906)	MaskLoss 0.8702 (0.6189)	MaskBCELoss 0.1819 (0.1340)	MaskDICELoss 0.6883 (0.4849)
Epoch: [4][338/500]	Time  8.971 ( 8.971)	Loss 1.1094 (1.5639)	CeLoss 1.1094 (0.4920)	SegCLSLoss 0.0000 (0.0111)	KLLoss 0.0000 (0.2570)	MaskLoss 0.0000 (0.5204)	MaskBCELoss 0.0000 (0.1463)	MaskDICELoss 0.0000 (0.3741)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 336 is less than current step: 499. Dropping entry: {'train/loss': 1.7033130288124085, 'train/ce_loss': 0.42958984375, 'train/seg_cls_loss': 0.01280517578125, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.13396815881133078, 'train/mask_dice_loss': 0.48492470681667327, 'train/mask_loss': 0.618892851471901, 'metrics/total_secs_per_batch': 8.058481216430664, 'metrics/data_secs_per_batch': 3.57908353805542, '_timestamp': 1740966877.934059}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 336 is less than current step: 499. Dropping entry: {'train/lr': 2.0326530612244896e-05, '_timestamp': 1740966877.9345229}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 337 is less than current step: 499. Dropping entry: {'train/loss': 1.5638935446739197, 'train/ce_loss': 0.4919921875, 'train/seg_cls_loss': 0.011138916015625, 'train/kl_loss': 0.25703125, 'train/mask_bce_loss': 0.14628370776772498, 'train/mask_dice_loss': 0.37413962185382843, 'train/mask_loss': 0.5204233258962632, 'metrics/total_secs_per_batch': 8.971485137939453, 'metrics/data_secs_per_batch': 4.5250746488571165, '_timestamp': 1740966886.9047308}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 337 is less than current step: 499. Dropping entry: {'train/lr': 2.020408163265306e-05, '_timestamp': 1740966886.9051297}).
Epoch: [4][339/500]	Time  9.072 ( 9.072)	Loss 2.1032 (1.9894)	CeLoss 0.2158 (0.3253)	SegCLSLoss 0.0167 (0.0161)	KLLoss 0.3633 (0.3256)	MaskLoss 0.9208 (0.8117)	MaskBCELoss 0.0264 (0.1064)	MaskDICELoss 0.8943 (0.7053)
[2025-03-02 19:55:03,354] [INFO] [logging.py:96:log_dist] [Rank 0] step=2340, skipped=0, lr=[2.0020408163265303e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:55:03,360] [INFO] [timer.py:215:stop] epoch=0/micro_step=23400/global_step=2340, RunningAvgSamplesPerSec=1.4531395422456308, CurrSamplesPerSec=1.3544919305387826, MemAllocated=30.95GB, MaxMemAllocated=37.23GB
Epoch: [4][340/500]	Time  7.384 ( 7.384)	Loss 2.5231 (1.5635)	CeLoss 0.1826 (0.3255)	SegCLSLoss 0.0159 (0.0135)	KLLoss 0.3809 (0.2594)	MaskLoss 1.1473 (0.6026)	MaskBCELoss 0.2179 (0.1419)	MaskDICELoss 0.9294 (0.4607)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 338 is less than current step: 499. Dropping entry: {'train/loss': 1.9894274473190308, 'train/ce_loss': 0.32529296875, 'train/seg_cls_loss': 0.016082763671875, 'train/kl_loss': 0.3255859375, 'train/mask_bce_loss': 0.10638588396832346, 'train/mask_dice_loss': 0.7052712053060531, 'train/mask_loss': 0.8116570770740509, 'metrics/total_secs_per_batch': 9.072203636169434, 'metrics/data_secs_per_batch': 3.8582534551620484, '_timestamp': 1740966895.976637}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 338 is less than current step: 499. Dropping entry: {'train/lr': 2.0081632653061222e-05, '_timestamp': 1740966895.9768386}).
Epoch: [4][341/500]	Time  7.446 ( 7.446)	Loss 1.9190 (1.6136)	CeLoss 0.1797 (0.4353)	SegCLSLoss 0.0260 (0.0142)	KLLoss 0.3633 (0.2916)	MaskLoss 0.8453 (0.5709)	MaskBCELoss 0.0183 (0.1241)	MaskDICELoss 0.8269 (0.4468)
Epoch: [4][342/500]	Time  6.895 ( 6.895)	Loss 1.5149 (1.5002)	CeLoss 0.2217 (0.5195)	SegCLSLoss 0.0106 (0.0092)	KLLoss 0.3672 (0.2191)	MaskLoss 0.6256 (0.4771)	MaskBCELoss 0.0821 (0.0676)	MaskDICELoss 0.5435 (0.4095)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 339 is less than current step: 499. Dropping entry: {'train/loss': 1.563501524925232, 'train/ce_loss': 0.32548828125, 'train/seg_cls_loss': 0.01353759765625, 'train/kl_loss': 0.259375, 'train/mask_bce_loss': 0.14190336922183633, 'train/mask_dice_loss': 0.4607458174228668, 'train/mask_loss': 0.6026491940021514, 'metrics/total_secs_per_batch': 7.384394884109497, 'metrics/data_secs_per_batch': 3.1321012020111083, '_timestamp': 1740966903.3612335}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 339 is less than current step: 499. Dropping entry: {'train/lr': 1.9959183673469388e-05, '_timestamp': 1740966903.3616652}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 340 is less than current step: 499. Dropping entry: {'train/loss': 1.6136372089385986, 'train/ce_loss': 0.43525390625, 'train/seg_cls_loss': 0.01416015625, 'train/kl_loss': 0.2916015625, 'train/mask_bce_loss': 0.12408818900585175, 'train/mask_dice_loss': 0.4468417435884476, 'train/mask_loss': 0.5709299296140671, 'metrics/total_secs_per_batch': 7.446244239807129, 'metrics/data_secs_per_batch': 3.3953362464904786, '_timestamp': 1740966910.807549}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 340 is less than current step: 499. Dropping entry: {'train/lr': 1.983673469387755e-05, '_timestamp': 1740966910.8079665}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 341 is less than current step: 499. Dropping entry: {'train/loss': 1.500175142288208, 'train/ce_loss': 0.51953125, 'train/seg_cls_loss': 0.0091552734375, 'train/kl_loss': 0.219140625, 'train/mask_bce_loss': 0.06760916393250227, 'train/mask_dice_loss': 0.40952918529510496, 'train/mask_loss': 0.477138352394104, 'metrics/total_secs_per_batch': 6.895216464996338, 'metrics/data_secs_per_batch': 2.9317147731781006, '_timestamp': 1740966917.702491}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 341 is less than current step: 499. Dropping entry: {'train/lr': 1.971428571428571e-05, '_timestamp': 1740966917.7027614}).
Epoch: [4][343/500]	Time  7.851 ( 7.851)	Loss 2.5248 (1.5442)	CeLoss 0.2158 (0.4824)	SegCLSLoss 0.0175 (0.0106)	KLLoss 0.3652 (0.2531)	MaskLoss 1.1315 (0.5154)	MaskBCELoss 0.2998 (0.0858)	MaskDICELoss 0.8318 (0.4296)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 342 is less than current step: 499. Dropping entry: {'train/loss': 1.5441792488098145, 'train/ce_loss': 0.482421875, 'train/seg_cls_loss': 0.010577392578125, 'train/kl_loss': 0.253125, 'train/mask_bce_loss': 0.08584352945908905, 'train/mask_dice_loss': 0.4295566439628601, 'train/mask_loss': 0.5154001772403717, 'metrics/total_secs_per_batch': 7.850754976272583, 'metrics/data_secs_per_batch': 3.5285524845123293, '_timestamp': 1740966925.5532186}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 342 is less than current step: 499. Dropping entry: {'train/lr': 1.9591836734693877e-05, '_timestamp': 1740966925.5534954}).
Epoch: [4][344/500]	Time 10.003 (10.003)	Loss 1.0269 (2.0582)	CeLoss 0.2695 (0.2469)	SegCLSLoss 0.0096 (0.0143)	KLLoss 0.3652 (0.3666)	MaskLoss 0.3582 (0.8836)	MaskBCELoss 0.1018 (0.2287)	MaskDICELoss 0.2564 (0.6549)
Epoch: [4][345/500]	Time  6.586 ( 6.586)	Loss 2.1097 (1.3537)	CeLoss 0.2236 (0.5274)	SegCLSLoss 0.0226 (0.0075)	KLLoss 0.3555 (0.1803)	MaskLoss 0.9191 (0.4021)	MaskBCELoss 0.2066 (0.1253)	MaskDICELoss 0.7125 (0.2768)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 343 is less than current step: 499. Dropping entry: {'train/loss': 2.058166134357452, 'train/ce_loss': 0.246875, 'train/seg_cls_loss': 0.014300537109375, 'train/kl_loss': 0.3666015625, 'train/mask_bce_loss': 0.22868334529921414, 'train/mask_dice_loss': 0.654891911149025, 'train/mask_loss': 0.883575239777565, 'metrics/total_secs_per_batch': 10.002710580825806, 'metrics/data_secs_per_batch': 4.203333044052124, '_timestamp': 1740966935.5560567}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 343 is less than current step: 499. Dropping entry: {'train/lr': 1.946938775510204e-05, '_timestamp': 1740966935.5564127}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 344 is less than current step: 499. Dropping entry: {'train/loss': 1.3536999702453614, 'train/ce_loss': 0.5273681640625, 'train/seg_cls_loss': 0.007513427734375, 'train/kl_loss': 0.1802734375, 'train/mask_bce_loss': 0.12531462125480175, 'train/mask_dice_loss': 0.2768161088228226, 'train/mask_loss': 0.4021307408809662, 'metrics/total_secs_per_batch': 6.585561513900757, 'metrics/data_secs_per_batch': 3.0171902179718018, '_timestamp': 1740966942.141499}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 344 is less than current step: 499. Dropping entry: {'train/lr': 1.93469387755102e-05, '_timestamp': 1740966942.141773}).
Epoch: [4][346/500]	Time  6.525 ( 6.525)	Loss 1.5000 (1.7742)	CeLoss 1.5000 (0.8021)	SegCLSLoss 0.0000 (0.0098)	KLLoss 0.0000 (0.1805)	MaskLoss 0.0000 (0.4745)	MaskBCELoss 0.0000 (0.0729)	MaskDICELoss 0.0000 (0.4016)
Epoch: [4][347/500]	Time  8.231 ( 8.231)	Loss 1.0859 (1.6044)	CeLoss 1.0859 (0.3674)	SegCLSLoss 0.0000 (0.0124)	KLLoss 0.0000 (0.2896)	MaskLoss 0.0000 (0.6009)	MaskBCELoss 0.0000 (0.0553)	MaskDICELoss 0.0000 (0.5456)
Epoch: [4][348/500]	Time  7.222 ( 7.222)	Loss 2.0625 (1.3770)	CeLoss 2.0625 (0.7253)	SegCLSLoss 0.0000 (0.0067)	KLLoss 0.0000 (0.1484)	MaskLoss 0.0000 (0.3167)	MaskBCELoss 0.0000 (0.0445)	MaskDICELoss 0.0000 (0.2722)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 345 is less than current step: 499. Dropping entry: {'train/loss': 1.7741998434066772, 'train/ce_loss': 0.8021484375, 'train/seg_cls_loss': 0.009820556640625, 'train/kl_loss': 0.18046875, 'train/mask_bce_loss': 0.0728792117908597, 'train/mask_dice_loss': 0.40157421231269835, 'train/mask_loss': 0.47445343136787416, 'metrics/total_secs_per_batch': 6.52480936050415, 'metrics/data_secs_per_batch': 3.5461901903152464, '_timestamp': 1740966948.6665118}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 345 is less than current step: 499. Dropping entry: {'train/lr': 1.9224489795918367e-05, '_timestamp': 1740966948.666884}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 346 is less than current step: 499. Dropping entry: {'train/loss': 1.6044201374053955, 'train/ce_loss': 0.3673828125, 'train/seg_cls_loss': 0.01239013671875, 'train/kl_loss': 0.2896484375, 'train/mask_bce_loss': 0.05531028788536787, 'train/mask_dice_loss': 0.5456302374601364, 'train/mask_loss': 0.6009405195713043, 'metrics/total_secs_per_batch': 8.230534553527832, 'metrics/data_secs_per_batch': 3.733433222770691, '_timestamp': 1740966956.896854}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 346 is less than current step: 499. Dropping entry: {'train/lr': 1.910204081632653e-05, '_timestamp': 1740966956.8971257}).
Epoch: [4][349/500]	Time  7.962 ( 7.962)	Loss 1.6448 (1.7170)	CeLoss 0.2217 (0.3011)	SegCLSLoss 0.0164 (0.0151)	KLLoss 0.3555 (0.2936)	MaskLoss 0.6896 (0.6896)	MaskBCELoss 0.0570 (0.0958)	MaskDICELoss 0.6325 (0.5938)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 347 is less than current step: 499. Dropping entry: {'train/loss': 1.3769772410392762, 'train/ce_loss': 0.72529296875, 'train/seg_cls_loss': 0.006744384765625, 'train/kl_loss': 0.1484375, 'train/mask_bce_loss': 0.044470032583922146, 'train/mask_dice_loss': 0.2721924126148224, 'train/mask_loss': 0.3166624456644058, 'metrics/total_secs_per_batch': 7.22158408164978, 'metrics/data_secs_per_batch': 3.4208226203918457, '_timestamp': 1740966964.118619}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 347 is less than current step: 499. Dropping entry: {'train/lr': 1.897959183673469e-05, '_timestamp': 1740966964.118961}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 348 is less than current step: 499. Dropping entry: {'train/loss': 1.7169965028762817, 'train/ce_loss': 0.30107421875, 'train/seg_cls_loss': 0.015106201171875, 'train/kl_loss': 0.2935546875, 'train/mask_bce_loss': 0.09578799828886986, 'train/mask_dice_loss': 0.593764939904213, 'train/mask_loss': 0.6895529329776764, 'metrics/total_secs_per_batch': 7.961582183837891, 'metrics/data_secs_per_batch': 3.311752510070801, '_timestamp': 1740966972.0800726}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 348 is less than current step: 499. Dropping entry: {'train/lr': 1.8857142857142856e-05, '_timestamp': 1740966972.0803628}).
[2025-03-02 19:56:20,045] [INFO] [logging.py:96:log_dist] [Rank 0] step=2350, skipped=0, lr=[1.8795918367346937e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:56:20,051] [INFO] [timer.py:215:stop] epoch=0/micro_step=23500/global_step=2350, RunningAvgSamplesPerSec=1.4524328937192323, CurrSamplesPerSec=1.2545929985802948, MemAllocated=31.27GB, MaxMemAllocated=37.23GB
Epoch: [4][350/500]	Time  7.972 ( 7.972)	Loss 2.4443 (2.1257)	CeLoss 0.1533 (0.2376)	SegCLSLoss 0.0234 (0.0200)	KLLoss 0.3789 (0.3330)	MaskLoss 1.1206 (0.9225)	MaskBCELoss 0.2828 (0.2097)	MaskDICELoss 0.8378 (0.7129)
Epoch: [4][351/500]	Time  6.193 ( 6.193)	Loss 1.6719 (1.7362)	CeLoss 0.1836 (0.5623)	SegCLSLoss 0.0304 (0.0139)	KLLoss 0.3516 (0.2543)	MaskLoss 0.7188 (0.5707)	MaskBCELoss 0.0816 (0.1047)	MaskDICELoss 0.6372 (0.4660)
Epoch: [4][352/500]	Time  7.048 ( 7.048)	Loss 0.2314 (1.5451)	CeLoss 0.2314 (0.5184)	SegCLSLoss 0.0000 (0.0084)	KLLoss 0.0000 (0.2188)	MaskLoss 0.0000 (0.5002)	MaskBCELoss 0.0000 (0.1350)	MaskDICELoss 0.0000 (0.3652)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 349 is less than current step: 499. Dropping entry: {'train/loss': 2.1257498979568483, 'train/ce_loss': 0.23759765625, 'train/seg_cls_loss': 0.0199951171875, 'train/kl_loss': 0.3330078125, 'train/mask_bce_loss': 0.20967635177075863, 'train/mask_dice_loss': 0.7128665536642075, 'train/mask_loss': 0.9225429117679596, 'metrics/total_secs_per_batch': 7.972294330596924, 'metrics/data_secs_per_batch': 3.808504557609558, '_timestamp': 1740966980.052139}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 349 is less than current step: 499. Dropping entry: {'train/lr': 1.873469387755102e-05, '_timestamp': 1740966980.0523975}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 350 is less than current step: 499. Dropping entry: {'train/loss': 1.7361587047576905, 'train/ce_loss': 0.5623046875, 'train/seg_cls_loss': 0.013909912109375, 'train/kl_loss': 0.254296875, 'train/mask_bce_loss': 0.10469676563516259, 'train/mask_dice_loss': 0.4659704864025116, 'train/mask_loss': 0.5706672489643096, 'metrics/total_secs_per_batch': 6.193023681640625, 'metrics/data_secs_per_batch': 2.4786402225494384, '_timestamp': 1740966986.245339}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 350 is less than current step: 499. Dropping entry: {'train/lr': 1.8612244897959182e-05, '_timestamp': 1740966986.2455986}).
Epoch: [4][353/500]	Time  8.047 ( 8.047)	Loss 1.6361 (1.8403)	CeLoss 0.2471 (0.4219)	SegCLSLoss 0.0120 (0.0150)	KLLoss 0.3652 (0.2936)	MaskLoss 0.6726 (0.6906)	MaskBCELoss 0.2719 (0.1924)	MaskDICELoss 0.4006 (0.4982)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 351 is less than current step: 499. Dropping entry: {'train/loss': 1.545053815841675, 'train/ce_loss': 0.518359375, 'train/seg_cls_loss': 0.00836181640625, 'train/kl_loss': 0.21875, 'train/mask_bce_loss': 0.1349803712219, 'train/mask_dice_loss': 0.3652320861816406, 'train/mask_loss': 0.5002124637365342, 'metrics/total_secs_per_batch': 7.047916412353516, 'metrics/data_secs_per_batch': 2.7182331800460817, '_timestamp': 1740966993.2933152}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 351 is less than current step: 499. Dropping entry: {'train/lr': 1.8489795918367345e-05, '_timestamp': 1740966993.293666}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 352 is less than current step: 499. Dropping entry: {'train/loss': 1.840281856060028, 'train/ce_loss': 0.421875, 'train/seg_cls_loss': 0.015020751953125, 'train/kl_loss': 0.2935546875, 'train/mask_bce_loss': 0.192378493770957, 'train/mask_dice_loss': 0.4982214212417603, 'train/mask_loss': 0.6905999124050141, 'metrics/total_secs_per_batch': 8.047181129455566, 'metrics/data_secs_per_batch': 3.549019718170166, '_timestamp': 1740967001.3405051}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 352 is less than current step: 499. Dropping entry: {'train/lr': 1.8367346938775508e-05, '_timestamp': 1740967001.3408546}).
Epoch: [4][354/500]	Time  7.808 ( 7.808)	Loss 1.9873 (1.9419)	CeLoss 0.1865 (0.3210)	SegCLSLoss 0.0228 (0.0159)	KLLoss 0.3555 (0.3287)	MaskLoss 0.8765 (0.7899)	MaskBCELoss 0.0228 (0.1122)	MaskDICELoss 0.8537 (0.6777)
Epoch: [4][355/500]	Time  7.582 ( 7.582)	Loss 2.7546 (1.4829)	CeLoss 0.1836 (0.4089)	SegCLSLoss 0.0211 (0.0100)	KLLoss 0.3750 (0.2191)	MaskLoss 1.2611 (0.5236)	MaskBCELoss 0.2936 (0.1016)	MaskDICELoss 0.9675 (0.4220)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 353 is less than current step: 499. Dropping entry: {'train/loss': 1.9419196963310241, 'train/ce_loss': 0.32099609375, 'train/seg_cls_loss': 0.015936279296875, 'train/kl_loss': 0.3287109375, 'train/mask_bce_loss': 0.11222743932157755, 'train/mask_dice_loss': 0.6776777029037475, 'train/mask_loss': 0.7899051487445832, 'metrics/total_secs_per_batch': 7.808204889297485, 'metrics/data_secs_per_batch': 3.4559757471084596, '_timestamp': 1740967009.1486316}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 353 is less than current step: 499. Dropping entry: {'train/lr': 1.824489795918367e-05, '_timestamp': 1740967009.148884}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 354 is less than current step: 499. Dropping entry: {'train/loss': 1.4829103231430054, 'train/ce_loss': 0.40888671875, 'train/seg_cls_loss': 0.0099853515625, 'train/kl_loss': 0.219140625, 'train/mask_bce_loss': 0.10158445313572884, 'train/mask_dice_loss': 0.4219996303319931, 'train/mask_loss': 0.5235840916633606, 'metrics/total_secs_per_batch': 7.582064867019653, 'metrics/data_secs_per_batch': 3.6523800134658813, '_timestamp': 1740967016.7307057}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 354 is less than current step: 499. Dropping entry: {'train/lr': 1.8122448979591837e-05, '_timestamp': 1740967016.7309701}).
Epoch: [4][356/500]	Time  9.742 ( 9.742)	Loss 1.1659 (1.5480)	CeLoss 0.2393 (0.3401)	SegCLSLoss 0.0117 (0.0145)	KLLoss 0.3691 (0.2898)	MaskLoss 0.4423 (0.5859)	MaskBCELoss 0.0923 (0.0841)	MaskDICELoss 0.3500 (0.5018)
Epoch: [4][357/500]	Time  7.975 ( 7.975)	Loss 1.2891 (1.6830)	CeLoss 1.2891 (0.4147)	SegCLSLoss 0.0000 (0.0159)	KLLoss 0.0000 (0.2912)	MaskLoss 0.0000 (0.6156)	MaskBCELoss 0.0000 (0.1181)	MaskDICELoss 0.0000 (0.4976)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 355 is less than current step: 499. Dropping entry: {'train/loss': 1.5479828655719756, 'train/ce_loss': 0.34013671875, 'train/seg_cls_loss': 0.014501953125, 'train/kl_loss': 0.28984375, 'train/mask_bce_loss': 0.0841298689134419, 'train/mask_dice_loss': 0.5017756134271621, 'train/mask_loss': 0.5859054923057556, 'metrics/total_secs_per_batch': 9.741816282272339, 'metrics/data_secs_per_batch': 4.664376497268677, '_timestamp': 1740967026.472529}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 355 is less than current step: 499. Dropping entry: {'train/lr': 1.7999999999999997e-05, '_timestamp': 1740967026.4727955}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 356 is less than current step: 499. Dropping entry: {'train/loss': 1.6830368995666505, 'train/ce_loss': 0.41474609375, 'train/seg_cls_loss': 0.015936279296875, 'train/kl_loss': 0.2912109375, 'train/mask_bce_loss': 0.1180621875450015, 'train/mask_dice_loss': 0.4975773483514786, 'train/mask_loss': 0.6156395345926284, 'metrics/total_secs_per_batch': 7.974989891052246, 'metrics/data_secs_per_batch': 3.496836042404175, '_timestamp': 1740967034.447558}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 356 is less than current step: 499. Dropping entry: {'train/lr': 1.787755102040816e-05, '_timestamp': 1740967034.447832}).
Epoch: [4][358/500]	Time  7.612 ( 7.612)	Loss 1.5511 (1.3969)	CeLoss 0.2393 (0.5994)	SegCLSLoss 0.0148 (0.0063)	KLLoss 0.3633 (0.1461)	MaskLoss 0.6339 (0.3898)	MaskBCELoss 0.0668 (0.0937)	MaskDICELoss 0.5671 (0.2962)
Epoch: [4][359/500]	Time  8.424 ( 8.424)	Loss 1.8069 (1.8479)	CeLoss 0.1758 (0.2996)	SegCLSLoss 0.0201 (0.0163)	KLLoss 0.3594 (0.2961)	MaskLoss 0.7926 (0.7552)	MaskBCELoss 0.0205 (0.1705)	MaskDICELoss 0.7721 (0.5848)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 357 is less than current step: 499. Dropping entry: {'train/loss': 1.396855890750885, 'train/ce_loss': 0.5994140625, 'train/seg_cls_loss': 0.00628662109375, 'train/kl_loss': 0.14609375, 'train/mask_bce_loss': 0.093672726303339, 'train/mask_dice_loss': 0.29616146683692934, 'train/mask_loss': 0.3898341953754425, 'metrics/total_secs_per_batch': 7.6116743087768555, 'metrics/data_secs_per_batch': 3.182922840118408, '_timestamp': 1740967042.0593464}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 357 is less than current step: 499. Dropping entry: {'train/lr': 1.7755102040816327e-05, '_timestamp': 1740967042.0596921}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 358 is less than current step: 499. Dropping entry: {'train/loss': 1.847869348526001, 'train/ce_loss': 0.299560546875, 'train/seg_cls_loss': 0.016314697265625, 'train/kl_loss': 0.29609375, 'train/mask_bce_loss': 0.17046424169093372, 'train/mask_dice_loss': 0.5847692519426346, 'train/mask_loss': 0.7552334845066071, 'metrics/total_secs_per_batch': 8.42443037033081, 'metrics/data_secs_per_batch': 3.189001131057739, '_timestamp': 1740967050.483645}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 358 is less than current step: 499. Dropping entry: {'train/lr': 1.7632653061224486e-05, '_timestamp': 1740967050.4839196}).
[2025-03-02 19:57:38,102] [INFO] [logging.py:96:log_dist] [Rank 0] step=2360, skipped=0, lr=[1.757142857142857e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:57:38,109] [INFO] [timer.py:215:stop] epoch=0/micro_step=23600/global_step=2360, RunningAvgSamplesPerSec=1.451610647690334, CurrSamplesPerSec=1.3114774191932534, MemAllocated=31.28GB, MaxMemAllocated=37.23GB
Epoch: [4][360/500]	Time  7.626 ( 7.626)	Loss 1.6602 (1.5396)	CeLoss 0.1914 (0.5435)	SegCLSLoss 0.0320 (0.0122)	KLLoss 0.3770 (0.2195)	MaskLoss 0.7080 (0.4840)	MaskBCELoss 0.0471 (0.0897)	MaskDICELoss 0.6609 (0.3943)
Epoch: [4][361/500]	Time  8.595 ( 8.595)	Loss 1.8080 (1.5893)	CeLoss 0.2383 (0.3157)	SegCLSLoss 0.0155 (0.0127)	KLLoss 0.3691 (0.2957)	MaskLoss 0.7624 (0.6187)	MaskBCELoss 0.0740 (0.1510)	MaskDICELoss 0.6884 (0.4677)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 359 is less than current step: 499. Dropping entry: {'train/loss': 1.539618718624115, 'train/ce_loss': 0.54345703125, 'train/seg_cls_loss': 0.012213134765625, 'train/kl_loss': 0.21953125, 'train/mask_bce_loss': 0.08973948583006859, 'train/mask_dice_loss': 0.3942788511514664, 'train/mask_loss': 0.48401833772659303, 'metrics/total_secs_per_batch': 7.626473903656006, 'metrics/data_secs_per_batch': 3.6215824842453004, '_timestamp': 1740967058.1099343}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 359 is less than current step: 499. Dropping entry: {'train/lr': 1.7510204081632653e-05, '_timestamp': 1740967058.110187}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 360 is less than current step: 499. Dropping entry: {'train/loss': 1.5893100440502166, 'train/ce_loss': 0.31572265625, 'train/seg_cls_loss': 0.0126708984375, 'train/kl_loss': 0.295703125, 'train/mask_bce_loss': 0.15098439641296862, 'train/mask_dice_loss': 0.46769405603408815, 'train/mask_loss': 0.6186784625053405, 'metrics/total_secs_per_batch': 8.595190525054932, 'metrics/data_secs_per_batch': 3.7565791606903076, '_timestamp': 1740967066.7053094}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 360 is less than current step: 499. Dropping entry: {'train/lr': 1.7387755102040816e-05, '_timestamp': 1740967066.7055802}).
Epoch: [4][362/500]	Time  8.411 ( 8.411)	Loss 0.7734 (1.3345)	CeLoss 0.7734 (0.4230)	SegCLSLoss 0.0000 (0.0108)	KLLoss 0.0000 (0.2209)	MaskLoss 0.0000 (0.4420)	MaskBCELoss 0.0000 (0.0685)	MaskDICELoss 0.0000 (0.3735)
Epoch: [4][363/500]	Time  6.884 ( 6.884)	Loss 0.7617 (1.3917)	CeLoss 0.7617 (0.3174)	SegCLSLoss 0.0000 (0.0114)	KLLoss 0.0000 (0.2561)	MaskLoss 0.0000 (0.5215)	MaskBCELoss 0.0000 (0.1231)	MaskDICELoss 0.0000 (0.3984)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 361 is less than current step: 499. Dropping entry: {'train/loss': 1.3345270991325378, 'train/ce_loss': 0.422998046875, 'train/seg_cls_loss': 0.0108154296875, 'train/kl_loss': 0.2208984375, 'train/mask_bce_loss': 0.06846342030912637, 'train/mask_dice_loss': 0.37353158593177793, 'train/mask_loss': 0.4419950008392334, 'metrics/total_secs_per_batch': 8.410994291305542, 'metrics/data_secs_per_batch': 4.103245687484741, '_timestamp': 1740967075.116321}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 361 is less than current step: 499. Dropping entry: {'train/lr': 1.726530612244898e-05, '_timestamp': 1740967075.1165922}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 362 is less than current step: 499. Dropping entry: {'train/loss': 1.3917017579078674, 'train/ce_loss': 0.3173828125, 'train/seg_cls_loss': 0.011431884765625, 'train/kl_loss': 0.2560546875, 'train/mask_bce_loss': 0.12311477698385716, 'train/mask_dice_loss': 0.39837087094783785, 'train/mask_loss': 0.5214856445789338, 'metrics/total_secs_per_batch': 6.884208917617798, 'metrics/data_secs_per_batch': 3.28245096206665, '_timestamp': 1740967082.0005243}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 362 is less than current step: 499. Dropping entry: {'train/lr': 1.7142857142857142e-05, '_timestamp': 1740967082.000783}).
Epoch: [4][364/500]	Time  7.247 ( 7.247)	Loss 2.4333 (1.4782)	CeLoss 0.2100 (0.4750)	SegCLSLoss 0.0173 (0.0106)	KLLoss 0.3633 (0.2213)	MaskLoss 1.0887 (0.4878)	MaskBCELoss 0.3117 (0.1467)	MaskDICELoss 0.7770 (0.3411)
Epoch: [4][365/500]	Time  8.649 ( 8.649)	Loss 0.1885 (2.0571)	CeLoss 0.1885 (0.2145)	SegCLSLoss 0.0000 (0.0169)	KLLoss 0.0000 (0.3281)	MaskLoss 0.0000 (0.9007)	MaskBCELoss 0.0000 (0.1814)	MaskDICELoss 0.0000 (0.7194)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 363 is less than current step: 499. Dropping entry: {'train/loss': 1.4782123804092406, 'train/ce_loss': 0.475, 'train/seg_cls_loss': 0.01063232421875, 'train/kl_loss': 0.2212890625, 'train/mask_bce_loss': 0.14672719687223434, 'train/mask_dice_loss': 0.34110945761203765, 'train/mask_loss': 0.48783665895462036, 'metrics/total_secs_per_batch': 7.24699330329895, 'metrics/data_secs_per_batch': 3.1992408275604247, '_timestamp': 1740967089.2475045}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 363 is less than current step: 499. Dropping entry: {'train/lr': 1.7020408163265305e-05, '_timestamp': 1740967089.2477708}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 364 is less than current step: 499. Dropping entry: {'train/loss': 2.0571388840675353, 'train/ce_loss': 0.214453125, 'train/seg_cls_loss': 0.016888427734375, 'train/kl_loss': 0.328125, 'train/mask_bce_loss': 0.18136411644518374, 'train/mask_dice_loss': 0.7193732857704163, 'train/mask_loss': 0.9007374048233032, 'metrics/total_secs_per_batch': 8.648660659790039, 'metrics/data_secs_per_batch': 3.853900337219238, '_timestamp': 1740967097.8961725}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 364 is less than current step: 499. Dropping entry: {'train/lr': 1.6897959183673468e-05, '_timestamp': 1740967097.8964427}).
Epoch: [4][366/500]	Time  8.210 ( 8.210)	Loss 1.7046 (1.6086)	CeLoss 0.2617 (0.3151)	SegCLSLoss 0.0120 (0.0130)	KLLoss 0.3613 (0.2914)	MaskLoss 0.6999 (0.6288)	MaskBCELoss 0.2169 (0.1105)	MaskDICELoss 0.4831 (0.5183)
Epoch: [4][367/500]	Time  6.260 ( 6.260)	Loss 1.8344 (1.5421)	CeLoss 0.2256 (0.7966)	SegCLSLoss 0.0170 (0.0085)	KLLoss 0.3613 (0.1479)	MaskLoss 0.7825 (0.3633)	MaskBCELoss 0.0173 (0.0361)	MaskDICELoss 0.7652 (0.3271)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 365 is less than current step: 499. Dropping entry: {'train/loss': 1.6086069285869598, 'train/ce_loss': 0.31513671875, 'train/seg_cls_loss': 0.013018798828125, 'train/kl_loss': 0.29140625, 'train/mask_bce_loss': 0.11049246750771999, 'train/mask_dice_loss': 0.5183227151632309, 'train/mask_loss': 0.6288151830434799, 'metrics/total_secs_per_batch': 8.209673166275024, 'metrics/data_secs_per_batch': 3.5141105890274047, '_timestamp': 1740967106.1061006}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 365 is less than current step: 499. Dropping entry: {'train/lr': 1.677551020408163e-05, '_timestamp': 1740967106.1064594}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 366 is less than current step: 499. Dropping entry: {'train/loss': 1.5421276092529297, 'train/ce_loss': 0.79658203125, 'train/seg_cls_loss': 0.008465576171875, 'train/kl_loss': 0.1478515625, 'train/mask_bce_loss': 0.03613698445260525, 'train/mask_dice_loss': 0.3271143138408661, 'train/mask_loss': 0.3632513046264648, 'metrics/total_secs_per_batch': 6.260087490081787, 'metrics/data_secs_per_batch': 3.015932011604309, '_timestamp': 1740967112.3659766}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 366 is less than current step: 499. Dropping entry: {'train/lr': 1.6653061224489794e-05, '_timestamp': 1740967112.3663466}).
Epoch: [4][368/500]	Time  7.976 ( 7.976)	Loss 2.2164 (1.6729)	CeLoss 0.1865 (0.5448)	SegCLSLoss 0.0200 (0.0095)	KLLoss 0.3594 (0.2174)	MaskLoss 0.9920 (0.5509)	MaskBCELoss 0.0244 (0.0948)	MaskDICELoss 0.9676 (0.4561)
Epoch: [4][369/500]	Time  8.038 ( 8.038)	Loss 1.3952 (1.6980)	CeLoss 0.2119 (0.4448)	SegCLSLoss 0.0187 (0.0136)	KLLoss 0.3574 (0.2906)	MaskLoss 0.5687 (0.6087)	MaskBCELoss 0.1206 (0.1473)	MaskDICELoss 0.4481 (0.4614)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 367 is less than current step: 499. Dropping entry: {'train/loss': 1.672900414466858, 'train/ce_loss': 0.54482421875, 'train/seg_cls_loss': 0.00950927734375, 'train/kl_loss': 0.2173828125, 'train/mask_bce_loss': 0.09477482731454075, 'train/mask_dice_loss': 0.4560796618461609, 'train/mask_loss': 0.5508544921875, 'metrics/total_secs_per_batch': 7.9762561321258545, 'metrics/data_secs_per_batch': 3.8317150592803957, '_timestamp': 1740967120.3422542}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 367 is less than current step: 499. Dropping entry: {'train/lr': 1.6530612244897957e-05, '_timestamp': 1740967120.3425415}).
[2025-03-02 19:58:58,224] [INFO] [logging.py:96:log_dist] [Rank 0] step=2370, skipped=0, lr=[1.6346938775510202e-05], mom=[(0.9, 0.95)]
[2025-03-02 19:58:58,231] [INFO] [timer.py:215:stop] epoch=0/micro_step=23700/global_step=2370, RunningAvgSamplesPerSec=1.4506128507249207, CurrSamplesPerSec=1.0152671164437146, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [4][370/500]	Time  9.851 ( 9.851)	Loss 1.1960 (1.5109)	CeLoss 0.2100 (0.3220)	SegCLSLoss 0.0103 (0.0135)	KLLoss 0.3633 (0.3268)	MaskLoss 0.4720 (0.5746)	MaskBCELoss 0.0567 (0.0948)	MaskDICELoss 0.4154 (0.4798)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 368 is less than current step: 499. Dropping entry: {'train/loss': 1.698005837202072, 'train/ce_loss': 0.444775390625, 'train/seg_cls_loss': 0.013641357421875, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.1472592016682029, 'train/mask_dice_loss': 0.46141168028116225, 'train/mask_loss': 0.6086708828806877, 'metrics/total_secs_per_batch': 8.038487195968628, 'metrics/data_secs_per_batch': 3.643179154396057, '_timestamp': 1740967128.3806436}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 368 is less than current step: 499. Dropping entry: {'train/lr': 1.640816326530612e-05, '_timestamp': 1740967128.3809092}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 369 is less than current step: 499. Dropping entry: {'train/loss': 1.510864520072937, 'train/ce_loss': 0.32197265625, 'train/seg_cls_loss': 0.013507080078125, 'train/kl_loss': 0.3267578125, 'train/mask_bce_loss': 0.09479500986635685, 'train/mask_dice_loss': 0.47977788001298904, 'train/mask_loss': 0.5745728984475136, 'metrics/total_secs_per_batch': 9.851065397262573, 'metrics/data_secs_per_batch': 4.215145897865296, '_timestamp': 1740967138.2315886}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 369 is less than current step: 499. Dropping entry: {'train/lr': 1.6285714285714283e-05, '_timestamp': 1740967138.2318728}).
Epoch: [4][371/500]	Time  8.202 ( 8.202)	Loss 1.8191 (1.5650)	CeLoss 0.1973 (0.3234)	SegCLSLoss 0.0203 (0.0135)	KLLoss 0.3574 (0.2916)	MaskLoss 0.7884 (0.6028)	MaskBCELoss 0.0172 (0.0833)	MaskDICELoss 0.7712 (0.5195)
Epoch: [4][372/500]	Time  7.938 ( 7.938)	Loss 1.3361 (1.4381)	CeLoss 0.2617 (0.4158)	SegCLSLoss 0.0119 (0.0119)	KLLoss 0.3652 (0.2908)	MaskLoss 0.5157 (0.4936)	MaskBCELoss 0.1277 (0.1068)	MaskDICELoss 0.3881 (0.3868)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 370 is less than current step: 499. Dropping entry: {'train/loss': 1.5650206208229065, 'train/ce_loss': 0.3234375, 'train/seg_cls_loss': 0.013470458984375, 'train/kl_loss': 0.2916015625, 'train/mask_bce_loss': 0.08330877181142568, 'train/mask_dice_loss': 0.5195140540599823, 'train/mask_loss': 0.6028228163719177, 'metrics/total_secs_per_batch': 8.202257871627808, 'metrics/data_secs_per_batch': 3.289468002319336, '_timestamp': 1740967146.4340093}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 370 is less than current step: 499. Dropping entry: {'train/lr': 1.6163265306122446e-05, '_timestamp': 1740967146.4341893}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 371 is less than current step: 499. Dropping entry: {'train/loss': 1.4380576968193055, 'train/ce_loss': 0.4158203125, 'train/seg_cls_loss': 0.011871337890625, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.10676455162465573, 'train/mask_dice_loss': 0.38682485520839693, 'train/mask_loss': 0.49358939826488496, 'metrics/total_secs_per_batch': 7.9378156661987305, 'metrics/data_secs_per_batch': 3.4494070053100585, '_timestamp': 1740967154.371853}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 371 is less than current step: 499. Dropping entry: {'train/lr': 1.6040816326530613e-05, '_timestamp': 1740967154.372122}).
Epoch: [4][373/500]	Time  8.092 ( 8.092)	Loss 2.0429 (1.7597)	CeLoss 0.2080 (0.2163)	SegCLSLoss 0.0214 (0.0181)	KLLoss 0.3477 (0.3654)	MaskLoss 0.8945 (0.7489)	MaskBCELoss 0.0293 (0.1825)	MaskDICELoss 0.8652 (0.5665)
Epoch: [4][374/500]	Time  6.350 ( 6.350)	Loss 1.1562 (1.3785)	CeLoss 1.1562 (0.4594)	SegCLSLoss 0.0000 (0.0093)	KLLoss 0.0000 (0.1846)	MaskLoss 0.0000 (0.4479)	MaskBCELoss 0.0000 (0.1091)	MaskDICELoss 0.0000 (0.3388)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 372 is less than current step: 499. Dropping entry: {'train/loss': 1.7596867501735687, 'train/ce_loss': 0.21630859375, 'train/seg_cls_loss': 0.018115234375, 'train/kl_loss': 0.3654296875, 'train/mask_bce_loss': 0.18245166540145874, 'train/mask_dice_loss': 0.5664835184812546, 'train/mask_loss': 0.7489351853728294, 'metrics/total_secs_per_batch': 8.09165644645691, 'metrics/data_secs_per_batch': 3.34177885055542, '_timestamp': 1740967162.4637456}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 372 is less than current step: 499. Dropping entry: {'train/lr': 1.5918367346938772e-05, '_timestamp': 1740967162.4641263}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 373 is less than current step: 499. Dropping entry: {'train/loss': 1.3784927368164062, 'train/ce_loss': 0.4593994140625, 'train/seg_cls_loss': 0.009320068359375, 'train/kl_loss': 0.1845703125, 'train/mask_bce_loss': 0.10913765579462051, 'train/mask_dice_loss': 0.3387879073619843, 'train/mask_loss': 0.44792556166648867, 'metrics/total_secs_per_batch': 6.349698305130005, 'metrics/data_secs_per_batch': 3.427891802787781, '_timestamp': 1740967168.8131983}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 373 is less than current step: 499. Dropping entry: {'train/lr': 1.579591836734694e-05, '_timestamp': 1740967168.8134723}).
Epoch: [4][375/500]	Time  7.620 ( 7.620)	Loss 2.8601 (1.9166)	CeLoss 0.0957 (0.4079)	SegCLSLoss 0.0422 (0.0167)	KLLoss 0.3750 (0.2932)	MaskLoss 1.3529 (0.7355)	MaskBCELoss 0.4899 (0.1903)	MaskDICELoss 0.8630 (0.5452)
Epoch: [4][376/500]	Time  8.116 ( 8.116)	Loss 1.4720 (1.7647)	CeLoss 0.2715 (0.3007)	SegCLSLoss 0.0112 (0.0153)	KLLoss 0.3633 (0.3285)	MaskLoss 0.5788 (0.7118)	MaskBCELoss 0.0615 (0.1270)	MaskDICELoss 0.5173 (0.5848)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 374 is less than current step: 499. Dropping entry: {'train/loss': 1.9166213989257812, 'train/ce_loss': 0.40791015625, 'train/seg_cls_loss': 0.01669921875, 'train/kl_loss': 0.2931640625, 'train/mask_bce_loss': 0.19026272806804628, 'train/mask_dice_loss': 0.5451964020729065, 'train/mask_loss': 0.7354591369628907, 'metrics/total_secs_per_batch': 7.619828939437866, 'metrics/data_secs_per_batch': 3.656763768196106, '_timestamp': 1740967176.4330604}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 374 is less than current step: 499. Dropping entry: {'train/lr': 1.5673469387755102e-05, '_timestamp': 1740967176.433335}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 375 is less than current step: 499. Dropping entry: {'train/loss': 1.7647472023963928, 'train/ce_loss': 0.30068359375, 'train/seg_cls_loss': 0.015283203125, 'train/kl_loss': 0.328515625, 'train/mask_bce_loss': 0.12699859393760562, 'train/mask_dice_loss': 0.5848183661699295, 'train/mask_loss': 0.711816954612732, 'metrics/total_secs_per_batch': 8.115690231323242, 'metrics/data_secs_per_batch': 3.483406662940979, '_timestamp': 1740967184.5487034}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 375 is less than current step: 499. Dropping entry: {'train/lr': 1.555102040816326e-05, '_timestamp': 1740967184.5489666}).
Epoch: [4][377/500]	Time  8.126 ( 8.126)	Loss 1.6276 (1.8571)	CeLoss 0.1768 (0.3054)	SegCLSLoss 0.0198 (0.0166)	KLLoss 0.3633 (0.3287)	MaskLoss 0.7025 (0.7552)	MaskBCELoss 0.0418 (0.1514)	MaskDICELoss 0.6607 (0.6038)
Epoch: [4][378/500]	Time  9.101 ( 9.101)	Loss 2.3310 (1.5895)	CeLoss 0.2393 (0.3747)	SegCLSLoss 0.0237 (0.0151)	KLLoss 0.3594 (0.2887)	MaskLoss 1.0220 (0.5892)	MaskBCELoss 0.0222 (0.0707)	MaskDICELoss 0.9998 (0.5185)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 376 is less than current step: 499. Dropping entry: {'train/loss': 1.8571494698524476, 'train/ce_loss': 0.30537109375, 'train/seg_cls_loss': 0.01663818359375, 'train/kl_loss': 0.3287109375, 'train/mask_bce_loss': 0.1513604074716568, 'train/mask_dice_loss': 0.6038256615400315, 'train/mask_loss': 0.7551860719919204, 'metrics/total_secs_per_batch': 8.126487255096436, 'metrics/data_secs_per_batch': 3.5998342990875245, '_timestamp': 1740967192.6752033}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 376 is less than current step: 499. Dropping entry: {'train/lr': 1.5428571428571428e-05, '_timestamp': 1740967192.6754677}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 377 is less than current step: 499. Dropping entry: {'train/loss': 1.5895423650741578, 'train/ce_loss': 0.37470703125, 'train/seg_cls_loss': 0.015057373046875, 'train/kl_loss': 0.288671875, 'train/mask_bce_loss': 0.07068912573158741, 'train/mask_dice_loss': 0.5184668198227882, 'train/mask_loss': 0.589155949652195, 'metrics/total_secs_per_batch': 9.10137391090393, 'metrics/data_secs_per_batch': 3.88103609085083, '_timestamp': 1740967201.7766075}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 377 is less than current step: 499. Dropping entry: {'train/lr': 1.530612244897959e-05, '_timestamp': 1740967201.7768893}).
Epoch: [4][379/500]	Time  9.299 ( 9.299)	Loss 1.9506 (1.0042)	CeLoss 0.1953 (0.2943)	SegCLSLoss 0.0244 (0.0109)	KLLoss 0.3535 (0.2182)	MaskLoss 0.8542 (0.3413)	MaskBCELoss 0.1324 (0.0472)	MaskDICELoss 0.7218 (0.2941)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 378 is less than current step: 499. Dropping entry: {'train/loss': 1.0041592597961426, 'train/ce_loss': 0.2943359375, 'train/seg_cls_loss': 0.01087646484375, 'train/kl_loss': 0.2181640625, 'train/mask_bce_loss': 0.04723326992243528, 'train/mask_dice_loss': 0.29405534267425537, 'train/mask_loss': 0.3412886142730713, 'metrics/total_secs_per_batch': 9.2989022731781, 'metrics/data_secs_per_batch': 3.9719664096832275, '_timestamp': 1740967211.0755742}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 378 is less than current step: 499. Dropping entry: {'train/lr': 1.5183673469387754e-05, '_timestamp': 1740967211.075768}).
[2025-03-02 20:00:19,333] [INFO] [logging.py:96:log_dist] [Rank 0] step=2380, skipped=0, lr=[1.5122448979591836e-05], mom=[(0.9, 0.95)]
[2025-03-02 20:00:19,340] [INFO] [timer.py:215:stop] epoch=0/micro_step=23800/global_step=2380, RunningAvgSamplesPerSec=1.4495375302167208, CurrSamplesPerSec=1.2100883006156173, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [4][380/500]	Time  8.265 ( 8.265)	Loss 1.8958 (1.7025)	CeLoss 0.2520 (0.3089)	SegCLSLoss 0.0129 (0.0132)	KLLoss 0.3691 (0.2881)	MaskLoss 0.8004 (0.6790)	MaskBCELoss 0.1490 (0.0824)	MaskDICELoss 0.6514 (0.5966)
Epoch: [4][381/500]	Time  6.173 ( 6.173)	Loss 1.3828 (1.1890)	CeLoss 1.3828 (0.4204)	SegCLSLoss 0.0000 (0.0082)	KLLoss 0.0000 (0.1826)	MaskLoss 0.0000 (0.3730)	MaskBCELoss 0.0000 (0.0920)	MaskDICELoss 0.0000 (0.2810)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 379 is less than current step: 499. Dropping entry: {'train/loss': 1.7024668455123901, 'train/ce_loss': 0.30888671875, 'train/seg_cls_loss': 0.01322021484375, 'train/kl_loss': 0.2880859375, 'train/mask_bce_loss': 0.08237863797694445, 'train/mask_dice_loss': 0.5966379702091217, 'train/mask_loss': 0.6790166065096855, 'metrics/total_secs_per_batch': 8.265435218811035, 'metrics/data_secs_per_batch': 3.5925158739089964, '_timestamp': 1740967219.3407953}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 379 is less than current step: 499. Dropping entry: {'train/lr': 1.5061224489795917e-05, '_timestamp': 1740967219.3411016}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 380 is less than current step: 499. Dropping entry: {'train/loss': 1.1890409111976623, 'train/ce_loss': 0.4204345703125, 'train/seg_cls_loss': 0.008209228515625, 'train/kl_loss': 0.1826171875, 'train/mask_bce_loss': 0.09197547733783722, 'train/mask_dice_loss': 0.28099956214427946, 'train/mask_loss': 0.37297504395246506, 'metrics/total_secs_per_batch': 6.172940015792847, 'metrics/data_secs_per_batch': 3.1489983081817625, '_timestamp': 1740967225.513887}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 380 is less than current step: 499. Dropping entry: {'train/lr': 1.493877551020408e-05, '_timestamp': 1740967225.5141551}).
Epoch: [4][382/500]	Time  7.585 ( 7.585)	Loss 1.2422 (1.7278)	CeLoss 1.2422 (0.5650)	SegCLSLoss 0.0000 (0.0081)	KLLoss 0.0000 (0.2559)	MaskLoss 0.0000 (0.5665)	MaskBCELoss 0.0000 (0.1418)	MaskDICELoss 0.0000 (0.4247)
Epoch: [4][383/500]	Time  7.867 ( 7.867)	Loss 0.7894 (1.4087)	CeLoss 0.2695 (0.5003)	SegCLSLoss 0.0137 (0.0132)	KLLoss 0.3672 (0.2521)	MaskLoss 0.2375 (0.4382)	MaskBCELoss 0.1185 (0.0417)	MaskDICELoss 0.1190 (0.3966)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 381 is less than current step: 499. Dropping entry: {'train/loss': 1.7277897000312805, 'train/ce_loss': 0.5650390625, 'train/seg_cls_loss': 0.008148193359375, 'train/kl_loss': 0.255859375, 'train/mask_bce_loss': 0.1417838804423809, 'train/mask_dice_loss': 0.42469885349273684, 'train/mask_loss': 0.5664827361702919, 'metrics/total_secs_per_batch': 7.584710359573364, 'metrics/data_secs_per_batch': 3.412080717086792, '_timestamp': 1740967233.098516}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 381 is less than current step: 499. Dropping entry: {'train/lr': 1.4816326530612245e-05, '_timestamp': 1740967233.098778}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 382 is less than current step: 499. Dropping entry: {'train/loss': 1.4086588680744172, 'train/ce_loss': 0.50029296875, 'train/seg_cls_loss': 0.01322021484375, 'train/kl_loss': 0.2521484375, 'train/mask_bce_loss': 0.04165622079744935, 'train/mask_dice_loss': 0.3965599358081818, 'train/mask_loss': 0.43821615278720855, 'metrics/total_secs_per_batch': 7.867228984832764, 'metrics/data_secs_per_batch': 3.0804288387298584, '_timestamp': 1740967240.965803}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 382 is less than current step: 499. Dropping entry: {'train/lr': 1.4693877551020406e-05, '_timestamp': 1740967240.966006}).
Epoch: [4][384/500]	Time  7.926 ( 7.926)	Loss 2.2013 (1.8616)	CeLoss 0.1904 (0.3608)	SegCLSLoss 0.0242 (0.0187)	KLLoss 0.3750 (0.3311)	MaskLoss 0.9806 (0.7292)	MaskBCELoss 0.2371 (0.1234)	MaskDICELoss 0.7435 (0.6058)
Epoch: [4][385/500]	Time  7.020 ( 7.020)	Loss 0.0835 (1.4748)	CeLoss 0.0835 (0.5252)	SegCLSLoss 0.0000 (0.0090)	KLLoss 0.0000 (0.2166)	MaskLoss 0.0000 (0.4616)	MaskBCELoss 0.0000 (0.1094)	MaskDICELoss 0.0000 (0.3522)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 383 is less than current step: 499. Dropping entry: {'train/loss': 1.861568236351013, 'train/ce_loss': 0.36083984375, 'train/seg_cls_loss': 0.018670654296875, 'train/kl_loss': 0.3310546875, 'train/mask_bce_loss': 0.12338821608573199, 'train/mask_dice_loss': 0.6057845547795295, 'train/mask_loss': 0.7291727676987648, 'metrics/total_secs_per_batch': 7.925720453262329, 'metrics/data_secs_per_batch': 3.4580080032348635, '_timestamp': 1740967248.8914785}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 383 is less than current step: 499. Dropping entry: {'train/lr': 1.457142857142857e-05, '_timestamp': 1740967248.8917327}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 384 is less than current step: 499. Dropping entry: {'train/loss': 1.4747684717178344, 'train/ce_loss': 0.525244140625, 'train/seg_cls_loss': 0.00902099609375, 'train/kl_loss': 0.2166015625, 'train/mask_bce_loss': 0.10941770635545253, 'train/mask_dice_loss': 0.35220968425273896, 'train/mask_loss': 0.461627396941185, 'metrics/total_secs_per_batch': 7.0202836990356445, 'metrics/data_secs_per_batch': 3.0333815813064575, '_timestamp': 1740967255.911868}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 384 is less than current step: 499. Dropping entry: {'train/lr': 1.4448979591836734e-05, '_timestamp': 1740967255.9121485}).
Epoch: [4][386/500]	Time  9.549 ( 9.549)	Loss 2.2475 (2.0982)	CeLoss 0.1650 (0.4166)	SegCLSLoss 0.0183 (0.0151)	KLLoss 0.3613 (0.2908)	MaskLoss 1.0188 (0.8225)	MaskBCELoss 0.0391 (0.1216)	MaskDICELoss 0.9797 (0.7009)
Epoch: [4][387/500]	Time  6.072 ( 6.072)	Loss 1.4141 (1.7367)	CeLoss 1.4141 (0.5581)	SegCLSLoss 0.0000 (0.0135)	KLLoss 0.0000 (0.2594)	MaskLoss 0.0000 (0.5731)	MaskBCELoss 0.0000 (0.1508)	MaskDICELoss 0.0000 (0.4222)
Epoch: [4][388/500]	Time  6.975 ( 6.975)	Loss 2.1479 (1.7172)	CeLoss 0.2402 (0.2946)	SegCLSLoss 0.0090 (0.0109)	KLLoss 0.3633 (0.2957)	MaskLoss 0.9333 (0.6936)	MaskBCELoss 0.4534 (0.2267)	MaskDICELoss 0.4799 (0.4669)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 385 is less than current step: 499. Dropping entry: {'train/loss': 2.0981725335121153, 'train/ce_loss': 0.4166015625, 'train/seg_cls_loss': 0.01510009765625, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.12159792967140674, 'train/mask_dice_loss': 0.7009258151054383, 'train/mask_loss': 0.8225237429141998, 'metrics/total_secs_per_batch': 9.549064874649048, 'metrics/data_secs_per_batch': 4.656855773925781, '_timestamp': 1740967265.4608912}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 385 is less than current step: 499. Dropping entry: {'train/lr': 1.4326530612244895e-05, '_timestamp': 1740967265.4611886}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 386 is less than current step: 499. Dropping entry: {'train/loss': 1.7367253601551056, 'train/ce_loss': 0.55810546875, 'train/seg_cls_loss': 0.013494873046875, 'train/kl_loss': 0.259375, 'train/mask_bce_loss': 0.15082015916705133, 'train/mask_dice_loss': 0.4222300246357918, 'train/mask_loss': 0.5730501800775528, 'metrics/total_secs_per_batch': 6.071884870529175, 'metrics/data_secs_per_batch': 2.6862205982208254, '_timestamp': 1740967271.5327642}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 386 is less than current step: 499. Dropping entry: {'train/lr': 1.420408163265306e-05, '_timestamp': 1740967271.5330193}).
Epoch: [4][389/500]	Time  9.420 ( 9.420)	Loss 0.0552 (1.7319)	CeLoss 0.0552 (0.1996)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.3295)	MaskLoss 0.0000 (0.7460)	MaskBCELoss 0.0000 (0.1341)	MaskDICELoss 0.0000 (0.6119)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 387 is less than current step: 499. Dropping entry: {'train/loss': 1.7172491788864135, 'train/ce_loss': 0.29462890625, 'train/seg_cls_loss': 0.0109375, 'train/kl_loss': 0.295703125, 'train/mask_bce_loss': 0.22668580282479525, 'train/mask_dice_loss': 0.46689972579479216, 'train/mask_loss': 0.6935855209827423, 'metrics/total_secs_per_batch': 6.975414276123047, 'metrics/data_secs_per_batch': 3.3766234159469604, '_timestamp': 1740967278.5082157}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 387 is less than current step: 499. Dropping entry: {'train/lr': 1.4081632653061223e-05, '_timestamp': 1740967278.5084944}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 388 is less than current step: 499. Dropping entry: {'train/loss': 1.7318613171577453, 'train/ce_loss': 0.199560546875, 'train/seg_cls_loss': 0.014166259765625, 'train/kl_loss': 0.3294921875, 'train/mask_bce_loss': 0.1341012722812593, 'train/mask_dice_loss': 0.6118830919265748, 'train/mask_loss': 0.7459843695163727, 'metrics/total_secs_per_batch': 9.420119524002075, 'metrics/data_secs_per_batch': 4.239566707611084, '_timestamp': 1740967287.9285767}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 388 is less than current step: 499. Dropping entry: {'train/lr': 1.3959183673469388e-05, '_timestamp': 1740967287.9289544}).
[2025-03-02 20:01:34,900] [INFO] [logging.py:96:log_dist] [Rank 0] step=2390, skipped=0, lr=[1.3897959183673468e-05], mom=[(0.9, 0.95)]
[2025-03-02 20:01:34,906] [INFO] [timer.py:215:stop] epoch=0/micro_step=23900/global_step=2390, RunningAvgSamplesPerSec=1.4489599264014126, CurrSamplesPerSec=1.4332624624910721, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [4][390/500]	Time  6.979 ( 6.979)	Loss 1.2468 (1.4836)	CeLoss 0.2139 (0.4297)	SegCLSLoss 0.0109 (0.0102)	KLLoss 0.3691 (0.2221)	MaskLoss 0.4955 (0.5132)	MaskBCELoss 0.0829 (0.1300)	MaskDICELoss 0.4126 (0.3832)
Epoch: [4][391/500]	Time  9.728 ( 9.728)	Loss 2.1924 (1.7368)	CeLoss 0.1504 (0.2181)	SegCLSLoss 0.0248 (0.0167)	KLLoss 0.3770 (0.3281)	MaskLoss 0.9956 (0.7387)	MaskBCELoss 0.2169 (0.1497)	MaskDICELoss 0.7787 (0.5889)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 389 is less than current step: 499. Dropping entry: {'train/loss': 1.4835686206817627, 'train/ce_loss': 0.429736328125, 'train/seg_cls_loss': 0.010186767578125, 'train/kl_loss': 0.2220703125, 'train/mask_bce_loss': 0.13004137799143792, 'train/mask_dice_loss': 0.38315405547618864, 'train/mask_loss': 0.5131954371929168, 'metrics/total_secs_per_batch': 6.978930234909058, 'metrics/data_secs_per_batch': 3.3741600036621096, '_timestamp': 1740967294.9070716}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 389 is less than current step: 499. Dropping entry: {'train/lr': 1.383673469387755e-05, '_timestamp': 1740967294.9073462}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 390 is less than current step: 499. Dropping entry: {'train/loss': 1.7368030786514281, 'train/ce_loss': 0.21806640625, 'train/seg_cls_loss': 0.016705322265625, 'train/kl_loss': 0.328125, 'train/mask_bce_loss': 0.14971622303128243, 'train/mask_dice_loss': 0.588949003815651, 'train/mask_loss': 0.7386652231216431, 'metrics/total_secs_per_batch': 9.727808475494385, 'metrics/data_secs_per_batch': 4.465573406219482, '_timestamp': 1740967304.6354296}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 390 is less than current step: 499. Dropping entry: {'train/lr': 1.3714285714285712e-05, '_timestamp': 1740967304.6358907}).
Epoch: [4][392/500]	Time  7.846 ( 7.846)	Loss 2.4913 (1.9914)	CeLoss 0.1816 (0.3355)	SegCLSLoss 0.0216 (0.0142)	KLLoss 0.3848 (0.3273)	MaskLoss 1.1304 (0.8081)	MaskBCELoss 0.1938 (0.2413)	MaskDICELoss 0.9366 (0.5668)
Epoch: [4][393/500]	Time  8.619 ( 8.619)	Loss 1.6639 (1.3980)	CeLoss 0.2041 (0.2428)	SegCLSLoss 0.0126 (0.0150)	KLLoss 0.3711 (0.3256)	MaskLoss 0.7084 (0.5576)	MaskBCELoss 0.1428 (0.0943)	MaskDICELoss 0.5656 (0.4633)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 391 is less than current step: 499. Dropping entry: {'train/loss': 1.9913522243499755, 'train/ce_loss': 0.335546875, 'train/seg_cls_loss': 0.0141845703125, 'train/kl_loss': 0.32734375, 'train/mask_bce_loss': 0.24132727067917586, 'train/mask_dice_loss': 0.5667511999607087, 'train/mask_loss': 0.8080784618854523, 'metrics/total_secs_per_batch': 7.846349239349365, 'metrics/data_secs_per_batch': 3.471208930015564, '_timestamp': 1740967312.4814239}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 391 is less than current step: 499. Dropping entry: {'train/lr': 1.3591836734693877e-05, '_timestamp': 1740967312.4817052}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 392 is less than current step: 499. Dropping entry: {'train/loss': 1.3980139017105102, 'train/ce_loss': 0.2427734375, 'train/seg_cls_loss': 0.0150146484375, 'train/kl_loss': 0.3255859375, 'train/mask_bce_loss': 0.09428097493946552, 'train/mask_dice_loss': 0.4632708981633186, 'train/mask_loss': 0.5575518697500229, 'metrics/total_secs_per_batch': 8.618722915649414, 'metrics/data_secs_per_batch': 3.980513262748718, '_timestamp': 1740967321.1001406}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 392 is less than current step: 499. Dropping entry: {'train/lr': 1.3469387755102039e-05, '_timestamp': 1740967321.1004167}).
Epoch: [4][394/500]	Time  8.189 ( 8.189)	Loss 1.8307 (1.7907)	CeLoss 0.2227 (0.2911)	SegCLSLoss 0.0164 (0.0129)	KLLoss 0.3633 (0.2930)	MaskLoss 0.7815 (0.7317)	MaskBCELoss 0.0389 (0.1709)	MaskDICELoss 0.7426 (0.5608)
Epoch: [4][395/500]	Time  8.172 ( 8.172)	Loss 2.5117 (1.4499)	CeLoss 0.2070 (0.2420)	SegCLSLoss 0.0178 (0.0140)	KLLoss 0.3633 (0.3279)	MaskLoss 1.1299 (0.5840)	MaskBCELoss 0.2783 (0.1372)	MaskDICELoss 0.8516 (0.4468)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 393 is less than current step: 499. Dropping entry: {'train/loss': 1.7906696200370789, 'train/ce_loss': 0.29111328125, 'train/seg_cls_loss': 0.012921142578125, 'train/kl_loss': 0.29296875, 'train/mask_bce_loss': 0.17091046795248985, 'train/mask_dice_loss': 0.5607524707913398, 'train/mask_loss': 0.7316629528999329, 'metrics/total_secs_per_batch': 8.189475774765015, 'metrics/data_secs_per_batch': 2.873405933380127, '_timestamp': 1740967329.289606}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 393 is less than current step: 499. Dropping entry: {'train/lr': 1.3346938775510203e-05, '_timestamp': 1740967329.2897918}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 394 is less than current step: 499. Dropping entry: {'train/loss': 1.4498748660087586, 'train/ce_loss': 0.2419921875, 'train/seg_cls_loss': 0.01397705078125, 'train/kl_loss': 0.3279296875, 'train/mask_bce_loss': 0.13715336471796036, 'train/mask_dice_loss': 0.44681727662682535, 'train/mask_loss': 0.5839706376194954, 'metrics/total_secs_per_batch': 8.171938180923462, 'metrics/data_secs_per_batch': 3.660582089424133, '_timestamp': 1740967337.4617445}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 394 is less than current step: 499. Dropping entry: {'train/lr': 1.3224489795918366e-05, '_timestamp': 1740967337.4620893}).
Epoch: [4][396/500]	Time  7.803 ( 7.803)	Loss 1.4375 (1.9551)	CeLoss 1.4375 (0.5437)	SegCLSLoss 0.0000 (0.0139)	KLLoss 0.0000 (0.2564)	MaskLoss 0.0000 (0.6893)	MaskBCELoss 0.0000 (0.1225)	MaskDICELoss 0.0000 (0.5669)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 395 is less than current step: 499. Dropping entry: {'train/loss': 1.9550572752952575, 'train/ce_loss': 0.54365234375, 'train/seg_cls_loss': 0.0138671875, 'train/kl_loss': 0.2564453125, 'train/mask_bce_loss': 0.12249065767973662, 'train/mask_dice_loss': 0.5668543934822082, 'train/mask_loss': 0.6893450558185578, 'metrics/total_secs_per_batch': 7.803454637527466, 'metrics/data_secs_per_batch': 3.965462899208069, '_timestamp': 1740967345.265035}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 395 is less than current step: 499. Dropping entry: {'train/lr': 1.3102040816326531e-05, '_timestamp': 1740967345.2652338}).
Epoch: [4][397/500]	Time  8.652 ( 8.652)	Loss 2.6674 (1.9701)	CeLoss 0.2168 (0.3506)	SegCLSLoss 0.0216 (0.0173)	KLLoss 0.3613 (0.2918)	MaskLoss 1.2019 (0.7909)	MaskBCELoss 0.3339 (0.1819)	MaskDICELoss 0.8680 (0.6089)
Epoch: [4][398/500]	Time  7.686 ( 7.686)	Loss 2.3299 (1.6740)	CeLoss 0.2139 (0.3485)	SegCLSLoss 0.0190 (0.0141)	KLLoss 0.3613 (0.2592)	MaskLoss 1.0350 (0.6461)	MaskBCELoss 0.2809 (0.1772)	MaskDICELoss 0.7541 (0.4689)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 396 is less than current step: 499. Dropping entry: {'train/loss': 1.9701191663742066, 'train/ce_loss': 0.350634765625, 'train/seg_cls_loss': 0.017279052734375, 'train/kl_loss': 0.291796875, 'train/mask_bce_loss': 0.18192653339356185, 'train/mask_dice_loss': 0.6089436113834381, 'train/mask_loss': 0.7908701360225677, 'metrics/total_secs_per_batch': 8.651799440383911, 'metrics/data_secs_per_batch': 3.9900253772735597, '_timestamp': 1740967353.9168174}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 396 is less than current step: 499. Dropping entry: {'train/lr': 1.2979591836734692e-05, '_timestamp': 1740967353.9171338}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 397 is less than current step: 499. Dropping entry: {'train/loss': 1.673966360092163, 'train/ce_loss': 0.348486328125, 'train/seg_cls_loss': 0.01407470703125, 'train/kl_loss': 0.2591796875, 'train/mask_bce_loss': 0.17723018415272235, 'train/mask_dice_loss': 0.4689082741737366, 'train/mask_loss': 0.6461384639143943, 'metrics/total_secs_per_batch': 7.68633770942688, 'metrics/data_secs_per_batch': 3.504475498199463, '_timestamp': 1740967361.6031609}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 397 is less than current step: 499. Dropping entry: {'train/lr': 1.2857142857142856e-05, '_timestamp': 1740967361.603422}).
Epoch: [4][399/500]	Time  8.609 ( 8.609)	Loss 1.0157 (1.5271)	CeLoss 0.2988 (0.3318)	SegCLSLoss 0.0095 (0.0136)	KLLoss 0.3672 (0.3252)	MaskLoss 0.3380 (0.5778)	MaskBCELoss 0.1300 (0.1218)	MaskDICELoss 0.2080 (0.4560)
[2025-03-02 20:02:58,905] [INFO] [logging.py:96:log_dist] [Rank 0] step=2400, skipped=0, lr=[1.26734693877551e-05], mom=[(0.9, 0.95)]
[2025-03-02 20:02:58,911] [INFO] [timer.py:215:stop] epoch=0/micro_step=24000/global_step=2400, RunningAvgSamplesPerSec=1.4476840537615046, CurrSamplesPerSec=1.1497100111925505, MemAllocated=31.56GB, MaxMemAllocated=37.23GB
Epoch: [4][400/500]	Time  8.699 ( 8.699)	Loss 2.2119 (1.8361)	CeLoss 0.2236 (0.3200)	SegCLSLoss 0.0183 (0.0130)	KLLoss 0.3613 (0.2906)	MaskLoss 0.9712 (0.7400)	MaskBCELoss 0.0501 (0.1217)	MaskDICELoss 0.9211 (0.6183)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 398 is less than current step: 499. Dropping entry: {'train/loss': 1.5270509719848633, 'train/ce_loss': 0.3318359375, 'train/seg_cls_loss': 0.01363525390625, 'train/kl_loss': 0.3251953125, 'train/mask_bce_loss': 0.12177367014810443, 'train/mask_dice_loss': 0.4560096263885498, 'train/mask_loss': 0.5777832865715027, 'metrics/total_secs_per_batch': 8.609467029571533, 'metrics/data_secs_per_batch': 4.1574266195297245, '_timestamp': 1740967370.212624}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 398 is less than current step: 499. Dropping entry: {'train/lr': 1.273469387755102e-05, '_timestamp': 1740967370.2128882}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 399 is less than current step: 499. Dropping entry: {'train/loss': 1.8360833287239076, 'train/ce_loss': 0.319970703125, 'train/seg_cls_loss': 0.0129638671875, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.12172619886696338, 'train/mask_dice_loss': 0.6183125287294388, 'train/mask_loss': 0.7400387346744537, 'metrics/total_secs_per_batch': 8.699354887008667, 'metrics/data_secs_per_batch': 3.6835615396499635, '_timestamp': 1740967378.9118104}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 399 is less than current step: 499. Dropping entry: {'train/lr': 1.2612244897959182e-05, '_timestamp': 1740967378.9120896}).
Epoch: [4][401/500]	Time  7.784 ( 7.784)	Loss 1.3469 (1.3595)	CeLoss 0.2021 (0.5184)	SegCLSLoss 0.0192 (0.0096)	KLLoss 0.3516 (0.2178)	MaskLoss 0.5499 (0.4072)	MaskBCELoss 0.1210 (0.0768)	MaskDICELoss 0.4289 (0.3305)
Epoch: [4][402/500]	Time  7.585 ( 7.585)	Loss 1.5046 (1.5082)	CeLoss 0.2734 (0.4231)	SegCLSLoss 0.0137 (0.0129)	KLLoss 0.3672 (0.2551)	MaskLoss 0.5931 (0.5266)	MaskBCELoss 0.0717 (0.0944)	MaskDICELoss 0.5214 (0.4321)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 400 is less than current step: 499. Dropping entry: {'train/loss': 1.3594961285591125, 'train/ce_loss': 0.518359375, 'train/seg_cls_loss': 0.009588623046875, 'train/kl_loss': 0.2177734375, 'train/mask_bce_loss': 0.07675766982138157, 'train/mask_dice_loss': 0.33048064112663267, 'train/mask_loss': 0.407238307595253, 'metrics/total_secs_per_batch': 7.783620357513428, 'metrics/data_secs_per_batch': 3.1159674882888795, '_timestamp': 1740967386.6955924}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 400 is less than current step: 499. Dropping entry: {'train/lr': 1.2489795918367346e-05, '_timestamp': 1740967386.6957748}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 401 is less than current step: 499. Dropping entry: {'train/loss': 1.5082025527954102, 'train/ce_loss': 0.42314453125, 'train/seg_cls_loss': 0.012908935546875, 'train/kl_loss': 0.255078125, 'train/mask_bce_loss': 0.09442097060382366, 'train/mask_dice_loss': 0.43214123845100405, 'train/mask_loss': 0.5265622138977051, 'metrics/total_secs_per_batch': 7.585331201553345, 'metrics/data_secs_per_batch': 3.1518896579742433, '_timestamp': 1740967394.2811482}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 401 is less than current step: 499. Dropping entry: {'train/lr': 1.236734693877551e-05, '_timestamp': 1740967394.2815437}).
Epoch: [4][403/500]	Time  7.596 ( 7.596)	Loss 1.8947 (2.0962)	CeLoss 0.2236 (0.3745)	SegCLSLoss 0.0204 (0.0137)	KLLoss 0.3770 (0.2895)	MaskLoss 0.8116 (0.8429)	MaskBCELoss 0.2020 (0.2322)	MaskDICELoss 0.6096 (0.6108)
Epoch: [4][404/500]	Time  6.540 ( 6.540)	Loss 1.3438 (1.5604)	CeLoss 1.3438 (0.7477)	SegCLSLoss 0.0000 (0.0119)	KLLoss 0.0000 (0.1803)	MaskLoss 0.0000 (0.3945)	MaskBCELoss 0.0000 (0.0632)	MaskDICELoss 0.0000 (0.3313)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 402 is less than current step: 499. Dropping entry: {'train/loss': 2.096186935901642, 'train/ce_loss': 0.37451171875, 'train/seg_cls_loss': 0.013653564453125, 'train/kl_loss': 0.289453125, 'train/mask_bce_loss': 0.232157166255638, 'train/mask_dice_loss': 0.610760498046875, 'train/mask_loss': 0.8429176688194275, 'metrics/total_secs_per_batch': 7.596032381057739, 'metrics/data_secs_per_batch': 3.6534455537796022, '_timestamp': 1740967401.8769927}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 402 is less than current step: 499. Dropping entry: {'train/lr': 1.224489795918367e-05, '_timestamp': 1740967401.8772798}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 403 is less than current step: 499. Dropping entry: {'train/loss': 1.5604232549667358, 'train/ce_loss': 0.74765625, 'train/seg_cls_loss': 0.01192626953125, 'train/kl_loss': 0.1802734375, 'train/mask_bce_loss': 0.06316783651709557, 'train/mask_dice_loss': 0.33130159676074983, 'train/mask_loss': 0.3944694399833679, 'metrics/total_secs_per_batch': 6.540477991104126, 'metrics/data_secs_per_batch': 3.124738883972168, '_timestamp': 1740967408.4175522}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 403 is less than current step: 499. Dropping entry: {'train/lr': 1.2122448979591836e-05, '_timestamp': 1740967408.4178746}).
Epoch: [4][405/500]	Time  8.160 ( 8.160)	Loss 1.1172 (1.6430)	CeLoss 1.1172 (0.3045)	SegCLSLoss 0.0000 (0.0151)	KLLoss 0.0000 (0.3332)	MaskLoss 0.0000 (0.6488)	MaskBCELoss 0.0000 (0.1268)	MaskDICELoss 0.0000 (0.5220)
Epoch: [4][406/500]	Time  8.132 ( 8.132)	Loss 1.5312 (1.4417)	CeLoss 1.5312 (0.3995)	SegCLSLoss 0.0000 (0.0126)	KLLoss 0.0000 (0.3291)	MaskLoss 0.0000 (0.5015)	MaskBCELoss 0.0000 (0.1048)	MaskDICELoss 0.0000 (0.3967)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 404 is less than current step: 499. Dropping entry: {'train/loss': 1.643024218082428, 'train/ce_loss': 0.3044921875, 'train/seg_cls_loss': 0.0151123046875, 'train/kl_loss': 0.333203125, 'train/mask_bce_loss': 0.12676013689488172, 'train/mask_dice_loss': 0.5220468908548355, 'train/mask_loss': 0.648807030916214, 'metrics/total_secs_per_batch': 8.159745216369629, 'metrics/data_secs_per_batch': 3.7625171661376955, '_timestamp': 1740967416.5772378}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 404 is less than current step: 499. Dropping entry: {'train/lr': 1.1999999999999999e-05, '_timestamp': 1740967416.5775492}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 405 is less than current step: 499. Dropping entry: {'train/loss': 1.441664183139801, 'train/ce_loss': 0.39951171875, 'train/seg_cls_loss': 0.0125732421875, 'train/kl_loss': 0.3291015625, 'train/mask_bce_loss': 0.10481215948238969, 'train/mask_dice_loss': 0.39673281125724313, 'train/mask_loss': 0.5015449695289135, 'metrics/total_secs_per_batch': 8.131658554077148, 'metrics/data_secs_per_batch': 3.799726438522339, '_timestamp': 1740967424.7088604}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 405 is less than current step: 499. Dropping entry: {'train/lr': 1.1877551020408163e-05, '_timestamp': 1740967424.7091334}).
Epoch: [4][407/500]	Time  6.802 ( 6.802)	Loss 1.4175 (1.6352)	CeLoss 0.2480 (0.3865)	SegCLSLoss 0.0123 (0.0146)	KLLoss 0.3652 (0.2945)	MaskLoss 0.5632 (0.6060)	MaskBCELoss 0.1906 (0.1285)	MaskDICELoss 0.3727 (0.4774)
Epoch: [4][408/500]	Time  7.665 ( 7.665)	Loss 2.7657 (1.9019)	CeLoss 0.2061 (0.5042)	SegCLSLoss 0.0220 (0.0126)	KLLoss 0.3711 (0.2557)	MaskLoss 1.2559 (0.6829)	MaskBCELoss 0.4904 (0.1977)	MaskDICELoss 0.7655 (0.4852)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 406 is less than current step: 499. Dropping entry: {'train/loss': 1.6351572513580321, 'train/ce_loss': 0.386474609375, 'train/seg_cls_loss': 0.01456298828125, 'train/kl_loss': 0.29453125, 'train/mask_bce_loss': 0.12853753492236136, 'train/mask_dice_loss': 0.47741998732089996, 'train/mask_loss': 0.6059575185179711, 'metrics/total_secs_per_batch': 6.801775932312012, 'metrics/data_secs_per_batch': 2.8650256156921388, '_timestamp': 1740967431.5106442}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 406 is less than current step: 499. Dropping entry: {'train/lr': 1.1755102040816325e-05, '_timestamp': 1740967431.510921}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 407 is less than current step: 499. Dropping entry: {'train/loss': 1.9018994450569153, 'train/ce_loss': 0.50419921875, 'train/seg_cls_loss': 0.01259765625, 'train/kl_loss': 0.2556640625, 'train/mask_bce_loss': 0.1977170415222645, 'train/mask_dice_loss': 0.48516627252101896, 'train/mask_loss': 0.6828833222389221, 'metrics/total_secs_per_batch': 7.6647045612335205, 'metrics/data_secs_per_batch': 3.574975609779358, '_timestamp': 1740967439.1755552}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 407 is less than current step: 499. Dropping entry: {'train/lr': 1.163265306122449e-05, '_timestamp': 1740967439.1759086}).
Epoch: [4][409/500]	Time  9.283 ( 9.283)	Loss 1.0000 (1.4329)	CeLoss 1.0000 (0.3880)	SegCLSLoss 0.0000 (0.0104)	KLLoss 0.0000 (0.2180)	MaskLoss 0.0000 (0.5089)	MaskBCELoss 0.0000 (0.0751)	MaskDICELoss 0.0000 (0.4338)
[2025-03-02 20:04:17,399] [INFO] [logging.py:96:log_dist] [Rank 0] step=2410, skipped=0, lr=[1.1448979591836734e-05], mom=[(0.9, 0.95)]
[2025-03-02 20:04:17,404] [INFO] [timer.py:215:stop] epoch=0/micro_step=24100/global_step=2410, RunningAvgSamplesPerSec=1.4468658947642148, CurrSamplesPerSec=1.1178546209370934, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [4][410/500]	Time  8.947 ( 8.947)	Loss 1.5807 (1.3431)	CeLoss 0.2871 (0.3473)	SegCLSLoss 0.0128 (0.0102)	KLLoss 0.3633 (0.2941)	MaskLoss 0.6253 (0.4807)	MaskBCELoss 0.1167 (0.1164)	MaskDICELoss 0.5086 (0.3642)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 408 is less than current step: 499. Dropping entry: {'train/loss': 1.4328820347785949, 'train/ce_loss': 0.388037109375, 'train/seg_cls_loss': 0.01043701171875, 'train/kl_loss': 0.21796875, 'train/mask_bce_loss': 0.07505493378266692, 'train/mask_dice_loss': 0.433842146396637, 'train/mask_loss': 0.5088970720767975, 'metrics/total_secs_per_batch': 9.283100128173828, 'metrics/data_secs_per_batch': 4.687963581085205, '_timestamp': 1740967448.4584446}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 408 is less than current step: 499. Dropping entry: {'train/lr': 1.1510204081632653e-05, '_timestamp': 1740967448.4586308}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 409 is less than current step: 499. Dropping entry: {'train/loss': 1.3430968821048737, 'train/ce_loss': 0.347265625, 'train/seg_cls_loss': 0.0102294921875, 'train/kl_loss': 0.294140625, 'train/mask_bce_loss': 0.11644923314452171, 'train/mask_dice_loss': 0.3642300501465797, 'train/mask_loss': 0.4806792914867401, 'metrics/total_secs_per_batch': 8.947139501571655, 'metrics/data_secs_per_batch': 3.673148274421692, '_timestamp': 1740967457.4054203}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 409 is less than current step: 499. Dropping entry: {'train/lr': 1.1387755102040814e-05, '_timestamp': 1740967457.4057026}).
Epoch: [4][411/500]	Time  8.160 ( 8.160)	Loss 2.6503 (1.6905)	CeLoss 0.2227 (0.6266)	SegCLSLoss 0.0142 (0.0117)	KLLoss 0.3613 (0.2170)	MaskLoss 1.1923 (0.5182)	MaskBCELoss 0.4450 (0.1122)	MaskDICELoss 0.7473 (0.4060)
Epoch: [4][412/500]	Time  7.094 ( 7.094)	Loss 1.3203 (1.5486)	CeLoss 1.3203 (0.6033)	SegCLSLoss 0.0000 (0.0111)	KLLoss 0.0000 (0.2180)	MaskLoss 0.0000 (0.4590)	MaskBCELoss 0.0000 (0.0561)	MaskDICELoss 0.0000 (0.4029)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 410 is less than current step: 499. Dropping entry: {'train/loss': 1.690513586997986, 'train/ce_loss': 0.6265625, 'train/seg_cls_loss': 0.011663818359375, 'train/kl_loss': 0.2169921875, 'train/mask_bce_loss': 0.11215910380706191, 'train/mask_dice_loss': 0.40599806904792785, 'train/mask_loss': 0.5181571841239929, 'metrics/total_secs_per_batch': 8.159799337387085, 'metrics/data_secs_per_batch': 3.210588479042053, '_timestamp': 1740967465.5653872}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 410 is less than current step: 499. Dropping entry: {'train/lr': 1.1265306122448979e-05, '_timestamp': 1740967465.565569}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 411 is less than current step: 499. Dropping entry: {'train/loss': 1.5486281275749207, 'train/ce_loss': 0.6033203125, 'train/seg_cls_loss': 0.011083984375, 'train/kl_loss': 0.21796875, 'train/mask_bce_loss': 0.056120861507952216, 'train/mask_dice_loss': 0.40286117792129517, 'train/mask_loss': 0.45898203253746034, 'metrics/total_secs_per_batch': 7.093783855438232, 'metrics/data_secs_per_batch': 3.038266491889954, '_timestamp': 1740967472.6591756}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 411 is less than current step: 499. Dropping entry: {'train/lr': 1.1142857142857142e-05, '_timestamp': 1740967472.6594434}).
Epoch: [4][413/500]	Time  7.528 ( 7.528)	Loss 2.5493 (1.7754)	CeLoss 0.1299 (0.4970)	SegCLSLoss 0.0259 (0.0124)	KLLoss 0.3613 (0.2201)	MaskLoss 1.1853 (0.6250)	MaskBCELoss 0.3239 (0.1768)	MaskDICELoss 0.8614 (0.4481)
Epoch: [4][414/500]	Time  7.076 ( 7.076)	Loss 1.5487 (1.9321)	CeLoss 0.2441 (0.2739)	SegCLSLoss 0.0106 (0.0162)	KLLoss 0.3633 (0.3285)	MaskLoss 0.6318 (0.8086)	MaskBCELoss 0.0998 (0.1535)	MaskDICELoss 0.5320 (0.6550)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 412 is less than current step: 499. Dropping entry: {'train/loss': 1.7754404067993164, 'train/ce_loss': 0.49697265625, 'train/seg_cls_loss': 0.012371826171875, 'train/kl_loss': 0.2201171875, 'train/mask_bce_loss': 0.17683927416801454, 'train/mask_dice_loss': 0.4481367886066437, 'train/mask_loss': 0.6249760627746582, 'metrics/total_secs_per_batch': 7.528259038925171, 'metrics/data_secs_per_batch': 3.039396381378174, '_timestamp': 1740967480.1876314}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 412 is less than current step: 499. Dropping entry: {'train/lr': 1.1020408163265306e-05, '_timestamp': 1740967480.187964}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 413 is less than current step: 499. Dropping entry: {'train/loss': 1.9321479082107544, 'train/ce_loss': 0.27392578125, 'train/seg_cls_loss': 0.016241455078125, 'train/kl_loss': 0.328515625, 'train/mask_bce_loss': 0.15351512432098388, 'train/mask_dice_loss': 0.6550392836332322, 'train/mask_loss': 0.8085544109344482, 'metrics/total_secs_per_batch': 7.075649738311768, 'metrics/data_secs_per_batch': 3.2421716690063476, '_timestamp': 1740967487.2631118}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 413 is less than current step: 499. Dropping entry: {'train/lr': 1.0897959183673468e-05, '_timestamp': 1740967487.263384}).
Epoch: [4][415/500]	Time  7.556 ( 7.556)	Loss 1.6644 (1.2441)	CeLoss 0.2363 (0.5933)	SegCLSLoss 0.0190 (0.0071)	KLLoss 0.3555 (0.1428)	MaskLoss 0.6916 (0.3166)	MaskBCELoss 0.0323 (0.0237)	MaskDICELoss 0.6593 (0.2928)
Epoch: [4][416/500]	Time  6.126 ( 6.126)	Loss 1.4262 (1.3136)	CeLoss 0.2988 (0.5945)	SegCLSLoss 0.0094 (0.0083)	KLLoss 0.3711 (0.1807)	MaskLoss 0.5432 (0.3483)	MaskBCELoss 0.1585 (0.0641)	MaskDICELoss 0.3847 (0.2842)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 414 is less than current step: 499. Dropping entry: {'train/loss': 1.2441411375999452, 'train/ce_loss': 0.59326171875, 'train/seg_cls_loss': 0.007147216796875, 'train/kl_loss': 0.1427734375, 'train/mask_bce_loss': 0.023717785999178887, 'train/mask_dice_loss': 0.29283520579338074, 'train/mask_loss': 0.31655299067497256, 'metrics/total_secs_per_batch': 7.55588960647583, 'metrics/data_secs_per_batch': 3.003135895729065, '_timestamp': 1740967494.8189983}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 414 is less than current step: 499. Dropping entry: {'train/lr': 1.0775510204081633e-05, '_timestamp': 1740967494.8193052}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 415 is less than current step: 499. Dropping entry: {'train/loss': 1.3135709762573242, 'train/ce_loss': 0.59453125, 'train/seg_cls_loss': 0.008306884765625, 'train/kl_loss': 0.1806640625, 'train/mask_bce_loss': 0.06411185041069985, 'train/mask_dice_loss': 0.28417753875255586, 'train/mask_loss': 0.3482893884181976, 'metrics/total_secs_per_batch': 6.125555038452148, 'metrics/data_secs_per_batch': 2.419837737083435, '_timestamp': 1740967500.944567}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 415 is less than current step: 499. Dropping entry: {'train/lr': 1.0653061224489796e-05, '_timestamp': 1740967500.9448283}).
Epoch: [4][417/500]	Time  6.780 ( 6.780)	Loss 1.5078 (1.7762)	CeLoss 1.5078 (0.7035)	SegCLSLoss 0.0000 (0.0098)	KLLoss 0.0000 (0.2561)	MaskLoss 0.0000 (0.5211)	MaskBCELoss 0.0000 (0.1416)	MaskDICELoss 0.0000 (0.3795)
Epoch: [4][418/500]	Time  7.381 ( 7.381)	Loss 1.2812 (1.3615)	CeLoss 1.2812 (0.4403)	SegCLSLoss 0.0000 (0.0106)	KLLoss 0.0000 (0.2525)	MaskLoss 0.0000 (0.4452)	MaskBCELoss 0.0000 (0.0605)	MaskDICELoss 0.0000 (0.3847)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 416 is less than current step: 499. Dropping entry: {'train/loss': 1.7762244820594788, 'train/ce_loss': 0.703515625, 'train/seg_cls_loss': 0.00977783203125, 'train/kl_loss': 0.2560546875, 'train/mask_bce_loss': 0.14157602097839117, 'train/mask_dice_loss': 0.3794952124357224, 'train/mask_loss': 0.5210712313652038, 'metrics/total_secs_per_batch': 6.780139684677124, 'metrics/data_secs_per_batch': 3.255308723449707, '_timestamp': 1740967507.7246914}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 416 is less than current step: 499. Dropping entry: {'train/lr': 1.0530612244897957e-05, '_timestamp': 1740967507.7249722}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 417 is less than current step: 499. Dropping entry: {'train/loss': 1.3615113198757172, 'train/ce_loss': 0.44033203125, 'train/seg_cls_loss': 0.0105712890625, 'train/kl_loss': 0.2525390625, 'train/mask_bce_loss': 0.06052537942305207, 'train/mask_dice_loss': 0.3846834108233452, 'train/mask_loss': 0.4452087879180908, 'metrics/total_secs_per_batch': 7.380501747131348, 'metrics/data_secs_per_batch': 3.4366238117218018, '_timestamp': 1740967515.1051962}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 417 is less than current step: 499. Dropping entry: {'train/lr': 1.0408163265306122e-05, '_timestamp': 1740967515.1054652}).
Epoch: [4][419/500]	Time  8.042 ( 8.042)	Loss 2.1370 (1.8181)	CeLoss 0.1484 (0.3807)	SegCLSLoss 0.0267 (0.0138)	KLLoss 0.3828 (0.2918)	MaskLoss 0.9684 (0.7006)	MaskBCELoss 0.2274 (0.1506)	MaskDICELoss 0.7410 (0.5500)
[2025-03-02 20:05:30,513] [INFO] [logging.py:96:log_dist] [Rank 0] step=2420, skipped=0, lr=[1.0224489795918366e-05], mom=[(0.9, 0.95)]
[2025-03-02 20:05:30,519] [INFO] [timer.py:215:stop] epoch=0/micro_step=24200/global_step=2420, RunningAvgSamplesPerSec=1.4465207345019762, CurrSamplesPerSec=1.3565756044773583, MemAllocated=31.45GB, MaxMemAllocated=37.23GB
Epoch: [4][420/500]	Time  7.373 ( 7.373)	Loss 1.6445 (1.8746)	CeLoss 0.2324 (0.4555)	SegCLSLoss 0.0192 (0.0139)	KLLoss 0.3613 (0.2926)	MaskLoss 0.6836 (0.6915)	MaskBCELoss 0.0599 (0.2008)	MaskDICELoss 0.6236 (0.4907)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 418 is less than current step: 499. Dropping entry: {'train/loss': 1.818145513534546, 'train/ce_loss': 0.3806640625, 'train/seg_cls_loss': 0.01383056640625, 'train/kl_loss': 0.291796875, 'train/mask_bce_loss': 0.1506249252706766, 'train/mask_dice_loss': 0.549951757490635, 'train/mask_loss': 0.7005766689777374, 'metrics/total_secs_per_batch': 8.041528463363647, 'metrics/data_secs_per_batch': 3.49006929397583, '_timestamp': 1740967523.1467147}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 418 is less than current step: 499. Dropping entry: {'train/lr': 1.0285714285714285e-05, '_timestamp': 1740967523.1469712}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 419 is less than current step: 499. Dropping entry: {'train/loss': 1.8745940923690796, 'train/ce_loss': 0.45546875, 'train/seg_cls_loss': 0.01387939453125, 'train/kl_loss': 0.292578125, 'train/mask_bce_loss': 0.20076278373599052, 'train/mask_dice_loss': 0.4907334804534912, 'train/mask_loss': 0.6914962619543076, 'metrics/total_secs_per_batch': 7.372949838638306, 'metrics/data_secs_per_batch': 3.0559173345565798, '_timestamp': 1740967530.5195076}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 419 is less than current step: 499. Dropping entry: {'train/lr': 1.0163265306122448e-05, '_timestamp': 1740967530.5197809}).
Epoch: [4][421/500]	Time  7.899 ( 7.899)	Loss 2.3247 (1.7076)	CeLoss 0.2598 (0.3918)	SegCLSLoss 0.0173 (0.0149)	KLLoss 0.3516 (0.2562)	MaskLoss 1.0110 (0.6414)	MaskBCELoss 0.0189 (0.0799)	MaskDICELoss 0.9921 (0.5615)
Epoch: [4][422/500]	Time  8.095 ( 8.095)	Loss 2.9942 (1.8428)	CeLoss 0.2637 (0.2234)	SegCLSLoss 0.0116 (0.0162)	KLLoss 0.3672 (0.3662)	MaskLoss 1.3438 (0.7873)	MaskBCELoss 0.6905 (0.2282)	MaskDICELoss 0.6533 (0.5592)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 420 is less than current step: 499. Dropping entry: {'train/loss': 1.7075859546661376, 'train/ce_loss': 0.391845703125, 'train/seg_cls_loss': 0.014935302734375, 'train/kl_loss': 0.25625, 'train/mask_bce_loss': 0.07987729851156473, 'train/mask_dice_loss': 0.5615377441048622, 'train/mask_loss': 0.6414150297641754, 'metrics/total_secs_per_batch': 7.899284362792969, 'metrics/data_secs_per_batch': 3.7215181827545165, '_timestamp': 1740967538.4191518}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 420 is less than current step: 499. Dropping entry: {'train/lr': 1.0040816326530611e-05, '_timestamp': 1740967538.4194841}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 421 is less than current step: 499. Dropping entry: {'train/loss': 1.8428494811058045, 'train/ce_loss': 0.2234375, 'train/seg_cls_loss': 0.016168212890625, 'train/kl_loss': 0.3662109375, 'train/mask_bce_loss': 0.22818372584879398, 'train/mask_dice_loss': 0.5591589868068695, 'train/mask_loss': 0.7873427137732506, 'metrics/total_secs_per_batch': 8.09501028060913, 'metrics/data_secs_per_batch': 3.7437216997146607, '_timestamp': 1740967546.5140166}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 421 is less than current step: 499. Dropping entry: {'train/lr': 9.918367346938776e-06, '_timestamp': 1740967546.514313}).
Epoch: [4][423/500]	Time  7.345 ( 7.345)	Loss 0.6507 (1.4648)	CeLoss 0.2969 (0.2728)	SegCLSLoss 0.0092 (0.0152)	KLLoss 0.3652 (0.2910)	MaskLoss 0.1564 (0.5777)	MaskBCELoss 0.0691 (0.1316)	MaskDICELoss 0.0873 (0.4461)
Epoch: [4][424/500]	Time  7.814 ( 7.814)	Loss 0.5808 (1.4762)	CeLoss 0.2852 (0.3835)	SegCLSLoss 0.0119 (0.0161)	KLLoss 0.3672 (0.3279)	MaskLoss 0.1263 (0.5258)	MaskBCELoss 0.0752 (0.1097)	MaskDICELoss 0.0511 (0.4161)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 422 is less than current step: 499. Dropping entry: {'train/loss': 1.4648043155670165, 'train/ce_loss': 0.272802734375, 'train/seg_cls_loss': 0.015240478515625, 'train/kl_loss': 0.291015625, 'train/mask_bce_loss': 0.13159591164439916, 'train/mask_dice_loss': 0.44609434008598325, 'train/mask_loss': 0.5776902467012406, 'metrics/total_secs_per_batch': 7.344546794891357, 'metrics/data_secs_per_batch': 3.4065437793731688, '_timestamp': 1740967553.8585215}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 422 is less than current step: 499. Dropping entry: {'train/lr': 9.795918367346939e-06, '_timestamp': 1740967553.858783}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 423 is less than current step: 499. Dropping entry: {'train/loss': 1.4761728703975678, 'train/ce_loss': 0.38349609375, 'train/seg_cls_loss': 0.01611328125, 'train/kl_loss': 0.3279296875, 'train/mask_bce_loss': 0.10971355009824038, 'train/mask_dice_loss': 0.41606820225715635, 'train/mask_loss': 0.5257817521691323, 'metrics/total_secs_per_batch': 7.8142969608306885, 'metrics/data_secs_per_batch': 3.692070817947388, '_timestamp': 1740967561.6728218}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 423 is less than current step: 499. Dropping entry: {'train/lr': 9.6734693877551e-06, '_timestamp': 1740967561.6730833}).
Epoch: [4][425/500]	Time  8.234 ( 8.234)	Loss 1.4753 (1.6061)	CeLoss 0.2451 (0.3216)	SegCLSLoss 0.0172 (0.0152)	KLLoss 0.3594 (0.3262)	MaskLoss 0.5931 (0.6221)	MaskBCELoss 0.0353 (0.0868)	MaskDICELoss 0.5579 (0.5353)
Epoch: [4][426/500]	Time  8.126 ( 8.126)	Loss 1.7066 (1.5095)	CeLoss 0.1680 (0.2104)	SegCLSLoss 0.0094 (0.0140)	KLLoss 0.3594 (0.3281)	MaskLoss 0.7488 (0.6297)	MaskBCELoss 0.0041 (0.1407)	MaskDICELoss 0.7447 (0.4889)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 424 is less than current step: 499. Dropping entry: {'train/loss': 1.6060930609703064, 'train/ce_loss': 0.32158203125, 'train/seg_cls_loss': 0.015234375, 'train/kl_loss': 0.326171875, 'train/mask_bce_loss': 0.08675416931509972, 'train/mask_dice_loss': 0.5353353261947632, 'train/mask_loss': 0.6220895022153854, 'metrics/total_secs_per_batch': 8.233667850494385, 'metrics/data_secs_per_batch': 3.6695428133010863, '_timestamp': 1740967569.9064913}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 424 is less than current step: 499. Dropping entry: {'train/lr': 9.551020408163265e-06, '_timestamp': 1740967569.9066813}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 425 is less than current step: 499. Dropping entry: {'train/loss': 1.5095322608947754, 'train/ce_loss': 0.21044921875, 'train/seg_cls_loss': 0.013958740234375, 'train/kl_loss': 0.328125, 'train/mask_bce_loss': 0.1407441701274365, 'train/mask_dice_loss': 0.48892431557178495, 'train/mask_loss': 0.6296684712171554, 'metrics/total_secs_per_batch': 8.125940322875977, 'metrics/data_secs_per_batch': 3.7746517419815064, '_timestamp': 1740967578.0324285}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 425 is less than current step: 499. Dropping entry: {'train/lr': 9.428571428571428e-06, '_timestamp': 1740967578.0327032}).
Epoch: [4][427/500]	Time  6.824 ( 6.824)	Loss 4.9048 (1.5601)	CeLoss 0.2617 (0.4428)	SegCLSLoss 0.0119 (0.0067)	KLLoss 0.3633 (0.1805)	MaskLoss 2.3000 (0.5478)	MaskBCELoss 1.6898 (0.2276)	MaskDICELoss 0.6102 (0.3203)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 426 is less than current step: 499. Dropping entry: {'train/loss': 1.56009584069252, 'train/ce_loss': 0.4427734375, 'train/seg_cls_loss': 0.006719970703125, 'train/kl_loss': 0.18046875, 'train/mask_bce_loss': 0.22755736364051699, 'train/mask_dice_loss': 0.32026399970054625, 'train/mask_loss': 0.5478213578462601, 'metrics/total_secs_per_batch': 6.824333667755127, 'metrics/data_secs_per_batch': 3.0632699012756346, '_timestamp': 1740967584.8568962}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 426 is less than current step: 499. Dropping entry: {'train/lr': 9.306122448979591e-06, '_timestamp': 1740967584.8572035}).
Epoch: [4][428/500]	Time  8.931 ( 8.931)	Loss 1.7064 (1.7914)	CeLoss 0.2275 (0.2926)	SegCLSLoss 0.0138 (0.0158)	KLLoss 0.3574 (0.3268)	MaskLoss 0.7185 (0.7292)	MaskBCELoss 0.0729 (0.1110)	MaskDICELoss 0.6456 (0.6183)
Epoch: [4][429/500]	Time  8.212 ( 8.212)	Loss 2.2062 (1.6789)	CeLoss 0.2246 (0.3698)	SegCLSLoss 0.0198 (0.0162)	KLLoss 0.3613 (0.2912)	MaskLoss 0.9684 (0.6359)	MaskBCELoss 0.0348 (0.0764)	MaskDICELoss 0.9336 (0.5595)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 427 is less than current step: 499. Dropping entry: {'train/loss': 1.791359782218933, 'train/ce_loss': 0.292578125, 'train/seg_cls_loss': 0.01580810546875, 'train/kl_loss': 0.3267578125, 'train/mask_bce_loss': 0.11095915269106627, 'train/mask_dice_loss': 0.6182656645774841, 'train/mask_loss': 0.7292248100042343, 'metrics/total_secs_per_batch': 8.930721044540405, 'metrics/data_secs_per_batch': 3.455078196525574, '_timestamp': 1740967593.7875113}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 427 is less than current step: 499. Dropping entry: {'train/lr': 9.183673469387754e-06, '_timestamp': 1740967593.787812}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 428 is less than current step: 499. Dropping entry: {'train/loss': 1.6789479732513428, 'train/ce_loss': 0.36982421875, 'train/seg_cls_loss': 0.016217041015625, 'train/kl_loss': 0.2912109375, 'train/mask_bce_loss': 0.07639377135783434, 'train/mask_dice_loss': 0.5595157474279404, 'train/mask_loss': 0.6359095215797425, 'metrics/total_secs_per_batch': 8.211543321609497, 'metrics/data_secs_per_batch': 3.503725290298462, '_timestamp': 1740967601.9990618}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 428 is less than current step: 499. Dropping entry: {'train/lr': 9.061224489795919e-06, '_timestamp': 1740967601.99932}).
[2025-03-02 20:06:50,533] [INFO] [logging.py:96:log_dist] [Rank 0] step=2430, skipped=0, lr=[8.999999999999999e-06], mom=[(0.9, 0.95)]
[2025-03-02 20:06:50,539] [INFO] [timer.py:215:stop] epoch=0/micro_step=24300/global_step=2430, RunningAvgSamplesPerSec=1.4455839522962903, CurrSamplesPerSec=1.171033783997897, MemAllocated=31.26GB, MaxMemAllocated=37.23GB
Epoch: [4][430/500]	Time  8.541 ( 8.541)	Loss 2.4220 (2.1185)	CeLoss 0.1826 (0.3778)	SegCLSLoss 0.0188 (0.0164)	KLLoss 0.3594 (0.3281)	MaskLoss 1.0967 (0.8498)	MaskBCELoss 0.2658 (0.1850)	MaskDICELoss 0.8309 (0.6648)
Epoch: [4][431/500]	Time  8.915 ( 8.915)	Loss 0.7890 (1.3984)	CeLoss 0.2148 (0.2822)	SegCLSLoss 0.0142 (0.0145)	KLLoss 0.3652 (0.2881)	MaskLoss 0.2656 (0.5401)	MaskBCELoss 0.1789 (0.1423)	MaskDICELoss 0.0866 (0.3978)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 429 is less than current step: 499. Dropping entry: {'train/loss': 2.1185014128684996, 'train/ce_loss': 0.37783203125, 'train/seg_cls_loss': 0.01636962890625, 'train/kl_loss': 0.328125, 'train/mask_bce_loss': 0.18495661541819572, 'train/mask_dice_loss': 0.6648214399814606, 'train/mask_loss': 0.8497780412435532, 'metrics/total_secs_per_batch': 8.54097032546997, 'metrics/data_secs_per_batch': 4.077309012413025, '_timestamp': 1740967610.5398548}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 429 is less than current step: 499. Dropping entry: {'train/lr': 8.93877551020408e-06, '_timestamp': 1740967610.5401247}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 430 is less than current step: 499. Dropping entry: {'train/loss': 1.3984130024909973, 'train/ce_loss': 0.2822265625, 'train/seg_cls_loss': 0.014544677734375, 'train/kl_loss': 0.2880859375, 'train/mask_bce_loss': 0.14233634918928145, 'train/mask_dice_loss': 0.3977881371974945, 'train/mask_loss': 0.540124487876892, 'metrics/total_secs_per_batch': 8.914963722229004, 'metrics/data_secs_per_batch': 3.7062407970428466, '_timestamp': 1740967619.4549966}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 430 is less than current step: 499. Dropping entry: {'train/lr': 8.816326530612243e-06, '_timestamp': 1740967619.4552684}).
Epoch: [4][432/500]	Time  8.086 ( 8.086)	Loss 1.8199 (1.4894)	CeLoss 0.1943 (0.3888)	SegCLSLoss 0.0194 (0.0133)	KLLoss 0.3594 (0.2906)	MaskLoss 0.7899 (0.5324)	MaskBCELoss 0.1034 (0.0734)	MaskDICELoss 0.6864 (0.4590)
Epoch: [4][433/500]	Time  6.148 ( 6.148)	Loss 2.3508 (1.5271)	CeLoss 0.2373 (0.7092)	SegCLSLoss 0.0247 (0.0097)	KLLoss 0.3477 (0.1803)	MaskLoss 1.0328 (0.3974)	MaskBCELoss 0.1108 (0.0848)	MaskDICELoss 0.9221 (0.3126)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 431 is less than current step: 499. Dropping entry: {'train/loss': 1.4894266188144685, 'train/ce_loss': 0.38876953125, 'train/seg_cls_loss': 0.013348388671875, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.07335869204252958, 'train/mask_dice_loss': 0.4590499311685562, 'train/mask_loss': 0.5324086219072341, 'metrics/total_secs_per_batch': 8.086401224136353, 'metrics/data_secs_per_batch': 3.2196222066879274, '_timestamp': 1740967627.5414112}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 431 is less than current step: 499. Dropping entry: {'train/lr': 8.693877551020408e-06, '_timestamp': 1740967627.541679}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 432 is less than current step: 499. Dropping entry: {'train/loss': 1.5271171450614929, 'train/ce_loss': 0.7091796875, 'train/seg_cls_loss': 0.00968017578125, 'train/kl_loss': 0.1802734375, 'train/mask_bce_loss': 0.08484844230115414, 'train/mask_dice_loss': 0.31259684562683104, 'train/mask_loss': 0.39744529128074646, 'metrics/total_secs_per_batch': 6.147540092468262, 'metrics/data_secs_per_batch': 2.43664231300354, '_timestamp': 1740967633.6889381}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 432 is less than current step: 499. Dropping entry: {'train/lr': 8.571428571428571e-06, '_timestamp': 1740967633.68925}).
Epoch: [4][434/500]	Time  8.453 ( 8.453)	Loss 2.6533 (1.5817)	CeLoss 0.2197 (0.2062)	SegCLSLoss 0.0175 (0.0116)	KLLoss 0.3730 (0.2906)	MaskLoss 1.1939 (0.6703)	MaskBCELoss 0.2809 (0.1491)	MaskDICELoss 0.9129 (0.5212)
Epoch: [4][435/500]	Time  7.599 ( 7.599)	Loss 0.6670 (1.6570)	CeLoss 0.3184 (0.4243)	SegCLSLoss 0.0129 (0.0120)	KLLoss 0.3652 (0.2896)	MaskLoss 0.1528 (0.5986)	MaskBCELoss 0.0596 (0.1141)	MaskDICELoss 0.0933 (0.4845)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 433 is less than current step: 499. Dropping entry: {'train/loss': 1.5816673278808593, 'train/ce_loss': 0.20625, 'train/seg_cls_loss': 0.011566162109375, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.14908144399523734, 'train/mask_dice_loss': 0.5211955666542053, 'train/mask_loss': 0.6702770024538041, 'metrics/total_secs_per_batch': 8.453081130981445, 'metrics/data_secs_per_batch': 3.753995156288147, '_timestamp': 1740967642.1421704}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 433 is less than current step: 499. Dropping entry: {'train/lr': 8.448979591836734e-06, '_timestamp': 1740967642.142385}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 434 is less than current step: 499. Dropping entry: {'train/loss': 1.6569508731365203, 'train/ce_loss': 0.42431640625, 'train/seg_cls_loss': 0.01204833984375, 'train/kl_loss': 0.2896484375, 'train/mask_bce_loss': 0.11411905102431774, 'train/mask_dice_loss': 0.4845223933458328, 'train/mask_loss': 0.5986414462327957, 'metrics/total_secs_per_batch': 7.598882675170898, 'metrics/data_secs_per_batch': 3.28971529006958, '_timestamp': 1740967649.7409248}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 434 is less than current step: 499. Dropping entry: {'train/lr': 8.326530612244897e-06, '_timestamp': 1740967649.741351}).
Epoch: [4][436/500]	Time  8.053 ( 8.053)	Loss 0.0669 (1.7456)	CeLoss 0.0669 (0.2914)	SegCLSLoss 0.0000 (0.0101)	KLLoss 0.0000 (0.2545)	MaskLoss 0.0000 (0.7118)	MaskBCELoss 0.0000 (0.2549)	MaskDICELoss 0.0000 (0.4569)
Epoch: [4][437/500]	Time  8.888 ( 8.888)	Loss 1.3562 (1.5833)	CeLoss 0.2051 (0.3582)	SegCLSLoss 0.0145 (0.0133)	KLLoss 0.3652 (0.2922)	MaskLoss 0.5541 (0.5946)	MaskBCELoss 0.0956 (0.1406)	MaskDICELoss 0.4585 (0.4540)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 435 is less than current step: 499. Dropping entry: {'train/loss': 1.745612269639969, 'train/ce_loss': 0.29140625, 'train/seg_cls_loss': 0.0100830078125, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.25487452913075687, 'train/mask_dice_loss': 0.45694525390863416, 'train/mask_loss': 0.7118197917938233, 'metrics/total_secs_per_batch': 8.053400993347168, 'metrics/data_secs_per_batch': 3.4757657766342165, '_timestamp': 1740967657.794301}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 435 is less than current step: 499. Dropping entry: {'train/lr': 8.20408163265306e-06, '_timestamp': 1740967657.7945633}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 436 is less than current step: 499. Dropping entry: {'train/loss': 1.5833297491073608, 'train/ce_loss': 0.358203125, 'train/seg_cls_loss': 0.013262939453125, 'train/kl_loss': 0.2921875, 'train/mask_bce_loss': 0.1406414106488228, 'train/mask_dice_loss': 0.45400198698043825, 'train/mask_loss': 0.5946433931589127, 'metrics/total_secs_per_batch': 8.888407945632935, 'metrics/data_secs_per_batch': 4.266176462173462, '_timestamp': 1740967666.6827593}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 436 is less than current step: 499. Dropping entry: {'train/lr': 8.081632653061223e-06, '_timestamp': 1740967666.6831357}).
Epoch: [4][438/500]	Time  7.604 ( 7.604)	Loss 1.4141 (1.8445)	CeLoss 1.4141 (0.5750)	SegCLSLoss 0.0000 (0.0108)	KLLoss 0.0000 (0.2598)	MaskLoss 0.0000 (0.6189)	MaskBCELoss 0.0000 (0.1118)	MaskDICELoss 0.0000 (0.5071)
Epoch: [4][439/500]	Time  8.221 ( 8.221)	Loss 2.6343 (2.1820)	CeLoss 0.2852 (0.3072)	SegCLSLoss 0.0140 (0.0158)	KLLoss 0.3496 (0.3205)	MaskLoss 1.1531 (0.9174)	MaskBCELoss 0.3773 (0.2548)	MaskDICELoss 0.7758 (0.6626)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 437 is less than current step: 499. Dropping entry: {'train/loss': 1.844520890712738, 'train/ce_loss': 0.575, 'train/seg_cls_loss': 0.010797119140625, 'train/kl_loss': 0.259765625, 'train/mask_bce_loss': 0.11175507809966803, 'train/mask_dice_loss': 0.5071362346410752, 'train/mask_loss': 0.6188913077116013, 'metrics/total_secs_per_batch': 7.6040732860565186, 'metrics/data_secs_per_batch': 3.658504605293274, '_timestamp': 1740967674.286828}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 437 is less than current step: 499. Dropping entry: {'train/lr': 7.959183673469386e-06, '_timestamp': 1740967674.2870204}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 438 is less than current step: 499. Dropping entry: {'train/loss': 2.18199565410614, 'train/ce_loss': 0.3072265625, 'train/seg_cls_loss': 0.015826416015625, 'train/kl_loss': 0.3205078125, 'train/mask_bce_loss': 0.25480316765606403, 'train/mask_dice_loss': 0.6625618368387223, 'train/mask_loss': 0.9173650026321412, 'metrics/total_secs_per_batch': 8.221319198608398, 'metrics/data_secs_per_batch': 3.802188587188721, '_timestamp': 1740967682.5081267}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 438 is less than current step: 499. Dropping entry: {'train/lr': 7.836734693877551e-06, '_timestamp': 1740967682.5084007}).
[2025-03-02 20:08:10,383] [INFO] [logging.py:96:log_dist] [Rank 0] step=2440, skipped=0, lr=[7.77551020408163e-06], mom=[(0.9, 0.95)]
[2025-03-02 20:08:10,388] [INFO] [timer.py:215:stop] epoch=0/micro_step=24400/global_step=2440, RunningAvgSamplesPerSec=1.4446706838400547, CurrSamplesPerSec=1.2690410436340571, MemAllocated=31.27GB, MaxMemAllocated=37.23GB
Epoch: [4][440/500]	Time  7.881 ( 7.881)	Loss 2.9771 (1.4427)	CeLoss 0.2188 (0.4208)	SegCLSLoss 0.0150 (0.0086)	KLLoss 0.3750 (0.2188)	MaskLoss 1.3567 (0.4980)	MaskBCELoss 0.6073 (0.1241)	MaskDICELoss 0.7494 (0.3738)
Epoch: [4][441/500]	Time  5.815 ( 5.815)	Loss 0.1816 (1.7787)	CeLoss 0.1816 (0.6152)	SegCLSLoss 0.0000 (0.0095)	KLLoss 0.0000 (0.1830)	MaskLoss 0.0000 (0.5701)	MaskBCELoss 0.0000 (0.2353)	MaskDICELoss 0.0000 (0.3348)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 439 is less than current step: 499. Dropping entry: {'train/loss': 1.4427005887031554, 'train/ce_loss': 0.420849609375, 'train/seg_cls_loss': 0.008563232421875, 'train/kl_loss': 0.21875, 'train/mask_bce_loss': 0.12413624580949545, 'train/mask_dice_loss': 0.37384978830814364, 'train/mask_loss': 0.49798603951931, 'metrics/total_secs_per_batch': 7.8814637660980225, 'metrics/data_secs_per_batch': 3.5456592559814455, '_timestamp': 1740967690.389454}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 439 is less than current step: 499. Dropping entry: {'train/lr': 7.714285714285714e-06, '_timestamp': 1740967690.3897555}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 440 is less than current step: 499. Dropping entry: {'train/loss': 1.778654897212982, 'train/ce_loss': 0.615234375, 'train/seg_cls_loss': 0.009454345703125, 'train/kl_loss': 0.1830078125, 'train/mask_bce_loss': 0.23528943341225386, 'train/mask_dice_loss': 0.3348485678434372, 'train/mask_loss': 0.5701379954814911, 'metrics/total_secs_per_batch': 5.8153040409088135, 'metrics/data_secs_per_batch': 2.633955407142639, '_timestamp': 1740967696.20504}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 440 is less than current step: 499. Dropping entry: {'train/lr': 7.591836734693877e-06, '_timestamp': 1740967696.2053423}).
Epoch: [4][442/500]	Time  9.341 ( 9.341)	Loss 0.4707 (1.7921)	CeLoss 0.4707 (0.2626)	SegCLSLoss 0.0000 (0.0162)	KLLoss 0.0000 (0.3279)	MaskLoss 0.0000 (0.7442)	MaskBCELoss 0.0000 (0.2129)	MaskDICELoss 0.0000 (0.5313)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 441 is less than current step: 499. Dropping entry: {'train/loss': 1.7921007513999938, 'train/ce_loss': 0.26259765625, 'train/seg_cls_loss': 0.01619873046875, 'train/kl_loss': 0.3279296875, 'train/mask_bce_loss': 0.2129217497538775, 'train/mask_dice_loss': 0.5313219845294952, 'train/mask_loss': 0.744243735074997, 'metrics/total_secs_per_batch': 9.340673208236694, 'metrics/data_secs_per_batch': 4.122303867340088, '_timestamp': 1740967705.5455694}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 441 is less than current step: 499. Dropping entry: {'train/lr': 7.46938775510204e-06, '_timestamp': 1740967705.5458283}).
Epoch: [4][443/500]	Time  8.202 ( 8.202)	Loss 1.1637 (1.6602)	CeLoss 0.2949 (0.2992)	SegCLSLoss 0.0094 (0.0127)	KLLoss 0.3574 (0.2924)	MaskLoss 0.4149 (0.6628)	MaskBCELoss 0.1273 (0.1392)	MaskDICELoss 0.2876 (0.5236)
Epoch: [4][444/500]	Time  6.704 ( 6.704)	Loss 1.7809 (1.4199)	CeLoss 0.2090 (0.4254)	SegCLSLoss 0.0190 (0.0131)	KLLoss 0.3594 (0.2539)	MaskLoss 0.7635 (0.4811)	MaskBCELoss 0.0442 (0.0876)	MaskDICELoss 0.7193 (0.3935)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 442 is less than current step: 499. Dropping entry: {'train/loss': 1.6602234601974488, 'train/ce_loss': 0.29921875, 'train/seg_cls_loss': 0.0127197265625, 'train/kl_loss': 0.2923828125, 'train/mask_bce_loss': 0.1391952684149146, 'train/mask_dice_loss': 0.5235824778676033, 'train/mask_loss': 0.6627777442336082, 'metrics/total_secs_per_batch': 8.202486753463745, 'metrics/data_secs_per_batch': 3.685332155227661, '_timestamp': 1740967713.7479944}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 442 is less than current step: 499. Dropping entry: {'train/lr': 7.346938775510203e-06, '_timestamp': 1740967713.7481813}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 443 is less than current step: 499. Dropping entry: {'train/loss': 1.4198593497276306, 'train/ce_loss': 0.425390625, 'train/seg_cls_loss': 0.0130615234375, 'train/kl_loss': 0.25390625, 'train/mask_bce_loss': 0.08762875385582447, 'train/mask_dice_loss': 0.3934923470020294, 'train/mask_loss': 0.48112109303474426, 'metrics/total_secs_per_batch': 6.704128742218018, 'metrics/data_secs_per_batch': 3.252589797973633, '_timestamp': 1740967720.4521792}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 443 is less than current step: 499. Dropping entry: {'train/lr': 7.224489795918367e-06, '_timestamp': 1740967720.4525008}).
Epoch: [4][445/500]	Time  9.212 ( 9.212)	Loss 1.8886 (1.6898)	CeLoss 0.2598 (0.2217)	SegCLSLoss 0.0131 (0.0170)	KLLoss 0.3711 (0.3250)	MaskLoss 0.7929 (0.7136)	MaskBCELoss 0.1308 (0.1373)	MaskDICELoss 0.6621 (0.5764)
Epoch: [4][446/500]	Time  8.516 ( 8.516)	Loss 0.8570 (1.0907)	CeLoss 0.2227 (0.2232)	SegCLSLoss 0.0127 (0.0107)	KLLoss 0.3652 (0.2568)	MaskLoss 0.2957 (0.4183)	MaskBCELoss 0.0959 (0.0802)	MaskDICELoss 0.1998 (0.3381)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 444 is less than current step: 499. Dropping entry: {'train/loss': 1.6897829413414, 'train/ce_loss': 0.2216796875, 'train/seg_cls_loss': 0.01697998046875, 'train/kl_loss': 0.325, 'train/mask_bce_loss': 0.13728432301431895, 'train/mask_dice_loss': 0.576357153058052, 'train/mask_loss': 0.7136414825916291, 'metrics/total_secs_per_batch': 9.211715459823608, 'metrics/data_secs_per_batch': 4.259916996955871, '_timestamp': 1740967729.6639314}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 444 is less than current step: 499. Dropping entry: {'train/lr': 7.10204081632653e-06, '_timestamp': 1740967729.664199}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 445 is less than current step: 499. Dropping entry: {'train/loss': 1.0907153189182281, 'train/ce_loss': 0.223193359375, 'train/seg_cls_loss': 0.010711669921875, 'train/kl_loss': 0.2568359375, 'train/mask_bce_loss': 0.08017692044377327, 'train/mask_dice_loss': 0.338105545938015, 'train/mask_loss': 0.41828246116638185, 'metrics/total_secs_per_batch': 8.516037225723267, 'metrics/data_secs_per_batch': 3.919875693321228, '_timestamp': 1740967738.1801844}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 445 is less than current step: 499. Dropping entry: {'train/lr': 6.979591836734694e-06, '_timestamp': 1740967738.1806066}).
Epoch: [4][447/500]	Time  7.567 ( 7.567)	Loss 1.0827 (1.4134)	CeLoss 0.2012 (0.6314)	SegCLSLoss 0.0125 (0.0084)	KLLoss 0.3711 (0.2184)	MaskLoss 0.4193 (0.3779)	MaskBCELoss 0.0499 (0.0738)	MaskDICELoss 0.3694 (0.3041)
Epoch: [4][448/500]	Time  6.605 ( 6.605)	Loss 2.3535 (1.2679)	CeLoss 0.2402 (0.3404)	SegCLSLoss 0.0188 (0.0109)	KLLoss 0.3535 (0.2201)	MaskLoss 1.0342 (0.4500)	MaskBCELoss 0.2336 (0.1204)	MaskDICELoss 0.8006 (0.3295)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 446 is less than current step: 499. Dropping entry: {'train/loss': 1.413366436958313, 'train/ce_loss': 0.631396484375, 'train/seg_cls_loss': 0.008447265625, 'train/kl_loss': 0.218359375, 'train/mask_bce_loss': 0.07382092047482729, 'train/mask_dice_loss': 0.3041269451379776, 'train/mask_loss': 0.37794786393642427, 'metrics/total_secs_per_batch': 7.567301511764526, 'metrics/data_secs_per_batch': 3.2859635591506957, '_timestamp': 1740967745.7472477}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 446 is less than current step: 499. Dropping entry: {'train/lr': 6.857142857142856e-06, '_timestamp': 1740967745.7475543}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 447 is less than current step: 499. Dropping entry: {'train/loss': 1.2678591370582581, 'train/ce_loss': 0.340380859375, 'train/seg_cls_loss': 0.010858154296875, 'train/kl_loss': 0.2201171875, 'train/mask_bce_loss': 0.12044929340481758, 'train/mask_dice_loss': 0.3295203194022179, 'train/mask_loss': 0.44996961057186124, 'metrics/total_secs_per_batch': 6.605467319488525, 'metrics/data_secs_per_batch': 3.303515839576721, '_timestamp': 1740967752.3529744}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 447 is less than current step: 499. Dropping entry: {'train/lr': 6.734693877551019e-06, '_timestamp': 1740967752.353318}).
Epoch: [4][449/500]	Time  8.667 ( 8.667)	Loss 2.3976 (1.7642)	CeLoss 0.2832 (0.4220)	SegCLSLoss 0.0099 (0.0161)	KLLoss 0.3633 (0.2934)	MaskLoss 1.0357 (0.6522)	MaskBCELoss 0.4498 (0.1976)	MaskDICELoss 0.5860 (0.4546)
[2025-03-02 20:09:26,863] [INFO] [logging.py:96:log_dist] [Rank 0] step=2450, skipped=0, lr=[6.5510204081632656e-06], mom=[(0.9, 0.95)]
[2025-03-02 20:09:26,868] [INFO] [timer.py:215:stop] epoch=0/micro_step=24500/global_step=2450, RunningAvgSamplesPerSec=1.4440530409731447, CurrSamplesPerSec=1.7098757449894977, MemAllocated=30.78GB, MaxMemAllocated=37.23GB
Epoch: [4][450/500]	Time  5.850 ( 5.850)	Loss 1.3594 (1.6835)	CeLoss 1.3594 (0.6530)	SegCLSLoss 0.0000 (0.0119)	KLLoss 0.0000 (0.2188)	MaskLoss 0.0000 (0.5012)	MaskBCELoss 0.0000 (0.1082)	MaskDICELoss 0.0000 (0.3931)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 448 is less than current step: 499. Dropping entry: {'train/loss': 1.7641510725021363, 'train/ce_loss': 0.42197265625, 'train/seg_cls_loss': 0.016131591796875, 'train/kl_loss': 0.293359375, 'train/mask_bce_loss': 0.19755288902670146, 'train/mask_dice_loss': 0.454639832675457, 'train/mask_loss': 0.6521927237510681, 'metrics/total_secs_per_batch': 8.666813373565674, 'metrics/data_secs_per_batch': 4.335351514816284, '_timestamp': 1740967761.0195599}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 448 is less than current step: 499. Dropping entry: {'train/lr': 6.612244897959183e-06, '_timestamp': 1740967761.0198941}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 449 is less than current step: 499. Dropping entry: {'train/loss': 1.6835489213466643, 'train/ce_loss': 0.65302734375, 'train/seg_cls_loss': 0.0118896484375, 'train/kl_loss': 0.21875, 'train/mask_bce_loss': 0.10818914975970984, 'train/mask_dice_loss': 0.3930579572916031, 'train/mask_loss': 0.5012471199035644, 'metrics/total_secs_per_batch': 5.8499672412872314, 'metrics/data_secs_per_batch': 2.6010756492614746, '_timestamp': 1740967766.8693452}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 449 is less than current step: 499. Dropping entry: {'train/lr': 6.489795918367346e-06, '_timestamp': 1740967766.8696167}).
Epoch: [4][451/500]	Time  7.487 ( 7.487)	Loss 0.1245 (1.5201)	CeLoss 0.1245 (0.1688)	SegCLSLoss 0.0000 (0.0155)	KLLoss 0.0000 (0.2574)	MaskLoss 0.0000 (0.6589)	MaskBCELoss 0.0000 (0.1250)	MaskDICELoss 0.0000 (0.5338)
Epoch: [4][452/500]	Time  9.007 ( 9.007)	Loss 1.5080 (1.8325)	CeLoss 0.1709 (0.4376)	SegCLSLoss 0.0267 (0.0152)	KLLoss 0.3555 (0.2879)	MaskLoss 0.6436 (0.6792)	MaskBCELoss 0.0311 (0.1266)	MaskDICELoss 0.6125 (0.5526)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 450 is less than current step: 499. Dropping entry: {'train/loss': 1.5200906634330749, 'train/ce_loss': 0.16884765625, 'train/seg_cls_loss': 0.015472412109375, 'train/kl_loss': 0.257421875, 'train/mask_bce_loss': 0.12503563594073058, 'train/mask_dice_loss': 0.5338378086686134, 'train/mask_loss': 0.6588734447956085, 'metrics/total_secs_per_batch': 7.487333297729492, 'metrics/data_secs_per_batch': 3.2329493284225466, '_timestamp': 1740967774.3569515}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 450 is less than current step: 499. Dropping entry: {'train/lr': 6.36734693877551e-06, '_timestamp': 1740967774.357279}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 451 is less than current step: 499. Dropping entry: {'train/loss': 1.8324662923812867, 'train/ce_loss': 0.43759765625, 'train/seg_cls_loss': 0.015191650390625, 'train/kl_loss': 0.287890625, 'train/mask_bce_loss': 0.12661469709128143, 'train/mask_dice_loss': 0.5526067316532135, 'train/mask_loss': 0.6792214334011077, 'metrics/total_secs_per_batch': 9.007289171218872, 'metrics/data_secs_per_batch': 4.24538562297821, '_timestamp': 1740967783.3642068}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 451 is less than current step: 499. Dropping entry: {'train/lr': 6.244897959183673e-06, '_timestamp': 1740967783.3645632}).
Epoch: [4][453/500]	Time 10.364 (10.364)	Loss 2.4346 (1.7690)	CeLoss 0.1562 (0.2134)	SegCLSLoss 0.0256 (0.0178)	KLLoss 0.3867 (0.3627)	MaskLoss 1.1138 (0.7551)	MaskBCELoss 0.2117 (0.1245)	MaskDICELoss 0.9021 (0.6306)
Epoch: [4][454/500]	Time  7.090 ( 7.090)	Loss 1.9127 (1.4745)	CeLoss 0.1875 (0.3919)	SegCLSLoss 0.0253 (0.0118)	KLLoss 0.3535 (0.2533)	MaskLoss 0.8387 (0.5255)	MaskBCELoss 0.0289 (0.0635)	MaskDICELoss 0.8098 (0.4620)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 452 is less than current step: 499. Dropping entry: {'train/loss': 1.769000381231308, 'train/ce_loss': 0.21337890625, 'train/seg_cls_loss': 0.017767333984375, 'train/kl_loss': 0.3626953125, 'train/mask_bce_loss': 0.12454663701355458, 'train/mask_dice_loss': 0.6305590257048607, 'train/mask_loss': 0.755105659365654, 'metrics/total_secs_per_batch': 10.363741874694824, 'metrics/data_secs_per_batch': 3.972018575668335, '_timestamp': 1740967793.7278926}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 452 is less than current step: 499. Dropping entry: {'train/lr': 6.122448979591835e-06, '_timestamp': 1740967793.7280848}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 453 is less than current step: 499. Dropping entry: {'train/loss': 1.4745059251785277, 'train/ce_loss': 0.39189453125, 'train/seg_cls_loss': 0.0118408203125, 'train/kl_loss': 0.2533203125, 'train/mask_bce_loss': 0.06353220790624618, 'train/mask_dice_loss': 0.4619531646370888, 'train/mask_loss': 0.5254853755235672, 'metrics/total_secs_per_batch': 7.089975595474243, 'metrics/data_secs_per_batch': 2.9494009494781492, '_timestamp': 1740967800.81795}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 453 is less than current step: 499. Dropping entry: {'train/lr': 5.999999999999999e-06, '_timestamp': 1740967800.818242}).
Epoch: [4][455/500]	Time  7.565 ( 7.565)	Loss 0.0698 (1.5500)	CeLoss 0.0698 (0.4481)	SegCLSLoss 0.0000 (0.0091)	KLLoss 0.0000 (0.2562)	MaskLoss 0.0000 (0.5357)	MaskBCELoss 0.0000 (0.1531)	MaskDICELoss 0.0000 (0.3826)
Epoch: [4][456/500]	Time  6.280 ( 6.280)	Loss 1.2734 (1.6904)	CeLoss 1.2734 (0.4221)	SegCLSLoss 0.0000 (0.0135)	KLLoss 0.0000 (0.2928)	MaskLoss 0.0000 (0.6162)	MaskBCELoss 0.0000 (0.1720)	MaskDICELoss 0.0000 (0.4442)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 454 is less than current step: 499. Dropping entry: {'train/loss': 1.549979168176651, 'train/ce_loss': 0.448095703125, 'train/seg_cls_loss': 0.009149169921875, 'train/kl_loss': 0.25625, 'train/mask_bce_loss': 0.1530963672325015, 'train/mask_dice_loss': 0.38256215304136276, 'train/mask_loss': 0.5356585204601287, 'metrics/total_secs_per_batch': 7.564754962921143, 'metrics/data_secs_per_batch': 3.6530828952789305, '_timestamp': 1740967808.3826208}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 454 is less than current step: 499. Dropping entry: {'train/lr': 5.877551020408162e-06, '_timestamp': 1740967808.382888}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 455 is less than current step: 499. Dropping entry: {'train/loss': 1.6903666973114013, 'train/ce_loss': 0.4220703125, 'train/seg_cls_loss': 0.013470458984375, 'train/kl_loss': 0.2927734375, 'train/mask_bce_loss': 0.171957544144243, 'train/mask_dice_loss': 0.4442218914628029, 'train/mask_loss': 0.616179445385933, 'metrics/total_secs_per_batch': 6.2799599170684814, 'metrics/data_secs_per_batch': 3.2530126333236695, '_timestamp': 1740967814.6628208}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 455 is less than current step: 499. Dropping entry: {'train/lr': 5.755102040816326e-06, '_timestamp': 1740967814.6631646}).
Epoch: [4][457/500]	Time  8.434 ( 8.434)	Loss 1.9698 (2.1636)	CeLoss 0.2383 (0.2327)	SegCLSLoss 0.0206 (0.0155)	KLLoss 0.3535 (0.3285)	MaskLoss 0.8423 (0.9450)	MaskBCELoss 0.1715 (0.2893)	MaskDICELoss 0.6709 (0.6557)
Epoch: [4][458/500]	Time  9.573 ( 9.573)	Loss 1.7511 (1.9892)	CeLoss 0.2256 (0.2264)	SegCLSLoss 0.0173 (0.0166)	KLLoss 0.3672 (0.3662)	MaskLoss 0.7398 (0.8588)	MaskBCELoss 0.0527 (0.1659)	MaskDICELoss 0.6871 (0.6930)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 456 is less than current step: 499. Dropping entry: {'train/loss': 2.1636464476585386, 'train/ce_loss': 0.23271484375, 'train/seg_cls_loss': 0.015545654296875, 'train/kl_loss': 0.328515625, 'train/mask_bce_loss': 0.28925029560923576, 'train/mask_dice_loss': 0.6557076841592788, 'train/mask_loss': 0.9449579924345016, 'metrics/total_secs_per_batch': 8.433655500411987, 'metrics/data_secs_per_batch': 3.6367544651031496, '_timestamp': 1740967823.0962508}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 456 is less than current step: 499. Dropping entry: {'train/lr': 5.632653061224489e-06, '_timestamp': 1740967823.0965388}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 457 is less than current step: 499. Dropping entry: {'train/loss': 1.989171040058136, 'train/ce_loss': 0.2263671875, 'train/seg_cls_loss': 0.01663818359375, 'train/kl_loss': 0.3662109375, 'train/mask_bce_loss': 0.16588290669023992, 'train/mask_dice_loss': 0.6929604142904282, 'train/mask_loss': 0.8588433265686035, 'metrics/total_secs_per_batch': 9.572824001312256, 'metrics/data_secs_per_batch': 4.4472291469573975, '_timestamp': 1740967832.6692758}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 457 is less than current step: 499. Dropping entry: {'train/lr': 5.510204081632653e-06, '_timestamp': 1740967832.6696217}).
Epoch: [4][459/500]	Time  7.863 ( 7.863)	Loss 2.4459 (1.8607)	CeLoss 0.1504 (0.3536)	SegCLSLoss 0.0275 (0.0152)	KLLoss 0.3887 (0.3266)	MaskLoss 1.1214 (0.7334)	MaskBCELoss 0.1721 (0.1568)	MaskDICELoss 0.9494 (0.5765)
[2025-03-02 20:10:47,925] [INFO] [logging.py:96:log_dist] [Rank 0] step=2460, skipped=0, lr=[5.326530612244898e-06], mom=[(0.9, 0.95)]
[2025-03-02 20:10:47,931] [INFO] [timer.py:215:stop] epoch=0/micro_step=24600/global_step=2460, RunningAvgSamplesPerSec=1.4430526122842355, CurrSamplesPerSec=1.3516661058228223, MemAllocated=30.7GB, MaxMemAllocated=37.23GB
Epoch: [4][460/500]	Time  7.400 ( 7.400)	Loss 1.1562 (1.7585)	CeLoss 1.1562 (0.3835)	SegCLSLoss 0.0000 (0.0141)	KLLoss 0.0000 (0.2551)	MaskLoss 0.0000 (0.6711)	MaskBCELoss 0.0000 (0.1190)	MaskDICELoss 0.0000 (0.5521)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 458 is less than current step: 499. Dropping entry: {'train/loss': 1.860690140724182, 'train/ce_loss': 0.35361328125, 'train/seg_cls_loss': 0.015155029296875, 'train/kl_loss': 0.3265625, 'train/mask_bce_loss': 0.15684022391214966, 'train/mask_dice_loss': 0.5765321865677834, 'train/mask_loss': 0.7333724170923233, 'metrics/total_secs_per_batch': 7.863188028335571, 'metrics/data_secs_per_batch': 3.7148473501205443, '_timestamp': 1740967840.5322614}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 458 is less than current step: 499. Dropping entry: {'train/lr': 5.387755102040816e-06, '_timestamp': 1740967840.5325334}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 459 is less than current step: 499. Dropping entry: {'train/loss': 1.7584851026535033, 'train/ce_loss': 0.3835205078125, 'train/seg_cls_loss': 0.0141357421875, 'train/kl_loss': 0.255078125, 'train/mask_bce_loss': 0.11898835292086005, 'train/mask_dice_loss': 0.5521120995283126, 'train/mask_loss': 0.6711004436016083, 'metrics/total_secs_per_batch': 7.399765729904175, 'metrics/data_secs_per_batch': 3.305294466018677, '_timestamp': 1740967847.9319203}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 459 is less than current step: 499. Dropping entry: {'train/lr': 5.2653061224489785e-06, '_timestamp': 1740967847.932255}).
Epoch: [4][461/500]	Time  6.833 ( 6.833)	Loss 2.5089 (1.4571)	CeLoss 0.1592 (0.4901)	SegCLSLoss 0.0261 (0.0111)	KLLoss 0.3672 (0.2182)	MaskLoss 1.1499 (0.4698)	MaskBCELoss 0.2381 (0.1245)	MaskDICELoss 0.9119 (0.3453)
Epoch: [4][462/500]	Time  7.454 ( 7.454)	Loss 0.0583 (1.3404)	CeLoss 0.0583 (0.7154)	SegCLSLoss 0.0000 (0.0063)	KLLoss 0.0000 (0.1457)	MaskLoss 0.0000 (0.3036)	MaskBCELoss 0.0000 (0.0341)	MaskDICELoss 0.0000 (0.2695)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 460 is less than current step: 499. Dropping entry: {'train/loss': 1.4570827186107635, 'train/ce_loss': 0.49013671875, 'train/seg_cls_loss': 0.0111083984375, 'train/kl_loss': 0.2181640625, 'train/mask_bce_loss': 0.12449554987251758, 'train/mask_dice_loss': 0.3452567487955093, 'train/mask_loss': 0.469752299785614, 'metrics/total_secs_per_batch': 6.832507610321045, 'metrics/data_secs_per_batch': 3.138304567337036, '_timestamp': 1740967854.7645814}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 460 is less than current step: 499. Dropping entry: {'train/lr': 5.142857142857142e-06, '_timestamp': 1740967854.7648535}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 461 is less than current step: 499. Dropping entry: {'train/loss': 1.3403968811035156, 'train/ce_loss': 0.7154052734375, 'train/seg_cls_loss': 0.00626220703125, 'train/kl_loss': 0.145703125, 'train/mask_bce_loss': 0.0341173960827291, 'train/mask_dice_loss': 0.2694916844367981, 'train/mask_loss': 0.3036090850830078, 'metrics/total_secs_per_batch': 7.4537739753723145, 'metrics/data_secs_per_batch': 3.573698616027832, '_timestamp': 1740967862.218316}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 461 is less than current step: 499. Dropping entry: {'train/lr': 5.0204081632653054e-06, '_timestamp': 1740967862.2185872}).
Epoch: [4][463/500]	Time  7.498 ( 7.498)	Loss 0.5273 (1.2713)	CeLoss 0.5273 (0.7138)	SegCLSLoss 0.0000 (0.0064)	KLLoss 0.0000 (0.1453)	MaskLoss 0.0000 (0.2698)	MaskBCELoss 0.0000 (0.0402)	MaskDICELoss 0.0000 (0.2296)
Epoch: [4][464/500]	Time  8.665 ( 8.665)	Loss 1.6576 (1.7627)	CeLoss 0.2344 (0.3103)	SegCLSLoss 0.0132 (0.0158)	KLLoss 0.3613 (0.3252)	MaskLoss 0.6901 (0.7059)	MaskBCELoss 0.1764 (0.1440)	MaskDICELoss 0.5137 (0.5619)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 462 is less than current step: 499. Dropping entry: {'train/loss': 1.2712612867355346, 'train/ce_loss': 0.71376953125, 'train/seg_cls_loss': 0.006439208984375, 'train/kl_loss': 0.1453125, 'train/mask_bce_loss': 0.040204570069909094, 'train/mask_dice_loss': 0.22960576117038728, 'train/mask_loss': 0.26981033086776735, 'metrics/total_secs_per_batch': 7.497512340545654, 'metrics/data_secs_per_batch': 3.167697048187256, '_timestamp': 1740967869.7158306}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 462 is less than current step: 499. Dropping entry: {'train/lr': 4.897959183673469e-06, '_timestamp': 1740967869.7160962}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 463 is less than current step: 499. Dropping entry: {'train/loss': 1.7627196431159973, 'train/ce_loss': 0.31025390625, 'train/seg_cls_loss': 0.0158447265625, 'train/kl_loss': 0.3251953125, 'train/mask_bce_loss': 0.14400389958173038, 'train/mask_dice_loss': 0.5619164645671845, 'train/mask_loss': 0.7059203684329987, 'metrics/total_secs_per_batch': 8.665106296539307, 'metrics/data_secs_per_batch': 3.208035969734192, '_timestamp': 1740967878.3811264}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 463 is less than current step: 499. Dropping entry: {'train/lr': 4.775510204081632e-06, '_timestamp': 1740967878.3814692}).
Epoch: [4][465/500]	Time  5.848 ( 5.848)	Loss 0.0791 (1.3257)	CeLoss 0.0791 (0.3813)	SegCLSLoss 0.0000 (0.0076)	KLLoss 0.0000 (0.1812)	MaskLoss 0.0000 (0.4613)	MaskBCELoss 0.0000 (0.1723)	MaskDICELoss 0.0000 (0.2890)
Epoch: [4][466/500]	Time  7.457 ( 7.457)	Loss 2.2377 (2.1676)	CeLoss 0.1787 (0.3015)	SegCLSLoss 0.0187 (0.0170)	KLLoss 0.3867 (0.3312)	MaskLoss 1.0056 (0.9122)	MaskBCELoss 0.2164 (0.2570)	MaskDICELoss 0.7892 (0.6552)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 464 is less than current step: 499. Dropping entry: {'train/loss': 1.3257279872894288, 'train/ce_loss': 0.38134765625, 'train/seg_cls_loss': 0.007574462890625, 'train/kl_loss': 0.18125, 'train/mask_bce_loss': 0.17226105998270214, 'train/mask_dice_loss': 0.2889916107058525, 'train/mask_loss': 0.46125267446041107, 'metrics/total_secs_per_batch': 5.848073720932007, 'metrics/data_secs_per_batch': 2.591878867149353, '_timestamp': 1740967884.2290514}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 464 is less than current step: 499. Dropping entry: {'train/lr': 4.6530612244897954e-06, '_timestamp': 1740967884.2293296}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 465 is less than current step: 499. Dropping entry: {'train/loss': 2.167558217048645, 'train/ce_loss': 0.30146484375, 'train/seg_cls_loss': 0.017034912109375, 'train/kl_loss': 0.33125, 'train/mask_bce_loss': 0.2569633319973946, 'train/mask_dice_loss': 0.6552337348461151, 'train/mask_loss': 0.9121970653533935, 'metrics/total_secs_per_batch': 7.457218885421753, 'metrics/data_secs_per_batch': 3.3706291913986206, '_timestamp': 1740967891.6862378}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 465 is less than current step: 499. Dropping entry: {'train/lr': 4.530612244897959e-06, '_timestamp': 1740967891.6864939}).
Epoch: [4][467/500]	Time  7.553 ( 7.553)	Loss 0.5312 (1.4119)	CeLoss 0.5312 (0.3337)	SegCLSLoss 0.0000 (0.0105)	KLLoss 0.0000 (0.2557)	MaskLoss 0.0000 (0.5236)	MaskBCELoss 0.0000 (0.0847)	MaskDICELoss 0.0000 (0.4389)
Epoch: [4][468/500]	Time  7.287 ( 7.287)	Loss 2.6344 (1.6956)	CeLoss 0.2061 (0.3827)	SegCLSLoss 0.0179 (0.0142)	KLLoss 0.3652 (0.2885)	MaskLoss 1.1912 (0.6384)	MaskBCELoss 0.3610 (0.0909)	MaskDICELoss 0.8302 (0.5475)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 466 is less than current step: 499. Dropping entry: {'train/loss': 1.4119166016578675, 'train/ce_loss': 0.33369140625, 'train/seg_cls_loss': 0.010516357421875, 'train/kl_loss': 0.2556640625, 'train/mask_bce_loss': 0.08470231108367443, 'train/mask_dice_loss': 0.43888294547796247, 'train/mask_loss': 0.5235852599143982, 'metrics/total_secs_per_batch': 7.553164958953857, 'metrics/data_secs_per_batch': 3.1643632888793944, '_timestamp': 1740967899.2393985}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 466 is less than current step: 499. Dropping entry: {'train/lr': 4.4081632653061216e-06, '_timestamp': 1740967899.2396543}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 467 is less than current step: 499. Dropping entry: {'train/loss': 1.6955835938453674, 'train/ce_loss': 0.38271484375, 'train/seg_cls_loss': 0.014166259765625, 'train/kl_loss': 0.2884765625, 'train/mask_bce_loss': 0.09086155369877816, 'train/mask_dice_loss': 0.5475064188241958, 'train/mask_loss': 0.6383679687976838, 'metrics/total_secs_per_batch': 7.287301778793335, 'metrics/data_secs_per_batch': 3.186877727508545, '_timestamp': 1740967906.526719}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 467 is less than current step: 499. Dropping entry: {'train/lr': 4.2857142857142855e-06, '_timestamp': 1740967906.526986}).
Epoch: [4][469/500]	Time  7.664 ( 7.664)	Loss 1.6818 (1.7825)	CeLoss 0.2891 (0.4690)	SegCLSLoss 0.0115 (0.0118)	KLLoss 0.3652 (0.2883)	MaskLoss 0.6749 (0.6394)	MaskBCELoss 0.1013 (0.1007)	MaskDICELoss 0.5736 (0.5387)
[2025-03-02 20:12:00,884] [INFO] [logging.py:96:log_dist] [Rank 0] step=2470, skipped=0, lr=[4.10204081632653e-06], mom=[(0.9, 0.95)]
[2025-03-02 20:12:00,891] [INFO] [timer.py:215:stop] epoch=0/micro_step=24700/global_step=2470, RunningAvgSamplesPerSec=1.4427446958552719, CurrSamplesPerSec=1.4928093907063513, MemAllocated=31.48GB, MaxMemAllocated=37.23GB
Epoch: [4][470/500]	Time  6.701 ( 6.701)	Loss 0.2148 (1.1932)	CeLoss 0.2148 (0.3223)	SegCLSLoss 0.0000 (0.0109)	KLLoss 0.0000 (0.2180)	MaskLoss 0.0000 (0.4220)	MaskBCELoss 0.0000 (0.0805)	MaskDICELoss 0.0000 (0.3414)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 468 is less than current step: 499. Dropping entry: {'train/loss': 1.782492470741272, 'train/ce_loss': 0.46904296875, 'train/seg_cls_loss': 0.0118408203125, 'train/kl_loss': 0.28828125, 'train/mask_bce_loss': 0.1006710384041071, 'train/mask_dice_loss': 0.5387197226285935, 'train/mask_loss': 0.6393907696008683, 'metrics/total_secs_per_batch': 7.664403915405273, 'metrics/data_secs_per_batch': 3.6062676906585693, '_timestamp': 1740967914.1913455}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 468 is less than current step: 499. Dropping entry: {'train/lr': 4.1632653061224485e-06, '_timestamp': 1740967914.1917205}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 469 is less than current step: 499. Dropping entry: {'train/loss': 1.1932496964931487, 'train/ce_loss': 0.322265625, 'train/seg_cls_loss': 0.01092529296875, 'train/kl_loss': 0.21796875, 'train/mask_bce_loss': 0.0805414505302906, 'train/mask_dice_loss': 0.3414251983165741, 'train/mask_loss': 0.4219666451215744, 'metrics/total_secs_per_batch': 6.700613498687744, 'metrics/data_secs_per_batch': 2.903677821159363, '_timestamp': 1740967920.8915684}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 469 is less than current step: 499. Dropping entry: {'train/lr': 4.0408163265306116e-06, '_timestamp': 1740967920.8918397}).
Epoch: [4][471/500]	Time  7.610 ( 7.610)	Loss 1.2924 (1.2924)	CeLoss 0.2471 (0.6487)	SegCLSLoss 0.0123 (0.0084)	KLLoss 0.3633 (0.1795)	MaskLoss 0.5007 (0.3107)	MaskBCELoss 0.1699 (0.0422)	MaskDICELoss 0.3308 (0.2685)
Epoch: [4][472/500]	Time  9.204 ( 9.204)	Loss 0.8157 (1.7877)	CeLoss 0.2373 (0.4563)	SegCLSLoss 0.0122 (0.0131)	KLLoss 0.3633 (0.2883)	MaskLoss 0.2682 (0.6479)	MaskBCELoss 0.0827 (0.1101)	MaskDICELoss 0.1855 (0.5378)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 470 is less than current step: 499. Dropping entry: {'train/loss': 1.2923709511756898, 'train/ce_loss': 0.64873046875, 'train/seg_cls_loss': 0.008428955078125, 'train/kl_loss': 0.1794921875, 'train/mask_bce_loss': 0.04224117435514927, 'train/mask_dice_loss': 0.26849507689476015, 'train/mask_loss': 0.31073625683784484, 'metrics/total_secs_per_batch': 7.610365390777588, 'metrics/data_secs_per_batch': 4.255894255638123, '_timestamp': 1740967928.502096}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 470 is less than current step: 499. Dropping entry: {'train/lr': 3.9183673469387755e-06, '_timestamp': 1740967928.502403}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 471 is less than current step: 499. Dropping entry: {'train/loss': 1.7876905143260955, 'train/ce_loss': 0.45634765625, 'train/seg_cls_loss': 0.01307373046875, 'train/kl_loss': 0.28828125, 'train/mask_bce_loss': 0.11011455385014415, 'train/mask_dice_loss': 0.5378322586417198, 'train/mask_loss': 0.6479468047618866, 'metrics/total_secs_per_batch': 9.203684568405151, 'metrics/data_secs_per_batch': 4.42967836856842, '_timestamp': 1740967937.705803}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 471 is less than current step: 499. Dropping entry: {'train/lr': 3.7959183673469385e-06, '_timestamp': 1740967937.7060878}).
Epoch: [4][473/500]	Time  7.200 ( 7.200)	Loss 1.7683 (1.8941)	CeLoss 0.2285 (0.4672)	SegCLSLoss 0.0114 (0.0137)	KLLoss 0.3652 (0.2557)	MaskLoss 0.7484 (0.6972)	MaskBCELoss 0.1459 (0.1627)	MaskDICELoss 0.6025 (0.5344)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 472 is less than current step: 499. Dropping entry: {'train/loss': 1.8941418886184693, 'train/ce_loss': 0.467236328125, 'train/seg_cls_loss': 0.01368408203125, 'train/kl_loss': 0.2556640625, 'train/mask_bce_loss': 0.16274289349094034, 'train/mask_dice_loss': 0.5344257175922393, 'train/mask_loss': 0.6971686065196991, 'metrics/total_secs_per_batch': 7.200176239013672, 'metrics/data_secs_per_batch': 3.3954514026641847, '_timestamp': 1740967944.9059825}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 472 is less than current step: 499. Dropping entry: {'train/lr': 3.6734693877551016e-06, '_timestamp': 1740967944.9063046}).
Epoch: [4][474/500]	Time  9.232 ( 9.232)	Loss 1.1521 (2.0256)	CeLoss 0.2461 (0.2094)	SegCLSLoss 0.0132 (0.0190)	KLLoss 0.3594 (0.3645)	MaskLoss 0.4325 (0.8852)	MaskBCELoss 0.1020 (0.2165)	MaskDICELoss 0.3305 (0.6687)
Epoch: [4][475/500]	Time  7.001 ( 7.001)	Loss 1.9418 (1.1278)	CeLoss 0.2637 (0.5590)	SegCLSLoss 0.0139 (0.0056)	KLLoss 0.3574 (0.1445)	MaskLoss 0.8176 (0.2758)	MaskBCELoss 0.0428 (0.0457)	MaskDICELoss 0.7748 (0.2301)
Epoch: [4][476/500]	Time  7.183 ( 7.183)	Loss 1.0547 (1.3154)	CeLoss 1.0547 (0.4410)	SegCLSLoss 0.0000 (0.0117)	KLLoss 0.0000 (0.2520)	MaskLoss 0.0000 (0.4216)	MaskBCELoss 0.0000 (0.0804)	MaskDICELoss 0.0000 (0.3413)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 473 is less than current step: 499. Dropping entry: {'train/loss': 2.02563853263855, 'train/ce_loss': 0.209375, 'train/seg_cls_loss': 0.01900634765625, 'train/kl_loss': 0.364453125, 'train/mask_bce_loss': 0.2164541020989418, 'train/mask_dice_loss': 0.6687284588813782, 'train/mask_loss': 0.8851825654506683, 'metrics/total_secs_per_batch': 9.231698751449585, 'metrics/data_secs_per_batch': 4.167919754981995, '_timestamp': 1740967954.1376102}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 473 is less than current step: 499. Dropping entry: {'train/lr': 3.551020408163265e-06, '_timestamp': 1740967954.137793}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 474 is less than current step: 499. Dropping entry: {'train/loss': 1.1278209626674651, 'train/ce_loss': 0.559033203125, 'train/seg_cls_loss': 0.005560302734375, 'train/kl_loss': 0.14453125, 'train/mask_bce_loss': 0.04565606974065304, 'train/mask_dice_loss': 0.2300952345132828, 'train/mask_loss': 0.2757513016462326, 'metrics/total_secs_per_batch': 7.001330375671387, 'metrics/data_secs_per_batch': 3.1524074554443358, '_timestamp': 1740967961.1391988}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 474 is less than current step: 499. Dropping entry: {'train/lr': 3.428571428571428e-06, '_timestamp': 1740967961.1395366}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 475 is less than current step: 499. Dropping entry: {'train/loss': 1.315411877632141, 'train/ce_loss': 0.441015625, 'train/seg_cls_loss': 0.011676025390625, 'train/kl_loss': 0.251953125, 'train/mask_bce_loss': 0.08036179132759572, 'train/mask_dice_loss': 0.34126016497612, 'train/mask_loss': 0.4216219514608383, 'metrics/total_secs_per_batch': 7.183316469192505, 'metrics/data_secs_per_batch': 3.54934446811676, '_timestamp': 1740967968.3223484}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 475 is less than current step: 499. Dropping entry: {'train/lr': 3.3061224489795916e-06, '_timestamp': 1740967968.322606}).
Epoch: [4][477/500]	Time  7.720 ( 7.720)	Loss 1.9280 (1.5072)	CeLoss 0.1924 (0.5390)	SegCLSLoss 0.0137 (0.0102)	KLLoss 0.3691 (0.2557)	MaskLoss 0.8459 (0.4686)	MaskBCELoss 0.0786 (0.1117)	MaskDICELoss 0.7672 (0.3569)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 476 is less than current step: 499. Dropping entry: {'train/loss': 1.5071748614311218, 'train/ce_loss': 0.53896484375, 'train/seg_cls_loss': 0.010235595703125, 'train/kl_loss': 0.2556640625, 'train/mask_bce_loss': 0.1116926833987236, 'train/mask_dice_loss': 0.35693380534648894, 'train/mask_loss': 0.46862649396061895, 'metrics/total_secs_per_batch': 7.719727039337158, 'metrics/data_secs_per_batch': 3.2535159587860107, '_timestamp': 1740967976.0420418}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 476 is less than current step: 499. Dropping entry: {'train/lr': 3.183673469387755e-06, '_timestamp': 1740967976.0423045}).
Epoch: [4][478/500]	Time  7.921 ( 7.921)	Loss 2.5416 (1.4996)	CeLoss 0.1934 (0.3244)	SegCLSLoss 0.0179 (0.0121)	KLLoss 0.3770 (0.2576)	MaskLoss 1.1507 (0.5717)	MaskBCELoss 0.3593 (0.1502)	MaskDICELoss 0.7914 (0.4216)
Epoch: [4][479/500]	Time  8.292 ( 8.292)	Loss 1.2217 (1.4716)	CeLoss 0.2539 (0.5124)	SegCLSLoss 0.0111 (0.0101)	KLLoss 0.3633 (0.2572)	MaskLoss 0.4624 (0.4642)	MaskBCELoss 0.0978 (0.0829)	MaskDICELoss 0.3646 (0.3812)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 477 is less than current step: 499. Dropping entry: {'train/loss': 1.4995748043060302, 'train/ce_loss': 0.324365234375, 'train/seg_cls_loss': 0.01207275390625, 'train/kl_loss': 0.2576171875, 'train/mask_bce_loss': 0.15016862452030183, 'train/mask_dice_loss': 0.4215670272707939, 'train/mask_loss': 0.5717356443405152, 'metrics/total_secs_per_batch': 7.920603275299072, 'metrics/data_secs_per_batch': 3.586903285980225, '_timestamp': 1740967983.9626036}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 477 is less than current step: 499. Dropping entry: {'train/lr': 3.0612244897959177e-06, '_timestamp': 1740967983.9628563}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 478 is less than current step: 499. Dropping entry: {'train/loss': 1.471614694595337, 'train/ce_loss': 0.51240234375, 'train/seg_cls_loss': 0.010089111328125, 'train/kl_loss': 0.2572265625, 'train/mask_bce_loss': 0.08292815452441574, 'train/mask_dice_loss': 0.38124834299087523, 'train/mask_loss': 0.46417649984359743, 'metrics/total_secs_per_batch': 8.291550636291504, 'metrics/data_secs_per_batch': 3.4794129610061644, '_timestamp': 1740967992.2542095}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 478 is less than current step: 499. Dropping entry: {'train/lr': 2.938775510204081e-06, '_timestamp': 1740967992.2544706}).
[2025-03-02 20:13:21,628] [INFO] [logging.py:96:log_dist] [Rank 0] step=2480, skipped=0, lr=[2.877551020408163e-06], mom=[(0.9, 0.95)]
[2025-03-02 20:13:21,634] [INFO] [timer.py:215:stop] epoch=0/micro_step=24800/global_step=2480, RunningAvgSamplesPerSec=1.4417861231796862, CurrSamplesPerSec=1.0662016730628623, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [4][480/500]	Time  9.381 ( 9.381)	Loss 2.3870 (1.9968)	CeLoss 0.2109 (0.2050)	SegCLSLoss 0.0200 (0.0176)	KLLoss 0.3555 (0.3271)	MaskLoss 1.0656 (0.8752)	MaskBCELoss 0.0794 (0.2180)	MaskDICELoss 0.9862 (0.6572)
Epoch: [4][481/500]	Time  7.085 ( 7.085)	Loss 1.6196 (1.8836)	CeLoss 0.2793 (0.4381)	SegCLSLoss 0.0126 (0.0158)	KLLoss 0.3613 (0.2943)	MaskLoss 0.6497 (0.7041)	MaskBCELoss 0.0310 (0.1374)	MaskDICELoss 0.6186 (0.5667)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 479 is less than current step: 499. Dropping entry: {'train/loss': 1.9968446969985962, 'train/ce_loss': 0.20498046875, 'train/seg_cls_loss': 0.0176025390625, 'train/kl_loss': 0.3271484375, 'train/mask_bce_loss': 0.2179728565737605, 'train/mask_dice_loss': 0.6572073101997375, 'train/mask_loss': 0.8751801729202271, 'metrics/total_secs_per_batch': 9.380579233169556, 'metrics/data_secs_per_batch': 4.1209312915802006, '_timestamp': 1740968001.6346922}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 479 is less than current step: 499. Dropping entry: {'train/lr': 2.8163265306122447e-06, '_timestamp': 1740968001.6350217}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 480 is less than current step: 499. Dropping entry: {'train/loss': 1.883575165271759, 'train/ce_loss': 0.4380859375, 'train/seg_cls_loss': 0.01578369140625, 'train/kl_loss': 0.2943359375, 'train/mask_bce_loss': 0.1374232482165098, 'train/mask_dice_loss': 0.5667178392410278, 'train/mask_loss': 0.7041410863399505, 'metrics/total_secs_per_batch': 7.085357427597046, 'metrics/data_secs_per_batch': 2.943577289581299, '_timestamp': 1740968008.7201803}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 480 is less than current step: 499. Dropping entry: {'train/lr': 2.693877551020408e-06, '_timestamp': 1740968008.7204494}).
Epoch: [4][482/500]	Time  7.160 ( 7.160)	Loss 1.9472 (1.4689)	CeLoss 0.2314 (0.2989)	SegCLSLoss 0.0240 (0.0140)	KLLoss 0.3633 (0.2896)	MaskLoss 0.8339 (0.5670)	MaskBCELoss 0.0082 (0.0735)	MaskDICELoss 0.8257 (0.4935)
Epoch: [4][483/500]	Time  8.375 ( 8.375)	Loss 2.2719 (1.6293)	CeLoss 0.3555 (0.2961)	SegCLSLoss 0.0229 (0.0127)	KLLoss 0.3594 (0.2893)	MaskLoss 0.9348 (0.6489)	MaskBCELoss 0.2176 (0.1213)	MaskDICELoss 0.7172 (0.5276)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 481 is less than current step: 499. Dropping entry: {'train/loss': 1.4689207315444945, 'train/ce_loss': 0.298876953125, 'train/seg_cls_loss': 0.013970947265625, 'train/kl_loss': 0.2896484375, 'train/mask_bce_loss': 0.07354950634762644, 'train/mask_dice_loss': 0.4934548079967499, 'train/mask_loss': 0.5670043110847474, 'metrics/total_secs_per_batch': 7.160428524017334, 'metrics/data_secs_per_batch': 3.2720604658126833, '_timestamp': 1740968015.8805747}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 481 is less than current step: 499. Dropping entry: {'train/lr': 2.571428571428571e-06, '_timestamp': 1740968015.880876}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 482 is less than current step: 499. Dropping entry: {'train/loss': 1.629342532157898, 'train/ce_loss': 0.296142578125, 'train/seg_cls_loss': 0.012701416015625, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.12126252390444278, 'train/mask_dice_loss': 0.5276128381490708, 'train/mask_loss': 0.64887535572052, 'metrics/total_secs_per_batch': 8.374543905258179, 'metrics/data_secs_per_batch': 3.6757184505462646, '_timestamp': 1740968024.2551124}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 482 is less than current step: 499. Dropping entry: {'train/lr': 2.4489795918367347e-06, '_timestamp': 1740968024.2553697}).
Epoch: [4][484/500]	Time  8.364 ( 8.364)	Loss 2.1302 (1.5933)	CeLoss 0.2598 (0.2937)	SegCLSLoss 0.0155 (0.0158)	KLLoss 0.3574 (0.3236)	MaskLoss 0.9138 (0.6298)	MaskBCELoss 0.0136 (0.0861)	MaskDICELoss 0.9002 (0.5437)
Epoch: [4][485/500]	Time  6.855 ( 6.855)	Loss 0.8290 (1.5879)	CeLoss 0.2363 (0.4974)	SegCLSLoss 0.0107 (0.0102)	KLLoss 0.3594 (0.2561)	MaskLoss 0.2758 (0.5300)	MaskBCELoss 0.0480 (0.1471)	MaskDICELoss 0.2278 (0.3828)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 483 is less than current step: 499. Dropping entry: {'train/loss': 1.5932873070240021, 'train/ce_loss': 0.29365234375, 'train/seg_cls_loss': 0.015838623046875, 'train/kl_loss': 0.3236328125, 'train/mask_bce_loss': 0.08605769574642182, 'train/mask_dice_loss': 0.5437402501702309, 'train/mask_loss': 0.6297979384660721, 'metrics/total_secs_per_batch': 8.36441946029663, 'metrics/data_secs_per_batch': 3.8189095973968508, '_timestamp': 1740968032.619555}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 483 is less than current step: 499. Dropping entry: {'train/lr': 2.3265306122448977e-06, '_timestamp': 1740968032.619826}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 484 is less than current step: 499. Dropping entry: {'train/loss': 1.5878504753112792, 'train/ce_loss': 0.49736328125, 'train/seg_cls_loss': 0.01015625, 'train/kl_loss': 0.2560546875, 'train/mask_bce_loss': 0.14711939841508864, 'train/mask_dice_loss': 0.3828409880399704, 'train/mask_loss': 0.5299603939056396, 'metrics/total_secs_per_batch': 6.855013608932495, 'metrics/data_secs_per_batch': 3.0002383947372437, '_timestamp': 1740968039.4745545}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 484 is less than current step: 499. Dropping entry: {'train/lr': 2.2040816326530608e-06, '_timestamp': 1740968039.4748254}).
Epoch: [4][486/500]	Time  5.345 ( 5.345)	Loss 1.5469 (1.4690)	CeLoss 1.5469 (0.8135)	SegCLSLoss 0.0000 (0.0075)	KLLoss 0.0000 (0.1822)	MaskLoss 0.0000 (0.3169)	MaskBCELoss 0.0000 (0.0694)	MaskDICELoss 0.0000 (0.2475)
Epoch: [4][487/500]	Time  9.817 ( 9.817)	Loss 2.1965 (1.6080)	CeLoss 0.2393 (0.2271)	SegCLSLoss 0.0146 (0.0135)	KLLoss 0.3711 (0.2918)	MaskLoss 0.9566 (0.6725)	MaskBCELoss 0.2501 (0.1201)	MaskDICELoss 0.7065 (0.5524)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 485 is less than current step: 499. Dropping entry: {'train/loss': 1.469000232219696, 'train/ce_loss': 0.813525390625, 'train/seg_cls_loss': 0.007470703125, 'train/kl_loss': 0.1822265625, 'train/mask_bce_loss': 0.06943563502281905, 'train/mask_dice_loss': 0.24748634696006774, 'train/mask_loss': 0.31692199110984803, 'metrics/total_secs_per_batch': 5.345370292663574, 'metrics/data_secs_per_batch': 2.2090172290802004, '_timestamp': 1740968044.8199742}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 485 is less than current step: 499. Dropping entry: {'train/lr': 2.0816326530612243e-06, '_timestamp': 1740968044.8202536}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 486 is less than current step: 499. Dropping entry: {'train/loss': 1.6080140233039857, 'train/ce_loss': 0.2271484375, 'train/seg_cls_loss': 0.013525390625, 'train/kl_loss': 0.291796875, 'train/mask_bce_loss': 0.12007753578945994, 'train/mask_dice_loss': 0.5523864924907684, 'train/mask_loss': 0.6724640309810639, 'metrics/total_secs_per_batch': 9.816590070724487, 'metrics/data_secs_per_batch': 4.197195768356323, '_timestamp': 1740968054.6367385}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 486 is less than current step: 499. Dropping entry: {'train/lr': 1.9591836734693877e-06, '_timestamp': 1740968054.637074}).
Epoch: [4][488/500]	Time  7.982 ( 7.982)	Loss 2.1639 (1.7842)	CeLoss 0.2441 (0.4141)	SegCLSLoss 0.0171 (0.0135)	KLLoss 0.3672 (0.2904)	MaskLoss 0.9374 (0.6672)	MaskBCELoss 0.0156 (0.1138)	MaskDICELoss 0.9219 (0.5534)
Epoch: [4][489/500]	Time  6.657 ( 6.657)	Loss 2.3964 (1.4186)	CeLoss 0.1787 (0.3442)	SegCLSLoss 0.0153 (0.0141)	KLLoss 0.3789 (0.2537)	MaskLoss 1.0859 (0.5208)	MaskBCELoss 0.3109 (0.1006)	MaskDICELoss 0.7750 (0.4201)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 487 is less than current step: 499. Dropping entry: {'train/loss': 1.7842314839363098, 'train/ce_loss': 0.4140625, 'train/seg_cls_loss': 0.01351318359375, 'train/kl_loss': 0.2904296875, 'train/mask_bce_loss': 0.11379999397322535, 'train/mask_dice_loss': 0.5534133970737457, 'train/mask_loss': 0.6672133803367615, 'metrics/total_secs_per_batch': 7.982238531112671, 'metrics/data_secs_per_batch': 3.2209779977798463, '_timestamp': 1740968062.6187897}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 487 is less than current step: 499. Dropping entry: {'train/lr': 1.8367346938775508e-06, '_timestamp': 1740968062.6189802}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 488 is less than current step: 499. Dropping entry: {'train/loss': 1.4185543179512023, 'train/ce_loss': 0.3442138671875, 'train/seg_cls_loss': 0.014117431640625, 'train/kl_loss': 0.2537109375, 'train/mask_bce_loss': 0.10063093956559896, 'train/mask_dice_loss': 0.4201330304145813, 'train/mask_loss': 0.5207639634609222, 'metrics/total_secs_per_batch': 6.656898736953735, 'metrics/data_secs_per_batch': 2.8986273288726805, '_timestamp': 1740968069.275683}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 488 is less than current step: 499. Dropping entry: {'train/lr': 1.714285714285714e-06, '_timestamp': 1740968069.275952}).
[2025-03-02 20:14:38,417] [INFO] [logging.py:96:log_dist] [Rank 0] step=2490, skipped=0, lr=[1.6530612244897958e-06], mom=[(0.9, 0.95)]
[2025-03-02 20:14:38,424] [INFO] [timer.py:215:stop] epoch=0/micro_step=24900/global_step=2490, RunningAvgSamplesPerSec=1.44116644169481, CurrSamplesPerSec=1.0931627469736065, MemAllocated=31.09GB, MaxMemAllocated=37.23GB
Epoch: [4][490/500]	Time  9.149 ( 9.149)	Loss 0.8246 (1.7733)	CeLoss 0.2598 (0.2230)	SegCLSLoss 0.0095 (0.0169)	KLLoss 0.3633 (0.3641)	MaskLoss 0.2619 (0.7527)	MaskBCELoss 0.0794 (0.1304)	MaskDICELoss 0.1826 (0.6224)
Epoch: [4][491/500]	Time  5.991 ( 5.991)	Loss 2.3205 (1.4988)	CeLoss 0.2637 (0.4566)	SegCLSLoss 0.0120 (0.0080)	KLLoss 0.3633 (0.2520)	MaskLoss 1.0069 (0.5062)	MaskBCELoss 0.3002 (0.1388)	MaskDICELoss 0.7067 (0.3675)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 489 is less than current step: 499. Dropping entry: {'train/loss': 1.7733344793319703, 'train/ce_loss': 0.223046875, 'train/seg_cls_loss': 0.016864013671875, 'train/kl_loss': 0.3640625, 'train/mask_bce_loss': 0.1303688800893724, 'train/mask_dice_loss': 0.622362807393074, 'train/mask_loss': 0.7527316987514496, 'metrics/total_secs_per_batch': 9.149251937866211, 'metrics/data_secs_per_batch': 3.940052366256714, '_timestamp': 1740968078.4247596}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 489 is less than current step: 499. Dropping entry: {'train/lr': 1.5918367346938775e-06, '_timestamp': 1740968078.4250345}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 490 is less than current step: 499. Dropping entry: {'train/loss': 1.4988231599330901, 'train/ce_loss': 0.456640625, 'train/seg_cls_loss': 0.0080322265625, 'train/kl_loss': 0.251953125, 'train/mask_bce_loss': 0.13878982588648797, 'train/mask_dice_loss': 0.3674577042460442, 'train/mask_loss': 0.5062475234270096, 'metrics/total_secs_per_batch': 5.991458177566528, 'metrics/data_secs_per_batch': 2.574280834197998, '_timestamp': 1740968084.4163914}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 490 is less than current step: 499. Dropping entry: {'train/lr': 1.4693877551020406e-06, '_timestamp': 1740968084.4166908}).
Epoch: [4][492/500]	Time  6.667 ( 6.667)	Loss 0.8828 (1.3933)	CeLoss 0.8828 (0.4811)	SegCLSLoss 0.0000 (0.0098)	KLLoss 0.0000 (0.2244)	MaskLoss 0.0000 (0.4424)	MaskBCELoss 0.0000 (0.0893)	MaskDICELoss 0.0000 (0.3531)
Epoch: [4][493/500]	Time  7.665 ( 7.665)	Loss 1.4549 (1.4411)	CeLoss 0.2656 (0.3853)	SegCLSLoss 0.0125 (0.0138)	KLLoss 0.3555 (0.2914)	MaskLoss 0.5741 (0.5100)	MaskBCELoss 0.1380 (0.1102)	MaskDICELoss 0.4361 (0.3998)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 491 is less than current step: 499. Dropping entry: {'train/loss': 1.3932919144630431, 'train/ce_loss': 0.4810546875, 'train/seg_cls_loss': 0.009796142578125, 'train/kl_loss': 0.2244140625, 'train/mask_bce_loss': 0.08933059200644493, 'train/mask_dice_loss': 0.35311613976955414, 'train/mask_loss': 0.4424467355012894, 'metrics/total_secs_per_batch': 6.667431831359863, 'metrics/data_secs_per_batch': 2.9074944257736206, '_timestamp': 1740968091.0838816}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 491 is less than current step: 499. Dropping entry: {'train/lr': 1.346938775510204e-06, '_timestamp': 1740968091.0841794}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 492 is less than current step: 499. Dropping entry: {'train/loss': 1.441141989827156, 'train/ce_loss': 0.38525390625, 'train/seg_cls_loss': 0.013824462890625, 'train/kl_loss': 0.29140625, 'train/mask_bce_loss': 0.11021586321294308, 'train/mask_dice_loss': 0.39980825036764145, 'train/mask_loss': 0.5100241139531135, 'metrics/total_secs_per_batch': 7.6654839515686035, 'metrics/data_secs_per_batch': 3.7733546257019044, '_timestamp': 1740968098.749429}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 492 is less than current step: 499. Dropping entry: {'train/lr': 1.2244897959183673e-06, '_timestamp': 1740968098.7497365}).
Epoch: [4][494/500]	Time  7.836 ( 7.836)	Loss 1.7350 (1.4315)	CeLoss 0.3262 (0.3846)	SegCLSLoss 0.0111 (0.0133)	KLLoss 0.3594 (0.2516)	MaskLoss 0.6839 (0.5076)	MaskBCELoss 0.0747 (0.0319)	MaskDICELoss 0.6093 (0.4757)
Epoch: [4][495/500]	Time  8.218 ( 8.218)	Loss 1.4766 (1.8438)	CeLoss 1.4766 (0.3400)	SegCLSLoss 0.0000 (0.0157)	KLLoss 0.0000 (0.3305)	MaskLoss 0.0000 (0.7316)	MaskBCELoss 0.0000 (0.1784)	MaskDICELoss 0.0000 (0.5532)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 493 is less than current step: 499. Dropping entry: {'train/loss': 1.4315358757972718, 'train/ce_loss': 0.3846435546875, 'train/seg_cls_loss': 0.01326904296875, 'train/kl_loss': 0.2515625, 'train/mask_bce_loss': 0.03188939401879907, 'train/mask_dice_loss': 0.4757364630699158, 'train/mask_loss': 0.5076258540153503, 'metrics/total_secs_per_batch': 7.835506439208984, 'metrics/data_secs_per_batch': 3.3460065364837646, '_timestamp': 1740968106.5848486}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 493 is less than current step: 499. Dropping entry: {'train/lr': 1.1020408163265304e-06, '_timestamp': 1740968106.5851097}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 494 is less than current step: 499. Dropping entry: {'train/loss': 1.843826127052307, 'train/ce_loss': 0.3400390625, 'train/seg_cls_loss': 0.015667724609375, 'train/kl_loss': 0.33046875, 'train/mask_bce_loss': 0.1783823280595243, 'train/mask_dice_loss': 0.5532475262880325, 'train/mask_loss': 0.7316298574209213, 'metrics/total_secs_per_batch': 8.21784496307373, 'metrics/data_secs_per_batch': 3.9615567922592163, '_timestamp': 1740968114.8026693}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 494 is less than current step: 499. Dropping entry: {'train/lr': 9.795918367346939e-07, '_timestamp': 1740968114.8029287}).
Epoch: [4][496/500]	Time  7.758 ( 7.758)	Loss 0.0718 (1.5052)	CeLoss 0.0718 (0.6077)	SegCLSLoss 0.0000 (0.0100)	KLLoss 0.0000 (0.1809)	MaskLoss 0.0000 (0.4373)	MaskBCELoss 0.0000 (0.0840)	MaskDICELoss 0.0000 (0.3533)
Epoch: [4][497/500]	Time  6.744 ( 6.744)	Loss 1.4037 (1.5839)	CeLoss 0.1982 (0.5351)	SegCLSLoss 0.0176 (0.0127)	KLLoss 0.3633 (0.2557)	MaskLoss 0.5803 (0.5084)	MaskBCELoss 0.0275 (0.1309)	MaskDICELoss 0.5528 (0.3774)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 495 is less than current step: 499. Dropping entry: {'train/loss': 1.5051814198493958, 'train/ce_loss': 0.607666015625, 'train/seg_cls_loss': 0.01002197265625, 'train/kl_loss': 0.180859375, 'train/mask_bce_loss': 0.08402794972062111, 'train/mask_dice_loss': 0.35325513780117035, 'train/mask_loss': 0.4372830927371979, 'metrics/total_secs_per_batch': 7.7579052448272705, 'metrics/data_secs_per_batch': 3.3710641384124758, '_timestamp': 1740968122.5605843}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 495 is less than current step: 499. Dropping entry: {'train/lr': 8.57142857142857e-07, '_timestamp': 1740968122.5608475}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 496 is less than current step: 499. Dropping entry: {'train/loss': 1.5839120268821716, 'train/ce_loss': 0.53505859375, 'train/seg_cls_loss': 0.012713623046875, 'train/kl_loss': 0.2556640625, 'train/mask_bce_loss': 0.13091283347457647, 'train/mask_dice_loss': 0.3774494305253029, 'train/mask_loss': 0.5083622626960278, 'metrics/total_secs_per_batch': 6.74397087097168, 'metrics/data_secs_per_batch': 2.4209791898727415, '_timestamp': 1740968129.3045535}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 496 is less than current step: 499. Dropping entry: {'train/lr': 7.346938775510203e-07, '_timestamp': 1740968129.304814}).
Epoch: [4][498/500]	Time  7.722 ( 7.722)	Loss 1.4900 (1.8867)	CeLoss 0.3223 (0.3383)	SegCLSLoss 0.0122 (0.0138)	KLLoss 0.3672 (0.2943)	MaskLoss 0.5624 (0.7561)	MaskBCELoss 0.0833 (0.1554)	MaskDICELoss 0.4791 (0.6007)
Epoch: [4][499/500]	Time  9.440 ( 9.440)	Loss 2.6187 (1.7790)	CeLoss 0.1904 (0.3066)	SegCLSLoss 0.0209 (0.0185)	KLLoss 0.3633 (0.3264)	MaskLoss 1.1907 (0.7152)	MaskBCELoss 0.4730 (0.1508)	MaskDICELoss 0.7177 (0.5644)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 497 is less than current step: 499. Dropping entry: {'train/loss': 1.8866660237312316, 'train/ce_loss': 0.3383056640625, 'train/seg_cls_loss': 0.013751220703125, 'train/kl_loss': 0.2943359375, 'train/mask_bce_loss': 0.1554229802452028, 'train/mask_dice_loss': 0.6006907731294632, 'train/mask_loss': 0.7561137616634369, 'metrics/total_secs_per_batch': 7.7223827838897705, 'metrics/data_secs_per_batch': 3.4808449268341066, '_timestamp': 1740968137.0269575}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 497 is less than current step: 499. Dropping entry: {'train/lr': 6.122448979591837e-07, '_timestamp': 1740968137.027242}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 498 is less than current step: 499. Dropping entry: {'train/loss': 1.779040378332138, 'train/ce_loss': 0.306640625, 'train/seg_cls_loss': 0.01845703125, 'train/kl_loss': 0.3263671875, 'train/mask_bce_loss': 0.15082407891750335, 'train/mask_dice_loss': 0.5643797010183335, 'train/mask_loss': 0.715203794836998, 'metrics/total_secs_per_batch': 9.44041895866394, 'metrics/data_secs_per_batch': 3.9646796226501464, '_timestamp': 1740968146.4673543}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 498 is less than current step: 499. Dropping entry: {'train/lr': 4.897959183673469e-07, '_timestamp': 1740968146.4676292}).
[2025-03-02 20:15:53,419] [INFO] [logging.py:96:log_dist] [Rank 0] step=2500, skipped=0, lr=[4.285714285714285e-07], mom=[(0.9, 0.95)]
[2025-03-02 20:15:53,425] [INFO] [timer.py:215:stop] epoch=0/micro_step=25000/global_step=2500, RunningAvgSamplesPerSec=1.4407009024215365, CurrSamplesPerSec=1.43743083036869, MemAllocated=30.8GB, MaxMemAllocated=37.23GB
Epoch: [4][500/500]	Time  6.958 ( 6.958)	Loss 1.0927 (1.5623)	CeLoss 0.1758 (0.3771)	SegCLSLoss 0.0201 (0.0171)	KLLoss 0.3633 (0.2906)	MaskLoss 0.4355 (0.5739)	MaskBCELoss 0.1275 (0.1183)	MaskDICELoss 0.3080 (0.4556)



















 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 198/200 [00:38<00:00,  4.56it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:39<00:00,  5.05it/s]
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 4 is less than current step: 499. Dropping entry: {'val/giou': 0.1879403293132782, 'val/ciou': 0.17601971328258514, '_timestamp': 1740968192.9995492}).
Epoch: [5][  1/500]	Time  7.702 ( 7.702)	Loss 2.2379 (1.7865)	CeLoss 0.1396 (0.4305)	SegCLSLoss 0.0205 (0.0117)	KLLoss 0.3809 (0.2547)	MaskLoss 1.0247 (0.6623)	MaskBCELoss 0.1862 (0.1355)	MaskDICELoss 0.8385 (0.5268)
Epoch: [5][  2/500]	Time  8.470 ( 8.470)	Loss 2.7115 (2.1070)	CeLoss 0.1533 (0.2276)	SegCLSLoss 0.0269 (0.0193)	KLLoss 0.3770 (0.3619)	MaskLoss 1.2532 (0.9166)	MaskBCELoss 0.3140 (0.1875)	MaskDICELoss 0.9393 (0.7291)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 499. Dropping entry: {'train/loss': 1.786455547809601, 'train/ce_loss': 0.43046875, 'train/seg_cls_loss': 0.011700439453125, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.1355433240532875, 'train/mask_dice_loss': 0.5267762362957, 'train/mask_loss': 0.662319564819336, 'metrics/total_secs_per_batch': 7.701764106750488, 'metrics/data_secs_per_batch': 3.097553563117981, '_timestamp': 1740968200.7080734}).
Epoch: [5][  3/500]	Time  8.275 ( 8.275)	Loss 2.4730 (1.4366)	CeLoss 0.1729 (0.1663)	SegCLSLoss 0.0251 (0.0155)	KLLoss 0.3770 (0.2545)	MaskLoss 1.1252 (0.6186)	MaskBCELoss 0.2853 (0.1258)	MaskDICELoss 0.8399 (0.4928)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 1 is less than current step: 499. Dropping entry: {'train/loss': 2.1069576859474184, 'train/ce_loss': 0.22763671875, 'train/seg_cls_loss': 0.01925048828125, 'train/kl_loss': 0.3619140625, 'train/mask_bce_loss': 0.18749692365527154, 'train/mask_dice_loss': 0.7291166722774506, 'train/mask_loss': 0.9166135847568512, 'metrics/total_secs_per_batch': 8.469777345657349, 'metrics/data_secs_per_batch': 4.059828472137451, '_timestamp': 1740968209.1779194}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 1 is less than current step: 499. Dropping entry: {'train/lr': 1.8367346938775507e-07, '_timestamp': 1740968209.1781988}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 2 is less than current step: 499. Dropping entry: {'train/loss': 1.4366293787956237, 'train/ce_loss': 0.16630859375, 'train/seg_cls_loss': 0.01552734375, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.1258050136268139, 'train/mask_dice_loss': 0.4927538216114044, 'train/mask_loss': 0.6185588359832763, 'metrics/total_secs_per_batch': 8.275202751159668, 'metrics/data_secs_per_batch': 3.74739351272583, '_timestamp': 1740968217.453144}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 2 is less than current step: 499. Dropping entry: {'train/lr': 6.122448979591837e-08, '_timestamp': 1740968217.4534576}).
Epoch: [5][  4/500]	Time  8.141 ( 8.141)	Loss 1.3414 (1.5715)	CeLoss 0.2852 (0.2963)	SegCLSLoss 0.0231 (0.0176)	KLLoss 0.3652 (0.3283)	MaskLoss 0.5037 (0.6167)	MaskBCELoss 0.0163 (0.0914)	MaskDICELoss 0.4874 (0.5254)
Epoch: [5][  5/500]	Time  8.503 ( 8.503)	Loss 1.6383 (1.9305)	CeLoss 0.2139 (0.3354)	SegCLSLoss 0.0209 (0.0192)	KLLoss 0.3672 (0.3330)	MaskLoss 0.6883 (0.7758)	MaskBCELoss 0.2394 (0.1465)	MaskDICELoss 0.4489 (0.6294)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 3 is less than current step: 499. Dropping entry: {'train/loss': 1.5715090394020081, 'train/ce_loss': 0.2962890625, 'train/seg_cls_loss': 0.01761474609375, 'train/kl_loss': 0.3283203125, 'train/mask_bce_loss': 0.09135008286684751, 'train/mask_dice_loss': 0.5253614693880081, 'train/mask_loss': 0.616711550951004, 'metrics/total_secs_per_batch': 8.141417503356934, 'metrics/data_secs_per_batch': 3.8230672121047973, '_timestamp': 1740968225.5945225}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 3 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968225.5948086}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 4 is less than current step: 499. Dropping entry: {'train/loss': 1.9304979920387269, 'train/ce_loss': 0.335400390625, 'train/seg_cls_loss': 0.019195556640625, 'train/kl_loss': 0.3330078125, 'train/mask_bce_loss': 0.1464771781116724, 'train/mask_dice_loss': 0.6293675154447556, 'train/mask_loss': 0.7758446961641312, 'metrics/total_secs_per_batch': 8.503334999084473, 'metrics/data_secs_per_batch': 4.10620436668396, '_timestamp': 1740968234.0978198}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 4 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968234.098153}).
Epoch: [5][  6/500]	Time  8.234 ( 8.234)	Loss 1.0859 (1.8794)	CeLoss 1.0859 (0.3777)	SegCLSLoss 0.0000 (0.0134)	KLLoss 0.0000 (0.2955)	MaskLoss 0.0000 (0.7327)	MaskBCELoss 0.0000 (0.1952)	MaskDICELoss 0.0000 (0.5375)
Epoch: [5][  7/500]	Time  8.448 ( 8.448)	Loss 1.6719 (1.7338)	CeLoss 1.6719 (0.3825)	SegCLSLoss 0.0000 (0.0114)	KLLoss 0.0000 (0.2908)	MaskLoss 0.0000 (0.6583)	MaskBCELoss 0.0000 (0.1341)	MaskDICELoss 0.0000 (0.5242)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 5 is less than current step: 499. Dropping entry: {'train/loss': 1.8793955385684966, 'train/ce_loss': 0.377734375, 'train/seg_cls_loss': 0.0133544921875, 'train/kl_loss': 0.2955078125, 'train/mask_bce_loss': 0.19515149891376496, 'train/mask_dice_loss': 0.5375150129199028, 'train/mask_loss': 0.7326665163040161, 'metrics/total_secs_per_batch': 8.233563423156738, 'metrics/data_secs_per_batch': 3.2626294136047362, '_timestamp': 1740968242.331381}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 5 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968242.33164}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 6 is less than current step: 499. Dropping entry: {'train/loss': 1.7338115632534028, 'train/ce_loss': 0.38251953125, 'train/seg_cls_loss': 0.011383056640625, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.13410356747335755, 'train/mask_dice_loss': 0.5241596400737762, 'train/mask_loss': 0.6582632035017013, 'metrics/total_secs_per_batch': 8.447502613067627, 'metrics/data_secs_per_batch': 4.003616142272949, '_timestamp': 1740968250.7789593}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 6 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968250.7792642}).
Epoch: [5][  8/500]	Time 10.128 (10.128)	Loss 0.0679 (1.7043)	CeLoss 0.0679 (0.2098)	SegCLSLoss 0.0000 (0.0183)	KLLoss 0.0000 (0.3234)	MaskLoss 0.0000 (0.7264)	MaskBCELoss 0.0000 (0.1087)	MaskDICELoss 0.0000 (0.6177)
Epoch: [5][  9/500]	Time  8.894 ( 8.894)	Loss 2.9358 (1.9874)	CeLoss 0.1816 (0.2616)	SegCLSLoss 0.0222 (0.0174)	KLLoss 0.3672 (0.3268)	MaskLoss 1.3536 (0.8422)	MaskBCELoss 0.4621 (0.1943)	MaskDICELoss 0.8915 (0.6479)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 7 is less than current step: 499. Dropping entry: {'train/loss': 1.704335629940033, 'train/ce_loss': 0.209814453125, 'train/seg_cls_loss': 0.01829833984375, 'train/kl_loss': 0.3234375, 'train/mask_bce_loss': 0.10874677803367376, 'train/mask_dice_loss': 0.6176642030477524, 'train/mask_loss': 0.726410973072052, 'metrics/total_secs_per_batch': 10.128055334091187, 'metrics/data_secs_per_batch': 4.616744613647461, '_timestamp': 1740968260.9070344}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 7 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968260.9073346}).
[2025-03-02 20:17:57,828] [INFO] [logging.py:96:log_dist] [Rank 0] step=2510, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:17:57,834] [INFO] [timer.py:215:stop] epoch=0/micro_step=25100/global_step=2510, RunningAvgSamplesPerSec=1.4394269604624716, CurrSamplesPerSec=1.2449473685741486, MemAllocated=31.26GB, MaxMemAllocated=37.23GB
Epoch: [5][ 10/500]	Time  8.034 ( 8.034)	Loss 1.9941 (1.6035)	CeLoss 0.1855 (0.4173)	SegCLSLoss 0.0265 (0.0135)	KLLoss 0.3555 (0.2889)	MaskLoss 0.8799 (0.5754)	MaskBCELoss 0.1136 (0.1133)	MaskDICELoss 0.7663 (0.4620)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 8 is less than current step: 499. Dropping entry: {'train/loss': 1.987372386455536, 'train/ce_loss': 0.26162109375, 'train/seg_cls_loss': 0.0174072265625, 'train/kl_loss': 0.3267578125, 'train/mask_bce_loss': 0.19430904276669025, 'train/mask_dice_loss': 0.6479122936725616, 'train/mask_loss': 0.842221337556839, 'metrics/total_secs_per_batch': 8.894043445587158, 'metrics/data_secs_per_batch': 3.5942430973052977, '_timestamp': 1740968269.800983}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 8 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968269.8012388}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 9 is less than current step: 499. Dropping entry: {'train/loss': 1.6034768760204314, 'train/ce_loss': 0.41728515625, 'train/seg_cls_loss': 0.013458251953125, 'train/kl_loss': 0.2888671875, 'train/mask_bce_loss': 0.11333747887983918, 'train/mask_dice_loss': 0.46203378587961197, 'train/mask_loss': 0.5753712564706802, 'metrics/total_secs_per_batch': 8.033945322036743, 'metrics/data_secs_per_batch': 3.7629197597503663, '_timestamp': 1740968277.8347743}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 9 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968277.8350344}).
Epoch: [5][ 11/500]	Time  8.703 ( 8.703)	Loss 1.0586 (1.8144)	CeLoss 0.2949 (0.4620)	SegCLSLoss 0.0139 (0.0120)	KLLoss 0.3652 (0.2885)	MaskLoss 0.3594 (0.6588)	MaskBCELoss 0.0114 (0.1119)	MaskDICELoss 0.3480 (0.5469)
Epoch: [5][ 12/500]	Time  7.416 ( 7.416)	Loss 1.7109 (1.7028)	CeLoss 1.7109 (0.6710)	SegCLSLoss 0.0000 (0.0094)	KLLoss 0.0000 (0.2547)	MaskLoss 0.0000 (0.5007)	MaskBCELoss 0.0000 (0.1155)	MaskDICELoss 0.0000 (0.3852)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 10 is less than current step: 499. Dropping entry: {'train/loss': 1.81443612575531, 'train/ce_loss': 0.46201171875, 'train/seg_cls_loss': 0.01201171875, 'train/kl_loss': 0.2884765625, 'train/mask_bce_loss': 0.11186863388866186, 'train/mask_dice_loss': 0.5469119220972061, 'train/mask_loss': 0.6587805688381195, 'metrics/total_secs_per_batch': 8.70308518409729, 'metrics/data_secs_per_batch': 4.180148291587829, '_timestamp': 1740968286.5380557}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 10 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968286.5383234}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 11 is less than current step: 499. Dropping entry: {'train/loss': 1.7027706027030944, 'train/ce_loss': 0.67099609375, 'train/seg_cls_loss': 0.009405517578125, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.1155020035803318, 'train/mask_dice_loss': 0.3851996958255768, 'train/mask_loss': 0.5007017076015472, 'metrics/total_secs_per_batch': 7.416221857070923, 'metrics/data_secs_per_batch': 2.95591778755188, '_timestamp': 1740968293.95426}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 11 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968293.9545262}).
Epoch: [5][ 13/500]	Time  7.782 ( 7.782)	Loss 1.2493 (1.6298)	CeLoss 0.2754 (0.2962)	SegCLSLoss 0.0089 (0.0142)	KLLoss 0.3672 (0.3307)	MaskLoss 0.4665 (0.6469)	MaskBCELoss 0.1543 (0.1513)	MaskDICELoss 0.3122 (0.4956)
Epoch: [5][ 14/500]	Time  8.173 ( 8.173)	Loss 0.6788 (1.1547)	CeLoss 0.5039 (0.4529)	SegCLSLoss 0.0123 (0.0083)	KLLoss 0.3770 (0.2564)	MaskLoss 0.0640 (0.3359)	MaskBCELoss 0.0327 (0.0687)	MaskDICELoss 0.0313 (0.2672)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 12 is less than current step: 499. Dropping entry: {'train/loss': 1.629750907421112, 'train/ce_loss': 0.29619140625, 'train/seg_cls_loss': 0.014202880859375, 'train/kl_loss': 0.3306640625, 'train/mask_bce_loss': 0.15128204142674803, 'train/mask_dice_loss': 0.4956246495246887, 'train/mask_loss': 0.646906702965498, 'metrics/total_secs_per_batch': 7.781646251678467, 'metrics/data_secs_per_batch': 3.648161196708679, '_timestamp': 1740968301.7359223}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 12 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968301.7361052}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 13 is less than current step: 499. Dropping entry: {'train/loss': 1.1546902179718017, 'train/ce_loss': 0.4529296875, 'train/seg_cls_loss': 0.008258056640625, 'train/kl_loss': 0.2564453125, 'train/mask_bce_loss': 0.06870809514075518, 'train/mask_dice_loss': 0.26723075807094576, 'train/mask_loss': 0.3359388574957848, 'metrics/total_secs_per_batch': 8.173229932785034, 'metrics/data_secs_per_batch': 3.7992599487304686, '_timestamp': 1740968309.909185}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 13 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968309.9094477}).
Epoch: [5][ 15/500]	Time  7.242 ( 7.242)	Loss 1.3750 (1.6448)	CeLoss 1.3750 (0.5711)	SegCLSLoss 0.0000 (0.0097)	KLLoss 0.0000 (0.2555)	MaskLoss 0.0000 (0.5216)	MaskBCELoss 0.0000 (0.1098)	MaskDICELoss 0.0000 (0.4119)
Epoch: [5][ 16/500]	Time  8.104 ( 8.104)	Loss 1.2885 (1.8986)	CeLoss 0.2695 (0.2531)	SegCLSLoss 0.0100 (0.0162)	KLLoss 0.3652 (0.3654)	MaskLoss 0.4880 (0.8005)	MaskBCELoss 0.1973 (0.2421)	MaskDICELoss 0.2907 (0.5585)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 14 is less than current step: 499. Dropping entry: {'train/loss': 1.6448456525802613, 'train/ce_loss': 0.57109375, 'train/seg_cls_loss': 0.009661865234375, 'train/kl_loss': 0.25546875, 'train/mask_bce_loss': 0.10977054284885526, 'train/mask_dice_loss': 0.4118710204958916, 'train/mask_loss': 0.5216415643692016, 'metrics/total_secs_per_batch': 7.242276191711426, 'metrics/data_secs_per_batch': 3.5399996757507326, '_timestamp': 1740968317.1515944}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 14 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968317.1519275}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 15 is less than current step: 499. Dropping entry: {'train/loss': 1.8986302375793458, 'train/ce_loss': 0.253125, 'train/seg_cls_loss': 0.016217041015625, 'train/kl_loss': 0.3654296875, 'train/mask_bce_loss': 0.2420747395604849, 'train/mask_dice_loss': 0.5584610879421235, 'train/mask_loss': 0.8005358189344406, 'metrics/total_secs_per_batch': 8.103801012039185, 'metrics/data_secs_per_batch': 3.8090259075164794, '_timestamp': 1740968325.2552502}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 15 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968325.2555144}).
Epoch: [5][ 17/500]	Time  8.283 ( 8.283)	Loss 0.9531 (1.7470)	CeLoss 0.9531 (0.2848)	SegCLSLoss 0.0000 (0.0151)	KLLoss 0.0000 (0.3262)	MaskLoss 0.0000 (0.7111)	MaskBCELoss 0.0000 (0.1342)	MaskDICELoss 0.0000 (0.5769)
Epoch: [5][ 18/500]	Time  7.483 ( 7.483)	Loss 1.9461 (1.2641)	CeLoss 0.2812 (0.2689)	SegCLSLoss 0.0112 (0.0107)	KLLoss 0.3652 (0.2543)	MaskLoss 0.8110 (0.4822)	MaskBCELoss 0.3589 (0.1040)	MaskDICELoss 0.4521 (0.3781)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 16 is less than current step: 499. Dropping entry: {'train/loss': 1.7470394134521485, 'train/ce_loss': 0.284765625, 'train/seg_cls_loss': 0.0151123046875, 'train/kl_loss': 0.326171875, 'train/mask_bce_loss': 0.13421664778143166, 'train/mask_dice_loss': 0.5768518641591072, 'train/mask_loss': 0.711068519949913, 'metrics/total_secs_per_batch': 8.283184051513672, 'metrics/data_secs_per_batch': 3.8990081548690796, '_timestamp': 1740968333.5384994}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 16 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968333.5387847}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 17 is less than current step: 499. Dropping entry: {'train/loss': 1.2640861749649048, 'train/ce_loss': 0.268896484375, 'train/seg_cls_loss': 0.01065673828125, 'train/kl_loss': 0.254296875, 'train/mask_bce_loss': 0.10401655863970519, 'train/mask_dice_loss': 0.37814860343933104, 'train/mask_loss': 0.4821651577949524, 'metrics/total_secs_per_batch': 7.4826672077178955, 'metrics/data_secs_per_batch': 3.568010377883911, '_timestamp': 1740968341.0211003}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 17 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968341.0213783}).
Epoch: [5][ 19/500]	Time  6.710 ( 6.710)	Loss 0.7930 (1.1622)	CeLoss 0.7930 (0.5327)	SegCLSLoss 0.0000 (0.0074)	KLLoss 0.0000 (0.1447)	MaskLoss 0.0000 (0.3057)	MaskBCELoss 0.0000 (0.0452)	MaskDICELoss 0.0000 (0.2605)
[2025-03-02 20:19:15,237] [INFO] [logging.py:96:log_dist] [Rank 0] step=2520, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:19:15,243] [INFO] [timer.py:215:stop] epoch=0/micro_step=25200/global_step=2520, RunningAvgSamplesPerSec=1.4387750955752772, CurrSamplesPerSec=1.3312332223290984, MemAllocated=30.69GB, MaxMemAllocated=37.23GB
Epoch: [5][ 20/500]	Time  7.513 ( 7.513)	Loss 1.7188 (1.7522)	CeLoss 1.7188 (0.5597)	SegCLSLoss 0.0000 (0.0115)	KLLoss 0.0000 (0.2543)	MaskLoss 0.0000 (0.5808)	MaskBCELoss 0.0000 (0.1310)	MaskDICELoss 0.0000 (0.4498)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 18 is less than current step: 499. Dropping entry: {'train/loss': 1.1622098684310913, 'train/ce_loss': 0.532666015625, 'train/seg_cls_loss': 0.00738525390625, 'train/kl_loss': 0.1447265625, 'train/mask_bce_loss': 0.04518904220312834, 'train/mask_dice_loss': 0.26050084829330444, 'train/mask_loss': 0.3056898951530457, 'metrics/total_secs_per_batch': 6.709905385971069, 'metrics/data_secs_per_batch': 3.0487491369247435, '_timestamp': 1740968347.7310095}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 18 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968347.7312794}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 19 is less than current step: 499. Dropping entry: {'train/loss': 1.752215838432312, 'train/ce_loss': 0.55966796875, 'train/seg_cls_loss': 0.011480712890625, 'train/kl_loss': 0.254296875, 'train/mask_bce_loss': 0.13100747391581535, 'train/mask_dice_loss': 0.44978795051574705, 'train/mask_loss': 0.5807954221963882, 'metrics/total_secs_per_batch': 7.51337194442749, 'metrics/data_secs_per_batch': 3.6251428604125975, '_timestamp': 1740968355.2441988}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 19 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968355.2444859}).
Epoch: [5][ 21/500]	Time  8.157 ( 8.157)	Loss 1.0402 (1.8145)	CeLoss 0.2832 (0.3248)	SegCLSLoss 0.0095 (0.0147)	KLLoss 0.3652 (0.3287)	MaskLoss 0.3580 (0.7246)	MaskBCELoss 0.0621 (0.1692)	MaskDICELoss 0.2959 (0.5554)
Epoch: [5][ 22/500]	Time  9.070 ( 9.070)	Loss 2.5230 (1.4954)	CeLoss 0.2852 (0.2803)	SegCLSLoss 0.0157 (0.0137)	KLLoss 0.3750 (0.2914)	MaskLoss 1.0965 (0.5896)	MaskBCELoss 0.3055 (0.1130)	MaskDICELoss 0.7909 (0.4766)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 20 is less than current step: 499. Dropping entry: {'train/loss': 1.8145208954811096, 'train/ce_loss': 0.3248046875, 'train/seg_cls_loss': 0.014715576171875, 'train/kl_loss': 0.3287109375, 'train/mask_bce_loss': 0.1692411180585623, 'train/mask_dice_loss': 0.5554021447896957, 'train/mask_loss': 0.7246432572603225, 'metrics/total_secs_per_batch': 8.156687498092651, 'metrics/data_secs_per_batch': 3.6263747453689574, '_timestamp': 1740968363.4011505}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 20 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968363.4014714}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 21 is less than current step: 499. Dropping entry: {'train/loss': 1.4953967571258544, 'train/ce_loss': 0.2802734375, 'train/seg_cls_loss': 0.0136962890625, 'train/kl_loss': 0.29140625, 'train/mask_bce_loss': 0.11297231148928404, 'train/mask_dice_loss': 0.47662060260772704, 'train/mask_loss': 0.5895929098129272, 'metrics/total_secs_per_batch': 9.070393562316895, 'metrics/data_secs_per_batch': 3.8190130472183226, '_timestamp': 1740968372.4714851}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 21 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968372.4717634}).
Epoch: [5][ 23/500]	Time  8.669 ( 8.669)	Loss 2.4968 (1.8777)	CeLoss 0.1719 (0.2380)	SegCLSLoss 0.0182 (0.0164)	KLLoss 0.3691 (0.3664)	MaskLoss 1.1395 (0.7976)	MaskBCELoss 0.2781 (0.1395)	MaskDICELoss 0.8614 (0.6580)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 22 is less than current step: 499. Dropping entry: {'train/loss': 1.8776512742042542, 'train/ce_loss': 0.23798828125, 'train/seg_cls_loss': 0.01639404296875, 'train/kl_loss': 0.36640625, 'train/mask_bce_loss': 0.13953463900834323, 'train/mask_dice_loss': 0.6580312192440033, 'train/mask_loss': 0.7975658535957336, 'metrics/total_secs_per_batch': 8.668716669082642, 'metrics/data_secs_per_batch': 3.976357936859131, '_timestamp': 1740968381.1402106}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 22 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968381.1405106}).
Epoch: [5][ 24/500]	Time  9.519 ( 9.519)	Loss 1.6632 (1.6913)	CeLoss 0.2520 (0.3002)	SegCLSLoss 0.0184 (0.0149)	KLLoss 0.3633 (0.2902)	MaskLoss 0.6822 (0.6772)	MaskBCELoss 0.0353 (0.1557)	MaskDICELoss 0.6469 (0.5215)
Epoch: [5][ 25/500]	Time  8.291 ( 8.291)	Loss 1.4078 (1.6378)	CeLoss 0.2314 (0.4460)	SegCLSLoss 0.0110 (0.0133)	KLLoss 0.3633 (0.2920)	MaskLoss 0.5672 (0.5780)	MaskBCELoss 0.0949 (0.1312)	MaskDICELoss 0.4722 (0.4468)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 23 is less than current step: 499. Dropping entry: {'train/loss': 1.6912519752979278, 'train/ce_loss': 0.300244140625, 'train/seg_cls_loss': 0.014892578125, 'train/kl_loss': 0.290234375, 'train/mask_bce_loss': 0.155721565335989, 'train/mask_dice_loss': 0.5215206354856491, 'train/mask_loss': 0.6772422105073929, 'metrics/total_secs_per_batch': 9.519097566604614, 'metrics/data_secs_per_batch': 4.501510214805603, '_timestamp': 1740968390.659345}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 23 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968390.6596768}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 24 is less than current step: 499. Dropping entry: {'train/loss': 1.6377910792827606, 'train/ce_loss': 0.44599609375, 'train/seg_cls_loss': 0.01328125, 'train/kl_loss': 0.2919921875, 'train/mask_bce_loss': 0.13122285809367895, 'train/mask_dice_loss': 0.44680353105068205, 'train/mask_loss': 0.5780263870954514, 'metrics/total_secs_per_batch': 8.29149842262268, 'metrics/data_secs_per_batch': 3.6541213035583495, '_timestamp': 1740968398.9508116}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 24 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968398.9510913}).
Epoch: [5][ 26/500]	Time  8.228 ( 8.228)	Loss 2.2174 (1.8543)	CeLoss 0.1777 (0.3438)	SegCLSLoss 0.0203 (0.0152)	KLLoss 0.3535 (0.3238)	MaskLoss 0.9974 (0.7353)	MaskBCELoss 0.1741 (0.1159)	MaskDICELoss 0.8233 (0.6194)
Epoch: [5][ 27/500]	Time  8.103 ( 8.103)	Loss 1.1484 (1.4707)	CeLoss 1.1484 (0.3135)	SegCLSLoss 0.0000 (0.0122)	KLLoss 0.0000 (0.2869)	MaskLoss 0.0000 (0.5611)	MaskBCELoss 0.0000 (0.0725)	MaskDICELoss 0.0000 (0.4885)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 25 is less than current step: 499. Dropping entry: {'train/loss': 1.8542954206466675, 'train/ce_loss': 0.34384765625, 'train/seg_cls_loss': 0.01519775390625, 'train/kl_loss': 0.323828125, 'train/mask_bce_loss': 0.11588899642229081, 'train/mask_dice_loss': 0.6193641796708107, 'train/mask_loss': 0.7352531850337982, 'metrics/total_secs_per_batch': 8.227554559707642, 'metrics/data_secs_per_batch': 4.0683749437332155, '_timestamp': 1740968407.1784983}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 25 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968407.178835}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 26 is less than current step: 499. Dropping entry: {'train/loss': 1.4706867635250092, 'train/ce_loss': 0.313525390625, 'train/seg_cls_loss': 0.012213134765625, 'train/kl_loss': 0.2869140625, 'train/mask_bce_loss': 0.07253033448942006, 'train/mask_dice_loss': 0.48852105885744096, 'train/mask_loss': 0.5610513895750046, 'metrics/total_secs_per_batch': 8.103290796279907, 'metrics/data_secs_per_batch': 3.894599270820618, '_timestamp': 1740968415.2816486}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 26 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968415.2819424}).
Epoch: [5][ 28/500]	Time  7.228 ( 7.228)	Loss 1.2832 (1.5907)	CeLoss 0.2461 (0.2627)	SegCLSLoss 0.0227 (0.0174)	KLLoss 0.3652 (0.3605)	MaskLoss 0.4941 (0.6417)	MaskBCELoss 0.0246 (0.0755)	MaskDICELoss 0.4696 (0.5662)
Epoch: [5][ 29/500]	Time  7.319 ( 7.319)	Loss 1.4531 (1.6553)	CeLoss 1.4531 (0.6207)	SegCLSLoss 0.0000 (0.0134)	KLLoss 0.0000 (0.2189)	MaskLoss 0.0000 (0.5028)	MaskBCELoss 0.0000 (0.1002)	MaskDICELoss 0.0000 (0.4026)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 27 is less than current step: 499. Dropping entry: {'train/loss': 1.590734714269638, 'train/ce_loss': 0.2626953125, 'train/seg_cls_loss': 0.01741943359375, 'train/kl_loss': 0.360546875, 'train/mask_bce_loss': 0.07547362595796585, 'train/mask_dice_loss': 0.5662316113710404, 'train/mask_loss': 0.6417052447795868, 'metrics/total_secs_per_batch': 7.2280333042144775, 'metrics/data_secs_per_batch': 3.237151575088501, '_timestamp': 1740968422.509679}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 27 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968422.5099576}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 28 is less than current step: 499. Dropping entry: {'train/loss': 1.6552556991577148, 'train/ce_loss': 0.620703125, 'train/seg_cls_loss': 0.013446044921875, 'train/kl_loss': 0.2189453125, 'train/mask_bce_loss': 0.10018071848899127, 'train/mask_dice_loss': 0.40264244079589845, 'train/mask_loss': 0.5028231501579284, 'metrics/total_secs_per_batch': 7.31947135925293, 'metrics/data_secs_per_batch': 3.1558250904083254, '_timestamp': 1740968429.8291976}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 28 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968429.8294957}).
[2025-03-02 20:20:36,782] [INFO] [logging.py:96:log_dist] [Rank 0] step=2530, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:20:36,788] [INFO] [timer.py:215:stop] epoch=0/micro_step=25300/global_step=2530, RunningAvgSamplesPerSec=1.437790740898098, CurrSamplesPerSec=1.437043782803526, MemAllocated=30.95GB, MaxMemAllocated=37.23GB
Epoch: [5][ 30/500]	Time  6.960 ( 6.960)	Loss 1.0000 (2.1039)	CeLoss 1.0000 (0.5501)	SegCLSLoss 0.0000 (0.0137)	KLLoss 0.0000 (0.2557)	MaskLoss 0.0000 (0.7606)	MaskBCELoss 0.0000 (0.2559)	MaskDICELoss 0.0000 (0.5047)
Epoch: [5][ 31/500]	Time  7.964 ( 7.964)	Loss 2.0984 (1.6087)	CeLoss 0.2734 (0.3798)	SegCLSLoss 0.0104 (0.0110)	KLLoss 0.3633 (0.2900)	MaskLoss 0.8910 (0.5969)	MaskBCELoss 0.0359 (0.1047)	MaskDICELoss 0.8551 (0.4922)
Exception in thread Thread-11 (_pin_memory_loop):
Traceback (most recent call last):
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 54, in _pin_memory_loop
    do_one_step()
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 31, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/multiprocessing/reductions.py", line 495, in rebuild_storage_fd
    fd = df.detach()
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/connection.py", line 502, in Client
    c = SocketClient(address)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/connection.py", line 630, in SocketClient
    s.connect(address)
FileNotFoundError: [Errno 2] No such file or directory
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 29 is less than current step: 499. Dropping entry: {'train/loss': 2.10385000705719, 'train/ce_loss': 0.55009765625, 'train/seg_cls_loss': 0.013690185546875, 'train/kl_loss': 0.2556640625, 'train/mask_bce_loss': 0.25591224506497384, 'train/mask_dice_loss': 0.5047041714191437, 'train/mask_loss': 0.760616409778595, 'metrics/total_secs_per_batch': 6.960317134857178, 'metrics/data_secs_per_batch': 2.9687466859817504, '_timestamp': 1740968436.7893176}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 29 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968436.7896144}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 30 is less than current step: 499. Dropping entry: {'train/loss': 1.6086992263793944, 'train/ce_loss': 0.379833984375, 'train/seg_cls_loss': 0.010968017578125, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.10467125382274389, 'train/mask_dice_loss': 0.4921832591295242, 'train/mask_loss': 0.5968545064330101, 'metrics/total_secs_per_batch': 7.963761329650879, 'metrics/data_secs_per_batch': 3.495733618736267, '_timestamp': 1740968444.7532015}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 30 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968444.7534728}).
Epoch: [5][ 32/500]	Time 10.191 (10.191)	Loss 2.3677 (1.6220)	CeLoss 0.2041 (0.3772)	SegCLSLoss 0.0206 (0.0141)	KLLoss 0.3516 (0.2900)	MaskLoss 1.0588 (0.6042)	MaskBCELoss 0.0820 (0.1289)	MaskDICELoss 0.9769 (0.4753)
Epoch: [5][ 33/500]	Time  8.543 ( 8.543)	Loss 0.0574 (1.8651)	CeLoss 0.0574 (0.2358)	SegCLSLoss 0.0000 (0.0133)	KLLoss 0.0000 (0.3246)	MaskLoss 0.0000 (0.7951)	MaskBCELoss 0.0000 (0.2125)	MaskDICELoss 0.0000 (0.5826)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 31 is less than current step: 499. Dropping entry: {'train/loss': 1.6220433115959167, 'train/ce_loss': 0.37724609375, 'train/seg_cls_loss': 0.0141357421875, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.128916871920228, 'train/mask_dice_loss': 0.4752688467502594, 'train/mask_loss': 0.6041857182979584, 'metrics/total_secs_per_batch': 10.191055536270142, 'metrics/data_secs_per_batch': 4.757735967636108, '_timestamp': 1740968454.9444053}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 31 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968454.9448}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 32 is less than current step: 499. Dropping entry: {'train/loss': 1.8650808572769164, 'train/ce_loss': 0.2358154296875, 'train/seg_cls_loss': 0.013275146484375, 'train/kl_loss': 0.324609375, 'train/mask_bce_loss': 0.21250140136107804, 'train/mask_dice_loss': 0.5826000809669495, 'train/mask_loss': 0.7951014637947083, 'metrics/total_secs_per_batch': 8.542858362197876, 'metrics/data_secs_per_batch': 3.952419137954712, '_timestamp': 1740968463.48714}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 32 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968463.4874165}).
Epoch: [5][ 34/500]	Time  6.511 ( 6.511)	Loss 1.5488 (1.4844)	CeLoss 0.2891 (0.4907)	SegCLSLoss 0.0106 (0.0122)	KLLoss 0.3633 (0.2510)	MaskLoss 0.6084 (0.4811)	MaskBCELoss 0.0754 (0.0970)	MaskDICELoss 0.5330 (0.3841)
Epoch: [5][ 35/500]	Time  8.319 ( 8.319)	Loss 2.0555 (1.5957)	CeLoss 0.2207 (0.3255)	SegCLSLoss 0.0140 (0.0139)	KLLoss 0.3691 (0.3254)	MaskLoss 0.8959 (0.6155)	MaskBCELoss 0.4548 (0.0951)	MaskDICELoss 0.4411 (0.5204)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 33 is less than current step: 499. Dropping entry: {'train/loss': 1.4844157814979553, 'train/ce_loss': 0.49072265625, 'train/seg_cls_loss': 0.012225341796875, 'train/kl_loss': 0.2509765625, 'train/mask_bce_loss': 0.0970428136177361, 'train/mask_dice_loss': 0.38408108949661257, 'train/mask_loss': 0.48112389743328093, 'metrics/total_secs_per_batch': 6.510629177093506, 'metrics/data_secs_per_batch': 2.3447241306304933, '_timestamp': 1740968469.9977586}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 33 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968469.9980297}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 34 is less than current step: 499. Dropping entry: {'train/loss': 1.5957402408123016, 'train/ce_loss': 0.32548828125, 'train/seg_cls_loss': 0.013909912109375, 'train/kl_loss': 0.325390625, 'train/mask_bce_loss': 0.09509933795779943, 'train/mask_dice_loss': 0.5203977271914482, 'train/mask_loss': 0.6154970705509186, 'metrics/total_secs_per_batch': 8.318815231323242, 'metrics/data_secs_per_batch': 3.6523135900497437, '_timestamp': 1740968478.31678}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 34 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968478.3171172}).
Epoch: [5][ 36/500]	Time  7.829 ( 7.829)	Loss 0.1318 (1.3261)	CeLoss 0.1318 (0.5391)	SegCLSLoss 0.0000 (0.0088)	KLLoss 0.0000 (0.2156)	MaskLoss 0.0000 (0.3806)	MaskBCELoss 0.0000 (0.0498)	MaskDICELoss 0.0000 (0.3307)
Epoch: [5][ 37/500]	Time  8.055 ( 8.055)	Loss 2.2779 (1.5101)	CeLoss 0.2139 (0.2439)	SegCLSLoss 0.0121 (0.0145)	KLLoss 0.3633 (0.3279)	MaskLoss 1.0110 (0.6129)	MaskBCELoss 0.3389 (0.1166)	MaskDICELoss 0.6721 (0.4964)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 35 is less than current step: 499. Dropping entry: {'train/loss': 1.326147699356079, 'train/ce_loss': 0.5390625, 'train/seg_cls_loss': 0.008758544921875, 'train/kl_loss': 0.215625, 'train/mask_bce_loss': 0.04984134174883366, 'train/mask_dice_loss': 0.3307129740715027, 'train/mask_loss': 0.380554324388504, 'metrics/total_secs_per_batch': 7.8285417556762695, 'metrics/data_secs_per_batch': 3.4249727964401244, '_timestamp': 1740968486.145135}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 35 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968486.1454}).
Epoch: [5][ 38/500]	Time  7.246 ( 7.246)	Loss 2.6228 (1.8058)	CeLoss 0.1465 (0.4158)	SegCLSLoss 0.0286 (0.0165)	KLLoss 0.3770 (0.2951)	MaskLoss 1.2118 (0.6762)	MaskBCELoss 0.4784 (0.1778)	MaskDICELoss 0.7333 (0.4984)
Epoch: [5][ 39/500]	Time  6.740 ( 6.740)	Loss 1.5439 (1.2211)	CeLoss 0.3223 (0.5354)	SegCLSLoss 0.0099 (0.0073)	KLLoss 0.3633 (0.2176)	MaskLoss 0.5893 (0.3299)	MaskBCELoss 0.1358 (0.0395)	MaskDICELoss 0.4535 (0.2905)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 36 is less than current step: 499. Dropping entry: {'train/loss': 1.5101219981908798, 'train/ce_loss': 0.2439453125, 'train/seg_cls_loss': 0.014544677734375, 'train/kl_loss': 0.3279296875, 'train/mask_bce_loss': 0.11656521130353212, 'train/mask_dice_loss': 0.4963571146130562, 'train/mask_loss': 0.6129223272204399, 'metrics/total_secs_per_batch': 8.054865837097168, 'metrics/data_secs_per_batch': 3.2803439617156984, '_timestamp': 1740968494.1999693}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 36 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968494.2002406}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 37 is less than current step: 499. Dropping entry: {'train/loss': 1.8058326601982118, 'train/ce_loss': 0.4158203125, 'train/seg_cls_loss': 0.016497802734375, 'train/kl_loss': 0.2951171875, 'train/mask_bce_loss': 0.17779314741492272, 'train/mask_dice_loss': 0.4984141856431961, 'train/mask_loss': 0.6762073293328286, 'metrics/total_secs_per_batch': 7.246175289154053, 'metrics/data_secs_per_batch': 3.4117947816848755, '_timestamp': 1740968501.4461884}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 37 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968501.4464533}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 38 is less than current step: 499. Dropping entry: {'train/loss': 1.221093511581421, 'train/ce_loss': 0.5354248046875, 'train/seg_cls_loss': 0.007318115234375, 'train/kl_loss': 0.217578125, 'train/mask_bce_loss': 0.03946690298616886, 'train/mask_dice_loss': 0.2904768198728561, 'train/mask_loss': 0.32994372099637986, 'metrics/total_secs_per_batch': 6.740247964859009, 'metrics/data_secs_per_batch': 2.8258653402328493, '_timestamp': 1740968508.18666}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 38 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968508.1870706}).
[2025-03-02 20:21:56,822] [INFO] [logging.py:96:log_dist] [Rank 0] step=2540, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:21:56,828] [INFO] [timer.py:215:stop] epoch=0/micro_step=25400/global_step=2540, RunningAvgSamplesPerSec=1.436955364209708, CurrSamplesPerSec=1.1573353449016355, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [5][ 40/500]	Time  8.642 ( 8.642)	Loss 1.8897 (1.7405)	CeLoss 0.2246 (0.4070)	SegCLSLoss 0.0204 (0.0157)	KLLoss 0.3555 (0.2549)	MaskLoss 0.8101 (0.6502)	MaskBCELoss 0.0555 (0.2038)	MaskDICELoss 0.7546 (0.4464)
Epoch: [5][ 41/500]	Time  6.687 ( 6.687)	Loss 2.5172 (1.5044)	CeLoss 0.2051 (0.4258)	SegCLSLoss 0.0195 (0.0126)	KLLoss 0.3691 (0.2562)	MaskLoss 1.1326 (0.5233)	MaskBCELoss 0.2223 (0.1149)	MaskDICELoss 0.9104 (0.4083)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 39 is less than current step: 499. Dropping entry: {'train/loss': 1.7404754877090454, 'train/ce_loss': 0.406982421875, 'train/seg_cls_loss': 0.01568603515625, 'train/kl_loss': 0.2548828125, 'train/mask_bce_loss': 0.20382756777107716, 'train/mask_dice_loss': 0.44636625051498413, 'train/mask_loss': 0.6501938045024872, 'metrics/total_secs_per_batch': 8.642430782318115, 'metrics/data_secs_per_batch': 3.1865931034088133, '_timestamp': 1740968516.8286717}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 39 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968516.8288496}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 40 is less than current step: 499. Dropping entry: {'train/loss': 1.50444438457489, 'train/ce_loss': 0.42578125, 'train/seg_cls_loss': 0.012615966796875, 'train/kl_loss': 0.25625, 'train/mask_bce_loss': 0.11493605803698301, 'train/mask_dice_loss': 0.4083310507237911, 'train/mask_loss': 0.5232671156525612, 'metrics/total_secs_per_batch': 6.686900615692139, 'metrics/data_secs_per_batch': 3.3073877811431887, '_timestamp': 1740968523.5157764}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 40 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968523.5160456}).
Epoch: [5][ 42/500]	Time  6.708 ( 6.708)	Loss 1.2879 (1.4763)	CeLoss 0.2354 (0.4923)	SegCLSLoss 0.0148 (0.0101)	KLLoss 0.3652 (0.2561)	MaskLoss 0.5043 (0.4767)	MaskBCELoss 0.2285 (0.0989)	MaskDICELoss 0.2758 (0.3777)
Epoch: [5][ 43/500]	Time  8.659 ( 8.659)	Loss 2.2508 (2.2526)	CeLoss 0.2061 (0.1980)	SegCLSLoss 0.0147 (0.0200)	KLLoss 0.3535 (0.3664)	MaskLoss 1.0009 (1.0041)	MaskBCELoss 0.2259 (0.2955)	MaskDICELoss 0.7750 (0.7086)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 41 is less than current step: 499. Dropping entry: {'train/loss': 1.4763484716415405, 'train/ce_loss': 0.49228515625, 'train/seg_cls_loss': 0.010107421875, 'train/kl_loss': 0.2560546875, 'train/mask_bce_loss': 0.09893649453297257, 'train/mask_dice_loss': 0.37773871421813965, 'train/mask_loss': 0.4766752064228058, 'metrics/total_secs_per_batch': 6.708498001098633, 'metrics/data_secs_per_batch': 2.625045967102051, '_timestamp': 1740968530.2242813}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 41 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968530.2244728}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 42 is less than current step: 499. Dropping entry: {'train/loss': 2.2526442885398863, 'train/ce_loss': 0.198046875, 'train/seg_cls_loss': 0.019970703125, 'train/kl_loss': 0.36640625, 'train/mask_bce_loss': 0.295465855486691, 'train/mask_dice_loss': 0.7085906535387039, 'train/mask_loss': 1.0040565192699433, 'metrics/total_secs_per_batch': 8.659369468688965, 'metrics/data_secs_per_batch': 3.4939305782318115, '_timestamp': 1740968538.8836467}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 42 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968538.8839257}).
Epoch: [5][ 44/500]	Time  6.838 ( 6.838)	Loss 0.7368 (1.5689)	CeLoss 0.2354 (0.3521)	SegCLSLoss 0.0117 (0.0112)	KLLoss 0.3613 (0.2900)	MaskLoss 0.2297 (0.5913)	MaskBCELoss 0.0458 (0.1192)	MaskDICELoss 0.1840 (0.4720)
Epoch: [5][ 45/500]	Time  7.878 ( 7.878)	Loss 2.0100 (1.8435)	CeLoss 0.1836 (0.3618)	SegCLSLoss 0.0254 (0.0168)	KLLoss 0.3652 (0.3230)	MaskLoss 0.8888 (0.7206)	MaskBCELoss 0.1493 (0.0864)	MaskDICELoss 0.7395 (0.6341)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 43 is less than current step: 499. Dropping entry: {'train/loss': 1.5688589692115784, 'train/ce_loss': 0.35205078125, 'train/seg_cls_loss': 0.011187744140625, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.1192372233606875, 'train/mask_dice_loss': 0.47202819734811785, 'train/mask_loss': 0.5912654250860214, 'metrics/total_secs_per_batch': 6.838259220123291, 'metrics/data_secs_per_batch': 2.9809227705001833, '_timestamp': 1740968545.7221098}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 43 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968545.7224422}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 44 is less than current step: 499. Dropping entry: {'train/loss': 1.8435474991798402, 'train/ce_loss': 0.36181640625, 'train/seg_cls_loss': 0.016796875, 'train/kl_loss': 0.323046875, 'train/mask_bce_loss': 0.08641315326094627, 'train/mask_dice_loss': 0.6341398924589157, 'train/mask_loss': 0.7205530405044556, 'metrics/total_secs_per_batch': 7.878336191177368, 'metrics/data_secs_per_batch': 4.085501980781555, '_timestamp': 1740968553.6002636}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 44 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968553.600541}).
Epoch: [5][ 46/500]	Time  6.983 ( 6.983)	Loss 1.5831 (1.5820)	CeLoss 0.2256 (0.5456)	SegCLSLoss 0.0117 (0.0098)	KLLoss 0.3809 (0.2197)	MaskLoss 0.6568 (0.5048)	MaskBCELoss 0.3997 (0.1267)	MaskDICELoss 0.2570 (0.3781)
Epoch: [5][ 47/500]	Time  8.639 ( 8.639)	Loss 1.9371 (1.8006)	CeLoss 0.2236 (0.3259)	SegCLSLoss 0.0153 (0.0176)	KLLoss 0.3613 (0.3254)	MaskLoss 0.8348 (0.7166)	MaskBCELoss 0.1283 (0.0879)	MaskDICELoss 0.7064 (0.6287)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 45 is less than current step: 499. Dropping entry: {'train/loss': 1.5819836616516114, 'train/ce_loss': 0.54560546875, 'train/seg_cls_loss': 0.009844970703125, 'train/kl_loss': 0.2197265625, 'train/mask_bce_loss': 0.12674652561545371, 'train/mask_dice_loss': 0.37806365787982943, 'train/mask_loss': 0.5048101812601089, 'metrics/total_secs_per_batch': 6.983084201812744, 'metrics/data_secs_per_batch': 3.5456522703170776, '_timestamp': 1740968560.5833178}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 45 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968560.5835876}).
Epoch: [5][ 48/500]	Time  7.777 ( 7.777)	Loss 0.7812 (1.3817)	CeLoss 0.7812 (0.3329)	SegCLSLoss 0.0000 (0.0115)	KLLoss 0.0000 (0.2197)	MaskLoss 0.0000 (0.5106)	MaskBCELoss 0.0000 (0.0771)	MaskDICELoss 0.0000 (0.4334)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 46 is less than current step: 499. Dropping entry: {'train/loss': 1.8005537271499634, 'train/ce_loss': 0.32587890625, 'train/seg_cls_loss': 0.017626953125, 'train/kl_loss': 0.325390625, 'train/mask_bce_loss': 0.08786056954413653, 'train/mask_dice_loss': 0.6287248820066452, 'train/mask_loss': 0.7165854454040528, 'metrics/total_secs_per_batch': 8.63865852355957, 'metrics/data_secs_per_batch': 3.7381752252578737, '_timestamp': 1740968569.2219434}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 46 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968569.2222047}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 47 is less than current step: 499. Dropping entry: {'train/loss': 1.381746768951416, 'train/ce_loss': 0.33291015625, 'train/seg_cls_loss': 0.01151123046875, 'train/kl_loss': 0.2197265625, 'train/mask_bce_loss': 0.07711039539426565, 'train/mask_dice_loss': 0.4334407150745392, 'train/mask_loss': 0.510551106929779, 'metrics/total_secs_per_batch': 7.776717185974121, 'metrics/data_secs_per_batch': 3.844930124282837, '_timestamp': 1740968576.998769}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 47 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968576.9990308}).
Epoch: [5][ 49/500]	Time  7.706 ( 7.706)	Loss 1.6926 (1.4930)	CeLoss 0.2578 (0.5543)	SegCLSLoss 0.0162 (0.0089)	KLLoss 0.3652 (0.2541)	MaskLoss 0.6949 (0.4542)	MaskBCELoss 0.0164 (0.0602)	MaskDICELoss 0.6785 (0.3941)
[2025-03-02 20:23:14,534] [INFO] [logging.py:96:log_dist] [Rank 0] step=2550, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:23:14,541] [INFO] [timer.py:215:stop] epoch=0/micro_step=25500/global_step=2550, RunningAvgSamplesPerSec=1.4362984763390994, CurrSamplesPerSec=1.0167291454853347, MemAllocated=31.27GB, MaxMemAllocated=37.23GB
Epoch: [5][ 50/500]	Time  9.837 ( 9.837)	Loss 1.7803 (1.7171)	CeLoss 0.3086 (0.2378)	SegCLSLoss 0.0156 (0.0157)	KLLoss 0.3633 (0.3646)	MaskLoss 0.7134 (0.7173)	MaskBCELoss 0.0311 (0.1211)	MaskDICELoss 0.6823 (0.5962)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 48 is less than current step: 499. Dropping entry: {'train/loss': 1.4929595470428467, 'train/ce_loss': 0.554296875, 'train/seg_cls_loss': 0.00888671875, 'train/kl_loss': 0.2541015625, 'train/mask_bce_loss': 0.060156593937426804, 'train/mask_dice_loss': 0.39408683925867083, 'train/mask_loss': 0.4542434334754944, 'metrics/total_secs_per_batch': 7.706036806106567, 'metrics/data_secs_per_batch': 2.664504146575928, '_timestamp': 1740968584.7046993}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 48 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968584.7048833}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 49 is less than current step: 499. Dropping entry: {'train/loss': 1.7171149611473084, 'train/ce_loss': 0.23779296875, 'train/seg_cls_loss': 0.015667724609375, 'train/kl_loss': 0.3646484375, 'train/mask_bce_loss': 0.12113335840404034, 'train/mask_dice_loss': 0.5961643636226654, 'train/mask_loss': 0.7172977268695832, 'metrics/total_secs_per_batch': 9.836872100830078, 'metrics/data_secs_per_batch': 4.31983711719513, '_timestamp': 1740968594.5415308}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 49 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968594.5418727}).
Epoch: [5][ 51/500]	Time  7.660 ( 7.660)	Loss 3.3227 (1.7886)	CeLoss 0.2197 (0.4348)	SegCLSLoss 0.0164 (0.0122)	KLLoss 0.3867 (0.2549)	MaskLoss 1.5285 (0.6611)	MaskBCELoss 0.7537 (0.1441)	MaskDICELoss 0.7748 (0.5170)
Epoch: [5][ 52/500]	Time  8.690 ( 8.690)	Loss 1.4361 (1.4467)	CeLoss 0.1436 (0.4099)	SegCLSLoss 0.0312 (0.0157)	KLLoss 0.3711 (0.2895)	MaskLoss 0.6199 (0.5000)	MaskBCELoss 0.1148 (0.0636)	MaskDICELoss 0.5052 (0.4364)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 50 is less than current step: 499. Dropping entry: {'train/loss': 1.7886080026626587, 'train/ce_loss': 0.434765625, 'train/seg_cls_loss': 0.0121826171875, 'train/kl_loss': 0.2548828125, 'train/mask_bce_loss': 0.14411038882099092, 'train/mask_dice_loss': 0.5169904887676239, 'train/mask_loss': 0.6611008763313293, 'metrics/total_secs_per_batch': 7.659964561462402, 'metrics/data_secs_per_batch': 3.300744581222534, '_timestamp': 1740968602.201698}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 50 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968602.2020664}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 51 is less than current step: 499. Dropping entry: {'train/loss': 1.4467112064361571, 'train/ce_loss': 0.40986328125, 'train/seg_cls_loss': 0.01571044921875, 'train/kl_loss': 0.289453125, 'train/mask_bce_loss': 0.06357075925916433, 'train/mask_dice_loss': 0.43639616668224335, 'train/mask_loss': 0.4999669253826141, 'metrics/total_secs_per_batch': 8.689767360687256, 'metrics/data_secs_per_batch': 3.2825621604919433, '_timestamp': 1740968610.8915203}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 51 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968610.8918447}).
Epoch: [5][ 53/500]	Time  8.120 ( 8.120)	Loss 1.1589 (1.5297)	CeLoss 0.2236 (0.4380)	SegCLSLoss 0.0131 (0.0112)	KLLoss 0.3613 (0.2191)	MaskLoss 0.4467 (0.5322)	MaskBCELoss 0.1059 (0.1531)	MaskDICELoss 0.3408 (0.3791)
Epoch: [5][ 54/500]	Time  6.366 ( 6.366)	Loss 1.4647 (1.6671)	CeLoss 0.1797 (0.4372)	SegCLSLoss 0.0303 (0.0132)	KLLoss 0.3594 (0.2922)	MaskLoss 0.6166 (0.5971)	MaskBCELoss 0.0564 (0.1699)	MaskDICELoss 0.5602 (0.4272)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 52 is less than current step: 499. Dropping entry: {'train/loss': 1.5297396898269653, 'train/ce_loss': 0.43798828125, 'train/seg_cls_loss': 0.011151123046875, 'train/kl_loss': 0.219140625, 'train/mask_bce_loss': 0.15305429957807065, 'train/mask_dice_loss': 0.37914952337741853, 'train/mask_loss': 0.5322038322687149, 'metrics/total_secs_per_batch': 8.119653701782227, 'metrics/data_secs_per_batch': 3.239928150177002, '_timestamp': 1740968619.0113544}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 52 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968619.0117424}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 53 is less than current step: 499. Dropping entry: {'train/loss': 1.667145025730133, 'train/ce_loss': 0.43720703125, 'train/seg_cls_loss': 0.013177490234375, 'train/kl_loss': 0.2921875, 'train/mask_bce_loss': 0.16989108603447675, 'train/mask_dice_loss': 0.4272068113088608, 'train/mask_loss': 0.5970978923141956, 'metrics/total_secs_per_batch': 6.366396903991699, 'metrics/data_secs_per_batch': 2.5460432291030886, '_timestamp': 1740968625.3776083}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 53 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968625.3779736}).
Epoch: [5][ 55/500]	Time  9.336 ( 9.336)	Loss 1.4164 (2.1291)	CeLoss 0.1924 (0.1946)	SegCLSLoss 0.0166 (0.0203)	KLLoss 0.3633 (0.3697)	MaskLoss 0.5896 (0.9438)	MaskBCELoss 0.0353 (0.1853)	MaskDICELoss 0.5543 (0.7585)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 54 is less than current step: 499. Dropping entry: {'train/loss': 2.1291462540626527, 'train/ce_loss': 0.194580078125, 'train/seg_cls_loss': 0.020257568359375, 'train/kl_loss': 0.3697265625, 'train/mask_bce_loss': 0.18531128065660596, 'train/mask_dice_loss': 0.7584610641002655, 'train/mask_loss': 0.943772342801094, 'metrics/total_secs_per_batch': 9.33604907989502, 'metrics/data_secs_per_batch': 4.309918546676636, '_timestamp': 1740968634.7135866}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 54 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968634.713941}).
Epoch: [5][ 56/500]	Time  9.783 ( 9.783)	Loss 1.9271 (1.5791)	CeLoss 0.1348 (0.3288)	SegCLSLoss 0.0276 (0.0163)	KLLoss 0.3633 (0.2900)	MaskLoss 0.8713 (0.6066)	MaskBCELoss 0.1255 (0.1006)	MaskDICELoss 0.7458 (0.5060)
Epoch: [5][ 57/500]	Time  8.663 ( 8.663)	Loss 1.1250 (1.6666)	CeLoss 1.1250 (0.5217)	SegCLSLoss 0.0000 (0.0109)	KLLoss 0.0000 (0.2551)	MaskLoss 0.0000 (0.5570)	MaskBCELoss 0.0000 (0.1111)	MaskDICELoss 0.0000 (0.4459)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 55 is less than current step: 499. Dropping entry: {'train/loss': 1.5791081190109253, 'train/ce_loss': 0.32880859375, 'train/seg_cls_loss': 0.016339111328125, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.10060358876362444, 'train/mask_dice_loss': 0.5059914752840996, 'train/mask_loss': 0.6065950661897659, 'metrics/total_secs_per_batch': 9.783456087112427, 'metrics/data_secs_per_batch': 4.4026700258255005, '_timestamp': 1740968644.497017}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 55 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968644.4973412}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 56 is less than current step: 499. Dropping entry: {'train/loss': 1.6666253924369812, 'train/ce_loss': 0.5216796875, 'train/seg_cls_loss': 0.010906982421875, 'train/kl_loss': 0.255078125, 'train/mask_bce_loss': 0.11111172921955585, 'train/mask_dice_loss': 0.4458825945854187, 'train/mask_loss': 0.5569943249225616, 'metrics/total_secs_per_batch': 8.663228750228882, 'metrics/data_secs_per_batch': 3.5609742403030396, '_timestamp': 1740968653.1603544}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 56 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968653.1607027}).
Epoch: [5][ 58/500]	Time  7.653 ( 7.653)	Loss 1.2691 (1.5026)	CeLoss 0.3984 (0.2709)	SegCLSLoss 0.0089 (0.0148)	KLLoss 0.3594 (0.3273)	MaskLoss 0.4158 (0.5958)	MaskBCELoss 0.2796 (0.1355)	MaskDICELoss 0.1362 (0.4604)
Epoch: [5][ 59/500]	Time  8.459 ( 8.459)	Loss 0.4560 (1.6100)	CeLoss 0.2539 (0.2189)	SegCLSLoss 0.0184 (0.0163)	KLLoss 0.3633 (0.3262)	MaskLoss 0.0776 (0.6752)	MaskBCELoss 0.0234 (0.0771)	MaskDICELoss 0.0542 (0.5981)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 57 is less than current step: 499. Dropping entry: {'train/loss': 1.5026263475418091, 'train/ce_loss': 0.2708984375, 'train/seg_cls_loss': 0.014849853515625, 'train/kl_loss': 0.32734375, 'train/mask_bce_loss': 0.13546773921698332, 'train/mask_dice_loss': 0.4603766843676567, 'train/mask_loss': 0.595844429731369, 'metrics/total_secs_per_batch': 7.652651309967041, 'metrics/data_secs_per_batch': 3.3800849676132203, '_timestamp': 1740968660.8128378}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 57 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968660.8131204}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 58 is less than current step: 499. Dropping entry: {'train/loss': 1.6100008696317674, 'train/ce_loss': 0.218896484375, 'train/seg_cls_loss': 0.01629638671875, 'train/kl_loss': 0.326171875, 'train/mask_bce_loss': 0.07711860444396734, 'train/mask_dice_loss': 0.5980722457170486, 'train/mask_loss': 0.6751908466219902, 'metrics/total_secs_per_batch': 8.459063053131104, 'metrics/data_secs_per_batch': 4.187683129310608, '_timestamp': 1740968669.271963}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 58 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968669.2722507}).
[2025-03-02 20:24:39,211] [INFO] [logging.py:96:log_dist] [Rank 0] step=2560, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:24:39,218] [INFO] [timer.py:215:stop] epoch=0/micro_step=25600/global_step=2560, RunningAvgSamplesPerSec=1.4350865920559097, CurrSamplesPerSec=1.0055256269144117, MemAllocated=31.36GB, MaxMemAllocated=37.23GB
Epoch: [5][ 60/500]	Time  9.947 ( 9.947)	Loss 1.4776 (1.4960)	CeLoss 0.1963 (0.3154)	SegCLSLoss 0.0167 (0.0131)	KLLoss 0.3672 (0.3279)	MaskLoss 0.6177 (0.5705)	MaskBCELoss 0.0433 (0.1247)	MaskDICELoss 0.5744 (0.4459)
Epoch: [5][ 61/500]	Time  8.348 ( 8.348)	Loss 1.5526 (1.9131)	CeLoss 0.2490 (0.3897)	SegCLSLoss 0.0114 (0.0122)	KLLoss 0.3652 (0.3270)	MaskLoss 0.6308 (0.7422)	MaskBCELoss 0.0907 (0.1837)	MaskDICELoss 0.5401 (0.5585)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 59 is less than current step: 499. Dropping entry: {'train/loss': 1.4959616243839264, 'train/ce_loss': 0.3154296875, 'train/seg_cls_loss': 0.013067626953125, 'train/kl_loss': 0.3279296875, 'train/mask_bce_loss': 0.12465776726603509, 'train/mask_dice_loss': 0.44588164165616034, 'train/mask_loss': 0.5705394119024276, 'metrics/total_secs_per_batch': 9.94678521156311, 'metrics/data_secs_per_batch': 4.314530348777771, '_timestamp': 1740968679.2185817}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 59 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968679.2189095}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 60 is less than current step: 499. Dropping entry: {'train/loss': 1.9131426453590392, 'train/ce_loss': 0.38974609375, 'train/seg_cls_loss': 0.012213134765625, 'train/kl_loss': 0.326953125, 'train/mask_bce_loss': 0.18366963567677885, 'train/mask_dice_loss': 0.5584973871707917, 'train/mask_loss': 0.7421670317649841, 'metrics/total_secs_per_batch': 8.348299264907837, 'metrics/data_secs_per_batch': 3.5596930742263795, '_timestamp': 1740968687.5671942}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 60 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968687.5675457}).
Epoch: [5][ 62/500]	Time  7.181 ( 7.181)	Loss 2.1042 (1.5519)	CeLoss 0.3066 (0.4688)	SegCLSLoss 0.0102 (0.0114)	KLLoss 0.3613 (0.2523)	MaskLoss 0.8783 (0.5260)	MaskBCELoss 0.2660 (0.1279)	MaskDICELoss 0.6123 (0.3981)
Epoch: [5][ 63/500]	Time  8.119 ( 8.119)	Loss 1.2965 (1.5585)	CeLoss 0.2344 (0.2915)	SegCLSLoss 0.0225 (0.0129)	KLLoss 0.3691 (0.2580)	MaskLoss 0.5067 (0.6174)	MaskBCELoss 0.2301 (0.1755)	MaskDICELoss 0.2765 (0.4419)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 61 is less than current step: 499. Dropping entry: {'train/loss': 1.551915955543518, 'train/ce_loss': 0.46884765625, 'train/seg_cls_loss': 0.01136474609375, 'train/kl_loss': 0.25234375, 'train/mask_bce_loss': 0.12787784300744534, 'train/mask_dice_loss': 0.3981289595365524, 'train/mask_loss': 0.5260067939758301, 'metrics/total_secs_per_batch': 7.181370973587036, 'metrics/data_secs_per_batch': 3.0750650644302366, '_timestamp': 1740968694.7484245}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 61 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968694.748721}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 62 is less than current step: 499. Dropping entry: {'train/loss': 1.558515238761902, 'train/ce_loss': 0.29150390625, 'train/seg_cls_loss': 0.012884521484375, 'train/kl_loss': 0.2580078125, 'train/mask_bce_loss': 0.1754520159214735, 'train/mask_dice_loss': 0.44194037318229673, 'train/mask_loss': 0.6173923909664154, 'metrics/total_secs_per_batch': 8.119384527206421, 'metrics/data_secs_per_batch': 3.3124359846115112, '_timestamp': 1740968702.8680642}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 62 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968702.8684385}).
Epoch: [5][ 64/500]	Time  7.823 ( 7.823)	Loss 1.3087 (1.5406)	CeLoss 0.2275 (0.3418)	SegCLSLoss 0.0142 (0.0153)	KLLoss 0.3672 (0.2891)	MaskLoss 0.5186 (0.5812)	MaskBCELoss 0.0652 (0.0909)	MaskDICELoss 0.4534 (0.4903)
Epoch: [5][ 65/500]	Time  7.897 ( 7.897)	Loss 0.9675 (0.9171)	CeLoss 0.1934 (0.3709)	SegCLSLoss 0.0129 (0.0082)	KLLoss 0.3613 (0.2170)	MaskLoss 0.3656 (0.2601)	MaskBCELoss 0.0807 (0.0519)	MaskDICELoss 0.2849 (0.2082)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 63 is less than current step: 499. Dropping entry: {'train/loss': 1.5405896306037903, 'train/ce_loss': 0.341845703125, 'train/seg_cls_loss': 0.0152587890625, 'train/kl_loss': 0.2890625, 'train/mask_bce_loss': 0.09094516877084971, 'train/mask_dice_loss': 0.4902627319097519, 'train/mask_loss': 0.5812079071998596, 'metrics/total_secs_per_batch': 7.823151350021362, 'metrics/data_secs_per_batch': 3.2293945789337157, '_timestamp': 1740968710.6909602}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 63 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968710.6913285}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 64 is less than current step: 499. Dropping entry: {'train/loss': 0.9170740365982055, 'train/ce_loss': 0.3709228515625, 'train/seg_cls_loss': 0.00821533203125, 'train/kl_loss': 0.2169921875, 'train/mask_bce_loss': 0.051869903970509765, 'train/mask_dice_loss': 0.2082174077630043, 'train/mask_loss': 0.2600873090326786, 'metrics/total_secs_per_batch': 7.897268295288086, 'metrics/data_secs_per_batch': 3.7734902143478393, '_timestamp': 1740968718.588176}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 64 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968718.5884883}).
Epoch: [5][ 66/500]	Time  8.979 ( 8.979)	Loss 1.2236 (1.7334)	CeLoss 0.2695 (0.2251)	SegCLSLoss 0.0159 (0.0174)	KLLoss 0.3652 (0.3633)	MaskLoss 0.4546 (0.7316)	MaskBCELoss 0.1924 (0.1405)	MaskDICELoss 0.2622 (0.5911)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 65 is less than current step: 499. Dropping entry: {'train/loss': 1.7334233403205872, 'train/ce_loss': 0.22509765625, 'train/seg_cls_loss': 0.0173828125, 'train/kl_loss': 0.36328125, 'train/mask_bce_loss': 0.1405017666518688, 'train/mask_dice_loss': 0.5911024749279022, 'train/mask_loss': 0.7316042482852936, 'metrics/total_secs_per_batch': 8.97913932800293, 'metrics/data_secs_per_batch': 4.214821624755859, '_timestamp': 1740968727.56744}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 65 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968727.5677607}).
Epoch: [5][ 67/500]	Time  7.130 ( 7.130)	Loss 1.8803 (1.5440)	CeLoss 0.2305 (0.4011)	SegCLSLoss 0.0146 (0.0133)	KLLoss 0.3613 (0.2531)	MaskLoss 0.8035 (0.5554)	MaskBCELoss 0.2668 (0.1102)	MaskDICELoss 0.5367 (0.4453)
Epoch: [5][ 68/500]	Time  8.048 ( 8.048)	Loss 2.2736 (1.7045)	CeLoss 0.2158 (0.4265)	SegCLSLoss 0.0153 (0.0142)	KLLoss 0.3652 (0.2906)	MaskLoss 1.0069 (0.6210)	MaskBCELoss 0.0071 (0.0804)	MaskDICELoss 0.9998 (0.5406)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 66 is less than current step: 499. Dropping entry: {'train/loss': 1.5439509868621826, 'train/ce_loss': 0.40107421875, 'train/seg_cls_loss': 0.013348388671875, 'train/kl_loss': 0.253125, 'train/mask_bce_loss': 0.1101579986512661, 'train/mask_dice_loss': 0.44526475071907046, 'train/mask_loss': 0.5554227441549301, 'metrics/total_secs_per_batch': 7.129842281341553, 'metrics/data_secs_per_batch': 3.0525657653808596, '_timestamp': 1740968734.6972027}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 66 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968734.6974978}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 67 is less than current step: 499. Dropping entry: {'train/loss': 1.704503321647644, 'train/ce_loss': 0.42646484375, 'train/seg_cls_loss': 0.014202880859375, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.080415508011356, 'train/mask_dice_loss': 0.5405861496925354, 'train/mask_loss': 0.621001660823822, 'metrics/total_secs_per_batch': 8.047564029693604, 'metrics/data_secs_per_batch': 3.435710835456848, '_timestamp': 1740968742.7447193}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 67 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968742.744991}).
Epoch: [5][ 69/500]	Time  7.958 ( 7.958)	Loss 1.9642 (1.7578)	CeLoss 0.3242 (0.4510)	SegCLSLoss 0.0126 (0.0156)	KLLoss 0.3652 (0.2883)	MaskLoss 0.7985 (0.6352)	MaskBCELoss 0.1332 (0.0787)	MaskDICELoss 0.6653 (0.5565)
[2025-03-02 20:26:00,106] [INFO] [logging.py:96:log_dist] [Rank 0] step=2570, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:26:00,113] [INFO] [timer.py:215:stop] epoch=0/micro_step=25700/global_step=2570, RunningAvgSamplesPerSec=1.434188946799972, CurrSamplesPerSec=1.0627056546817242, MemAllocated=31.26GB, MaxMemAllocated=37.23GB
Epoch: [5][ 70/500]	Time  9.412 ( 9.412)	Loss 2.1011 (1.4579)	CeLoss 0.2129 (0.2265)	SegCLSLoss 0.0245 (0.0137)	KLLoss 0.3672 (0.2559)	MaskLoss 0.9197 (0.5995)	MaskBCELoss 0.2047 (0.0723)	MaskDICELoss 0.7150 (0.5272)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 68 is less than current step: 499. Dropping entry: {'train/loss': 1.7578336238861083, 'train/ce_loss': 0.4509765625, 'train/seg_cls_loss': 0.015594482421875, 'train/kl_loss': 0.28828125, 'train/mask_bce_loss': 0.0786764589138329, 'train/mask_dice_loss': 0.5564903408288956, 'train/mask_loss': 0.6351668059825897, 'metrics/total_secs_per_batch': 7.957503318786621, 'metrics/data_secs_per_batch': 3.853419804573059, '_timestamp': 1740968750.7022545}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 68 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968750.702581}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 69 is less than current step: 499. Dropping entry: {'train/loss': 1.4579402923583984, 'train/ce_loss': 0.22646484375, 'train/seg_cls_loss': 0.013690185546875, 'train/kl_loss': 0.255859375, 'train/mask_bce_loss': 0.07232396081089973, 'train/mask_dice_loss': 0.5271539926528931, 'train/mask_loss': 0.5994779646396637, 'metrics/total_secs_per_batch': 9.411521434783936, 'metrics/data_secs_per_batch': 3.903841829299927, '_timestamp': 1740968760.1135883}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 69 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968760.1138506}).
Epoch: [5][ 71/500]	Time  8.446 ( 8.446)	Loss 2.3265 (2.0169)	CeLoss 0.2539 (0.3059)	SegCLSLoss 0.0227 (0.0159)	KLLoss 0.3652 (0.3262)	MaskLoss 1.0119 (0.8351)	MaskBCELoss 0.2608 (0.1613)	MaskDICELoss 0.7510 (0.6739)
Epoch: [5][ 72/500]	Time  5.675 ( 5.675)	Loss 2.5942 (1.3179)	CeLoss 0.1514 (0.5855)	SegCLSLoss 0.0251 (0.0102)	KLLoss 0.3672 (0.2191)	MaskLoss 1.1965 (0.3527)	MaskBCELoss 0.2564 (0.0801)	MaskDICELoss 0.9401 (0.2726)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 70 is less than current step: 499. Dropping entry: {'train/loss': 2.016873908042908, 'train/ce_loss': 0.305859375, 'train/seg_cls_loss': 0.015948486328125, 'train/kl_loss': 0.326171875, 'train/mask_bce_loss': 0.16129413284361363, 'train/mask_dice_loss': 0.6738517999649047, 'train/mask_loss': 0.8351459383964539, 'metrics/total_secs_per_batch': 8.446479082107544, 'metrics/data_secs_per_batch': 3.8471479415893555, '_timestamp': 1740968768.5602424}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 70 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968768.5605583}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 71 is less than current step: 499. Dropping entry: {'train/loss': 1.3178698778152467, 'train/ce_loss': 0.585546875, 'train/seg_cls_loss': 0.01015625, 'train/kl_loss': 0.219140625, 'train/mask_bce_loss': 0.08005534326657653, 'train/mask_dice_loss': 0.2726295977830887, 'train/mask_loss': 0.35268493741750717, 'metrics/total_secs_per_batch': 5.6751484870910645, 'metrics/data_secs_per_batch': 2.5212714672088623, '_timestamp': 1740968774.235598}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 71 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968774.2359452}).
Epoch: [5][ 73/500]	Time  8.495 ( 8.495)	Loss 2.2253 (1.9914)	CeLoss 0.1758 (0.5393)	SegCLSLoss 0.0337 (0.0155)	KLLoss 0.3633 (0.2928)	MaskLoss 0.9984 (0.7076)	MaskBCELoss 0.1218 (0.1145)	MaskDICELoss 0.8766 (0.5931)
Epoch: [5][ 74/500]	Time  9.496 ( 9.496)	Loss 2.0126 (1.8662)	CeLoss 0.2119 (0.1979)	SegCLSLoss 0.0250 (0.0189)	KLLoss 0.3555 (0.3250)	MaskLoss 0.8764 (0.8132)	MaskBCELoss 0.1818 (0.1300)	MaskDICELoss 0.6946 (0.6833)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 72 is less than current step: 499. Dropping entry: {'train/loss': 1.9914198756217956, 'train/ce_loss': 0.5392578125, 'train/seg_cls_loss': 0.01549072265625, 'train/kl_loss': 0.2927734375, 'train/mask_bce_loss': 0.11450787473004312, 'train/mask_dice_loss': 0.5931161373853684, 'train/mask_loss': 0.7076240122318268, 'metrics/total_secs_per_batch': 8.494862079620361, 'metrics/data_secs_per_batch': 3.839016389846802, '_timestamp': 1740968782.7302787}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 72 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968782.7306087}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 73 is less than current step: 499. Dropping entry: {'train/loss': 1.866230809688568, 'train/ce_loss': 0.1978515625, 'train/seg_cls_loss': 0.018914794921875, 'train/kl_loss': 0.325, 'train/mask_bce_loss': 0.1299697406589985, 'train/mask_dice_loss': 0.6832726180553437, 'train/mask_loss': 0.8132423520088196, 'metrics/total_secs_per_batch': 9.496358871459961, 'metrics/data_secs_per_batch': 4.3845356702804565, '_timestamp': 1740968792.2266195}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 73 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968792.226898}).
Epoch: [5][ 75/500]	Time  7.549 ( 7.549)	Loss 0.0508 (1.7029)	CeLoss 0.0508 (0.3563)	SegCLSLoss 0.0000 (0.0115)	KLLoss 0.0000 (0.2568)	MaskLoss 0.0000 (0.6577)	MaskBCELoss 0.0000 (0.1749)	MaskDICELoss 0.0000 (0.4828)
Epoch: [5][ 76/500]	Time  7.964 ( 7.964)	Loss 1.4609 (1.5738)	CeLoss 1.4609 (0.5410)	SegCLSLoss 0.0000 (0.0103)	KLLoss 0.0000 (0.2170)	MaskLoss 0.0000 (0.5030)	MaskBCELoss 0.0000 (0.0673)	MaskDICELoss 0.0000 (0.4358)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 74 is less than current step: 499. Dropping entry: {'train/loss': 1.702912974357605, 'train/ce_loss': 0.35625, 'train/seg_cls_loss': 0.011505126953125, 'train/kl_loss': 0.2568359375, 'train/mask_bce_loss': 0.1748539052903652, 'train/mask_dice_loss': 0.4828037589788437, 'train/mask_loss': 0.6576576739549637, 'metrics/total_secs_per_batch': 7.549284934997559, 'metrics/data_secs_per_batch': 3.192282271385193, '_timestamp': 1740968799.7758899}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 74 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968799.776074}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 75 is less than current step: 499. Dropping entry: {'train/loss': 1.5737630367279052, 'train/ce_loss': 0.541015625, 'train/seg_cls_loss': 0.010284423828125, 'train/kl_loss': 0.2169921875, 'train/mask_bce_loss': 0.06727582179009914, 'train/mask_dice_loss': 0.4357678025960922, 'train/mask_loss': 0.5030436217784882, 'metrics/total_secs_per_batch': 7.964403390884399, 'metrics/data_secs_per_batch': 3.4043054819107055, '_timestamp': 1740968807.7403378}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 75 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968807.740606}).
Epoch: [5][ 77/500]	Time  7.625 ( 7.625)	Loss 1.2103 (1.2713)	CeLoss 0.2754 (0.5971)	SegCLSLoss 0.0101 (0.0110)	KLLoss 0.3633 (0.1807)	MaskLoss 0.4459 (0.3252)	MaskBCELoss 0.0657 (0.0594)	MaskDICELoss 0.3803 (0.2658)
Epoch: [5][ 78/500]	Time  7.882 ( 7.882)	Loss 1.9853 (2.0536)	CeLoss 0.2490 (0.2860)	SegCLSLoss 0.0090 (0.0171)	KLLoss 0.3633 (0.3254)	MaskLoss 0.8471 (0.8633)	MaskBCELoss 0.3089 (0.1372)	MaskDICELoss 0.5382 (0.7261)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 76 is less than current step: 499. Dropping entry: {'train/loss': 1.2712626338005066, 'train/ce_loss': 0.597119140625, 'train/seg_cls_loss': 0.010986328125, 'train/kl_loss': 0.1806640625, 'train/mask_bce_loss': 0.05940576940774918, 'train/mask_dice_loss': 0.26575189977884295, 'train/mask_loss': 0.3251576811075211, 'metrics/total_secs_per_batch': 7.625228404998779, 'metrics/data_secs_per_batch': 3.05427405834198, '_timestamp': 1740968815.3655484}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 76 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968815.3658175}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 77 is less than current step: 499. Dropping entry: {'train/loss': 2.05363587141037, 'train/ce_loss': 0.28603515625, 'train/seg_cls_loss': 0.017083740234375, 'train/kl_loss': 0.325390625, 'train/mask_bce_loss': 0.1371948763728142, 'train/mask_dice_loss': 0.7260976582765579, 'train/mask_loss': 0.8632925391197205, 'metrics/total_secs_per_batch': 7.882028102874756, 'metrics/data_secs_per_batch': 3.10116970539093, '_timestamp': 1740968823.247654}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 77 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968823.247955}).
Epoch: [5][ 79/500]	Time  6.146 ( 6.146)	Loss 2.7489 (1.5601)	CeLoss 0.1660 (0.5271)	SegCLSLoss 0.0256 (0.0114)	KLLoss 0.3809 (0.2551)	MaskLoss 1.2660 (0.5010)	MaskBCELoss 0.5582 (0.1018)	MaskDICELoss 0.7079 (0.3992)
[2025-03-02 20:27:15,697] [INFO] [logging.py:96:log_dist] [Rank 0] step=2580, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:27:15,703] [INFO] [timer.py:215:stop] epoch=0/micro_step=25800/global_step=2580, RunningAvgSamplesPerSec=1.4337221659976924, CurrSamplesPerSec=1.5852337373761196, MemAllocated=30.71GB, MaxMemAllocated=37.23GB
Epoch: [5][ 80/500]	Time  6.310 ( 6.310)	Loss 1.7422 (1.5082)	CeLoss 1.7422 (0.6369)	SegCLSLoss 0.0000 (0.0082)	KLLoss 0.0000 (0.1828)	MaskLoss 0.0000 (0.4243)	MaskBCELoss 0.0000 (0.0893)	MaskDICELoss 0.0000 (0.3350)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 78 is less than current step: 499. Dropping entry: {'train/loss': 1.5601131796836853, 'train/ce_loss': 0.52705078125, 'train/seg_cls_loss': 0.011370849609375, 'train/kl_loss': 0.255078125, 'train/mask_bce_loss': 0.10175316613167525, 'train/mask_dice_loss': 0.3992018572986126, 'train/mask_loss': 0.5009550303220749, 'metrics/total_secs_per_batch': 6.146449089050293, 'metrics/data_secs_per_batch': 2.393576669692993, '_timestamp': 1740968829.3942623}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 78 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968829.3946106}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 79 is less than current step: 499. Dropping entry: {'train/loss': 1.508154022693634, 'train/ce_loss': 0.636865234375, 'train/seg_cls_loss': 0.008245849609375, 'train/kl_loss': 0.1828125, 'train/mask_bce_loss': 0.08929968569427729, 'train/mask_dice_loss': 0.3349677562713623, 'train/mask_loss': 0.424267441034317, 'metrics/total_secs_per_batch': 6.31002402305603, 'metrics/data_secs_per_batch': 3.0102893114089966, '_timestamp': 1740968835.703875}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 79 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968835.704149}).
Epoch: [5][ 81/500]	Time  9.421 ( 9.421)	Loss 1.6093 (1.8286)	CeLoss 0.2168 (0.4044)	SegCLSLoss 0.0242 (0.0132)	KLLoss 0.3613 (0.2906)	MaskLoss 0.6718 (0.6943)	MaskBCELoss 0.0693 (0.1058)	MaskDICELoss 0.6025 (0.5884)
Epoch: [5][ 82/500]	Time  8.758 ( 8.758)	Loss 0.4473 (1.7912)	CeLoss 0.4473 (0.2270)	SegCLSLoss 0.0000 (0.0148)	KLLoss 0.0000 (0.2863)	MaskLoss 0.0000 (0.7641)	MaskBCELoss 0.0000 (0.1755)	MaskDICELoss 0.0000 (0.5886)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 80 is less than current step: 499. Dropping entry: {'train/loss': 1.8285923838615417, 'train/ce_loss': 0.40439453125, 'train/seg_cls_loss': 0.013189697265625, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.10584416165947914, 'train/mask_dice_loss': 0.5884324818849563, 'train/mask_loss': 0.6942766398191452, 'metrics/total_secs_per_batch': 9.420741558074951, 'metrics/data_secs_per_batch': 4.4120395421981815, '_timestamp': 1740968845.1247885}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 80 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968845.125066}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 81 is less than current step: 499. Dropping entry: {'train/loss': 1.7912075757980346, 'train/ce_loss': 0.226953125, 'train/seg_cls_loss': 0.014752197265625, 'train/kl_loss': 0.286328125, 'train/mask_bce_loss': 0.17552990652620792, 'train/mask_dice_loss': 0.5885797351598739, 'train/mask_loss': 0.7641096383333206, 'metrics/total_secs_per_batch': 8.757901906967163, 'metrics/data_secs_per_batch': 4.262234449386597, '_timestamp': 1740968853.8829145}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 81 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968853.883266}).
Epoch: [5][ 83/500]	Time  9.013 ( 9.013)	Loss 1.8600 (1.7989)	CeLoss 0.1807 (0.2153)	SegCLSLoss 0.0256 (0.0171)	KLLoss 0.3711 (0.3314)	MaskLoss 0.8148 (0.7708)	MaskBCELoss 0.0698 (0.1512)	MaskDICELoss 0.7450 (0.6196)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 82 is less than current step: 499. Dropping entry: {'train/loss': 1.7988780856132507, 'train/ce_loss': 0.21533203125, 'train/seg_cls_loss': 0.01708984375, 'train/kl_loss': 0.3314453125, 'train/mask_bce_loss': 0.15119950771331786, 'train/mask_dice_loss': 0.6196262389421463, 'train/mask_loss': 0.7708257555961608, 'metrics/total_secs_per_batch': 9.013449668884277, 'metrics/data_secs_per_batch': 3.7394773244857786, '_timestamp': 1740968862.8961618}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 82 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968862.8964643}).
Epoch: [5][ 84/500]	Time  7.614 ( 7.614)	Loss 1.1373 (1.4159)	CeLoss 0.2715 (0.3619)	SegCLSLoss 0.0170 (0.0137)	KLLoss 0.3672 (0.2578)	MaskLoss 0.4104 (0.5107)	MaskBCELoss 0.2047 (0.0900)	MaskDICELoss 0.2058 (0.4207)
Epoch: [5][ 85/500]	Time  8.346 ( 8.346)	Loss 2.3377 (1.8639)	CeLoss 0.1621 (0.3695)	SegCLSLoss 0.0242 (0.0142)	KLLoss 0.3789 (0.2914)	MaskLoss 1.0629 (0.7290)	MaskBCELoss 0.2944 (0.1309)	MaskDICELoss 0.7685 (0.5981)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 83 is less than current step: 499. Dropping entry: {'train/loss': 1.4159153699874878, 'train/ce_loss': 0.361865234375, 'train/seg_cls_loss': 0.013739013671875, 'train/kl_loss': 0.2578125, 'train/mask_bce_loss': 0.090007846057415, 'train/mask_dice_loss': 0.42065979540348053, 'train/mask_loss': 0.5106676429510116, 'metrics/total_secs_per_batch': 7.61369252204895, 'metrics/data_secs_per_batch': 3.272514510154724, '_timestamp': 1740968870.5099332}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 83 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968870.510157}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 84 is less than current step: 499. Dropping entry: {'train/loss': 1.8639475345611571, 'train/ce_loss': 0.36953125, 'train/seg_cls_loss': 0.01422119140625, 'train/kl_loss': 0.29140625, 'train/mask_bce_loss': 0.1309386719018221, 'train/mask_dice_loss': 0.5981054127216339, 'train/mask_loss': 0.7290440738201142, 'metrics/total_secs_per_batch': 8.345570802688599, 'metrics/data_secs_per_batch': 3.427268648147583, '_timestamp': 1740968878.8554082}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 84 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968878.8556716}).
Epoch: [5][ 86/500]	Time  7.932 ( 7.932)	Loss 1.1613 (1.6092)	CeLoss 0.2314 (0.4824)	SegCLSLoss 0.0148 (0.0124)	KLLoss 0.3633 (0.2533)	MaskLoss 0.4429 (0.5476)	MaskBCELoss 0.0880 (0.0692)	MaskDICELoss 0.3549 (0.4785)
Epoch: [5][ 87/500]	Time  7.710 ( 7.710)	Loss 2.2544 (2.0070)	CeLoss 0.1816 (0.4125)	SegCLSLoss 0.0165 (0.0130)	KLLoss 0.3672 (0.2918)	MaskLoss 1.0139 (0.7794)	MaskBCELoss 0.0143 (0.1471)	MaskDICELoss 0.9996 (0.6323)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 85 is less than current step: 499. Dropping entry: {'train/loss': 1.6091851115226745, 'train/ce_loss': 0.482421875, 'train/seg_cls_loss': 0.012359619140625, 'train/kl_loss': 0.2533203125, 'train/mask_bce_loss': 0.06915289750322699, 'train/mask_dice_loss': 0.4784572184085846, 'train/mask_loss': 0.5476101249456405, 'metrics/total_secs_per_batch': 7.93195652961731, 'metrics/data_secs_per_batch': 3.6560837984085084, '_timestamp': 1740968886.7874033}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 85 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968886.787675}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 86 is less than current step: 499. Dropping entry: {'train/loss': 2.0069672226905824, 'train/ce_loss': 0.4125, 'train/seg_cls_loss': 0.012957763671875, 'train/kl_loss': 0.291796875, 'train/mask_bce_loss': 0.1470888577401638, 'train/mask_dice_loss': 0.6323224633932114, 'train/mask_loss': 0.779411318898201, 'metrics/total_secs_per_batch': 7.70982027053833, 'metrics/data_secs_per_batch': 3.017451047897339, '_timestamp': 1740968894.4973974}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 86 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968894.4977438}).
Epoch: [5][ 88/500]	Time  7.883 ( 7.883)	Loss 1.6754 (1.6011)	CeLoss 0.2930 (0.3776)	SegCLSLoss 0.0111 (0.0122)	KLLoss 0.3613 (0.2562)	MaskLoss 0.6697 (0.5958)	MaskBCELoss 0.1023 (0.1017)	MaskDICELoss 0.5675 (0.4941)
Epoch: [5][ 89/500]	Time  8.400 ( 8.400)	Loss 1.0156 (1.2448)	CeLoss 1.0156 (0.2867)	SegCLSLoss 0.0000 (0.0136)	KLLoss 0.0000 (0.2938)	MaskLoss 0.0000 (0.4608)	MaskBCELoss 0.0000 (0.0871)	MaskDICELoss 0.0000 (0.3738)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 87 is less than current step: 499. Dropping entry: {'train/loss': 1.6011257767677307, 'train/ce_loss': 0.37763671875, 'train/seg_cls_loss': 0.01220703125, 'train/kl_loss': 0.25625, 'train/mask_bce_loss': 0.10170724680647254, 'train/mask_dice_loss': 0.4941193014383316, 'train/mask_loss': 0.5958265542984009, 'metrics/total_secs_per_batch': 7.883248805999756, 'metrics/data_secs_per_batch': 3.17936635017395, '_timestamp': 1740968902.3805122}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 87 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968902.3807917}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 88 is less than current step: 499. Dropping entry: {'train/loss': 1.2447915494441986, 'train/ce_loss': 0.286669921875, 'train/seg_cls_loss': 0.013641357421875, 'train/kl_loss': 0.29375, 'train/mask_bce_loss': 0.08707691580057145, 'train/mask_dice_loss': 0.37377100586891177, 'train/mask_loss': 0.46084791123867036, 'metrics/total_secs_per_batch': 8.400497674942017, 'metrics/data_secs_per_batch': 3.456120491027832, '_timestamp': 1740968910.7809765}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 88 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968910.7812803}).
[2025-03-02 20:28:38,881] [INFO] [logging.py:96:log_dist] [Rank 0] step=2590, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:28:38,887] [INFO] [timer.py:215:stop] epoch=0/micro_step=25900/global_step=2590, RunningAvgSamplesPerSec=1.4326568281063792, CurrSamplesPerSec=1.2337874169511516, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [5][ 90/500]	Time  8.107 ( 8.107)	Loss 2.2313 (1.7118)	CeLoss 0.2930 (0.3487)	SegCLSLoss 0.0167 (0.0131)	KLLoss 0.3691 (0.2922)	MaskLoss 0.9467 (0.6636)	MaskBCELoss 0.0143 (0.1490)	MaskDICELoss 0.9324 (0.5147)
Epoch: [5][ 91/500]	Time  6.364 ( 6.364)	Loss 0.9648 (1.7352)	CeLoss 0.9648 (0.6294)	SegCLSLoss 0.0000 (0.0092)	KLLoss 0.0000 (0.2184)	MaskLoss 0.0000 (0.5397)	MaskBCELoss 0.0000 (0.0861)	MaskDICELoss 0.0000 (0.4536)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 89 is less than current step: 499. Dropping entry: {'train/loss': 1.7117589712142944, 'train/ce_loss': 0.34873046875, 'train/seg_cls_loss': 0.0131103515625, 'train/kl_loss': 0.2921875, 'train/mask_bce_loss': 0.14897900763899088, 'train/mask_dice_loss': 0.5146641373634339, 'train/mask_loss': 0.6636431455612183, 'metrics/total_secs_per_batch': 8.106708526611328, 'metrics/data_secs_per_batch': 3.3234605312347414, '_timestamp': 1740968918.8875961}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 89 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968918.8879468}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 90 is less than current step: 499. Dropping entry: {'train/loss': 1.7352018117904664, 'train/ce_loss': 0.62939453125, 'train/seg_cls_loss': 0.009173583984375, 'train/kl_loss': 0.218359375, 'train/mask_bce_loss': 0.08610329232178629, 'train/mask_dice_loss': 0.4535679191350937, 'train/mask_loss': 0.5396712064743042, 'metrics/total_secs_per_batch': 6.363534688949585, 'metrics/data_secs_per_batch': 3.150216507911682, '_timestamp': 1740968925.2512565}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 90 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968925.2515352}).
Epoch: [5][ 92/500]	Time  8.082 ( 8.082)	Loss 2.7079 (1.7251)	CeLoss 0.1270 (0.4253)	SegCLSLoss 0.0276 (0.0169)	KLLoss 0.3867 (0.2951)	MaskLoss 1.2641 (0.6310)	MaskBCELoss 0.5091 (0.1467)	MaskDICELoss 0.7550 (0.4843)
Epoch: [5][ 93/500]	Time  9.313 ( 9.313)	Loss 1.0397 (1.4522)	CeLoss 0.2363 (0.3134)	SegCLSLoss 0.0103 (0.0119)	KLLoss 0.3633 (0.2910)	MaskLoss 0.3812 (0.5518)	MaskBCELoss 0.0431 (0.0453)	MaskDICELoss 0.3381 (0.5065)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 91 is less than current step: 499. Dropping entry: {'train/loss': 1.725068175792694, 'train/ce_loss': 0.42529296875, 'train/seg_cls_loss': 0.01688232421875, 'train/kl_loss': 0.2951171875, 'train/mask_bce_loss': 0.1466980345547199, 'train/mask_dice_loss': 0.4842930942773819, 'train/mask_loss': 0.630991131067276, 'metrics/total_secs_per_batch': 8.082396507263184, 'metrics/data_secs_per_batch': 3.5269272565841674, '_timestamp': 1740968933.3345098}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 91 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968933.335059}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 92 is less than current step: 499. Dropping entry: {'train/loss': 1.4522231459617614, 'train/ce_loss': 0.31337890625, 'train/seg_cls_loss': 0.0118896484375, 'train/kl_loss': 0.291015625, 'train/mask_bce_loss': 0.04530833316966891, 'train/mask_dice_loss': 0.5065356463193893, 'train/mask_loss': 0.5518439739942551, 'metrics/total_secs_per_batch': 9.313201427459717, 'metrics/data_secs_per_batch': 3.8428755044937133, '_timestamp': 1740968942.647307}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 92 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968942.6477747}).
Epoch: [5][ 94/500]	Time  7.244 ( 7.244)	Loss 0.0747 (1.1866)	CeLoss 0.0747 (0.4900)	SegCLSLoss 0.0000 (0.0081)	KLLoss 0.0000 (0.1836)	MaskLoss 0.0000 (0.3372)	MaskBCELoss 0.0000 (0.0580)	MaskDICELoss 0.0000 (0.2792)
Epoch: [5][ 95/500]	Time  7.163 ( 7.163)	Loss 1.7706 (1.6208)	CeLoss 0.1787 (0.2959)	SegCLSLoss 0.0229 (0.0140)	KLLoss 0.3613 (0.2926)	MaskLoss 0.7720 (0.6443)	MaskBCELoss 0.0108 (0.0510)	MaskDICELoss 0.7612 (0.5934)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 93 is less than current step: 499. Dropping entry: {'train/loss': 1.1866437911987304, 'train/ce_loss': 0.4900146484375, 'train/seg_cls_loss': 0.008087158203125, 'train/kl_loss': 0.18359375, 'train/mask_bce_loss': 0.05802047438919544, 'train/mask_dice_loss': 0.2791612923145294, 'train/mask_loss': 0.3371817648410797, 'metrics/total_secs_per_batch': 7.243798494338989, 'metrics/data_secs_per_batch': 2.7201185703277586, '_timestamp': 1740968949.8907428}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 93 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968949.891101}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 94 is less than current step: 499. Dropping entry: {'train/loss': 1.6207669496536254, 'train/ce_loss': 0.2958984375, 'train/seg_cls_loss': 0.0140380859375, 'train/kl_loss': 0.292578125, 'train/mask_bce_loss': 0.05095899878069758, 'train/mask_dice_loss': 0.5933600336313247, 'train/mask_loss': 0.6443190306425095, 'metrics/total_secs_per_batch': 7.162503004074097, 'metrics/data_secs_per_batch': 2.896898365020752, '_timestamp': 1740968957.0532818}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 94 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968957.0536578}).
Epoch: [5][ 96/500]	Time  8.454 ( 8.454)	Loss 2.5352 (1.7628)	CeLoss 0.2373 (0.3778)	SegCLSLoss 0.0121 (0.0111)	KLLoss 0.3711 (0.2551)	MaskLoss 1.1270 (0.6768)	MaskBCELoss 0.3429 (0.1428)	MaskDICELoss 0.7841 (0.5340)
Epoch: [5][ 97/500]	Time  6.408 ( 6.408)	Loss 2.2082 (1.7990)	CeLoss 0.2490 (0.5843)	SegCLSLoss 0.0179 (0.0102)	KLLoss 0.3652 (0.2207)	MaskLoss 0.9566 (0.5939)	MaskBCELoss 0.3092 (0.1541)	MaskDICELoss 0.6474 (0.4398)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 95 is less than current step: 499. Dropping entry: {'train/loss': 1.76277996301651, 'train/ce_loss': 0.37783203125, 'train/seg_cls_loss': 0.011083984375, 'train/kl_loss': 0.255078125, 'train/mask_bce_loss': 0.14281002879142762, 'train/mask_dice_loss': 0.5340389370918274, 'train/mask_loss': 0.6768489718437195, 'metrics/total_secs_per_batch': 8.453854084014893, 'metrics/data_secs_per_batch': 3.9845540523529053, '_timestamp': 1740968965.5071342}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 95 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968965.5075023}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 96 is less than current step: 499. Dropping entry: {'train/loss': 1.7990406990051269, 'train/ce_loss': 0.58427734375, 'train/seg_cls_loss': 0.010205078125, 'train/kl_loss': 0.220703125, 'train/mask_bce_loss': 0.15406403475208208, 'train/mask_dice_loss': 0.439792263507843, 'train/mask_loss': 0.5938562989234925, 'metrics/total_secs_per_batch': 6.407520294189453, 'metrics/data_secs_per_batch': 2.537665867805481, '_timestamp': 1740968971.9147415}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 96 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968971.9151156}).
Epoch: [5][ 98/500]	Time  7.429 ( 7.429)	Loss 2.2858 (1.2879)	CeLoss 0.2305 (0.5474)	SegCLSLoss 0.0220 (0.0091)	KLLoss 0.3555 (0.1828)	MaskLoss 1.0042 (0.3588)	MaskBCELoss 0.1785 (0.0746)	MaskDICELoss 0.8258 (0.2842)
Epoch: [5][ 99/500]	Time  7.899 ( 7.899)	Loss 1.1016 (1.7269)	CeLoss 1.1016 (0.4871)	SegCLSLoss 0.0000 (0.0094)	KLLoss 0.0000 (0.2549)	MaskLoss 0.0000 (0.6046)	MaskBCELoss 0.0000 (0.1516)	MaskDICELoss 0.0000 (0.4530)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 97 is less than current step: 499. Dropping entry: {'train/loss': 1.2878827095031737, 'train/ce_loss': 0.54736328125, 'train/seg_cls_loss': 0.00914306640625, 'train/kl_loss': 0.1828125, 'train/mask_bce_loss': 0.07461247239261866, 'train/mask_dice_loss': 0.2841726213693619, 'train/mask_loss': 0.3587850987911224, 'metrics/total_secs_per_batch': 7.429447889328003, 'metrics/data_secs_per_batch': 3.181676459312439, '_timestamp': 1740968979.3441463}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 97 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968979.344521}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 98 is less than current step: 499. Dropping entry: {'train/loss': 1.7268727898597718, 'train/ce_loss': 0.487109375, 'train/seg_cls_loss': 0.009423828125, 'train/kl_loss': 0.2548828125, 'train/mask_bce_loss': 0.15161137804389, 'train/mask_dice_loss': 0.45303596556186676, 'train/mask_loss': 0.6046473383903503, 'metrics/total_secs_per_batch': 7.898813962936401, 'metrics/data_secs_per_batch': 3.522836971282959, '_timestamp': 1740968987.2430253}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 98 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968987.2434266}).
[2025-03-02 20:29:56,189] [INFO] [logging.py:96:log_dist] [Rank 0] step=2600, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:29:56,194] [INFO] [timer.py:215:stop] epoch=0/micro_step=26000/global_step=2600, RunningAvgSamplesPerSec=1.4320652348332585, CurrSamplesPerSec=1.117276196016882, MemAllocated=31.23GB, MaxMemAllocated=37.23GB
Epoch: [5][100/500]	Time  8.952 ( 8.952)	Loss 1.0834 (1.4724)	CeLoss 0.2773 (0.2256)	SegCLSLoss 0.0088 (0.0149)	KLLoss 0.3633 (0.3283)	MaskLoss 0.3825 (0.6032)	MaskBCELoss 0.0836 (0.1136)	MaskDICELoss 0.2989 (0.4896)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 99 is less than current step: 499. Dropping entry: {'train/loss': 1.4723776519298553, 'train/ce_loss': 0.225634765625, 'train/seg_cls_loss': 0.01494140625, 'train/kl_loss': 0.3283203125, 'train/mask_bce_loss': 0.11356381657533347, 'train/mask_dice_loss': 0.4896415963768959, 'train/mask_loss': 0.6032054156064988, 'metrics/total_secs_per_batch': 8.952367782592773, 'metrics/data_secs_per_batch': 3.8918993711471557, '_timestamp': 1740968996.19512}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 99 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740968996.1955054}).
Epoch: [5][101/500]	Time  8.660 ( 8.660)	Loss 2.7992 (2.0942)	CeLoss 0.2500 (0.3668)	SegCLSLoss 0.0149 (0.0133)	KLLoss 0.3672 (0.3271)	MaskLoss 1.2522 (0.8438)	MaskBCELoss 0.6482 (0.2248)	MaskDICELoss 0.6040 (0.6191)
Epoch: [5][102/500]	Time  7.162 ( 7.162)	Loss 1.6525 (1.6437)	CeLoss 0.1729 (0.4871)	SegCLSLoss 0.0228 (0.0123)	KLLoss 0.3613 (0.2549)	MaskLoss 0.7159 (0.5623)	MaskBCELoss 0.0589 (0.0656)	MaskDICELoss 0.6570 (0.4967)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 100 is less than current step: 499. Dropping entry: {'train/loss': 2.094196283817291, 'train/ce_loss': 0.366796875, 'train/seg_cls_loss': 0.013275146484375, 'train/kl_loss': 0.3271484375, 'train/mask_bce_loss': 0.22476917509920896, 'train/mask_dice_loss': 0.6190574765205383, 'train/mask_loss': 0.8438266575336456, 'metrics/total_secs_per_batch': 8.659840106964111, 'metrics/data_secs_per_batch': 4.11185667514801, '_timestamp': 1740969004.8551037}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 100 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969004.855436}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 101 is less than current step: 499. Dropping entry: {'train/loss': 1.6437175273895264, 'train/ce_loss': 0.487109375, 'train/seg_cls_loss': 0.012274169921875, 'train/kl_loss': 0.2548828125, 'train/mask_bce_loss': 0.06564609007909894, 'train/mask_dice_loss': 0.49669118225574493, 'train/mask_loss': 0.5623372673988343, 'metrics/total_secs_per_batch': 7.161804676055908, 'metrics/data_secs_per_batch': 3.3930474519729614, '_timestamp': 1740969012.017662}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 101 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969012.0181756}).
Epoch: [5][103/500]	Time  8.892 ( 8.892)	Loss 2.6420 (1.5351)	CeLoss 0.1855 (0.2797)	SegCLSLoss 0.0247 (0.0126)	KLLoss 0.3594 (0.3260)	MaskLoss 1.2038 (0.6082)	MaskBCELoss 0.3161 (0.1680)	MaskDICELoss 0.8877 (0.4401)
Epoch: [5][104/500]	Time  6.821 ( 6.821)	Loss 0.8789 (1.5884)	CeLoss 0.8789 (0.4926)	SegCLSLoss 0.0000 (0.0100)	KLLoss 0.0000 (0.2195)	MaskLoss 0.0000 (0.5343)	MaskBCELoss 0.0000 (0.0959)	MaskDICELoss 0.0000 (0.4384)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 102 is less than current step: 499. Dropping entry: {'train/loss': 1.5350683093070985, 'train/ce_loss': 0.2796875, 'train/seg_cls_loss': 0.01263427734375, 'train/kl_loss': 0.3259765625, 'train/mask_bce_loss': 0.16801125332713127, 'train/mask_dice_loss': 0.4401478819549084, 'train/mask_loss': 0.6081591352820397, 'metrics/total_secs_per_batch': 8.89225172996521, 'metrics/data_secs_per_batch': 4.114417457580567, '_timestamp': 1740969020.9096684}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 102 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969020.910204}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 103 is less than current step: 499. Dropping entry: {'train/loss': 1.5884245395660401, 'train/ce_loss': 0.492578125, 'train/seg_cls_loss': 0.01002197265625, 'train/kl_loss': 0.21953125, 'train/mask_bce_loss': 0.09593843463808298, 'train/mask_dice_loss': 0.4384105294942856, 'train/mask_loss': 0.5343489706516266, 'metrics/total_secs_per_batch': 6.8205273151397705, 'metrics/data_secs_per_batch': 3.3052438259124757, '_timestamp': 1740969027.730063}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 103 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969027.7305312}).
Epoch: [5][105/500]	Time  9.923 ( 9.923)	Loss 1.8279 (1.8723)	CeLoss 0.2441 (0.2447)	SegCLSLoss 0.0124 (0.0155)	KLLoss 0.3633 (0.3627)	MaskLoss 0.7704 (0.7918)	MaskBCELoss 0.0991 (0.1864)	MaskDICELoss 0.6713 (0.6054)
Epoch: [5][106/500]	Time  9.642 ( 9.642)	Loss 1.0625 (1.5086)	CeLoss 1.0625 (0.3820)	SegCLSLoss 0.0000 (0.0099)	KLLoss 0.0000 (0.2168)	MaskLoss 0.0000 (0.5499)	MaskBCELoss 0.0000 (0.0634)	MaskDICELoss 0.0000 (0.4865)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 104 is less than current step: 499. Dropping entry: {'train/loss': 1.872330105304718, 'train/ce_loss': 0.2447265625, 'train/seg_cls_loss': 0.015478515625, 'train/kl_loss': 0.3626953125, 'train/mask_bce_loss': 0.1864312194287777, 'train/mask_dice_loss': 0.6053979098796844, 'train/mask_loss': 0.7918291300535202, 'metrics/total_secs_per_batch': 9.923242330551147, 'metrics/data_secs_per_batch': 4.592704343795776, '_timestamp': 1740969037.653021}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 104 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969037.6532824}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 105 is less than current step: 499. Dropping entry: {'train/loss': 1.5085955142974854, 'train/ce_loss': 0.3820068359375, 'train/seg_cls_loss': 0.009893798828125, 'train/kl_loss': 0.216796875, 'train/mask_bce_loss': 0.06338342605158687, 'train/mask_dice_loss': 0.48653199672698977, 'train/mask_loss': 0.5499154329299927, 'metrics/total_secs_per_batch': 9.642237901687622, 'metrics/data_secs_per_batch': 5.197588205337524, '_timestamp': 1740969047.2954288}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 105 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969047.2958217}).
Epoch: [5][107/500]	Time  8.401 ( 8.401)	Loss 1.4141 (1.9084)	CeLoss 1.4141 (0.3967)	SegCLSLoss 0.0000 (0.0133)	KLLoss 0.0000 (0.2928)	MaskLoss 0.0000 (0.7379)	MaskBCELoss 0.0000 (0.1508)	MaskDICELoss 0.0000 (0.5871)
Epoch: [5][108/500]	Time  6.674 ( 6.674)	Loss 1.1328 (1.7820)	CeLoss 1.1328 (0.5401)	SegCLSLoss 0.0000 (0.0095)	KLLoss 0.0000 (0.2230)	MaskLoss 0.0000 (0.6073)	MaskBCELoss 0.0000 (0.2275)	MaskDICELoss 0.0000 (0.3798)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 106 is less than current step: 499. Dropping entry: {'train/loss': 1.9083873867988586, 'train/ce_loss': 0.3966796875, 'train/seg_cls_loss': 0.013299560546875, 'train/kl_loss': 0.2927734375, 'train/mask_bce_loss': 0.15075525920838118, 'train/mask_dice_loss': 0.5871298253536225, 'train/mask_loss': 0.7378850936889648, 'metrics/total_secs_per_batch': 8.401003122329712, 'metrics/data_secs_per_batch': 3.9491260766983034, '_timestamp': 1740969055.6965463}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 106 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969055.6970594}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 107 is less than current step: 499. Dropping entry: {'train/loss': 1.7819776654243469, 'train/ce_loss': 0.54013671875, 'train/seg_cls_loss': 0.00947265625, 'train/kl_loss': 0.223046875, 'train/mask_bce_loss': 0.22751733660697937, 'train/mask_dice_loss': 0.3798289269208908, 'train/mask_loss': 0.6073462665081024, 'metrics/total_secs_per_batch': 6.673591136932373, 'metrics/data_secs_per_batch': 3.0581196784973144, '_timestamp': 1740969062.3698099}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 107 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969062.3701453}).
Epoch: [5][109/500]	Time  6.958 ( 6.958)	Loss 1.7411 (1.6191)	CeLoss 0.2354 (0.5784)	SegCLSLoss 0.0164 (0.0093)	KLLoss 0.3691 (0.2553)	MaskLoss 0.7309 (0.5052)	MaskBCELoss 0.0983 (0.1166)	MaskDICELoss 0.6326 (0.3885)
[2025-03-02 20:31:17,826] [INFO] [logging.py:96:log_dist] [Rank 0] step=2610, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:31:17,830] [INFO] [timer.py:215:stop] epoch=0/micro_step=26100/global_step=2610, RunningAvgSamplesPerSec=1.4311387646342242, CurrSamplesPerSec=1.1762826883116908, MemAllocated=31.27GB, MaxMemAllocated=37.23GB
Epoch: [5][110/500]	Time  8.504 ( 8.504)	Loss 1.8246 (1.8984)	CeLoss 0.1826 (0.2194)	SegCLSLoss 0.0273 (0.0184)	KLLoss 0.3691 (0.3652)	MaskLoss 0.7956 (0.8167)	MaskBCELoss 0.0404 (0.1165)	MaskDICELoss 0.7552 (0.7002)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 108 is less than current step: 499. Dropping entry: {'train/loss': 1.6191163778305053, 'train/ce_loss': 0.57841796875, 'train/seg_cls_loss': 0.009344482421875, 'train/kl_loss': 0.2552734375, 'train/mask_bce_loss': 0.1166321326047182, 'train/mask_dice_loss': 0.3885315299034119, 'train/mask_loss': 0.5051636666059494, 'metrics/total_secs_per_batch': 6.9580724239349365, 'metrics/data_secs_per_batch': 3.0695873498916626, '_timestamp': 1740969069.3283837}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 108 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969069.3288963}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 109 is less than current step: 499. Dropping entry: {'train/loss': 1.898397010564804, 'train/ce_loss': 0.219384765625, 'train/seg_cls_loss': 0.018365478515625, 'train/kl_loss': 0.365234375, 'train/mask_bce_loss': 0.11652149353176355, 'train/mask_dice_loss': 0.7001574963331223, 'train/mask_loss': 0.8166789948940277, 'metrics/total_secs_per_batch': 8.50383710861206, 'metrics/data_secs_per_batch': 3.7696942567825316, '_timestamp': 1740969077.831558}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 109 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969077.831937}).
Epoch: [5][111/500]	Time  7.045 ( 7.045)	Loss 2.1055 (1.4094)	CeLoss 0.2217 (0.4612)	SegCLSLoss 0.0111 (0.0095)	KLLoss 0.3652 (0.2912)	MaskLoss 0.9209 (0.4572)	MaskBCELoss 0.1722 (0.0888)	MaskDICELoss 0.7487 (0.3684)
Epoch: [5][112/500]	Time  8.114 ( 8.114)	Loss 1.6804 (1.9696)	CeLoss 0.2520 (0.4786)	SegCLSLoss 0.0187 (0.0168)	KLLoss 0.3555 (0.2930)	MaskLoss 0.6918 (0.7269)	MaskBCELoss 0.0273 (0.1522)	MaskDICELoss 0.6645 (0.5747)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 110 is less than current step: 499. Dropping entry: {'train/loss': 1.4094111382961274, 'train/ce_loss': 0.46123046875, 'train/seg_cls_loss': 0.00947265625, 'train/kl_loss': 0.2912109375, 'train/mask_bce_loss': 0.0888397628441453, 'train/mask_dice_loss': 0.36840487122535703, 'train/mask_loss': 0.4572446346282959, 'metrics/total_secs_per_batch': 7.045185089111328, 'metrics/data_secs_per_batch': 2.8831104278564452, '_timestamp': 1740969084.8770065}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 110 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969084.877393}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 111 is less than current step: 499. Dropping entry: {'train/loss': 1.9696490287780761, 'train/ce_loss': 0.478564453125, 'train/seg_cls_loss': 0.01676025390625, 'train/kl_loss': 0.29296875, 'train/mask_bce_loss': 0.15221437979489566, 'train/mask_dice_loss': 0.5746511459350586, 'train/mask_loss': 0.7268655300140381, 'metrics/total_secs_per_batch': 8.113760948181152, 'metrics/data_secs_per_batch': 4.051590800285339, '_timestamp': 1740969092.990951}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 111 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969092.9914434}).
Epoch: [5][113/500]	Time  7.591 ( 7.591)	Loss 0.7930 (1.7259)	CeLoss 0.7930 (0.7087)	SegCLSLoss 0.0000 (0.0077)	KLLoss 0.0000 (0.1814)	MaskLoss 0.0000 (0.4975)	MaskBCELoss 0.0000 (0.2214)	MaskDICELoss 0.0000 (0.2761)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 112 is less than current step: 499. Dropping entry: {'train/loss': 1.7259169220924377, 'train/ce_loss': 0.70869140625, 'train/seg_cls_loss': 0.007696533203125, 'train/kl_loss': 0.1814453125, 'train/mask_bce_loss': 0.2214029349386692, 'train/mask_dice_loss': 0.2761258348822594, 'train/mask_loss': 0.49752877205610274, 'metrics/total_secs_per_batch': 7.5909740924835205, 'metrics/data_secs_per_batch': 2.9955846548080443, '_timestamp': 1740969100.5823278}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 112 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969100.582952}).
Epoch: [5][114/500]	Time  9.029 ( 9.029)	Loss 2.0110 (1.7342)	CeLoss 0.2490 (0.2876)	SegCLSLoss 0.0159 (0.0195)	KLLoss 0.3867 (0.3264)	MaskLoss 0.8570 (0.7019)	MaskBCELoss 0.0191 (0.0443)	MaskDICELoss 0.8380 (0.6576)
Epoch: [5][115/500]	Time  9.226 ( 9.226)	Loss 1.5046 (1.4367)	CeLoss 0.2393 (0.2279)	SegCLSLoss 0.0116 (0.0123)	KLLoss 0.3613 (0.2885)	MaskLoss 0.6117 (0.5869)	MaskBCELoss 0.0331 (0.0989)	MaskDICELoss 0.5786 (0.4880)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 113 is less than current step: 499. Dropping entry: {'train/loss': 1.7341703653335572, 'train/ce_loss': 0.28759765625, 'train/seg_cls_loss': 0.01951904296875, 'train/kl_loss': 0.3263671875, 'train/mask_bce_loss': 0.04428164269775152, 'train/mask_dice_loss': 0.6576179742813111, 'train/mask_loss': 0.7018996238708496, 'metrics/total_secs_per_batch': 9.029273271560669, 'metrics/data_secs_per_batch': 3.9623874187469483, '_timestamp': 1740969109.6109915}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 113 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969109.6113477}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 114 is less than current step: 499. Dropping entry: {'train/loss': 1.4367321491241456, 'train/ce_loss': 0.2279296875, 'train/seg_cls_loss': 0.01234130859375, 'train/kl_loss': 0.2884765625, 'train/mask_bce_loss': 0.09887880589812995, 'train/mask_dice_loss': 0.48799312561750413, 'train/mask_loss': 0.5868719339370727, 'metrics/total_secs_per_batch': 9.226296663284302, 'metrics/data_secs_per_batch': 3.8828521966934204, '_timestamp': 1740969118.837346}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 114 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969118.8377323}).
Epoch: [5][116/500]	Time  8.727 ( 8.727)	Loss 2.4238 (2.1473)	CeLoss 0.2236 (0.2197)	SegCLSLoss 0.0176 (0.0195)	KLLoss 0.3613 (0.3623)	MaskLoss 1.0772 (0.9408)	MaskBCELoss 0.3185 (0.2018)	MaskDICELoss 0.7587 (0.7390)
Epoch: [5][117/500]	Time  8.868 ( 8.868)	Loss 0.9883 (1.4030)	CeLoss 0.9883 (0.3855)	SegCLSLoss 0.0000 (0.0139)	KLLoss 0.0000 (0.2898)	MaskLoss 0.0000 (0.4905)	MaskBCELoss 0.0000 (0.1001)	MaskDICELoss 0.0000 (0.3905)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 115 is less than current step: 499. Dropping entry: {'train/loss': 2.1473243951797487, 'train/ce_loss': 0.2197265625, 'train/seg_cls_loss': 0.019488525390625, 'train/kl_loss': 0.3623046875, 'train/mask_bce_loss': 0.20175497429445385, 'train/mask_dice_loss': 0.73899707198143, 'train/mask_loss': 0.9407520413398742, 'metrics/total_secs_per_batch': 8.726538896560669, 'metrics/data_secs_per_batch': 3.5769330739974974, '_timestamp': 1740969127.5636916}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 115 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969127.563983}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 116 is less than current step: 499. Dropping entry: {'train/loss': 1.4029544055461884, 'train/ce_loss': 0.385546875, 'train/seg_cls_loss': 0.013897705078125, 'train/kl_loss': 0.28984375, 'train/mask_bce_loss': 0.1000559899955988, 'train/mask_dice_loss': 0.3904837131500244, 'train/mask_loss': 0.49053970277309417, 'metrics/total_secs_per_batch': 8.867788553237915, 'metrics/data_secs_per_batch': 3.745056128501892, '_timestamp': 1740969136.432586}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 116 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969136.4333808}).
Epoch: [5][118/500]	Time  7.049 ( 7.049)	Loss 1.6250 (1.6479)	CeLoss 1.6250 (0.7596)	SegCLSLoss 0.0000 (0.0110)	KLLoss 0.0000 (0.2172)	MaskLoss 0.0000 (0.4304)	MaskBCELoss 0.0000 (0.0451)	MaskDICELoss 0.0000 (0.3853)
Epoch: [5][119/500]	Time  7.432 ( 7.432)	Loss 1.1030 (1.6402)	CeLoss 0.2539 (0.4940)	SegCLSLoss 0.0095 (0.0132)	KLLoss 0.3711 (0.2525)	MaskLoss 0.4040 (0.5573)	MaskBCELoss 0.1635 (0.1018)	MaskDICELoss 0.2405 (0.4555)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 117 is less than current step: 499. Dropping entry: {'train/loss': 1.647922396659851, 'train/ce_loss': 0.7595703125, 'train/seg_cls_loss': 0.01102294921875, 'train/kl_loss': 0.2171875, 'train/mask_bce_loss': 0.04508677758276462, 'train/mask_dice_loss': 0.3853197365999222, 'train/mask_loss': 0.4304065078496933, 'metrics/total_secs_per_batch': 7.049174547195435, 'metrics/data_secs_per_batch': 3.152870225906372, '_timestamp': 1740969143.480673}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 117 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969143.4809415}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 118 is less than current step: 499. Dropping entry: {'train/loss': 1.6401576757431031, 'train/ce_loss': 0.49404296875, 'train/seg_cls_loss': 0.01324462890625, 'train/kl_loss': 0.2525390625, 'train/mask_bce_loss': 0.10182779580354691, 'train/mask_dice_loss': 0.45545806735754013, 'train/mask_loss': 0.5572858691215515, 'metrics/total_secs_per_batch': 7.432011365890503, 'metrics/data_secs_per_batch': 3.319667863845825, '_timestamp': 1740969150.912894}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 118 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969150.913229}).
[2025-03-02 20:32:38,748] [INFO] [logging.py:96:log_dist] [Rank 0] step=2620, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:32:38,754] [INFO] [timer.py:215:stop] epoch=0/micro_step=26200/global_step=2620, RunningAvgSamplesPerSec=1.4302762412877905, CurrSamplesPerSec=1.2754827172758394, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [5][120/500]	Time  7.842 ( 7.842)	Loss 2.1481 (1.8997)	CeLoss 0.2197 (0.2363)	SegCLSLoss 0.0126 (0.0152)	KLLoss 0.3691 (0.3688)	MaskLoss 0.9432 (0.8093)	MaskBCELoss 0.0966 (0.2047)	MaskDICELoss 0.8465 (0.6047)
Epoch: [5][121/500]	Time  8.252 ( 8.252)	Loss 0.0562 (1.2915)	CeLoss 0.0562 (0.2412)	SegCLSLoss 0.0000 (0.0137)	KLLoss 0.0000 (0.3295)	MaskLoss 0.0000 (0.5051)	MaskBCELoss 0.0000 (0.1692)	MaskDICELoss 0.0000 (0.3359)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 119 is less than current step: 499. Dropping entry: {'train/loss': 1.8997227668762207, 'train/ce_loss': 0.236328125, 'train/seg_cls_loss': 0.015179443359375, 'train/kl_loss': 0.36875, 'train/mask_bce_loss': 0.2046609604731202, 'train/mask_dice_loss': 0.604673083126545, 'train/mask_loss': 0.8093340426683426, 'metrics/total_secs_per_batch': 7.842045545578003, 'metrics/data_secs_per_batch': 3.421008348464966, '_timestamp': 1740969158.7545738}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 119 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969158.7548854}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 120 is less than current step: 499. Dropping entry: {'train/loss': 1.2915481567382812, 'train/ce_loss': 0.241162109375, 'train/seg_cls_loss': 0.013677978515625, 'train/kl_loss': 0.3294921875, 'train/mask_bce_loss': 0.16921802759170532, 'train/mask_dice_loss': 0.3359066277742386, 'train/mask_loss': 0.5051246613264084, 'metrics/total_secs_per_batch': 8.251777410507202, 'metrics/data_secs_per_batch': 3.963913679122925, '_timestamp': 1740969167.0065687}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 120 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969167.0068862}).
Epoch: [5][122/500]	Time  8.247 ( 8.247)	Loss 1.3906 (1.5178)	CeLoss 1.3906 (0.6217)	SegCLSLoss 0.0000 (0.0098)	KLLoss 0.0000 (0.2156)	MaskLoss 0.0000 (0.4347)	MaskBCELoss 0.0000 (0.0770)	MaskDICELoss 0.0000 (0.3577)
Epoch: [5][123/500]	Time  8.150 ( 8.150)	Loss 2.2140 (1.5439)	CeLoss 0.2256 (0.4172)	SegCLSLoss 0.0198 (0.0104)	KLLoss 0.3613 (0.2535)	MaskLoss 0.9712 (0.5481)	MaskBCELoss 0.0690 (0.0833)	MaskDICELoss 0.9023 (0.4648)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 121 is less than current step: 499. Dropping entry: {'train/loss': 1.517779326438904, 'train/ce_loss': 0.6216796875, 'train/seg_cls_loss': 0.009832763671875, 'train/kl_loss': 0.215625, 'train/mask_bce_loss': 0.07697619237005711, 'train/mask_dice_loss': 0.3576947122812271, 'train/mask_loss': 0.43467090725898744, 'metrics/total_secs_per_batch': 8.247421741485596, 'metrics/data_secs_per_batch': 3.4255504608154297, '_timestamp': 1740969175.2541494}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 121 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969175.2544932}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 122 is less than current step: 499. Dropping entry: {'train/loss': 1.5438941240310669, 'train/ce_loss': 0.4171875, 'train/seg_cls_loss': 0.010418701171875, 'train/kl_loss': 0.253515625, 'train/mask_bce_loss': 0.08327594064176083, 'train/mask_dice_loss': 0.46484301388263705, 'train/mask_loss': 0.5481189459562301, 'metrics/total_secs_per_batch': 8.150142669677734, 'metrics/data_secs_per_batch': 3.4910852193832396, '_timestamp': 1740969183.4040987}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 122 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969183.404432}).
Epoch: [5][124/500]	Time  7.366 ( 7.366)	Loss 1.6707 (1.6502)	CeLoss 0.2002 (0.7160)	SegCLSLoss 0.0210 (0.0104)	KLLoss 0.3555 (0.1812)	MaskLoss 0.7123 (0.4556)	MaskBCELoss 0.1815 (0.1312)	MaskDICELoss 0.5308 (0.3244)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 123 is less than current step: 499. Dropping entry: {'train/loss': 1.6501797199249268, 'train/ce_loss': 0.716015625, 'train/seg_cls_loss': 0.01038818359375, 'train/kl_loss': 0.18125, 'train/mask_bce_loss': 0.1311945103108883, 'train/mask_dice_loss': 0.32436410188674925, 'train/mask_loss': 0.4555586099624634, 'metrics/total_secs_per_batch': 7.366295576095581, 'metrics/data_secs_per_batch': 2.800126242637634, '_timestamp': 1740969190.7704966}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 123 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969190.7708044}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 124 is less than current step: 499. Dropping entry: {'train/loss': 1.6163374662399292, 'train/ce_loss': 0.3556640625, 'train/seg_cls_loss': 0.014202880859375, 'train/kl_loss': 0.28828125, 'train/mask_bce_loss': 0.1031813882291317, 'train/mask_dice_loss': 0.5091377377510071, 'train/mask_loss': 0.6123191267251968, 'metrics/total_secs_per_batch': 8.355653285980225, 'metrics/data_secs_per_batch': 3.716026854515076, '_timestamp': 1740969199.1263936}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 124 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969199.1267953}).
Epoch: [5][125/500]	Time  8.356 ( 8.356)	Loss 2.1931 (1.6163)	CeLoss 0.2715 (0.3557)	SegCLSLoss 0.0187 (0.0142)	KLLoss 0.3594 (0.2883)	MaskLoss 0.9384 (0.6123)	MaskBCELoss 0.1054 (0.1032)	MaskDICELoss 0.8330 (0.5091)
Epoch: [5][126/500]	Time  6.494 ( 6.494)	Loss 2.3067 (1.4917)	CeLoss 0.2070 (0.4508)	SegCLSLoss 0.0176 (0.0115)	KLLoss 0.3594 (0.2535)	MaskLoss 1.0274 (0.5049)	MaskBCELoss 0.0298 (0.1137)	MaskDICELoss 0.9976 (0.3911)
Epoch: [5][127/500]	Time  6.498 ( 6.498)	Loss 0.0723 (1.2961)	CeLoss 0.0723 (0.5137)	SegCLSLoss 0.0000 (0.0100)	KLLoss 0.0000 (0.1793)	MaskLoss 0.0000 (0.3797)	MaskBCELoss 0.0000 (0.0315)	MaskDICELoss 0.0000 (0.3482)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 125 is less than current step: 499. Dropping entry: {'train/loss': 1.491732931137085, 'train/ce_loss': 0.45078125, 'train/seg_cls_loss': 0.011541748046875, 'train/kl_loss': 0.253515625, 'train/mask_bce_loss': 0.1137183791026473, 'train/mask_dice_loss': 0.3911324694752693, 'train/mask_loss': 0.5048508420586586, 'metrics/total_secs_per_batch': 6.494178056716919, 'metrics/data_secs_per_batch': 2.9703558444976808, '_timestamp': 1740969205.6204326}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 125 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969205.6207764}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 126 is less than current step: 499. Dropping entry: {'train/loss': 1.2960864782333374, 'train/ce_loss': 0.513671875, 'train/seg_cls_loss': 0.01004638671875, 'train/kl_loss': 0.179296875, 'train/mask_bce_loss': 0.03152647633105517, 'train/mask_dice_loss': 0.3482062131166458, 'train/mask_loss': 0.3797326982021332, 'metrics/total_secs_per_batch': 6.497961759567261, 'metrics/data_secs_per_batch': 3.163695812225342, '_timestamp': 1740969212.1182003}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 126 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969212.1184638}).
Epoch: [5][128/500]	Time  7.500 ( 7.500)	Loss 1.9719 (1.1979)	CeLoss 0.2012 (0.3760)	SegCLSLoss 0.0245 (0.0099)	KLLoss 0.3633 (0.2160)	MaskLoss 0.8609 (0.3975)	MaskBCELoss 0.3564 (0.0895)	MaskDICELoss 0.5045 (0.3080)
Epoch: [5][129/500]	Time  8.939 ( 8.939)	Loss 2.2645 (1.9802)	CeLoss 0.2158 (0.2176)	SegCLSLoss 0.0166 (0.0179)	KLLoss 0.3633 (0.3639)	MaskLoss 1.0014 (0.8584)	MaskBCELoss 0.2729 (0.2092)	MaskDICELoss 0.7285 (0.6492)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 127 is less than current step: 499. Dropping entry: {'train/loss': 1.1978554993867874, 'train/ce_loss': 0.376025390625, 'train/seg_cls_loss': 0.009869384765625, 'train/kl_loss': 0.216015625, 'train/mask_bce_loss': 0.08953319350257516, 'train/mask_dice_loss': 0.3080029308795929, 'train/mask_loss': 0.39753612130880356, 'metrics/total_secs_per_batch': 7.500227451324463, 'metrics/data_secs_per_batch': 3.357563090324402, '_timestamp': 1740969219.6185834}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 127 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969219.6188061}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 128 is less than current step: 499. Dropping entry: {'train/loss': 1.9801842451095581, 'train/ce_loss': 0.217578125, 'train/seg_cls_loss': 0.0179443359375, 'train/kl_loss': 0.3638671875, 'train/mask_bce_loss': 0.20921281902119518, 'train/mask_dice_loss': 0.6491898685693741, 'train/mask_loss': 0.8584026753902435, 'metrics/total_secs_per_batch': 8.939427137374878, 'metrics/data_secs_per_batch': 4.235050082206726, '_timestamp': 1740969228.5578818}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 128 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969228.5581696}).
[2025-03-02 20:33:55,532] [INFO] [logging.py:96:log_dist] [Rank 0] step=2630, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:33:55,538] [INFO] [timer.py:215:stop] epoch=0/micro_step=26300/global_step=2630, RunningAvgSamplesPerSec=1.4297429179223335, CurrSamplesPerSec=1.4328233230059562, MemAllocated=30.79GB, MaxMemAllocated=37.23GB
Epoch: [5][130/500]	Time  6.981 ( 6.981)	Loss 1.5226 (1.5048)	CeLoss 0.2891 (0.4284)	SegCLSLoss 0.0112 (0.0112)	KLLoss 0.3594 (0.2527)	MaskLoss 0.5963 (0.5228)	MaskBCELoss 0.1364 (0.1213)	MaskDICELoss 0.4599 (0.4014)
Epoch: [5][131/500]	Time  9.483 ( 9.483)	Loss 2.2229 (1.6538)	CeLoss 0.2520 (0.2243)	SegCLSLoss 0.0182 (0.0135)	KLLoss 0.3770 (0.3283)	MaskLoss 0.9620 (0.6949)	MaskBCELoss 0.0417 (0.1517)	MaskDICELoss 0.9203 (0.5432)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 129 is less than current step: 499. Dropping entry: {'train/loss': 1.5047938346862793, 'train/ce_loss': 0.42841796875, 'train/seg_cls_loss': 0.011212158203125, 'train/kl_loss': 0.252734375, 'train/mask_bce_loss': 0.12132167499512433, 'train/mask_dice_loss': 0.4014365792274475, 'train/mask_loss': 0.5227582484483719, 'metrics/total_secs_per_batch': 6.980803489685059, 'metrics/data_secs_per_batch': 3.1570935249328613, '_timestamp': 1740969235.5384734}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 129 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969235.5387366}).
Epoch: [5][132/500]	Time  9.398 ( 9.398)	Loss 1.1811 (1.5637)	CeLoss 0.2598 (0.2228)	SegCLSLoss 0.0135 (0.0150)	KLLoss 0.3613 (0.3254)	MaskLoss 0.4401 (0.6505)	MaskBCELoss 0.0503 (0.0609)	MaskDICELoss 0.3899 (0.5897)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 130 is less than current step: 499. Dropping entry: {'train/loss': 1.6537654876708985, 'train/ce_loss': 0.224267578125, 'train/seg_cls_loss': 0.013470458984375, 'train/kl_loss': 0.3283203125, 'train/mask_bce_loss': 0.15170206800103186, 'train/mask_dice_loss': 0.5432226598262787, 'train/mask_loss': 0.6949247121810913, 'metrics/total_secs_per_batch': 9.483187913894653, 'metrics/data_secs_per_batch': 4.753783392906189, '_timestamp': 1740969245.021946}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 130 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969245.022313}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 131 is less than current step: 499. Dropping entry: {'train/loss': 1.563658857345581, 'train/ce_loss': 0.222802734375, 'train/seg_cls_loss': 0.015032958984375, 'train/kl_loss': 0.325390625, 'train/mask_bce_loss': 0.06085310643538833, 'train/mask_dice_loss': 0.5896530836820603, 'train/mask_loss': 0.6505061835050583, 'metrics/total_secs_per_batch': 9.397720575332642, 'metrics/data_secs_per_batch': 4.433365249633789, '_timestamp': 1740969254.4197257}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 131 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969254.4200501}).
Epoch: [5][133/500]	Time  8.345 ( 8.345)	Loss 1.7001 (1.7543)	CeLoss 0.2061 (0.3378)	SegCLSLoss 0.0198 (0.0122)	KLLoss 0.3555 (0.2879)	MaskLoss 0.7241 (0.6907)	MaskBCELoss 0.0565 (0.0669)	MaskDICELoss 0.6676 (0.6239)
Epoch: [5][134/500]	Time  9.518 ( 9.518)	Loss 2.0618 (1.9610)	CeLoss 0.2275 (0.2063)	SegCLSLoss 0.0110 (0.0153)	KLLoss 0.3672 (0.3295)	MaskLoss 0.8962 (0.8570)	MaskBCELoss 0.0974 (0.1845)	MaskDICELoss 0.7987 (0.6725)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 132 is less than current step: 499. Dropping entry: {'train/loss': 1.754286527633667, 'train/ce_loss': 0.337841796875, 'train/seg_cls_loss': 0.012225341796875, 'train/kl_loss': 0.287890625, 'train/mask_bce_loss': 0.06688124109059572, 'train/mask_dice_loss': 0.6238606542348861, 'train/mask_loss': 0.6907419055700302, 'metrics/total_secs_per_batch': 8.345332145690918, 'metrics/data_secs_per_batch': 3.6846894025802612, '_timestamp': 1740969262.7648747}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 132 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969262.765058}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 133 is less than current step: 499. Dropping entry: {'train/loss': 1.9610167264938354, 'train/ce_loss': 0.20634765625, 'train/seg_cls_loss': 0.015313720703125, 'train/kl_loss': 0.3294921875, 'train/mask_bce_loss': 0.18451707623898983, 'train/mask_dice_loss': 0.6725049436092376, 'train/mask_loss': 0.8570220351219178, 'metrics/total_secs_per_batch': 9.518238306045532, 'metrics/data_secs_per_batch': 4.270288062095642, '_timestamp': 1740969272.2831748}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 133 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969272.2834654}).
Epoch: [5][135/500]	Time  8.273 ( 8.273)	Loss 1.7814 (1.8187)	CeLoss 0.2754 (0.2488)	SegCLSLoss 0.0121 (0.0163)	KLLoss 0.3633 (0.3260)	MaskLoss 0.7315 (0.7646)	MaskBCELoss 0.2240 (0.1292)	MaskDICELoss 0.5075 (0.6354)
Epoch: [5][136/500]	Time  6.180 ( 6.180)	Loss 0.7656 (1.1686)	CeLoss 0.7656 (0.7537)	SegCLSLoss 0.0000 (0.0037)	KLLoss 0.0000 (0.1086)	MaskLoss 0.0000 (0.2011)	MaskBCELoss 0.0000 (0.0194)	MaskDICELoss 0.0000 (0.1818)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 134 is less than current step: 499. Dropping entry: {'train/loss': 1.8187443375587464, 'train/ce_loss': 0.248779296875, 'train/seg_cls_loss': 0.01629638671875, 'train/kl_loss': 0.3259765625, 'train/mask_bce_loss': 0.12919745938852428, 'train/mask_dice_loss': 0.6354237288236618, 'train/mask_loss': 0.7646211802959442, 'metrics/total_secs_per_batch': 8.273077487945557, 'metrics/data_secs_per_batch': 3.5578508377075195, '_timestamp': 1740969280.5562189}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 134 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969280.5564964}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 135 is less than current step: 499. Dropping entry: {'train/loss': 1.1686471164226533, 'train/ce_loss': 0.7536865234375, 'train/seg_cls_loss': 0.003704833984375, 'train/kl_loss': 0.10859375, 'train/mask_bce_loss': 0.019352759420871734, 'train/mask_dice_loss': 0.18177987933158873, 'train/mask_loss': 0.2011326402425766, 'metrics/total_secs_per_batch': 6.180193185806274, 'metrics/data_secs_per_batch': 2.927150249481201, '_timestamp': 1740969286.7366188}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 135 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969286.7369676}).
Epoch: [5][137/500]	Time  9.913 ( 9.913)	Loss 1.9380 (1.9191)	CeLoss 0.2285 (0.2832)	SegCLSLoss 0.0200 (0.0162)	KLLoss 0.3496 (0.3234)	MaskLoss 0.8323 (0.7978)	MaskBCELoss 0.0455 (0.1577)	MaskDICELoss 0.7867 (0.6401)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 136 is less than current step: 499. Dropping entry: {'train/loss': 1.9190653443336487, 'train/ce_loss': 0.283203125, 'train/seg_cls_loss': 0.016204833984375, 'train/kl_loss': 0.3234375, 'train/mask_bce_loss': 0.15767123848199843, 'train/mask_dice_loss': 0.6401426881551743, 'train/mask_loss': 0.7978139221668243, 'metrics/total_secs_per_batch': 9.912907361984253, 'metrics/data_secs_per_batch': 4.665691494941711, '_timestamp': 1740969296.649371}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 136 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969296.6496592}).
Epoch: [5][138/500]	Time 10.133 (10.133)	Loss 1.5553 (1.7946)	CeLoss 0.2021 (0.2029)	SegCLSLoss 0.0187 (0.0158)	KLLoss 0.3594 (0.3279)	MaskLoss 0.6536 (0.7756)	MaskBCELoss 0.0541 (0.1538)	MaskDICELoss 0.5995 (0.6218)
Epoch: [5][139/500]	Time  7.883 ( 7.883)	Loss 2.2570 (1.2859)	CeLoss 0.1992 (0.2183)	SegCLSLoss 0.0242 (0.0142)	KLLoss 0.3535 (0.2893)	MaskLoss 1.0054 (0.5159)	MaskBCELoss 0.0812 (0.0766)	MaskDICELoss 0.9242 (0.4393)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 137 is less than current step: 499. Dropping entry: {'train/loss': 1.7945736050605774, 'train/ce_loss': 0.2029296875, 'train/seg_cls_loss': 0.015753173828125, 'train/kl_loss': 0.3279296875, 'train/mask_bce_loss': 0.15378464437089862, 'train/mask_dice_loss': 0.6217736542224884, 'train/mask_loss': 0.7755582928657532, 'metrics/total_secs_per_batch': 10.133066892623901, 'metrics/data_secs_per_batch': 4.51944899559021, '_timestamp': 1740969306.7826445}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 137 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969306.782979}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 138 is less than current step: 499. Dropping entry: {'train/loss': 1.2858686566352844, 'train/ce_loss': 0.218310546875, 'train/seg_cls_loss': 0.0141845703125, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.07657629884779453, 'train/mask_dice_loss': 0.4393072262406349, 'train/mask_loss': 0.5158835262060165, 'metrics/total_secs_per_batch': 7.883446931838989, 'metrics/data_secs_per_batch': 3.3862626791000365, '_timestamp': 1740969314.6658843}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 138 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969314.6661654}).
[2025-03-02 20:35:23,542] [INFO] [logging.py:96:log_dist] [Rank 0] step=2640, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:35:23,547] [INFO] [timer.py:215:stop] epoch=0/micro_step=26400/global_step=2640, RunningAvgSamplesPerSec=1.428345266234936, CurrSamplesPerSec=1.1259937570668168, MemAllocated=31.09GB, MaxMemAllocated=37.23GB
Epoch: [5][140/500]	Time  8.883 ( 8.883)	Loss 2.0963 (1.7966)	CeLoss 0.3281 (0.2079)	SegCLSLoss 0.0142 (0.0170)	KLLoss 0.3672 (0.3287)	MaskLoss 0.8616 (0.7737)	MaskBCELoss 0.0423 (0.1529)	MaskDICELoss 0.8193 (0.6208)
Epoch: [5][141/500]	Time  7.593 ( 7.593)	Loss 1.0547 (1.5787)	CeLoss 1.0547 (0.4476)	SegCLSLoss 0.0000 (0.0099)	KLLoss 0.0000 (0.2898)	MaskLoss 0.0000 (0.5484)	MaskBCELoss 0.0000 (0.0980)	MaskDICELoss 0.0000 (0.4505)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 139 is less than current step: 499. Dropping entry: {'train/loss': 1.7965816855430603, 'train/ce_loss': 0.207861328125, 'train/seg_cls_loss': 0.017010498046875, 'train/kl_loss': 0.3287109375, 'train/mask_bce_loss': 0.15287772472947836, 'train/mask_dice_loss': 0.6207793384790421, 'train/mask_loss': 0.7736570656299591, 'metrics/total_secs_per_batch': 8.882625341415405, 'metrics/data_secs_per_batch': 4.273451399803162, '_timestamp': 1740969323.5483022}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 139 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969323.5485687}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 140 is less than current step: 499. Dropping entry: {'train/loss': 1.5787333488464355, 'train/ce_loss': 0.44755859375, 'train/seg_cls_loss': 0.009918212890625, 'train/kl_loss': 0.28984375, 'train/mask_bce_loss': 0.09797371067106724, 'train/mask_dice_loss': 0.45047498792409896, 'train/mask_loss': 0.5484487026929855, 'metrics/total_secs_per_batch': 7.592672348022461, 'metrics/data_secs_per_batch': 3.990171504020691, '_timestamp': 1740969331.1413238}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 140 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969331.141665}).
Epoch: [5][142/500]	Time  6.923 ( 6.923)	Loss 2.7607 (1.7733)	CeLoss 0.1787 (0.3964)	SegCLSLoss 0.0194 (0.0173)	KLLoss 0.3770 (0.2926)	MaskLoss 1.2671 (0.6694)	MaskBCELoss 0.4486 (0.1828)	MaskDICELoss 0.8184 (0.4867)
Epoch: [5][143/500]	Time  8.910 ( 8.910)	Loss 1.3981 (1.6966)	CeLoss 0.1934 (0.2902)	SegCLSLoss 0.0139 (0.0121)	KLLoss 0.3691 (0.3277)	MaskLoss 0.5804 (0.6836)	MaskBCELoss 0.0134 (0.1152)	MaskDICELoss 0.5670 (0.5684)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 141 is less than current step: 499. Dropping entry: {'train/loss': 1.7732609033584594, 'train/ce_loss': 0.39638671875, 'train/seg_cls_loss': 0.017327880859375, 'train/kl_loss': 0.292578125, 'train/mask_bce_loss': 0.18278411198407413, 'train/mask_dice_loss': 0.48665884882211685, 'train/mask_loss': 0.6694429516792297, 'metrics/total_secs_per_batch': 6.922880172729492, 'metrics/data_secs_per_batch': 3.038091230392456, '_timestamp': 1740969338.0640068}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 141 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969338.0642426}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 142 is less than current step: 499. Dropping entry: {'train/loss': 1.6965918958187103, 'train/ce_loss': 0.290234375, 'train/seg_cls_loss': 0.012060546875, 'train/kl_loss': 0.327734375, 'train/mask_bce_loss': 0.11520692892372608, 'train/mask_dice_loss': 0.5683917440474033, 'train/mask_loss': 0.6835986703634263, 'metrics/total_secs_per_batch': 8.909908533096313, 'metrics/data_secs_per_batch': 4.06376371383667, '_timestamp': 1740969346.9740093}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 142 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969346.9743118}).
Epoch: [5][144/500]	Time  8.843 ( 8.843)	Loss 2.1485 (1.5244)	CeLoss 0.2334 (0.1752)	SegCLSLoss 0.0100 (0.0145)	KLLoss 0.3652 (0.2928)	MaskLoss 0.9366 (0.6563)	MaskBCELoss 0.4329 (0.1432)	MaskDICELoss 0.5036 (0.5131)
Epoch: [5][145/500]	Time  7.522 ( 7.522)	Loss 0.0596 (1.2709)	CeLoss 0.0596 (0.4769)	SegCLSLoss 0.0000 (0.0068)	KLLoss 0.0000 (0.1797)	MaskLoss 0.0000 (0.3863)	MaskBCELoss 0.0000 (0.0883)	MaskDICELoss 0.0000 (0.2980)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 143 is less than current step: 499. Dropping entry: {'train/loss': 1.5244057536125184, 'train/ce_loss': 0.175244140625, 'train/seg_cls_loss': 0.014483642578125, 'train/kl_loss': 0.2927734375, 'train/mask_bce_loss': 0.14319572634994984, 'train/mask_dice_loss': 0.5130745261907578, 'train/mask_loss': 0.6562702476978302, 'metrics/total_secs_per_batch': 8.84329867362976, 'metrics/data_secs_per_batch': 3.6327904224395753, '_timestamp': 1740969355.817451}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 143 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969355.8176787}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 144 is less than current step: 499. Dropping entry: {'train/loss': 1.2708652853965758, 'train/ce_loss': 0.476904296875, 'train/seg_cls_loss': 0.0068115234375, 'train/kl_loss': 0.1796875, 'train/mask_bce_loss': 0.08826571200042962, 'train/mask_dice_loss': 0.29802141785621644, 'train/mask_loss': 0.3862871378660202, 'metrics/total_secs_per_batch': 7.522447109222412, 'metrics/data_secs_per_batch': 3.0284717082977295, '_timestamp': 1740969363.3397136}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 144 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969363.3399909}).
Epoch: [5][146/500]	Time  7.087 ( 7.087)	Loss 2.3279 (1.8612)	CeLoss 0.1992 (0.5581)	SegCLSLoss 0.0234 (0.0120)	KLLoss 0.3574 (0.2564)	MaskLoss 1.0409 (0.6357)	MaskBCELoss 0.0459 (0.0944)	MaskDICELoss 0.9949 (0.5413)
Epoch: [5][147/500]	Time  8.110 ( 8.110)	Loss 2.6074 (1.2096)	CeLoss 0.2158 (0.2834)	SegCLSLoss 0.0172 (0.0088)	KLLoss 0.3633 (0.2166)	MaskLoss 1.1729 (0.4501)	MaskBCELoss 0.4838 (0.0752)	MaskDICELoss 0.6890 (0.3749)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 145 is less than current step: 499. Dropping entry: {'train/loss': 1.8611613273620606, 'train/ce_loss': 0.55810546875, 'train/seg_cls_loss': 0.0119873046875, 'train/kl_loss': 0.2564453125, 'train/mask_bce_loss': 0.09439062234014273, 'train/mask_dice_loss': 0.5412681758403778, 'train/mask_loss': 0.6356587886810303, 'metrics/total_secs_per_batch': 7.087008953094482, 'metrics/data_secs_per_batch': 3.0336995124816895, '_timestamp': 1740969370.4266913}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 145 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969370.4269512}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 146 is less than current step: 499. Dropping entry: {'train/loss': 1.209565395116806, 'train/ce_loss': 0.2833984375, 'train/seg_cls_loss': 0.008843994140625, 'train/kl_loss': 0.2166015625, 'train/mask_bce_loss': 0.07522362750023603, 'train/mask_dice_loss': 0.374871563911438, 'train/mask_loss': 0.4500952035188675, 'metrics/total_secs_per_batch': 8.109983444213867, 'metrics/data_secs_per_batch': 3.5464973926544188, '_timestamp': 1740969378.5367162}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 146 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969378.5370004}).
Epoch: [5][148/500]	Time  8.096 ( 8.096)	Loss 1.2079 (1.5761)	CeLoss 0.2314 (0.4238)	SegCLSLoss 0.0092 (0.0175)	KLLoss 0.3633 (0.2936)	MaskLoss 0.4672 (0.5570)	MaskBCELoss 0.1425 (0.0735)	MaskDICELoss 0.3247 (0.4835)
Epoch: [5][149/500]	Time  7.271 ( 7.271)	Loss 0.6282 (1.6860)	CeLoss 0.2217 (0.3625)	SegCLSLoss 0.0126 (0.0139)	KLLoss 0.3613 (0.3258)	MaskLoss 0.1823 (0.6420)	MaskBCELoss 0.0489 (0.1305)	MaskDICELoss 0.1334 (0.5114)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 147 is less than current step: 499. Dropping entry: {'train/loss': 1.576126503944397, 'train/ce_loss': 0.423828125, 'train/seg_cls_loss': 0.01749267578125, 'train/kl_loss': 0.2935546875, 'train/mask_bce_loss': 0.07350801415741444, 'train/mask_dice_loss': 0.483500549197197, 'train/mask_loss': 0.5570085614919662, 'metrics/total_secs_per_batch': 8.095857620239258, 'metrics/data_secs_per_batch': 3.385750579833984, '_timestamp': 1740969386.6327436}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 147 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969386.633077}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 148 is less than current step: 499. Dropping entry: {'train/loss': 1.6859572768211364, 'train/ce_loss': 0.3625, 'train/seg_cls_loss': 0.013873291015625, 'train/kl_loss': 0.32578125, 'train/mask_bce_loss': 0.13053777613677084, 'train/mask_dice_loss': 0.5114154547452927, 'train/mask_loss': 0.6419532299041748, 'metrics/total_secs_per_batch': 7.2710816860198975, 'metrics/data_secs_per_batch': 3.471893072128296, '_timestamp': 1740969393.9036562}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 148 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969393.9039388}).
[2025-03-02 20:36:41,229] [INFO] [logging.py:96:log_dist] [Rank 0] step=2650, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:36:41,235] [INFO] [timer.py:215:stop] epoch=0/micro_step=26500/global_step=2650, RunningAvgSamplesPerSec=1.4277550076245646, CurrSamplesPerSec=1.364055907408846, MemAllocated=30.94GB, MaxMemAllocated=37.23GB
Epoch: [5][150/500]	Time  7.333 ( 7.333)	Loss 0.7573 (1.6695)	CeLoss 0.2969 (0.5243)	SegCLSLoss 0.0130 (0.0140)	KLLoss 0.3574 (0.2572)	MaskLoss 0.2097 (0.5564)	MaskBCELoss 0.1047 (0.1307)	MaskDICELoss 0.1050 (0.4257)
Epoch: [5][151/500]	Time  7.684 ( 7.684)	Loss 1.7989 (1.4595)	CeLoss 0.1748 (0.4193)	SegCLSLoss 0.0232 (0.0114)	KLLoss 0.3535 (0.2529)	MaskLoss 0.7886 (0.5046)	MaskBCELoss 0.0765 (0.0715)	MaskDICELoss 0.7121 (0.4331)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 149 is less than current step: 499. Dropping entry: {'train/loss': 1.6694800972938537, 'train/ce_loss': 0.524267578125, 'train/seg_cls_loss': 0.014007568359375, 'train/kl_loss': 0.2572265625, 'train/mask_bce_loss': 0.1306950729340315, 'train/mask_dice_loss': 0.4256758354604244, 'train/mask_loss': 0.5563709139823914, 'metrics/total_secs_per_batch': 7.3326451778411865, 'metrics/data_secs_per_batch': 3.314984583854675, '_timestamp': 1740969401.2361238}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 149 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969401.2364066}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 150 is less than current step: 499. Dropping entry: {'train/loss': 1.4595010161399842, 'train/ce_loss': 0.4193359375, 'train/seg_cls_loss': 0.011444091796875, 'train/kl_loss': 0.2529296875, 'train/mask_bce_loss': 0.0714605800807476, 'train/mask_dice_loss': 0.4330946087837219, 'train/mask_loss': 0.5045551955699921, 'metrics/total_secs_per_batch': 7.683578014373779, 'metrics/data_secs_per_batch': 3.3430535793304443, '_timestamp': 1740969408.9199011}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 150 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969408.920252}).
Epoch: [5][152/500]	Time  8.276 ( 8.276)	Loss 0.9062 (1.4983)	CeLoss 0.9062 (0.3333)	SegCLSLoss 0.0000 (0.0099)	KLLoss 0.0000 (0.2217)	MaskLoss 0.0000 (0.5689)	MaskBCELoss 0.0000 (0.0946)	MaskDICELoss 0.0000 (0.4743)
Epoch: [5][153/500]	Time  6.067 ( 6.067)	Loss 1.2422 (1.2077)	CeLoss 1.2422 (0.6300)	SegCLSLoss 0.0000 (0.0072)	KLLoss 0.0000 (0.1482)	MaskLoss 0.0000 (0.2796)	MaskBCELoss 0.0000 (0.0613)	MaskDICELoss 0.0000 (0.2184)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 151 is less than current step: 499. Dropping entry: {'train/loss': 1.4982799410820007, 'train/ce_loss': 0.333251953125, 'train/seg_cls_loss': 0.009930419921875, 'train/kl_loss': 0.2216796875, 'train/mask_bce_loss': 0.09456790648400784, 'train/mask_dice_loss': 0.47432302832603457, 'train/mask_loss': 0.5688909351825714, 'metrics/total_secs_per_batch': 8.276346445083618, 'metrics/data_secs_per_batch': 3.7098152875900268, '_timestamp': 1740969417.1962183}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 151 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969417.196491}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 152 is less than current step: 499. Dropping entry: {'train/loss': 1.2076740503311156, 'train/ce_loss': 0.630029296875, 'train/seg_cls_loss': 0.007220458984375, 'train/kl_loss': 0.1482421875, 'train/mask_bce_loss': 0.061271358840167524, 'train/mask_dice_loss': 0.21837132871150972, 'train/mask_loss': 0.27964268922805785, 'metrics/total_secs_per_batch': 6.066907167434692, 'metrics/data_secs_per_batch': 2.674135994911194, '_timestamp': 1740969423.2631395}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 152 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969423.2634194}).
Epoch: [5][154/500]	Time  7.942 ( 7.942)	Loss 1.7578 (1.6817)	CeLoss 1.7578 (0.4771)	SegCLSLoss 0.0000 (0.0113)	KLLoss 0.0000 (0.2535)	MaskLoss 0.0000 (0.5869)	MaskBCELoss 0.0000 (0.1305)	MaskDICELoss 0.0000 (0.4564)
Epoch: [5][155/500]	Time  7.320 ( 7.320)	Loss 2.6825 (1.6236)	CeLoss 0.2354 (0.5421)	SegCLSLoss 0.0133 (0.0105)	KLLoss 0.3613 (0.2191)	MaskLoss 1.2026 (0.5272)	MaskBCELoss 0.6643 (0.1447)	MaskDICELoss 0.5383 (0.3825)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 153 is less than current step: 499. Dropping entry: {'train/loss': 1.6816862225532532, 'train/ce_loss': 0.47705078125, 'train/seg_cls_loss': 0.011328125, 'train/kl_loss': 0.253515625, 'train/mask_bce_loss': 0.13050825726240872, 'train/mask_dice_loss': 0.45637978315353395, 'train/mask_loss': 0.5868880391120911, 'metrics/total_secs_per_batch': 7.942340135574341, 'metrics/data_secs_per_batch': 3.6016651153564454, '_timestamp': 1740969431.2056544}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 153 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969431.206016}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 154 is less than current step: 499. Dropping entry: {'train/loss': 1.6235660672187806, 'train/ce_loss': 0.54208984375, 'train/seg_cls_loss': 0.0104736328125, 'train/kl_loss': 0.219140625, 'train/mask_bce_loss': 0.14473947361111641, 'train/mask_dice_loss': 0.3824976682662964, 'train/mask_loss': 0.5272371366620063, 'metrics/total_secs_per_batch': 7.320055961608887, 'metrics/data_secs_per_batch': 2.797775459289551, '_timestamp': 1740969438.5255556}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 154 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969438.5258403}).
Epoch: [5][156/500]	Time  7.069 ( 7.069)	Loss 1.7751 (1.2249)	CeLoss 0.1748 (0.5783)	SegCLSLoss 0.0167 (0.0054)	KLLoss 0.3594 (0.1461)	MaskLoss 0.7777 (0.3146)	MaskBCELoss 0.0460 (0.0615)	MaskDICELoss 0.7317 (0.2530)
Epoch: [5][157/500]	Time  8.724 ( 8.724)	Loss 0.8359 (1.5534)	CeLoss 0.8359 (0.3437)	SegCLSLoss 0.0000 (0.0133)	KLLoss 0.0000 (0.2523)	MaskLoss 0.0000 (0.5889)	MaskBCELoss 0.0000 (0.0926)	MaskDICELoss 0.0000 (0.4963)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 155 is less than current step: 499. Dropping entry: {'train/loss': 1.224925971031189, 'train/ce_loss': 0.5783447265625, 'train/seg_cls_loss': 0.00540771484375, 'train/kl_loss': 0.14609375, 'train/mask_bce_loss': 0.06150307133793831, 'train/mask_dice_loss': 0.25304731726646423, 'train/mask_loss': 0.31455038785934447, 'metrics/total_secs_per_batch': 7.068974733352661, 'metrics/data_secs_per_batch': 3.165234637260437, '_timestamp': 1740969445.5945156}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 155 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969445.5947876}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 156 is less than current step: 499. Dropping entry: {'train/loss': 1.5533509850502014, 'train/ce_loss': 0.343701171875, 'train/seg_cls_loss': 0.013311767578125, 'train/kl_loss': 0.25234375, 'train/mask_bce_loss': 0.09262296184897423, 'train/mask_dice_loss': 0.4962839543819427, 'train/mask_loss': 0.5889069169759751, 'metrics/total_secs_per_batch': 8.724335432052612, 'metrics/data_secs_per_batch': 4.104448199272156, '_timestamp': 1740969454.3190894}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 156 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969454.3194396}).
Epoch: [5][158/500]	Time  7.627 ( 7.627)	Loss 1.4743 (1.4715)	CeLoss 0.2539 (0.3965)	SegCLSLoss 0.0111 (0.0129)	KLLoss 0.3691 (0.2902)	MaskLoss 0.5887 (0.5198)	MaskBCELoss 0.0403 (0.0593)	MaskDICELoss 0.5484 (0.4604)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 157 is less than current step: 499. Dropping entry: {'train/loss': 1.4714590668678285, 'train/ce_loss': 0.396484375, 'train/seg_cls_loss': 0.012872314453125, 'train/kl_loss': 0.290234375, 'train/mask_bce_loss': 0.059330395376309755, 'train/mask_dice_loss': 0.4604323521256447, 'train/mask_loss': 0.5197627365589141, 'metrics/total_secs_per_batch': 7.627331018447876, 'metrics/data_secs_per_batch': 3.4691953659057617, '_timestamp': 1740969461.9461863}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 157 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969461.9464543}).
Epoch: [5][159/500]	Time  8.342 ( 8.342)	Loss 2.2964 (1.3529)	CeLoss 0.2109 (0.3126)	SegCLSLoss 0.0212 (0.0115)	KLLoss 0.3477 (0.2541)	MaskLoss 1.0203 (0.5046)	MaskBCELoss 0.0416 (0.0878)	MaskDICELoss 0.9787 (0.4168)
[2025-03-02 20:37:58,708] [INFO] [logging.py:96:log_dist] [Rank 0] step=2660, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:37:58,714] [INFO] [timer.py:215:stop] epoch=0/micro_step=26600/global_step=2660, RunningAvgSamplesPerSec=1.4271856996696983, CurrSamplesPerSec=1.1868867294373124, MemAllocated=31.26GB, MaxMemAllocated=37.23GB
Epoch: [5][160/500]	Time  8.427 ( 8.427)	Loss 2.1695 (1.6445)	CeLoss 0.1650 (0.4633)	SegCLSLoss 0.0259 (0.0112)	KLLoss 0.3496 (0.2527)	MaskLoss 0.9783 (0.5752)	MaskBCELoss 0.1845 (0.1644)	MaskDICELoss 0.7938 (0.4108)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 158 is less than current step: 499. Dropping entry: {'train/loss': 1.3529479384422303, 'train/ce_loss': 0.31259765625, 'train/seg_cls_loss': 0.0114501953125, 'train/kl_loss': 0.2541015625, 'train/mask_bce_loss': 0.08783192858099938, 'train/mask_dice_loss': 0.4167670369148254, 'train/mask_loss': 0.5045989662408829, 'metrics/total_secs_per_batch': 8.341737747192383, 'metrics/data_secs_per_batch': 3.4783923625946045, '_timestamp': 1740969470.2879417}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 158 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969470.2882154}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 159 is less than current step: 499. Dropping entry: {'train/loss': 1.644483494758606, 'train/ce_loss': 0.46328125, 'train/seg_cls_loss': 0.0111572265625, 'train/kl_loss': 0.252734375, 'train/mask_bce_loss': 0.16440273970365524, 'train/mask_dice_loss': 0.4107686966657639, 'train/mask_loss': 0.5751714408397675, 'metrics/total_secs_per_batch': 8.426998376846313, 'metrics/data_secs_per_batch': 3.9079273462295534, '_timestamp': 1740969478.7148423}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 159 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969478.715134}).
Epoch: [5][161/500]	Time  8.114 ( 8.114)	Loss 1.8297 (1.5148)	CeLoss 0.2754 (0.2078)	SegCLSLoss 0.0129 (0.0174)	KLLoss 0.3594 (0.3307)	MaskLoss 0.7566 (0.6326)	MaskBCELoss 0.0747 (0.0934)	MaskDICELoss 0.6819 (0.5393)
Epoch: [5][162/500]	Time  8.872 ( 8.872)	Loss 1.1064 (1.5086)	CeLoss 0.2412 (0.3657)	SegCLSLoss 0.0097 (0.0134)	KLLoss 0.3691 (0.3297)	MaskLoss 0.4116 (0.5518)	MaskBCELoss 0.0405 (0.1418)	MaskDICELoss 0.3711 (0.4099)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 160 is less than current step: 499. Dropping entry: {'train/loss': 1.5148075878620149, 'train/ce_loss': 0.207763671875, 'train/seg_cls_loss': 0.017437744140625, 'train/kl_loss': 0.3306640625, 'train/mask_bce_loss': 0.09336415901780129, 'train/mask_dice_loss': 0.539283774793148, 'train/mask_loss': 0.6326479375362396, 'metrics/total_secs_per_batch': 8.113861322402954, 'metrics/data_secs_per_batch': 3.5586033344268797, '_timestamp': 1740969486.8289065}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 160 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969486.8292243}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 161 is less than current step: 499. Dropping entry: {'train/loss': 1.5086199522018433, 'train/ce_loss': 0.36572265625, 'train/seg_cls_loss': 0.013385009765625, 'train/kl_loss': 0.3296875, 'train/mask_bce_loss': 0.14184478316456078, 'train/mask_dice_loss': 0.40992612540721896, 'train/mask_loss': 0.5517709121108055, 'metrics/total_secs_per_batch': 8.87198543548584, 'metrics/data_secs_per_batch': 3.477519226074219, '_timestamp': 1740969495.7007878}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 161 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969495.7010586}).
Epoch: [5][163/500]	Time  7.695 ( 7.695)	Loss 1.1250 (1.4215)	CeLoss 1.1250 (0.3857)	SegCLSLoss 0.0000 (0.0111)	KLLoss 0.0000 (0.2203)	MaskLoss 0.0000 (0.5041)	MaskBCELoss 0.0000 (0.1577)	MaskDICELoss 0.0000 (0.3465)
Epoch: [5][164/500]	Time  7.917 ( 7.917)	Loss 1.2969 (1.3376)	CeLoss 1.2969 (0.4831)	SegCLSLoss 0.0000 (0.0118)	KLLoss 0.0000 (0.2545)	MaskLoss 0.0000 (0.4117)	MaskBCELoss 0.0000 (0.0551)	MaskDICELoss 0.0000 (0.3566)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 162 is less than current step: 499. Dropping entry: {'train/loss': 1.4215278625488281, 'train/ce_loss': 0.3857421875, 'train/seg_cls_loss': 0.011126708984375, 'train/kl_loss': 0.2203125, 'train/mask_bce_loss': 0.15767299383878708, 'train/mask_dice_loss': 0.34645031988620756, 'train/mask_loss': 0.5041233122348785, 'metrics/total_secs_per_batch': 7.695271730422974, 'metrics/data_secs_per_batch': 3.6376877784729005, '_timestamp': 1740969503.396071}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 162 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969503.396258}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 163 is less than current step: 499. Dropping entry: {'train/loss': 1.3376405596733094, 'train/ce_loss': 0.483056640625, 'train/seg_cls_loss': 0.01180419921875, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.05511092059314251, 'train/mask_dice_loss': 0.3565804399549961, 'train/mask_loss': 0.41169137358665464, 'metrics/total_secs_per_batch': 7.9167821407318115, 'metrics/data_secs_per_batch': 4.310535669326782, '_timestamp': 1740969511.3128457}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 163 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969511.313115}).
Epoch: [5][165/500]	Time  9.082 ( 9.082)	Loss 1.8402 (1.9858)	CeLoss 0.2051 (0.3771)	SegCLSLoss 0.0178 (0.0160)	KLLoss 0.3652 (0.3275)	MaskLoss 0.7951 (0.7841)	MaskBCELoss 0.0496 (0.1394)	MaskDICELoss 0.7455 (0.6447)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 164 is less than current step: 499. Dropping entry: {'train/loss': 1.985827386379242, 'train/ce_loss': 0.37705078125, 'train/seg_cls_loss': 0.01595458984375, 'train/kl_loss': 0.3275390625, 'train/mask_bce_loss': 0.13939675437286497, 'train/mask_dice_loss': 0.6447278589010239, 'train/mask_loss': 0.784124618768692, 'metrics/total_secs_per_batch': 9.08185625076294, 'metrics/data_secs_per_batch': 4.287919735908508, '_timestamp': 1740969520.3947937}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 164 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969520.395105}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 165 is less than current step: 499. Dropping entry: {'train/loss': 1.3999233484268188, 'train/ce_loss': 0.3030029296875, 'train/seg_cls_loss': 0.01256103515625, 'train/kl_loss': 0.2162109375, 'train/mask_bce_loss': 0.06153004760853946, 'train/mask_dice_loss': 0.4730629801750183, 'train/mask_loss': 0.5345930278301239, 'metrics/total_secs_per_batch': 9.083303213119507, 'metrics/data_secs_per_batch': 4.295485997200013, '_timestamp': 1740969529.478131}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 165 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969529.4784517}).
Epoch: [5][166/500]	Time  9.083 ( 9.083)	Loss 1.5346 (1.3999)	CeLoss 0.1982 (0.3030)	SegCLSLoss 0.0256 (0.0126)	KLLoss 0.3516 (0.2162)	MaskLoss 0.6442 (0.5346)	MaskBCELoss 0.0041 (0.0615)	MaskDICELoss 0.6401 (0.4731)
Epoch: [5][167/500]	Time  8.163 ( 8.163)	Loss 1.4877 (1.8201)	CeLoss 0.2354 (0.4842)	SegCLSLoss 0.0112 (0.0101)	KLLoss 0.3711 (0.2529)	MaskLoss 0.6052 (0.6527)	MaskBCELoss 0.1370 (0.1709)	MaskDICELoss 0.4681 (0.4818)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 166 is less than current step: 499. Dropping entry: {'train/loss': 1.8200763940811158, 'train/ce_loss': 0.4841796875, 'train/seg_cls_loss': 0.01005859375, 'train/kl_loss': 0.2529296875, 'train/mask_bce_loss': 0.17092832252383233, 'train/mask_dice_loss': 0.481785649061203, 'train/mask_loss': 0.6527139723300934, 'metrics/total_secs_per_batch': 8.163394212722778, 'metrics/data_secs_per_batch': 3.3429885625839235, '_timestamp': 1740969537.64139}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 166 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969537.6416554}).
Epoch: [5][168/500]	Time  8.010 ( 8.010)	Loss 1.2946 (1.5338)	CeLoss 0.2217 (0.3994)	SegCLSLoss 0.0215 (0.0140)	KLLoss 0.3516 (0.2871)	MaskLoss 0.5135 (0.5492)	MaskBCELoss 0.0546 (0.1370)	MaskDICELoss 0.4589 (0.4122)
Epoch: [5][169/500]	Time  8.256 ( 8.256)	Loss 1.4219 (1.7904)	CeLoss 1.4219 (0.4614)	SegCLSLoss 0.0000 (0.0148)	KLLoss 0.0000 (0.2879)	MaskLoss 0.0000 (0.6464)	MaskBCELoss 0.0000 (0.0734)	MaskDICELoss 0.0000 (0.5730)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 167 is less than current step: 499. Dropping entry: {'train/loss': 1.5338181018829347, 'train/ce_loss': 0.3994140625, 'train/seg_cls_loss': 0.014007568359375, 'train/kl_loss': 0.287109375, 'train/mask_bce_loss': 0.13702489724382758, 'train/mask_dice_loss': 0.41215953379869463, 'train/mask_loss': 0.549184438586235, 'metrics/total_secs_per_batch': 8.010491132736206, 'metrics/data_secs_per_batch': 3.522117328643799, '_timestamp': 1740969545.651905}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 167 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969545.6521823}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 168 is less than current step: 499. Dropping entry: {'train/loss': 1.79038348197937, 'train/ce_loss': 0.46142578125, 'train/seg_cls_loss': 0.01478271484375, 'train/kl_loss': 0.287890625, 'train/mask_bce_loss': 0.07338952738791704, 'train/mask_dice_loss': 0.5729740917682647, 'train/mask_loss': 0.6463636219501495, 'metrics/total_secs_per_batch': 8.256345987319946, 'metrics/data_secs_per_batch': 3.713169240951538, '_timestamp': 1740969553.9082484}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 168 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969553.9085124}).
[2025-03-02 20:39:21,212] [INFO] [logging.py:96:log_dist] [Rank 0] step=2670, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:39:21,217] [INFO] [timer.py:215:stop] epoch=0/micro_step=26700/global_step=2670, RunningAvgSamplesPerSec=1.426237881248729, CurrSamplesPerSec=1.3682570920752666, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [5][170/500]	Time  7.310 ( 7.310)	Loss 2.6227 (1.8004)	CeLoss 0.2031 (0.4246)	SegCLSLoss 0.0135 (0.0127)	KLLoss 0.3574 (0.2900)	MaskLoss 1.1883 (0.6702)	MaskBCELoss 0.4699 (0.1788)	MaskDICELoss 0.7185 (0.4915)
Epoch: [5][171/500]	Time  7.501 ( 7.501)	Loss 1.2344 (1.7133)	CeLoss 1.2344 (0.2896)	SegCLSLoss 0.0000 (0.0164)	KLLoss 0.0000 (0.2889)	MaskLoss 0.0000 (0.6932)	MaskBCELoss 0.0000 (0.1317)	MaskDICELoss 0.0000 (0.5615)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 169 is less than current step: 499. Dropping entry: {'train/loss': 1.8004213452339173, 'train/ce_loss': 0.424609375, 'train/seg_cls_loss': 0.0126708984375, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.1787635341286659, 'train/mask_dice_loss': 0.49146667420864104, 'train/mask_loss': 0.6702301979064942, 'metrics/total_secs_per_batch': 7.310103416442871, 'metrics/data_secs_per_batch': 3.159550738334656, '_timestamp': 1740969561.2181826}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 169 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969561.2184498}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 170 is less than current step: 499. Dropping entry: {'train/loss': 1.7132880091667175, 'train/ce_loss': 0.289599609375, 'train/seg_cls_loss': 0.016387939453125, 'train/kl_loss': 0.2888671875, 'train/mask_bce_loss': 0.13171839928254486, 'train/mask_dice_loss': 0.5615222901105881, 'train/mask_loss': 0.693240687251091, 'metrics/total_secs_per_batch': 7.500731945037842, 'metrics/data_secs_per_batch': 3.6087831258773804, '_timestamp': 1740969568.7191021}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 170 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969568.7193863}).
Epoch: [5][172/500]	Time  8.205 ( 8.205)	Loss 1.1875 (2.0063)	CeLoss 1.1875 (0.3205)	SegCLSLoss 0.0000 (0.0158)	KLLoss 0.0000 (0.3287)	MaskLoss 0.0000 (0.8224)	MaskBCELoss 0.0000 (0.1939)	MaskDICELoss 0.0000 (0.6284)
Epoch: [5][173/500]	Time  7.522 ( 7.522)	Loss 1.2656 (1.6605)	CeLoss 1.2656 (0.5442)	SegCLSLoss 0.0000 (0.0082)	KLLoss 0.0000 (0.1830)	MaskLoss 0.0000 (0.5469)	MaskBCELoss 0.0000 (0.1656)	MaskDICELoss 0.0000 (0.3814)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 171 is less than current step: 499. Dropping entry: {'train/loss': 2.0062824845314027, 'train/ce_loss': 0.3205078125, 'train/seg_cls_loss': 0.01583251953125, 'train/kl_loss': 0.3287109375, 'train/mask_bce_loss': 0.19394850339740516, 'train/mask_dice_loss': 0.6284310311079025, 'train/mask_loss': 0.822379532456398, 'metrics/total_secs_per_batch': 8.205236673355103, 'metrics/data_secs_per_batch': 3.8022451400756836, '_timestamp': 1740969576.9243724}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 171 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969576.9246473}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 172 is less than current step: 499. Dropping entry: {'train/loss': 1.6605229377746582, 'train/ce_loss': 0.54423828125, 'train/seg_cls_loss': 0.008197021484375, 'train/kl_loss': 0.1830078125, 'train/mask_bce_loss': 0.16555515956133604, 'train/mask_dice_loss': 0.3813566923141479, 'train/mask_loss': 0.5469118475914001, 'metrics/total_secs_per_batch': 7.522253751754761, 'metrics/data_secs_per_batch': 3.19021954536438, '_timestamp': 1740969584.446782}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 172 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969584.4471102}).
Epoch: [5][174/500]	Time  8.579 ( 8.579)	Loss 1.9451 (1.8321)	CeLoss 0.2090 (0.2217)	SegCLSLoss 0.0156 (0.0156)	KLLoss 0.3535 (0.3227)	MaskLoss 0.8466 (0.7853)	MaskBCELoss 0.0206 (0.1240)	MaskDICELoss 0.8259 (0.6613)
Epoch: [5][175/500]	Time  7.955 ( 7.955)	Loss 1.0781 (1.5558)	CeLoss 1.0781 (0.4766)	SegCLSLoss 0.0000 (0.0125)	KLLoss 0.0000 (0.2162)	MaskLoss 0.0000 (0.5257)	MaskBCELoss 0.0000 (0.1057)	MaskDICELoss 0.0000 (0.4200)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 173 is less than current step: 499. Dropping entry: {'train/loss': 1.8320893406867982, 'train/ce_loss': 0.221728515625, 'train/seg_cls_loss': 0.0156494140625, 'train/kl_loss': 0.32265625, 'train/mask_bce_loss': 0.1239888877607882, 'train/mask_dice_loss': 0.6612696409225464, 'train/mask_loss': 0.785258537530899, 'metrics/total_secs_per_batch': 8.578787088394165, 'metrics/data_secs_per_batch': 4.107734417915344, '_timestamp': 1740969593.0253935}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 173 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969593.0256882}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 174 is less than current step: 499. Dropping entry: {'train/loss': 1.5557926297187805, 'train/ce_loss': 0.4765625, 'train/seg_cls_loss': 0.01253662109375, 'train/kl_loss': 0.2162109375, 'train/mask_bce_loss': 0.10565295414999128, 'train/mask_dice_loss': 0.42004607915878295, 'train/mask_loss': 0.5256990313529968, 'metrics/total_secs_per_batch': 7.955456495285034, 'metrics/data_secs_per_batch': 3.4286516666412354, '_timestamp': 1740969600.9810622}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 174 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969600.9814112}).
Epoch: [5][176/500]	Time  7.841 ( 7.841)	Loss 2.4093 (1.5472)	CeLoss 0.1797 (0.2694)	SegCLSLoss 0.0178 (0.0156)	KLLoss 0.3691 (0.3266)	MaskLoss 1.0919 (0.6186)	MaskBCELoss 0.2871 (0.1199)	MaskDICELoss 0.8047 (0.4987)
Epoch: [5][177/500]	Time  7.917 ( 7.917)	Loss 1.2422 (1.6074)	CeLoss 1.2422 (0.4378)	SegCLSLoss 0.0000 (0.0104)	KLLoss 0.0000 (0.2172)	MaskLoss 0.0000 (0.5713)	MaskBCELoss 0.0000 (0.0826)	MaskDICELoss 0.0000 (0.4887)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 175 is less than current step: 499. Dropping entry: {'train/loss': 1.5471741735935212, 'train/ce_loss': 0.26943359375, 'train/seg_cls_loss': 0.01561279296875, 'train/kl_loss': 0.3265625, 'train/mask_bce_loss': 0.11986343711614608, 'train/mask_dice_loss': 0.49869434386491773, 'train/mask_loss': 0.6185577854514122, 'metrics/total_secs_per_batch': 7.8408777713775635, 'metrics/data_secs_per_batch': 3.3473978281021117, '_timestamp': 1740969608.8217623}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 175 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969608.822045}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 176 is less than current step: 499. Dropping entry: {'train/loss': 1.6073667526245117, 'train/ce_loss': 0.4377685546875, 'train/seg_cls_loss': 0.010430908203125, 'train/kl_loss': 0.2171875, 'train/mask_bce_loss': 0.08258643988519906, 'train/mask_dice_loss': 0.48868727684020996, 'train/mask_loss': 0.5712737143039703, 'metrics/total_secs_per_batch': 7.916993618011475, 'metrics/data_secs_per_batch': 3.5096864461898805, '_timestamp': 1740969616.7388976}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 176 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969616.7392342}).
Epoch: [5][178/500]	Time  7.547 ( 7.547)	Loss 1.4297 (1.5513)	CeLoss 1.4297 (0.4279)	SegCLSLoss 0.0000 (0.0114)	KLLoss 0.0000 (0.2551)	MaskLoss 0.0000 (0.5461)	MaskBCELoss 0.0000 (0.1268)	MaskDICELoss 0.0000 (0.4193)
Epoch: [5][179/500]	Time  7.914 ( 7.914)	Loss 1.8841 (2.0070)	CeLoss 0.1895 (0.2617)	SegCLSLoss 0.0239 (0.0162)	KLLoss 0.3477 (0.3609)	MaskLoss 0.8239 (0.8506)	MaskBCELoss 0.0043 (0.1744)	MaskDICELoss 0.8196 (0.6762)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 177 is less than current step: 499. Dropping entry: {'train/loss': 1.5513197660446167, 'train/ce_loss': 0.427880859375, 'train/seg_cls_loss': 0.011358642578125, 'train/kl_loss': 0.255078125, 'train/mask_bce_loss': 0.1268269056454301, 'train/mask_dice_loss': 0.41931638196110727, 'train/mask_loss': 0.5461432814598084, 'metrics/total_secs_per_batch': 7.547233581542969, 'metrics/data_secs_per_batch': 3.5054593086242676, '_timestamp': 1740969624.2859561}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 177 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969624.2862372}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 178 is less than current step: 499. Dropping entry: {'train/loss': 2.007021188735962, 'train/ce_loss': 0.26171875, 'train/seg_cls_loss': 0.016168212890625, 'train/kl_loss': 0.3609375, 'train/mask_bce_loss': 0.1744099091272801, 'train/mask_dice_loss': 0.6761710107326507, 'train/mask_loss': 0.8505809128284454, 'metrics/total_secs_per_batch': 7.914187908172607, 'metrics/data_secs_per_batch': 3.6834233045578, '_timestamp': 1740969632.2001472}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 178 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969632.2004395}).
[2025-03-02 20:40:40,282] [INFO] [logging.py:96:log_dist] [Rank 0] step=2680, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:40:40,287] [INFO] [timer.py:215:stop] epoch=0/micro_step=26800/global_step=2680, RunningAvgSamplesPerSec=1.425558960612771, CurrSamplesPerSec=1.2366800795746247, MemAllocated=31.26GB, MaxMemAllocated=37.23GB
Epoch: [5][180/500]	Time  8.088 ( 8.088)	Loss 2.2496 (1.8841)	CeLoss 0.1885 (0.5389)	SegCLSLoss 0.0206 (0.0110)	KLLoss 0.3809 (0.2602)	MaskLoss 1.0062 (0.6569)	MaskBCELoss 0.0189 (0.1423)	MaskDICELoss 0.9872 (0.5146)
Epoch: [5][181/500]	Time  6.893 ( 6.893)	Loss 1.1094 (1.2954)	CeLoss 1.1094 (0.4750)	SegCLSLoss 0.0000 (0.0073)	KLLoss 0.0000 (0.1805)	MaskLoss 0.0000 (0.3994)	MaskBCELoss 0.0000 (0.0628)	MaskDICELoss 0.0000 (0.3366)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 179 is less than current step: 499. Dropping entry: {'train/loss': 1.8840961098670959, 'train/ce_loss': 0.5388671875, 'train/seg_cls_loss': 0.011016845703125, 'train/kl_loss': 0.26015625, 'train/mask_bce_loss': 0.14233132004737853, 'train/mask_dice_loss': 0.5146093219518661, 'train/mask_loss': 0.656940633058548, 'metrics/total_secs_per_batch': 8.087752342224121, 'metrics/data_secs_per_batch': 3.747336435317993, '_timestamp': 1740969640.2877927}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 179 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969640.288013}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 180 is less than current step: 499. Dropping entry: {'train/loss': 1.295419406890869, 'train/ce_loss': 0.4750244140625, 'train/seg_cls_loss': 0.007275390625, 'train/kl_loss': 0.18046875, 'train/mask_bce_loss': 0.06276018554344773, 'train/mask_dice_loss': 0.33664628565311433, 'train/mask_loss': 0.3994064748287201, 'metrics/total_secs_per_batch': 6.89332127571106, 'metrics/data_secs_per_batch': 3.2748043060302736, '_timestamp': 1740969647.1812475}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 180 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969647.1815188}).
Epoch: [5][182/500]	Time  6.220 ( 6.220)	Loss 1.3281 (1.3557)	CeLoss 1.3281 (0.5729)	SegCLSLoss 0.0000 (0.0098)	KLLoss 0.0000 (0.2174)	MaskLoss 0.0000 (0.3779)	MaskBCELoss 0.0000 (0.0496)	MaskDICELoss 0.0000 (0.3283)
Epoch: [5][183/500]	Time  7.793 ( 7.793)	Loss 1.2285 (1.5454)	CeLoss 0.3184 (0.5804)	SegCLSLoss 0.0090 (0.0113)	KLLoss 0.3633 (0.2549)	MaskLoss 0.4346 (0.4670)	MaskBCELoss 0.0665 (0.0897)	MaskDICELoss 0.3680 (0.3773)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 181 is less than current step: 499. Dropping entry: {'train/loss': 1.3556582152843475, 'train/ce_loss': 0.57294921875, 'train/seg_cls_loss': 0.009759521484375, 'train/kl_loss': 0.2173828125, 'train/mask_bce_loss': 0.04961069412529469, 'train/mask_dice_loss': 0.32831607609987257, 'train/mask_loss': 0.37792677283287046, 'metrics/total_secs_per_batch': 6.220272064208984, 'metrics/data_secs_per_batch': 2.742762017250061, '_timestamp': 1740969653.4014812}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 181 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969653.4017456}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 182 is less than current step: 499. Dropping entry: {'train/loss': 1.5454121947288513, 'train/ce_loss': 0.58037109375, 'train/seg_cls_loss': 0.011279296875, 'train/kl_loss': 0.2548828125, 'train/mask_bce_loss': 0.08970970287919044, 'train/mask_dice_loss': 0.37728348672389983, 'train/mask_loss': 0.4669931888580322, 'metrics/total_secs_per_batch': 7.793094635009766, 'metrics/data_secs_per_batch': 3.2828525066375733, '_timestamp': 1740969661.1946754}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 182 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969661.1950357}).
Epoch: [5][184/500]	Time  8.975 ( 8.975)	Loss 1.9506 (2.0503)	CeLoss 0.2129 (0.2452)	SegCLSLoss 0.0262 (0.0157)	KLLoss 0.3613 (0.3605)	MaskLoss 0.8444 (0.8804)	MaskBCELoss 0.1268 (0.1769)	MaskDICELoss 0.7177 (0.7035)
Epoch: [5][185/500]	Time  8.039 ( 8.039)	Loss 1.7656 (1.5046)	CeLoss 0.1865 (0.5597)	SegCLSLoss 0.0237 (0.0096)	KLLoss 0.3477 (0.2170)	MaskLoss 0.7666 (0.4592)	MaskBCELoss 0.0106 (0.1112)	MaskDICELoss 0.7560 (0.3480)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 183 is less than current step: 499. Dropping entry: {'train/loss': 2.050325095653534, 'train/ce_loss': 0.24521484375, 'train/seg_cls_loss': 0.015673828125, 'train/kl_loss': 0.360546875, 'train/mask_bce_loss': 0.17690791143104434, 'train/mask_dice_loss': 0.7035280644893647, 'train/mask_loss': 0.8804359763860703, 'metrics/total_secs_per_batch': 8.974631071090698, 'metrics/data_secs_per_batch': 4.182690882682801, '_timestamp': 1740969670.1692467}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 183 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969670.169545}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 184 is less than current step: 499. Dropping entry: {'train/loss': 1.5046475172042846, 'train/ce_loss': 0.559716796875, 'train/seg_cls_loss': 0.009576416015625, 'train/kl_loss': 0.2169921875, 'train/mask_bce_loss': 0.11122426316142082, 'train/mask_dice_loss': 0.3479598373174667, 'train/mask_loss': 0.45918411016464233, 'metrics/total_secs_per_batch': 8.039214611053467, 'metrics/data_secs_per_batch': 3.5109745502471923, '_timestamp': 1740969678.2086504}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 184 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969678.2089887}).
Epoch: [5][186/500]	Time  9.342 ( 9.342)	Loss 1.9593 (1.9139)	CeLoss 0.2031 (0.2279)	SegCLSLoss 0.0234 (0.0163)	KLLoss 0.3516 (0.3615)	MaskLoss 0.8546 (0.8209)	MaskBCELoss 0.0249 (0.1582)	MaskDICELoss 0.8297 (0.6628)
Epoch: [5][187/500]	Time  7.514 ( 7.514)	Loss 0.4648 (1.1335)	CeLoss 0.4648 (0.3719)	SegCLSLoss 0.0000 (0.0081)	KLLoss 0.0000 (0.1818)	MaskLoss 0.0000 (0.3696)	MaskBCELoss 0.0000 (0.0632)	MaskDICELoss 0.0000 (0.3065)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 185 is less than current step: 499. Dropping entry: {'train/loss': 1.9138526141643524, 'train/ce_loss': 0.2279296875, 'train/seg_cls_loss': 0.016265869140625, 'train/kl_loss': 0.3615234375, 'train/mask_bce_loss': 0.15818640273064374, 'train/mask_dice_loss': 0.6627535730600357, 'train/mask_loss': 0.8209399670362473, 'metrics/total_secs_per_batch': 9.342331171035767, 'metrics/data_secs_per_batch': 4.101294326782226, '_timestamp': 1740969687.5507996}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 185 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969687.551072}).
Epoch: [5][188/500]	Time  8.340 ( 8.340)	Loss 1.7668 (1.6600)	CeLoss 0.2832 (0.4720)	SegCLSLoss 0.0117 (0.0113)	KLLoss 0.3633 (0.2922)	MaskLoss 0.7203 (0.5765)	MaskBCELoss 0.1151 (0.1126)	MaskDICELoss 0.6052 (0.4639)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 186 is less than current step: 499. Dropping entry: {'train/loss': 1.1334864139556884, 'train/ce_loss': 0.3719482421875, 'train/seg_cls_loss': 0.00809326171875, 'train/kl_loss': 0.1818359375, 'train/mask_bce_loss': 0.06317627923563122, 'train/mask_dice_loss': 0.3064599961042404, 'train/mask_loss': 0.36963626742362976, 'metrics/total_secs_per_batch': 7.513868808746338, 'metrics/data_secs_per_batch': 3.439500117301941, '_timestamp': 1740969695.0646539}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 186 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969695.064931}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 187 is less than current step: 499. Dropping entry: {'train/loss': 1.659974455833435, 'train/ce_loss': 0.47197265625, 'train/seg_cls_loss': 0.011273193359375, 'train/kl_loss': 0.2921875, 'train/mask_bce_loss': 0.11258074678480626, 'train/mask_dice_loss': 0.46389085054397583, 'train/mask_loss': 0.5764716029167175, 'metrics/total_secs_per_batch': 8.3404221534729, 'metrics/data_secs_per_batch': 3.8012449741363525, '_timestamp': 1740969703.4051507}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 187 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969703.405433}).
Epoch: [5][189/500]	Time  8.172 ( 8.172)	Loss 1.3404 (1.4494)	CeLoss 0.2578 (0.4335)	SegCLSLoss 0.0118 (0.0120)	KLLoss 0.3633 (0.2926)	MaskLoss 0.5198 (0.4904)	MaskBCELoss 0.0410 (0.0861)	MaskDICELoss 0.4787 (0.4043)
[2025-03-02 20:41:59,182] [INFO] [logging.py:96:log_dist] [Rank 0] step=2690, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:41:59,187] [INFO] [timer.py:215:stop] epoch=0/micro_step=26900/global_step=2690, RunningAvgSamplesPerSec=1.4248984913827367, CurrSamplesPerSec=1.31406931629925, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [5][190/500]	Time  7.612 ( 7.612)	Loss 0.8088 (1.6338)	CeLoss 0.2793 (0.4920)	SegCLSLoss 0.0095 (0.0118)	KLLoss 0.3613 (0.2910)	MaskLoss 0.2443 (0.5533)	MaskBCELoss 0.0479 (0.1053)	MaskDICELoss 0.1964 (0.4480)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 188 is less than current step: 499. Dropping entry: {'train/loss': 1.4493951559066773, 'train/ce_loss': 0.43349609375, 'train/seg_cls_loss': 0.011993408203125, 'train/kl_loss': 0.292578125, 'train/mask_bce_loss': 0.08610735437832773, 'train/mask_dice_loss': 0.4043128788471222, 'train/mask_loss': 0.49042023420333863, 'metrics/total_secs_per_batch': 8.1715829372406, 'metrics/data_secs_per_batch': 3.540143918991089, '_timestamp': 1740969711.5766752}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 188 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969711.576962}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 189 is less than current step: 499. Dropping entry: {'train/loss': 1.6337676465511322, 'train/ce_loss': 0.4919921875, 'train/seg_cls_loss': 0.011810302734375, 'train/kl_loss': 0.291015625, 'train/mask_bce_loss': 0.10533117894083262, 'train/mask_dice_loss': 0.44797841310501096, 'train/mask_loss': 0.5533095940947532, 'metrics/total_secs_per_batch': 7.611563444137573, 'metrics/data_secs_per_batch': 3.187512516975403, '_timestamp': 1740969719.188099}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 189 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969719.1884115}).
Epoch: [5][191/500]	Time  7.715 ( 7.715)	Loss 2.6455 (1.5916)	CeLoss 0.1709 (0.2647)	SegCLSLoss 0.0204 (0.0112)	KLLoss 0.3711 (0.2592)	MaskLoss 1.2138 (0.6477)	MaskBCELoss 0.3670 (0.1709)	MaskDICELoss 0.8469 (0.4769)
Epoch: [5][192/500]	Time  8.012 ( 8.012)	Loss 0.0654 (1.3712)	CeLoss 0.0654 (0.4384)	SegCLSLoss 0.0000 (0.0099)	KLLoss 0.0000 (0.2174)	MaskLoss 0.0000 (0.4531)	MaskBCELoss 0.0000 (0.0953)	MaskDICELoss 0.0000 (0.3578)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 190 is less than current step: 499. Dropping entry: {'train/loss': 1.5916216135025025, 'train/ce_loss': 0.264697265625, 'train/seg_cls_loss': 0.011151123046875, 'train/kl_loss': 0.2591796875, 'train/mask_bce_loss': 0.1708613596856594, 'train/mask_dice_loss': 0.4768537521362305, 'train/mask_loss': 0.6477151155471802, 'metrics/total_secs_per_batch': 7.714966535568237, 'metrics/data_secs_per_batch': 3.087656307220459, '_timestamp': 1740969726.90328}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 190 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969726.903655}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 191 is less than current step: 499. Dropping entry: {'train/loss': 1.3712090253829956, 'train/ce_loss': 0.43837890625, 'train/seg_cls_loss': 0.00992431640625, 'train/kl_loss': 0.2173828125, 'train/mask_bce_loss': 0.09533534944057465, 'train/mask_dice_loss': 0.3577984690666199, 'train/mask_loss': 0.4531338155269623, 'metrics/total_secs_per_batch': 8.01155948638916, 'metrics/data_secs_per_batch': 3.3572067499160765, '_timestamp': 1740969734.9147646}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 191 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969734.9150362}).
Epoch: [5][193/500]	Time  8.676 ( 8.676)	Loss 2.3860 (1.7907)	CeLoss 0.2314 (0.2863)	SegCLSLoss 0.0171 (0.0153)	KLLoss 0.3652 (0.3250)	MaskLoss 1.0543 (0.7321)	MaskBCELoss 0.2863 (0.1584)	MaskDICELoss 0.7680 (0.5737)
Epoch: [5][194/500]	Time  7.657 ( 7.657)	Loss 2.6460 (1.9591)	CeLoss 0.2148 (0.4612)	SegCLSLoss 0.0171 (0.0137)	KLLoss 0.3594 (0.2924)	MaskLoss 1.1931 (0.7308)	MaskBCELoss 0.3776 (0.1810)	MaskDICELoss 0.8155 (0.5498)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 192 is less than current step: 499. Dropping entry: {'train/loss': 1.7907252073287965, 'train/ce_loss': 0.286328125, 'train/seg_cls_loss': 0.015313720703125, 'train/kl_loss': 0.325, 'train/mask_bce_loss': 0.15844484232366085, 'train/mask_dice_loss': 0.5736853271722794, 'train/mask_loss': 0.7321301698684692, 'metrics/total_secs_per_batch': 8.675756692886353, 'metrics/data_secs_per_batch': 3.644352173805237, '_timestamp': 1740969743.5905216}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 192 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969743.590719}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 193 is less than current step: 499. Dropping entry: {'train/loss': 1.9591121554374695, 'train/ce_loss': 0.46123046875, 'train/seg_cls_loss': 0.01370849609375, 'train/kl_loss': 0.2923828125, 'train/mask_bce_loss': 0.1809808738529682, 'train/mask_dice_loss': 0.5498447269201279, 'train/mask_loss': 0.7308256030082703, 'metrics/total_secs_per_batch': 7.6573145389556885, 'metrics/data_secs_per_batch': 3.3830491065979005, '_timestamp': 1740969751.2478592}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 193 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969751.2481418}).
Epoch: [5][195/500]	Time  8.191 ( 8.191)	Loss 1.0625 (1.5468)	CeLoss 1.0625 (0.4017)	SegCLSLoss 0.0000 (0.0133)	KLLoss 0.0000 (0.2938)	MaskLoss 0.0000 (0.5545)	MaskBCELoss 0.0000 (0.1294)	MaskDICELoss 0.0000 (0.4251)
Epoch: [5][196/500]	Time  8.528 ( 8.528)	Loss 1.4313 (1.5096)	CeLoss 0.2178 (0.4165)	SegCLSLoss 0.0226 (0.0135)	KLLoss 0.3594 (0.2889)	MaskLoss 0.5829 (0.5287)	MaskBCELoss 0.0339 (0.1238)	MaskDICELoss 0.5490 (0.4050)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 194 is less than current step: 499. Dropping entry: {'train/loss': 1.546784019470215, 'train/ce_loss': 0.40166015625, 'train/seg_cls_loss': 0.0132568359375, 'train/kl_loss': 0.29375, 'train/mask_bce_loss': 0.12936242315918206, 'train/mask_dice_loss': 0.42513310015201566, 'train/mask_loss': 0.5544955253601074, 'metrics/total_secs_per_batch': 8.191358089447021, 'metrics/data_secs_per_batch': 3.825581192970276, '_timestamp': 1740969759.4393454}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 194 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969759.4395704}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 195 is less than current step: 499. Dropping entry: {'train/loss': 1.5096330046653748, 'train/ce_loss': 0.41650390625, 'train/seg_cls_loss': 0.013525390625, 'train/kl_loss': 0.2888671875, 'train/mask_bce_loss': 0.12376580499112606, 'train/mask_dice_loss': 0.4049764767289162, 'train/mask_loss': 0.5287422761321068, 'metrics/total_secs_per_batch': 8.527737379074097, 'metrics/data_secs_per_batch': 3.495429182052612, '_timestamp': 1740969767.966984}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 195 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969767.9672668}).
Epoch: [5][197/500]	Time  8.074 ( 8.074)	Loss 0.9239 (1.7861)	CeLoss 0.1641 (0.3919)	SegCLSLoss 0.0288 (0.0152)	KLLoss 0.3633 (0.3254)	MaskLoss 0.3545 (0.6773)	MaskBCELoss 0.0733 (0.0912)	MaskDICELoss 0.2812 (0.5860)
Epoch: [5][198/500]	Time  7.673 ( 7.673)	Loss 2.0508 (1.8845)	CeLoss 0.2324 (0.4498)	SegCLSLoss 0.0137 (0.0153)	KLLoss 0.3730 (0.2955)	MaskLoss 0.8867 (0.6987)	MaskBCELoss 0.0097 (0.1251)	MaskDICELoss 0.8770 (0.5736)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 196 is less than current step: 499. Dropping entry: {'train/loss': 1.7861430823802948, 'train/ce_loss': 0.39189453125, 'train/seg_cls_loss': 0.01519775390625, 'train/kl_loss': 0.325390625, 'train/mask_bce_loss': 0.09122249647043645, 'train/mask_dice_loss': 0.586028727889061, 'train/mask_loss': 0.677251210808754, 'metrics/total_secs_per_batch': 8.073826551437378, 'metrics/data_secs_per_batch': 3.6259074926376345, '_timestamp': 1740969776.0409985}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 196 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969776.0413525}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 197 is less than current step: 499. Dropping entry: {'train/loss': 1.8845019817352295, 'train/ce_loss': 0.4498046875, 'train/seg_cls_loss': 0.015264892578125, 'train/kl_loss': 0.2955078125, 'train/mask_bce_loss': 0.12513212198391557, 'train/mask_dice_loss': 0.5736130148172378, 'train/mask_loss': 0.6987451434135437, 'metrics/total_secs_per_batch': 7.673327922821045, 'metrics/data_secs_per_batch': 3.3146453619003298, '_timestamp': 1740969783.7141404}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 197 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969783.714414}).
Epoch: [5][199/500]	Time  7.914 ( 7.914)	Loss 1.2518 (1.5013)	CeLoss 0.2832 (0.5225)	SegCLSLoss 0.0117 (0.0112)	KLLoss 0.3652 (0.2525)	MaskLoss 0.4628 (0.4739)	MaskBCELoss 0.1213 (0.0620)	MaskDICELoss 0.3416 (0.4118)
[2025-03-02 20:43:18,609] [INFO] [logging.py:96:log_dist] [Rank 0] step=2700, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:43:18,615] [INFO] [timer.py:215:stop] epoch=0/micro_step=27000/global_step=2700, RunningAvgSamplesPerSec=1.4242038833255237, CurrSamplesPerSec=1.4314466443387641, MemAllocated=30.91GB, MaxMemAllocated=37.23GB
Epoch: [5][200/500]	Time  6.987 ( 6.987)	Loss 0.8984 (1.7266)	CeLoss 0.8984 (0.5311)	SegCLSLoss 0.0000 (0.0128)	KLLoss 0.0000 (0.2529)	MaskLoss 0.0000 (0.5818)	MaskBCELoss 0.0000 (0.1137)	MaskDICELoss 0.0000 (0.4681)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 198 is less than current step: 499. Dropping entry: {'train/loss': 1.5013255953788758, 'train/ce_loss': 0.5224609375, 'train/seg_cls_loss': 0.011248779296875, 'train/kl_loss': 0.2525390625, 'train/mask_bce_loss': 0.062020196206867696, 'train/mask_dice_loss': 0.41183596253395083, 'train/mask_loss': 0.47385615706443784, 'metrics/total_secs_per_batch': 7.914299488067627, 'metrics/data_secs_per_batch': 3.1143343448638916, '_timestamp': 1740969791.628402}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 198 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969791.62869}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 199 is less than current step: 499. Dropping entry: {'train/loss': 1.726551628112793, 'train/ce_loss': 0.531103515625, 'train/seg_cls_loss': 0.01279296875, 'train/kl_loss': 0.2529296875, 'train/mask_bce_loss': 0.11373192723840475, 'train/mask_dice_loss': 0.4680985748767853, 'train/mask_loss': 0.5818305015563965, 'metrics/total_secs_per_batch': 6.987492084503174, 'metrics/data_secs_per_batch': 3.3588804721832277, '_timestamp': 1740969798.6158023}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 199 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969798.6161366}).
Epoch: [5][201/500]	Time  7.428 ( 7.428)	Loss 1.8028 (1.5623)	CeLoss 0.2305 (0.3684)	SegCLSLoss 0.0120 (0.0150)	KLLoss 0.3652 (0.2910)	MaskLoss 0.7647 (0.5786)	MaskBCELoss 0.2491 (0.1564)	MaskDICELoss 0.5156 (0.4222)
Epoch: [5][202/500]	Time  7.544 ( 7.544)	Loss 2.2260 (2.0729)	CeLoss 0.1875 (0.4086)	SegCLSLoss 0.0216 (0.0153)	KLLoss 0.3594 (0.2924)	MaskLoss 0.9958 (0.8137)	MaskBCELoss 0.1057 (0.2035)	MaskDICELoss 0.8901 (0.6102)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 200 is less than current step: 499. Dropping entry: {'train/loss': 1.562285029888153, 'train/ce_loss': 0.368359375, 'train/seg_cls_loss': 0.014959716796875, 'train/kl_loss': 0.291015625, 'train/mask_bce_loss': 0.1564066119492054, 'train/mask_dice_loss': 0.422196826338768, 'train/mask_loss': 0.5786034405231476, 'metrics/total_secs_per_batch': 7.427632093429565, 'metrics/data_secs_per_batch': 3.256286144256592, '_timestamp': 1740969806.0435476}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 200 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969806.0438244}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 201 is less than current step: 499. Dropping entry: {'train/loss': 2.0728946685791017, 'train/ce_loss': 0.40859375, 'train/seg_cls_loss': 0.015277099609375, 'train/kl_loss': 0.2923828125, 'train/mask_bce_loss': 0.20352985383942723, 'train/mask_dice_loss': 0.6102124035358429, 'train/mask_loss': 0.8137422502040863, 'metrics/total_secs_per_batch': 7.544011354446411, 'metrics/data_secs_per_batch': 3.4753350973129273, '_timestamp': 1740969813.587564}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 201 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969813.5878422}).
Epoch: [5][203/500]	Time  8.331 ( 8.331)	Loss 1.9177 (2.0741)	CeLoss 0.1748 (0.2069)	SegCLSLoss 0.0222 (0.0201)	KLLoss 0.3516 (0.3627)	MaskLoss 0.8485 (0.9106)	MaskBCELoss 0.1142 (0.1737)	MaskDICELoss 0.7344 (0.7368)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 202 is less than current step: 499. Dropping entry: {'train/loss': 2.074091303348541, 'train/ce_loss': 0.20693359375, 'train/seg_cls_loss': 0.02005615234375, 'train/kl_loss': 0.3626953125, 'train/mask_bce_loss': 0.17374695809558033, 'train/mask_dice_loss': 0.7368338525295257, 'train/mask_loss': 0.9105808198451996, 'metrics/total_secs_per_batch': 8.330824851989746, 'metrics/data_secs_per_batch': 3.4193379402160646, '_timestamp': 1740969821.918584}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 202 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969821.9189305}).
Epoch: [5][204/500]	Time  8.901 ( 8.901)	Loss 1.4913 (1.6097)	CeLoss 0.1875 (0.2645)	SegCLSLoss 0.0205 (0.0149)	KLLoss 0.3809 (0.3281)	MaskLoss 0.6280 (0.6524)	MaskBCELoss 0.0546 (0.1341)	MaskDICELoss 0.5734 (0.5183)
Epoch: [5][205/500]	Time  8.287 ( 8.287)	Loss 0.5273 (1.2768)	CeLoss 0.5273 (0.3275)	SegCLSLoss 0.0000 (0.0101)	KLLoss 0.0000 (0.2568)	MaskLoss 0.0000 (0.4593)	MaskBCELoss 0.0000 (0.1246)	MaskDICELoss 0.0000 (0.3347)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 203 is less than current step: 499. Dropping entry: {'train/loss': 1.6096934795379638, 'train/ce_loss': 0.264453125, 'train/seg_cls_loss': 0.014935302734375, 'train/kl_loss': 0.328125, 'train/mask_bce_loss': 0.13411232121288777, 'train/mask_dice_loss': 0.5182930126786232, 'train/mask_loss': 0.6524053320288659, 'metrics/total_secs_per_batch': 8.901181697845459, 'metrics/data_secs_per_batch': 4.167762017250061, '_timestamp': 1740969830.8195884}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 203 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969830.8199008}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 204 is less than current step: 499. Dropping entry: {'train/loss': 1.2768394947052002, 'train/ce_loss': 0.3275390625, 'train/seg_cls_loss': 0.01005859375, 'train/kl_loss': 0.2568359375, 'train/mask_bce_loss': 0.12461434216238558, 'train/mask_dice_loss': 0.3346550136804581, 'train/mask_loss': 0.4592693611979485, 'metrics/total_secs_per_batch': 8.286999225616455, 'metrics/data_secs_per_batch': 4.264831972122193, '_timestamp': 1740969839.106629}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 204 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969839.106918}).
Epoch: [5][206/500]	Time  7.484 ( 7.484)	Loss 2.1302 (1.9349)	CeLoss 0.1963 (0.3147)	SegCLSLoss 0.0183 (0.0159)	KLLoss 0.3633 (0.3262)	MaskLoss 0.9445 (0.7898)	MaskBCELoss 0.0063 (0.1423)	MaskDICELoss 0.9382 (0.6475)
Epoch: [5][207/500]	Time  7.492 ( 7.492)	Loss 2.1544 (1.8674)	CeLoss 0.2139 (0.5396)	SegCLSLoss 0.0225 (0.0123)	KLLoss 0.3594 (0.2553)	MaskLoss 0.9473 (0.6481)	MaskBCELoss 0.2099 (0.1459)	MaskDICELoss 0.7374 (0.5022)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 205 is less than current step: 499. Dropping entry: {'train/loss': 1.9349205017089843, 'train/ce_loss': 0.31474609375, 'train/seg_cls_loss': 0.015948486328125, 'train/kl_loss': 0.326171875, 'train/mask_bce_loss': 0.14230797705240547, 'train/mask_dice_loss': 0.6475155591964722, 'train/mask_loss': 0.7898235321044922, 'metrics/total_secs_per_batch': 7.4839396476745605, 'metrics/data_secs_per_batch': 3.4361459970474244, '_timestamp': 1740969846.590506}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 205 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969846.5907743}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 206 is less than current step: 499. Dropping entry: {'train/loss': 1.8674372792243958, 'train/ce_loss': 0.5396484375, 'train/seg_cls_loss': 0.012286376953125, 'train/kl_loss': 0.2552734375, 'train/mask_bce_loss': 0.14593411199748516, 'train/mask_dice_loss': 0.5021888136863708, 'train/mask_loss': 0.6481229186058044, 'metrics/total_secs_per_batch': 7.491516590118408, 'metrics/data_secs_per_batch': 3.1967325687408445, '_timestamp': 1740969854.0820293}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 206 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969854.0823007}).
Epoch: [5][208/500]	Time  7.653 ( 7.653)	Loss 1.3918 (1.6842)	CeLoss 0.2520 (0.5080)	SegCLSLoss 0.0111 (0.0106)	KLLoss 0.3633 (0.2588)	MaskLoss 0.5484 (0.5725)	MaskBCELoss 0.1641 (0.1049)	MaskDICELoss 0.3843 (0.4676)
Epoch: [5][209/500]	Time  7.181 ( 7.181)	Loss 2.6147 (1.7229)	CeLoss 0.1826 (0.3151)	SegCLSLoss 0.0334 (0.0150)	KLLoss 0.3789 (0.2887)	MaskLoss 1.1887 (0.6857)	MaskBCELoss 0.2933 (0.1043)	MaskDICELoss 0.8954 (0.5815)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 207 is less than current step: 499. Dropping entry: {'train/loss': 1.6842399001121522, 'train/ce_loss': 0.5080078125, 'train/seg_cls_loss': 0.01060791015625, 'train/kl_loss': 0.2587890625, 'train/mask_bce_loss': 0.10491738808341325, 'train/mask_dice_loss': 0.46757366359233854, 'train/mask_loss': 0.5724910467863082, 'metrics/total_secs_per_batch': 7.652658224105835, 'metrics/data_secs_per_batch': 3.111324596405029, '_timestamp': 1740969861.73477}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 207 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969861.7350981}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 208 is less than current step: 499. Dropping entry: {'train/loss': 1.722942066192627, 'train/ce_loss': 0.31513671875, 'train/seg_cls_loss': 0.0150390625, 'train/kl_loss': 0.288671875, 'train/mask_bce_loss': 0.10426735430955887, 'train/mask_dice_loss': 0.5814712405204773, 'train/mask_loss': 0.6857385993003845, 'metrics/total_secs_per_batch': 7.18100905418396, 'metrics/data_secs_per_batch': 3.040812301635742, '_timestamp': 1740969868.915724}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 208 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969868.916008}).
[2025-03-02 20:44:36,096] [INFO] [logging.py:96:log_dist] [Rank 0] step=2710, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:44:36,102] [INFO] [timer.py:215:stop] epoch=0/micro_step=27100/global_step=2710, RunningAvgSamplesPerSec=1.423660277521048, CurrSamplesPerSec=1.3916146463693666, MemAllocated=31.26GB, MaxMemAllocated=37.23GB
Epoch: [5][210/500]	Time  7.187 ( 7.187)	Loss 2.2572 (1.3464)	CeLoss 0.2207 (0.6251)	SegCLSLoss 0.0161 (0.0077)	KLLoss 0.3574 (0.1797)	MaskLoss 0.9968 (0.3498)	MaskBCELoss 0.0163 (0.0654)	MaskDICELoss 0.9805 (0.2845)
Epoch: [5][211/500]	Time  8.468 ( 8.468)	Loss 2.5047 (1.8124)	CeLoss 0.1826 (0.2154)	SegCLSLoss 0.0197 (0.0206)	KLLoss 0.3652 (0.3619)	MaskLoss 1.1381 (0.7752)	MaskBCELoss 0.2858 (0.1225)	MaskDICELoss 0.8523 (0.6526)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 209 is less than current step: 499. Dropping entry: {'train/loss': 1.3463746786117554, 'train/ce_loss': 0.62509765625, 'train/seg_cls_loss': 0.007733154296875, 'train/kl_loss': 0.1796875, 'train/mask_bce_loss': 0.06538669690489769, 'train/mask_dice_loss': 0.2844607949256897, 'train/mask_loss': 0.3498474955558777, 'metrics/total_secs_per_batch': 7.18743109703064, 'metrics/data_secs_per_batch': 3.0996506214141846, '_timestamp': 1740969876.102954}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 209 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969876.1032276}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 210 is less than current step: 499. Dropping entry: {'train/loss': 1.8123947978019714, 'train/ce_loss': 0.2154296875, 'train/seg_cls_loss': 0.02059326171875, 'train/kl_loss': 0.3619140625, 'train/mask_bce_loss': 0.12254867628216744, 'train/mask_dice_loss': 0.6526428461074829, 'train/mask_loss': 0.775191530585289, 'metrics/total_secs_per_batch': 8.467719078063965, 'metrics/data_secs_per_batch': 3.6052156686782837, '_timestamp': 1740969884.5708904}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 210 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969884.571182}).
Epoch: [5][212/500]	Time  7.083 ( 7.083)	Loss 2.3691 (1.4043)	CeLoss 0.2432 (0.3500)	SegCLSLoss 0.0114 (0.0113)	KLLoss 0.3691 (0.2555)	MaskLoss 1.0420 (0.5116)	MaskBCELoss 0.0426 (0.0445)	MaskDICELoss 0.9993 (0.4671)
Epoch: [5][213/500]	Time  8.325 ( 8.325)	Loss 1.3047 (1.7405)	CeLoss 1.3047 (0.3573)	SegCLSLoss 0.0000 (0.0112)	KLLoss 0.0000 (0.2914)	MaskLoss 0.0000 (0.6742)	MaskBCELoss 0.0000 (0.1513)	MaskDICELoss 0.0000 (0.5229)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 211 is less than current step: 499. Dropping entry: {'train/loss': 1.4042877376079559, 'train/ce_loss': 0.35, 'train/seg_cls_loss': 0.011297607421875, 'train/kl_loss': 0.25546875, 'train/mask_bce_loss': 0.04449949455447495, 'train/mask_dice_loss': 0.467068187892437, 'train/mask_loss': 0.5115676820278168, 'metrics/total_secs_per_batch': 7.082743883132935, 'metrics/data_secs_per_batch': 3.01387300491333, '_timestamp': 1740969891.6536298}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 211 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969891.6539996}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 212 is less than current step: 499. Dropping entry: {'train/loss': 1.7405012488365172, 'train/ce_loss': 0.35732421875, 'train/seg_cls_loss': 0.01119384765625, 'train/kl_loss': 0.29140625, 'train/mask_bce_loss': 0.1512712544761598, 'train/mask_dice_loss': 0.5229344487190246, 'train/mask_loss': 0.6742057085037232, 'metrics/total_secs_per_batch': 8.325273513793945, 'metrics/data_secs_per_batch': 3.379291129112244, '_timestamp': 1740969899.9788682}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 212 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969899.9791453}).
Epoch: [5][214/500]	Time  6.926 ( 6.926)	Loss 1.2282 (1.6789)	CeLoss 0.2256 (0.4772)	SegCLSLoss 0.0205 (0.0159)	KLLoss 0.3477 (0.2879)	MaskLoss 0.4783 (0.5823)	MaskBCELoss 0.0505 (0.1093)	MaskDICELoss 0.4279 (0.4729)
Epoch: [5][215/500]	Time  7.377 ( 7.377)	Loss 1.2195 (1.3269)	CeLoss 0.2441 (0.4422)	SegCLSLoss 0.0176 (0.0102)	KLLoss 0.3672 (0.2154)	MaskLoss 0.4642 (0.4289)	MaskBCELoss 0.0836 (0.0730)	MaskDICELoss 0.3806 (0.3558)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 213 is less than current step: 499. Dropping entry: {'train/loss': 1.6788725972175598, 'train/ce_loss': 0.47724609375, 'train/seg_cls_loss': 0.015887451171875, 'train/kl_loss': 0.287890625, 'train/mask_bce_loss': 0.10933681426104158, 'train/mask_dice_loss': 0.4729217439889908, 'train/mask_loss': 0.5822585701942444, 'metrics/total_secs_per_batch': 6.926301002502441, 'metrics/data_secs_per_batch': 3.0743024587631225, '_timestamp': 1740969906.9051838}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 213 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969906.9054954}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 214 is less than current step: 499. Dropping entry: {'train/loss': 1.3269010066986084, 'train/ce_loss': 0.4421875, 'train/seg_cls_loss': 0.010150146484375, 'train/kl_loss': 0.2154296875, 'train/mask_bce_loss': 0.07304159458726645, 'train/mask_dice_loss': 0.35583860278129575, 'train/mask_loss': 0.42888018786907195, 'metrics/total_secs_per_batch': 7.377370834350586, 'metrics/data_secs_per_batch': 3.2464609384536742, '_timestamp': 1740969914.2825778}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 214 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969914.2828567}).
Epoch: [5][216/500]	Time  8.628 ( 8.628)	Loss 1.9065 (1.9701)	CeLoss 0.2295 (0.2918)	SegCLSLoss 0.0110 (0.0171)	KLLoss 0.3633 (0.3279)	MaskLoss 0.8175 (0.8185)	MaskBCELoss 0.1633 (0.1625)	MaskDICELoss 0.6542 (0.6559)
Epoch: [5][217/500]	Time  7.316 ( 7.316)	Loss 2.3359 (1.1901)	CeLoss 0.2656 (0.3962)	SegCLSLoss 0.0114 (0.0071)	KLLoss 0.3613 (0.2170)	MaskLoss 1.0136 (0.3842)	MaskBCELoss 0.0184 (0.0476)	MaskDICELoss 0.9952 (0.3366)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 215 is less than current step: 499. Dropping entry: {'train/loss': 1.9700614929199218, 'train/ce_loss': 0.291796875, 'train/seg_cls_loss': 0.017144775390625, 'train/kl_loss': 0.3279296875, 'train/mask_bce_loss': 0.16253717429935932, 'train/mask_dice_loss': 0.6559408456087112, 'train/mask_loss': 0.8184780240058899, 'metrics/total_secs_per_batch': 8.62807583808899, 'metrics/data_secs_per_batch': 3.6155205726623536, '_timestamp': 1740969922.9107509}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 215 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969922.9110923}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 216 is less than current step: 499. Dropping entry: {'train/loss': 1.1901057958602905, 'train/ce_loss': 0.3962158203125, 'train/seg_cls_loss': 0.00709228515625, 'train/kl_loss': 0.2169921875, 'train/mask_bce_loss': 0.04763473849743605, 'train/mask_dice_loss': 0.33661492764949796, 'train/mask_loss': 0.3842496633529663, 'metrics/total_secs_per_batch': 7.316076755523682, 'metrics/data_secs_per_batch': 3.256357479095459, '_timestamp': 1740969930.2267025}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 216 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969930.227003}).
Epoch: [5][218/500]	Time  7.569 ( 7.569)	Loss 2.1586 (1.3170)	CeLoss 0.2383 (0.3541)	SegCLSLoss 0.0106 (0.0099)	KLLoss 0.3574 (0.2908)	MaskLoss 0.9396 (0.4644)	MaskBCELoss 0.3518 (0.1139)	MaskDICELoss 0.5878 (0.3505)
Epoch: [5][219/500]	Time  6.513 ( 6.513)	Loss 1.3438 (1.7245)	CeLoss 1.3438 (0.5029)	SegCLSLoss 0.0000 (0.0106)	KLLoss 0.0000 (0.2227)	MaskLoss 0.0000 (0.5970)	MaskBCELoss 0.0000 (0.1140)	MaskDICELoss 0.0000 (0.4829)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 217 is less than current step: 499. Dropping entry: {'train/loss': 1.3169775903224945, 'train/ce_loss': 0.3541015625, 'train/seg_cls_loss': 0.00986328125, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.11390257515013218, 'train/mask_dice_loss': 0.35054325461387636, 'train/mask_loss': 0.46444582641124726, 'metrics/total_secs_per_batch': 7.56919527053833, 'metrics/data_secs_per_batch': 3.814500427246094, '_timestamp': 1740969937.7959523}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 217 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969937.7962353}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 218 is less than current step: 499. Dropping entry: {'train/loss': 1.7244678139686584, 'train/ce_loss': 0.5029296875, 'train/seg_cls_loss': 0.01060791015625, 'train/kl_loss': 0.22265625, 'train/mask_bce_loss': 0.11400343030691147, 'train/mask_dice_loss': 0.4829472780227661, 'train/mask_loss': 0.5969507157802582, 'metrics/total_secs_per_batch': 6.51276969909668, 'metrics/data_secs_per_batch': 2.8567222118377686, '_timestamp': 1740969944.3086932}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 218 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969944.308965}).
[2025-03-02 20:45:51,769] [INFO] [logging.py:96:log_dist] [Rank 0] step=2720, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:45:51,776] [INFO] [timer.py:215:stop] epoch=0/micro_step=27200/global_step=2720, RunningAvgSamplesPerSec=1.4232562083976248, CurrSamplesPerSec=1.3392996928231926, MemAllocated=30.95GB, MaxMemAllocated=37.23GB
Epoch: [5][220/500]	Time  7.468 ( 7.468)	Loss 1.8553 (1.4611)	CeLoss 0.1934 (0.3013)	SegCLSLoss 0.0197 (0.0137)	KLLoss 0.3691 (0.2912)	MaskLoss 0.8075 (0.5618)	MaskBCELoss 0.2968 (0.1195)	MaskDICELoss 0.5107 (0.4423)
Epoch: [5][221/500]	Time  7.688 ( 7.688)	Loss 0.9840 (1.2421)	CeLoss 0.2852 (0.2764)	SegCLSLoss 0.0106 (0.0095)	KLLoss 0.3633 (0.2533)	MaskLoss 0.3279 (0.4678)	MaskBCELoss 0.0970 (0.0676)	MaskDICELoss 0.2309 (0.4002)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 219 is less than current step: 499. Dropping entry: {'train/loss': 1.4610891222953797, 'train/ce_loss': 0.30126953125, 'train/seg_cls_loss': 0.01368408203125, 'train/kl_loss': 0.2912109375, 'train/mask_bce_loss': 0.11946799317374826, 'train/mask_dice_loss': 0.4423265755176544, 'train/mask_loss': 0.5617945611476898, 'metrics/total_secs_per_batch': 7.468118190765381, 'metrics/data_secs_per_batch': 3.6315783262252808, '_timestamp': 1740969951.7766302}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 219 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969951.7768953}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 220 is less than current step: 499. Dropping entry: {'train/loss': 1.2421288549900056, 'train/ce_loss': 0.2763671875, 'train/seg_cls_loss': 0.009478759765625, 'train/kl_loss': 0.2533203125, 'train/mask_bce_loss': 0.06760149998590351, 'train/mask_dice_loss': 0.4002402797341347, 'train/mask_loss': 0.46784177124500276, 'metrics/total_secs_per_batch': 7.688267707824707, 'metrics/data_secs_per_batch': 3.4015241861343384, '_timestamp': 1740969959.465108}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 220 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969959.4653711}).
Epoch: [5][222/500]	Time  7.638 ( 7.638)	Loss 2.0256 (1.5836)	CeLoss 0.2656 (0.3491)	SegCLSLoss 0.0139 (0.0096)	KLLoss 0.3633 (0.2910)	MaskLoss 0.8575 (0.6002)	MaskBCELoss 0.1286 (0.1837)	MaskDICELoss 0.7289 (0.4165)
Epoch: [5][223/500]	Time  6.045 ( 6.045)	Loss 2.4780 (1.4841)	CeLoss 0.2988 (0.6843)	SegCLSLoss 0.0139 (0.0075)	KLLoss 0.3613 (0.1805)	MaskLoss 1.0671 (0.3888)	MaskBCELoss 0.2589 (0.1279)	MaskDICELoss 0.8082 (0.2609)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 221 is less than current step: 499. Dropping entry: {'train/loss': 1.5836232423782348, 'train/ce_loss': 0.34912109375, 'train/seg_cls_loss': 0.009637451171875, 'train/kl_loss': 0.291015625, 'train/mask_bce_loss': 0.18368869237601756, 'train/mask_dice_loss': 0.41647253185510635, 'train/mask_loss': 0.600161224603653, 'metrics/total_secs_per_batch': 7.637695789337158, 'metrics/data_secs_per_batch': 3.6912891149520872, '_timestamp': 1740969967.1027894}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 221 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969967.103084}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 222 is less than current step: 499. Dropping entry: {'train/loss': 1.484080618619919, 'train/ce_loss': 0.68427734375, 'train/seg_cls_loss': 0.007452392578125, 'train/kl_loss': 0.18046875, 'train/mask_bce_loss': 0.12788788825273514, 'train/mask_dice_loss': 0.2609297662973404, 'train/mask_loss': 0.38881764709949496, 'metrics/total_secs_per_batch': 6.044583320617676, 'metrics/data_secs_per_batch': 2.5660266637802125, '_timestamp': 1740969973.1473622}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 222 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969973.1476746}).
Epoch: [5][224/500]	Time  7.439 ( 7.439)	Loss 2.4449 (1.7993)	CeLoss 0.1748 (0.2163)	SegCLSLoss 0.0204 (0.0203)	KLLoss 0.3672 (0.3646)	MaskLoss 1.1121 (0.7681)	MaskBCELoss 0.3264 (0.1394)	MaskDICELoss 0.7857 (0.6287)
Epoch: [5][225/500]	Time  7.604 ( 7.604)	Loss 1.0678 (1.6626)	CeLoss 0.2949 (0.3859)	SegCLSLoss 0.0120 (0.0134)	KLLoss 0.3613 (0.2555)	MaskLoss 0.3659 (0.6223)	MaskBCELoss 0.0568 (0.1176)	MaskDICELoss 0.3091 (0.5047)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 223 is less than current step: 499. Dropping entry: {'train/loss': 1.7993352949619292, 'train/ce_loss': 0.21630859375, 'train/seg_cls_loss': 0.0202880859375, 'train/kl_loss': 0.3646484375, 'train/mask_bce_loss': 0.13937153154984117, 'train/mask_dice_loss': 0.628704309463501, 'train/mask_loss': 0.7680758506059646, 'metrics/total_secs_per_batch': 7.4394824504852295, 'metrics/data_secs_per_batch': 3.4509702920913696, '_timestamp': 1740969980.5869052}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 223 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969980.5872247}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 224 is less than current step: 499. Dropping entry: {'train/loss': 1.6626230716705321, 'train/ce_loss': 0.3859375, 'train/seg_cls_loss': 0.01337890625, 'train/kl_loss': 0.25546875, 'train/mask_bce_loss': 0.11762641873210669, 'train/mask_dice_loss': 0.5047007441520691, 'train/mask_loss': 0.6223271518945694, 'metrics/total_secs_per_batch': 7.603532314300537, 'metrics/data_secs_per_batch': 3.790739727020264, '_timestamp': 1740969988.190443}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 224 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969988.1908095}).
Epoch: [5][226/500]	Time  7.764 ( 7.764)	Loss 2.4103 (1.5209)	CeLoss 0.2344 (0.4825)	SegCLSLoss 0.0199 (0.0127)	KLLoss 0.3633 (0.2176)	MaskLoss 1.0645 (0.5050)	MaskBCELoss 0.3790 (0.1181)	MaskDICELoss 0.6855 (0.3870)
Epoch: [5][227/500]	Time  8.324 ( 8.324)	Loss 0.9430 (1.5630)	CeLoss 0.2344 (0.5006)	SegCLSLoss 0.0097 (0.0118)	KLLoss 0.3633 (0.2908)	MaskLoss 0.3338 (0.5137)	MaskBCELoss 0.0826 (0.0741)	MaskDICELoss 0.2512 (0.4396)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 225 is less than current step: 499. Dropping entry: {'train/loss': 1.5209378004074097, 'train/ce_loss': 0.48251953125, 'train/seg_cls_loss': 0.012744140625, 'train/kl_loss': 0.217578125, 'train/mask_bce_loss': 0.11805246807634831, 'train/mask_dice_loss': 0.3869965195655823, 'train/mask_loss': 0.5050489842891693, 'metrics/total_secs_per_batch': 7.7637317180633545, 'metrics/data_secs_per_batch': 3.6682190895080566, '_timestamp': 1740969995.9541166}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 225 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740969995.9543624}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 226 is less than current step: 499. Dropping entry: {'train/loss': 1.5629818797111512, 'train/ce_loss': 0.5005859375, 'train/seg_cls_loss': 0.01180419921875, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.07411899352446198, 'train/mask_dice_loss': 0.4395985096693039, 'train/mask_loss': 0.51371750831604, 'metrics/total_secs_per_batch': 8.324379205703735, 'metrics/data_secs_per_batch': 3.4265785694122313, '_timestamp': 1740970004.2785273}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 226 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970004.278824}).
Epoch: [5][228/500]	Time  9.037 ( 9.037)	Loss 2.2114 (2.2209)	CeLoss 0.2070 (0.2278)	SegCLSLoss 0.0127 (0.0178)	KLLoss 0.3555 (0.3668)	MaskLoss 0.9817 (0.9737)	MaskBCELoss 0.0322 (0.2057)	MaskDICELoss 0.9495 (0.7679)
Epoch: [5][229/500]	Time  8.998 ( 8.998)	Loss 0.2217 (1.4240)	CeLoss 0.2217 (0.2426)	SegCLSLoss 0.0000 (0.0106)	KLLoss 0.0000 (0.2900)	MaskLoss 0.0000 (0.5736)	MaskBCELoss 0.0000 (0.1796)	MaskDICELoss 0.0000 (0.3940)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 227 is less than current step: 499. Dropping entry: {'train/loss': 2.2208906531333925, 'train/ce_loss': 0.22783203125, 'train/seg_cls_loss': 0.01778564453125, 'train/kl_loss': 0.366796875, 'train/mask_bce_loss': 0.20573154133744537, 'train/mask_dice_loss': 0.7679462194442749, 'train/mask_loss': 0.9736777484416962, 'metrics/total_secs_per_batch': 9.03740906715393, 'metrics/data_secs_per_batch': 4.017972135543824, '_timestamp': 1740970013.3159351}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 227 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970013.3162296}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 228 is less than current step: 499. Dropping entry: {'train/loss': 1.4239518880844115, 'train/ce_loss': 0.242578125, 'train/seg_cls_loss': 0.0106201171875, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.17962521817535163, 'train/mask_dice_loss': 0.39397179782390596, 'train/mask_loss': 0.5735970318317414, 'metrics/total_secs_per_batch': 8.997957706451416, 'metrics/data_secs_per_batch': 3.7519363164901733, '_timestamp': 1740970022.314007}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 228 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970022.314324}).
[2025-03-02 20:47:11,154] [INFO] [logging.py:96:log_dist] [Rank 0] step=2730, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:47:11,160] [INFO] [timer.py:215:stop] epoch=0/micro_step=27300/global_step=2730, RunningAvgSamplesPerSec=1.4225799811879487, CurrSamplesPerSec=1.130454295951321, MemAllocated=30.69GB, MaxMemAllocated=37.23GB
Epoch: [5][230/500]	Time  8.848 ( 8.848)	Loss 0.9023 (1.4182)	CeLoss 0.9023 (0.5178)	SegCLSLoss 0.0000 (0.0100)	KLLoss 0.0000 (0.2568)	MaskLoss 0.0000 (0.4349)	MaskBCELoss 0.0000 (0.1069)	MaskDICELoss 0.0000 (0.3279)
Epoch: [5][231/500]	Time  6.357 ( 6.357)	Loss 2.5497 (1.7632)	CeLoss 0.2070 (0.4692)	SegCLSLoss 0.0107 (0.0098)	KLLoss 0.3730 (0.2559)	MaskLoss 1.1503 (0.6317)	MaskBCELoss 0.1584 (0.1436)	MaskDICELoss 0.9920 (0.4881)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 229 is less than current step: 499. Dropping entry: {'train/loss': 1.418168205022812, 'train/ce_loss': 0.5177734375, 'train/seg_cls_loss': 0.0099609375, 'train/kl_loss': 0.2568359375, 'train/mask_bce_loss': 0.1069347720593214, 'train/mask_dice_loss': 0.3279305711388588, 'train/mask_loss': 0.4348653554916382, 'metrics/total_secs_per_batch': 8.8477041721344, 'metrics/data_secs_per_batch': 4.329736375808716, '_timestamp': 1740970031.161441}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 229 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970031.1617453}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 230 is less than current step: 499. Dropping entry: {'train/loss': 1.7631865620613099, 'train/ce_loss': 0.469189453125, 'train/seg_cls_loss': 0.009759521484375, 'train/kl_loss': 0.255859375, 'train/mask_bce_loss': 0.14360097609460354, 'train/mask_dice_loss': 0.4881387799978256, 'train/mask_loss': 0.6317397654056549, 'metrics/total_secs_per_batch': 6.357398509979248, 'metrics/data_secs_per_batch': 2.7088959932327272, '_timestamp': 1740970037.519054}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 230 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970037.5193477}).
Epoch: [5][232/500]	Time  7.968 ( 7.968)	Loss 2.0759 (1.6174)	CeLoss 0.1934 (0.4521)	SegCLSLoss 0.0190 (0.0139)	KLLoss 0.3555 (0.2518)	MaskLoss 0.9188 (0.5666)	MaskBCELoss 0.0384 (0.0879)	MaskDICELoss 0.8804 (0.4787)
Epoch: [5][233/500]	Time  7.983 ( 7.983)	Loss 2.0333 (1.2485)	CeLoss 0.1992 (0.4209)	SegCLSLoss 0.0175 (0.0094)	KLLoss 0.3535 (0.2164)	MaskLoss 0.8951 (0.4006)	MaskBCELoss 0.0081 (0.0236)	MaskDICELoss 0.8870 (0.3770)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 231 is less than current step: 499. Dropping entry: {'train/loss': 1.617361605167389, 'train/ce_loss': 0.4521484375, 'train/seg_cls_loss': 0.01392822265625, 'train/kl_loss': 0.2517578125, 'train/mask_bce_loss': 0.08791698450222611, 'train/mask_dice_loss': 0.478673979640007, 'train/mask_loss': 0.5665909707546234, 'metrics/total_secs_per_batch': 7.967558145523071, 'metrics/data_secs_per_batch': 3.2684030532836914, '_timestamp': 1740970045.4865546}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 231 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970045.486872}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 232 is less than current step: 499. Dropping entry: {'train/loss': 1.2484952449798583, 'train/ce_loss': 0.4208984375, 'train/seg_cls_loss': 0.00943603515625, 'train/kl_loss': 0.21640625, 'train/mask_bce_loss': 0.023578745126724244, 'train/mask_dice_loss': 0.3769872322678566, 'train/mask_loss': 0.4005659818649292, 'metrics/total_secs_per_batch': 7.983483552932739, 'metrics/data_secs_per_batch': 3.1699115514755247, '_timestamp': 1740970053.4700289}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 232 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970053.470297}).
Epoch: [5][234/500]	Time  8.632 ( 8.632)	Loss 1.1875 (1.7104)	CeLoss 1.1875 (0.4077)	SegCLSLoss 0.0000 (0.0135)	KLLoss 0.0000 (0.2916)	MaskLoss 0.0000 (0.6335)	MaskBCELoss 0.0000 (0.1400)	MaskDICELoss 0.0000 (0.4935)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 233 is less than current step: 499. Dropping entry: {'train/loss': 1.7103974223136902, 'train/ce_loss': 0.40771484375, 'train/seg_cls_loss': 0.0134521484375, 'train/kl_loss': 0.2916015625, 'train/mask_bce_loss': 0.14002242982387542, 'train/mask_dice_loss': 0.49349658787250517, 'train/mask_loss': 0.6335190176963806, 'metrics/total_secs_per_batch': 8.631705522537231, 'metrics/data_secs_per_batch': 3.820013332366943, '_timestamp': 1740970062.1017866}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 233 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970062.102079}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 234 is less than current step: 499. Dropping entry: {'train/loss': 1.591255009174347, 'train/ce_loss': 0.292529296875, 'train/seg_cls_loss': 0.012115478515625, 'train/kl_loss': 0.2193359375, 'train/mask_bce_loss': 0.19169752970337867, 'train/mask_dice_loss': 0.44384695291519166, 'train/mask_loss': 0.63554447889328, 'metrics/total_secs_per_batch': 7.892574071884155, 'metrics/data_secs_per_batch': 2.9883541822433473, '_timestamp': 1740970069.9943633}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 234 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970069.9947178}).
Epoch: [5][235/500]	Time  7.893 ( 7.893)	Loss 2.8709 (1.5913)	CeLoss 0.1006 (0.2925)	SegCLSLoss 0.0361 (0.0121)	KLLoss 0.3789 (0.2193)	MaskLoss 1.3573 (0.6355)	MaskBCELoss 0.5693 (0.1917)	MaskDICELoss 0.7880 (0.4438)
Epoch: [5][236/500]	Time  6.886 ( 6.886)	Loss 1.3876 (1.6106)	CeLoss 0.2090 (0.3146)	SegCLSLoss 0.0186 (0.0148)	KLLoss 0.3613 (0.3238)	MaskLoss 0.5669 (0.6283)	MaskBCELoss 0.0039 (0.1095)	MaskDICELoss 0.5630 (0.5188)
Epoch: [5][237/500]	Time  7.826 ( 7.826)	Loss 1.6142 (1.3697)	CeLoss 0.1865 (0.4240)	SegCLSLoss 0.0198 (0.0114)	KLLoss 0.3594 (0.2549)	MaskLoss 0.6909 (0.4571)	MaskBCELoss 0.0139 (0.1099)	MaskDICELoss 0.6770 (0.3473)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 235 is less than current step: 499. Dropping entry: {'train/loss': 1.6106040477752686, 'train/ce_loss': 0.31455078125, 'train/seg_cls_loss': 0.01483154296875, 'train/kl_loss': 0.323828125, 'train/mask_bce_loss': 0.10945097585208714, 'train/mask_dice_loss': 0.5188002571463585, 'train/mask_loss': 0.6282512366771698, 'metrics/total_secs_per_batch': 6.886359214782715, 'metrics/data_secs_per_batch': 3.1019278526306153, '_timestamp': 1740970076.8806875}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 235 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970076.8809612}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 236 is less than current step: 499. Dropping entry: {'train/loss': 1.3697189688682556, 'train/ce_loss': 0.4240234375, 'train/seg_cls_loss': 0.011419677734375, 'train/kl_loss': 0.2548828125, 'train/mask_bce_loss': 0.10987480077892542, 'train/mask_dice_loss': 0.34725031554698943, 'train/mask_loss': 0.4571251101791859, 'metrics/total_secs_per_batch': 7.825673341751099, 'metrics/data_secs_per_batch': 3.41378767490387, '_timestamp': 1740970084.7063606}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 236 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970084.7066228}).
Epoch: [5][238/500]	Time  7.274 ( 7.274)	Loss 0.0583 (1.4288)	CeLoss 0.0583 (0.4453)	SegCLSLoss 0.0000 (0.0114)	KLLoss 0.0000 (0.2531)	MaskLoss 0.0000 (0.4762)	MaskBCELoss 0.0000 (0.0890)	MaskDICELoss 0.0000 (0.3872)
Epoch: [5][239/500]	Time  7.111 ( 7.111)	Loss 1.1484 (1.4493)	CeLoss 1.1484 (0.6270)	SegCLSLoss 0.0000 (0.0105)	KLLoss 0.0000 (0.2162)	MaskLoss 0.0000 (0.3978)	MaskBCELoss 0.0000 (0.0816)	MaskDICELoss 0.0000 (0.3162)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 237 is less than current step: 499. Dropping entry: {'train/loss': 1.428799557685852, 'train/ce_loss': 0.4452880859375, 'train/seg_cls_loss': 0.01141357421875, 'train/kl_loss': 0.253125, 'train/mask_bce_loss': 0.0889788631349802, 'train/mask_dice_loss': 0.38720070421695707, 'train/mask_loss': 0.47617957144975664, 'metrics/total_secs_per_batch': 7.273661136627197, 'metrics/data_secs_per_batch': 3.4202525854110717, '_timestamp': 1740970091.9802108}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 237 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970091.980552}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 238 is less than current step: 499. Dropping entry: {'train/loss': 1.449326354265213, 'train/ce_loss': 0.626953125, 'train/seg_cls_loss': 0.010540771484375, 'train/kl_loss': 0.2162109375, 'train/mask_bce_loss': 0.08161795325577259, 'train/mask_dice_loss': 0.3161897510290146, 'train/mask_loss': 0.3978077083826065, 'metrics/total_secs_per_batch': 7.111179828643799, 'metrics/data_secs_per_batch': 3.6213836431503297, '_timestamp': 1740970099.091248}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 238 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970099.0915306}).
[2025-03-02 20:48:28,304] [INFO] [logging.py:96:log_dist] [Rank 0] step=2740, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:48:28,310] [INFO] [timer.py:215:stop] epoch=0/micro_step=27400/global_step=2740, RunningAvgSamplesPerSec=1.4220744327698518, CurrSamplesPerSec=1.084840088728004, MemAllocated=31.46GB, MaxMemAllocated=37.23GB
Epoch: [5][240/500]	Time  9.220 ( 9.220)	Loss 1.2569 (1.7586)	CeLoss 0.1621 (0.3142)	SegCLSLoss 0.0181 (0.0146)	KLLoss 0.3633 (0.3260)	MaskLoss 0.5249 (0.7023)	MaskBCELoss 0.0249 (0.1147)	MaskDICELoss 0.5000 (0.5877)
Epoch: [5][241/500]	Time  6.773 ( 6.773)	Loss 0.0752 (1.4231)	CeLoss 0.0752 (0.5952)	SegCLSLoss 0.0000 (0.0070)	KLLoss 0.0000 (0.1443)	MaskLoss 0.0000 (0.4050)	MaskBCELoss 0.0000 (0.1450)	MaskDICELoss 0.0000 (0.2600)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 239 is less than current step: 499. Dropping entry: {'train/loss': 1.7585732102394105, 'train/ce_loss': 0.31416015625, 'train/seg_cls_loss': 0.01461181640625, 'train/kl_loss': 0.3259765625, 'train/mask_bce_loss': 0.11467475560493767, 'train/mask_dice_loss': 0.5876587197184563, 'train/mask_loss': 0.7023334801197052, 'metrics/total_secs_per_batch': 9.219559907913208, 'metrics/data_secs_per_batch': 4.539926505088806, '_timestamp': 1740970108.3106003}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 239 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970108.310891}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 240 is less than current step: 499. Dropping entry: {'train/loss': 1.4231292486190796, 'train/ce_loss': 0.595166015625, 'train/seg_cls_loss': 0.00699462890625, 'train/kl_loss': 0.1443359375, 'train/mask_bce_loss': 0.1450446869712323, 'train/mask_dice_loss': 0.2599525511264801, 'train/mask_loss': 0.4049972414970398, 'metrics/total_secs_per_batch': 6.77320122718811, 'metrics/data_secs_per_batch': 2.856855297088623, '_timestamp': 1740970115.083984}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 240 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970115.0842502}).
Epoch: [5][242/500]	Time  7.539 ( 7.539)	Loss 2.0495 (1.5266)	CeLoss 0.1846 (0.3878)	SegCLSLoss 0.0228 (0.0137)	KLLoss 0.3574 (0.2879)	MaskLoss 0.9085 (0.5517)	MaskBCELoss 0.2167 (0.1256)	MaskDICELoss 0.6918 (0.4261)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 241 is less than current step: 499. Dropping entry: {'train/loss': 1.5266075015068055, 'train/ce_loss': 0.38779296875, 'train/seg_cls_loss': 0.01365966796875, 'train/kl_loss': 0.287890625, 'train/mask_bce_loss': 0.12555264662951232, 'train/mask_dice_loss': 0.42613000571727755, 'train/mask_loss': 0.5516826510429382, 'metrics/total_secs_per_batch': 7.53942084312439, 'metrics/data_secs_per_batch': 3.2395092725753782, '_timestamp': 1740970122.6234448}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 241 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970122.623731}).
Epoch: [5][243/500]	Time  8.026 ( 8.026)	Loss 1.6410 (1.6914)	CeLoss 0.2217 (0.3644)	SegCLSLoss 0.0142 (0.0134)	KLLoss 0.3594 (0.3244)	MaskLoss 0.6887 (0.6439)	MaskBCELoss 0.2607 (0.1098)	MaskDICELoss 0.4280 (0.5341)
Epoch: [5][244/500]	Time  9.185 ( 9.185)	Loss 2.3165 (1.8728)	CeLoss 0.2148 (0.3006)	SegCLSLoss 0.0168 (0.0156)	KLLoss 0.3613 (0.3232)	MaskLoss 1.0284 (0.7659)	MaskBCELoss 0.0742 (0.1055)	MaskDICELoss 0.9542 (0.6604)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 242 is less than current step: 499. Dropping entry: {'train/loss': 1.6914206504821778, 'train/ce_loss': 0.36435546875, 'train/seg_cls_loss': 0.013385009765625, 'train/kl_loss': 0.3244140625, 'train/mask_bce_loss': 0.10980224078521132, 'train/mask_dice_loss': 0.5341014564037323, 'train/mask_loss': 0.64390369951725, 'metrics/total_secs_per_batch': 8.025907516479492, 'metrics/data_secs_per_batch': 3.6300903081893923, '_timestamp': 1740970130.6492836}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 242 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970130.6495988}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 243 is less than current step: 499. Dropping entry: {'train/loss': 1.8728015184402467, 'train/ce_loss': 0.3005859375, 'train/seg_cls_loss': 0.0156494140625, 'train/kl_loss': 0.3232421875, 'train/mask_bce_loss': 0.10551129542291164, 'train/mask_dice_loss': 0.6604304671287536, 'train/mask_loss': 0.7659417569637299, 'metrics/total_secs_per_batch': 9.184696197509766, 'metrics/data_secs_per_batch': 4.402924323081971, '_timestamp': 1740970139.8340752}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 243 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970139.8344398}).
Epoch: [5][245/500]	Time  7.964 ( 7.964)	Loss 2.0368 (1.8378)	CeLoss 0.2402 (0.3218)	SegCLSLoss 0.0237 (0.0187)	KLLoss 0.3652 (0.3277)	MaskLoss 0.8739 (0.7370)	MaskBCELoss 0.0960 (0.1620)	MaskDICELoss 0.7779 (0.5749)
Epoch: [5][246/500]	Time  9.570 ( 9.570)	Loss 2.0731 (1.8840)	CeLoss 0.2119 (0.3041)	SegCLSLoss 0.0137 (0.0143)	KLLoss 0.3672 (0.3275)	MaskLoss 0.9086 (0.7700)	MaskBCELoss 0.1186 (0.1506)	MaskDICELoss 0.7900 (0.6194)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 244 is less than current step: 499. Dropping entry: {'train/loss': 1.837810254096985, 'train/ce_loss': 0.32177734375, 'train/seg_cls_loss': 0.018670654296875, 'train/kl_loss': 0.327734375, 'train/mask_bce_loss': 0.16202510260045527, 'train/mask_dice_loss': 0.5749464362859726, 'train/mask_loss': 0.7369715332984924, 'metrics/total_secs_per_batch': 7.96392560005188, 'metrics/data_secs_per_batch': 3.505251336097717, '_timestamp': 1740970147.7980645}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 244 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970147.7983844}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 245 is less than current step: 499. Dropping entry: {'train/loss': 1.884029495716095, 'train/ce_loss': 0.3041015625, 'train/seg_cls_loss': 0.0142578125, 'train/kl_loss': 0.3275390625, 'train/mask_bce_loss': 0.1505566243082285, 'train/mask_dice_loss': 0.619436627626419, 'train/mask_loss': 0.7699932634830475, 'metrics/total_secs_per_batch': 9.569934606552124, 'metrics/data_secs_per_batch': 4.476336812973022, '_timestamp': 1740970157.3680239}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 245 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970157.3683279}).
Epoch: [5][247/500]	Time  9.141 ( 9.141)	Loss 0.9531 (1.7673)	CeLoss 0.9531 (0.3484)	SegCLSLoss 0.0000 (0.0132)	KLLoss 0.0000 (0.2918)	MaskLoss 0.0000 (0.6915)	MaskBCELoss 0.0000 (0.1121)	MaskDICELoss 0.0000 (0.5795)
Epoch: [5][248/500]	Time  7.652 ( 7.652)	Loss 1.2706 (1.5943)	CeLoss 0.2373 (0.4708)	SegCLSLoss 0.0199 (0.0124)	KLLoss 0.3652 (0.2562)	MaskLoss 0.4937 (0.5457)	MaskBCELoss 0.0269 (0.1110)	MaskDICELoss 0.4668 (0.4347)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 246 is less than current step: 499. Dropping entry: {'train/loss': 1.767290508747101, 'train/ce_loss': 0.3484375, 'train/seg_cls_loss': 0.01324462890625, 'train/kl_loss': 0.291796875, 'train/mask_bce_loss': 0.11205216012895107, 'train/mask_dice_loss': 0.5794544160366059, 'train/mask_loss': 0.6915065765380859, 'metrics/total_secs_per_batch': 9.140972137451172, 'metrics/data_secs_per_batch': 4.75946569442749, '_timestamp': 1740970166.508939}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 246 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970166.509231}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 247 is less than current step: 499. Dropping entry: {'train/loss': 1.5942967534065247, 'train/ce_loss': 0.47080078125, 'train/seg_cls_loss': 0.0124267578125, 'train/kl_loss': 0.25625, 'train/mask_bce_loss': 0.11100290548056364, 'train/mask_dice_loss': 0.4346806347370148, 'train/mask_loss': 0.5456835359334946, 'metrics/total_secs_per_batch': 7.652374505996704, 'metrics/data_secs_per_batch': 3.5486064910888673, '_timestamp': 1740970174.1612878}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 247 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970174.1615822}).
Epoch: [5][249/500]	Time  8.047 ( 8.047)	Loss 0.1406 (1.4365)	CeLoss 0.1406 (0.3292)	SegCLSLoss 0.0000 (0.0146)	KLLoss 0.0000 (0.2213)	MaskLoss 0.0000 (0.5389)	MaskBCELoss 0.0000 (0.1404)	MaskDICELoss 0.0000 (0.3985)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 248 is less than current step: 499. Dropping entry: {'train/loss': 1.4364720582962036, 'train/ce_loss': 0.329248046875, 'train/seg_cls_loss': 0.01455078125, 'train/kl_loss': 0.2212890625, 'train/mask_bce_loss': 0.14043641984462737, 'train/mask_dice_loss': 0.3985027432441711, 'train/mask_loss': 0.5389391541481018, 'metrics/total_secs_per_batch': 8.047265768051147, 'metrics/data_secs_per_batch': 3.737735152244568, '_timestamp': 1740970182.2087638}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 248 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970182.2091248}).
[2025-03-02 20:49:50,446] [INFO] [logging.py:96:log_dist] [Rank 0] step=2750, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:49:50,451] [INFO] [timer.py:215:stop] epoch=0/micro_step=27500/global_step=2750, RunningAvgSamplesPerSec=1.4212058995710184, CurrSamplesPerSec=1.2132697561264452, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [5][250/500]	Time  8.244 ( 8.244)	Loss 1.8703 (1.7551)	CeLoss 0.3027 (0.3816)	SegCLSLoss 0.0113 (0.0140)	KLLoss 0.3613 (0.2883)	MaskLoss 0.7633 (0.6689)	MaskBCELoss 0.2423 (0.0901)	MaskDICELoss 0.5210 (0.5788)
Epoch: [5][251/500]	Time  8.137 ( 8.137)	Loss 2.6081 (1.8216)	CeLoss 0.1660 (0.3491)	SegCLSLoss 0.0200 (0.0168)	KLLoss 0.3828 (0.2916)	MaskLoss 1.1971 (0.7175)	MaskBCELoss 0.2638 (0.1381)	MaskDICELoss 0.9333 (0.5794)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 249 is less than current step: 499. Dropping entry: {'train/loss': 1.755104923248291, 'train/ce_loss': 0.381640625, 'train/seg_cls_loss': 0.014031982421875, 'train/kl_loss': 0.28828125, 'train/mask_bce_loss': 0.09009484890848399, 'train/mask_dice_loss': 0.5787662029266357, 'train/mask_loss': 0.6688610553741455, 'metrics/total_secs_per_batch': 8.244080781936646, 'metrics/data_secs_per_batch': 3.8422221422195433, '_timestamp': 1740970190.4524744}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 249 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970190.4527702}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 250 is less than current step: 499. Dropping entry: {'train/loss': 1.821579349040985, 'train/ce_loss': 0.34912109375, 'train/seg_cls_loss': 0.016796875, 'train/kl_loss': 0.2916015625, 'train/mask_bce_loss': 0.13813710752874614, 'train/mask_dice_loss': 0.5793908655643463, 'train/mask_loss': 0.717527961730957, 'metrics/total_secs_per_batch': 8.1367826461792, 'metrics/data_secs_per_batch': 3.374238896369934, '_timestamp': 1740970198.589387}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 250 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970198.5896518}).
Epoch: [5][252/500]	Time  8.405 ( 8.405)	Loss 1.6550 (1.3585)	CeLoss 0.2031 (0.3861)	SegCLSLoss 0.0229 (0.0147)	KLLoss 0.3477 (0.2496)	MaskLoss 0.7025 (0.4700)	MaskBCELoss 0.0127 (0.0512)	MaskDICELoss 0.6898 (0.4187)
Epoch: [5][253/500]	Time  7.720 ( 7.720)	Loss 0.1807 (1.0847)	CeLoss 0.1807 (0.2723)	SegCLSLoss 0.0000 (0.0116)	KLLoss 0.0000 (0.2164)	MaskLoss 0.0000 (0.3926)	MaskBCELoss 0.0000 (0.1027)	MaskDICELoss 0.0000 (0.2898)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 251 is less than current step: 499. Dropping entry: {'train/loss': 1.358468496799469, 'train/ce_loss': 0.386083984375, 'train/seg_cls_loss': 0.01474609375, 'train/kl_loss': 0.249609375, 'train/mask_bce_loss': 0.05123257637023926, 'train/mask_dice_loss': 0.4187487244606018, 'train/mask_loss': 0.46998130083084105, 'metrics/total_secs_per_batch': 8.405259370803833, 'metrics/data_secs_per_batch': 3.7080947160720825, '_timestamp': 1740970206.994856}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 251 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970206.9952033}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 252 is less than current step: 499. Dropping entry: {'train/loss': 1.084673857688904, 'train/ce_loss': 0.272314453125, 'train/seg_cls_loss': 0.0116455078125, 'train/kl_loss': 0.21640625, 'train/mask_bce_loss': 0.10271895714104176, 'train/mask_dice_loss': 0.28983769863843917, 'train/mask_loss': 0.3925566554069519, 'metrics/total_secs_per_batch': 7.720132827758789, 'metrics/data_secs_per_batch': 3.444576644897461, '_timestamp': 1740970214.7148397}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 252 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970214.7151322}).
Epoch: [5][254/500]	Time  7.548 ( 7.548)	Loss 1.2560 (1.4490)	CeLoss 0.2314 (0.3535)	SegCLSLoss 0.0259 (0.0142)	KLLoss 0.3691 (0.2908)	MaskLoss 0.4874 (0.5297)	MaskBCELoss 0.1125 (0.1027)	MaskDICELoss 0.3749 (0.4270)
Epoch: [5][255/500]	Time  6.661 ( 6.661)	Loss 1.8405 (1.4668)	CeLoss 0.1904 (0.8266)	SegCLSLoss 0.0154 (0.0085)	KLLoss 0.3652 (0.1467)	MaskLoss 0.8031 (0.3106)	MaskBCELoss 0.0371 (0.0387)	MaskDICELoss 0.7660 (0.2719)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 253 is less than current step: 499. Dropping entry: {'train/loss': 1.4490258932113647, 'train/ce_loss': 0.353515625, 'train/seg_cls_loss': 0.014215087890625, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.10271541774272919, 'train/mask_dice_loss': 0.42697330117225646, 'train/mask_loss': 0.5296887189149857, 'metrics/total_secs_per_batch': 7.548351287841797, 'metrics/data_secs_per_batch': 3.717314434051514, '_timestamp': 1740970222.2632096}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 253 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970222.2635539}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 254 is less than current step: 499. Dropping entry: {'train/loss': 1.4667550206184388, 'train/ce_loss': 0.8265625, 'train/seg_cls_loss': 0.008538818359375, 'train/kl_loss': 0.1466796875, 'train/mask_bce_loss': 0.03870122935622931, 'train/mask_dice_loss': 0.2719223827123642, 'train/mask_loss': 0.3106236070394516, 'metrics/total_secs_per_batch': 6.66067910194397, 'metrics/data_secs_per_batch': 2.241538381576538, '_timestamp': 1740970228.923852}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 254 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970228.9241307}).
Epoch: [5][256/500]	Time  7.651 ( 7.651)	Loss 1.3167 (1.5670)	CeLoss 0.3340 (0.3384)	SegCLSLoss 0.0135 (0.0155)	KLLoss 0.3594 (0.2883)	MaskLoss 0.4709 (0.5963)	MaskBCELoss 0.0433 (0.1237)	MaskDICELoss 0.4275 (0.4725)
Epoch: [5][257/500]	Time  7.978 ( 7.978)	Loss 2.0090 (1.6500)	CeLoss 0.2461 (0.3318)	SegCLSLoss 0.0190 (0.0181)	KLLoss 0.3535 (0.2908)	MaskLoss 0.8590 (0.6400)	MaskBCELoss 0.0240 (0.1167)	MaskDICELoss 0.8350 (0.5233)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 255 is less than current step: 499. Dropping entry: {'train/loss': 1.5670363903045654, 'train/ce_loss': 0.33837890625, 'train/seg_cls_loss': 0.015509033203125, 'train/kl_loss': 0.28828125, 'train/mask_bce_loss': 0.12374199363403023, 'train/mask_dice_loss': 0.47252034246921537, 'train/mask_loss': 0.5962623357772827, 'metrics/total_secs_per_batch': 7.650882005691528, 'metrics/data_secs_per_batch': 3.557601714134216, '_timestamp': 1740970236.5747313}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 255 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970236.575015}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 256 is less than current step: 499. Dropping entry: {'train/loss': 1.6499685287475585, 'train/ce_loss': 0.3318359375, 'train/seg_cls_loss': 0.0180908203125, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.11670368518680334, 'train/mask_dice_loss': 0.5233196496963501, 'train/mask_loss': 0.6400233328342437, 'metrics/total_secs_per_batch': 7.978358507156372, 'metrics/data_secs_per_batch': 3.5827431678771973, '_timestamp': 1740970244.5532207}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 256 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970244.5535614}).
Epoch: [5][258/500]	Time  6.443 ( 6.443)	Loss 1.6490 (1.9361)	CeLoss 0.2178 (0.3884)	SegCLSLoss 0.0167 (0.0147)	KLLoss 0.3750 (0.2945)	MaskLoss 0.6927 (0.7555)	MaskBCELoss 0.2758 (0.1651)	MaskDICELoss 0.4168 (0.5904)
Epoch: [5][259/500]	Time  7.285 ( 7.285)	Loss 1.1719 (1.4858)	CeLoss 1.1719 (0.5064)	SegCLSLoss 0.0000 (0.0126)	KLLoss 0.0000 (0.2545)	MaskLoss 0.0000 (0.4738)	MaskBCELoss 0.0000 (0.0725)	MaskDICELoss 0.0000 (0.4013)
[2025-03-02 20:51:04,954] [INFO] [logging.py:96:log_dist] [Rank 0] step=2760, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:51:04,960] [INFO] [timer.py:215:stop] epoch=0/micro_step=27600/global_step=2760, RunningAvgSamplesPerSec=1.420903239288937, CurrSamplesPerSec=1.497244256446144, MemAllocated=30.7GB, MaxMemAllocated=37.23GB
Epoch: [5][260/500]	Time  6.680 ( 6.680)	Loss 1.2891 (1.7016)	CeLoss 1.2891 (0.5232)	SegCLSLoss 0.0000 (0.0125)	KLLoss 0.0000 (0.2188)	MaskLoss 0.0000 (0.5751)	MaskBCELoss 0.0000 (0.0860)	MaskDICELoss 0.0000 (0.4891)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 257 is less than current step: 499. Dropping entry: {'train/loss': 1.93613703250885, 'train/ce_loss': 0.38837890625, 'train/seg_cls_loss': 0.01470947265625, 'train/kl_loss': 0.29453125, 'train/mask_bce_loss': 0.16508049853146076, 'train/mask_dice_loss': 0.5903903484344483, 'train/mask_loss': 0.7554708600044251, 'metrics/total_secs_per_batch': 6.44331169128418, 'metrics/data_secs_per_batch': 2.8906916618347167, '_timestamp': 1740970250.996337}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 257 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970250.99662}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 258 is less than current step: 499. Dropping entry: {'train/loss': 1.485823678970337, 'train/ce_loss': 0.5064453125, 'train/seg_cls_loss': 0.012591552734375, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.07250064164400101, 'train/mask_dice_loss': 0.4012705683708191, 'train/mask_loss': 0.4737712055444717, 'metrics/total_secs_per_batch': 7.2845892906188965, 'metrics/data_secs_per_batch': 3.2146223545074464, '_timestamp': 1740970258.2809603}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 258 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970258.281231}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 259 is less than current step: 499. Dropping entry: {'train/loss': 1.7016389608383178, 'train/ce_loss': 0.5232421875, 'train/seg_cls_loss': 0.012518310546875, 'train/kl_loss': 0.21875, 'train/mask_bce_loss': 0.08599845506250858, 'train/mask_dice_loss': 0.4890886008739471, 'train/mask_loss': 0.5750870704650879, 'metrics/total_secs_per_batch': 6.680471181869507, 'metrics/data_secs_per_batch': 3.0767903566360473, '_timestamp': 1740970264.9612677}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 259 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970264.9615505}).
Epoch: [5][261/500]	Time  8.581 ( 8.581)	Loss 2.3808 (1.7232)	CeLoss 0.2910 (0.4592)	SegCLSLoss 0.0101 (0.0135)	KLLoss 0.3594 (0.2895)	MaskLoss 1.0244 (0.6143)	MaskBCELoss 0.3483 (0.1476)	MaskDICELoss 0.6761 (0.4667)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 260 is less than current step: 499. Dropping entry: {'train/loss': 1.7232417225837708, 'train/ce_loss': 0.4591796875, 'train/seg_cls_loss': 0.013531494140625, 'train/kl_loss': 0.289453125, 'train/mask_bce_loss': 0.14759871372953057, 'train/mask_dice_loss': 0.4666588693857193, 'train/mask_loss': 0.6142575800418854, 'metrics/total_secs_per_batch': 8.58073616027832, 'metrics/data_secs_per_batch': 3.433801054954529, '_timestamp': 1740970273.5424364}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 260 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970273.5428634}).
Epoch: [5][262/500]	Time  7.958 ( 7.958)	Loss 2.2820 (1.7867)	CeLoss 0.1523 (0.4885)	SegCLSLoss 0.0194 (0.0125)	KLLoss 0.3613 (0.2545)	MaskLoss 1.0419 (0.6333)	MaskBCELoss 0.2155 (0.1219)	MaskDICELoss 0.8264 (0.5114)
Epoch: [5][263/500]	Time  7.639 ( 7.639)	Loss 2.2407 (1.6104)	CeLoss 0.1543 (0.2850)	SegCLSLoss 0.0306 (0.0168)	KLLoss 0.3750 (0.3271)	MaskLoss 1.0168 (0.6422)	MaskBCELoss 0.3564 (0.1007)	MaskDICELoss 0.6604 (0.5415)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 261 is less than current step: 499. Dropping entry: {'train/loss': 1.7867029666900636, 'train/ce_loss': 0.4884765625, 'train/seg_cls_loss': 0.012451171875, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.121949477866292, 'train/mask_dice_loss': 0.5113922446966171, 'train/mask_loss': 0.6333417236804962, 'metrics/total_secs_per_batch': 7.957696437835693, 'metrics/data_secs_per_batch': 3.440860223770142, '_timestamp': 1740970281.4999557}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 261 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970281.5002854}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 262 is less than current step: 499. Dropping entry: {'train/loss': 1.6104368627071382, 'train/ce_loss': 0.2849609375, 'train/seg_cls_loss': 0.016796875, 'train/kl_loss': 0.3271484375, 'train/mask_bce_loss': 0.1006765550468117, 'train/mask_dice_loss': 0.541504767537117, 'train/mask_loss': 0.642181321978569, 'metrics/total_secs_per_batch': 7.638891696929932, 'metrics/data_secs_per_batch': 3.5789665937423707, '_timestamp': 1740970289.138796}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 262 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970289.1390734}).
Epoch: [5][264/500]	Time 10.021 (10.021)	Loss 2.5801 (1.4859)	CeLoss 0.2129 (0.1863)	SegCLSLoss 0.0154 (0.0122)	KLLoss 0.3672 (0.2900)	MaskLoss 1.1612 (0.6324)	MaskBCELoss 0.5100 (0.1433)	MaskDICELoss 0.6512 (0.4891)
Epoch: [5][265/500]	Time  8.089 ( 8.089)	Loss 1.4193 (1.6762)	CeLoss 0.3047 (0.2896)	SegCLSLoss 0.0119 (0.0124)	KLLoss 0.3633 (0.2906)	MaskLoss 0.5358 (0.6758)	MaskBCELoss 0.1255 (0.1452)	MaskDICELoss 0.4103 (0.5306)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 263 is less than current step: 499. Dropping entry: {'train/loss': 1.4859262228012085, 'train/ce_loss': 0.186279296875, 'train/seg_cls_loss': 0.0121826171875, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.14325170314405114, 'train/mask_dice_loss': 0.4891401261091232, 'train/mask_loss': 0.6323918238282203, 'metrics/total_secs_per_batch': 10.02074646949768, 'metrics/data_secs_per_batch': 4.790916323661804, '_timestamp': 1740970299.1594837}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 263 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970299.1597073}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 264 is less than current step: 499. Dropping entry: {'train/loss': 1.6762473702430725, 'train/ce_loss': 0.28955078125, 'train/seg_cls_loss': 0.012432861328125, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.14518516659736633, 'train/mask_dice_loss': 0.530585004389286, 'train/mask_loss': 0.6757701724767685, 'metrics/total_secs_per_batch': 8.088869333267212, 'metrics/data_secs_per_batch': 2.954033041000366, '_timestamp': 1740970307.248393}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 264 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970307.2486637}).
Epoch: [5][266/500]	Time  6.410 ( 6.410)	Loss 0.7773 (1.4218)	CeLoss 0.7773 (0.6646)	SegCLSLoss 0.0000 (0.0066)	KLLoss 0.0000 (0.1824)	MaskLoss 0.0000 (0.3678)	MaskBCELoss 0.0000 (0.0712)	MaskDICELoss 0.0000 (0.2966)
Epoch: [5][267/500]	Time  9.531 ( 9.531)	Loss 2.0382 (1.5356)	CeLoss 0.2109 (0.3631)	SegCLSLoss 0.0199 (0.0130)	KLLoss 0.3594 (0.3236)	MaskLoss 0.8912 (0.5668)	MaskBCELoss 0.0548 (0.0632)	MaskDICELoss 0.8364 (0.5036)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 265 is less than current step: 499. Dropping entry: {'train/loss': 1.4217882037162781, 'train/ce_loss': 0.6646484375, 'train/seg_cls_loss': 0.006561279296875, 'train/kl_loss': 0.182421875, 'train/mask_bce_loss': 0.07119123265147209, 'train/mask_dice_loss': 0.29663645923137666, 'train/mask_loss': 0.3678276926279068, 'metrics/total_secs_per_batch': 6.409925222396851, 'metrics/data_secs_per_batch': 2.877249264717102, '_timestamp': 1740970313.6585176}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 265 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970313.6588743}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 266 is less than current step: 499. Dropping entry: {'train/loss': 1.5355844616889953, 'train/ce_loss': 0.3630859375, 'train/seg_cls_loss': 0.0129638671875, 'train/kl_loss': 0.3236328125, 'train/mask_bce_loss': 0.06318641472607851, 'train/mask_dice_loss': 0.5035804271697998, 'train/mask_loss': 0.5667668357491493, 'metrics/total_secs_per_batch': 9.53052043914795, 'metrics/data_secs_per_batch': 4.299600720405579, '_timestamp': 1740970323.1888907}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 266 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970323.1891763}).
Epoch: [5][268/500]	Time  8.194 ( 8.194)	Loss 1.7681 (1.2769)	CeLoss 0.2012 (0.3872)	SegCLSLoss 0.0177 (0.0122)	KLLoss 0.3789 (0.2912)	MaskLoss 0.7600 (0.4271)	MaskBCELoss 0.0060 (0.0584)	MaskDICELoss 0.7540 (0.3688)
Epoch: [5][269/500]	Time  6.614 ( 6.614)	Loss 1.3906 (1.6536)	CeLoss 1.3906 (0.4707)	SegCLSLoss 0.0000 (0.0114)	KLLoss 0.0000 (0.2199)	MaskLoss 0.0000 (0.5777)	MaskBCELoss 0.0000 (0.1389)	MaskDICELoss 0.0000 (0.4388)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 267 is less than current step: 499. Dropping entry: {'train/loss': 1.276919025182724, 'train/ce_loss': 0.38720703125, 'train/seg_cls_loss': 0.01224365234375, 'train/kl_loss': 0.2912109375, 'train/mask_bce_loss': 0.058380723698064685, 'train/mask_dice_loss': 0.3687506660819054, 'train/mask_loss': 0.42713138461112976, 'metrics/total_secs_per_batch': 8.194133996963501, 'metrics/data_secs_per_batch': 3.2030564069747927, '_timestamp': 1740970331.383069}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 267 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970331.3832743}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 268 is less than current step: 499. Dropping entry: {'train/loss': 1.653593897819519, 'train/ce_loss': 0.470654296875, 'train/seg_cls_loss': 0.01138916015625, 'train/kl_loss': 0.219921875, 'train/mask_bce_loss': 0.1389258697628975, 'train/mask_dice_loss': 0.4387744039297104, 'train/mask_loss': 0.5777002692222595, 'metrics/total_secs_per_batch': 6.613510847091675, 'metrics/data_secs_per_batch': 3.213020348548889, '_timestamp': 1740970337.9966242}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 268 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970337.996929}).
[2025-03-02 20:52:24,784] [INFO] [logging.py:96:log_dist] [Rank 0] step=2770, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:52:24,790] [INFO] [timer.py:215:stop] epoch=0/micro_step=27700/global_step=2770, RunningAvgSamplesPerSec=1.4202151028886536, CurrSamplesPerSec=1.4721564974958306, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [5][270/500]	Time  6.794 ( 6.794)	Loss 2.1386 (1.5308)	CeLoss 0.2432 (0.6907)	SegCLSLoss 0.0139 (0.0072)	KLLoss 0.3535 (0.1805)	MaskLoss 0.9267 (0.4092)	MaskBCELoss 0.1598 (0.0860)	MaskDICELoss 0.7669 (0.3231)
Epoch: [5][271/500]	Time  8.823 ( 8.823)	Loss 1.2408 (1.4690)	CeLoss 0.2812 (0.2288)	SegCLSLoss 0.0121 (0.0140)	KLLoss 0.3613 (0.3277)	MaskLoss 0.4583 (0.6002)	MaskBCELoss 0.1325 (0.1112)	MaskDICELoss 0.3258 (0.4890)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 269 is less than current step: 499. Dropping entry: {'train/loss': 1.5308217048645019, 'train/ce_loss': 0.6906982421875, 'train/seg_cls_loss': 0.00716552734375, 'train/kl_loss': 0.18046875, 'train/mask_bce_loss': 0.08604592978954315, 'train/mask_dice_loss': 0.3231271207332611, 'train/mask_loss': 0.4091730654239655, 'metrics/total_secs_per_batch': 6.794422388076782, 'metrics/data_secs_per_batch': 3.1473997116088865, '_timestamp': 1740970344.7907405}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 269 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970344.7910025}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 270 is less than current step: 499. Dropping entry: {'train/loss': 1.46903795003891, 'train/ce_loss': 0.22880859375, 'train/seg_cls_loss': 0.01395263671875, 'train/kl_loss': 0.327734375, 'train/mask_bce_loss': 0.11116988211870193, 'train/mask_dice_loss': 0.4890229195356369, 'train/mask_loss': 0.6001927971839904, 'metrics/total_secs_per_batch': 8.82348346710205, 'metrics/data_secs_per_batch': 3.826900315284729, '_timestamp': 1740970353.6144037}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 270 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970353.6147177}).
Epoch: [5][272/500]	Time  9.038 ( 9.038)	Loss 1.2831 (1.7500)	CeLoss 0.1611 (0.3234)	SegCLSLoss 0.0244 (0.0140)	KLLoss 0.3750 (0.3256)	MaskLoss 0.5361 (0.6935)	MaskBCELoss 0.0699 (0.1099)	MaskDICELoss 0.4662 (0.5836)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 271 is less than current step: 499. Dropping entry: {'train/loss': 1.7499505639076234, 'train/ce_loss': 0.3234375, 'train/seg_cls_loss': 0.013983154296875, 'train/kl_loss': 0.3255859375, 'train/mask_bce_loss': 0.10991361327469348, 'train/mask_dice_loss': 0.5836163461208344, 'train/mask_loss': 0.6935299694538116, 'metrics/total_secs_per_batch': 9.037811756134033, 'metrics/data_secs_per_batch': 4.436764168739319, '_timestamp': 1740970362.6523125}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 271 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970362.6526058}).
Epoch: [5][273/500]	Time  8.595 ( 8.595)	Loss 3.6267 (1.8441)	CeLoss 0.1123 (0.3308)	SegCLSLoss 0.0327 (0.0154)	KLLoss 0.3691 (0.2922)	MaskLoss 1.7303 (0.7381)	MaskBCELoss 1.0940 (0.2379)	MaskDICELoss 0.6363 (0.5002)
Epoch: [5][274/500]	Time  8.043 ( 8.043)	Loss 1.8328 (1.8264)	CeLoss 0.1709 (0.3535)	SegCLSLoss 0.0239 (0.0171)	KLLoss 0.3613 (0.2939)	MaskLoss 0.8070 (0.7176)	MaskBCELoss 0.0302 (0.1328)	MaskDICELoss 0.7768 (0.5848)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 272 is less than current step: 499. Dropping entry: {'train/loss': 1.8440627574920654, 'train/ce_loss': 0.33076171875, 'train/seg_cls_loss': 0.01541748046875, 'train/kl_loss': 0.2921875, 'train/mask_bce_loss': 0.23791220579296352, 'train/mask_dice_loss': 0.5001836091279983, 'train/mask_loss': 0.7380958318710327, 'metrics/total_secs_per_batch': 8.595240116119385, 'metrics/data_secs_per_batch': 3.959980273246765, '_timestamp': 1740970371.2474635}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 272 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970371.2477489}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 273 is less than current step: 499. Dropping entry: {'train/loss': 1.8263662099838256, 'train/ce_loss': 0.353466796875, 'train/seg_cls_loss': 0.01712646484375, 'train/kl_loss': 0.2939453125, 'train/mask_bce_loss': 0.13276322837918997, 'train/mask_dice_loss': 0.5848144173622132, 'train/mask_loss': 0.7175776302814484, 'metrics/total_secs_per_batch': 8.043224573135376, 'metrics/data_secs_per_batch': 3.52121045589447, '_timestamp': 1740970379.2907164}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 273 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970379.290988}).
Epoch: [5][275/500]	Time  8.671 ( 8.671)	Loss 2.1308 (1.7387)	CeLoss 0.3008 (0.4312)	SegCLSLoss 0.0121 (0.0123)	KLLoss 0.3594 (0.2891)	MaskLoss 0.8945 (0.6365)	MaskBCELoss 0.5067 (0.1517)	MaskDICELoss 0.3878 (0.4848)
Epoch: [5][276/500]	Time  6.636 ( 6.636)	Loss 1.0666 (1.5935)	CeLoss 0.2314 (0.4273)	SegCLSLoss 0.0106 (0.0136)	KLLoss 0.3594 (0.2539)	MaskLoss 0.3975 (0.5671)	MaskBCELoss 0.0870 (0.2089)	MaskDICELoss 0.3105 (0.3583)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 274 is less than current step: 499. Dropping entry: {'train/loss': 1.7387280702590941, 'train/ce_loss': 0.43115234375, 'train/seg_cls_loss': 0.012322998046875, 'train/kl_loss': 0.2890625, 'train/mask_bce_loss': 0.15172279705293476, 'train/mask_dice_loss': 0.4847799092531204, 'train/mask_loss': 0.6365027070045471, 'metrics/total_secs_per_batch': 8.670849084854126, 'metrics/data_secs_per_batch': 3.886622428894043, '_timestamp': 1740970387.9615731}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 274 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970387.9618409}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 275 is less than current step: 499. Dropping entry: {'train/loss': 1.5934951841831206, 'train/ce_loss': 0.427294921875, 'train/seg_cls_loss': 0.01357421875, 'train/kl_loss': 0.25390625, 'train/mask_bce_loss': 0.20886513069272042, 'train/mask_dice_loss': 0.3582681968808174, 'train/mask_loss': 0.567133316397667, 'metrics/total_secs_per_batch': 6.635901927947998, 'metrics/data_secs_per_batch': 2.6794041872024534, '_timestamp': 1740970394.5974545}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 275 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970394.5977883}).
Epoch: [5][277/500]	Time  6.140 ( 6.140)	Loss 1.1641 (1.7037)	CeLoss 1.1641 (0.6436)	SegCLSLoss 0.0000 (0.0126)	KLLoss 0.0000 (0.2180)	MaskLoss 0.0000 (0.5160)	MaskBCELoss 0.0000 (0.1579)	MaskDICELoss 0.0000 (0.3580)
Epoch: [5][278/500]	Time  6.840 ( 6.840)	Loss 2.4937 (1.5328)	CeLoss 0.2139 (0.5013)	SegCLSLoss 0.0234 (0.0111)	KLLoss 0.3789 (0.2191)	MaskLoss 1.1150 (0.5020)	MaskBCELoss 0.2460 (0.0940)	MaskDICELoss 0.8690 (0.4080)
Epoch: [5][279/500]	Time  8.192 ( 8.192)	Loss 2.3093 (1.7209)	CeLoss 0.2471 (0.4237)	SegCLSLoss 0.0162 (0.0121)	KLLoss 0.3555 (0.2920)	MaskLoss 1.0091 (0.6309)	MaskBCELoss 0.0331 (0.1267)	MaskDICELoss 0.9760 (0.5042)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 276 is less than current step: 499. Dropping entry: {'train/loss': 1.7036549925804139, 'train/ce_loss': 0.6435546875, 'train/seg_cls_loss': 0.012591552734375, 'train/kl_loss': 0.21796875, 'train/mask_bce_loss': 0.1579379953444004, 'train/mask_dice_loss': 0.35804966390132903, 'train/mask_loss': 0.515987652540207, 'metrics/total_secs_per_batch': 6.13990330696106, 'metrics/data_secs_per_batch': 2.580828237533569, '_timestamp': 1740970400.7372725}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 276 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970400.7374523}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 277 is less than current step: 499. Dropping entry: {'train/loss': 1.5327967643737792, 'train/ce_loss': 0.501318359375, 'train/seg_cls_loss': 0.011053466796875, 'train/kl_loss': 0.219140625, 'train/mask_bce_loss': 0.09404007457196713, 'train/mask_dice_loss': 0.40797843039035797, 'train/mask_loss': 0.5020184934139251, 'metrics/total_secs_per_batch': 6.8396430015563965, 'metrics/data_secs_per_batch': 3.12122642993927, '_timestamp': 1740970407.577053}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 277 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970407.5774019}).
[2025-03-02 20:53:46,047] [INFO] [logging.py:96:log_dist] [Rank 0] step=2780, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:53:46,053] [INFO] [timer.py:215:stop] epoch=0/micro_step=27800/global_step=2780, RunningAvgSamplesPerSec=1.4194285209876676, CurrSamplesPerSec=0.9724749936813415, MemAllocated=31.26GB, MaxMemAllocated=37.23GB
Epoch: [5][280/500]	Time 10.285 (10.285)	Loss 2.4157 (2.0567)	CeLoss 0.2217 (0.2114)	SegCLSLoss 0.0164 (0.0201)	KLLoss 0.3652 (0.3609)	MaskLoss 1.0750 (0.8997)	MaskBCELoss 0.3316 (0.1169)	MaskDICELoss 0.7434 (0.7828)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 278 is less than current step: 499. Dropping entry: {'train/loss': 1.7209315538406371, 'train/ce_loss': 0.42373046875, 'train/seg_cls_loss': 0.012078857421875, 'train/kl_loss': 0.2919921875, 'train/mask_bce_loss': 0.1267199380323291, 'train/mask_dice_loss': 0.5041560009121895, 'train/mask_loss': 0.6308759361505508, 'metrics/total_secs_per_batch': 8.192435503005981, 'metrics/data_secs_per_batch': 3.989261341094971, '_timestamp': 1740970415.7694345}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 278 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970415.7696376}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 279 is less than current step: 499. Dropping entry: {'train/loss': 2.056698775291443, 'train/ce_loss': 0.21142578125, 'train/seg_cls_loss': 0.0201171875, 'train/kl_loss': 0.3609375, 'train/mask_bce_loss': 0.11687895404174924, 'train/mask_dice_loss': 0.7828083157539367, 'train/mask_loss': 0.8996872723102569, 'metrics/total_secs_per_batch': 10.28458857536316, 'metrics/data_secs_per_batch': 4.8319145202636715, '_timestamp': 1740970426.0538692}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 279 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970426.054157}).
Epoch: [5][281/500]	Time  8.976 ( 8.976)	Loss 1.6016 (1.7478)	CeLoss 0.2305 (0.3638)	SegCLSLoss 0.0170 (0.0127)	KLLoss 0.3594 (0.3270)	MaskLoss 0.6631 (0.6723)	MaskBCELoss 0.0477 (0.1229)	MaskDICELoss 0.6154 (0.5494)
Epoch: [5][282/500]	Time 10.125 (10.125)	Loss 1.8975 (2.0985)	CeLoss 0.2021 (0.2361)	SegCLSLoss 0.0178 (0.0166)	KLLoss 0.3613 (0.3600)	MaskLoss 0.8252 (0.9088)	MaskBCELoss 0.0193 (0.1572)	MaskDICELoss 0.8060 (0.7516)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 280 is less than current step: 499. Dropping entry: {'train/loss': 1.7478466629981995, 'train/ce_loss': 0.36376953125, 'train/seg_cls_loss': 0.01270751953125, 'train/kl_loss': 0.326953125, 'train/mask_bce_loss': 0.12292196936905384, 'train/mask_dice_loss': 0.5493900328874588, 'train/mask_loss': 0.6723120003938675, 'metrics/total_secs_per_batch': 8.97641134262085, 'metrics/data_secs_per_batch': 3.76470091342926, '_timestamp': 1740970435.0304573}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 280 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970435.0307245}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 281 is less than current step: 499. Dropping entry: {'train/loss': 2.0984567999839783, 'train/ce_loss': 0.2361328125, 'train/seg_cls_loss': 0.0166259765625, 'train/kl_loss': 0.3599609375, 'train/mask_bce_loss': 0.1572207510471344, 'train/mask_dice_loss': 0.751577976346016, 'train/mask_loss': 0.9087987244129181, 'metrics/total_secs_per_batch': 10.124999284744263, 'metrics/data_secs_per_batch': 4.338670015335083, '_timestamp': 1740970445.1554785}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 281 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970445.1557634}).
Epoch: [5][283/500]	Time  8.767 ( 8.767)	Loss 2.0310 (1.4931)	CeLoss 0.2793 (0.2861)	SegCLSLoss 0.0128 (0.0112)	KLLoss 0.3672 (0.2561)	MaskLoss 0.8544 (0.5879)	MaskBCELoss 0.0775 (0.1423)	MaskDICELoss 0.7768 (0.4456)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 282 is less than current step: 499. Dropping entry: {'train/loss': 1.4931389331817626, 'train/ce_loss': 0.2861328125, 'train/seg_cls_loss': 0.01123046875, 'train/kl_loss': 0.2560546875, 'train/mask_bce_loss': 0.14231636952608823, 'train/mask_dice_loss': 0.445561683177948, 'train/mask_loss': 0.5878780603408813, 'metrics/total_secs_per_batch': 8.76702618598938, 'metrics/data_secs_per_batch': 3.8622546195983887, '_timestamp': 1740970453.9226122}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 282 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970453.9229295}).
Epoch: [5][284/500]	Time  9.200 ( 9.200)	Loss 1.3172 (1.6472)	CeLoss 0.2715 (0.3580)	SegCLSLoss 0.0157 (0.0125)	KLLoss 0.3613 (0.3271)	MaskLoss 0.5004 (0.6250)	MaskBCELoss 0.0869 (0.0842)	MaskDICELoss 0.4134 (0.5408)
Epoch: [5][285/500]	Time  8.335 ( 8.335)	Loss 1.4490 (1.3541)	CeLoss 0.2393 (0.4540)	SegCLSLoss 0.0137 (0.0103)	KLLoss 0.3633 (0.2535)	MaskLoss 0.5829 (0.4347)	MaskBCELoss 0.1188 (0.0724)	MaskDICELoss 0.4641 (0.3623)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 283 is less than current step: 499. Dropping entry: {'train/loss': 1.6472387075424195, 'train/ce_loss': 0.3580078125, 'train/seg_cls_loss': 0.012506103515625, 'train/kl_loss': 0.3271484375, 'train/mask_bce_loss': 0.08416640935465694, 'train/mask_dice_loss': 0.540820126235485, 'train/mask_loss': 0.6249865353107452, 'metrics/total_secs_per_batch': 9.199887037277222, 'metrics/data_secs_per_batch': 3.850818657875061, '_timestamp': 1740970463.1224144}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 283 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970463.1227124}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 284 is less than current step: 499. Dropping entry: {'train/loss': 1.3541275143623352, 'train/ce_loss': 0.45400390625, 'train/seg_cls_loss': 0.01031494140625, 'train/kl_loss': 0.253515625, 'train/mask_bce_loss': 0.0723865669220686, 'train/mask_dice_loss': 0.36234320923686025, 'train/mask_loss': 0.43472976982593536, 'metrics/total_secs_per_batch': 8.335193395614624, 'metrics/data_secs_per_batch': 3.594464063644409, '_timestamp': 1740970471.4575922}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 284 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970471.4578676}).
Epoch: [5][286/500]	Time  6.023 ( 6.023)	Loss 1.4062 (1.5050)	CeLoss 1.4062 (0.5825)	SegCLSLoss 0.0000 (0.0105)	KLLoss 0.0000 (0.2152)	MaskLoss 0.0000 (0.4478)	MaskBCELoss 0.0000 (0.0792)	MaskDICELoss 0.0000 (0.3687)
Epoch: [5][287/500]	Time  7.789 ( 7.789)	Loss 2.8973 (1.8109)	CeLoss 0.1680 (0.3636)	SegCLSLoss 0.0216 (0.0147)	KLLoss 0.3867 (0.2920)	MaskLoss 1.3402 (0.7054)	MaskBCELoss 0.5796 (0.1620)	MaskDICELoss 0.7607 (0.5434)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 285 is less than current step: 499. Dropping entry: {'train/loss': 1.5050363540649414, 'train/ce_loss': 0.58251953125, 'train/seg_cls_loss': 0.010528564453125, 'train/kl_loss': 0.215234375, 'train/mask_bce_loss': 0.07916278960183262, 'train/mask_dice_loss': 0.3686678886413574, 'train/mask_loss': 0.4478306770324707, 'metrics/total_secs_per_batch': 6.023426294326782, 'metrics/data_secs_per_batch': 2.854215216636658, '_timestamp': 1740970477.481016}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 285 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970477.4813614}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 286 is less than current step: 499. Dropping entry: {'train/loss': 1.8108723700046538, 'train/ce_loss': 0.36357421875, 'train/seg_cls_loss': 0.01470947265625, 'train/kl_loss': 0.2919921875, 'train/mask_bce_loss': 0.16195431314408779, 'train/mask_dice_loss': 0.5434330336749553, 'train/mask_loss': 0.7053873434662818, 'metrics/total_secs_per_batch': 7.788967609405518, 'metrics/data_secs_per_batch': 3.3607819080352783, '_timestamp': 1740970485.2702167}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 286 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970485.2705684}).
Epoch: [5][288/500]	Time  8.460 ( 8.460)	Loss 1.5078 (1.6749)	CeLoss 1.5078 (0.5250)	SegCLSLoss 0.0000 (0.0131)	KLLoss 0.0000 (0.3287)	MaskLoss 0.0000 (0.5549)	MaskBCELoss 0.0000 (0.1342)	MaskDICELoss 0.0000 (0.4206)
Epoch: [5][289/500]	Time  8.373 ( 8.373)	Loss 2.1700 (1.5747)	CeLoss 0.2305 (0.3884)	SegCLSLoss 0.0176 (0.0132)	KLLoss 0.3496 (0.2865)	MaskLoss 0.9483 (0.5755)	MaskBCELoss 0.0212 (0.0597)	MaskDICELoss 0.9270 (0.5158)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 287 is less than current step: 499. Dropping entry: {'train/loss': 1.674936193227768, 'train/ce_loss': 0.525, 'train/seg_cls_loss': 0.013092041015625, 'train/kl_loss': 0.3287109375, 'train/mask_bce_loss': 0.1342441054061055, 'train/mask_dice_loss': 0.42060680985450744, 'train/mask_loss': 0.5548509210348129, 'metrics/total_secs_per_batch': 8.460279703140259, 'metrics/data_secs_per_batch': 3.704697060585022, '_timestamp': 1740970493.7302938}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 287 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970493.7306314}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 288 is less than current step: 499. Dropping entry: {'train/loss': 1.574708592891693, 'train/ce_loss': 0.38837890625, 'train/seg_cls_loss': 0.013177490234375, 'train/kl_loss': 0.2865234375, 'train/mask_bce_loss': 0.059687814116477965, 'train/mask_dice_loss': 0.5158012419939041, 'train/mask_loss': 0.575489056110382, 'metrics/total_secs_per_batch': 8.372510433197021, 'metrics/data_secs_per_batch': 3.6598664283752442, '_timestamp': 1740970502.1027942}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 288 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970502.1030757}).
[2025-03-02 20:55:10,262] [INFO] [logging.py:96:log_dist] [Rank 0] step=2790, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:55:10,269] [INFO] [timer.py:215:stop] epoch=0/micro_step=27900/global_step=2790, RunningAvgSamplesPerSec=1.4184353734258652, CurrSamplesPerSec=1.2246528518474247, MemAllocated=31.15GB, MaxMemAllocated=37.23GB
Epoch: [5][290/500]	Time  8.167 ( 8.167)	Loss 2.5283 (1.5746)	CeLoss 0.1484 (0.2851)	SegCLSLoss 0.0302 (0.0151)	KLLoss 0.3809 (0.2979)	MaskLoss 1.1636 (0.6261)	MaskBCELoss 0.4336 (0.1268)	MaskDICELoss 0.7299 (0.4993)
Epoch: [5][291/500]	Time  7.997 ( 7.997)	Loss 1.7617 (1.5409)	CeLoss 0.2090 (0.4003)	SegCLSLoss 0.0096 (0.0100)	KLLoss 0.3672 (0.2574)	MaskLoss 0.7559 (0.5548)	MaskBCELoss 0.1751 (0.1195)	MaskDICELoss 0.5807 (0.4353)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 289 is less than current step: 499. Dropping entry: {'train/loss': 1.5746046423912048, 'train/ce_loss': 0.28505859375, 'train/seg_cls_loss': 0.01512451171875, 'train/kl_loss': 0.2978515625, 'train/mask_bce_loss': 0.1267530051060021, 'train/mask_dice_loss': 0.4993432581424713, 'train/mask_loss': 0.6260962605476379, 'metrics/total_secs_per_batch': 8.167126178741455, 'metrics/data_secs_per_batch': 3.355680513381958, '_timestamp': 1740970510.2697048}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 289 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970510.2698894}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 290 is less than current step: 499. Dropping entry: {'train/loss': 1.5409347534179687, 'train/ce_loss': 0.400341796875, 'train/seg_cls_loss': 0.009979248046875, 'train/kl_loss': 0.257421875, 'train/mask_bce_loss': 0.11952502466738224, 'train/mask_dice_loss': 0.43529293686151505, 'train/mask_loss': 0.5548179566860199, 'metrics/total_secs_per_batch': 7.996710300445557, 'metrics/data_secs_per_batch': 3.58797287940979, '_timestamp': 1740970518.2668247}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 290 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970518.267183}).
Epoch: [5][292/500]	Time  8.005 ( 8.005)	Loss 1.5808 (1.5914)	CeLoss 0.1953 (0.4790)	SegCLSLoss 0.0216 (0.0127)	KLLoss 0.3555 (0.2891)	MaskLoss 0.6693 (0.5385)	MaskBCELoss 0.1001 (0.0912)	MaskDICELoss 0.5692 (0.4473)
Epoch: [5][293/500]	Time  7.898 ( 7.898)	Loss 1.2179 (1.4673)	CeLoss 0.2295 (0.4164)	SegCLSLoss 0.0101 (0.0093)	KLLoss 0.3574 (0.2535)	MaskLoss 0.4742 (0.5103)	MaskBCELoss 0.0103 (0.1001)	MaskDICELoss 0.4639 (0.4102)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 291 is less than current step: 499. Dropping entry: {'train/loss': 1.5913901686668397, 'train/ce_loss': 0.47900390625, 'train/seg_cls_loss': 0.012689208984375, 'train/kl_loss': 0.2890625, 'train/mask_bce_loss': 0.09120119921863079, 'train/mask_dice_loss': 0.4473161414265633, 'train/mask_loss': 0.5385173410177231, 'metrics/total_secs_per_batch': 8.004851579666138, 'metrics/data_secs_per_batch': 3.081975746154785, '_timestamp': 1740970526.2714674}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 291 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970526.271733}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 292 is less than current step: 499. Dropping entry: {'train/loss': 1.4672937035560607, 'train/ce_loss': 0.41640625, 'train/seg_cls_loss': 0.009295654296875, 'train/kl_loss': 0.253515625, 'train/mask_bce_loss': 0.10010301759466529, 'train/mask_dice_loss': 0.41015516221523285, 'train/mask_loss': 0.5102581918239594, 'metrics/total_secs_per_batch': 7.898359298706055, 'metrics/data_secs_per_batch': 3.452091121673584, '_timestamp': 1740970534.1698482}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 292 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970534.1702032}).
Epoch: [5][294/500]	Time  8.222 ( 8.222)	Loss 1.7307 (1.7752)	CeLoss 0.1846 (0.4708)	SegCLSLoss 0.0222 (0.0122)	KLLoss 0.3496 (0.2873)	MaskLoss 0.7501 (0.6350)	MaskBCELoss 0.0066 (0.1246)	MaskDICELoss 0.7436 (0.5104)
Epoch: [5][295/500]	Time  6.969 ( 6.969)	Loss 1.7862 (1.8138)	CeLoss 0.2363 (0.3945)	SegCLSLoss 0.0111 (0.0133)	KLLoss 0.3633 (0.2975)	MaskLoss 0.7535 (0.6912)	MaskBCELoss 0.2840 (0.2093)	MaskDICELoss 0.4694 (0.4819)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 293 is less than current step: 499. Dropping entry: {'train/loss': 1.7752309322357178, 'train/ce_loss': 0.47080078125, 'train/seg_cls_loss': 0.012237548828125, 'train/kl_loss': 0.2873046875, 'train/mask_bce_loss': 0.12457655109465123, 'train/mask_dice_loss': 0.51040218770504, 'train/mask_loss': 0.6349787533283233, 'metrics/total_secs_per_batch': 8.222285270690918, 'metrics/data_secs_per_batch': 3.6138725757598875, '_timestamp': 1740970542.3921323}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 293 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970542.3923304}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 294 is less than current step: 499. Dropping entry: {'train/loss': 1.813762903213501, 'train/ce_loss': 0.39453125, 'train/seg_cls_loss': 0.013336181640625, 'train/kl_loss': 0.2974609375, 'train/mask_bce_loss': 0.2092760108411312, 'train/mask_dice_loss': 0.48188277184963224, 'train/mask_loss': 0.6911588042974472, 'metrics/total_secs_per_batch': 6.9689459800720215, 'metrics/data_secs_per_batch': 3.1136401891708374, '_timestamp': 1740970549.3611028}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 294 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970549.3614736}).
Epoch: [5][296/500]	Time  9.777 ( 9.777)	Loss 2.0027 (1.8887)	CeLoss 0.1895 (0.3261)	SegCLSLoss 0.0277 (0.0160)	KLLoss 0.3672 (0.3277)	MaskLoss 0.8812 (0.7610)	MaskBCELoss 0.0570 (0.1585)	MaskDICELoss 0.8243 (0.6025)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 295 is less than current step: 499. Dropping entry: {'train/loss': 1.88869366645813, 'train/ce_loss': 0.32607421875, 'train/seg_cls_loss': 0.01602783203125, 'train/kl_loss': 0.327734375, 'train/mask_bce_loss': 0.15846712030470372, 'train/mask_dice_loss': 0.6025300979614258, 'train/mask_loss': 0.7609972178936004, 'metrics/total_secs_per_batch': 9.776923894882202, 'metrics/data_secs_per_batch': 4.288429045677185, '_timestamp': 1740970559.1382654}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 295 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970559.1386511}).
Epoch: [5][297/500]	Time  8.114 ( 8.114)	Loss 2.3005 (1.7664)	CeLoss 0.2852 (0.4434)	SegCLSLoss 0.0090 (0.0111)	KLLoss 0.3633 (0.2916)	MaskLoss 0.9872 (0.6442)	MaskBCELoss 0.2767 (0.1199)	MaskDICELoss 0.7105 (0.5243)
Epoch: [5][298/500]	Time  7.731 ( 7.731)	Loss 1.8516 (1.4823)	CeLoss 0.2676 (0.3710)	SegCLSLoss 0.0090 (0.0117)	KLLoss 0.3613 (0.2523)	MaskLoss 0.7715 (0.5402)	MaskBCELoss 0.1868 (0.0698)	MaskDICELoss 0.5847 (0.4703)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 296 is less than current step: 499. Dropping entry: {'train/loss': 1.7663925647735597, 'train/ce_loss': 0.443359375, 'train/seg_cls_loss': 0.01107177734375, 'train/kl_loss': 0.2916015625, 'train/mask_bce_loss': 0.11990447100251914, 'train/mask_dice_loss': 0.5242781430482865, 'train/mask_loss': 0.6441826194524765, 'metrics/total_secs_per_batch': 8.113974332809448, 'metrics/data_secs_per_batch': 3.1659811973571776, '_timestamp': 1740970567.2520084}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 296 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970567.2522888}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 297 is less than current step: 499. Dropping entry: {'train/loss': 1.4823335826396942, 'train/ce_loss': 0.3709716796875, 'train/seg_cls_loss': 0.01170654296875, 'train/kl_loss': 0.25234375, 'train/mask_bce_loss': 0.06984519958496094, 'train/mask_dice_loss': 0.4703084021806717, 'train/mask_loss': 0.5401536017656327, 'metrics/total_secs_per_batch': 7.731180191040039, 'metrics/data_secs_per_batch': 3.277909517288208, '_timestamp': 1740970574.9831488}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 297 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970574.9834669}).
Epoch: [5][299/500]	Time  8.145 ( 8.145)	Loss 1.7925 (1.8468)	CeLoss 0.2520 (0.2047)	SegCLSLoss 0.0179 (0.0177)	KLLoss 0.3652 (0.3318)	MaskLoss 0.7468 (0.7999)	MaskBCELoss 0.0352 (0.1918)	MaskDICELoss 0.7116 (0.6081)
[2025-03-02 20:56:29,624] [INFO] [logging.py:96:log_dist] [Rank 0] step=2800, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:56:29,630] [INFO] [timer.py:215:stop] epoch=0/micro_step=28000/global_step=2800, RunningAvgSamplesPerSec=1.417799401592421, CurrSamplesPerSec=1.5381546181487036, MemAllocated=30.69GB, MaxMemAllocated=37.23GB
Epoch: [5][300/500]	Time  6.503 ( 6.503)	Loss 1.3750 (1.5560)	CeLoss 1.3750 (0.6014)	SegCLSLoss 0.0000 (0.0124)	KLLoss 0.0000 (0.2199)	MaskLoss 0.0000 (0.4631)	MaskBCELoss 0.0000 (0.0828)	MaskDICELoss 0.0000 (0.3804)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 298 is less than current step: 499. Dropping entry: {'train/loss': 1.8467687129974366, 'train/ce_loss': 0.2046875, 'train/seg_cls_loss': 0.017669677734375, 'train/kl_loss': 0.3318359375, 'train/mask_bce_loss': 0.1918163876980543, 'train/mask_dice_loss': 0.6081304892897605, 'train/mask_loss': 0.7999468624591828, 'metrics/total_secs_per_batch': 8.144912958145142, 'metrics/data_secs_per_batch': 3.750644493103027, '_timestamp': 1740970583.128084}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 298 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970583.1283414}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 299 is less than current step: 499. Dropping entry: {'train/loss': 1.5560479402542113, 'train/ce_loss': 0.6013671875, 'train/seg_cls_loss': 0.01241455078125, 'train/kl_loss': 0.219921875, 'train/mask_bce_loss': 0.08275569975376129, 'train/mask_dice_loss': 0.38037568628787993, 'train/mask_loss': 0.4631313860416412, 'metrics/total_secs_per_batch': 6.502906084060669, 'metrics/data_secs_per_batch': 3.24537136554718, '_timestamp': 1740970589.6308084}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 299 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970589.6310985}).
Epoch: [5][301/500]	Time  7.209 ( 7.209)	Loss 1.0691 (1.8984)	CeLoss 0.3945 (0.5710)	SegCLSLoss 0.0093 (0.0115)	KLLoss 0.3594 (0.2545)	MaskLoss 0.3178 (0.6483)	MaskBCELoss 0.1946 (0.1615)	MaskDICELoss 0.1231 (0.4868)
Epoch: [5][302/500]	Time  6.620 ( 6.620)	Loss 2.0648 (1.5483)	CeLoss 0.2119 (0.4278)	SegCLSLoss 0.0177 (0.0131)	KLLoss 0.3555 (0.2543)	MaskLoss 0.9045 (0.5443)	MaskBCELoss 0.0780 (0.1090)	MaskDICELoss 0.8265 (0.4352)
Epoch: [5][303/500]	Time  6.687 ( 6.687)	Loss 1.1172 (1.0333)	CeLoss 1.1172 (0.4636)	SegCLSLoss 0.0000 (0.0067)	KLLoss 0.0000 (0.1449)	MaskLoss 0.0000 (0.2759)	MaskBCELoss 0.0000 (0.0648)	MaskDICELoss 0.0000 (0.2110)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 300 is less than current step: 499. Dropping entry: {'train/loss': 1.8984392642974854, 'train/ce_loss': 0.57099609375, 'train/seg_cls_loss': 0.011474609375, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.1614845057018101, 'train/mask_dice_loss': 0.4868073970079422, 'train/mask_loss': 0.6482918947935105, 'metrics/total_secs_per_batch': 7.208539009094238, 'metrics/data_secs_per_batch': 3.032710146903992, '_timestamp': 1740970596.839555}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 300 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970596.839892}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 301 is less than current step: 499. Dropping entry: {'train/loss': 1.5483447670936585, 'train/ce_loss': 0.427783203125, 'train/seg_cls_loss': 0.013104248046875, 'train/kl_loss': 0.254296875, 'train/mask_bce_loss': 0.1090343775227666, 'train/mask_dice_loss': 0.43523076474666594, 'train/mask_loss': 0.5442651480436325, 'metrics/total_secs_per_batch': 6.620051383972168, 'metrics/data_secs_per_batch': 2.591369152069092, '_timestamp': 1740970603.4596946}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 301 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970603.4600978}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 302 is less than current step: 499. Dropping entry: {'train/loss': 1.033275407552719, 'train/ce_loss': 0.463623046875, 'train/seg_cls_loss': 0.006683349609375, 'train/kl_loss': 0.144921875, 'train/mask_bce_loss': 0.06484445445239544, 'train/mask_dice_loss': 0.21104617565870284, 'train/mask_loss': 0.27589063048362733, 'metrics/total_secs_per_batch': 6.6874213218688965, 'metrics/data_secs_per_batch': 3.0718162298202514, '_timestamp': 1740970610.1470132}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 302 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970610.147295}).
Epoch: [5][304/500]	Time  8.606 ( 8.606)	Loss 1.5129 (1.6300)	CeLoss 0.1895 (0.3021)	SegCLSLoss 0.0208 (0.0149)	KLLoss 0.3613 (0.3281)	MaskLoss 0.6383 (0.6438)	MaskBCELoss 0.0819 (0.0913)	MaskDICELoss 0.5564 (0.5525)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 303 is less than current step: 499. Dropping entry: {'train/loss': 1.6299944877624513, 'train/ce_loss': 0.3021484375, 'train/seg_cls_loss': 0.014874267578125, 'train/kl_loss': 0.328125, 'train/mask_bce_loss': 0.09129873039200902, 'train/mask_dice_loss': 0.5524582758545875, 'train/mask_loss': 0.6437570065259933, 'metrics/total_secs_per_batch': 8.605894565582275, 'metrics/data_secs_per_batch': 4.11248197555542, '_timestamp': 1740970618.7529585}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 303 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970618.7532506}).
Epoch: [5][305/500]	Time  9.445 ( 9.445)	Loss 1.4094 (1.9370)	CeLoss 0.1895 (0.3468)	SegCLSLoss 0.0193 (0.0147)	KLLoss 0.3574 (0.3273)	MaskLoss 0.5875 (0.7751)	MaskBCELoss 0.0255 (0.1676)	MaskDICELoss 0.5620 (0.6075)
Epoch: [5][306/500]	Time  7.883 ( 7.883)	Loss 1.8113 (1.5258)	CeLoss 0.2100 (0.3719)	SegCLSLoss 0.0164 (0.0140)	KLLoss 0.3633 (0.2918)	MaskLoss 0.7787 (0.5590)	MaskBCELoss 0.0499 (0.0898)	MaskDICELoss 0.7288 (0.4692)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 304 is less than current step: 499. Dropping entry: {'train/loss': 1.9370127320289612, 'train/ce_loss': 0.34677734375, 'train/seg_cls_loss': 0.014654541015625, 'train/kl_loss': 0.32734375, 'train/mask_bce_loss': 0.16755926460027695, 'train/mask_dice_loss': 0.6075389087200165, 'train/mask_loss': 0.7750981628894806, 'metrics/total_secs_per_batch': 9.445169448852539, 'metrics/data_secs_per_batch': 4.256686973571777, '_timestamp': 1740970628.1981454}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 304 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970628.1984355}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 305 is less than current step: 499. Dropping entry: {'train/loss': 1.5258049070835114, 'train/ce_loss': 0.371875, 'train/seg_cls_loss': 0.013970947265625, 'train/kl_loss': 0.291796875, 'train/mask_bce_loss': 0.08980592107400298, 'train/mask_dice_loss': 0.4691902816295624, 'train/mask_loss': 0.5589962035417557, 'metrics/total_secs_per_batch': 7.882523775100708, 'metrics/data_secs_per_batch': 3.0612217664718626, '_timestamp': 1740970636.0806546}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 305 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970636.0809948}).
Epoch: [5][307/500]	Time  6.273 ( 6.273)	Loss 2.2531 (1.3255)	CeLoss 0.1719 (0.5323)	SegCLSLoss 0.0182 (0.0082)	KLLoss 0.3555 (0.2182)	MaskLoss 1.0182 (0.3836)	MaskBCELoss 0.0307 (0.0549)	MaskDICELoss 0.9875 (0.3287)
Epoch: [5][308/500]	Time  8.344 ( 8.344)	Loss 2.3721 (1.5636)	CeLoss 0.2070 (0.3425)	SegCLSLoss 0.0181 (0.0132)	KLLoss 0.3887 (0.2906)	MaskLoss 1.0591 (0.5929)	MaskBCELoss 0.2318 (0.0803)	MaskDICELoss 0.8273 (0.5126)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 306 is less than current step: 499. Dropping entry: {'train/loss': 1.3255079686641693, 'train/ce_loss': 0.53232421875, 'train/seg_cls_loss': 0.00819091796875, 'train/kl_loss': 0.2181640625, 'train/mask_bce_loss': 0.054855924332514405, 'train/mask_dice_loss': 0.3286988392472267, 'train/mask_loss': 0.38355475962162017, 'metrics/total_secs_per_batch': 6.27348518371582, 'metrics/data_secs_per_batch': 2.619965410232544, '_timestamp': 1740970642.3541439}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 306 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970642.3545735}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 307 is less than current step: 499. Dropping entry: {'train/loss': 1.563624382019043, 'train/ce_loss': 0.342529296875, 'train/seg_cls_loss': 0.0132080078125, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.08026322033256292, 'train/mask_dice_loss': 0.5126085460186005, 'train/mask_loss': 0.5928717613220215, 'metrics/total_secs_per_batch': 8.343887090682983, 'metrics/data_secs_per_batch': 3.7855367183685305, '_timestamp': 1740970650.6980095}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 307 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970650.6983309}).
Epoch: [5][309/500]	Time  7.592 ( 7.592)	Loss 1.4453 (1.4592)	CeLoss 1.4453 (0.4210)	SegCLSLoss 0.0000 (0.0116)	KLLoss 0.0000 (0.2504)	MaskLoss 0.0000 (0.5035)	MaskBCELoss 0.0000 (0.0796)	MaskDICELoss 0.0000 (0.4239)
[2025-03-02 20:57:45,259] [INFO] [logging.py:96:log_dist] [Rank 0] step=2810, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:57:45,264] [INFO] [timer.py:215:stop] epoch=0/micro_step=28100/global_step=2810, RunningAvgSamplesPerSec=1.4174351137406642, CurrSamplesPerSec=1.4339990620534355, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [5][310/500]	Time  6.975 ( 6.975)	Loss 2.3787 (1.6562)	CeLoss 0.2539 (0.5080)	SegCLSLoss 0.0165 (0.0139)	KLLoss 0.3633 (0.2594)	MaskLoss 1.0399 (0.5577)	MaskBCELoss 0.3927 (0.1216)	MaskDICELoss 0.6472 (0.4361)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 308 is less than current step: 499. Dropping entry: {'train/loss': 1.459221851825714, 'train/ce_loss': 0.42099609375, 'train/seg_cls_loss': 0.01163330078125, 'train/kl_loss': 0.250390625, 'train/mask_bce_loss': 0.07963900025933981, 'train/mask_dice_loss': 0.4238977074623108, 'train/mask_loss': 0.503536707162857, 'metrics/total_secs_per_batch': 7.59236478805542, 'metrics/data_secs_per_batch': 3.227036714553833, '_timestamp': 1740970658.2904038}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 308 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970658.2906086}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 309 is less than current step: 499. Dropping entry: {'train/loss': 1.65616272687912, 'train/ce_loss': 0.5080078125, 'train/seg_cls_loss': 0.013909912109375, 'train/kl_loss': 0.259375, 'train/mask_bce_loss': 0.12158195972442627, 'train/mask_dice_loss': 0.43608924746513367, 'train/mask_loss': 0.5576712056994438, 'metrics/total_secs_per_batch': 6.975114583969116, 'metrics/data_secs_per_batch': 2.853972864151001, '_timestamp': 1740970665.2652986}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 309 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970665.2655854}).
Epoch: [5][311/500]	Time  7.626 ( 7.626)	Loss 2.3016 (1.6689)	CeLoss 0.2422 (0.4109)	SegCLSLoss 0.0260 (0.0147)	KLLoss 0.3555 (0.2883)	MaskLoss 1.0053 (0.6109)	MaskBCELoss 0.0099 (0.0790)	MaskDICELoss 0.9954 (0.5319)
Epoch: [5][312/500]	Time  7.585 ( 7.585)	Loss 2.0496 (1.7448)	CeLoss 0.2129 (0.4019)	SegCLSLoss 0.0181 (0.0135)	KLLoss 0.3496 (0.2881)	MaskLoss 0.8969 (0.6538)	MaskBCELoss 0.0263 (0.1173)	MaskDICELoss 0.8706 (0.5365)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 310 is less than current step: 499. Dropping entry: {'train/loss': 1.6689108967781068, 'train/ce_loss': 0.4109375, 'train/seg_cls_loss': 0.0146728515625, 'train/kl_loss': 0.28828125, 'train/mask_bce_loss': 0.07895365282893181, 'train/mask_dice_loss': 0.5319177970290184, 'train/mask_loss': 0.6108714520931244, 'metrics/total_secs_per_batch': 7.625532627105713, 'metrics/data_secs_per_batch': 3.6840279579162596, '_timestamp': 1740970672.8910344}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 310 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970672.8912292}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 311 is less than current step: 499. Dropping entry: {'train/loss': 1.7448029577732087, 'train/ce_loss': 0.40185546875, 'train/seg_cls_loss': 0.0135009765625, 'train/kl_loss': 0.2880859375, 'train/mask_bce_loss': 0.11725349090993405, 'train/mask_dice_loss': 0.5365444764494895, 'train/mask_loss': 0.6537979692220688, 'metrics/total_secs_per_batch': 7.584748268127441, 'metrics/data_secs_per_batch': 3.8714629888534544, '_timestamp': 1740970680.475797}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 311 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970680.4761593}).
Epoch: [5][313/500]	Time  7.422 ( 7.422)	Loss 1.1797 (1.7346)	CeLoss 1.1797 (0.3870)	SegCLSLoss 0.0000 (0.0146)	KLLoss 0.0000 (0.2893)	MaskLoss 0.0000 (0.6558)	MaskBCELoss 0.0000 (0.1433)	MaskDICELoss 0.0000 (0.5125)
Epoch: [5][314/500]	Time  8.594 ( 8.594)	Loss 1.6801 (2.0647)	CeLoss 0.2422 (0.2457)	SegCLSLoss 0.0147 (0.0178)	KLLoss 0.3672 (0.3299)	MaskLoss 0.6965 (0.8885)	MaskBCELoss 0.1525 (0.2015)	MaskDICELoss 0.5440 (0.6869)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 312 is less than current step: 499. Dropping entry: {'train/loss': 1.7345559477806092, 'train/ce_loss': 0.38701171875, 'train/seg_cls_loss': 0.01458740234375, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.14332882333546876, 'train/mask_dice_loss': 0.5124745428562164, 'train/mask_loss': 0.6558033645153045, 'metrics/total_secs_per_batch': 7.42227840423584, 'metrics/data_secs_per_batch': 3.225102162361145, '_timestamp': 1740970687.8980155}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 312 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970687.8981984}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 313 is less than current step: 499. Dropping entry: {'train/loss': 2.0647042393684387, 'train/ce_loss': 0.245703125, 'train/seg_cls_loss': 0.01778564453125, 'train/kl_loss': 0.3298828125, 'train/mask_bce_loss': 0.2015196330845356, 'train/mask_dice_loss': 0.6869359970092773, 'train/mask_loss': 0.8884556353092193, 'metrics/total_secs_per_batch': 8.594199419021606, 'metrics/data_secs_per_batch': 4.192233061790466, '_timestamp': 1740970696.4922633}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 313 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970696.4926271}).
Epoch: [5][315/500]	Time  7.382 ( 7.382)	Loss 1.3574 (1.4284)	CeLoss 0.3066 (0.3648)	SegCLSLoss 0.0117 (0.0134)	KLLoss 0.3594 (0.2904)	MaskLoss 0.5049 (0.5140)	MaskBCELoss 0.1605 (0.1557)	MaskDICELoss 0.3443 (0.3583)
Epoch: [5][316/500]	Time  6.046 ( 6.046)	Loss 1.7862 (1.1702)	CeLoss 0.2656 (0.6695)	SegCLSLoss 0.0132 (0.0033)	KLLoss 0.3574 (0.1086)	MaskLoss 0.7398 (0.2441)	MaskBCELoss 0.0515 (0.0528)	MaskDICELoss 0.6883 (0.1913)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 314 is less than current step: 499. Dropping entry: {'train/loss': 1.428361451625824, 'train/ce_loss': 0.364794921875, 'train/seg_cls_loss': 0.01337890625, 'train/kl_loss': 0.2904296875, 'train/mask_bce_loss': 0.15569423735141755, 'train/mask_dice_loss': 0.358266769349575, 'train/mask_loss': 0.5139609992504119, 'metrics/total_secs_per_batch': 7.381507158279419, 'metrics/data_secs_per_batch': 3.034714412689209, '_timestamp': 1740970703.8737574}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 314 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970703.8740425}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 315 is less than current step: 499. Dropping entry: {'train/loss': 1.170191729068756, 'train/ce_loss': 0.669482421875, 'train/seg_cls_loss': 0.0033447265625, 'train/kl_loss': 0.10859375, 'train/mask_bce_loss': 0.05276297405362129, 'train/mask_dice_loss': 0.19134168624877929, 'train/mask_loss': 0.24410465359687805, 'metrics/total_secs_per_batch': 6.0459253787994385, 'metrics/data_secs_per_batch': 2.4162927150726317, '_timestamp': 1740970709.919715}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 315 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970709.9200635}).
Epoch: [5][317/500]	Time  8.183 ( 8.183)	Loss 1.6854 (1.6634)	CeLoss 0.2324 (0.3414)	SegCLSLoss 0.0192 (0.0150)	KLLoss 0.3555 (0.3244)	MaskLoss 0.7040 (0.6412)	MaskBCELoss 0.0669 (0.1188)	MaskDICELoss 0.6371 (0.5224)
Epoch: [5][318/500]	Time  8.858 ( 8.858)	Loss 2.1992 (1.8012)	CeLoss 0.2207 (0.3829)	SegCLSLoss 0.0204 (0.0172)	KLLoss 0.3555 (0.2859)	MaskLoss 0.9668 (0.6905)	MaskBCELoss 0.1147 (0.0589)	MaskDICELoss 0.8521 (0.6317)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 316 is less than current step: 499. Dropping entry: {'train/loss': 1.6633885264396668, 'train/ce_loss': 0.341357421875, 'train/seg_cls_loss': 0.014984130859375, 'train/kl_loss': 0.3244140625, 'train/mask_bce_loss': 0.11880981419235467, 'train/mask_dice_loss': 0.522357115149498, 'train/mask_loss': 0.6411669254302979, 'metrics/total_secs_per_batch': 8.182624101638794, 'metrics/data_secs_per_batch': 3.6070403099060058, '_timestamp': 1740970718.1023662}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 316 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970718.1027014}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 317 is less than current step: 499. Dropping entry: {'train/loss': 1.8011773586273194, 'train/ce_loss': 0.38291015625, 'train/seg_cls_loss': 0.0171875, 'train/kl_loss': 0.2859375, 'train/mask_bce_loss': 0.05887752720154822, 'train/mask_dice_loss': 0.6316525638103485, 'train/mask_loss': 0.6905300855636597, 'metrics/total_secs_per_batch': 8.858110427856445, 'metrics/data_secs_per_batch': 4.146571350097656, '_timestamp': 1740970726.960456}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 317 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970726.9607553}).
Epoch: [5][319/500]	Time  7.116 ( 7.116)	Loss 2.2845 (1.7716)	CeLoss 0.2422 (0.6376)	SegCLSLoss 0.0192 (0.0123)	KLLoss 0.3594 (0.2223)	MaskLoss 0.9987 (0.5529)	MaskBCELoss 0.0143 (0.0836)	MaskDICELoss 0.9844 (0.4693)
[2025-03-02 20:59:02,231] [INFO] [logging.py:96:log_dist] [Rank 0] step=2820, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 20:59:02,236] [INFO] [timer.py:215:stop] epoch=0/micro_step=28200/global_step=2820, RunningAvgSamplesPerSec=1.4169782643706073, CurrSamplesPerSec=1.2256036639445471, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [5][320/500]	Time  8.161 ( 8.161)	Loss 1.5541 (1.6851)	CeLoss 0.2178 (0.3705)	SegCLSLoss 0.0189 (0.0171)	KLLoss 0.3555 (0.3254)	MaskLoss 0.6452 (0.6369)	MaskBCELoss 0.0795 (0.1319)	MaskDICELoss 0.5657 (0.5049)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 318 is less than current step: 499. Dropping entry: {'train/loss': 1.7716313123703002, 'train/ce_loss': 0.63759765625, 'train/seg_cls_loss': 0.012261962890625, 'train/kl_loss': 0.222265625, 'train/mask_bce_loss': 0.0835642266087234, 'train/mask_dice_loss': 0.4692924529314041, 'train/mask_loss': 0.5528566837310791, 'metrics/total_secs_per_batch': 7.116157531738281, 'metrics/data_secs_per_batch': 2.898305606842041, '_timestamp': 1740970734.0765862}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 318 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970734.0768669}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 319 is less than current step: 499. Dropping entry: {'train/loss': 1.6851293683052062, 'train/ce_loss': 0.3705078125, 'train/seg_cls_loss': 0.017095947265625, 'train/kl_loss': 0.325390625, 'train/mask_bce_loss': 0.1319207103922963, 'train/mask_dice_loss': 0.5049310833215713, 'train/mask_loss': 0.6368517875671387, 'metrics/total_secs_per_batch': 8.16084361076355, 'metrics/data_secs_per_batch': 3.310256028175354, '_timestamp': 1740970742.2372913}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 319 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970742.2376094}).
Epoch: [5][321/500]	Time  5.906 ( 5.906)	Loss 1.3388 (1.3650)	CeLoss 0.2539 (0.6810)	SegCLSLoss 0.0161 (0.0082)	KLLoss 0.3613 (0.1484)	MaskLoss 0.5210 (0.3327)	MaskBCELoss 0.0168 (0.0648)	MaskDICELoss 0.5042 (0.2679)
Epoch: [5][322/500]	Time 10.627 (10.627)	Loss 1.3660 (1.9514)	CeLoss 0.2480 (0.2536)	SegCLSLoss 0.0172 (0.0158)	KLLoss 0.3633 (0.3646)	MaskLoss 0.5365 (0.8268)	MaskBCELoss 0.1380 (0.1596)	MaskDICELoss 0.3985 (0.6672)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 320 is less than current step: 499. Dropping entry: {'train/loss': 1.364995265007019, 'train/ce_loss': 0.681005859375, 'train/seg_cls_loss': 0.0081787109375, 'train/kl_loss': 0.1484375, 'train/mask_bce_loss': 0.06477353312075138, 'train/mask_dice_loss': 0.2679438292980194, 'train/mask_loss': 0.3327173590660095, 'metrics/total_secs_per_batch': 5.906184911727905, 'metrics/data_secs_per_batch': 2.2817830801010133, '_timestamp': 1740970748.1436756}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 320 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970748.143984}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 321 is less than current step: 499. Dropping entry: {'train/loss': 1.9513798356056213, 'train/ce_loss': 0.25361328125, 'train/seg_cls_loss': 0.01578369140625, 'train/kl_loss': 0.3646484375, 'train/mask_bce_loss': 0.15959266535937786, 'train/mask_dice_loss': 0.6671714603900909, 'train/mask_loss': 0.8267641365528107, 'metrics/total_secs_per_batch': 10.626904010772705, 'metrics/data_secs_per_batch': 4.617709922790527, '_timestamp': 1740970758.7705197}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 321 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970758.7708035}).
Epoch: [5][323/500]	Time  6.775 ( 6.775)	Loss 2.6528 (1.8714)	CeLoss 0.2256 (0.4317)	SegCLSLoss 0.0154 (0.0141)	KLLoss 0.3535 (0.2525)	MaskLoss 1.1916 (0.7035)	MaskBCELoss 0.3789 (0.0980)	MaskDICELoss 0.8127 (0.6055)
Epoch: [5][324/500]	Time  7.365 ( 7.365)	Loss 2.4870 (1.5368)	CeLoss 0.1855 (0.2868)	SegCLSLoss 0.0258 (0.0143)	KLLoss 0.3770 (0.2963)	MaskLoss 1.1253 (0.6067)	MaskBCELoss 0.3760 (0.0857)	MaskDICELoss 0.7493 (0.5210)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 322 is less than current step: 499. Dropping entry: {'train/loss': 1.8713699221611022, 'train/ce_loss': 0.43173828125, 'train/seg_cls_loss': 0.014105224609375, 'train/kl_loss': 0.2525390625, 'train/mask_bce_loss': 0.09803173672407865, 'train/mask_dice_loss': 0.6054754793643952, 'train/mask_loss': 0.7035072088241577, 'metrics/total_secs_per_batch': 6.774980306625366, 'metrics/data_secs_per_batch': 3.083556604385376, '_timestamp': 1740970765.545572}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 322 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970765.5459309}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 323 is less than current step: 499. Dropping entry: {'train/loss': 1.5367668509483337, 'train/ce_loss': 0.286767578125, 'train/seg_cls_loss': 0.014312744140625, 'train/kl_loss': 0.2962890625, 'train/mask_bce_loss': 0.08572347564622759, 'train/mask_dice_loss': 0.5209656119346618, 'train/mask_loss': 0.6066890865564346, 'metrics/total_secs_per_batch': 7.3646509647369385, 'metrics/data_secs_per_batch': 3.1246933221817015, '_timestamp': 1740970772.9103956}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 323 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970772.9107504}).
Epoch: [5][325/500]	Time  8.255 ( 8.255)	Loss 0.1846 (1.7049)	CeLoss 0.1846 (0.3025)	SegCLSLoss 0.0000 (0.0149)	KLLoss 0.0000 (0.2566)	MaskLoss 0.0000 (0.6846)	MaskBCELoss 0.0000 (0.1869)	MaskDICELoss 0.0000 (0.4977)
Epoch: [5][326/500]	Time  6.718 ( 6.718)	Loss 1.2109 (1.7812)	CeLoss 1.2109 (0.4092)	SegCLSLoss 0.0000 (0.0137)	KLLoss 0.0000 (0.2926)	MaskLoss 0.0000 (0.6681)	MaskBCELoss 0.0000 (0.1768)	MaskDICELoss 0.0000 (0.4913)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 324 is less than current step: 499. Dropping entry: {'train/loss': 1.704875123500824, 'train/ce_loss': 0.3025390625, 'train/seg_cls_loss': 0.01488037109375, 'train/kl_loss': 0.256640625, 'train/mask_bce_loss': 0.18691750094294549, 'train/mask_dice_loss': 0.4976978003978729, 'train/mask_loss': 0.6846152931451798, 'metrics/total_secs_per_batch': 8.255069971084595, 'metrics/data_secs_per_batch': 3.710894536972046, '_timestamp': 1740970781.1652968}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 324 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970781.1654944}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 325 is less than current step: 499. Dropping entry: {'train/loss': 1.7812430441379548, 'train/ce_loss': 0.4091796875, 'train/seg_cls_loss': 0.01370849609375, 'train/kl_loss': 0.292578125, 'train/mask_bce_loss': 0.1767709545791149, 'train/mask_dice_loss': 0.49129197001457214, 'train/mask_loss': 0.6680629193782807, 'metrics/total_secs_per_batch': 6.718043327331543, 'metrics/data_secs_per_batch': 3.4529287815093994, '_timestamp': 1740970787.8833032}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 325 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970787.8835835}).
Epoch: [5][327/500]	Time  7.402 ( 7.402)	Loss 1.3969 (1.6640)	CeLoss 0.2012 (0.4081)	SegCLSLoss 0.0219 (0.0136)	KLLoss 0.3633 (0.2934)	MaskLoss 0.5744 (0.6099)	MaskBCELoss 0.1203 (0.1367)	MaskDICELoss 0.4542 (0.4732)
Epoch: [5][328/500]	Time  8.670 ( 8.670)	Loss 1.2734 (1.3534)	CeLoss 1.2734 (0.3915)	SegCLSLoss 0.0000 (0.0109)	KLLoss 0.0000 (0.2152)	MaskLoss 0.0000 (0.4675)	MaskBCELoss 0.0000 (0.0351)	MaskDICELoss 0.0000 (0.4324)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 326 is less than current step: 499. Dropping entry: {'train/loss': 1.664017629623413, 'train/ce_loss': 0.40810546875, 'train/seg_cls_loss': 0.013623046875, 'train/kl_loss': 0.293359375, 'train/mask_bce_loss': 0.13670283760875462, 'train/mask_dice_loss': 0.47323564887046815, 'train/mask_loss': 0.6099384903907776, 'metrics/total_secs_per_batch': 7.402300834655762, 'metrics/data_secs_per_batch': 3.0201704263687135, '_timestamp': 1740970795.2856052}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 326 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970795.285886}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 327 is less than current step: 499. Dropping entry: {'train/loss': 1.3534181594848633, 'train/ce_loss': 0.39150390625, 'train/seg_cls_loss': 0.010894775390625, 'train/kl_loss': 0.215234375, 'train/mask_bce_loss': 0.035060820542275904, 'train/mask_dice_loss': 0.4324197471141815, 'train/mask_loss': 0.4674805760383606, 'metrics/total_secs_per_batch': 8.670482397079468, 'metrics/data_secs_per_batch': 3.723516011238098, '_timestamp': 1740970803.956265}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 327 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970803.9566076}).
Epoch: [5][329/500]	Time  8.426 ( 8.426)	Loss 1.7850 (2.0954)	CeLoss 0.1641 (0.3346)	SegCLSLoss 0.0253 (0.0174)	KLLoss 0.3555 (0.3291)	MaskLoss 0.7861 (0.8596)	MaskBCELoss 0.0376 (0.1859)	MaskDICELoss 0.7485 (0.6737)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 328 is less than current step: 499. Dropping entry: {'train/loss': 2.095351982116699, 'train/ce_loss': 0.3345703125, 'train/seg_cls_loss': 0.017449951171875, 'train/kl_loss': 0.3291015625, 'train/mask_bce_loss': 0.185869275406003, 'train/mask_dice_loss': 0.673720782995224, 'train/mask_loss': 0.8595900595188141, 'metrics/total_secs_per_batch': 8.4258713722229, 'metrics/data_secs_per_batch': 3.6701066493988037, '_timestamp': 1740970812.3819764}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 328 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970812.3821774}).
[2025-03-02 21:00:21,194] [INFO] [logging.py:96:log_dist] [Rank 0] step=2830, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:00:21,199] [INFO] [timer.py:215:stop] epoch=0/micro_step=28300/global_step=2830, RunningAvgSamplesPerSec=1.4163837317332533, CurrSamplesPerSec=1.1342058784382754, MemAllocated=31.26GB, MaxMemAllocated=37.23GB
Epoch: [5][330/500]	Time  8.818 ( 8.818)	Loss 1.6708 (1.2613)	CeLoss 0.2637 (0.4548)	SegCLSLoss 0.0161 (0.0082)	KLLoss 0.3711 (0.2188)	MaskLoss 0.6811 (0.3903)	MaskBCELoss 0.0811 (0.0833)	MaskDICELoss 0.6000 (0.3070)
Epoch: [5][331/500]	Time  8.461 ( 8.461)	Loss 2.5393 (2.0447)	CeLoss 0.2949 (0.2203)	SegCLSLoss 0.0182 (0.0170)	KLLoss 0.3848 (0.3334)	MaskLoss 1.0978 (0.8911)	MaskBCELoss 0.2377 (0.2138)	MaskDICELoss 0.8600 (0.6773)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 329 is less than current step: 499. Dropping entry: {'train/loss': 1.2612510055303574, 'train/ce_loss': 0.454833984375, 'train/seg_cls_loss': 0.00816650390625, 'train/kl_loss': 0.21875, 'train/mask_bce_loss': 0.08334612580947579, 'train/mask_dice_loss': 0.3069717586040497, 'train/mask_loss': 0.3903178792446852, 'metrics/total_secs_per_batch': 8.818312168121338, 'metrics/data_secs_per_batch': 3.649972438812256, '_timestamp': 1740970821.2001417}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 329 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970821.2004595}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 330 is less than current step: 499. Dropping entry: {'train/loss': 2.044745397567749, 'train/ce_loss': 0.220263671875, 'train/seg_cls_loss': 0.016986083984375, 'train/kl_loss': 0.3333984375, 'train/mask_bce_loss': 0.2138160891830921, 'train/mask_dice_loss': 0.677306592464447, 'train/mask_loss': 0.8911226868629456, 'metrics/total_secs_per_batch': 8.461338996887207, 'metrics/data_secs_per_batch': 3.921369171142578, '_timestamp': 1740970829.6616435}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 330 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970829.6619377}).
Epoch: [5][332/500]	Time  7.579 ( 7.579)	Loss 1.4602 (1.4599)	CeLoss 0.2617 (0.3050)	SegCLSLoss 0.0114 (0.0125)	KLLoss 0.3633 (0.2930)	MaskLoss 0.5777 (0.5595)	MaskBCELoss 0.0687 (0.0800)	MaskDICELoss 0.5091 (0.4795)
Epoch: [5][333/500]	Time  9.100 ( 9.100)	Loss 2.2896 (1.8583)	CeLoss 0.2520 (0.3122)	SegCLSLoss 0.0115 (0.0140)	KLLoss 0.3594 (0.3229)	MaskLoss 0.9983 (0.7535)	MaskBCELoss 0.0032 (0.1764)	MaskDICELoss 0.9951 (0.5771)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 331 is less than current step: 499. Dropping entry: {'train/loss': 1.4598818361759185, 'train/ce_loss': 0.30498046875, 'train/seg_cls_loss': 0.012518310546875, 'train/kl_loss': 0.29296875, 'train/mask_bce_loss': 0.07996884630993009, 'train/mask_dice_loss': 0.4795130953192711, 'train/mask_loss': 0.559481930732727, 'metrics/total_secs_per_batch': 7.579092264175415, 'metrics/data_secs_per_batch': 3.3732235193252564, '_timestamp': 1740970837.2407074}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 331 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970837.2409854}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 332 is less than current step: 499. Dropping entry: {'train/loss': 1.8583404302597046, 'train/ce_loss': 0.31220703125, 'train/seg_cls_loss': 0.014007568359375, 'train/kl_loss': 0.3228515625, 'train/mask_bce_loss': 0.17640732631552963, 'train/mask_dice_loss': 0.5771281212568283, 'train/mask_loss': 0.75353544652462, 'metrics/total_secs_per_batch': 9.100042819976807, 'metrics/data_secs_per_batch': 4.281576871871948, '_timestamp': 1740970846.3409572}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 332 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970846.3413115}).
Epoch: [5][334/500]	Time  8.082 ( 8.082)	Loss 2.4257 (1.8535)	CeLoss 0.1436 (0.4073)	SegCLSLoss 0.0229 (0.0152)	KLLoss 0.3711 (0.2932)	MaskLoss 1.1166 (0.7047)	MaskBCELoss 0.2638 (0.1452)	MaskDICELoss 0.8529 (0.5595)
Epoch: [5][335/500]	Time  7.474 ( 7.474)	Loss 0.0608 (1.5454)	CeLoss 0.0608 (0.3975)	SegCLSLoss 0.0000 (0.0134)	KLLoss 0.0000 (0.2561)	MaskLoss 0.0000 (0.5577)	MaskBCELoss 0.0000 (0.1237)	MaskDICELoss 0.0000 (0.4340)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 333 is less than current step: 499. Dropping entry: {'train/loss': 1.8535017848014832, 'train/ce_loss': 0.40732421875, 'train/seg_cls_loss': 0.015167236328125, 'train/kl_loss': 0.2931640625, 'train/mask_bce_loss': 0.1452098112553358, 'train/mask_dice_loss': 0.5595195859670639, 'train/mask_loss': 0.7047294080257416, 'metrics/total_secs_per_batch': 8.081947088241577, 'metrics/data_secs_per_batch': 2.9964947938919066, '_timestamp': 1740970854.4228637}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 333 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970854.4232583}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 334 is less than current step: 499. Dropping entry: {'train/loss': 1.5453539848327638, 'train/ce_loss': 0.3974853515625, 'train/seg_cls_loss': 0.01341552734375, 'train/kl_loss': 0.2560546875, 'train/mask_bce_loss': 0.12373549658805132, 'train/mask_dice_loss': 0.43398788273334504, 'train/mask_loss': 0.5577233791351318, 'metrics/total_secs_per_batch': 7.474358797073364, 'metrics/data_secs_per_batch': 2.9925266027450563, '_timestamp': 1740970861.8970842}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 334 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970861.8973684}).
Epoch: [5][336/500]	Time  7.112 ( 7.112)	Loss 0.0884 (1.4751)	CeLoss 0.0884 (0.3402)	SegCLSLoss 0.0000 (0.0124)	KLLoss 0.0000 (0.2549)	MaskLoss 0.0000 (0.5515)	MaskBCELoss 0.0000 (0.1373)	MaskDICELoss 0.0000 (0.4142)
Epoch: [5][337/500]	Time  7.441 ( 7.441)	Loss 2.0415 (1.5467)	CeLoss 0.2109 (0.4537)	SegCLSLoss 0.0208 (0.0111)	KLLoss 0.3711 (0.2205)	MaskLoss 0.8918 (0.5326)	MaskBCELoss 0.3784 (0.1750)	MaskDICELoss 0.5135 (0.3576)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 335 is less than current step: 499. Dropping entry: {'train/loss': 1.475133466720581, 'train/ce_loss': 0.340185546875, 'train/seg_cls_loss': 0.012384033203125, 'train/kl_loss': 0.2548828125, 'train/mask_bce_loss': 0.1373250601813197, 'train/mask_dice_loss': 0.41418210193514826, 'train/mask_loss': 0.5515071615576744, 'metrics/total_secs_per_batch': 7.111793756484985, 'metrics/data_secs_per_batch': 3.316589903831482, '_timestamp': 1740970869.00893}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 335 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970869.0091348}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 336 is less than current step: 499. Dropping entry: {'train/loss': 1.5467201352119446, 'train/ce_loss': 0.4536865234375, 'train/seg_cls_loss': 0.01109619140625, 'train/kl_loss': 0.2205078125, 'train/mask_bce_loss': 0.1749867968261242, 'train/mask_dice_loss': 0.3576139837503433, 'train/mask_loss': 0.5326007902622223, 'metrics/total_secs_per_batch': 7.440539121627808, 'metrics/data_secs_per_batch': 3.322321128845215, '_timestamp': 1740970876.449522}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 336 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970876.4498746}).
Epoch: [5][338/500]	Time  6.715 ( 6.715)	Loss 1.1406 (1.7019)	CeLoss 1.1406 (0.3489)	SegCLSLoss 0.0000 (0.0147)	KLLoss 0.0000 (0.2559)	MaskLoss 0.0000 (0.6600)	MaskBCELoss 0.0000 (0.1796)	MaskDICELoss 0.0000 (0.4804)
Epoch: [5][339/500]	Time  9.079 ( 9.079)	Loss 1.7223 (1.7817)	CeLoss 0.1777 (0.2884)	SegCLSLoss 0.0210 (0.0168)	KLLoss 0.3574 (0.3254)	MaskLoss 0.7488 (0.7262)	MaskBCELoss 0.0273 (0.1378)	MaskDICELoss 0.7215 (0.5884)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 337 is less than current step: 499. Dropping entry: {'train/loss': 1.701857566833496, 'train/ce_loss': 0.348876953125, 'train/seg_cls_loss': 0.014697265625, 'train/kl_loss': 0.255859375, 'train/mask_bce_loss': 0.17964015481993556, 'train/mask_dice_loss': 0.48039505928754805, 'train/mask_loss': 0.6600352138280868, 'metrics/total_secs_per_batch': 6.715140342712402, 'metrics/data_secs_per_batch': 3.2264171838760376, '_timestamp': 1740970883.1645253}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 337 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970883.1647975}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 338 is less than current step: 499. Dropping entry: {'train/loss': 1.7816859245300294, 'train/ce_loss': 0.28837890625, 'train/seg_cls_loss': 0.01676025390625, 'train/kl_loss': 0.325390625, 'train/mask_bce_loss': 0.13780223866924643, 'train/mask_dice_loss': 0.5883922934532165, 'train/mask_loss': 0.7261945307254791, 'metrics/total_secs_per_batch': 9.079414367675781, 'metrics/data_secs_per_batch': 3.7213661432266236, '_timestamp': 1740970892.2439666}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 338 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970892.2442534}).
[2025-03-02 21:01:38,947] [INFO] [logging.py:96:log_dist] [Rank 0] step=2840, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:01:38,952] [INFO] [timer.py:215:stop] epoch=0/micro_step=28400/global_step=2840, RunningAvgSamplesPerSec=1.4158793666893423, CurrSamplesPerSec=1.4908736284352477, MemAllocated=30.81GB, MaxMemAllocated=37.23GB
Epoch: [5][340/500]	Time  6.709 ( 6.709)	Loss 4.0895 (1.9605)	CeLoss 0.1299 (0.3981)	SegCLSLoss 0.0272 (0.0127)	KLLoss 0.3789 (0.2928)	MaskLoss 1.9539 (0.7634)	MaskBCELoss 1.1305 (0.2905)	MaskDICELoss 0.8234 (0.4729)
Epoch: [5][341/500]	Time  9.074 ( 9.074)	Loss 1.0853 (1.8848)	CeLoss 0.3301 (0.3437)	SegCLSLoss 0.0112 (0.0154)	KLLoss 0.3594 (0.2887)	MaskLoss 0.3571 (0.7523)	MaskBCELoss 0.0413 (0.1357)	MaskDICELoss 0.3158 (0.6166)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 339 is less than current step: 499. Dropping entry: {'train/loss': 1.9605339169502258, 'train/ce_loss': 0.39814453125, 'train/seg_cls_loss': 0.012652587890625, 'train/kl_loss': 0.2927734375, 'train/mask_bce_loss': 0.2905092053115368, 'train/mask_dice_loss': 0.4728632107377052, 'train/mask_loss': 0.7633724197745323, 'metrics/total_secs_per_batch': 6.709199905395508, 'metrics/data_secs_per_batch': 2.6180134773254395, '_timestamp': 1740970898.9530814}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 339 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970898.9534125}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 340 is less than current step: 499. Dropping entry: {'train/loss': 1.8848185539245605, 'train/ce_loss': 0.34365234375, 'train/seg_cls_loss': 0.015386962890625, 'train/kl_loss': 0.288671875, 'train/mask_bce_loss': 0.13571725282818078, 'train/mask_dice_loss': 0.6166041105985641, 'train/mask_loss': 0.7523213654756546, 'metrics/total_secs_per_batch': 9.073655128479004, 'metrics/data_secs_per_batch': 3.523636221885681, '_timestamp': 1740970908.0269322}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 340 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970908.027237}).
Epoch: [5][342/500]	Time  7.477 ( 7.477)	Loss 2.3905 (1.7607)	CeLoss 0.1816 (0.3304)	SegCLSLoss 0.0199 (0.0145)	KLLoss 0.3770 (0.2928)	MaskLoss 1.0810 (0.6970)	MaskBCELoss 0.2428 (0.1307)	MaskDICELoss 0.8382 (0.5663)
Epoch: [5][343/500]	Time  8.267 ( 8.267)	Loss 1.2500 (1.5340)	CeLoss 1.2500 (0.3305)	SegCLSLoss 0.0000 (0.0148)	KLLoss 0.0000 (0.3254)	MaskLoss 0.0000 (0.5819)	MaskBCELoss 0.0000 (0.1016)	MaskDICELoss 0.0000 (0.4803)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 341 is less than current step: 499. Dropping entry: {'train/loss': 1.7606762409210206, 'train/ce_loss': 0.33037109375, 'train/seg_cls_loss': 0.01446533203125, 'train/kl_loss': 0.2927734375, 'train/mask_bce_loss': 0.13066416308283807, 'train/mask_dice_loss': 0.5663243502378463, 'train/mask_loss': 0.6969885200262069, 'metrics/total_secs_per_batch': 7.477379560470581, 'metrics/data_secs_per_batch': 3.620038104057312, '_timestamp': 1740970915.50434}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 341 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970915.5046322}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 342 is less than current step: 499. Dropping entry: {'train/loss': 1.533952271938324, 'train/ce_loss': 0.33046875, 'train/seg_cls_loss': 0.014776611328125, 'train/kl_loss': 0.325390625, 'train/mask_bce_loss': 0.10159460678696633, 'train/mask_dice_loss': 0.48027411103248596, 'train/mask_loss': 0.581868714094162, 'metrics/total_secs_per_batch': 8.266509532928467, 'metrics/data_secs_per_batch': 3.660406255722046, '_timestamp': 1740970923.7707903}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 342 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970923.7710779}).
Epoch: [5][344/500]	Time  7.598 ( 7.598)	Loss 2.2222 (1.6267)	CeLoss 0.2266 (0.2761)	SegCLSLoss 0.0236 (0.0134)	KLLoss 0.3633 (0.2908)	MaskLoss 0.9734 (0.6574)	MaskBCELoss 0.1138 (0.1049)	MaskDICELoss 0.8596 (0.5526)
Epoch: [5][345/500]	Time  8.181 ( 8.181)	Loss 0.0525 (1.2678)	CeLoss 0.0525 (0.3868)	SegCLSLoss 0.0000 (0.0102)	KLLoss 0.0000 (0.2174)	MaskLoss 0.0000 (0.4273)	MaskBCELoss 0.0000 (0.0575)	MaskDICELoss 0.0000 (0.3698)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 343 is less than current step: 499. Dropping entry: {'train/loss': 1.6267473697662354, 'train/ce_loss': 0.27607421875, 'train/seg_cls_loss': 0.01339111328125, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.10486359130591154, 'train/mask_dice_loss': 0.5525530427694321, 'train/mask_loss': 0.6574166357517243, 'metrics/total_secs_per_batch': 7.5976402759552, 'metrics/data_secs_per_batch': 3.0863034248352053, '_timestamp': 1740970931.3686035}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 343 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970931.3689954}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 344 is less than current step: 499. Dropping entry: {'train/loss': 1.2678267359733582, 'train/ce_loss': 0.3867919921875, 'train/seg_cls_loss': 0.010198974609375, 'train/kl_loss': 0.2173828125, 'train/mask_bce_loss': 0.05747377322986722, 'train/mask_dice_loss': 0.36981117725372314, 'train/mask_loss': 0.4272849529981613, 'metrics/total_secs_per_batch': 8.181105136871338, 'metrics/data_secs_per_batch': 3.654163861274719, '_timestamp': 1740970939.5498726}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 344 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970939.5502844}).
Epoch: [5][346/500]	Time  8.039 ( 8.039)	Loss 2.5746 (1.7161)	CeLoss 0.1299 (0.2502)	SegCLSLoss 0.0256 (0.0135)	KLLoss 0.3652 (0.3240)	MaskLoss 1.1975 (0.7133)	MaskBCELoss 0.2904 (0.1919)	MaskDICELoss 0.9070 (0.5214)
Epoch: [5][347/500]	Time  8.916 ( 8.916)	Loss 2.1226 (1.6042)	CeLoss 0.2178 (0.3975)	SegCLSLoss 0.0190 (0.0111)	KLLoss 0.3574 (0.2514)	MaskLoss 0.9295 (0.5880)	MaskBCELoss 0.0388 (0.1330)	MaskDICELoss 0.8907 (0.4550)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 345 is less than current step: 499. Dropping entry: {'train/loss': 1.716126799583435, 'train/ce_loss': 0.2501953125, 'train/seg_cls_loss': 0.01348876953125, 'train/kl_loss': 0.3240234375, 'train/mask_bce_loss': 0.19194579031318426, 'train/mask_dice_loss': 0.5213910579681397, 'train/mask_loss': 0.7133368462324142, 'metrics/total_secs_per_batch': 8.038896083831787, 'metrics/data_secs_per_batch': 3.4207260370254517, '_timestamp': 1740970947.5884497}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 345 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970947.5887318}).
Epoch: [5][348/500]	Time  7.944 ( 7.944)	Loss 0.0723 (1.5087)	CeLoss 0.0723 (0.2635)	SegCLSLoss 0.0000 (0.0133)	KLLoss 0.0000 (0.2547)	MaskLoss 0.0000 (0.6063)	MaskBCELoss 0.0000 (0.0849)	MaskDICELoss 0.0000 (0.5215)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 346 is less than current step: 499. Dropping entry: {'train/loss': 1.6041610836982727, 'train/ce_loss': 0.3974609375, 'train/seg_cls_loss': 0.011090087890625, 'train/kl_loss': 0.2513671875, 'train/mask_bce_loss': 0.1329651538282633, 'train/mask_dice_loss': 0.45500402748584745, 'train/mask_loss': 0.587969183921814, 'metrics/total_secs_per_batch': 8.915933609008789, 'metrics/data_secs_per_batch': 3.7032083749771116, '_timestamp': 1740970956.5044057}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 346 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970956.5047715}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 347 is less than current step: 499. Dropping entry: {'train/loss': 1.5086757302284242, 'train/ce_loss': 0.2634765625, 'train/seg_cls_loss': 0.013330078125, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.08485576789826155, 'train/mask_dice_loss': 0.5214840590953826, 'train/mask_loss': 0.6063398152589798, 'metrics/total_secs_per_batch': 7.943873167037964, 'metrics/data_secs_per_batch': 4.152971005439758, '_timestamp': 1740970964.448366}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 347 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970964.4486833}).
Epoch: [5][349/500]	Time  6.895 ( 6.895)	Loss 2.2483 (1.5324)	CeLoss 0.2158 (0.4657)	SegCLSLoss 0.0138 (0.0109)	KLLoss 0.3594 (0.2170)	MaskLoss 0.9952 (0.5198)	MaskBCELoss 0.0102 (0.0504)	MaskDICELoss 0.9851 (0.4694)
[2025-03-02 21:02:59,173] [INFO] [logging.py:96:log_dist] [Rank 0] step=2850, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:02:59,179] [INFO] [timer.py:215:stop] epoch=0/micro_step=28500/global_step=2850, RunningAvgSamplesPerSec=1.415204977543113, CurrSamplesPerSec=1.2763567664356985, MemAllocated=31.84GB, MaxMemAllocated=37.23GB
Epoch: [5][350/500]	Time  7.836 ( 7.836)	Loss 0.1719 (1.5485)	CeLoss 0.1719 (0.3858)	SegCLSLoss 0.0000 (0.0118)	KLLoss 0.0000 (0.2541)	MaskLoss 0.0000 (0.5657)	MaskBCELoss 0.0000 (0.0711)	MaskDICELoss 0.0000 (0.4946)
Epoch: [5][351/500]	Time  5.898 ( 5.898)	Loss 1.1016 (1.5888)	CeLoss 1.1016 (0.5845)	SegCLSLoss 0.0000 (0.0123)	KLLoss 0.0000 (0.2174)	MaskLoss 0.0000 (0.4881)	MaskBCELoss 0.0000 (0.0445)	MaskDICELoss 0.0000 (0.4436)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 348 is less than current step: 499. Dropping entry: {'train/loss': 1.532411313056946, 'train/ce_loss': 0.465673828125, 'train/seg_cls_loss': 0.010906982421875, 'train/kl_loss': 0.2169921875, 'train/mask_bce_loss': 0.050388341676443814, 'train/mask_dice_loss': 0.4694061726331711, 'train/mask_loss': 0.519794511795044, 'metrics/total_secs_per_batch': 6.895325422286987, 'metrics/data_secs_per_batch': 2.86756546497345, '_timestamp': 1740970971.3434424}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 348 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970971.3437097}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 349 is less than current step: 499. Dropping entry: {'train/loss': 1.5484650015830994, 'train/ce_loss': 0.38583984375, 'train/seg_cls_loss': 0.011773681640625, 'train/kl_loss': 0.2541015625, 'train/mask_bce_loss': 0.07108935667201877, 'train/mask_dice_loss': 0.4946470528841019, 'train/mask_loss': 0.5657364070415497, 'metrics/total_secs_per_batch': 7.836223602294922, 'metrics/data_secs_per_batch': 3.6187265872955323, '_timestamp': 1740970979.1795995}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 349 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970979.1798959}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 350 is less than current step: 499. Dropping entry: {'train/loss': 1.5887704730033874, 'train/ce_loss': 0.58447265625, 'train/seg_cls_loss': 0.01231689453125, 'train/kl_loss': 0.2173828125, 'train/mask_bce_loss': 0.04453377220779657, 'train/mask_dice_loss': 0.4436014711856842, 'train/mask_loss': 0.48813524544239045, 'metrics/total_secs_per_batch': 5.898348569869995, 'metrics/data_secs_per_batch': 2.4818198919296264, '_timestamp': 1740970985.0782137}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 350 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970985.0785067}).
Epoch: [5][352/500]	Time  7.301 ( 7.301)	Loss 1.7412 (1.5650)	CeLoss 0.1875 (0.2404)	SegCLSLoss 0.0168 (0.0167)	KLLoss 0.3633 (0.3275)	MaskLoss 0.7544 (0.6418)	MaskBCELoss 0.0505 (0.1141)	MaskDICELoss 0.7039 (0.5277)
Epoch: [5][353/500]	Time  8.396 ( 8.396)	Loss 2.2681 (2.1600)	CeLoss 0.2188 (0.2158)	SegCLSLoss 0.0173 (0.0146)	KLLoss 0.3672 (0.3250)	MaskLoss 1.0022 (0.9522)	MaskBCELoss 0.2323 (0.4011)	MaskDICELoss 0.7699 (0.5511)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 351 is less than current step: 499. Dropping entry: {'train/loss': 1.5649832308292388, 'train/ce_loss': 0.2404296875, 'train/seg_cls_loss': 0.016729736328125, 'train/kl_loss': 0.3275390625, 'train/mask_bce_loss': 0.11408936958760023, 'train/mask_dice_loss': 0.5277284011244774, 'train/mask_loss': 0.6418177872896195, 'metrics/total_secs_per_batch': 7.300853729248047, 'metrics/data_secs_per_batch': 3.504215216636658, '_timestamp': 1740970992.3789463}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 351 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740970992.3792093}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 352 is less than current step: 499. Dropping entry: {'train/loss': 2.1600181102752685, 'train/ce_loss': 0.2157958984375, 'train/seg_cls_loss': 0.0145751953125, 'train/kl_loss': 0.325, 'train/mask_bce_loss': 0.4011148585006595, 'train/mask_dice_loss': 0.5511232107877732, 'train/mask_loss': 0.9522380620241165, 'metrics/total_secs_per_batch': 8.396151542663574, 'metrics/data_secs_per_batch': 3.5403215885162354, '_timestamp': 1740971000.7750952}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 352 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971000.7753594}).
Epoch: [5][354/500]	Time  6.585 ( 6.585)	Loss 1.2656 (1.5038)	CeLoss 1.2656 (0.7077)	SegCLSLoss 0.0000 (0.0071)	KLLoss 0.0000 (0.2213)	MaskLoss 0.0000 (0.3851)	MaskBCELoss 0.0000 (0.0778)	MaskDICELoss 0.0000 (0.3073)
Epoch: [5][355/500]	Time  8.438 ( 8.438)	Loss 1.6829 (1.9808)	CeLoss 0.1836 (0.2473)	SegCLSLoss 0.0233 (0.0204)	KLLoss 0.3535 (0.3238)	MaskLoss 0.7262 (0.8454)	MaskBCELoss 0.1070 (0.1580)	MaskDICELoss 0.6192 (0.6875)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 353 is less than current step: 499. Dropping entry: {'train/loss': 1.5038083851337434, 'train/ce_loss': 0.70771484375, 'train/seg_cls_loss': 0.007135009765625, 'train/kl_loss': 0.2212890625, 'train/mask_bce_loss': 0.07782240770757198, 'train/mask_dice_loss': 0.30728490799665453, 'train/mask_loss': 0.38510731756687167, 'metrics/total_secs_per_batch': 6.584900617599487, 'metrics/data_secs_per_batch': 3.074262571334839, '_timestamp': 1740971007.3600075}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 353 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971007.3601906}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 354 is less than current step: 499. Dropping entry: {'train/loss': 1.9807526469230652, 'train/ce_loss': 0.247265625, 'train/seg_cls_loss': 0.0204345703125, 'train/kl_loss': 0.323828125, 'train/mask_bce_loss': 0.1579536311328411, 'train/mask_dice_loss': 0.6874519884586334, 'train/mask_loss': 0.8454056203365325, 'metrics/total_secs_per_batch': 8.438148021697998, 'metrics/data_secs_per_batch': 3.6543818950653075, '_timestamp': 1740971015.798137}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 354 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971015.7984128}).
Epoch: [5][356/500]	Time  8.036 ( 8.036)	Loss 2.0379 (1.7801)	CeLoss 0.2178 (0.3032)	SegCLSLoss 0.0197 (0.0149)	KLLoss 0.3594 (0.2934)	MaskLoss 0.8871 (0.7201)	MaskBCELoss 0.0343 (0.0933)	MaskDICELoss 0.8528 (0.6267)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 355 is less than current step: 499. Dropping entry: {'train/loss': 1.7800909459590912, 'train/ce_loss': 0.303173828125, 'train/seg_cls_loss': 0.01485595703125, 'train/kl_loss': 0.293359375, 'train/mask_bce_loss': 0.09330669767223299, 'train/mask_dice_loss': 0.6267436683177948, 'train/mask_loss': 0.7200503677129746, 'metrics/total_secs_per_batch': 8.036405086517334, 'metrics/data_secs_per_batch': 3.7676639795303344, '_timestamp': 1740971023.8347714}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 355 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971023.8351464}).
Epoch: [5][357/500]	Time  7.882 ( 7.882)	Loss 1.0379 (1.5228)	CeLoss 0.2988 (0.3895)	SegCLSLoss 0.0109 (0.0155)	KLLoss 0.3652 (0.3297)	MaskLoss 0.3481 (0.5462)	MaskBCELoss 0.0946 (0.0898)	MaskDICELoss 0.2535 (0.4565)
Epoch: [5][358/500]	Time  7.824 ( 7.824)	Loss 1.3048 (1.3117)	CeLoss 0.3027 (0.3539)	SegCLSLoss 0.0130 (0.0115)	KLLoss 0.3711 (0.2932)	MaskLoss 0.4796 (0.4612)	MaskBCELoss 0.1557 (0.1213)	MaskDICELoss 0.3239 (0.3399)
Epoch: [5][359/500]	Time  4.288 ( 4.288)	Loss 0.8125 (1.0995)	CeLoss 0.8125 (0.6619)	SegCLSLoss 0.0000 (0.0057)	KLLoss 0.0000 (0.1084)	MaskLoss 0.0000 (0.2120)	MaskBCELoss 0.0000 (0.1114)	MaskDICELoss 0.0000 (0.1006)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 356 is less than current step: 499. Dropping entry: {'train/loss': 1.5227611541748047, 'train/ce_loss': 0.389453125, 'train/seg_cls_loss': 0.015484619140625, 'train/kl_loss': 0.3296875, 'train/mask_bce_loss': 0.08976947069168091, 'train/mask_dice_loss': 0.4564743906259537, 'train/mask_loss': 0.546243867278099, 'metrics/total_secs_per_batch': 7.881982326507568, 'metrics/data_secs_per_batch': 3.504452848434448, '_timestamp': 1740971031.7165558}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 356 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971031.7167466}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 357 is less than current step: 499. Dropping entry: {'train/loss': 1.3116820096969604, 'train/ce_loss': 0.353857421875, 'train/seg_cls_loss': 0.01151123046875, 'train/kl_loss': 0.2931640625, 'train/mask_bce_loss': 0.12130844034254551, 'train/mask_dice_loss': 0.339928075671196, 'train/mask_loss': 0.4612365126609802, 'metrics/total_secs_per_batch': 7.8240509033203125, 'metrics/data_secs_per_batch': 3.44170024394989, '_timestamp': 1740971039.5405996}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 357 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971039.5408642}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 358 is less than current step: 499. Dropping entry: {'train/loss': 1.0994699597358704, 'train/ce_loss': 0.6619140625, 'train/seg_cls_loss': 0.00572509765625, 'train/kl_loss': 0.1083984375, 'train/mask_bce_loss': 0.11137593165040016, 'train/mask_dice_loss': 0.10061490684747695, 'train/mask_loss': 0.2119908392429352, 'metrics/total_secs_per_batch': 4.287791728973389, 'metrics/data_secs_per_batch': 2.1489528894424437, '_timestamp': 1740971043.828396}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 358 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971043.828662}).
[2025-03-02 21:04:11,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=2860, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:04:11,773] [INFO] [timer.py:215:stop] epoch=0/micro_step=28600/global_step=2860, RunningAvgSamplesPerSec=1.4150703173871462, CurrSamplesPerSec=1.258738467539671, MemAllocated=31.09GB, MaxMemAllocated=37.23GB
Epoch: [5][360/500]	Time  7.946 ( 7.946)	Loss 1.5347 (1.4170)	CeLoss 0.2910 (0.2103)	SegCLSLoss 0.0121 (0.0141)	KLLoss 0.3672 (0.3277)	MaskLoss 0.6004 (0.5834)	MaskBCELoss 0.1982 (0.1349)	MaskDICELoss 0.4021 (0.4485)
Epoch: [5][361/500]	Time  6.225 ( 6.225)	Loss 1.9721 (1.7444)	CeLoss 0.2988 (0.6086)	SegCLSLoss 0.0090 (0.0104)	KLLoss 0.3633 (0.2533)	MaskLoss 0.8161 (0.5527)	MaskBCELoss 0.3648 (0.0913)	MaskDICELoss 0.4514 (0.4614)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 359 is less than current step: 499. Dropping entry: {'train/loss': 1.4169607043266297, 'train/ce_loss': 0.21025390625, 'train/seg_cls_loss': 0.014068603515625, 'train/kl_loss': 0.327734375, 'train/mask_bce_loss': 0.1348589312285185, 'train/mask_dice_loss': 0.4485237643122673, 'train/mask_loss': 0.5833826959133148, 'metrics/total_secs_per_batch': 7.945916652679443, 'metrics/data_secs_per_batch': 3.653956723213196, '_timestamp': 1740971051.774137}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 359 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971051.7743957}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 360 is less than current step: 499. Dropping entry: {'train/loss': 1.7443716883659364, 'train/ce_loss': 0.60859375, 'train/seg_cls_loss': 0.01038818359375, 'train/kl_loss': 0.2533203125, 'train/mask_bce_loss': 0.09131158329546452, 'train/mask_dice_loss': 0.4613918513059616, 'train/mask_loss': 0.5527034252882004, 'metrics/total_secs_per_batch': 6.224916696548462, 'metrics/data_secs_per_batch': 2.9926605463027953, '_timestamp': 1740971057.9992726}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 360 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971057.9996212}).
Epoch: [5][362/500]	Time  7.760 ( 7.760)	Loss 0.9805 (1.4402)	CeLoss 0.9805 (0.3804)	SegCLSLoss 0.0000 (0.0094)	KLLoss 0.0000 (0.2539)	MaskLoss 0.0000 (0.5149)	MaskBCELoss 0.0000 (0.1293)	MaskDICELoss 0.0000 (0.3856)
Epoch: [5][363/500]	Time  8.001 ( 8.001)	Loss 1.9538 (1.5966)	CeLoss 0.2793 (0.5190)	SegCLSLoss 0.0136 (0.0122)	KLLoss 0.3594 (0.2523)	MaskLoss 0.8167 (0.5233)	MaskBCELoss 0.0276 (0.0356)	MaskDICELoss 0.7891 (0.4876)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 361 is less than current step: 499. Dropping entry: {'train/loss': 1.4401587963104248, 'train/ce_loss': 0.38037109375, 'train/seg_cls_loss': 0.009405517578125, 'train/kl_loss': 0.25390625, 'train/mask_bce_loss': 0.12929687649011612, 'train/mask_dice_loss': 0.3855579227209091, 'train/mask_loss': 0.5148547917604447, 'metrics/total_secs_per_batch': 7.760175943374634, 'metrics/data_secs_per_batch': 3.736599850654602, '_timestamp': 1740971065.7594726}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 361 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971065.7597575}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 362 is less than current step: 499. Dropping entry: {'train/loss': 1.5965803623199464, 'train/ce_loss': 0.518994140625, 'train/seg_cls_loss': 0.01224365234375, 'train/kl_loss': 0.25234375, 'train/mask_bce_loss': 0.035620676912367345, 'train/mask_dice_loss': 0.4876450955867767, 'train/mask_loss': 0.5232657611370086, 'metrics/total_secs_per_batch': 8.001249074935913, 'metrics/data_secs_per_batch': 3.376118040084839, '_timestamp': 1740971073.7606916}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 362 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971073.7609546}).
Epoch: [5][364/500]	Time  7.896 ( 7.896)	Loss 1.4393 (1.6136)	CeLoss 0.2100 (0.2366)	SegCLSLoss 0.0248 (0.0143)	KLLoss 0.3535 (0.3262)	MaskLoss 0.5907 (0.6686)	MaskBCELoss 0.0036 (0.2071)	MaskDICELoss 0.5872 (0.4615)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 363 is less than current step: 499. Dropping entry: {'train/loss': 1.6135851979255675, 'train/ce_loss': 0.2365966796875, 'train/seg_cls_loss': 0.014324951171875, 'train/kl_loss': 0.326171875, 'train/mask_bce_loss': 0.2070864450884983, 'train/mask_dice_loss': 0.46153476238250735, 'train/mask_loss': 0.6686212062835694, 'metrics/total_secs_per_batch': 7.895639896392822, 'metrics/data_secs_per_batch': 3.7905327558517454, '_timestamp': 1740971081.6563282}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 363 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971081.6566062}).
Epoch: [5][365/500]	Time 10.125 (10.125)	Loss 1.4062 (2.1390)	CeLoss 1.4062 (0.3375)	SegCLSLoss 0.0000 (0.0169)	KLLoss 0.0000 (0.3264)	MaskLoss 0.0000 (0.8802)	MaskBCELoss 0.0000 (0.2386)	MaskDICELoss 0.0000 (0.6416)
Epoch: [5][366/500]	Time  8.529 ( 8.529)	Loss 1.3750 (1.5715)	CeLoss 1.3750 (0.3923)	SegCLSLoss 0.0000 (0.0118)	KLLoss 0.0000 (0.2512)	MaskLoss 0.0000 (0.5741)	MaskBCELoss 0.0000 (0.1201)	MaskDICELoss 0.0000 (0.4540)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 364 is less than current step: 499. Dropping entry: {'train/loss': 2.138987731933594, 'train/ce_loss': 0.3375, 'train/seg_cls_loss': 0.016912841796875, 'train/kl_loss': 0.3263671875, 'train/mask_bce_loss': 0.23861937364563346, 'train/mask_dice_loss': 0.641567862033844, 'train/mask_loss': 0.8801872253417968, 'metrics/total_secs_per_batch': 10.125234842300415, 'metrics/data_secs_per_batch': 5.448353862762451, '_timestamp': 1740971091.7815633}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 364 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971091.7818277}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 365 is less than current step: 499. Dropping entry: {'train/loss': 1.571469283103943, 'train/ce_loss': 0.39228515625, 'train/seg_cls_loss': 0.01180419921875, 'train/kl_loss': 0.251171875, 'train/mask_bce_loss': 0.12009796313941479, 'train/mask_dice_loss': 0.45401559621095655, 'train/mask_loss': 0.5741135537624359, 'metrics/total_secs_per_batch': 8.529348134994507, 'metrics/data_secs_per_batch': 4.107370185852051, '_timestamp': 1740971100.3109787}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 365 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971100.311275}).
Epoch: [5][367/500]	Time  7.660 ( 7.660)	Loss 1.2770 (1.7457)	CeLoss 0.2930 (0.5426)	SegCLSLoss 0.0120 (0.0112)	KLLoss 0.3633 (0.2547)	MaskLoss 0.4705 (0.5859)	MaskBCELoss 0.0631 (0.1447)	MaskDICELoss 0.4075 (0.4412)
Epoch: [5][368/500]	Time  8.194 ( 8.194)	Loss 2.7892 (1.5055)	CeLoss 0.2949 (0.3288)	SegCLSLoss 0.0114 (0.0151)	KLLoss 0.3633 (0.3291)	MaskLoss 1.2256 (0.5681)	MaskBCELoss 0.4498 (0.1495)	MaskDICELoss 0.7759 (0.4186)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 366 is less than current step: 499. Dropping entry: {'train/loss': 1.7457178473472594, 'train/ce_loss': 0.542578125, 'train/seg_cls_loss': 0.011224365234375, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.14472523517906666, 'train/mask_dice_loss': 0.4412196338176727, 'train/mask_loss': 0.5859448611736298, 'metrics/total_secs_per_batch': 7.659844160079956, 'metrics/data_secs_per_batch': 3.493449306488037, '_timestamp': 1740971107.9707932}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 366 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971107.971099}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 367 is less than current step: 499. Dropping entry: {'train/loss': 1.505515557527542, 'train/ce_loss': 0.32880859375, 'train/seg_cls_loss': 0.015093994140625, 'train/kl_loss': 0.3291015625, 'train/mask_bce_loss': 0.1494951056316495, 'train/mask_dice_loss': 0.4186435371637344, 'train/mask_loss': 0.5681386381387711, 'metrics/total_secs_per_batch': 8.193507194519043, 'metrics/data_secs_per_batch': 3.379845142364502, '_timestamp': 1740971116.1642892}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 367 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971116.1645648}).
Epoch: [5][369/500]	Time  6.563 ( 6.563)	Loss 1.0794 (1.4469)	CeLoss 0.2090 (0.7757)	SegCLSLoss 0.0117 (0.0065)	KLLoss 0.3672 (0.1469)	MaskLoss 0.4137 (0.3267)	MaskBCELoss 0.1458 (0.0585)	MaskDICELoss 0.2679 (0.2681)
[2025-03-02 21:05:29,506] [INFO] [logging.py:96:log_dist] [Rank 0] step=2870, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:05:29,512] [INFO] [timer.py:215:stop] epoch=0/micro_step=28700/global_step=2870, RunningAvgSamplesPerSec=1.4145776444498819, CurrSamplesPerSec=1.4741389951163635, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [5][370/500]	Time  6.785 ( 6.785)	Loss 2.3657 (1.5600)	CeLoss 0.2178 (0.5670)	SegCLSLoss 0.0145 (0.0099)	KLLoss 0.3652 (0.1820)	MaskLoss 1.0520 (0.4850)	MaskBCELoss 0.3442 (0.0755)	MaskDICELoss 0.7078 (0.4095)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 368 is less than current step: 499. Dropping entry: {'train/loss': 1.4468955636024474, 'train/ce_loss': 0.77568359375, 'train/seg_cls_loss': 0.006488037109375, 'train/kl_loss': 0.146875, 'train/mask_bce_loss': 0.058541445434093474, 'train/mask_dice_loss': 0.2681289821863174, 'train/mask_loss': 0.32667043805122375, 'metrics/total_secs_per_batch': 6.563382387161255, 'metrics/data_secs_per_batch': 3.00587944984436, '_timestamp': 1740971122.727629}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 368 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971122.7278883}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 369 is less than current step: 499. Dropping entry: {'train/loss': 1.559960699081421, 'train/ce_loss': 0.5669921875, 'train/seg_cls_loss': 0.009942626953125, 'train/kl_loss': 0.18203125, 'train/mask_bce_loss': 0.07547373985871672, 'train/mask_dice_loss': 0.40948707461357114, 'train/mask_loss': 0.48496081233024596, 'metrics/total_secs_per_batch': 6.785115957260132, 'metrics/data_secs_per_batch': 3.0406202554702757, '_timestamp': 1740971129.5125942}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 369 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971129.5128663}).
Epoch: [5][371/500]	Time  7.743 ( 7.743)	Loss 0.9231 (1.5884)	CeLoss 0.1875 (0.5221)	SegCLSLoss 0.0187 (0.0121)	KLLoss 0.3672 (0.2568)	MaskLoss 0.3444 (0.5173)	MaskBCELoss 0.0207 (0.0928)	MaskDICELoss 0.3236 (0.4245)
Epoch: [5][372/500]	Time  6.376 ( 6.376)	Loss 0.7739 (1.4316)	CeLoss 0.2285 (0.4753)	SegCLSLoss 0.0099 (0.0111)	KLLoss 0.3633 (0.2193)	MaskLoss 0.2522 (0.4644)	MaskBCELoss 0.0468 (0.1267)	MaskDICELoss 0.2053 (0.3377)
Epoch: [5][373/500]	Time  6.090 ( 6.090)	Loss 1.5488 (1.8652)	CeLoss 0.1719 (0.2439)	SegCLSLoss 0.0227 (0.0177)	KLLoss 0.3633 (0.3279)	MaskLoss 0.6645 (0.7898)	MaskBCELoss 0.0476 (0.1844)	MaskDICELoss 0.6170 (0.6054)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 370 is less than current step: 499. Dropping entry: {'train/loss': 1.5883537530899048, 'train/ce_loss': 0.5220703125, 'train/seg_cls_loss': 0.012103271484375, 'train/kl_loss': 0.2568359375, 'train/mask_bce_loss': 0.09282165188342333, 'train/mask_dice_loss': 0.4244509249925613, 'train/mask_loss': 0.5172725796699524, 'metrics/total_secs_per_batch': 7.742532253265381, 'metrics/data_secs_per_batch': 3.080027151107788, '_timestamp': 1740971137.2553353}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 370 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971137.2556133}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 371 is less than current step: 499. Dropping entry: {'train/loss': 1.4316094398498536, 'train/ce_loss': 0.47529296875, 'train/seg_cls_loss': 0.011090087890625, 'train/kl_loss': 0.2193359375, 'train/mask_bce_loss': 0.1266918506473303, 'train/mask_dice_loss': 0.3377456828951836, 'train/mask_loss': 0.46443753242492675, 'metrics/total_secs_per_batch': 6.376192092895508, 'metrics/data_secs_per_batch': 3.3979222536087037, '_timestamp': 1740971143.6315613}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 371 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971143.631891}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 372 is less than current step: 499. Dropping entry: {'train/loss': 1.8652124881744385, 'train/ce_loss': 0.2439453125, 'train/seg_cls_loss': 0.017742919921875, 'train/kl_loss': 0.3279296875, 'train/mask_bce_loss': 0.18444017358124257, 'train/mask_dice_loss': 0.6053926378488541, 'train/mask_loss': 0.7898328185081482, 'metrics/total_secs_per_batch': 6.089973449707031, 'metrics/data_secs_per_batch': 2.340555214881897, '_timestamp': 1740971149.721501}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 372 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971149.7218084}).
Epoch: [5][374/500]	Time  6.761 ( 6.761)	Loss 2.5984 (1.8910)	CeLoss 0.1855 (0.2298)	SegCLSLoss 0.0165 (0.0158)	KLLoss 0.3633 (0.3652)	MaskLoss 1.1844 (0.8083)	MaskBCELoss 0.3162 (0.2296)	MaskDICELoss 0.8682 (0.5786)
Epoch: [5][375/500]	Time  6.095 ( 6.095)	Loss 0.1836 (1.1194)	CeLoss 0.1836 (0.4069)	SegCLSLoss 0.0000 (0.0073)	KLLoss 0.0000 (0.1811)	MaskLoss 0.0000 (0.3453)	MaskBCELoss 0.0000 (0.0652)	MaskDICELoss 0.0000 (0.2801)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 373 is less than current step: 499. Dropping entry: {'train/loss': 1.8910205245018006, 'train/ce_loss': 0.22978515625, 'train/seg_cls_loss': 0.0158203125, 'train/kl_loss': 0.365234375, 'train/mask_bce_loss': 0.2296344993636012, 'train/mask_dice_loss': 0.578619909286499, 'train/mask_loss': 0.8082544088363648, 'metrics/total_secs_per_batch': 6.760871887207031, 'metrics/data_secs_per_batch': 3.721873950958252, '_timestamp': 1740971156.482345}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 373 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971156.4825325}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 374 is less than current step: 499. Dropping entry: {'train/loss': 1.1194349229335785, 'train/ce_loss': 0.406884765625, 'train/seg_cls_loss': 0.00726318359375, 'train/kl_loss': 0.1810546875, 'train/mask_bce_loss': 0.06522825807332992, 'train/mask_dice_loss': 0.2800604835152626, 'train/mask_loss': 0.3452887415885925, 'metrics/total_secs_per_batch': 6.095384120941162, 'metrics/data_secs_per_batch': 2.8335822582244874, '_timestamp': 1740971162.577747}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 374 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971162.5780275}).
Epoch: [5][376/500]	Time  5.921 ( 5.921)	Loss 0.5666 (1.5396)	CeLoss 0.2168 (0.2993)	SegCLSLoss 0.0095 (0.0156)	KLLoss 0.3652 (0.2938)	MaskLoss 0.1544 (0.6015)	MaskBCELoss 0.0526 (0.1163)	MaskDICELoss 0.1018 (0.4853)
Epoch: [5][377/500]	Time  6.560 ( 6.560)	Loss 2.7656 (1.8320)	CeLoss 0.1406 (0.2951)	SegCLSLoss 0.0247 (0.0148)	KLLoss 0.3750 (0.3262)	MaskLoss 1.2871 (0.7482)	MaskBCELoss 0.5620 (0.1801)	MaskDICELoss 0.7250 (0.5681)
Epoch: [5][378/500]	Time  6.060 ( 6.060)	Loss 1.1161 (1.3984)	CeLoss 0.1914 (0.2361)	SegCLSLoss 0.0176 (0.0124)	KLLoss 0.3594 (0.2889)	MaskLoss 0.4399 (0.5637)	MaskBCELoss 0.0249 (0.0747)	MaskDICELoss 0.4150 (0.4890)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 375 is less than current step: 499. Dropping entry: {'train/loss': 1.5395630836486816, 'train/ce_loss': 0.299267578125, 'train/seg_cls_loss': 0.015643310546875, 'train/kl_loss': 0.29375, 'train/mask_bce_loss': 0.11625514118932187, 'train/mask_dice_loss': 0.48526467829942704, 'train/mask_loss': 0.6015198126435279, 'metrics/total_secs_per_batch': 5.920565843582153, 'metrics/data_secs_per_batch': 2.7419543743133543, '_timestamp': 1740971168.4984884}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 375 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971168.4988186}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 376 is less than current step: 499. Dropping entry: {'train/loss': 1.8319982767105103, 'train/ce_loss': 0.2951171875, 'train/seg_cls_loss': 0.014825439453125, 'train/kl_loss': 0.326171875, 'train/mask_bce_loss': 0.1801022358238697, 'train/mask_dice_loss': 0.5681234538555145, 'train/mask_loss': 0.7482256889343262, 'metrics/total_secs_per_batch': 6.560095548629761, 'metrics/data_secs_per_batch': 2.997677040100098, '_timestamp': 1740971175.0584698}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 376 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971175.058774}).
Epoch: [5][379/500]	Time  5.318 ( 5.318)	Loss 2.5861 (1.6611)	CeLoss 0.2021 (0.5302)	SegCLSLoss 0.0141 (0.0113)	KLLoss 0.3652 (0.2217)	MaskLoss 1.1700 (0.5515)	MaskBCELoss 0.2969 (0.1316)	MaskDICELoss 0.8731 (0.4200)
[2025-03-02 21:06:32,578] [INFO] [logging.py:96:log_dist] [Rank 0] step=2880, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:06:32,583] [INFO] [timer.py:215:stop] epoch=0/micro_step=28800/global_step=2880, RunningAvgSamplesPerSec=1.415108570643419, CurrSamplesPerSec=1.6268871046231352, MemAllocated=30.94GB, MaxMemAllocated=37.23GB
Epoch: [5][380/500]	Time  6.148 ( 6.148)	Loss 2.3631 (1.7402)	CeLoss 0.2617 (0.3452)	SegCLSLoss 0.0112 (0.0129)	KLLoss 0.3574 (0.3227)	MaskLoss 1.0302 (0.6783)	MaskBCELoss 0.6149 (0.1723)	MaskDICELoss 0.4153 (0.5060)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 377 is less than current step: 499. Dropping entry: {'train/loss': 1.3983765244483948, 'train/ce_loss': 0.2361328125, 'train/seg_cls_loss': 0.01241455078125, 'train/kl_loss': 0.2888671875, 'train/mask_bce_loss': 0.07468993533402682, 'train/mask_dice_loss': 0.48904909491539, 'train/mask_loss': 0.5637390375137329, 'metrics/total_secs_per_batch': 6.059950113296509, 'metrics/data_secs_per_batch': 2.762062501907349, '_timestamp': 1740971181.1184695}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 377 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971181.118818}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 378 is less than current step: 499. Dropping entry: {'train/loss': 1.6610590934753418, 'train/ce_loss': 0.53017578125, 'train/seg_cls_loss': 0.011279296875, 'train/kl_loss': 0.2216796875, 'train/mask_bce_loss': 0.13155752047896385, 'train/mask_dice_loss': 0.41996812224388125, 'train/mask_loss': 0.5515256404876709, 'metrics/total_secs_per_batch': 5.317772150039673, 'metrics/data_secs_per_batch': 2.170302653312683, '_timestamp': 1740971186.4362035}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 378 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971186.4364867}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 379 is less than current step: 499. Dropping entry: {'train/loss': 1.740223240852356, 'train/ce_loss': 0.34521484375, 'train/seg_cls_loss': 0.012860107421875, 'train/kl_loss': 0.32265625, 'train/mask_bce_loss': 0.17231758754933252, 'train/mask_dice_loss': 0.505997148156166, 'train/mask_loss': 0.678314745426178, 'metrics/total_secs_per_batch': 6.148396253585815, 'metrics/data_secs_per_batch': 2.958888220787048, '_timestamp': 1740971192.584476}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 379 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971192.584859}).
Epoch: [5][381/500]	Time  5.236 ( 5.236)	Loss 0.0913 (1.1901)	CeLoss 0.0913 (0.4841)	SegCLSLoss 0.0000 (0.0082)	KLLoss 0.0000 (0.2164)	MaskLoss 0.0000 (0.3401)	MaskBCELoss 0.0000 (0.0722)	MaskDICELoss 0.0000 (0.2679)
Epoch: [5][382/500]	Time  6.853 ( 6.853)	Loss 0.1152 (1.8705)	CeLoss 0.1152 (0.2035)	SegCLSLoss 0.0000 (0.0166)	KLLoss 0.0000 (0.3289)	MaskLoss 0.0000 (0.8130)	MaskBCELoss 0.0000 (0.1832)	MaskDICELoss 0.0000 (0.6298)
Epoch: [5][383/500]	Time  6.018 ( 6.018)	Loss 1.9688 (1.8692)	CeLoss 0.2061 (0.3442)	SegCLSLoss 0.0179 (0.0138)	KLLoss 0.3535 (0.3291)	MaskLoss 0.8594 (0.7425)	MaskBCELoss 0.0904 (0.2018)	MaskDICELoss 0.7690 (0.5408)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 380 is less than current step: 499. Dropping entry: {'train/loss': 1.1900726139545441, 'train/ce_loss': 0.484130859375, 'train/seg_cls_loss': 0.008245849609375, 'train/kl_loss': 0.21640625, 'train/mask_bce_loss': 0.07222326658666134, 'train/mask_dice_loss': 0.26785697788000107, 'train/mask_loss': 0.3400802493095398, 'metrics/total_secs_per_batch': 5.235638618469238, 'metrics/data_secs_per_batch': 2.5900871992111205, '_timestamp': 1740971197.8202028}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 380 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971197.8204877}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 381 is less than current step: 499. Dropping entry: {'train/loss': 1.8705360352993012, 'train/ce_loss': 0.203466796875, 'train/seg_cls_loss': 0.0166015625, 'train/kl_loss': 0.32890625, 'train/mask_bce_loss': 0.18319494361057878, 'train/mask_dice_loss': 0.6298074290156365, 'train/mask_loss': 0.8130023792386055, 'metrics/total_secs_per_batch': 6.853185415267944, 'metrics/data_secs_per_batch': 3.0548988819122314, '_timestamp': 1740971204.6740634}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 381 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971204.674539}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 382 is less than current step: 499. Dropping entry: {'train/loss': 1.8692224740982055, 'train/ce_loss': 0.34423828125, 'train/seg_cls_loss': 0.013751220703125, 'train/kl_loss': 0.3291015625, 'train/mask_bce_loss': 0.20175653509795666, 'train/mask_dice_loss': 0.540764856338501, 'train/mask_loss': 0.7425213873386383, 'metrics/total_secs_per_batch': 6.017570734024048, 'metrics/data_secs_per_batch': 2.6632959127426146, '_timestamp': 1740971210.6915236}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 382 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971210.6920426}).
Epoch: [5][384/500]	Time  7.120 ( 7.120)	Loss 2.2148 (1.8013)	CeLoss 0.1953 (0.2999)	SegCLSLoss 0.0166 (0.0154)	KLLoss 0.3633 (0.3277)	MaskLoss 0.9877 (0.7305)	MaskBCELoss 0.0335 (0.1982)	MaskDICELoss 0.9543 (0.5323)
Epoch: [5][385/500]	Time  5.904 ( 5.904)	Loss 1.2734 (1.4886)	CeLoss 1.2734 (0.5152)	SegCLSLoss 0.0000 (0.0097)	KLLoss 0.0000 (0.2186)	MaskLoss 0.0000 (0.4733)	MaskBCELoss 0.0000 (0.0889)	MaskDICELoss 0.0000 (0.3844)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 383 is less than current step: 499. Dropping entry: {'train/loss': 1.801288866996765, 'train/ce_loss': 0.29990234375, 'train/seg_cls_loss': 0.01536865234375, 'train/kl_loss': 0.327734375, 'train/mask_bce_loss': 0.19816547241061927, 'train/mask_dice_loss': 0.5323129311203957, 'train/mask_loss': 0.7304784119129181, 'metrics/total_secs_per_batch': 7.120344638824463, 'metrics/data_secs_per_batch': 3.0423704385757446, '_timestamp': 1740971217.8113723}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 383 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971217.8117127}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 384 is less than current step: 499. Dropping entry: {'train/loss': 1.4885736227035522, 'train/ce_loss': 0.515234375, 'train/seg_cls_loss': 0.00965576171875, 'train/kl_loss': 0.2185546875, 'train/mask_bce_loss': 0.08889034762978554, 'train/mask_dice_loss': 0.3844491884112358, 'train/mask_loss': 0.4733395308256149, 'metrics/total_secs_per_batch': 5.903704643249512, 'metrics/data_secs_per_batch': 2.67978036403656, '_timestamp': 1740971223.7153738}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 384 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971223.715823}).
Epoch: [5][386/500]	Time  5.135 ( 5.135)	Loss 2.5978 (1.7066)	CeLoss 0.2695 (0.3599)	SegCLSLoss 0.0110 (0.0102)	KLLoss 0.3652 (0.2924)	MaskLoss 1.1426 (0.6561)	MaskBCELoss 0.7680 (0.2153)	MaskDICELoss 0.3746 (0.4408)
Epoch: [5][387/500]	Time  6.083 ( 6.083)	Loss 0.8323 (1.5212)	CeLoss 0.2217 (0.3127)	SegCLSLoss 0.0099 (0.0132)	KLLoss 0.3652 (0.2887)	MaskLoss 0.2843 (0.5864)	MaskBCELoss 0.0698 (0.1057)	MaskDICELoss 0.2145 (0.4807)
Epoch: [5][388/500]	Time  5.777 ( 5.777)	Loss 2.5361 (1.5700)	CeLoss 0.1875 (0.2879)	SegCLSLoss 0.0184 (0.0123)	KLLoss 0.3828 (0.2918)	MaskLoss 1.1508 (0.6234)	MaskBCELoss 0.2911 (0.1403)	MaskDICELoss 0.8597 (0.4831)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 385 is less than current step: 499. Dropping entry: {'train/loss': 1.7065884590148925, 'train/ce_loss': 0.35986328125, 'train/seg_cls_loss': 0.0101806640625, 'train/kl_loss': 0.2923828125, 'train/mask_bce_loss': 0.21529747489839793, 'train/mask_dice_loss': 0.4407799482345581, 'train/mask_loss': 0.6560774251818657, 'metrics/total_secs_per_batch': 5.134525537490845, 'metrics/data_secs_per_batch': 2.340692949295044, '_timestamp': 1740971228.8497107}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 385 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971228.850113}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 386 is less than current step: 499. Dropping entry: {'train/loss': 1.5211684107780457, 'train/ce_loss': 0.3126953125, 'train/seg_cls_loss': 0.013232421875, 'train/kl_loss': 0.288671875, 'train/mask_bce_loss': 0.10568738300353289, 'train/mask_dice_loss': 0.48072690665721896, 'train/mask_loss': 0.5864142924547195, 'metrics/total_secs_per_batch': 6.083376407623291, 'metrics/data_secs_per_batch': 2.8069621324539185, '_timestamp': 1740971234.932966}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 386 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971234.9332912}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 387 is less than current step: 499. Dropping entry: {'train/loss': 1.5699607729911804, 'train/ce_loss': 0.2879150390625, 'train/seg_cls_loss': 0.01229248046875, 'train/kl_loss': 0.291796875, 'train/mask_bce_loss': 0.14030641466379165, 'train/mask_dice_loss': 0.48308948874473573, 'train/mask_loss': 0.6233959063887596, 'metrics/total_secs_per_batch': 5.776700735092163, 'metrics/data_secs_per_batch': 2.6390634059906004, '_timestamp': 1740971240.7099514}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 387 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971240.7103608}).
Epoch: [5][389/500]	Time  6.766 ( 6.766)	Loss 2.4592 (2.0112)	CeLoss 0.1079 (0.2034)	SegCLSLoss 0.0327 (0.0200)	KLLoss 0.3770 (0.3645)	MaskLoss 1.1486 (0.8806)	MaskBCELoss 0.2627 (0.1876)	MaskDICELoss 0.8859 (0.6929)
[2025-03-02 21:07:33,362] [INFO] [logging.py:96:log_dist] [Rank 0] step=2890, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:07:33,368] [INFO] [timer.py:215:stop] epoch=0/micro_step=28900/global_step=2890, RunningAvgSamplesPerSec=1.4157951978020213, CurrSamplesPerSec=1.697511391503515, MemAllocated=30.95GB, MaxMemAllocated=37.23GB
Epoch: [5][390/500]	Time  5.893 ( 5.893)	Loss 1.8322 (1.4079)	CeLoss 0.1855 (0.2481)	SegCLSLoss 0.0251 (0.0110)	KLLoss 0.3535 (0.2543)	MaskLoss 0.7994 (0.5644)	MaskBCELoss 0.1390 (0.1095)	MaskDICELoss 0.6604 (0.4549)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 388 is less than current step: 499. Dropping entry: {'train/loss': 2.01119949221611, 'train/ce_loss': 0.203369140625, 'train/seg_cls_loss': 0.02000732421875, 'train/kl_loss': 0.364453125, 'train/mask_bce_loss': 0.1876308975741267, 'train/mask_dice_loss': 0.6929200254380703, 'train/mask_loss': 0.880550916492939, 'metrics/total_secs_per_batch': 6.766425132751465, 'metrics/data_secs_per_batch': 3.0978185176849364, '_timestamp': 1740971247.4762251}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 388 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971247.4766083}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 389 is less than current step: 499. Dropping entry: {'train/loss': 1.4078548789024352, 'train/ce_loss': 0.2481201171875, 'train/seg_cls_loss': 0.01103515625, 'train/kl_loss': 0.254296875, 'train/mask_bce_loss': 0.10948956608772278, 'train/mask_dice_loss': 0.4548993051052094, 'train/mask_loss': 0.5643888741731644, 'metrics/total_secs_per_batch': 5.892988681793213, 'metrics/data_secs_per_batch': 2.8514070987701414, '_timestamp': 1740971253.3691127}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 389 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971253.3695302}).
Epoch: [5][391/500]	Time  6.679 ( 6.679)	Loss 1.9646 (1.4476)	CeLoss 0.2324 (0.2998)	SegCLSLoss 0.0242 (0.0156)	KLLoss 0.3535 (0.2883)	MaskLoss 0.8427 (0.5556)	MaskBCELoss 0.1252 (0.1305)	MaskDICELoss 0.7175 (0.4251)
Epoch: [5][392/500]	Time  5.632 ( 5.632)	Loss 0.9922 (1.7263)	CeLoss 0.9922 (0.5408)	SegCLSLoss 0.0000 (0.0106)	KLLoss 0.0000 (0.2561)	MaskLoss 0.0000 (0.5773)	MaskBCELoss 0.0000 (0.1470)	MaskDICELoss 0.0000 (0.4303)
Epoch: [5][393/500]	Time  5.419 ( 5.419)	Loss 1.4640 (1.4458)	CeLoss 0.2432 (0.4347)	SegCLSLoss 0.0123 (0.0101)	KLLoss 0.3594 (0.2535)	MaskLoss 0.5894 (0.4904)	MaskBCELoss 0.0332 (0.0555)	MaskDICELoss 0.5562 (0.4348)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 390 is less than current step: 499. Dropping entry: {'train/loss': 1.4476023077964784, 'train/ce_loss': 0.2998046875, 'train/seg_cls_loss': 0.015582275390625, 'train/kl_loss': 0.28828125, 'train/mask_bce_loss': 0.13052901662886143, 'train/mask_dice_loss': 0.42505924105644227, 'train/mask_loss': 0.5555882632732392, 'metrics/total_secs_per_batch': 6.678777694702148, 'metrics/data_secs_per_batch': 2.696928310394287, '_timestamp': 1740971260.0479681}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 390 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971260.0483778}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 391 is less than current step: 499. Dropping entry: {'train/loss': 1.726313292980194, 'train/ce_loss': 0.5408203125, 'train/seg_cls_loss': 0.010589599609375, 'train/kl_loss': 0.2560546875, 'train/mask_bce_loss': 0.1470038380473852, 'train/mask_dice_loss': 0.4303129702806473, 'train/mask_loss': 0.5773168087005616, 'metrics/total_secs_per_batch': 5.631764888763428, 'metrics/data_secs_per_batch': 2.4605273008346558, '_timestamp': 1740971265.6805167}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 391 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971265.681045}).
Epoch: [5][394/500]	Time  5.743 ( 5.743)	Loss 1.0214 (1.9320)	CeLoss 0.3105 (0.4493)	SegCLSLoss 0.0112 (0.0108)	KLLoss 0.3691 (0.2910)	MaskLoss 0.3340 (0.7242)	MaskBCELoss 0.1204 (0.2082)	MaskDICELoss 0.2136 (0.5160)
Epoch: [5][395/500]	Time  6.260 ( 6.260)	Loss 2.6867 (1.7884)	CeLoss 0.2061 (0.3772)	SegCLSLoss 0.0178 (0.0150)	KLLoss 0.3691 (0.3268)	MaskLoss 1.2174 (0.6855)	MaskBCELoss 0.3592 (0.1968)	MaskDICELoss 0.8582 (0.4886)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 392 is less than current step: 499. Dropping entry: {'train/loss': 1.4458368062973022, 'train/ce_loss': 0.43466796875, 'train/seg_cls_loss': 0.010064697265625, 'train/kl_loss': 0.253515625, 'train/mask_bce_loss': 0.055524796806275846, 'train/mask_dice_loss': 0.43482526242733, 'train/mask_loss': 0.49035006016492844, 'metrics/total_secs_per_batch': 5.419276475906372, 'metrics/data_secs_per_batch': 2.317453145980835, '_timestamp': 1740971271.0988855}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 392 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971271.0992289}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 393 is less than current step: 499. Dropping entry: {'train/loss': 1.9319882035255431, 'train/ce_loss': 0.44931640625, 'train/seg_cls_loss': 0.010772705078125, 'train/kl_loss': 0.291015625, 'train/mask_bce_loss': 0.208242604508996, 'train/mask_dice_loss': 0.5160034492611885, 'train/mask_loss': 0.7242460399866104, 'metrics/total_secs_per_batch': 5.742819547653198, 'metrics/data_secs_per_batch': 2.7974336624145506, '_timestamp': 1740971276.8420856}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 393 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971276.8425395}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 394 is less than current step: 499. Dropping entry: {'train/loss': 1.7884172558784486, 'train/ce_loss': 0.37724609375, 'train/seg_cls_loss': 0.014984130859375, 'train/kl_loss': 0.3267578125, 'train/mask_bce_loss': 0.1968322390690446, 'train/mask_dice_loss': 0.4886361569166183, 'train/mask_loss': 0.6854684010148049, 'metrics/total_secs_per_batch': 6.260314702987671, 'metrics/data_secs_per_batch': 2.640001392364502, '_timestamp': 1740971283.1024175}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 394 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971283.1028035}).
Epoch: [5][396/500]	Time  5.417 ( 5.417)	Loss 2.0955 (1.8395)	CeLoss 0.1914 (0.4248)	SegCLSLoss 0.0172 (0.0157)	KLLoss 0.3691 (0.2539)	MaskLoss 0.9291 (0.6908)	MaskBCELoss 0.0101 (0.1329)	MaskDICELoss 0.9190 (0.5580)
Epoch: [5][397/500]	Time  6.437 ( 6.437)	Loss 1.3413 (1.0929)	CeLoss 0.2559 (0.3066)	SegCLSLoss 0.0157 (0.0093)	KLLoss 0.3594 (0.1811)	MaskLoss 0.5212 (0.3817)	MaskBCELoss 0.0337 (0.0491)	MaskDICELoss 0.4875 (0.3327)
Epoch: [5][398/500]	Time  5.152 ( 5.152)	Loss 1.9169 (1.5268)	CeLoss 0.2217 (0.5854)	SegCLSLoss 0.0178 (0.0105)	KLLoss 0.3555 (0.2135)	MaskLoss 0.8256 (0.4574)	MaskBCELoss 0.0072 (0.0513)	MaskDICELoss 0.8184 (0.4061)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 395 is less than current step: 499. Dropping entry: {'train/loss': 1.839460849761963, 'train/ce_loss': 0.4248046875, 'train/seg_cls_loss': 0.015704345703125, 'train/kl_loss': 0.25390625, 'train/mask_bce_loss': 0.13286636886186898, 'train/mask_dice_loss': 0.557957798242569, 'train/mask_loss': 0.690824168920517, 'metrics/total_secs_per_batch': 5.417129755020142, 'metrics/data_secs_per_batch': 2.4216338634490966, '_timestamp': 1740971288.519218}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 395 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971288.5195131}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 396 is less than current step: 499. Dropping entry: {'train/loss': 1.0928744196891784, 'train/ce_loss': 0.306640625, 'train/seg_cls_loss': 0.00927734375, 'train/kl_loss': 0.1810546875, 'train/mask_bce_loss': 0.0490821210667491, 'train/mask_dice_loss': 0.3326578393578529, 'train/mask_loss': 0.3817399561405182, 'metrics/total_secs_per_batch': 6.437230587005615, 'metrics/data_secs_per_batch': 2.848598861694336, '_timestamp': 1740971294.956663}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 396 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971294.9570284}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 397 is less than current step: 499. Dropping entry: {'train/loss': 1.5267560064792634, 'train/ce_loss': 0.58544921875, 'train/seg_cls_loss': 0.01048583984375, 'train/kl_loss': 0.2134765625, 'train/mask_bce_loss': 0.05127828395925462, 'train/mask_dice_loss': 0.4060938522219658, 'train/mask_loss': 0.4573721408843994, 'metrics/total_secs_per_batch': 5.1516945362091064, 'metrics/data_secs_per_batch': 2.0162355661392213, '_timestamp': 1740971300.1083202}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 397 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971300.108745}).
Epoch: [5][399/500]	Time  5.438 ( 5.438)	Loss 1.1719 (1.6296)	CeLoss 1.1719 (0.4553)	SegCLSLoss 0.0000 (0.0129)	KLLoss 0.0000 (0.2547)	MaskLoss 0.0000 (0.5712)	MaskBCELoss 0.0000 (0.1235)	MaskDICELoss 0.0000 (0.4477)
[2025-03-02 21:08:31,313] [INFO] [logging.py:96:log_dist] [Rank 0] step=2900, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:08:31,319] [INFO] [timer.py:215:stop] epoch=0/micro_step=29000/global_step=2900, RunningAvgSamplesPerSec=1.4166739093007708, CurrSamplesPerSec=1.732283184891885, MemAllocated=30.69GB, MaxMemAllocated=37.23GB
Epoch: [5][400/500]	Time  5.774 ( 5.774)	Loss 1.2031 (1.8202)	CeLoss 1.2031 (0.4409)	SegCLSLoss 0.0000 (0.0151)	KLLoss 0.0000 (0.2922)	MaskLoss 0.0000 (0.6715)	MaskBCELoss 0.0000 (0.0929)	MaskDICELoss 0.0000 (0.5785)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 398 is less than current step: 499. Dropping entry: {'train/loss': 1.6296199798583983, 'train/ce_loss': 0.4552734375, 'train/seg_cls_loss': 0.012945556640625, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.12354782596230507, 'train/mask_dice_loss': 0.4476586371660233, 'train/mask_loss': 0.5712064683437348, 'metrics/total_secs_per_batch': 5.43798565864563, 'metrics/data_secs_per_batch': 2.363734722137451, '_timestamp': 1740971305.5461433}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 398 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971305.5464387}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 399 is less than current step: 499. Dropping entry: {'train/loss': 1.8202362179756164, 'train/ce_loss': 0.44091796875, 'train/seg_cls_loss': 0.0151123046875, 'train/kl_loss': 0.2921875, 'train/mask_bce_loss': 0.09294681986793876, 'train/mask_dice_loss': 0.5785482287406921, 'train/mask_loss': 0.6714950501918793, 'metrics/total_secs_per_batch': 5.774388074874878, 'metrics/data_secs_per_batch': 2.4511400938034056, '_timestamp': 1740971311.3204064}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 399 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971311.3207805}).
Epoch: [5][401/500]	Time  5.325 ( 5.325)	Loss 0.0645 (1.6722)	CeLoss 0.0645 (0.4833)	SegCLSLoss 0.0000 (0.0135)	KLLoss 0.0000 (0.2184)	MaskLoss 0.0000 (0.5801)	MaskBCELoss 0.0000 (0.0880)	MaskDICELoss 0.0000 (0.4921)
Epoch: [5][402/500]	Time  5.591 ( 5.591)	Loss 1.7347 (1.4622)	CeLoss 0.2012 (0.5021)	SegCLSLoss 0.0187 (0.0108)	KLLoss 0.3574 (0.2168)	MaskLoss 0.7443 (0.4665)	MaskBCELoss 0.0278 (0.0634)	MaskDICELoss 0.7166 (0.4031)
Epoch: [5][403/500]	Time  6.171 ( 6.171)	Loss 0.5618 (1.3381)	CeLoss 0.2500 (0.3027)	SegCLSLoss 0.0091 (0.0115)	KLLoss 0.3652 (0.2889)	MaskLoss 0.1354 (0.5004)	MaskBCELoss 0.0689 (0.0741)	MaskDICELoss 0.0665 (0.4263)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 400 is less than current step: 499. Dropping entry: {'train/loss': 1.67216796875, 'train/ce_loss': 0.48330078125, 'train/seg_cls_loss': 0.0135498046875, 'train/kl_loss': 0.218359375, 'train/mask_bce_loss': 0.08798678992316127, 'train/mask_dice_loss': 0.4921401619911194, 'train/mask_loss': 0.580126953125, 'metrics/total_secs_per_batch': 5.325291156768799, 'metrics/data_secs_per_batch': 2.2168709516525267, '_timestamp': 1740971316.6457944}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 400 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971316.6460743}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 401 is less than current step: 499. Dropping entry: {'train/loss': 1.4622154593467713, 'train/ce_loss': 0.50205078125, 'train/seg_cls_loss': 0.010784912109375, 'train/kl_loss': 0.216796875, 'train/mask_bce_loss': 0.06341821160167456, 'train/mask_dice_loss': 0.4030899107456207, 'train/mask_loss': 0.46650812327861785, 'metrics/total_secs_per_batch': 5.5906922817230225, 'metrics/data_secs_per_batch': 2.451393222808838, '_timestamp': 1740971322.2367215}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 401 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971322.2370842}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 402 is less than current step: 499. Dropping entry: {'train/loss': 1.3381213545799255, 'train/ce_loss': 0.3027099609375, 'train/seg_cls_loss': 0.01148681640625, 'train/kl_loss': 0.2888671875, 'train/mask_bce_loss': 0.0741086384281516, 'train/mask_dice_loss': 0.42626308649778366, 'train/mask_loss': 0.5003717124462128, 'metrics/total_secs_per_batch': 6.1712119579315186, 'metrics/data_secs_per_batch': 2.880874752998352, '_timestamp': 1740971328.4077184}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 402 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971328.4079185}).
Epoch: [5][404/500]	Time  6.680 ( 6.680)	Loss 1.2629 (2.1933)	CeLoss 0.2402 (0.2138)	SegCLSLoss 0.0108 (0.0184)	KLLoss 0.3594 (0.3650)	MaskLoss 0.4908 (0.9669)	MaskBCELoss 0.1958 (0.2545)	MaskDICELoss 0.2951 (0.7124)
Epoch: [5][405/500]	Time  5.980 ( 5.980)	Loss 2.3880 (1.7906)	CeLoss 0.0977 (0.2961)	SegCLSLoss 0.0383 (0.0172)	KLLoss 0.3906 (0.3283)	MaskLoss 1.1159 (0.7266)	MaskBCELoss 0.3262 (0.1646)	MaskDICELoss 0.7896 (0.5620)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 403 is less than current step: 499. Dropping entry: {'train/loss': 2.1933403253555297, 'train/ce_loss': 0.21376953125, 'train/seg_cls_loss': 0.018389892578125, 'train/kl_loss': 0.3650390625, 'train/mask_bce_loss': 0.25448381369933487, 'train/mask_dice_loss': 0.7124011904001236, 'train/mask_loss': 0.9668850034475327, 'metrics/total_secs_per_batch': 6.679831266403198, 'metrics/data_secs_per_batch': 3.2460850715637206, '_timestamp': 1740971335.08772}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 403 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971335.08807}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 404 is less than current step: 499. Dropping entry: {'train/loss': 1.790554678440094, 'train/ce_loss': 0.29609375, 'train/seg_cls_loss': 0.017193603515625, 'train/kl_loss': 0.3283203125, 'train/mask_bce_loss': 0.1645984104834497, 'train/mask_dice_loss': 0.5620265871286392, 'train/mask_loss': 0.7266250014305115, 'metrics/total_secs_per_batch': 5.980407238006592, 'metrics/data_secs_per_batch': 2.5440866231918333, '_timestamp': 1740971341.0680227}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 404 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971341.0683138}).
Epoch: [5][406/500]	Time  6.228 ( 6.228)	Loss 2.2344 (1.8715)	CeLoss 2.2344 (0.6770)	SegCLSLoss 0.0000 (0.0125)	KLLoss 0.0000 (0.2549)	MaskLoss 0.0000 (0.5814)	MaskBCELoss 0.0000 (0.1632)	MaskDICELoss 0.0000 (0.4182)
Epoch: [5][407/500]	Time  6.599 ( 6.599)	Loss 2.8010 (1.8498)	CeLoss 0.1826 (0.2342)	SegCLSLoss 0.0234 (0.0169)	KLLoss 0.3691 (0.3633)	MaskLoss 1.2848 (0.7856)	MaskBCELoss 0.4888 (0.1619)	MaskDICELoss 0.7960 (0.6237)
Epoch: [5][408/500]	Time  6.767 ( 6.767)	Loss 1.8563 (1.8685)	CeLoss 0.1719 (0.2936)	SegCLSLoss 0.0217 (0.0185)	KLLoss 0.3555 (0.3229)	MaskLoss 0.8188 (0.7667)	MaskBCELoss 0.0295 (0.0877)	MaskDICELoss 0.7893 (0.6790)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 405 is less than current step: 499. Dropping entry: {'train/loss': 1.8715008795261383, 'train/ce_loss': 0.676953125, 'train/seg_cls_loss': 0.012451171875, 'train/kl_loss': 0.2548828125, 'train/mask_bce_loss': 0.16318989619612695, 'train/mask_dice_loss': 0.4181659996509552, 'train/mask_loss': 0.5813559025526047, 'metrics/total_secs_per_batch': 6.228275537490845, 'metrics/data_secs_per_batch': 2.678458333015442, '_timestamp': 1740971347.2964628}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 405 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971347.2966897}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 406 is less than current step: 499. Dropping entry: {'train/loss': 1.8498247146606446, 'train/ce_loss': 0.2341796875, 'train/seg_cls_loss': 0.0168701171875, 'train/kl_loss': 0.36328125, 'train/mask_bce_loss': 0.1618917055428028, 'train/mask_dice_loss': 0.6236651673913002, 'train/mask_loss': 0.7855568826198578, 'metrics/total_secs_per_batch': 6.599095106124878, 'metrics/data_secs_per_batch': 2.98750216960907, '_timestamp': 1740971353.8954952}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 406 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971353.895822}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 407 is less than current step: 499. Dropping entry: {'train/loss': 1.8684539914131164, 'train/ce_loss': 0.2935546875, 'train/seg_cls_loss': 0.018548583984375, 'train/kl_loss': 0.3228515625, 'train/mask_bce_loss': 0.08766973428428174, 'train/mask_dice_loss': 0.6790279656648636, 'train/mask_loss': 0.7666976928710938, 'metrics/total_secs_per_batch': 6.7672834396362305, 'metrics/data_secs_per_batch': 3.0610080003738402, '_timestamp': 1740971360.6627653}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 407 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971360.663096}).
Epoch: [5][409/500]	Time  4.639 ( 4.639)	Loss 2.5273 (1.5338)	CeLoss 0.2236 (0.6125)	SegCLSLoss 0.0199 (0.0100)	KLLoss 0.3691 (0.1844)	MaskLoss 1.1289 (0.4489)	MaskBCELoss 0.2479 (0.0820)	MaskDICELoss 0.8810 (0.3669)
[2025-03-02 21:09:31,092] [INFO] [logging.py:96:log_dist] [Rank 0] step=2910, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:09:31,098] [INFO] [timer.py:215:stop] epoch=0/micro_step=29100/global_step=2910, RunningAvgSamplesPerSec=1.417421240499046, CurrSamplesPerSec=1.7254629512690367, MemAllocated=31.09GB, MaxMemAllocated=37.23GB
Epoch: [5][410/500]	Time  5.797 ( 5.797)	Loss 1.8541 (1.7569)	CeLoss 0.2383 (0.3476)	SegCLSLoss 0.0102 (0.0137)	KLLoss 0.3672 (0.2924)	MaskLoss 0.7874 (0.6866)	MaskBCELoss 0.1045 (0.1361)	MaskDICELoss 0.6829 (0.5506)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 408 is less than current step: 499. Dropping entry: {'train/loss': 1.533796775341034, 'train/ce_loss': 0.612548828125, 'train/seg_cls_loss': 0.0099853515625, 'train/kl_loss': 0.184375, 'train/mask_bce_loss': 0.0820039289072156, 'train/mask_dice_loss': 0.3669012814760208, 'train/mask_loss': 0.448905211687088, 'metrics/total_secs_per_batch': 4.6393678188323975, 'metrics/data_secs_per_batch': 2.164119291305542, '_timestamp': 1740971365.3021853}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 408 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971365.3025053}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 409 is less than current step: 499. Dropping entry: {'train/loss': 1.7568668007850647, 'train/ce_loss': 0.34755859375, 'train/seg_cls_loss': 0.01365966796875, 'train/kl_loss': 0.2923828125, 'train/mask_bce_loss': 0.13607168421149254, 'train/mask_dice_loss': 0.5505648225545883, 'train/mask_loss': 0.6866365134716034, 'metrics/total_secs_per_batch': 5.797475337982178, 'metrics/data_secs_per_batch': 2.5996827125549316, '_timestamp': 1740971371.0993009}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 409 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971371.0995984}).
Epoch: [5][411/500]	Time  6.839 ( 6.839)	Loss 2.4029 (1.7115)	CeLoss 0.2178 (0.2037)	SegCLSLoss 0.0132 (0.0145)	KLLoss 0.3594 (0.3264)	MaskLoss 1.0716 (0.7341)	MaskBCELoss 0.3214 (0.1488)	MaskDICELoss 0.7502 (0.5852)
Epoch: [5][412/500]	Time  5.082 ( 5.082)	Loss 1.7394 (1.3241)	CeLoss 0.2969 (0.5229)	SegCLSLoss 0.0209 (0.0112)	KLLoss 0.3652 (0.2547)	MaskLoss 0.6978 (0.3850)	MaskBCELoss 0.2986 (0.0763)	MaskDICELoss 0.3992 (0.3087)
Epoch: [5][413/500]	Time  5.502 ( 5.502)	Loss 1.7802 (1.4345)	CeLoss 0.1826 (0.6688)	SegCLSLoss 0.0271 (0.0088)	KLLoss 0.3535 (0.1818)	MaskLoss 0.7744 (0.3715)	MaskBCELoss 0.0078 (0.0512)	MaskDICELoss 0.7666 (0.3203)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 410 is less than current step: 499. Dropping entry: {'train/loss': 1.7115389347076415, 'train/ce_loss': 0.2036865234375, 'train/seg_cls_loss': 0.01451416015625, 'train/kl_loss': 0.3263671875, 'train/mask_bce_loss': 0.14882450802251695, 'train/mask_dice_loss': 0.5852286458015442, 'train/mask_loss': 0.7340531468391418, 'metrics/total_secs_per_batch': 6.839472770690918, 'metrics/data_secs_per_batch': 3.1140920639038088, '_timestamp': 1740971377.9392033}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 410 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971377.9394474}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 411 is less than current step: 499. Dropping entry: {'train/loss': 1.3241245329380036, 'train/ce_loss': 0.52294921875, 'train/seg_cls_loss': 0.01121826171875, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.07634619530290365, 'train/mask_dice_loss': 0.3086652889847755, 'train/mask_loss': 0.3850114792585373, 'metrics/total_secs_per_batch': 5.081871509552002, 'metrics/data_secs_per_batch': 2.468583869934082, '_timestamp': 1740971383.0209038}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 411 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971383.0212965}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 412 is less than current step: 499. Dropping entry: {'train/loss': 1.4344737470149993, 'train/ce_loss': 0.66884765625, 'train/seg_cls_loss': 0.008843994140625, 'train/kl_loss': 0.1818359375, 'train/mask_bce_loss': 0.051220062840729955, 'train/mask_dice_loss': 0.3202648550271988, 'train/mask_loss': 0.3714849203824997, 'metrics/total_secs_per_batch': 5.502099514007568, 'metrics/data_secs_per_batch': 2.63625705242157, '_timestamp': 1740971388.5231757}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 412 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971388.5235364}).
Epoch: [5][414/500]	Time  5.119 ( 5.119)	Loss 1.1797 (1.0834)	CeLoss 1.1797 (0.5091)	SegCLSLoss 0.0000 (0.0065)	KLLoss 0.0000 (0.1441)	MaskLoss 0.0000 (0.2782)	MaskBCELoss 0.0000 (0.0601)	MaskDICELoss 0.0000 (0.2181)
Epoch: [5][415/500]	Time  6.129 ( 6.129)	Loss 1.1437 (1.3844)	CeLoss 0.2695 (0.1982)	SegCLSLoss 0.0110 (0.0118)	KLLoss 0.3711 (0.2916)	MaskLoss 0.4156 (0.5756)	MaskBCELoss 0.0468 (0.1230)	MaskDICELoss 0.3688 (0.4526)
Epoch: [5][416/500]	Time  5.060 ( 5.060)	Loss 2.3874 (1.5863)	CeLoss 0.2461 (0.6857)	SegCLSLoss 0.0145 (0.0092)	KLLoss 0.3555 (0.2166)	MaskLoss 1.0492 (0.4370)	MaskBCELoss 0.3133 (0.0687)	MaskDICELoss 0.7358 (0.3683)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 413 is less than current step: 499. Dropping entry: {'train/loss': 1.083392322063446, 'train/ce_loss': 0.509130859375, 'train/seg_cls_loss': 0.006549072265625, 'train/kl_loss': 0.144140625, 'train/mask_bce_loss': 0.06012412086129189, 'train/mask_dice_loss': 0.21811988651752473, 'train/mask_loss': 0.27824400663375853, 'metrics/total_secs_per_batch': 5.119102478027344, 'metrics/data_secs_per_batch': 2.4381885528564453, '_timestamp': 1740971393.6422787}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 413 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971393.64264}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 414 is less than current step: 499. Dropping entry: {'train/loss': 1.3844436526298523, 'train/ce_loss': 0.1982421875, 'train/seg_cls_loss': 0.011810302734375, 'train/kl_loss': 0.2916015625, 'train/mask_bce_loss': 0.12302800267934799, 'train/mask_dice_loss': 0.45259226486086845, 'train/mask_loss': 0.5756202593445778, 'metrics/total_secs_per_batch': 6.1285481452941895, 'metrics/data_secs_per_batch': 2.808062529563904, '_timestamp': 1740971399.7706056}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 414 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971399.7709029}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 415 is less than current step: 499. Dropping entry: {'train/loss': 1.5862710237503053, 'train/ce_loss': 0.6857421875, 'train/seg_cls_loss': 0.00921630859375, 'train/kl_loss': 0.2166015625, 'train/mask_bce_loss': 0.0686820775270462, 'train/mask_dice_loss': 0.3683499157428741, 'train/mask_loss': 0.43703198730945586, 'metrics/total_secs_per_batch': 5.059572458267212, 'metrics/data_secs_per_batch': 2.4428961515426635, '_timestamp': 1740971404.830354}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 415 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971404.8307767}).
Epoch: [5][417/500]	Time  6.134 ( 6.134)	Loss 0.7294 (1.4732)	CeLoss 0.2539 (0.1985)	SegCLSLoss 0.0134 (0.0168)	KLLoss 0.3613 (0.3305)	MaskLoss 0.2162 (0.6166)	MaskBCELoss 0.0690 (0.1675)	MaskDICELoss 0.1473 (0.4491)
Epoch: [5][418/500]	Time  7.082 ( 7.082)	Loss 2.3500 (1.6365)	CeLoss 0.2080 (0.3345)	SegCLSLoss 0.0162 (0.0129)	KLLoss 0.3750 (0.3273)	MaskLoss 1.0481 (0.6313)	MaskBCELoss 0.2818 (0.1272)	MaskDICELoss 0.7662 (0.5041)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 416 is less than current step: 499. Dropping entry: {'train/loss': 1.47316312789917, 'train/ce_loss': 0.198486328125, 'train/seg_cls_loss': 0.016839599609375, 'train/kl_loss': 0.33046875, 'train/mask_bce_loss': 0.1674771884921938, 'train/mask_dice_loss': 0.44908484518527986, 'train/mask_loss': 0.6165620332583786, 'metrics/total_secs_per_batch': 6.134283781051636, 'metrics/data_secs_per_batch': 2.756583070755005, '_timestamp': 1740971410.9644597}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 416 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971410.9647684}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 417 is less than current step: 499. Dropping entry: {'train/loss': 1.636517596244812, 'train/ce_loss': 0.33447265625, 'train/seg_cls_loss': 0.012939453125, 'train/kl_loss': 0.32734375, 'train/mask_bce_loss': 0.12721452228724955, 'train/mask_dice_loss': 0.5041302025318146, 'train/mask_loss': 0.6313447237014771, 'metrics/total_secs_per_batch': 7.08208155632019, 'metrics/data_secs_per_batch': 3.3932686328887938, '_timestamp': 1740971418.0473237}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 417 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971418.047879}).
Epoch: [5][419/500]	Time  5.896 ( 5.896)	Loss 1.5625 (1.5053)	CeLoss 1.5625 (0.5752)	SegCLSLoss 0.0000 (0.0090)	KLLoss 0.0000 (0.2184)	MaskLoss 0.0000 (0.4519)	MaskBCELoss 0.0000 (0.1209)	MaskDICELoss 0.0000 (0.3310)
[2025-03-02 21:10:29,403] [INFO] [logging.py:96:log_dist] [Rank 0] step=2920, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:10:29,408] [INFO] [timer.py:215:stop] epoch=0/micro_step=29200/global_step=2920, RunningAvgSamplesPerSec=1.4182656124408897, CurrSamplesPerSec=1.829922314215376, MemAllocated=31.56GB, MaxMemAllocated=37.23GB
Epoch: [5][420/500]	Time  5.467 ( 5.467)	Loss 0.5625 (1.5403)	CeLoss 0.5625 (0.6044)	SegCLSLoss 0.0000 (0.0099)	KLLoss 0.0000 (0.2158)	MaskLoss 0.0000 (0.4547)	MaskBCELoss 0.0000 (0.0527)	MaskDICELoss 0.0000 (0.4020)
Epoch: [5][421/500]	Time  5.856 ( 5.856)	Loss 2.0460 (1.6220)	CeLoss 0.1943 (0.4758)	SegCLSLoss 0.0284 (0.0125)	KLLoss 0.3496 (0.2551)	MaskLoss 0.9009 (0.5571)	MaskBCELoss 0.0478 (0.0809)	MaskDICELoss 0.8532 (0.4762)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 418 is less than current step: 499. Dropping entry: {'train/loss': 1.505329042673111, 'train/ce_loss': 0.5751708984375, 'train/seg_cls_loss': 0.0090087890625, 'train/kl_loss': 0.218359375, 'train/mask_bce_loss': 0.12091345805674791, 'train/mask_dice_loss': 0.3309820145368576, 'train/mask_loss': 0.451895472407341, 'metrics/total_secs_per_batch': 5.895735502243042, 'metrics/data_secs_per_batch': 2.940018582344055, '_timestamp': 1740971423.9423974}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 418 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971423.9427662}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 419 is less than current step: 499. Dropping entry: {'train/loss': 1.540334975719452, 'train/ce_loss': 0.60439453125, 'train/seg_cls_loss': 0.0098876953125, 'train/kl_loss': 0.2158203125, 'train/mask_bce_loss': 0.05273612355813384, 'train/mask_dice_loss': 0.4020016834139824, 'train/mask_loss': 0.45473780781030654, 'metrics/total_secs_per_batch': 5.466641902923584, 'metrics/data_secs_per_batch': 2.136139965057373, '_timestamp': 1740971429.4088826}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 419 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971429.4092762}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 420 is less than current step: 499. Dropping entry: {'train/loss': 1.6219823002815246, 'train/ce_loss': 0.475830078125, 'train/seg_cls_loss': 0.01246337890625, 'train/kl_loss': 0.255078125, 'train/mask_bce_loss': 0.08091813158243895, 'train/mask_dice_loss': 0.47619117200374605, 'train/mask_loss': 0.5571093082427978, 'metrics/total_secs_per_batch': 5.856244802474976, 'metrics/data_secs_per_batch': 2.798880124092102, '_timestamp': 1740971435.2652617}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 420 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971435.2655714}).
Epoch: [5][422/500]	Time  6.151 ( 6.151)	Loss 2.4582 (1.7490)	CeLoss 0.1934 (0.2987)	SegCLSLoss 0.0255 (0.0166)	KLLoss 0.3594 (0.3254)	MaskLoss 1.1080 (0.7047)	MaskBCELoss 0.1475 (0.0778)	MaskDICELoss 0.9606 (0.6269)
Epoch: [5][423/500]	Time  5.659 ( 5.659)	Loss 2.3355 (1.5866)	CeLoss 0.1777 (0.4954)	SegCLSLoss 0.0295 (0.0116)	KLLoss 0.3555 (0.2176)	MaskLoss 1.0535 (0.5317)	MaskBCELoss 0.4875 (0.1418)	MaskDICELoss 0.5659 (0.3899)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 421 is less than current step: 499. Dropping entry: {'train/loss': 1.7489598631858825, 'train/ce_loss': 0.29873046875, 'train/seg_cls_loss': 0.0166259765625, 'train/kl_loss': 0.325390625, 'train/mask_bce_loss': 0.07783956471830607, 'train/mask_dice_loss': 0.6268649570643902, 'train/mask_loss': 0.7047045215964317, 'metrics/total_secs_per_batch': 6.1514856815338135, 'metrics/data_secs_per_batch': 2.818920922279358, '_timestamp': 1740971441.41692}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 421 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971441.4172919}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 422 is less than current step: 499. Dropping entry: {'train/loss': 1.5866286635398865, 'train/ce_loss': 0.495361328125, 'train/seg_cls_loss': 0.0116455078125, 'train/kl_loss': 0.217578125, 'train/mask_bce_loss': 0.14179192781448363, 'train/mask_dice_loss': 0.3899257183074951, 'train/mask_loss': 0.5317176401615142, 'metrics/total_secs_per_batch': 5.658812046051025, 'metrics/data_secs_per_batch': 2.441689443588257, '_timestamp': 1740971447.0760043}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 422 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971447.0764399}).
Epoch: [5][424/500]	Time  5.458 ( 5.458)	Loss 1.4930 (1.8677)	CeLoss 0.2480 (0.5129)	SegCLSLoss 0.0118 (0.0126)	KLLoss 0.3574 (0.2496)	MaskLoss 0.6020 (0.6618)	MaskBCELoss 0.0626 (0.0995)	MaskDICELoss 0.5394 (0.5623)
Epoch: [5][425/500]	Time  5.079 ( 5.079)	Loss 2.6388 (1.6479)	CeLoss 0.1719 (0.5403)	SegCLSLoss 0.0204 (0.0122)	KLLoss 0.3594 (0.2525)	MaskLoss 1.2105 (0.5381)	MaskBCELoss 0.3504 (0.0753)	MaskDICELoss 0.8601 (0.4628)
Epoch: [5][426/500]	Time  6.347 ( 6.347)	Loss 0.5029 (1.5786)	CeLoss 0.2656 (0.3268)	SegCLSLoss 0.0104 (0.0140)	KLLoss 0.3652 (0.3258)	MaskLoss 0.0972 (0.6060)	MaskBCELoss 0.0284 (0.1253)	MaskDICELoss 0.0688 (0.4807)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 423 is less than current step: 499. Dropping entry: {'train/loss': 1.8677281856536865, 'train/ce_loss': 0.512890625, 'train/seg_cls_loss': 0.012615966796875, 'train/kl_loss': 0.249609375, 'train/mask_bce_loss': 0.09950428418815135, 'train/mask_dice_loss': 0.5623383104801178, 'train/mask_loss': 0.6618425965309143, 'metrics/total_secs_per_batch': 5.458118677139282, 'metrics/data_secs_per_batch': 2.3902233123779295, '_timestamp': 1740971452.533733}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 423 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971452.534109}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 424 is less than current step: 499. Dropping entry: {'train/loss': 1.6478552222251892, 'train/ce_loss': 0.54033203125, 'train/seg_cls_loss': 0.01219482421875, 'train/kl_loss': 0.2525390625, 'train/mask_bce_loss': 0.07531863558106125, 'train/mask_dice_loss': 0.4628179520368576, 'train/mask_loss': 0.5381365939974785, 'metrics/total_secs_per_batch': 5.079201936721802, 'metrics/data_secs_per_batch': 2.1234524488449096, '_timestamp': 1740971457.6130345}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 424 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971457.6134188}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 425 is less than current step: 499. Dropping entry: {'train/loss': 1.578611809015274, 'train/ce_loss': 0.326806640625, 'train/seg_cls_loss': 0.013983154296875, 'train/kl_loss': 0.32578125, 'train/mask_bce_loss': 0.12533067725598812, 'train/mask_dice_loss': 0.4806744381785393, 'train/mask_loss': 0.606005122512579, 'metrics/total_secs_per_batch': 6.347384452819824, 'metrics/data_secs_per_batch': 3.070433783531189, '_timestamp': 1740971463.9603474}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 425 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971463.960767}).
Epoch: [5][427/500]	Time  5.821 ( 5.821)	Loss 0.9336 (1.8880)	CeLoss 0.9336 (0.4005)	SegCLSLoss 0.0000 (0.0138)	KLLoss 0.0000 (0.2959)	MaskLoss 0.0000 (0.7257)	MaskBCELoss 0.0000 (0.2080)	MaskDICELoss 0.0000 (0.5177)
Epoch: [5][428/500]	Time  5.423 ( 5.423)	Loss 1.3642 (1.3786)	CeLoss 0.2070 (0.4531)	SegCLSLoss 0.0136 (0.0112)	KLLoss 0.3633 (0.1822)	MaskLoss 0.5571 (0.4508)	MaskBCELoss 0.1319 (0.1096)	MaskDICELoss 0.4252 (0.3412)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 426 is less than current step: 499. Dropping entry: {'train/loss': 1.8880128383636474, 'train/ce_loss': 0.40048828125, 'train/seg_cls_loss': 0.013763427734375, 'train/kl_loss': 0.2958984375, 'train/mask_bce_loss': 0.208042236790061, 'train/mask_dice_loss': 0.5177024677395821, 'train/mask_loss': 0.7257447004318237, 'metrics/total_secs_per_batch': 5.821033716201782, 'metrics/data_secs_per_batch': 2.5926870346069335, '_timestamp': 1740971469.7815537}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 426 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971469.7818263}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 427 is less than current step: 499. Dropping entry: {'train/loss': 1.3786486983299255, 'train/ce_loss': 0.453125, 'train/seg_cls_loss': 0.011248779296875, 'train/kl_loss': 0.1822265625, 'train/mask_bce_loss': 0.10963954105973243, 'train/mask_dice_loss': 0.3411594182252884, 'train/mask_loss': 0.45079895853996277, 'metrics/total_secs_per_batch': 5.4227213859558105, 'metrics/data_secs_per_batch': 2.5519490480422973, '_timestamp': 1740971475.204081}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 427 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971475.2044137}).
Epoch: [5][429/500]	Time  6.837 ( 6.837)	Loss 1.7420 (1.6614)	CeLoss 0.2832 (0.3021)	SegCLSLoss 0.0135 (0.0138)	KLLoss 0.3594 (0.2900)	MaskLoss 0.7089 (0.6617)	MaskBCELoss 0.1870 (0.1909)	MaskDICELoss 0.5219 (0.4708)
[2025-03-02 21:11:28,573] [INFO] [logging.py:96:log_dist] [Rank 0] step=2930, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:11:28,579] [INFO] [timer.py:215:stop] epoch=0/micro_step=29300/global_step=2930, RunningAvgSamplesPerSec=1.419046020402556, CurrSamplesPerSec=1.529828267156643, MemAllocated=30.71GB, MaxMemAllocated=37.23GB
Epoch: [5][430/500]	Time  6.539 ( 6.539)	Loss 1.4688 (1.8375)	CeLoss 1.4688 (0.3517)	SegCLSLoss 0.0000 (0.0148)	KLLoss 0.0000 (0.3275)	MaskLoss 0.0000 (0.7230)	MaskBCELoss 0.0000 (0.1339)	MaskDICELoss 0.0000 (0.5891)
Epoch: [5][431/500]	Time  5.839 ( 5.839)	Loss 1.6051 (1.6527)	CeLoss 0.1455 (0.4471)	SegCLSLoss 0.0266 (0.0135)	KLLoss 0.3750 (0.2602)	MaskLoss 0.7039 (0.5863)	MaskBCELoss 0.1292 (0.1326)	MaskDICELoss 0.5747 (0.4537)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 428 is less than current step: 499. Dropping entry: {'train/loss': 1.6614078283309937, 'train/ce_loss': 0.302099609375, 'train/seg_cls_loss': 0.01376953125, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.19092784579843283, 'train/mask_dice_loss': 0.47080633640289304, 'train/mask_loss': 0.6617341876029968, 'metrics/total_secs_per_batch': 6.836669206619263, 'metrics/data_secs_per_batch': 3.2393261671066282, '_timestamp': 1740971482.0411}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 428 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971482.0414464}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 429 is less than current step: 499. Dropping entry: {'train/loss': 1.837492835521698, 'train/ce_loss': 0.35166015625, 'train/seg_cls_loss': 0.01475830078125, 'train/kl_loss': 0.3275390625, 'train/mask_bce_loss': 0.13387082004919648, 'train/mask_dice_loss': 0.589123648405075, 'train/mask_loss': 0.722994464635849, 'metrics/total_secs_per_batch': 6.539027690887451, 'metrics/data_secs_per_batch': 3.0811758756637575, '_timestamp': 1740971488.5796053}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 429 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971488.5799844}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 430 is less than current step: 499. Dropping entry: {'train/loss': 1.6526673555374145, 'train/ce_loss': 0.4470703125, 'train/seg_cls_loss': 0.01346435546875, 'train/kl_loss': 0.26015625, 'train/mask_bce_loss': 0.1325669089332223, 'train/mask_dice_loss': 0.4537277013063431, 'train/mask_loss': 0.5862946093082428, 'metrics/total_secs_per_batch': 5.839051961898804, 'metrics/data_secs_per_batch': 2.4876752614974977, '_timestamp': 1740971494.4192648}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 430 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971494.4196758}).
Epoch: [5][432/500]	Time  5.571 ( 5.571)	Loss 0.0625 (1.0990)	CeLoss 0.0625 (0.5067)	SegCLSLoss 0.0000 (0.0075)	KLLoss 0.0000 (0.1527)	MaskLoss 0.0000 (0.2867)	MaskBCELoss 0.0000 (0.0245)	MaskDICELoss 0.0000 (0.2622)
Epoch: [5][433/500]	Time  6.464 ( 6.464)	Loss 1.9198 (1.6782)	CeLoss 0.2598 (0.3196)	SegCLSLoss 0.0101 (0.0136)	KLLoss 0.3633 (0.3258)	MaskLoss 0.8085 (0.6594)	MaskBCELoss 0.5679 (0.0921)	MaskDICELoss 0.2407 (0.5673)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 431 is less than current step: 499. Dropping entry: {'train/loss': 1.0990171432495117, 'train/ce_loss': 0.50673828125, 'train/seg_cls_loss': 0.007470703125, 'train/kl_loss': 0.152734375, 'train/mask_bce_loss': 0.024493534164503217, 'train/mask_dice_loss': 0.26217323541641235, 'train/mask_loss': 0.28666676878929137, 'metrics/total_secs_per_batch': 5.571383714675903, 'metrics/data_secs_per_batch': 2.8648653745651247, '_timestamp': 1740971499.9903874}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 431 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971499.9907868}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 432 is less than current step: 499. Dropping entry: {'train/loss': 1.6781822204589845, 'train/ce_loss': 0.31962890625, 'train/seg_cls_loss': 0.0135986328125, 'train/kl_loss': 0.32578125, 'train/mask_bce_loss': 0.09209120031446219, 'train/mask_dice_loss': 0.5672635763883591, 'train/mask_loss': 0.6593547761440277, 'metrics/total_secs_per_batch': 6.463818073272705, 'metrics/data_secs_per_batch': 3.0706557750701906, '_timestamp': 1740971506.4542143}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 432 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971506.4545805}).
Epoch: [5][434/500]	Time  6.346 ( 6.346)	Loss 1.8109 (1.7231)	CeLoss 0.2080 (0.2102)	SegCLSLoss 0.0156 (0.0185)	KLLoss 0.3496 (0.3594)	MaskLoss 0.7805 (0.7339)	MaskBCELoss 0.0012 (0.1274)	MaskDICELoss 0.7792 (0.6064)
Epoch: [5][435/500]	Time  5.642 ( 5.642)	Loss 1.0406 (1.8679)	CeLoss 0.2949 (0.3009)	SegCLSLoss 0.0093 (0.0156)	KLLoss 0.3633 (0.3260)	MaskLoss 0.3523 (0.7633)	MaskBCELoss 0.0755 (0.0576)	MaskDICELoss 0.2768 (0.7057)
Epoch: [5][436/500]	Time  6.209 ( 6.209)	Loss 1.4453 (1.7123)	CeLoss 1.4453 (0.3717)	SegCLSLoss 0.0000 (0.0134)	KLLoss 0.0000 (0.3277)	MaskLoss 0.0000 (0.6504)	MaskBCELoss 0.0000 (0.1399)	MaskDICELoss 0.0000 (0.5105)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 433 is less than current step: 499. Dropping entry: {'train/loss': 1.723114362359047, 'train/ce_loss': 0.210205078125, 'train/seg_cls_loss': 0.018463134765625, 'train/kl_loss': 0.359375, 'train/mask_bce_loss': 0.1274321474134922, 'train/mask_dice_loss': 0.6064394831657409, 'train/mask_loss': 0.733871640264988, 'metrics/total_secs_per_batch': 6.346290349960327, 'metrics/data_secs_per_batch': 2.8419188022613526, '_timestamp': 1740971512.800806}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 433 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971512.8012643}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 434 is less than current step: 499. Dropping entry: {'train/loss': 1.8678898811340332, 'train/ce_loss': 0.30087890625, 'train/seg_cls_loss': 0.015631103515625, 'train/kl_loss': 0.3259765625, 'train/mask_bce_loss': 0.057618661876767875, 'train/mask_dice_loss': 0.7057208001613617, 'train/mask_loss': 0.7633394658565521, 'metrics/total_secs_per_batch': 5.641950368881226, 'metrics/data_secs_per_batch': 2.846200633049011, '_timestamp': 1740971518.4423378}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 434 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971518.4426763}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 435 is less than current step: 499. Dropping entry: {'train/loss': 1.7122544765472412, 'train/ce_loss': 0.3716796875, 'train/seg_cls_loss': 0.01339111328125, 'train/kl_loss': 0.327734375, 'train/mask_bce_loss': 0.13988007009029388, 'train/mask_dice_loss': 0.5105342745780945, 'train/mask_loss': 0.6504143416881562, 'metrics/total_secs_per_batch': 6.2090184688568115, 'metrics/data_secs_per_batch': 2.9446336030960083, '_timestamp': 1740971524.6516652}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 435 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971524.6520853}).
Epoch: [5][437/500]	Time  4.464 ( 4.464)	Loss 2.4048 (1.6623)	CeLoss 0.2314 (0.6630)	SegCLSLoss 0.0219 (0.0103)	KLLoss 0.3594 (0.2201)	MaskLoss 1.0637 (0.4862)	MaskBCELoss 0.0989 (0.0849)	MaskDICELoss 0.9648 (0.4013)
Epoch: [5][438/500]	Time  5.174 ( 5.174)	Loss 1.1641 (1.8019)	CeLoss 1.1641 (0.6219)	SegCLSLoss 0.0000 (0.0096)	KLLoss 0.0000 (0.2195)	MaskLoss 0.0000 (0.5766)	MaskBCELoss 0.0000 (0.1379)	MaskDICELoss 0.0000 (0.4387)
Epoch: [5][439/500]	Time  5.406 ( 5.406)	Loss 0.0825 (1.2089)	CeLoss 0.0825 (0.4708)	SegCLSLoss 0.0000 (0.0089)	KLLoss 0.0000 (0.1471)	MaskLoss 0.0000 (0.3594)	MaskBCELoss 0.0000 (0.1279)	MaskDICELoss 0.0000 (0.2316)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 436 is less than current step: 499. Dropping entry: {'train/loss': 1.6623484373092652, 'train/ce_loss': 0.66298828125, 'train/seg_cls_loss': 0.010260009765625, 'train/kl_loss': 0.2201171875, 'train/mask_bce_loss': 0.08488839119672775, 'train/mask_dice_loss': 0.4013151332736015, 'train/mask_loss': 0.4862035244703293, 'metrics/total_secs_per_batch': 4.4637792110443115, 'metrics/data_secs_per_batch': 2.0226651430130005, '_timestamp': 1740971529.11518}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 436 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971529.115544}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 437 is less than current step: 499. Dropping entry: {'train/loss': 1.8018617033958435, 'train/ce_loss': 0.621875, 'train/seg_cls_loss': 0.00955810546875, 'train/kl_loss': 0.21953125, 'train/mask_bce_loss': 0.1379376132041216, 'train/mask_dice_loss': 0.43867682814598086, 'train/mask_loss': 0.5766144394874573, 'metrics/total_secs_per_batch': 5.173530101776123, 'metrics/data_secs_per_batch': 2.2200701236724854, '_timestamp': 1740971534.2886603}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 437 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971534.2890112}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 438 is less than current step: 499. Dropping entry: {'train/loss': 1.2089009284973145, 'train/ce_loss': 0.470849609375, 'train/seg_cls_loss': 0.00892333984375, 'train/kl_loss': 0.1470703125, 'train/mask_bce_loss': 0.12785379961133003, 'train/mask_dice_loss': 0.2315771386027336, 'train/mask_loss': 0.35943093001842497, 'metrics/total_secs_per_batch': 5.4063591957092285, 'metrics/data_secs_per_batch': 2.2361016511917113, '_timestamp': 1740971539.695232}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 438 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971539.6956651}).
[2025-03-02 21:12:25,510] [INFO] [logging.py:96:log_dist] [Rank 0] step=2940, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:12:25,515] [INFO] [timer.py:215:stop] epoch=0/micro_step=29400/global_step=2940, RunningAvgSamplesPerSec=1.4199753219794091, CurrSamplesPerSec=1.7184398958571434, MemAllocated=30.78GB, MaxMemAllocated=37.23GB
Epoch: [5][440/500]	Time  5.821 ( 5.821)	Loss 0.7422 (1.9424)	CeLoss 0.7422 (0.4894)	SegCLSLoss 0.0000 (0.0130)	KLLoss 0.0000 (0.2561)	MaskLoss 0.0000 (0.7105)	MaskBCELoss 0.0000 (0.1790)	MaskDICELoss 0.0000 (0.5315)
Epoch: [5][441/500]	Time  6.584 ( 6.584)	Loss 2.8447 (1.5571)	CeLoss 0.1699 (0.3055)	SegCLSLoss 0.0249 (0.0156)	KLLoss 0.3828 (0.3299)	MaskLoss 1.3120 (0.6054)	MaskBCELoss 0.5334 (0.1444)	MaskDICELoss 0.7786 (0.4611)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 439 is less than current step: 499. Dropping entry: {'train/loss': 1.942353117465973, 'train/ce_loss': 0.48935546875, 'train/seg_cls_loss': 0.0129638671875, 'train/kl_loss': 0.2560546875, 'train/mask_bce_loss': 0.1789990570396185, 'train/mask_dice_loss': 0.5315329730510712, 'train/mask_loss': 0.7105320364236831, 'metrics/total_secs_per_batch': 5.821373224258423, 'metrics/data_secs_per_batch': 2.1277312278747558, '_timestamp': 1740971545.5161002}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 439 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971545.5163062}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 440 is less than current step: 499. Dropping entry: {'train/loss': 1.557066124677658, 'train/ce_loss': 0.30546875, 'train/seg_cls_loss': 0.015631103515625, 'train/kl_loss': 0.3298828125, 'train/mask_bce_loss': 0.14435675237327814, 'train/mask_dice_loss': 0.46108060628175734, 'train/mask_loss': 0.6054373621940613, 'metrics/total_secs_per_batch': 6.583844423294067, 'metrics/data_secs_per_batch': 2.906304717063904, '_timestamp': 1740971552.100484}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 440 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971552.100881}).
Epoch: [5][442/500]	Time  5.886 ( 5.886)	Loss 2.4392 (1.8830)	CeLoss 0.1011 (0.3519)	SegCLSLoss 0.0410 (0.0184)	KLLoss 0.3770 (0.2926)	MaskLoss 1.1400 (0.7464)	MaskBCELoss 0.2369 (0.1113)	MaskDICELoss 0.9031 (0.6351)
Epoch: [5][443/500]	Time  6.037 ( 6.037)	Loss 1.8633 (1.4126)	CeLoss 0.1963 (0.4485)	SegCLSLoss 0.0193 (0.0097)	KLLoss 0.3613 (0.2523)	MaskLoss 0.8105 (0.4670)	MaskBCELoss 0.0471 (0.0408)	MaskDICELoss 0.7634 (0.4262)
Epoch: [5][444/500]	Time  6.273 ( 6.273)	Loss 0.9767 (1.3629)	CeLoss 0.2295 (0.3115)	SegCLSLoss 0.0159 (0.0128)	KLLoss 0.3613 (0.2896)	MaskLoss 0.3516 (0.5080)	MaskBCELoss 0.0087 (0.0729)	MaskDICELoss 0.3429 (0.4351)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 441 is less than current step: 499. Dropping entry: {'train/loss': 1.8830268859863282, 'train/ce_loss': 0.351904296875, 'train/seg_cls_loss': 0.018426513671875, 'train/kl_loss': 0.292578125, 'train/mask_bce_loss': 0.111333735473454, 'train/mask_dice_loss': 0.6350625157356262, 'train/mask_loss': 0.7463962495326996, 'metrics/total_secs_per_batch': 5.8860321044921875, 'metrics/data_secs_per_batch': 2.698474335670471, '_timestamp': 1740971557.9862807}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 441 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971557.9866035}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 442 is less than current step: 499. Dropping entry: {'train/loss': 1.412550926208496, 'train/ce_loss': 0.4484619140625, 'train/seg_cls_loss': 0.009710693359375, 'train/kl_loss': 0.25234375, 'train/mask_bce_loss': 0.04083437146618962, 'train/mask_dice_loss': 0.42617106437683105, 'train/mask_loss': 0.46700544059276583, 'metrics/total_secs_per_batch': 6.036805152893066, 'metrics/data_secs_per_batch': 2.415482759475708, '_timestamp': 1740971564.0232012}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 442 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971564.0235705}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 443 is less than current step: 499. Dropping entry: {'train/loss': 1.3628542304039002, 'train/ce_loss': 0.311474609375, 'train/seg_cls_loss': 0.012774658203125, 'train/kl_loss': 0.2896484375, 'train/mask_bce_loss': 0.07288281843066216, 'train/mask_dice_loss': 0.4350823789834976, 'train/mask_loss': 0.5079651951789856, 'metrics/total_secs_per_batch': 6.273165464401245, 'metrics/data_secs_per_batch': 2.7610830307006835, '_timestamp': 1740971570.296187}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 443 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971570.2964911}).
Epoch: [5][445/500]	Time  4.767 ( 4.767)	Loss 2.3747 (1.4334)	CeLoss 0.3223 (0.6227)	SegCLSLoss 0.0103 (0.0071)	KLLoss 0.3750 (0.1855)	MaskLoss 1.0047 (0.3943)	MaskBCELoss 0.0052 (0.0854)	MaskDICELoss 0.9995 (0.3089)
Epoch: [5][446/500]	Time  6.016 ( 6.016)	Loss 0.0806 (1.4266)	CeLoss 0.0806 (0.3427)	SegCLSLoss 0.0000 (0.0112)	KLLoss 0.0000 (0.2189)	MaskLoss 0.0000 (0.5281)	MaskBCELoss 0.0000 (0.0978)	MaskDICELoss 0.0000 (0.4303)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 444 is less than current step: 499. Dropping entry: {'train/loss': 1.4333977699279785, 'train/ce_loss': 0.62265625, 'train/seg_cls_loss': 0.007135009765625, 'train/kl_loss': 0.185546875, 'train/mask_bce_loss': 0.08543865587562323, 'train/mask_dice_loss': 0.3088969558477402, 'train/mask_loss': 0.39433560371398924, 'metrics/total_secs_per_batch': 4.766823768615723, 'metrics/data_secs_per_batch': 2.2769466400146485, '_timestamp': 1740971575.063244}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 444 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971575.0636413}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 445 is less than current step: 499. Dropping entry: {'train/loss': 1.4265711545944213, 'train/ce_loss': 0.342724609375, 'train/seg_cls_loss': 0.01116943359375, 'train/kl_loss': 0.2189453125, 'train/mask_bce_loss': 0.09783029910176992, 'train/mask_dice_loss': 0.43027460277080537, 'train/mask_loss': 0.5281049013137817, 'metrics/total_secs_per_batch': 6.016066551208496, 'metrics/data_secs_per_batch': 2.756243371963501, '_timestamp': 1740971581.0792038}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 445 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971581.0795405}).
Epoch: [5][447/500]	Time  6.077 ( 6.077)	Loss 0.2852 (1.4882)	CeLoss 0.2852 (0.3828)	SegCLSLoss 0.0000 (0.0114)	KLLoss 0.0000 (0.2928)	MaskLoss 0.0000 (0.5351)	MaskBCELoss 0.0000 (0.0826)	MaskDICELoss 0.0000 (0.4525)
Epoch: [5][448/500]	Time  6.615 ( 6.615)	Loss 0.2891 (1.6540)	CeLoss 0.2891 (0.3925)	SegCLSLoss 0.0000 (0.0104)	KLLoss 0.0000 (0.2525)	MaskLoss 0.0000 (0.6156)	MaskBCELoss 0.0000 (0.1121)	MaskDICELoss 0.0000 (0.5035)
Epoch: [5][449/500]	Time  5.583 ( 5.583)	Loss 2.8712 (1.6615)	CeLoss 0.1240 (0.4898)	SegCLSLoss 0.0320 (0.0116)	KLLoss 0.3711 (0.2514)	MaskLoss 1.3472 (0.5703)	MaskBCELoss 0.4686 (0.1172)	MaskDICELoss 0.8786 (0.4531)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 446 is less than current step: 499. Dropping entry: {'train/loss': 1.4882121205329895, 'train/ce_loss': 0.3828125, 'train/seg_cls_loss': 0.0113525390625, 'train/kl_loss': 0.2927734375, 'train/mask_bce_loss': 0.08261642272118479, 'train/mask_dice_loss': 0.4525052815675735, 'train/mask_loss': 0.5351217001676559, 'metrics/total_secs_per_batch': 6.07650089263916, 'metrics/data_secs_per_batch': 2.484514355659485, '_timestamp': 1740971587.1556468}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 446 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971587.155987}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 447 is less than current step: 499. Dropping entry: {'train/loss': 1.653969955444336, 'train/ce_loss': 0.39248046875, 'train/seg_cls_loss': 0.01041259765625, 'train/kl_loss': 0.2525390625, 'train/mask_bce_loss': 0.11209513656795025, 'train/mask_dice_loss': 0.5034640610218049, 'train/mask_loss': 0.615559196472168, 'metrics/total_secs_per_batch': 6.615350961685181, 'metrics/data_secs_per_batch': 3.0644800901412963, '_timestamp': 1740971593.7712622}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 447 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971593.771647}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 448 is less than current step: 499. Dropping entry: {'train/loss': 1.661502367258072, 'train/ce_loss': 0.48984375, 'train/seg_cls_loss': 0.01162109375, 'train/kl_loss': 0.2513671875, 'train/mask_bce_loss': 0.11722609773278236, 'train/mask_dice_loss': 0.45307586789131166, 'train/mask_loss': 0.5703019738197327, 'metrics/total_secs_per_batch': 5.582766056060791, 'metrics/data_secs_per_batch': 2.4743487358093263, '_timestamp': 1740971599.3537278}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 448 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971599.3540452}).
[2025-03-02 21:13:25,543] [INFO] [logging.py:96:log_dist] [Rank 0] step=2950, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:13:25,549] [INFO] [timer.py:215:stop] epoch=0/micro_step=29500/global_step=2950, RunningAvgSamplesPerSec=1.4206873007668037, CurrSamplesPerSec=1.6142076927547186, MemAllocated=31.66GB, MaxMemAllocated=37.23GB
Epoch: [5][450/500]	Time  6.197 ( 6.197)	Loss 1.3618 (1.6281)	CeLoss 0.2314 (0.3339)	SegCLSLoss 0.0134 (0.0119)	KLLoss 0.3555 (0.2539)	MaskLoss 0.5442 (0.6315)	MaskBCELoss 0.0605 (0.1586)	MaskDICELoss 0.4836 (0.4728)
Epoch: [5][451/500]	Time  7.378 ( 7.378)	Loss 1.5149 (1.7441)	CeLoss 0.2393 (0.2973)	SegCLSLoss 0.0112 (0.0151)	KLLoss 0.3613 (0.3256)	MaskLoss 0.6168 (0.7034)	MaskBCELoss 0.1233 (0.1298)	MaskDICELoss 0.4935 (0.5736)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 449 is less than current step: 499. Dropping entry: {'train/loss': 1.6280987024307252, 'train/ce_loss': 0.33388671875, 'train/seg_cls_loss': 0.01192626953125, 'train/kl_loss': 0.25390625, 'train/mask_bce_loss': 0.15863759499043226, 'train/mask_dice_loss': 0.47284340262413027, 'train/mask_loss': 0.631480997800827, 'metrics/total_secs_per_batch': 6.196760654449463, 'metrics/data_secs_per_batch': 2.7772902250289917, '_timestamp': 1740971605.5503278}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 449 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971605.5506945}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 450 is less than current step: 499. Dropping entry: {'train/loss': 1.7441172838211059, 'train/ce_loss': 0.297265625, 'train/seg_cls_loss': 0.01512451171875, 'train/kl_loss': 0.3255859375, 'train/mask_bce_loss': 0.1298306666314602, 'train/mask_dice_loss': 0.573575633764267, 'train/mask_loss': 0.7034063041210175, 'metrics/total_secs_per_batch': 7.377758979797363, 'metrics/data_secs_per_batch': 3.387050986289978, '_timestamp': 1740971612.9281995}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 450 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971612.9285026}).
Epoch: [5][452/500]	Time  6.830 ( 6.830)	Loss 2.2248 (1.9025)	CeLoss 0.2988 (0.3037)	SegCLSLoss 0.0143 (0.0155)	KLLoss 0.3691 (0.3254)	MaskLoss 0.9405 (0.7792)	MaskBCELoss 0.2440 (0.1193)	MaskDICELoss 0.6965 (0.6599)
Epoch: [5][453/500]	Time  6.432 ( 6.432)	Loss 2.0139 (2.1029)	CeLoss 0.2539 (0.1971)	SegCLSLoss 0.0186 (0.0165)	KLLoss 0.3750 (0.3320)	MaskLoss 0.8566 (0.9322)	MaskBCELoss 0.0712 (0.2973)	MaskDICELoss 0.7854 (0.6349)
Epoch: [5][454/500]	Time  5.482 ( 5.482)	Loss 1.5598 (1.6588)	CeLoss 0.2178 (0.2926)	SegCLSLoss 0.0226 (0.0179)	KLLoss 0.3750 (0.2912)	MaskLoss 0.6461 (0.6640)	MaskBCELoss 0.1980 (0.1922)	MaskDICELoss 0.4481 (0.4719)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 451 is less than current step: 499. Dropping entry: {'train/loss': 1.9024755716323853, 'train/ce_loss': 0.3037109375, 'train/seg_cls_loss': 0.01552734375, 'train/kl_loss': 0.325390625, 'train/mask_bce_loss': 0.1192612286657095, 'train/mask_dice_loss': 0.6599062472581864, 'train/mask_loss': 0.7791674792766571, 'metrics/total_secs_per_batch': 6.829529285430908, 'metrics/data_secs_per_batch': 2.930297541618347, '_timestamp': 1740971619.7578816}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 451 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971619.7582679}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 452 is less than current step: 499. Dropping entry: {'train/loss': 2.1028892159461976, 'train/ce_loss': 0.197119140625, 'train/seg_cls_loss': 0.016455078125, 'train/kl_loss': 0.33203125, 'train/mask_bce_loss': 0.2972824227064848, 'train/mask_dice_loss': 0.6349483162164689, 'train/mask_loss': 0.9322307467460632, 'metrics/total_secs_per_batch': 6.432161331176758, 'metrics/data_secs_per_batch': 2.9354385137557983, '_timestamp': 1740971626.1899123}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 452 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971626.1902046}).
Epoch: [5][455/500]	Time  3.943 ( 3.943)	Loss 1.1250 (1.1585)	CeLoss 1.1250 (0.7161)	SegCLSLoss 0.0000 (0.0044)	KLLoss 0.0000 (0.1102)	MaskLoss 0.0000 (0.2147)	MaskBCELoss 0.0000 (0.0455)	MaskDICELoss 0.0000 (0.1691)
Epoch: [5][456/500]	Time  5.555 ( 5.555)	Loss 1.1745 (1.0984)	CeLoss 0.2520 (0.4381)	SegCLSLoss 0.0110 (0.0107)	KLLoss 0.3613 (0.2186)	MaskLoss 0.4408 (0.3166)	MaskBCELoss 0.0244 (0.0649)	MaskDICELoss 0.4163 (0.2517)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 453 is less than current step: 499. Dropping entry: {'train/loss': 1.6588395953178405, 'train/ce_loss': 0.292578125, 'train/seg_cls_loss': 0.017919921875, 'train/kl_loss': 0.2912109375, 'train/mask_bce_loss': 0.19216527864336969, 'train/mask_dice_loss': 0.4718736410140991, 'train/mask_loss': 0.6640389204025269, 'metrics/total_secs_per_batch': 5.4822838306427, 'metrics/data_secs_per_batch': 2.430259084701538, '_timestamp': 1740971631.6722496}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 453 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971631.6725762}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 454 is less than current step: 499. Dropping entry: {'train/loss': 1.1585260272026061, 'train/ce_loss': 0.716064453125, 'train/seg_cls_loss': 0.004364013671875, 'train/kl_loss': 0.11015625, 'train/mask_bce_loss': 0.045547911897301674, 'train/mask_dice_loss': 0.16913990974426268, 'train/mask_loss': 0.2146878182888031, 'metrics/total_secs_per_batch': 3.9430270195007324, 'metrics/data_secs_per_batch': 1.9443207502365112, '_timestamp': 1740971635.6153762}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 454 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971635.6157155}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 455 is less than current step: 499. Dropping entry: {'train/loss': 1.0984491646289825, 'train/ce_loss': 0.438134765625, 'train/seg_cls_loss': 0.010662841796875, 'train/kl_loss': 0.2185546875, 'train/mask_bce_loss': 0.06487090084701777, 'train/mask_dice_loss': 0.25171208307147025, 'train/mask_loss': 0.3165829822421074, 'metrics/total_secs_per_batch': 5.555089712142944, 'metrics/data_secs_per_batch': 2.1861403465270994, '_timestamp': 1740971641.1703446}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 455 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971641.1706238}).
Epoch: [5][457/500]	Time  5.540 ( 5.540)	Loss 1.0518 (1.2169)	CeLoss 0.2852 (0.3766)	SegCLSLoss 0.0220 (0.0111)	KLLoss 0.3711 (0.2570)	MaskLoss 0.3589 (0.4044)	MaskBCELoss 0.2103 (0.1120)	MaskDICELoss 0.1486 (0.2925)
Epoch: [5][458/500]	Time  6.698 ( 6.698)	Loss 1.0694 (1.3899)	CeLoss 0.2070 (0.3242)	SegCLSLoss 0.0102 (0.0129)	KLLoss 0.3633 (0.2895)	MaskLoss 0.4107 (0.5150)	MaskBCELoss 0.0723 (0.0587)	MaskDICELoss 0.3383 (0.4563)
Epoch: [5][459/500]	Time  6.361 ( 6.361)	Loss 1.8331 (1.4069)	CeLoss 0.2402 (0.3477)	SegCLSLoss 0.0131 (0.0118)	KLLoss 0.3633 (0.2906)	MaskLoss 0.7749 (0.5122)	MaskBCELoss 0.1241 (0.0633)	MaskDICELoss 0.6508 (0.4489)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 456 is less than current step: 499. Dropping entry: {'train/loss': 1.2168607473373414, 'train/ce_loss': 0.3765625, 'train/seg_cls_loss': 0.01107177734375, 'train/kl_loss': 0.25703125, 'train/mask_bce_loss': 0.11196495406329632, 'train/mask_dice_loss': 0.292461521551013, 'train/mask_loss': 0.4044264733791351, 'metrics/total_secs_per_batch': 5.540335178375244, 'metrics/data_secs_per_batch': 2.7454951763153077, '_timestamp': 1740971646.710659}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 456 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971646.7108543}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 457 is less than current step: 499. Dropping entry: {'train/loss': 1.3898648977279664, 'train/ce_loss': 0.32421875, 'train/seg_cls_loss': 0.0129150390625, 'train/kl_loss': 0.289453125, 'train/mask_bce_loss': 0.058708816207945345, 'train/mask_dice_loss': 0.4563408255577087, 'train/mask_loss': 0.5150496393442154, 'metrics/total_secs_per_batch': 6.6977458000183105, 'metrics/data_secs_per_batch': 2.8373104333877563, '_timestamp': 1740971653.408591}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 457 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971653.408937}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 458 is less than current step: 499. Dropping entry: {'train/loss': 1.4069012761116029, 'train/ce_loss': 0.34765625, 'train/seg_cls_loss': 0.0117919921875, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.06332786791026593, 'train/mask_dice_loss': 0.44886300414800645, 'train/mask_loss': 0.512190866470337, 'metrics/total_secs_per_batch': 6.360997200012207, 'metrics/data_secs_per_batch': 2.9810900688171387, '_timestamp': 1740971659.7693799}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 458 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971659.769652}).
[2025-03-02 21:14:25,154] [INFO] [logging.py:96:log_dist] [Rank 0] step=2960, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:14:25,160] [INFO] [timer.py:215:stop] epoch=0/micro_step=29600/global_step=2960, RunningAvgSamplesPerSec=1.4214239716195651, CurrSamplesPerSec=1.8551892237683911, MemAllocated=31.64GB, MaxMemAllocated=37.23GB
Epoch: [5][460/500]	Time  5.392 ( 5.392)	Loss 0.2129 (1.3880)	CeLoss 0.2129 (0.5590)	SegCLSLoss 0.0000 (0.0096)	KLLoss 0.0000 (0.2219)	MaskLoss 0.0000 (0.4009)	MaskBCELoss 0.0000 (0.0730)	MaskDICELoss 0.0000 (0.3280)
Epoch: [5][461/500]	Time  6.163 ( 6.163)	Loss 2.4775 (2.0462)	CeLoss 0.1846 (0.3615)	SegCLSLoss 0.0242 (0.0154)	KLLoss 0.3652 (0.2889)	MaskLoss 1.1226 (0.8239)	MaskBCELoss 0.3151 (0.1952)	MaskDICELoss 0.8075 (0.6287)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 459 is less than current step: 499. Dropping entry: {'train/loss': 1.388032740354538, 'train/ce_loss': 0.558984375, 'train/seg_cls_loss': 0.009552001953125, 'train/kl_loss': 0.221875, 'train/mask_bce_loss': 0.07297283820807934, 'train/mask_dice_loss': 0.327977129817009, 'train/mask_loss': 0.40094996988773346, 'metrics/total_secs_per_batch': 5.391897201538086, 'metrics/data_secs_per_batch': 2.225894570350647, '_timestamp': 1740971665.1611733}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 459 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971665.1614027}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 460 is less than current step: 499. Dropping entry: {'train/loss': 2.046233057975769, 'train/ce_loss': 0.3615234375, 'train/seg_cls_loss': 0.015386962890625, 'train/kl_loss': 0.2888671875, 'train/mask_bce_loss': 0.19520049281418322, 'train/mask_dice_loss': 0.6287461221218109, 'train/mask_loss': 0.8239466190338135, 'metrics/total_secs_per_batch': 6.162806510925293, 'metrics/data_secs_per_batch': 2.7053088426589964, '_timestamp': 1740971671.3241246}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 460 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971671.324405}).
Epoch: [5][462/500]	Time  6.103 ( 6.103)	Loss 1.5107 (1.6345)	CeLoss 0.2773 (0.2777)	SegCLSLoss 0.0156 (0.0160)	KLLoss 0.3633 (0.2926)	MaskLoss 0.5942 (0.6596)	MaskBCELoss 0.0319 (0.1170)	MaskDICELoss 0.5623 (0.5426)
Epoch: [5][463/500]	Time  5.665 ( 5.665)	Loss 1.4922 (1.5211)	CeLoss 1.4922 (0.3898)	SegCLSLoss 0.0000 (0.0123)	KLLoss 0.0000 (0.2527)	MaskLoss 0.0000 (0.5500)	MaskBCELoss 0.0000 (0.1113)	MaskDICELoss 0.0000 (0.4387)
Epoch: [5][464/500]	Time  6.768 ( 6.768)	Loss 1.0933 (1.5994)	CeLoss 0.2178 (0.2079)	SegCLSLoss 0.0104 (0.0144)	KLLoss 0.3652 (0.3285)	MaskLoss 0.4168 (0.6757)	MaskBCELoss 0.1376 (0.0698)	MaskDICELoss 0.2791 (0.6060)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 461 is less than current step: 499. Dropping entry: {'train/loss': 1.6345173597335816, 'train/ce_loss': 0.277734375, 'train/seg_cls_loss': 0.015997314453125, 'train/kl_loss': 0.292578125, 'train/mask_bce_loss': 0.11703000804409384, 'train/mask_dice_loss': 0.5425626501441002, 'train/mask_loss': 0.6595926612615586, 'metrics/total_secs_per_batch': 6.1025378704071045, 'metrics/data_secs_per_batch': 2.25203537940979, '_timestamp': 1740971677.4266357}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 461 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971677.4269211}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 462 is less than current step: 499. Dropping entry: {'train/loss': 1.5210935354232789, 'train/ce_loss': 0.389794921875, 'train/seg_cls_loss': 0.0122802734375, 'train/kl_loss': 0.252734375, 'train/mask_bce_loss': 0.11130186403170228, 'train/mask_dice_loss': 0.4387224346399307, 'train/mask_loss': 0.5500242948532105, 'metrics/total_secs_per_batch': 5.6649885177612305, 'metrics/data_secs_per_batch': 2.4556224822998045, '_timestamp': 1740971683.0918024}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 462 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971683.0921414}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 463 is less than current step: 499. Dropping entry: {'train/loss': 1.5994222283363342, 'train/ce_loss': 0.20791015625, 'train/seg_cls_loss': 0.01436767578125, 'train/kl_loss': 0.328515625, 'train/mask_bce_loss': 0.0697526665404439, 'train/mask_dice_loss': 0.6059838324785233, 'train/mask_loss': 0.6757365018129349, 'metrics/total_secs_per_batch': 6.768130779266357, 'metrics/data_secs_per_batch': 2.955263137817383, '_timestamp': 1740971689.8597975}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 463 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971689.8600788}).
Epoch: [5][465/500]	Time  5.769 ( 5.769)	Loss 1.9511 (1.4156)	CeLoss 0.2314 (0.3661)	SegCLSLoss 0.0132 (0.0110)	KLLoss 0.3633 (0.2197)	MaskLoss 0.8379 (0.5109)	MaskBCELoss 0.0346 (0.0978)	MaskDICELoss 0.8032 (0.4130)
Epoch: [5][466/500]	Time  7.311 ( 7.311)	Loss 0.0732 (1.5774)	CeLoss 0.0732 (0.1983)	SegCLSLoss 0.0000 (0.0156)	KLLoss 0.0000 (0.3266)	MaskLoss 0.0000 (0.6694)	MaskBCELoss 0.0000 (0.1308)	MaskDICELoss 0.0000 (0.5385)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 464 is less than current step: 499. Dropping entry: {'train/loss': 1.4155553817749023, 'train/ce_loss': 0.36611328125, 'train/seg_cls_loss': 0.010968017578125, 'train/kl_loss': 0.2197265625, 'train/mask_bce_loss': 0.09783416986465454, 'train/mask_dice_loss': 0.41301967203617096, 'train/mask_loss': 0.5108538508415222, 'metrics/total_secs_per_batch': 5.769150495529175, 'metrics/data_secs_per_batch': 2.6414971351623535, '_timestamp': 1740971695.629204}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 464 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971695.6296742}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 465 is less than current step: 499. Dropping entry: {'train/loss': 1.5773986339569093, 'train/ce_loss': 0.19833984375, 'train/seg_cls_loss': 0.01556396484375, 'train/kl_loss': 0.3265625, 'train/mask_bce_loss': 0.13082537949085235, 'train/mask_dice_loss': 0.5385380059480667, 'train/mask_loss': 0.6693633794784546, 'metrics/total_secs_per_batch': 7.3113014698028564, 'metrics/data_secs_per_batch': 3.180266571044922, '_timestamp': 1740971702.9402914}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 465 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971702.9406486}).
Epoch: [5][467/500]	Time  5.556 ( 5.556)	Loss 2.1161 (1.6739)	CeLoss 0.1992 (0.5605)	SegCLSLoss 0.0203 (0.0136)	KLLoss 0.3535 (0.2529)	MaskLoss 0.9360 (0.5406)	MaskBCELoss 0.0166 (0.0898)	MaskDICELoss 0.9194 (0.4508)
Epoch: [5][468/500]	Time  6.002 ( 6.002)	Loss 2.0864 (1.6339)	CeLoss 0.3457 (0.3558)	SegCLSLoss 0.0128 (0.0120)	KLLoss 0.3633 (0.2900)	MaskLoss 0.8489 (0.6216)	MaskBCELoss 0.2635 (0.1017)	MaskDICELoss 0.5854 (0.5198)
Epoch: [5][469/500]	Time  6.333 ( 6.333)	Loss 1.7983 (1.5845)	CeLoss 0.2930 (0.3667)	SegCLSLoss 0.0126 (0.0142)	KLLoss 0.3594 (0.2914)	MaskLoss 0.7322 (0.5908)	MaskBCELoss 0.1038 (0.0878)	MaskDICELoss 0.6284 (0.5030)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 466 is less than current step: 499. Dropping entry: {'train/loss': 1.6738715052604676, 'train/ce_loss': 0.560546875, 'train/seg_cls_loss': 0.01358642578125, 'train/kl_loss': 0.2529296875, 'train/mask_bce_loss': 0.0898266328498721, 'train/mask_dice_loss': 0.4508200645446777, 'train/mask_loss': 0.5406466960906983, 'metrics/total_secs_per_batch': 5.555864572525024, 'metrics/data_secs_per_batch': 2.367622399330139, '_timestamp': 1740971708.4961998}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 466 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971708.496424}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 467 is less than current step: 499. Dropping entry: {'train/loss': 1.6338776469230651, 'train/ce_loss': 0.35576171875, 'train/seg_cls_loss': 0.01197509765625, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.10173502787947655, 'train/mask_dice_loss': 0.5198424637317658, 'train/mask_loss': 0.6215774893760682, 'metrics/total_secs_per_batch': 6.002496004104614, 'metrics/data_secs_per_batch': 2.8300026655197144, '_timestamp': 1740971714.498618}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 467 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971714.498925}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 468 is less than current step: 499. Dropping entry: {'train/loss': 1.5845149040222168, 'train/ce_loss': 0.366650390625, 'train/seg_cls_loss': 0.0141845703125, 'train/kl_loss': 0.29140625, 'train/mask_bce_loss': 0.08780889147892594, 'train/mask_dice_loss': 0.5029592886567116, 'train/mask_loss': 0.5907681792974472, 'metrics/total_secs_per_batch': 6.333154916763306, 'metrics/data_secs_per_batch': 2.573688769340515, '_timestamp': 1740971720.8317878}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 468 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971720.832082}).
[2025-03-02 21:15:26,854] [INFO] [logging.py:96:log_dist] [Rank 0] step=2970, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:15:26,860] [INFO] [timer.py:215:stop] epoch=0/micro_step=29700/global_step=2970, RunningAvgSamplesPerSec=1.422014124770622, CurrSamplesPerSec=1.659088772180941, MemAllocated=30.75GB, MaxMemAllocated=37.23GB
Epoch: [5][470/500]	Time  6.029 ( 6.029)	Loss 1.0234 (1.3408)	CeLoss 1.0234 (0.4287)	SegCLSLoss 0.0000 (0.0094)	KLLoss 0.0000 (0.2539)	MaskLoss 0.0000 (0.4410)	MaskBCELoss 0.0000 (0.0671)	MaskDICELoss 0.0000 (0.3738)
Epoch: [5][471/500]	Time  6.260 ( 6.260)	Loss 2.3798 (1.9402)	CeLoss 0.2324 (0.4108)	SegCLSLoss 0.0201 (0.0152)	KLLoss 0.3770 (0.2936)	MaskLoss 1.0502 (0.7462)	MaskBCELoss 0.3519 (0.1130)	MaskDICELoss 0.6983 (0.6332)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 469 is less than current step: 499. Dropping entry: {'train/loss': 1.3408489227294922, 'train/ce_loss': 0.428662109375, 'train/seg_cls_loss': 0.009393310546875, 'train/kl_loss': 0.25390625, 'train/mask_bce_loss': 0.06713483780622483, 'train/mask_dice_loss': 0.37382183969020844, 'train/mask_loss': 0.4409566730260849, 'metrics/total_secs_per_batch': 6.029064178466797, 'metrics/data_secs_per_batch': 2.953059673309326, '_timestamp': 1740971726.8607142}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 469 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971726.8610678}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 470 is less than current step: 499. Dropping entry: {'train/loss': 1.9402397274971008, 'train/ce_loss': 0.41083984375, 'train/seg_cls_loss': 0.015167236328125, 'train/kl_loss': 0.2935546875, 'train/mask_bce_loss': 0.11301108468323946, 'train/mask_dice_loss': 0.6332318365573884, 'train/mask_loss': 0.7462429106235504, 'metrics/total_secs_per_batch': 6.26049017906189, 'metrics/data_secs_per_batch': 2.914706015586853, '_timestamp': 1740971733.1213286}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 470 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971733.121617}).
Epoch: [5][472/500]	Time  4.648 ( 4.648)	Loss 1.5430 (1.3597)	CeLoss 0.2490 (0.6771)	SegCLSLoss 0.0106 (0.0085)	KLLoss 0.3633 (0.2182)	MaskLoss 0.6260 (0.3284)	MaskBCELoss 0.1224 (0.0762)	MaskDICELoss 0.5036 (0.2522)
Epoch: [5][473/500]	Time  7.072 ( 7.072)	Loss 1.7443 (1.6866)	CeLoss 0.2119 (0.3303)	SegCLSLoss 0.0154 (0.0172)	KLLoss 0.3613 (0.3223)	MaskLoss 0.7442 (0.6577)	MaskBCELoss 0.0650 (0.0796)	MaskDICELoss 0.6792 (0.5781)
Epoch: [5][474/500]	Time  5.066 ( 5.066)	Loss 2.2763 (1.5747)	CeLoss 0.2256 (0.4854)	SegCLSLoss 0.0245 (0.0133)	KLLoss 0.3516 (0.2545)	MaskLoss 1.0014 (0.5287)	MaskBCELoss 0.0189 (0.1000)	MaskDICELoss 0.9826 (0.4286)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 471 is less than current step: 499. Dropping entry: {'train/loss': 1.3596760213375092, 'train/ce_loss': 0.67705078125, 'train/seg_cls_loss': 0.00853271484375, 'train/kl_loss': 0.2181640625, 'train/mask_bce_loss': 0.07618308831006289, 'train/mask_dice_loss': 0.25219007693231105, 'train/mask_loss': 0.32837316691875457, 'metrics/total_secs_per_batch': 4.6482813358306885, 'metrics/data_secs_per_batch': 1.8999230146408081, '_timestamp': 1740971737.7696033}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 471 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971737.7699559}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 472 is less than current step: 499. Dropping entry: {'train/loss': 1.6865921378135682, 'train/ce_loss': 0.3302734375, 'train/seg_cls_loss': 0.01724853515625, 'train/kl_loss': 0.322265625, 'train/mask_bce_loss': 0.07956361752003431, 'train/mask_dice_loss': 0.5781367480754852, 'train/mask_loss': 0.6577003538608551, 'metrics/total_secs_per_batch': 7.071986675262451, 'metrics/data_secs_per_batch': 3.00342652797699, '_timestamp': 1740971744.8416302}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 472 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971744.8419328}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 473 is less than current step: 499. Dropping entry: {'train/loss': 1.5747022390365601, 'train/ce_loss': 0.4853515625, 'train/seg_cls_loss': 0.013275146484375, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.1000397864729166, 'train/mask_dice_loss': 0.4286199063062668, 'train/mask_loss': 0.5286597013473511, 'metrics/total_secs_per_batch': 5.065813779830933, 'metrics/data_secs_per_batch': 1.9357155561447144, '_timestamp': 1740971749.9076314}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 473 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971749.9079907}).
Epoch: [5][475/500]	Time  4.687 ( 4.687)	Loss 1.6484 (1.4149)	CeLoss 1.6484 (0.5852)	SegCLSLoss 0.0000 (0.0095)	KLLoss 0.0000 (0.1855)	MaskLoss 0.0000 (0.4032)	MaskBCELoss 0.0000 (0.0692)	MaskDICELoss 0.0000 (0.3340)
Epoch: [5][476/500]	Time  5.649 ( 5.649)	Loss 2.6351 (1.6793)	CeLoss 0.3750 (0.4375)	SegCLSLoss 0.0099 (0.0110)	KLLoss 0.3555 (0.2514)	MaskLoss 1.1095 (0.6056)	MaskBCELoss 0.5006 (0.1338)	MaskDICELoss 0.6090 (0.4718)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 474 is less than current step: 499. Dropping entry: {'train/loss': 1.4148502111434937, 'train/ce_loss': 0.5852294921875, 'train/seg_cls_loss': 0.009478759765625, 'train/kl_loss': 0.185546875, 'train/mask_bce_loss': 0.06923199966549873, 'train/mask_dice_loss': 0.3340060979127884, 'train/mask_loss': 0.4032380998134613, 'metrics/total_secs_per_batch': 4.687171697616577, 'metrics/data_secs_per_batch': 2.2382248401641847, '_timestamp': 1740971754.5945938}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 474 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971754.5948758}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 475 is less than current step: 499. Dropping entry: {'train/loss': 1.6792617797851563, 'train/ce_loss': 0.437548828125, 'train/seg_cls_loss': 0.01104736328125, 'train/kl_loss': 0.2513671875, 'train/mask_bce_loss': 0.13379099806770683, 'train/mask_dice_loss': 0.4717822700738907, 'train/mask_loss': 0.6055732637643814, 'metrics/total_secs_per_batch': 5.6485912799835205, 'metrics/data_secs_per_batch': 2.4531228303909303, '_timestamp': 1740971760.2434113}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 475 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971760.2437644}).
Epoch: [5][477/500]	Time  6.098 ( 6.098)	Loss 1.8496 (1.7970)	CeLoss 0.2441 (0.2819)	SegCLSLoss 0.0129 (0.0181)	KLLoss 0.3613 (0.3285)	MaskLoss 0.7822 (0.7367)	MaskBCELoss 0.2675 (0.1180)	MaskDICELoss 0.5147 (0.6187)
Epoch: [5][478/500]	Time  5.770 ( 5.770)	Loss 2.3310 (1.7260)	CeLoss 0.2559 (0.2958)	SegCLSLoss 0.0243 (0.0171)	KLLoss 0.3555 (0.2881)	MaskLoss 1.0142 (0.6965)	MaskBCELoss 0.0167 (0.0695)	MaskDICELoss 0.9974 (0.6271)
Epoch: [5][479/500]	Time  6.386 ( 6.386)	Loss 1.1962 (1.4349)	CeLoss 0.2002 (0.3448)	SegCLSLoss 0.0164 (0.0120)	KLLoss 0.3691 (0.2924)	MaskLoss 0.4755 (0.5274)	MaskBCELoss 0.0474 (0.0607)	MaskDICELoss 0.4281 (0.4667)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 476 is less than current step: 499. Dropping entry: {'train/loss': 1.7969990253448487, 'train/ce_loss': 0.28193359375, 'train/seg_cls_loss': 0.018084716796875, 'train/kl_loss': 0.328515625, 'train/mask_bce_loss': 0.1180185442790389, 'train/mask_dice_loss': 0.6186645746231079, 'train/mask_loss': 0.7366831183433533, 'metrics/total_secs_per_batch': 6.097990989685059, 'metrics/data_secs_per_batch': 2.8764432191848757, '_timestamp': 1740971766.3411694}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 476 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971766.341447}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 477 is less than current step: 499. Dropping entry: {'train/loss': 1.7259624361991883, 'train/ce_loss': 0.29580078125, 'train/seg_cls_loss': 0.017138671875, 'train/kl_loss': 0.2880859375, 'train/mask_bce_loss': 0.06946509517729282, 'train/mask_dice_loss': 0.6270610332489014, 'train/mask_loss': 0.6965261340141297, 'metrics/total_secs_per_batch': 5.770201683044434, 'metrics/data_secs_per_batch': 2.3964265823364257, '_timestamp': 1740971772.1115773}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 477 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971772.1119342}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 478 is less than current step: 499. Dropping entry: {'train/loss': 1.4348833024501801, 'train/ce_loss': 0.34482421875, 'train/seg_cls_loss': 0.012030029296875, 'train/kl_loss': 0.2923828125, 'train/mask_bce_loss': 0.06065428480505943, 'train/mask_dice_loss': 0.46669947504997256, 'train/mask_loss': 0.52735376060009, 'metrics/total_secs_per_batch': 6.385504722595215, 'metrics/data_secs_per_batch': 2.807816243171692, '_timestamp': 1740971778.4970548}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 478 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971778.497443}).
[2025-03-02 21:16:24,440] [INFO] [logging.py:96:log_dist] [Rank 0] step=2980, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:16:24,445] [INFO] [timer.py:215:stop] epoch=0/micro_step=29800/global_step=2980, RunningAvgSamplesPerSec=1.4228804513279776, CurrSamplesPerSec=1.6813435502760046, MemAllocated=30.69GB, MaxMemAllocated=37.23GB
Epoch: [5][480/500]	Time  5.950 ( 5.950)	Loss 1.3047 (1.6026)	CeLoss 1.3047 (0.4523)	SegCLSLoss 0.0000 (0.0120)	KLLoss 0.0000 (0.2910)	MaskLoss 0.0000 (0.5578)	MaskBCELoss 0.0000 (0.1075)	MaskDICELoss 0.0000 (0.4502)
Epoch: [5][481/500]	Time  5.237 ( 5.237)	Loss 0.8594 (1.8083)	CeLoss 0.8594 (0.4132)	SegCLSLoss 0.0000 (0.0131)	KLLoss 0.0000 (0.2564)	MaskLoss 0.0000 (0.6815)	MaskBCELoss 0.0000 (0.1846)	MaskDICELoss 0.0000 (0.4969)
Epoch: [5][482/500]	Time  5.543 ( 5.543)	Loss 1.0469 (1.4822)	CeLoss 1.0469 (0.3942)	SegCLSLoss 0.0000 (0.0107)	KLLoss 0.0000 (0.2537)	MaskLoss 0.0000 (0.5285)	MaskBCELoss 0.0000 (0.0936)	MaskDICELoss 0.0000 (0.4349)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 479 is less than current step: 499. Dropping entry: {'train/loss': 1.6026171326637269, 'train/ce_loss': 0.45234375, 'train/seg_cls_loss': 0.012017822265625, 'train/kl_loss': 0.291015625, 'train/mask_bce_loss': 0.10751387290656567, 'train/mask_dice_loss': 0.4502400033175945, 'train/mask_loss': 0.5577538728713989, 'metrics/total_secs_per_batch': 5.949625730514526, 'metrics/data_secs_per_batch': 2.714332175254822, '_timestamp': 1740971784.446601}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 479 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971784.4469461}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 480 is less than current step: 499. Dropping entry: {'train/loss': 1.808332097530365, 'train/ce_loss': 0.413232421875, 'train/seg_cls_loss': 0.013079833984375, 'train/kl_loss': 0.2564453125, 'train/mask_bce_loss': 0.18461512140929698, 'train/mask_dice_loss': 0.49687028080224993, 'train/mask_loss': 0.6814853996038437, 'metrics/total_secs_per_batch': 5.23683762550354, 'metrics/data_secs_per_batch': 2.4165271043777468, '_timestamp': 1740971789.6835632}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 480 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971789.6839166}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 481 is less than current step: 499. Dropping entry: {'train/loss': 1.4822144985198975, 'train/ce_loss': 0.39423828125, 'train/seg_cls_loss': 0.010711669921875, 'train/kl_loss': 0.2537109375, 'train/mask_bce_loss': 0.09358270727097988, 'train/mask_dice_loss': 0.4349268853664398, 'train/mask_loss': 0.5285095900297165, 'metrics/total_secs_per_batch': 5.542523145675659, 'metrics/data_secs_per_batch': 2.700536108016968, '_timestamp': 1740971795.225947}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 481 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971795.2263496}).
Epoch: [5][483/500]	Time  4.129 ( 4.129)	Loss 0.6607 (1.2326)	CeLoss 0.2021 (0.6158)	SegCLSLoss 0.0093 (0.0081)	KLLoss 0.3691 (0.2215)	MaskLoss 0.2083 (0.2952)	MaskBCELoss 0.0820 (0.0598)	MaskDICELoss 0.1263 (0.2354)
Epoch: [5][484/500]	Time  6.276 ( 6.276)	Loss 2.6556 (1.5300)	CeLoss 0.1533 (0.3980)	SegCLSLoss 0.0265 (0.0115)	KLLoss 0.3750 (0.2557)	MaskLoss 1.2252 (0.5500)	MaskBCELoss 0.4898 (0.1353)	MaskDICELoss 0.7355 (0.4147)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 482 is less than current step: 499. Dropping entry: {'train/loss': 1.2326155215501786, 'train/ce_loss': 0.6158203125, 'train/seg_cls_loss': 0.008050537109375, 'train/kl_loss': 0.221484375, 'train/mask_bce_loss': 0.05980896800756454, 'train/mask_dice_loss': 0.23540505170822143, 'train/mask_loss': 0.2952140122652054, 'metrics/total_secs_per_batch': 4.12901496887207, 'metrics/data_secs_per_batch': 1.7893247127532959, '_timestamp': 1740971799.3556304}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 482 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971799.356075}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 483 is less than current step: 499. Dropping entry: {'train/loss': 1.5300450205802918, 'train/ce_loss': 0.398046875, 'train/seg_cls_loss': 0.011529541015625, 'train/kl_loss': 0.2556640625, 'train/mask_bce_loss': 0.13531954362988471, 'train/mask_dice_loss': 0.41471273005008696, 'train/mask_loss': 0.5500322699546814, 'metrics/total_secs_per_batch': 6.2755186557769775, 'metrics/data_secs_per_batch': 2.911105155944824, '_timestamp': 1740971805.6306217}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 483 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971805.6310387}).
Epoch: [5][485/500]	Time  7.320 ( 7.320)	Loss 1.2891 (1.2837)	CeLoss 1.2891 (0.3120)	SegCLSLoss 0.0000 (0.0129)	KLLoss 0.0000 (0.2533)	MaskLoss 0.0000 (0.4700)	MaskBCELoss 0.0000 (0.0652)	MaskDICELoss 0.0000 (0.4048)
Epoch: [5][486/500]	Time  6.781 ( 6.781)	Loss 1.6283 (1.9632)	CeLoss 0.2910 (0.2353)	SegCLSLoss 0.0142 (0.0174)	KLLoss 0.3711 (0.3643)	MaskLoss 0.6462 (0.8415)	MaskBCELoss 0.1267 (0.2221)	MaskDICELoss 0.5195 (0.6195)
Epoch: [5][487/500]	Time  5.163 ( 5.163)	Loss 1.3783 (1.4254)	CeLoss 0.2480 (0.6038)	SegCLSLoss 0.0097 (0.0078)	KLLoss 0.3633 (0.1822)	MaskLoss 0.5446 (0.3997)	MaskBCELoss 0.0544 (0.0657)	MaskDICELoss 0.4902 (0.3341)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 484 is less than current step: 499. Dropping entry: {'train/loss': 1.2837296664714812, 'train/ce_loss': 0.311962890625, 'train/seg_cls_loss': 0.01290283203125, 'train/kl_loss': 0.2533203125, 'train/mask_bce_loss': 0.06519684344530105, 'train/mask_dice_loss': 0.40481739640235903, 'train/mask_loss': 0.47001424729824065, 'metrics/total_secs_per_batch': 7.319755554199219, 'metrics/data_secs_per_batch': 3.68748254776001, '_timestamp': 1740971812.9505901}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 484 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971812.9510207}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 485 is less than current step: 499. Dropping entry: {'train/loss': 1.9632495164871215, 'train/ce_loss': 0.23525390625, 'train/seg_cls_loss': 0.017449951171875, 'train/kl_loss': 0.3642578125, 'train/mask_bce_loss': 0.22205465454608203, 'train/mask_dice_loss': 0.6194822058081627, 'train/mask_loss': 0.8415368616580963, 'metrics/total_secs_per_batch': 6.781006336212158, 'metrics/data_secs_per_batch': 3.0067092895507814, '_timestamp': 1740971819.7313306}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 485 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971819.7316902}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 486 is less than current step: 499. Dropping entry: {'train/loss': 1.4254204034805298, 'train/ce_loss': 0.603759765625, 'train/seg_cls_loss': 0.00782470703125, 'train/kl_loss': 0.1822265625, 'train/mask_bce_loss': 0.06569577418267727, 'train/mask_dice_loss': 0.3340505748987198, 'train/mask_loss': 0.39974634647369384, 'metrics/total_secs_per_batch': 5.1626105308532715, 'metrics/data_secs_per_batch': 2.383627510070801, '_timestamp': 1740971824.894182}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 486 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971824.8947027}).
Epoch: [5][488/500]	Time  4.926 ( 4.926)	Loss 0.9805 (1.5763)	CeLoss 0.9805 (0.5719)	SegCLSLoss 0.0000 (0.0097)	KLLoss 0.0000 (0.2193)	MaskLoss 0.0000 (0.4888)	MaskBCELoss 0.0000 (0.0737)	MaskDICELoss 0.0000 (0.4151)
Epoch: [5][489/500]	Time  5.655 ( 5.655)	Loss 0.7305 (1.5097)	CeLoss 0.7305 (0.4146)	SegCLSLoss 0.0000 (0.0090)	KLLoss 0.0000 (0.2176)	MaskLoss 0.0000 (0.5344)	MaskBCELoss 0.0000 (0.1769)	MaskDICELoss 0.0000 (0.3575)
[2025-03-02 21:17:21,602] [INFO] [logging.py:96:log_dist] [Rank 0] step=2990, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:17:21,608] [INFO] [timer.py:215:stop] epoch=0/micro_step=29900/global_step=2990, RunningAvgSamplesPerSec=1.4237709626491994, CurrSamplesPerSec=1.6310519199616758, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [5][490/500]	Time  6.134 ( 6.134)	Loss 2.3816 (1.6325)	CeLoss 0.1982 (0.4072)	SegCLSLoss 0.0149 (0.0120)	KLLoss 0.3711 (0.2562)	MaskLoss 1.0692 (0.5968)	MaskBCELoss 0.2804 (0.1598)	MaskDICELoss 0.7888 (0.4370)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 487 is less than current step: 499. Dropping entry: {'train/loss': 1.5763303279876708, 'train/ce_loss': 0.571875, 'train/seg_cls_loss': 0.009747314453125, 'train/kl_loss': 0.2193359375, 'train/mask_bce_loss': 0.07372545227408409, 'train/mask_dice_loss': 0.4150744616985321, 'train/mask_loss': 0.4887999176979065, 'metrics/total_secs_per_batch': 4.926366329193115, 'metrics/data_secs_per_batch': 2.3917784690856934, '_timestamp': 1740971829.820259}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 487 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971829.8205817}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 488 is less than current step: 499. Dropping entry: {'train/loss': 1.5096715450286866, 'train/ce_loss': 0.4145751953125, 'train/seg_cls_loss': 0.008953857421875, 'train/kl_loss': 0.217578125, 'train/mask_bce_loss': 0.17690306734293698, 'train/mask_dice_loss': 0.35746151506900786, 'train/mask_loss': 0.534364578127861, 'metrics/total_secs_per_batch': 5.65519380569458, 'metrics/data_secs_per_batch': 2.7557739496231077, '_timestamp': 1740971835.4761567}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 488 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971835.476599}).
Epoch: [5][491/500]	Time  5.738 ( 5.738)	Loss 2.1837 (1.6142)	CeLoss 0.2422 (0.3607)	SegCLSLoss 0.0200 (0.0153)	KLLoss 0.3750 (0.3320)	MaskLoss 0.9473 (0.6063)	MaskBCELoss 0.4843 (0.1871)	MaskDICELoss 0.4630 (0.4192)
Epoch: [5][492/500]	Time  6.324 ( 6.324)	Loss 2.3066 (1.3323)	CeLoss 0.1777 (0.3351)	SegCLSLoss 0.0325 (0.0135)	KLLoss 0.3613 (0.2881)	MaskLoss 1.0380 (0.4809)	MaskBCELoss 0.0748 (0.0481)	MaskDICELoss 0.9633 (0.4328)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 489 is less than current step: 499. Dropping entry: {'train/loss': 1.6325209140777588, 'train/ce_loss': 0.4072265625, 'train/seg_cls_loss': 0.0120361328125, 'train/kl_loss': 0.25625, 'train/mask_bce_loss': 0.15981274284422398, 'train/mask_dice_loss': 0.43696531355381013, 'train/mask_loss': 0.596778056025505, 'metrics/total_secs_per_batch': 6.1335883140563965, 'metrics/data_secs_per_batch': 2.5151328086853026, '_timestamp': 1740971841.6087856}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 489 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971841.6089878}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 490 is less than current step: 499. Dropping entry: {'train/loss': 1.6142160534858703, 'train/ce_loss': 0.3607421875, 'train/seg_cls_loss': 0.01527099609375, 'train/kl_loss': 0.33203125, 'train/mask_bce_loss': 0.187087719142437, 'train/mask_dice_loss': 0.41923906952142714, 'train/mask_loss': 0.6063267812132835, 'metrics/total_secs_per_batch': 5.7378058433532715, 'metrics/data_secs_per_batch': 2.7745497941970827, '_timestamp': 1740971847.3469045}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 490 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971847.3472571}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 491 is less than current step: 499. Dropping entry: {'train/loss': 1.3322518527507783, 'train/ce_loss': 0.33505859375, 'train/seg_cls_loss': 0.013458251953125, 'train/kl_loss': 0.2880859375, 'train/mask_bce_loss': 0.048056760570034385, 'train/mask_dice_loss': 0.43281526044011115, 'train/mask_loss': 0.48087202981114385, 'metrics/total_secs_per_batch': 6.324402332305908, 'metrics/data_secs_per_batch': 3.1261226892471314, '_timestamp': 1740971853.6714559}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 491 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971853.6718233}).
Epoch: [5][493/500]	Time  6.230 ( 6.230)	Loss 0.1064 (1.5794)	CeLoss 0.1064 (0.3360)	SegCLSLoss 0.0000 (0.0133)	KLLoss 0.0000 (0.2877)	MaskLoss 0.0000 (0.6038)	MaskBCELoss 0.0000 (0.0971)	MaskDICELoss 0.0000 (0.5067)
Epoch: [5][494/500]	Time  4.789 ( 4.789)	Loss 1.2266 (1.4547)	CeLoss 1.2266 (0.7038)	SegCLSLoss 0.0000 (0.0105)	KLLoss 0.0000 (0.1791)	MaskLoss 0.0000 (0.3638)	MaskBCELoss 0.0000 (0.0585)	MaskDICELoss 0.0000 (0.3054)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 492 is less than current step: 499. Dropping entry: {'train/loss': 1.5794039964675903, 'train/ce_loss': 0.33603515625, 'train/seg_cls_loss': 0.013323974609375, 'train/kl_loss': 0.2876953125, 'train/mask_bce_loss': 0.09711409769952298, 'train/mask_dice_loss': 0.5066992312669754, 'train/mask_loss': 0.6038133323192596, 'metrics/total_secs_per_batch': 6.229722261428833, 'metrics/data_secs_per_batch': 2.793691372871399, '_timestamp': 1740971859.901011}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 492 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971859.9012926}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 493 is less than current step: 499. Dropping entry: {'train/loss': 1.4547198355197906, 'train/ce_loss': 0.70380859375, 'train/seg_cls_loss': 0.010467529296875, 'train/kl_loss': 0.1791015625, 'train/mask_bce_loss': 0.05846004839986563, 'train/mask_dice_loss': 0.30537449568510056, 'train/mask_loss': 0.3638345390558243, 'metrics/total_secs_per_batch': 4.788597583770752, 'metrics/data_secs_per_batch': 2.673218870162964, '_timestamp': 1740971864.689606}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 493 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971864.6899457}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 494 is less than current step: 499. Dropping entry: {'train/loss': 1.48528048992157, 'train/ce_loss': 0.4884765625, 'train/seg_cls_loss': 0.010333251953125, 'train/kl_loss': 0.2154296875, 'train/mask_bce_loss': 0.07450181879103183, 'train/mask_dice_loss': 0.4105700790882111, 'train/mask_loss': 0.48507189750671387, 'metrics/total_secs_per_batch': 5.436148405075073, 'metrics/data_secs_per_batch': 2.5415720701217652, '_timestamp': 1740971870.1260216}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 494 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971870.1265066}).
Epoch: [5][495/500]	Time  5.436 ( 5.436)	Loss 1.2058 (1.4853)	CeLoss 0.2178 (0.4885)	SegCLSLoss 0.0157 (0.0103)	KLLoss 0.3633 (0.2154)	MaskLoss 0.4721 (0.4851)	MaskBCELoss 0.0191 (0.0745)	MaskDICELoss 0.4530 (0.4106)
Epoch: [5][496/500]	Time  5.947 ( 5.947)	Loss 0.0425 (1.4131)	CeLoss 0.0425 (0.6311)	SegCLSLoss 0.0000 (0.0079)	KLLoss 0.0000 (0.2170)	MaskLoss 0.0000 (0.3783)	MaskBCELoss 0.0000 (0.1185)	MaskDICELoss 0.0000 (0.2598)
Epoch: [5][497/500]	Time  5.954 ( 5.954)	Loss 1.7516 (1.9501)	CeLoss 0.3027 (0.4072)	SegCLSLoss 0.0151 (0.0166)	KLLoss 0.3574 (0.2938)	MaskLoss 0.7029 (0.7526)	MaskBCELoss 0.1590 (0.1469)	MaskDICELoss 0.5440 (0.6057)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 495 is less than current step: 499. Dropping entry: {'train/loss': 1.4130926966667174, 'train/ce_loss': 0.631103515625, 'train/seg_cls_loss': 0.0079345703125, 'train/kl_loss': 0.2169921875, 'train/mask_bce_loss': 0.11846928782761097, 'train/mask_dice_loss': 0.25978115797042844, 'train/mask_loss': 0.3782504454255104, 'metrics/total_secs_per_batch': 5.946917533874512, 'metrics/data_secs_per_batch': 2.150301957130432, '_timestamp': 1740971876.0728817}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 495 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971876.073116}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 496 is less than current step: 499. Dropping entry: {'train/loss': 1.9501432418823241, 'train/ce_loss': 0.4072265625, 'train/seg_cls_loss': 0.016619873046875, 'train/kl_loss': 0.29375, 'train/mask_bce_loss': 0.14690006729215382, 'train/mask_dice_loss': 0.6056617796421051, 'train/mask_loss': 0.7525618553161622, 'metrics/total_secs_per_batch': 5.954114675521851, 'metrics/data_secs_per_batch': 2.585088872909546, '_timestamp': 1740971882.0268097}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 496 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971882.0271323}).
Epoch: [5][498/500]	Time  5.635 ( 5.635)	Loss 1.5234 (1.8356)	CeLoss 1.5234 (0.4127)	SegCLSLoss 0.0000 (0.0116)	KLLoss 0.0000 (0.3281)	MaskLoss 0.0000 (0.6918)	MaskBCELoss 0.0000 (0.1369)	MaskDICELoss 0.0000 (0.5549)
Epoch: [5][499/500]	Time  5.652 ( 5.652)	Loss 1.4653 (1.5928)	CeLoss 0.3457 (0.4581)	SegCLSLoss 0.0117 (0.0124)	KLLoss 0.3652 (0.2930)	MaskLoss 0.5383 (0.5494)	MaskBCELoss 0.0649 (0.0511)	MaskDICELoss 0.4734 (0.4984)
[2025-03-02 21:18:19,227] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:18:19,232] [INFO] [timer.py:215:stop] epoch=0/micro_step=30000/global_step=3000, RunningAvgSamplesPerSec=1.4246251860961692, CurrSamplesPerSec=1.6895783172157606, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [5][500/500]	Time  5.920 ( 5.920)	Loss 2.0837 (1.6751)	CeLoss 0.2578 (0.4115)	SegCLSLoss 0.0190 (0.0116)	KLLoss 0.3574 (0.2539)	MaskLoss 0.8905 (0.6161)	MaskBCELoss 0.0048 (0.0838)	MaskDICELoss 0.8856 (0.5324)
  2%|██▎                                                                                                                                                   | 3/200 [00:01<01:19,  2.48it/s][34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 497 is less than current step: 499. Dropping entry: {'train/loss': 1.8356093645095826, 'train/ce_loss': 0.4126953125, 'train/seg_cls_loss': 0.011602783203125, 'train/kl_loss': 0.328125, 'train/mask_bce_loss': 0.1369101579301059, 'train/mask_dice_loss': 0.554917973279953, 'train/mask_loss': 0.6918281331658364, 'metrics/total_secs_per_batch': 5.634538412094116, 'metrics/data_secs_per_batch': 2.625757670402527, '_timestamp': 1740971887.6614642}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 497 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971887.6618314}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 498 is less than current step: 499. Dropping entry: {'train/loss': 1.5928377628326416, 'train/ce_loss': 0.45810546875, 'train/seg_cls_loss': 0.01236572265625, 'train/kl_loss': 0.29296875, 'train/mask_bce_loss': 0.05107519212178886, 'train/mask_dice_loss': 0.49837103188037873, 'train/mask_loss': 0.5494462311267853, 'metrics/total_secs_per_batch': 5.651939153671265, 'metrics/data_secs_per_batch': 2.612579345703125, '_timestamp': 1740971893.313238}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 498 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971893.3136146}).













100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:28<00:00,  6.92it/s]
giou: 0.1879, ciou: 0.1759
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 5 is less than current step: 499. Dropping entry: {'val/giou': 0.1878635585308075, 'val/ciou': 0.17587274312973022, '_timestamp': 1740971928.1297576}).
Epoch: [6][  1/500]	Time  5.274 ( 5.274)	Loss 1.7554 (1.5216)	CeLoss 0.2158 (0.2763)	SegCLSLoss 0.0131 (0.0163)	KLLoss 0.3633 (0.3271)	MaskLoss 0.7478 (0.6020)	MaskBCELoss 0.0641 (0.1045)	MaskDICELoss 0.6837 (0.4975)
Epoch: [6][  2/500]	Time  5.917 ( 5.917)	Loss 0.1934 (1.4387)	CeLoss 0.1934 (0.5209)	SegCLSLoss 0.0000 (0.0086)	KLLoss 0.0000 (0.2164)	MaskLoss 0.0000 (0.4460)	MaskBCELoss 0.0000 (0.1156)	MaskDICELoss 0.0000 (0.3304)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 499. Dropping entry: {'train/loss': 1.5215681433677672, 'train/ce_loss': 0.27626953125, 'train/seg_cls_loss': 0.01634521484375, 'train/kl_loss': 0.3271484375, 'train/mask_bce_loss': 0.10451157502830029, 'train/mask_dice_loss': 0.4975322589278221, 'train/mask_loss': 0.6020438268780708, 'metrics/total_secs_per_batch': 5.273688554763794, 'metrics/data_secs_per_batch': 2.3266977310180663, '_timestamp': 1740971933.4105003}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 1 is less than current step: 499. Dropping entry: {'train/loss': 1.4386950135231018, 'train/ce_loss': 0.5208984375, 'train/seg_cls_loss': 0.008599853515625, 'train/kl_loss': 0.21640625, 'train/mask_bce_loss': 0.11560239950194955, 'train/mask_dice_loss': 0.33040527403354647, 'train/mask_loss': 0.4460076630115509, 'metrics/total_secs_per_batch': 5.916973829269409, 'metrics/data_secs_per_batch': 2.2707130908966064, '_timestamp': 1740971939.3274121}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 1 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971939.3277025}).
Epoch: [6][  3/500]	Time  7.097 ( 7.097)	Loss 2.2842 (1.4645)	CeLoss 0.1216 (0.2460)	SegCLSLoss 0.0325 (0.0117)	KLLoss 0.3691 (0.2547)	MaskLoss 1.0547 (0.5936)	MaskBCELoss 0.1980 (0.1529)	MaskDICELoss 0.8567 (0.4407)
Epoch: [6][  4/500]	Time  5.923 ( 5.923)	Loss 1.3984 (1.5168)	CeLoss 1.3984 (0.3181)	SegCLSLoss 0.0000 (0.0143)	KLLoss 0.0000 (0.2932)	MaskLoss 0.0000 (0.5809)	MaskBCELoss 0.0000 (0.1060)	MaskDICELoss 0.0000 (0.4749)
Epoch: [6][  5/500]	Time  6.288 ( 6.288)	Loss 2.4054 (1.8173)	CeLoss 0.2041 (0.2246)	SegCLSLoss 0.0203 (0.0192)	KLLoss 0.3652 (0.3668)	MaskLoss 1.0777 (0.7733)	MaskBCELoss 0.2726 (0.2017)	MaskDICELoss 0.8051 (0.5715)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 2 is less than current step: 499. Dropping entry: {'train/loss': 1.464481782913208, 'train/ce_loss': 0.24599609375, 'train/seg_cls_loss': 0.01168212890625, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.1529421169310808, 'train/mask_dice_loss': 0.44070015102624893, 'train/mask_loss': 0.5936422616243362, 'metrics/total_secs_per_batch': 7.09689736366272, 'metrics/data_secs_per_batch': 3.3885387420654296, '_timestamp': 1740971946.424406}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 2 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971946.4248238}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 3 is less than current step: 499. Dropping entry: {'train/loss': 1.5167702078819274, 'train/ce_loss': 0.31806640625, 'train/seg_cls_loss': 0.014312744140625, 'train/kl_loss': 0.2931640625, 'train/mask_bce_loss': 0.10603443719446659, 'train/mask_dice_loss': 0.47486044019460677, 'train/mask_loss': 0.580894873291254, 'metrics/total_secs_per_batch': 5.922731876373291, 'metrics/data_secs_per_batch': 2.9110071659088135, '_timestamp': 1740971952.3470724}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 3 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971952.3473828}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 4 is less than current step: 499. Dropping entry: {'train/loss': 1.8173267424106598, 'train/ce_loss': 0.224609375, 'train/seg_cls_loss': 0.01920166015625, 'train/kl_loss': 0.366796875, 'train/mask_bce_loss': 0.20171585716307164, 'train/mask_dice_loss': 0.5715471178293228, 'train/mask_loss': 0.7732629805803299, 'metrics/total_secs_per_batch': 6.2884743213653564, 'metrics/data_secs_per_batch': 2.947472429275513, '_timestamp': 1740971958.635795}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 4 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971958.6361856}).
Epoch: [6][  6/500]	Time  6.114 ( 6.114)	Loss 1.6743 (1.5275)	CeLoss 0.2227 (0.3561)	SegCLSLoss 0.0132 (0.0124)	KLLoss 0.3633 (0.2537)	MaskLoss 0.7043 (0.5699)	MaskBCELoss 0.0644 (0.0843)	MaskDICELoss 0.6399 (0.4856)
Epoch: [6][  7/500]	Time  6.359 ( 6.359)	Loss 2.1876 (1.5937)	CeLoss 0.2520 (0.2938)	SegCLSLoss 0.0131 (0.0116)	KLLoss 0.3711 (0.2922)	MaskLoss 0.9463 (0.6324)	MaskBCELoss 0.2674 (0.1186)	MaskDICELoss 0.6789 (0.5138)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 5 is less than current step: 499. Dropping entry: {'train/loss': 1.5275068998336792, 'train/ce_loss': 0.3560546875, 'train/seg_cls_loss': 0.012359619140625, 'train/kl_loss': 0.2537109375, 'train/mask_bce_loss': 0.08426810316741466, 'train/mask_dice_loss': 0.48558885902166365, 'train/mask_loss': 0.5698569685220718, 'metrics/total_secs_per_batch': 6.113631248474121, 'metrics/data_secs_per_batch': 2.852713632583618, '_timestamp': 1740971964.7492766}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 5 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971964.7496989}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 6 is less than current step: 499. Dropping entry: {'train/loss': 1.5937063097953796, 'train/ce_loss': 0.29375, 'train/seg_cls_loss': 0.01156005859375, 'train/kl_loss': 0.2921875, 'train/mask_bce_loss': 0.11861480120569468, 'train/mask_dice_loss': 0.5137852221727371, 'train/mask_loss': 0.6324000179767608, 'metrics/total_secs_per_batch': 6.35880446434021, 'metrics/data_secs_per_batch': 2.7695971012115477, '_timestamp': 1740971971.1082013}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 6 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971971.108576}).
Epoch: [6][  8/500]	Time  7.158 ( 7.158)	Loss 2.1492 (1.7834)	CeLoss 0.2090 (0.2289)	SegCLSLoss 0.0165 (0.0144)	KLLoss 0.3633 (0.3264)	MaskLoss 0.9476 (0.7573)	MaskBCELoss 0.0126 (0.1228)	MaskDICELoss 0.9351 (0.6345)
Epoch: [6][  9/500]	Time  5.338 ( 5.338)	Loss 1.2188 (1.9709)	CeLoss 1.2188 (0.5915)	SegCLSLoss 0.0000 (0.0108)	KLLoss 0.0000 (0.2529)	MaskLoss 0.0000 (0.6744)	MaskBCELoss 0.0000 (0.2012)	MaskDICELoss 0.0000 (0.4732)
[2025-03-02 21:19:49,320] [INFO] [logging.py:96:log_dist] [Rank 0] step=3010, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:19:49,326] [INFO] [timer.py:215:stop] epoch=0/micro_step=30100/global_step=3010, RunningAvgSamplesPerSec=1.425233832678996, CurrSamplesPerSec=1.7479345944777271, MemAllocated=31.39GB, MaxMemAllocated=37.23GB
Epoch: [6][ 10/500]	Time  5.723 ( 5.723)	Loss 1.6202 (1.7169)	CeLoss 0.2021 (0.4293)	SegCLSLoss 0.0205 (0.0138)	KLLoss 0.3672 (0.2908)	MaskLoss 0.6851 (0.6258)	MaskBCELoss 0.0551 (0.1224)	MaskDICELoss 0.6300 (0.5034)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 7 is less than current step: 499. Dropping entry: {'train/loss': 1.7833815395832062, 'train/ce_loss': 0.22890625, 'train/seg_cls_loss': 0.014434814453125, 'train/kl_loss': 0.3263671875, 'train/mask_bce_loss': 0.12277174284681677, 'train/mask_dice_loss': 0.6344952017068863, 'train/mask_loss': 0.7572669357061386, 'metrics/total_secs_per_batch': 7.157661437988281, 'metrics/data_secs_per_batch': 3.2418790578842165, '_timestamp': 1740971978.2656403}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 7 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971978.265936}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 8 is less than current step: 499. Dropping entry: {'train/loss': 1.9708727478981019, 'train/ce_loss': 0.59150390625, 'train/seg_cls_loss': 0.01083984375, 'train/kl_loss': 0.2529296875, 'train/mask_bce_loss': 0.20116430073976516, 'train/mask_dice_loss': 0.47318809032440184, 'train/mask_loss': 0.6743523895740509, 'metrics/total_secs_per_batch': 5.338284015655518, 'metrics/data_secs_per_batch': 2.5313973426818848, '_timestamp': 1740971983.604043}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 8 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971983.6043754}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 9 is less than current step: 499. Dropping entry: {'train/loss': 1.7168728828430175, 'train/ce_loss': 0.429296875, 'train/seg_cls_loss': 0.013818359375, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.12236816249787807, 'train/mask_dice_loss': 0.5034022390842438, 'train/mask_loss': 0.6257704108953476, 'metrics/total_secs_per_batch': 5.722897291183472, 'metrics/data_secs_per_batch': 2.5967465162277223, '_timestamp': 1740971989.3266883}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 9 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971989.3270228}).
Epoch: [6][ 11/500]	Time  6.052 ( 6.052)	Loss 0.2119 (1.3034)	CeLoss 0.2119 (0.3330)	SegCLSLoss 0.0000 (0.0132)	KLLoss 0.0000 (0.2525)	MaskLoss 0.0000 (0.4694)	MaskBCELoss 0.0000 (0.0393)	MaskDICELoss 0.0000 (0.4300)
Epoch: [6][ 12/500]	Time  6.252 ( 6.252)	Loss 1.7341 (1.4338)	CeLoss 0.1719 (0.3416)	SegCLSLoss 0.0281 (0.0110)	KLLoss 0.3555 (0.2562)	MaskLoss 0.7567 (0.5306)	MaskBCELoss 0.0424 (0.0944)	MaskDICELoss 0.7142 (0.4362)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 10 is less than current step: 499. Dropping entry: {'train/loss': 1.3033536970615387, 'train/ce_loss': 0.3330078125, 'train/seg_cls_loss': 0.01318359375, 'train/kl_loss': 0.2525390625, 'train/mask_bce_loss': 0.03931052228435874, 'train/mask_dice_loss': 0.43004211187362673, 'train/mask_loss': 0.4693526357412338, 'metrics/total_secs_per_batch': 6.051868677139282, 'metrics/data_secs_per_batch': 2.346059203147888, '_timestamp': 1740971995.3793976}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 10 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740971995.379951}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 11 is less than current step: 499. Dropping entry: {'train/loss': 1.4338207602500916, 'train/ce_loss': 0.341552734375, 'train/seg_cls_loss': 0.01097412109375, 'train/kl_loss': 0.25625, 'train/mask_bce_loss': 0.09439777161460369, 'train/mask_dice_loss': 0.436160072684288, 'train/mask_loss': 0.5305578500032425, 'metrics/total_secs_per_batch': 6.252259254455566, 'metrics/data_secs_per_batch': 2.704929494857788, '_timestamp': 1740972001.631142}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 11 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972001.63151}).
Epoch: [6][ 13/500]	Time  7.021 ( 7.021)	Loss 0.0869 (1.7155)	CeLoss 0.0869 (0.1877)	SegCLSLoss 0.0000 (0.0150)	KLLoss 0.0000 (0.2930)	MaskLoss 0.0000 (0.7457)	MaskBCELoss 0.0000 (0.1897)	MaskDICELoss 0.0000 (0.5560)
Epoch: [6][ 14/500]	Time  6.005 ( 6.005)	Loss 0.8079 (1.2409)	CeLoss 0.2471 (0.2981)	SegCLSLoss 0.0101 (0.0135)	KLLoss 0.3613 (0.2891)	MaskLoss 0.2594 (0.4536)	MaskBCELoss 0.0419 (0.0852)	MaskDICELoss 0.2175 (0.3684)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 12 is less than current step: 499. Dropping entry: {'train/loss': 1.7154816389083862, 'train/ce_loss': 0.1876953125, 'train/seg_cls_loss': 0.014971923828125, 'train/kl_loss': 0.29296875, 'train/mask_bce_loss': 0.1896856464445591, 'train/mask_dice_loss': 0.5559946238994599, 'train/mask_loss': 0.7456802695989608, 'metrics/total_secs_per_batch': 7.021219253540039, 'metrics/data_secs_per_batch': 3.40220365524292, '_timestamp': 1740972008.6524074}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 12 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972008.6527746}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 13 is less than current step: 499. Dropping entry: {'train/loss': 1.2408891379833222, 'train/ce_loss': 0.298095703125, 'train/seg_cls_loss': 0.013519287109375, 'train/kl_loss': 0.2890625, 'train/mask_bce_loss': 0.08521345257759094, 'train/mask_dice_loss': 0.36836099848151205, 'train/mask_loss': 0.4535744547843933, 'metrics/total_secs_per_batch': 6.005162715911865, 'metrics/data_secs_per_batch': 2.8266426801681517, '_timestamp': 1740972014.6576967}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 13 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972014.6582363}).
Epoch: [6][ 15/500]	Time  6.417 ( 6.417)	Loss 2.3099 (1.5442)	CeLoss 0.2109 (0.2198)	SegCLSLoss 0.0244 (0.0149)	KLLoss 0.3574 (0.2873)	MaskLoss 1.0260 (0.6442)	MaskBCELoss 0.1304 (0.0778)	MaskDICELoss 0.8956 (0.5664)
Epoch: [6][ 16/500]	Time  5.363 ( 5.363)	Loss 1.2578 (1.6610)	CeLoss 1.2578 (0.5524)	SegCLSLoss 0.0000 (0.0148)	KLLoss 0.0000 (0.2232)	MaskLoss 0.0000 (0.5395)	MaskBCELoss 0.0000 (0.1309)	MaskDICELoss 0.0000 (0.4086)
Epoch: [6][ 17/500]	Time  5.619 ( 5.619)	Loss 1.8732 (1.7849)	CeLoss 0.1904 (0.3060)	SegCLSLoss 0.0249 (0.0166)	KLLoss 0.3633 (0.2936)	MaskLoss 0.8169 (0.7206)	MaskBCELoss 0.0284 (0.1683)	MaskDICELoss 0.7886 (0.5523)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 14 is less than current step: 499. Dropping entry: {'train/loss': 1.5441638708114624, 'train/ce_loss': 0.21982421875, 'train/seg_cls_loss': 0.014886474609375, 'train/kl_loss': 0.2873046875, 'train/mask_bce_loss': 0.07775430791079999, 'train/mask_dice_loss': 0.566446790099144, 'train/mask_loss': 0.6442010998725891, 'metrics/total_secs_per_batch': 6.41712212562561, 'metrics/data_secs_per_batch': 2.9898560285568236, '_timestamp': 1740972021.074667}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 14 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972021.075017}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 15 is less than current step: 499. Dropping entry: {'train/loss': 1.6610209226608277, 'train/ce_loss': 0.55244140625, 'train/seg_cls_loss': 0.014752197265625, 'train/kl_loss': 0.2232421875, 'train/mask_bce_loss': 0.13092029187828302, 'train/mask_dice_loss': 0.4085745453834534, 'train/mask_loss': 0.5394948363304138, 'metrics/total_secs_per_batch': 5.362760782241821, 'metrics/data_secs_per_batch': 2.6607057571411135, '_timestamp': 1740972026.437703}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 15 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972026.4381094}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 16 is less than current step: 499. Dropping entry: {'train/loss': 1.784855741262436, 'train/ce_loss': 0.30595703125, 'train/seg_cls_loss': 0.016619873046875, 'train/kl_loss': 0.2935546875, 'train/mask_bce_loss': 0.16829494526609778, 'train/mask_dice_loss': 0.5523067504167557, 'train/mask_loss': 0.7206016957759858, 'metrics/total_secs_per_batch': 5.619456052780151, 'metrics/data_secs_per_batch': 2.2073472261428835, '_timestamp': 1740972032.0568502}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 16 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972032.057294}).
Epoch: [6][ 18/500]	Time  4.721 ( 4.721)	Loss 1.8912 (1.6277)	CeLoss 0.2285 (0.7114)	SegCLSLoss 0.0179 (0.0082)	KLLoss 0.3555 (0.1820)	MaskLoss 0.8089 (0.4468)	MaskBCELoss 0.0343 (0.1085)	MaskDICELoss 0.7745 (0.3383)
Epoch: [6][ 19/500]	Time  5.411 ( 5.411)	Loss 2.4364 (1.7967)	CeLoss 0.2266 (0.6472)	SegCLSLoss 0.0137 (0.0127)	KLLoss 0.3672 (0.2213)	MaskLoss 1.0834 (0.5605)	MaskBCELoss 0.2408 (0.1504)	MaskDICELoss 0.8426 (0.4101)
[2025-03-02 21:20:48,437] [INFO] [logging.py:96:log_dist] [Rank 0] step=3020, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:20:48,442] [INFO] [timer.py:215:stop] epoch=0/micro_step=30200/global_step=3020, RunningAvgSamplesPerSec=1.4259788729233496, CurrSamplesPerSec=1.599298769376556, MemAllocated=31.1GB, MaxMemAllocated=37.23GB
Epoch: [6][ 20/500]	Time  6.255 ( 6.255)	Loss 2.0222 (1.8279)	CeLoss 0.2891 (0.2712)	SegCLSLoss 0.0166 (0.0148)	KLLoss 0.3633 (0.3307)	MaskLoss 0.8441 (0.7582)	MaskBCELoss 0.0034 (0.1399)	MaskDICELoss 0.8407 (0.6183)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 17 is less than current step: 499. Dropping entry: {'train/loss': 1.6276603817939759, 'train/ce_loss': 0.71142578125, 'train/seg_cls_loss': 0.008245849609375, 'train/kl_loss': 0.18203125, 'train/mask_bce_loss': 0.10852363891899586, 'train/mask_dice_loss': 0.338314363360405, 'train/mask_loss': 0.4468380033969879, 'metrics/total_secs_per_batch': 4.72127628326416, 'metrics/data_secs_per_batch': 2.194723677635193, '_timestamp': 1740972036.77817}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 17 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972036.7785075}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 18 is less than current step: 499. Dropping entry: {'train/loss': 1.7966971158981324, 'train/ce_loss': 0.64716796875, 'train/seg_cls_loss': 0.012652587890625, 'train/kl_loss': 0.2212890625, 'train/mask_bce_loss': 0.15039657279849053, 'train/mask_dice_loss': 0.41011017858982085, 'train/mask_loss': 0.5605067610740662, 'metrics/total_secs_per_batch': 5.410931348800659, 'metrics/data_secs_per_batch': 2.386802339553833, '_timestamp': 1740972042.1891289}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 18 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972042.1894848}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 19 is less than current step: 499. Dropping entry: {'train/loss': 1.8279180645942688, 'train/ce_loss': 0.27119140625, 'train/seg_cls_loss': 0.014752197265625, 'train/kl_loss': 0.3306640625, 'train/mask_bce_loss': 0.13988355428446084, 'train/mask_dice_loss': 0.6183137536048889, 'train/mask_loss': 0.7581973046064376, 'metrics/total_secs_per_batch': 6.254706144332886, 'metrics/data_secs_per_batch': 2.8885221242904664, '_timestamp': 1740972048.4435885}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 19 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972048.4439712}).
Epoch: [6][ 21/500]	Time  6.110 ( 6.110)	Loss 1.2500 (1.3097)	CeLoss 1.2500 (0.4420)	SegCLSLoss 0.0000 (0.0109)	KLLoss 0.0000 (0.2549)	MaskLoss 0.0000 (0.4183)	MaskBCELoss 0.0000 (0.0554)	MaskDICELoss 0.0000 (0.3628)
Epoch: [6][ 22/500]	Time  5.327 ( 5.327)	Loss 1.7642 (1.6453)	CeLoss 0.1699 (0.4094)	SegCLSLoss 0.0228 (0.0127)	KLLoss 0.3477 (0.2926)	MaskLoss 0.7737 (0.6002)	MaskBCELoss 0.1770 (0.1248)	MaskDICELoss 0.5967 (0.4754)
Epoch: [6][ 23/500]	Time  5.936 ( 5.936)	Loss 2.5977 (1.7740)	CeLoss 0.2129 (0.4398)	SegCLSLoss 0.0184 (0.0136)	KLLoss 0.3926 (0.2914)	MaskLoss 1.1680 (0.6492)	MaskBCELoss 0.4423 (0.1275)	MaskDICELoss 0.7256 (0.5217)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 20 is less than current step: 499. Dropping entry: {'train/loss': 1.3097266554832458, 'train/ce_loss': 0.442041015625, 'train/seg_cls_loss': 0.010888671875, 'train/kl_loss': 0.2548828125, 'train/mask_bce_loss': 0.055440942756831646, 'train/mask_dice_loss': 0.36282570362091066, 'train/mask_loss': 0.41826664060354235, 'metrics/total_secs_per_batch': 6.109649419784546, 'metrics/data_secs_per_batch': 2.8064733743667603, '_timestamp': 1740972054.5534453}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 20 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972054.553826}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 21 is less than current step: 499. Dropping entry: {'train/loss': 1.6453249156475067, 'train/ce_loss': 0.409375, 'train/seg_cls_loss': 0.012713623046875, 'train/kl_loss': 0.292578125, 'train/mask_bce_loss': 0.12478740941733121, 'train/mask_dice_loss': 0.47536527514457705, 'train/mask_loss': 0.6001526862382889, 'metrics/total_secs_per_batch': 5.326817989349365, 'metrics/data_secs_per_batch': 2.3104125022888184, '_timestamp': 1740972059.880191}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 21 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972059.8805058}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 22 is less than current step: 499. Dropping entry: {'train/loss': 1.7740464508533478, 'train/ce_loss': 0.43984375, 'train/seg_cls_loss': 0.013629150390625, 'train/kl_loss': 0.29140625, 'train/mask_bce_loss': 0.1274947921279818, 'train/mask_dice_loss': 0.5216866359114647, 'train/mask_loss': 0.6491814285516739, 'metrics/total_secs_per_batch': 5.935931205749512, 'metrics/data_secs_per_batch': 2.8191009759902954, '_timestamp': 1740972065.8167553}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 22 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972065.817251}).
Epoch: [6][ 24/500]	Time  7.201 ( 7.201)	Loss 1.1328 (1.3915)	CeLoss 1.1328 (0.5319)	SegCLSLoss 0.0000 (0.0080)	KLLoss 0.0000 (0.1832)	MaskLoss 0.0000 (0.4187)	MaskBCELoss 0.0000 (0.0985)	MaskDICELoss 0.0000 (0.3202)
Epoch: [6][ 25/500]	Time  6.452 ( 6.452)	Loss 1.5247 (1.6634)	CeLoss 0.2441 (0.2926)	SegCLSLoss 0.0188 (0.0157)	KLLoss 0.3613 (0.3256)	MaskLoss 0.6178 (0.6653)	MaskBCELoss 0.1480 (0.1202)	MaskDICELoss 0.4698 (0.5451)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 23 is less than current step: 499. Dropping entry: {'train/loss': 1.3914739727973937, 'train/ce_loss': 0.53193359375, 'train/seg_cls_loss': 0.008013916015625, 'train/kl_loss': 0.183203125, 'train/mask_bce_loss': 0.09845648482441902, 'train/mask_dice_loss': 0.3202297240495682, 'train/mask_loss': 0.4186862111091614, 'metrics/total_secs_per_batch': 7.201094388961792, 'metrics/data_secs_per_batch': 3.448890280723572, '_timestamp': 1740972073.0172904}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 23 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972073.017624}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 24 is less than current step: 499. Dropping entry: {'train/loss': 1.6633984684944152, 'train/ce_loss': 0.292578125, 'train/seg_cls_loss': 0.015728759765625, 'train/kl_loss': 0.3255859375, 'train/mask_bce_loss': 0.12018304727971554, 'train/mask_dice_loss': 0.5451099097728729, 'train/mask_loss': 0.6652929723262787, 'metrics/total_secs_per_batch': 6.451793193817139, 'metrics/data_secs_per_batch': 2.688548231124878, '_timestamp': 1740972079.4690356}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 24 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972079.4693837}).
Epoch: [6][ 26/500]	Time  7.270 ( 7.270)	Loss 1.7216 (1.4047)	CeLoss 0.2324 (0.2941)	SegCLSLoss 0.0139 (0.0108)	KLLoss 0.3555 (0.2525)	MaskLoss 0.7231 (0.5400)	MaskBCELoss 0.0534 (0.0882)	MaskDICELoss 0.6697 (0.4518)
Epoch: [6][ 27/500]	Time  6.312 ( 6.312)	Loss 2.3603 (1.5467)	CeLoss 0.1992 (0.3362)	SegCLSLoss 0.0204 (0.0139)	KLLoss 0.3789 (0.2555)	MaskLoss 1.0566 (0.5891)	MaskBCELoss 0.2320 (0.0786)	MaskDICELoss 0.8246 (0.5105)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 25 is less than current step: 499. Dropping entry: {'train/loss': 1.404682993888855, 'train/ce_loss': 0.294091796875, 'train/seg_cls_loss': 0.01082763671875, 'train/kl_loss': 0.2525390625, 'train/mask_bce_loss': 0.088193129748106, 'train/mask_dice_loss': 0.4518192678689957, 'train/mask_loss': 0.540012389421463, 'metrics/total_secs_per_batch': 7.27017068862915, 'metrics/data_secs_per_batch': 2.9456738233566284, '_timestamp': 1740972086.7393808}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 25 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972086.739652}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 26 is less than current step: 499. Dropping entry: {'train/loss': 1.546653151512146, 'train/ce_loss': 0.33623046875, 'train/seg_cls_loss': 0.013909912109375, 'train/kl_loss': 0.25546875, 'train/mask_bce_loss': 0.07858665948733687, 'train/mask_dice_loss': 0.5104869872331619, 'train/mask_loss': 0.5890736430883408, 'metrics/total_secs_per_batch': 6.311511516571045, 'metrics/data_secs_per_batch': 2.773022270202637, '_timestamp': 1740972093.050773}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 26 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972093.0511243}).
Epoch: [6][ 28/500]	Time  5.827 ( 5.827)	Loss 2.2654 (1.4851)	CeLoss 0.1191 (0.4361)	SegCLSLoss 0.0303 (0.0127)	KLLoss 0.3809 (0.2551)	MaskLoss 1.0463 (0.5084)	MaskBCELoss 0.2184 (0.0819)	MaskDICELoss 0.8278 (0.4265)
Epoch: [6][ 29/500]	Time  5.271 ( 5.271)	Loss 2.0953 (1.6141)	CeLoss 0.2451 (0.5360)	SegCLSLoss 0.0172 (0.0127)	KLLoss 0.3555 (0.2523)	MaskLoss 0.9031 (0.5232)	MaskBCELoss 0.0459 (0.1236)	MaskDICELoss 0.8572 (0.3996)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 27 is less than current step: 499. Dropping entry: {'train/loss': 1.4851472616195678, 'train/ce_loss': 0.4361328125, 'train/seg_cls_loss': 0.012701416015625, 'train/kl_loss': 0.255078125, 'train/mask_bce_loss': 0.08189562987536192, 'train/mask_dice_loss': 0.4265471324324608, 'train/mask_loss': 0.5084427714347839, 'metrics/total_secs_per_batch': 5.826718807220459, 'metrics/data_secs_per_batch': 2.7650907754898073, '_timestamp': 1740972098.87818}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 27 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972098.8786583}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 28 is less than current step: 499. Dropping entry: {'train/loss': 1.6141257464885712, 'train/ce_loss': 0.53603515625, 'train/seg_cls_loss': 0.012701416015625, 'train/kl_loss': 0.25234375, 'train/mask_bce_loss': 0.12360140029340982, 'train/mask_dice_loss': 0.3995747610926628, 'train/mask_loss': 0.52317616045475, 'metrics/total_secs_per_batch': 5.270675897598267, 'metrics/data_secs_per_batch': 2.437760066986084, '_timestamp': 1740972104.1486986}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 28 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972104.1492362}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 29 is less than current step: 499. Dropping entry: {'train/loss': 1.3935000240802764, 'train/ce_loss': 0.530078125, 'train/seg_cls_loss': 0.00863037109375, 'train/kl_loss': 0.2158203125, 'train/mask_bce_loss': 0.045129529945552346, 'train/mask_dice_loss': 0.3736907869577408, 'train/mask_loss': 0.4188203185796738, 'metrics/total_secs_per_batch': 6.128405809402466, 'metrics/data_secs_per_batch': 2.6632763862609865, '_timestamp': 1740972110.2765856}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 29 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972110.277107}).
[2025-03-02 21:21:50,271] [INFO] [logging.py:96:log_dist] [Rank 0] step=3030, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:21:50,275] [INFO] [timer.py:215:stop] epoch=0/micro_step=30300/global_step=3030, RunningAvgSamplesPerSec=1.4265372385031878, CurrSamplesPerSec=1.632500551133342, MemAllocated=30.73GB, MaxMemAllocated=37.23GB
Epoch: [6][ 30/500]	Time  6.128 ( 6.128)	Loss 1.2734 (1.3935)	CeLoss 1.2734 (0.5301)	SegCLSLoss 0.0000 (0.0086)	KLLoss 0.0000 (0.2158)	MaskLoss 0.0000 (0.4188)	MaskBCELoss 0.0000 (0.0451)	MaskDICELoss 0.0000 (0.3737)
Epoch: [6][ 31/500]	Time  6.145 ( 6.145)	Loss 1.2266 (1.4034)	CeLoss 1.2266 (0.4446)	SegCLSLoss 0.0000 (0.0115)	KLLoss 0.0000 (0.2572)	MaskLoss 0.0000 (0.4635)	MaskBCELoss 0.0000 (0.0845)	MaskDICELoss 0.0000 (0.3790)
Epoch: [6][ 32/500]	Time  7.215 ( 7.215)	Loss 0.9928 (1.4510)	CeLoss 0.2891 (0.3983)	SegCLSLoss 0.0107 (0.0143)	KLLoss 0.3652 (0.2537)	MaskLoss 0.3304 (0.5100)	MaskBCELoss 0.1641 (0.0589)	MaskDICELoss 0.1663 (0.4511)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 30 is less than current step: 499. Dropping entry: {'train/loss': 1.4033564925193787, 'train/ce_loss': 0.444580078125, 'train/seg_cls_loss': 0.011517333984375, 'train/kl_loss': 0.2572265625, 'train/mask_bce_loss': 0.0845004715025425, 'train/mask_dice_loss': 0.37901860028505324, 'train/mask_loss': 0.4635190799832344, 'metrics/total_secs_per_batch': 6.144937753677368, 'metrics/data_secs_per_batch': 2.729979658126831, '_timestamp': 1740972116.4215114}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 30 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972116.421849}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 31 is less than current step: 499. Dropping entry: {'train/loss': 1.4510018527507782, 'train/ce_loss': 0.398291015625, 'train/seg_cls_loss': 0.014312744140625, 'train/kl_loss': 0.2537109375, 'train/mask_bce_loss': 0.05890779346227646, 'train/mask_dice_loss': 0.4511390194296837, 'train/mask_loss': 0.5100468158721924, 'metrics/total_secs_per_batch': 7.2152416706085205, 'metrics/data_secs_per_batch': 3.6567522287368774, '_timestamp': 1740972123.6374907}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 31 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972123.6381235}).
Epoch: [6][ 33/500]	Time  7.152 ( 7.152)	Loss 1.3380 (1.8963)	CeLoss 0.1982 (0.3877)	SegCLSLoss 0.0161 (0.0141)	KLLoss 0.3594 (0.3236)	MaskLoss 0.5479 (0.7345)	MaskBCELoss 0.0184 (0.1505)	MaskDICELoss 0.5295 (0.5840)
Epoch: [6][ 34/500]	Time  7.758 ( 7.758)	Loss 0.0659 (1.6097)	CeLoss 0.0659 (0.3063)	SegCLSLoss 0.0000 (0.0167)	KLLoss 0.0000 (0.2887)	MaskLoss 0.0000 (0.6332)	MaskBCELoss 0.0000 (0.0757)	MaskDICELoss 0.0000 (0.5574)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 32 is less than current step: 499. Dropping entry: {'train/loss': 1.8962875843048095, 'train/ce_loss': 0.3876953125, 'train/seg_cls_loss': 0.01412353515625, 'train/kl_loss': 0.3236328125, 'train/mask_bce_loss': 0.15050327046774328, 'train/mask_dice_loss': 0.5840174555778503, 'train/mask_loss': 0.7345207393169403, 'metrics/total_secs_per_batch': 7.152304172515869, 'metrics/data_secs_per_batch': 2.9028417825698853, '_timestamp': 1740972130.789256}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 32 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972130.789698}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 33 is less than current step: 499. Dropping entry: {'train/loss': 1.6097212314605713, 'train/ce_loss': 0.30625, 'train/seg_cls_loss': 0.0167236328125, 'train/kl_loss': 0.288671875, 'train/mask_bce_loss': 0.07573782429099082, 'train/mask_dice_loss': 0.5574186876416206, 'train/mask_loss': 0.6331565141677856, 'metrics/total_secs_per_batch': 7.7583723068237305, 'metrics/data_secs_per_batch': 3.185683751106262, '_timestamp': 1740972138.5483418}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 33 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972138.548893}).
Epoch: [6][ 35/500]	Time  6.280 ( 6.280)	Loss 2.5262 (1.8734)	CeLoss 0.1494 (0.3134)	SegCLSLoss 0.0240 (0.0134)	KLLoss 0.3789 (0.2584)	MaskLoss 1.1635 (0.7638)	MaskBCELoss 0.3664 (0.1860)	MaskDICELoss 0.7971 (0.5779)
Epoch: [6][ 36/500]	Time  6.276 ( 6.276)	Loss 2.2588 (1.9308)	CeLoss 0.1924 (0.3030)	SegCLSLoss 0.0199 (0.0163)	KLLoss 0.3574 (0.3248)	MaskLoss 1.0102 (0.7935)	MaskBCELoss 0.0244 (0.1605)	MaskDICELoss 0.9858 (0.6330)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 34 is less than current step: 499. Dropping entry: {'train/loss': 1.8733527183532714, 'train/ce_loss': 0.31337890625, 'train/seg_cls_loss': 0.013421630859375, 'train/kl_loss': 0.2583984375, 'train/mask_bce_loss': 0.18595332372933626, 'train/mask_dice_loss': 0.577871459722519, 'train/mask_loss': 0.7638247966766357, 'metrics/total_secs_per_batch': 6.2802910804748535, 'metrics/data_secs_per_batch': 2.372467303276062, '_timestamp': 1740972144.8278553}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 34 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972144.8283267}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 35 is less than current step: 499. Dropping entry: {'train/loss': 1.930842661857605, 'train/ce_loss': 0.30302734375, 'train/seg_cls_loss': 0.016259765625, 'train/kl_loss': 0.3248046875, 'train/mask_bce_loss': 0.16045039696618915, 'train/mask_dice_loss': 0.6330471061170101, 'train/mask_loss': 0.7934975013136863, 'metrics/total_secs_per_batch': 6.276059865951538, 'metrics/data_secs_per_batch': 2.951241207122803, '_timestamp': 1740972151.1040583}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 35 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972151.1046023}).
Epoch: [6][ 37/500]	Time  7.345 ( 7.345)	Loss 1.4688 (1.8452)	CeLoss 1.4688 (0.3608)	SegCLSLoss 0.0000 (0.0175)	KLLoss 0.0000 (0.3266)	MaskLoss 0.0000 (0.7213)	MaskBCELoss 0.0000 (0.1364)	MaskDICELoss 0.0000 (0.5850)
Epoch: [6][ 38/500]	Time  5.601 ( 5.601)	Loss 0.7891 (1.3361)	CeLoss 0.7891 (0.5502)	SegCLSLoss 0.0000 (0.0078)	KLLoss 0.0000 (0.1809)	MaskLoss 0.0000 (0.3820)	MaskBCELoss 0.0000 (0.0618)	MaskDICELoss 0.0000 (0.3202)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 36 is less than current step: 499. Dropping entry: {'train/loss': 1.8452157735824586, 'train/ce_loss': 0.36083984375, 'train/seg_cls_loss': 0.017498779296875, 'train/kl_loss': 0.3265625, 'train/mask_bce_loss': 0.1363506879657507, 'train/mask_dice_loss': 0.584987661242485, 'train/mask_loss': 0.7213383555412293, 'metrics/total_secs_per_batch': 7.344944477081299, 'metrics/data_secs_per_batch': 3.483202576637268, '_timestamp': 1740972158.449359}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 36 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972158.449869}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 37 is less than current step: 499. Dropping entry: {'train/loss': 1.3361443400382995, 'train/ce_loss': 0.550244140625, 'train/seg_cls_loss': 0.007769775390625, 'train/kl_loss': 0.180859375, 'train/mask_bce_loss': 0.06178294774144888, 'train/mask_dice_loss': 0.3201808273792267, 'train/mask_loss': 0.38196377754211425, 'metrics/total_secs_per_batch': 5.6007983684539795, 'metrics/data_secs_per_batch': 2.8231467723846437, '_timestamp': 1740972164.0502431}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 37 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972164.050757}).
Epoch: [6][ 39/500]	Time  7.014 ( 7.014)	Loss 2.4304 (1.4506)	CeLoss 0.1562 (0.2622)	SegCLSLoss 0.0236 (0.0125)	KLLoss 0.3633 (0.2545)	MaskLoss 1.1131 (0.5783)	MaskBCELoss 0.2854 (0.0837)	MaskDICELoss 0.8278 (0.4945)
[2025-03-02 21:22:56,657] [INFO] [logging.py:96:log_dist] [Rank 0] step=3040, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:22:56,662] [INFO] [timer.py:215:stop] epoch=0/micro_step=30400/global_step=3040, RunningAvgSamplesPerSec=1.426827279968565, CurrSamplesPerSec=1.786757322053637, MemAllocated=30.7GB, MaxMemAllocated=37.23GB
Epoch: [6][ 40/500]	Time  5.599 ( 5.599)	Loss 1.2734 (1.8083)	CeLoss 1.2734 (0.5196)	SegCLSLoss 0.0000 (0.0099)	KLLoss 0.0000 (0.2543)	MaskLoss 0.0000 (0.6289)	MaskBCELoss 0.0000 (0.1806)	MaskDICELoss 0.0000 (0.4483)
Epoch: [6][ 41/500]	Time  5.085 ( 5.085)	Loss 2.1990 (1.7250)	CeLoss 0.2734 (0.7294)	SegCLSLoss 0.0376 (0.0124)	KLLoss 0.3613 (0.2172)	MaskLoss 0.9354 (0.4839)	MaskBCELoss 0.0249 (0.0593)	MaskDICELoss 0.9105 (0.4247)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 38 is less than current step: 499. Dropping entry: {'train/loss': 1.450594460964203, 'train/ce_loss': 0.2622314453125, 'train/seg_cls_loss': 0.01253662109375, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.0837498165667057, 'train/mask_dice_loss': 0.49451372027397156, 'train/mask_loss': 0.5782635390758515, 'metrics/total_secs_per_batch': 7.014355659484863, 'metrics/data_secs_per_batch': 3.116465759277344, '_timestamp': 1740972171.0640337}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 38 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972171.0645392}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 39 is less than current step: 499. Dropping entry: {'train/loss': 1.8082724690437317, 'train/ce_loss': 0.51962890625, 'train/seg_cls_loss': 0.009881591796875, 'train/kl_loss': 0.254296875, 'train/mask_bce_loss': 0.1805783823132515, 'train/mask_dice_loss': 0.4483137220144272, 'train/mask_loss': 0.6288920938968658, 'metrics/total_secs_per_batch': 5.5991411209106445, 'metrics/data_secs_per_batch': 2.835331583023071, '_timestamp': 1740972176.6629455}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 39 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972176.6633222}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 40 is less than current step: 499. Dropping entry: {'train/loss': 1.7250094175338746, 'train/ce_loss': 0.72939453125, 'train/seg_cls_loss': 0.012445068359375, 'train/kl_loss': 0.2171875, 'train/mask_bce_loss': 0.05928572751581669, 'train/mask_dice_loss': 0.42465455532073976, 'train/mask_loss': 0.48394027501344683, 'metrics/total_secs_per_batch': 5.085040330886841, 'metrics/data_secs_per_batch': 2.539805698394775, '_timestamp': 1740972181.7486324}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 40 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972181.7493188}).
Epoch: [6][ 42/500]	Time  6.735 ( 6.735)	Loss 1.6807 (1.7199)	CeLoss 0.2295 (0.2027)	SegCLSLoss 0.0116 (0.0144)	KLLoss 0.3594 (0.2883)	MaskLoss 0.7046 (0.7406)	MaskBCELoss 0.1285 (0.2571)	MaskDICELoss 0.5761 (0.4835)
Epoch: [6][ 43/500]	Time  6.074 ( 6.074)	Loss 1.2556 (1.5465)	CeLoss 0.2412 (0.3369)	SegCLSLoss 0.0125 (0.0149)	KLLoss 0.3594 (0.3270)	MaskLoss 0.4862 (0.5848)	MaskBCELoss 0.0561 (0.0924)	MaskDICELoss 0.4301 (0.4924)
Epoch: [6][ 44/500]	Time  5.598 ( 5.598)	Loss 2.3170 (1.3622)	CeLoss 0.2393 (0.6360)	SegCLSLoss 0.0161 (0.0084)	KLLoss 0.3809 (0.2197)	MaskLoss 1.0159 (0.3499)	MaskBCELoss 0.2237 (0.0502)	MaskDICELoss 0.7922 (0.2998)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 41 is less than current step: 499. Dropping entry: {'train/loss': 1.7199336171150208, 'train/ce_loss': 0.202685546875, 'train/seg_cls_loss': 0.014385986328125, 'train/kl_loss': 0.28828125, 'train/mask_bce_loss': 0.2571019187569618, 'train/mask_dice_loss': 0.48350453972816465, 'train/mask_loss': 0.7406064569950104, 'metrics/total_secs_per_batch': 6.735060453414917, 'metrics/data_secs_per_batch': 3.0613582134246826, '_timestamp': 1740972188.4832587}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 41 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972188.4835975}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 42 is less than current step: 499. Dropping entry: {'train/loss': 1.5464581966400146, 'train/ce_loss': 0.336865234375, 'train/seg_cls_loss': 0.01492919921875, 'train/kl_loss': 0.326953125, 'train/mask_bce_loss': 0.09235292002558708, 'train/mask_dice_loss': 0.4924484327435493, 'train/mask_loss': 0.5848013490438462, 'metrics/total_secs_per_batch': 6.074179649353027, 'metrics/data_secs_per_batch': 2.627402400970459, '_timestamp': 1740972194.5586863}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 42 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972194.5591545}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 43 is less than current step: 499. Dropping entry: {'train/loss': 1.3621571779251098, 'train/ce_loss': 0.63603515625, 'train/seg_cls_loss': 0.008416748046875, 'train/kl_loss': 0.2197265625, 'train/mask_bce_loss': 0.050173313729465006, 'train/mask_dice_loss': 0.29975293576717377, 'train/mask_loss': 0.3499262511730194, 'metrics/total_secs_per_batch': 5.597668647766113, 'metrics/data_secs_per_batch': 2.472094416618347, '_timestamp': 1740972200.1553729}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 43 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972200.1558566}).
Epoch: [6][ 45/500]	Time  7.284 ( 7.284)	Loss 2.2670 (1.6562)	CeLoss 0.2100 (0.3439)	SegCLSLoss 0.0186 (0.0129)	KLLoss 0.3594 (0.2529)	MaskLoss 1.0065 (0.6405)	MaskBCELoss 0.0125 (0.0991)	MaskDICELoss 0.9941 (0.5414)
Epoch: [6][ 46/500]	Time  6.185 ( 6.185)	Loss 1.0547 (1.2933)	CeLoss 1.0547 (0.4146)	SegCLSLoss 0.0000 (0.0095)	KLLoss 0.0000 (0.1805)	MaskLoss 0.0000 (0.4280)	MaskBCELoss 0.0000 (0.0768)	MaskDICELoss 0.0000 (0.3512)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 44 is less than current step: 499. Dropping entry: {'train/loss': 1.6562063097953796, 'train/ce_loss': 0.3439453125, 'train/seg_cls_loss': 0.01285400390625, 'train/kl_loss': 0.2529296875, 'train/mask_bce_loss': 0.09910016665235162, 'train/mask_dice_loss': 0.541405326128006, 'train/mask_loss': 0.6405054986476898, 'metrics/total_secs_per_batch': 7.284345865249634, 'metrics/data_secs_per_batch': 3.1683738231658936, '_timestamp': 1740972207.4394174}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 44 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972207.4397302}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 45 is less than current step: 499. Dropping entry: {'train/loss': 1.293329393863678, 'train/ce_loss': 0.414599609375, 'train/seg_cls_loss': 0.00946044921875, 'train/kl_loss': 0.18046875, 'train/mask_bce_loss': 0.07679444812238216, 'train/mask_dice_loss': 0.3511934965848923, 'train/mask_loss': 0.42798793911933897, 'metrics/total_secs_per_batch': 6.185246229171753, 'metrics/data_secs_per_batch': 2.8624619007110597, '_timestamp': 1740972213.6247673}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 45 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972213.6251183}).
Epoch: [6][ 47/500]	Time  6.766 ( 6.766)	Loss 0.6745 (1.5222)	CeLoss 0.2422 (0.5627)	SegCLSLoss 0.0094 (0.0101)	KLLoss 0.3633 (0.2539)	MaskLoss 0.1956 (0.4645)	MaskBCELoss 0.0803 (0.1177)	MaskDICELoss 0.1154 (0.3468)
Epoch: [6][ 48/500]	Time  6.835 ( 6.835)	Loss 2.5847 (1.7465)	CeLoss 0.1504 (0.2327)	SegCLSLoss 0.0187 (0.0161)	KLLoss 0.3770 (0.3656)	MaskLoss 1.1937 (0.7345)	MaskBCELoss 0.4681 (0.1605)	MaskDICELoss 0.7256 (0.5740)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 46 is less than current step: 499. Dropping entry: {'train/loss': 1.5221965193748475, 'train/ce_loss': 0.5626953125, 'train/seg_cls_loss': 0.010076904296875, 'train/kl_loss': 0.25390625, 'train/mask_bce_loss': 0.11767025627195835, 'train/mask_dice_loss': 0.34679714366793635, 'train/mask_loss': 0.46446740031242373, 'metrics/total_secs_per_batch': 6.765822649002075, 'metrics/data_secs_per_batch': 2.7679279088974, '_timestamp': 1740972220.3906212}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 46 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972220.3910284}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 47 is less than current step: 499. Dropping entry: {'train/loss': 1.7465278923511505, 'train/ce_loss': 0.23271484375, 'train/seg_cls_loss': 0.01605224609375, 'train/kl_loss': 0.365625, 'train/mask_bce_loss': 0.16054383013397455, 'train/mask_dice_loss': 0.5739994168281555, 'train/mask_loss': 0.7345432430505753, 'metrics/total_secs_per_batch': 6.835202217102051, 'metrics/data_secs_per_batch': 3.1712443113327025, '_timestamp': 1740972227.225874}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 47 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972227.2263756}).
Epoch: [6][ 49/500]	Time  6.250 ( 6.250)	Loss 0.8867 (1.6147)	CeLoss 0.8867 (0.4921)	SegCLSLoss 0.0000 (0.0115)	KLLoss 0.0000 (0.2557)	MaskLoss 0.0000 (0.5457)	MaskBCELoss 0.0000 (0.1413)	MaskDICELoss 0.0000 (0.4044)
[2025-03-02 21:24:01,282] [INFO] [logging.py:96:log_dist] [Rank 0] step=3050, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:24:01,288] [INFO] [timer.py:215:stop] epoch=0/micro_step=30500/global_step=3050, RunningAvgSamplesPerSec=1.4271932422564457, CurrSamplesPerSec=1.2801443522150096, MemAllocated=31.57GB, MaxMemAllocated=37.23GB
Epoch: [6][ 50/500]	Time  7.813 ( 7.813)	Loss 0.1055 (1.7433)	CeLoss 0.1055 (0.3036)	SegCLSLoss 0.0000 (0.0126)	KLLoss 0.0000 (0.2576)	MaskLoss 0.0000 (0.7040)	MaskBCELoss 0.0000 (0.1993)	MaskDICELoss 0.0000 (0.5046)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 48 is less than current step: 499. Dropping entry: {'train/loss': 1.6147304594516754, 'train/ce_loss': 0.49208984375, 'train/seg_cls_loss': 0.0114501953125, 'train/kl_loss': 0.2556640625, 'train/mask_bce_loss': 0.14131690450012685, 'train/mask_dice_loss': 0.4044272184371948, 'train/mask_loss': 0.5457441329956054, 'metrics/total_secs_per_batch': 6.249691486358643, 'metrics/data_secs_per_batch': 3.0358585119247437, '_timestamp': 1740972233.4753616}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 48 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972233.4756644}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 49 is less than current step: 499. Dropping entry: {'train/loss': 1.7432767152786255, 'train/ce_loss': 0.30361328125, 'train/seg_cls_loss': 0.012603759765625, 'train/kl_loss': 0.2576171875, 'train/mask_bce_loss': 0.19932954832911493, 'train/mask_dice_loss': 0.5046330213546752, 'train/mask_loss': 0.7039625763893127, 'metrics/total_secs_per_batch': 7.813485145568848, 'metrics/data_secs_per_batch': 3.7281485319137575, '_timestamp': 1740972241.288675}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 49 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972241.2890356}).
Epoch: [6][ 51/500]	Time  7.052 ( 7.052)	Loss 0.1182 (1.7396)	CeLoss 0.1182 (0.2168)	SegCLSLoss 0.0000 (0.0146)	KLLoss 0.0000 (0.3236)	MaskLoss 0.0000 (0.7417)	MaskBCELoss 0.0000 (0.0950)	MaskDICELoss 0.0000 (0.6467)
Epoch: [6][ 52/500]	Time  4.816 ( 4.816)	Loss 1.4297 (1.7954)	CeLoss 1.4297 (0.7845)	SegCLSLoss 0.0000 (0.0109)	KLLoss 0.0000 (0.1834)	MaskLoss 0.0000 (0.4935)	MaskBCELoss 0.0000 (0.0850)	MaskDICELoss 0.0000 (0.4085)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 50 is less than current step: 499. Dropping entry: {'train/loss': 1.7395657896995544, 'train/ce_loss': 0.216796875, 'train/seg_cls_loss': 0.01458740234375, 'train/kl_loss': 0.3236328125, 'train/mask_bce_loss': 0.09498035851866007, 'train/mask_dice_loss': 0.6466775417327881, 'train/mask_loss': 0.741657891869545, 'metrics/total_secs_per_batch': 7.052052736282349, 'metrics/data_secs_per_batch': 3.1163002490997314, '_timestamp': 1740972248.340824}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 50 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972248.3410144}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 51 is less than current step: 499. Dropping entry: {'train/loss': 1.79536372423172, 'train/ce_loss': 0.78447265625, 'train/seg_cls_loss': 0.0109130859375, 'train/kl_loss': 0.1833984375, 'train/mask_bce_loss': 0.0849626086652279, 'train/mask_dice_loss': 0.40852004289627075, 'train/mask_loss': 0.4934826552867889, 'metrics/total_secs_per_batch': 4.816330909729004, 'metrics/data_secs_per_batch': 2.421676230430603, '_timestamp': 1740972253.157359}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 51 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972253.157703}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 52 is less than current step: 499. Dropping entry: {'train/loss': 1.743697029352188, 'train/ce_loss': 0.3125, 'train/seg_cls_loss': 0.01514892578125, 'train/kl_loss': 0.3263671875, 'train/mask_bce_loss': 0.12155288457870483, 'train/mask_dice_loss': 0.5739772632718086, 'train/mask_loss': 0.6955301433801651, 'metrics/total_secs_per_batch': 7.564588308334351, 'metrics/data_secs_per_batch': 3.2639287948608398, '_timestamp': 1740972260.7219849}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 52 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972260.7223642}).
Epoch: [6][ 53/500]	Time  7.565 ( 7.565)	Loss 1.9838 (1.7437)	CeLoss 0.2305 (0.3125)	SegCLSLoss 0.0227 (0.0151)	KLLoss 0.3711 (0.3264)	MaskLoss 0.8522 (0.6955)	MaskBCELoss 0.2050 (0.1216)	MaskDICELoss 0.6472 (0.5740)
Epoch: [6][ 54/500]	Time  5.672 ( 5.672)	Loss 1.6448 (1.4859)	CeLoss 0.2422 (0.5067)	SegCLSLoss 0.0098 (0.0094)	KLLoss 0.3652 (0.2541)	MaskLoss 0.6808 (0.4746)	MaskBCELoss 0.1321 (0.1229)	MaskDICELoss 0.5487 (0.3516)
Epoch: [6][ 55/500]	Time  6.298 ( 6.298)	Loss 2.4565 (1.8235)	CeLoss 0.1738 (0.3828)	SegCLSLoss 0.0281 (0.0152)	KLLoss 0.3750 (0.2926)	MaskLoss 1.1159 (0.7019)	MaskBCELoss 0.2029 (0.1616)	MaskDICELoss 0.9131 (0.5403)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 53 is less than current step: 499. Dropping entry: {'train/loss': 1.485908579826355, 'train/ce_loss': 0.50673828125, 'train/seg_cls_loss': 0.009423828125, 'train/kl_loss': 0.2541015625, 'train/mask_bce_loss': 0.12294547185301781, 'train/mask_dice_loss': 0.3516494452953339, 'train/mask_loss': 0.4745949149131775, 'metrics/total_secs_per_batch': 5.672030448913574, 'metrics/data_secs_per_batch': 2.575073409080505, '_timestamp': 1740972266.3938231}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 53 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972266.394129}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 54 is less than current step: 499. Dropping entry: {'train/loss': 1.8235007405281067, 'train/ce_loss': 0.3828125, 'train/seg_cls_loss': 0.01519775390625, 'train/kl_loss': 0.292578125, 'train/mask_bce_loss': 0.16160888420417904, 'train/mask_dice_loss': 0.5403270363807678, 'train/mask_loss': 0.7019359201192856, 'metrics/total_secs_per_batch': 6.297805070877075, 'metrics/data_secs_per_batch': 3.2380940437316896, '_timestamp': 1740972272.6920507}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 54 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972272.6925192}).
Epoch: [6][ 56/500]	Time  6.493 ( 6.493)	Loss 1.5046 (1.5283)	CeLoss 0.3066 (0.3449)	SegCLSLoss 0.0105 (0.0167)	KLLoss 0.3691 (0.3291)	MaskLoss 0.5775 (0.5710)	MaskBCELoss 0.1561 (0.0620)	MaskDICELoss 0.4214 (0.5090)
Epoch: [6][ 57/500]	Time  5.262 ( 5.262)	Loss 1.5140 (1.6977)	CeLoss 0.2695 (0.6388)	SegCLSLoss 0.0256 (0.0114)	KLLoss 0.3594 (0.2174)	MaskLoss 0.5978 (0.5157)	MaskBCELoss 0.3387 (0.1010)	MaskDICELoss 0.2591 (0.4147)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 55 is less than current step: 499. Dropping entry: {'train/loss': 1.528269237279892, 'train/ce_loss': 0.344921875, 'train/seg_cls_loss': 0.0167236328125, 'train/kl_loss': 0.3291015625, 'train/mask_bce_loss': 0.06201987247914076, 'train/mask_dice_loss': 0.5089995056390763, 'train/mask_loss': 0.5710193797945976, 'metrics/total_secs_per_batch': 6.492840528488159, 'metrics/data_secs_per_batch': 2.94449622631073, '_timestamp': 1740972279.1845558}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 55 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972279.1849325}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 56 is less than current step: 499. Dropping entry: {'train/loss': 1.6976844906806945, 'train/ce_loss': 0.63876953125, 'train/seg_cls_loss': 0.011358642578125, 'train/kl_loss': 0.2173828125, 'train/mask_bce_loss': 0.10099188722670079, 'train/mask_dice_loss': 0.41474488377571106, 'train/mask_loss': 0.5157367765903473, 'metrics/total_secs_per_batch': 5.261815309524536, 'metrics/data_secs_per_batch': 2.2748981952667235, '_timestamp': 1740972284.447792}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 56 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972284.4483907}).
Epoch: [6][ 58/500]	Time  6.855 ( 6.855)	Loss 2.4444 (1.5445)	CeLoss 0.1807 (0.2147)	SegCLSLoss 0.0106 (0.0132)	KLLoss 0.3613 (0.3271)	MaskLoss 1.1113 (0.6453)	MaskBCELoss 0.5125 (0.2012)	MaskDICELoss 0.5988 (0.4441)
Epoch: [6][ 59/500]	Time  6.260 ( 6.260)	Loss 1.6875 (1.8026)	CeLoss 1.6875 (0.3888)	SegCLSLoss 0.0000 (0.0134)	KLLoss 0.0000 (0.2529)	MaskLoss 0.0000 (0.6909)	MaskBCELoss 0.0000 (0.1944)	MaskDICELoss 0.0000 (0.4965)
[2025-03-02 21:25:02,876] [INFO] [logging.py:96:log_dist] [Rank 0] step=3060, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:25:02,881] [INFO] [timer.py:215:stop] epoch=0/micro_step=30600/global_step=3060, RunningAvgSamplesPerSec=1.4277590000422722, CurrSamplesPerSec=1.8800667971501617, MemAllocated=31.56GB, MaxMemAllocated=37.23GB
Epoch: [6][ 60/500]	Time  5.321 ( 5.321)	Loss 1.5979 (1.2380)	CeLoss 0.2412 (0.4183)	SegCLSLoss 0.0165 (0.0086)	KLLoss 0.3672 (0.2188)	MaskLoss 0.6564 (0.3968)	MaskBCELoss 0.0310 (0.0897)	MaskDICELoss 0.6253 (0.3071)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 57 is less than current step: 499. Dropping entry: {'train/loss': 1.5444569826126098, 'train/ce_loss': 0.21474609375, 'train/seg_cls_loss': 0.013232421875, 'train/kl_loss': 0.3271484375, 'train/mask_bce_loss': 0.2011747259646654, 'train/mask_dice_loss': 0.44410063326358795, 'train/mask_loss': 0.6452753663063049, 'metrics/total_secs_per_batch': 6.8552937507629395, 'metrics/data_secs_per_batch': 3.0695327520370483, '_timestamp': 1740972291.30159}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 57 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972291.301872}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 58 is less than current step: 499. Dropping entry: {'train/loss': 1.8026133060455323, 'train/ce_loss': 0.38876953125, 'train/seg_cls_loss': 0.013409423828125, 'train/kl_loss': 0.2529296875, 'train/mask_bce_loss': 0.19435974447987975, 'train/mask_dice_loss': 0.4965465247631073, 'train/mask_loss': 0.6909062683582305, 'metrics/total_secs_per_batch': 6.259848594665527, 'metrics/data_secs_per_batch': 2.907150483131409, '_timestamp': 1740972297.561628}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 58 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972297.5619977}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 59 is less than current step: 499. Dropping entry: {'train/loss': 1.2380354523658752, 'train/ce_loss': 0.41826171875, 'train/seg_cls_loss': 0.008636474609375, 'train/kl_loss': 0.21875, 'train/mask_bce_loss': 0.08965144148096442, 'train/mask_dice_loss': 0.3071006625890732, 'train/mask_loss': 0.3967520982027054, 'metrics/total_secs_per_batch': 5.320937871932983, 'metrics/data_secs_per_batch': 1.988192081451416, '_timestamp': 1740972302.8821778}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 59 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972302.8824806}).
Epoch: [6][ 61/500]	Time  5.945 ( 5.945)	Loss 2.3144 (1.6282)	CeLoss 0.1924 (0.4168)	SegCLSLoss 0.0212 (0.0145)	KLLoss 0.3594 (0.2553)	MaskLoss 1.0376 (0.5894)	MaskBCELoss 0.0531 (0.0971)	MaskDICELoss 0.9845 (0.4923)
Epoch: [6][ 62/500]	Time  6.877 ( 6.877)	Loss 0.7835 (1.9401)	CeLoss 0.2256 (0.2131)	SegCLSLoss 0.0157 (0.0188)	KLLoss 0.3652 (0.3604)	MaskLoss 0.2570 (0.8407)	MaskBCELoss 0.0644 (0.1195)	MaskDICELoss 0.1926 (0.7212)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 60 is less than current step: 499. Dropping entry: {'train/loss': 1.6282355785369873, 'train/ce_loss': 0.416796875, 'train/seg_cls_loss': 0.01448974609375, 'train/kl_loss': 0.2552734375, 'train/mask_bce_loss': 0.0971064992249012, 'train/mask_dice_loss': 0.49230425655841825, 'train/mask_loss': 0.5894107580184936, 'metrics/total_secs_per_batch': 5.945068359375, 'metrics/data_secs_per_batch': 2.4845962524414062, '_timestamp': 1740972308.827745}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 60 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972308.828169}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 61 is less than current step: 499. Dropping entry: {'train/loss': 1.9401394426822662, 'train/ce_loss': 0.2130859375, 'train/seg_cls_loss': 0.018829345703125, 'train/kl_loss': 0.3603515625, 'train/mask_bce_loss': 0.11945928037166595, 'train/mask_dice_loss': 0.7212159052491188, 'train/mask_loss': 0.8406751930713654, 'metrics/total_secs_per_batch': 6.876967906951904, 'metrics/data_secs_per_batch': 3.101781415939331, '_timestamp': 1740972315.7045507}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 61 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972315.70489}).
Epoch: [6][ 63/500]	Time  6.971 ( 6.971)	Loss 1.7273 (1.3235)	CeLoss 0.2812 (0.2176)	SegCLSLoss 0.0104 (0.0129)	KLLoss 0.3672 (0.3275)	MaskLoss 0.7015 (0.5332)	MaskBCELoss 0.2330 (0.0859)	MaskDICELoss 0.4686 (0.4474)
Epoch: [6][ 64/500]	Time  6.225 ( 6.225)	Loss 0.8516 (1.4893)	CeLoss 0.8516 (0.3388)	SegCLSLoss 0.0000 (0.0125)	KLLoss 0.0000 (0.2176)	MaskLoss 0.0000 (0.5613)	MaskBCELoss 0.0000 (0.0796)	MaskDICELoss 0.0000 (0.4818)
Epoch: [6][ 65/500]	Time  5.781 ( 5.781)	Loss 1.6335 (1.3063)	CeLoss 0.2754 (0.4615)	SegCLSLoss 0.0108 (0.0086)	KLLoss 0.3574 (0.2172)	MaskLoss 0.6585 (0.4095)	MaskBCELoss 0.1453 (0.0382)	MaskDICELoss 0.5133 (0.3713)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 62 is less than current step: 499. Dropping entry: {'train/loss': 1.323521077632904, 'train/ce_loss': 0.217578125, 'train/seg_cls_loss': 0.0128662109375, 'train/kl_loss': 0.3275390625, 'train/mask_bce_loss': 0.08588039632886649, 'train/mask_dice_loss': 0.4473645091056824, 'train/mask_loss': 0.5332449078559875, 'metrics/total_secs_per_batch': 6.97104024887085, 'metrics/data_secs_per_batch': 3.0631071805953978, '_timestamp': 1740972322.6755962}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 62 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972322.6760595}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 63 is less than current step: 499. Dropping entry: {'train/loss': 1.4892917633056642, 'train/ce_loss': 0.33876953125, 'train/seg_cls_loss': 0.01248779296875, 'train/kl_loss': 0.217578125, 'train/mask_bce_loss': 0.0795797349885106, 'train/mask_dice_loss': 0.48176535964012146, 'train/mask_loss': 0.5613450884819031, 'metrics/total_secs_per_batch': 6.224954605102539, 'metrics/data_secs_per_batch': 2.9161442518234253, '_timestamp': 1740972328.9004025}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 63 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972328.9006956}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 64 is less than current step: 499. Dropping entry: {'train/loss': 1.3063188076019288, 'train/ce_loss': 0.4614990234375, 'train/seg_cls_loss': 0.008563232421875, 'train/kl_loss': 0.2171875, 'train/mask_bce_loss': 0.0382117772474885, 'train/mask_dice_loss': 0.3713074818253517, 'train/mask_loss': 0.40951925963163377, 'metrics/total_secs_per_batch': 5.78050971031189, 'metrics/data_secs_per_batch': 2.1754108667373657, '_timestamp': 1740972334.6809838}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 64 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972334.6812844}).
Epoch: [6][ 66/500]	Time  6.841 ( 6.841)	Loss 2.5141 (1.9956)	CeLoss 0.2246 (0.2852)	SegCLSLoss 0.0192 (0.0167)	KLLoss 0.3672 (0.3301)	MaskLoss 1.1213 (0.8345)	MaskBCELoss 0.4058 (0.1560)	MaskDICELoss 0.7155 (0.6785)
Epoch: [6][ 67/500]	Time  6.867 ( 6.867)	Loss 1.7105 (1.1992)	CeLoss 0.2158 (0.3553)	SegCLSLoss 0.0165 (0.0089)	KLLoss 0.3652 (0.2201)	MaskLoss 0.7254 (0.4087)	MaskBCELoss 0.0235 (0.0986)	MaskDICELoss 0.7019 (0.3101)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 65 is less than current step: 499. Dropping entry: {'train/loss': 1.9956233859062196, 'train/ce_loss': 0.28515625, 'train/seg_cls_loss': 0.0167236328125, 'train/kl_loss': 0.330078125, 'train/mask_bce_loss': 0.1560215897858143, 'train/mask_dice_loss': 0.6784600228071213, 'train/mask_loss': 0.8344816207885742, 'metrics/total_secs_per_batch': 6.84086012840271, 'metrics/data_secs_per_batch': 3.1570468187332152, '_timestamp': 1740972341.522324}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 65 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972341.5227199}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 66 is less than current step: 499. Dropping entry: {'train/loss': 1.1992123901844025, 'train/ce_loss': 0.3552734375, 'train/seg_cls_loss': 0.008941650390625, 'train/kl_loss': 0.2201171875, 'train/mask_bce_loss': 0.09860000368207693, 'train/mask_dice_loss': 0.31011264622211454, 'train/mask_loss': 0.4087126463651657, 'metrics/total_secs_per_batch': 6.867186069488525, 'metrics/data_secs_per_batch': 3.278649640083313, '_timestamp': 1740972348.3890264}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 66 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972348.3893993}).
Epoch: [6][ 68/500]	Time  6.308 ( 6.308)	Loss 1.7835 (1.7188)	CeLoss 0.1953 (0.2863)	SegCLSLoss 0.0147 (0.0132)	KLLoss 0.3574 (0.2941)	MaskLoss 0.7726 (0.6984)	MaskBCELoss 0.1351 (0.1340)	MaskDICELoss 0.6376 (0.5644)
Epoch: [6][ 69/500]	Time  5.824 ( 5.824)	Loss 1.3295 (1.6147)	CeLoss 0.2656 (0.4707)	SegCLSLoss 0.0117 (0.0103)	KLLoss 0.3613 (0.2902)	MaskLoss 0.5105 (0.5549)	MaskBCELoss 0.1003 (0.1230)	MaskDICELoss 0.4101 (0.4319)
[2025-03-02 21:26:06,024] [INFO] [logging.py:96:log_dist] [Rank 0] step=3070, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:26:06,030] [INFO] [timer.py:215:stop] epoch=0/micro_step=30700/global_step=3070, RunningAvgSamplesPerSec=1.4282179377821405, CurrSamplesPerSec=1.8152538741110975, MemAllocated=30.7GB, MaxMemAllocated=37.23GB
Epoch: [6][ 70/500]	Time  5.510 ( 5.510)	Loss 1.1406 (1.5910)	CeLoss 1.1406 (0.5064)	SegCLSLoss 0.0000 (0.0120)	KLLoss 0.0000 (0.2207)	MaskLoss 0.0000 (0.5283)	MaskBCELoss 0.0000 (0.1031)	MaskDICELoss 0.0000 (0.4251)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 67 is less than current step: 499. Dropping entry: {'train/loss': 1.7188021183013915, 'train/ce_loss': 0.286279296875, 'train/seg_cls_loss': 0.013238525390625, 'train/kl_loss': 0.294140625, 'train/mask_bce_loss': 0.1340393664315343, 'train/mask_dice_loss': 0.5643997877836228, 'train/mask_loss': 0.6984391450881958, 'metrics/total_secs_per_batch': 6.308187961578369, 'metrics/data_secs_per_batch': 2.893472409248352, '_timestamp': 1740972354.697419}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 67 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972354.6977918}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 68 is less than current step: 499. Dropping entry: {'train/loss': 1.6146921336650848, 'train/ce_loss': 0.470703125, 'train/seg_cls_loss': 0.010302734375, 'train/kl_loss': 0.290234375, 'train/mask_bce_loss': 0.1229616291821003, 'train/mask_dice_loss': 0.4319430246949196, 'train/mask_loss': 0.5549046605825424, 'metrics/total_secs_per_batch': 5.824036598205566, 'metrics/data_secs_per_batch': 2.7794057607650755, '_timestamp': 1740972360.5211911}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 68 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972360.5214581}).
Epoch: [6][ 71/500]	Time  5.710 ( 5.710)	Loss 1.6370 (1.5980)	CeLoss 0.2070 (0.5351)	SegCLSLoss 0.0189 (0.0107)	KLLoss 0.3633 (0.2529)	MaskLoss 0.6916 (0.5161)	MaskBCELoss 0.0267 (0.0708)	MaskDICELoss 0.6648 (0.4453)
Epoch: [6][ 72/500]	Time  5.561 ( 5.561)	Loss 2.8113 (1.9475)	CeLoss 0.1367 (0.4753)	SegCLSLoss 0.0410 (0.0193)	KLLoss 0.3789 (0.2557)	MaskLoss 1.3080 (0.7184)	MaskBCELoss 0.5636 (0.1618)	MaskDICELoss 0.7444 (0.5566)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 69 is less than current step: 499. Dropping entry: {'train/loss': 1.591029119491577, 'train/ce_loss': 0.506396484375, 'train/seg_cls_loss': 0.011962890625, 'train/kl_loss': 0.220703125, 'train/mask_bce_loss': 0.10314083229750395, 'train/mask_dice_loss': 0.4251129895448685, 'train/mask_loss': 0.528253823518753, 'metrics/total_secs_per_batch': 5.51043701171875, 'metrics/data_secs_per_batch': 2.520778465270996, '_timestamp': 1740972366.031483}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 69 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972366.031778}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 70 is less than current step: 499. Dropping entry: {'train/loss': 1.597986674308777, 'train/ce_loss': 0.53505859375, 'train/seg_cls_loss': 0.01065673828125, 'train/kl_loss': 0.2529296875, 'train/mask_bce_loss': 0.07082635257393122, 'train/mask_dice_loss': 0.445256832242012, 'train/mask_loss': 0.5160831868648529, 'metrics/total_secs_per_batch': 5.710184812545776, 'metrics/data_secs_per_batch': 2.550880765914917, '_timestamp': 1740972371.7421782}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 70 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972371.74256}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 71 is less than current step: 499. Dropping entry: {'train/loss': 1.9475236535072327, 'train/ce_loss': 0.475341796875, 'train/seg_cls_loss': 0.01927490234375, 'train/kl_loss': 0.2556640625, 'train/mask_bce_loss': 0.16180879343301058, 'train/mask_dice_loss': 0.5565819323062897, 'train/mask_loss': 0.7183907330036163, 'metrics/total_secs_per_batch': 5.56093430519104, 'metrics/data_secs_per_batch': 2.6658632516860963, '_timestamp': 1740972377.3028462}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 71 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972377.3031695}).
Epoch: [6][ 73/500]	Time  6.595 ( 6.595)	Loss 2.3896 (1.9136)	CeLoss 0.1699 (0.2830)	SegCLSLoss 0.0200 (0.0156)	KLLoss 0.3574 (0.3285)	MaskLoss 1.0874 (0.7951)	MaskBCELoss 0.2756 (0.1373)	MaskDICELoss 0.8118 (0.6577)
Epoch: [6][ 74/500]	Time  5.503 ( 5.503)	Loss 2.1118 (1.5091)	CeLoss 0.2139 (0.5100)	SegCLSLoss 0.0198 (0.0132)	KLLoss 0.3496 (0.2180)	MaskLoss 0.9270 (0.4853)	MaskBCELoss 0.0092 (0.0631)	MaskDICELoss 0.9178 (0.4222)
Epoch: [6][ 75/500]	Time  5.984 ( 5.984)	Loss 1.9682 (1.6760)	CeLoss 0.1895 (0.2966)	SegCLSLoss 0.0155 (0.0161)	KLLoss 0.3633 (0.3246)	MaskLoss 0.8674 (0.6693)	MaskBCELoss 0.0541 (0.1178)	MaskDICELoss 0.8133 (0.5515)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 72 is less than current step: 499. Dropping entry: {'train/loss': 1.913649582862854, 'train/ce_loss': 0.2830078125, 'train/seg_cls_loss': 0.01556396484375, 'train/kl_loss': 0.328515625, 'train/mask_bce_loss': 0.13732074624858798, 'train/mask_dice_loss': 0.6577364504337311, 'train/mask_loss': 0.7950572073459625, 'metrics/total_secs_per_batch': 6.595476865768433, 'metrics/data_secs_per_batch': 2.968773293495178, '_timestamp': 1740972383.8984723}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 72 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972383.898832}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 73 is less than current step: 499. Dropping entry: {'train/loss': 1.5091297388076783, 'train/ce_loss': 0.5099609375, 'train/seg_cls_loss': 0.0132080078125, 'train/kl_loss': 0.21796875, 'train/mask_bce_loss': 0.06312807211652398, 'train/mask_dice_loss': 0.4221985101699829, 'train/mask_loss': 0.48532657623291015, 'metrics/total_secs_per_batch': 5.502640962600708, 'metrics/data_secs_per_batch': 2.3115542411804197, '_timestamp': 1740972389.401005}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 73 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972389.4013088}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 74 is less than current step: 499. Dropping entry: {'train/loss': 1.6759902119636536, 'train/ce_loss': 0.29658203125, 'train/seg_cls_loss': 0.016094970703125, 'train/kl_loss': 0.324609375, 'train/mask_bce_loss': 0.11782523286528886, 'train/mask_dice_loss': 0.5515175312757492, 'train/mask_loss': 0.6693427741527558, 'metrics/total_secs_per_batch': 5.983642101287842, 'metrics/data_secs_per_batch': 2.7355973720550537, '_timestamp': 1740972395.3846915}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 74 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972395.3851328}).
Epoch: [6][ 76/500]	Time  6.957 ( 6.957)	Loss 1.5994 (1.7178)	CeLoss 0.2559 (0.3297)	SegCLSLoss 0.0094 (0.0123)	KLLoss 0.3633 (0.3254)	MaskLoss 0.6513 (0.6747)	MaskBCELoss 0.2648 (0.1518)	MaskDICELoss 0.3865 (0.5230)
Epoch: [6][ 77/500]	Time  5.018 ( 5.018)	Loss 0.1807 (1.3213)	CeLoss 0.1807 (0.6419)	SegCLSLoss 0.0000 (0.0072)	KLLoss 0.0000 (0.1811)	MaskLoss 0.0000 (0.3289)	MaskBCELoss 0.0000 (0.0642)	MaskDICELoss 0.0000 (0.2646)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 75 is less than current step: 499. Dropping entry: {'train/loss': 1.7178285479545594, 'train/ce_loss': 0.3296875, 'train/seg_cls_loss': 0.0123291015625, 'train/kl_loss': 0.325390625, 'train/mask_bce_loss': 0.15177672114223242, 'train/mask_dice_loss': 0.5229578584432601, 'train/mask_loss': 0.6747345685958862, 'metrics/total_secs_per_batch': 6.956627368927002, 'metrics/data_secs_per_batch': 3.1331278800964357, '_timestamp': 1740972402.3413606}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 75 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972402.3416884}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 76 is less than current step: 499. Dropping entry: {'train/loss': 1.321303129196167, 'train/ce_loss': 0.64189453125, 'train/seg_cls_loss': 0.007196044921875, 'train/kl_loss': 0.1810546875, 'train/mask_bce_loss': 0.06421787459403276, 'train/mask_dice_loss': 0.2646465837955475, 'train/mask_loss': 0.32886446118354795, 'metrics/total_secs_per_batch': 5.01840615272522, 'metrics/data_secs_per_batch': 2.398276948928833, '_timestamp': 1740972407.3595903}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 76 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972407.3598704}).
Epoch: [6][ 78/500]	Time  6.347 ( 6.347)	Loss 2.0044 (1.2642)	CeLoss 0.2080 (0.2360)	SegCLSLoss 0.0197 (0.0113)	KLLoss 0.3496 (0.2512)	MaskLoss 0.8762 (0.4987)	MaskBCELoss 0.0136 (0.0298)	MaskDICELoss 0.8626 (0.4689)
Epoch: [6][ 79/500]	Time  5.905 ( 5.905)	Loss 2.3637 (1.6318)	CeLoss 0.3477 (0.6144)	SegCLSLoss 0.0118 (0.0103)	KLLoss 0.3633 (0.2180)	MaskLoss 0.9865 (0.4952)	MaskBCELoss 0.4147 (0.1166)	MaskDICELoss 0.5718 (0.3786)
[2025-03-02 21:27:06,303] [INFO] [logging.py:96:log_dist] [Rank 0] step=3080, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:27:06,309] [INFO] [timer.py:215:stop] epoch=0/micro_step=30800/global_step=3080, RunningAvgSamplesPerSec=1.4288645740567874, CurrSamplesPerSec=1.4933972520847387, MemAllocated=31.46GB, MaxMemAllocated=37.23GB
Epoch: [6][ 80/500]	Time  6.698 ( 6.698)	Loss 1.2048 (1.6776)	CeLoss 0.2197 (0.3223)	SegCLSLoss 0.0190 (0.0151)	KLLoss 0.3613 (0.3273)	MaskLoss 0.4696 (0.6575)	MaskBCELoss 0.0169 (0.1257)	MaskDICELoss 0.4527 (0.5318)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 77 is less than current step: 499. Dropping entry: {'train/loss': 1.2641800701618195, 'train/ce_loss': 0.23603515625, 'train/seg_cls_loss': 0.011328125, 'train/kl_loss': 0.251171875, 'train/mask_bce_loss': 0.029825027706101535, 'train/mask_dice_loss': 0.4688665769994259, 'train/mask_loss': 0.49869160950183866, 'metrics/total_secs_per_batch': 6.347304344177246, 'metrics/data_secs_per_batch': 2.666516327857971, '_timestamp': 1740972413.7069988}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 77 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972413.7073417}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 78 is less than current step: 499. Dropping entry: {'train/loss': 1.6317908406257629, 'train/ce_loss': 0.61435546875, 'train/seg_cls_loss': 0.0102783203125, 'train/kl_loss': 0.21796875, 'train/mask_bce_loss': 0.1166282942518592, 'train/mask_dice_loss': 0.3785639971494675, 'train/mask_loss': 0.49519229829311373, 'metrics/total_secs_per_batch': 5.905084848403931, 'metrics/data_secs_per_batch': 2.757546830177307, '_timestamp': 1740972419.6119514}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 78 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972419.6122954}).
Epoch: [6][ 81/500]	Time  6.389 ( 6.389)	Loss 1.6361 (1.7072)	CeLoss 0.2207 (0.2396)	SegCLSLoss 0.0116 (0.0149)	KLLoss 0.3594 (0.3660)	MaskLoss 0.6872 (0.7117)	MaskBCELoss 0.1345 (0.1704)	MaskDICELoss 0.5527 (0.5412)
Epoch: [6][ 82/500]	Time  5.585 ( 5.585)	Loss 1.6094 (1.5916)	CeLoss 1.6094 (0.4831)	SegCLSLoss 0.0000 (0.0145)	KLLoss 0.0000 (0.2158)	MaskLoss 0.0000 (0.5400)	MaskBCELoss 0.0000 (0.0750)	MaskDICELoss 0.0000 (0.4649)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 79 is less than current step: 499. Dropping entry: {'train/loss': 1.6775902211666107, 'train/ce_loss': 0.322265625, 'train/seg_cls_loss': 0.015093994140625, 'train/kl_loss': 0.32734375, 'train/mask_bce_loss': 0.12566699469462037, 'train/mask_dice_loss': 0.5318292915821076, 'train/mask_loss': 0.6574962913990021, 'metrics/total_secs_per_batch': 6.69780707359314, 'metrics/data_secs_per_batch': 3.107509183883667, '_timestamp': 1740972426.309705}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 79 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972426.310077}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 80 is less than current step: 499. Dropping entry: {'train/loss': 1.7071907281875611, 'train/ce_loss': 0.23955078125, 'train/seg_cls_loss': 0.0149169921875, 'train/kl_loss': 0.366015625, 'train/mask_bce_loss': 0.1704276602715254, 'train/mask_dice_loss': 0.541224329918623, 'train/mask_loss': 0.7116519972681999, 'metrics/total_secs_per_batch': 6.38898777961731, 'metrics/data_secs_per_batch': 2.8823090553283692, '_timestamp': 1740972432.6987772}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 80 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972432.6990705}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 81 is less than current step: 499. Dropping entry: {'train/loss': 1.5916419267654418, 'train/ce_loss': 0.48310546875, 'train/seg_cls_loss': 0.014459228515625, 'train/kl_loss': 0.2158203125, 'train/mask_bce_loss': 0.0750321821309626, 'train/mask_dice_loss': 0.46492941975593566, 'train/mask_loss': 0.5399615883827209, 'metrics/total_secs_per_batch': 5.584676265716553, 'metrics/data_secs_per_batch': 2.617809796333313, '_timestamp': 1740972438.2836676}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 81 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972438.2840292}).
Epoch: [6][ 83/500]	Time  6.652 ( 6.652)	Loss 1.6692 (1.6854)	CeLoss 0.2305 (0.3514)	SegCLSLoss 0.0131 (0.0153)	KLLoss 0.3594 (0.3248)	MaskLoss 0.6979 (0.6468)	MaskBCELoss 0.0558 (0.1376)	MaskDICELoss 0.6421 (0.5092)
Epoch: [6][ 84/500]	Time  6.237 ( 6.237)	Loss 1.2506 (1.5535)	CeLoss 0.1943 (0.3277)	SegCLSLoss 0.0197 (0.0123)	KLLoss 0.3555 (0.2564)	MaskLoss 0.5052 (0.5969)	MaskBCELoss 0.0936 (0.0712)	MaskDICELoss 0.4115 (0.5257)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 82 is less than current step: 499. Dropping entry: {'train/loss': 1.6853506803512572, 'train/ce_loss': 0.3513671875, 'train/seg_cls_loss': 0.015338134765625, 'train/kl_loss': 0.3248046875, 'train/mask_bce_loss': 0.13764846343547105, 'train/mask_dice_loss': 0.5091772735118866, 'train/mask_loss': 0.6468257308006287, 'metrics/total_secs_per_batch': 6.651580810546875, 'metrics/data_secs_per_batch': 3.0248547315597536, '_timestamp': 1740972444.9350607}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 82 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972444.9354358}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 83 is less than current step: 499. Dropping entry: {'train/loss': 1.5534546613693236, 'train/ce_loss': 0.327685546875, 'train/seg_cls_loss': 0.012261962890625, 'train/kl_loss': 0.2564453125, 'train/mask_bce_loss': 0.07123454082757234, 'train/mask_dice_loss': 0.5256832271814347, 'train/mask_loss': 0.5969177663326264, 'metrics/total_secs_per_batch': 6.236867666244507, 'metrics/data_secs_per_batch': 2.5019493103027344, '_timestamp': 1740972451.1719193}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 83 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972451.1721928}).
Epoch: [6][ 85/500]	Time  5.798 ( 5.798)	Loss 2.2137 (1.5987)	CeLoss 0.2080 (0.5615)	SegCLSLoss 0.0181 (0.0106)	KLLoss 0.3594 (0.2166)	MaskLoss 0.9809 (0.5051)	MaskBCELoss 0.1392 (0.0452)	MaskDICELoss 0.8417 (0.4599)
Epoch: [6][ 86/500]	Time  7.308 ( 7.308)	Loss 1.7188 (1.6671)	CeLoss 1.7188 (0.4578)	SegCLSLoss 0.0000 (0.0098)	KLLoss 0.0000 (0.2539)	MaskLoss 0.0000 (0.5894)	MaskBCELoss 0.0000 (0.1586)	MaskDICELoss 0.0000 (0.4308)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 84 is less than current step: 499. Dropping entry: {'train/loss': 1.598712110519409, 'train/ce_loss': 0.5615234375, 'train/seg_cls_loss': 0.010626220703125, 'train/kl_loss': 0.2166015625, 'train/mask_bce_loss': 0.04524994269013405, 'train/mask_dice_loss': 0.4598678171634674, 'train/mask_loss': 0.5051177591085434, 'metrics/total_secs_per_batch': 5.798226594924927, 'metrics/data_secs_per_batch': 2.82498939037323, '_timestamp': 1740972456.9702265}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 84 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972456.9706361}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 85 is less than current step: 499. Dropping entry: {'train/loss': 1.6670878291130067, 'train/ce_loss': 0.457763671875, 'train/seg_cls_loss': 0.009796142578125, 'train/kl_loss': 0.25390625, 'train/mask_bce_loss': 0.1585970526561141, 'train/mask_dice_loss': 0.4308306485414505, 'train/mask_loss': 0.5894277036190033, 'metrics/total_secs_per_batch': 7.308332443237305, 'metrics/data_secs_per_batch': 3.2314799308776854, '_timestamp': 1740972464.2787044}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 85 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972464.279084}).
Epoch: [6][ 87/500]	Time  7.577 ( 7.577)	Loss 1.5886 (1.1827)	CeLoss 0.2598 (0.2091)	SegCLSLoss 0.0112 (0.0091)	KLLoss 0.3633 (0.2174)	MaskLoss 0.6429 (0.4737)	MaskBCELoss 0.0986 (0.0712)	MaskDICELoss 0.5443 (0.4026)
Epoch: [6][ 88/500]	Time  5.857 ( 5.857)	Loss 1.8155 (1.5458)	CeLoss 0.2871 (0.4276)	SegCLSLoss 0.0099 (0.0112)	KLLoss 0.3652 (0.2537)	MaskLoss 0.7427 (0.5436)	MaskBCELoss 0.1052 (0.0786)	MaskDICELoss 0.6375 (0.4650)
Epoch: [6][ 89/500]	Time  5.978 ( 5.978)	Loss 2.2142 (1.4874)	CeLoss 0.2188 (0.3081)	SegCLSLoss 0.0182 (0.0131)	KLLoss 0.3633 (0.2896)	MaskLoss 0.9753 (0.5717)	MaskBCELoss 0.0096 (0.0841)	MaskDICELoss 0.9657 (0.4877)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 86 is less than current step: 499. Dropping entry: {'train/loss': 1.1827402591705323, 'train/ce_loss': 0.20908203125, 'train/seg_cls_loss': 0.00909423828125, 'train/kl_loss': 0.2173828125, 'train/mask_bce_loss': 0.07116168066859245, 'train/mask_dice_loss': 0.4025814920663834, 'train/mask_loss': 0.47374317049980164, 'metrics/total_secs_per_batch': 7.576796054840088, 'metrics/data_secs_per_batch': 3.408592462539673, '_timestamp': 1740972471.8553097}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 86 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972471.855601}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 87 is less than current step: 499. Dropping entry: {'train/loss': 1.545759129524231, 'train/ce_loss': 0.42763671875, 'train/seg_cls_loss': 0.0112060546875, 'train/kl_loss': 0.2537109375, 'train/mask_bce_loss': 0.07863060841336847, 'train/mask_dice_loss': 0.4649520605802536, 'train/mask_loss': 0.5435826629400253, 'metrics/total_secs_per_batch': 5.856704235076904, 'metrics/data_secs_per_batch': 2.878099513053894, '_timestamp': 1740972477.7121716}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 87 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972477.7125125}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 88 is less than current step: 499. Dropping entry: {'train/loss': 1.4874434381723405, 'train/ce_loss': 0.30810546875, 'train/seg_cls_loss': 0.013092041015625, 'train/kl_loss': 0.2896484375, 'train/mask_bce_loss': 0.08408546773716807, 'train/mask_dice_loss': 0.4876635953783989, 'train/mask_loss': 0.5717490509152412, 'metrics/total_secs_per_batch': 5.978335857391357, 'metrics/data_secs_per_batch': 2.3593398571014403, '_timestamp': 1740972483.6903667}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 88 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972483.6907272}).
[2025-03-02 21:28:10,240] [INFO] [logging.py:96:log_dist] [Rank 0] step=3090, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:28:10,246] [INFO] [timer.py:215:stop] epoch=0/micro_step=30900/global_step=3090, RunningAvgSamplesPerSec=1.4292654751418874, CurrSamplesPerSec=1.5255478926032475, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [6][ 90/500]	Time  6.557 ( 6.557)	Loss 1.9492 (1.7720)	CeLoss 0.2637 (0.2838)	SegCLSLoss 0.0098 (0.0132)	KLLoss 0.3633 (0.3264)	MaskLoss 0.8213 (0.7244)	MaskBCELoss 0.3817 (0.2223)	MaskDICELoss 0.4396 (0.5021)
Epoch: [6][ 91/500]	Time  6.216 ( 6.216)	Loss 2.4177 (1.5651)	CeLoss 0.1611 (0.3929)	SegCLSLoss 0.0197 (0.0130)	KLLoss 0.3789 (0.2924)	MaskLoss 1.1043 (0.5682)	MaskBCELoss 0.2631 (0.0953)	MaskDICELoss 0.8413 (0.4728)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 89 is less than current step: 499. Dropping entry: {'train/loss': 1.772047793865204, 'train/ce_loss': 0.2837890625, 'train/seg_cls_loss': 0.013177490234375, 'train/kl_loss': 0.3263671875, 'train/mask_bce_loss': 0.222310727648437, 'train/mask_dice_loss': 0.5020920783281326, 'train/mask_loss': 0.724402803182602, 'metrics/total_secs_per_batch': 6.556724309921265, 'metrics/data_secs_per_batch': 2.982714319229126, '_timestamp': 1740972490.2468665}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 89 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972490.247135}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 90 is less than current step: 499. Dropping entry: {'train/loss': 1.5650634527206422, 'train/ce_loss': 0.39287109375, 'train/seg_cls_loss': 0.013018798828125, 'train/kl_loss': 0.2923828125, 'train/mask_bce_loss': 0.09533653883263468, 'train/mask_dice_loss': 0.4728397220373154, 'train/mask_loss': 0.5681762635707855, 'metrics/total_secs_per_batch': 6.216269016265869, 'metrics/data_secs_per_batch': 2.7355658531188967, '_timestamp': 1740972496.4636223}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 90 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972496.463992}).
Epoch: [6][ 92/500]	Time  5.527 ( 5.527)	Loss 1.2031 (1.6865)	CeLoss 1.2031 (0.5598)	SegCLSLoss 0.0000 (0.0116)	KLLoss 0.0000 (0.2543)	MaskLoss 0.0000 (0.5477)	MaskBCELoss 0.0000 (0.0896)	MaskDICELoss 0.0000 (0.4581)
Epoch: [6][ 93/500]	Time  6.255 ( 6.255)	Loss 0.0811 (1.1410)	CeLoss 0.0811 (0.4236)	SegCLSLoss 0.0000 (0.0082)	KLLoss 0.0000 (0.2162)	MaskLoss 0.0000 (0.3458)	MaskBCELoss 0.0000 (0.0572)	MaskDICELoss 0.0000 (0.2886)
Epoch: [6][ 94/500]	Time  4.816 ( 4.816)	Loss 0.9648 (1.3862)	CeLoss 0.9648 (0.7198)	SegCLSLoss 0.0000 (0.0074)	KLLoss 0.0000 (0.1453)	MaskLoss 0.0000 (0.3241)	MaskBCELoss 0.0000 (0.0366)	MaskDICELoss 0.0000 (0.2874)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 91 is less than current step: 499. Dropping entry: {'train/loss': 1.6864500999450684, 'train/ce_loss': 0.559765625, 'train/seg_cls_loss': 0.01160888671875, 'train/kl_loss': 0.254296875, 'train/mask_bce_loss': 0.08955453597009182, 'train/mask_dice_loss': 0.45811385810375216, 'train/mask_loss': 0.5476683914661408, 'metrics/total_secs_per_batch': 5.526814937591553, 'metrics/data_secs_per_batch': 2.492096495628357, '_timestamp': 1740972501.9901538}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 91 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972501.990424}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 92 is less than current step: 499. Dropping entry: {'train/loss': 1.1410236954689026, 'train/ce_loss': 0.4236328125, 'train/seg_cls_loss': 0.008245849609375, 'train/kl_loss': 0.2162109375, 'train/mask_bce_loss': 0.05715765943750739, 'train/mask_dice_loss': 0.28859833255410194, 'train/mask_loss': 0.34575599133968354, 'metrics/total_secs_per_batch': 6.255475997924805, 'metrics/data_secs_per_batch': 2.7142926454544067, '_timestamp': 1740972508.245832}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 92 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972508.246198}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 93 is less than current step: 499. Dropping entry: {'train/loss': 1.386237633228302, 'train/ce_loss': 0.71982421875, 'train/seg_cls_loss': 0.007427978515625, 'train/kl_loss': 0.1453125, 'train/mask_bce_loss': 0.036633594706654546, 'train/mask_dice_loss': 0.2874422401189804, 'train/mask_loss': 0.3240758299827576, 'metrics/total_secs_per_batch': 4.815693140029907, 'metrics/data_secs_per_batch': 2.2301822185516356, '_timestamp': 1740972513.0615325}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 93 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972513.0619063}).
Epoch: [6][ 95/500]	Time  4.960 ( 4.960)	Loss 2.2122 (1.5565)	CeLoss 0.3086 (0.4928)	SegCLSLoss 0.0140 (0.0099)	KLLoss 0.3652 (0.2172)	MaskLoss 0.9293 (0.5185)	MaskBCELoss 0.1390 (0.0552)	MaskDICELoss 0.7903 (0.4632)
Epoch: [6][ 96/500]	Time  6.407 ( 6.407)	Loss 2.3587 (1.9563)	CeLoss 0.1982 (0.2041)	SegCLSLoss 0.0206 (0.0169)	KLLoss 0.3672 (0.3287)	MaskLoss 1.0563 (0.8554)	MaskBCELoss 0.2768 (0.2293)	MaskDICELoss 0.7796 (0.6261)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 94 is less than current step: 499. Dropping entry: {'train/loss': 1.5564701795578002, 'train/ce_loss': 0.4927734375, 'train/seg_cls_loss': 0.00985107421875, 'train/kl_loss': 0.2171875, 'train/mask_bce_loss': 0.055226501822471616, 'train/mask_dice_loss': 0.46324296593666076, 'train/mask_loss': 0.5184694707393647, 'metrics/total_secs_per_batch': 4.95960545539856, 'metrics/data_secs_per_batch': 2.0870140552520753, '_timestamp': 1740972518.0209413}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 94 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972518.0213003}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 95 is less than current step: 499. Dropping entry: {'train/loss': 1.956285560131073, 'train/ce_loss': 0.2041015625, 'train/seg_cls_loss': 0.0168701171875, 'train/kl_loss': 0.3287109375, 'train/mask_bce_loss': 0.2293108280748129, 'train/mask_dice_loss': 0.6260780215263366, 'train/mask_loss': 0.8553888618946075, 'metrics/total_secs_per_batch': 6.406787872314453, 'metrics/data_secs_per_batch': 2.788484311103821, '_timestamp': 1740972524.4277496}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 95 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972524.4280295}).
Epoch: [6][ 97/500]	Time  7.640 ( 7.640)	Loss 1.9584 (1.7607)	CeLoss 0.2734 (0.3454)	SegCLSLoss 0.0106 (0.0132)	KLLoss 0.3711 (0.2895)	MaskLoss 0.8210 (0.6899)	MaskBCELoss 0.1277 (0.0994)	MaskDICELoss 0.6933 (0.5904)
Epoch: [6][ 98/500]	Time  6.421 ( 6.421)	Loss 2.8630 (1.9174)	CeLoss 0.2500 (0.3063)	SegCLSLoss 0.0139 (0.0129)	KLLoss 0.3613 (0.3311)	MaskLoss 1.2850 (0.7857)	MaskBCELoss 0.5265 (0.2407)	MaskDICELoss 0.7585 (0.5450)
Epoch: [6][ 99/500]	Time  6.045 ( 6.045)	Loss 2.5945 (1.7933)	CeLoss 0.2227 (0.4267)	SegCLSLoss 0.0193 (0.0151)	KLLoss 0.3691 (0.2916)	MaskLoss 1.1625 (0.6649)	MaskBCELoss 0.3658 (0.1107)	MaskDICELoss 0.7966 (0.5542)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 96 is less than current step: 499. Dropping entry: {'train/loss': 1.7606767296791077, 'train/ce_loss': 0.34541015625, 'train/seg_cls_loss': 0.013177490234375, 'train/kl_loss': 0.289453125, 'train/mask_bce_loss': 0.09941521175205707, 'train/mask_dice_loss': 0.5904446333646775, 'train/mask_loss': 0.6898598492145538, 'metrics/total_secs_per_batch': 7.640289068222046, 'metrics/data_secs_per_batch': 3.570992612838745, '_timestamp': 1740972532.0680742}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 96 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972532.0683782}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 97 is less than current step: 499. Dropping entry: {'train/loss': 1.9173812329769135, 'train/ce_loss': 0.30634765625, 'train/seg_cls_loss': 0.012921142578125, 'train/kl_loss': 0.3310546875, 'train/mask_bce_loss': 0.24071926139295102, 'train/mask_dice_loss': 0.545022115111351, 'train/mask_loss': 0.7857413858175277, 'metrics/total_secs_per_batch': 6.421266078948975, 'metrics/data_secs_per_batch': 2.739369297027588, '_timestamp': 1740972538.4894195}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 97 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972538.489744}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 98 is less than current step: 499. Dropping entry: {'train/loss': 1.7933099627494813, 'train/ce_loss': 0.42666015625, 'train/seg_cls_loss': 0.01514892578125, 'train/kl_loss': 0.2916015625, 'train/mask_bce_loss': 0.11071068290621042, 'train/mask_dice_loss': 0.5542060166597367, 'train/mask_loss': 0.664916706085205, 'metrics/total_secs_per_batch': 6.045050144195557, 'metrics/data_secs_per_batch': 2.48156623840332, '_timestamp': 1740972544.534369}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 98 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972544.5346563}).
[2025-03-02 21:29:10,486] [INFO] [logging.py:96:log_dist] [Rank 0] step=3100, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:29:10,492] [INFO] [timer.py:215:stop] epoch=0/micro_step=31000/global_step=3100, RunningAvgSamplesPerSec=1.4299075324410637, CurrSamplesPerSec=1.6785143959194806, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [6][100/500]	Time  5.959 ( 5.959)	Loss 2.7210 (1.4151)	CeLoss 0.2988 (0.3843)	SegCLSLoss 0.0209 (0.0112)	KLLoss 0.3672 (0.2906)	MaskLoss 1.1876 (0.4980)	MaskBCELoss 0.5718 (0.1363)	MaskDICELoss 0.6159 (0.3617)
Epoch: [6][101/500]	Time  5.111 ( 5.111)	Loss 1.7213 (1.5121)	CeLoss 0.2188 (0.6283)	SegCLSLoss 0.0221 (0.0086)	KLLoss 0.3516 (0.1791)	MaskLoss 0.7278 (0.4307)	MaskBCELoss 0.1435 (0.0837)	MaskDICELoss 0.5843 (0.3471)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 99 is less than current step: 499. Dropping entry: {'train/loss': 1.4150922000408173, 'train/ce_loss': 0.38427734375, 'train/seg_cls_loss': 0.01124267578125, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.13631974142044784, 'train/mask_dice_loss': 0.3616560444235802, 'train/mask_loss': 0.4979757867753506, 'metrics/total_secs_per_batch': 5.959229230880737, 'metrics/data_secs_per_batch': 2.7027337074279787, '_timestamp': 1740972550.4933636}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 99 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972550.4935443}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 100 is less than current step: 499. Dropping entry: {'train/loss': 1.5121255099773407, 'train/ce_loss': 0.628271484375, 'train/seg_cls_loss': 0.008563232421875, 'train/kl_loss': 0.1791015625, 'train/mask_bce_loss': 0.08365037124603987, 'train/mask_dice_loss': 0.3470949947834015, 'train/mask_loss': 0.43074536621570586, 'metrics/total_secs_per_batch': 5.111250400543213, 'metrics/data_secs_per_batch': 2.1126264095306397, '_timestamp': 1740972555.604924}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 100 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972555.60522}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 101 is less than current step: 499. Dropping entry: {'train/loss': 1.099473124742508, 'train/ce_loss': 0.4994140625, 'train/seg_cls_loss': 0.008941650390625, 'train/kl_loss': 0.1849609375, 'train/mask_bce_loss': 0.0505043565761298, 'train/mask_dice_loss': 0.23800174295902252, 'train/mask_loss': 0.2885060966014862, 'metrics/total_secs_per_batch': 5.374834775924683, 'metrics/data_secs_per_batch': 2.729858326911926, '_timestamp': 1740972560.979706}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 101 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972560.9799943}).
Epoch: [6][102/500]	Time  5.375 ( 5.375)	Loss 1.3959 (1.0995)	CeLoss 0.1465 (0.4994)	SegCLSLoss 0.0287 (0.0089)	KLLoss 0.3652 (0.1850)	MaskLoss 0.5993 (0.2885)	MaskBCELoss 0.0028 (0.0505)	MaskDICELoss 0.5965 (0.2380)
Epoch: [6][103/500]	Time  5.972 ( 5.972)	Loss 0.7775 (1.2326)	CeLoss 0.4688 (0.3941)	SegCLSLoss 0.0114 (0.0102)	KLLoss 0.3594 (0.2160)	MaskLoss 0.1329 (0.4058)	MaskBCELoss 0.0469 (0.0522)	MaskDICELoss 0.0860 (0.3537)
Epoch: [6][104/500]	Time  5.734 ( 5.734)	Loss 1.0938 (1.5094)	CeLoss 1.0938 (0.4914)	SegCLSLoss 0.0000 (0.0122)	KLLoss 0.0000 (0.2189)	MaskLoss 0.0000 (0.4951)	MaskBCELoss 0.0000 (0.1042)	MaskDICELoss 0.0000 (0.3909)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 102 is less than current step: 499. Dropping entry: {'train/loss': 1.232571280002594, 'train/ce_loss': 0.394140625, 'train/seg_cls_loss': 0.010223388671875, 'train/kl_loss': 0.216015625, 'train/mask_bce_loss': 0.05217692293226719, 'train/mask_dice_loss': 0.35365950465202334, 'train/mask_loss': 0.4058364197611809, 'metrics/total_secs_per_batch': 5.972058534622192, 'metrics/data_secs_per_batch': 2.8982653617858887, '_timestamp': 1740972566.9519026}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 102 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972566.9521275}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 103 is less than current step: 499. Dropping entry: {'train/loss': 1.50938378572464, 'train/ce_loss': 0.491357421875, 'train/seg_cls_loss': 0.012158203125, 'train/kl_loss': 0.2189453125, 'train/mask_bce_loss': 0.10418035006150603, 'train/mask_dice_loss': 0.39094121754169464, 'train/mask_loss': 0.49512157440185545, 'metrics/total_secs_per_batch': 5.7342283725738525, 'metrics/data_secs_per_batch': 2.433965229988098, '_timestamp': 1740972572.6860213}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 103 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972572.6863103}).
Epoch: [6][105/500]	Time  6.140 ( 6.140)	Loss 2.3708 (1.6918)	CeLoss 0.2520 (0.2786)	SegCLSLoss 0.0164 (0.0125)	KLLoss 0.3652 (0.2895)	MaskLoss 1.0370 (0.6891)	MaskBCELoss 0.0374 (0.1138)	MaskDICELoss 0.9996 (0.5753)
Epoch: [6][106/500]	Time  6.486 ( 6.486)	Loss 1.4907 (1.8565)	CeLoss 0.2422 (0.2172)	SegCLSLoss 0.0125 (0.0149)	KLLoss 0.3652 (0.3320)	MaskLoss 0.6027 (0.7994)	MaskBCELoss 0.1188 (0.1796)	MaskDICELoss 0.4839 (0.6198)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 104 is less than current step: 499. Dropping entry: {'train/loss': 1.6917643189430236, 'train/ce_loss': 0.2785888671875, 'train/seg_cls_loss': 0.01253662109375, 'train/kl_loss': 0.289453125, 'train/mask_bce_loss': 0.11380131386686117, 'train/mask_dice_loss': 0.5753059566020966, 'train/mask_loss': 0.6891072571277619, 'metrics/total_secs_per_batch': 6.1395673751831055, 'metrics/data_secs_per_batch': 2.654021167755127, '_timestamp': 1740972578.8255117}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 104 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972578.8257847}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 105 is less than current step: 499. Dropping entry: {'train/loss': 1.8565465569496156, 'train/ce_loss': 0.2171875, 'train/seg_cls_loss': 0.014892578125, 'train/kl_loss': 0.33203125, 'train/mask_bce_loss': 0.1795805547386408, 'train/mask_dice_loss': 0.6197864860296249, 'train/mask_loss': 0.7993670284748078, 'metrics/total_secs_per_batch': 6.485903978347778, 'metrics/data_secs_per_batch': 2.867573952674866, '_timestamp': 1740972585.3114345}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 105 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972585.3117049}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 106 is less than current step: 499. Dropping entry: {'train/loss': 1.569234162569046, 'train/ce_loss': 0.352294921875, 'train/seg_cls_loss': 0.01385498046875, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.1827924839220941, 'train/mask_dice_loss': 0.4096126973628998, 'train/mask_loss': 0.592405167222023, 'metrics/total_secs_per_batch': 5.684149265289307, 'metrics/data_secs_per_batch': 2.202337551116943, '_timestamp': 1740972590.9955804}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 106 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972590.9958599}).
Epoch: [6][107/500]	Time  5.684 ( 5.684)	Loss 0.0532 (1.5692)	CeLoss 0.0532 (0.3523)	SegCLSLoss 0.0000 (0.0139)	KLLoss 0.0000 (0.2547)	MaskLoss 0.0000 (0.5924)	MaskBCELoss 0.0000 (0.1828)	MaskDICELoss 0.0000 (0.4096)
Epoch: [6][108/500]	Time  6.334 ( 6.334)	Loss 1.3281 (1.7375)	CeLoss 1.3281 (0.4821)	SegCLSLoss 0.0000 (0.0123)	KLLoss 0.0000 (0.2529)	MaskLoss 0.0000 (0.6118)	MaskBCELoss 0.0000 (0.1313)	MaskDICELoss 0.0000 (0.4805)
Epoch: [6][109/500]	Time  6.776 ( 6.776)	Loss 1.1214 (1.6624)	CeLoss 0.1934 (0.2050)	SegCLSLoss 0.0103 (0.0132)	KLLoss 0.3594 (0.3268)	MaskLoss 0.4435 (0.7090)	MaskBCELoss 0.1331 (0.1508)	MaskDICELoss 0.3104 (0.5581)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 107 is less than current step: 499. Dropping entry: {'train/loss': 1.7375497460365295, 'train/ce_loss': 0.482080078125, 'train/seg_cls_loss': 0.012274169921875, 'train/kl_loss': 0.2529296875, 'train/mask_bce_loss': 0.13127951389178633, 'train/mask_dice_loss': 0.4805373400449753, 'train/mask_loss': 0.6118168473243714, 'metrics/total_secs_per_batch': 6.334063529968262, 'metrics/data_secs_per_batch': 3.2078585386276246, '_timestamp': 1740972597.329614}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 107 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972597.3298752}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 108 is less than current step: 499. Dropping entry: {'train/loss': 1.662370991706848, 'train/ce_loss': 0.20498046875, 'train/seg_cls_loss': 0.01317138671875, 'train/kl_loss': 0.3267578125, 'train/mask_bce_loss': 0.15082926927134394, 'train/mask_dice_loss': 0.5581394284963608, 'train/mask_loss': 0.7089686959981918, 'metrics/total_secs_per_batch': 6.775772333145142, 'metrics/data_secs_per_batch': 2.9687893629074096, '_timestamp': 1740972604.1054878}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 108 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972604.1057658}).
[2025-03-02 21:30:10,232] [INFO] [logging.py:96:log_dist] [Rank 0] step=3110, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:30:10,238] [INFO] [timer.py:215:stop] epoch=0/micro_step=31100/global_step=3110, RunningAvgSamplesPerSec=1.4305789704006537, CurrSamplesPerSec=1.6308810015201012, MemAllocated=31.45GB, MaxMemAllocated=37.23GB
Epoch: [6][110/500]	Time  6.133 ( 6.133)	Loss 1.5724 (1.2127)	CeLoss 0.2695 (0.3106)	SegCLSLoss 0.0166 (0.0109)	KLLoss 0.3594 (0.2533)	MaskLoss 0.6299 (0.4359)	MaskBCELoss 0.0602 (0.0688)	MaskDICELoss 0.5697 (0.3671)
Epoch: [6][111/500]	Time  6.266 ( 6.266)	Loss 1.1467 (1.6435)	CeLoss 0.2236 (0.3098)	SegCLSLoss 0.0103 (0.0139)	KLLoss 0.3672 (0.2895)	MaskLoss 0.4405 (0.6489)	MaskBCELoss 0.0748 (0.0893)	MaskDICELoss 0.3657 (0.5597)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 109 is less than current step: 499. Dropping entry: {'train/loss': 1.2127004146575928, 'train/ce_loss': 0.31064453125, 'train/seg_cls_loss': 0.0109130859375, 'train/kl_loss': 0.2533203125, 'train/mask_bce_loss': 0.06883983612060547, 'train/mask_dice_loss': 0.36705139502882955, 'train/mask_loss': 0.43589122742414477, 'metrics/total_secs_per_batch': 6.1332783699035645, 'metrics/data_secs_per_batch': 2.8118823766708374, '_timestamp': 1740972610.238522}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 109 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972610.238791}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 110 is less than current step: 499. Dropping entry: {'train/loss': 1.6435094714164733, 'train/ce_loss': 0.309814453125, 'train/seg_cls_loss': 0.01387939453125, 'train/kl_loss': 0.289453125, 'train/mask_bce_loss': 0.08925613276660442, 'train/mask_dice_loss': 0.5596714645624161, 'train/mask_loss': 0.6489275991916656, 'metrics/total_secs_per_batch': 6.266226530075073, 'metrics/data_secs_per_batch': 2.8099331140518187, '_timestamp': 1740972616.5049517}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 110 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972616.5052428}).
Epoch: [6][112/500]	Time  6.277 ( 6.277)	Loss 2.1192 (1.5623)	CeLoss 0.2412 (0.4941)	SegCLSLoss 0.0187 (0.0104)	KLLoss 0.3574 (0.2143)	MaskLoss 0.9170 (0.5208)	MaskBCELoss 0.2741 (0.0777)	MaskDICELoss 0.6429 (0.4431)
Epoch: [6][113/500]	Time  6.898 ( 6.898)	Loss 1.8926 (1.5003)	CeLoss 0.2441 (0.3113)	SegCLSLoss 0.0189 (0.0136)	KLLoss 0.3574 (0.2898)	MaskLoss 0.8018 (0.5767)	MaskBCELoss 0.0387 (0.0698)	MaskDICELoss 0.7631 (0.5069)
Epoch: [6][114/500]	Time  6.934 ( 6.934)	Loss 1.3438 (1.0881)	CeLoss 1.3438 (0.3402)	SegCLSLoss 0.0000 (0.0097)	KLLoss 0.0000 (0.2529)	MaskLoss 0.0000 (0.3586)	MaskBCELoss 0.0000 (0.0429)	MaskDICELoss 0.0000 (0.3157)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 111 is less than current step: 499. Dropping entry: {'train/loss': 1.562334454059601, 'train/ce_loss': 0.494140625, 'train/seg_cls_loss': 0.010406494140625, 'train/kl_loss': 0.2142578125, 'train/mask_bce_loss': 0.07769021922722459, 'train/mask_dice_loss': 0.44312543272972105, 'train/mask_loss': 0.5208156526088714, 'metrics/total_secs_per_batch': 6.276525259017944, 'metrics/data_secs_per_batch': 2.7810495615005495, '_timestamp': 1740972622.781697}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 111 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972622.7820458}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 112 is less than current step: 499. Dropping entry: {'train/loss': 1.5003208637237548, 'train/ce_loss': 0.3113037109375, 'train/seg_cls_loss': 0.013580322265625, 'train/kl_loss': 0.28984375, 'train/mask_bce_loss': 0.06984750377014279, 'train/mask_dice_loss': 0.5068876355886459, 'train/mask_loss': 0.576735132932663, 'metrics/total_secs_per_batch': 6.897658824920654, 'metrics/data_secs_per_batch': 2.88302800655365, '_timestamp': 1740972629.6791377}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 112 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972629.6794145}).
Epoch: [6][115/500]	Time  6.141 ( 6.141)	Loss 1.1837 (1.7875)	CeLoss 0.2598 (0.4833)	SegCLSLoss 0.0123 (0.0130)	KLLoss 0.3633 (0.2520)	MaskLoss 0.4405 (0.6362)	MaskBCELoss 0.0322 (0.1542)	MaskDICELoss 0.4083 (0.4820)
Epoch: [6][116/500]	Time  6.274 ( 6.274)	Loss 0.5781 (1.6585)	CeLoss 0.5781 (0.3415)	SegCLSLoss 0.0000 (0.0117)	KLLoss 0.0000 (0.2566)	MaskLoss 0.0000 (0.6427)	MaskBCELoss 0.0000 (0.1782)	MaskDICELoss 0.0000 (0.4645)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 113 is less than current step: 499. Dropping entry: {'train/loss': 1.0880655825138092, 'train/ce_loss': 0.340185546875, 'train/seg_cls_loss': 0.0097412109375, 'train/kl_loss': 0.2529296875, 'train/mask_bce_loss': 0.04292335249483585, 'train/mask_dice_loss': 0.31568462327122687, 'train/mask_loss': 0.35860797613859174, 'metrics/total_secs_per_batch': 6.933877944946289, 'metrics/data_secs_per_batch': 3.4036112070083617, '_timestamp': 1740972636.6130066}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 113 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972636.6132832}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 114 is less than current step: 499. Dropping entry: {'train/loss': 1.7875274419784546, 'train/ce_loss': 0.48330078125, 'train/seg_cls_loss': 0.01295166015625, 'train/kl_loss': 0.251953125, 'train/mask_bce_loss': 0.1542142003774643, 'train/mask_dice_loss': 0.4820299863815308, 'train/mask_loss': 0.6362441748380661, 'metrics/total_secs_per_batch': 6.14129376411438, 'metrics/data_secs_per_batch': 2.7362197160720827, '_timestamp': 1740972642.7543244}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 114 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972642.754656}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 115 is less than current step: 499. Dropping entry: {'train/loss': 1.6584823548793792, 'train/ce_loss': 0.341455078125, 'train/seg_cls_loss': 0.011712646484375, 'train/kl_loss': 0.256640625, 'train/mask_bce_loss': 0.1782130863517523, 'train/mask_dice_loss': 0.46448022723197935, 'train/mask_loss': 0.6426933258771896, 'metrics/total_secs_per_batch': 6.274181365966797, 'metrics/data_secs_per_batch': 3.0723822355270385, '_timestamp': 1740972649.0284898}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 115 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972649.02876}).
Epoch: [6][117/500]	Time  6.756 ( 6.756)	Loss 1.8874 (1.9446)	CeLoss 0.1846 (0.3794)	SegCLSLoss 0.0211 (0.0134)	KLLoss 0.3535 (0.2896)	MaskLoss 0.8285 (0.7646)	MaskBCELoss 0.0105 (0.1867)	MaskDICELoss 0.8180 (0.5779)
Epoch: [6][118/500]	Time  6.308 ( 6.308)	Loss 1.8561 (1.6563)	CeLoss 0.2002 (0.2613)	SegCLSLoss 0.0247 (0.0176)	KLLoss 0.3516 (0.3229)	MaskLoss 0.8040 (0.6769)	MaskBCELoss 0.0517 (0.1571)	MaskDICELoss 0.7523 (0.5198)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 116 is less than current step: 499. Dropping entry: {'train/loss': 1.9445509552955627, 'train/ce_loss': 0.379443359375, 'train/seg_cls_loss': 0.013421630859375, 'train/kl_loss': 0.2896484375, 'train/mask_bce_loss': 0.18672501957044005, 'train/mask_dice_loss': 0.5778600007295609, 'train/mask_loss': 0.7645850121974945, 'metrics/total_secs_per_batch': 6.755596160888672, 'metrics/data_secs_per_batch': 2.8946476936340333, '_timestamp': 1740972655.784146}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 116 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972655.7844265}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 117 is less than current step: 499. Dropping entry: {'train/loss': 1.6563339054584503, 'train/ce_loss': 0.261279296875, 'train/seg_cls_loss': 0.017608642578125, 'train/kl_loss': 0.3228515625, 'train/mask_bce_loss': 0.1570771207101643, 'train/mask_dice_loss': 0.5198202930390835, 'train/mask_loss': 0.6768974095582962, 'metrics/total_secs_per_batch': 6.308032512664795, 'metrics/data_secs_per_batch': 2.7906256437301638, '_timestamp': 1740972662.0924027}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 117 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972662.09277}).
Epoch: [6][119/500]	Time  5.653 ( 5.653)	Loss 1.7031 (1.4675)	CeLoss 1.7031 (0.8297)	SegCLSLoss 0.0000 (0.0075)	KLLoss 0.0000 (0.1818)	MaskLoss 0.0000 (0.3080)	MaskBCELoss 0.0000 (0.0418)	MaskDICELoss 0.0000 (0.2662)
[2025-03-02 21:31:13,939] [INFO] [logging.py:96:log_dist] [Rank 0] step=3120, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:31:13,945] [INFO] [timer.py:215:stop] epoch=0/micro_step=31200/global_step=3120, RunningAvgSamplesPerSec=1.430986488767215, CurrSamplesPerSec=1.6130816128985488, MemAllocated=31.26GB, MaxMemAllocated=37.23GB
Epoch: [6][120/500]	Time  6.201 ( 6.201)	Loss 1.9800 (1.6166)	CeLoss 0.1768 (0.3965)	SegCLSLoss 0.0225 (0.0136)	KLLoss 0.3574 (0.2893)	MaskLoss 0.8787 (0.5923)	MaskBCELoss 0.0149 (0.0581)	MaskDICELoss 0.8637 (0.5342)
Epoch: [6][121/500]	Time  6.010 ( 6.010)	Loss 0.7109 (1.2112)	CeLoss 0.7109 (0.4959)	SegCLSLoss 0.0000 (0.0067)	KLLoss 0.0000 (0.1451)	MaskLoss 0.0000 (0.3487)	MaskBCELoss 0.0000 (0.0600)	MaskDICELoss 0.0000 (0.2887)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 118 is less than current step: 499. Dropping entry: {'train/loss': 1.467464530467987, 'train/ce_loss': 0.8296875, 'train/seg_cls_loss': 0.007525634765625, 'train/kl_loss': 0.1818359375, 'train/mask_bce_loss': 0.041772221215069294, 'train/mask_dice_loss': 0.266178785264492, 'train/mask_loss': 0.3079509995877743, 'metrics/total_secs_per_batch': 5.653353214263916, 'metrics/data_secs_per_batch': 3.076620173454285, '_timestamp': 1740972667.7454896}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 118 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972667.7457576}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 119 is less than current step: 499. Dropping entry: {'train/loss': 1.6166029930114747, 'train/ce_loss': 0.396484375, 'train/seg_cls_loss': 0.01361083984375, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.058121775090694425, 'train/mask_dice_loss': 0.53416408598423, 'train/mask_loss': 0.5922858595848084, 'metrics/total_secs_per_batch': 6.200900077819824, 'metrics/data_secs_per_batch': 2.598453092575073, '_timestamp': 1740972673.9463375}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 119 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972673.946675}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 120 is less than current step: 499. Dropping entry: {'train/loss': 1.2111621022224426, 'train/ce_loss': 0.495947265625, 'train/seg_cls_loss': 0.00672607421875, 'train/kl_loss': 0.1451171875, 'train/mask_bce_loss': 0.059966300055384635, 'train/mask_dice_loss': 0.288705575466156, 'train/mask_loss': 0.3486718714237213, 'metrics/total_secs_per_batch': 6.009526014328003, 'metrics/data_secs_per_batch': 2.6833572149276734, '_timestamp': 1740972679.9559574}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 120 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972679.9562728}).
Epoch: [6][122/500]	Time  6.924 ( 6.924)	Loss 1.7504 (2.0707)	CeLoss 0.2188 (0.2187)	SegCLSLoss 0.0161 (0.0188)	KLLoss 0.3555 (0.3596)	MaskLoss 0.7443 (0.9033)	MaskBCELoss 0.0367 (0.1812)	MaskDICELoss 0.7076 (0.7221)
Epoch: [6][123/500]	Time  6.833 ( 6.833)	Loss 1.4856 (1.8051)	CeLoss 0.2676 (0.2515)	SegCLSLoss 0.0089 (0.0139)	KLLoss 0.3633 (0.3617)	MaskLoss 0.5885 (0.7552)	MaskBCELoss 0.0679 (0.1377)	MaskDICELoss 0.5206 (0.6175)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 121 is less than current step: 499. Dropping entry: {'train/loss': 2.0707014203071594, 'train/ce_loss': 0.21865234375, 'train/seg_cls_loss': 0.0188232421875, 'train/kl_loss': 0.3595703125, 'train/mask_bce_loss': 0.1811583997681737, 'train/mask_dice_loss': 0.7221122264862061, 'train/mask_loss': 0.9032706320285797, 'metrics/total_secs_per_batch': 6.92380690574646, 'metrics/data_secs_per_batch': 3.0672566175460814, '_timestamp': 1740972686.8797104}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 121 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972686.8799968}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 122 is less than current step: 499. Dropping entry: {'train/loss': 1.8051103949546814, 'train/ce_loss': 0.25146484375, 'train/seg_cls_loss': 0.013885498046875, 'train/kl_loss': 0.36171875, 'train/mask_bce_loss': 0.1377061450853944, 'train/mask_dice_loss': 0.6175345852971077, 'train/mask_loss': 0.7552407264709473, 'metrics/total_secs_per_batch': 6.833373785018921, 'metrics/data_secs_per_batch': 3.064449429512024, '_timestamp': 1740972693.7131097}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 122 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972693.713386}).
Epoch: [6][124/500]	Time  6.364 ( 6.364)	Loss 2.2804 (1.4840)	CeLoss 0.3008 (0.3977)	SegCLSLoss 0.0215 (0.0116)	KLLoss 0.3672 (0.2521)	MaskLoss 0.9654 (0.5274)	MaskBCELoss 0.5403 (0.1341)	MaskDICELoss 0.4251 (0.3933)
Epoch: [6][125/500]	Time  6.308 ( 6.308)	Loss 3.0544 (1.8642)	CeLoss 0.1738 (0.4683)	SegCLSLoss 0.0146 (0.0136)	KLLoss 0.3672 (0.2539)	MaskLoss 1.4188 (0.6819)	MaskBCELoss 0.5785 (0.1231)	MaskDICELoss 0.8403 (0.5588)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 123 is less than current step: 499. Dropping entry: {'train/loss': 1.4839858889579773, 'train/ce_loss': 0.397705078125, 'train/seg_cls_loss': 0.011572265625, 'train/kl_loss': 0.2521484375, 'train/mask_bce_loss': 0.13407323472201824, 'train/mask_dice_loss': 0.393344521522522, 'train/mask_loss': 0.5274177551269531, 'metrics/total_secs_per_batch': 6.363537788391113, 'metrics/data_secs_per_batch': 2.7026755571365357, '_timestamp': 1740972700.0768752}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 123 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972700.0773215}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 124 is less than current step: 499. Dropping entry: {'train/loss': 1.8641836285591125, 'train/ce_loss': 0.46826171875, 'train/seg_cls_loss': 0.013592529296875, 'train/kl_loss': 0.25390625, 'train/mask_bce_loss': 0.12314181588590145, 'train/mask_dice_loss': 0.5588035166263581, 'train/mask_loss': 0.6819453299045563, 'metrics/total_secs_per_batch': 6.307831048965454, 'metrics/data_secs_per_batch': 2.927255654335022, '_timestamp': 1740972706.3844757}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 124 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972706.3848114}).
Epoch: [6][126/500]	Time  6.478 ( 6.478)	Loss 0.3691 (1.8494)	CeLoss 0.3691 (0.2921)	SegCLSLoss 0.0000 (0.0156)	KLLoss 0.0000 (0.2943)	MaskLoss 0.0000 (0.7599)	MaskBCELoss 0.0000 (0.1769)	MaskDICELoss 0.0000 (0.5830)
Epoch: [6][127/500]	Time  5.431 ( 5.431)	Loss 0.9297 (1.4688)	CeLoss 0.9297 (0.3930)	SegCLSLoss 0.0000 (0.0104)	KLLoss 0.0000 (0.2164)	MaskLoss 0.0000 (0.5244)	MaskBCELoss 0.0000 (0.0714)	MaskDICELoss 0.0000 (0.4530)
Epoch: [6][128/500]	Time  6.056 ( 6.056)	Loss 1.8445 (1.6126)	CeLoss 0.2490 (0.3655)	SegCLSLoss 0.0168 (0.0132)	KLLoss 0.3809 (0.2918)	MaskLoss 0.7738 (0.6056)	MaskBCELoss 0.0120 (0.1086)	MaskDICELoss 0.7618 (0.4971)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 125 is less than current step: 499. Dropping entry: {'train/loss': 1.8494078159332275, 'train/ce_loss': 0.292138671875, 'train/seg_cls_loss': 0.015618896484375, 'train/kl_loss': 0.2943359375, 'train/mask_bce_loss': 0.17693458078429103, 'train/mask_dice_loss': 0.5829743981361389, 'train/mask_loss': 0.7599089801311493, 'metrics/total_secs_per_batch': 6.477966785430908, 'metrics/data_secs_per_batch': 3.109337902069092, '_timestamp': 1740972712.862448}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 125 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972712.8627098}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 126 is less than current step: 499. Dropping entry: {'train/loss': 1.4687811493873597, 'train/ce_loss': 0.39296875, 'train/seg_cls_loss': 0.01038818359375, 'train/kl_loss': 0.21640625, 'train/mask_bce_loss': 0.07135708015412093, 'train/mask_dice_loss': 0.4530237436294556, 'train/mask_loss': 0.5243808209896088, 'metrics/total_secs_per_batch': 5.431394100189209, 'metrics/data_secs_per_batch': 2.6169676542282105, '_timestamp': 1740972718.2939408}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 126 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972718.2942545}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 127 is less than current step: 499. Dropping entry: {'train/loss': 1.6126178085803986, 'train/ce_loss': 0.36552734375, 'train/seg_cls_loss': 0.013165283203125, 'train/kl_loss': 0.291796875, 'train/mask_bce_loss': 0.10855082208290696, 'train/mask_dice_loss': 0.49707449078559873, 'train/mask_loss': 0.6056253165006638, 'metrics/total_secs_per_batch': 6.055782079696655, 'metrics/data_secs_per_batch': 2.3342642068862913, '_timestamp': 1740972724.3497891}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 127 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972724.3501086}).
Epoch: [6][129/500]	Time  7.044 ( 7.044)	Loss 2.0903 (1.5099)	CeLoss 0.3398 (0.2922)	SegCLSLoss 0.0114 (0.0150)	KLLoss 0.3652 (0.2939)	MaskLoss 0.8538 (0.5905)	MaskBCELoss 0.1294 (0.1473)	MaskDICELoss 0.7244 (0.4432)
[2025-03-02 21:32:16,946] [INFO] [logging.py:96:log_dist] [Rank 0] step=3130, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:32:16,951] [INFO] [timer.py:215:stop] epoch=0/micro_step=31300/global_step=3130, RunningAvgSamplesPerSec=1.431437634813392, CurrSamplesPerSec=1.7998017958523025, MemAllocated=30.75GB, MaxMemAllocated=37.23GB
Epoch: [6][130/500]	Time  5.558 ( 5.558)	Loss 1.0703 (1.8080)	CeLoss 1.0703 (0.5521)	SegCLSLoss 0.0000 (0.0106)	KLLoss 0.0000 (0.2525)	MaskLoss 0.0000 (0.6128)	MaskBCELoss 0.0000 (0.1453)	MaskDICELoss 0.0000 (0.4675)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 128 is less than current step: 499. Dropping entry: {'train/loss': 1.509893798828125, 'train/ce_loss': 0.2921875, 'train/seg_cls_loss': 0.015032958984375, 'train/kl_loss': 0.2939453125, 'train/mask_bce_loss': 0.14732757350429893, 'train/mask_dice_loss': 0.4431662082672119, 'train/mask_loss': 0.5904937803745269, 'metrics/total_secs_per_batch': 7.0444746017456055, 'metrics/data_secs_per_batch': 3.079433012008667, '_timestamp': 1740972731.3941443}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 128 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972731.3944886}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 129 is less than current step: 499. Dropping entry: {'train/loss': 1.8080109119415284, 'train/ce_loss': 0.55205078125, 'train/seg_cls_loss': 0.01058349609375, 'train/kl_loss': 0.2525390625, 'train/mask_bce_loss': 0.14525690916925668, 'train/mask_dice_loss': 0.4675375998020172, 'train/mask_loss': 0.6127945005893707, 'metrics/total_secs_per_batch': 5.558038949966431, 'metrics/data_secs_per_batch': 2.7709502458572386, '_timestamp': 1740972736.9520519}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 129 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972736.9524145}).
Epoch: [6][131/500]	Time  6.751 ( 6.751)	Loss 0.5957 (1.4760)	CeLoss 0.2451 (0.2099)	SegCLSLoss 0.0195 (0.0159)	KLLoss 0.3633 (0.3275)	MaskLoss 0.1524 (0.6128)	MaskBCELoss 0.0189 (0.1494)	MaskDICELoss 0.1335 (0.4634)
Epoch: [6][132/500]	Time  5.710 ( 5.710)	Loss 1.7869 (1.3357)	CeLoss 0.2578 (0.3873)	SegCLSLoss 0.0143 (0.0142)	KLLoss 0.3555 (0.2906)	MaskLoss 0.7431 (0.4561)	MaskBCELoss 0.0346 (0.0701)	MaskDICELoss 0.7084 (0.3860)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 130 is less than current step: 499. Dropping entry: {'train/loss': 1.476008129119873, 'train/ce_loss': 0.20986328125, 'train/seg_cls_loss': 0.015869140625, 'train/kl_loss': 0.3275390625, 'train/mask_bce_loss': 0.14940028190612792, 'train/mask_dice_loss': 0.4633596420288086, 'train/mask_loss': 0.612759929895401, 'metrics/total_secs_per_batch': 6.750943660736084, 'metrics/data_secs_per_batch': 3.1918387413024902, '_timestamp': 1740972743.7032611}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 130 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972743.7034943}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 131 is less than current step: 499. Dropping entry: {'train/loss': 1.335727608203888, 'train/ce_loss': 0.3873046875, 'train/seg_cls_loss': 0.014215087890625, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.07010812144726515, 'train/mask_dice_loss': 0.38603693842887876, 'train/mask_loss': 0.4561450555920601, 'metrics/total_secs_per_batch': 5.710434198379517, 'metrics/data_secs_per_batch': 2.259332871437073, '_timestamp': 1740972749.413592}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 131 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972749.4138744}).
Epoch: [6][133/500]	Time  6.986 ( 6.986)	Loss 1.1094 (1.4132)	CeLoss 1.1094 (0.3341)	SegCLSLoss 0.0000 (0.0119)	KLLoss 0.0000 (0.2521)	MaskLoss 0.0000 (0.5239)	MaskBCELoss 0.0000 (0.0732)	MaskDICELoss 0.0000 (0.4506)
Epoch: [6][134/500]	Time  6.241 ( 6.241)	Loss 2.4638 (1.7911)	CeLoss 0.1475 (0.1846)	SegCLSLoss 0.0249 (0.0172)	KLLoss 0.3730 (0.3311)	MaskLoss 1.1333 (0.7823)	MaskBCELoss 0.2124 (0.1783)	MaskDICELoss 0.9209 (0.6040)
Epoch: [6][135/500]	Time  5.821 ( 5.821)	Loss 2.3001 (1.3900)	CeLoss 0.2314 (0.4170)	SegCLSLoss 0.0095 (0.0091)	KLLoss 0.3613 (0.2182)	MaskLoss 1.0133 (0.4732)	MaskBCELoss 0.1651 (0.0987)	MaskDICELoss 0.8482 (0.3745)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 132 is less than current step: 499. Dropping entry: {'train/loss': 1.4131550908088684, 'train/ce_loss': 0.33408203125, 'train/seg_cls_loss': 0.0119384765625, 'train/kl_loss': 0.2521484375, 'train/mask_bce_loss': 0.07323428057134151, 'train/mask_dice_loss': 0.4506284147500992, 'train/mask_loss': 0.5238626956939697, 'metrics/total_secs_per_batch': 6.985780477523804, 'metrics/data_secs_per_batch': 3.2166796207427977, '_timestamp': 1740972756.3994002}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 132 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972756.3997414}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 133 is less than current step: 499. Dropping entry: {'train/loss': 1.7911131262779236, 'train/ce_loss': 0.1845703125, 'train/seg_cls_loss': 0.01722412109375, 'train/kl_loss': 0.3310546875, 'train/mask_bce_loss': 0.17831542640924453, 'train/mask_dice_loss': 0.6039598852396011, 'train/mask_loss': 0.7822753190994263, 'metrics/total_secs_per_batch': 6.240537166595459, 'metrics/data_secs_per_batch': 2.9303104639053346, '_timestamp': 1740972762.6399028}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 133 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972762.6401808}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 134 is less than current step: 499. Dropping entry: {'train/loss': 1.390018677711487, 'train/ce_loss': 0.4169921875, 'train/seg_cls_loss': 0.00914306640625, 'train/kl_loss': 0.2181640625, 'train/mask_bce_loss': 0.09872260130941868, 'train/mask_dice_loss': 0.37446057200431826, 'train/mask_loss': 0.4731831669807434, 'metrics/total_secs_per_batch': 5.820529937744141, 'metrics/data_secs_per_batch': 2.8468472957611084, '_timestamp': 1740972768.4605107}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 134 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972768.460836}).
Epoch: [6][136/500]	Time  5.491 ( 5.491)	Loss 2.5343 (1.5965)	CeLoss 0.1270 (0.6607)	SegCLSLoss 0.0278 (0.0099)	KLLoss 0.3730 (0.2219)	MaskLoss 1.1783 (0.4544)	MaskBCELoss 0.3515 (0.1022)	MaskDICELoss 0.8267 (0.3522)
Epoch: [6][137/500]	Time  6.698 ( 6.698)	Loss 1.9065 (1.8123)	CeLoss 0.2021 (0.2207)	SegCLSLoss 0.0189 (0.0185)	KLLoss 0.3496 (0.3592)	MaskLoss 0.8297 (0.7731)	MaskBCELoss 0.0259 (0.1334)	MaskDICELoss 0.8038 (0.6397)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 135 is less than current step: 499. Dropping entry: {'train/loss': 1.5965115070343017, 'train/ce_loss': 0.6607421875, 'train/seg_cls_loss': 0.00994873046875, 'train/kl_loss': 0.221875, 'train/mask_bce_loss': 0.10220658145844937, 'train/mask_dice_loss': 0.35220152139663696, 'train/mask_loss': 0.4544081032276154, 'metrics/total_secs_per_batch': 5.490973472595215, 'metrics/data_secs_per_batch': 2.338284230232239, '_timestamp': 1740972773.9514754}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 135 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972773.9518723}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 136 is less than current step: 499. Dropping entry: {'train/loss': 1.8123164653778077, 'train/ce_loss': 0.220703125, 'train/seg_cls_loss': 0.018548583984375, 'train/kl_loss': 0.3591796875, 'train/mask_bce_loss': 0.13339453949593008, 'train/mask_dice_loss': 0.6397070288658142, 'train/mask_loss': 0.7731015687808395, 'metrics/total_secs_per_batch': 6.698104619979858, 'metrics/data_secs_per_batch': 3.0547857761383055, '_timestamp': 1740972780.6496356}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 136 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972780.65001}).
Epoch: [6][138/500]	Time  6.605 ( 6.605)	Loss 2.5342 (1.7275)	CeLoss 0.2324 (0.3321)	SegCLSLoss 0.0176 (0.0124)	KLLoss 0.3633 (0.2893)	MaskLoss 1.1284 (0.6800)	MaskBCELoss 0.3532 (0.1267)	MaskDICELoss 0.7752 (0.5532)
Epoch: [6][139/500]	Time  7.591 ( 7.591)	Loss 2.8136 (1.5054)	CeLoss 0.2129 (0.2317)	SegCLSLoss 0.0171 (0.0119)	KLLoss 0.3613 (0.2535)	MaskLoss 1.2779 (0.6212)	MaskBCELoss 0.3301 (0.1481)	MaskDICELoss 0.9478 (0.4731)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 137 is less than current step: 499. Dropping entry: {'train/loss': 1.7274701476097107, 'train/ce_loss': 0.3321044921875, 'train/seg_cls_loss': 0.01239013671875, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.12671688590198754, 'train/mask_dice_loss': 0.5532413363456726, 'train/mask_loss': 0.6799582362174987, 'metrics/total_secs_per_batch': 6.604552745819092, 'metrics/data_secs_per_batch': 3.2404189109802246, '_timestamp': 1740972787.254193}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 137 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972787.2545357}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 138 is less than current step: 499. Dropping entry: {'train/loss': 1.505388116836548, 'train/ce_loss': 0.231689453125, 'train/seg_cls_loss': 0.011871337890625, 'train/kl_loss': 0.253515625, 'train/mask_bce_loss': 0.14811087250709534, 'train/mask_dice_loss': 0.4730646342039108, 'train/mask_loss': 0.6211755096912384, 'metrics/total_secs_per_batch': 7.5913732051849365, 'metrics/data_secs_per_batch': 3.0764807224273683, '_timestamp': 1740972794.8454618}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 138 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972794.8457644}).
[2025-03-02 21:33:21,259] [INFO] [logging.py:96:log_dist] [Rank 0] step=3140, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:33:21,263] [INFO] [timer.py:215:stop] epoch=0/micro_step=31400/global_step=3140, RunningAvgSamplesPerSec=1.4318011906840924, CurrSamplesPerSec=1.5583265469551915, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [6][140/500]	Time  6.419 ( 6.419)	Loss 1.5503 (1.5191)	CeLoss 0.2188 (0.4978)	SegCLSLoss 0.0107 (0.0132)	KLLoss 0.3633 (0.2539)	MaskLoss 0.6443 (0.4946)	MaskBCELoss 0.1237 (0.0562)	MaskDICELoss 0.5206 (0.4385)
Epoch: [6][141/500]	Time  6.468 ( 6.468)	Loss 1.9437 (1.5187)	CeLoss 0.2324 (0.3498)	SegCLSLoss 0.0117 (0.0103)	KLLoss 0.3672 (0.2557)	MaskLoss 0.8341 (0.5691)	MaskBCELoss 0.1247 (0.1496)	MaskDICELoss 0.7094 (0.4195)
Epoch: [6][142/500]	Time  6.183 ( 6.183)	Loss 0.6328 (1.6235)	CeLoss 0.6328 (0.4082)	SegCLSLoss 0.0000 (0.0115)	KLLoss 0.0000 (0.2514)	MaskLoss 0.0000 (0.5922)	MaskBCELoss 0.0000 (0.0916)	MaskDICELoss 0.0000 (0.5006)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 139 is less than current step: 499. Dropping entry: {'train/loss': 1.519055140018463, 'train/ce_loss': 0.49775390625, 'train/seg_cls_loss': 0.013226318359375, 'train/kl_loss': 0.25390625, 'train/mask_bce_loss': 0.0561820711940527, 'train/mask_dice_loss': 0.43845292627811433, 'train/mask_loss': 0.49463499784469606, 'metrics/total_secs_per_batch': 6.419004917144775, 'metrics/data_secs_per_batch': 2.5839120149612427, '_timestamp': 1740972801.2644277}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 139 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972801.2648044}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 140 is less than current step: 499. Dropping entry: {'train/loss': 1.5186968922615052, 'train/ce_loss': 0.349755859375, 'train/seg_cls_loss': 0.01026611328125, 'train/kl_loss': 0.2556640625, 'train/mask_bce_loss': 0.1496050449088216, 'train/mask_dice_loss': 0.41953345388174057, 'train/mask_loss': 0.5691384911537171, 'metrics/total_secs_per_batch': 6.467877388000488, 'metrics/data_secs_per_batch': 2.7250540971755983, '_timestamp': 1740972807.7323027}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 140 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972807.732648}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 141 is less than current step: 499. Dropping entry: {'train/loss': 1.6234919905662537, 'train/ce_loss': 0.408203125, 'train/seg_cls_loss': 0.011505126953125, 'train/kl_loss': 0.2513671875, 'train/mask_bce_loss': 0.09162413361482322, 'train/mask_dice_loss': 0.5005906254053116, 'train/mask_loss': 0.5922147572040558, 'metrics/total_secs_per_batch': 6.183205604553223, 'metrics/data_secs_per_batch': 3.007970905303955, '_timestamp': 1740972813.915734}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 141 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972813.9161239}).
Epoch: [6][143/500]	Time  5.042 ( 5.042)	Loss 0.9965 (1.3541)	CeLoss 0.2227 (0.4902)	SegCLSLoss 0.0104 (0.0096)	KLLoss 0.3672 (0.2178)	MaskLoss 0.3664 (0.4188)	MaskBCELoss 0.0725 (0.0808)	MaskDICELoss 0.2939 (0.3380)
Epoch: [6][144/500]	Time  6.228 ( 6.228)	Loss 1.4766 (1.5924)	CeLoss 1.4766 (0.4303)	SegCLSLoss 0.0000 (0.0110)	KLLoss 0.0000 (0.2213)	MaskLoss 0.0000 (0.5671)	MaskBCELoss 0.0000 (0.1429)	MaskDICELoss 0.0000 (0.4242)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 142 is less than current step: 499. Dropping entry: {'train/loss': 1.3541120529174804, 'train/ce_loss': 0.490234375, 'train/seg_cls_loss': 0.009588623046875, 'train/kl_loss': 0.2177734375, 'train/mask_bce_loss': 0.08076349096372723, 'train/mask_dice_loss': 0.33799174427986145, 'train/mask_loss': 0.4187552332878113, 'metrics/total_secs_per_batch': 5.0415050983428955, 'metrics/data_secs_per_batch': 2.2900096654891966, '_timestamp': 1740972818.9570482}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 142 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972818.9572513}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 143 is less than current step: 499. Dropping entry: {'train/loss': 1.592358660697937, 'train/ce_loss': 0.430322265625, 'train/seg_cls_loss': 0.010986328125, 'train/kl_loss': 0.2212890625, 'train/mask_bce_loss': 0.1428729247301817, 'train/mask_dice_loss': 0.4241804122924805, 'train/mask_loss': 0.5670533537864685, 'metrics/total_secs_per_batch': 6.227826356887817, 'metrics/data_secs_per_batch': 2.989688348770142, '_timestamp': 1740972825.184838}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 143 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972825.1851075}).
Epoch: [6][145/500]	Time  7.034 ( 7.034)	Loss 2.1729 (1.7249)	CeLoss 0.2041 (0.3378)	SegCLSLoss 0.0137 (0.0156)	KLLoss 0.3574 (0.3236)	MaskLoss 0.9634 (0.6735)	MaskBCELoss 0.0210 (0.0516)	MaskDICELoss 0.9424 (0.6219)
Epoch: [6][146/500]	Time  6.902 ( 6.902)	Loss 0.2002 (1.9670)	CeLoss 0.2002 (0.2082)	SegCLSLoss 0.0000 (0.0165)	KLLoss 0.0000 (0.3244)	MaskLoss 0.0000 (0.8591)	MaskBCELoss 0.0000 (0.1791)	MaskDICELoss 0.0000 (0.6801)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 144 is less than current step: 499. Dropping entry: {'train/loss': 1.7248987436294556, 'train/ce_loss': 0.33779296875, 'train/seg_cls_loss': 0.0156494140625, 'train/kl_loss': 0.3236328125, 'train/mask_bce_loss': 0.05155738983303308, 'train/mask_dice_loss': 0.6219271391630172, 'train/mask_loss': 0.6734845280647278, 'metrics/total_secs_per_batch': 7.033952951431274, 'metrics/data_secs_per_batch': 3.0587093114852903, '_timestamp': 1740972832.2190237}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 144 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972832.2193975}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 145 is less than current step: 499. Dropping entry: {'train/loss': 1.9669875502586365, 'train/ce_loss': 0.208203125, 'train/seg_cls_loss': 0.016497802734375, 'train/kl_loss': 0.3244140625, 'train/mask_bce_loss': 0.17907407991588115, 'train/mask_dice_loss': 0.6800544440746308, 'train/mask_loss': 0.859128525853157, 'metrics/total_secs_per_batch': 6.90186071395874, 'metrics/data_secs_per_batch': 3.0288028478622437, '_timestamp': 1740972839.1209116}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 145 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972839.1212747}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 146 is less than current step: 499. Dropping entry: {'train/loss': 1.546992403268814, 'train/ce_loss': 0.33466796875, 'train/seg_cls_loss': 0.013812255859375, 'train/kl_loss': 0.328125, 'train/mask_bce_loss': 0.15426094755530356, 'train/mask_dice_loss': 0.431881732493639, 'train/mask_loss': 0.5861426830291748, 'metrics/total_secs_per_batch': 6.488265037536621, 'metrics/data_secs_per_batch': 2.777176761627197, '_timestamp': 1740972845.6089425}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 146 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972845.6092203}).
Epoch: [6][147/500]	Time  6.488 ( 6.488)	Loss 1.0546 (1.5470)	CeLoss 0.2061 (0.3347)	SegCLSLoss 0.0172 (0.0138)	KLLoss 0.3652 (0.3281)	MaskLoss 0.4013 (0.5861)	MaskBCELoss 0.0256 (0.1543)	MaskDICELoss 0.3758 (0.4319)
Epoch: [6][148/500]	Time  6.582 ( 6.582)	Loss 1.5875 (1.8832)	CeLoss 0.2119 (0.2490)	SegCLSLoss 0.0130 (0.0171)	KLLoss 0.3633 (0.3664)	MaskLoss 0.6658 (0.7942)	MaskBCELoss 0.1959 (0.2335)	MaskDICELoss 0.4699 (0.5607)
Epoch: [6][149/500]	Time  6.313 ( 6.313)	Loss 1.4297 (1.9500)	CeLoss 1.4297 (0.3267)	SegCLSLoss 0.0000 (0.0165)	KLLoss 0.0000 (0.3260)	MaskLoss 0.0000 (0.7913)	MaskBCELoss 0.0000 (0.1042)	MaskDICELoss 0.0000 (0.6871)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 147 is less than current step: 499. Dropping entry: {'train/loss': 1.8831680834293365, 'train/ce_loss': 0.2490234375, 'train/seg_cls_loss': 0.017071533203125, 'train/kl_loss': 0.36640625, 'train/mask_bce_loss': 0.23351707868278027, 'train/mask_dice_loss': 0.560654842108488, 'train/mask_loss': 0.7941719308495522, 'metrics/total_secs_per_batch': 6.5820207595825195, 'metrics/data_secs_per_batch': 2.772444653511047, '_timestamp': 1740972852.1909826}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 147 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972852.1911724}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 148 is less than current step: 499. Dropping entry: {'train/loss': 1.9500469744205475, 'train/ce_loss': 0.32666015625, 'train/seg_cls_loss': 0.0164794921875, 'train/kl_loss': 0.3259765625, 'train/mask_bce_loss': 0.10418511815369129, 'train/mask_dice_loss': 0.6871469542384148, 'train/mask_loss': 0.7913320690393448, 'metrics/total_secs_per_batch': 6.313478708267212, 'metrics/data_secs_per_batch': 2.7958433389663697, '_timestamp': 1740972858.5044575}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 148 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972858.5047238}).
[2025-03-02 21:34:25,057] [INFO] [logging.py:96:log_dist] [Rank 0] step=3150, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:34:25,063] [INFO] [timer.py:215:stop] epoch=0/micro_step=31500/global_step=3150, RunningAvgSamplesPerSec=1.4321956263231392, CurrSamplesPerSec=1.5247262943875308, MemAllocated=30.73GB, MaxMemAllocated=37.23GB
Epoch: [6][150/500]	Time  6.560 ( 6.560)	Loss 1.6328 (1.7407)	CeLoss 1.6328 (0.3757)	SegCLSLoss 0.0000 (0.0152)	KLLoss 0.0000 (0.3250)	MaskLoss 0.0000 (0.6624)	MaskBCELoss 0.0000 (0.1346)	MaskDICELoss 0.0000 (0.5278)
Epoch: [6][151/500]	Time  6.575 ( 6.575)	Loss 1.7854 (1.7961)	CeLoss 0.2363 (0.3289)	SegCLSLoss 0.0223 (0.0139)	KLLoss 0.3613 (0.2936)	MaskLoss 0.7511 (0.7156)	MaskBCELoss 0.0315 (0.1265)	MaskDICELoss 0.7196 (0.5892)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 149 is less than current step: 499. Dropping entry: {'train/loss': 1.740710973739624, 'train/ce_loss': 0.37568359375, 'train/seg_cls_loss': 0.01519775390625, 'train/kl_loss': 0.325, 'train/mask_bce_loss': 0.13462706157006324, 'train/mask_dice_loss': 0.5278182759881019, 'train/mask_loss': 0.6624453365802765, 'metrics/total_secs_per_batch': 6.560149908065796, 'metrics/data_secs_per_batch': 3.096862030029297, '_timestamp': 1740972865.0644035}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 149 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972865.0646753}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 150 is less than current step: 499. Dropping entry: {'train/loss': 1.7960801005363465, 'train/ce_loss': 0.328857421875, 'train/seg_cls_loss': 0.013922119140625, 'train/kl_loss': 0.2935546875, 'train/mask_bce_loss': 0.1264821307733655, 'train/mask_dice_loss': 0.5891604542732238, 'train/mask_loss': 0.7156425893306733, 'metrics/total_secs_per_batch': 6.5745673179626465, 'metrics/data_secs_per_batch': 2.932298493385315, '_timestamp': 1740972871.6393974}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 150 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972871.6397493}).
Epoch: [6][152/500]	Time  5.282 ( 5.282)	Loss 1.1016 (1.5556)	CeLoss 1.1016 (0.5857)	SegCLSLoss 0.0000 (0.0089)	KLLoss 0.0000 (0.1793)	MaskLoss 0.0000 (0.4736)	MaskBCELoss 0.0000 (0.0723)	MaskDICELoss 0.0000 (0.4013)
Epoch: [6][153/500]	Time  5.463 ( 5.463)	Loss 1.9609 (1.6274)	CeLoss 1.9609 (0.6761)	SegCLSLoss 0.0000 (0.0091)	KLLoss 0.0000 (0.1840)	MaskLoss 0.0000 (0.4641)	MaskBCELoss 0.0000 (0.1548)	MaskDICELoss 0.0000 (0.3092)
Epoch: [6][154/500]	Time  5.712 ( 5.712)	Loss 0.7978 (1.5553)	CeLoss 0.2109 (0.5668)	SegCLSLoss 0.0284 (0.0115)	KLLoss 0.3555 (0.2533)	MaskLoss 0.2680 (0.4784)	MaskBCELoss 0.0540 (0.1086)	MaskDICELoss 0.2140 (0.3699)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 151 is less than current step: 499. Dropping entry: {'train/loss': 1.555553638935089, 'train/ce_loss': 0.5857421875, 'train/seg_cls_loss': 0.00892333984375, 'train/kl_loss': 0.179296875, 'train/mask_bce_loss': 0.07234032656997443, 'train/mask_dice_loss': 0.401286119222641, 'train/mask_loss': 0.47362643480300903, 'metrics/total_secs_per_batch': 5.282433986663818, 'metrics/data_secs_per_batch': 2.1252994537353516, '_timestamp': 1740972876.9216313}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 151 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972876.9219117}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 152 is less than current step: 499. Dropping entry: {'train/loss': 1.6273598313331603, 'train/ce_loss': 0.67607421875, 'train/seg_cls_loss': 0.00911865234375, 'train/kl_loss': 0.183984375, 'train/mask_bce_loss': 0.15482480227947235, 'train/mask_dice_loss': 0.30924572646617887, 'train/mask_loss': 0.4640705347061157, 'metrics/total_secs_per_batch': 5.463033437728882, 'metrics/data_secs_per_batch': 2.8467576026916506, '_timestamp': 1740972882.3847136}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 152 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972882.3850012}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 153 is less than current step: 499. Dropping entry: {'train/loss': 1.5553313791751862, 'train/ce_loss': 0.566796875, 'train/seg_cls_loss': 0.011468505859375, 'train/kl_loss': 0.2533203125, 'train/mask_bce_loss': 0.10856151208281517, 'train/mask_dice_loss': 0.3698854178190231, 'train/mask_loss': 0.47844693064689636, 'metrics/total_secs_per_batch': 5.7115278244018555, 'metrics/data_secs_per_batch': 3.096298432350159, '_timestamp': 1740972888.0963721}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 153 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972888.0967212}).
Epoch: [6][155/500]	Time  6.683 ( 6.683)	Loss 2.6708 (1.9863)	CeLoss 0.2139 (0.2295)	SegCLSLoss 0.0120 (0.0150)	KLLoss 0.3633 (0.3264)	MaskLoss 1.2075 (0.8586)	MaskBCELoss 0.5403 (0.1759)	MaskDICELoss 0.6671 (0.6827)
Epoch: [6][156/500]	Time  6.332 ( 6.332)	Loss 1.6798 (1.6911)	CeLoss 0.2324 (0.2746)	SegCLSLoss 0.0168 (0.0179)	KLLoss 0.3555 (0.3248)	MaskLoss 0.7012 (0.6876)	MaskBCELoss 0.0712 (0.0755)	MaskDICELoss 0.6301 (0.6121)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 154 is less than current step: 499. Dropping entry: {'train/loss': 1.9862964868545532, 'train/ce_loss': 0.2294921875, 'train/seg_cls_loss': 0.01495361328125, 'train/kl_loss': 0.3263671875, 'train/mask_bce_loss': 0.1758864130359143, 'train/mask_dice_loss': 0.6826915264129638, 'train/mask_loss': 0.8585779339075088, 'metrics/total_secs_per_batch': 6.682585000991821, 'metrics/data_secs_per_batch': 2.922981333732605, '_timestamp': 1740972894.7787874}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 154 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972894.7790828}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 155 is less than current step: 499. Dropping entry: {'train/loss': 1.6911277651786805, 'train/ce_loss': 0.274609375, 'train/seg_cls_loss': 0.017901611328125, 'train/kl_loss': 0.3248046875, 'train/mask_bce_loss': 0.07546622585505247, 'train/mask_dice_loss': 0.6120898485183716, 'train/mask_loss': 0.6875560700893402, 'metrics/total_secs_per_batch': 6.332334995269775, 'metrics/data_secs_per_batch': 2.54332857131958, '_timestamp': 1740972901.11113}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 155 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972901.111428}).
Epoch: [6][157/500]	Time  5.471 ( 5.471)	Loss 1.4045 (1.7798)	CeLoss 0.2090 (0.6798)	SegCLSLoss 0.0164 (0.0091)	KLLoss 0.3574 (0.2168)	MaskLoss 0.5763 (0.5369)	MaskBCELoss 0.0170 (0.0809)	MaskDICELoss 0.5593 (0.4560)
Epoch: [6][158/500]	Time  5.772 ( 5.772)	Loss 0.0977 (1.4972)	CeLoss 0.0977 (0.3561)	SegCLSLoss 0.0000 (0.0110)	KLLoss 0.0000 (0.2514)	MaskLoss 0.0000 (0.5554)	MaskBCELoss 0.0000 (0.0909)	MaskDICELoss 0.0000 (0.4645)
Epoch: [6][159/500]	Time  5.894 ( 5.894)	Loss 1.3740 (1.4762)	CeLoss 0.2432 (0.4386)	SegCLSLoss 0.0151 (0.0117)	KLLoss 0.3652 (0.2207)	MaskLoss 0.5435 (0.5049)	MaskBCELoss 0.0186 (0.1083)	MaskDICELoss 0.5249 (0.3966)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 156 is less than current step: 499. Dropping entry: {'train/loss': 1.7798424124717713, 'train/ce_loss': 0.67978515625, 'train/seg_cls_loss': 0.009063720703125, 'train/kl_loss': 0.216796875, 'train/mask_bce_loss': 0.08086044304072856, 'train/mask_dice_loss': 0.45603341460227964, 'train/mask_loss': 0.5368938624858857, 'metrics/total_secs_per_batch': 5.470513582229614, 'metrics/data_secs_per_batch': 2.483642101287842, '_timestamp': 1740972906.5816278}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 156 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972906.5819137}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 157 is less than current step: 499. Dropping entry: {'train/loss': 1.497210955619812, 'train/ce_loss': 0.3560546875, 'train/seg_cls_loss': 0.01104736328125, 'train/kl_loss': 0.2513671875, 'train/mask_bce_loss': 0.0909179611131549, 'train/mask_dice_loss': 0.464523458480835, 'train/mask_loss': 0.555441427230835, 'metrics/total_secs_per_batch': 5.7723047733306885, 'metrics/data_secs_per_batch': 2.53789381980896, '_timestamp': 1740972912.354133}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 157 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972912.3544745}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 158 is less than current step: 499. Dropping entry: {'train/loss': 1.4762425780296327, 'train/ce_loss': 0.438623046875, 'train/seg_cls_loss': 0.011724853515625, 'train/kl_loss': 0.220703125, 'train/mask_bce_loss': 0.10833677500486374, 'train/mask_dice_loss': 0.3965569704771042, 'train/mask_loss': 0.5048937499523163, 'metrics/total_secs_per_batch': 5.89384126663208, 'metrics/data_secs_per_batch': 2.687558150291443, '_timestamp': 1740972918.2478352}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 158 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972918.248123}).
[2025-03-02 21:35:25,298] [INFO] [logging.py:96:log_dist] [Rank 0] step=3160, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:35:25,304] [INFO] [timer.py:215:stop] epoch=0/micro_step=31600/global_step=3160, RunningAvgSamplesPerSec=1.4328191399628225, CurrSamplesPerSec=1.4173252240091818, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [6][160/500]	Time  7.057 ( 7.057)	Loss 1.6341 (1.3899)	CeLoss 0.2695 (0.3777)	SegCLSLoss 0.0134 (0.0129)	KLLoss 0.3633 (0.2891)	MaskLoss 0.6608 (0.4884)	MaskBCELoss 0.1531 (0.0573)	MaskDICELoss 0.5077 (0.4312)
Epoch: [6][161/500]	Time  5.441 ( 5.441)	Loss 2.2887 (1.7729)	CeLoss 0.2617 (0.6721)	SegCLSLoss 0.0227 (0.0118)	KLLoss 0.3574 (0.2141)	MaskLoss 0.9900 (0.5368)	MaskBCELoss 0.0144 (0.0828)	MaskDICELoss 0.9757 (0.4540)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 159 is less than current step: 499. Dropping entry: {'train/loss': 1.3899397075176239, 'train/ce_loss': 0.377734375, 'train/seg_cls_loss': 0.012860107421875, 'train/kl_loss': 0.2890625, 'train/mask_bce_loss': 0.057252540998160836, 'train/mask_dice_loss': 0.43117434084415435, 'train/mask_loss': 0.4884268820285797, 'metrics/total_secs_per_batch': 7.057159185409546, 'metrics/data_secs_per_batch': 3.3202696800231934, '_timestamp': 1740972925.3047338}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 159 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972925.3049245}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 160 is less than current step: 499. Dropping entry: {'train/loss': 1.772855019569397, 'train/ce_loss': 0.6720703125, 'train/seg_cls_loss': 0.011773681640625, 'train/kl_loss': 0.2140625, 'train/mask_bce_loss': 0.08278996078297496, 'train/mask_dice_loss': 0.45397935807704926, 'train/mask_loss': 0.5367693126201629, 'metrics/total_secs_per_batch': 5.441024303436279, 'metrics/data_secs_per_batch': 2.7433414697647094, '_timestamp': 1740972930.7459688}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 160 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972930.7462425}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 161 is less than current step: 499. Dropping entry: {'train/loss': 1.3258901596069337, 'train/ce_loss': 0.52958984375, 'train/seg_cls_loss': 0.0079833984375, 'train/kl_loss': 0.18125, 'train/mask_bce_loss': 0.03168158708140254, 'train/mask_dice_loss': 0.3554334118962288, 'train/mask_loss': 0.3871150016784668, 'metrics/total_secs_per_batch': 5.123695135116577, 'metrics/data_secs_per_batch': 2.537673234939575, '_timestamp': 1740972935.8696942}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 161 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972935.8699944}).
Epoch: [6][162/500]	Time  5.124 ( 5.124)	Loss 1.0625 (1.3259)	CeLoss 1.0625 (0.5296)	SegCLSLoss 0.0000 (0.0080)	KLLoss 0.0000 (0.1812)	MaskLoss 0.0000 (0.3871)	MaskBCELoss 0.0000 (0.0317)	MaskDICELoss 0.0000 (0.3554)
Epoch: [6][163/500]	Time  5.525 ( 5.525)	Loss 2.6621 (1.8243)	CeLoss 0.2266 (0.3168)	SegCLSLoss 0.0177 (0.0175)	KLLoss 0.3633 (0.3238)	MaskLoss 1.1953 (0.7331)	MaskBCELoss 0.4804 (0.1076)	MaskDICELoss 0.7149 (0.6255)
Epoch: [6][164/500]	Time  6.450 ( 6.450)	Loss 0.9844 (1.6816)	CeLoss 0.9844 (0.3959)	SegCLSLoss 0.0000 (0.0155)	KLLoss 0.0000 (0.2943)	MaskLoss 0.0000 (0.6242)	MaskBCELoss 0.0000 (0.1250)	MaskDICELoss 0.0000 (0.4992)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 162 is less than current step: 499. Dropping entry: {'train/loss': 1.824277514219284, 'train/ce_loss': 0.316796875, 'train/seg_cls_loss': 0.017523193359375, 'train/kl_loss': 0.323828125, 'train/mask_bce_loss': 0.10762994308024645, 'train/mask_dice_loss': 0.6255049198865891, 'train/mask_loss': 0.7331348627805709, 'metrics/total_secs_per_batch': 5.524644136428833, 'metrics/data_secs_per_batch': 2.346798896789551, '_timestamp': 1740972941.3944185}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 162 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972941.3947277}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 163 is less than current step: 499. Dropping entry: {'train/loss': 1.6816180229187012, 'train/ce_loss': 0.3958984375, 'train/seg_cls_loss': 0.015509033203125, 'train/kl_loss': 0.2943359375, 'train/mask_bce_loss': 0.12502936087548733, 'train/mask_dice_loss': 0.4991781011223793, 'train/mask_loss': 0.6242074489593505, 'metrics/total_secs_per_batch': 6.450117826461792, 'metrics/data_secs_per_batch': 3.2969363927841187, '_timestamp': 1740972947.8446972}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 163 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972947.845121}).
Epoch: [6][165/500]	Time  5.865 ( 5.865)	Loss 2.4154 (1.6144)	CeLoss 0.1826 (0.4029)	SegCLSLoss 0.0208 (0.0146)	KLLoss 0.3730 (0.2895)	MaskLoss 1.0925 (0.5877)	MaskBCELoss 0.2253 (0.1069)	MaskDICELoss 0.8672 (0.4808)
Epoch: [6][166/500]	Time  6.258 ( 6.258)	Loss 1.2656 (1.6463)	CeLoss 1.2656 (0.3353)	SegCLSLoss 0.0000 (0.0152)	KLLoss 0.0000 (0.3252)	MaskLoss 0.0000 (0.6354)	MaskBCELoss 0.0000 (0.1030)	MaskDICELoss 0.0000 (0.5324)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 164 is less than current step: 499. Dropping entry: {'train/loss': 1.6143800258636474, 'train/ce_loss': 0.4029296875, 'train/seg_cls_loss': 0.0146484375, 'train/kl_loss': 0.289453125, 'train/mask_bce_loss': 0.10690287910401822, 'train/mask_dice_loss': 0.48075587674975395, 'train/mask_loss': 0.5876587629318237, 'metrics/total_secs_per_batch': 5.8646392822265625, 'metrics/data_secs_per_batch': 2.7522767066955565, '_timestamp': 1740972953.7091343}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 164 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972953.7094343}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 165 is less than current step: 499. Dropping entry: {'train/loss': 1.6462874054908752, 'train/ce_loss': 0.33525390625, 'train/seg_cls_loss': 0.0151611328125, 'train/kl_loss': 0.3251953125, 'train/mask_bce_loss': 0.1030456529930234, 'train/mask_dice_loss': 0.5324027441442013, 'train/mask_loss': 0.6354483962059021, 'metrics/total_secs_per_batch': 6.258360385894775, 'metrics/data_secs_per_batch': 3.1077499628067016, '_timestamp': 1740972959.9674275}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 165 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972959.9677072}).
Epoch: [6][167/500]	Time  6.862 ( 6.862)	Loss 1.2654 (1.6608)	CeLoss 0.2334 (0.3147)	SegCLSLoss 0.0240 (0.0141)	KLLoss 0.3555 (0.3260)	MaskLoss 0.4921 (0.6530)	MaskBCELoss 0.0521 (0.1123)	MaskDICELoss 0.4400 (0.5407)
Epoch: [6][168/500]	Time  6.808 ( 6.808)	Loss 1.6520 (1.9562)	CeLoss 0.1953 (0.4416)	SegCLSLoss 0.0200 (0.0159)	KLLoss 0.3633 (0.2908)	MaskLoss 0.7054 (0.7386)	MaskBCELoss 0.1069 (0.1526)	MaskDICELoss 0.5985 (0.5861)
Epoch: [6][169/500]	Time  6.310 ( 6.310)	Loss 1.7830 (1.7091)	CeLoss 0.1816 (0.4265)	SegCLSLoss 0.0223 (0.0140)	KLLoss 0.3555 (0.2533)	MaskLoss 0.7773 (0.6252)	MaskBCELoss 0.0514 (0.0882)	MaskDICELoss 0.7259 (0.5370)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 166 is less than current step: 499. Dropping entry: {'train/loss': 1.6607692897319795, 'train/ce_loss': 0.31474609375, 'train/seg_cls_loss': 0.014093017578125, 'train/kl_loss': 0.3259765625, 'train/mask_bce_loss': 0.11233765929937363, 'train/mask_dice_loss': 0.5407032087445259, 'train/mask_loss': 0.653040862083435, 'metrics/total_secs_per_batch': 6.861685276031494, 'metrics/data_secs_per_batch': 3.1560232639312744, '_timestamp': 1740972966.8291466}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 166 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972966.8294291}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 167 is less than current step: 499. Dropping entry: {'train/loss': 1.9561615586280823, 'train/ce_loss': 0.4416015625, 'train/seg_cls_loss': 0.015911865234375, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.15256740795448423, 'train/mask_dice_loss': 0.586060231924057, 'train/mask_loss': 0.7386276483535766, 'metrics/total_secs_per_batch': 6.807752847671509, 'metrics/data_secs_per_batch': 2.868523097038269, '_timestamp': 1740972973.6368866}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 167 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972973.6371813}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 168 is less than current step: 499. Dropping entry: {'train/loss': 1.7091482400894165, 'train/ce_loss': 0.42646484375, 'train/seg_cls_loss': 0.01400146484375, 'train/kl_loss': 0.2533203125, 'train/mask_bce_loss': 0.08820548867806792, 'train/mask_dice_loss': 0.5369740962982178, 'train/mask_loss': 0.6251795947551727, 'metrics/total_secs_per_batch': 6.310061931610107, 'metrics/data_secs_per_batch': 2.637420129776001, '_timestamp': 1740972979.946995}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 168 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972979.9473624}).
[2025-03-02 21:36:25,142] [INFO] [logging.py:96:log_dist] [Rank 0] step=3170, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:36:25,148] [INFO] [timer.py:215:stop] epoch=0/micro_step=31700/global_step=3170, RunningAvgSamplesPerSec=1.4334649408598759, CurrSamplesPerSec=1.9229763939892133, MemAllocated=30.7GB, MaxMemAllocated=37.23GB
Epoch: [6][170/500]	Time  5.202 ( 5.202)	Loss 1.6016 (1.5282)	CeLoss 1.6016 (0.6051)	SegCLSLoss 0.0000 (0.0079)	KLLoss 0.0000 (0.1811)	MaskLoss 0.0000 (0.4505)	MaskBCELoss 0.0000 (0.1116)	MaskDICELoss 0.0000 (0.3390)
Epoch: [6][171/500]	Time  7.178 ( 7.178)	Loss 1.9069 (1.8115)	CeLoss 0.2217 (0.3290)	SegCLSLoss 0.0159 (0.0160)	KLLoss 0.3594 (0.3248)	MaskLoss 0.8207 (0.7211)	MaskBCELoss 0.0245 (0.1093)	MaskDICELoss 0.7962 (0.6118)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 169 is less than current step: 499. Dropping entry: {'train/loss': 1.5281638979911805, 'train/ce_loss': 0.605126953125, 'train/seg_cls_loss': 0.00791015625, 'train/kl_loss': 0.1810546875, 'train/mask_bce_loss': 0.11158052831888199, 'train/mask_dice_loss': 0.33895160257816315, 'train/mask_loss': 0.45053213834762573, 'metrics/total_secs_per_batch': 5.201990365982056, 'metrics/data_secs_per_batch': 2.689052438735962, '_timestamp': 1740972985.1488361}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 169 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972985.149154}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 170 is less than current step: 499. Dropping entry: {'train/loss': 1.8114720284938812, 'train/ce_loss': 0.32900390625, 'train/seg_cls_loss': 0.01600341796875, 'train/kl_loss': 0.3248046875, 'train/mask_bce_loss': 0.10930308401584625, 'train/mask_dice_loss': 0.6117649435997009, 'train/mask_loss': 0.7210680335760117, 'metrics/total_secs_per_batch': 7.177863597869873, 'metrics/data_secs_per_batch': 3.0589945316314697, '_timestamp': 1740972992.3268695}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 170 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972992.3271413}).
Epoch: [6][172/500]	Time  4.683 ( 4.683)	Loss 1.1032 (1.7302)	CeLoss 0.2393 (0.7814)	SegCLSLoss 0.0114 (0.0099)	KLLoss 0.3633 (0.2170)	MaskLoss 0.4110 (0.4610)	MaskBCELoss 0.1208 (0.0990)	MaskDICELoss 0.2902 (0.3621)
Epoch: [6][173/500]	Time  6.319 ( 6.319)	Loss 1.7378 (1.6064)	CeLoss 0.2402 (0.3710)	SegCLSLoss 0.0123 (0.0140)	KLLoss 0.3672 (0.3236)	MaskLoss 0.7273 (0.5980)	MaskBCELoss 0.4565 (0.1310)	MaskDICELoss 0.2707 (0.4670)
Epoch: [6][174/500]	Time  5.952 ( 5.952)	Loss 1.7890 (1.6089)	CeLoss 0.1914 (0.3385)	SegCLSLoss 0.0237 (0.0106)	KLLoss 0.3652 (0.2539)	MaskLoss 0.7744 (0.6198)	MaskBCELoss 0.0548 (0.1719)	MaskDICELoss 0.7195 (0.4479)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 171 is less than current step: 499. Dropping entry: {'train/loss': 1.730188822746277, 'train/ce_loss': 0.7814453125, 'train/seg_cls_loss': 0.00986328125, 'train/kl_loss': 0.2169921875, 'train/mask_bce_loss': 0.09898584261536598, 'train/mask_dice_loss': 0.36205583810806274, 'train/mask_loss': 0.4610416769981384, 'metrics/total_secs_per_batch': 4.682840824127197, 'metrics/data_secs_per_batch': 2.121235156059265, '_timestamp': 1740972997.0096633}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 171 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740972997.0099418}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 172 is less than current step: 499. Dropping entry: {'train/loss': 1.6064406394958497, 'train/ce_loss': 0.37099609375, 'train/seg_cls_loss': 0.014031982421875, 'train/kl_loss': 0.3236328125, 'train/mask_bce_loss': 0.13098883433267475, 'train/mask_dice_loss': 0.46700688600540163, 'train/mask_loss': 0.5979957222938538, 'metrics/total_secs_per_batch': 6.318763732910156, 'metrics/data_secs_per_batch': 2.6077077865600584, '_timestamp': 1740973003.3284588}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 172 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973003.3287601}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 173 is less than current step: 499. Dropping entry: {'train/loss': 1.6088645249605178, 'train/ce_loss': 0.338525390625, 'train/seg_cls_loss': 0.010577392578125, 'train/kl_loss': 0.25390625, 'train/mask_bce_loss': 0.17189435267355294, 'train/mask_dice_loss': 0.447894361615181, 'train/mask_loss': 0.6197887137532234, 'metrics/total_secs_per_batch': 5.951596021652222, 'metrics/data_secs_per_batch': 2.63068311214447, '_timestamp': 1740973009.280034}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 173 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973009.2804039}).
Epoch: [6][175/500]	Time  5.935 ( 5.935)	Loss 2.5319 (1.8033)	CeLoss 0.1455 (0.3794)	SegCLSLoss 0.0273 (0.0159)	KLLoss 0.3633 (0.2898)	MaskLoss 1.1683 (0.6934)	MaskBCELoss 0.3343 (0.0835)	MaskDICELoss 0.8340 (0.6100)
Epoch: [6][176/500]	Time  5.458 ( 5.458)	Loss 1.8735 (1.2413)	CeLoss 0.2334 (0.4674)	SegCLSLoss 0.0148 (0.0082)	KLLoss 0.3574 (0.1824)	MaskLoss 0.7981 (0.3757)	MaskBCELoss 0.0525 (0.0964)	MaskDICELoss 0.7456 (0.2793)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 174 is less than current step: 499. Dropping entry: {'train/loss': 1.8032633900642394, 'train/ce_loss': 0.37939453125, 'train/seg_cls_loss': 0.015887451171875, 'train/kl_loss': 0.28984375, 'train/mask_bce_loss': 0.08345071244984865, 'train/mask_dice_loss': 0.6099778681993484, 'train/mask_loss': 0.6934285759925842, 'metrics/total_secs_per_batch': 5.9349141120910645, 'metrics/data_secs_per_batch': 2.764272165298462, '_timestamp': 1740973015.2149901}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 174 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973015.215337}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 175 is less than current step: 499. Dropping entry: {'train/loss': 1.2412638902664184, 'train/ce_loss': 0.4673583984375, 'train/seg_cls_loss': 0.00823974609375, 'train/kl_loss': 0.182421875, 'train/mask_bce_loss': 0.09641910400241613, 'train/mask_dice_loss': 0.27930316925048826, 'train/mask_loss': 0.375722274184227, 'metrics/total_secs_per_batch': 5.457879543304443, 'metrics/data_secs_per_batch': 2.5542959451675413, '_timestamp': 1740973020.6728275}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 175 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973020.6731102}).
Epoch: [6][177/500]	Time  6.644 ( 6.644)	Loss 1.9553 (2.1733)	CeLoss 0.1562 (0.2166)	SegCLSLoss 0.0278 (0.0169)	KLLoss 0.3633 (0.3664)	MaskLoss 0.8746 (0.9559)	MaskBCELoss 0.0497 (0.2505)	MaskDICELoss 0.8250 (0.7054)
Epoch: [6][178/500]	Time  4.558 ( 4.558)	Loss 1.6659 (1.7116)	CeLoss 0.1846 (0.7416)	SegCLSLoss 0.0188 (0.0087)	KLLoss 0.3633 (0.1824)	MaskLoss 0.7177 (0.4737)	MaskBCELoss 0.0327 (0.1244)	MaskDICELoss 0.6850 (0.3493)
Epoch: [6][179/500]	Time  6.842 ( 6.842)	Loss 2.1284 (1.8340)	CeLoss 0.2422 (0.3208)	SegCLSLoss 0.0102 (0.0166)	KLLoss 0.3613 (0.3285)	MaskLoss 0.9226 (0.7360)	MaskBCELoss 0.2566 (0.1688)	MaskDICELoss 0.6659 (0.5672)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 176 is less than current step: 499. Dropping entry: {'train/loss': 2.173260200023651, 'train/ce_loss': 0.2166015625, 'train/seg_cls_loss': 0.0168701171875, 'train/kl_loss': 0.36640625, 'train/mask_bce_loss': 0.2505139257758856, 'train/mask_dice_loss': 0.7053544580936432, 'train/mask_loss': 0.9558683812618256, 'metrics/total_secs_per_batch': 6.643987655639648, 'metrics/data_secs_per_batch': 2.9295337438583373, '_timestamp': 1740973027.3168397}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 176 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973027.3171234}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 177 is less than current step: 499. Dropping entry: {'train/loss': 1.7115892171859741, 'train/ce_loss': 0.7416015625, 'train/seg_cls_loss': 0.00870361328125, 'train/kl_loss': 0.182421875, 'train/mask_bce_loss': 0.12440890446305275, 'train/mask_dice_loss': 0.34925679564476014, 'train/mask_loss': 0.47366570234298705, 'metrics/total_secs_per_batch': 4.557644844055176, 'metrics/data_secs_per_batch': 2.088806962966919, '_timestamp': 1740973031.874643}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 177 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973031.8750582}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 178 is less than current step: 499. Dropping entry: {'train/loss': 1.8340300977230073, 'train/ce_loss': 0.320751953125, 'train/seg_cls_loss': 0.0166015625, 'train/kl_loss': 0.328515625, 'train/mask_bce_loss': 0.16875603795051575, 'train/mask_dice_loss': 0.5672043189406395, 'train/mask_loss': 0.7359603598713875, 'metrics/total_secs_per_batch': 6.84205961227417, 'metrics/data_secs_per_batch': 2.9402604579925535, '_timestamp': 1740973038.7165394}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 178 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973038.7168207}).
[2025-03-02 21:37:24,433] [INFO] [logging.py:96:log_dist] [Rank 0] step=3180, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:37:24,439] [INFO] [timer.py:215:stop] epoch=0/micro_step=31800/global_step=3180, RunningAvgSamplesPerSec=1.4341430060111167, CurrSamplesPerSec=1.7475049238236402, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [6][180/500]	Time  5.724 ( 5.724)	Loss 2.2285 (1.8509)	CeLoss 0.2285 (0.4936)	SegCLSLoss 0.0161 (0.0110)	KLLoss 0.3633 (0.2549)	MaskLoss 0.9775 (0.6632)	MaskBCELoss 0.0162 (0.1281)	MaskDICELoss 0.9613 (0.5350)
Epoch: [6][181/500]	Time  6.905 ( 6.905)	Loss 2.2923 (1.3764)	CeLoss 0.2148 (0.2157)	SegCLSLoss 0.0115 (0.0112)	KLLoss 0.3652 (0.2557)	MaskLoss 1.0173 (0.5647)	MaskBCELoss 0.2151 (0.0754)	MaskDICELoss 0.8022 (0.4893)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 179 is less than current step: 499. Dropping entry: {'train/loss': 1.8509221613407134, 'train/ce_loss': 0.4935546875, 'train/seg_cls_loss': 0.01099853515625, 'train/kl_loss': 0.2548828125, 'train/mask_bce_loss': 0.12811099337413906, 'train/mask_dice_loss': 0.5350454062223434, 'train/mask_loss': 0.6631563872098922, 'metrics/total_secs_per_batch': 5.724037408828735, 'metrics/data_secs_per_batch': 2.5197757959365843, '_timestamp': 1740973044.4404345}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 179 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973044.440757}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 180 is less than current step: 499. Dropping entry: {'train/loss': 1.376449453830719, 'train/ce_loss': 0.2157470703125, 'train/seg_cls_loss': 0.011248779296875, 'train/kl_loss': 0.2556640625, 'train/mask_bce_loss': 0.07538073267787695, 'train/mask_dice_loss': 0.48929663598537443, 'train/mask_loss': 0.5646773636341095, 'metrics/total_secs_per_batch': 6.904852628707886, 'metrics/data_secs_per_batch': 2.9637588977813722, '_timestamp': 1740973051.3455935}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 180 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973051.3459234}).
Epoch: [6][182/500]	Time  7.259 ( 7.259)	Loss 0.2617 (1.5128)	CeLoss 0.2617 (0.3524)	SegCLSLoss 0.0000 (0.0136)	KLLoss 0.0000 (0.2545)	MaskLoss 0.0000 (0.5640)	MaskBCELoss 0.0000 (0.1286)	MaskDICELoss 0.0000 (0.4354)
Epoch: [6][183/500]	Time  6.001 ( 6.001)	Loss 1.5312 (1.5028)	CeLoss 1.5312 (0.4936)	SegCLSLoss 0.0000 (0.0094)	KLLoss 0.0000 (0.2199)	MaskLoss 0.0000 (0.4911)	MaskBCELoss 0.0000 (0.1506)	MaskDICELoss 0.0000 (0.3406)
Epoch: [6][184/500]	Time  6.343 ( 6.343)	Loss 1.8748 (1.2643)	CeLoss 0.1777 (0.3325)	SegCLSLoss 0.0205 (0.0133)	KLLoss 0.3594 (0.2898)	MaskLoss 0.8256 (0.4481)	MaskBCELoss 0.0626 (0.0888)	MaskDICELoss 0.7630 (0.3593)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 181 is less than current step: 499. Dropping entry: {'train/loss': 1.5127629458904266, 'train/ce_loss': 0.352392578125, 'train/seg_cls_loss': 0.013629150390625, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.1286188391968608, 'train/mask_dice_loss': 0.4354286402463913, 'train/mask_loss': 0.5640474766492843, 'metrics/total_secs_per_batch': 7.259204387664795, 'metrics/data_secs_per_batch': 3.437137484550476, '_timestamp': 1740973058.604638}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 181 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973058.6049163}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 182 is less than current step: 499. Dropping entry: {'train/loss': 1.5027936577796936, 'train/ce_loss': 0.4935546875, 'train/seg_cls_loss': 0.009417724609375, 'train/kl_loss': 0.219921875, 'train/mask_bce_loss': 0.15058235377073287, 'train/mask_dice_loss': 0.3405605688691139, 'train/mask_loss': 0.49114291965961454, 'metrics/total_secs_per_batch': 6.000596046447754, 'metrics/data_secs_per_batch': 2.835644841194153, '_timestamp': 1740973064.6052415}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 182 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973064.6055224}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 183 is less than current step: 499. Dropping entry: {'train/loss': 1.2643167078495026, 'train/ce_loss': 0.332470703125, 'train/seg_cls_loss': 0.0132568359375, 'train/kl_loss': 0.28984375, 'train/mask_bce_loss': 0.08882641587406397, 'train/mask_dice_loss': 0.3593231439590454, 'train/mask_loss': 0.4481495633721352, 'metrics/total_secs_per_batch': 6.34315824508667, 'metrics/data_secs_per_batch': 2.7936275959014893, '_timestamp': 1740973070.948606}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 183 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973070.9489517}).
Epoch: [6][185/500]	Time  5.467 ( 5.467)	Loss 1.2394 (1.3506)	CeLoss 0.2373 (0.4638)	SegCLSLoss 0.0098 (0.0101)	KLLoss 0.3594 (0.2900)	MaskLoss 0.4810 (0.4263)	MaskBCELoss 0.1028 (0.0936)	MaskDICELoss 0.3783 (0.3326)
Epoch: [6][186/500]	Time  6.557 ( 6.557)	Loss 1.9976 (1.4364)	CeLoss 0.2363 (0.3276)	SegCLSLoss 0.0186 (0.0133)	KLLoss 0.3555 (0.2902)	MaskLoss 0.8582 (0.5365)	MaskBCELoss 0.0500 (0.1070)	MaskDICELoss 0.8082 (0.4295)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 184 is less than current step: 499. Dropping entry: {'train/loss': 1.3505792140960693, 'train/ce_loss': 0.46376953125, 'train/seg_cls_loss': 0.01007080078125, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.09363287948071956, 'train/mask_dice_loss': 0.33263327926397324, 'train/mask_loss': 0.42626615762710574, 'metrics/total_secs_per_batch': 5.467443227767944, 'metrics/data_secs_per_batch': 2.6567814350128174, '_timestamp': 1740973076.4158602}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 184 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973076.4161348}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 185 is less than current step: 499. Dropping entry: {'train/loss': 1.4363962888717652, 'train/ce_loss': 0.32763671875, 'train/seg_cls_loss': 0.01326904296875, 'train/kl_loss': 0.290234375, 'train/mask_bce_loss': 0.10696108704432845, 'train/mask_dice_loss': 0.42949878573417666, 'train/mask_loss': 0.5364598721265793, 'metrics/total_secs_per_batch': 6.556877136230469, 'metrics/data_secs_per_batch': 3.126832675933838, '_timestamp': 1740973082.9727147}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 185 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973082.9730468}).
Epoch: [6][187/500]	Time  5.398 ( 5.398)	Loss 2.2774 (1.2291)	CeLoss 0.2266 (0.5263)	SegCLSLoss 0.0179 (0.0070)	KLLoss 0.3613 (0.1838)	MaskLoss 1.0029 (0.3405)	MaskBCELoss 0.0065 (0.0847)	MaskDICELoss 0.9965 (0.2558)
Epoch: [6][188/500]	Time  5.493 ( 5.493)	Loss 0.9062 (1.4220)	CeLoss 0.9062 (0.5826)	SegCLSLoss 0.0000 (0.0077)	KLLoss 0.0000 (0.2186)	MaskLoss 0.0000 (0.4069)	MaskBCELoss 0.0000 (0.1242)	MaskDICELoss 0.0000 (0.2827)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 186 is less than current step: 499. Dropping entry: {'train/loss': 1.229143387079239, 'train/ce_loss': 0.526318359375, 'train/seg_cls_loss': 0.007025146484375, 'train/kl_loss': 0.1837890625, 'train/mask_bce_loss': 0.08474375470541418, 'train/mask_dice_loss': 0.2557800754904747, 'train/mask_loss': 0.34052383303642275, 'metrics/total_secs_per_batch': 5.397967100143433, 'metrics/data_secs_per_batch': 2.400916266441345, '_timestamp': 1740973088.3707092}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 186 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973088.371062}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 187 is less than current step: 499. Dropping entry: {'train/loss': 1.4219781100749969, 'train/ce_loss': 0.5826171875, 'train/seg_cls_loss': 0.0076904296875, 'train/kl_loss': 0.2185546875, 'train/mask_bce_loss': 0.12415640093386174, 'train/mask_dice_loss': 0.2827310964465141, 'train/mask_loss': 0.4068874932825565, 'metrics/total_secs_per_batch': 5.492835283279419, 'metrics/data_secs_per_batch': 2.3364808559417725, '_timestamp': 1740973093.8635085}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 187 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973093.8637855}).
Epoch: [6][189/500]	Time  7.520 ( 7.520)	Loss 1.9504 (1.8432)	CeLoss 0.1885 (0.1896)	SegCLSLoss 0.0195 (0.0155)	KLLoss 0.3594 (0.2914)	MaskLoss 0.8580 (0.8083)	MaskBCELoss 0.0135 (0.1677)	MaskDICELoss 0.8445 (0.6405)
[2025-03-02 21:38:27,332] [INFO] [logging.py:96:log_dist] [Rank 0] step=3190, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:38:27,337] [INFO] [timer.py:215:stop] epoch=0/micro_step=31900/global_step=3190, RunningAvgSamplesPerSec=1.4345846156808417, CurrSamplesPerSec=1.6798492298405467, MemAllocated=31.26GB, MaxMemAllocated=37.23GB
Epoch: [6][190/500]	Time  5.955 ( 5.955)	Loss 2.7436 (1.7916)	CeLoss 0.1865 (0.3771)	SegCLSLoss 0.0183 (0.0150)	KLLoss 0.3770 (0.2584)	MaskLoss 1.2556 (0.6907)	MaskBCELoss 0.3310 (0.1417)	MaskDICELoss 0.9246 (0.5490)
Epoch: [6][191/500]	Time  5.799 ( 5.799)	Loss 1.1135 (1.6961)	CeLoss 0.1816 (0.3317)	SegCLSLoss 0.0212 (0.0139)	KLLoss 0.3711 (0.3291)	MaskLoss 0.4420 (0.6622)	MaskBCELoss 0.1220 (0.1120)	MaskDICELoss 0.3200 (0.5502)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 188 is less than current step: 499. Dropping entry: {'train/loss': 1.843227469921112, 'train/ce_loss': 0.1896484375, 'train/seg_cls_loss': 0.0155029296875, 'train/kl_loss': 0.29140625, 'train/mask_bce_loss': 0.16771933604031802, 'train/mask_dice_loss': 0.6405399143695831, 'train/mask_loss': 0.8082592487335205, 'metrics/total_secs_per_batch': 7.52032732963562, 'metrics/data_secs_per_batch': 3.384505105018616, '_timestamp': 1740973101.3840158}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 188 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973101.38436}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 189 is less than current step: 499. Dropping entry: {'train/loss': 1.7915702223777772, 'train/ce_loss': 0.37705078125, 'train/seg_cls_loss': 0.0149658203125, 'train/kl_loss': 0.2583984375, 'train/mask_bce_loss': 0.14168600919656454, 'train/mask_dice_loss': 0.548972150683403, 'train/mask_loss': 0.6906581580638885, 'metrics/total_secs_per_batch': 5.954713821411133, 'metrics/data_secs_per_batch': 2.605703353881836, '_timestamp': 1740973107.3383899}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 189 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973107.338662}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 190 is less than current step: 499. Dropping entry: {'train/loss': 1.6961170494556428, 'train/ce_loss': 0.33173828125, 'train/seg_cls_loss': 0.01392822265625, 'train/kl_loss': 0.3291015625, 'train/mask_bce_loss': 0.11197772519662977, 'train/mask_dice_loss': 0.5501921281218529, 'train/mask_loss': 0.6621698588132858, 'metrics/total_secs_per_batch': 5.798587322235107, 'metrics/data_secs_per_batch': 2.395881247520447, '_timestamp': 1740973113.1371863}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 190 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973113.1374772}).
Epoch: [6][192/500]	Time  6.601 ( 6.601)	Loss 2.3348 (1.5794)	CeLoss 0.1748 (0.5453)	SegCLSLoss 0.0167 (0.0105)	KLLoss 0.3652 (0.2160)	MaskLoss 1.0571 (0.5037)	MaskBCELoss 0.3667 (0.0633)	MaskDICELoss 0.6904 (0.4404)
Epoch: [6][193/500]	Time  6.168 ( 6.168)	Loss 1.5542 (1.4849)	CeLoss 0.2754 (0.2763)	SegCLSLoss 0.0178 (0.0156)	KLLoss 0.3555 (0.2926)	MaskLoss 0.6170 (0.5859)	MaskBCELoss 0.0391 (0.0921)	MaskDICELoss 0.5779 (0.4938)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 191 is less than current step: 499. Dropping entry: {'train/loss': 1.5794350743293761, 'train/ce_loss': 0.545263671875, 'train/seg_cls_loss': 0.010528564453125, 'train/kl_loss': 0.216015625, 'train/mask_bce_loss': 0.06333370599895716, 'train/mask_dice_loss': 0.4403730869293213, 'train/mask_loss': 0.5037067890167236, 'metrics/total_secs_per_batch': 6.601101636886597, 'metrics/data_secs_per_batch': 2.3499061822891236, '_timestamp': 1740973119.7383962}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 191 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973119.7387116}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 192 is less than current step: 499. Dropping entry: {'train/loss': 1.4848990261554718, 'train/ce_loss': 0.27626953125, 'train/seg_cls_loss': 0.01561279296875, 'train/kl_loss': 0.292578125, 'train/mask_bce_loss': 0.09208109062165022, 'train/mask_dice_loss': 0.4937766373157501, 'train/mask_loss': 0.5858577221632004, 'metrics/total_secs_per_batch': 6.168093681335449, 'metrics/data_secs_per_batch': 2.54771683216095, '_timestamp': 1740973125.9065514}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 192 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973125.906879}).
Epoch: [6][194/500]	Time  6.283 ( 6.283)	Loss 2.0910 (1.9323)	CeLoss 0.2451 (0.3202)	SegCLSLoss 0.0159 (0.0142)	KLLoss 0.3555 (0.2908)	MaskLoss 0.9010 (0.7878)	MaskBCELoss 0.0267 (0.2070)	MaskDICELoss 0.8742 (0.5808)
Epoch: [6][195/500]	Time  5.943 ( 5.943)	Loss 2.1197 (1.5353)	CeLoss 0.1963 (0.4577)	SegCLSLoss 0.0201 (0.0120)	KLLoss 0.3574 (0.2557)	MaskLoss 0.9388 (0.5229)	MaskBCELoss 0.0768 (0.1078)	MaskDICELoss 0.8620 (0.4151)
Epoch: [6][196/500]	Time  7.024 ( 7.024)	Loss 2.4987 (1.5698)	CeLoss 0.1797 (0.1854)	SegCLSLoss 0.0250 (0.0151)	KLLoss 0.3867 (0.2963)	MaskLoss 1.1341 (0.6736)	MaskBCELoss 0.3492 (0.1578)	MaskDICELoss 0.7849 (0.5158)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 193 is less than current step: 499. Dropping entry: {'train/loss': 1.9322946071624756, 'train/ce_loss': 0.32021484375, 'train/seg_cls_loss': 0.014208984375, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.2070153636857867, 'train/mask_dice_loss': 0.5808116018772125, 'train/mask_loss': 0.7878269672393798, 'metrics/total_secs_per_batch': 6.282870054244995, 'metrics/data_secs_per_batch': 2.893411087989807, '_timestamp': 1740973132.1892536}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 193 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973132.1896029}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 194 is less than current step: 499. Dropping entry: {'train/loss': 1.5353232264518737, 'train/ce_loss': 0.45771484375, 'train/seg_cls_loss': 0.0119873046875, 'train/kl_loss': 0.2556640625, 'train/mask_bce_loss': 0.1078066099435091, 'train/mask_dice_loss': 0.4151284396648407, 'train/mask_loss': 0.5229350626468658, 'metrics/total_secs_per_batch': 5.9434123039245605, 'metrics/data_secs_per_batch': 2.70698766708374, '_timestamp': 1740973138.1328702}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 194 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973138.133236}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 195 is less than current step: 499. Dropping entry: {'train/loss': 1.569803774356842, 'train/ce_loss': 0.1853515625, 'train/seg_cls_loss': 0.01514892578125, 'train/kl_loss': 0.2962890625, 'train/mask_bce_loss': 0.15781378522515296, 'train/mask_dice_loss': 0.5158332124352455, 'train/mask_loss': 0.6736470073461532, 'metrics/total_secs_per_batch': 7.023958683013916, 'metrics/data_secs_per_batch': 2.997766947746277, '_timestamp': 1740973145.1566436}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 195 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973145.1569216}).
Epoch: [6][197/500]	Time  7.196 ( 7.196)	Loss 1.1620 (1.6018)	CeLoss 0.2441 (0.2198)	SegCLSLoss 0.0130 (0.0142)	KLLoss 0.3574 (0.3260)	MaskLoss 0.4384 (0.6713)	MaskBCELoss 0.1080 (0.1713)	MaskDICELoss 0.3304 (0.5000)
Epoch: [6][198/500]	Time  5.434 ( 5.434)	Loss 1.9014 (2.0747)	CeLoss 0.2539 (0.4048)	SegCLSLoss 0.0089 (0.0106)	KLLoss 0.3672 (0.2918)	MaskLoss 0.8033 (0.8178)	MaskBCELoss 0.2772 (0.2644)	MaskDICELoss 0.5261 (0.5534)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 196 is less than current step: 499. Dropping entry: {'train/loss': 1.601811057329178, 'train/ce_loss': 0.21982421875, 'train/seg_cls_loss': 0.01424560546875, 'train/kl_loss': 0.3259765625, 'train/mask_bce_loss': 0.17128187902271746, 'train/mask_dice_loss': 0.49998496621847155, 'train/mask_loss': 0.6712668538093567, 'metrics/total_secs_per_batch': 7.195709466934204, 'metrics/data_secs_per_batch': 3.3954605579376222, '_timestamp': 1740973152.3523703}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 196 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973152.3526742}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 197 is less than current step: 499. Dropping entry: {'train/loss': 2.0747313499450684, 'train/ce_loss': 0.40478515625, 'train/seg_cls_loss': 0.010552978515625, 'train/kl_loss': 0.291796875, 'train/mask_bce_loss': 0.26442944779992106, 'train/mask_dice_loss': 0.5534049689769744, 'train/mask_loss': 0.8178344249725342, 'metrics/total_secs_per_batch': 5.434258937835693, 'metrics/data_secs_per_batch': 2.2863165378570556, '_timestamp': 1740973157.7866828}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 197 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973157.7870085}).
Epoch: [6][199/500]	Time  5.722 ( 5.722)	Loss 1.0312 (1.6109)	CeLoss 1.0312 (0.4547)	SegCLSLoss 0.0000 (0.0137)	KLLoss 0.0000 (0.2584)	MaskLoss 0.0000 (0.5618)	MaskBCELoss 0.0000 (0.1694)	MaskDICELoss 0.0000 (0.3924)
[2025-03-02 21:39:29,348] [INFO] [logging.py:96:log_dist] [Rank 0] step=3200, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:39:29,354] [INFO] [timer.py:215:stop] epoch=0/micro_step=32000/global_step=3200, RunningAvgSamplesPerSec=1.4350804950230986, CurrSamplesPerSec=1.710737534250509, MemAllocated=31.51GB, MaxMemAllocated=37.23GB
Epoch: [6][200/500]	Time  5.847 ( 5.847)	Loss 0.0608 (1.5881)	CeLoss 0.0608 (0.4106)	SegCLSLoss 0.0000 (0.0103)	KLLoss 0.0000 (0.2561)	MaskLoss 0.0000 (0.5734)	MaskBCELoss 0.0000 (0.1307)	MaskDICELoss 0.0000 (0.4427)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 198 is less than current step: 499. Dropping entry: {'train/loss': 1.610873031616211, 'train/ce_loss': 0.4546875, 'train/seg_cls_loss': 0.013702392578125, 'train/kl_loss': 0.2583984375, 'train/mask_bce_loss': 0.1694333955645561, 'train/mask_dice_loss': 0.39235077798366547, 'train/mask_loss': 0.56178417801857, 'metrics/total_secs_per_batch': 5.721615314483643, 'metrics/data_secs_per_batch': 2.7165469408035277, '_timestamp': 1740973163.5082552}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 198 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973163.508554}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 199 is less than current step: 499. Dropping entry: {'train/loss': 1.5881498813629151, 'train/ce_loss': 0.4105712890625, 'train/seg_cls_loss': 0.010272216796875, 'train/kl_loss': 0.2560546875, 'train/mask_bce_loss': 0.1307420741766691, 'train/mask_dice_loss': 0.4426663666963577, 'train/mask_loss': 0.5734084367752075, 'metrics/total_secs_per_batch': 5.847064733505249, 'metrics/data_secs_per_batch': 2.666918230056763, '_timestamp': 1740973169.3550994}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 199 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973169.355377}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 200 is less than current step: 499. Dropping entry: {'train/loss': 2.029895269870758, 'train/ce_loss': 0.409375, 'train/seg_cls_loss': 0.013916015625, 'train/kl_loss': 0.2947265625, 'train/mask_bce_loss': 0.14239261038601397, 'train/mask_dice_loss': 0.6495569825172425, 'train/mask_loss': 0.7919495880603791, 'metrics/total_secs_per_batch': 6.409250736236572, 'metrics/data_secs_per_batch': 2.9432841300964356, '_timestamp': 1740973175.764578}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 200 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973175.7649274}).
Epoch: [6][201/500]	Time  6.409 ( 6.409)	Loss 2.0332 (2.0299)	CeLoss 0.2754 (0.4094)	SegCLSLoss 0.0099 (0.0139)	KLLoss 0.3652 (0.2947)	MaskLoss 0.8574 (0.7919)	MaskBCELoss 0.2588 (0.1424)	MaskDICELoss 0.5986 (0.6496)
Epoch: [6][202/500]	Time  6.578 ( 6.578)	Loss 1.8825 (1.4707)	CeLoss 0.2178 (0.3166)	SegCLSLoss 0.0272 (0.0138)	KLLoss 0.3633 (0.2877)	MaskLoss 0.8075 (0.5593)	MaskBCELoss 0.0359 (0.1084)	MaskDICELoss 0.7715 (0.4509)
Epoch: [6][203/500]	Time  7.114 ( 7.114)	Loss 1.4711 (1.4527)	CeLoss 0.2139 (0.3357)	SegCLSLoss 0.0183 (0.0114)	KLLoss 0.3711 (0.2180)	MaskLoss 0.6057 (0.5448)	MaskBCELoss 0.0324 (0.0452)	MaskDICELoss 0.5733 (0.4996)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 201 is less than current step: 499. Dropping entry: {'train/loss': 1.4706528782844543, 'train/ce_loss': 0.3166015625, 'train/seg_cls_loss': 0.013836669921875, 'train/kl_loss': 0.2876953125, 'train/mask_bce_loss': 0.10836052028462291, 'train/mask_dice_loss': 0.45094052851200106, 'train/mask_loss': 0.5593010485172272, 'metrics/total_secs_per_batch': 6.578431606292725, 'metrics/data_secs_per_batch': 3.192397856712341, '_timestamp': 1740973182.3429642}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 201 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973182.3431535}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 202 is less than current step: 499. Dropping entry: {'train/loss': 1.4527215361595154, 'train/ce_loss': 0.3356689453125, 'train/seg_cls_loss': 0.011431884765625, 'train/kl_loss': 0.21796875, 'train/mask_bce_loss': 0.04519098154269159, 'train/mask_dice_loss': 0.4995657742023468, 'train/mask_loss': 0.5447567641735077, 'metrics/total_secs_per_batch': 7.113762378692627, 'metrics/data_secs_per_batch': 3.5447601556777952, '_timestamp': 1740973189.4567804}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 202 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973189.4570699}).
Epoch: [6][204/500]	Time  6.379 ( 6.379)	Loss 1.9151 (1.6023)	CeLoss 0.1875 (0.3570)	SegCLSLoss 0.0211 (0.0147)	KLLoss 0.3633 (0.2527)	MaskLoss 0.8404 (0.6063)	MaskBCELoss 0.0596 (0.0377)	MaskDICELoss 0.7807 (0.5686)
Epoch: [6][205/500]	Time  6.365 ( 6.365)	Loss 0.8516 (1.7127)	CeLoss 0.8516 (0.3270)	SegCLSLoss 0.0000 (0.0120)	KLLoss 0.0000 (0.2918)	MaskLoss 0.0000 (0.6752)	MaskBCELoss 0.0000 (0.1767)	MaskDICELoss 0.0000 (0.4985)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 203 is less than current step: 499. Dropping entry: {'train/loss': 1.6023240566253663, 'train/ce_loss': 0.35703125, 'train/seg_cls_loss': 0.014666748046875, 'train/kl_loss': 0.252734375, 'train/mask_bce_loss': 0.0377033936791122, 'train/mask_dice_loss': 0.5686343997716904, 'train/mask_loss': 0.6063377916812897, 'metrics/total_secs_per_batch': 6.3787126541137695, 'metrics/data_secs_per_batch': 2.615810012817383, '_timestamp': 1740973195.8356175}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 203 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973195.835962}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 204 is less than current step: 499. Dropping entry: {'train/loss': 1.712714570760727, 'train/ce_loss': 0.326953125, 'train/seg_cls_loss': 0.01199951171875, 'train/kl_loss': 0.291796875, 'train/mask_bce_loss': 0.17674597166478634, 'train/mask_dice_loss': 0.4984589576721191, 'train/mask_loss': 0.6752049386501312, 'metrics/total_secs_per_batch': 6.365223407745361, 'metrics/data_secs_per_batch': 3.101645040512085, '_timestamp': 1740973202.200724}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 204 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973202.200998}).
Epoch: [6][206/500]	Time  7.045 ( 7.045)	Loss 1.7176 (1.8875)	CeLoss 0.2178 (0.1966)	SegCLSLoss 0.0159 (0.0166)	KLLoss 0.3633 (0.3270)	MaskLoss 0.7279 (0.8249)	MaskBCELoss 0.0696 (0.0876)	MaskDICELoss 0.6583 (0.7373)
Epoch: [6][207/500]	Time  6.765 ( 6.765)	Loss 0.0752 (1.4233)	CeLoss 0.0752 (0.2813)	SegCLSLoss 0.0000 (0.0131)	KLLoss 0.0000 (0.2881)	MaskLoss 0.0000 (0.5533)	MaskBCELoss 0.0000 (0.1166)	MaskDICELoss 0.0000 (0.4367)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 205 is less than current step: 499. Dropping entry: {'train/loss': 1.8874948143959045, 'train/ce_loss': 0.196630859375, 'train/seg_cls_loss': 0.016644287109375, 'train/kl_loss': 0.326953125, 'train/mask_bce_loss': 0.08757362132892013, 'train/mask_dice_loss': 0.7373017132282257, 'train/mask_loss': 0.8248753249645233, 'metrics/total_secs_per_batch': 7.045452117919922, 'metrics/data_secs_per_batch': 3.0443915843963625, '_timestamp': 1740973209.2462049}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 205 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973209.2464864}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 206 is less than current step: 499. Dropping entry: {'train/loss': 1.4233395218849183, 'train/ce_loss': 0.28134765625, 'train/seg_cls_loss': 0.013055419921875, 'train/kl_loss': 0.2880859375, 'train/mask_bce_loss': 0.11659821271896362, 'train/mask_dice_loss': 0.4367219388484955, 'train/mask_loss': 0.5533201515674591, 'metrics/total_secs_per_batch': 6.764570236206055, 'metrics/data_secs_per_batch': 3.2096829652786254, '_timestamp': 1740973216.0117588}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 206 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973216.012669}).
Epoch: [6][208/500]	Time  6.543 ( 6.543)	Loss 1.9444 (1.6096)	CeLoss 0.2275 (0.5130)	SegCLSLoss 0.0181 (0.0112)	KLLoss 0.3535 (0.2545)	MaskLoss 0.8365 (0.5329)	MaskBCELoss 0.0363 (0.0693)	MaskDICELoss 0.8001 (0.4636)
Epoch: [6][209/500]	Time  6.700 ( 6.700)	Loss 1.7250 (1.6526)	CeLoss 0.2461 (0.3206)	SegCLSLoss 0.0123 (0.0147)	KLLoss 0.3711 (0.2877)	MaskLoss 0.7180 (0.6480)	MaskBCELoss 0.2772 (0.1653)	MaskDICELoss 0.4408 (0.4827)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 207 is less than current step: 499. Dropping entry: {'train/loss': 1.60964115858078, 'train/ce_loss': 0.51298828125, 'train/seg_cls_loss': 0.01121826171875, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.06929800221696496, 'train/mask_dice_loss': 0.4635987490415573, 'train/mask_loss': 0.5328967452049256, 'metrics/total_secs_per_batch': 6.542683839797974, 'metrics/data_secs_per_batch': 3.1756137371063233, '_timestamp': 1740973222.5534568}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 207 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973222.5537581}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 208 is less than current step: 499. Dropping entry: {'train/loss': 1.6525755882263184, 'train/ce_loss': 0.32060546875, 'train/seg_cls_loss': 0.01466064453125, 'train/kl_loss': 0.2876953125, 'train/mask_bce_loss': 0.1653259065002203, 'train/mask_dice_loss': 0.4826904147863388, 'train/mask_loss': 0.6480163097381592, 'metrics/total_secs_per_batch': 6.700236797332764, 'metrics/data_secs_per_batch': 2.910177493095398, '_timestamp': 1740973229.2536225}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 208 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973229.253903}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 209 is less than current step: 499. Dropping entry: {'train/loss': 1.5801714301109313, 'train/ce_loss': 0.3293701171875, 'train/seg_cls_loss': 0.012554931640625, 'train/kl_loss': 0.2572265625, 'train/mask_bce_loss': 0.08968694247305393, 'train/mask_dice_loss': 0.5198934227228165, 'train/mask_loss': 0.6095803558826447, 'metrics/total_secs_per_batch': 6.854227066040039, 'metrics/data_secs_per_batch': 3.0868208169937135, '_timestamp': 1740973236.1079254}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 209 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973236.1083536}).
[2025-03-02 21:40:36,102] [INFO] [logging.py:96:log_dist] [Rank 0] step=3210, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:40:36,107] [INFO] [timer.py:215:stop] epoch=0/micro_step=32100/global_step=3210, RunningAvgSamplesPerSec=1.435269551571879, CurrSamplesPerSec=1.459356204735271, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [6][210/500]	Time  6.854 ( 6.854)	Loss 1.9729 (1.5802)	CeLoss 0.2383 (0.3294)	SegCLSLoss 0.0141 (0.0126)	KLLoss 0.3711 (0.2572)	MaskLoss 0.8458 (0.6096)	MaskBCELoss 0.1098 (0.0897)	MaskDICELoss 0.7361 (0.5199)
Epoch: [6][211/500]	Time  6.272 ( 6.272)	Loss 1.9793 (1.6199)	CeLoss 0.2891 (0.3917)	SegCLSLoss 0.0117 (0.0112)	KLLoss 0.3574 (0.2535)	MaskLoss 0.8246 (0.5987)	MaskBCELoss 0.2818 (0.1265)	MaskDICELoss 0.5428 (0.4722)
Epoch: [6][212/500]	Time  6.375 ( 6.375)	Loss 2.1566 (1.7740)	CeLoss 0.1719 (0.3662)	SegCLSLoss 0.0192 (0.0155)	KLLoss 0.3555 (0.2863)	MaskLoss 0.9699 (0.6856)	MaskBCELoss 0.1410 (0.0873)	MaskDICELoss 0.8289 (0.5983)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 210 is less than current step: 499. Dropping entry: {'train/loss': 1.6199029207229614, 'train/ce_loss': 0.3917236328125, 'train/seg_cls_loss': 0.011163330078125, 'train/kl_loss': 0.253515625, 'train/mask_bce_loss': 0.12648802511394025, 'train/mask_dice_loss': 0.47217192500829697, 'train/mask_loss': 0.5986599445343017, 'metrics/total_secs_per_batch': 6.271945953369141, 'metrics/data_secs_per_batch': 2.7241639137268066, '_timestamp': 1740973242.3799112}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 210 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973242.3801954}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 211 is less than current step: 499. Dropping entry: {'train/loss': 1.774038064479828, 'train/ce_loss': 0.3662109375, 'train/seg_cls_loss': 0.0155029296875, 'train/kl_loss': 0.286328125, 'train/mask_bce_loss': 0.08728255536407233, 'train/mask_dice_loss': 0.598320472240448, 'train/mask_loss': 0.6856030225753784, 'metrics/total_secs_per_batch': 6.37542724609375, 'metrics/data_secs_per_batch': 2.6381223678588865, '_timestamp': 1740973248.755281}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 211 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973248.7555623}).
Epoch: [6][213/500]	Time  5.991 ( 5.991)	Loss 1.2734 (1.4183)	CeLoss 1.2734 (0.5156)	SegCLSLoss 0.0000 (0.0102)	KLLoss 0.0000 (0.2170)	MaskLoss 0.0000 (0.4381)	MaskBCELoss 0.0000 (0.1338)	MaskDICELoss 0.0000 (0.3043)
Epoch: [6][214/500]	Time  4.535 ( 4.535)	Loss 1.9659 (1.5816)	CeLoss 0.2598 (0.6858)	SegCLSLoss 0.0092 (0.0091)	KLLoss 0.3613 (0.1822)	MaskLoss 0.8326 (0.4366)	MaskBCELoss 0.1696 (0.0971)	MaskDICELoss 0.6630 (0.3394)
Epoch: [6][215/500]	Time  5.533 ( 5.533)	Loss 2.2594 (1.7739)	CeLoss 0.3184 (0.5874)	SegCLSLoss 0.0182 (0.0109)	KLLoss 0.3613 (0.2150)	MaskLoss 0.9480 (0.5798)	MaskBCELoss 0.0342 (0.0648)	MaskDICELoss 0.9138 (0.5150)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 212 is less than current step: 499. Dropping entry: {'train/loss': 1.4182669639587402, 'train/ce_loss': 0.515576171875, 'train/seg_cls_loss': 0.01021728515625, 'train/kl_loss': 0.2169921875, 'train/mask_bce_loss': 0.13376417085528375, 'train/mask_dice_loss': 0.30434879660606384, 'train/mask_loss': 0.4381129741668701, 'metrics/total_secs_per_batch': 5.990683317184448, 'metrics/data_secs_per_batch': 2.5861222982406615, '_timestamp': 1740973254.745933}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 212 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973254.7461967}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 213 is less than current step: 499. Dropping entry: {'train/loss': 1.581554400920868, 'train/ce_loss': 0.685791015625, 'train/seg_cls_loss': 0.009130859375, 'train/kl_loss': 0.1822265625, 'train/mask_bce_loss': 0.09713165014982224, 'train/mask_dice_loss': 0.3394463360309601, 'train/mask_loss': 0.43657798171043394, 'metrics/total_secs_per_batch': 4.535315990447998, 'metrics/data_secs_per_batch': 1.8680428504943847, '_timestamp': 1740973259.2814012}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 213 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973259.281743}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 214 is less than current step: 499. Dropping entry: {'train/loss': 1.7738901019096374, 'train/ce_loss': 0.58740234375, 'train/seg_cls_loss': 0.010858154296875, 'train/kl_loss': 0.2150390625, 'train/mask_bce_loss': 0.06480688527226448, 'train/mask_dice_loss': 0.5149604141712188, 'train/mask_loss': 0.5797672927379608, 'metrics/total_secs_per_batch': 5.532771587371826, 'metrics/data_secs_per_batch': 2.373042130470276, '_timestamp': 1740973264.8140368}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 214 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973264.8143172}).
Epoch: [6][216/500]	Time  5.684 ( 5.684)	Loss 1.9426 (1.8203)	CeLoss 0.2168 (0.5742)	SegCLSLoss 0.0118 (0.0139)	KLLoss 0.3652 (0.2523)	MaskLoss 0.8414 (0.6068)	MaskBCELoss 0.1160 (0.0755)	MaskDICELoss 0.7254 (0.5313)
Epoch: [6][217/500]	Time  6.223 ( 6.223)	Loss 1.6665 (1.8518)	CeLoss 0.2090 (0.3496)	SegCLSLoss 0.0131 (0.0137)	KLLoss 0.3633 (0.2910)	MaskLoss 0.7073 (0.7331)	MaskBCELoss 0.0376 (0.2205)	MaskDICELoss 0.6697 (0.5125)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 215 is less than current step: 499. Dropping entry: {'train/loss': 1.8202701568603517, 'train/ce_loss': 0.57421875, 'train/seg_cls_loss': 0.013946533203125, 'train/kl_loss': 0.25234375, 'train/mask_bce_loss': 0.07547547072172164, 'train/mask_dice_loss': 0.5313392817974091, 'train/mask_loss': 0.6068147599697113, 'metrics/total_secs_per_batch': 5.684446573257446, 'metrics/data_secs_per_batch': 2.6667858839035032, '_timestamp': 1740973270.4984941}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 215 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973270.4988453}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 216 is less than current step: 499. Dropping entry: {'train/loss': 1.8517966151237488, 'train/ce_loss': 0.349609375, 'train/seg_cls_loss': 0.013690185546875, 'train/kl_loss': 0.291015625, 'train/mask_bce_loss': 0.22054202891886235, 'train/mask_dice_loss': 0.5125340268015861, 'train/mask_loss': 0.7330760419368744, 'metrics/total_secs_per_batch': 6.223208427429199, 'metrics/data_secs_per_batch': 2.557549571990967, '_timestamp': 1740973276.7216992}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 216 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973276.721993}).
Epoch: [6][218/500]	Time  5.110 ( 5.110)	Loss 1.4609 (1.4105)	CeLoss 1.4609 (0.5906)	SegCLSLoss 0.0000 (0.0072)	KLLoss 0.0000 (0.2168)	MaskLoss 0.0000 (0.3974)	MaskBCELoss 0.0000 (0.1239)	MaskDICELoss 0.0000 (0.2735)
Epoch: [6][219/500]	Time  6.498 ( 6.498)	Loss 1.9023 (1.5393)	CeLoss 0.1885 (0.3095)	SegCLSLoss 0.0226 (0.0138)	KLLoss 0.3594 (0.2582)	MaskLoss 0.8330 (0.5984)	MaskBCELoss 0.0409 (0.1839)	MaskDICELoss 0.7921 (0.4146)
[2025-03-02 21:41:34,407] [INFO] [logging.py:96:log_dist] [Rank 0] step=3220, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:41:34,413] [INFO] [timer.py:215:stop] epoch=0/micro_step=32200/global_step=3220, RunningAvgSamplesPerSec=1.4359983862819896, CurrSamplesPerSec=1.6437002913692396, MemAllocated=31.09GB, MaxMemAllocated=37.23GB
Epoch: [6][220/500]	Time  6.086 ( 6.086)	Loss 0.5206 (1.4727)	CeLoss 0.2754 (0.4400)	SegCLSLoss 0.0112 (0.0110)	KLLoss 0.3633 (0.2537)	MaskLoss 0.1011 (0.5009)	MaskBCELoss 0.0485 (0.0923)	MaskDICELoss 0.0526 (0.4086)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 217 is less than current step: 499. Dropping entry: {'train/loss': 1.4105468213558197, 'train/ce_loss': 0.590625, 'train/seg_cls_loss': 0.007232666015625, 'train/kl_loss': 0.216796875, 'train/mask_bce_loss': 0.12390768509358167, 'train/mask_dice_loss': 0.2734555795788765, 'train/mask_loss': 0.3973632588982582, 'metrics/total_secs_per_batch': 5.1095170974731445, 'metrics/data_secs_per_batch': 2.3068514823913575, '_timestamp': 1740973281.8312218}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 217 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973281.831503}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 218 is less than current step: 499. Dropping entry: {'train/loss': 1.5392712950706482, 'train/ce_loss': 0.30947265625, 'train/seg_cls_loss': 0.01380615234375, 'train/kl_loss': 0.258203125, 'train/mask_bce_loss': 0.1838609293103218, 'train/mask_dice_loss': 0.41458331048488617, 'train/mask_loss': 0.5984442442655563, 'metrics/total_secs_per_batch': 6.497625350952148, 'metrics/data_secs_per_batch': 2.667944312095642, '_timestamp': 1740973288.3288653}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 218 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973288.3292408}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 219 is less than current step: 499. Dropping entry: {'train/loss': 1.4727400541305542, 'train/ce_loss': 0.4400390625, 'train/seg_cls_loss': 0.0110107421875, 'train/kl_loss': 0.2537109375, 'train/mask_bce_loss': 0.09228493832051754, 'train/mask_dice_loss': 0.4086358599364758, 'train/mask_loss': 0.5009208038449288, 'metrics/total_secs_per_batch': 6.085575819015503, 'metrics/data_secs_per_batch': 2.7525738954544066, '_timestamp': 1740973294.4143784}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 219 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973294.41473}).
Epoch: [6][221/500]	Time  5.743 ( 5.743)	Loss 0.9100 (1.5491)	CeLoss 0.2910 (0.4481)	SegCLSLoss 0.0131 (0.0112)	KLLoss 0.3633 (0.2938)	MaskLoss 0.2880 (0.5329)	MaskBCELoss 0.0872 (0.1371)	MaskDICELoss 0.2008 (0.3958)
Epoch: [6][222/500]	Time  5.608 ( 5.608)	Loss 1.7326 (1.6093)	CeLoss 0.2109 (0.4388)	SegCLSLoss 0.0134 (0.0121)	KLLoss 0.3633 (0.2553)	MaskLoss 0.7393 (0.5693)	MaskBCELoss 0.0562 (0.1300)	MaskDICELoss 0.6831 (0.4393)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 220 is less than current step: 499. Dropping entry: {'train/loss': 1.549074912071228, 'train/ce_loss': 0.44814453125, 'train/seg_cls_loss': 0.01124267578125, 'train/kl_loss': 0.29375, 'train/mask_bce_loss': 0.13714027013629676, 'train/mask_dice_loss': 0.39579563215374947, 'train/mask_loss': 0.5329358994960784, 'metrics/total_secs_per_batch': 5.7431089878082275, 'metrics/data_secs_per_batch': 2.4513820886611937, '_timestamp': 1740973300.1575737}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 220 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973300.15787}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 221 is less than current step: 499. Dropping entry: {'train/loss': 1.6092973709106446, 'train/ce_loss': 0.43876953125, 'train/seg_cls_loss': 0.0121337890625, 'train/kl_loss': 0.2552734375, 'train/mask_bce_loss': 0.12999738319776952, 'train/mask_dice_loss': 0.43934856355190277, 'train/mask_loss': 0.5693459510803223, 'metrics/total_secs_per_batch': 5.608266830444336, 'metrics/data_secs_per_batch': 2.553636312484741, '_timestamp': 1740973305.7658439}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 221 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973305.7662027}).
Epoch: [6][223/500]	Time  7.232 ( 7.232)	Loss 2.3066 (1.7319)	CeLoss 0.1885 (0.1933)	SegCLSLoss 0.0303 (0.0208)	KLLoss 0.3555 (0.3266)	MaskLoss 1.0342 (0.7477)	MaskBCELoss 0.0556 (0.1357)	MaskDICELoss 0.9786 (0.6121)
Epoch: [6][224/500]	Time  7.959 ( 7.959)	Loss 2.4433 (2.0472)	CeLoss 0.2080 (0.2244)	SegCLSLoss 0.0159 (0.0163)	KLLoss 0.3633 (0.3609)	MaskLoss 1.0957 (0.8890)	MaskBCELoss 0.3781 (0.1710)	MaskDICELoss 0.7176 (0.7180)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 222 is less than current step: 499. Dropping entry: {'train/loss': 1.7318576455116272, 'train/ce_loss': 0.193310546875, 'train/seg_cls_loss': 0.020849609375, 'train/kl_loss': 0.3265625, 'train/mask_bce_loss': 0.13565305788069965, 'train/mask_dice_loss': 0.6120873063802719, 'train/mask_loss': 0.7477403610944748, 'metrics/total_secs_per_batch': 7.2321882247924805, 'metrics/data_secs_per_batch': 3.21327428817749, '_timestamp': 1740973312.9980412}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 222 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973312.9982526}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 223 is less than current step: 499. Dropping entry: {'train/loss': 2.047205126285553, 'train/ce_loss': 0.2244140625, 'train/seg_cls_loss': 0.016326904296875, 'train/kl_loss': 0.3609375, 'train/mask_bce_loss': 0.1710019361227751, 'train/mask_dice_loss': 0.7180303066968918, 'train/mask_loss': 0.8890322536230088, 'metrics/total_secs_per_batch': 7.958669900894165, 'metrics/data_secs_per_batch': 3.6399502515792848, '_timestamp': 1740973320.9566863}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 223 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973320.956993}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 224 is less than current step: 499. Dropping entry: {'train/loss': 1.3406807839870454, 'train/ce_loss': 0.5087890625, 'train/seg_cls_loss': 0.007904052734375, 'train/kl_loss': 0.2177734375, 'train/mask_bce_loss': 0.061268425546586514, 'train/mask_dice_loss': 0.3418844699859619, 'train/mask_loss': 0.4031528949737549, 'metrics/total_secs_per_batch': 5.153245449066162, 'metrics/data_secs_per_batch': 2.458039903640747, '_timestamp': 1740973326.1099696}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 224 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973326.110319}).
Epoch: [6][225/500]	Time  5.153 ( 5.153)	Loss 2.8987 (1.3407)	CeLoss 0.2812 (0.5088)	SegCLSLoss 0.0128 (0.0079)	KLLoss 0.3633 (0.2178)	MaskLoss 1.2872 (0.4032)	MaskBCELoss 0.2978 (0.0613)	MaskDICELoss 0.9895 (0.3419)
Epoch: [6][226/500]	Time  6.149 ( 6.149)	Loss 1.0863 (1.7914)	CeLoss 0.1787 (0.3307)	SegCLSLoss 0.0183 (0.0143)	KLLoss 0.3633 (0.2881)	MaskLoss 0.4313 (0.7125)	MaskBCELoss 0.0192 (0.1534)	MaskDICELoss 0.4122 (0.5590)
Epoch: [6][227/500]	Time  6.606 ( 6.606)	Loss 1.5149 (1.5871)	CeLoss 0.2207 (0.3021)	SegCLSLoss 0.0259 (0.0143)	KLLoss 0.3613 (0.2920)	MaskLoss 0.6227 (0.6242)	MaskBCELoss 0.0408 (0.1052)	MaskDICELoss 0.5819 (0.5190)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 225 is less than current step: 499. Dropping entry: {'train/loss': 1.7914273977279662, 'train/ce_loss': 0.3306640625, 'train/seg_cls_loss': 0.0142578125, 'train/kl_loss': 0.2880859375, 'train/mask_bce_loss': 0.15344779826700689, 'train/mask_dice_loss': 0.559013944864273, 'train/mask_loss': 0.7124617397785187, 'metrics/total_secs_per_batch': 6.148833990097046, 'metrics/data_secs_per_batch': 2.8627883434295653, '_timestamp': 1740973332.258805}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 225 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973332.2590804}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 226 is less than current step: 499. Dropping entry: {'train/loss': 1.5870954513549804, 'train/ce_loss': 0.30205078125, 'train/seg_cls_loss': 0.014349365234375, 'train/kl_loss': 0.2919921875, 'train/mask_bce_loss': 0.10524449748918414, 'train/mask_dice_loss': 0.5189672887325287, 'train/mask_loss': 0.6242117881774902, 'metrics/total_secs_per_batch': 6.606350660324097, 'metrics/data_secs_per_batch': 2.870958399772644, '_timestamp': 1740973338.8651288}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 226 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973338.8654225}).
Epoch: [6][228/500]	Time  6.038 ( 6.038)	Loss 1.1797 (1.5973)	CeLoss 1.1797 (0.5265)	SegCLSLoss 0.0000 (0.0115)	KLLoss 0.0000 (0.2545)	MaskLoss 0.0000 (0.5199)	MaskBCELoss 0.0000 (0.0745)	MaskDICELoss 0.0000 (0.4453)
Epoch: [6][229/500]	Time  5.884 ( 5.884)	Loss 1.8378 (1.6814)	CeLoss 0.2031 (0.3077)	SegCLSLoss 0.0170 (0.0184)	KLLoss 0.3594 (0.3287)	MaskLoss 0.7949 (0.6657)	MaskBCELoss 0.3136 (0.1463)	MaskDICELoss 0.4812 (0.5194)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 227 is less than current step: 499. Dropping entry: {'train/loss': 1.5973264932632447, 'train/ce_loss': 0.52646484375, 'train/seg_cls_loss': 0.011529541015625, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.07451154291629791, 'train/mask_dice_loss': 0.4453431159257889, 'train/mask_loss': 0.5198546588420868, 'metrics/total_secs_per_batch': 6.037837028503418, 'metrics/data_secs_per_batch': 2.5277536630630495, '_timestamp': 1740973344.9031246}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 227 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973344.9034903}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 228 is less than current step: 499. Dropping entry: {'train/loss': 1.6813546895980835, 'train/ce_loss': 0.30771484375, 'train/seg_cls_loss': 0.0184326171875, 'train/kl_loss': 0.3287109375, 'train/mask_bce_loss': 0.1462902430444956, 'train/mask_dice_loss': 0.5194359183311462, 'train/mask_loss': 0.6657261610031128, 'metrics/total_secs_per_batch': 5.884326457977295, 'metrics/data_secs_per_batch': 2.6947978734970093, '_timestamp': 1740973350.7873132}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 228 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973350.7876575}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 229 is less than current step: 499. Dropping entry: {'train/loss': 1.5633212864398955, 'train/ce_loss': 0.300390625, 'train/seg_cls_loss': 0.0176025390625, 'train/kl_loss': 0.2935546875, 'train/mask_bce_loss': 0.0964719619601965, 'train/mask_dice_loss': 0.5160480678081513, 'train/mask_loss': 0.6125200241804123, 'metrics/total_secs_per_batch': 5.351212978363037, 'metrics/data_secs_per_batch': 2.329767179489136, '_timestamp': 1740973356.1383324}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 229 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973356.1386187}).
[2025-03-02 21:42:36,132] [INFO] [logging.py:96:log_dist] [Rank 0] step=3230, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:42:36,137] [INFO] [timer.py:215:stop] epoch=0/micro_step=32300/global_step=3230, RunningAvgSamplesPerSec=1.4365049447579188, CurrSamplesPerSec=1.8693237723357192, MemAllocated=31.45GB, MaxMemAllocated=37.23GB
Epoch: [6][230/500]	Time  5.351 ( 5.351)	Loss 1.9926 (1.5633)	CeLoss 0.2314 (0.3004)	SegCLSLoss 0.0206 (0.0176)	KLLoss 0.3574 (0.2936)	MaskLoss 0.8576 (0.6125)	MaskBCELoss 0.0491 (0.0965)	MaskDICELoss 0.8085 (0.5160)
Epoch: [6][231/500]	Time  5.460 ( 5.460)	Loss 1.7209 (1.3320)	CeLoss 0.2734 (0.4606)	SegCLSLoss 0.0115 (0.0080)	KLLoss 0.3633 (0.1803)	MaskLoss 0.7022 (0.4245)	MaskBCELoss 0.1117 (0.0712)	MaskDICELoss 0.5905 (0.3533)
Epoch: [6][232/500]	Time  5.834 ( 5.834)	Loss 2.3947 (1.7959)	CeLoss 0.3105 (0.6191)	SegCLSLoss 0.0189 (0.0101)	KLLoss 0.4199 (0.2619)	MaskLoss 1.0157 (0.5726)	MaskBCELoss 0.0159 (0.1361)	MaskDICELoss 0.9998 (0.4365)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 230 is less than current step: 499. Dropping entry: {'train/loss': 1.3320191144943236, 'train/ce_loss': 0.46064453125, 'train/seg_cls_loss': 0.008038330078125, 'train/kl_loss': 0.1802734375, 'train/mask_bce_loss': 0.07120555615983903, 'train/mask_dice_loss': 0.3532512664794922, 'train/mask_loss': 0.42445682287216185, 'metrics/total_secs_per_batch': 5.460155487060547, 'metrics/data_secs_per_batch': 2.3577922344207765, '_timestamp': 1740973361.5986843}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 230 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973361.5989573}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 231 is less than current step: 499. Dropping entry: {'train/loss': 1.795945680141449, 'train/ce_loss': 0.619140625, 'train/seg_cls_loss': 0.010125732421875, 'train/kl_loss': 0.2619140625, 'train/mask_bce_loss': 0.13611631654202938, 'train/mask_dice_loss': 0.4364658832550049, 'train/mask_loss': 0.5725822061300277, 'metrics/total_secs_per_batch': 5.833949565887451, 'metrics/data_secs_per_batch': 2.741812324523926, '_timestamp': 1740973367.432624}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 231 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973367.4329853}).
Epoch: [6][233/500]	Time  6.315 ( 6.315)	Loss 2.6705 (1.8010)	CeLoss 0.1875 (0.3047)	SegCLSLoss 0.0156 (0.0163)	KLLoss 0.3574 (0.3260)	MaskLoss 1.2200 (0.7278)	MaskBCELoss 0.2896 (0.1082)	MaskDICELoss 0.9304 (0.6196)
Epoch: [6][234/500]	Time  6.811 ( 6.811)	Loss 1.2578 (1.6609)	CeLoss 1.2578 (0.3990)	SegCLSLoss 0.0000 (0.0139)	KLLoss 0.0000 (0.2908)	MaskLoss 0.0000 (0.6130)	MaskBCELoss 0.0000 (0.0882)	MaskDICELoss 0.0000 (0.5248)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 232 is less than current step: 499. Dropping entry: {'train/loss': 1.8009639143943788, 'train/ce_loss': 0.3046875, 'train/seg_cls_loss': 0.016253662109375, 'train/kl_loss': 0.3259765625, 'train/mask_bce_loss': 0.10820785723626614, 'train/mask_dice_loss': 0.6195689916610718, 'train/mask_loss': 0.7277768611907959, 'metrics/total_secs_per_batch': 6.315445423126221, 'metrics/data_secs_per_batch': 2.8206601619720457, '_timestamp': 1740973373.748053}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 232 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973373.7483335}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 233 is less than current step: 499. Dropping entry: {'train/loss': 1.660897147655487, 'train/ce_loss': 0.3990234375, 'train/seg_cls_loss': 0.013861083984375, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.0882068295031786, 'train/mask_dice_loss': 0.5248101115226745, 'train/mask_loss': 0.6130169332027435, 'metrics/total_secs_per_batch': 6.810703754425049, 'metrics/data_secs_per_batch': 3.2465285062789917, '_timestamp': 1740973380.5587637}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 233 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973380.5590355}).
Epoch: [6][235/500]	Time  7.809 ( 7.809)	Loss 2.1015 (1.5738)	CeLoss 0.1719 (0.2856)	SegCLSLoss 0.0260 (0.0152)	KLLoss 0.3516 (0.3246)	MaskLoss 0.9409 (0.6239)	MaskBCELoss 0.0246 (0.0947)	MaskDICELoss 0.9163 (0.5293)
Epoch: [6][236/500]	Time  4.879 ( 4.879)	Loss 2.0939 (1.5503)	CeLoss 0.2402 (0.4986)	SegCLSLoss 0.0177 (0.0109)	KLLoss 0.3574 (0.2162)	MaskLoss 0.9044 (0.5124)	MaskBCELoss 0.0161 (0.1561)	MaskDICELoss 0.8882 (0.3563)
Epoch: [6][237/500]	Time  5.859 ( 5.859)	Loss 1.0312 (1.5930)	CeLoss 1.0312 (0.3952)	SegCLSLoss 0.0000 (0.0134)	KLLoss 0.0000 (0.2529)	MaskLoss 0.0000 (0.5829)	MaskBCELoss 0.0000 (0.0666)	MaskDICELoss 0.0000 (0.5162)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 234 is less than current step: 499. Dropping entry: {'train/loss': 1.5737680792808533, 'train/ce_loss': 0.28564453125, 'train/seg_cls_loss': 0.01524658203125, 'train/kl_loss': 0.324609375, 'train/mask_bce_loss': 0.09467931184917688, 'train/mask_dice_loss': 0.5292652636766434, 'train/mask_loss': 0.6239445775747299, 'metrics/total_secs_per_batch': 7.808835744857788, 'metrics/data_secs_per_batch': 3.6677319526672365, '_timestamp': 1740973388.3675785}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 234 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973388.367883}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 235 is less than current step: 499. Dropping entry: {'train/loss': 1.5503489434719087, 'train/ce_loss': 0.4986328125, 'train/seg_cls_loss': 0.010943603515625, 'train/kl_loss': 0.2162109375, 'train/mask_bce_loss': 0.15613040402531625, 'train/mask_dice_loss': 0.3562999352812767, 'train/mask_loss': 0.5124303489923477, 'metrics/total_secs_per_batch': 4.878633975982666, 'metrics/data_secs_per_batch': 2.2138776779174805, '_timestamp': 1740973393.2462387}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 235 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973393.246608}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 236 is less than current step: 499. Dropping entry: {'train/loss': 1.5930380702018738, 'train/ce_loss': 0.395166015625, 'train/seg_cls_loss': 0.013421630859375, 'train/kl_loss': 0.2529296875, 'train/mask_bce_loss': 0.06663811802864075, 'train/mask_dice_loss': 0.5162334531545639, 'train/mask_loss': 0.5828715741634369, 'metrics/total_secs_per_batch': 5.859060525894165, 'metrics/data_secs_per_batch': 2.898219585418701, '_timestamp': 1740973399.1055088}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 236 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973399.1058817}).
Epoch: [6][238/500]	Time  6.585 ( 6.585)	Loss 2.2745 (1.5789)	CeLoss 0.2871 (0.3968)	SegCLSLoss 0.0113 (0.0122)	KLLoss 0.3633 (0.2537)	MaskLoss 0.9722 (0.5753)	MaskBCELoss 0.1447 (0.0825)	MaskDICELoss 0.8275 (0.4929)
Epoch: [6][239/500]	Time  5.764 ( 5.764)	Loss 0.9375 (1.6069)	CeLoss 0.9375 (0.4014)	SegCLSLoss 0.0000 (0.0122)	KLLoss 0.0000 (0.2902)	MaskLoss 0.0000 (0.5852)	MaskBCELoss 0.0000 (0.1068)	MaskDICELoss 0.0000 (0.4784)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 237 is less than current step: 499. Dropping entry: {'train/loss': 1.5789116978645326, 'train/ce_loss': 0.39677734375, 'train/seg_cls_loss': 0.012152099609375, 'train/kl_loss': 0.2537109375, 'train/mask_bce_loss': 0.08245330210775137, 'train/mask_dice_loss': 0.4928912192583084, 'train/mask_loss': 0.5753445208072663, 'metrics/total_secs_per_batch': 6.585372686386108, 'metrics/data_secs_per_batch': 2.565256214141846, '_timestamp': 1740973405.6907}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 237 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973405.690906}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 238 is less than current step: 499. Dropping entry: {'train/loss': 1.6069381952285766, 'train/ce_loss': 0.4013671875, 'train/seg_cls_loss': 0.012237548828125, 'train/kl_loss': 0.290234375, 'train/mask_bce_loss': 0.10677812350913882, 'train/mask_dice_loss': 0.4784292608499527, 'train/mask_loss': 0.5852073818445206, 'metrics/total_secs_per_batch': 5.763823747634888, 'metrics/data_secs_per_batch': 2.53707435131073, '_timestamp': 1740973411.454544}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 238 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973411.4548266}).
[2025-03-02 21:43:37,403] [INFO] [logging.py:96:log_dist] [Rank 0] step=3240, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:43:37,409] [INFO] [timer.py:215:stop] epoch=0/micro_step=32400/global_step=3240, RunningAvgSamplesPerSec=1.4370375448136925, CurrSamplesPerSec=1.6795129012799892, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [6][240/500]	Time  5.956 ( 5.956)	Loss 1.8340 (1.8146)	CeLoss 0.2031 (0.4968)	SegCLSLoss 0.0156 (0.0139)	KLLoss 0.3574 (0.2588)	MaskLoss 0.7939 (0.6426)	MaskBCELoss 0.0985 (0.1217)	MaskDICELoss 0.6955 (0.5209)
Epoch: [6][241/500]	Time  6.713 ( 6.713)	Loss 1.4248 (2.0039)	CeLoss 0.1738 (0.2836)	SegCLSLoss 0.0203 (0.0160)	KLLoss 0.3613 (0.3285)	MaskLoss 0.6025 (0.8398)	MaskBCELoss 0.0497 (0.2542)	MaskDICELoss 0.5528 (0.5856)
Epoch: [6][242/500]	Time  4.825 ( 4.825)	Loss 1.6204 (1.5861)	CeLoss 0.2373 (0.2828)	SegCLSLoss 0.0121 (0.0121)	KLLoss 0.3613 (0.2920)	MaskLoss 0.6705 (0.6341)	MaskBCELoss 0.0758 (0.1433)	MaskDICELoss 0.5948 (0.4908)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 239 is less than current step: 499. Dropping entry: {'train/loss': 1.8145702958106995, 'train/ce_loss': 0.49677734375, 'train/seg_cls_loss': 0.01390380859375, 'train/kl_loss': 0.2587890625, 'train/mask_bce_loss': 0.12171643935143947, 'train/mask_dice_loss': 0.5208714365959167, 'train/mask_loss': 0.6425878822803497, 'metrics/total_secs_per_batch': 5.955717086791992, 'metrics/data_secs_per_batch': 2.4143117904663085, '_timestamp': 1740973417.410038}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 239 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973417.4103005}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 240 is less than current step: 499. Dropping entry: {'train/loss': 2.003940773010254, 'train/ce_loss': 0.28359375, 'train/seg_cls_loss': 0.015997314453125, 'train/kl_loss': 0.328515625, 'train/mask_bce_loss': 0.2541919649578631, 'train/mask_dice_loss': 0.5856202185153961, 'train/mask_loss': 0.8398121774196625, 'metrics/total_secs_per_batch': 6.713377475738525, 'metrics/data_secs_per_batch': 3.072382855415344, '_timestamp': 1740973424.1236231}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 240 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973424.1239069}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 241 is less than current step: 499. Dropping entry: {'train/loss': 1.5861303210258484, 'train/ce_loss': 0.282763671875, 'train/seg_cls_loss': 0.012109375, 'train/kl_loss': 0.2919921875, 'train/mask_bce_loss': 0.14326561510097235, 'train/mask_dice_loss': 0.49079075157642366, 'train/mask_loss': 0.6340563714504241, 'metrics/total_secs_per_batch': 4.824577331542969, 'metrics/data_secs_per_batch': 2.0689977169036866, '_timestamp': 1740973428.9482167}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 241 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973428.9484994}).
Epoch: [6][243/500]	Time  6.443 ( 6.443)	Loss 1.8568 (1.9778)	CeLoss 0.2695 (0.4976)	SegCLSLoss 0.0110 (0.0104)	KLLoss 0.3672 (0.2910)	MaskLoss 0.7721 (0.7229)	MaskBCELoss 0.0135 (0.1580)	MaskDICELoss 0.7586 (0.5649)
Epoch: [6][244/500]	Time  5.152 ( 5.152)	Loss 1.2891 (1.4816)	CeLoss 1.2891 (0.6647)	SegCLSLoss 0.0000 (0.0074)	KLLoss 0.0000 (0.1811)	MaskLoss 0.0000 (0.3976)	MaskBCELoss 0.0000 (0.0444)	MaskDICELoss 0.0000 (0.3532)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 242 is less than current step: 499. Dropping entry: {'train/loss': 1.9778222680091857, 'train/ce_loss': 0.49755859375, 'train/seg_cls_loss': 0.010400390625, 'train/kl_loss': 0.291015625, 'train/mask_bce_loss': 0.15800198987126352, 'train/mask_dice_loss': 0.5648935019969941, 'train/mask_loss': 0.722895497083664, 'metrics/total_secs_per_batch': 6.443011522293091, 'metrics/data_secs_per_batch': 2.7049911975860597, '_timestamp': 1740973435.3912005}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 242 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973435.3914757}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 243 is less than current step: 499. Dropping entry: {'train/loss': 1.481558585166931, 'train/ce_loss': 0.66474609375, 'train/seg_cls_loss': 0.0073974609375, 'train/kl_loss': 0.1810546875, 'train/mask_bce_loss': 0.044445961923338474, 'train/mask_dice_loss': 0.3531692624092102, 'train/mask_loss': 0.39761523604393006, 'metrics/total_secs_per_batch': 5.151846170425415, 'metrics/data_secs_per_batch': 2.603360652923584, '_timestamp': 1740973440.543046}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 243 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973440.543319}).
Epoch: [6][245/500]	Time  6.227 ( 6.227)	Loss 2.6078 (1.5927)	CeLoss 0.1787 (0.4129)	SegCLSLoss 0.0193 (0.0127)	KLLoss 0.3828 (0.2580)	MaskLoss 1.1906 (0.5738)	MaskBCELoss 0.2660 (0.0753)	MaskDICELoss 0.9247 (0.4985)
Epoch: [6][246/500]	Time  7.626 ( 7.626)	Loss 2.2597 (1.9168)	CeLoss 0.1826 (0.3734)	SegCLSLoss 0.0238 (0.0155)	KLLoss 0.3555 (0.2906)	MaskLoss 1.0146 (0.7534)	MaskBCELoss 0.0153 (0.1459)	MaskDICELoss 0.9994 (0.6074)
Epoch: [6][247/500]	Time  6.533 ( 6.533)	Loss 1.5815 (1.6336)	CeLoss 0.2266 (0.3509)	SegCLSLoss 0.0139 (0.0114)	KLLoss 0.3711 (0.2902)	MaskLoss 0.6560 (0.6240)	MaskBCELoss 0.1148 (0.1342)	MaskDICELoss 0.5411 (0.4898)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 244 is less than current step: 499. Dropping entry: {'train/loss': 1.5926512897014617, 'train/ce_loss': 0.412890625, 'train/seg_cls_loss': 0.01273193359375, 'train/kl_loss': 0.2580078125, 'train/mask_bce_loss': 0.07532108696177602, 'train/mask_dice_loss': 0.4984947741031647, 'train/mask_loss': 0.5738158613443375, 'metrics/total_secs_per_batch': 6.227285861968994, 'metrics/data_secs_per_batch': 2.6405496835708617, '_timestamp': 1740973446.7704523}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 244 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973446.7708}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 245 is less than current step: 499. Dropping entry: {'train/loss': 1.9167919993400573, 'train/ce_loss': 0.3734375, 'train/seg_cls_loss': 0.0154541015625, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.14593792567029595, 'train/mask_dice_loss': 0.6074287682771683, 'train/mask_loss': 0.7533666789531708, 'metrics/total_secs_per_batch': 7.626055002212524, 'metrics/data_secs_per_batch': 2.927415704727173, '_timestamp': 1740973454.3964381}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 245 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973454.3967252}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 246 is less than current step: 499. Dropping entry: {'train/loss': 1.6335503995418548, 'train/ce_loss': 0.35087890625, 'train/seg_cls_loss': 0.011407470703125, 'train/kl_loss': 0.290234375, 'train/mask_bce_loss': 0.1341694502159953, 'train/mask_dice_loss': 0.48978348076343536, 'train/mask_loss': 0.623952916264534, 'metrics/total_secs_per_batch': 6.532577276229858, 'metrics/data_secs_per_batch': 3.0631278276443483, '_timestamp': 1740973460.9289856}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 246 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973460.92927}).
Epoch: [6][248/500]	Time  5.000 ( 5.000)	Loss 0.5658 (1.4390)	CeLoss 0.1670 (0.5365)	SegCLSLoss 0.0087 (0.0102)	KLLoss 0.3555 (0.2549)	MaskLoss 0.1794 (0.4360)	MaskBCELoss 0.0956 (0.1237)	MaskDICELoss 0.0838 (0.3123)
Epoch: [6][249/500]	Time  5.819 ( 5.819)	Loss 2.2070 (1.4130)	CeLoss 0.2139 (0.5672)	SegCLSLoss 0.0199 (0.0082)	KLLoss 0.3574 (0.2195)	MaskLoss 0.9736 (0.4098)	MaskBCELoss 0.4495 (0.1469)	MaskDICELoss 0.5241 (0.2629)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 247 is less than current step: 499. Dropping entry: {'train/loss': 1.4390285193920136, 'train/ce_loss': 0.5365234375, 'train/seg_cls_loss': 0.010247802734375, 'train/kl_loss': 0.2548828125, 'train/mask_bce_loss': 0.12373385485261679, 'train/mask_dice_loss': 0.31228431314229965, 'train/mask_loss': 0.4360181733965874, 'metrics/total_secs_per_batch': 4.999788999557495, 'metrics/data_secs_per_batch': 2.2412100315093992, '_timestamp': 1740973465.9289513}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 247 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973465.9292943}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 248 is less than current step: 499. Dropping entry: {'train/loss': 1.4130497574806213, 'train/ce_loss': 0.5671875, 'train/seg_cls_loss': 0.00821533203125, 'train/kl_loss': 0.21953125, 'train/mask_bce_loss': 0.14691998213529586, 'train/mask_dice_loss': 0.2629252105951309, 'train/mask_loss': 0.40984518826007843, 'metrics/total_secs_per_batch': 5.819447994232178, 'metrics/data_secs_per_batch': 2.5614381551742555, '_timestamp': 1740973471.7482479}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 248 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973471.7485273}).
[2025-03-02 21:44:38,875] [INFO] [logging.py:96:log_dist] [Rank 0] step=3250, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:44:38,880] [INFO] [timer.py:215:stop] epoch=0/micro_step=32500/global_step=3250, RunningAvgSamplesPerSec=1.4375545400947352, CurrSamplesPerSec=1.4021692131766539, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [6][250/500]	Time  7.133 ( 7.133)	Loss 3.1864 (1.9397)	CeLoss 0.3574 (0.2681)	SegCLSLoss 0.0203 (0.0190)	KLLoss 0.3633 (0.3623)	MaskLoss 1.3911 (0.8129)	MaskBCELoss 0.5263 (0.1692)	MaskDICELoss 0.8648 (0.6437)
Epoch: [6][251/500]	Time  5.980 ( 5.980)	Loss 1.1953 (2.0149)	CeLoss 1.1953 (0.4021)	SegCLSLoss 0.0000 (0.0174)	KLLoss 0.0000 (0.2902)	MaskLoss 0.0000 (0.7875)	MaskBCELoss 0.0000 (0.1395)	MaskDICELoss 0.0000 (0.6480)
Epoch: [6][252/500]	Time  4.620 ( 4.620)	Loss 0.8320 (1.4434)	CeLoss 0.8320 (0.5711)	SegCLSLoss 0.0000 (0.0104)	KLLoss 0.0000 (0.2162)	MaskLoss 0.0000 (0.4227)	MaskBCELoss 0.0000 (0.0747)	MaskDICELoss 0.0000 (0.3480)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 249 is less than current step: 499. Dropping entry: {'train/loss': 1.9397291660308837, 'train/ce_loss': 0.26806640625, 'train/seg_cls_loss': 0.019036865234375, 'train/kl_loss': 0.3623046875, 'train/mask_bce_loss': 0.16923707388341427, 'train/mask_dice_loss': 0.6436939090490341, 'train/mask_loss': 0.8129309803247452, 'metrics/total_secs_per_batch': 7.133401155471802, 'metrics/data_secs_per_batch': 3.1154672622680666, '_timestamp': 1740973478.8814414}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 249 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973478.8817258}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 250 is less than current step: 499. Dropping entry: {'train/loss': 2.0148541212081907, 'train/ce_loss': 0.40205078125, 'train/seg_cls_loss': 0.0173828125, 'train/kl_loss': 0.290234375, 'train/mask_bce_loss': 0.1394601443782449, 'train/mask_dice_loss': 0.6480450391769409, 'train/mask_loss': 0.7875051915645599, 'metrics/total_secs_per_batch': 5.9796812534332275, 'metrics/data_secs_per_batch': 2.861836624145508, '_timestamp': 1740973484.8615284}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 250 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973484.8618798}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 251 is less than current step: 499. Dropping entry: {'train/loss': 1.4433843493461609, 'train/ce_loss': 0.57109375, 'train/seg_cls_loss': 0.0104248046875, 'train/kl_loss': 0.2162109375, 'train/mask_bce_loss': 0.07468225378543139, 'train/mask_dice_loss': 0.34803531169891355, 'train/mask_loss': 0.4227175712585449, 'metrics/total_secs_per_batch': 4.620140790939331, 'metrics/data_secs_per_batch': 2.177886128425598, '_timestamp': 1740973489.4814882}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 251 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973489.4817696}).
Epoch: [6][253/500]	Time  5.680 ( 5.680)	Loss 1.6479 (1.6721)	CeLoss 0.2119 (0.5652)	SegCLSLoss 0.0145 (0.0105)	KLLoss 0.3711 (0.2184)	MaskLoss 0.6960 (0.5399)	MaskBCELoss 0.0305 (0.0606)	MaskDICELoss 0.6655 (0.4794)
Epoch: [6][254/500]	Time  6.163 ( 6.163)	Loss 2.6570 (1.8636)	CeLoss 0.1631 (0.3230)	SegCLSLoss 0.0147 (0.0136)	KLLoss 0.3633 (0.2920)	MaskLoss 1.2250 (0.7522)	MaskBCELoss 0.8008 (0.2141)	MaskDICELoss 0.4241 (0.5381)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 252 is less than current step: 499. Dropping entry: {'train/loss': 1.672062838077545, 'train/ce_loss': 0.565234375, 'train/seg_cls_loss': 0.010455322265625, 'train/kl_loss': 0.218359375, 'train/mask_bce_loss': 0.06057829149067402, 'train/mask_dice_loss': 0.479359382390976, 'train/mask_loss': 0.5399376690387726, 'metrics/total_secs_per_batch': 5.6802239418029785, 'metrics/data_secs_per_batch': 2.5861114025115968, '_timestamp': 1740973495.1616883}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 252 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973495.1620498}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 253 is less than current step: 499. Dropping entry: {'train/loss': 1.863609004020691, 'train/ce_loss': 0.323046875, 'train/seg_cls_loss': 0.01357421875, 'train/kl_loss': 0.2919921875, 'train/mask_bce_loss': 0.2141172207891941, 'train/mask_dice_loss': 0.5380974322557449, 'train/mask_loss': 0.7522146552801132, 'metrics/total_secs_per_batch': 6.162694931030273, 'metrics/data_secs_per_batch': 3.0508874654769897, '_timestamp': 1740973501.324394}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 253 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973501.3246646}).
Epoch: [6][255/500]	Time  5.878 ( 5.878)	Loss 2.0498 (1.5523)	CeLoss 0.2158 (0.3462)	SegCLSLoss 0.0212 (0.0174)	KLLoss 0.3555 (0.3258)	MaskLoss 0.8940 (0.5823)	MaskBCELoss 0.0329 (0.0638)	MaskDICELoss 0.8612 (0.5186)
Epoch: [6][256/500]	Time  5.484 ( 5.484)	Loss 2.0647 (1.8077)	CeLoss 0.2598 (0.5365)	SegCLSLoss 0.0203 (0.0120)	KLLoss 0.3594 (0.2188)	MaskLoss 0.8800 (0.6218)	MaskBCELoss 0.0022 (0.1479)	MaskDICELoss 0.8778 (0.4738)
Epoch: [6][257/500]	Time  6.690 ( 6.690)	Loss 2.1520 (1.7537)	CeLoss 0.2734 (0.3637)	SegCLSLoss 0.0192 (0.0128)	KLLoss 0.3477 (0.3260)	MaskLoss 0.9168 (0.6754)	MaskBCELoss 0.0275 (0.1755)	MaskDICELoss 0.8894 (0.4998)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 254 is less than current step: 499. Dropping entry: {'train/loss': 1.552279955148697, 'train/ce_loss': 0.34619140625, 'train/seg_cls_loss': 0.017413330078125, 'train/kl_loss': 0.32578125, 'train/mask_bce_loss': 0.06376156257465482, 'train/mask_dice_loss': 0.5185795903205872, 'train/mask_loss': 0.582341143488884, 'metrics/total_secs_per_batch': 5.877922296524048, 'metrics/data_secs_per_batch': 2.54484167098999, '_timestamp': 1740973507.2023022}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 254 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973507.20259}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 255 is less than current step: 499. Dropping entry: {'train/loss': 1.8077436089515686, 'train/ce_loss': 0.536474609375, 'train/seg_cls_loss': 0.01204833984375, 'train/kl_loss': 0.21875, 'train/mask_bce_loss': 0.14794094040989875, 'train/mask_dice_loss': 0.4738263487815857, 'train/mask_loss': 0.6217673003673554, 'metrics/total_secs_per_batch': 5.484293222427368, 'metrics/data_secs_per_batch': 2.5025537967681886, '_timestamp': 1740973512.6868358}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 255 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973512.6872818}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 256 is less than current step: 499. Dropping entry: {'train/loss': 1.7536695837974547, 'train/ce_loss': 0.363671875, 'train/seg_cls_loss': 0.012762451171875, 'train/kl_loss': 0.3259765625, 'train/mask_bce_loss': 0.1755258424207568, 'train/mask_dice_loss': 0.4998440995812416, 'train/mask_loss': 0.6753699451684951, 'metrics/total_secs_per_batch': 6.689713716506958, 'metrics/data_secs_per_batch': 2.857617998123169, '_timestamp': 1740973519.3763099}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 256 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973519.376578}).
Epoch: [6][258/500]	Time  6.220 ( 6.220)	Loss 1.7491 (1.8335)	CeLoss 0.2363 (0.3343)	SegCLSLoss 0.0110 (0.0152)	KLLoss 0.3672 (0.3281)	MaskLoss 0.7349 (0.7293)	MaskBCELoss 0.2023 (0.1698)	MaskDICELoss 0.5327 (0.5595)
Epoch: [6][259/500]	Time  6.129 ( 6.129)	Loss 2.2939 (1.5900)	CeLoss 0.1416 (0.4623)	SegCLSLoss 0.0249 (0.0125)	KLLoss 0.4004 (0.2574)	MaskLoss 1.0498 (0.5479)	MaskBCELoss 0.2396 (0.0906)	MaskDICELoss 0.8101 (0.4572)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 257 is less than current step: 499. Dropping entry: {'train/loss': 1.8334591269493103, 'train/ce_loss': 0.33427734375, 'train/seg_cls_loss': 0.015240478515625, 'train/kl_loss': 0.328125, 'train/mask_bce_loss': 0.16983107840642334, 'train/mask_dice_loss': 0.5594961315393447, 'train/mask_loss': 0.7293272137641906, 'metrics/total_secs_per_batch': 6.2198405265808105, 'metrics/data_secs_per_batch': 2.7059861421585083, '_timestamp': 1740973525.5961664}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 257 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973525.5964413}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 258 is less than current step: 499. Dropping entry: {'train/loss': 1.589956247806549, 'train/ce_loss': 0.4623046875, 'train/seg_cls_loss': 0.01251220703125, 'train/kl_loss': 0.257421875, 'train/mask_bce_loss': 0.09061574693769217, 'train/mask_dice_loss': 0.4572432368993759, 'train/mask_loss': 0.547858989238739, 'metrics/total_secs_per_batch': 6.128540515899658, 'metrics/data_secs_per_batch': 2.771372103691101, '_timestamp': 1740973531.7247076}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 258 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973531.7250373}).
[2025-03-02 21:45:39,228] [INFO] [logging.py:96:log_dist] [Rank 0] step=3260, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:45:39,234] [INFO] [timer.py:215:stop] epoch=0/micro_step=32600/global_step=3260, RunningAvgSamplesPerSec=1.438139722918892, CurrSamplesPerSec=1.3317782017807638, MemAllocated=31.28GB, MaxMemAllocated=37.23GB
Epoch: [6][260/500]	Time  7.510 ( 7.510)	Loss 2.8753 (1.8045)	CeLoss 0.1855 (0.4490)	SegCLSLoss 0.0206 (0.0122)	KLLoss 0.3633 (0.2902)	MaskLoss 1.3215 (0.6599)	MaskBCELoss 0.5386 (0.1752)	MaskDICELoss 0.7829 (0.4847)
Epoch: [6][261/500]	Time  7.204 ( 7.204)	Loss 2.0558 (1.8036)	CeLoss 0.1797 (0.1811)	SegCLSLoss 0.0300 (0.0157)	KLLoss 0.3672 (0.2934)	MaskLoss 0.9126 (0.7927)	MaskBCELoss 0.0197 (0.1815)	MaskDICELoss 0.8929 (0.6112)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 259 is less than current step: 499. Dropping entry: {'train/loss': 1.8045026063919067, 'train/ce_loss': 0.4490234375, 'train/seg_cls_loss': 0.012237548828125, 'train/kl_loss': 0.290234375, 'train/mask_bce_loss': 0.17517663035541772, 'train/mask_dice_loss': 0.48474067002534865, 'train/mask_loss': 0.6599173098802567, 'metrics/total_secs_per_batch': 7.510357618331909, 'metrics/data_secs_per_batch': 3.521840810775757, '_timestamp': 1740973539.2348607}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 259 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973539.235047}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 260 is less than current step: 499. Dropping entry: {'train/loss': 1.8035847425460816, 'train/ce_loss': 0.1810546875, 'train/seg_cls_loss': 0.015655517578125, 'train/kl_loss': 0.293359375, 'train/mask_bce_loss': 0.1815498536452651, 'train/mask_dice_loss': 0.6111849039793015, 'train/mask_loss': 0.7927347540855407, 'metrics/total_secs_per_batch': 7.204337120056152, 'metrics/data_secs_per_batch': 3.2358807563781737, '_timestamp': 1740973546.4396203}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 260 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973546.4400427}).
Epoch: [6][262/500]	Time  6.451 ( 6.451)	Loss 1.1344 (1.4939)	CeLoss 0.2598 (0.2141)	SegCLSLoss 0.0110 (0.0157)	KLLoss 0.3672 (0.3236)	MaskLoss 0.4158 (0.6199)	MaskBCELoss 0.1115 (0.0714)	MaskDICELoss 0.3043 (0.5485)
Epoch: [6][263/500]	Time  6.757 ( 6.757)	Loss 1.8231 (1.7168)	CeLoss 0.1953 (0.3458)	SegCLSLoss 0.0221 (0.0138)	KLLoss 0.3555 (0.3283)	MaskLoss 0.7904 (0.6654)	MaskBCELoss 0.0722 (0.0901)	MaskDICELoss 0.7182 (0.5754)
Epoch: [6][264/500]	Time  5.540 ( 5.540)	Loss 1.6967 (1.3143)	CeLoss 0.2676 (0.5235)	SegCLSLoss 0.0186 (0.0082)	KLLoss 0.3613 (0.1809)	MaskLoss 0.6921 (0.3843)	MaskBCELoss 0.0100 (0.0605)	MaskDICELoss 0.6821 (0.3238)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 261 is less than current step: 499. Dropping entry: {'train/loss': 1.4939124226570129, 'train/ce_loss': 0.2140625, 'train/seg_cls_loss': 0.01573486328125, 'train/kl_loss': 0.3236328125, 'train/mask_bce_loss': 0.07138016670942307, 'train/mask_dice_loss': 0.5484764188528061, 'train/mask_loss': 0.6198565930128097, 'metrics/total_secs_per_batch': 6.451424598693848, 'metrics/data_secs_per_batch': 3.0240252733230593, '_timestamp': 1740973552.890874}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 261 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973552.8911512}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 262 is less than current step: 499. Dropping entry: {'train/loss': 1.7168146193027496, 'train/ce_loss': 0.34580078125, 'train/seg_cls_loss': 0.01376953125, 'train/kl_loss': 0.3283203125, 'train/mask_bce_loss': 0.09007432237267494, 'train/mask_dice_loss': 0.5753642454743385, 'train/mask_loss': 0.6654385596513748, 'metrics/total_secs_per_batch': 6.75710916519165, 'metrics/data_secs_per_batch': 2.7148433208465574, '_timestamp': 1740973559.64797}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 262 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973559.648257}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 263 is less than current step: 499. Dropping entry: {'train/loss': 1.3143473505973815, 'train/ce_loss': 0.52353515625, 'train/seg_cls_loss': 0.008245849609375, 'train/kl_loss': 0.180859375, 'train/mask_bce_loss': 0.06052003214135766, 'train/mask_dice_loss': 0.3238020777702332, 'train/mask_loss': 0.3843221127986908, 'metrics/total_secs_per_batch': 5.54003381729126, 'metrics/data_secs_per_batch': 2.320767951011658, '_timestamp': 1740973565.1879663}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 263 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973565.1882956}).
Epoch: [6][265/500]	Time  6.620 ( 6.620)	Loss 0.1162 (1.1978)	CeLoss 0.1162 (0.2610)	SegCLSLoss 0.0000 (0.0103)	KLLoss 0.0000 (0.2160)	MaskLoss 0.0000 (0.4550)	MaskBCELoss 0.0000 (0.0722)	MaskDICELoss 0.0000 (0.3828)
Epoch: [6][266/500]	Time  6.346 ( 6.346)	Loss 1.5378 (1.4408)	CeLoss 0.2051 (0.2571)	SegCLSLoss 0.0208 (0.0122)	KLLoss 0.3516 (0.2527)	MaskLoss 0.6439 (0.5763)	MaskBCELoss 0.0113 (0.1209)	MaskDICELoss 0.6326 (0.4554)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 264 is less than current step: 499. Dropping entry: {'train/loss': 1.1977936148643493, 'train/ce_loss': 0.26103515625, 'train/seg_cls_loss': 0.010321044921875, 'train/kl_loss': 0.216015625, 'train/mask_bce_loss': 0.07224333323538304, 'train/mask_dice_loss': 0.38280582129955293, 'train/mask_loss': 0.4550491511821747, 'metrics/total_secs_per_batch': 6.619893789291382, 'metrics/data_secs_per_batch': 2.9235235691070556, '_timestamp': 1740973571.8079076}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 264 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973571.8081784}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 265 is less than current step: 499. Dropping entry: {'train/loss': 1.4408400893211364, 'train/ce_loss': 0.25712890625, 'train/seg_cls_loss': 0.0122314453125, 'train/kl_loss': 0.252734375, 'train/mask_bce_loss': 0.12086929595097899, 'train/mask_dice_loss': 0.4554101310670376, 'train/mask_loss': 0.576279417425394, 'metrics/total_secs_per_batch': 6.346328496932983, 'metrics/data_secs_per_batch': 3.0275543928146362, '_timestamp': 1740973578.1543584}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 265 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973578.1546955}).
Epoch: [6][267/500]	Time  6.343 ( 6.343)	Loss 2.3678 (1.4027)	CeLoss 0.2178 (0.3186)	SegCLSLoss 0.0205 (0.0126)	KLLoss 0.3691 (0.2557)	MaskLoss 1.0511 (0.5260)	MaskBCELoss 0.2517 (0.1070)	MaskDICELoss 0.7994 (0.4190)
Epoch: [6][268/500]	Time  6.861 ( 6.861)	Loss 2.3650 (2.0179)	CeLoss 0.1602 (0.2213)	SegCLSLoss 0.0275 (0.0154)	KLLoss 0.3691 (0.3652)	MaskLoss 1.0770 (0.8764)	MaskBCELoss 0.2611 (0.2497)	MaskDICELoss 0.8159 (0.6267)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 266 is less than current step: 499. Dropping entry: {'train/loss': 1.402701810002327, 'train/ce_loss': 0.318603515625, 'train/seg_cls_loss': 0.012603759765625, 'train/kl_loss': 0.2556640625, 'train/mask_bce_loss': 0.1069867430254817, 'train/mask_dice_loss': 0.4190467864274979, 'train/mask_loss': 0.5260335221886635, 'metrics/total_secs_per_batch': 6.343370676040649, 'metrics/data_secs_per_batch': 3.0551648855209352, '_timestamp': 1740973584.4977844}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 266 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973584.4981334}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 267 is less than current step: 499. Dropping entry: {'train/loss': 2.0178726077079774, 'train/ce_loss': 0.2212890625, 'train/seg_cls_loss': 0.015411376953125, 'train/kl_loss': 0.365234375, 'train/mask_bce_loss': 0.24971743803471327, 'train/mask_dice_loss': 0.6266505271196365, 'train/mask_loss': 0.8763679504394531, 'metrics/total_secs_per_batch': 6.860773801803589, 'metrics/data_secs_per_batch': 3.1110768795013426, '_timestamp': 1740973591.3584156}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 267 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973591.3587077}).
Epoch: [6][269/500]	Time  5.871 ( 5.871)	Loss 2.2596 (1.6993)	CeLoss 0.2129 (0.2790)	SegCLSLoss 0.0135 (0.0119)	KLLoss 0.3691 (0.2928)	MaskLoss 1.0019 (0.6926)	MaskBCELoss 0.4545 (0.2295)	MaskDICELoss 0.5474 (0.4631)
[2025-03-02 21:46:43,326] [INFO] [logging.py:96:log_dist] [Rank 0] step=3270, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:46:43,331] [INFO] [timer.py:215:stop] epoch=0/micro_step=32700/global_step=3270, RunningAvgSamplesPerSec=1.4384847145028803, CurrSamplesPerSec=1.6389665958739643, MemAllocated=30.72GB, MaxMemAllocated=37.23GB
Epoch: [6][270/500]	Time  6.103 ( 6.103)	Loss 1.0000 (1.9513)	CeLoss 1.0000 (0.5103)	SegCLSLoss 0.0000 (0.0123)	KLLoss 0.0000 (0.2537)	MaskLoss 0.0000 (0.7047)	MaskBCELoss 0.0000 (0.2015)	MaskDICELoss 0.0000 (0.5031)
Epoch: [6][271/500]	Time  5.210 ( 5.210)	Loss 1.7267 (1.3905)	CeLoss 0.2354 (0.4536)	SegCLSLoss 0.0121 (0.0118)	KLLoss 0.3633 (0.2545)	MaskLoss 0.7247 (0.4528)	MaskBCELoss 0.3308 (0.0711)	MaskDICELoss 0.3938 (0.3817)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 268 is less than current step: 499. Dropping entry: {'train/loss': 1.6992600381374359, 'train/ce_loss': 0.278955078125, 'train/seg_cls_loss': 0.011895751953125, 'train/kl_loss': 0.2927734375, 'train/mask_bce_loss': 0.22948891911655664, 'train/mask_dice_loss': 0.46313426494598386, 'train/mask_loss': 0.692623183131218, 'metrics/total_secs_per_batch': 5.8709800243377686, 'metrics/data_secs_per_batch': 2.4685149431228637, '_timestamp': 1740973597.229364}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 268 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973597.229563}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 269 is less than current step: 499. Dropping entry: {'train/loss': 1.951339888572693, 'train/ce_loss': 0.51025390625, 'train/seg_cls_loss': 0.012298583984375, 'train/kl_loss': 0.2537109375, 'train/mask_bce_loss': 0.20154737532138825, 'train/mask_dice_loss': 0.5031264901161194, 'train/mask_loss': 0.7046738684177398, 'metrics/total_secs_per_batch': 6.102996826171875, 'metrics/data_secs_per_batch': 3.2291393995285036, '_timestamp': 1740973603.332176}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 269 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973603.3324718}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 270 is less than current step: 499. Dropping entry: {'train/loss': 1.3904824018478394, 'train/ce_loss': 0.45361328125, 'train/seg_cls_loss': 0.01180419921875, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.07109364089556039, 'train/mask_dice_loss': 0.38166709095239637, 'train/mask_loss': 0.45276073217391966, 'metrics/total_secs_per_batch': 5.209833145141602, 'metrics/data_secs_per_batch': 2.033644461631775, '_timestamp': 1740973608.5421913}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 270 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973608.542462}).
Epoch: [6][272/500]	Time  6.705 ( 6.705)	Loss 0.1089 (1.7713)	CeLoss 0.1089 (0.3331)	SegCLSLoss 0.0000 (0.0130)	KLLoss 0.0000 (0.2924)	MaskLoss 0.0000 (0.7013)	MaskBCELoss 0.0000 (0.1973)	MaskDICELoss 0.0000 (0.5040)
Epoch: [6][273/500]	Time  5.600 ( 5.600)	Loss 1.7045 (1.6956)	CeLoss 0.2949 (0.1947)	SegCLSLoss 0.0107 (0.0185)	KLLoss 0.3633 (0.3268)	MaskLoss 0.6833 (0.7293)	MaskBCELoss 0.1111 (0.1238)	MaskDICELoss 0.5722 (0.6055)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 271 is less than current step: 499. Dropping entry: {'train/loss': 1.771267294883728, 'train/ce_loss': 0.333056640625, 'train/seg_cls_loss': 0.0129638671875, 'train/kl_loss': 0.2923828125, 'train/mask_bce_loss': 0.19734639972448348, 'train/mask_dice_loss': 0.5039854943752289, 'train/mask_loss': 0.7013318955898284, 'metrics/total_secs_per_batch': 6.705352544784546, 'metrics/data_secs_per_batch': 2.825305438041687, '_timestamp': 1740973615.2475748}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 271 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973615.2478566}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 272 is less than current step: 499. Dropping entry: {'train/loss': 1.6956104636192322, 'train/ce_loss': 0.1947265625, 'train/seg_cls_loss': 0.018499755859375, 'train/kl_loss': 0.3267578125, 'train/mask_bce_loss': 0.1238197486847639, 'train/mask_dice_loss': 0.605528450012207, 'train/mask_loss': 0.7293481916189194, 'metrics/total_secs_per_batch': 5.600124835968018, 'metrics/data_secs_per_batch': 2.528061318397522, '_timestamp': 1740973620.847672}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 272 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973620.8479674}).
Epoch: [6][274/500]	Time  6.758 ( 6.758)	Loss 2.0144 (1.7840)	CeLoss 0.1709 (0.2265)	SegCLSLoss 0.0282 (0.0178)	KLLoss 0.3516 (0.3604)	MaskLoss 0.8974 (0.7563)	MaskBCELoss 0.1164 (0.1060)	MaskDICELoss 0.7810 (0.6504)
Epoch: [6][275/500]	Time  6.795 ( 6.795)	Loss 2.1066 (1.9782)	CeLoss 0.2041 (0.3521)	SegCLSLoss 0.0125 (0.0164)	KLLoss 0.3594 (0.3268)	MaskLoss 0.9303 (0.7926)	MaskBCELoss 0.1334 (0.1257)	MaskDICELoss 0.7969 (0.6670)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 273 is less than current step: 499. Dropping entry: {'train/loss': 1.7839950919151306, 'train/ce_loss': 0.22646484375, 'train/seg_cls_loss': 0.017779541015625, 'train/kl_loss': 0.3603515625, 'train/mask_bce_loss': 0.10595175605267286, 'train/mask_dice_loss': 0.6503524333238602, 'train/mask_loss': 0.7563041865825653, 'metrics/total_secs_per_batch': 6.758077383041382, 'metrics/data_secs_per_batch': 2.8937835931777953, '_timestamp': 1740973627.6059139}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 273 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973627.6062708}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 274 is less than current step: 499. Dropping entry: {'train/loss': 1.9781946897506715, 'train/ce_loss': 0.3521484375, 'train/seg_cls_loss': 0.016448974609375, 'train/kl_loss': 0.3267578125, 'train/mask_bce_loss': 0.12565325601026417, 'train/mask_dice_loss': 0.6669597029685974, 'train/mask_loss': 0.7926129698753357, 'metrics/total_secs_per_batch': 6.7949652671813965, 'metrics/data_secs_per_batch': 3.124189782142639, '_timestamp': 1740973634.4007487}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 274 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973634.401028}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 275 is less than current step: 499. Dropping entry: {'train/loss': 2.024401903152466, 'train/ce_loss': 0.416064453125, 'train/seg_cls_loss': 0.015765380859375, 'train/kl_loss': 0.2916015625, 'train/mask_bce_loss': 0.12989949774928392, 'train/mask_dice_loss': 0.6558366060256958, 'train/mask_loss': 0.7857361078262329, 'metrics/total_secs_per_batch': 6.170556545257568, 'metrics/data_secs_per_batch': 2.6677386283874513, '_timestamp': 1740973640.5712519}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 275 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973640.571457}).
Epoch: [6][276/500]	Time  6.171 ( 6.171)	Loss 1.8736 (2.0244)	CeLoss 0.2285 (0.4161)	SegCLSLoss 0.0159 (0.0158)	KLLoss 0.3555 (0.2916)	MaskLoss 0.8011 (0.7857)	MaskBCELoss 0.0295 (0.1299)	MaskDICELoss 0.7716 (0.6558)
Epoch: [6][277/500]	Time  5.973 ( 5.973)	Loss 1.5526 (1.7389)	CeLoss 0.1904 (0.4888)	SegCLSLoss 0.0236 (0.0135)	KLLoss 0.3535 (0.2541)	MaskLoss 0.6577 (0.6088)	MaskBCELoss 0.0867 (0.1247)	MaskDICELoss 0.5709 (0.4841)
Epoch: [6][278/500]	Time  5.974 ( 5.974)	Loss 4.0502 (1.9344)	CeLoss 0.1973 (0.4292)	SegCLSLoss 0.0184 (0.0121)	KLLoss 0.3789 (0.2537)	MaskLoss 1.9030 (0.7369)	MaskBCELoss 1.1408 (0.2311)	MaskDICELoss 0.7622 (0.5059)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 276 is less than current step: 499. Dropping entry: {'train/loss': 1.7388510942459106, 'train/ce_loss': 0.48876953125, 'train/seg_cls_loss': 0.01348876953125, 'train/kl_loss': 0.2541015625, 'train/mask_bce_loss': 0.12472368283197284, 'train/mask_dice_loss': 0.4841061502695084, 'train/mask_loss': 0.6088298320770263, 'metrics/total_secs_per_batch': 5.973334550857544, 'metrics/data_secs_per_batch': 2.442065405845642, '_timestamp': 1740973646.5448468}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 276 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973646.5452156}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 277 is less than current step: 499. Dropping entry: {'train/loss': 1.934356117248535, 'train/ce_loss': 0.429248046875, 'train/seg_cls_loss': 0.01209716796875, 'train/kl_loss': 0.2537109375, 'train/mask_bce_loss': 0.2310612941160798, 'train/mask_dice_loss': 0.5058677613735199, 'train/mask_loss': 0.7369290471076966, 'metrics/total_secs_per_batch': 5.974280118942261, 'metrics/data_secs_per_batch': 2.7854362964630126, '_timestamp': 1740973652.5189276}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 277 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973652.5192883}).
Epoch: [6][279/500]	Time  4.625 ( 4.625)	Loss 1.2500 (1.3829)	CeLoss 1.2500 (0.5535)	SegCLSLoss 0.0000 (0.0082)	KLLoss 0.0000 (0.1844)	MaskLoss 0.0000 (0.4034)	MaskBCELoss 0.0000 (0.0868)	MaskDICELoss 0.0000 (0.3166)
[2025-03-02 21:47:42,625] [INFO] [logging.py:96:log_dist] [Rank 0] step=3280, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:47:42,630] [INFO] [timer.py:215:stop] epoch=0/micro_step=32800/global_step=3280, RunningAvgSamplesPerSec=1.4391308887366576, CurrSamplesPerSec=1.8231222343547222, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [6][280/500]	Time  5.487 ( 5.487)	Loss 1.2384 (1.5856)	CeLoss 0.3066 (0.2610)	SegCLSLoss 0.0090 (0.0130)	KLLoss 0.3672 (0.2904)	MaskLoss 0.4454 (0.6446)	MaskBCELoss 0.0595 (0.1354)	MaskDICELoss 0.3859 (0.5092)
Epoch: [6][281/500]	Time  6.094 ( 6.094)	Loss 1.2443 (1.1005)	CeLoss 0.3750 (0.4640)	SegCLSLoss 0.0093 (0.0080)	KLLoss 0.3594 (0.1834)	MaskLoss 0.4151 (0.3072)	MaskBCELoss 0.2369 (0.0886)	MaskDICELoss 0.1782 (0.2186)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 278 is less than current step: 499. Dropping entry: {'train/loss': 1.382856583595276, 'train/ce_loss': 0.553515625, 'train/seg_cls_loss': 0.0082275390625, 'train/kl_loss': 0.184375, 'train/mask_bce_loss': 0.08679905198514462, 'train/mask_dice_loss': 0.31659212708473206, 'train/mask_loss': 0.40339118242263794, 'metrics/total_secs_per_batch': 4.625133037567139, 'metrics/data_secs_per_batch': 2.3542743921279907, '_timestamp': 1740973657.1440425}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 278 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973657.1443233}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 279 is less than current step: 499. Dropping entry: {'train/loss': 1.5856050252914429, 'train/ce_loss': 0.260986328125, 'train/seg_cls_loss': 0.013031005859375, 'train/kl_loss': 0.2904296875, 'train/mask_bce_loss': 0.13542124601081013, 'train/mask_dice_loss': 0.5092123180627823, 'train/mask_loss': 0.6446335643529892, 'metrics/total_secs_per_batch': 5.486964464187622, 'metrics/data_secs_per_batch': 2.3975561618804933, '_timestamp': 1740973662.6309905}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 279 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973662.6313825}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 280 is less than current step: 499. Dropping entry: {'train/loss': 1.1005042672157288, 'train/ce_loss': 0.46396484375, 'train/seg_cls_loss': 0.008001708984375, 'train/kl_loss': 0.1833984375, 'train/mask_bce_loss': 0.08856358481571078, 'train/mask_dice_loss': 0.21862214654684067, 'train/mask_loss': 0.3071857303380966, 'metrics/total_secs_per_batch': 6.094482183456421, 'metrics/data_secs_per_batch': 3.1614081144332884, '_timestamp': 1740973668.7256863}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 280 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973668.7260232}).
Epoch: [6][282/500]	Time  5.421 ( 5.421)	Loss 1.2072 (1.5070)	CeLoss 0.1904 (0.4986)	SegCLSLoss 0.0120 (0.0106)	KLLoss 0.3652 (0.2170)	MaskLoss 0.4874 (0.4907)	MaskBCELoss 0.0705 (0.0793)	MaskDICELoss 0.4169 (0.4114)
Epoch: [6][283/500]	Time  6.576 ( 6.576)	Loss 2.5404 (1.8275)	CeLoss 0.2051 (0.3937)	SegCLSLoss 0.0199 (0.0142)	KLLoss 0.3867 (0.2545)	MaskLoss 1.1432 (0.7007)	MaskBCELoss 0.3457 (0.1897)	MaskDICELoss 0.7976 (0.5110)
Epoch: [6][284/500]	Time  5.009 ( 5.009)	Loss 1.2056 (1.5200)	CeLoss 0.1416 (0.4787)	SegCLSLoss 0.0278 (0.0135)	KLLoss 0.3711 (0.2547)	MaskLoss 0.5066 (0.5046)	MaskBCELoss 0.0104 (0.1220)	MaskDICELoss 0.4962 (0.3826)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 281 is less than current step: 499. Dropping entry: {'train/loss': 1.506992506980896, 'train/ce_loss': 0.498583984375, 'train/seg_cls_loss': 0.01063232421875, 'train/kl_loss': 0.2169921875, 'train/mask_bce_loss': 0.07933467738330365, 'train/mask_dice_loss': 0.41139301359653474, 'train/mask_loss': 0.49072769582271575, 'metrics/total_secs_per_batch': 5.421267747879028, 'metrics/data_secs_per_batch': 2.65580415725708, '_timestamp': 1740973674.1467545}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 281 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973674.147018}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 282 is less than current step: 499. Dropping entry: {'train/loss': 1.8275298714637755, 'train/ce_loss': 0.39375, 'train/seg_cls_loss': 0.01424560546875, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.18968643974512817, 'train/mask_dice_loss': 0.5109925508499146, 'train/mask_loss': 0.7006789982318878, 'metrics/total_secs_per_batch': 6.575718402862549, 'metrics/data_secs_per_batch': 2.890248727798462, '_timestamp': 1740973680.7224853}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 282 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973680.7227566}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 283 is less than current step: 499. Dropping entry: {'train/loss': 1.52003253698349, 'train/ce_loss': 0.4787109375, 'train/seg_cls_loss': 0.01348876953125, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.1220058100298047, 'train/mask_dice_loss': 0.3825905382633209, 'train/mask_loss': 0.5045963481068612, 'metrics/total_secs_per_batch': 5.009317874908447, 'metrics/data_secs_per_batch': 2.2929996252059937, '_timestamp': 1740973685.7318072}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 283 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973685.7320995}).
Epoch: [6][285/500]	Time  5.399 ( 5.399)	Loss 2.4184 (1.8984)	CeLoss 0.1807 (0.3714)	SegCLSLoss 0.0197 (0.0171)	KLLoss 0.3789 (0.3264)	MaskLoss 1.0949 (0.7430)	MaskBCELoss 0.1896 (0.1167)	MaskDICELoss 0.9053 (0.6263)
Epoch: [6][286/500]	Time  5.586 ( 5.586)	Loss 1.9363 (1.4980)	CeLoss 0.2041 (0.4812)	SegCLSLoss 0.0184 (0.0131)	KLLoss 0.3535 (0.2557)	MaskLoss 0.8441 (0.4923)	MaskBCELoss 0.0478 (0.1361)	MaskDICELoss 0.7963 (0.3562)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 284 is less than current step: 499. Dropping entry: {'train/loss': 1.8984023690223695, 'train/ce_loss': 0.371435546875, 'train/seg_cls_loss': 0.017144775390625, 'train/kl_loss': 0.3263671875, 'train/mask_bce_loss': 0.11666814093478024, 'train/mask_dice_loss': 0.6262830585241318, 'train/mask_loss': 0.7429512023925782, 'metrics/total_secs_per_batch': 5.398596525192261, 'metrics/data_secs_per_batch': 2.607093834877014, '_timestamp': 1740973691.130605}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 284 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973691.1309474}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 285 is less than current step: 499. Dropping entry: {'train/loss': 1.4979712247848511, 'train/ce_loss': 0.48115234375, 'train/seg_cls_loss': 0.013079833984375, 'train/kl_loss': 0.2556640625, 'train/mask_bce_loss': 0.13610647581517696, 'train/mask_dice_loss': 0.3561896711587906, 'train/mask_loss': 0.4922961503267288, 'metrics/total_secs_per_batch': 5.585885524749756, 'metrics/data_secs_per_batch': 2.5288966417312624, '_timestamp': 1740973696.7163165}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 285 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973696.7166684}).
Epoch: [6][287/500]	Time  4.954 ( 4.954)	Loss 0.6318 (1.1301)	CeLoss 0.3086 (0.5096)	SegCLSLoss 0.0127 (0.0103)	KLLoss 0.3613 (0.2178)	MaskLoss 0.1401 (0.2966)	MaskBCELoss 0.0792 (0.0732)	MaskDICELoss 0.0609 (0.2235)
Epoch: [6][288/500]	Time  5.140 ( 5.140)	Loss 1.3281 (1.3202)	CeLoss 1.3281 (0.6107)	SegCLSLoss 0.0000 (0.0099)	KLLoss 0.0000 (0.1816)	MaskLoss 0.0000 (0.3431)	MaskBCELoss 0.0000 (0.0510)	MaskDICELoss 0.0000 (0.2921)
Epoch: [6][289/500]	Time  6.054 ( 6.054)	Loss 1.7812 (1.7160)	CeLoss 1.7812 (0.4693)	SegCLSLoss 0.0000 (0.0162)	KLLoss 0.0000 (0.2928)	MaskLoss 0.0000 (0.6045)	MaskBCELoss 0.0000 (0.1289)	MaskDICELoss 0.0000 (0.4756)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 286 is less than current step: 499. Dropping entry: {'train/loss': 1.1301361322402954, 'train/ce_loss': 0.5095947265625, 'train/seg_cls_loss': 0.01029052734375, 'train/kl_loss': 0.2177734375, 'train/mask_bce_loss': 0.07319725137203932, 'train/mask_dice_loss': 0.22345041148364544, 'train/mask_loss': 0.2966476589441299, 'metrics/total_secs_per_batch': 4.953595876693726, 'metrics/data_secs_per_batch': 2.382639026641846, '_timestamp': 1740973701.6698844}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 286 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973701.670166}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 287 is less than current step: 499. Dropping entry: {'train/loss': 1.320163643360138, 'train/ce_loss': 0.6107421875, 'train/seg_cls_loss': 0.00994873046875, 'train/kl_loss': 0.181640625, 'train/mask_bce_loss': 0.051032828725874425, 'train/mask_dice_loss': 0.2921056389808655, 'train/mask_loss': 0.34313846230506895, 'metrics/total_secs_per_batch': 5.1397833824157715, 'metrics/data_secs_per_batch': 2.183162045478821, '_timestamp': 1740973706.8096719}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 287 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973706.8099496}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 288 is less than current step: 499. Dropping entry: {'train/loss': 1.716022801399231, 'train/ce_loss': 0.4693359375, 'train/seg_cls_loss': 0.016229248046875, 'train/kl_loss': 0.2927734375, 'train/mask_bce_loss': 0.12894610725343228, 'train/mask_dice_loss': 0.47559850513935087, 'train/mask_loss': 0.6045446038246155, 'metrics/total_secs_per_batch': 6.053539037704468, 'metrics/data_secs_per_batch': 2.722383737564087, '_timestamp': 1740973712.8632092}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 288 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973712.8634899}).
[2025-03-02 21:48:38,789] [INFO] [logging.py:96:log_dist] [Rank 0] step=3290, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:48:38,795] [INFO] [timer.py:215:stop] epoch=0/micro_step=32900/global_step=3290, RunningAvgSamplesPerSec=1.439971287829009, CurrSamplesPerSec=1.6859804765230768, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [6][290/500]	Time  5.933 ( 5.933)	Loss 1.5251 (1.9523)	CeLoss 0.2451 (0.2427)	SegCLSLoss 0.0166 (0.0195)	KLLoss 0.3594 (0.3666)	MaskLoss 0.6180 (0.8317)	MaskBCELoss 0.0032 (0.1641)	MaskDICELoss 0.6148 (0.6676)
Epoch: [6][291/500]	Time  6.431 ( 6.431)	Loss 1.5916 (1.8586)	CeLoss 0.2256 (0.4152)	SegCLSLoss 0.0134 (0.0142)	KLLoss 0.3555 (0.2895)	MaskLoss 0.6620 (0.7035)	MaskBCELoss 0.1368 (0.1024)	MaskDICELoss 0.5252 (0.6012)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 289 is less than current step: 499. Dropping entry: {'train/loss': 1.952348381280899, 'train/ce_loss': 0.24267578125, 'train/seg_cls_loss': 0.01951904296875, 'train/kl_loss': 0.3666015625, 'train/mask_bce_loss': 0.16410632084589452, 'train/mask_dice_loss': 0.6676342651247978, 'train/mask_loss': 0.8317405842244625, 'metrics/total_secs_per_batch': 5.93282675743103, 'metrics/data_secs_per_batch': 2.6502144575119018, '_timestamp': 1740973718.7958486}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 289 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973718.7961242}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 290 is less than current step: 499. Dropping entry: {'train/loss': 1.8586495518684387, 'train/ce_loss': 0.415234375, 'train/seg_cls_loss': 0.014154052734375, 'train/kl_loss': 0.289453125, 'train/mask_bce_loss': 0.10235238643363118, 'train/mask_dice_loss': 0.6011911332607269, 'train/mask_loss': 0.7035435318946839, 'metrics/total_secs_per_batch': 6.431368827819824, 'metrics/data_secs_per_batch': 2.935785436630249, '_timestamp': 1740973725.227464}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 290 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973725.2277708}).
Epoch: [6][292/500]	Time  6.761 ( 6.761)	Loss 1.5878 (1.5419)	CeLoss 0.2676 (0.3047)	SegCLSLoss 0.0122 (0.0124)	KLLoss 0.3633 (0.3270)	MaskLoss 0.6386 (0.5992)	MaskBCELoss 0.1558 (0.0814)	MaskDICELoss 0.4828 (0.5177)
Epoch: [6][293/500]	Time  5.105 ( 5.105)	Loss 1.2872 (1.4044)	CeLoss 0.2354 (0.5201)	SegCLSLoss 0.0105 (0.0094)	KLLoss 0.3633 (0.2182)	MaskLoss 0.5049 (0.4289)	MaskBCELoss 0.1206 (0.0660)	MaskDICELoss 0.3844 (0.3629)
Epoch: [6][294/500]	Time  6.029 ( 6.029)	Loss 0.9859 (1.6268)	CeLoss 0.1982 (0.4036)	SegCLSLoss 0.0121 (0.0149)	KLLoss 0.3633 (0.2861)	MaskLoss 0.3728 (0.5936)	MaskBCELoss 0.0272 (0.0753)	MaskDICELoss 0.3456 (0.5182)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 291 is less than current step: 499. Dropping entry: {'train/loss': 1.5418857991695405, 'train/ce_loss': 0.3046875, 'train/seg_cls_loss': 0.012408447265625, 'train/kl_loss': 0.326953125, 'train/mask_bce_loss': 0.08143916218541562, 'train/mask_dice_loss': 0.5177264019846917, 'train/mask_loss': 0.5991655588150024, 'metrics/total_secs_per_batch': 6.760839223861694, 'metrics/data_secs_per_batch': 2.743497896194458, '_timestamp': 1740973731.988377}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 291 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973731.988728}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 292 is less than current step: 499. Dropping entry: {'train/loss': 1.404439961910248, 'train/ce_loss': 0.520068359375, 'train/seg_cls_loss': 0.0093994140625, 'train/kl_loss': 0.2181640625, 'train/mask_bce_loss': 0.0659956126473844, 'train/mask_dice_loss': 0.36290893852710726, 'train/mask_loss': 0.42890454530715943, 'metrics/total_secs_per_batch': 5.105378866195679, 'metrics/data_secs_per_batch': 2.0129701852798463, '_timestamp': 1740973737.0936668}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 292 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973737.0940392}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 293 is less than current step: 499. Dropping entry: {'train/loss': 1.6267515957355498, 'train/ce_loss': 0.40361328125, 'train/seg_cls_loss': 0.01488037109375, 'train/kl_loss': 0.2861328125, 'train/mask_bce_loss': 0.07533139446750284, 'train/mask_dice_loss': 0.5182201772928238, 'train/mask_loss': 0.5935515731573104, 'metrics/total_secs_per_batch': 6.029036283493042, 'metrics/data_secs_per_batch': 2.7833654165267943, '_timestamp': 1740973743.122682}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 293 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973743.1230242}).
Epoch: [6][295/500]	Time  5.789 ( 5.789)	Loss 2.6024 (1.3635)	CeLoss 0.2207 (0.3697)	SegCLSLoss 0.0228 (0.0101)	KLLoss 0.3574 (0.2182)	MaskLoss 1.1674 (0.4833)	MaskBCELoss 0.2920 (0.0644)	MaskDICELoss 0.8754 (0.4190)
Epoch: [6][296/500]	Time  5.115 ( 5.115)	Loss 0.8945 (1.5156)	CeLoss 0.8945 (0.5305)	SegCLSLoss 0.0000 (0.0118)	KLLoss 0.0000 (0.2547)	MaskLoss 0.0000 (0.4769)	MaskBCELoss 0.0000 (0.0639)	MaskDICELoss 0.0000 (0.4129)
Epoch: [6][297/500]	Time  5.285 ( 5.285)	Loss 1.1016 (1.5673)	CeLoss 1.1016 (0.4796)	SegCLSLoss 0.0000 (0.0107)	KLLoss 0.0000 (0.2174)	MaskLoss 0.0000 (0.5305)	MaskBCELoss 0.0000 (0.1031)	MaskDICELoss 0.0000 (0.4273)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 294 is less than current step: 499. Dropping entry: {'train/loss': 1.363493013381958, 'train/ce_loss': 0.369677734375, 'train/seg_cls_loss': 0.01005859375, 'train/kl_loss': 0.2181640625, 'train/mask_bce_loss': 0.06435482371598482, 'train/mask_dice_loss': 0.41897860914468765, 'train/mask_loss': 0.4833334356546402, 'metrics/total_secs_per_batch': 5.788858652114868, 'metrics/data_secs_per_batch': 2.2702703714370727, '_timestamp': 1740973748.9115665}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 294 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973748.9118512}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 295 is less than current step: 499. Dropping entry: {'train/loss': 1.515631228685379, 'train/ce_loss': 0.53046875, 'train/seg_cls_loss': 0.011767578125, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.06393526494503021, 'train/mask_dice_loss': 0.412923327088356, 'train/mask_loss': 0.47685858979821205, 'metrics/total_secs_per_batch': 5.115299224853516, 'metrics/data_secs_per_batch': 2.512324571609497, '_timestamp': 1740973754.0268521}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 295 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973754.0271292}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 296 is less than current step: 499. Dropping entry: {'train/loss': 1.5673097252845765, 'train/ce_loss': 0.479638671875, 'train/seg_cls_loss': 0.010687255859375, 'train/kl_loss': 0.2173828125, 'train/mask_bce_loss': 0.10314123481512069, 'train/mask_dice_loss': 0.4273153871297836, 'train/mask_loss': 0.5304566204547883, 'metrics/total_secs_per_batch': 5.284524440765381, 'metrics/data_secs_per_batch': 2.4840814113616942, '_timestamp': 1740973759.3119805}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 296 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973759.3123085}).
Epoch: [6][298/500]	Time  7.419 ( 7.419)	Loss 1.3087 (1.7102)	CeLoss 0.2695 (0.2237)	SegCLSLoss 0.0095 (0.0186)	KLLoss 0.3672 (0.3613)	MaskLoss 0.4991 (0.7202)	MaskBCELoss 0.1487 (0.1516)	MaskDICELoss 0.3504 (0.5686)
Epoch: [6][299/500]	Time  6.951 ( 6.951)	Loss 1.1394 (1.8511)	CeLoss 0.2578 (0.3808)	SegCLSLoss 0.0119 (0.0132)	KLLoss 0.3691 (0.2928)	MaskLoss 0.4193 (0.7171)	MaskBCELoss 0.1004 (0.1372)	MaskDICELoss 0.3189 (0.5798)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 297 is less than current step: 499. Dropping entry: {'train/loss': 1.7102022826671601, 'train/ce_loss': 0.22373046875, 'train/seg_cls_loss': 0.0186279296875, 'train/kl_loss': 0.361328125, 'train/mask_bce_loss': 0.15161622054874896, 'train/mask_dice_loss': 0.5686216592788697, 'train/mask_loss': 0.7202378779649734, 'metrics/total_secs_per_batch': 7.418704271316528, 'metrics/data_secs_per_batch': 3.424187159538269, '_timestamp': 1740973766.7300968}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 297 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973766.7303727}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 298 is less than current step: 499. Dropping entry: {'train/loss': 1.851128911972046, 'train/ce_loss': 0.38076171875, 'train/seg_cls_loss': 0.013226318359375, 'train/kl_loss': 0.2927734375, 'train/mask_bce_loss': 0.13723280129488558, 'train/mask_dice_loss': 0.5798355489969254, 'train/mask_loss': 0.7170683562755584, 'metrics/total_secs_per_batch': 6.950953483581543, 'metrics/data_secs_per_batch': 3.2598448038101195, '_timestamp': 1740973773.6810524}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 298 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973773.6813228}).
[2025-03-02 21:49:39,421] [INFO] [logging.py:96:log_dist] [Rank 0] step=3300, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:49:39,426] [INFO] [timer.py:215:stop] epoch=0/micro_step=33000/global_step=3300, RunningAvgSamplesPerSec=1.4405265277611727, CurrSamplesPerSec=1.7407897873099147, MemAllocated=30.69GB, MaxMemAllocated=37.23GB
Epoch: [6][300/500]	Time  5.747 ( 5.747)	Loss 0.7500 (1.7857)	CeLoss 0.7500 (0.3098)	SegCLSLoss 0.0000 (0.0124)	KLLoss 0.0000 (0.2598)	MaskLoss 0.0000 (0.7218)	MaskBCELoss 0.0000 (0.2462)	MaskDICELoss 0.0000 (0.4756)
Epoch: [6][301/500]	Time  6.295 ( 6.295)	Loss 2.2440 (1.8972)	CeLoss 0.2734 (0.4144)	SegCLSLoss 0.0133 (0.0152)	KLLoss 0.3633 (0.2932)	MaskLoss 0.9638 (0.7231)	MaskBCELoss 0.1930 (0.1634)	MaskDICELoss 0.7708 (0.5597)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 299 is less than current step: 499. Dropping entry: {'train/loss': 1.785661554336548, 'train/ce_loss': 0.3098388671875, 'train/seg_cls_loss': 0.0124267578125, 'train/kl_loss': 0.259765625, 'train/mask_bce_loss': 0.24617436435073614, 'train/mask_dice_loss': 0.475623694062233, 'train/mask_loss': 0.7217980474233627, 'metrics/total_secs_per_batch': 5.746631860733032, 'metrics/data_secs_per_batch': 2.8735583066940307, '_timestamp': 1740973779.428078}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 299 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973779.4288332}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 300 is less than current step: 499. Dropping entry: {'train/loss': 1.897212064266205, 'train/ce_loss': 0.41435546875, 'train/seg_cls_loss': 0.015203857421875, 'train/kl_loss': 0.2931640625, 'train/mask_bce_loss': 0.16343952864408492, 'train/mask_dice_loss': 0.5596782147884369, 'train/mask_loss': 0.7231177598237991, 'metrics/total_secs_per_batch': 6.2948009967803955, 'metrics/data_secs_per_batch': 2.5075880527496337, '_timestamp': 1740973785.7225845}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 300 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973785.72288}).
Epoch: [6][302/500]	Time  5.514 ( 5.514)	Loss 2.0578 (1.8526)	CeLoss 0.1836 (0.4181)	SegCLSLoss 0.0204 (0.0152)	KLLoss 0.3672 (0.2938)	MaskLoss 0.9137 (0.6987)	MaskBCELoss 0.2642 (0.1458)	MaskDICELoss 0.6494 (0.5529)
Epoch: [6][303/500]	Time  7.246 ( 7.246)	Loss 0.9375 (2.0285)	CeLoss 0.9375 (0.2801)	SegCLSLoss 0.0000 (0.0170)	KLLoss 0.0000 (0.3256)	MaskLoss 0.0000 (0.8536)	MaskBCELoss 0.0000 (0.1391)	MaskDICELoss 0.0000 (0.7145)
Epoch: [6][304/500]	Time  6.640 ( 6.640)	Loss 0.1006 (1.6850)	CeLoss 0.1006 (0.3746)	SegCLSLoss 0.0000 (0.0117)	KLLoss 0.0000 (0.2529)	MaskLoss 0.0000 (0.6395)	MaskBCELoss 0.0000 (0.0802)	MaskDICELoss 0.0000 (0.5593)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 301 is less than current step: 499. Dropping entry: {'train/loss': 1.8525739371776582, 'train/ce_loss': 0.41806640625, 'train/seg_cls_loss': 0.0151611328125, 'train/kl_loss': 0.29375, 'train/mask_bce_loss': 0.14584469199180602, 'train/mask_dice_loss': 0.5529032155871392, 'train/mask_loss': 0.6987479060888291, 'metrics/total_secs_per_batch': 5.513858795166016, 'metrics/data_secs_per_batch': 2.29614896774292, '_timestamp': 1740973791.2362828}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 301 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973791.23654}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 302 is less than current step: 499. Dropping entry: {'train/loss': 2.028500699996948, 'train/ce_loss': 0.280078125, 'train/seg_cls_loss': 0.01702880859375, 'train/kl_loss': 0.3255859375, 'train/mask_bce_loss': 0.13913204353302716, 'train/mask_dice_loss': 0.7144737541675568, 'train/mask_loss': 0.8536058127880096, 'metrics/total_secs_per_batch': 7.245991945266724, 'metrics/data_secs_per_batch': 3.264362406730652, '_timestamp': 1740973798.4823427}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 302 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973798.4825323}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 303 is less than current step: 499. Dropping entry: {'train/loss': 1.6849745273590089, 'train/ce_loss': 0.374609375, 'train/seg_cls_loss': 0.01168212890625, 'train/kl_loss': 0.2529296875, 'train/mask_bce_loss': 0.08019521739333868, 'train/mask_dice_loss': 0.5592646926641465, 'train/mask_loss': 0.6394599199295044, 'metrics/total_secs_per_batch': 6.639951944351196, 'metrics/data_secs_per_batch': 3.3088101863861086, '_timestamp': 1740973805.1223142}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 303 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973805.1225922}).
Epoch: [6][305/500]	Time  5.877 ( 5.877)	Loss 1.8592 (1.5519)	CeLoss 0.2236 (0.3897)	SegCLSLoss 0.0188 (0.0169)	KLLoss 0.3613 (0.3273)	MaskLoss 0.7949 (0.5605)	MaskBCELoss 0.0180 (0.0588)	MaskDICELoss 0.7769 (0.5017)
Epoch: [6][306/500]	Time  6.202 ( 6.202)	Loss 1.1938 (1.7871)	CeLoss 0.2676 (0.4297)	SegCLSLoss 0.0096 (0.0139)	KLLoss 0.3633 (0.2883)	MaskLoss 0.4426 (0.6608)	MaskBCELoss 0.1019 (0.1514)	MaskDICELoss 0.3407 (0.5094)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 304 is less than current step: 499. Dropping entry: {'train/loss': 1.551869261264801, 'train/ce_loss': 0.38974609375, 'train/seg_cls_loss': 0.016949462890625, 'train/kl_loss': 0.32734375, 'train/mask_bce_loss': 0.058807290624827147, 'train/mask_dice_loss': 0.5016976565122604, 'train/mask_loss': 0.560504949092865, 'metrics/total_secs_per_batch': 5.876845359802246, 'metrics/data_secs_per_batch': 2.6913811445236204, '_timestamp': 1740973810.9991703}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 304 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973810.999457}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 305 is less than current step: 499. Dropping entry: {'train/loss': 1.7870851516723634, 'train/ce_loss': 0.4296875, 'train/seg_cls_loss': 0.01390380859375, 'train/kl_loss': 0.28828125, 'train/mask_bce_loss': 0.15138776563107967, 'train/mask_dice_loss': 0.5094399690628052, 'train/mask_loss': 0.6608277350664139, 'metrics/total_secs_per_batch': 6.201697111129761, 'metrics/data_secs_per_batch': 2.9794395685195925, '_timestamp': 1740973817.2008417}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 305 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973817.201181}).
Epoch: [6][307/500]	Time  5.389 ( 5.389)	Loss 0.9414 (1.3801)	CeLoss 0.9414 (0.5505)	SegCLSLoss 0.0000 (0.0088)	KLLoss 0.0000 (0.1801)	MaskLoss 0.0000 (0.4037)	MaskBCELoss 0.0000 (0.0646)	MaskDICELoss 0.0000 (0.3390)
Epoch: [6][308/500]	Time  5.152 ( 5.152)	Loss 2.3318 (1.5943)	CeLoss 0.1816 (0.5771)	SegCLSLoss 0.0197 (0.0129)	KLLoss 0.3594 (0.2566)	MaskLoss 1.0521 (0.4926)	MaskBCELoss 0.3066 (0.1361)	MaskDICELoss 0.7455 (0.3565)
Epoch: [6][309/500]	Time  5.369 ( 5.369)	Loss 2.5008 (1.8549)	CeLoss 0.1426 (0.4995)	SegCLSLoss 0.0239 (0.0115)	KLLoss 0.3828 (0.2600)	MaskLoss 1.1542 (0.6618)	MaskBCELoss 0.2295 (0.2245)	MaskDICELoss 0.9247 (0.4373)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 306 is less than current step: 499. Dropping entry: {'train/loss': 1.3800555348396302, 'train/ce_loss': 0.550537109375, 'train/seg_cls_loss': 0.008782958984375, 'train/kl_loss': 0.180078125, 'train/mask_bce_loss': 0.06463823458179832, 'train/mask_dice_loss': 0.3390369892120361, 'train/mask_loss': 0.4036752223968506, 'metrics/total_secs_per_batch': 5.389289379119873, 'metrics/data_secs_per_batch': 2.35433452129364, '_timestamp': 1740973822.590111}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 306 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973822.5903778}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 307 is less than current step: 499. Dropping entry: {'train/loss': 1.5943124294281006, 'train/ce_loss': 0.5771484375, 'train/seg_cls_loss': 0.0128662109375, 'train/kl_loss': 0.256640625, 'train/mask_bce_loss': 0.13607505932450295, 'train/mask_dice_loss': 0.3565401419997215, 'train/mask_loss': 0.49261519908905027, 'metrics/total_secs_per_batch': 5.152069568634033, 'metrics/data_secs_per_batch': 2.1721896648406984, '_timestamp': 1740973827.742467}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 307 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973827.7429745}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 308 is less than current step: 499. Dropping entry: {'train/loss': 1.8549003958702088, 'train/ce_loss': 0.49951171875, 'train/seg_cls_loss': 0.011529541015625, 'train/kl_loss': 0.2599609375, 'train/mask_bce_loss': 0.2244942456483841, 'train/mask_dice_loss': 0.43733094483613966, 'train/mask_loss': 0.6618251979351044, 'metrics/total_secs_per_batch': 5.368805646896362, 'metrics/data_secs_per_batch': 2.644194507598877, '_timestamp': 1740973833.111115}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 308 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973833.1114306}).
[2025-03-02 21:50:39,040] [INFO] [logging.py:96:log_dist] [Rank 0] step=3310, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:50:39,046] [INFO] [timer.py:215:stop] epoch=0/micro_step=33100/global_step=3310, RunningAvgSamplesPerSec=1.4411423903696745, CurrSamplesPerSec=1.6850807520636424, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [6][310/500]	Time  5.936 ( 5.936)	Loss 2.0342 (1.5029)	CeLoss 0.2402 (0.2969)	SegCLSLoss 0.0182 (0.0137)	KLLoss 0.3574 (0.2555)	MaskLoss 0.8745 (0.5868)	MaskBCELoss 0.0391 (0.1205)	MaskDICELoss 0.8355 (0.4662)
Epoch: [6][311/500]	Time  6.613 ( 6.613)	Loss 1.4600 (1.5256)	CeLoss 0.3184 (0.3919)	SegCLSLoss 0.0194 (0.0131)	KLLoss 0.3652 (0.2896)	MaskLoss 0.5474 (0.5491)	MaskBCELoss 0.0023 (0.0812)	MaskDICELoss 0.5451 (0.4679)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 309 is less than current step: 499. Dropping entry: {'train/loss': 1.5029423594474793, 'train/ce_loss': 0.296875, 'train/seg_cls_loss': 0.01370849609375, 'train/kl_loss': 0.25546875, 'train/mask_bce_loss': 0.12054232768714428, 'train/mask_dice_loss': 0.4662315875291824, 'train/mask_loss': 0.5867739200592041, 'metrics/total_secs_per_batch': 5.936146259307861, 'metrics/data_secs_per_batch': 2.8567479848861694, '_timestamp': 1740973839.0470057}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 309 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973839.0473025}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 310 is less than current step: 499. Dropping entry: {'train/loss': 1.5256493806838989, 'train/ce_loss': 0.39189453125, 'train/seg_cls_loss': 0.013104248046875, 'train/kl_loss': 0.2896484375, 'train/mask_bce_loss': 0.08117244793102145, 'train/mask_dice_loss': 0.4678827226161957, 'train/mask_loss': 0.5490551710128784, 'metrics/total_secs_per_batch': 6.613374471664429, 'metrics/data_secs_per_batch': 2.793845009803772, '_timestamp': 1740973845.6605978}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 310 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973845.660905}).
Epoch: [6][312/500]	Time  6.155 ( 6.155)	Loss 1.4598 (1.8076)	CeLoss 0.2500 (0.3271)	SegCLSLoss 0.0116 (0.0161)	KLLoss 0.3555 (0.3234)	MaskLoss 0.5844 (0.7199)	MaskBCELoss 0.0652 (0.1032)	MaskDICELoss 0.5192 (0.6167)
Epoch: [6][313/500]	Time  6.040 ( 6.040)	Loss 2.2790 (1.5867)	CeLoss 0.2295 (0.3364)	SegCLSLoss 0.0142 (0.0142)	KLLoss 0.3711 (0.3314)	MaskLoss 1.0028 (0.6051)	MaskBCELoss 0.2211 (0.1129)	MaskDICELoss 0.7817 (0.4922)
Epoch: [6][314/500]	Time  6.962 ( 6.962)	Loss 1.3375 (1.7647)	CeLoss 0.2930 (0.3327)	SegCLSLoss 0.0152 (0.0157)	KLLoss 0.3652 (0.3299)	MaskLoss 0.4998 (0.6956)	MaskBCELoss 0.1431 (0.1442)	MaskDICELoss 0.3567 (0.5514)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 311 is less than current step: 499. Dropping entry: {'train/loss': 1.8076409339904784, 'train/ce_loss': 0.32705078125, 'train/seg_cls_loss': 0.01605224609375, 'train/kl_loss': 0.3234375, 'train/mask_bce_loss': 0.10321935471147299, 'train/mask_dice_loss': 0.6167143896222115, 'train/mask_loss': 0.7199337422847748, 'metrics/total_secs_per_batch': 6.155043363571167, 'metrics/data_secs_per_batch': 2.941639256477356, '_timestamp': 1740973851.8155859}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 311 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973851.8159173}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 312 is less than current step: 499. Dropping entry: {'train/loss': 1.5866999834775926, 'train/ce_loss': 0.33642578125, 'train/seg_cls_loss': 0.01416015625, 'train/kl_loss': 0.3314453125, 'train/mask_bce_loss': 0.11287713833153248, 'train/mask_dice_loss': 0.49219161570072173, 'train/mask_loss': 0.6050687536597252, 'metrics/total_secs_per_batch': 6.039680242538452, 'metrics/data_secs_per_batch': 2.510732889175415, '_timestamp': 1740973857.8552783}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 312 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973857.8555508}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 313 is less than current step: 499. Dropping entry: {'train/loss': 1.76473730802536, 'train/ce_loss': 0.33271484375, 'train/seg_cls_loss': 0.0156982421875, 'train/kl_loss': 0.3298828125, 'train/mask_bce_loss': 0.14420193424448371, 'train/mask_dice_loss': 0.5513991385698318, 'train/mask_loss': 0.6956010669469833, 'metrics/total_secs_per_batch': 6.961970567703247, 'metrics/data_secs_per_batch': 2.53262345790863, '_timestamp': 1740973864.8172646}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 313 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973864.8175502}).
Epoch: [6][315/500]	Time  6.776 ( 6.776)	Loss 2.7568 (1.5515)	CeLoss 0.2402 (0.3482)	SegCLSLoss 0.0167 (0.0101)	KLLoss 0.3555 (0.2529)	MaskLoss 1.2358 (0.5865)	MaskBCELoss 0.3003 (0.0967)	MaskDICELoss 0.9355 (0.4897)
Epoch: [6][316/500]	Time  5.115 ( 5.115)	Loss 1.8214 (1.7362)	CeLoss 0.2471 (0.5624)	SegCLSLoss 0.0094 (0.0110)	KLLoss 0.3652 (0.2205)	MaskLoss 0.7662 (0.5731)	MaskBCELoss 0.1320 (0.1655)	MaskDICELoss 0.6342 (0.4076)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 314 is less than current step: 499. Dropping entry: {'train/loss': 1.5514995813369752, 'train/ce_loss': 0.3481689453125, 'train/seg_cls_loss': 0.010064697265625, 'train/kl_loss': 0.2529296875, 'train/mask_bce_loss': 0.09674057960510254, 'train/mask_dice_loss': 0.4897391855716705, 'train/mask_loss': 0.586479765176773, 'metrics/total_secs_per_batch': 6.775534629821777, 'metrics/data_secs_per_batch': 3.0295694351196287, '_timestamp': 1740973871.592795}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 314 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973871.5930755}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 315 is less than current step: 499. Dropping entry: {'train/loss': 1.7361916661262513, 'train/ce_loss': 0.56240234375, 'train/seg_cls_loss': 0.011041259765625, 'train/kl_loss': 0.2205078125, 'train/mask_bce_loss': 0.16550892777740955, 'train/mask_dice_loss': 0.40761621594429015, 'train/mask_loss': 0.5731251299381256, 'metrics/total_secs_per_batch': 5.115299463272095, 'metrics/data_secs_per_batch': 2.0856789350509644, '_timestamp': 1740973876.7080796}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 315 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973876.7083542}).
Epoch: [6][317/500]	Time  4.897 ( 4.897)	Loss 1.8102 (1.6883)	CeLoss 0.1924 (0.5909)	SegCLSLoss 0.0157 (0.0089)	KLLoss 0.3672 (0.2197)	MaskLoss 0.7869 (0.5355)	MaskBCELoss 0.2789 (0.1784)	MaskDICELoss 0.5080 (0.3571)
Epoch: [6][318/500]	Time  6.670 ( 6.670)	Loss 1.5160 (1.5747)	CeLoss 0.2061 (0.2926)	SegCLSLoss 0.0093 (0.0134)	KLLoss 0.3633 (0.3271)	MaskLoss 0.6345 (0.6214)	MaskBCELoss 0.3738 (0.1819)	MaskDICELoss 0.2606 (0.4395)
Epoch: [6][319/500]	Time  6.158 ( 6.158)	Loss 0.7500 (1.5919)	CeLoss 0.7500 (0.4041)	SegCLSLoss 0.0000 (0.0124)	KLLoss 0.0000 (0.2898)	MaskLoss 0.0000 (0.5762)	MaskBCELoss 0.0000 (0.0751)	MaskDICELoss 0.0000 (0.5012)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 316 is less than current step: 499. Dropping entry: {'train/loss': 1.6883465886116027, 'train/ce_loss': 0.59091796875, 'train/seg_cls_loss': 0.008905029296875, 'train/kl_loss': 0.2197265625, 'train/mask_bce_loss': 0.17840600460767747, 'train/mask_dice_loss': 0.35707589983940125, 'train/mask_loss': 0.5354818940162659, 'metrics/total_secs_per_batch': 4.896604061126709, 'metrics/data_secs_per_batch': 2.117467522621155, '_timestamp': 1740973881.6047068}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 316 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973881.60498}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 317 is less than current step: 499. Dropping entry: {'train/loss': 1.5746977090835572, 'train/ce_loss': 0.292578125, 'train/seg_cls_loss': 0.013372802734375, 'train/kl_loss': 0.3271484375, 'train/mask_bce_loss': 0.181875017657876, 'train/mask_dice_loss': 0.43950703740119934, 'train/mask_loss': 0.6213820576667786, 'metrics/total_secs_per_batch': 6.669682502746582, 'metrics/data_secs_per_batch': 2.7984917640686033, '_timestamp': 1740973888.2746167}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 317 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973888.274973}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 318 is less than current step: 499. Dropping entry: {'train/loss': 1.591930514574051, 'train/ce_loss': 0.4041015625, 'train/seg_cls_loss': 0.012445068359375, 'train/kl_loss': 0.28984375, 'train/mask_bce_loss': 0.07508451594039797, 'train/mask_dice_loss': 0.5011541813611984, 'train/mask_loss': 0.5762386932969094, 'metrics/total_secs_per_batch': 6.157935380935669, 'metrics/data_secs_per_batch': 3.030826735496521, '_timestamp': 1740973894.432342}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 318 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973894.4326215}).
[2025-03-02 21:51:40,919] [INFO] [logging.py:96:log_dist] [Rank 0] step=3320, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:51:40,925] [INFO] [timer.py:215:stop] epoch=0/micro_step=33200/global_step=3320, RunningAvgSamplesPerSec=1.441613440986092, CurrSamplesPerSec=1.5403706324393747, MemAllocated=30.95GB, MaxMemAllocated=37.23GB
Epoch: [6][320/500]	Time  6.494 ( 6.494)	Loss 2.0587 (1.8941)	CeLoss 0.1934 (0.3529)	SegCLSLoss 0.0247 (0.0154)	KLLoss 0.3535 (0.3262)	MaskLoss 0.9087 (0.7504)	MaskBCELoss 0.0082 (0.1496)	MaskDICELoss 0.9005 (0.6008)
Epoch: [6][321/500]	Time  6.104 ( 6.104)	Loss 1.7254 (1.5041)	CeLoss 0.2002 (0.5477)	SegCLSLoss 0.0206 (0.0098)	KLLoss 0.3535 (0.2553)	MaskLoss 0.7397 (0.4630)	MaskBCELoss 0.0515 (0.0883)	MaskDICELoss 0.6882 (0.3747)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 319 is less than current step: 499. Dropping entry: {'train/loss': 1.8941416025161744, 'train/ce_loss': 0.3529296875, 'train/seg_cls_loss': 0.015399169921875, 'train/kl_loss': 0.326171875, 'train/mask_bce_loss': 0.14960522865876555, 'train/mask_dice_loss': 0.6007858842611313, 'train/mask_loss': 0.7503911137580872, 'metrics/total_secs_per_batch': 6.49352765083313, 'metrics/data_secs_per_batch': 2.884703254699707, '_timestamp': 1740973900.925663}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 319 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973900.925941}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 320 is less than current step: 499. Dropping entry: {'train/loss': 1.5041465520858766, 'train/ce_loss': 0.54765625, 'train/seg_cls_loss': 0.00975341796875, 'train/kl_loss': 0.2552734375, 'train/mask_bce_loss': 0.08827963396906853, 'train/mask_dice_loss': 0.3747311383485794, 'train/mask_loss': 0.463010773062706, 'metrics/total_secs_per_batch': 6.103777170181274, 'metrics/data_secs_per_batch': 2.5856512069702147, '_timestamp': 1740973907.0298321}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 320 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973907.0301843}).
Epoch: [6][322/500]	Time  5.702 ( 5.702)	Loss 1.2266 (1.3834)	CeLoss 1.2266 (0.6150)	SegCLSLoss 0.0000 (0.0108)	KLLoss 0.0000 (0.1811)	MaskLoss 0.0000 (0.3725)	MaskBCELoss 0.0000 (0.0276)	MaskDICELoss 0.0000 (0.3449)
Epoch: [6][323/500]	Time  5.867 ( 5.867)	Loss 2.3674 (1.6390)	CeLoss 0.1357 (0.4502)	SegCLSLoss 0.0255 (0.0128)	KLLoss 0.3809 (0.2574)	MaskLoss 1.0904 (0.5783)	MaskBCELoss 0.2098 (0.1395)	MaskDICELoss 0.8806 (0.4388)
Epoch: [6][324/500]	Time  6.289 ( 6.289)	Loss 1.2031 (1.5348)	CeLoss 1.2031 (0.3250)	SegCLSLoss 0.0000 (0.0184)	KLLoss 0.0000 (0.3254)	MaskLoss 0.0000 (0.5840)	MaskBCELoss 0.0000 (0.1403)	MaskDICELoss 0.0000 (0.4437)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 321 is less than current step: 499. Dropping entry: {'train/loss': 1.38343745470047, 'train/ce_loss': 0.6150390625, 'train/seg_cls_loss': 0.010797119140625, 'train/kl_loss': 0.1810546875, 'train/mask_bce_loss': 0.027562759397551418, 'train/mask_dice_loss': 0.3449176847934723, 'train/mask_loss': 0.372480446100235, 'metrics/total_secs_per_batch': 5.702485799789429, 'metrics/data_secs_per_batch': 2.730943298339844, '_timestamp': 1740973912.7321336}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 321 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973912.732403}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 322 is less than current step: 499. Dropping entry: {'train/loss': 1.6389761567115784, 'train/ce_loss': 0.4501953125, 'train/seg_cls_loss': 0.012847900390625, 'train/kl_loss': 0.257421875, 'train/mask_bce_loss': 0.13951673731207848, 'train/mask_dice_loss': 0.43876039385795595, 'train/mask_loss': 0.5782771408557892, 'metrics/total_secs_per_batch': 5.867018461227417, 'metrics/data_secs_per_batch': 2.497820734977722, '_timestamp': 1740973918.599139}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 322 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973918.5994115}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 323 is less than current step: 499. Dropping entry: {'train/loss': 1.534817525744438, 'train/ce_loss': 0.325, 'train/seg_cls_loss': 0.018414306640625, 'train/kl_loss': 0.325390625, 'train/mask_bce_loss': 0.14027688233181834, 'train/mask_dice_loss': 0.4437334477901459, 'train/mask_loss': 0.5840103209018708, 'metrics/total_secs_per_batch': 6.289451360702515, 'metrics/data_secs_per_batch': 2.9580571174621584, '_timestamp': 1740973924.8886216}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 323 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973924.888911}).
Epoch: [6][325/500]	Time  7.022 ( 7.022)	Loss 2.4563 (1.5455)	CeLoss 0.2852 (0.3970)	SegCLSLoss 0.0170 (0.0104)	KLLoss 0.3633 (0.2529)	MaskLoss 1.0631 (0.5588)	MaskBCELoss 0.3107 (0.0961)	MaskDICELoss 0.7524 (0.4627)
Epoch: [6][326/500]	Time  6.230 ( 6.230)	Loss 1.8276 (1.4421)	CeLoss 0.2969 (0.3872)	SegCLSLoss 0.0142 (0.0121)	KLLoss 0.3574 (0.2926)	MaskLoss 0.7439 (0.5097)	MaskBCELoss 0.0403 (0.0707)	MaskDICELoss 0.7036 (0.4390)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 324 is less than current step: 499. Dropping entry: {'train/loss': 1.5454804301261902, 'train/ce_loss': 0.397021484375, 'train/seg_cls_loss': 0.01043701171875, 'train/kl_loss': 0.2529296875, 'train/mask_bce_loss': 0.0961026031523943, 'train/mask_dice_loss': 0.46269717812538147, 'train/mask_loss': 0.558799785375595, 'metrics/total_secs_per_batch': 7.021768569946289, 'metrics/data_secs_per_batch': 3.055766463279724, '_timestamp': 1740973931.9105816}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 324 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973931.9110117}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 325 is less than current step: 499. Dropping entry: {'train/loss': 1.4420856773853301, 'train/ce_loss': 0.38720703125, 'train/seg_cls_loss': 0.012078857421875, 'train/kl_loss': 0.292578125, 'train/mask_bce_loss': 0.07070201225578784, 'train/mask_dice_loss': 0.4390126973390579, 'train/mask_loss': 0.5097147077322006, 'metrics/total_secs_per_batch': 6.23044753074646, 'metrics/data_secs_per_batch': 3.02887761592865, '_timestamp': 1740973938.140916}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 325 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973938.1411977}).
Epoch: [6][327/500]	Time  5.680 ( 5.680)	Loss 1.4527 (1.3433)	CeLoss 0.2090 (0.3708)	SegCLSLoss 0.0148 (0.0113)	KLLoss 0.3652 (0.2186)	MaskLoss 0.5994 (0.4725)	MaskBCELoss 0.1938 (0.0725)	MaskDICELoss 0.4056 (0.4000)
Epoch: [6][328/500]	Time  7.055 ( 7.055)	Loss 1.8552 (2.0122)	CeLoss 0.2363 (0.3177)	SegCLSLoss 0.0123 (0.0151)	KLLoss 0.3633 (0.3289)	MaskLoss 0.7879 (0.8270)	MaskBCELoss 0.3467 (0.2400)	MaskDICELoss 0.4413 (0.5870)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 326 is less than current step: 499. Dropping entry: {'train/loss': 1.3433336317539215, 'train/ce_loss': 0.370751953125, 'train/seg_cls_loss': 0.011322021484375, 'train/kl_loss': 0.2185546875, 'train/mask_bce_loss': 0.07248224541544915, 'train/mask_dice_loss': 0.39999022483825686, 'train/mask_loss': 0.4724724739789963, 'metrics/total_secs_per_batch': 5.6799376010894775, 'metrics/data_secs_per_batch': 2.879651093482971, '_timestamp': 1740973943.8207703}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 326 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973943.8211107}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 327 is less than current step: 499. Dropping entry: {'train/loss': 2.0121689081192016, 'train/ce_loss': 0.31767578125, 'train/seg_cls_loss': 0.01505126953125, 'train/kl_loss': 0.32890625, 'train/mask_bce_loss': 0.23998954612761736, 'train/mask_dice_loss': 0.5869933545589447, 'train/mask_loss': 0.8269828915596008, 'metrics/total_secs_per_batch': 7.054853916168213, 'metrics/data_secs_per_batch': 3.2664470911026, '_timestamp': 1740973950.8756824}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 327 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973950.8759692}).
Epoch: [6][329/500]	Time  6.173 ( 6.173)	Loss 2.1163 (1.6289)	CeLoss 0.1875 (0.3534)	SegCLSLoss 0.0190 (0.0150)	KLLoss 0.3574 (0.2910)	MaskLoss 0.9419 (0.6196)	MaskBCELoss 0.0558 (0.1647)	MaskDICELoss 0.8861 (0.4548)
[2025-03-02 21:52:43,980] [INFO] [logging.py:96:log_dist] [Rank 0] step=3330, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:52:43,987] [INFO] [timer.py:215:stop] epoch=0/micro_step=33300/global_step=3330, RunningAvgSamplesPerSec=1.442008062058522, CurrSamplesPerSec=1.4413623839244416, MemAllocated=31.58GB, MaxMemAllocated=37.23GB
Epoch: [6][330/500]	Time  6.939 ( 6.939)	Loss 1.1095 (1.5971)	CeLoss 0.2236 (0.2229)	SegCLSLoss 0.0173 (0.0170)	KLLoss 0.3613 (0.3617)	MaskLoss 0.4200 (0.6646)	MaskBCELoss 0.0263 (0.0748)	MaskDICELoss 0.3937 (0.5899)
Epoch: [6][331/500]	Time  5.423 ( 5.423)	Loss 1.4327 (1.5656)	CeLoss 0.2393 (0.5100)	SegCLSLoss 0.0116 (0.0120)	KLLoss 0.3594 (0.2512)	MaskLoss 0.5757 (0.5122)	MaskBCELoss 0.0140 (0.0444)	MaskDICELoss 0.5617 (0.4679)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 328 is less than current step: 499. Dropping entry: {'train/loss': 1.6288616597652434, 'train/ce_loss': 0.353369140625, 'train/seg_cls_loss': 0.014996337890625, 'train/kl_loss': 0.291015625, 'train/mask_bce_loss': 0.1647185727953911, 'train/mask_dice_loss': 0.45483921766281127, 'train/mask_loss': 0.6195577949285507, 'metrics/total_secs_per_batch': 6.172611474990845, 'metrics/data_secs_per_batch': 2.443717622756958, '_timestamp': 1740973957.0482647}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 328 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973957.0485394}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 329 is less than current step: 499. Dropping entry: {'train/loss': 1.5971309185028075, 'train/ce_loss': 0.2228515625, 'train/seg_cls_loss': 0.016961669921875, 'train/kl_loss': 0.36171875, 'train/mask_bce_loss': 0.0747747511137277, 'train/mask_dice_loss': 0.5898551642894745, 'train/mask_loss': 0.6646299198269844, 'metrics/total_secs_per_batch': 6.939486503601074, 'metrics/data_secs_per_batch': 3.091224193572998, '_timestamp': 1740973963.9875479}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 329 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973963.9878345}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 330 is less than current step: 499. Dropping entry: {'train/loss': 1.5656459093093873, 'train/ce_loss': 0.5099609375, 'train/seg_cls_loss': 0.011968994140625, 'train/kl_loss': 0.251171875, 'train/mask_bce_loss': 0.0443612570874393, 'train/mask_dice_loss': 0.467856228351593, 'train/mask_loss': 0.5122174859046936, 'metrics/total_secs_per_batch': 5.423089504241943, 'metrics/data_secs_per_batch': 2.3653905153274537, '_timestamp': 1740973969.411014}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 330 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973969.4113536}).
Epoch: [6][332/500]	Time  5.317 ( 5.317)	Loss 2.6636 (1.5885)	CeLoss 0.1982 (0.5628)	SegCLSLoss 0.0176 (0.0114)	KLLoss 0.3789 (0.2201)	MaskLoss 1.2092 (0.4991)	MaskBCELoss 0.2587 (0.0949)	MaskDICELoss 0.9506 (0.4042)
Epoch: [6][333/500]	Time  6.444 ( 6.444)	Loss 0.0623 (1.7391)	CeLoss 0.0623 (0.3282)	SegCLSLoss 0.0000 (0.0137)	KLLoss 0.0000 (0.2520)	MaskLoss 0.0000 (0.6894)	MaskBCELoss 0.0000 (0.1639)	MaskDICELoss 0.0000 (0.5254)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 331 is less than current step: 499. Dropping entry: {'train/loss': 1.5885334134101867, 'train/ce_loss': 0.56279296875, 'train/seg_cls_loss': 0.0113525390625, 'train/kl_loss': 0.2201171875, 'train/mask_bce_loss': 0.0948745921254158, 'train/mask_dice_loss': 0.4042260944843292, 'train/mask_loss': 0.49910069108009336, 'metrics/total_secs_per_batch': 5.317421913146973, 'metrics/data_secs_per_batch': 2.1892919063568117, '_timestamp': 1740973974.7282612}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 331 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973974.7285419}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 332 is less than current step: 499. Dropping entry: {'train/loss': 1.7390905976295472, 'train/ce_loss': 0.3282470703125, 'train/seg_cls_loss': 0.013739013671875, 'train/kl_loss': 0.251953125, 'train/mask_bce_loss': 0.16391141954809427, 'train/mask_dice_loss': 0.5254458904266357, 'train/mask_loss': 0.6893573105335236, 'metrics/total_secs_per_batch': 6.444251537322998, 'metrics/data_secs_per_batch': 2.7805732250213624, '_timestamp': 1740973981.172524}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 332 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973981.1728084}).
Epoch: [6][334/500]	Time  6.994 ( 6.994)	Loss 2.7277 (1.9035)	CeLoss 0.2051 (0.2137)	SegCLSLoss 0.0149 (0.0130)	KLLoss 0.3691 (0.3285)	MaskLoss 1.2389 (0.8250)	MaskBCELoss 0.6491 (0.2716)	MaskDICELoss 0.5898 (0.5535)
Epoch: [6][335/500]	Time  7.313 ( 7.313)	Loss 2.3225 (1.7663)	CeLoss 0.1523 (0.2872)	SegCLSLoss 0.0198 (0.0151)	KLLoss 0.3906 (0.3307)	MaskLoss 1.0607 (0.7191)	MaskBCELoss 0.2500 (0.1102)	MaskDICELoss 0.8107 (0.6089)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 333 is less than current step: 499. Dropping entry: {'train/loss': 1.9035359740257263, 'train/ce_loss': 0.213720703125, 'train/seg_cls_loss': 0.013043212890625, 'train/kl_loss': 0.328515625, 'train/mask_bce_loss': 0.2715806484222412, 'train/mask_dice_loss': 0.5534539341926574, 'train/mask_loss': 0.8250345826148987, 'metrics/total_secs_per_batch': 6.993542432785034, 'metrics/data_secs_per_batch': 3.211694598197937, '_timestamp': 1740973988.1661065}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 333 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973988.1663818}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 334 is less than current step: 499. Dropping entry: {'train/loss': 1.7663474678993225, 'train/ce_loss': 0.28720703125, 'train/seg_cls_loss': 0.01513671875, 'train/kl_loss': 0.3306640625, 'train/mask_bce_loss': 0.11019980879500509, 'train/mask_dice_loss': 0.6089114248752594, 'train/mask_loss': 0.7191112339496613, 'metrics/total_secs_per_batch': 7.31331205368042, 'metrics/data_secs_per_batch': 3.132101821899414, '_timestamp': 1740973995.4794068}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 334 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740973995.4796891}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 335 is less than current step: 499. Dropping entry: {'train/loss': 1.4173743367195129, 'train/ce_loss': 0.31337890625, 'train/seg_cls_loss': 0.0107177734375, 'train/kl_loss': 0.2203125, 'train/mask_bce_loss': 0.08479364225640892, 'train/mask_dice_loss': 0.45348337292671204, 'train/mask_loss': 0.538277018070221, 'metrics/total_secs_per_batch': 5.564860820770264, 'metrics/data_secs_per_batch': 2.243071413040161, '_timestamp': 1740974001.0442145}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 335 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974001.0444894}).
Epoch: [6][336/500]	Time  5.565 ( 5.565)	Loss 0.0747 (1.4174)	CeLoss 0.0747 (0.3134)	SegCLSLoss 0.0000 (0.0107)	KLLoss 0.0000 (0.2203)	MaskLoss 0.0000 (0.5383)	MaskBCELoss 0.0000 (0.0848)	MaskDICELoss 0.0000 (0.4535)
Epoch: [6][337/500]	Time  6.361 ( 6.361)	Loss 1.1106 (1.8682)	CeLoss 0.2617 (0.4074)	SegCLSLoss 0.0109 (0.0128)	KLLoss 0.3691 (0.2908)	MaskLoss 0.4030 (0.7127)	MaskBCELoss 0.1319 (0.1472)	MaskDICELoss 0.2710 (0.5655)
Epoch: [6][338/500]	Time  7.070 ( 7.070)	Loss 1.2472 (1.6987)	CeLoss 0.2734 (0.2327)	SegCLSLoss 0.0109 (0.0136)	KLLoss 0.3633 (0.3287)	MaskLoss 0.4654 (0.7130)	MaskBCELoss 0.1015 (0.1256)	MaskDICELoss 0.3639 (0.5874)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 336 is less than current step: 499. Dropping entry: {'train/loss': 1.8681676864624024, 'train/ce_loss': 0.407421875, 'train/seg_cls_loss': 0.012774658203125, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.14719958528876304, 'train/mask_dice_loss': 0.5654975354671479, 'train/mask_loss': 0.7126971244812011, 'metrics/total_secs_per_batch': 6.3606061935424805, 'metrics/data_secs_per_batch': 2.8804898500442504, '_timestamp': 1740974007.4048097}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 336 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974007.4050586}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 337 is less than current step: 499. Dropping entry: {'train/loss': 1.6986980438232422, 'train/ce_loss': 0.23271484375, 'train/seg_cls_loss': 0.01361083984375, 'train/kl_loss': 0.3287109375, 'train/mask_bce_loss': 0.12564275301992894, 'train/mask_dice_loss': 0.5873781442642212, 'train/mask_loss': 0.7130208849906922, 'metrics/total_secs_per_batch': 7.0702924728393555, 'metrics/data_secs_per_batch': 3.2317565441131593, '_timestamp': 1740974014.4753428}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 337 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974014.4756975}).
Epoch: [6][339/500]	Time  6.760 ( 6.760)	Loss 2.0551 (1.7005)	CeLoss 0.1875 (0.2442)	SegCLSLoss 0.0238 (0.0169)	KLLoss 0.3633 (0.3252)	MaskLoss 0.9099 (0.7076)	MaskBCELoss 0.0629 (0.0726)	MaskDICELoss 0.8469 (0.6351)
[2025-03-02 21:53:47,911] [INFO] [logging.py:96:log_dist] [Rank 0] step=3340, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:53:47,917] [INFO] [timer.py:215:stop] epoch=0/micro_step=33400/global_step=3340, RunningAvgSamplesPerSec=1.442346389778596, CurrSamplesPerSec=1.4965777453253355, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [6][340/500]	Time  6.684 ( 6.684)	Loss 2.4124 (1.7141)	CeLoss 0.2275 (0.3338)	SegCLSLoss 0.0181 (0.0147)	KLLoss 0.3789 (0.2918)	MaskLoss 1.0695 (0.6720)	MaskBCELoss 0.1716 (0.1276)	MaskDICELoss 0.8979 (0.5444)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 338 is less than current step: 499. Dropping entry: {'train/loss': 1.7004977345466614, 'train/ce_loss': 0.24423828125, 'train/seg_cls_loss': 0.0168701171875, 'train/kl_loss': 0.3251953125, 'train/mask_bce_loss': 0.07255295151844621, 'train/mask_dice_loss': 0.6350689679384232, 'train/mask_loss': 0.7076219141483306, 'metrics/total_secs_per_batch': 6.759519338607788, 'metrics/data_secs_per_batch': 2.979204869270325, '_timestamp': 1740974021.2346802}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 338 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974021.2349634}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 339 is less than current step: 499. Dropping entry: {'train/loss': 1.7140637636184692, 'train/ce_loss': 0.333837890625, 'train/seg_cls_loss': 0.01474609375, 'train/kl_loss': 0.291796875, 'train/mask_bce_loss': 0.12762840762734412, 'train/mask_dice_loss': 0.5443692818284035, 'train/mask_loss': 0.6719976931810379, 'metrics/total_secs_per_batch': 6.683516025543213, 'metrics/data_secs_per_batch': 2.9138388872146606, '_timestamp': 1740974027.9179857}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 339 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974027.9182615}).
Epoch: [6][341/500]	Time  5.561 ( 5.561)	Loss 2.4214 (1.6339)	CeLoss 0.2334 (0.5109)	SegCLSLoss 0.0183 (0.0122)	KLLoss 0.3691 (0.2172)	MaskLoss 1.0710 (0.5474)	MaskBCELoss 0.2713 (0.1292)	MaskDICELoss 0.7998 (0.4183)
Epoch: [6][342/500]	Time  5.149 ( 5.149)	Loss 0.6484 (1.8177)	CeLoss 0.6484 (0.5169)	SegCLSLoss 0.0000 (0.0167)	KLLoss 0.0000 (0.2578)	MaskLoss 0.0000 (0.6333)	MaskBCELoss 0.0000 (0.1123)	MaskDICELoss 0.0000 (0.5210)
Epoch: [6][343/500]	Time  6.382 ( 6.382)	Loss 1.7587 (1.7192)	CeLoss 0.2832 (0.2563)	SegCLSLoss 0.0136 (0.0136)	KLLoss 0.3672 (0.3629)	MaskLoss 0.7163 (0.7100)	MaskBCELoss 0.3109 (0.1609)	MaskDICELoss 0.4053 (0.5491)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 340 is less than current step: 499. Dropping entry: {'train/loss': 1.6339213848114014, 'train/ce_loss': 0.5109375, 'train/seg_cls_loss': 0.01221923828125, 'train/kl_loss': 0.2171875, 'train/mask_bce_loss': 0.12917308402247726, 'train/mask_dice_loss': 0.41825635731220245, 'train/mask_loss': 0.5474294483661651, 'metrics/total_secs_per_batch': 5.561352014541626, 'metrics/data_secs_per_batch': 2.3746387243270872, '_timestamp': 1740974033.4795303}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 340 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974033.4798107}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 341 is less than current step: 499. Dropping entry: {'train/loss': 1.8176881909370421, 'train/ce_loss': 0.51689453125, 'train/seg_cls_loss': 0.01666259765625, 'train/kl_loss': 0.2578125, 'train/mask_bce_loss': 0.11234392300248146, 'train/mask_dice_loss': 0.5209630697965622, 'train/mask_loss': 0.6333069890737534, 'metrics/total_secs_per_batch': 5.148890018463135, 'metrics/data_secs_per_batch': 2.6205101728439333, '_timestamp': 1740974038.6284146}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 341 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974038.6286948}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 342 is less than current step: 499. Dropping entry: {'train/loss': 1.7192170023918152, 'train/ce_loss': 0.25634765625, 'train/seg_cls_loss': 0.013555908203125, 'train/kl_loss': 0.362890625, 'train/mask_bce_loss': 0.16089771836996078, 'train/mask_dice_loss': 0.5490525677800179, 'train/mask_loss': 0.7099502921104431, 'metrics/total_secs_per_batch': 6.382377624511719, 'metrics/data_secs_per_batch': 2.6809151411056518, '_timestamp': 1740974045.010823}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 342 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974045.0111184}).
Epoch: [6][344/500]	Time  4.993 ( 4.993)	Loss 1.4375 (1.9347)	CeLoss 1.4375 (0.5994)	SegCLSLoss 0.0000 (0.0131)	KLLoss 0.0000 (0.2262)	MaskLoss 0.0000 (0.6531)	MaskBCELoss 0.0000 (0.2394)	MaskDICELoss 0.0000 (0.4137)
Epoch: [6][345/500]	Time  6.551 ( 6.551)	Loss 1.7862 (1.6717)	CeLoss 0.2334 (0.2708)	SegCLSLoss 0.0172 (0.0123)	KLLoss 0.3633 (0.2906)	MaskLoss 0.7534 (0.6828)	MaskBCELoss 0.0248 (0.1019)	MaskDICELoss 0.7286 (0.5809)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 343 is less than current step: 499. Dropping entry: {'train/loss': 1.934747338294983, 'train/ce_loss': 0.5994140625, 'train/seg_cls_loss': 0.01314697265625, 'train/kl_loss': 0.226171875, 'train/mask_bce_loss': 0.23943601651117205, 'train/mask_dice_loss': 0.41367983520030976, 'train/mask_loss': 0.6531158566474915, 'metrics/total_secs_per_batch': 4.993497610092163, 'metrics/data_secs_per_batch': 2.3489492654800417, '_timestamp': 1740974050.004323}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 343 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974050.0046108}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 344 is less than current step: 499. Dropping entry: {'train/loss': 1.6716825366020203, 'train/ce_loss': 0.27080078125, 'train/seg_cls_loss': 0.012274169921875, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.10189061481505632, 'train/mask_dice_loss': 0.5808744907379151, 'train/mask_loss': 0.682765108346939, 'metrics/total_secs_per_batch': 6.551255464553833, 'metrics/data_secs_per_batch': 2.9077751874923705, '_timestamp': 1740974056.5555594}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 344 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974056.555837}).
Epoch: [6][346/500]	Time  7.681 ( 7.681)	Loss 1.9037 (1.4964)	CeLoss 0.1621 (0.3624)	SegCLSLoss 0.0205 (0.0130)	KLLoss 0.3711 (0.2908)	MaskLoss 0.8473 (0.5493)	MaskBCELoss 0.0192 (0.1188)	MaskDICELoss 0.8281 (0.4305)
Epoch: [6][347/500]	Time  6.004 ( 6.004)	Loss 1.0312 (1.9391)	CeLoss 1.0312 (0.3631)	SegCLSLoss 0.0000 (0.0137)	KLLoss 0.0000 (0.2871)	MaskLoss 0.0000 (0.7703)	MaskBCELoss 0.0000 (0.1566)	MaskDICELoss 0.0000 (0.6137)
Epoch: [6][348/500]	Time  5.726 ( 5.726)	Loss 2.4896 (1.2189)	CeLoss 0.2451 (0.3701)	SegCLSLoss 0.0129 (0.0067)	KLLoss 0.3594 (0.1816)	MaskLoss 1.1012 (0.4136)	MaskBCELoss 0.4859 (0.1594)	MaskDICELoss 0.6154 (0.2542)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 345 is less than current step: 499. Dropping entry: {'train/loss': 1.4964499950408936, 'train/ce_loss': 0.36240234375, 'train/seg_cls_loss': 0.01298828125, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.11876980718225241, 'train/mask_dice_loss': 0.4305294007062912, 'train/mask_loss': 0.5492992103099823, 'metrics/total_secs_per_batch': 7.680567502975464, 'metrics/data_secs_per_batch': 3.427607011795044, '_timestamp': 1740974064.2362685}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 345 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974064.236623}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 346 is less than current step: 499. Dropping entry: {'train/loss': 1.93906888961792, 'train/ce_loss': 0.3630859375, 'train/seg_cls_loss': 0.013739013671875, 'train/kl_loss': 0.287109375, 'train/mask_bce_loss': 0.15660192128270864, 'train/mask_dice_loss': 0.6137137770652771, 'train/mask_loss': 0.77031569480896, 'metrics/total_secs_per_batch': 6.004162073135376, 'metrics/data_secs_per_batch': 2.6915467739105225, '_timestamp': 1740974070.2402744}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 346 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974070.2405424}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 347 is less than current step: 499. Dropping entry: {'train/loss': 1.21891028881073, 'train/ce_loss': 0.3701171875, 'train/seg_cls_loss': 0.0067138671875, 'train/kl_loss': 0.181640625, 'train/mask_bce_loss': 0.1593752309679985, 'train/mask_dice_loss': 0.2542303055524826, 'train/mask_loss': 0.41360554099082947, 'metrics/total_secs_per_batch': 5.726468563079834, 'metrics/data_secs_per_batch': 2.7811930179595947, '_timestamp': 1740974075.9667861}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 347 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974075.9670658}).
Epoch: [6][349/500]	Time  6.842 ( 6.842)	Loss 1.6564 (1.2472)	CeLoss 0.2520 (0.3627)	SegCLSLoss 0.0153 (0.0092)	KLLoss 0.3633 (0.2555)	MaskLoss 0.6798 (0.4271)	MaskBCELoss 0.0607 (0.0689)	MaskDICELoss 0.6191 (0.3582)
[2025-03-02 21:54:47,031] [INFO] [logging.py:96:log_dist] [Rank 0] step=3350, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:54:47,037] [INFO] [timer.py:215:stop] epoch=0/micro_step=33500/global_step=3350, RunningAvgSamplesPerSec=1.442981913834511, CurrSamplesPerSec=2.365271755609909, MemAllocated=30.65GB, MaxMemAllocated=37.23GB
Epoch: [6][350/500]	Time  4.229 ( 4.229)	Loss 1.5325 (1.3237)	CeLoss 0.1699 (0.7177)	SegCLSLoss 0.0311 (0.0085)	KLLoss 0.3672 (0.1818)	MaskLoss 0.6549 (0.2917)	MaskBCELoss 0.2282 (0.0619)	MaskDICELoss 0.4268 (0.2298)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 348 is less than current step: 499. Dropping entry: {'train/loss': 1.2471619367599487, 'train/ce_loss': 0.3626953125, 'train/seg_cls_loss': 0.009197998046875, 'train/kl_loss': 0.25546875, 'train/mask_bce_loss': 0.06894166776910424, 'train/mask_dice_loss': 0.3582037538290024, 'train/mask_loss': 0.4271454155445099, 'metrics/total_secs_per_batch': 6.842348098754883, 'metrics/data_secs_per_batch': 2.9957545518875124, '_timestamp': 1740974082.8091273}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 348 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974082.8093936}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 349 is less than current step: 499. Dropping entry: {'train/loss': 1.3236743807792664, 'train/ce_loss': 0.71767578125, 'train/seg_cls_loss': 0.00850830078125, 'train/kl_loss': 0.1818359375, 'train/mask_bce_loss': 0.061924977228045464, 'train/mask_dice_loss': 0.22979502007365227, 'train/mask_loss': 0.2917199984192848, 'metrics/total_secs_per_batch': 4.2294347286224365, 'metrics/data_secs_per_batch': 2.033227252960205, '_timestamp': 1740974087.038371}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 349 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974087.0386567}).
Epoch: [6][351/500]	Time  6.422 ( 6.422)	Loss 2.3707 (1.5484)	CeLoss 0.2070 (0.2918)	SegCLSLoss 0.0159 (0.0124)	KLLoss 0.3594 (0.2934)	MaskLoss 1.0603 (0.6105)	MaskBCELoss 0.3860 (0.1399)	MaskDICELoss 0.6743 (0.4706)
Epoch: [6][352/500]	Time  6.556 ( 6.556)	Loss 0.2402 (1.5902)	CeLoss 0.2402 (0.4224)	SegCLSLoss 0.0000 (0.0098)	KLLoss 0.0000 (0.2559)	MaskLoss 0.0000 (0.5686)	MaskBCELoss 0.0000 (0.1584)	MaskDICELoss 0.0000 (0.4102)
Epoch: [6][353/500]	Time  6.469 ( 6.469)	Loss 0.2432 (1.4414)	CeLoss 0.2432 (0.3979)	SegCLSLoss 0.0000 (0.0117)	KLLoss 0.0000 (0.2176)	MaskLoss 0.0000 (0.5078)	MaskBCELoss 0.0000 (0.0692)	MaskDICELoss 0.0000 (0.4386)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 350 is less than current step: 499. Dropping entry: {'train/loss': 1.5484016001224519, 'train/ce_loss': 0.291796875, 'train/seg_cls_loss': 0.01239013671875, 'train/kl_loss': 0.293359375, 'train/mask_bce_loss': 0.13988782074302436, 'train/mask_dice_loss': 0.47064109295606615, 'train/mask_loss': 0.6105289205908775, 'metrics/total_secs_per_batch': 6.421689987182617, 'metrics/data_secs_per_batch': 2.8858563184738157, '_timestamp': 1740974093.4602969}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 350 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974093.460569}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 351 is less than current step: 499. Dropping entry: {'train/loss': 1.5901642441749573, 'train/ce_loss': 0.42236328125, 'train/seg_cls_loss': 0.009759521484375, 'train/kl_loss': 0.255859375, 'train/mask_bce_loss': 0.15843611545860767, 'train/mask_dice_loss': 0.41018115878105166, 'train/mask_loss': 0.5686172768473625, 'metrics/total_secs_per_batch': 6.556361198425293, 'metrics/data_secs_per_batch': 2.600594639778137, '_timestamp': 1740974100.0168662}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 351 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974100.0172162}).
Epoch: [6][354/500]	Time  5.608 ( 5.608)	Loss 0.7327 (1.3032)	CeLoss 0.2598 (0.4989)	SegCLSLoss 0.0119 (0.0076)	KLLoss 0.3633 (0.2193)	MaskLoss 0.2150 (0.3893)	MaskBCELoss 0.0571 (0.0956)	MaskDICELoss 0.1578 (0.2937)
Epoch: [6][355/500]	Time  4.965 ( 4.965)	Loss 3.3568 (1.4975)	CeLoss 0.2285 (0.6032)	SegCLSLoss 0.0204 (0.0115)	KLLoss 0.3691 (0.2197)	MaskLoss 1.5407 (0.4334)	MaskBCELoss 0.8429 (0.1436)	MaskDICELoss 0.6978 (0.2898)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 352 is less than current step: 499. Dropping entry: {'train/loss': 1.4414039254188538, 'train/ce_loss': 0.397900390625, 'train/seg_cls_loss': 0.01165771484375, 'train/kl_loss': 0.217578125, 'train/mask_bce_loss': 0.06920239217579364, 'train/mask_dice_loss': 0.43863335251808167, 'train/mask_loss': 0.507835739850998, 'metrics/total_secs_per_batch': 6.468688726425171, 'metrics/data_secs_per_batch': 2.765182685852051, '_timestamp': 1740974106.485374}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 352 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974106.48568}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 353 is less than current step: 499. Dropping entry: {'train/loss': 1.3032427906990052, 'train/ce_loss': 0.498876953125, 'train/seg_cls_loss': 0.007574462890625, 'train/kl_loss': 0.2193359375, 'train/mask_bce_loss': 0.09563639578409493, 'train/mask_dice_loss': 0.29365588575601576, 'train/mask_loss': 0.3892922818660736, 'metrics/total_secs_per_batch': 5.608051538467407, 'metrics/data_secs_per_batch': 2.4035455703735353, '_timestamp': 1740974112.0933638}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 353 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974112.0936294}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 354 is less than current step: 499. Dropping entry: {'train/loss': 1.497541183233261, 'train/ce_loss': 0.60322265625, 'train/seg_cls_loss': 0.0114990234375, 'train/kl_loss': 0.2197265625, 'train/mask_bce_loss': 0.14359867684543132, 'train/mask_dice_loss': 0.28979106470942495, 'train/mask_loss': 0.4333897322416306, 'metrics/total_secs_per_batch': 4.96517276763916, 'metrics/data_secs_per_batch': 2.037567377090454, '_timestamp': 1740974117.058526}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 354 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974117.058791}).
Epoch: [6][356/500]	Time  6.449 ( 6.449)	Loss 1.8986 (1.6524)	CeLoss 0.3711 (0.3072)	SegCLSLoss 0.0095 (0.0140)	KLLoss 0.3594 (0.3312)	MaskLoss 0.7442 (0.6526)	MaskBCELoss 0.0043 (0.1383)	MaskDICELoss 0.7399 (0.5143)
Epoch: [6][357/500]	Time  6.886 ( 6.886)	Loss 1.7878 (1.8209)	CeLoss 0.2139 (0.4250)	SegCLSLoss 0.0205 (0.0159)	KLLoss 0.3535 (0.2900)	MaskLoss 0.7640 (0.6795)	MaskBCELoss 0.0298 (0.1051)	MaskDICELoss 0.7343 (0.5743)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 355 is less than current step: 499. Dropping entry: {'train/loss': 1.6523901283740998, 'train/ce_loss': 0.3072265625, 'train/seg_cls_loss': 0.01395263671875, 'train/kl_loss': 0.33125, 'train/mask_bce_loss': 0.1382882953621447, 'train/mask_dice_loss': 0.5142739564180374, 'train/mask_loss': 0.6525622546672821, 'metrics/total_secs_per_batch': 6.4488725662231445, 'metrics/data_secs_per_batch': 3.0760307788848875, '_timestamp': 1740974123.5074553}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 355 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974123.507725}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 356 is less than current step: 499. Dropping entry: {'train/loss': 1.8209450602531434, 'train/ce_loss': 0.425, 'train/seg_cls_loss': 0.015869140625, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.10514511596411466, 'train/mask_dice_loss': 0.5743215411901474, 'train/mask_loss': 0.6794666588306427, 'metrics/total_secs_per_batch': 6.886358737945557, 'metrics/data_secs_per_batch': 3.24073703289032, '_timestamp': 1740974130.393835}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 356 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974130.3941514}).
Epoch: [6][358/500]	Time  7.098 ( 7.098)	Loss 1.5848 (1.1316)	CeLoss 0.1992 (0.1861)	SegCLSLoss 0.0139 (0.0118)	KLLoss 0.3711 (0.2531)	MaskLoss 0.6708 (0.4573)	MaskBCELoss 0.0878 (0.0588)	MaskDICELoss 0.5830 (0.3985)
Epoch: [6][359/500]	Time  5.946 ( 5.946)	Loss 2.0039 (1.4474)	CeLoss 0.2412 (0.4497)	SegCLSLoss 0.0206 (0.0104)	KLLoss 0.3594 (0.2521)	MaskLoss 0.8584 (0.4837)	MaskBCELoss 0.0340 (0.0671)	MaskDICELoss 0.8244 (0.4166)
[2025-03-02 21:55:49,794] [INFO] [logging.py:96:log_dist] [Rank 0] step=3360, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:55:49,799] [INFO] [timer.py:215:stop] epoch=0/micro_step=33600/global_step=3360, RunningAvgSamplesPerSec=1.4433882723810425, CurrSamplesPerSec=1.5720546276749416, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [6][360/500]	Time  6.363 ( 6.363)	Loss 1.7107 (1.7093)	CeLoss 0.2100 (0.3388)	SegCLSLoss 0.0148 (0.0107)	KLLoss 0.3691 (0.2578)	MaskLoss 0.7284 (0.6695)	MaskBCELoss 0.0841 (0.1112)	MaskDICELoss 0.6443 (0.5583)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 357 is less than current step: 499. Dropping entry: {'train/loss': 1.131600034236908, 'train/ce_loss': 0.1860595703125, 'train/seg_cls_loss': 0.0117919921875, 'train/kl_loss': 0.253125, 'train/mask_bce_loss': 0.05875178258866072, 'train/mask_dice_loss': 0.39853993952274325, 'train/mask_loss': 0.4572917178273201, 'metrics/total_secs_per_batch': 7.098078966140747, 'metrics/data_secs_per_batch': 3.181863212585449, '_timestamp': 1740974137.4919257}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 357 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974137.4922051}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 358 is less than current step: 499. Dropping entry: {'train/loss': 1.447382402420044, 'train/ce_loss': 0.449658203125, 'train/seg_cls_loss': 0.010400390625, 'train/kl_loss': 0.2521484375, 'train/mask_bce_loss': 0.06708556916564704, 'train/mask_dice_loss': 0.4165909856557846, 'train/mask_loss': 0.48367656022310257, 'metrics/total_secs_per_batch': 5.945921182632446, 'metrics/data_secs_per_batch': 2.393409752845764, '_timestamp': 1740974143.4377768}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 358 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974143.4380662}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 359 is less than current step: 499. Dropping entry: {'train/loss': 1.7093109607696533, 'train/ce_loss': 0.33876953125, 'train/seg_cls_loss': 0.010736083984375, 'train/kl_loss': 0.2578125, 'train/mask_bce_loss': 0.11124115819111466, 'train/mask_dice_loss': 0.5583069145679473, 'train/mask_loss': 0.6695480704307556, 'metrics/total_secs_per_batch': 6.362699270248413, 'metrics/data_secs_per_batch': 2.785479116439819, '_timestamp': 1740974149.8003411}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 359 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974149.8006313}).
Epoch: [6][361/500]	Time  7.384 ( 7.384)	Loss 1.6880 (1.9144)	CeLoss 0.2891 (0.2508)	SegCLSLoss 0.0110 (0.0156)	KLLoss 0.3652 (0.3604)	MaskLoss 0.6780 (0.8098)	MaskBCELoss 0.0613 (0.1085)	MaskDICELoss 0.6167 (0.7013)
Epoch: [6][362/500]	Time  6.694 ( 6.694)	Loss 2.4840 (1.3865)	CeLoss 0.1611 (0.2290)	SegCLSLoss 0.0221 (0.0144)	KLLoss 0.3945 (0.3291)	MaskLoss 1.1365 (0.5586)	MaskBCELoss 0.2591 (0.0982)	MaskDICELoss 0.8775 (0.4604)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 360 is less than current step: 499. Dropping entry: {'train/loss': 1.9143769860267639, 'train/ce_loss': 0.25078125, 'train/seg_cls_loss': 0.015618896484375, 'train/kl_loss': 0.3603515625, 'train/mask_bce_loss': 0.10845798281952738, 'train/mask_dice_loss': 0.7013183921575546, 'train/mask_loss': 0.8097763597965241, 'metrics/total_secs_per_batch': 7.384427785873413, 'metrics/data_secs_per_batch': 3.347505211830139, '_timestamp': 1740974157.1851099}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 360 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974157.1854336}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 361 is less than current step: 499. Dropping entry: {'train/loss': 1.3864943623542785, 'train/ce_loss': 0.22900390625, 'train/seg_cls_loss': 0.014361572265625, 'train/kl_loss': 0.3291015625, 'train/mask_bce_loss': 0.09822016470134258, 'train/mask_dice_loss': 0.46040787547826767, 'train/mask_loss': 0.5586280435323715, 'metrics/total_secs_per_batch': 6.69364070892334, 'metrics/data_secs_per_batch': 3.037969708442688, '_timestamp': 1740974163.878646}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 361 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974163.8789475}).
Epoch: [6][363/500]	Time  6.155 ( 6.155)	Loss 2.2151 (1.4310)	CeLoss 0.0889 (0.2761)	SegCLSLoss 0.0347 (0.0135)	KLLoss 0.3906 (0.2580)	MaskLoss 1.0348 (0.5611)	MaskBCELoss 0.1489 (0.1204)	MaskDICELoss 0.8859 (0.4406)
Epoch: [6][364/500]	Time  5.799 ( 5.799)	Loss 1.9776 (1.8586)	CeLoss 0.2910 (0.5465)	SegCLSLoss 0.0129 (0.0120)	KLLoss 0.3516 (0.2545)	MaskLoss 0.8228 (0.6402)	MaskBCELoss 0.0069 (0.0951)	MaskDICELoss 0.8159 (0.5451)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 362 is less than current step: 499. Dropping entry: {'train/loss': 1.4310388803482055, 'train/ce_loss': 0.276123046875, 'train/seg_cls_loss': 0.013525390625, 'train/kl_loss': 0.2580078125, 'train/mask_bce_loss': 0.12040772289037704, 'train/mask_dice_loss': 0.4406439334154129, 'train/mask_loss': 0.5610516667366028, 'metrics/total_secs_per_batch': 6.155436038970947, 'metrics/data_secs_per_batch': 2.7849905252456666, '_timestamp': 1740974170.034069}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 362 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974170.034274}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 363 is less than current step: 499. Dropping entry: {'train/loss': 1.85856374502182, 'train/ce_loss': 0.546484375, 'train/seg_cls_loss': 0.011962890625, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.09509446453303098, 'train/mask_dice_loss': 0.5451249122619629, 'train/mask_loss': 0.6402193635702134, 'metrics/total_secs_per_batch': 5.7993574142456055, 'metrics/data_secs_per_batch': 2.723667931556702, '_timestamp': 1740974175.8333762}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 363 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974175.833734}).
Epoch: [6][365/500]	Time  5.993 ( 5.993)	Loss 1.5938 (1.5081)	CeLoss 1.5938 (0.5406)	SegCLSLoss 0.0000 (0.0119)	KLLoss 0.0000 (0.2160)	MaskLoss 0.0000 (0.4699)	MaskBCELoss 0.0000 (0.0457)	MaskDICELoss 0.0000 (0.4241)
Epoch: [6][366/500]	Time  5.619 ( 5.619)	Loss 2.0296 (1.3867)	CeLoss 0.2734 (0.3056)	SegCLSLoss 0.0107 (0.0123)	KLLoss 0.3594 (0.2922)	MaskLoss 0.8576 (0.5228)	MaskBCELoss 0.1059 (0.0957)	MaskDICELoss 0.7516 (0.4271)
Epoch: [6][367/500]	Time  5.236 ( 5.236)	Loss 2.0781 (1.4964)	CeLoss 2.0781 (0.8384)	SegCLSLoss 0.0000 (0.0062)	KLLoss 0.0000 (0.1445)	MaskLoss 0.0000 (0.3203)	MaskBCELoss 0.0000 (0.0500)	MaskDICELoss 0.0000 (0.2703)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 364 is less than current step: 499. Dropping entry: {'train/loss': 1.5080758154392242, 'train/ce_loss': 0.540625, 'train/seg_cls_loss': 0.01185302734375, 'train/kl_loss': 0.216015625, 'train/mask_bce_loss': 0.04571840530261397, 'train/mask_dice_loss': 0.424139803647995, 'train/mask_loss': 0.46985821425914764, 'metrics/total_secs_per_batch': 5.993260622024536, 'metrics/data_secs_per_batch': 2.8205498695373534, '_timestamp': 1740974181.8266742}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 364 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974181.8269467}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 365 is less than current step: 499. Dropping entry: {'train/loss': 1.3867475152015687, 'train/ce_loss': 0.30556640625, 'train/seg_cls_loss': 0.012274169921875, 'train/kl_loss': 0.2921875, 'train/mask_bce_loss': 0.09570917920209468, 'train/mask_dice_loss': 0.42710795551538466, 'train/mask_loss': 0.5228171363472939, 'metrics/total_secs_per_batch': 5.618593215942383, 'metrics/data_secs_per_batch': 2.374617600440979, '_timestamp': 1740974187.445266}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 365 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974187.4455504}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 366 is less than current step: 499. Dropping entry: {'train/loss': 1.496362817287445, 'train/ce_loss': 0.838427734375, 'train/seg_cls_loss': 0.00615234375, 'train/kl_loss': 0.14453125, 'train/mask_bce_loss': 0.04995473269373178, 'train/mask_dice_loss': 0.2703214049339294, 'train/mask_loss': 0.32027613520622256, 'metrics/total_secs_per_batch': 5.235947847366333, 'metrics/data_secs_per_batch': 1.8763334035873414, '_timestamp': 1740974192.681164}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 366 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974192.6814342}).
Epoch: [6][368/500]	Time  5.984 ( 5.984)	Loss 0.7225 (1.7718)	CeLoss 0.2695 (0.3379)	SegCLSLoss 0.0098 (0.0164)	KLLoss 0.3555 (0.3227)	MaskLoss 0.2070 (0.6967)	MaskBCELoss 0.0520 (0.1412)	MaskDICELoss 0.1550 (0.5556)
Epoch: [6][369/500]	Time  5.906 ( 5.906)	Loss 2.3105 (1.1347)	CeLoss 0.2422 (0.4182)	SegCLSLoss 0.0095 (0.0062)	KLLoss 0.3691 (0.1816)	MaskLoss 1.0137 (0.3475)	MaskBCELoss 0.0260 (0.0415)	MaskDICELoss 0.9876 (0.3059)
[2025-03-02 21:56:50,352] [INFO] [logging.py:96:log_dist] [Rank 0] step=3370, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:56:50,358] [INFO] [timer.py:215:stop] epoch=0/micro_step=33700/global_step=3370, RunningAvgSamplesPerSec=1.443928843442804, CurrSamplesPerSec=1.7285386306358834, MemAllocated=30.87GB, MaxMemAllocated=37.23GB
Epoch: [6][370/500]	Time  5.787 ( 5.787)	Loss 0.9844 (1.7410)	CeLoss 0.9844 (0.5646)	SegCLSLoss 0.0000 (0.0120)	KLLoss 0.0000 (0.2527)	MaskLoss 0.0000 (0.5725)	MaskBCELoss 0.0000 (0.0805)	MaskDICELoss 0.0000 (0.4920)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 367 is less than current step: 499. Dropping entry: {'train/loss': 1.7717679738998413, 'train/ce_loss': 0.337939453125, 'train/seg_cls_loss': 0.0164306640625, 'train/kl_loss': 0.32265625, 'train/mask_bce_loss': 0.14116616789251565, 'train/mask_dice_loss': 0.555557657778263, 'train/mask_loss': 0.6967238262295723, 'metrics/total_secs_per_batch': 5.984451055526733, 'metrics/data_secs_per_batch': 2.6346620082855225, '_timestamp': 1740974198.6656835}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 367 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974198.6659575}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 368 is less than current step: 499. Dropping entry: {'train/loss': 1.134697687625885, 'train/ce_loss': 0.418212890625, 'train/seg_cls_loss': 0.0061767578125, 'train/kl_loss': 0.181640625, 'train/mask_bce_loss': 0.04154911597725004, 'train/mask_dice_loss': 0.305902262032032, 'train/mask_loss': 0.3474513828754425, 'metrics/total_secs_per_batch': 5.906418800354004, 'metrics/data_secs_per_batch': 2.5372442245483398, '_timestamp': 1740974204.5721257}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 368 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974204.572407}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 369 is less than current step: 499. Dropping entry: {'train/loss': 1.7409725427627563, 'train/ce_loss': 0.5646484375, 'train/seg_cls_loss': 0.01199951171875, 'train/kl_loss': 0.252734375, 'train/mask_bce_loss': 0.08049560813233256, 'train/mask_dice_loss': 0.4919926106929779, 'train/mask_loss': 0.5724882185459137, 'metrics/total_secs_per_batch': 5.786917448043823, 'metrics/data_secs_per_batch': 2.6528513193130494, '_timestamp': 1740974210.3588347}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 369 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974210.359102}).
Epoch: [6][371/500]	Time  6.470 ( 6.470)	Loss 2.6086 (1.8466)	CeLoss 0.1787 (0.2846)	SegCLSLoss 0.0214 (0.0167)	KLLoss 0.3750 (0.2949)	MaskLoss 1.1910 (0.7621)	MaskBCELoss 0.2320 (0.1081)	MaskDICELoss 0.9590 (0.6540)
Epoch: [6][372/500]	Time  6.778 ( 6.778)	Loss 2.0920 (1.5969)	CeLoss 0.1934 (0.4238)	SegCLSLoss 0.0175 (0.0115)	KLLoss 0.3477 (0.2891)	MaskLoss 0.9278 (0.5691)	MaskBCELoss 0.0219 (0.1136)	MaskDICELoss 0.9059 (0.4555)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 370 is less than current step: 499. Dropping entry: {'train/loss': 1.8465949177742005, 'train/ce_loss': 0.284619140625, 'train/seg_cls_loss': 0.01666259765625, 'train/kl_loss': 0.294921875, 'train/mask_bce_loss': 0.10807317774742842, 'train/mask_dice_loss': 0.654042649269104, 'train/mask_loss': 0.7621158301830292, 'metrics/total_secs_per_batch': 6.469854116439819, 'metrics/data_secs_per_batch': 2.541352868080139, '_timestamp': 1740974216.828932}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 370 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974216.8292289}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 371 is less than current step: 499. Dropping entry: {'train/loss': 1.596878218650818, 'train/ce_loss': 0.423828125, 'train/seg_cls_loss': 0.011468505859375, 'train/kl_loss': 0.2890625, 'train/mask_bce_loss': 0.11360151953995228, 'train/mask_dice_loss': 0.4555407091975212, 'train/mask_loss': 0.5691422268748283, 'metrics/total_secs_per_batch': 6.777777910232544, 'metrics/data_secs_per_batch': 2.619470477104187, '_timestamp': 1740974223.60674}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 371 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974223.6071455}).
Epoch: [6][373/500]	Time  5.505 ( 5.505)	Loss 1.3493 (1.5213)	CeLoss 0.3574 (0.5130)	SegCLSLoss 0.0140 (0.0122)	KLLoss 0.3750 (0.2182)	MaskLoss 0.4735 (0.4902)	MaskBCELoss 0.2408 (0.1024)	MaskDICELoss 0.2327 (0.3878)
Epoch: [6][374/500]	Time  5.634 ( 5.634)	Loss 1.0781 (1.6997)	CeLoss 1.0781 (0.5482)	SegCLSLoss 0.0000 (0.0109)	KLLoss 0.0000 (0.2549)	MaskLoss 0.0000 (0.5604)	MaskBCELoss 0.0000 (0.1181)	MaskDICELoss 0.0000 (0.4423)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 372 is less than current step: 499. Dropping entry: {'train/loss': 1.5213443398475648, 'train/ce_loss': 0.5130126953125, 'train/seg_cls_loss': 0.01217041015625, 'train/kl_loss': 0.2181640625, 'train/mask_bce_loss': 0.10235427506268024, 'train/mask_dice_loss': 0.3878466963768005, 'train/mask_loss': 0.4902009695768356, 'metrics/total_secs_per_batch': 5.505134582519531, 'metrics/data_secs_per_batch': 2.6197781324386598, '_timestamp': 1740974229.1118248}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 372 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974229.1121328}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 373 is less than current step: 499. Dropping entry: {'train/loss': 1.6997228503227233, 'train/ce_loss': 0.5482421875, 'train/seg_cls_loss': 0.01094970703125, 'train/kl_loss': 0.2548828125, 'train/mask_bce_loss': 0.11806578668765724, 'train/mask_dice_loss': 0.4422936782240868, 'train/mask_loss': 0.560359463095665, 'metrics/total_secs_per_batch': 5.633512735366821, 'metrics/data_secs_per_batch': 2.6934866189956663, '_timestamp': 1740974234.745397}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 373 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974234.7457151}).
Epoch: [6][375/500]	Time  6.479 ( 6.479)	Loss 1.8041 (1.4752)	CeLoss 0.3535 (0.2663)	SegCLSLoss 0.0101 (0.0137)	KLLoss 0.3574 (0.3244)	MaskLoss 0.7048 (0.5849)	MaskBCELoss 0.1056 (0.1178)	MaskDICELoss 0.5991 (0.4671)
Epoch: [6][376/500]	Time  4.529 ( 4.529)	Loss 0.9727 (1.3891)	CeLoss 0.9727 (0.7238)	SegCLSLoss 0.0000 (0.0107)	KLLoss 0.0000 (0.2152)	MaskLoss 0.0000 (0.3192)	MaskBCELoss 0.0000 (0.1155)	MaskDICELoss 0.0000 (0.2036)
Epoch: [6][377/500]	Time  5.868 ( 5.868)	Loss 2.6354 (1.9070)	CeLoss 0.2002 (0.2913)	SegCLSLoss 0.0160 (0.0169)	KLLoss 0.3613 (0.2941)	MaskLoss 1.1957 (0.7888)	MaskBCELoss 0.3304 (0.2101)	MaskDICELoss 0.8652 (0.5787)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 374 is less than current step: 499. Dropping entry: {'train/loss': 1.4752169311046601, 'train/ce_loss': 0.26630859375, 'train/seg_cls_loss': 0.013653564453125, 'train/kl_loss': 0.3244140625, 'train/mask_bce_loss': 0.11778489071875811, 'train/mask_dice_loss': 0.46708920150995253, 'train/mask_loss': 0.58487409055233, 'metrics/total_secs_per_batch': 6.47931981086731, 'metrics/data_secs_per_batch': 2.777656888961792, '_timestamp': 1740974241.2245567}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 374 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974241.2248187}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 375 is less than current step: 499. Dropping entry: {'train/loss': 1.3891019761562347, 'train/ce_loss': 0.723828125, 'train/seg_cls_loss': 0.010675048828125, 'train/kl_loss': 0.215234375, 'train/mask_bce_loss': 0.11554956799373031, 'train/mask_dice_loss': 0.2036107987165451, 'train/mask_loss': 0.31916036307811735, 'metrics/total_secs_per_batch': 4.529293775558472, 'metrics/data_secs_per_batch': 2.0979371786117555, '_timestamp': 1740974245.7539158}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 375 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974245.754199}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 376 is less than current step: 499. Dropping entry: {'train/loss': 1.9069884300231934, 'train/ce_loss': 0.29130859375, 'train/seg_cls_loss': 0.016888427734375, 'train/kl_loss': 0.294140625, 'train/mask_bce_loss': 0.2101454772055149, 'train/mask_dice_loss': 0.5787003129720688, 'train/mask_loss': 0.7888457894325256, 'metrics/total_secs_per_batch': 5.868041276931763, 'metrics/data_secs_per_batch': 2.426231527328491, '_timestamp': 1740974251.6220052}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 376 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974251.6222885}).
Epoch: [6][378/500]	Time  6.841 ( 6.841)	Loss 1.8158 (1.7234)	CeLoss 0.2285 (0.4893)	SegCLSLoss 0.0210 (0.0106)	KLLoss 0.3535 (0.2523)	MaskLoss 0.7702 (0.6018)	MaskBCELoss 0.0239 (0.1154)	MaskDICELoss 0.7463 (0.4864)
Epoch: [6][379/500]	Time  4.571 ( 4.571)	Loss 2.5512 (1.6872)	CeLoss 0.1226 (0.5663)	SegCLSLoss 0.0253 (0.0092)	KLLoss 0.3867 (0.2199)	MaskLoss 1.1887 (0.5473)	MaskBCELoss 0.3204 (0.1252)	MaskDICELoss 0.8683 (0.4221)
[2025-03-02 21:57:49,868] [INFO] [logging.py:96:log_dist] [Rank 0] step=3380, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:57:49,874] [INFO] [timer.py:215:stop] epoch=0/micro_step=33800/global_step=3380, RunningAvgSamplesPerSec=1.4445309858127457, CurrSamplesPerSec=1.4620061076333566, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [6][380/500]	Time  6.842 ( 6.842)	Loss 1.5426 (2.1115)	CeLoss 0.2910 (0.3534)	SegCLSLoss 0.0100 (0.0129)	KLLoss 0.3613 (0.2934)	MaskLoss 0.6053 (0.8612)	MaskBCELoss 0.1005 (0.2780)	MaskDICELoss 0.5048 (0.5832)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 377 is less than current step: 499. Dropping entry: {'train/loss': 1.7233667612075805, 'train/ce_loss': 0.4892578125, 'train/seg_cls_loss': 0.01060791015625, 'train/kl_loss': 0.25234375, 'train/mask_bce_loss': 0.1153748270124197, 'train/mask_dice_loss': 0.48639644086360934, 'train/mask_loss': 0.6017712652683258, 'metrics/total_secs_per_batch': 6.840606451034546, 'metrics/data_secs_per_batch': 2.8878970623016356, '_timestamp': 1740974258.4625793}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 377 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974258.462856}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 378 is less than current step: 499. Dropping entry: {'train/loss': 1.687226700782776, 'train/ce_loss': 0.566259765625, 'train/seg_cls_loss': 0.00919189453125, 'train/kl_loss': 0.219921875, 'train/mask_bce_loss': 0.12520817816257476, 'train/mask_dice_loss': 0.42206729352474215, 'train/mask_loss': 0.5472754597663879, 'metrics/total_secs_per_batch': 4.571401834487915, 'metrics/data_secs_per_batch': 2.0431785106658937, '_timestamp': 1740974263.0340328}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 378 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974263.0343213}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 379 is less than current step: 499. Dropping entry: {'train/loss': 2.111505115032196, 'train/ce_loss': 0.3534423828125, 'train/seg_cls_loss': 0.012945556640625, 'train/kl_loss': 0.293359375, 'train/mask_bce_loss': 0.27795255817472936, 'train/mask_dice_loss': 0.5832077145576477, 'train/mask_loss': 0.861160272359848, 'metrics/total_secs_per_batch': 6.841660737991333, 'metrics/data_secs_per_batch': 2.778548240661621, '_timestamp': 1740974269.8754683}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 379 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974269.8757436}).
Epoch: [6][381/500]	Time  6.346 ( 6.346)	Loss 0.8842 (1.3762)	CeLoss 0.2754 (0.3466)	SegCLSLoss 0.0104 (0.0113)	KLLoss 0.3613 (0.2557)	MaskLoss 0.2839 (0.4993)	MaskBCELoss 0.0309 (0.0778)	MaskDICELoss 0.2530 (0.4215)
Epoch: [6][382/500]	Time  6.246 ( 6.246)	Loss 1.0268 (1.1861)	CeLoss 0.2617 (0.2999)	SegCLSLoss 0.0096 (0.0109)	KLLoss 0.3633 (0.2553)	MaskLoss 0.3620 (0.4276)	MaskBCELoss 0.1253 (0.0850)	MaskDICELoss 0.2368 (0.3425)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 380 is less than current step: 499. Dropping entry: {'train/loss': 1.3761733233928681, 'train/ce_loss': 0.34658203125, 'train/seg_cls_loss': 0.0112548828125, 'train/kl_loss': 0.2556640625, 'train/mask_bce_loss': 0.07778323397506029, 'train/mask_dice_loss': 0.42153389379382133, 'train/mask_loss': 0.499317130446434, 'metrics/total_secs_per_batch': 6.345849990844727, 'metrics/data_secs_per_batch': 2.8091463327407835, '_timestamp': 1740974276.2215102}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 380 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974276.221786}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 381 is less than current step: 499. Dropping entry: {'train/loss': 1.1861254096031189, 'train/ce_loss': 0.299853515625, 'train/seg_cls_loss': 0.01094970703125, 'train/kl_loss': 0.2552734375, 'train/mask_bce_loss': 0.08501602597534656, 'train/mask_dice_loss': 0.3425437480211258, 'train/mask_loss': 0.42755977287888525, 'metrics/total_secs_per_batch': 6.246423006057739, 'metrics/data_secs_per_batch': 2.5677729845046997, '_timestamp': 1740974282.4679544}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 381 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974282.4682362}).
Epoch: [6][383/500]	Time  6.262 ( 6.262)	Loss 2.0288 (1.8629)	CeLoss 0.2295 (0.4478)	SegCLSLoss 0.0165 (0.0151)	KLLoss 0.3633 (0.2896)	MaskLoss 0.8777 (0.6893)	MaskBCELoss 0.0375 (0.1349)	MaskDICELoss 0.8402 (0.5544)
Epoch: [6][384/500]	Time  5.958 ( 5.958)	Loss 1.4920 (1.9955)	CeLoss 0.1992 (0.4378)	SegCLSLoss 0.0136 (0.0127)	KLLoss 0.3633 (0.2896)	MaskLoss 0.6249 (0.7612)	MaskBCELoss 0.0659 (0.1943)	MaskDICELoss 0.5591 (0.5668)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 382 is less than current step: 499. Dropping entry: {'train/loss': 1.8629296898841858, 'train/ce_loss': 0.44775390625, 'train/seg_cls_loss': 0.015087890625, 'train/kl_loss': 0.2896484375, 'train/mask_bce_loss': 0.13486364316195248, 'train/mask_dice_loss': 0.5544137001037598, 'train/mask_loss': 0.6892773449420929, 'metrics/total_secs_per_batch': 6.261556148529053, 'metrics/data_secs_per_batch': 2.888665962219238, '_timestamp': 1740974288.729532}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 382 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974288.729819}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 383 is less than current step: 499. Dropping entry: {'train/loss': 1.9954522490501403, 'train/ce_loss': 0.43779296875, 'train/seg_cls_loss': 0.01273193359375, 'train/kl_loss': 0.2896484375, 'train/mask_bce_loss': 0.19433999862521886, 'train/mask_dice_loss': 0.566813862323761, 'train/mask_loss': 0.7611538589000701, 'metrics/total_secs_per_batch': 5.957682847976685, 'metrics/data_secs_per_batch': 2.3889297008514405, '_timestamp': 1740974294.6871843}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 383 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974294.6874583}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 384 is less than current step: 499. Dropping entry: {'train/loss': 2.308665382862091, 'train/ce_loss': 0.27080078125, 'train/seg_cls_loss': 0.01688232421875, 'train/kl_loss': 0.3296875, 'train/mask_bce_loss': 0.5246470993384719, 'train/mask_dice_loss': 0.47348443865776063, 'train/mask_loss': 0.9981315150856972, 'metrics/total_secs_per_batch': 6.155478000640869, 'metrics/data_secs_per_batch': 2.7618908643722535, '_timestamp': 1740974300.8426416}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 384 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974300.8429413}).
Epoch: [6][385/500]	Time  6.155 ( 6.155)	Loss 4.2475 (2.3087)	CeLoss 0.1641 (0.2708)	SegCLSLoss 0.0245 (0.0169)	KLLoss 0.3867 (0.3297)	MaskLoss 2.0163 (0.9981)	MaskBCELoss 1.4817 (0.5246)	MaskDICELoss 0.5346 (0.4735)
Epoch: [6][386/500]	Time  5.800 ( 5.800)	Loss 1.5182 (1.3937)	CeLoss 0.2539 (0.3889)	SegCLSLoss 0.0101 (0.0098)	KLLoss 0.3672 (0.2535)	MaskLoss 0.6107 (0.4872)	MaskBCELoss 0.1145 (0.1218)	MaskDICELoss 0.4962 (0.3654)
Epoch: [6][387/500]	Time  5.727 ( 5.727)	Loss 1.1043 (1.4366)	CeLoss 0.2197 (0.4637)	SegCLSLoss 0.0103 (0.0096)	KLLoss 0.3633 (0.2551)	MaskLoss 0.4213 (0.4711)	MaskBCELoss 0.0637 (0.0910)	MaskDICELoss 0.3576 (0.3801)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 385 is less than current step: 499. Dropping entry: {'train/loss': 1.3937128067016602, 'train/ce_loss': 0.3888671875, 'train/seg_cls_loss': 0.009832763671875, 'train/kl_loss': 0.253515625, 'train/mask_bce_loss': 0.12184191253036261, 'train/mask_dice_loss': 0.3653953462839127, 'train/mask_loss': 0.48723726272583007, 'metrics/total_secs_per_batch': 5.800300121307373, 'metrics/data_secs_per_batch': 2.520817017555237, '_timestamp': 1740974306.6430023}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 385 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974306.6433318}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 386 is less than current step: 499. Dropping entry: {'train/loss': 1.4365513801574707, 'train/ce_loss': 0.463671875, 'train/seg_cls_loss': 0.009649658203125, 'train/kl_loss': 0.255078125, 'train/mask_bce_loss': 0.09103735890239477, 'train/mask_dice_loss': 0.3800703465938568, 'train/mask_loss': 0.4711077183485031, 'metrics/total_secs_per_batch': 5.7273619174957275, 'metrics/data_secs_per_batch': 2.629942274093628, '_timestamp': 1740974312.3703027}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 386 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974312.3706527}).
Epoch: [6][388/500]	Time  5.132 ( 5.132)	Loss 1.2188 (1.6050)	CeLoss 1.2188 (0.6539)	SegCLSLoss 0.0000 (0.0087)	KLLoss 0.0000 (0.2148)	MaskLoss 0.0000 (0.4626)	MaskBCELoss 0.0000 (0.0975)	MaskDICELoss 0.0000 (0.3651)
Epoch: [6][389/500]	Time  7.141 ( 7.141)	Loss 2.3828 (1.6764)	CeLoss 0.1465 (0.3538)	SegCLSLoss 0.0259 (0.0133)	KLLoss 0.3750 (0.2525)	MaskLoss 1.0928 (0.6452)	MaskBCELoss 0.3149 (0.1141)	MaskDICELoss 0.7778 (0.5311)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 387 is less than current step: 499. Dropping entry: {'train/loss': 1.6049563884735107, 'train/ce_loss': 0.65390625, 'train/seg_cls_loss': 0.0087158203125, 'train/kl_loss': 0.21484375, 'train/mask_bce_loss': 0.09753583539277315, 'train/mask_dice_loss': 0.36509861052036285, 'train/mask_loss': 0.4626344472169876, 'metrics/total_secs_per_batch': 5.132330417633057, 'metrics/data_secs_per_batch': 2.4522668600082396, '_timestamp': 1740974317.502594}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 387 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974317.5028615}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 388 is less than current step: 499. Dropping entry: {'train/loss': 1.6764468193054198, 'train/ce_loss': 0.3538330078125, 'train/seg_cls_loss': 0.01334228515625, 'train/kl_loss': 0.2525390625, 'train/mask_bce_loss': 0.11407184600830078, 'train/mask_dice_loss': 0.5311217755079269, 'train/mask_loss': 0.64519362449646, 'metrics/total_secs_per_batch': 7.1412742137908936, 'metrics/data_secs_per_batch': 2.993222975730896, '_timestamp': 1740974324.6438985}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 388 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974324.6442344}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 389 is less than current step: 499. Dropping entry: {'train/loss': 1.6730951070785522, 'train/ce_loss': 0.366943359375, 'train/seg_cls_loss': 0.013238525390625, 'train/kl_loss': 0.2537109375, 'train/mask_bce_loss': 0.08334914571605623, 'train/mask_dice_loss': 0.5537111043930054, 'train/mask_loss': 0.6370602428913117, 'metrics/total_secs_per_batch': 6.285199880599976, 'metrics/data_secs_per_batch': 3.0677224159240724, '_timestamp': 1740974330.9289072}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 389 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974330.9291825}).
[2025-03-02 21:58:50,922] [INFO] [logging.py:96:log_dist] [Rank 0] step=3390, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:58:50,928] [INFO] [timer.py:215:stop] epoch=0/micro_step=33900/global_step=3390, RunningAvgSamplesPerSec=1.4450353149226638, CurrSamplesPerSec=1.5914495647033116, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [6][390/500]	Time  6.285 ( 6.285)	Loss 2.4737 (1.6731)	CeLoss 0.2012 (0.3669)	SegCLSLoss 0.0173 (0.0132)	KLLoss 0.3594 (0.2537)	MaskLoss 1.1138 (0.6371)	MaskBCELoss 0.4585 (0.0833)	MaskDICELoss 0.6553 (0.5537)
Epoch: [6][391/500]	Time  6.017 ( 6.017)	Loss 1.5237 (1.5863)	CeLoss 0.2041 (0.5171)	SegCLSLoss 0.0205 (0.0101)	KLLoss 0.3730 (0.2172)	MaskLoss 0.6359 (0.5213)	MaskBCELoss 0.1468 (0.1098)	MaskDICELoss 0.4891 (0.4115)
Epoch: [6][392/500]	Time  6.055 ( 6.055)	Loss 0.8281 (1.7151)	CeLoss 0.8281 (0.4525)	SegCLSLoss 0.0000 (0.0113)	KLLoss 0.0000 (0.2541)	MaskLoss 0.0000 (0.6157)	MaskBCELoss 0.0000 (0.1221)	MaskDICELoss 0.0000 (0.4937)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 390 is less than current step: 499. Dropping entry: {'train/loss': 1.5863181591033935, 'train/ce_loss': 0.51708984375, 'train/seg_cls_loss': 0.01005859375, 'train/kl_loss': 0.2171875, 'train/mask_bce_loss': 0.1098276674747467, 'train/mask_dice_loss': 0.41145641505718233, 'train/mask_loss': 0.5212840914726258, 'metrics/total_secs_per_batch': 6.016573190689087, 'metrics/data_secs_per_batch': 2.7966281890869142, '_timestamp': 1740974336.9457407}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 390 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974336.94611}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 391 is less than current step: 499. Dropping entry: {'train/loss': 1.7150810003280639, 'train/ce_loss': 0.4525390625, 'train/seg_cls_loss': 0.011334228515625, 'train/kl_loss': 0.2541015625, 'train/mask_bce_loss': 0.12205612501129508, 'train/mask_dice_loss': 0.4936874955892563, 'train/mask_loss': 0.615743613243103, 'metrics/total_secs_per_batch': 6.0550925731658936, 'metrics/data_secs_per_batch': 3.1201094150543214, '_timestamp': 1740974343.0007966}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 391 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974343.001083}).
Epoch: [6][393/500]	Time  5.034 ( 5.034)	Loss 1.7500 (1.5675)	CeLoss 0.2852 (0.4868)	SegCLSLoss 0.0098 (0.0126)	KLLoss 0.3613 (0.2525)	MaskLoss 0.7109 (0.5245)	MaskBCELoss 0.2419 (0.1284)	MaskDICELoss 0.4690 (0.3961)
Epoch: [6][394/500]	Time  5.498 ( 5.498)	Loss 0.7422 (1.5590)	CeLoss 0.7422 (0.5696)	SegCLSLoss 0.0000 (0.0100)	KLLoss 0.0000 (0.1820)	MaskLoss 0.0000 (0.4831)	MaskBCELoss 0.0000 (0.0750)	MaskDICELoss 0.0000 (0.4081)
Epoch: [6][395/500]	Time  4.769 ( 4.769)	Loss 2.4725 (1.8054)	CeLoss 0.1846 (0.4955)	SegCLSLoss 0.0193 (0.0147)	KLLoss 0.3711 (0.2236)	MaskLoss 1.1205 (0.6402)	MaskBCELoss 0.2775 (0.1716)	MaskDICELoss 0.8430 (0.4685)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 392 is less than current step: 499. Dropping entry: {'train/loss': 1.5675053715705871, 'train/ce_loss': 0.48681640625, 'train/seg_cls_loss': 0.01260986328125, 'train/kl_loss': 0.2525390625, 'train/mask_bce_loss': 0.12838597632944584, 'train/mask_dice_loss': 0.39613820016384127, 'train/mask_loss': 0.5245241701602936, 'metrics/total_secs_per_batch': 5.033817529678345, 'metrics/data_secs_per_batch': 2.4192510843276978, '_timestamp': 1740974348.0346394}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 392 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974348.0350044}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 393 is less than current step: 499. Dropping entry: {'train/loss': 1.5589509844779967, 'train/ce_loss': 0.56962890625, 'train/seg_cls_loss': 0.00997314453125, 'train/kl_loss': 0.18203125, 'train/mask_bce_loss': 0.07500353958457709, 'train/mask_dice_loss': 0.4081340551376343, 'train/mask_loss': 0.48313760161399844, 'metrics/total_secs_per_batch': 5.497709035873413, 'metrics/data_secs_per_batch': 2.406230115890503, '_timestamp': 1740974353.5323656}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 393 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974353.5326772}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 394 is less than current step: 499. Dropping entry: {'train/loss': 1.805422842502594, 'train/ce_loss': 0.495458984375, 'train/seg_cls_loss': 0.01470947265625, 'train/kl_loss': 0.2236328125, 'train/mask_bce_loss': 0.1716306450776756, 'train/mask_dice_loss': 0.4685319483280182, 'train/mask_loss': 0.640162593126297, 'metrics/total_secs_per_batch': 4.768914699554443, 'metrics/data_secs_per_batch': 2.2787832975387574, '_timestamp': 1740974358.3013506}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 394 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974358.301744}).
Epoch: [6][396/500]	Time  6.518 ( 6.518)	Loss 0.7031 (1.4067)	CeLoss 0.7031 (0.3148)	SegCLSLoss 0.0000 (0.0147)	KLLoss 0.0000 (0.2904)	MaskLoss 0.0000 (0.5277)	MaskBCELoss 0.0000 (0.0762)	MaskDICELoss 0.0000 (0.4515)
Epoch: [6][397/500]	Time  6.527 ( 6.527)	Loss 1.8773 (2.0110)	CeLoss 0.2891 (0.3554)	SegCLSLoss 0.0150 (0.0164)	KLLoss 0.3594 (0.3262)	MaskLoss 0.7726 (0.8073)	MaskBCELoss 0.2092 (0.1565)	MaskDICELoss 0.5634 (0.6509)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 395 is less than current step: 499. Dropping entry: {'train/loss': 1.4066802859306335, 'train/ce_loss': 0.31484375, 'train/seg_cls_loss': 0.01466064453125, 'train/kl_loss': 0.2904296875, 'train/mask_bce_loss': 0.07618172280490398, 'train/mask_dice_loss': 0.451523645222187, 'train/mask_loss': 0.5277053728699684, 'metrics/total_secs_per_batch': 6.518251419067383, 'metrics/data_secs_per_batch': 2.9393004417419433, '_timestamp': 1740974364.8195539}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 395 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974364.819863}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 396 is less than current step: 499. Dropping entry: {'train/loss': 2.010966193675995, 'train/ce_loss': 0.35537109375, 'train/seg_cls_loss': 0.01639404296875, 'train/kl_loss': 0.326171875, 'train/mask_bce_loss': 0.1564766237512231, 'train/mask_dice_loss': 0.6508619397878647, 'train/mask_loss': 0.807338559627533, 'metrics/total_secs_per_batch': 6.526608943939209, 'metrics/data_secs_per_batch': 2.7504099369049073, '_timestamp': 1740974371.346151}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 396 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974371.346437}).
Epoch: [6][398/500]	Time  6.152 ( 6.152)	Loss 2.8549 (1.5943)	CeLoss 0.2158 (0.3389)	SegCLSLoss 0.0172 (0.0116)	KLLoss 0.3594 (0.2504)	MaskLoss 1.2976 (0.6122)	MaskBCELoss 0.6345 (0.1234)	MaskDICELoss 0.6631 (0.4888)
Epoch: [6][399/500]	Time  4.587 ( 4.587)	Loss 2.4929 (1.5944)	CeLoss 0.2695 (0.4904)	SegCLSLoss 0.0186 (0.0118)	KLLoss 0.3652 (0.2189)	MaskLoss 1.0882 (0.5383)	MaskBCELoss 0.3877 (0.1210)	MaskDICELoss 0.7005 (0.4172)
[2025-03-02 21:59:48,579] [INFO] [logging.py:96:log_dist] [Rank 0] step=3400, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 21:59:48,584] [INFO] [timer.py:215:stop] epoch=0/micro_step=34000/global_step=3400, RunningAvgSamplesPerSec=1.44574601381076, CurrSamplesPerSec=1.5389042497059178, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [6][400/500]	Time  6.500 ( 6.500)	Loss 2.2097 (1.7154)	CeLoss 0.2061 (0.3421)	SegCLSLoss 0.0195 (0.0136)	KLLoss 0.3594 (0.2924)	MaskLoss 0.9789 (0.6687)	MaskBCELoss 0.0490 (0.1300)	MaskDICELoss 0.9299 (0.5387)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 397 is less than current step: 499. Dropping entry: {'train/loss': 1.59428973197937, 'train/ce_loss': 0.338916015625, 'train/seg_cls_loss': 0.011566162109375, 'train/kl_loss': 0.250390625, 'train/mask_bce_loss': 0.12338406024500728, 'train/mask_dice_loss': 0.4887754559516907, 'train/mask_loss': 0.6121595144271851, 'metrics/total_secs_per_batch': 6.152453899383545, 'metrics/data_secs_per_batch': 3.0108691930770872, '_timestamp': 1740974377.498634}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 397 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974377.4989965}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 398 is less than current step: 499. Dropping entry: {'train/loss': 1.594430673122406, 'train/ce_loss': 0.490380859375, 'train/seg_cls_loss': 0.011834716796875, 'train/kl_loss': 0.2189453125, 'train/mask_bce_loss': 0.12100583463907241, 'train/mask_dice_loss': 0.4172495394945145, 'train/mask_loss': 0.5382553726434708, 'metrics/total_secs_per_batch': 4.586697578430176, 'metrics/data_secs_per_batch': 2.1351261138916016, '_timestamp': 1740974382.085298}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 398 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974382.08556}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 399 is less than current step: 499. Dropping entry: {'train/loss': 1.7153849720954895, 'train/ce_loss': 0.34208984375, 'train/seg_cls_loss': 0.0135986328125, 'train/kl_loss': 0.2923828125, 'train/mask_bce_loss': 0.12995372149161993, 'train/mask_dice_loss': 0.5387250870466233, 'train/mask_loss': 0.6686788201332092, 'metrics/total_secs_per_batch': 6.499776840209961, 'metrics/data_secs_per_batch': 2.945702910423279, '_timestamp': 1740974388.5848863}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 399 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974388.5851614}).
Epoch: [6][401/500]	Time  6.340 ( 6.340)	Loss 1.4297 (1.6502)	CeLoss 1.4297 (0.6826)	SegCLSLoss 0.0000 (0.0099)	KLLoss 0.0000 (0.2176)	MaskLoss 0.0000 (0.4703)	MaskBCELoss 0.0000 (0.0541)	MaskDICELoss 0.0000 (0.4162)
Epoch: [6][402/500]	Time  5.119 ( 5.119)	Loss 0.2051 (1.5644)	CeLoss 0.2051 (0.6823)	SegCLSLoss 0.0000 (0.0089)	KLLoss 0.0000 (0.1822)	MaskLoss 0.0000 (0.4297)	MaskBCELoss 0.0000 (0.0490)	MaskDICELoss 0.0000 (0.3807)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 400 is less than current step: 499. Dropping entry: {'train/loss': 1.6502142071723938, 'train/ce_loss': 0.6826171875, 'train/seg_cls_loss': 0.009912109375, 'train/kl_loss': 0.217578125, 'train/mask_bce_loss': 0.05413014655932784, 'train/mask_dice_loss': 0.4161917924880981, 'train/mask_loss': 0.4703219383955002, 'metrics/total_secs_per_batch': 6.339895009994507, 'metrics/data_secs_per_batch': 3.1503143310546875, '_timestamp': 1740974394.9250026}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 400 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974394.925303}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 401 is less than current step: 499. Dropping entry: {'train/loss': 1.5643958926200867, 'train/ce_loss': 0.68232421875, 'train/seg_cls_loss': 0.0088623046875, 'train/kl_loss': 0.1822265625, 'train/mask_bce_loss': 0.049037301121279596, 'train/mask_dice_loss': 0.38067041635513305, 'train/mask_loss': 0.4297077119350433, 'metrics/total_secs_per_batch': 5.118881464004517, 'metrics/data_secs_per_batch': 2.193969464302063, '_timestamp': 1740974400.043828}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 401 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974400.0441172}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 402 is less than current step: 499. Dropping entry: {'train/loss': 1.4412764191627503, 'train/ce_loss': 0.631640625, 'train/seg_cls_loss': 0.009332275390625, 'train/kl_loss': 0.2181640625, 'train/mask_bce_loss': 0.062434670887887476, 'train/mask_dice_loss': 0.3292484611272812, 'train/mask_loss': 0.3916831314563751, 'metrics/total_secs_per_batch': 4.999638319015503, 'metrics/data_secs_per_batch': 2.4965239763259888, '_timestamp': 1740974405.0434947}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 402 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974405.0437784}).
Epoch: [6][403/500]	Time  5.000 ( 5.000)	Loss 0.7891 (1.4413)	CeLoss 0.7891 (0.6316)	SegCLSLoss 0.0000 (0.0093)	KLLoss 0.0000 (0.2182)	MaskLoss 0.0000 (0.3917)	MaskBCELoss 0.0000 (0.0624)	MaskDICELoss 0.0000 (0.3292)
Epoch: [6][404/500]	Time  5.786 ( 5.786)	Loss 1.1135 (1.5549)	CeLoss 0.3125 (0.4959)	SegCLSLoss 0.0160 (0.0107)	KLLoss 0.3652 (0.2527)	MaskLoss 0.3780 (0.5142)	MaskBCELoss 0.0896 (0.0943)	MaskDICELoss 0.2884 (0.4199)
Epoch: [6][405/500]	Time  6.117 ( 6.117)	Loss 1.0234 (1.4695)	CeLoss 1.0234 (0.2391)	SegCLSLoss 0.0000 (0.0124)	KLLoss 0.0000 (0.2182)	MaskLoss 0.0000 (0.6012)	MaskBCELoss 0.0000 (0.1759)	MaskDICELoss 0.0000 (0.4253)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 403 is less than current step: 499. Dropping entry: {'train/loss': 1.5549277901649474, 'train/ce_loss': 0.4958984375, 'train/seg_cls_loss': 0.010748291015625, 'train/kl_loss': 0.252734375, 'train/mask_bce_loss': 0.09429095908999444, 'train/mask_dice_loss': 0.4199405312538147, 'train/mask_loss': 0.5142314821481705, 'metrics/total_secs_per_batch': 5.786040544509888, 'metrics/data_secs_per_batch': 2.5309489488601686, '_timestamp': 1740974410.8296695}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 403 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974410.8300238}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 404 is less than current step: 499. Dropping entry: {'train/loss': 1.4695496380329132, 'train/ce_loss': 0.2390625, 'train/seg_cls_loss': 0.012445068359375, 'train/kl_loss': 0.2181640625, 'train/mask_bce_loss': 0.1759421367198229, 'train/mask_dice_loss': 0.4252877414226532, 'train/mask_loss': 0.6012298852205277, 'metrics/total_secs_per_batch': 6.11708664894104, 'metrics/data_secs_per_batch': 2.8204336166381836, '_timestamp': 1740974416.9466155}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 404 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974416.946894}).
Epoch: [6][406/500]	Time  4.856 ( 4.856)	Loss 1.0938 (1.3660)	CeLoss 1.0938 (0.6198)	SegCLSLoss 0.0000 (0.0077)	KLLoss 0.0000 (0.1816)	MaskLoss 0.0000 (0.3621)	MaskBCELoss 0.0000 (0.0363)	MaskDICELoss 0.0000 (0.3258)
Epoch: [6][407/500]	Time  6.646 ( 6.646)	Loss 1.2273 (1.8275)	CeLoss 0.2285 (0.2770)	SegCLSLoss 0.0259 (0.0156)	KLLoss 0.3672 (0.2896)	MaskLoss 0.4750 (0.7569)	MaskBCELoss 0.0670 (0.1489)	MaskDICELoss 0.4080 (0.6080)
Epoch: [6][408/500]	Time  5.955 ( 5.955)	Loss 2.7886 (1.4432)	CeLoss 0.2305 (0.3980)	SegCLSLoss 0.0144 (0.0109)	KLLoss 0.3633 (0.2527)	MaskLoss 1.2576 (0.5074)	MaskBCELoss 0.4356 (0.1316)	MaskDICELoss 0.8220 (0.3757)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 405 is less than current step: 499. Dropping entry: {'train/loss': 1.3659748435020447, 'train/ce_loss': 0.61982421875, 'train/seg_cls_loss': 0.007684326171875, 'train/kl_loss': 0.181640625, 'train/mask_bce_loss': 0.03630843944847584, 'train/mask_dice_loss': 0.3257805347442627, 'train/mask_loss': 0.3620889723300934, 'metrics/total_secs_per_batch': 4.855530023574829, 'metrics/data_secs_per_batch': 2.535969924926758, '_timestamp': 1740974421.8021095}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 405 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974421.802379}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 406 is less than current step: 499. Dropping entry: {'train/loss': 1.8275251030921935, 'train/ce_loss': 0.277001953125, 'train/seg_cls_loss': 0.015625, 'train/kl_loss': 0.2896484375, 'train/mask_bce_loss': 0.14888182748109102, 'train/mask_dice_loss': 0.6080203533172608, 'train/mask_loss': 0.7569021701812744, 'metrics/total_secs_per_batch': 6.645802021026611, 'metrics/data_secs_per_batch': 2.9367101192474365, '_timestamp': 1740974428.447926}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 406 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974428.4482007}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 407 is less than current step: 499. Dropping entry: {'train/loss': 1.4431784689426421, 'train/ce_loss': 0.3979736328125, 'train/seg_cls_loss': 0.01087646484375, 'train/kl_loss': 0.252734375, 'train/mask_bce_loss': 0.13163028862327336, 'train/mask_dice_loss': 0.3757377594709396, 'train/mask_loss': 0.5073680490255356, 'metrics/total_secs_per_batch': 5.954624891281128, 'metrics/data_secs_per_batch': 2.4842060565948487, '_timestamp': 1740974434.4025733}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 407 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974434.4028547}).
Epoch: [6][409/500]	Time  6.343 ( 6.343)	Loss 0.9570 (1.7717)	CeLoss 0.9570 (0.4552)	SegCLSLoss 0.0000 (0.0125)	KLLoss 0.0000 (0.2893)	MaskLoss 0.0000 (0.6407)	MaskBCELoss 0.0000 (0.1627)	MaskDICELoss 0.0000 (0.4781)
[2025-03-02 22:00:46,986] [INFO] [logging.py:96:log_dist] [Rank 0] step=3410, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:00:46,991] [INFO] [timer.py:215:stop] epoch=0/micro_step=34100/global_step=3410, RunningAvgSamplesPerSec=1.446407056371575, CurrSamplesPerSec=1.6011088000929599, MemAllocated=31.48GB, MaxMemAllocated=37.23GB
Epoch: [6][410/500]	Time  6.247 ( 6.247)	Loss 1.9915 (1.4614)	CeLoss 0.1787 (0.3117)	SegCLSLoss 0.0253 (0.0146)	KLLoss 0.3594 (0.2883)	MaskLoss 0.8820 (0.5569)	MaskBCELoss 0.0381 (0.0494)	MaskDICELoss 0.8438 (0.5074)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 408 is less than current step: 499. Dropping entry: {'train/loss': 1.7716850519180298, 'train/ce_loss': 0.45517578125, 'train/seg_cls_loss': 0.0125, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.1626650333404541, 'train/mask_dice_loss': 0.47806029617786405, 'train/mask_loss': 0.6407253295183182, 'metrics/total_secs_per_batch': 6.342807292938232, 'metrics/data_secs_per_batch': 3.2251901388168336, '_timestamp': 1740974440.7453735}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 408 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974440.745647}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 409 is less than current step: 499. Dropping entry: {'train/loss': 1.4613899528980254, 'train/ce_loss': 0.31171875, 'train/seg_cls_loss': 0.01455078125, 'train/kl_loss': 0.28828125, 'train/mask_bce_loss': 0.049433895666152236, 'train/mask_dice_loss': 0.5074329435825348, 'train/mask_loss': 0.5568668380379677, 'metrics/total_secs_per_batch': 6.247226238250732, 'metrics/data_secs_per_batch': 2.989760661125183, '_timestamp': 1740974446.9923909}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 409 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974446.9926672}).
Epoch: [6][411/500]	Time  6.212 ( 6.212)	Loss 1.8760 (1.4145)	CeLoss 0.2012 (0.4339)	SegCLSLoss 0.0198 (0.0121)	KLLoss 0.3633 (0.2518)	MaskLoss 0.8145 (0.4748)	MaskBCELoss 0.0564 (0.0800)	MaskDICELoss 0.7581 (0.3947)
Epoch: [6][412/500]	Time  6.648 ( 6.648)	Loss 2.0759 (1.5151)	CeLoss 0.2324 (0.3497)	SegCLSLoss 0.0178 (0.0134)	KLLoss 0.3574 (0.2904)	MaskLoss 0.8993 (0.5648)	MaskBCELoss 0.0627 (0.0661)	MaskDICELoss 0.8366 (0.4987)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 410 is less than current step: 499. Dropping entry: {'train/loss': 1.4145346760749817, 'train/ce_loss': 0.43388671875, 'train/seg_cls_loss': 0.012139892578125, 'train/kl_loss': 0.2517578125, 'train/mask_bce_loss': 0.08004833608865738, 'train/mask_dice_loss': 0.3947483032941818, 'train/mask_loss': 0.47479662895202634, 'metrics/total_secs_per_batch': 6.212039947509766, 'metrics/data_secs_per_batch': 2.945267581939697, '_timestamp': 1740974453.2046795}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 410 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974453.204945}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 411 is less than current step: 499. Dropping entry: {'train/loss': 1.515099322795868, 'train/ce_loss': 0.34970703125, 'train/seg_cls_loss': 0.013372802734375, 'train/kl_loss': 0.2904296875, 'train/mask_bce_loss': 0.06610726825892925, 'train/mask_dice_loss': 0.49871778935194017, 'train/mask_loss': 0.5648250505328178, 'metrics/total_secs_per_batch': 6.647743225097656, 'metrics/data_secs_per_batch': 3.189089560508728, '_timestamp': 1740974459.8523872}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 411 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974459.8526664}).
Epoch: [6][413/500]	Time  6.379 ( 6.379)	Loss 1.6762 (1.9474)	CeLoss 0.1875 (0.3492)	SegCLSLoss 0.0211 (0.0163)	KLLoss 0.3496 (0.2918)	MaskLoss 0.7214 (0.7804)	MaskBCELoss 0.0562 (0.2276)	MaskDICELoss 0.6652 (0.5528)
Epoch: [6][414/500]	Time  5.939 ( 5.939)	Loss 1.8429 (1.7987)	CeLoss 0.1855 (0.4156)	SegCLSLoss 0.0267 (0.0165)	KLLoss 0.3633 (0.2900)	MaskLoss 0.8038 (0.6729)	MaskBCELoss 0.0249 (0.1427)	MaskDICELoss 0.7788 (0.5302)
Epoch: [6][415/500]	Time  5.795 ( 5.795)	Loss 1.6046 (1.0269)	CeLoss 0.2412 (0.3941)	SegCLSLoss 0.0123 (0.0060)	KLLoss 0.3633 (0.1820)	MaskLoss 0.6607 (0.3057)	MaskBCELoss 0.1269 (0.0856)	MaskDICELoss 0.5338 (0.2201)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 412 is less than current step: 499. Dropping entry: {'train/loss': 1.9473857522010802, 'train/ce_loss': 0.34921875, 'train/seg_cls_loss': 0.0162841796875, 'train/kl_loss': 0.291796875, 'train/mask_bce_loss': 0.2276273902505636, 'train/mask_dice_loss': 0.5528037637472153, 'train/mask_loss': 0.7804311573505401, 'metrics/total_secs_per_batch': 6.379125118255615, 'metrics/data_secs_per_batch': 2.6295560121536257, '_timestamp': 1740974466.231533}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 412 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974466.2318537}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 413 is less than current step: 499. Dropping entry: {'train/loss': 1.7986665844917298, 'train/ce_loss': 0.415625, 'train/seg_cls_loss': 0.01649169921875, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.14273543059825897, 'train/mask_dice_loss': 0.5301818370819091, 'train/mask_loss': 0.6729172766208649, 'metrics/total_secs_per_batch': 5.939335584640503, 'metrics/data_secs_per_batch': 2.734895706176758, '_timestamp': 1740974472.1708922}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 413 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974472.171184}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 414 is less than current step: 499. Dropping entry: {'train/loss': 1.0268572926521302, 'train/ce_loss': 0.394140625, 'train/seg_cls_loss': 0.00599365234375, 'train/kl_loss': 0.18203125, 'train/mask_bce_loss': 0.08556269742548465, 'train/mask_dice_loss': 0.2201022744178772, 'train/mask_loss': 0.3056649684906006, 'metrics/total_secs_per_batch': 5.794568061828613, 'metrics/data_secs_per_batch': 2.4796416759490967, '_timestamp': 1740974477.9655645}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 414 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974477.9659567}).
Epoch: [6][416/500]	Time  6.245 ( 6.245)	Loss 0.1787 (1.6814)	CeLoss 0.1787 (0.4416)	SegCLSLoss 0.0000 (0.0123)	KLLoss 0.0000 (0.2195)	MaskLoss 0.0000 (0.6059)	MaskBCELoss 0.0000 (0.2005)	MaskDICELoss 0.0000 (0.4054)
Epoch: [6][417/500]	Time  7.053 ( 7.053)	Loss 2.6038 (1.7906)	CeLoss 0.2383 (0.3079)	SegCLSLoss 0.0162 (0.0168)	KLLoss 0.3711 (0.3277)	MaskLoss 1.1603 (0.7207)	MaskBCELoss 0.3967 (0.1193)	MaskDICELoss 0.7636 (0.6014)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 415 is less than current step: 499. Dropping entry: {'train/loss': 1.681368738412857, 'train/ce_loss': 0.4416015625, 'train/seg_cls_loss': 0.01231689453125, 'train/kl_loss': 0.21953125, 'train/mask_bce_loss': 0.20052345767617225, 'train/mask_dice_loss': 0.40539527833461764, 'train/mask_loss': 0.6059187442064286, 'metrics/total_secs_per_batch': 6.244566202163696, 'metrics/data_secs_per_batch': 2.7359094619750977, '_timestamp': 1740974484.2100992}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 415 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974484.2104225}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 416 is less than current step: 499. Dropping entry: {'train/loss': 1.7906092882156373, 'train/ce_loss': 0.30791015625, 'train/seg_cls_loss': 0.016796875, 'train/kl_loss': 0.327734375, 'train/mask_bce_loss': 0.11929643172770739, 'train/mask_dice_loss': 0.6013988375663757, 'train/mask_loss': 0.7206952661275864, 'metrics/total_secs_per_batch': 7.052872657775879, 'metrics/data_secs_per_batch': 3.27559072971344, '_timestamp': 1740974491.2628818}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 416 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974491.2631662}).
Epoch: [6][418/500]	Time  7.120 ( 7.120)	Loss 1.5867 (1.7400)	CeLoss 0.2539 (0.2633)	SegCLSLoss 0.0122 (0.0146)	KLLoss 0.3633 (0.3275)	MaskLoss 0.6449 (0.7182)	MaskBCELoss 0.1183 (0.1285)	MaskDICELoss 0.5267 (0.5897)
Epoch: [6][419/500]	Time  6.659 ( 6.659)	Loss 2.4180 (1.9510)	CeLoss 0.1641 (0.2232)	SegCLSLoss 0.0194 (0.0167)	KLLoss 0.3809 (0.3672)	MaskLoss 1.1030 (0.8411)	MaskBCELoss 0.2970 (0.1933)	MaskDICELoss 0.8060 (0.6478)
[2025-03-02 22:01:49,881] [INFO] [logging.py:96:log_dist] [Rank 0] step=3420, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:01:49,886] [INFO] [timer.py:215:stop] epoch=0/micro_step=34200/global_step=3420, RunningAvgSamplesPerSec=1.4467900168297247, CurrSamplesPerSec=2.0647839309031792, MemAllocated=30.93GB, MaxMemAllocated=37.23GB
Epoch: [6][420/500]	Time  4.845 ( 4.845)	Loss 1.9282 (1.7273)	CeLoss 0.4023 (0.6310)	SegCLSLoss 0.0093 (0.0101)	KLLoss 0.3613 (0.2152)	MaskLoss 0.7434 (0.5352)	MaskBCELoss 0.3424 (0.1056)	MaskDICELoss 0.4010 (0.4295)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 417 is less than current step: 499. Dropping entry: {'train/loss': 1.7400485277175903, 'train/ce_loss': 0.26328125, 'train/seg_cls_loss': 0.014630126953125, 'train/kl_loss': 0.3275390625, 'train/mask_bce_loss': 0.12851475272327662, 'train/mask_dice_loss': 0.5896540224552155, 'train/mask_loss': 0.7181687772274017, 'metrics/total_secs_per_batch': 7.119832754135132, 'metrics/data_secs_per_batch': 3.060852360725403, '_timestamp': 1740974498.3827064}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 417 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974498.3829782}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 418 is less than current step: 499. Dropping entry: {'train/loss': 1.951018476486206, 'train/ce_loss': 0.2232421875, 'train/seg_cls_loss': 0.01666259765625, 'train/kl_loss': 0.3671875, 'train/mask_bce_loss': 0.19332247581332923, 'train/mask_dice_loss': 0.6477629363536834, 'train/mask_loss': 0.8410854041576385, 'metrics/total_secs_per_batch': 6.65941596031189, 'metrics/data_secs_per_batch': 3.1211594581604003, '_timestamp': 1740974505.0423648}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 418 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974505.0427318}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 419 is less than current step: 499. Dropping entry: {'train/loss': 1.7273491501808167, 'train/ce_loss': 0.63095703125, 'train/seg_cls_loss': 0.01005859375, 'train/kl_loss': 0.215234375, 'train/mask_bce_loss': 0.10564893186092376, 'train/mask_dice_loss': 0.4295100152492523, 'train/mask_loss': 0.5351589500904084, 'metrics/total_secs_per_batch': 4.8450236320495605, 'metrics/data_secs_per_batch': 1.9920865297317505, '_timestamp': 1740974509.8869607}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 419 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974509.887229}).
Epoch: [6][421/500]	Time  6.201 ( 6.201)	Loss 0.6154 (1.5886)	CeLoss 0.2520 (0.3231)	SegCLSLoss 0.0143 (0.0143)	KLLoss 0.3711 (0.3264)	MaskLoss 0.1592 (0.6130)	MaskBCELoss 0.0814 (0.1342)	MaskDICELoss 0.0778 (0.4787)
Epoch: [6][422/500]	Time  7.267 ( 7.267)	Loss 1.4297 (1.8884)	CeLoss 1.4297 (0.3303)	SegCLSLoss 0.0000 (0.0165)	KLLoss 0.0000 (0.3275)	MaskLoss 0.0000 (0.7585)	MaskBCELoss 0.0000 (0.1305)	MaskDICELoss 0.0000 (0.6280)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 420 is less than current step: 499. Dropping entry: {'train/loss': 1.5886215269565582, 'train/ce_loss': 0.32314453125, 'train/seg_cls_loss': 0.014337158203125, 'train/kl_loss': 0.3263671875, 'train/mask_bce_loss': 0.13421687930822374, 'train/mask_dice_loss': 0.47874620333313944, 'train/mask_loss': 0.6129630774259567, 'metrics/total_secs_per_batch': 6.200944423675537, 'metrics/data_secs_per_batch': 2.6151551246643066, '_timestamp': 1740974516.0881202}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 420 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974516.088309}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 421 is less than current step: 499. Dropping entry: {'train/loss': 1.8883673071861267, 'train/ce_loss': 0.3302734375, 'train/seg_cls_loss': 0.016485595703125, 'train/kl_loss': 0.3275390625, 'train/mask_bce_loss': 0.1304937506094575, 'train/mask_dice_loss': 0.6279965519905091, 'train/mask_loss': 0.7584902942180634, 'metrics/total_secs_per_batch': 7.266948461532593, 'metrics/data_secs_per_batch': 3.3550484418869018, '_timestamp': 1740974523.355071}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 421 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974523.3553407}).
Epoch: [6][423/500]	Time  6.487 ( 6.487)	Loss 2.0274 (1.7874)	CeLoss 0.1992 (0.2248)	SegCLSLoss 0.0189 (0.0192)	KLLoss 0.3555 (0.3625)	MaskLoss 0.8917 (0.7583)	MaskBCELoss 0.0318 (0.1196)	MaskDICELoss 0.8599 (0.6387)
Epoch: [6][424/500]	Time  6.445 ( 6.445)	Loss 1.6376 (1.8374)	CeLoss 0.2637 (0.3240)	SegCLSLoss 0.0146 (0.0161)	KLLoss 0.3555 (0.2943)	MaskLoss 0.6655 (0.7379)	MaskBCELoss 0.0428 (0.1489)	MaskDICELoss 0.6227 (0.5890)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 422 is less than current step: 499. Dropping entry: {'train/loss': 1.7873947143554687, 'train/ce_loss': 0.2248046875, 'train/seg_cls_loss': 0.019195556640625, 'train/kl_loss': 0.3625, 'train/mask_bce_loss': 0.11957111414521933, 'train/mask_dice_loss': 0.6387258648872376, 'train/mask_loss': 0.7582969903945923, 'metrics/total_secs_per_batch': 6.487431287765503, 'metrics/data_secs_per_batch': 2.9985711097717287, '_timestamp': 1740974529.8424928}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 422 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974529.8427734}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 423 is less than current step: 499. Dropping entry: {'train/loss': 1.8374191403388977, 'train/ce_loss': 0.3240234375, 'train/seg_cls_loss': 0.01605224609375, 'train/kl_loss': 0.2943359375, 'train/mask_bce_loss': 0.14885637238621713, 'train/mask_dice_loss': 0.5890426635742188, 'train/mask_loss': 0.7378990292549134, 'metrics/total_secs_per_batch': 6.4446399211883545, 'metrics/data_secs_per_batch': 2.8402672290802, '_timestamp': 1740974536.2871623}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 423 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974536.2875123}).
Epoch: [6][425/500]	Time  5.625 ( 5.625)	Loss 1.5970 (1.6268)	CeLoss 0.2578 (0.4418)	SegCLSLoss 0.0127 (0.0139)	KLLoss 0.3633 (0.2539)	MaskLoss 0.6481 (0.5763)	MaskBCELoss 0.1004 (0.1433)	MaskDICELoss 0.5477 (0.4330)
Epoch: [6][426/500]	Time  6.048 ( 6.048)	Loss 1.3066 (2.0162)	CeLoss 0.2275 (0.3528)	SegCLSLoss 0.0164 (0.0168)	KLLoss 0.3594 (0.3271)	MaskLoss 0.5175 (0.8114)	MaskBCELoss 0.0204 (0.2323)	MaskDICELoss 0.4971 (0.5790)
Epoch: [6][427/500]	Time  6.565 ( 6.565)	Loss 2.1984 (1.6700)	CeLoss 0.2520 (0.2149)	SegCLSLoss 0.0238 (0.0168)	KLLoss 0.3398 (0.3254)	MaskLoss 0.9507 (0.7071)	MaskBCELoss 0.0638 (0.0706)	MaskDICELoss 0.8869 (0.6364)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 424 is less than current step: 499. Dropping entry: {'train/loss': 1.626809400320053, 'train/ce_loss': 0.441796875, 'train/seg_cls_loss': 0.01387939453125, 'train/kl_loss': 0.25390625, 'train/mask_bce_loss': 0.14333304334431887, 'train/mask_dice_loss': 0.4329622954130173, 'train/mask_loss': 0.5762953251600266, 'metrics/total_secs_per_batch': 5.625224590301514, 'metrics/data_secs_per_batch': 2.0278332471847533, '_timestamp': 1740974541.9123635}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 424 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974541.9126458}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 425 is less than current step: 499. Dropping entry: {'train/loss': 2.0162035465240478, 'train/ce_loss': 0.35283203125, 'train/seg_cls_loss': 0.016802978515625, 'train/kl_loss': 0.3271484375, 'train/mask_bce_loss': 0.23232373632490635, 'train/mask_dice_loss': 0.5790495127439499, 'train/mask_loss': 0.8113732516765595, 'metrics/total_secs_per_batch': 6.048414707183838, 'metrics/data_secs_per_batch': 2.7424681901931764, '_timestamp': 1740974547.960955}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 425 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974547.9613142}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 426 is less than current step: 499. Dropping entry: {'train/loss': 1.6699881970882415, 'train/ce_loss': 0.21494140625, 'train/seg_cls_loss': 0.016802978515625, 'train/kl_loss': 0.325390625, 'train/mask_bce_loss': 0.07064728373661637, 'train/mask_dice_loss': 0.6364171087741852, 'train/mask_loss': 0.7070643931627274, 'metrics/total_secs_per_batch': 6.5650763511657715, 'metrics/data_secs_per_batch': 2.7760812997817994, '_timestamp': 1740974554.5259173}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 426 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974554.5262089}).
Epoch: [6][428/500]	Time  5.913 ( 5.913)	Loss 1.9893 (1.5812)	CeLoss 0.2637 (0.5278)	SegCLSLoss 0.0115 (0.0097)	KLLoss 0.3652 (0.2152)	MaskLoss 0.8413 (0.5136)	MaskBCELoss 0.1151 (0.0957)	MaskDICELoss 0.7263 (0.4179)
Epoch: [6][429/500]	Time  7.094 ( 7.094)	Loss 2.1496 (1.7565)	CeLoss 0.2090 (0.2436)	SegCLSLoss 0.0289 (0.0160)	KLLoss 0.3535 (0.3260)	MaskLoss 0.9449 (0.7361)	MaskBCELoss 0.1124 (0.1700)	MaskDICELoss 0.8326 (0.5660)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 427 is less than current step: 499. Dropping entry: {'train/loss': 1.581215512752533, 'train/ce_loss': 0.52783203125, 'train/seg_cls_loss': 0.009710693359375, 'train/kl_loss': 0.215234375, 'train/mask_bce_loss': 0.09567856397479772, 'train/mask_dice_loss': 0.41787841618061067, 'train/mask_loss': 0.5135569870471954, 'metrics/total_secs_per_batch': 5.9129016399383545, 'metrics/data_secs_per_batch': 2.294274187088013, '_timestamp': 1740974560.4387848}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 427 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974560.4390597}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 428 is less than current step: 499. Dropping entry: {'train/loss': 1.7565334677696227, 'train/ce_loss': 0.243603515625, 'train/seg_cls_loss': 0.01597900390625, 'train/kl_loss': 0.3259765625, 'train/mask_bce_loss': 0.17003209050744772, 'train/mask_dice_loss': 0.566022726893425, 'train/mask_loss': 0.7360548138618469, 'metrics/total_secs_per_batch': 7.093740701675415, 'metrics/data_secs_per_batch': 2.963206648826599, '_timestamp': 1740974567.5324912}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 428 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974567.5328283}).
[2025-03-02 22:02:53,247] [INFO] [logging.py:96:log_dist] [Rank 0] step=3430, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:02:53,254] [INFO] [timer.py:215:stop] epoch=0/micro_step=34300/global_step=3430, RunningAvgSamplesPerSec=1.4471420172120615, CurrSamplesPerSec=1.7480566886120652, MemAllocated=30.95GB, MaxMemAllocated=37.23GB
Epoch: [6][430/500]	Time  5.722 ( 5.722)	Loss 1.0642 (1.1328)	CeLoss 0.2520 (0.2212)	SegCLSLoss 0.0110 (0.0094)	KLLoss 0.3574 (0.2904)	MaskLoss 0.3856 (0.4389)	MaskBCELoss 0.0313 (0.1084)	MaskDICELoss 0.3543 (0.3305)
Epoch: [6][431/500]	Time  5.320 ( 5.320)	Loss 2.5181 (1.6641)	CeLoss 0.1484 (0.5590)	SegCLSLoss 0.0234 (0.0110)	KLLoss 0.3789 (0.2535)	MaskLoss 1.1599 (0.5372)	MaskBCELoss 0.2937 (0.0951)	MaskDICELoss 0.8662 (0.4421)
Epoch: [6][432/500]	Time  5.917 ( 5.917)	Loss 1.8482 (1.6967)	CeLoss 0.2812 (0.4147)	SegCLSLoss 0.0132 (0.0148)	KLLoss 0.3633 (0.2893)	MaskLoss 0.7620 (0.6228)	MaskBCELoss 0.1378 (0.0987)	MaskDICELoss 0.6242 (0.5241)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 429 is less than current step: 499. Dropping entry: {'train/loss': 1.1327878743410111, 'train/ce_loss': 0.22119140625, 'train/seg_cls_loss': 0.00943603515625, 'train/kl_loss': 0.2904296875, 'train/mask_bce_loss': 0.10838184393942356, 'train/mask_dice_loss': 0.330473037622869, 'train/mask_loss': 0.43885487616062163, 'metrics/total_secs_per_batch': 5.722232341766357, 'metrics/data_secs_per_batch': 2.789094018936157, '_timestamp': 1740974573.2545319}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 429 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974573.2547877}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 430 is less than current step: 499. Dropping entry: {'train/loss': 1.6641058206558228, 'train/ce_loss': 0.558984375, 'train/seg_cls_loss': 0.011004638671875, 'train/kl_loss': 0.253515625, 'train/mask_bce_loss': 0.09505247678607702, 'train/mask_dice_loss': 0.44212739169597626, 'train/mask_loss': 0.5371798634529114, 'metrics/total_secs_per_batch': 5.319831609725952, 'metrics/data_secs_per_batch': 2.1429239988327025, '_timestamp': 1740974578.574567}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 430 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974578.5748417}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 431 is less than current step: 499. Dropping entry: {'train/loss': 1.6967341899871826, 'train/ce_loss': 0.41474609375, 'train/seg_cls_loss': 0.014752197265625, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.09870315417647361, 'train/mask_dice_loss': 0.5240779876708984, 'train/mask_loss': 0.6227811366319657, 'metrics/total_secs_per_batch': 5.9169604778289795, 'metrics/data_secs_per_batch': 2.5475744247436523, '_timestamp': 1740974584.4915578}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 431 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974584.4918294}).
Epoch: [6][433/500]	Time  7.148 ( 7.148)	Loss 1.3559 (1.3173)	CeLoss 0.3008 (0.2704)	SegCLSLoss 0.0104 (0.0121)	KLLoss 0.3652 (0.2930)	MaskLoss 0.5061 (0.5057)	MaskBCELoss 0.0663 (0.0937)	MaskDICELoss 0.4398 (0.4121)
Epoch: [6][434/500]	Time  6.279 ( 6.279)	Loss 3.1496 (2.1883)	CeLoss 0.1699 (0.2179)	SegCLSLoss 0.0222 (0.0142)	KLLoss 0.3574 (0.3643)	MaskLoss 1.4664 (0.9635)	MaskBCELoss 0.4697 (0.2390)	MaskDICELoss 0.9967 (0.7245)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 432 is less than current step: 499. Dropping entry: {'train/loss': 1.3172590374946593, 'train/ce_loss': 0.27041015625, 'train/seg_cls_loss': 0.012060546875, 'train/kl_loss': 0.29296875, 'train/mask_bce_loss': 0.09366047568619251, 'train/mask_dice_loss': 0.4120881885290146, 'train/mask_loss': 0.5057486653327942, 'metrics/total_secs_per_batch': 7.148312568664551, 'metrics/data_secs_per_batch': 3.4510245323181152, '_timestamp': 1740974591.6399002}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 432 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974591.6401944}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 433 is less than current step: 499. Dropping entry: {'train/loss': 2.188338339328766, 'train/ce_loss': 0.21787109375, 'train/seg_cls_loss': 0.01417236328125, 'train/kl_loss': 0.3642578125, 'train/mask_bce_loss': 0.23898079469799996, 'train/mask_dice_loss': 0.724475485086441, 'train/mask_loss': 0.9634562849998474, 'metrics/total_secs_per_batch': 6.278878688812256, 'metrics/data_secs_per_batch': 2.8162926912307737, '_timestamp': 1740974597.9187188}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 433 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974597.9189897}).
Epoch: [6][435/500]	Time  5.854 ( 5.854)	Loss 1.8184 (1.7323)	CeLoss 0.2148 (0.5619)	SegCLSLoss 0.0209 (0.0128)	KLLoss 0.3535 (0.2533)	MaskLoss 0.7783 (0.5691)	MaskBCELoss 0.0249 (0.0887)	MaskDICELoss 0.7535 (0.4804)
Epoch: [6][436/500]	Time  6.150 ( 6.150)	Loss 0.9844 (1.5555)	CeLoss 0.9844 (0.3785)	SegCLSLoss 0.0000 (0.0113)	KLLoss 0.0000 (0.2879)	MaskLoss 0.0000 (0.5713)	MaskBCELoss 0.0000 (0.1214)	MaskDICELoss 0.0000 (0.4500)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 434 is less than current step: 499. Dropping entry: {'train/loss': 1.7323285341262817, 'train/ce_loss': 0.5619140625, 'train/seg_cls_loss': 0.012841796875, 'train/kl_loss': 0.2533203125, 'train/mask_bce_loss': 0.0886934109032154, 'train/mask_dice_loss': 0.480400538444519, 'train/mask_loss': 0.5690939515829087, 'metrics/total_secs_per_batch': 5.854122161865234, 'metrics/data_secs_per_batch': 2.4046076059341432, '_timestamp': 1740974603.772987}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 434 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974603.7733111}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 435 is less than current step: 499. Dropping entry: {'train/loss': 1.5555387288331985, 'train/ce_loss': 0.378515625, 'train/seg_cls_loss': 0.011322021484375, 'train/kl_loss': 0.287890625, 'train/mask_bce_loss': 0.12135737799108029, 'train/mask_dice_loss': 0.44996667504310606, 'train/mask_loss': 0.5713240519165993, 'metrics/total_secs_per_batch': 6.150209188461304, 'metrics/data_secs_per_batch': 2.9329762935638426, '_timestamp': 1740974609.92321}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 435 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974609.923536}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 436 is less than current step: 499. Dropping entry: {'train/loss': 1.7317676901817323, 'train/ce_loss': 0.38486328125, 'train/seg_cls_loss': 0.01275634765625, 'train/kl_loss': 0.29140625, 'train/mask_bce_loss': 0.09562571123242378, 'train/mask_dice_loss': 0.5600530639290809, 'train/mask_loss': 0.6556787729263306, 'metrics/total_secs_per_batch': 6.218281269073486, 'metrics/data_secs_per_batch': 2.6636919021606444, '_timestamp': 1740974616.1413589}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 436 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974616.1416936}).
Epoch: [6][437/500]	Time  6.218 ( 6.218)	Loss 1.3818 (1.7318)	CeLoss 0.2930 (0.3849)	SegCLSLoss 0.0107 (0.0128)	KLLoss 0.3633 (0.2914)	MaskLoss 0.5229 (0.6557)	MaskBCELoss 0.1489 (0.0956)	MaskDICELoss 0.3740 (0.5601)
Epoch: [6][438/500]	Time  5.548 ( 5.548)	Loss 2.1732 (1.3394)	CeLoss 0.2617 (0.3806)	SegCLSLoss 0.0124 (0.0102)	KLLoss 0.3555 (0.2508)	MaskLoss 0.9352 (0.4643)	MaskBCELoss 0.0871 (0.0835)	MaskDICELoss 0.8481 (0.3808)
Epoch: [6][439/500]	Time  6.365 ( 6.365)	Loss 1.8100 (1.5063)	CeLoss 0.2002 (0.4089)	SegCLSLoss 0.0166 (0.0089)	KLLoss 0.3711 (0.2559)	MaskLoss 0.7820 (0.5336)	MaskBCELoss 0.0660 (0.0793)	MaskDICELoss 0.7160 (0.4543)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 437 is less than current step: 499. Dropping entry: {'train/loss': 1.3394306480884552, 'train/ce_loss': 0.38056640625, 'train/seg_cls_loss': 0.010247802734375, 'train/kl_loss': 0.25078125, 'train/mask_bce_loss': 0.08353488370776177, 'train/mask_dice_loss': 0.38076052963733675, 'train/mask_loss': 0.46429540365934374, 'metrics/total_secs_per_batch': 5.548229455947876, 'metrics/data_secs_per_batch': 2.325745677947998, '_timestamp': 1740974621.6896236}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 437 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974621.6899772}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 438 is less than current step: 499. Dropping entry: {'train/loss': 1.506277573108673, 'train/ce_loss': 0.40888671875, 'train/seg_cls_loss': 0.00887451171875, 'train/kl_loss': 0.255859375, 'train/mask_bce_loss': 0.07934055142104626, 'train/mask_dice_loss': 0.45426698327064513, 'train/mask_loss': 0.5336075365543366, 'metrics/total_secs_per_batch': 6.364878177642822, 'metrics/data_secs_per_batch': 2.489898920059204, '_timestamp': 1740974628.0545206}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 438 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974628.0548089}).
[2025-03-02 22:03:53,276] [INFO] [logging.py:96:log_dist] [Rank 0] step=3440, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:03:53,280] [INFO] [timer.py:215:stop] epoch=0/micro_step=34400/global_step=3440, RunningAvgSamplesPerSec=1.4476957836715008, CurrSamplesPerSec=1.9137747712408697, MemAllocated=30.78GB, MaxMemAllocated=37.23GB
Epoch: [6][440/500]	Time  5.227 ( 5.227)	Loss 0.9297 (1.5545)	CeLoss 0.9297 (0.4862)	SegCLSLoss 0.0000 (0.0109)	KLLoss 0.0000 (0.2545)	MaskLoss 0.0000 (0.5186)	MaskBCELoss 0.0000 (0.1308)	MaskDICELoss 0.0000 (0.3878)
Epoch: [6][441/500]	Time  5.145 ( 5.145)	Loss 1.5738 (1.2120)	CeLoss 0.1895 (0.5177)	SegCLSLoss 0.0258 (0.0114)	KLLoss 0.3711 (0.1836)	MaskLoss 0.6673 (0.3351)	MaskBCELoss 0.2617 (0.1037)	MaskDICELoss 0.4056 (0.2315)
Epoch: [6][442/500]	Time  6.359 ( 6.359)	Loss 2.3286 (1.8149)	CeLoss 0.2500 (0.3612)	SegCLSLoss 0.0095 (0.0160)	KLLoss 0.3594 (0.3262)	MaskLoss 1.0198 (0.7067)	MaskBCELoss 0.0235 (0.0933)	MaskDICELoss 0.9963 (0.6135)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 439 is less than current step: 499. Dropping entry: {'train/loss': 1.5544823169708253, 'train/ce_loss': 0.48623046875, 'train/seg_cls_loss': 0.010894775390625, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.1308469269424677, 'train/mask_dice_loss': 0.38775164932012557, 'train/mask_loss': 0.5185985684394836, 'metrics/total_secs_per_batch': 5.227019786834717, 'metrics/data_secs_per_batch': 2.8572057723999023, '_timestamp': 1740974633.2814379}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 439 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974633.2817664}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 440 is less than current step: 499. Dropping entry: {'train/loss': 1.212012279033661, 'train/ce_loss': 0.5176513671875, 'train/seg_cls_loss': 0.01141357421875, 'train/kl_loss': 0.18359375, 'train/mask_bce_loss': 0.10366308204829693, 'train/mask_dice_loss': 0.23145681917667388, 'train/mask_loss': 0.33511990308761597, 'metrics/total_secs_per_batch': 5.145051717758179, 'metrics/data_secs_per_batch': 2.39926278591156, '_timestamp': 1740974638.4265902}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 440 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974638.4268713}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 441 is less than current step: 499. Dropping entry: {'train/loss': 1.8149433970451354, 'train/ce_loss': 0.36123046875, 'train/seg_cls_loss': 0.015985107421875, 'train/kl_loss': 0.326171875, 'train/mask_bce_loss': 0.09327351674437523, 'train/mask_dice_loss': 0.6134657680988311, 'train/mask_loss': 0.7067392855882645, 'metrics/total_secs_per_batch': 6.359421491622925, 'metrics/data_secs_per_batch': 2.6728718519210815, '_timestamp': 1740974644.78599}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 441 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974644.786266}).
Epoch: [6][443/500]	Time  5.619 ( 5.619)	Loss 1.4416 (1.2919)	CeLoss 0.2949 (0.4950)	SegCLSLoss 0.0178 (0.0126)	KLLoss 0.3652 (0.2174)	MaskLoss 0.5499 (0.3844)	MaskBCELoss 0.0290 (0.0838)	MaskDICELoss 0.5209 (0.3006)
Epoch: [6][444/500]	Time  7.112 ( 7.112)	Loss 2.0735 (1.4728)	CeLoss 0.2139 (0.4415)	SegCLSLoss 0.0210 (0.0120)	KLLoss 0.3730 (0.2520)	MaskLoss 0.9059 (0.5000)	MaskBCELoss 0.1537 (0.0565)	MaskDICELoss 0.7521 (0.4435)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 442 is less than current step: 499. Dropping entry: {'train/loss': 1.2919163703918457, 'train/ce_loss': 0.494970703125, 'train/seg_cls_loss': 0.012646484375, 'train/kl_loss': 0.2173828125, 'train/mask_bce_loss': 0.08381210323423147, 'train/mask_dice_loss': 0.30059823393821716, 'train/mask_loss': 0.3844103366136551, 'metrics/total_secs_per_batch': 5.619344234466553, 'metrics/data_secs_per_batch': 2.7206654787063598, '_timestamp': 1740974650.4053485}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 442 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974650.4056456}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 443 is less than current step: 499. Dropping entry: {'train/loss': 1.47283176779747, 'train/ce_loss': 0.4414794921875, 'train/seg_cls_loss': 0.01197509765625, 'train/kl_loss': 0.251953125, 'train/mask_bce_loss': 0.05646392591297626, 'train/mask_dice_loss': 0.4434895679354668, 'train/mask_loss': 0.49995348155498504, 'metrics/total_secs_per_batch': 7.1119842529296875, 'metrics/data_secs_per_batch': 3.1365684986114504, '_timestamp': 1740974657.517317}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 443 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974657.5175912}).
Epoch: [6][445/500]	Time  5.437 ( 5.437)	Loss 0.8320 (1.2917)	CeLoss 0.8320 (0.4683)	SegCLSLoss 0.0000 (0.0132)	KLLoss 0.0000 (0.2553)	MaskLoss 0.0000 (0.3957)	MaskBCELoss 0.0000 (0.0645)	MaskDICELoss 0.0000 (0.3312)
Epoch: [6][446/500]	Time  6.687 ( 6.687)	Loss 2.8557 (1.8893)	CeLoss 0.1177 (0.2037)	SegCLSLoss 0.0422 (0.0214)	KLLoss 0.3672 (0.3615)	MaskLoss 1.3399 (0.8192)	MaskBCELoss 0.6229 (0.1389)	MaskDICELoss 0.7170 (0.6803)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 444 is less than current step: 499. Dropping entry: {'train/loss': 1.2917052298784255, 'train/ce_loss': 0.46826171875, 'train/seg_cls_loss': 0.01317138671875, 'train/kl_loss': 0.2552734375, 'train/mask_bce_loss': 0.06449486762285232, 'train/mask_dice_loss': 0.33116244375705717, 'train/mask_loss': 0.3956573113799095, 'metrics/total_secs_per_batch': 5.436845302581787, 'metrics/data_secs_per_batch': 2.4830072641372682, '_timestamp': 1740974662.9541898}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 444 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974662.95447}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 445 is less than current step: 499. Dropping entry: {'train/loss': 1.8892756432294846, 'train/ce_loss': 0.203662109375, 'train/seg_cls_loss': 0.021368408203125, 'train/kl_loss': 0.3615234375, 'train/mask_bce_loss': 0.13890317261684687, 'train/mask_dice_loss': 0.6803440362215042, 'train/mask_loss': 0.8192472085356712, 'metrics/total_secs_per_batch': 6.686988830566406, 'metrics/data_secs_per_batch': 3.00193510055542, '_timestamp': 1740974669.641266}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 445 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974669.6415808}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 446 is less than current step: 499. Dropping entry: {'train/loss': 1.3556524872779847, 'train/ce_loss': 0.3203125, 'train/seg_cls_loss': 0.01109619140625, 'train/kl_loss': 0.253125, 'train/mask_bce_loss': 0.14965816736221313, 'train/mask_dice_loss': 0.3523379847407341, 'train/mask_loss': 0.5019961506128311, 'metrics/total_secs_per_batch': 6.506767749786377, 'metrics/data_secs_per_batch': 2.8831066608428957, '_timestamp': 1740974676.148068}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 446 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974676.1483874}).
Epoch: [6][447/500]	Time  6.507 ( 6.507)	Loss 1.5347 (1.3557)	CeLoss 0.2812 (0.3203)	SegCLSLoss 0.0258 (0.0111)	KLLoss 0.3633 (0.2531)	MaskLoss 0.6013 (0.5020)	MaskBCELoss 0.1761 (0.1497)	MaskDICELoss 0.4252 (0.3523)
Epoch: [6][448/500]	Time  5.144 ( 5.144)	Loss 0.9990 (1.6596)	CeLoss 0.2139 (0.6144)	SegCLSLoss 0.0091 (0.0103)	KLLoss 0.3672 (0.2197)	MaskLoss 0.3716 (0.5090)	MaskBCELoss 0.1619 (0.1243)	MaskDICELoss 0.2096 (0.3848)
Epoch: [6][449/500]	Time  4.931 ( 4.931)	Loss 2.0519 (1.5939)	CeLoss 0.2090 (0.4679)	SegCLSLoss 0.0148 (0.0110)	KLLoss 0.3633 (0.2182)	MaskLoss 0.8990 (0.5493)	MaskBCELoss 0.0623 (0.0870)	MaskDICELoss 0.8367 (0.4622)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 447 is less than current step: 499. Dropping entry: {'train/loss': 1.6596018612384795, 'train/ce_loss': 0.61435546875, 'train/seg_cls_loss': 0.01026611328125, 'train/kl_loss': 0.2197265625, 'train/mask_bce_loss': 0.12425151187926531, 'train/mask_dice_loss': 0.3847974702715874, 'train/mask_loss': 0.5090489745140075, 'metrics/total_secs_per_batch': 5.143956899642944, 'metrics/data_secs_per_batch': 2.10065701007843, '_timestamp': 1740974681.291962}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 447 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974681.2922494}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 448 is less than current step: 499. Dropping entry: {'train/loss': 1.593923604488373, 'train/ce_loss': 0.467919921875, 'train/seg_cls_loss': 0.010992431640625, 'train/kl_loss': 0.2181640625, 'train/mask_bce_loss': 0.08704503271728754, 'train/mask_dice_loss': 0.46223609447479247, 'train/mask_loss': 0.5492811381816864, 'metrics/total_secs_per_batch': 4.930851936340332, 'metrics/data_secs_per_batch': 2.178525447845459, '_timestamp': 1740974686.22273}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 448 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974686.2230656}).
[2025-03-02 22:04:55,626] [INFO] [logging.py:96:log_dist] [Rank 0] step=3450, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:04:55,631] [INFO] [timer.py:215:stop] epoch=0/micro_step=34500/global_step=3450, RunningAvgSamplesPerSec=1.4481054256902794, CurrSamplesPerSec=1.0629636110874223, MemAllocated=31.27GB, MaxMemAllocated=37.23GB
Epoch: [6][450/500]	Time  9.409 ( 9.409)	Loss 1.6867 (1.4848)	CeLoss 0.1621 (0.2598)	SegCLSLoss 0.0253 (0.0161)	KLLoss 0.3711 (0.2945)	MaskLoss 0.7374 (0.5936)	MaskBCELoss 0.0396 (0.1400)	MaskDICELoss 0.6978 (0.4535)
Epoch: [6][451/500]	Time  6.036 ( 6.036)	Loss 2.3121 (1.5302)	CeLoss 0.1416 (0.3600)	SegCLSLoss 0.0398 (0.0119)	KLLoss 0.3770 (0.2566)	MaskLoss 1.0564 (0.5692)	MaskBCELoss 0.2653 (0.1897)	MaskDICELoss 0.7911 (0.3795)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 449 is less than current step: 499. Dropping entry: {'train/loss': 1.4847934544086456, 'train/ce_loss': 0.2597900390625, 'train/seg_cls_loss': 0.016070556640625, 'train/kl_loss': 0.29453125, 'train/mask_bce_loss': 0.14002105444669724, 'train/mask_dice_loss': 0.4535353422164917, 'train/mask_loss': 0.5935563936829567, 'metrics/total_secs_per_batch': 9.409265518188477, 'metrics/data_secs_per_batch': 2.5501413106918336, '_timestamp': 1740974695.6318052}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 449 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974695.6320844}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 450 is less than current step: 499. Dropping entry: {'train/loss': 1.5301878154277802, 'train/ce_loss': 0.360009765625, 'train/seg_cls_loss': 0.011907958984375, 'train/kl_loss': 0.256640625, 'train/mask_bce_loss': 0.18965288810431957, 'train/mask_dice_loss': 0.3795181602239609, 'train/mask_loss': 0.5691710561513901, 'metrics/total_secs_per_batch': 6.036410808563232, 'metrics/data_secs_per_batch': 2.560747742652893, '_timestamp': 1740974701.668427}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 450 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974701.668717}).
Epoch: [6][452/500]	Time  4.990 ( 4.990)	Loss 0.8477 (1.6455)	CeLoss 0.8477 (0.7228)	SegCLSLoss 0.0000 (0.0079)	KLLoss 0.0000 (0.1840)	MaskLoss 0.0000 (0.4501)	MaskBCELoss 0.0000 (0.0964)	MaskDICELoss 0.0000 (0.3537)
Epoch: [6][453/500]	Time  6.198 ( 6.198)	Loss 1.9956 (1.8161)	CeLoss 0.1582 (0.2150)	SegCLSLoss 0.0284 (0.0197)	KLLoss 0.3516 (0.3625)	MaskLoss 0.8938 (0.7774)	MaskBCELoss 0.1791 (0.1724)	MaskDICELoss 0.7148 (0.6050)
Epoch: [6][454/500]	Time  5.616 ( 5.616)	Loss 1.3672 (1.6339)	CeLoss 1.3672 (0.7767)	SegCLSLoss 0.0000 (0.0078)	KLLoss 0.0000 (0.1801)	MaskLoss 0.0000 (0.4177)	MaskBCELoss 0.0000 (0.0822)	MaskDICELoss 0.0000 (0.3355)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 451 is less than current step: 499. Dropping entry: {'train/loss': 1.645510971546173, 'train/ce_loss': 0.72275390625, 'train/seg_cls_loss': 0.00791015625, 'train/kl_loss': 0.183984375, 'train/mask_bce_loss': 0.09643450658768415, 'train/mask_dice_loss': 0.3537135601043701, 'train/mask_loss': 0.45014806389808654, 'metrics/total_secs_per_batch': 4.989837884902954, 'metrics/data_secs_per_batch': 2.59722957611084, '_timestamp': 1740974706.6582823}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 451 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974706.6585495}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 452 is less than current step: 499. Dropping entry: {'train/loss': 1.8161020159721375, 'train/ce_loss': 0.2150390625, 'train/seg_cls_loss': 0.019708251953125, 'train/kl_loss': 0.3625, 'train/mask_bce_loss': 0.17240474391728638, 'train/mask_dice_loss': 0.605031019449234, 'train/mask_loss': 0.7774357676506043, 'metrics/total_secs_per_batch': 6.197965383529663, 'metrics/data_secs_per_batch': 2.6399434566497804, '_timestamp': 1740974712.8562396}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 452 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974712.8565166}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 453 is less than current step: 499. Dropping entry: {'train/loss': 1.6339455604553224, 'train/ce_loss': 0.77666015625, 'train/seg_cls_loss': 0.007794189453125, 'train/kl_loss': 0.180078125, 'train/mask_bce_loss': 0.0821889616549015, 'train/mask_dice_loss': 0.3355162352323532, 'train/mask_loss': 0.4177051901817322, 'metrics/total_secs_per_batch': 5.615808963775635, 'metrics/data_secs_per_batch': 2.3421319723129272, '_timestamp': 1740974718.472071}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 453 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974718.4723582}).
Epoch: [6][455/500]	Time  6.445 ( 6.445)	Loss 1.2564 (1.3272)	CeLoss 0.2168 (0.3793)	SegCLSLoss 0.0116 (0.0099)	KLLoss 0.3652 (0.2537)	MaskLoss 0.4983 (0.4588)	MaskBCELoss 0.1054 (0.0839)	MaskDICELoss 0.3930 (0.3749)
Epoch: [6][456/500]	Time  5.506 ( 5.506)	Loss 1.8695 (1.6721)	CeLoss 0.2158 (0.5703)	SegCLSLoss 0.0156 (0.0112)	KLLoss 0.3652 (0.2154)	MaskLoss 0.8049 (0.5373)	MaskBCELoss 0.0534 (0.0807)	MaskDICELoss 0.7515 (0.4565)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 454 is less than current step: 499. Dropping entry: {'train/loss': 1.3272141337394714, 'train/ce_loss': 0.379296875, 'train/seg_cls_loss': 0.00985107421875, 'train/kl_loss': 0.2537109375, 'train/mask_bce_loss': 0.08394680730998516, 'train/mask_dice_loss': 0.3748751014471054, 'train/mask_loss': 0.4588219106197357, 'metrics/total_secs_per_batch': 6.445312023162842, 'metrics/data_secs_per_batch': 2.8567445278167725, '_timestamp': 1740974724.917398}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 454 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974724.9176857}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 455 is less than current step: 499. Dropping entry: {'train/loss': 1.6720614671707152, 'train/ce_loss': 0.5703125, 'train/seg_cls_loss': 0.011181640625, 'train/kl_loss': 0.2154296875, 'train/mask_bce_loss': 0.0807232616469264, 'train/mask_dice_loss': 0.4565281867980957, 'train/mask_loss': 0.5372514367103577, 'metrics/total_secs_per_batch': 5.506110429763794, 'metrics/data_secs_per_batch': 2.0769481658935547, '_timestamp': 1740974730.4234772}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 455 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974730.4237483}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 456 is less than current step: 499. Dropping entry: {'train/loss': 1.7243544340133667, 'train/ce_loss': 0.42197265625, 'train/seg_cls_loss': 0.012579345703125, 'train/kl_loss': 0.291015625, 'train/mask_bce_loss': 0.10939242329914123, 'train/mask_dice_loss': 0.5239761888980865, 'train/mask_loss': 0.6333686053752899, 'metrics/total_secs_per_batch': 5.663451671600342, 'metrics/data_secs_per_batch': 2.946806859970093, '_timestamp': 1740974736.0869792}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 456 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974736.0873513}).
Epoch: [6][457/500]	Time  5.663 ( 5.663)	Loss 2.1557 (1.7244)	CeLoss 0.2109 (0.4220)	SegCLSLoss 0.0122 (0.0126)	KLLoss 0.3691 (0.2910)	MaskLoss 0.9509 (0.6334)	MaskBCELoss 0.0140 (0.1094)	MaskDICELoss 0.9369 (0.5240)
Epoch: [6][458/500]	Time  5.608 ( 5.608)	Loss 1.5234 (1.5933)	CeLoss 1.5234 (0.4612)	SegCLSLoss 0.0000 (0.0179)	KLLoss 0.0000 (0.2908)	MaskLoss 0.0000 (0.5471)	MaskBCELoss 0.0000 (0.1064)	MaskDICELoss 0.0000 (0.4406)
Epoch: [6][459/500]	Time  6.496 ( 6.496)	Loss 1.2425 (1.5076)	CeLoss 0.2656 (0.4377)	SegCLSLoss 0.0107 (0.0112)	KLLoss 0.3691 (0.2893)	MaskLoss 0.4670 (0.5177)	MaskBCELoss 0.0473 (0.0957)	MaskDICELoss 0.4196 (0.4219)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 457 is less than current step: 499. Dropping entry: {'train/loss': 1.5932604908943175, 'train/ce_loss': 0.461181640625, 'train/seg_cls_loss': 0.017852783203125, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.10644516441971064, 'train/mask_dice_loss': 0.44062453620135783, 'train/mask_loss': 0.5470696985721588, 'metrics/total_secs_per_batch': 5.607578754425049, 'metrics/data_secs_per_batch': 2.365265965461731, '_timestamp': 1740974741.694744}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 457 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974741.695101}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 458 is less than current step: 499. Dropping entry: {'train/loss': 1.5075887322425843, 'train/ce_loss': 0.4376953125, 'train/seg_cls_loss': 0.01123046875, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.09572748839855194, 'train/mask_dice_loss': 0.42193406224250796, 'train/mask_loss': 0.5176615566015244, 'metrics/total_secs_per_batch': 6.495626211166382, 'metrics/data_secs_per_batch': 2.85886812210083, '_timestamp': 1740974748.1901498}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 458 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974748.1904204}).
[2025-03-02 22:05:54,615] [INFO] [logging.py:96:log_dist] [Rank 0] step=3460, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:05:54,621] [INFO] [timer.py:215:stop] epoch=0/micro_step=34600/global_step=3460, RunningAvgSamplesPerSec=1.4487168365699552, CurrSamplesPerSec=1.555091327742628, MemAllocated=31.11GB, MaxMemAllocated=37.23GB
Epoch: [6][460/500]	Time  6.432 ( 6.432)	Loss 2.4189 (1.9222)	CeLoss 0.1670 (0.5115)	SegCLSLoss 0.0250 (0.0128)	KLLoss 0.3750 (0.2910)	MaskLoss 1.1010 (0.6875)	MaskBCELoss 0.3555 (0.1364)	MaskDICELoss 0.7455 (0.5511)
Epoch: [6][461/500]	Time  5.859 ( 5.859)	Loss 2.2079 (1.9416)	CeLoss 0.2578 (0.3270)	SegCLSLoss 0.0128 (0.0169)	KLLoss 0.3633 (0.3242)	MaskLoss 0.9536 (0.7868)	MaskBCELoss 0.2410 (0.1202)	MaskDICELoss 0.7126 (0.6666)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 459 is less than current step: 499. Dropping entry: {'train/loss': 1.922184443473816, 'train/ce_loss': 0.5115234375, 'train/seg_cls_loss': 0.01279296875, 'train/kl_loss': 0.291015625, 'train/mask_bce_loss': 0.13639044035226106, 'train/mask_dice_loss': 0.5510689646005631, 'train/mask_loss': 0.6874594151973724, 'metrics/total_secs_per_batch': 6.432096481323242, 'metrics/data_secs_per_batch': 2.8041966199874877, '_timestamp': 1740974754.622072}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 459 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974754.6223645}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 460 is less than current step: 499. Dropping entry: {'train/loss': 1.9416305541992187, 'train/ce_loss': 0.326953125, 'train/seg_cls_loss': 0.01693115234375, 'train/kl_loss': 0.32421875, 'train/mask_bce_loss': 0.1201622447464615, 'train/mask_dice_loss': 0.6666198223829269, 'train/mask_loss': 0.7867820739746094, 'metrics/total_secs_per_batch': 5.859375953674316, 'metrics/data_secs_per_batch': 2.3561028718948362, '_timestamp': 1740974760.4816103}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 460 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974760.4818807}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 461 is less than current step: 499. Dropping entry: {'train/loss': 1.2164636373519897, 'train/ce_loss': 0.494140625, 'train/seg_cls_loss': 0.007232666015625, 'train/kl_loss': 0.18046875, 'train/mask_bce_loss': 0.07642102828249335, 'train/mask_dice_loss': 0.2738517940044403, 'train/mask_loss': 0.35027283132076265, 'metrics/total_secs_per_batch': 5.571448564529419, 'metrics/data_secs_per_batch': 2.521519899368286, '_timestamp': 1740974766.0533152}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 461 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974766.0536911}).
Epoch: [6][462/500]	Time  5.571 ( 5.571)	Loss 0.8750 (1.2165)	CeLoss 0.8750 (0.4941)	SegCLSLoss 0.0000 (0.0072)	KLLoss 0.0000 (0.1805)	MaskLoss 0.0000 (0.3503)	MaskBCELoss 0.0000 (0.0764)	MaskDICELoss 0.0000 (0.2739)
Epoch: [6][463/500]	Time  6.341 ( 6.341)	Loss 1.1399 (1.1096)	CeLoss 0.2539 (0.3486)	SegCLSLoss 0.0118 (0.0080)	KLLoss 0.3633 (0.1801)	MaskLoss 0.4215 (0.3694)	MaskBCELoss 0.1162 (0.0621)	MaskDICELoss 0.3053 (0.3073)
Epoch: [6][464/500]	Time  6.468 ( 6.468)	Loss 0.0771 (1.5716)	CeLoss 0.0771 (0.2766)	SegCLSLoss 0.0000 (0.0151)	KLLoss 0.0000 (0.2895)	MaskLoss 0.0000 (0.6291)	MaskBCELoss 0.0000 (0.0491)	MaskDICELoss 0.0000 (0.5800)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 462 is less than current step: 499. Dropping entry: {'train/loss': 1.1096038460731505, 'train/ce_loss': 0.348583984375, 'train/seg_cls_loss': 0.008013916015625, 'train/kl_loss': 0.180078125, 'train/mask_bce_loss': 0.062080034520477057, 'train/mask_dice_loss': 0.30734591782093046, 'train/mask_loss': 0.3694259524345398, 'metrics/total_secs_per_batch': 6.3408074378967285, 'metrics/data_secs_per_batch': 2.766923713684082, '_timestamp': 1740974772.3939447}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 462 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974772.3941514}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 463 is less than current step: 499. Dropping entry: {'train/loss': 1.571611714363098, 'train/ce_loss': 0.2765625, 'train/seg_cls_loss': 0.015069580078125, 'train/kl_loss': 0.289453125, 'train/mask_bce_loss': 0.049120758194476366, 'train/mask_dice_loss': 0.5799956351518631, 'train/mask_loss': 0.6291163980960846, 'metrics/total_secs_per_batch': 6.4682981967926025, 'metrics/data_secs_per_batch': 2.629799222946167, '_timestamp': 1740974778.8625417}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 463 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974778.862969}).
Epoch: [6][465/500]	Time  7.631 ( 7.631)	Loss 1.7303 (1.6265)	CeLoss 0.2539 (0.3050)	SegCLSLoss 0.0217 (0.0170)	KLLoss 0.3477 (0.3254)	MaskLoss 0.7148 (0.6403)	MaskBCELoss 0.0607 (0.0954)	MaskDICELoss 0.6540 (0.5449)
Epoch: [6][466/500]	Time  6.339 ( 6.339)	Loss 2.3453 (1.9095)	CeLoss 0.2070 (0.4070)	SegCLSLoss 0.0217 (0.0144)	KLLoss 0.3691 (0.2932)	MaskLoss 1.0457 (0.7329)	MaskBCELoss 0.1965 (0.1778)	MaskDICELoss 0.8492 (0.5550)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 464 is less than current step: 499. Dropping entry: {'train/loss': 1.626546996831894, 'train/ce_loss': 0.30498046875, 'train/seg_cls_loss': 0.017022705078125, 'train/kl_loss': 0.325390625, 'train/mask_bce_loss': 0.0954461444169283, 'train/mask_dice_loss': 0.5448781341314316, 'train/mask_loss': 0.6403242766857147, 'metrics/total_secs_per_batch': 7.63066029548645, 'metrics/data_secs_per_batch': 3.446605110168457, '_timestamp': 1740974786.492866}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 464 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974786.4931254}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 465 is less than current step: 499. Dropping entry: {'train/loss': 1.9094546556472778, 'train/ce_loss': 0.40703125, 'train/seg_cls_loss': 0.014361572265625, 'train/kl_loss': 0.2931640625, 'train/mask_bce_loss': 0.17784750312566758, 'train/mask_dice_loss': 0.5550048291683197, 'train/mask_loss': 0.7328523397445679, 'metrics/total_secs_per_batch': 6.338715314865112, 'metrics/data_secs_per_batch': 2.4944966554641725, '_timestamp': 1740974792.831607}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 465 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974792.8319035}).
Epoch: [6][467/500]	Time  6.612 ( 6.612)	Loss 0.9453 (1.2684)	CeLoss 0.9453 (0.3651)	SegCLSLoss 0.0000 (0.0118)	KLLoss 0.0000 (0.2178)	MaskLoss 0.0000 (0.4378)	MaskBCELoss 0.0000 (0.0385)	MaskDICELoss 0.0000 (0.3993)
Epoch: [6][468/500]	Time  6.521 ( 6.521)	Loss 1.5755 (1.5059)	CeLoss 0.2500 (0.3032)	SegCLSLoss 0.0125 (0.0115)	KLLoss 0.3555 (0.2908)	MaskLoss 0.6423 (0.5839)	MaskBCELoss 0.0850 (0.1271)	MaskDICELoss 0.5573 (0.4568)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 466 is less than current step: 499. Dropping entry: {'train/loss': 1.2683542609214782, 'train/ce_loss': 0.365087890625, 'train/seg_cls_loss': 0.011761474609375, 'train/kl_loss': 0.2177734375, 'train/mask_bce_loss': 0.038526087161153556, 'train/mask_dice_loss': 0.3992887377738953, 'train/mask_loss': 0.43781482577323916, 'metrics/total_secs_per_batch': 6.612332582473755, 'metrics/data_secs_per_batch': 2.912531542778015, '_timestamp': 1740974799.4441373}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 466 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974799.444509}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 467 is less than current step: 499. Dropping entry: {'train/loss': 1.5058876156806946, 'train/ce_loss': 0.30322265625, 'train/seg_cls_loss': 0.011505126953125, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.12705981489270926, 'train/mask_dice_loss': 0.45679219216108324, 'train/mask_loss': 0.5838520050048828, 'metrics/total_secs_per_batch': 6.52053689956665, 'metrics/data_secs_per_batch': 2.7411715745925904, '_timestamp': 1740974805.964433}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 467 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974805.9647174}).
Epoch: [6][469/500]	Time  6.821 ( 6.821)	Loss 2.4403 (1.8232)	CeLoss 0.2441 (0.2088)	SegCLSLoss 0.0156 (0.0154)	KLLoss 0.3652 (0.3307)	MaskLoss 1.0756 (0.7868)	MaskBCELoss 0.4245 (0.1507)	MaskDICELoss 0.6511 (0.6362)
[2025-03-02 22:06:59,331] [INFO] [logging.py:96:log_dist] [Rank 0] step=3470, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:06:59,337] [INFO] [timer.py:215:stop] epoch=0/micro_step=34700/global_step=3470, RunningAvgSamplesPerSec=1.4489785733948104, CurrSamplesPerSec=1.5264717565872545, MemAllocated=30.7GB, MaxMemAllocated=37.23GB
Epoch: [6][470/500]	Time  6.553 ( 6.553)	Loss 1.1641 (1.7081)	CeLoss 1.1641 (0.4738)	SegCLSLoss 0.0000 (0.0148)	KLLoss 0.0000 (0.2521)	MaskLoss 0.0000 (0.6009)	MaskBCELoss 0.0000 (0.0727)	MaskDICELoss 0.0000 (0.5282)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 468 is less than current step: 499. Dropping entry: {'train/loss': 1.823192536830902, 'train/ce_loss': 0.2087890625, 'train/seg_cls_loss': 0.015350341796875, 'train/kl_loss': 0.3306640625, 'train/mask_bce_loss': 0.1506579475477338, 'train/mask_dice_loss': 0.636182451248169, 'train/mask_loss': 0.7868404060602188, 'metrics/total_secs_per_batch': 6.820980787277222, 'metrics/data_secs_per_batch': 2.982727384567261, '_timestamp': 1740974812.7854843}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 468 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974812.7858307}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 469 is less than current step: 499. Dropping entry: {'train/loss': 1.7080854058265686, 'train/ce_loss': 0.473828125, 'train/seg_cls_loss': 0.01480712890625, 'train/kl_loss': 0.2521484375, 'train/mask_bce_loss': 0.07268387682270258, 'train/mask_dice_loss': 0.5281849831342698, 'train/mask_loss': 0.6008688509464264, 'metrics/total_secs_per_batch': 6.55277419090271, 'metrics/data_secs_per_batch': 3.239196705818176, '_timestamp': 1740974819.3380075}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 469 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974819.3382888}).
Epoch: [6][471/500]	Time  7.244 ( 7.244)	Loss 2.1223 (1.8011)	CeLoss 0.2852 (0.2759)	SegCLSLoss 0.0160 (0.0161)	KLLoss 0.3652 (0.3299)	MaskLoss 0.8961 (0.7418)	MaskBCELoss 0.3293 (0.2088)	MaskDICELoss 0.5668 (0.5329)
Epoch: [6][472/500]	Time  6.461 ( 6.461)	Loss 0.9766 (1.4682)	CeLoss 0.9766 (0.3009)	SegCLSLoss 0.0000 (0.0109)	KLLoss 0.0000 (0.2906)	MaskLoss 0.0000 (0.5663)	MaskBCELoss 0.0000 (0.0891)	MaskDICELoss 0.0000 (0.4772)
Epoch: [6][473/500]	Time  4.828 ( 4.828)	Loss 2.3671 (1.9030)	CeLoss 0.2793 (0.5505)	SegCLSLoss 0.0114 (0.0111)	KLLoss 0.3613 (0.2553)	MaskLoss 1.0234 (0.6607)	MaskBCELoss 0.3882 (0.1929)	MaskDICELoss 0.6351 (0.4678)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 470 is less than current step: 499. Dropping entry: {'train/loss': 1.8010648608207702, 'train/ce_loss': 0.275927734375, 'train/seg_cls_loss': 0.016064453125, 'train/kl_loss': 0.3298828125, 'train/mask_bce_loss': 0.20884774066507816, 'train/mask_dice_loss': 0.5329444617033005, 'train/mask_loss': 0.7417921930551529, 'metrics/total_secs_per_batch': 7.243725538253784, 'metrics/data_secs_per_batch': 3.1730455636978148, '_timestamp': 1740974826.5820487}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 470 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974826.5824015}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 471 is less than current step: 499. Dropping entry: {'train/loss': 1.4682146787643433, 'train/ce_loss': 0.300927734375, 'train/seg_cls_loss': 0.010858154296875, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.08908533593639731, 'train/mask_dice_loss': 0.4771753132343292, 'train/mask_loss': 0.5662606567144394, 'metrics/total_secs_per_batch': 6.4611711502075195, 'metrics/data_secs_per_batch': 3.1399333477020264, '_timestamp': 1740974833.0431514}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 471 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974833.0434427}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 472 is less than current step: 499. Dropping entry: {'train/loss': 1.9029783964157105, 'train/ce_loss': 0.550537109375, 'train/seg_cls_loss': 0.01112060546875, 'train/kl_loss': 0.2552734375, 'train/mask_bce_loss': 0.19285937286913396, 'train/mask_dice_loss': 0.46780951619148253, 'train/mask_loss': 0.6606688857078552, 'metrics/total_secs_per_batch': 4.8281097412109375, 'metrics/data_secs_per_batch': 2.3754465103149416, '_timestamp': 1740974837.8712327}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 472 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974837.8715239}).
Epoch: [6][474/500]	Time  6.708 ( 6.708)	Loss 1.2188 (1.5274)	CeLoss 1.2188 (0.3464)	SegCLSLoss 0.0000 (0.0160)	KLLoss 0.0000 (0.3311)	MaskLoss 0.0000 (0.5700)	MaskBCELoss 0.0000 (0.1168)	MaskDICELoss 0.0000 (0.4532)
Epoch: [6][475/500]	Time  7.308 ( 7.308)	Loss 1.4701 (1.5672)	CeLoss 0.1953 (0.2894)	SegCLSLoss 0.0193 (0.0122)	KLLoss 0.3633 (0.2547)	MaskLoss 0.6144 (0.6231)	MaskBCELoss 0.0126 (0.1174)	MaskDICELoss 0.6018 (0.5057)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 473 is less than current step: 499. Dropping entry: {'train/loss': 1.5273661196231842, 'train/ce_loss': 0.34638671875, 'train/seg_cls_loss': 0.016046142578125, 'train/kl_loss': 0.3310546875, 'train/mask_bce_loss': 0.1168421046808362, 'train/mask_dice_loss': 0.4531886175274849, 'train/mask_loss': 0.5700307160615921, 'metrics/total_secs_per_batch': 6.70807957649231, 'metrics/data_secs_per_batch': 3.209560227394104, '_timestamp': 1740974844.5792923}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 473 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974844.579566}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 474 is less than current step: 499. Dropping entry: {'train/loss': 1.5672014355659485, 'train/ce_loss': 0.289404296875, 'train/seg_cls_loss': 0.012237548828125, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.11736747957766056, 'train/mask_dice_loss': 0.505710780620575, 'train/mask_loss': 0.6230782568454742, 'metrics/total_secs_per_batch': 7.308128118515015, 'metrics/data_secs_per_batch': 3.0555967330932616, '_timestamp': 1740974851.8874578}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 474 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974851.8877528}).
Epoch: [6][476/500]	Time  5.536 ( 5.536)	Loss 0.9766 (1.4982)	CeLoss 0.9766 (0.5334)	SegCLSLoss 0.0000 (0.0105)	KLLoss 0.0000 (0.2160)	MaskLoss 0.0000 (0.4689)	MaskBCELoss 0.0000 (0.0814)	MaskDICELoss 0.0000 (0.3875)
Epoch: [6][477/500]	Time  4.761 ( 4.761)	Loss 1.0938 (1.4875)	CeLoss 1.0938 (0.7675)	SegCLSLoss 0.0000 (0.0078)	KLLoss 0.0000 (0.1445)	MaskLoss 0.0000 (0.3507)	MaskBCELoss 0.0000 (0.0664)	MaskDICELoss 0.0000 (0.2843)
Epoch: [6][478/500]	Time  7.038 ( 7.038)	Loss 0.9267 (1.7824)	CeLoss 0.2393 (0.2139)	SegCLSLoss 0.0120 (0.0179)	KLLoss 0.3633 (0.3625)	MaskLoss 0.3227 (0.7618)	MaskBCELoss 0.0283 (0.1098)	MaskDICELoss 0.2945 (0.6520)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 475 is less than current step: 499. Dropping entry: {'train/loss': 1.498156440258026, 'train/ce_loss': 0.5333984375, 'train/seg_cls_loss': 0.01053466796875, 'train/kl_loss': 0.216015625, 'train/mask_bce_loss': 0.08139153569936752, 'train/mask_dice_loss': 0.38746207654476167, 'train/mask_loss': 0.46885361075401305, 'metrics/total_secs_per_batch': 5.535840749740601, 'metrics/data_secs_per_batch': 2.730832409858704, '_timestamp': 1740974857.4233594}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 475 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974857.4236996}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 476 is less than current step: 499. Dropping entry: {'train/loss': 1.4875168919563293, 'train/ce_loss': 0.76748046875, 'train/seg_cls_loss': 0.00784912109375, 'train/kl_loss': 0.14453125, 'train/mask_bce_loss': 0.06639830358326435, 'train/mask_dice_loss': 0.2843425631523132, 'train/mask_loss': 0.3507408738136292, 'metrics/total_secs_per_batch': 4.761396169662476, 'metrics/data_secs_per_batch': 1.9473590135574341, '_timestamp': 1740974862.1847725}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 476 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974862.1850808}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 477 is less than current step: 499. Dropping entry: {'train/loss': 1.782439386844635, 'train/ce_loss': 0.2138671875, 'train/seg_cls_loss': 0.017901611328125, 'train/kl_loss': 0.3625, 'train/mask_bce_loss': 0.10978974690660834, 'train/mask_dice_loss': 0.6519865781068802, 'train/mask_loss': 0.761776328086853, 'metrics/total_secs_per_batch': 7.038033485412598, 'metrics/data_secs_per_batch': 3.2269869089126586, '_timestamp': 1740974869.222715}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 477 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974869.2229884}).
Epoch: [6][479/500]	Time  4.298 ( 4.298)	Loss 0.0913 (1.3080)	CeLoss 0.0913 (0.6903)	SegCLSLoss 0.0000 (0.0117)	KLLoss 0.0000 (0.1812)	MaskLoss 0.0000 (0.2968)	MaskBCELoss 0.0000 (0.0654)	MaskDICELoss 0.0000 (0.2314)
[2025-03-02 22:07:59,723] [INFO] [logging.py:96:log_dist] [Rank 0] step=3480, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:07:59,729] [INFO] [timer.py:215:stop] epoch=0/micro_step=34800/global_step=3480, RunningAvgSamplesPerSec=1.4495000026358615, CurrSamplesPerSec=1.6108944801558638, MemAllocated=31.27GB, MaxMemAllocated=37.23GB
Epoch: [6][480/500]	Time  6.209 ( 6.209)	Loss 2.7712 (1.4984)	CeLoss 0.2539 (0.3742)	SegCLSLoss 0.0143 (0.0116)	KLLoss 0.3809 (0.2908)	MaskLoss 1.2352 (0.5447)	MaskBCELoss 0.2957 (0.1064)	MaskDICELoss 0.9395 (0.4382)
Epoch: [6][481/500]	Time  5.676 ( 5.676)	Loss 0.9814 (1.8717)	CeLoss 0.3965 (0.4694)	SegCLSLoss 0.0101 (0.0140)	KLLoss 0.3594 (0.2938)	MaskLoss 0.2720 (0.6830)	MaskBCELoss 0.1354 (0.2309)	MaskDICELoss 0.1366 (0.4521)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 478 is less than current step: 499. Dropping entry: {'train/loss': 1.3080412149429321, 'train/ce_loss': 0.690283203125, 'train/seg_cls_loss': 0.01173095703125, 'train/kl_loss': 0.18125, 'train/mask_bce_loss': 0.065382787771523, 'train/mask_dice_loss': 0.23138683289289474, 'train/mask_loss': 0.29676962941884993, 'metrics/total_secs_per_batch': 4.2980592250823975, 'metrics/data_secs_per_batch': 1.8306457281112671, '_timestamp': 1740974873.5207586}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 478 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974873.521025}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 479 is less than current step: 499. Dropping entry: {'train/loss': 1.4984450936317444, 'train/ce_loss': 0.37421875, 'train/seg_cls_loss': 0.01163330078125, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.10643318509683013, 'train/mask_dice_loss': 0.4382483571767807, 'train/mask_loss': 0.5446815460920333, 'metrics/total_secs_per_batch': 6.209283828735352, 'metrics/data_secs_per_batch': 2.749332571029663, '_timestamp': 1740974879.7298725}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 479 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974879.7301552}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 480 is less than current step: 499. Dropping entry: {'train/loss': 1.8717403292655945, 'train/ce_loss': 0.469384765625, 'train/seg_cls_loss': 0.014013671875, 'train/kl_loss': 0.29375, 'train/mask_bce_loss': 0.23088299632072448, 'train/mask_dice_loss': 0.45210630595684054, 'train/mask_loss': 0.6829893052577972, 'metrics/total_secs_per_batch': 5.675846338272095, 'metrics/data_secs_per_batch': 2.7688180208206177, '_timestamp': 1740974885.4059367}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 480 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974885.4062178}).
Epoch: [6][482/500]	Time  5.531 ( 5.531)	Loss 1.2188 (1.8878)	CeLoss 1.2188 (0.5718)	SegCLSLoss 0.0000 (0.0130)	KLLoss 0.0000 (0.2527)	MaskLoss 0.0000 (0.6420)	MaskBCELoss 0.0000 (0.2333)	MaskDICELoss 0.0000 (0.4088)
Epoch: [6][483/500]	Time  6.437 ( 6.437)	Loss 1.1406 (1.7241)	CeLoss 1.1406 (0.4142)	SegCLSLoss 0.0000 (0.0132)	KLLoss 0.0000 (0.2912)	MaskLoss 0.0000 (0.6372)	MaskBCELoss 0.0000 (0.1177)	MaskDICELoss 0.0000 (0.5195)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 481 is less than current step: 499. Dropping entry: {'train/loss': 1.887774795293808, 'train/ce_loss': 0.57177734375, 'train/seg_cls_loss': 0.01298828125, 'train/kl_loss': 0.252734375, 'train/mask_bce_loss': 0.2332626586779952, 'train/mask_dice_loss': 0.40876926109194756, 'train/mask_loss': 0.6420319229364395, 'metrics/total_secs_per_batch': 5.531484842300415, 'metrics/data_secs_per_batch': 2.5750357866287232, '_timestamp': 1740974890.937389}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 481 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974890.9375796}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 482 is less than current step: 499. Dropping entry: {'train/loss': 1.724124026298523, 'train/ce_loss': 0.41416015625, 'train/seg_cls_loss': 0.013189697265625, 'train/kl_loss': 0.2912109375, 'train/mask_bce_loss': 0.11765863690525294, 'train/mask_dice_loss': 0.5195498704910279, 'train/mask_loss': 0.6372085034847259, 'metrics/total_secs_per_batch': 6.437226057052612, 'metrics/data_secs_per_batch': 3.12979257106781, '_timestamp': 1740974897.3746288}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 482 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974897.374891}).
Epoch: [6][484/500]	Time  5.983 ( 5.983)	Loss 1.0949 (1.3121)	CeLoss 0.2852 (0.4518)	SegCLSLoss 0.0132 (0.0098)	KLLoss 0.3613 (0.2193)	MaskLoss 0.3834 (0.4166)	MaskBCELoss 0.0630 (0.0568)	MaskDICELoss 0.3204 (0.3598)
Epoch: [6][485/500]	Time  5.493 ( 5.493)	Loss 2.3738 (1.4257)	CeLoss 0.1484 (0.3154)	SegCLSLoss 0.0273 (0.0135)	KLLoss 0.3789 (0.2592)	MaskLoss 1.0868 (0.5388)	MaskBCELoss 0.2548 (0.0719)	MaskDICELoss 0.8320 (0.4669)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 483 is less than current step: 499. Dropping entry: {'train/loss': 1.312136447429657, 'train/ce_loss': 0.451806640625, 'train/seg_cls_loss': 0.009832763671875, 'train/kl_loss': 0.2193359375, 'train/mask_bce_loss': 0.05683707678690553, 'train/mask_dice_loss': 0.3597536027431488, 'train/mask_loss': 0.41659067273139955, 'metrics/total_secs_per_batch': 5.983106374740601, 'metrics/data_secs_per_batch': 2.894434905052185, '_timestamp': 1740974903.357846}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 483 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974903.358196}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 484 is less than current step: 499. Dropping entry: {'train/loss': 1.4256784558296203, 'train/ce_loss': 0.315380859375, 'train/seg_cls_loss': 0.013507080078125, 'train/kl_loss': 0.2591796875, 'train/mask_bce_loss': 0.0718535378575325, 'train/mask_dice_loss': 0.46693783402442934, 'train/mask_loss': 0.5387913763523102, 'metrics/total_secs_per_batch': 5.493173122406006, 'metrics/data_secs_per_batch': 2.6072876930236815, '_timestamp': 1740974908.851007}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 484 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974908.8513057}).
Epoch: [6][486/500]	Time  7.715 ( 7.715)	Loss 1.6345 (2.0084)	CeLoss 0.3203 (0.3941)	SegCLSLoss 0.0099 (0.0142)	KLLoss 0.3555 (0.3246)	MaskLoss 0.6366 (0.7872)	MaskBCELoss 0.0850 (0.1574)	MaskDICELoss 0.5516 (0.6298)
Epoch: [6][487/500]	Time  5.215 ( 5.215)	Loss 0.7411 (1.3479)	CeLoss 0.2246 (0.4583)	SegCLSLoss 0.0095 (0.0120)	KLLoss 0.3672 (0.2188)	MaskLoss 0.2377 (0.4309)	MaskBCELoss 0.0417 (0.0856)	MaskDICELoss 0.1961 (0.3453)
Epoch: [6][488/500]	Time  6.120 ( 6.120)	Loss 1.3520 (1.9051)	CeLoss 0.2617 (0.5344)	SegCLSLoss 0.0117 (0.0144)	KLLoss 0.3594 (0.2592)	MaskLoss 0.5246 (0.6689)	MaskBCELoss 0.0610 (0.1998)	MaskDICELoss 0.4637 (0.4691)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 485 is less than current step: 499. Dropping entry: {'train/loss': 2.0083635687828063, 'train/ce_loss': 0.394140625, 'train/seg_cls_loss': 0.014178466796875, 'train/kl_loss': 0.324609375, 'train/mask_bce_loss': 0.15738010182976722, 'train/mask_dice_loss': 0.629809494316578, 'train/mask_loss': 0.7871896028518677, 'metrics/total_secs_per_batch': 7.715011358261108, 'metrics/data_secs_per_batch': 3.2026615142822266, '_timestamp': 1740974916.5659785}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 485 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974916.5662801}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 486 is less than current step: 499. Dropping entry: {'train/loss': 1.3479251384735107, 'train/ce_loss': 0.458251953125, 'train/seg_cls_loss': 0.012030029296875, 'train/kl_loss': 0.21875, 'train/mask_bce_loss': 0.08563374131917953, 'train/mask_dice_loss': 0.34528682976961134, 'train/mask_loss': 0.43092057555913926, 'metrics/total_secs_per_batch': 5.215393781661987, 'metrics/data_secs_per_batch': 2.653924655914307, '_timestamp': 1740974921.7813168}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 486 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974921.7816696}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 487 is less than current step: 499. Dropping entry: {'train/loss': 1.9051031708717345, 'train/ce_loss': 0.534375, 'train/seg_cls_loss': 0.014373779296875, 'train/kl_loss': 0.2591796875, 'train/mask_bce_loss': 0.19979080371558666, 'train/mask_dice_loss': 0.4690693706274033, 'train/mask_loss': 0.6688601672649384, 'metrics/total_secs_per_batch': 6.12048602104187, 'metrics/data_secs_per_batch': 2.4026126861572266, '_timestamp': 1740974927.9018009}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 487 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974927.9020731}).
Epoch: [6][489/500]	Time  6.474 ( 6.474)	Loss 1.2907 (1.6498)	CeLoss 0.3477 (0.2784)	SegCLSLoss 0.0115 (0.0148)	KLLoss 0.3633 (0.2924)	MaskLoss 0.4501 (0.6673)	MaskBCELoss 0.0841 (0.1325)	MaskDICELoss 0.3660 (0.5348)
[2025-03-02 22:08:59,978] [INFO] [logging.py:96:log_dist] [Rank 0] step=3490, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:08:59,984] [INFO] [timer.py:215:stop] epoch=0/micro_step=34900/global_step=3490, RunningAvgSamplesPerSec=1.4500270776780932, CurrSamplesPerSec=1.7833100316588995, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [6][490/500]	Time  5.609 ( 5.609)	Loss 2.0505 (1.6600)	CeLoss 0.2695 (0.3917)	SegCLSLoss 0.0134 (0.0127)	KLLoss 0.3594 (0.2889)	MaskLoss 0.8700 (0.6166)	MaskBCELoss 0.1668 (0.1664)	MaskDICELoss 0.7032 (0.4502)
Epoch: [6][491/500]	Time  5.324 ( 5.324)	Loss 0.7422 (1.4748)	CeLoss 0.7422 (0.4667)	SegCLSLoss 0.0000 (0.0098)	KLLoss 0.0000 (0.2176)	MaskLoss 0.0000 (0.4907)	MaskBCELoss 0.0000 (0.1040)	MaskDICELoss 0.0000 (0.3868)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 488 is less than current step: 499. Dropping entry: {'train/loss': 1.6497834205627442, 'train/ce_loss': 0.27841796875, 'train/seg_cls_loss': 0.014764404296875, 'train/kl_loss': 0.2923828125, 'train/mask_bce_loss': 0.13254958279430867, 'train/mask_dice_loss': 0.5347737580537796, 'train/mask_loss': 0.6673233360052109, 'metrics/total_secs_per_batch': 6.47369384765625, 'metrics/data_secs_per_batch': 2.991345000267029, '_timestamp': 1740974934.375597}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 488 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974934.37589}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 489 is less than current step: 499. Dropping entry: {'train/loss': 1.6600101947784425, 'train/ce_loss': 0.39169921875, 'train/seg_cls_loss': 0.012713623046875, 'train/kl_loss': 0.2888671875, 'train/mask_bce_loss': 0.1663928259164095, 'train/mask_dice_loss': 0.4501845300197601, 'train/mask_loss': 0.6165773630142212, 'metrics/total_secs_per_batch': 5.609223365783691, 'metrics/data_secs_per_batch': 2.66444354057312, '_timestamp': 1740974939.9846766}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 489 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974939.985035}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 490 is less than current step: 499. Dropping entry: {'train/loss': 1.4748003125190734, 'train/ce_loss': 0.466650390625, 'train/seg_cls_loss': 0.009832763671875, 'train/kl_loss': 0.217578125, 'train/mask_bce_loss': 0.10396507494151593, 'train/mask_dice_loss': 0.3867798000574112, 'train/mask_loss': 0.49074487686157225, 'metrics/total_secs_per_batch': 5.323933362960815, 'metrics/data_secs_per_batch': 2.6907069444656373, '_timestamp': 1740974945.3087358}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 490 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974945.3090289}).
Epoch: [6][492/500]	Time  4.484 ( 4.484)	Loss 1.9051 (1.5533)	CeLoss 0.2500 (0.6493)	SegCLSLoss 0.0165 (0.0084)	KLLoss 0.3574 (0.1822)	MaskLoss 0.8061 (0.4408)	MaskBCELoss 0.0210 (0.1085)	MaskDICELoss 0.7851 (0.3323)
Epoch: [6][493/500]	Time  6.862 ( 6.862)	Loss 1.6589 (1.2041)	CeLoss 0.1826 (0.2936)	SegCLSLoss 0.0176 (0.0133)	KLLoss 0.3574 (0.2543)	MaskLoss 0.7162 (0.4391)	MaskBCELoss 0.0211 (0.0721)	MaskDICELoss 0.6951 (0.3670)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 491 is less than current step: 499. Dropping entry: {'train/loss': 1.5533360838890076, 'train/ce_loss': 0.64931640625, 'train/seg_cls_loss': 0.00836181640625, 'train/kl_loss': 0.1822265625, 'train/mask_bce_loss': 0.10851730350404978, 'train/mask_dice_loss': 0.33231089264154434, 'train/mask_loss': 0.4408281981945038, 'metrics/total_secs_per_batch': 4.483845233917236, 'metrics/data_secs_per_batch': 1.9654887676239015, '_timestamp': 1740974949.7925248}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 491 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974949.792718}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 492 is less than current step: 499. Dropping entry: {'train/loss': 1.2040916442871095, 'train/ce_loss': 0.293603515625, 'train/seg_cls_loss': 0.013311767578125, 'train/kl_loss': 0.254296875, 'train/mask_bce_loss': 0.07213962115347386, 'train/mask_dice_loss': 0.3669911563396454, 'train/mask_loss': 0.4391307756304741, 'metrics/total_secs_per_batch': 6.862192869186401, 'metrics/data_secs_per_batch': 2.907641816139221, '_timestamp': 1740974956.6548216}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 492 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974956.6551552}).
Epoch: [6][494/500]	Time  5.039 ( 5.039)	Loss 1.4327 (1.4959)	CeLoss 0.2598 (0.5288)	SegCLSLoss 0.0151 (0.0100)	KLLoss 0.3613 (0.2174)	MaskLoss 0.5640 (0.4700)	MaskBCELoss 0.0310 (0.0943)	MaskDICELoss 0.5330 (0.3757)
Epoch: [6][495/500]	Time  4.651 ( 4.651)	Loss 1.4198 (1.0823)	CeLoss 0.2734 (0.5741)	SegCLSLoss 0.0094 (0.0042)	KLLoss 0.3672 (0.1102)	MaskLoss 0.5527 (0.2475)	MaskBCELoss 0.1061 (0.0442)	MaskDICELoss 0.4466 (0.2034)
Epoch: [6][496/500]	Time  6.575 ( 6.575)	Loss 2.4508 (1.7420)	CeLoss 0.1250 (0.3050)	SegCLSLoss 0.0269 (0.0142)	KLLoss 0.3711 (0.2922)	MaskLoss 1.1375 (0.7006)	MaskBCELoss 0.3414 (0.2073)	MaskDICELoss 0.7961 (0.4933)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 493 is less than current step: 499. Dropping entry: {'train/loss': 1.4959195375442504, 'train/ce_loss': 0.5287841796875, 'train/seg_cls_loss': 0.010028076171875, 'train/kl_loss': 0.2173828125, 'train/mask_bce_loss': 0.09431402720510959, 'train/mask_dice_loss': 0.3756794273853302, 'train/mask_loss': 0.46999346017837523, 'metrics/total_secs_per_batch': 5.038516283035278, 'metrics/data_secs_per_batch': 2.242226815223694, '_timestamp': 1740974961.6932995}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 493 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974961.6936119}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 494 is less than current step: 499. Dropping entry: {'train/loss': 1.0823291182518004, 'train/ce_loss': 0.574072265625, 'train/seg_cls_loss': 0.00423583984375, 'train/kl_loss': 0.11015625, 'train/mask_bce_loss': 0.04416954405605793, 'train/mask_dice_loss': 0.20336708724498748, 'train/mask_loss': 0.24753662347793579, 'metrics/total_secs_per_batch': 4.650573968887329, 'metrics/data_secs_per_batch': 2.1602455139160157, '_timestamp': 1740974966.3437998}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 494 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974966.344067}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 495 is less than current step: 499. Dropping entry: {'train/loss': 1.742024040222168, 'train/ce_loss': 0.30498046875, 'train/seg_cls_loss': 0.01419677734375, 'train/kl_loss': 0.2921875, 'train/mask_bce_loss': 0.2072805542498827, 'train/mask_dice_loss': 0.49327246844768524, 'train/mask_loss': 0.700553035736084, 'metrics/total_secs_per_batch': 6.574924945831299, 'metrics/data_secs_per_batch': 2.807205867767334, '_timestamp': 1740974972.9187365}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 495 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974972.9190125}).
Epoch: [6][497/500]	Time  6.597 ( 6.597)	Loss 2.7270 (1.7490)	CeLoss 0.1816 (0.2919)	SegCLSLoss 0.0310 (0.0200)	KLLoss 0.3789 (0.2924)	MaskLoss 1.2458 (0.7090)	MaskBCELoss 0.3664 (0.1515)	MaskDICELoss 0.8794 (0.5575)
Epoch: [6][498/500]	Time  5.949 ( 5.949)	Loss 1.0703 (0.9247)	CeLoss 1.0703 (0.5109)	SegCLSLoss 0.0000 (0.0043)	KLLoss 0.0000 (0.1076)	MaskLoss 0.0000 (0.2004)	MaskBCELoss 0.0000 (0.0169)	MaskDICELoss 0.0000 (0.1835)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 496 is less than current step: 499. Dropping entry: {'train/loss': 1.7490041851997375, 'train/ce_loss': 0.2918701171875, 'train/seg_cls_loss': 0.020001220703125, 'train/kl_loss': 0.2923828125, 'train/mask_bce_loss': 0.1515282940119505, 'train/mask_dice_loss': 0.5575074821710586, 'train/mask_loss': 0.7090357720851899, 'metrics/total_secs_per_batch': 6.596712589263916, 'metrics/data_secs_per_batch': 2.7019580602645874, '_timestamp': 1740974979.5154963}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 496 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974979.515777}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 497 is less than current step: 499. Dropping entry: {'train/loss': 0.9247144103050232, 'train/ce_loss': 0.5109375, 'train/seg_cls_loss': 0.004296875, 'train/kl_loss': 0.1076171875, 'train/mask_bce_loss': 0.016893430426716805, 'train/mask_dice_loss': 0.18354971408843995, 'train/mask_loss': 0.2004431426525116, 'metrics/total_secs_per_batch': 5.948636293411255, 'metrics/data_secs_per_batch': 2.956420350074768, '_timestamp': 1740974985.4643981}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 497 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974985.464771}).
Epoch: [6][499/500]	Time  6.566 ( 6.566)	Loss 2.3743 (1.5638)	CeLoss 0.2656 (0.4109)	SegCLSLoss 0.0155 (0.0113)	KLLoss 0.3535 (0.2500)	MaskLoss 1.0328 (0.5612)	MaskBCELoss 0.0451 (0.0684)	MaskDICELoss 0.9878 (0.4927)
  0%|                                                                                                                                                            | 0/200 [00:00<?, ?it/s]
[2025-03-02 22:09:56,961] [INFO] [logging.py:96:log_dist] [Rank 0] step=3500, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:09:56,967] [INFO] [timer.py:215:stop] epoch=0/micro_step=35000/global_step=3500, RunningAvgSamplesPerSec=1.4507483582702545, CurrSamplesPerSec=2.02599333982435, MemAllocated=31.24GB, MaxMemAllocated=37.23GB




 27%|███████████████████████████████████████▋                                                                                                           | 54/200 [00:09<00:19,  7.51it/s][34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 498 is less than current step: 499. Dropping entry: {'train/loss': 1.5638148784637451, 'train/ce_loss': 0.4109375, 'train/seg_cls_loss': 0.01131591796875, 'train/kl_loss': 0.25, 'train/mask_bce_loss': 0.06842799931764602, 'train/mask_dice_loss': 0.49272748827934265, 'train/mask_loss': 0.5611554980278015, 'metrics/total_secs_per_batch': 6.5663275718688965, 'metrics/data_secs_per_batch': 3.021357226371765, '_timestamp': 1740974992.0304368}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 498 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740974992.030708}).










100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:30<00:00,  7.16it/s]

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:30<00:00,  6.57it/s]
Epoch: [7][  1/500]	Time  6.489 ( 6.489)	Loss 3.0288 (1.4678)	CeLoss 0.2246 (0.1929)	SegCLSLoss 0.0193 (0.0119)	KLLoss 0.3574 (0.2547)	MaskLoss 1.3796 (0.6219)	MaskBCELoss 0.5184 (0.1480)	MaskDICELoss 0.8613 (0.4739)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 6 is less than current step: 499. Dropping entry: {'val/giou': 0.1878635585308075, 'val/ciou': 0.17587274312973022, '_timestamp': 1740975027.4112914}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 499. Dropping entry: {'train/loss': 1.467829430103302, 'train/ce_loss': 0.19287109375, 'train/seg_cls_loss': 0.011920166015625, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.14798194579780102, 'train/mask_dice_loss': 0.4738722205162048, 'train/mask_loss': 0.6218541815876961, 'metrics/total_secs_per_batch': 6.488771200180054, 'metrics/data_secs_per_batch': 2.865085983276367, '_timestamp': 1740975033.9070785}).
Epoch: [7][  2/500]	Time  6.675 ( 6.675)	Loss 2.4736 (1.7272)	CeLoss 0.2539 (0.2068)	SegCLSLoss 0.0165 (0.0158)	KLLoss 0.3691 (0.3258)	MaskLoss 1.0874 (0.7399)	MaskBCELoss 0.3628 (0.1300)	MaskDICELoss 0.7246 (0.6099)
Epoch: [7][  3/500]	Time  6.651 ( 6.651)	Loss 0.0796 (1.6964)	CeLoss 0.0796 (0.4110)	SegCLSLoss 0.0000 (0.0114)	KLLoss 0.0000 (0.2547)	MaskLoss 0.0000 (0.6272)	MaskBCELoss 0.0000 (0.1580)	MaskDICELoss 0.0000 (0.4691)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 1 is less than current step: 499. Dropping entry: {'train/loss': 1.7271543145179749, 'train/ce_loss': 0.2068359375, 'train/seg_cls_loss': 0.015771484375, 'train/kl_loss': 0.32578125, 'train/mask_bce_loss': 0.130005251057446, 'train/mask_dice_loss': 0.6099391102790832, 'train/mask_loss': 0.7399443447589874, 'metrics/total_secs_per_batch': 6.675371885299683, 'metrics/data_secs_per_batch': 3.1233265876770018, '_timestamp': 1740975040.5825667}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 1 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975040.5829287}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 2 is less than current step: 499. Dropping entry: {'train/loss': 1.6963922023773192, 'train/ce_loss': 0.410986328125, 'train/seg_cls_loss': 0.011383056640625, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.15802676612511277, 'train/mask_dice_loss': 0.46914881467819214, 'train/mask_loss': 0.6271755814552307, 'metrics/total_secs_per_batch': 6.651468753814697, 'metrics/data_secs_per_batch': 2.87292640209198, '_timestamp': 1740975047.2338822}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 2 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975047.2341862}).
Epoch: [7][  4/500]	Time  6.481 ( 6.481)	Loss 1.8245 (1.6476)	CeLoss 0.1670 (0.2045)	SegCLSLoss 0.0222 (0.0156)	KLLoss 0.3555 (0.3268)	MaskLoss 0.8058 (0.7013)	MaskBCELoss 0.0266 (0.1009)	MaskDICELoss 0.7792 (0.6004)
Epoch: [7][  5/500]	Time  6.284 ( 6.284)	Loss 1.1875 (1.6937)	CeLoss 1.1875 (0.5110)	SegCLSLoss 0.0000 (0.0105)	KLLoss 0.0000 (0.2516)	MaskLoss 0.0000 (0.5761)	MaskBCELoss 0.0000 (0.1360)	MaskDICELoss 0.0000 (0.4402)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 3 is less than current step: 499. Dropping entry: {'train/loss': 1.647569477558136, 'train/ce_loss': 0.204541015625, 'train/seg_cls_loss': 0.015570068359375, 'train/kl_loss': 0.3267578125, 'train/mask_bce_loss': 0.10086726453155279, 'train/mask_dice_loss': 0.6003832757472992, 'train/mask_loss': 0.7012505441904068, 'metrics/total_secs_per_batch': 6.480673551559448, 'metrics/data_secs_per_batch': 2.811803674697876, '_timestamp': 1740975053.7144775}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 3 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975053.7146754}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 4 is less than current step: 499. Dropping entry: {'train/loss': 1.6937400341033935, 'train/ce_loss': 0.510986328125, 'train/seg_cls_loss': 0.01048583984375, 'train/kl_loss': 0.2515625, 'train/mask_bce_loss': 0.135966454166919, 'train/mask_dice_loss': 0.4401760116219521, 'train/mask_loss': 0.5761424630880356, 'metrics/total_secs_per_batch': 6.283956289291382, 'metrics/data_secs_per_batch': 3.1373494625091554, '_timestamp': 1740975059.9986646}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 4 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975059.999032}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 5 is less than current step: 499. Dropping entry: {'train/loss': 1.999937653541565, 'train/ce_loss': 0.3994140625, 'train/seg_cls_loss': 0.015478515625, 'train/kl_loss': 0.296484375, 'train/mask_bce_loss': 0.2171559024602175, 'train/mask_dice_loss': 0.5644535422325134, 'train/mask_loss': 0.781609445810318, 'metrics/total_secs_per_batch': 5.900239944458008, 'metrics/data_secs_per_batch': 2.435624384880066, '_timestamp': 1740975065.8987632}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 5 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975065.8990552}).
Epoch: [7][  6/500]	Time  5.900 ( 5.900)	Loss 1.7565 (1.9999)	CeLoss 0.3477 (0.3994)	SegCLSLoss 0.0096 (0.0155)	KLLoss 0.3613 (0.2965)	MaskLoss 0.6849 (0.7816)	MaskBCELoss 0.3489 (0.2172)	MaskDICELoss 0.3359 (0.5645)
Epoch: [7][  7/500]	Time  6.040 ( 6.040)	Loss 1.3047 (1.1715)	CeLoss 1.3047 (0.4018)	SegCLSLoss 0.0000 (0.0096)	KLLoss 0.0000 (0.2557)	MaskLoss 0.0000 (0.3698)	MaskBCELoss 0.0000 (0.0724)	MaskDICELoss 0.0000 (0.2974)
Epoch: [7][  8/500]	Time  5.754 ( 5.754)	Loss 1.4931 (1.8282)	CeLoss 0.2734 (0.4343)	SegCLSLoss 0.0139 (0.0119)	KLLoss 0.3594 (0.2908)	MaskLoss 0.5884 (0.6793)	MaskBCELoss 0.0676 (0.2045)	MaskDICELoss 0.5208 (0.4748)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 6 is less than current step: 499. Dropping entry: {'train/loss': 1.1715032160282135, 'train/ce_loss': 0.4017822265625, 'train/seg_cls_loss': 0.00963134765625, 'train/kl_loss': 0.2556640625, 'train/mask_bce_loss': 0.07235085628926755, 'train/mask_dice_loss': 0.2974217474460602, 'train/mask_loss': 0.36977260410785673, 'metrics/total_secs_per_batch': 6.039886713027954, 'metrics/data_secs_per_batch': 2.7623573303222657, '_timestamp': 1740975071.9386299}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 6 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975071.9389331}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 7 is less than current step: 499. Dropping entry: {'train/loss': 1.8282287180423737, 'train/ce_loss': 0.43427734375, 'train/seg_cls_loss': 0.011895751953125, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.20445443615317344, 'train/mask_dice_loss': 0.47479663491249086, 'train/mask_loss': 0.6792510718107223, 'metrics/total_secs_per_batch': 5.754040956497192, 'metrics/data_secs_per_batch': 2.6522034883499144, '_timestamp': 1740975077.6929357}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 7 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975077.6933188}).
Epoch: [7][  9/500]	Time  7.106 ( 7.106)	Loss 1.4531 (1.9059)	CeLoss 1.4531 (0.3503)	SegCLSLoss 0.0000 (0.0126)	KLLoss 0.0000 (0.3281)	MaskLoss 0.0000 (0.7581)	MaskBCELoss 0.0000 (0.1281)	MaskDICELoss 0.0000 (0.6300)
[2025-03-02 22:11:29,951] [INFO] [logging.py:96:log_dist] [Rank 0] step=3510, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:11:29,957] [INFO] [timer.py:215:stop] epoch=0/micro_step=35100/global_step=3510, RunningAvgSamplesPerSec=1.451132621496411, CurrSamplesPerSec=1.9388136457626333, MemAllocated=30.81GB, MaxMemAllocated=37.23GB
Epoch: [7][ 10/500]	Time  5.159 ( 5.159)	Loss 2.0813 (1.6259)	CeLoss 0.1445 (0.4558)	SegCLSLoss 0.0203 (0.0139)	KLLoss 0.3711 (0.2553)	MaskLoss 0.9450 (0.5689)	MaskBCELoss 0.1861 (0.1161)	MaskDICELoss 0.7589 (0.4528)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 8 is less than current step: 499. Dropping entry: {'train/loss': 1.9059088706970215, 'train/ce_loss': 0.35029296875, 'train/seg_cls_loss': 0.01263427734375, 'train/kl_loss': 0.328125, 'train/mask_bce_loss': 0.12813775772228836, 'train/mask_dice_loss': 0.6299924582242966, 'train/mask_loss': 0.7581302016973496, 'metrics/total_secs_per_batch': 7.105908155441284, 'metrics/data_secs_per_batch': 3.423723649978638, '_timestamp': 1740975084.7985513}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 8 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975084.7988253}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 9 is less than current step: 499. Dropping entry: {'train/loss': 1.625913918018341, 'train/ce_loss': 0.45576171875, 'train/seg_cls_loss': 0.013897705078125, 'train/kl_loss': 0.2552734375, 'train/mask_bce_loss': 0.11611188016831875, 'train/mask_dice_loss': 0.45275328159332273, 'train/mask_loss': 0.5688651621341705, 'metrics/total_secs_per_batch': 5.159357070922852, 'metrics/data_secs_per_batch': 2.5221748113632203, '_timestamp': 1740975089.957751}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 9 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975089.9580743}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 10 is less than current step: 499. Dropping entry: {'train/loss': 1.3688876807689667, 'train/ce_loss': 0.321728515625, 'train/seg_cls_loss': 0.01014404296875, 'train/kl_loss': 0.2884765625, 'train/mask_bce_loss': 0.10176040967926384, 'train/mask_dice_loss': 0.4048758134245872, 'train/mask_loss': 0.5066362231969833, 'metrics/total_secs_per_batch': 6.154212236404419, 'metrics/data_secs_per_batch': 2.741623139381409, '_timestamp': 1740975096.1123395}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 10 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975096.112688}).
Epoch: [7][ 11/500]	Time  6.154 ( 6.154)	Loss 1.2266 (1.3689)	CeLoss 1.2266 (0.3217)	SegCLSLoss 0.0000 (0.0101)	KLLoss 0.0000 (0.2885)	MaskLoss 0.0000 (0.5066)	MaskBCELoss 0.0000 (0.1018)	MaskDICELoss 0.0000 (0.4049)
Epoch: [7][ 12/500]	Time  5.948 ( 5.948)	Loss 2.2814 (1.7606)	CeLoss 0.2324 (0.3389)	SegCLSLoss 0.0190 (0.0132)	KLLoss 0.3496 (0.2891)	MaskLoss 1.0020 (0.6930)	MaskBCELoss 0.0036 (0.1542)	MaskDICELoss 0.9984 (0.5388)
Epoch: [7][ 13/500]	Time  6.310 ( 6.310)	Loss 1.6272 (1.5449)	CeLoss 0.3027 (0.4426)	SegCLSLoss 0.0099 (0.0124)	KLLoss 0.3555 (0.2512)	MaskLoss 0.6417 (0.5355)	MaskBCELoss 0.1804 (0.0778)	MaskDICELoss 0.4614 (0.4577)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 11 is less than current step: 499. Dropping entry: {'train/loss': 1.7606350779533386, 'train/ce_loss': 0.3388671875, 'train/seg_cls_loss': 0.01324462890625, 'train/kl_loss': 0.2890625, 'train/mask_bce_loss': 0.15423669281881303, 'train/mask_dice_loss': 0.5387761563062667, 'train/mask_loss': 0.6930128514766694, 'metrics/total_secs_per_batch': 5.94806694984436, 'metrics/data_secs_per_batch': 2.4288835287094117, '_timestamp': 1740975102.0601726}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 11 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975102.060451}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 12 is less than current step: 499. Dropping entry: {'train/loss': 1.54489306807518, 'train/ce_loss': 0.442578125, 'train/seg_cls_loss': 0.012408447265625, 'train/kl_loss': 0.251171875, 'train/mask_bce_loss': 0.07776301391422749, 'train/mask_dice_loss': 0.4577206403017044, 'train/mask_loss': 0.5354836493730545, 'metrics/total_secs_per_batch': 6.310040235519409, 'metrics/data_secs_per_batch': 2.8997062921524046, '_timestamp': 1740975108.3702242}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 12 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975108.3705027}).
Epoch: [7][ 14/500]	Time  5.847 ( 5.847)	Loss 1.4820 (1.5691)	CeLoss 0.2988 (0.3983)	SegCLSLoss 0.0129 (0.0104)	KLLoss 0.3613 (0.2580)	MaskLoss 0.5711 (0.5699)	MaskBCELoss 0.0197 (0.1632)	MaskDICELoss 0.5513 (0.4067)
Epoch: [7][ 15/500]	Time  6.208 ( 6.208)	Loss 2.1094 (1.6935)	CeLoss 2.1094 (0.6146)	SegCLSLoss 0.0000 (0.0109)	KLLoss 0.0000 (0.2564)	MaskLoss 0.0000 (0.5239)	MaskBCELoss 0.0000 (0.1018)	MaskDICELoss 0.0000 (0.4222)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 13 is less than current step: 499. Dropping entry: {'train/loss': 1.569091272354126, 'train/ce_loss': 0.39833984375, 'train/seg_cls_loss': 0.01044921875, 'train/kl_loss': 0.2580078125, 'train/mask_bce_loss': 0.16317365765571595, 'train/mask_dice_loss': 0.406723552942276, 'train/mask_loss': 0.5698972091078758, 'metrics/total_secs_per_batch': 5.847385883331299, 'metrics/data_secs_per_batch': 2.556218957901001, '_timestamp': 1740975114.217675}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 13 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975114.217981}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 14 is less than current step: 499. Dropping entry: {'train/loss': 1.6934934735298157, 'train/ce_loss': 0.6146484375, 'train/seg_cls_loss': 0.01092529296875, 'train/kl_loss': 0.2564453125, 'train/mask_bce_loss': 0.10176732540130615, 'train/mask_dice_loss': 0.4221766859292984, 'train/mask_loss': 0.523944017291069, 'metrics/total_secs_per_batch': 6.2083234786987305, 'metrics/data_secs_per_batch': 3.121951937675476, '_timestamp': 1740975120.4261749}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 14 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975120.426538}).
Epoch: [7][ 16/500]	Time  6.571 ( 6.571)	Loss 1.1450 (1.5615)	CeLoss 0.3008 (0.1912)	SegCLSLoss 0.0111 (0.0141)	KLLoss 0.3652 (0.2932)	MaskLoss 0.4006 (0.6669)	MaskBCELoss 0.0756 (0.1205)	MaskDICELoss 0.3250 (0.5463)
Epoch: [7][ 17/500]	Time  5.706 ( 5.706)	Loss 2.0238 (2.0021)	CeLoss 0.1777 (0.5391)	SegCLSLoss 0.0286 (0.0108)	KLLoss 0.3594 (0.2545)	MaskLoss 0.8976 (0.7161)	MaskBCELoss 0.0204 (0.2122)	MaskDICELoss 0.8772 (0.5039)
Epoch: [7][ 18/500]	Time  6.436 ( 6.436)	Loss 1.6309 (1.8740)	CeLoss 0.2119 (0.3124)	SegCLSLoss 0.0206 (0.0160)	KLLoss 0.3574 (0.3279)	MaskLoss 0.6865 (0.7603)	MaskBCELoss 0.1525 (0.1657)	MaskDICELoss 0.5341 (0.5946)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 15 is less than current step: 499. Dropping entry: {'train/loss': 1.5615253090858459, 'train/ce_loss': 0.191162109375, 'train/seg_cls_loss': 0.014068603515625, 'train/kl_loss': 0.2931640625, 'train/mask_bce_loss': 0.1205312043428421, 'train/mask_dice_loss': 0.5463398516178131, 'train/mask_loss': 0.6668710470199585, 'metrics/total_secs_per_batch': 6.57093071937561, 'metrics/data_secs_per_batch': 3.023769974708557, '_timestamp': 1740975126.9969032}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 15 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975126.9972057}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 16 is less than current step: 499. Dropping entry: {'train/loss': 2.002116322517395, 'train/ce_loss': 0.5390625, 'train/seg_cls_loss': 0.010784912109375, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.2121714200824499, 'train/mask_dice_loss': 0.5039258122444152, 'train/mask_loss': 0.716097229719162, 'metrics/total_secs_per_batch': 5.7058117389678955, 'metrics/data_secs_per_batch': 2.5988221645355223, '_timestamp': 1740975132.7026842}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 16 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975132.702969}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 17 is less than current step: 499. Dropping entry: {'train/loss': 1.8739571571350098, 'train/ce_loss': 0.31240234375, 'train/seg_cls_loss': 0.015966796875, 'train/kl_loss': 0.3279296875, 'train/mask_bce_loss': 0.16568569634109737, 'train/mask_dice_loss': 0.5945838913321495, 'train/mask_loss': 0.7602695927023888, 'metrics/total_secs_per_batch': 6.435710430145264, 'metrics/data_secs_per_batch': 2.6048990964889525, '_timestamp': 1740975139.1384304}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 17 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975139.138717}).
Epoch: [7][ 19/500]	Time  6.183 ( 6.183)	Loss 2.7150 (1.9087)	CeLoss 0.1680 (0.2382)	SegCLSLoss 0.0292 (0.0177)	KLLoss 0.3691 (0.3635)	MaskLoss 1.2477 (0.8127)	MaskBCELoss 0.4916 (0.2443)	MaskDICELoss 0.7561 (0.5684)
[2025-03-02 22:12:31,401] [INFO] [logging.py:96:log_dist] [Rank 0] step=3520, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:12:31,406] [INFO] [timer.py:215:stop] epoch=0/micro_step=35200/global_step=3520, RunningAvgSamplesPerSec=1.4515802037119023, CurrSamplesPerSec=1.6436372317433805, MemAllocated=31.26GB, MaxMemAllocated=37.23GB
Epoch: [7][ 20/500]	Time  6.086 ( 6.086)	Loss 1.8408 (1.8914)	CeLoss 0.2246 (0.4134)	SegCLSLoss 0.0225 (0.0143)	KLLoss 0.3672 (0.3240)	MaskLoss 0.7837 (0.7191)	MaskBCELoss 0.0641 (0.1586)	MaskDICELoss 0.7196 (0.5605)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 18 is less than current step: 499. Dropping entry: {'train/loss': 1.9086715459823609, 'train/ce_loss': 0.23818359375, 'train/seg_cls_loss': 0.01766357421875, 'train/kl_loss': 0.3634765625, 'train/mask_bce_loss': 0.24430007115006447, 'train/mask_dice_loss': 0.5684341415762901, 'train/mask_loss': 0.8127342209219932, 'metrics/total_secs_per_batch': 6.183146715164185, 'metrics/data_secs_per_batch': 2.974379849433899, '_timestamp': 1740975145.3215632}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 18 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975145.3218474}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 19 is less than current step: 499. Dropping entry: {'train/loss': 1.8914068698883058, 'train/ce_loss': 0.41337890625, 'train/seg_cls_loss': 0.014349365234375, 'train/kl_loss': 0.3240234375, 'train/mask_bce_loss': 0.1585968886502087, 'train/mask_dice_loss': 0.5605440393090249, 'train/mask_loss': 0.7191409349441529, 'metrics/total_secs_per_batch': 6.085703134536743, 'metrics/data_secs_per_batch': 2.889665675163269, '_timestamp': 1740975151.407169}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 19 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975151.4075358}).
Epoch: [7][ 21/500]	Time  6.659 ( 6.659)	Loss 0.1836 (2.0047)	CeLoss 0.1836 (0.3067)	SegCLSLoss 0.0000 (0.0155)	KLLoss 0.0000 (0.2967)	MaskLoss 0.0000 (0.8302)	MaskBCELoss 0.0000 (0.2232)	MaskDICELoss 0.0000 (0.6070)
Epoch: [7][ 22/500]	Time  5.606 ( 5.606)	Loss 1.0703 (1.7033)	CeLoss 1.0703 (0.4836)	SegCLSLoss 0.0000 (0.0112)	KLLoss 0.0000 (0.2533)	MaskLoss 0.0000 (0.5944)	MaskBCELoss 0.0000 (0.1338)	MaskDICELoss 0.0000 (0.4606)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 20 is less than current step: 499. Dropping entry: {'train/loss': 2.004747784137726, 'train/ce_loss': 0.30673828125, 'train/seg_cls_loss': 0.01549072265625, 'train/kl_loss': 0.2966796875, 'train/mask_bce_loss': 0.22319087125360965, 'train/mask_dice_loss': 0.60701504945755, 'train/mask_loss': 0.8302059233188629, 'metrics/total_secs_per_batch': 6.659158706665039, 'metrics/data_secs_per_batch': 3.110032391548157, '_timestamp': 1740975158.0664399}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 20 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975158.0667367}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 21 is less than current step: 499. Dropping entry: {'train/loss': 1.7032923221588134, 'train/ce_loss': 0.48359375, 'train/seg_cls_loss': 0.011151123046875, 'train/kl_loss': 0.2533203125, 'train/mask_bce_loss': 0.1337511156219989, 'train/mask_dice_loss': 0.4606196641921997, 'train/mask_loss': 0.5943707704544068, 'metrics/total_secs_per_batch': 5.60602593421936, 'metrics/data_secs_per_batch': 2.3719762563705444, '_timestamp': 1740975163.6724496}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 21 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975163.67272}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 22 is less than current step: 499. Dropping entry: {'train/loss': 1.6937172889709473, 'train/ce_loss': 0.180078125, 'train/seg_cls_loss': 0.01419677734375, 'train/kl_loss': 0.2919921875, 'train/mask_bce_loss': 0.2142700335010886, 'train/mask_dice_loss': 0.5244831353425979, 'train/mask_loss': 0.7387531638145447, 'metrics/total_secs_per_batch': 6.507003307342529, 'metrics/data_secs_per_batch': 2.7282348394393923, '_timestamp': 1740975170.179452}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 22 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975170.179723}).
Epoch: [7][ 23/500]	Time  6.507 ( 6.507)	Loss 2.7341 (1.6937)	CeLoss 0.1875 (0.1801)	SegCLSLoss 0.0161 (0.0142)	KLLoss 0.3770 (0.2920)	MaskLoss 1.2508 (0.7388)	MaskBCELoss 0.4530 (0.2143)	MaskDICELoss 0.7978 (0.5245)
Epoch: [7][ 24/500]	Time  7.250 ( 7.250)	Loss 1.1484 (1.2270)	CeLoss 1.1484 (0.2989)	SegCLSLoss 0.0000 (0.0125)	KLLoss 0.0000 (0.2545)	MaskLoss 0.0000 (0.4482)	MaskBCELoss 0.0000 (0.0901)	MaskDICELoss 0.0000 (0.3581)
Epoch: [7][ 25/500]	Time  5.919 ( 5.919)	Loss 1.2907 (1.9219)	CeLoss 0.2344 (0.3087)	SegCLSLoss 0.0107 (0.0162)	KLLoss 0.3711 (0.3305)	MaskLoss 0.5067 (0.7861)	MaskBCELoss 0.1265 (0.1663)	MaskDICELoss 0.3802 (0.6198)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 23 is less than current step: 499. Dropping entry: {'train/loss': 1.2269729912281035, 'train/ce_loss': 0.298876953125, 'train/seg_cls_loss': 0.0125244140625, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.09010063521564007, 'train/mask_dice_loss': 0.3580782413482666, 'train/mask_loss': 0.4481788843870163, 'metrics/total_secs_per_batch': 7.25041651725769, 'metrics/data_secs_per_batch': 3.3051559209823607, '_timestamp': 1740975177.4299712}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 23 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975177.4302685}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 24 is less than current step: 499. Dropping entry: {'train/loss': 1.9219013094902038, 'train/ce_loss': 0.30869140625, 'train/seg_cls_loss': 0.016156005859375, 'train/kl_loss': 0.33046875, 'train/mask_bce_loss': 0.16630083080381156, 'train/mask_dice_loss': 0.6198451340198516, 'train/mask_loss': 0.786145955324173, 'metrics/total_secs_per_batch': 5.918833494186401, 'metrics/data_secs_per_batch': 2.7173433780670164, '_timestamp': 1740975183.3489525}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 24 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975183.34933}).
Epoch: [7][ 26/500]	Time  6.225 ( 6.225)	Loss 2.1098 (1.8467)	CeLoss 0.1973 (0.4949)	SegCLSLoss 0.0179 (0.0167)	KLLoss 0.3633 (0.2891)	MaskLoss 0.9338 (0.6573)	MaskBCELoss 0.1306 (0.0889)	MaskDICELoss 0.8032 (0.5684)
Epoch: [7][ 27/500]	Time  5.511 ( 5.511)	Loss 0.7969 (1.3922)	CeLoss 0.7969 (0.5548)	SegCLSLoss 0.0000 (0.0107)	KLLoss 0.0000 (0.1801)	MaskLoss 0.0000 (0.4069)	MaskBCELoss 0.0000 (0.1113)	MaskDICELoss 0.0000 (0.2957)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 25 is less than current step: 499. Dropping entry: {'train/loss': 1.8467158555984498, 'train/ce_loss': 0.494921875, 'train/seg_cls_loss': 0.016693115234375, 'train/kl_loss': 0.2890625, 'train/mask_bce_loss': 0.08885334501974285, 'train/mask_dice_loss': 0.5684401273727417, 'train/mask_loss': 0.6572934627532959, 'metrics/total_secs_per_batch': 6.224849224090576, 'metrics/data_secs_per_batch': 2.948513960838318, '_timestamp': 1740975189.5735586}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 25 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975189.5738506}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 26 is less than current step: 499. Dropping entry: {'train/loss': 1.392167866230011, 'train/ce_loss': 0.554833984375, 'train/seg_cls_loss': 0.0106689453125, 'train/kl_loss': 0.180078125, 'train/mask_bce_loss': 0.11129384934902191, 'train/mask_dice_loss': 0.2956543505191803, 'train/mask_loss': 0.40694819390773773, 'metrics/total_secs_per_batch': 5.511483669281006, 'metrics/data_secs_per_batch': 2.8220333576202394, '_timestamp': 1740975195.085081}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 26 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975195.0853631}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 27 is less than current step: 499. Dropping entry: {'train/loss': 1.7616516828536988, 'train/ce_loss': 0.478759765625, 'train/seg_cls_loss': 0.012811279296875, 'train/kl_loss': 0.255078125, 'train/mask_bce_loss': 0.143500935100019, 'train/mask_dice_loss': 0.4821002870798111, 'train/mask_loss': 0.6256012320518494, 'metrics/total_secs_per_batch': 5.910330295562744, 'metrics/data_secs_per_batch': 2.819878578186035, '_timestamp': 1740975200.9955728}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 27 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975200.9959333}).
Epoch: [7][ 28/500]	Time  5.910 ( 5.910)	Loss 1.4297 (1.7617)	CeLoss 1.4297 (0.4788)	SegCLSLoss 0.0000 (0.0128)	KLLoss 0.0000 (0.2551)	MaskLoss 0.0000 (0.6256)	MaskBCELoss 0.0000 (0.1435)	MaskDICELoss 0.0000 (0.4821)
Epoch: [7][ 29/500]	Time  7.013 ( 7.013)	Loss 0.8906 (1.6562)	CeLoss 0.8906 (0.2934)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.2949)	MaskLoss 0.0000 (0.6631)	MaskBCELoss 0.0000 (0.1696)	MaskDICELoss 0.0000 (0.4936)
[2025-03-02 22:13:33,195] [INFO] [logging.py:96:log_dist] [Rank 0] step=3530, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:13:33,201] [INFO] [timer.py:215:stop] epoch=0/micro_step=35300/global_step=3530, RunningAvgSamplesPerSec=1.4520048821926448, CurrSamplesPerSec=1.9260239089766644, MemAllocated=30.72GB, MaxMemAllocated=37.23GB
Epoch: [7][ 30/500]	Time  5.194 ( 5.194)	Loss 1.1641 (1.6373)	CeLoss 1.1641 (0.5402)	SegCLSLoss 0.0000 (0.0102)	KLLoss 0.0000 (0.2553)	MaskLoss 0.0000 (0.5332)	MaskBCELoss 0.0000 (0.1483)	MaskDICELoss 0.0000 (0.3849)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 28 is less than current step: 499. Dropping entry: {'train/loss': 1.6561755299568177, 'train/ce_loss': 0.293359375, 'train/seg_cls_loss': 0.0142333984375, 'train/kl_loss': 0.294921875, 'train/mask_bce_loss': 0.1695812962949276, 'train/mask_dice_loss': 0.49356506764888763, 'train/mask_loss': 0.663146361708641, 'metrics/total_secs_per_batch': 7.0130510330200195, 'metrics/data_secs_per_batch': 3.4184720277786256, '_timestamp': 1740975208.008499}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 28 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975208.008803}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 29 is less than current step: 499. Dropping entry: {'train/loss': 1.6372929632663726, 'train/ce_loss': 0.540234375, 'train/seg_cls_loss': 0.010235595703125, 'train/kl_loss': 0.2552734375, 'train/mask_bce_loss': 0.14829711467027665, 'train/mask_dice_loss': 0.3849001653492451, 'train/mask_loss': 0.5331972688436508, 'metrics/total_secs_per_batch': 5.19370436668396, 'metrics/data_secs_per_batch': 2.5360286235809326, '_timestamp': 1740975213.2019618}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 29 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975213.202235}).
Epoch: [7][ 31/500]	Time  6.681 ( 6.681)	Loss 1.0078 (1.8653)	CeLoss 1.0078 (0.3115)	SegCLSLoss 0.0000 (0.0147)	KLLoss 0.0000 (0.3275)	MaskLoss 0.0000 (0.7567)	MaskBCELoss 0.0000 (0.1721)	MaskDICELoss 0.0000 (0.5847)
Epoch: [7][ 32/500]	Time  7.292 ( 7.292)	Loss 1.9952 (2.0121)	CeLoss 0.2061 (0.3038)	SegCLSLoss 0.0181 (0.0140)	KLLoss 0.3711 (0.2922)	MaskLoss 0.8716 (0.8362)	MaskBCELoss 0.0104 (0.1769)	MaskDICELoss 0.8612 (0.6592)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 30 is less than current step: 499. Dropping entry: {'train/loss': 1.8653494715690613, 'train/ce_loss': 0.3115234375, 'train/seg_cls_loss': 0.014739990234375, 'train/kl_loss': 0.3275390625, 'train/mask_bce_loss': 0.1720546267228201, 'train/mask_dice_loss': 0.5846923679113388, 'train/mask_loss': 0.7567469984292984, 'metrics/total_secs_per_batch': 6.681442737579346, 'metrics/data_secs_per_batch': 3.258824014663696, '_timestamp': 1740975219.883601}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 30 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975219.883883}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 31 is less than current step: 499. Dropping entry: {'train/loss': 2.012083685398102, 'train/ce_loss': 0.30380859375, 'train/seg_cls_loss': 0.013958740234375, 'train/kl_loss': 0.2921875, 'train/mask_bce_loss': 0.17692424794659017, 'train/mask_dice_loss': 0.6592445552349091, 'train/mask_loss': 0.8361687958240509, 'metrics/total_secs_per_batch': 7.292446613311768, 'metrics/data_secs_per_batch': 3.449776291847229, '_timestamp': 1740975227.176407}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 31 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975227.176856}).
Epoch: [7][ 33/500]	Time  6.372 ( 6.372)	Loss 2.1255 (1.6519)	CeLoss 0.2363 (0.3982)	SegCLSLoss 0.0089 (0.0118)	KLLoss 0.3652 (0.2885)	MaskLoss 0.9241 (0.6094)	MaskBCELoss 0.2991 (0.1332)	MaskDICELoss 0.6250 (0.4762)
Epoch: [7][ 34/500]	Time  7.221 ( 7.221)	Loss 1.1546 (1.6544)	CeLoss 0.2031 (0.2941)	SegCLSLoss 0.0102 (0.0137)	KLLoss 0.3633 (0.3252)	MaskLoss 0.4553 (0.6606)	MaskBCELoss 0.1217 (0.1192)	MaskDICELoss 0.3335 (0.5414)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 32 is less than current step: 499. Dropping entry: {'train/loss': 1.6519322514533996, 'train/ce_loss': 0.3982421875, 'train/seg_cls_loss': 0.011846923828125, 'train/kl_loss': 0.2884765625, 'train/mask_bce_loss': 0.13324551340192556, 'train/mask_dice_loss': 0.47616787254810333, 'train/mask_loss': 0.6094133943319321, 'metrics/total_secs_per_batch': 6.371817588806152, 'metrics/data_secs_per_batch': 2.6735341787338256, '_timestamp': 1740975233.5479426}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 32 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975233.548286}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 33 is less than current step: 499. Dropping entry: {'train/loss': 1.6543673813343047, 'train/ce_loss': 0.294140625, 'train/seg_cls_loss': 0.013653564453125, 'train/kl_loss': 0.3251953125, 'train/mask_bce_loss': 0.11920925825834275, 'train/mask_dice_loss': 0.5413728758692742, 'train/mask_loss': 0.6605821259319782, 'metrics/total_secs_per_batch': 7.221355676651001, 'metrics/data_secs_per_batch': 3.20739860534668, '_timestamp': 1740975240.769329}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 33 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975240.7696416}).
Epoch: [7][ 35/500]	Time  6.204 ( 6.204)	Loss 0.9766 (1.6862)	CeLoss 0.9766 (0.3666)	SegCLSLoss 0.0000 (0.0135)	KLLoss 0.0000 (0.2916)	MaskLoss 0.0000 (0.6417)	MaskBCELoss 0.0000 (0.1617)	MaskDICELoss 0.0000 (0.4800)
Epoch: [7][ 36/500]	Time  6.961 ( 6.961)	Loss 1.9288 (1.5587)	CeLoss 0.2188 (0.2266)	SegCLSLoss 0.0176 (0.0146)	KLLoss 0.3555 (0.3641)	MaskLoss 0.8326 (0.6442)	MaskBCELoss 0.0089 (0.1153)	MaskDICELoss 0.8237 (0.5288)
Epoch: [7][ 37/500]	Time  5.515 ( 5.515)	Loss 1.1719 (1.4345)	CeLoss 1.1719 (0.6400)	SegCLSLoss 0.0000 (0.0075)	KLLoss 0.0000 (0.1816)	MaskLoss 0.0000 (0.3865)	MaskBCELoss 0.0000 (0.0255)	MaskDICELoss 0.0000 (0.3609)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 34 is less than current step: 499. Dropping entry: {'train/loss': 1.6862342476844787, 'train/ce_loss': 0.3666015625, 'train/seg_cls_loss': 0.013494873046875, 'train/kl_loss': 0.2916015625, 'train/mask_bce_loss': 0.16174169573932887, 'train/mask_dice_loss': 0.4799593985080719, 'train/mask_loss': 0.6417010962963104, 'metrics/total_secs_per_batch': 6.203649520874023, 'metrics/data_secs_per_batch': 2.8476043224334715, '_timestamp': 1740975246.9729035}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 34 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975246.9731808}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 35 is less than current step: 499. Dropping entry: {'train/loss': 1.5587271451950073, 'train/ce_loss': 0.2265625, 'train/seg_cls_loss': 0.0146240234375, 'train/kl_loss': 0.3640625, 'train/mask_bce_loss': 0.1153269677888602, 'train/mask_dice_loss': 0.5288315162062645, 'train/mask_loss': 0.644158485531807, 'metrics/total_secs_per_batch': 6.961367130279541, 'metrics/data_secs_per_batch': 3.0308218002319336, '_timestamp': 1740975253.9342444}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 35 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975253.9345405}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 36 is less than current step: 499. Dropping entry: {'train/loss': 1.4344548463821412, 'train/ce_loss': 0.6399658203125, 'train/seg_cls_loss': 0.007525634765625, 'train/kl_loss': 0.181640625, 'train/mask_bce_loss': 0.025512385508045556, 'train/mask_dice_loss': 0.36094110608100893, 'train/mask_loss': 0.3864534914493561, 'metrics/total_secs_per_batch': 5.51538610458374, 'metrics/data_secs_per_batch': 2.507645583152771, '_timestamp': 1740975259.4496577}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 36 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975259.4499722}).
Epoch: [7][ 38/500]	Time  6.250 ( 6.250)	Loss 1.6530 (1.3225)	CeLoss 0.2070 (0.3794)	SegCLSLoss 0.0138 (0.0117)	KLLoss 0.3613 (0.2186)	MaskLoss 0.7015 (0.4577)	MaskBCELoss 0.1103 (0.1180)	MaskDICELoss 0.5912 (0.3397)
Epoch: [7][ 39/500]	Time  5.607 ( 5.607)	Loss 1.3984 (1.7982)	CeLoss 1.3984 (0.5380)	SegCLSLoss 0.0000 (0.0143)	KLLoss 0.0000 (0.2547)	MaskLoss 0.0000 (0.6138)	MaskBCELoss 0.0000 (0.1181)	MaskDICELoss 0.0000 (0.4956)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 37 is less than current step: 499. Dropping entry: {'train/loss': 1.3224872946739197, 'train/ce_loss': 0.37939453125, 'train/seg_cls_loss': 0.01168212890625, 'train/kl_loss': 0.2185546875, 'train/mask_bce_loss': 0.11801904141902923, 'train/mask_dice_loss': 0.3397089898586273, 'train/mask_loss': 0.4577280282974243, 'metrics/total_secs_per_batch': 6.249579668045044, 'metrics/data_secs_per_batch': 3.001053881645203, '_timestamp': 1740975265.6992135}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 37 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975265.6995037}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 38 is less than current step: 499. Dropping entry: {'train/loss': 1.798212993144989, 'train/ce_loss': 0.538037109375, 'train/seg_cls_loss': 0.014288330078125, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.1181260284036398, 'train/mask_dice_loss': 0.49562891721725466, 'train/mask_loss': 0.6137549459934235, 'metrics/total_secs_per_batch': 5.607147455215454, 'metrics/data_secs_per_batch': 2.5906147003173827, '_timestamp': 1740975271.3064656}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 38 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975271.3067837}).
[2025-03-02 22:14:37,494] [INFO] [logging.py:96:log_dist] [Rank 0] step=3540, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:14:37,500] [INFO] [timer.py:215:stop] epoch=0/micro_step=35400/global_step=3540, RunningAvgSamplesPerSec=1.452307411842628, CurrSamplesPerSec=1.6147773216381727, MemAllocated=30.68GB, MaxMemAllocated=37.23GB
Epoch: [7][ 40/500]	Time  6.195 ( 6.195)	Loss 1.5469 (2.5137)	CeLoss 1.5469 (0.5282)	SegCLSLoss 0.0000 (0.0139)	KLLoss 0.0000 (0.2531)	MaskLoss 0.0000 (0.9767)	MaskBCELoss 0.0000 (0.4558)	MaskDICELoss 0.0000 (0.5208)
Epoch: [7][ 41/500]	Time  5.876 ( 5.876)	Loss 1.6432 (1.7408)	CeLoss 0.2061 (0.4939)	SegCLSLoss 0.0256 (0.0139)	KLLoss 0.3652 (0.2520)	MaskLoss 0.6937 (0.6073)	MaskBCELoss 0.0164 (0.0779)	MaskDICELoss 0.6773 (0.5294)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 39 is less than current step: 499. Dropping entry: {'train/loss': 2.5137349128723145, 'train/ce_loss': 0.52822265625, 'train/seg_cls_loss': 0.01390380859375, 'train/kl_loss': 0.253125, 'train/mask_bce_loss': 0.4558441072702408, 'train/mask_dice_loss': 0.5208475708961486, 'train/mask_loss': 0.9766916692256927, 'metrics/total_secs_per_batch': 6.194572925567627, 'metrics/data_secs_per_batch': 2.93838951587677, '_timestamp': 1740975277.5007987}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 39 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975277.501112}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 40 is less than current step: 499. Dropping entry: {'train/loss': 1.7407991647720338, 'train/ce_loss': 0.4939453125, 'train/seg_cls_loss': 0.013916015625, 'train/kl_loss': 0.251953125, 'train/mask_bce_loss': 0.07786430651322007, 'train/mask_dice_loss': 0.5294005095958709, 'train/mask_loss': 0.6072648167610168, 'metrics/total_secs_per_batch': 5.8761374950408936, 'metrics/data_secs_per_batch': 2.3877862215042116, '_timestamp': 1740975283.3770876}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 40 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975283.377371}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 41 is less than current step: 499. Dropping entry: {'train/loss': 1.9373013496398925, 'train/ce_loss': 0.35810546875, 'train/seg_cls_loss': 0.013720703125, 'train/kl_loss': 0.328515625, 'train/mask_bce_loss': 0.13842842485755683, 'train/mask_dice_loss': 0.6311499699950218, 'train/mask_loss': 0.7695783972740173, 'metrics/total_secs_per_batch': 6.833676815032959, 'metrics/data_secs_per_batch': 2.9673129320144653, '_timestamp': 1740975290.2108722}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 41 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975290.2111902}).
Epoch: [7][ 42/500]	Time  6.834 ( 6.834)	Loss 2.2514 (1.9373)	CeLoss 0.2373 (0.3581)	SegCLSLoss 0.0212 (0.0137)	KLLoss 0.3516 (0.3285)	MaskLoss 0.9841 (0.7696)	MaskBCELoss 0.1525 (0.1384)	MaskDICELoss 0.8316 (0.6311)
Epoch: [7][ 43/500]	Time  4.951 ( 4.951)	Loss 2.1239 (1.5581)	CeLoss 0.2109 (0.6515)	SegCLSLoss 0.0354 (0.0123)	KLLoss 0.3594 (0.2531)	MaskLoss 0.9301 (0.4375)	MaskBCELoss 0.0159 (0.0293)	MaskDICELoss 0.9142 (0.4082)
Epoch: [7][ 44/500]	Time  6.652 ( 6.652)	Loss 1.4872 (1.7987)	CeLoss 0.2295 (0.2941)	SegCLSLoss 0.0172 (0.0172)	KLLoss 0.3535 (0.3314)	MaskLoss 0.6069 (0.7313)	MaskBCELoss 0.1180 (0.1804)	MaskDICELoss 0.4889 (0.5509)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 42 is less than current step: 499. Dropping entry: {'train/loss': 1.5580759346485138, 'train/ce_loss': 0.65146484375, 'train/seg_cls_loss': 0.012335205078125, 'train/kl_loss': 0.253125, 'train/mask_bce_loss': 0.029314669966697692, 'train/mask_dice_loss': 0.408170560747385, 'train/mask_loss': 0.4374852284789085, 'metrics/total_secs_per_batch': 4.950606107711792, 'metrics/data_secs_per_batch': 2.169032669067383, '_timestamp': 1740975295.1615608}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 42 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975295.1619108}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 43 is less than current step: 499. Dropping entry: {'train/loss': 1.7986698150634766, 'train/ce_loss': 0.294140625, 'train/seg_cls_loss': 0.01719970703125, 'train/kl_loss': 0.3314453125, 'train/mask_bce_loss': 0.1803812573198229, 'train/mask_dice_loss': 0.5509360820055008, 'train/mask_loss': 0.7313173443078995, 'metrics/total_secs_per_batch': 6.651546478271484, 'metrics/data_secs_per_batch': 2.8277968168258667, '_timestamp': 1740975301.8129563}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 43 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975301.8132892}).
Epoch: [7][ 45/500]	Time  6.440 ( 6.440)	Loss 1.1250 (1.5327)	CeLoss 1.1250 (0.2865)	SegCLSLoss 0.0000 (0.0143)	KLLoss 0.0000 (0.2920)	MaskLoss 0.0000 (0.6049)	MaskBCELoss 0.0000 (0.1294)	MaskDICELoss 0.0000 (0.4755)
Epoch: [7][ 46/500]	Time  6.329 ( 6.329)	Loss 2.5223 (1.8377)	CeLoss 0.1982 (0.4983)	SegCLSLoss 0.0264 (0.0151)	KLLoss 0.3555 (0.2932)	MaskLoss 1.1381 (0.6512)	MaskBCELoss 0.2313 (0.1704)	MaskDICELoss 0.9068 (0.4808)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 44 is less than current step: 499. Dropping entry: {'train/loss': 1.5327349781990052, 'train/ce_loss': 0.2865234375, 'train/seg_cls_loss': 0.014263916015625, 'train/kl_loss': 0.2919921875, 'train/mask_bce_loss': 0.12938798405230045, 'train/mask_dice_loss': 0.47550489008426666, 'train/mask_loss': 0.6048928678035737, 'metrics/total_secs_per_batch': 6.439744710922241, 'metrics/data_secs_per_batch': 3.2259437322616575, '_timestamp': 1740975308.2526848}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 44 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975308.25292}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 45 is less than current step: 499. Dropping entry: {'train/loss': 1.8376969814300537, 'train/ce_loss': 0.49833984375, 'train/seg_cls_loss': 0.01513671875, 'train/kl_loss': 0.2931640625, 'train/mask_bce_loss': 0.17037940509617328, 'train/mask_dice_loss': 0.48084212094545364, 'train/mask_loss': 0.6512215346097946, 'metrics/total_secs_per_batch': 6.328593015670776, 'metrics/data_secs_per_batch': 3.126651668548584, '_timestamp': 1740975314.5814836}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 45 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975314.5819168}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 46 is less than current step: 499. Dropping entry: {'train/loss': 1.6793712019920348, 'train/ce_loss': 0.602197265625, 'train/seg_cls_loss': 0.01226806640625, 'train/kl_loss': 0.182421875, 'train/mask_bce_loss': 0.11586955664679408, 'train/mask_dice_loss': 0.410583633184433, 'train/mask_loss': 0.5264531791210174, 'metrics/total_secs_per_batch': 5.702948808670044, 'metrics/data_secs_per_batch': 2.761408233642578, '_timestamp': 1740975320.2842476}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 46 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975320.28453}).
Epoch: [7][ 47/500]	Time  5.703 ( 5.703)	Loss 1.2734 (1.6794)	CeLoss 1.2734 (0.6022)	SegCLSLoss 0.0000 (0.0123)	KLLoss 0.0000 (0.1824)	MaskLoss 0.0000 (0.5265)	MaskBCELoss 0.0000 (0.1159)	MaskDICELoss 0.0000 (0.4106)
Epoch: [7][ 48/500]	Time  7.398 ( 7.398)	Loss 2.4474 (1.9249)	CeLoss 0.2354 (0.2337)	SegCLSLoss 0.0112 (0.0180)	KLLoss 0.3633 (0.3586)	MaskLoss 1.0850 (0.8232)	MaskBCELoss 0.3151 (0.0860)	MaskDICELoss 0.7699 (0.7372)
Epoch: [7][ 49/500]	Time  6.072 ( 6.072)	Loss 1.4904 (1.4371)	CeLoss 0.2773 (0.3927)	SegCLSLoss 0.0189 (0.0089)	KLLoss 0.3633 (0.2182)	MaskLoss 0.5831 (0.5089)	MaskBCELoss 0.1418 (0.1152)	MaskDICELoss 0.4413 (0.3937)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 47 is less than current step: 499. Dropping entry: {'train/loss': 1.9248630285263062, 'train/ce_loss': 0.23369140625, 'train/seg_cls_loss': 0.01796875, 'train/kl_loss': 0.35859375, 'train/mask_bce_loss': 0.0859833910362795, 'train/mask_dice_loss': 0.7371903032064437, 'train/mask_loss': 0.8231736987829208, 'metrics/total_secs_per_batch': 7.397717475891113, 'metrics/data_secs_per_batch': 3.380404734611511, '_timestamp': 1740975327.681947}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 47 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975327.6822276}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 48 is less than current step: 499. Dropping entry: {'train/loss': 1.4371212005615235, 'train/ce_loss': 0.39267578125, 'train/seg_cls_loss': 0.00889892578125, 'train/kl_loss': 0.2181640625, 'train/mask_bce_loss': 0.11523044016212225, 'train/mask_dice_loss': 0.3936621904373169, 'train/mask_loss': 0.5088926315307617, 'metrics/total_secs_per_batch': 6.071871280670166, 'metrics/data_secs_per_batch': 2.6624706268310545, '_timestamp': 1740975333.7538018}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 48 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975333.7540958}).
[2025-03-02 22:15:39,588] [INFO] [logging.py:96:log_dist] [Rank 0] step=3550, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:15:39,594] [INFO] [timer.py:215:stop] epoch=0/micro_step=35500/global_step=3550, RunningAvgSamplesPerSec=1.452710270190921, CurrSamplesPerSec=1.7124697343728967, MemAllocated=31.28GB, MaxMemAllocated=37.23GB
Epoch: [7][ 50/500]	Time  5.841 ( 5.841)	Loss 2.5158 (1.6338)	CeLoss 0.2422 (0.4601)	SegCLSLoss 0.0179 (0.0137)	KLLoss 0.3672 (0.2902)	MaskLoss 1.1134 (0.5689)	MaskBCELoss 0.3197 (0.1010)	MaskDICELoss 0.7936 (0.4679)
Epoch: [7][ 51/500]	Time  6.588 ( 6.588)	Loss 2.6161 (1.8438)	CeLoss 0.1875 (0.2133)	SegCLSLoss 0.0232 (0.0153)	KLLoss 0.3965 (0.3299)	MaskLoss 1.1889 (0.7949)	MaskBCELoss 0.2625 (0.2043)	MaskDICELoss 0.9264 (0.5906)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 49 is less than current step: 499. Dropping entry: {'train/loss': 1.6338013589382172, 'train/ce_loss': 0.46005859375, 'train/seg_cls_loss': 0.013714599609375, 'train/kl_loss': 0.290234375, 'train/mask_bce_loss': 0.10104627916589379, 'train/mask_dice_loss': 0.46785635203123094, 'train/mask_loss': 0.5689026325941086, 'metrics/total_secs_per_batch': 5.8410608768463135, 'metrics/data_secs_per_batch': 2.5218489170074463, '_timestamp': 1740975339.5947773}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 49 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975339.5951214}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 50 is less than current step: 499. Dropping entry: {'train/loss': 1.8437692642211914, 'train/ce_loss': 0.21328125, 'train/seg_cls_loss': 0.01533203125, 'train/kl_loss': 0.3298828125, 'train/mask_bce_loss': 0.20426116064190863, 'train/mask_dice_loss': 0.5906215302646161, 'train/mask_loss': 0.7948826789855957, 'metrics/total_secs_per_batch': 6.587542533874512, 'metrics/data_secs_per_batch': 3.019187307357788, '_timestamp': 1740975346.182468}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 50 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975346.1827514}).
Epoch: [7][ 52/500]	Time  5.772 ( 5.772)	Loss 1.6812 (1.8626)	CeLoss 0.2480 (0.6448)	SegCLSLoss 0.0121 (0.0091)	KLLoss 0.3594 (0.2199)	MaskLoss 0.6961 (0.5955)	MaskBCELoss 0.2326 (0.1846)	MaskDICELoss 0.4635 (0.4109)
Epoch: [7][ 53/500]	Time  5.714 ( 5.714)	Loss 1.1172 (1.3469)	CeLoss 0.2227 (0.4301)	SegCLSLoss 0.0166 (0.0132)	KLLoss 0.3730 (0.2201)	MaskLoss 0.4248 (0.4442)	MaskBCELoss 0.1111 (0.0952)	MaskDICELoss 0.3137 (0.3490)
Epoch: [7][ 54/500]	Time  6.507 ( 6.507)	Loss 1.0625 (1.8431)	CeLoss 1.0625 (0.3207)	SegCLSLoss 0.0000 (0.0138)	KLLoss 0.0000 (0.3266)	MaskLoss 0.0000 (0.7412)	MaskBCELoss 0.0000 (0.1998)	MaskDICELoss 0.0000 (0.5415)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 51 is less than current step: 499. Dropping entry: {'train/loss': 1.8626033425331117, 'train/ce_loss': 0.64482421875, 'train/seg_cls_loss': 0.009100341796875, 'train/kl_loss': 0.219921875, 'train/mask_bce_loss': 0.18462910428643226, 'train/mask_dice_loss': 0.4108815401792526, 'train/mask_loss': 0.5955106556415558, 'metrics/total_secs_per_batch': 5.772143363952637, 'metrics/data_secs_per_batch': 2.2755839824676514, '_timestamp': 1740975351.9545712}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 51 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975351.954852}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 52 is less than current step: 499. Dropping entry: {'train/loss': 1.3468671202659608, 'train/ce_loss': 0.4301025390625, 'train/seg_cls_loss': 0.01322021484375, 'train/kl_loss': 0.2201171875, 'train/mask_bce_loss': 0.09518501348793507, 'train/mask_dice_loss': 0.3489883065223694, 'train/mask_loss': 0.4441733092069626, 'metrics/total_secs_per_batch': 5.713511228561401, 'metrics/data_secs_per_batch': 2.5244803190231324, '_timestamp': 1740975357.6683033}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 52 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975357.668653}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 53 is less than current step: 499. Dropping entry: {'train/loss': 1.8431094884872437, 'train/ce_loss': 0.320703125, 'train/seg_cls_loss': 0.013848876953125, 'train/kl_loss': 0.3265625, 'train/mask_bce_loss': 0.19975130688399076, 'train/mask_dice_loss': 0.5414811611175537, 'train/mask_loss': 0.7412324756383896, 'metrics/total_secs_per_batch': 6.5067548751831055, 'metrics/data_secs_per_batch': 3.019550895690918, '_timestamp': 1740975364.1748774}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 53 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975364.175168}).
Epoch: [7][ 55/500]	Time  6.390 ( 6.390)	Loss 2.5671 (1.9765)	CeLoss 0.1719 (0.2873)	SegCLSLoss 0.0188 (0.0158)	KLLoss 0.3730 (0.3291)	MaskLoss 1.1742 (0.8242)	MaskBCELoss 0.2533 (0.1780)	MaskDICELoss 0.9209 (0.6462)
Epoch: [7][ 56/500]	Time  6.860 ( 6.860)	Loss 1.8749 (1.4765)	CeLoss 0.3066 (0.3713)	SegCLSLoss 0.0182 (0.0109)	KLLoss 0.3535 (0.2514)	MaskLoss 0.7617 (0.5372)	MaskBCELoss 0.0306 (0.1161)	MaskDICELoss 0.7310 (0.4210)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 54 is less than current step: 499. Dropping entry: {'train/loss': 1.9765275716781616, 'train/ce_loss': 0.2873046875, 'train/seg_cls_loss': 0.0158447265625, 'train/kl_loss': 0.3291015625, 'train/mask_bce_loss': 0.17799247596412898, 'train/mask_dice_loss': 0.6462088227272034, 'train/mask_loss': 0.8242012917995453, 'metrics/total_secs_per_batch': 6.39005446434021, 'metrics/data_secs_per_batch': 2.9229045629501345, '_timestamp': 1740975370.5650747}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 54 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975370.565424}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 55 is less than current step: 499. Dropping entry: {'train/loss': 1.4764690071344375, 'train/ce_loss': 0.371337890625, 'train/seg_cls_loss': 0.010888671875, 'train/kl_loss': 0.2513671875, 'train/mask_bce_loss': 0.11614935621619224, 'train/mask_dice_loss': 0.42103535011410714, 'train/mask_loss': 0.5371846958994866, 'metrics/total_secs_per_batch': 6.859973192214966, 'metrics/data_secs_per_batch': 3.4738463163375854, '_timestamp': 1740975377.424879}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 55 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975377.4251566}).
Epoch: [7][ 57/500]	Time  6.284 ( 6.284)	Loss 1.5312 (1.4832)	CeLoss 1.5312 (0.4769)	SegCLSLoss 0.0000 (0.0104)	KLLoss 0.0000 (0.2545)	MaskLoss 0.0000 (0.4878)	MaskBCELoss 0.0000 (0.0897)	MaskDICELoss 0.0000 (0.3981)
Epoch: [7][ 58/500]	Time  6.857 ( 6.857)	Loss 1.4703 (1.8613)	CeLoss 0.2422 (0.2192)	SegCLSLoss 0.0096 (0.0177)	KLLoss 0.3594 (0.3295)	MaskLoss 0.5935 (0.8001)	MaskBCELoss 0.3142 (0.1578)	MaskDICELoss 0.2793 (0.6422)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 56 is less than current step: 499. Dropping entry: {'train/loss': 1.4831511974334717, 'train/ce_loss': 0.47685546875, 'train/seg_cls_loss': 0.010430908203125, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.08973823375999927, 'train/mask_dice_loss': 0.39807759821414945, 'train/mask_loss': 0.4878158360719681, 'metrics/total_secs_per_batch': 6.28399395942688, 'metrics/data_secs_per_batch': 3.22125449180603, '_timestamp': 1740975383.709075}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 56 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975383.7094152}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 57 is less than current step: 499. Dropping entry: {'train/loss': 1.861251986026764, 'train/ce_loss': 0.21923828125, 'train/seg_cls_loss': 0.0177001953125, 'train/kl_loss': 0.3294921875, 'train/mask_bce_loss': 0.15784626137465238, 'train/mask_dice_loss': 0.6422133326530457, 'train/mask_loss': 0.8000596046447754, 'metrics/total_secs_per_batch': 6.856692790985107, 'metrics/data_secs_per_batch': 3.245118498802185, '_timestamp': 1740975390.5656016}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 57 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975390.5658875}).
Epoch: [7][ 59/500]	Time  6.756 ( 6.756)	Loss 2.1914 (1.9122)	CeLoss 0.2617 (0.2979)	SegCLSLoss 0.0138 (0.0166)	KLLoss 0.3594 (0.3242)	MaskLoss 0.9434 (0.7869)	MaskBCELoss 0.2471 (0.1566)	MaskDICELoss 0.6963 (0.6302)
[2025-03-02 22:16:43,354] [INFO] [logging.py:96:log_dist] [Rank 0] step=3560, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:16:43,359] [INFO] [timer.py:215:stop] epoch=0/micro_step=35600/global_step=3560, RunningAvgSamplesPerSec=1.4530118953937052, CurrSamplesPerSec=1.656460335271075, MemAllocated=31.26GB, MaxMemAllocated=37.23GB
Epoch: [7][ 60/500]	Time  6.039 ( 6.039)	Loss 2.2601 (1.6285)	CeLoss 0.2178 (0.4197)	SegCLSLoss 0.0198 (0.0129)	KLLoss 0.3555 (0.2520)	MaskLoss 0.9982 (0.5885)	MaskBCELoss 0.0256 (0.0698)	MaskDICELoss 0.9726 (0.5187)
Epoch: [7][ 61/500]	Time  5.998 ( 5.998)	Loss 0.5927 (1.3615)	CeLoss 0.2598 (0.5294)	SegCLSLoss 0.0097 (0.0088)	KLLoss 0.3652 (0.2176)	MaskLoss 0.1460 (0.4030)	MaskBCELoss 0.0421 (0.0604)	MaskDICELoss 0.1039 (0.3426)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 58 is less than current step: 499. Dropping entry: {'train/loss': 1.912239384651184, 'train/ce_loss': 0.29794921875, 'train/seg_cls_loss': 0.016595458984375, 'train/kl_loss': 0.32421875, 'train/mask_bce_loss': 0.15664109708741308, 'train/mask_dice_loss': 0.6302402928471565, 'train/mask_loss': 0.7868813872337341, 'metrics/total_secs_per_batch': 6.7562689781188965, 'metrics/data_secs_per_batch': 2.682883071899414, '_timestamp': 1740975397.321856}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 58 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975397.3221817}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 59 is less than current step: 499. Dropping entry: {'train/loss': 1.6284722566604615, 'train/ce_loss': 0.4197021484375, 'train/seg_cls_loss': 0.01290283203125, 'train/kl_loss': 0.251953125, 'train/mask_bce_loss': 0.06978595126420259, 'train/mask_dice_loss': 0.5187299638986588, 'train/mask_loss': 0.5885159254074097, 'metrics/total_secs_per_batch': 6.03860068321228, 'metrics/data_secs_per_batch': 2.6748011827468874, '_timestamp': 1740975403.3603523}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 59 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975403.360712}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 60 is less than current step: 499. Dropping entry: {'train/loss': 1.3615160942077638, 'train/ce_loss': 0.52939453125, 'train/seg_cls_loss': 0.008819580078125, 'train/kl_loss': 0.217578125, 'train/mask_bce_loss': 0.060442838445305826, 'train/mask_dice_loss': 0.34258084148168566, 'train/mask_loss': 0.40302367210388185, 'metrics/total_secs_per_batch': 5.997723817825317, 'metrics/data_secs_per_batch': 2.474552321434021, '_timestamp': 1740975409.3582637}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 60 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975409.3585541}).
Epoch: [7][ 62/500]	Time  5.014 ( 5.014)	Loss 1.0391 (1.7883)	CeLoss 1.0391 (0.8234)	SegCLSLoss 0.0000 (0.0091)	KLLoss 0.0000 (0.1840)	MaskLoss 0.0000 (0.4709)	MaskBCELoss 0.0000 (0.1029)	MaskDICELoss 0.0000 (0.3680)
Epoch: [7][ 63/500]	Time  5.433 ( 5.433)	Loss 2.4854 (1.3522)	CeLoss 0.2227 (0.6519)	SegCLSLoss 0.0204 (0.0080)	KLLoss 0.3555 (0.1795)	MaskLoss 1.1089 (0.3391)	MaskBCELoss 0.2547 (0.0568)	MaskDICELoss 0.8542 (0.2823)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 61 is less than current step: 499. Dropping entry: {'train/loss': 1.7882818341255189, 'train/ce_loss': 0.8234375, 'train/seg_cls_loss': 0.00911865234375, 'train/kl_loss': 0.183984375, 'train/mask_bce_loss': 0.10286515764892101, 'train/mask_dice_loss': 0.36803357005119325, 'train/mask_loss': 0.4708987295627594, 'metrics/total_secs_per_batch': 5.013887882232666, 'metrics/data_secs_per_batch': 2.4033636093139648, '_timestamp': 1740975414.372092}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 61 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975414.3723717}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 62 is less than current step: 499. Dropping entry: {'train/loss': 1.3522020697593689, 'train/ce_loss': 0.65185546875, 'train/seg_cls_loss': 0.008026123046875, 'train/kl_loss': 0.1794921875, 'train/mask_bce_loss': 0.05678606890141964, 'train/mask_dice_loss': 0.282303249835968, 'train/mask_loss': 0.33908931612968446, 'metrics/total_secs_per_batch': 5.433139324188232, 'metrics/data_secs_per_batch': 2.051161789894104, '_timestamp': 1740975419.8052247}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 62 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975419.8055136}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 63 is less than current step: 499. Dropping entry: {'train/loss': 1.6316142439842225, 'train/ce_loss': 0.316015625, 'train/seg_cls_loss': 0.013922119140625, 'train/kl_loss': 0.3291015625, 'train/mask_bce_loss': 0.14238234870135785, 'train/mask_dice_loss': 0.4955927610397339, 'train/mask_loss': 0.6379750981926918, 'metrics/total_secs_per_batch': 6.460809946060181, 'metrics/data_secs_per_batch': 2.8993447542190554, '_timestamp': 1740975426.2662337}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 63 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975426.2665808}).
Epoch: [7][ 64/500]	Time  6.461 ( 6.461)	Loss 0.5644 (1.6316)	CeLoss 0.2363 (0.3160)	SegCLSLoss 0.0095 (0.0139)	KLLoss 0.3711 (0.3291)	MaskLoss 0.1435 (0.6380)	MaskBCELoss 0.0390 (0.1424)	MaskDICELoss 0.1045 (0.4956)
Epoch: [7][ 65/500]	Time  4.950 ( 4.950)	Loss 2.2570 (1.5828)	CeLoss 0.2598 (0.4393)	SegCLSLoss 0.0204 (0.0140)	KLLoss 0.3496 (0.2908)	MaskLoss 0.9761 (0.5538)	MaskBCELoss 0.0145 (0.0928)	MaskDICELoss 0.9616 (0.4609)
Epoch: [7][ 66/500]	Time  6.572 ( 6.572)	Loss 0.8362 (1.2236)	CeLoss 0.2080 (0.4421)	SegCLSLoss 0.0103 (0.0092)	KLLoss 0.3672 (0.2217)	MaskLoss 0.2931 (0.3773)	MaskBCELoss 0.1117 (0.0927)	MaskDICELoss 0.1814 (0.2846)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 64 is less than current step: 499. Dropping entry: {'train/loss': 1.582772135734558, 'train/ce_loss': 0.4392578125, 'train/seg_cls_loss': 0.0139892578125, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.09284159913659096, 'train/mask_dice_loss': 0.46094681322574615, 'train/mask_loss': 0.5537883996963501, 'metrics/total_secs_per_batch': 4.949803590774536, 'metrics/data_secs_per_batch': 1.8202341556549073, '_timestamp': 1740975431.215846}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 64 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975431.2161233}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 65 is less than current step: 499. Dropping entry: {'train/loss': 1.2235708355903625, 'train/ce_loss': 0.442138671875, 'train/seg_cls_loss': 0.009222412109375, 'train/kl_loss': 0.2216796875, 'train/mask_bce_loss': 0.09268193691968918, 'train/mask_dice_loss': 0.2846308171749115, 'train/mask_loss': 0.3773127555847168, 'metrics/total_secs_per_batch': 6.571710109710693, 'metrics/data_secs_per_batch': 2.478945517539978, '_timestamp': 1740975437.7877853}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 65 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975437.7881968}).
Epoch: [7][ 67/500]	Time  5.913 ( 5.913)	Loss 1.6124 (1.5218)	CeLoss 0.2197 (0.4683)	SegCLSLoss 0.0214 (0.0100)	KLLoss 0.3477 (0.2520)	MaskLoss 0.6734 (0.5116)	MaskBCELoss 0.0143 (0.0592)	MaskDICELoss 0.6591 (0.4523)
Epoch: [7][ 68/500]	Time  7.026 ( 7.026)	Loss 1.6214 (1.4779)	CeLoss 0.2559 (0.1896)	SegCLSLoss 0.0142 (0.0103)	KLLoss 0.3594 (0.2521)	MaskLoss 0.6613 (0.6290)	MaskBCELoss 0.0191 (0.1268)	MaskDICELoss 0.6422 (0.5023)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 66 is less than current step: 499. Dropping entry: {'train/loss': 1.5217661738395691, 'train/ce_loss': 0.46826171875, 'train/seg_cls_loss': 0.010003662109375, 'train/kl_loss': 0.251953125, 'train/mask_bce_loss': 0.05922139899339527, 'train/mask_dice_loss': 0.452345272898674, 'train/mask_loss': 0.51156667470932, 'metrics/total_secs_per_batch': 5.913299322128296, 'metrics/data_secs_per_batch': 2.717993664741516, '_timestamp': 1740975443.7010918}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 66 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975443.7014525}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 67 is less than current step: 499. Dropping entry: {'train/loss': 1.4779024124145508, 'train/ce_loss': 0.1896484375, 'train/seg_cls_loss': 0.01025390625, 'train/kl_loss': 0.2521484375, 'train/mask_bce_loss': 0.1267642811872065, 'train/mask_dice_loss': 0.5022748112678528, 'train/mask_loss': 0.6290390849113464, 'metrics/total_secs_per_batch': 7.025654077529907, 'metrics/data_secs_per_batch': 3.11160352230072, '_timestamp': 1740975450.726571}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 67 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975450.7268596}).
Epoch: [7][ 69/500]	Time  5.857 ( 5.857)	Loss 1.5547 (1.8155)	CeLoss 1.5547 (0.6415)	SegCLSLoss 0.0000 (0.0086)	KLLoss 0.0000 (0.2195)	MaskLoss 0.0000 (0.5738)	MaskBCELoss 0.0000 (0.1630)	MaskDICELoss 0.0000 (0.4108)
[2025-03-02 22:17:42,644] [INFO] [logging.py:96:log_dist] [Rank 0] step=3570, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:17:42,650] [INFO] [timer.py:215:stop] epoch=0/micro_step=35700/global_step=3570, RunningAvgSamplesPerSec=1.4535769434396066, CurrSamplesPerSec=1.6486133529182057, MemAllocated=31.58GB, MaxMemAllocated=37.23GB
Epoch: [7][ 70/500]	Time  6.067 ( 6.067)	Loss 0.4004 (1.6857)	CeLoss 0.4004 (0.4358)	SegCLSLoss 0.0000 (0.0128)	KLLoss 0.0000 (0.2510)	MaskLoss 0.0000 (0.6091)	MaskBCELoss 0.0000 (0.0949)	MaskDICELoss 0.0000 (0.5142)
Epoch: [7][ 71/500]	Time  5.993 ( 5.993)	Loss 1.1833 (1.4354)	CeLoss 0.1924 (0.3426)	SegCLSLoss 0.0135 (0.0138)	KLLoss 0.3633 (0.2877)	MaskLoss 0.4740 (0.5286)	MaskBCELoss 0.0010 (0.0550)	MaskDICELoss 0.4730 (0.4735)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 68 is less than current step: 499. Dropping entry: {'train/loss': 1.8154990255832673, 'train/ce_loss': 0.64150390625, 'train/seg_cls_loss': 0.0086181640625, 'train/kl_loss': 0.21953125, 'train/mask_bce_loss': 0.16299299597740174, 'train/mask_dice_loss': 0.4108209729194641, 'train/mask_loss': 0.5738139659166336, 'metrics/total_secs_per_batch': 5.8570356369018555, 'metrics/data_secs_per_batch': 2.752734565734863, '_timestamp': 1740975456.5834434}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 68 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975456.5836985}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 69 is less than current step: 499. Dropping entry: {'train/loss': 1.6856562316417694, 'train/ce_loss': 0.43583984375, 'train/seg_cls_loss': 0.01282958984375, 'train/kl_loss': 0.2509765625, 'train/mask_bce_loss': 0.0948813202790916, 'train/mask_dice_loss': 0.5142065569758415, 'train/mask_loss': 0.6090878695249557, 'metrics/total_secs_per_batch': 6.0671234130859375, 'metrics/data_secs_per_batch': 2.7008541107177733, '_timestamp': 1740975462.6504626}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 69 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975462.6506493}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 70 is less than current step: 499. Dropping entry: {'train/loss': 1.4354214191436767, 'train/ce_loss': 0.342578125, 'train/seg_cls_loss': 0.01376953125, 'train/kl_loss': 0.2876953125, 'train/mask_bce_loss': 0.05504705832572654, 'train/mask_dice_loss': 0.47350349724292756, 'train/mask_loss': 0.5285505592823029, 'metrics/total_secs_per_batch': 5.993028879165649, 'metrics/data_secs_per_batch': 2.9855416297912596, '_timestamp': 1740975468.6438222}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 70 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975468.6441493}).
Epoch: [7][ 72/500]	Time  5.808 ( 5.808)	Loss 1.4055 (1.8066)	CeLoss 0.2617 (0.5227)	SegCLSLoss 0.0087 (0.0138)	KLLoss 0.3633 (0.2941)	MaskLoss 0.5514 (0.6238)	MaskBCELoss 0.0895 (0.1246)	MaskDICELoss 0.4619 (0.4993)
Epoch: [7][ 73/500]	Time  6.930 ( 6.930)	Loss 0.2969 (1.2624)	CeLoss 0.2969 (0.3082)	SegCLSLoss 0.0000 (0.0097)	KLLoss 0.0000 (0.2553)	MaskLoss 0.0000 (0.4617)	MaskBCELoss 0.0000 (0.0719)	MaskDICELoss 0.0000 (0.3898)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 71 is less than current step: 499. Dropping entry: {'train/loss': 1.8065876960754395, 'train/ce_loss': 0.52265625, 'train/seg_cls_loss': 0.013800048828125, 'train/kl_loss': 0.294140625, 'train/mask_bce_loss': 0.12455162424594164, 'train/mask_dice_loss': 0.4992500454187393, 'train/mask_loss': 0.6238016664981842, 'metrics/total_secs_per_batch': 5.80826997756958, 'metrics/data_secs_per_batch': 2.392964172363281, '_timestamp': 1740975474.4520273}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 71 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975474.4523318}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 72 is less than current step: 499. Dropping entry: {'train/loss': 1.2624104499816895, 'train/ce_loss': 0.308203125, 'train/seg_cls_loss': 0.009735107421875, 'train/kl_loss': 0.2552734375, 'train/mask_bce_loss': 0.07188205211423337, 'train/mask_dice_loss': 0.38984074369072913, 'train/mask_loss': 0.46172279119491577, 'metrics/total_secs_per_batch': 6.93038010597229, 'metrics/data_secs_per_batch': 2.908934736251831, '_timestamp': 1740975481.3823955}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 72 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975481.382681}).
Epoch: [7][ 74/500]	Time  6.870 ( 6.870)	Loss 1.3261 (1.1013)	CeLoss 0.2422 (0.2041)	SegCLSLoss 0.0113 (0.0128)	KLLoss 0.3711 (0.2898)	MaskLoss 0.5205 (0.4309)	MaskBCELoss 0.2141 (0.1373)	MaskDICELoss 0.3064 (0.2936)
Epoch: [7][ 75/500]	Time  6.691 ( 6.691)	Loss 2.1480 (1.6605)	CeLoss 0.1650 (0.2380)	SegCLSLoss 0.0258 (0.0143)	KLLoss 0.3516 (0.2861)	MaskLoss 0.9676 (0.6933)	MaskBCELoss 0.0111 (0.0844)	MaskDICELoss 0.9565 (0.6089)
Epoch: [7][ 76/500]	Time  5.549 ( 5.549)	Loss 0.1680 (1.5932)	CeLoss 0.1680 (0.5292)	SegCLSLoss 0.0000 (0.0103)	KLLoss 0.0000 (0.1779)	MaskLoss 0.0000 (0.5206)	MaskBCELoss 0.0000 (0.1084)	MaskDICELoss 0.0000 (0.4121)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 73 is less than current step: 499. Dropping entry: {'train/loss': 1.1013169169425965, 'train/ce_loss': 0.204052734375, 'train/seg_cls_loss': 0.01275634765625, 'train/kl_loss': 0.28984375, 'train/mask_bce_loss': 0.1372881418094039, 'train/mask_dice_loss': 0.2935705125331879, 'train/mask_loss': 0.4308586522936821, 'metrics/total_secs_per_batch': 6.870450973510742, 'metrics/data_secs_per_batch': 2.9941012382507326, '_timestamp': 1740975488.2528417}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 73 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975488.2531898}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 74 is less than current step: 499. Dropping entry: {'train/loss': 1.6604851961135865, 'train/ce_loss': 0.238037109375, 'train/seg_cls_loss': 0.01429443359375, 'train/kl_loss': 0.2861328125, 'train/mask_bce_loss': 0.08436788623221218, 'train/mask_dice_loss': 0.608936220407486, 'train/mask_loss': 0.6933041095733643, 'metrics/total_secs_per_batch': 6.690647840499878, 'metrics/data_secs_per_batch': 2.9684865951538084, '_timestamp': 1740975494.943464}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 74 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975494.9437397}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 75 is less than current step: 499. Dropping entry: {'train/loss': 1.593190360069275, 'train/ce_loss': 0.52919921875, 'train/seg_cls_loss': 0.010260009765625, 'train/kl_loss': 0.1779296875, 'train/mask_bce_loss': 0.10844220854341984, 'train/mask_dice_loss': 0.4121275722980499, 'train/mask_loss': 0.5205697894096375, 'metrics/total_secs_per_batch': 5.549262046813965, 'metrics/data_secs_per_batch': 2.5421451807022093, '_timestamp': 1740975500.4927232}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 75 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975500.4930182}).
Epoch: [7][ 77/500]	Time  6.594 ( 6.594)	Loss 1.3367 (1.6096)	CeLoss 0.2578 (0.4436)	SegCLSLoss 0.0129 (0.0118)	KLLoss 0.3594 (0.2904)	MaskLoss 0.5189 (0.5657)	MaskBCELoss 0.1495 (0.1429)	MaskDICELoss 0.3694 (0.4227)
Epoch: [7][ 78/500]	Time  6.105 ( 6.105)	Loss 1.3910 (1.8119)	CeLoss 0.2715 (0.3812)	SegCLSLoss 0.0099 (0.0122)	KLLoss 0.3633 (0.2576)	MaskLoss 0.5383 (0.6993)	MaskBCELoss 0.0771 (0.2503)	MaskDICELoss 0.4611 (0.4490)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 76 is less than current step: 499. Dropping entry: {'train/loss': 1.6096437662839889, 'train/ce_loss': 0.4435546875, 'train/seg_cls_loss': 0.01180419921875, 'train/kl_loss': 0.2904296875, 'train/mask_bce_loss': 0.14293299419805408, 'train/mask_dice_loss': 0.42272872626781466, 'train/mask_loss': 0.5656617268919945, 'metrics/total_secs_per_batch': 6.594381093978882, 'metrics/data_secs_per_batch': 3.360087537765503, '_timestamp': 1740975507.0871809}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 76 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975507.0874996}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 77 is less than current step: 499. Dropping entry: {'train/loss': 1.811941909790039, 'train/ce_loss': 0.38125, 'train/seg_cls_loss': 0.012176513671875, 'train/kl_loss': 0.2576171875, 'train/mask_bce_loss': 0.2502715644426644, 'train/mask_dice_loss': 0.4490099310874939, 'train/mask_loss': 0.6992815017700196, 'metrics/total_secs_per_batch': 6.104506731033325, 'metrics/data_secs_per_batch': 2.8343050718307494, '_timestamp': 1740975513.1917675}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 77 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975513.192162}).
Epoch: [7][ 79/500]	Time  5.171 ( 5.171)	Loss 1.5324 (1.6416)	CeLoss 0.2354 (0.6686)	SegCLSLoss 0.0132 (0.0095)	KLLoss 0.3633 (0.1811)	MaskLoss 0.6266 (0.4750)	MaskBCELoss 0.1009 (0.0707)	MaskDICELoss 0.5257 (0.4043)
[2025-03-02 22:18:43,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=3580, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:18:43,294] [INFO] [timer.py:215:stop] epoch=0/micro_step=35800/global_step=3580, RunningAvgSamplesPerSec=1.4540592153838436, CurrSamplesPerSec=2.028040143774433, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [7][ 80/500]	Time  4.933 ( 4.933)	Loss 1.5183 (1.5100)	CeLoss 0.2520 (0.5676)	SegCLSLoss 0.0109 (0.0106)	KLLoss 0.3672 (0.1832)	MaskLoss 0.6117 (0.4593)	MaskBCELoss 0.1676 (0.1224)	MaskDICELoss 0.4441 (0.3369)
Epoch: [7][ 81/500]	Time  7.554 ( 7.554)	Loss 0.2832 (1.4287)	CeLoss 0.2832 (0.2208)	SegCLSLoss 0.0000 (0.0119)	KLLoss 0.0000 (0.2916)	MaskLoss 0.0000 (0.5863)	MaskBCELoss 0.0000 (0.0614)	MaskDICELoss 0.0000 (0.5249)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 78 is less than current step: 499. Dropping entry: {'train/loss': 1.6415696024894715, 'train/ce_loss': 0.6685791015625, 'train/seg_cls_loss': 0.009490966796875, 'train/kl_loss': 0.1810546875, 'train/mask_bce_loss': 0.07072150930762292, 'train/mask_dice_loss': 0.40429915189743043, 'train/mask_loss': 0.47502065896987916, 'metrics/total_secs_per_batch': 5.1707823276519775, 'metrics/data_secs_per_batch': 2.1637035608291626, '_timestamp': 1740975518.3624153}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 78 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975518.3627663}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 79 is less than current step: 499. Dropping entry: {'train/loss': 1.5099908232688903, 'train/ce_loss': 0.5675537109375, 'train/seg_cls_loss': 0.010614013671875, 'train/kl_loss': 0.183203125, 'train/mask_bce_loss': 0.12239083759486676, 'train/mask_dice_loss': 0.3369136661291122, 'train/mask_loss': 0.4593044936656952, 'metrics/total_secs_per_batch': 4.932521820068359, 'metrics/data_secs_per_batch': 2.1567875623703, '_timestamp': 1740975523.294747}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 79 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975523.2950344}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 80 is less than current step: 499. Dropping entry: {'train/loss': 1.4287079453468323, 'train/ce_loss': 0.220751953125, 'train/seg_cls_loss': 0.0118896484375, 'train/kl_loss': 0.2916015625, 'train/mask_bce_loss': 0.06137370588257909, 'train/mask_dice_loss': 0.5248796880245209, 'train/mask_loss': 0.5862533897161484, 'metrics/total_secs_per_batch': 7.55441427230835, 'metrics/data_secs_per_batch': 3.3839885234832763, '_timestamp': 1740975530.8493526}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 80 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975530.8496277}).
Epoch: [7][ 82/500]	Time  6.741 ( 6.741)	Loss 2.4812 (1.8759)	CeLoss 0.1504 (0.3054)	SegCLSLoss 0.0219 (0.0160)	KLLoss 0.3770 (0.2898)	MaskLoss 1.1410 (0.7667)	MaskBCELoss 0.2217 (0.0933)	MaskDICELoss 0.9193 (0.6733)
Epoch: [7][ 83/500]	Time  6.388 ( 6.388)	Loss 1.9764 (1.8271)	CeLoss 0.1738 (0.3958)	SegCLSLoss 0.0199 (0.0143)	KLLoss 0.3652 (0.2930)	MaskLoss 0.8779 (0.6973)	MaskBCELoss 0.1904 (0.1189)	MaskDICELoss 0.6874 (0.5784)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 81 is less than current step: 499. Dropping entry: {'train/loss': 1.87587651014328, 'train/ce_loss': 0.305419921875, 'train/seg_cls_loss': 0.0160400390625, 'train/kl_loss': 0.28984375, 'train/mask_bce_loss': 0.09333249237388372, 'train/mask_dice_loss': 0.6733411133289338, 'train/mask_loss': 0.76667360663414, 'metrics/total_secs_per_batch': 6.740752458572388, 'metrics/data_secs_per_batch': 3.0279852151870728, '_timestamp': 1740975537.5901248}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 81 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975537.590399}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 82 is less than current step: 499. Dropping entry: {'train/loss': 1.8270998120307922, 'train/ce_loss': 0.39580078125, 'train/seg_cls_loss': 0.014251708984375, 'train/kl_loss': 0.29296875, 'train/mask_bce_loss': 0.11891286121681333, 'train/mask_dice_loss': 0.5783772945404053, 'train/mask_loss': 0.6972901463508606, 'metrics/total_secs_per_batch': 6.388017416000366, 'metrics/data_secs_per_batch': 2.9930493116378782, '_timestamp': 1740975543.978339}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 82 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975543.978686}).
Epoch: [7][ 84/500]	Time  5.513 ( 5.513)	Loss 0.5555 (1.0815)	CeLoss 0.2637 (0.3868)	SegCLSLoss 0.0157 (0.0086)	KLLoss 0.3594 (0.1785)	MaskLoss 0.1244 (0.3363)	MaskBCELoss 0.0511 (0.0234)	MaskDICELoss 0.0733 (0.3129)
Epoch: [7][ 85/500]	Time  6.625 ( 6.625)	Loss 1.7459 (1.7160)	CeLoss 0.2256 (0.3708)	SegCLSLoss 0.0147 (0.0132)	KLLoss 0.3633 (0.2910)	MaskLoss 0.7382 (0.6547)	MaskBCELoss 0.0134 (0.0932)	MaskDICELoss 0.7247 (0.5615)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 83 is less than current step: 499. Dropping entry: {'train/loss': 1.081495815515518, 'train/ce_loss': 0.386767578125, 'train/seg_cls_loss': 0.008575439453125, 'train/kl_loss': 0.178515625, 'train/mask_bce_loss': 0.023435581475496292, 'train/mask_dice_loss': 0.31289338171482084, 'train/mask_loss': 0.3363289624452591, 'metrics/total_secs_per_batch': 5.512996196746826, 'metrics/data_secs_per_batch': 2.3959869384765624, '_timestamp': 1740975549.491224}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 83 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975549.4916036}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 84 is less than current step: 499. Dropping entry: {'train/loss': 1.716003954410553, 'train/ce_loss': 0.37080078125, 'train/seg_cls_loss': 0.013238525390625, 'train/kl_loss': 0.291015625, 'train/mask_bce_loss': 0.09323929836973548, 'train/mask_dice_loss': 0.561491197347641, 'train/mask_loss': 0.654730498790741, 'metrics/total_secs_per_batch': 6.625104904174805, 'metrics/data_secs_per_batch': 2.9569648265838624, '_timestamp': 1740975556.1164727}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 84 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975556.116839}).
Epoch: [7][ 86/500]	Time  6.746 ( 6.746)	Loss 1.7316 (1.6983)	CeLoss 0.2852 (0.2095)	SegCLSLoss 0.0134 (0.0157)	KLLoss 0.3594 (0.2881)	MaskLoss 0.7027 (0.7261)	MaskBCELoss 0.2830 (0.1199)	MaskDICELoss 0.4197 (0.6062)
Epoch: [7][ 87/500]	Time  6.095 ( 6.095)	Loss 1.2201 (1.2458)	CeLoss 0.2373 (0.3643)	SegCLSLoss 0.0099 (0.0102)	KLLoss 0.3691 (0.2186)	MaskLoss 0.4704 (0.4273)	MaskBCELoss 0.2880 (0.1143)	MaskDICELoss 0.1824 (0.3130)
Epoch: [7][ 88/500]	Time  5.940 ( 5.940)	Loss 1.2109 (1.7462)	CeLoss 1.2109 (0.5290)	SegCLSLoss 0.0000 (0.0120)	KLLoss 0.0000 (0.2582)	MaskLoss 0.0000 (0.5927)	MaskBCELoss 0.0000 (0.1097)	MaskDICELoss 0.0000 (0.4830)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 85 is less than current step: 499. Dropping entry: {'train/loss': 1.6983310222625732, 'train/ce_loss': 0.209521484375, 'train/seg_cls_loss': 0.015667724609375, 'train/kl_loss': 0.2880859375, 'train/mask_bce_loss': 0.11989529244601727, 'train/mask_dice_loss': 0.606247752904892, 'train/mask_loss': 0.7261430561542511, 'metrics/total_secs_per_batch': 6.746349811553955, 'metrics/data_secs_per_batch': 3.0625287294387817, '_timestamp': 1740975562.862683}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 85 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975562.862998}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 86 is less than current step: 499. Dropping entry: {'train/loss': 1.245779800415039, 'train/ce_loss': 0.3642578125, 'train/seg_cls_loss': 0.01021728515625, 'train/kl_loss': 0.2185546875, 'train/mask_bce_loss': 0.11429480426013469, 'train/mask_dice_loss': 0.3129896193742752, 'train/mask_loss': 0.4272844299674034, 'metrics/total_secs_per_batch': 6.095212936401367, 'metrics/data_secs_per_batch': 2.617295575141907, '_timestamp': 1740975568.957854}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 86 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975568.9582012}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 87 is less than current step: 499. Dropping entry: {'train/loss': 1.746172308921814, 'train/ce_loss': 0.52900390625, 'train/seg_cls_loss': 0.012030029296875, 'train/kl_loss': 0.258203125, 'train/mask_bce_loss': 0.10969156753271818, 'train/mask_dice_loss': 0.48302349746227263, 'train/mask_loss': 0.592715060710907, 'metrics/total_secs_per_batch': 5.939525365829468, 'metrics/data_secs_per_batch': 2.7552603006362917, '_timestamp': 1740975574.8975034}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 87 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975574.8978446}).
Epoch: [7][ 89/500]	Time  5.221 ( 5.221)	Loss 1.1641 (1.6533)	CeLoss 1.1641 (0.6938)	SegCLSLoss 0.0000 (0.0084)	KLLoss 0.0000 (0.1822)	MaskLoss 0.0000 (0.4685)	MaskBCELoss 0.0000 (0.1456)	MaskDICELoss 0.0000 (0.3229)
[2025-03-02 22:19:46,265] [INFO] [logging.py:96:log_dist] [Rank 0] step=3590, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:19:46,270] [INFO] [timer.py:215:stop] epoch=0/micro_step=35900/global_step=3590, RunningAvgSamplesPerSec=1.4544016748921402, CurrSamplesPerSec=1.625694246905213, MemAllocated=31.26GB, MaxMemAllocated=37.23GB
Epoch: [7][ 90/500]	Time  6.153 ( 6.153)	Loss 1.5022 (1.2823)	CeLoss 0.1689 (0.3927)	SegCLSLoss 0.0280 (0.0123)	KLLoss 0.3828 (0.2545)	MaskLoss 0.6407 (0.4290)	MaskBCELoss 0.0840 (0.0403)	MaskDICELoss 0.5567 (0.3887)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 88 is less than current step: 499. Dropping entry: {'train/loss': 1.6532648086547852, 'train/ce_loss': 0.69384765625, 'train/seg_cls_loss': 0.00838623046875, 'train/kl_loss': 0.1822265625, 'train/mask_bce_loss': 0.14561150185763835, 'train/mask_dice_loss': 0.32291542291641234, 'train/mask_loss': 0.4685269296169281, 'metrics/total_secs_per_batch': 5.220903635025024, 'metrics/data_secs_per_batch': 2.321281361579895, '_timestamp': 1740975580.118308}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 88 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975580.1185958}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 89 is less than current step: 499. Dropping entry: {'train/loss': 1.2822604417800902, 'train/ce_loss': 0.392724609375, 'train/seg_cls_loss': 0.012347412109375, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.04033286487683654, 'train/mask_dice_loss': 0.3887124091386795, 'train/mask_loss': 0.429045270383358, 'metrics/total_secs_per_batch': 6.152936935424805, 'metrics/data_secs_per_batch': 2.8612176895141603, '_timestamp': 1740975586.271089}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 89 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975586.271409}).
Epoch: [7][ 91/500]	Time  6.470 ( 6.470)	Loss 1.5015 (1.3628)	CeLoss 0.2812 (0.3230)	SegCLSLoss 0.0112 (0.0129)	KLLoss 0.3613 (0.2895)	MaskLoss 0.5887 (0.5020)	MaskBCELoss 0.1088 (0.1069)	MaskDICELoss 0.4798 (0.3951)
Epoch: [7][ 92/500]	Time  6.087 ( 6.087)	Loss 1.6986 (1.8346)	CeLoss 0.2578 (0.3145)	SegCLSLoss 0.0146 (0.0137)	KLLoss 0.3848 (0.3277)	MaskLoss 0.6970 (0.7403)	MaskBCELoss 0.1128 (0.2413)	MaskDICELoss 0.5842 (0.4990)
Epoch: [7][ 93/500]	Time  6.039 ( 6.039)	Loss 0.7311 (1.3852)	CeLoss 0.2324 (0.5235)	SegCLSLoss 0.0106 (0.0114)	KLLoss 0.3574 (0.2529)	MaskLoss 0.2288 (0.4153)	MaskBCELoss 0.0478 (0.0485)	MaskDICELoss 0.1810 (0.3667)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 90 is less than current step: 499. Dropping entry: {'train/loss': 1.3627618670463562, 'train/ce_loss': 0.3229736328125, 'train/seg_cls_loss': 0.012884521484375, 'train/kl_loss': 0.289453125, 'train/mask_bce_loss': 0.10692384978756309, 'train/mask_dice_loss': 0.3950991630554199, 'train/mask_loss': 0.5020230144262314, 'metrics/total_secs_per_batch': 6.469738245010376, 'metrics/data_secs_per_batch': 3.0867406606674193, '_timestamp': 1740975592.7410038}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 90 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975592.7413309}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 91 is less than current step: 499. Dropping entry: {'train/loss': 1.834615421295166, 'train/ce_loss': 0.314453125, 'train/seg_cls_loss': 0.013739013671875, 'train/kl_loss': 0.327734375, 'train/mask_bce_loss': 0.24126573260873557, 'train/mask_dice_loss': 0.4990400329232216, 'train/mask_loss': 0.7403057664632797, 'metrics/total_secs_per_batch': 6.086815357208252, 'metrics/data_secs_per_batch': 2.8377935886383057, '_timestamp': 1740975598.827859}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 91 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975598.8281596}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 92 is less than current step: 499. Dropping entry: {'train/loss': 1.3851770997047423, 'train/ce_loss': 0.52353515625, 'train/seg_cls_loss': 0.011419677734375, 'train/kl_loss': 0.2529296875, 'train/mask_bce_loss': 0.04854404255747795, 'train/mask_dice_loss': 0.36674958243966105, 'train/mask_loss': 0.41529362350702287, 'metrics/total_secs_per_batch': 6.039309978485107, 'metrics/data_secs_per_batch': 2.4406868696212767, '_timestamp': 1740975604.8672931}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 92 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975604.8676229}).
Epoch: [7][ 94/500]	Time  6.441 ( 6.441)	Loss 0.6992 (1.3204)	CeLoss 0.6992 (0.2736)	SegCLSLoss 0.0000 (0.0131)	KLLoss 0.0000 (0.2900)	MaskLoss 0.0000 (0.5056)	MaskBCELoss 0.0000 (0.0233)	MaskDICELoss 0.0000 (0.4823)
Epoch: [7][ 95/500]	Time  7.047 ( 7.047)	Loss 1.2940 (1.6608)	CeLoss 0.2256 (0.2153)	SegCLSLoss 0.0121 (0.0178)	KLLoss 0.3633 (0.3285)	MaskLoss 0.5132 (0.7020)	MaskBCELoss 0.1117 (0.1153)	MaskDICELoss 0.4015 (0.5867)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 93 is less than current step: 499. Dropping entry: {'train/loss': 1.3204456925392152, 'train/ce_loss': 0.2736328125, 'train/seg_cls_loss': 0.01312255859375, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.02325508650392294, 'train/mask_dice_loss': 0.4823290675878525, 'train/mask_loss': 0.5055841594934464, 'metrics/total_secs_per_batch': 6.441312313079834, 'metrics/data_secs_per_batch': 3.0100390911102295, '_timestamp': 1740975611.308504}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 93 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975611.3088422}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 94 is less than current step: 499. Dropping entry: {'train/loss': 1.6607506275177002, 'train/ce_loss': 0.2153076171875, 'train/seg_cls_loss': 0.017791748046875, 'train/kl_loss': 0.328515625, 'train/mask_bce_loss': 0.11532101836055517, 'train/mask_dice_loss': 0.5866973489522934, 'train/mask_loss': 0.7020183801651001, 'metrics/total_secs_per_batch': 7.047084808349609, 'metrics/data_secs_per_batch': 2.8778152227401734, '_timestamp': 1740975618.3556595}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 94 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975618.3560174}).
Epoch: [7][ 96/500]	Time  4.957 ( 4.957)	Loss 2.1442 (1.4098)	CeLoss 0.3770 (0.7134)	SegCLSLoss 0.0109 (0.0078)	KLLoss 0.3613 (0.1801)	MaskLoss 0.8631 (0.3372)	MaskBCELoss 0.1971 (0.0929)	MaskDICELoss 0.6660 (0.2443)
Epoch: [7][ 97/500]	Time  6.305 ( 6.305)	Loss 2.5236 (1.7347)	CeLoss 0.1216 (0.3287)	SegCLSLoss 0.0261 (0.0117)	KLLoss 0.3789 (0.2559)	MaskLoss 1.1754 (0.6873)	MaskBCELoss 0.3671 (0.1873)	MaskDICELoss 0.8083 (0.5000)
Epoch: [7][ 98/500]	Time  6.357 ( 6.357)	Loss 1.3638 (2.0052)	CeLoss 0.1992 (0.4347)	SegCLSLoss 0.0181 (0.0144)	KLLoss 0.3613 (0.2951)	MaskLoss 0.5598 (0.7669)	MaskBCELoss 0.0260 (0.1686)	MaskDICELoss 0.5338 (0.5983)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 95 is less than current step: 499. Dropping entry: {'train/loss': 1.4097503840923309, 'train/ce_loss': 0.71337890625, 'train/seg_cls_loss': 0.007781982421875, 'train/kl_loss': 0.180078125, 'train/mask_bce_loss': 0.09294293373823166, 'train/mask_dice_loss': 0.24425648674368858, 'train/mask_loss': 0.33719941675662995, 'metrics/total_secs_per_batch': 4.956578969955444, 'metrics/data_secs_per_batch': 2.201687788963318, '_timestamp': 1740975623.3121407}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 95 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975623.3124523}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 96 is less than current step: 499. Dropping entry: {'train/loss': 1.734706664085388, 'train/ce_loss': 0.3287109375, 'train/seg_cls_loss': 0.011749267578125, 'train/kl_loss': 0.255859375, 'train/mask_bce_loss': 0.18727779760956764, 'train/mask_dice_loss': 0.4999729812145233, 'train/mask_loss': 0.6872507870197296, 'metrics/total_secs_per_batch': 6.305455923080444, 'metrics/data_secs_per_batch': 2.794143724441528, '_timestamp': 1740975629.6175125}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 96 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975629.6177886}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 97 is less than current step: 499. Dropping entry: {'train/loss': 2.0051698684692383, 'train/ce_loss': 0.43466796875, 'train/seg_cls_loss': 0.014447021484375, 'train/kl_loss': 0.2951171875, 'train/mask_bce_loss': 0.16861116606742144, 'train/mask_dice_loss': 0.5982804119586944, 'train/mask_loss': 0.7668915808200836, 'metrics/total_secs_per_batch': 6.3570027351379395, 'metrics/data_secs_per_batch': 2.8623606681823732, '_timestamp': 1740975635.9747212}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 97 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975635.9750903}).
Epoch: [7][ 99/500]	Time  5.819 ( 5.819)	Loss 0.9805 (1.6335)	CeLoss 0.9805 (0.3994)	SegCLSLoss 0.0000 (0.0154)	KLLoss 0.0000 (0.2895)	MaskLoss 0.0000 (0.5987)	MaskBCELoss 0.0000 (0.0766)	MaskDICELoss 0.0000 (0.5220)
[2025-03-02 22:20:48,399] [INFO] [logging.py:96:log_dist] [Rank 0] step=3600, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:20:48,405] [INFO] [timer.py:215:stop] epoch=0/micro_step=36000/global_step=3600, RunningAvgSamplesPerSec=1.4547918590009534, CurrSamplesPerSec=1.512525773550914, MemAllocated=30.7GB, MaxMemAllocated=37.23GB
Epoch: [7][100/500]	Time  6.613 ( 6.613)	Loss 1.6641 (1.7090)	CeLoss 1.6641 (0.3532)	SegCLSLoss 0.0000 (0.0131)	KLLoss 0.0000 (0.2939)	MaskLoss 0.0000 (0.6597)	MaskBCELoss 0.0000 (0.1067)	MaskDICELoss 0.0000 (0.5530)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 98 is less than current step: 499. Dropping entry: {'train/loss': 1.6335391998291016, 'train/ce_loss': 0.3994140625, 'train/seg_cls_loss': 0.015423583984375, 'train/kl_loss': 0.289453125, 'train/mask_bce_loss': 0.0766349732875824, 'train/mask_dice_loss': 0.5220193818211556, 'train/mask_loss': 0.5986543595790863, 'metrics/total_secs_per_batch': 5.818816661834717, 'metrics/data_secs_per_batch': 2.807321000099182, '_timestamp': 1740975641.7933857}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 98 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975641.793663}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 99 is less than current step: 499. Dropping entry: {'train/loss': 1.7090373039245605, 'train/ce_loss': 0.35322265625, 'train/seg_cls_loss': 0.013134765625, 'train/kl_loss': 0.2939453125, 'train/mask_bce_loss': 0.10669734156690538, 'train/mask_dice_loss': 0.5530459254980087, 'train/mask_loss': 0.6597432732582093, 'metrics/total_secs_per_batch': 6.613081932067871, 'metrics/data_secs_per_batch': 3.149109959602356, '_timestamp': 1740975648.4063327}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 99 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975648.4066732}).
Epoch: [7][101/500]	Time  6.183 ( 6.183)	Loss 1.1725 (1.6743)	CeLoss 0.3691 (0.4596)	SegCLSLoss 0.0137 (0.0116)	KLLoss 0.3613 (0.2918)	MaskLoss 0.3812 (0.5900)	MaskBCELoss 0.2052 (0.1114)	MaskDICELoss 0.1760 (0.4786)
Epoch: [7][102/500]	Time  5.572 ( 5.572)	Loss 0.8438 (1.1539)	CeLoss 0.8438 (0.4442)	SegCLSLoss 0.0000 (0.0088)	KLLoss 0.0000 (0.2166)	MaskLoss 0.0000 (0.3418)	MaskBCELoss 0.0000 (0.0993)	MaskDICELoss 0.0000 (0.2425)
Epoch: [7][103/500]	Time  5.942 ( 5.942)	Loss 1.3154 (1.8655)	CeLoss 0.2129 (0.3371)	SegCLSLoss 0.0150 (0.0152)	KLLoss 0.3652 (0.2906)	MaskLoss 0.5288 (0.7458)	MaskBCELoss 0.0788 (0.1618)	MaskDICELoss 0.4500 (0.5841)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 100 is less than current step: 499. Dropping entry: {'train/loss': 1.6742833256721497, 'train/ce_loss': 0.4595703125, 'train/seg_cls_loss': 0.0115966796875, 'train/kl_loss': 0.291796875, 'train/mask_bce_loss': 0.11139940116554499, 'train/mask_dice_loss': 0.4785742834210396, 'train/mask_loss': 0.5899736821651459, 'metrics/total_secs_per_batch': 6.182581186294556, 'metrics/data_secs_per_batch': 2.676096200942993, '_timestamp': 1740975654.589037}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 100 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975654.5893266}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 101 is less than current step: 499. Dropping entry: {'train/loss': 1.1538554787635804, 'train/ce_loss': 0.44423828125, 'train/seg_cls_loss': 0.00877685546875, 'train/kl_loss': 0.2166015625, 'train/mask_bce_loss': 0.09926419258117676, 'train/mask_dice_loss': 0.2425072930753231, 'train/mask_loss': 0.34177148938179014, 'metrics/total_secs_per_batch': 5.572386264801025, 'metrics/data_secs_per_batch': 2.6682172775268556, '_timestamp': 1740975660.1614592}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 101 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975660.1617482}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 102 is less than current step: 499. Dropping entry: {'train/loss': 1.8654536843299865, 'train/ce_loss': 0.337109375, 'train/seg_cls_loss': 0.015179443359375, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.16176172224804758, 'train/mask_dice_loss': 0.5840510457754136, 'train/mask_loss': 0.7458127677440644, 'metrics/total_secs_per_batch': 5.9423651695251465, 'metrics/data_secs_per_batch': 2.7652060985565186, '_timestamp': 1740975666.1039813}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 102 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975666.1043408}).
Epoch: [7][104/500]	Time  6.210 ( 6.210)	Loss 1.7444 (1.8169)	CeLoss 0.3320 (0.3532)	SegCLSLoss 0.0097 (0.0139)	KLLoss 0.3574 (0.2910)	MaskLoss 0.6867 (0.7138)	MaskBCELoss 0.0750 (0.1003)	MaskDICELoss 0.6117 (0.6135)
Epoch: [7][105/500]	Time  5.973 ( 5.973)	Loss 1.9296 (1.7988)	CeLoss 0.2041 (0.4267)	SegCLSLoss 0.0176 (0.0157)	KLLoss 0.3555 (0.2932)	MaskLoss 0.8408 (0.6675)	MaskBCELoss 0.0274 (0.0787)	MaskDICELoss 0.8134 (0.5888)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 103 is less than current step: 499. Dropping entry: {'train/loss': 1.8168859958648682, 'train/ce_loss': 0.35322265625, 'train/seg_cls_loss': 0.013885498046875, 'train/kl_loss': 0.291015625, 'train/mask_bce_loss': 0.10025318134576082, 'train/mask_dice_loss': 0.6135120809078216, 'train/mask_loss': 0.7137652575969696, 'metrics/total_secs_per_batch': 6.209722995758057, 'metrics/data_secs_per_batch': 2.8914417028427124, '_timestamp': 1740975672.3135252}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 103 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975672.3137197}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 104 is less than current step: 499. Dropping entry: {'train/loss': 1.7988227486610413, 'train/ce_loss': 0.42666015625, 'train/seg_cls_loss': 0.01568603515625, 'train/kl_loss': 0.2931640625, 'train/mask_bce_loss': 0.0787415822967887, 'train/mask_dice_loss': 0.5887850314378739, 'train/mask_loss': 0.6675266116857529, 'metrics/total_secs_per_batch': 5.973472356796265, 'metrics/data_secs_per_batch': 2.5526572465896606, '_timestamp': 1740975678.2870233}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 104 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975678.287318}).
Epoch: [7][106/500]	Time  5.022 ( 5.022)	Loss 0.9688 (1.4449)	CeLoss 0.9688 (0.4827)	SegCLSLoss 0.0000 (0.0094)	KLLoss 0.0000 (0.2172)	MaskLoss 0.0000 (0.4678)	MaskBCELoss 0.0000 (0.0898)	MaskDICELoss 0.0000 (0.3781)
Epoch: [7][107/500]	Time  5.067 ( 5.067)	Loss 2.1310 (1.8962)	CeLoss 0.2334 (0.4419)	SegCLSLoss 0.0182 (0.0123)	KLLoss 0.3594 (0.2873)	MaskLoss 0.9268 (0.7098)	MaskBCELoss 0.1329 (0.2254)	MaskDICELoss 0.7939 (0.4844)
Epoch: [7][108/500]	Time  7.432 ( 7.432)	Loss 0.1377 (2.1430)	CeLoss 0.1377 (0.2030)	SegCLSLoss 0.0000 (0.0147)	KLLoss 0.0000 (0.2896)	MaskLoss 0.0000 (0.9518)	MaskBCELoss 0.0000 (0.2739)	MaskDICELoss 0.0000 (0.6779)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 105 is less than current step: 499. Dropping entry: {'train/loss': 1.4448569893836976, 'train/ce_loss': 0.48271484375, 'train/seg_cls_loss': 0.009368896484375, 'train/kl_loss': 0.2171875, 'train/mask_bce_loss': 0.08978570327162742, 'train/mask_dice_loss': 0.3780529499053955, 'train/mask_loss': 0.46783865094184873, 'metrics/total_secs_per_batch': 5.022200107574463, 'metrics/data_secs_per_batch': 2.2947038888931273, '_timestamp': 1740975683.3091986}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 105 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975683.3094797}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 106 is less than current step: 499. Dropping entry: {'train/loss': 1.8961642622947692, 'train/ce_loss': 0.44189453125, 'train/seg_cls_loss': 0.012261962890625, 'train/kl_loss': 0.2873046875, 'train/mask_bce_loss': 0.22539645601063968, 'train/mask_dice_loss': 0.48440440744161606, 'train/mask_loss': 0.7098008751869201, 'metrics/total_secs_per_batch': 5.066890239715576, 'metrics/data_secs_per_batch': 2.164462113380432, '_timestamp': 1740975688.3761513}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 106 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975688.3764591}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 107 is less than current step: 499. Dropping entry: {'train/loss': 2.1430060863494873, 'train/ce_loss': 0.202978515625, 'train/seg_cls_loss': 0.014697265625, 'train/kl_loss': 0.2896484375, 'train/mask_bce_loss': 0.273925005691126, 'train/mask_dice_loss': 0.6779247283935547, 'train/mask_loss': 0.9518497228622437, 'metrics/total_secs_per_batch': 7.432387351989746, 'metrics/data_secs_per_batch': 3.2930529832839968, '_timestamp': 1740975695.8086135}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 107 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975695.8089416}).
Epoch: [7][109/500]	Time  7.505 ( 7.505)	Loss 1.6256 (1.1356)	CeLoss 0.3145 (0.3434)	SegCLSLoss 0.0124 (0.0075)	KLLoss 0.3672 (0.1807)	MaskLoss 0.6341 (0.3851)	MaskBCELoss 0.2359 (0.0661)	MaskDICELoss 0.3982 (0.3190)
[2025-03-02 22:21:48,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=3610, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:21:48,940] [INFO] [timer.py:215:stop] epoch=0/micro_step=36100/global_step=3610, RunningAvgSamplesPerSec=1.4552739935606034, CurrSamplesPerSec=1.7775308834200978, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [7][110/500]	Time  5.627 ( 5.627)	Loss 2.1460 (1.3002)	CeLoss 0.2871 (0.4511)	SegCLSLoss 0.0126 (0.0117)	KLLoss 0.3633 (0.2199)	MaskLoss 0.9079 (0.4107)	MaskBCELoss 0.0292 (0.0342)	MaskDICELoss 0.8787 (0.3764)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 108 is less than current step: 499. Dropping entry: {'train/loss': 1.1355654358863831, 'train/ce_loss': 0.343359375, 'train/seg_cls_loss': 0.007452392578125, 'train/kl_loss': 0.1806640625, 'train/mask_bce_loss': 0.06607855148613453, 'train/mask_dice_loss': 0.31898933053016665, 'train/mask_loss': 0.3850678771734238, 'metrics/total_secs_per_batch': 7.505163908004761, 'metrics/data_secs_per_batch': 3.4134860754013063, '_timestamp': 1740975703.3136296}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 108 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975703.313972}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 109 is less than current step: 499. Dropping entry: {'train/loss': 1.3001691475510597, 'train/ce_loss': 0.45107421875, 'train/seg_cls_loss': 0.011700439453125, 'train/kl_loss': 0.219921875, 'train/mask_bce_loss': 0.03423901763744652, 'train/mask_dice_loss': 0.37644125074148177, 'train/mask_loss': 0.4106802770867944, 'metrics/total_secs_per_batch': 5.627406597137451, 'metrics/data_secs_per_batch': 2.57112455368042, '_timestamp': 1740975708.9408596}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 109 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975708.9411514}).
Epoch: [7][111/500]	Time  5.537 ( 5.537)	Loss 1.6005 (1.7529)	CeLoss 0.2109 (0.5225)	SegCLSLoss 0.0200 (0.0129)	KLLoss 0.3555 (0.2549)	MaskLoss 0.6723 (0.5992)	MaskBCELoss 0.0362 (0.1992)	MaskDICELoss 0.6362 (0.3999)
Epoch: [7][112/500]	Time  6.891 ( 6.891)	Loss 2.5774 (1.5394)	CeLoss 0.2168 (0.2989)	SegCLSLoss 0.0216 (0.0142)	KLLoss 0.3613 (0.2535)	MaskLoss 1.1569 (0.6039)	MaskBCELoss 0.3898 (0.0844)	MaskDICELoss 0.7671 (0.5195)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 110 is less than current step: 499. Dropping entry: {'train/loss': 1.7528936922550202, 'train/ce_loss': 0.5224609375, 'train/seg_cls_loss': 0.012884521484375, 'train/kl_loss': 0.2548828125, 'train/mask_bce_loss': 0.19924370162189006, 'train/mask_dice_loss': 0.39990822672843934, 'train/mask_loss': 0.5991519242525101, 'metrics/total_secs_per_batch': 5.5374791622161865, 'metrics/data_secs_per_batch': 2.1867855310440065, '_timestamp': 1740975714.4787242}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 110 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975714.4790797}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 111 is less than current step: 499. Dropping entry: {'train/loss': 1.5393574595451356, 'train/ce_loss': 0.298876953125, 'train/seg_cls_loss': 0.014166259765625, 'train/kl_loss': 0.253515625, 'train/mask_bce_loss': 0.08443376123905182, 'train/mask_dice_loss': 0.5194979071617126, 'train/mask_loss': 0.6039316654205322, 'metrics/total_secs_per_batch': 6.89136528968811, 'metrics/data_secs_per_batch': 3.1120090246200562, '_timestamp': 1740975721.3699133}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 111 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975721.3702703}).
Epoch: [7][113/500]	Time  5.590 ( 5.590)	Loss 1.9125 (1.4961)	CeLoss 0.2061 (0.4681)	SegCLSLoss 0.0107 (0.0099)	KLLoss 0.3672 (0.2150)	MaskLoss 0.8322 (0.5009)	MaskBCELoss 0.1731 (0.0332)	MaskDICELoss 0.6591 (0.4676)
Epoch: [7][114/500]	Time  6.257 ( 6.257)	Loss 2.7381 (1.8136)	CeLoss 0.1562 (0.3497)	SegCLSLoss 0.0273 (0.0153)	KLLoss 0.3750 (0.3311)	MaskLoss 1.2655 (0.7115)	MaskBCELoss 0.2976 (0.1244)	MaskDICELoss 0.9679 (0.5871)
Epoch: [7][115/500]	Time  6.354 ( 6.354)	Loss 2.2651 (1.8146)	CeLoss 0.2227 (0.2845)	SegCLSLoss 0.0187 (0.0178)	KLLoss 0.3770 (0.3273)	MaskLoss 0.9978 (0.7442)	MaskBCELoss 0.2887 (0.1049)	MaskDICELoss 0.7091 (0.6394)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 112 is less than current step: 499. Dropping entry: {'train/loss': 1.4961444854736328, 'train/ce_loss': 0.46806640625, 'train/seg_cls_loss': 0.0098876953125, 'train/kl_loss': 0.2150390625, 'train/mask_bce_loss': 0.033234660234302285, 'train/mask_dice_loss': 0.46762077808380126, 'train/mask_loss': 0.5008554309606552, 'metrics/total_secs_per_batch': 5.59020471572876, 'metrics/data_secs_per_batch': 2.504104161262512, '_timestamp': 1740975726.960194}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 112 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975726.9606056}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 113 is less than current step: 499. Dropping entry: {'train/loss': 1.8136130690574646, 'train/ce_loss': 0.34970703125, 'train/seg_cls_loss': 0.01531982421875, 'train/kl_loss': 0.3310546875, 'train/mask_bce_loss': 0.12443375289440155, 'train/mask_dice_loss': 0.587060272693634, 'train/mask_loss': 0.7114940285682678, 'metrics/total_secs_per_batch': 6.257233619689941, 'metrics/data_secs_per_batch': 2.951712656021118, '_timestamp': 1740975733.217373}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 113 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975733.217664}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 114 is less than current step: 499. Dropping entry: {'train/loss': 1.8146331846714019, 'train/ce_loss': 0.28447265625, 'train/seg_cls_loss': 0.017816162109375, 'train/kl_loss': 0.32734375, 'train/mask_bce_loss': 0.10485966820269824, 'train/mask_dice_loss': 0.6393709689378738, 'train/mask_loss': 0.7442306309938431, 'metrics/total_secs_per_batch': 6.353968620300293, 'metrics/data_secs_per_batch': 2.7367130517959595, '_timestamp': 1740975739.571664}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 114 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975739.5720422}).
Epoch: [7][116/500]	Time  5.744 ( 5.744)	Loss 1.5309 (1.3893)	CeLoss 0.2393 (0.3794)	SegCLSLoss 0.0101 (0.0126)	KLLoss 0.3555 (0.2900)	MaskLoss 0.6258 (0.4872)	MaskBCELoss 0.0521 (0.0916)	MaskDICELoss 0.5738 (0.3956)
Epoch: [7][117/500]	Time  5.918 ( 5.918)	Loss 0.9330 (1.7638)	CeLoss 0.3848 (0.3204)	SegCLSLoss 0.0107 (0.0148)	KLLoss 0.3613 (0.3291)	MaskLoss 0.2536 (0.7015)	MaskBCELoss 0.1134 (0.2314)	MaskDICELoss 0.1402 (0.4701)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 115 is less than current step: 499. Dropping entry: {'train/loss': 1.3892670273780823, 'train/ce_loss': 0.37939453125, 'train/seg_cls_loss': 0.01260986328125, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.09160764962434768, 'train/mask_dice_loss': 0.39560399055480955, 'train/mask_loss': 0.487211637198925, 'metrics/total_secs_per_batch': 5.744329452514648, 'metrics/data_secs_per_batch': 2.5442343235015867, '_timestamp': 1740975745.3156602}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 115 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975745.3160105}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 116 is less than current step: 499. Dropping entry: {'train/loss': 1.763820904493332, 'train/ce_loss': 0.32041015625, 'train/seg_cls_loss': 0.01478271484375, 'train/kl_loss': 0.3291015625, 'train/mask_bce_loss': 0.23144341260194778, 'train/mask_dice_loss': 0.47009595334529874, 'train/mask_loss': 0.7015393584966659, 'metrics/total_secs_per_batch': 5.917896270751953, 'metrics/data_secs_per_batch': 2.807798981666565, '_timestamp': 1740975751.2335799}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 116 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975751.2338731}).
Epoch: [7][118/500]	Time  6.655 ( 6.655)	Loss 1.6381 (1.7087)	CeLoss 0.2354 (0.3022)	SegCLSLoss 0.0179 (0.0150)	KLLoss 0.3574 (0.3275)	MaskLoss 0.6794 (0.6830)	MaskBCELoss 0.0395 (0.1440)	MaskDICELoss 0.6399 (0.5390)
Epoch: [7][119/500]	Time  7.870 ( 7.870)	Loss 2.7762 (2.1157)	CeLoss 0.1621 (0.2175)	SegCLSLoss 0.0225 (0.0140)	KLLoss 0.3691 (0.3266)	MaskLoss 1.2826 (0.9295)	MaskBCELoss 0.5157 (0.3026)	MaskDICELoss 0.7669 (0.6269)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 117 is less than current step: 499. Dropping entry: {'train/loss': 1.7087035596370697, 'train/ce_loss': 0.30224609375, 'train/seg_cls_loss': 0.014990234375, 'train/kl_loss': 0.3275390625, 'train/mask_bce_loss': 0.14404703341424466, 'train/mask_dice_loss': 0.5389668554067611, 'train/mask_loss': 0.6830138847231865, 'metrics/total_secs_per_batch': 6.655430555343628, 'metrics/data_secs_per_batch': 3.0691452264785766, '_timestamp': 1740975757.8890853}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 117 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975757.8894315}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 118 is less than current step: 499. Dropping entry: {'train/loss': 2.115670394897461, 'train/ce_loss': 0.21748046875, 'train/seg_cls_loss': 0.013958740234375, 'train/kl_loss': 0.3265625, 'train/mask_bce_loss': 0.30255402233451606, 'train/mask_dice_loss': 0.6269120305776597, 'train/mask_loss': 0.929466062784195, 'metrics/total_secs_per_batch': 7.870120525360107, 'metrics/data_secs_per_batch': 3.326159191131592, '_timestamp': 1740975765.759487}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 118 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975765.760021}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 119 is less than current step: 499. Dropping entry: {'train/loss': 1.8507362604141235, 'train/ce_loss': 0.3341796875, 'train/seg_cls_loss': 0.015924072265625, 'train/kl_loss': 0.327734375, 'train/mask_bce_loss': 0.14070448651909828, 'train/mask_dice_loss': 0.5971636399626732, 'train/mask_loss': 0.7378681272268295, 'metrics/total_secs_per_batch': 5.833110570907593, 'metrics/data_secs_per_batch': 2.639852023124695, '_timestamp': 1740975771.5920403}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 119 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975771.5923252}).
[2025-03-02 22:22:51,585] [INFO] [logging.py:96:log_dist] [Rank 0] step=3620, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:22:51,591] [INFO] [timer.py:215:stop] epoch=0/micro_step=36200/global_step=3620, RunningAvgSamplesPerSec=1.455629897361038, CurrSamplesPerSec=1.7150462194763119, MemAllocated=30.72GB, MaxMemAllocated=37.23GB
Epoch: [7][120/500]	Time  5.833 ( 5.833)	Loss 1.3984 (1.8507)	CeLoss 1.3984 (0.3342)	SegCLSLoss 0.0000 (0.0159)	KLLoss 0.0000 (0.3277)	MaskLoss 0.0000 (0.7379)	MaskBCELoss 0.0000 (0.1407)	MaskDICELoss 0.0000 (0.5972)
Epoch: [7][121/500]	Time  6.140 ( 6.140)	Loss 1.2499 (1.4568)	CeLoss 0.3516 (0.4467)	SegCLSLoss 0.0096 (0.0118)	KLLoss 0.3555 (0.2883)	MaskLoss 0.4297 (0.4878)	MaskBCELoss 0.2930 (0.0750)	MaskDICELoss 0.1366 (0.4128)
Epoch: [7][122/500]	Time  5.660 ( 5.660)	Loss 1.0709 (1.2669)	CeLoss 0.1904 (0.3021)	SegCLSLoss 0.0095 (0.0095)	KLLoss 0.3711 (0.2580)	MaskLoss 0.4192 (0.4671)	MaskBCELoss 0.1122 (0.1428)	MaskDICELoss 0.3071 (0.3243)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 120 is less than current step: 499. Dropping entry: {'train/loss': 1.456779944896698, 'train/ce_loss': 0.4466796875, 'train/seg_cls_loss': 0.011822509765625, 'train/kl_loss': 0.28828125, 'train/mask_bce_loss': 0.07499247957020998, 'train/mask_dice_loss': 0.4127724826335907, 'train/mask_loss': 0.4877649635076523, 'metrics/total_secs_per_batch': 6.139636039733887, 'metrics/data_secs_per_batch': 2.7586784839630125, '_timestamp': 1740975777.7319663}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 120 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975777.7322834}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 121 is less than current step: 499. Dropping entry: {'train/loss': 1.2668744623661041, 'train/ce_loss': 0.302099609375, 'train/seg_cls_loss': 0.00950927734375, 'train/kl_loss': 0.2580078125, 'train/mask_bce_loss': 0.14283501133322715, 'train/mask_dice_loss': 0.324269200861454, 'train/mask_loss': 0.4671042114496231, 'metrics/total_secs_per_batch': 5.66026759147644, 'metrics/data_secs_per_batch': 2.281608057022095, '_timestamp': 1740975783.392276}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 121 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975783.3926003}).
Epoch: [7][123/500]	Time  6.389 ( 6.389)	Loss 0.6101 (1.7931)	CeLoss 0.2520 (0.2649)	SegCLSLoss 0.0090 (0.0175)	KLLoss 0.3555 (0.3260)	MaskLoss 0.1596 (0.7434)	MaskBCELoss 0.0502 (0.0971)	MaskDICELoss 0.1093 (0.6463)
Epoch: [7][124/500]	Time  6.080 ( 6.080)	Loss 1.1913 (1.8329)	CeLoss 0.3359 (0.6717)	SegCLSLoss 0.0113 (0.0123)	KLLoss 0.3633 (0.2502)	MaskLoss 0.4062 (0.5651)	MaskBCELoss 0.0202 (0.0409)	MaskDICELoss 0.3860 (0.5242)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 122 is less than current step: 499. Dropping entry: {'train/loss': 1.7931106805801391, 'train/ce_loss': 0.264892578125, 'train/seg_cls_loss': 0.01749267578125, 'train/kl_loss': 0.3259765625, 'train/mask_bce_loss': 0.0971439590677619, 'train/mask_dice_loss': 0.6462863683700562, 'train/mask_loss': 0.7434303343296051, 'metrics/total_secs_per_batch': 6.388845205307007, 'metrics/data_secs_per_batch': 2.82335364818573, '_timestamp': 1740975789.78101}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 122 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975789.7812872}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 123 is less than current step: 499. Dropping entry: {'train/loss': 1.8328951478004456, 'train/ce_loss': 0.6716796875, 'train/seg_cls_loss': 0.0123291015625, 'train/kl_loss': 0.2501953125, 'train/mask_bce_loss': 0.04086554003879428, 'train/mask_dice_loss': 0.5242148339748383, 'train/mask_loss': 0.5650803774595261, 'metrics/total_secs_per_batch': 6.0798869132995605, 'metrics/data_secs_per_batch': 2.645221471786499, '_timestamp': 1740975795.860898}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 123 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975795.8611906}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 124 is less than current step: 499. Dropping entry: {'train/loss': 1.5384759783744812, 'train/ce_loss': 0.308349609375, 'train/seg_cls_loss': 0.011700439453125, 'train/kl_loss': 0.2912109375, 'train/mask_bce_loss': 0.19951427076011896, 'train/mask_dice_loss': 0.3980684459209442, 'train/mask_loss': 0.5975827164947987, 'metrics/total_secs_per_batch': 5.569430828094482, 'metrics/data_secs_per_batch': 2.5593064785003663, '_timestamp': 1740975801.4302893}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 124 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975801.4305735}).
Epoch: [7][125/500]	Time  5.569 ( 5.569)	Loss 1.0703 (1.5385)	CeLoss 1.0703 (0.3083)	SegCLSLoss 0.0000 (0.0117)	KLLoss 0.0000 (0.2912)	MaskLoss 0.0000 (0.5976)	MaskBCELoss 0.0000 (0.1995)	MaskDICELoss 0.0000 (0.3981)
Epoch: [7][126/500]	Time  6.397 ( 6.397)	Loss 0.9469 (1.7336)	CeLoss 0.3281 (0.2344)	SegCLSLoss 0.0090 (0.0177)	KLLoss 0.3594 (0.3607)	MaskLoss 0.2899 (0.7271)	MaskBCELoss 0.0906 (0.1313)	MaskDICELoss 0.1992 (0.5958)
Epoch: [7][127/500]	Time  6.310 ( 6.310)	Loss 1.5526 (1.6583)	CeLoss 0.1943 (0.3124)	SegCLSLoss 0.0253 (0.0171)	KLLoss 0.3516 (0.3256)	MaskLoss 0.6552 (0.6523)	MaskBCELoss 0.0534 (0.0776)	MaskDICELoss 0.6018 (0.5747)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 125 is less than current step: 499. Dropping entry: {'train/loss': 1.7336437463760377, 'train/ce_loss': 0.234375, 'train/seg_cls_loss': 0.017681884765625, 'train/kl_loss': 0.3607421875, 'train/mask_bce_loss': 0.13129745237529278, 'train/mask_dice_loss': 0.5958271309733391, 'train/mask_loss': 0.7271245926618576, 'metrics/total_secs_per_batch': 6.3968682289123535, 'metrics/data_secs_per_batch': 3.0123302698135377, '_timestamp': 1740975807.8272336}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 125 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975807.827434}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 126 is less than current step: 499. Dropping entry: {'train/loss': 1.6582907497882844, 'train/ce_loss': 0.31240234375, 'train/seg_cls_loss': 0.017083740234375, 'train/kl_loss': 0.3255859375, 'train/mask_bce_loss': 0.07761661796830595, 'train/mask_dice_loss': 0.574673292785883, 'train/mask_loss': 0.6522899061441422, 'metrics/total_secs_per_batch': 6.310197830200195, 'metrics/data_secs_per_batch': 2.7976038217544557, '_timestamp': 1740975814.1373641}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 126 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975814.137641}).
Epoch: [7][128/500]	Time  5.748 ( 5.748)	Loss 1.6138 (1.6230)	CeLoss 0.1953 (0.6005)	SegCLSLoss 0.0187 (0.0121)	KLLoss 0.3594 (0.2514)	MaskLoss 0.6863 (0.4955)	MaskBCELoss 0.0655 (0.0733)	MaskDICELoss 0.6208 (0.4223)
Epoch: [7][129/500]	Time  5.678 ( 5.678)	Loss 1.2284 (1.7513)	CeLoss 0.1592 (0.3758)	SegCLSLoss 0.0378 (0.0173)	KLLoss 0.3672 (0.2914)	MaskLoss 0.5068 (0.6689)	MaskBCELoss 0.1199 (0.1801)	MaskDICELoss 0.3869 (0.4888)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 127 is less than current step: 499. Dropping entry: {'train/loss': 1.6230313539505006, 'train/ce_loss': 0.60048828125, 'train/seg_cls_loss': 0.01209716796875, 'train/kl_loss': 0.2513671875, 'train/mask_bce_loss': 0.07328558769077062, 'train/mask_dice_loss': 0.4222632795572281, 'train/mask_loss': 0.4955488681793213, 'metrics/total_secs_per_batch': 5.747503757476807, 'metrics/data_secs_per_batch': 2.616848039627075, '_timestamp': 1740975819.885033}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 127 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975819.8853662}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 128 is less than current step: 499. Dropping entry: {'train/loss': 1.7513392388820648, 'train/ce_loss': 0.37578125, 'train/seg_cls_loss': 0.01729736328125, 'train/kl_loss': 0.29140625, 'train/mask_bce_loss': 0.18008956238627433, 'train/mask_dice_loss': 0.48879294395446776, 'train/mask_loss': 0.6688825100660324, 'metrics/total_secs_per_batch': 5.67768406867981, 'metrics/data_secs_per_batch': 2.3639406681060793, '_timestamp': 1740975825.5626144}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 128 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975825.5629132}).
[2025-03-02 22:23:53,212] [INFO] [logging.py:96:log_dist] [Rank 0] step=3630, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:23:53,218] [INFO] [timer.py:215:stop] epoch=0/micro_step=36300/global_step=3630, RunningAvgSamplesPerSec=1.456043752517222, CurrSamplesPerSec=1.3062985468154957, MemAllocated=31.26GB, MaxMemAllocated=37.23GB
Epoch: [7][130/500]	Time  7.657 ( 7.657)	Loss 0.9989 (1.7343)	CeLoss 0.2236 (0.4533)	SegCLSLoss 0.0217 (0.0145)	KLLoss 0.3672 (0.2887)	MaskLoss 0.3637 (0.6225)	MaskBCELoss 0.0869 (0.1224)	MaskDICELoss 0.2768 (0.5000)
Epoch: [7][131/500]	Time  5.592 ( 5.592)	Loss 1.6484 (1.5339)	CeLoss 1.6484 (0.5075)	SegCLSLoss 0.0000 (0.0106)	KLLoss 0.0000 (0.2152)	MaskLoss 0.0000 (0.4999)	MaskBCELoss 0.0000 (0.1483)	MaskDICELoss 0.0000 (0.3515)
Epoch: [7][132/500]	Time  6.138 ( 6.138)	Loss 1.0493 (1.6855)	CeLoss 0.2559 (0.3541)	SegCLSLoss 0.0095 (0.0107)	KLLoss 0.3672 (0.2932)	MaskLoss 0.3762 (0.6483)	MaskBCELoss 0.0832 (0.1099)	MaskDICELoss 0.2931 (0.5385)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 129 is less than current step: 499. Dropping entry: {'train/loss': 1.7342842996120453, 'train/ce_loss': 0.4533203125, 'train/seg_cls_loss': 0.014453125, 'train/kl_loss': 0.288671875, 'train/mask_bce_loss': 0.122431068867445, 'train/mask_dice_loss': 0.5000333368778229, 'train/mask_loss': 0.622464406490326, 'metrics/total_secs_per_batch': 7.656853437423706, 'metrics/data_secs_per_batch': 3.105012369155884, '_timestamp': 1740975833.2193398}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 129 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975833.2196846}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 130 is less than current step: 499. Dropping entry: {'train/loss': 1.5338852286338807, 'train/ce_loss': 0.50751953125, 'train/seg_cls_loss': 0.01055908203125, 'train/kl_loss': 0.215234375, 'train/mask_bce_loss': 0.14832589942961932, 'train/mask_dice_loss': 0.3515268683433533, 'train/mask_loss': 0.4998527646064758, 'metrics/total_secs_per_batch': 5.591536045074463, 'metrics/data_secs_per_batch': 2.599058723449707, '_timestamp': 1740975838.811015}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 130 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975838.8113027}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 131 is less than current step: 499. Dropping entry: {'train/loss': 1.685510802268982, 'train/ce_loss': 0.3541015625, 'train/seg_cls_loss': 0.0107177734375, 'train/kl_loss': 0.2931640625, 'train/mask_bce_loss': 0.10985219739377498, 'train/mask_dice_loss': 0.5384696006774903, 'train/mask_loss': 0.6483217954635621, 'metrics/total_secs_per_batch': 6.13826322555542, 'metrics/data_secs_per_batch': 2.741987705230713, '_timestamp': 1740975844.9492004}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 131 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975844.949476}).
Epoch: [7][133/500]	Time  6.703 ( 6.703)	Loss 1.6202 (2.0677)	CeLoss 0.1924 (0.3023)	SegCLSLoss 0.0200 (0.0134)	KLLoss 0.3574 (0.3271)	MaskLoss 0.6910 (0.8628)	MaskBCELoss 0.0206 (0.2726)	MaskDICELoss 0.6704 (0.5902)
Epoch: [7][134/500]	Time  5.601 ( 5.601)	Loss 1.6276 (1.4802)	CeLoss 0.1777 (0.4893)	SegCLSLoss 0.0187 (0.0121)	KLLoss 0.3613 (0.2541)	MaskLoss 0.7020 (0.4798)	MaskBCELoss 0.0313 (0.0595)	MaskDICELoss 0.6707 (0.4203)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 132 is less than current step: 499. Dropping entry: {'train/loss': 2.0676607728004455, 'train/ce_loss': 0.30234375, 'train/seg_cls_loss': 0.013397216796875, 'train/kl_loss': 0.3271484375, 'train/mask_bce_loss': 0.27260319516062737, 'train/mask_dice_loss': 0.5902310952544212, 'train/mask_loss': 0.8628342837095261, 'metrics/total_secs_per_batch': 6.702561855316162, 'metrics/data_secs_per_batch': 2.7421786069869993, '_timestamp': 1740975851.652007}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 132 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975851.652243}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 133 is less than current step: 499. Dropping entry: {'train/loss': 1.4802164673805236, 'train/ce_loss': 0.4892578125, 'train/seg_cls_loss': 0.012060546875, 'train/kl_loss': 0.2541015625, 'train/mask_bce_loss': 0.05945650767534971, 'train/mask_dice_loss': 0.4203001528978348, 'train/mask_loss': 0.47975666224956515, 'metrics/total_secs_per_batch': 5.600854873657227, 'metrics/data_secs_per_batch': 2.287123990058899, '_timestamp': 1740975857.2527122}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 133 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975857.2530117}).
Epoch: [7][135/500]	Time  5.785 ( 5.785)	Loss 1.6945 (1.3936)	CeLoss 0.2354 (0.3377)	SegCLSLoss 0.0099 (0.0123)	KLLoss 0.3594 (0.2893)	MaskLoss 0.7096 (0.5105)	MaskBCELoss 0.1678 (0.0839)	MaskDICELoss 0.5417 (0.4266)
Epoch: [7][136/500]	Time  6.411 ( 6.411)	Loss 2.2315 (1.7508)	CeLoss 0.1865 (0.3027)	SegCLSLoss 0.0108 (0.0125)	KLLoss 0.3730 (0.2881)	MaskLoss 1.0010 (0.7066)	MaskBCELoss 0.0044 (0.0839)	MaskDICELoss 0.9966 (0.6228)
Epoch: [7][137/500]	Time  5.720 ( 5.720)	Loss 2.3962 (1.7429)	CeLoss 0.2578 (0.4524)	SegCLSLoss 0.0182 (0.0122)	KLLoss 0.3711 (0.2928)	MaskLoss 1.0457 (0.6275)	MaskBCELoss 0.2244 (0.1275)	MaskDICELoss 0.8214 (0.5000)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 134 is less than current step: 499. Dropping entry: {'train/loss': 1.3935621201992034, 'train/ce_loss': 0.3376953125, 'train/seg_cls_loss': 0.01229248046875, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.08394534289836883, 'train/mask_dice_loss': 0.4265564069151878, 'train/mask_loss': 0.5105017572641373, 'metrics/total_secs_per_batch': 5.785161972045898, 'metrics/data_secs_per_batch': 2.6241268634796144, '_timestamp': 1740975863.0378537}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 134 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975863.0381281}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 135 is less than current step: 499. Dropping entry: {'train/loss': 1.7507878541946411, 'train/ce_loss': 0.302734375, 'train/seg_cls_loss': 0.012493896484375, 'train/kl_loss': 0.2880859375, 'train/mask_bce_loss': 0.08385644806548953, 'train/mask_dice_loss': 0.6227874845266342, 'train/mask_loss': 0.7066439330577851, 'metrics/total_secs_per_batch': 6.411276578903198, 'metrics/data_secs_per_batch': 3.1190494298934937, '_timestamp': 1740975869.4493175}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 135 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975869.4496686}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 136 is less than current step: 499. Dropping entry: {'train/loss': 1.7428953766822814, 'train/ce_loss': 0.45244140625, 'train/seg_cls_loss': 0.01219482421875, 'train/kl_loss': 0.2927734375, 'train/mask_bce_loss': 0.12747587375342845, 'train/mask_dice_loss': 0.4999776691198349, 'train/mask_loss': 0.6274535447359085, 'metrics/total_secs_per_batch': 5.71998143196106, 'metrics/data_secs_per_batch': 2.5503095626831054, '_timestamp': 1740975875.1692173}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 136 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975875.1695416}).
Epoch: [7][138/500]	Time  6.129 ( 6.129)	Loss 1.1247 (1.6579)	CeLoss 0.2617 (0.3192)	SegCLSLoss 0.0115 (0.0121)	KLLoss 0.3633 (0.3262)	MaskLoss 0.4100 (0.6499)	MaskBCELoss 0.0485 (0.1361)	MaskDICELoss 0.3616 (0.5139)
Epoch: [7][139/500]	Time  6.075 ( 6.075)	Loss 1.4395 (1.5795)	CeLoss 0.1787 (0.3892)	SegCLSLoss 0.0281 (0.0136)	KLLoss 0.3594 (0.2555)	MaskLoss 0.6055 (0.5790)	MaskBCELoss 0.0303 (0.1333)	MaskDICELoss 0.5752 (0.4457)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 137 is less than current step: 499. Dropping entry: {'train/loss': 1.6578705310821533, 'train/ce_loss': 0.31923828125, 'train/seg_cls_loss': 0.0121337890625, 'train/kl_loss': 0.326171875, 'train/mask_bce_loss': 0.13607196239754557, 'train/mask_dice_loss': 0.5138593912124634, 'train/mask_loss': 0.6499313622713089, 'metrics/total_secs_per_batch': 6.128884792327881, 'metrics/data_secs_per_batch': 2.8898154497146606, '_timestamp': 1740975881.2980225}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 137 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975881.2983344}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 138 is less than current step: 499. Dropping entry: {'train/loss': 1.5794974565505981, 'train/ce_loss': 0.38916015625, 'train/seg_cls_loss': 0.013623046875, 'train/kl_loss': 0.25546875, 'train/mask_bce_loss': 0.1332923114299774, 'train/mask_dice_loss': 0.4457142323255539, 'train/mask_loss': 0.5790065407752991, 'metrics/total_secs_per_batch': 6.074725389480591, 'metrics/data_secs_per_batch': 2.6843780040740968, '_timestamp': 1740975887.3729808}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 138 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975887.3734124}).
[2025-03-02 22:24:53,447] [INFO] [logging.py:96:log_dist] [Rank 0] step=3640, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:24:53,453] [INFO] [timer.py:215:stop] epoch=0/micro_step=36400/global_step=3640, RunningAvgSamplesPerSec=1.4565368257468037, CurrSamplesPerSec=1.6449475337419233, MemAllocated=31.09GB, MaxMemAllocated=37.23GB
Epoch: [7][140/500]	Time  6.081 ( 6.081)	Loss 0.9305 (1.4918)	CeLoss 0.2344 (0.2324)	SegCLSLoss 0.0170 (0.0139)	KLLoss 0.3652 (0.2893)	MaskLoss 0.3256 (0.6118)	MaskBCELoss 0.1748 (0.1051)	MaskDICELoss 0.1508 (0.5067)
Epoch: [7][141/500]	Time  6.554 ( 6.554)	Loss 2.8113 (1.7786)	CeLoss 0.2412 (0.4055)	SegCLSLoss 0.0209 (0.0116)	KLLoss 0.3750 (0.2582)	MaskLoss 1.2611 (0.6708)	MaskBCELoss 0.3846 (0.1922)	MaskDICELoss 0.8766 (0.4786)
Epoch: [7][142/500]	Time  5.529 ( 5.529)	Loss 1.7753 (1.6230)	CeLoss 0.2139 (0.4688)	SegCLSLoss 0.0095 (0.0112)	KLLoss 0.3633 (0.2912)	MaskLoss 0.7597 (0.5597)	MaskBCELoss 0.2040 (0.1478)	MaskDICELoss 0.5557 (0.4119)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 139 is less than current step: 499. Dropping entry: {'train/loss': 1.4918481707572937, 'train/ce_loss': 0.232421875, 'train/seg_cls_loss': 0.01390380859375, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.10510381776839495, 'train/mask_dice_loss': 0.5066894203424454, 'train/mask_loss': 0.6117932319641113, 'metrics/total_secs_per_batch': 6.0812554359436035, 'metrics/data_secs_per_batch': 2.5901687860488893, '_timestamp': 1740975893.4537885}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 139 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975893.4540894}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 140 is less than current step: 499. Dropping entry: {'train/loss': 1.7786423802375793, 'train/ce_loss': 0.40546875, 'train/seg_cls_loss': 0.01163330078125, 'train/kl_loss': 0.258203125, 'train/mask_bce_loss': 0.1922151580452919, 'train/mask_dice_loss': 0.47860015630722047, 'train/mask_loss': 0.6708153188228607, 'metrics/total_secs_per_batch': 6.5539870262146, 'metrics/data_secs_per_batch': 3.2462544202804566, '_timestamp': 1740975900.0079706}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 140 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975900.008347}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 141 is less than current step: 499. Dropping entry: {'train/loss': 1.6230022490024567, 'train/ce_loss': 0.46884765625, 'train/seg_cls_loss': 0.011199951171875, 'train/kl_loss': 0.2912109375, 'train/mask_bce_loss': 0.14779820777475833, 'train/mask_dice_loss': 0.4119450956583023, 'train/mask_loss': 0.5597433120012283, 'metrics/total_secs_per_batch': 5.5287816524505615, 'metrics/data_secs_per_batch': 2.6019763708114625, '_timestamp': 1740975905.5369668}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 141 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975905.5373979}).
Epoch: [7][143/500]	Time  5.757 ( 5.757)	Loss 1.8027 (1.8757)	CeLoss 0.4180 (0.3945)	SegCLSLoss 0.0090 (0.0116)	KLLoss 0.3613 (0.3250)	MaskLoss 0.6728 (0.7215)	MaskBCELoss 0.2057 (0.1920)	MaskDICELoss 0.4672 (0.5295)
Epoch: [7][144/500]	Time  6.926 ( 6.926)	Loss 1.4989 (1.6470)	CeLoss 0.2422 (0.2256)	SegCLSLoss 0.0128 (0.0155)	KLLoss 0.3633 (0.3275)	MaskLoss 0.6068 (0.6903)	MaskBCELoss 0.1173 (0.0820)	MaskDICELoss 0.4895 (0.6083)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 142 is less than current step: 499. Dropping entry: {'train/loss': 1.875739824771881, 'train/ce_loss': 0.39453125, 'train/seg_cls_loss': 0.011553955078125, 'train/kl_loss': 0.325, 'train/mask_bce_loss': 0.191967736184597, 'train/mask_dice_loss': 0.5294959247112274, 'train/mask_loss': 0.7214636623859405, 'metrics/total_secs_per_batch': 5.756703615188599, 'metrics/data_secs_per_batch': 2.5159671545028686, '_timestamp': 1740975911.2934477}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 142 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975911.29372}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 143 is less than current step: 499. Dropping entry: {'train/loss': 1.647040605545044, 'train/ce_loss': 0.225634765625, 'train/seg_cls_loss': 0.015472412109375, 'train/kl_loss': 0.3275390625, 'train/mask_bce_loss': 0.08197471220046282, 'train/mask_dice_loss': 0.6083180353045463, 'train/mask_loss': 0.6902927458286285, 'metrics/total_secs_per_batch': 6.92586350440979, 'metrics/data_secs_per_batch': 3.3391326904296874, '_timestamp': 1740975918.2193913}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 143 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975918.2197762}).
Epoch: [7][145/500]	Time  6.917 ( 6.917)	Loss 2.1240 (1.8842)	CeLoss 0.2158 (0.2022)	SegCLSLoss 0.0231 (0.0220)	KLLoss 0.3594 (0.3615)	MaskLoss 0.9302 (0.8173)	MaskBCELoss 0.0374 (0.1144)	MaskDICELoss 0.8928 (0.7029)
Epoch: [7][146/500]	Time  5.682 ( 5.682)	Loss 1.1588 (1.4176)	CeLoss 0.2324 (0.3945)	SegCLSLoss 0.0094 (0.0138)	KLLoss 0.3633 (0.2910)	MaskLoss 0.4427 (0.4936)	MaskBCELoss 0.2342 (0.1353)	MaskDICELoss 0.2085 (0.3583)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 144 is less than current step: 499. Dropping entry: {'train/loss': 1.884195590019226, 'train/ce_loss': 0.202197265625, 'train/seg_cls_loss': 0.0219970703125, 'train/kl_loss': 0.3615234375, 'train/mask_bce_loss': 0.11441647354513407, 'train/mask_dice_loss': 0.7029254764318467, 'train/mask_loss': 0.817341947555542, 'metrics/total_secs_per_batch': 6.9167304039001465, 'metrics/data_secs_per_batch': 3.1005544662475586, '_timestamp': 1740975925.1360927}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 144 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975925.1364007}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 145 is less than current step: 499. Dropping entry: {'train/loss': 1.4176475882530213, 'train/ce_loss': 0.39453125, 'train/seg_cls_loss': 0.0138427734375, 'train/kl_loss': 0.291015625, 'train/mask_bce_loss': 0.1352745696902275, 'train/mask_dice_loss': 0.35831484869122504, 'train/mask_loss': 0.4935894191265106, 'metrics/total_secs_per_batch': 5.68208384513855, 'metrics/data_secs_per_batch': 2.4318164825439452, '_timestamp': 1740975930.81832}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 145 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975930.8186533}).
Epoch: [7][147/500]	Time  6.429 ( 6.429)	Loss 0.5173 (1.5538)	CeLoss 0.2812 (0.2366)	SegCLSLoss 0.0142 (0.0156)	KLLoss 0.3652 (0.3279)	MaskLoss 0.0956 (0.6382)	MaskBCELoss 0.0464 (0.1582)	MaskDICELoss 0.0492 (0.4800)
Epoch: [7][148/500]	Time  4.550 ( 4.550)	Loss 2.1206 (1.6454)	CeLoss 0.1914 (0.6556)	SegCLSLoss 0.0140 (0.0096)	KLLoss 0.3633 (0.2229)	MaskLoss 0.9431 (0.4813)	MaskBCELoss 0.0425 (0.1090)	MaskDICELoss 0.9006 (0.3723)
Epoch: [7][149/500]	Time  5.795 ( 5.795)	Loss 1.2344 (1.8249)	CeLoss 1.2344 (0.4216)	SegCLSLoss 0.0000 (0.0178)	KLLoss 0.0000 (0.2896)	MaskLoss 0.0000 (0.6828)	MaskBCELoss 0.0000 (0.1142)	MaskDICELoss 0.0000 (0.5685)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 146 is less than current step: 499. Dropping entry: {'train/loss': 1.5537923753261567, 'train/ce_loss': 0.23662109375, 'train/seg_cls_loss': 0.015625, 'train/kl_loss': 0.3279296875, 'train/mask_bce_loss': 0.1582204578211531, 'train/mask_dice_loss': 0.48000385947525503, 'train/mask_loss': 0.6382243119180202, 'metrics/total_secs_per_batch': 6.428796768188477, 'metrics/data_secs_per_batch': 2.8823250770568847, '_timestamp': 1740975937.2469418}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 146 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975937.2472146}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 147 is less than current step: 499. Dropping entry: {'train/loss': 1.6453924417495727, 'train/ce_loss': 0.655615234375, 'train/seg_cls_loss': 0.009637451171875, 'train/kl_loss': 0.2228515625, 'train/mask_bce_loss': 0.10904747880995273, 'train/mask_dice_loss': 0.3722913175821304, 'train/mask_loss': 0.481338794529438, 'metrics/total_secs_per_batch': 4.54975152015686, 'metrics/data_secs_per_batch': 1.9752266645431518, '_timestamp': 1740975941.7967427}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 147 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975941.79704}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 148 is less than current step: 499. Dropping entry: {'train/loss': 1.8249320268630982, 'train/ce_loss': 0.421630859375, 'train/seg_cls_loss': 0.017767333984375, 'train/kl_loss': 0.2896484375, 'train/mask_bce_loss': 0.11424353867769241, 'train/mask_dice_loss': 0.5685349762439728, 'train/mask_loss': 0.6827785015106201, 'metrics/total_secs_per_batch': 5.795265436172485, 'metrics/data_secs_per_batch': 2.87085816860199, '_timestamp': 1740975947.5921466}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 148 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975947.5924857}).
[2025-03-02 22:25:54,267] [INFO] [logging.py:96:log_dist] [Rank 0] step=3650, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:25:54,273] [INFO] [timer.py:215:stop] epoch=0/micro_step=36500/global_step=3650, RunningAvgSamplesPerSec=1.456993455002076, CurrSamplesPerSec=1.4970184224139882, MemAllocated=31.45GB, MaxMemAllocated=37.23GB
Epoch: [7][150/500]	Time  6.682 ( 6.682)	Loss 1.3101 (1.4360)	CeLoss 0.2354 (0.1785)	SegCLSLoss 0.0137 (0.0126)	KLLoss 0.3594 (0.2525)	MaskLoss 0.5164 (0.6130)	MaskBCELoss 0.0359 (0.0997)	MaskDICELoss 0.4805 (0.5134)
Epoch: [7][151/500]	Time  6.814 ( 6.814)	Loss 0.9537 (1.4042)	CeLoss 0.3926 (0.2968)	SegCLSLoss 0.0110 (0.0137)	KLLoss 0.3691 (0.2893)	MaskLoss 0.2591 (0.5357)	MaskBCELoss 0.0966 (0.0889)	MaskDICELoss 0.1625 (0.4468)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 149 is less than current step: 499. Dropping entry: {'train/loss': 1.435980212688446, 'train/ce_loss': 0.1785400390625, 'train/seg_cls_loss': 0.01259765625, 'train/kl_loss': 0.2525390625, 'train/mask_bce_loss': 0.09969360530376434, 'train/mask_dice_loss': 0.5133526533842087, 'train/mask_loss': 0.613046258687973, 'metrics/total_secs_per_batch': 6.681753635406494, 'metrics/data_secs_per_batch': 3.144978666305542, '_timestamp': 1740975954.2735465}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 149 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975954.2738328}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 150 is less than current step: 499. Dropping entry: {'train/loss': 1.4042124211788178, 'train/ce_loss': 0.2968017578125, 'train/seg_cls_loss': 0.013714599609375, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.08887193351984024, 'train/mask_dice_loss': 0.4468158230185509, 'train/mask_loss': 0.5356877565383911, 'metrics/total_secs_per_batch': 6.81409478187561, 'metrics/data_secs_per_batch': 2.9298237562179565, '_timestamp': 1740975961.0878496}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 150 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975961.088139}).
Epoch: [7][152/500]	Time  6.904 ( 6.904)	Loss 2.2526 (1.5825)	CeLoss 0.2402 (0.2917)	SegCLSLoss 0.0132 (0.0137)	KLLoss 0.3613 (0.2902)	MaskLoss 0.9847 (0.6275)	MaskBCELoss 0.2099 (0.1199)	MaskDICELoss 0.7748 (0.5075)
Epoch: [7][153/500]	Time  5.872 ( 5.872)	Loss 2.2337 (1.2870)	CeLoss 0.1162 (0.3998)	SegCLSLoss 0.0266 (0.0091)	KLLoss 0.3730 (0.2178)	MaskLoss 1.0334 (0.4306)	MaskBCELoss 0.2512 (0.1079)	MaskDICELoss 0.7822 (0.3226)
Epoch: [7][154/500]	Time  5.965 ( 5.965)	Loss 1.2989 (1.7862)	CeLoss 0.2578 (0.2360)	SegCLSLoss 0.0093 (0.0156)	KLLoss 0.3633 (0.3682)	MaskLoss 0.5000 (0.7529)	MaskBCELoss 0.0632 (0.2070)	MaskDICELoss 0.4369 (0.5459)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 151 is less than current step: 499. Dropping entry: {'train/loss': 1.5825304090976715, 'train/ce_loss': 0.291650390625, 'train/seg_cls_loss': 0.01365966796875, 'train/kl_loss': 0.290234375, 'train/mask_bce_loss': 0.1199392793700099, 'train/mask_dice_loss': 0.5075319699943066, 'train/mask_loss': 0.6274712562561036, 'metrics/total_secs_per_batch': 6.9042205810546875, 'metrics/data_secs_per_batch': 3.394442415237427, '_timestamp': 1740975967.9921272}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 151 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975967.992459}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 152 is less than current step: 499. Dropping entry: {'train/loss': 1.2870327472686767, 'train/ce_loss': 0.399755859375, 'train/seg_cls_loss': 0.00908203125, 'train/kl_loss': 0.2177734375, 'train/mask_bce_loss': 0.10792846642434598, 'train/mask_dice_loss': 0.32262402921915057, 'train/mask_loss': 0.4305525004863739, 'metrics/total_secs_per_batch': 5.872029066085815, 'metrics/data_secs_per_batch': 2.9226847887039185, '_timestamp': 1740975973.8641057}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 152 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975973.8644516}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 153 is less than current step: 499. Dropping entry: {'train/loss': 1.7861773371696472, 'train/ce_loss': 0.23603515625, 'train/seg_cls_loss': 0.015606689453125, 'train/kl_loss': 0.3681640625, 'train/mask_bce_loss': 0.20699725863523782, 'train/mask_dice_loss': 0.5458570182323456, 'train/mask_loss': 0.7528542891144753, 'metrics/total_secs_per_batch': 5.9652204513549805, 'metrics/data_secs_per_batch': 2.6531737089157104, '_timestamp': 1740975979.8293326}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 153 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975979.8296297}).
Epoch: [7][155/500]	Time  5.552 ( 5.552)	Loss 2.2891 (1.7943)	CeLoss 0.2002 (0.5104)	SegCLSLoss 0.0215 (0.0127)	KLLoss 0.3535 (0.2531)	MaskLoss 1.0215 (0.6260)	MaskBCELoss 0.1601 (0.1990)	MaskDICELoss 0.8614 (0.4271)
Epoch: [7][156/500]	Time  5.772 ( 5.772)	Loss 1.9990 (1.6482)	CeLoss 0.2285 (0.4461)	SegCLSLoss 0.0165 (0.0119)	KLLoss 0.3672 (0.2570)	MaskLoss 0.8628 (0.5851)	MaskBCELoss 0.0517 (0.0757)	MaskDICELoss 0.8111 (0.5094)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 154 is less than current step: 499. Dropping entry: {'train/loss': 1.794286859035492, 'train/ce_loss': 0.51044921875, 'train/seg_cls_loss': 0.012664794921875, 'train/kl_loss': 0.253125, 'train/mask_bce_loss': 0.19898452386260032, 'train/mask_dice_loss': 0.4270651608705521, 'train/mask_loss': 0.6260496824979782, 'metrics/total_secs_per_batch': 5.552143812179565, 'metrics/data_secs_per_batch': 2.549012017250061, '_timestamp': 1740975985.3814728}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 154 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975985.3818307}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 155 is less than current step: 499. Dropping entry: {'train/loss': 1.6481927394866944, 'train/ce_loss': 0.44609375, 'train/seg_cls_loss': 0.011932373046875, 'train/kl_loss': 0.25703125, 'train/mask_bce_loss': 0.07574746888130904, 'train/mask_dice_loss': 0.509384062886238, 'train/mask_loss': 0.5851315408945084, 'metrics/total_secs_per_batch': 5.772438287734985, 'metrics/data_secs_per_batch': 2.1946778774261473, '_timestamp': 1740975991.1540844}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 155 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975991.1544352}).
Epoch: [7][157/500]	Time  7.068 ( 7.068)	Loss 1.4873 (2.0364)	CeLoss 0.2695 (0.3380)	SegCLSLoss 0.0150 (0.0158)	KLLoss 0.3594 (0.3295)	MaskLoss 0.5874 (0.8287)	MaskBCELoss 0.0565 (0.1480)	MaskDICELoss 0.5309 (0.6807)
Epoch: [7][158/500]	Time  6.636 ( 6.636)	Loss 0.0908 (1.5872)	CeLoss 0.0908 (0.2208)	SegCLSLoss 0.0000 (0.0156)	KLLoss 0.0000 (0.3291)	MaskLoss 0.0000 (0.6627)	MaskBCELoss 0.0000 (0.1084)	MaskDICELoss 0.0000 (0.5543)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 156 is less than current step: 499. Dropping entry: {'train/loss': 2.0363898813724517, 'train/ce_loss': 0.338037109375, 'train/seg_cls_loss': 0.01578369140625, 'train/kl_loss': 0.3294921875, 'train/mask_bce_loss': 0.14801968466490506, 'train/mask_dice_loss': 0.6807221382856369, 'train/mask_loss': 0.8287418097257614, 'metrics/total_secs_per_batch': 7.067920207977295, 'metrics/data_secs_per_batch': 3.4383111715316774, '_timestamp': 1740975998.2218137}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 156 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740975998.222095}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 157 is less than current step: 499. Dropping entry: {'train/loss': 1.5871646046638488, 'train/ce_loss': 0.22080078125, 'train/seg_cls_loss': 0.015631103515625, 'train/kl_loss': 0.3291015625, 'train/mask_bce_loss': 0.10839759977534413, 'train/mask_dice_loss': 0.5543253183364868, 'train/mask_loss': 0.6627229169011116, 'metrics/total_secs_per_batch': 6.635894775390625, 'metrics/data_secs_per_batch': 2.900624322891235, '_timestamp': 1740976004.8577065}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 157 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976004.8579946}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 158 is less than current step: 499. Dropping entry: {'train/loss': 1.6247487545013428, 'train/ce_loss': 0.4111328125, 'train/seg_cls_loss': 0.012115478515625, 'train/kl_loss': 0.324609375, 'train/mask_bce_loss': 0.09841451095417142, 'train/mask_dice_loss': 0.48915517032146455, 'train/mask_loss': 0.5875696785748005, 'metrics/total_secs_per_batch': 5.964015483856201, 'metrics/data_secs_per_batch': 2.720256495475769, '_timestamp': 1740976010.8217022}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 158 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976010.8219826}).
Epoch: [7][159/500]	Time  5.964 ( 5.964)	Loss 2.1194 (1.6247)	CeLoss 0.2217 (0.4111)	SegCLSLoss 0.0209 (0.0121)	KLLoss 0.3516 (0.3246)	MaskLoss 0.9259 (0.5876)	MaskBCELoss 0.0613 (0.0984)	MaskDICELoss 0.8646 (0.4892)
[2025-03-02 22:26:56,218] [INFO] [logging.py:96:log_dist] [Rank 0] step=3660, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:26:56,224] [INFO] [timer.py:215:stop] epoch=0/micro_step=36600/global_step=3660, RunningAvgSamplesPerSec=1.4573821266615168, CurrSamplesPerSec=1.851212904552175, MemAllocated=30.95GB, MaxMemAllocated=37.23GB
Epoch: [7][160/500]	Time  5.403 ( 5.403)	Loss 0.6885 (1.3537)	CeLoss 0.2090 (0.4675)	SegCLSLoss 0.0206 (0.0111)	KLLoss 0.3809 (0.1859)	MaskLoss 0.2153 (0.4310)	MaskBCELoss 0.0971 (0.1082)	MaskDICELoss 0.1182 (0.3228)
Epoch: [7][161/500]	Time  5.632 ( 5.632)	Loss 0.5547 (1.8142)	CeLoss 0.5547 (0.5014)	SegCLSLoss 0.0000 (0.0115)	KLLoss 0.0000 (0.2920)	MaskLoss 0.0000 (0.6387)	MaskBCELoss 0.0000 (0.1247)	MaskDICELoss 0.0000 (0.5140)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 159 is less than current step: 499. Dropping entry: {'train/loss': 1.3536710917949677, 'train/ce_loss': 0.467529296875, 'train/seg_cls_loss': 0.01114501953125, 'train/kl_loss': 0.1859375, 'train/mask_bce_loss': 0.10822491496801376, 'train/mask_dice_loss': 0.32276102602481843, 'train/mask_loss': 0.43098593652248385, 'metrics/total_secs_per_batch': 5.403405427932739, 'metrics/data_secs_per_batch': 2.2982396125793456, '_timestamp': 1740976016.2249327}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 159 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976016.225128}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 160 is less than current step: 499. Dropping entry: {'train/loss': 1.8141767740249635, 'train/ce_loss': 0.5013671875, 'train/seg_cls_loss': 0.011480712890625, 'train/kl_loss': 0.2919921875, 'train/mask_bce_loss': 0.12470937930047513, 'train/mask_dice_loss': 0.5139707952737809, 'train/mask_loss': 0.6386801719665527, 'metrics/total_secs_per_batch': 5.632018804550171, 'metrics/data_secs_per_batch': 2.5859182834625245, '_timestamp': 1740976021.857144}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 160 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976021.8574176}).
Epoch: [7][162/500]	Time  7.009 ( 7.009)	Loss 1.5464 (1.9906)	CeLoss 0.2471 (0.3442)	SegCLSLoss 0.0123 (0.0162)	KLLoss 0.3730 (0.3314)	MaskLoss 0.6277 (0.8023)	MaskBCELoss 0.0736 (0.1145)	MaskDICELoss 0.5541 (0.6877)
Epoch: [7][163/500]	Time  6.863 ( 6.863)	Loss 2.4481 (1.7133)	CeLoss 0.2734 (0.3366)	SegCLSLoss 0.0239 (0.0136)	KLLoss 0.3535 (0.3244)	MaskLoss 1.0639 (0.6689)	MaskBCELoss 0.0797 (0.0823)	MaskDICELoss 0.9842 (0.5866)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 161 is less than current step: 499. Dropping entry: {'train/loss': 1.990574049949646, 'train/ce_loss': 0.34423828125, 'train/seg_cls_loss': 0.01624755859375, 'train/kl_loss': 0.3314453125, 'train/mask_bce_loss': 0.11453359080478549, 'train/mask_dice_loss': 0.6877358585596085, 'train/mask_loss': 0.8022694528102875, 'metrics/total_secs_per_batch': 7.0091633796691895, 'metrics/data_secs_per_batch': 2.8078752040863035, '_timestamp': 1740976028.8664489}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 161 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976028.8667896}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 162 is less than current step: 499. Dropping entry: {'train/loss': 1.7133491158485412, 'train/ce_loss': 0.33662109375, 'train/seg_cls_loss': 0.0136474609375, 'train/kl_loss': 0.3244140625, 'train/mask_bce_loss': 0.08228491791523992, 'train/mask_dice_loss': 0.5865966796875, 'train/mask_loss': 0.668881607055664, 'metrics/total_secs_per_batch': 6.862767219543457, 'metrics/data_secs_per_batch': 3.2590919733047485, '_timestamp': 1740976035.7291374}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 162 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976035.7294264}).
Epoch: [7][164/500]	Time  7.193 ( 7.193)	Loss 1.4195 (1.7348)	CeLoss 0.1982 (0.2446)	SegCLSLoss 0.0168 (0.0148)	KLLoss 0.3633 (0.3623)	MaskLoss 0.5882 (0.7232)	MaskBCELoss 0.0804 (0.0555)	MaskDICELoss 0.5078 (0.6677)
Epoch: [7][165/500]	Time  5.158 ( 5.158)	Loss 0.6423 (1.7181)	CeLoss 0.2109 (0.2949)	SegCLSLoss 0.0179 (0.0158)	KLLoss 0.3672 (0.3266)	MaskLoss 0.1932 (0.6911)	MaskBCELoss 0.0472 (0.1513)	MaskDICELoss 0.1460 (0.5398)
Epoch: [7][166/500]	Time  4.416 ( 4.416)	Loss 2.4743 (1.4316)	CeLoss 0.2051 (0.7369)	SegCLSLoss 0.0234 (0.0079)	KLLoss 0.3555 (0.1451)	MaskLoss 1.1112 (0.3381)	MaskBCELoss 0.2988 (0.0856)	MaskDICELoss 0.8124 (0.2525)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 163 is less than current step: 499. Dropping entry: {'train/loss': 1.7347973108291626, 'train/ce_loss': 0.24462890625, 'train/seg_cls_loss': 0.014801025390625, 'train/kl_loss': 0.3623046875, 'train/mask_bce_loss': 0.0555041691288352, 'train/mask_dice_loss': 0.6677050232887268, 'train/mask_loss': 0.7232091814279556, 'metrics/total_secs_per_batch': 7.1930365562438965, 'metrics/data_secs_per_batch': 3.2841299295425417, '_timestamp': 1740976042.9224772}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 163 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976042.9227352}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 164 is less than current step: 499. Dropping entry: {'train/loss': 1.7180575907230378, 'train/ce_loss': 0.294921875, 'train/seg_cls_loss': 0.0158203125, 'train/kl_loss': 0.3265625, 'train/mask_bce_loss': 0.15133359031751753, 'train/mask_dice_loss': 0.5397752821445465, 'train/mask_loss': 0.6911088705062867, 'metrics/total_secs_per_batch': 5.157824993133545, 'metrics/data_secs_per_batch': 2.4840172290802003, '_timestamp': 1740976048.0800018}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 164 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976048.0802894}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 165 is less than current step: 499. Dropping entry: {'train/loss': 1.4316248178482056, 'train/ce_loss': 0.7369140625, 'train/seg_cls_loss': 0.00787353515625, 'train/kl_loss': 0.1451171875, 'train/mask_bce_loss': 0.08559159738942981, 'train/mask_dice_loss': 0.25248644053936004, 'train/mask_loss': 0.3380780339241028, 'metrics/total_secs_per_batch': 4.415631294250488, 'metrics/data_secs_per_batch': 1.7388033151626587, '_timestamp': 1740976052.4958475}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 165 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976052.4962869}).
Epoch: [7][167/500]	Time  5.388 ( 5.388)	Loss 2.5911 (1.3973)	CeLoss 0.1943 (0.3363)	SegCLSLoss 0.0200 (0.0102)	KLLoss 0.3633 (0.2186)	MaskLoss 1.1754 (0.5169)	MaskBCELoss 0.2699 (0.1352)	MaskDICELoss 0.9055 (0.3817)
Epoch: [7][168/500]	Time  5.976 ( 5.976)	Loss 2.3660 (1.6292)	CeLoss 0.1914 (0.3243)	SegCLSLoss 0.0231 (0.0136)	KLLoss 0.3613 (0.2891)	MaskLoss 1.0634 (0.6346)	MaskBCELoss 0.2521 (0.1222)	MaskDICELoss 0.8113 (0.5124)
Epoch: [7][169/500]	Time  5.845 ( 5.845)	Loss 1.8335 (1.7421)	CeLoss 0.1836 (0.5099)	SegCLSLoss 0.0248 (0.0105)	KLLoss 0.3496 (0.2514)	MaskLoss 0.8010 (0.6010)	MaskBCELoss 0.0159 (0.1522)	MaskDICELoss 0.7851 (0.4488)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 166 is less than current step: 499. Dropping entry: {'train/loss': 1.3972529351711274, 'train/ce_loss': 0.336279296875, 'train/seg_cls_loss': 0.010211181640625, 'train/kl_loss': 0.2185546875, 'train/mask_bce_loss': 0.1351996049284935, 'train/mask_dice_loss': 0.38171298503875734, 'train/mask_loss': 0.5169125884771347, 'metrics/total_secs_per_batch': 5.38777232170105, 'metrics/data_secs_per_batch': 2.2804131746292113, '_timestamp': 1740976057.8834207}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 166 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976057.8836977}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 167 is less than current step: 499. Dropping entry: {'train/loss': 1.6292019009590148, 'train/ce_loss': 0.32431640625, 'train/seg_cls_loss': 0.0136474609375, 'train/kl_loss': 0.2890625, 'train/mask_bce_loss': 0.12218402940779924, 'train/mask_dice_loss': 0.5123876303434372, 'train/mask_loss': 0.6345716565847397, 'metrics/total_secs_per_batch': 5.976269483566284, 'metrics/data_secs_per_batch': 2.767596411705017, '_timestamp': 1740976063.8596532}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 167 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976063.8600104}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 168 is less than current step: 499. Dropping entry: {'train/loss': 1.7420710921287537, 'train/ce_loss': 0.50986328125, 'train/seg_cls_loss': 0.010516357421875, 'train/kl_loss': 0.2513671875, 'train/mask_bce_loss': 0.1521552262827754, 'train/mask_dice_loss': 0.44881196320056915, 'train/mask_loss': 0.6009671866893769, 'metrics/total_secs_per_batch': 5.844533205032349, 'metrics/data_secs_per_batch': 2.495290446281433, '_timestamp': 1740976069.7043827}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 168 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976069.704715}).
[2025-03-02 22:27:55,987] [INFO] [logging.py:96:log_dist] [Rank 0] step=3670, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:27:55,993] [INFO] [timer.py:215:stop] epoch=0/micro_step=36700/global_step=3670, RunningAvgSamplesPerSec=1.457895380528924, CurrSamplesPerSec=1.5903008537105088, MemAllocated=31.26GB, MaxMemAllocated=37.23GB
Epoch: [7][170/500]	Time  6.290 ( 6.290)	Loss 2.6284 (1.8668)	CeLoss 0.2080 (0.3364)	SegCLSLoss 0.0216 (0.0155)	KLLoss 0.3672 (0.2904)	MaskLoss 1.1863 (0.7468)	MaskBCELoss 0.3487 (0.1154)	MaskDICELoss 0.8375 (0.6314)
Epoch: [7][171/500]	Time  6.339 ( 6.339)	Loss 2.3147 (1.9589)	CeLoss 0.2520 (0.4113)	SegCLSLoss 0.0103 (0.0119)	KLLoss 0.3613 (0.2904)	MaskLoss 1.0109 (0.7563)	MaskBCELoss 0.1745 (0.2175)	MaskDICELoss 0.8363 (0.5387)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 169 is less than current step: 499. Dropping entry: {'train/loss': 1.8668169617652892, 'train/ce_loss': 0.33642578125, 'train/seg_cls_loss': 0.015478515625, 'train/kl_loss': 0.2904296875, 'train/mask_bce_loss': 0.11541884765028954, 'train/mask_dice_loss': 0.6313685506582261, 'train/mask_loss': 0.7467874050140381, 'metrics/total_secs_per_batch': 6.2899556159973145, 'metrics/data_secs_per_batch': 2.757462668418884, '_timestamp': 1740976075.9940126}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 169 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976075.9943073}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 170 is less than current step: 499. Dropping entry: {'train/loss': 1.9588622093200683, 'train/ce_loss': 0.411328125, 'train/seg_cls_loss': 0.011865234375, 'train/kl_loss': 0.2904296875, 'train/mask_bce_loss': 0.217541609890759, 'train/mask_dice_loss': 0.5387449711561203, 'train/mask_loss': 0.7562865689396858, 'metrics/total_secs_per_batch': 6.33867335319519, 'metrics/data_secs_per_batch': 2.6865360498428346, '_timestamp': 1740976082.3330896}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 170 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976082.3334649}).
Epoch: [7][172/500]	Time  5.793 ( 5.793)	Loss 1.5781 (2.0359)	CeLoss 1.5781 (0.4508)	SegCLSLoss 0.0000 (0.0158)	KLLoss 0.0000 (0.2895)	MaskLoss 0.0000 (0.7741)	MaskBCELoss 0.0000 (0.1462)	MaskDICELoss 0.0000 (0.6279)
Epoch: [7][173/500]	Time  5.820 ( 5.820)	Loss 0.0845 (1.4429)	CeLoss 0.0845 (0.4188)	SegCLSLoss 0.0000 (0.0114)	KLLoss 0.0000 (0.2566)	MaskLoss 0.0000 (0.4961)	MaskBCELoss 0.0000 (0.1446)	MaskDICELoss 0.0000 (0.3515)
Epoch: [7][174/500]	Time  4.486 ( 4.486)	Loss 2.0055 (1.7468)	CeLoss 0.2695 (0.6358)	SegCLSLoss 0.0099 (0.0081)	KLLoss 0.3633 (0.2189)	MaskLoss 0.8465 (0.5424)	MaskBCELoss 0.0122 (0.1269)	MaskDICELoss 0.8343 (0.4155)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 171 is less than current step: 499. Dropping entry: {'train/loss': 2.035925018787384, 'train/ce_loss': 0.45078125, 'train/seg_cls_loss': 0.015765380859375, 'train/kl_loss': 0.289453125, 'train/mask_bce_loss': 0.14618119904771448, 'train/mask_dice_loss': 0.6279336750507355, 'train/mask_loss': 0.774114853143692, 'metrics/total_secs_per_batch': 5.792562246322632, 'metrics/data_secs_per_batch': 2.6066392183303835, '_timestamp': 1740976088.1254344}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 171 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976088.1257296}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 172 is less than current step: 499. Dropping entry: {'train/loss': 1.442851710319519, 'train/ce_loss': 0.418798828125, 'train/seg_cls_loss': 0.011395263671875, 'train/kl_loss': 0.256640625, 'train/mask_bce_loss': 0.14458862021565438, 'train/mask_dice_loss': 0.35151985883712766, 'train/mask_loss': 0.4961084723472595, 'metrics/total_secs_per_batch': 5.819868087768555, 'metrics/data_secs_per_batch': 2.484974575042725, '_timestamp': 1740976093.9455597}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 172 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976093.9459248}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 173 is less than current step: 499. Dropping entry: {'train/loss': 1.746840500831604, 'train/ce_loss': 0.63583984375, 'train/seg_cls_loss': 0.008050537109375, 'train/kl_loss': 0.2189453125, 'train/mask_bce_loss': 0.12690731091424823, 'train/mask_dice_loss': 0.41550707817077637, 'train/mask_loss': 0.5424143731594085, 'metrics/total_secs_per_batch': 4.4856956005096436, 'metrics/data_secs_per_batch': 2.3361552000045775, '_timestamp': 1740976098.4310744}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 173 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976098.431378}).
Epoch: [7][175/500]	Time  6.567 ( 6.567)	Loss 2.1021 (2.0590)	CeLoss 0.2930 (0.3053)	SegCLSLoss 0.0121 (0.0165)	KLLoss 0.3613 (0.3275)	MaskLoss 0.8831 (0.8564)	MaskBCELoss 0.0376 (0.1251)	MaskDICELoss 0.8455 (0.7313)
Epoch: [7][176/500]	Time  5.378 ( 5.378)	Loss 0.9336 (1.6134)	CeLoss 0.9336 (0.4447)	SegCLSLoss 0.0000 (0.0149)	KLLoss 0.0000 (0.2562)	MaskLoss 0.0000 (0.5678)	MaskBCELoss 0.0000 (0.1378)	MaskDICELoss 0.0000 (0.4300)
Epoch: [7][177/500]	Time  5.881 ( 5.881)	Loss 0.9570 (1.7776)	CeLoss 0.9570 (0.5467)	SegCLSLoss 0.0000 (0.0124)	KLLoss 0.0000 (0.2596)	MaskLoss 0.0000 (0.5993)	MaskBCELoss 0.0000 (0.1200)	MaskDICELoss 0.0000 (0.4793)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 174 is less than current step: 499. Dropping entry: {'train/loss': 2.058990955352783, 'train/ce_loss': 0.3052734375, 'train/seg_cls_loss': 0.0164794921875, 'train/kl_loss': 0.3275390625, 'train/mask_bce_loss': 0.12508732499554753, 'train/mask_dice_loss': 0.7313124537467957, 'train/mask_loss': 0.8563997805118561, 'metrics/total_secs_per_batch': 6.567253589630127, 'metrics/data_secs_per_batch': 2.861125135421753, '_timestamp': 1740976104.998284}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 174 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976104.9985764}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 175 is less than current step: 499. Dropping entry: {'train/loss': 1.6133659839630128, 'train/ce_loss': 0.4447265625, 'train/seg_cls_loss': 0.01488037109375, 'train/kl_loss': 0.25625, 'train/mask_bce_loss': 0.1377910038456321, 'train/mask_dice_loss': 0.4300247967243195, 'train/mask_loss': 0.5678157985210419, 'metrics/total_secs_per_batch': 5.378039836883545, 'metrics/data_secs_per_batch': 2.550539660453796, '_timestamp': 1740976110.3764744}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 175 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976110.3768265}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 176 is less than current step: 499. Dropping entry: {'train/loss': 1.777563178539276, 'train/ce_loss': 0.5466796875, 'train/seg_cls_loss': 0.012396240234375, 'train/kl_loss': 0.2595703125, 'train/mask_bce_loss': 0.11997514367103576, 'train/mask_dice_loss': 0.4793044954538345, 'train/mask_loss': 0.5992796361446381, 'metrics/total_secs_per_batch': 5.8814027309417725, 'metrics/data_secs_per_batch': 2.7473206520080566, '_timestamp': 1740976116.257676}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 176 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976116.2579598}).
Epoch: [7][178/500]	Time  6.344 ( 6.344)	Loss 1.9448 (1.6540)	CeLoss 0.2061 (0.3558)	SegCLSLoss 0.0210 (0.0141)	KLLoss 0.3516 (0.2875)	MaskLoss 0.8464 (0.6312)	MaskBCELoss 0.0553 (0.1324)	MaskDICELoss 0.7911 (0.4988)
Epoch: [7][179/500]	Time  5.282 ( 5.282)	Loss 1.0781 (1.1933)	CeLoss 1.0781 (0.6725)	SegCLSLoss 0.0000 (0.0065)	KLLoss 0.0000 (0.1457)	MaskLoss 0.0000 (0.2514)	MaskBCELoss 0.0000 (0.0550)	MaskDICELoss 0.0000 (0.1964)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 177 is less than current step: 499. Dropping entry: {'train/loss': 1.654011571407318, 'train/ce_loss': 0.35576171875, 'train/seg_cls_loss': 0.014093017578125, 'train/kl_loss': 0.2875, 'train/mask_bce_loss': 0.13244339562952517, 'train/mask_dice_loss': 0.49876160770654676, 'train/mask_loss': 0.631205004453659, 'metrics/total_secs_per_batch': 6.34383749961853, 'metrics/data_secs_per_batch': 2.5672784566879274, '_timestamp': 1740976122.6017087}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 177 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976122.6020691}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 178 is less than current step: 499. Dropping entry: {'train/loss': 1.1933299779891968, 'train/ce_loss': 0.672509765625, 'train/seg_cls_loss': 0.006500244140625, 'train/kl_loss': 0.145703125, 'train/mask_bce_loss': 0.05499414969235659, 'train/mask_dice_loss': 0.1963827520608902, 'train/mask_loss': 0.25137690305709837, 'metrics/total_secs_per_batch': 5.282382011413574, 'metrics/data_secs_per_batch': 2.580943059921265, '_timestamp': 1740976127.8839624}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 178 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976127.8842611}).
[2025-03-02 22:28:53,938] [INFO] [logging.py:96:log_dist] [Rank 0] step=3680, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:28:53,943] [INFO] [timer.py:215:stop] epoch=0/micro_step=36800/global_step=3680, RunningAvgSamplesPerSec=1.458511405003426, CurrSamplesPerSec=1.6504858112974294, MemAllocated=31.28GB, MaxMemAllocated=37.23GB
Epoch: [7][180/500]	Time  6.061 ( 6.061)	Loss 1.8619 (1.9128)	CeLoss 0.1953 (0.4767)	SegCLSLoss 0.0271 (0.0157)	KLLoss 0.3711 (0.2893)	MaskLoss 0.8079 (0.6997)	MaskBCELoss 0.2127 (0.1586)	MaskDICELoss 0.5952 (0.5411)
Epoch: [7][181/500]	Time  6.636 ( 6.636)	Loss 2.2449 (2.1330)	CeLoss 0.2129 (0.3177)	SegCLSLoss 0.0168 (0.0162)	KLLoss 0.3555 (0.3285)	MaskLoss 0.9936 (0.8871)	MaskBCELoss 0.0382 (0.2400)	MaskDICELoss 0.9554 (0.6471)
Epoch: [7][182/500]	Time  6.377 ( 6.377)	Loss 1.9215 (1.3690)	CeLoss 0.2354 (0.3825)	SegCLSLoss 0.0211 (0.0117)	KLLoss 0.3613 (0.2178)	MaskLoss 0.8201 (0.4795)	MaskBCELoss 0.0606 (0.0820)	MaskDICELoss 0.7595 (0.3975)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 179 is less than current step: 499. Dropping entry: {'train/loss': 1.912754225730896, 'train/ce_loss': 0.47666015625, 'train/seg_cls_loss': 0.01566162109375, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.15864187176339328, 'train/mask_dice_loss': 0.5410946071147918, 'train/mask_loss': 0.6997364819049835, 'metrics/total_secs_per_batch': 6.060516119003296, 'metrics/data_secs_per_batch': 2.802249383926392, '_timestamp': 1740976133.9443226}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 179 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976133.9446812}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 180 is less than current step: 499. Dropping entry: {'train/loss': 2.1329859375953673, 'train/ce_loss': 0.31767578125, 'train/seg_cls_loss': 0.0162353515625, 'train/kl_loss': 0.328515625, 'train/mask_bce_loss': 0.23996423184871674, 'train/mask_dice_loss': 0.6471341997385025, 'train/mask_loss': 0.8870984315872192, 'metrics/total_secs_per_batch': 6.6362388134002686, 'metrics/data_secs_per_batch': 2.858365535736084, '_timestamp': 1740976140.5806866}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 180 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976140.5809703}).
Epoch: [7][183/500]	Time  5.456 ( 5.456)	Loss 2.1010 (1.3839)	CeLoss 0.2393 (0.4422)	SegCLSLoss 0.0127 (0.0100)	KLLoss 0.3594 (0.2193)	MaskLoss 0.9099 (0.4574)	MaskBCELoss 0.0059 (0.0588)	MaskDICELoss 0.9040 (0.3986)
Epoch: [7][184/500]	Time  6.245 ( 6.245)	Loss 1.5460 (1.6179)	CeLoss 0.3105 (0.4382)	SegCLSLoss 0.0149 (0.0110)	KLLoss 0.3691 (0.2590)	MaskLoss 0.5953 (0.5741)	MaskBCELoss 0.2893 (0.1762)	MaskDICELoss 0.3059 (0.3979)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 181 is less than current step: 499. Dropping entry: {'train/loss': 1.3690235137939453, 'train/ce_loss': 0.38251953125, 'train/seg_cls_loss': 0.011712646484375, 'train/kl_loss': 0.2177734375, 'train/mask_bce_loss': 0.08196375370025635, 'train/mask_dice_loss': 0.39751870930194855, 'train/mask_loss': 0.47948246002197265, 'metrics/total_secs_per_batch': 6.37736964225769, 'metrics/data_secs_per_batch': 2.9083280086517336, '_timestamp': 1740976146.9581351}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 181 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976146.958473}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 182 is less than current step: 499. Dropping entry: {'train/loss': 1.383923387527466, 'train/ce_loss': 0.4421875, 'train/seg_cls_loss': 0.010009765625, 'train/kl_loss': 0.2193359375, 'train/mask_bce_loss': 0.05879126382060349, 'train/mask_dice_loss': 0.39864895343780515, 'train/mask_loss': 0.4574402093887329, 'metrics/total_secs_per_batch': 5.45575213432312, 'metrics/data_secs_per_batch': 2.5388697147369386, '_timestamp': 1740976152.4137802}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 182 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976152.4141407}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 183 is less than current step: 499. Dropping entry: {'train/loss': 1.6179330348968506, 'train/ce_loss': 0.43818359375, 'train/seg_cls_loss': 0.011029052734375, 'train/kl_loss': 0.258984375, 'train/mask_bce_loss': 0.17620415911078452, 'train/mask_dice_loss': 0.39789906442165374, 'train/mask_loss': 0.5741032361984253, 'metrics/total_secs_per_batch': 6.245422840118408, 'metrics/data_secs_per_batch': 2.4676031351089476, '_timestamp': 1740976158.6594918}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 183 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976158.6598544}).
Epoch: [7][185/500]	Time  6.074 ( 6.074)	Loss 1.2817 (1.8934)	CeLoss 0.2734 (0.2262)	SegCLSLoss 0.0093 (0.0180)	KLLoss 0.3652 (0.3619)	MaskLoss 0.4836 (0.8111)	MaskBCELoss 0.0990 (0.1021)	MaskDICELoss 0.3846 (0.7090)
Epoch: [7][186/500]	Time  6.169 ( 6.169)	Loss 0.0593 (1.5000)	CeLoss 0.0593 (0.3275)	SegCLSLoss 0.0000 (0.0149)	KLLoss 0.0000 (0.2867)	MaskLoss 0.0000 (0.5683)	MaskBCELoss 0.0000 (0.1145)	MaskDICELoss 0.0000 (0.4537)
Epoch: [7][187/500]	Time  5.893 ( 5.893)	Loss 2.3111 (1.5324)	CeLoss 0.2324 (0.4359)	SegCLSLoss 0.0156 (0.0116)	KLLoss 0.3691 (0.2182)	MaskLoss 1.0169 (0.5345)	MaskBCELoss 0.0250 (0.1077)	MaskDICELoss 0.9919 (0.4268)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 184 is less than current step: 499. Dropping entry: {'train/loss': 1.8934320986270905, 'train/ce_loss': 0.226171875, 'train/seg_cls_loss': 0.01798095703125, 'train/kl_loss': 0.3619140625, 'train/mask_bce_loss': 0.10206945408135652, 'train/mask_dice_loss': 0.7090020567178726, 'train/mask_loss': 0.8110715061426162, 'metrics/total_secs_per_batch': 6.0742998123168945, 'metrics/data_secs_per_batch': 2.72551052570343, '_timestamp': 1740976164.7335892}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 184 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976164.7338774}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 185 is less than current step: 499. Dropping entry: {'train/loss': 1.500049924850464, 'train/ce_loss': 0.3275146484375, 'train/seg_cls_loss': 0.01490478515625, 'train/kl_loss': 0.28671875, 'train/mask_bce_loss': 0.11450146045535803, 'train/mask_dice_loss': 0.45374860167503356, 'train/mask_loss': 0.5682500600814819, 'metrics/total_secs_per_batch': 6.169072389602661, 'metrics/data_secs_per_batch': 2.770829463005066, '_timestamp': 1740976170.9028563}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 185 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976170.9031959}).
Epoch: [7][188/500]	Time  5.693 ( 5.693)	Loss 1.7098 (1.2169)	CeLoss 0.3984 (0.3994)	SegCLSLoss 0.0126 (0.0121)	KLLoss 0.3633 (0.2973)	MaskLoss 0.6342 (0.3907)	MaskBCELoss 0.1031 (0.0808)	MaskDICELoss 0.5311 (0.3099)
Epoch: [7][189/500]	Time  6.281 ( 6.281)	Loss 1.3125 (1.5632)	CeLoss 1.3125 (0.3274)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.2924)	MaskLoss 0.0000 (0.5997)	MaskBCELoss 0.0000 (0.0871)	MaskDICELoss 0.0000 (0.5126)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 186 is less than current step: 499. Dropping entry: {'train/loss': 1.5323869228363036, 'train/ce_loss': 0.4359375, 'train/seg_cls_loss': 0.01158447265625, 'train/kl_loss': 0.2181640625, 'train/mask_bce_loss': 0.10766416247934103, 'train/mask_dice_loss': 0.4268154129385948, 'train/mask_loss': 0.5344795823097229, 'metrics/total_secs_per_batch': 5.892592430114746, 'metrics/data_secs_per_batch': 2.3574870109558104, '_timestamp': 1740976176.795204}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 186 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976176.795469}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 187 is less than current step: 499. Dropping entry: {'train/loss': 1.2168864965438844, 'train/ce_loss': 0.399365234375, 'train/seg_cls_loss': 0.01209716796875, 'train/kl_loss': 0.297265625, 'train/mask_bce_loss': 0.08079066388309002, 'train/mask_dice_loss': 0.3099035620689392, 'train/mask_loss': 0.39069422483444216, 'metrics/total_secs_per_batch': 5.692589282989502, 'metrics/data_secs_per_batch': 2.4897785425186156, '_timestamp': 1740976182.4878495}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 187 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976182.4881501}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 188 is less than current step: 499. Dropping entry: {'train/loss': 1.5632429838180542, 'train/ce_loss': 0.3273681640625, 'train/seg_cls_loss': 0.01417236328125, 'train/kl_loss': 0.2923828125, 'train/mask_bce_loss': 0.0871085312217474, 'train/mask_dice_loss': 0.51256715208292, 'train/mask_loss': 0.5996756851673126, 'metrics/total_secs_per_batch': 6.280960321426392, 'metrics/data_secs_per_batch': 2.991734528541565, '_timestamp': 1740976188.7689714}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 188 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976188.7693255}).
[2025-03-02 22:29:55,517] [INFO] [logging.py:96:log_dist] [Rank 0] step=3690, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:29:55,523] [INFO] [timer.py:215:stop] epoch=0/micro_step=36900/global_step=3690, RunningAvgSamplesPerSec=1.4589151088945715, CurrSamplesPerSec=1.4807287100218545, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [7][190/500]	Time  6.755 ( 6.755)	Loss 2.0764 (1.5839)	CeLoss 0.1826 (0.2337)	SegCLSLoss 0.0238 (0.0162)	KLLoss 0.3633 (0.3613)	MaskLoss 0.9230 (0.6529)	MaskBCELoss 0.0164 (0.1028)	MaskDICELoss 0.9066 (0.5501)
Epoch: [7][191/500]	Time  6.869 ( 6.869)	Loss 0.0566 (1.2743)	CeLoss 0.0566 (0.2093)	SegCLSLoss 0.0000 (0.0111)	KLLoss 0.0000 (0.2918)	MaskLoss 0.0000 (0.5152)	MaskBCELoss 0.0000 (0.1415)	MaskDICELoss 0.0000 (0.3738)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 189 is less than current step: 499. Dropping entry: {'train/loss': 1.5838549375534057, 'train/ce_loss': 0.23369140625, 'train/seg_cls_loss': 0.01622314453125, 'train/kl_loss': 0.361328125, 'train/mask_bce_loss': 0.10281044412404299, 'train/mask_dice_loss': 0.5500545144081116, 'train/mask_loss': 0.6528649628162384, 'metrics/total_secs_per_batch': 6.755299806594849, 'metrics/data_secs_per_batch': 3.1020012378692625, '_timestamp': 1740976195.5238729}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 189 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976195.5241463}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 190 is less than current step: 499. Dropping entry: {'train/loss': 1.27428041100502, 'train/ce_loss': 0.2092529296875, 'train/seg_cls_loss': 0.011090087890625, 'train/kl_loss': 0.291796875, 'train/mask_bce_loss': 0.14145615547895432, 'train/mask_dice_loss': 0.3737724184989929, 'train/mask_loss': 0.5152285724878312, 'metrics/total_secs_per_batch': 6.868770122528076, 'metrics/data_secs_per_batch': 2.9458157539367678, '_timestamp': 1740976202.3930416}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 190 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976202.393413}).
Epoch: [7][192/500]	Time  5.554 ( 5.554)	Loss 2.3081 (1.7564)	CeLoss 0.1787 (0.4458)	SegCLSLoss 0.0260 (0.0134)	KLLoss 0.3555 (0.2916)	MaskLoss 1.0408 (0.6374)	MaskBCELoss 0.0448 (0.0894)	MaskDICELoss 0.9960 (0.5480)
Epoch: [7][193/500]	Time  5.329 ( 5.329)	Loss 1.0938 (1.5491)	CeLoss 0.2520 (0.4712)	SegCLSLoss 0.0118 (0.0144)	KLLoss 0.3750 (0.2559)	MaskLoss 0.3994 (0.5225)	MaskBCELoss 0.0758 (0.0690)	MaskDICELoss 0.3236 (0.4536)
Epoch: [7][194/500]	Time  5.437 ( 5.437)	Loss 0.2334 (1.3631)	CeLoss 0.2334 (0.6064)	SegCLSLoss 0.0000 (0.0060)	KLLoss 0.0000 (0.1426)	MaskLoss 0.0000 (0.3697)	MaskBCELoss 0.0000 (0.0736)	MaskDICELoss 0.0000 (0.2961)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 191 is less than current step: 499. Dropping entry: {'train/loss': 1.7563781976699828, 'train/ce_loss': 0.44580078125, 'train/seg_cls_loss': 0.01343994140625, 'train/kl_loss': 0.2916015625, 'train/mask_bce_loss': 0.0893672642763704, 'train/mask_dice_loss': 0.5480015471577644, 'train/mask_loss': 0.6373688027262687, 'metrics/total_secs_per_batch': 5.554219722747803, 'metrics/data_secs_per_batch': 2.376052403450012, '_timestamp': 1740976207.9470987}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 191 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976207.9473863}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 192 is less than current step: 499. Dropping entry: {'train/loss': 1.5490952551364898, 'train/ce_loss': 0.47119140625, 'train/seg_cls_loss': 0.014385986328125, 'train/kl_loss': 0.255859375, 'train/mask_bce_loss': 0.06895133102370891, 'train/mask_dice_loss': 0.4535943418741226, 'train/mask_loss': 0.5225456714630127, 'metrics/total_secs_per_batch': 5.329159259796143, 'metrics/data_secs_per_batch': 2.280159020423889, '_timestamp': 1740976213.276477}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 192 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976213.276843}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 193 is less than current step: 499. Dropping entry: {'train/loss': 1.3630578517913818, 'train/ce_loss': 0.6064453125, 'train/seg_cls_loss': 0.006036376953125, 'train/kl_loss': 0.142578125, 'train/mask_bce_loss': 0.07358422875404358, 'train/mask_dice_loss': 0.2960794627666473, 'train/mask_loss': 0.3696636915206909, 'metrics/total_secs_per_batch': 5.436907529830933, 'metrics/data_secs_per_batch': 2.221820569038391, '_timestamp': 1740976218.713172}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 193 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976218.7134914}).
Epoch: [7][195/500]	Time  6.453 ( 6.453)	Loss 1.6745 (1.7160)	CeLoss 0.2227 (0.5167)	SegCLSLoss 0.0106 (0.0113)	KLLoss 0.3672 (0.2576)	MaskLoss 0.7054 (0.5840)	MaskBCELoss 0.2123 (0.1345)	MaskDICELoss 0.4931 (0.4495)
Epoch: [7][196/500]	Time  6.437 ( 6.437)	Loss 1.4531 (2.0228)	CeLoss 1.4531 (0.3373)	SegCLSLoss 0.0000 (0.0155)	KLLoss 0.0000 (0.2934)	MaskLoss 0.0000 (0.8243)	MaskBCELoss 0.0000 (0.2401)	MaskDICELoss 0.0000 (0.5842)
Epoch: [7][197/500]	Time  4.823 ( 4.823)	Loss 1.7109 (1.6138)	CeLoss 1.7109 (0.7848)	SegCLSLoss 0.0000 (0.0084)	KLLoss 0.0000 (0.1783)	MaskLoss 0.0000 (0.4034)	MaskBCELoss 0.0000 (0.0970)	MaskDICELoss 0.0000 (0.3064)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 194 is less than current step: 499. Dropping entry: {'train/loss': 1.7160474956035614, 'train/ce_loss': 0.51669921875, 'train/seg_cls_loss': 0.011309814453125, 'train/kl_loss': 0.2576171875, 'train/mask_bce_loss': 0.1344738389365375, 'train/mask_dice_loss': 0.449526459723711, 'train/mask_loss': 0.5840003177523613, 'metrics/total_secs_per_batch': 6.452811002731323, 'metrics/data_secs_per_batch': 2.6528143167495726, '_timestamp': 1740976225.1661785}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 194 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976225.1664085}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 195 is less than current step: 499. Dropping entry: {'train/loss': 2.022811841964722, 'train/ce_loss': 0.337255859375, 'train/seg_cls_loss': 0.015484619140625, 'train/kl_loss': 0.293359375, 'train/mask_bce_loss': 0.2401170875877142, 'train/mask_dice_loss': 0.5842038780450821, 'train/mask_loss': 0.8243209660053253, 'metrics/total_secs_per_batch': 6.43665337562561, 'metrics/data_secs_per_batch': 3.172989273071289, '_timestamp': 1740976231.602633}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 195 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976231.602926}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 196 is less than current step: 499. Dropping entry: {'train/loss': 1.6138052582740783, 'train/ce_loss': 0.784765625, 'train/seg_cls_loss': 0.0083984375, 'train/kl_loss': 0.1783203125, 'train/mask_bce_loss': 0.09699526019394397, 'train/mask_dice_loss': 0.3063917487859726, 'train/mask_loss': 0.4033869981765747, 'metrics/total_secs_per_batch': 4.823044300079346, 'metrics/data_secs_per_batch': 2.4273263931274416, '_timestamp': 1740976236.4257631}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 196 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976236.4260938}).
Epoch: [7][198/500]	Time  5.955 ( 5.955)	Loss 1.4755 (1.7138)	CeLoss 0.2793 (0.3030)	SegCLSLoss 0.0112 (0.0109)	KLLoss 0.3633 (0.2928)	MaskLoss 0.5766 (0.6879)	MaskBCELoss 0.0530 (0.1628)	MaskDICELoss 0.5237 (0.5251)
Epoch: [7][199/500]	Time  5.246 ( 5.246)	Loss 1.8461 (1.1512)	CeLoss 0.1953 (0.5119)	SegCLSLoss 0.0228 (0.0088)	KLLoss 0.3613 (0.1801)	MaskLoss 0.8015 (0.3084)	MaskBCELoss 0.0322 (0.0193)	MaskDICELoss 0.7693 (0.2891)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 197 is less than current step: 499. Dropping entry: {'train/loss': 1.7138190627098084, 'train/ce_loss': 0.3030029296875, 'train/seg_cls_loss': 0.010870361328125, 'train/kl_loss': 0.2927734375, 'train/mask_bce_loss': 0.1627836567349732, 'train/mask_dice_loss': 0.5250951111316681, 'train/mask_loss': 0.6878787755966187, 'metrics/total_secs_per_batch': 5.95496392250061, 'metrics/data_secs_per_batch': 2.700583481788635, '_timestamp': 1740976242.3807733}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 197 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976242.381107}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 198 is less than current step: 499. Dropping entry: {'train/loss': 1.151153066754341, 'train/ce_loss': 0.5118896484375, 'train/seg_cls_loss': 0.008843994140625, 'train/kl_loss': 0.180078125, 'train/mask_bce_loss': 0.01931900130584836, 'train/mask_dice_loss': 0.289082233607769, 'train/mask_loss': 0.3084012344479561, 'metrics/total_secs_per_batch': 5.246258020401001, 'metrics/data_secs_per_batch': 2.485930252075195, '_timestamp': 1740976247.626904}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 198 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976247.627181}).
[2025-03-02 22:30:54,049] [INFO] [logging.py:96:log_dist] [Rank 0] step=3700, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:30:54,054] [INFO] [timer.py:215:stop] epoch=0/micro_step=37000/global_step=3700, RunningAvgSamplesPerSec=1.4594924027490246, CurrSamplesPerSec=1.5559328252249105, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [7][200/500]	Time  6.429 ( 6.429)	Loss 1.2755 (1.9468)	CeLoss 0.2578 (0.3320)	SegCLSLoss 0.0099 (0.0161)	KLLoss 0.3594 (0.3301)	MaskLoss 0.4883 (0.7868)	MaskBCELoss 0.0611 (0.1548)	MaskDICELoss 0.4272 (0.6320)
Epoch: [7][201/500]	Time  5.233 ( 5.233)	Loss 2.3993 (1.2649)	CeLoss 0.1055 (0.4414)	SegCLSLoss 0.0325 (0.0092)	KLLoss 0.3828 (0.2195)	MaskLoss 1.1196 (0.3985)	MaskBCELoss 0.2787 (0.0949)	MaskDICELoss 0.8408 (0.3036)
Epoch: [7][202/500]	Time  5.967 ( 5.967)	Loss 2.6656 (1.8953)	CeLoss 0.2344 (0.4330)	SegCLSLoss 0.0151 (0.0130)	KLLoss 0.3672 (0.2926)	MaskLoss 1.1931 (0.7133)	MaskBCELoss 0.4271 (0.1779)	MaskDICELoss 0.7661 (0.5354)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 199 is less than current step: 499. Dropping entry: {'train/loss': 1.9468156337738036, 'train/ce_loss': 0.33203125, 'train/seg_cls_loss': 0.016064453125, 'train/kl_loss': 0.330078125, 'train/mask_bce_loss': 0.1548100158572197, 'train/mask_dice_loss': 0.631976705789566, 'train/mask_loss': 0.7867867231369019, 'metrics/total_secs_per_batch': 6.428638696670532, 'metrics/data_secs_per_batch': 2.5801838397979737, '_timestamp': 1740976254.0554302}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 199 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976254.055788}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 200 is less than current step: 499. Dropping entry: {'train/loss': 1.2649142444133759, 'train/ce_loss': 0.441357421875, 'train/seg_cls_loss': 0.009197998046875, 'train/kl_loss': 0.21953125, 'train/mask_bce_loss': 0.09491022825241088, 'train/mask_dice_loss': 0.303586933016777, 'train/mask_loss': 0.3984971553087234, 'metrics/total_secs_per_batch': 5.232642650604248, 'metrics/data_secs_per_batch': 2.1477955102920534, '_timestamp': 1740976259.288155}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 200 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976259.2884324}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 201 is less than current step: 499. Dropping entry: {'train/loss': 1.895311748981476, 'train/ce_loss': 0.4330078125, 'train/seg_cls_loss': 0.0129638671875, 'train/kl_loss': 0.292578125, 'train/mask_bce_loss': 0.1778804781381041, 'train/mask_dice_loss': 0.5354492455720902, 'train/mask_loss': 0.7133297026157379, 'metrics/total_secs_per_batch': 5.967119216918945, 'metrics/data_secs_per_batch': 2.632953906059265, '_timestamp': 1740976265.2552934}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 201 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976265.2555711}).
Epoch: [7][203/500]	Time  4.896 ( 4.896)	Loss 1.7869 (1.7494)	CeLoss 0.2002 (0.4536)	SegCLSLoss 0.0243 (0.0149)	KLLoss 0.3535 (0.2598)	MaskLoss 0.7699 (0.6313)	MaskBCELoss 0.5108 (0.2050)	MaskDICELoss 0.2591 (0.4262)
Epoch: [7][204/500]	Time  5.818 ( 5.818)	Loss 1.3593 (1.8830)	CeLoss 0.2754 (0.4184)	SegCLSLoss 0.0094 (0.0169)	KLLoss 0.3711 (0.2922)	MaskLoss 0.5215 (0.7135)	MaskBCELoss 0.0572 (0.1393)	MaskDICELoss 0.4643 (0.5742)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 202 is less than current step: 499. Dropping entry: {'train/loss': 1.7494253396987915, 'train/ce_loss': 0.453564453125, 'train/seg_cls_loss': 0.0149169921875, 'train/kl_loss': 0.259765625, 'train/mask_bce_loss': 0.20504852421581746, 'train/mask_dice_loss': 0.42620711028575897, 'train/mask_loss': 0.6312556356191635, 'metrics/total_secs_per_batch': 4.895763158798218, 'metrics/data_secs_per_batch': 2.2395179510116576, '_timestamp': 1740976270.151179}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 202 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976270.1515052}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 203 is less than current step: 499. Dropping entry: {'train/loss': 1.882996106147766, 'train/ce_loss': 0.418359375, 'train/seg_cls_loss': 0.016888427734375, 'train/kl_loss': 0.2921875, 'train/mask_bce_loss': 0.13931283485144377, 'train/mask_dice_loss': 0.5741578787565231, 'train/mask_loss': 0.7134707033634186, 'metrics/total_secs_per_batch': 5.818201541900635, 'metrics/data_secs_per_batch': 2.687899589538574, '_timestamp': 1740976275.969387}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 203 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976275.9697008}).
Epoch: [7][205/500]	Time  6.615 ( 6.615)	Loss 1.7855 (1.9852)	CeLoss 0.2295 (0.2346)	SegCLSLoss 0.0140 (0.0149)	KLLoss 0.3691 (0.3660)	MaskLoss 0.7560 (0.8534)	MaskBCELoss 0.0816 (0.2499)	MaskDICELoss 0.6744 (0.6035)
Epoch: [7][206/500]	Time  7.326 ( 7.326)	Loss 1.4699 (1.7965)	CeLoss 0.2129 (0.2002)	SegCLSLoss 0.0104 (0.0174)	KLLoss 0.3633 (0.3283)	MaskLoss 0.6080 (0.7772)	MaskBCELoss 0.0532 (0.1137)	MaskDICELoss 0.5548 (0.6635)
Epoch: [7][207/500]	Time  7.342 ( 7.342)	Loss 1.7513 (1.9564)	CeLoss 0.1875 (0.1908)	SegCLSLoss 0.0267 (0.0213)	KLLoss 0.3711 (0.3619)	MaskLoss 0.7565 (0.8593)	MaskBCELoss 0.0921 (0.1244)	MaskDICELoss 0.6644 (0.7349)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 204 is less than current step: 499. Dropping entry: {'train/loss': 1.9852365493774413, 'train/ce_loss': 0.2345703125, 'train/seg_cls_loss': 0.01494140625, 'train/kl_loss': 0.366015625, 'train/mask_bce_loss': 0.24991671731695533, 'train/mask_dice_loss': 0.6034925580024719, 'train/mask_loss': 0.8534092903137207, 'metrics/total_secs_per_batch': 6.615177154541016, 'metrics/data_secs_per_batch': 3.0888091325759888, '_timestamp': 1740976282.5844939}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 204 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976282.584795}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 205 is less than current step: 499. Dropping entry: {'train/loss': 1.7964680194854736, 'train/ce_loss': 0.2001953125, 'train/seg_cls_loss': 0.0173828125, 'train/kl_loss': 0.3283203125, 'train/mask_bce_loss': 0.11372441789135337, 'train/mask_dice_loss': 0.6635134994983674, 'train/mask_loss': 0.7772379279136657, 'metrics/total_secs_per_batch': 7.3257575035095215, 'metrics/data_secs_per_batch': 3.125536632537842, '_timestamp': 1740976289.9104393}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 205 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976289.9108014}).
Epoch: [7][208/500]	Time  6.001 ( 6.001)	Loss 2.2203 (1.9159)	CeLoss 0.1885 (0.3516)	SegCLSLoss 0.0193 (0.0137)	KLLoss 0.3652 (0.3271)	MaskLoss 0.9929 (0.7625)	MaskBCELoss 0.3440 (0.2048)	MaskDICELoss 0.6490 (0.5577)
Epoch: [7][209/500]	Time  6.298 ( 6.298)	Loss 1.8014 (1.6593)	CeLoss 0.2334 (0.4072)	SegCLSLoss 0.0131 (0.0107)	KLLoss 0.3633 (0.2533)	MaskLoss 0.7620 (0.6105)	MaskBCELoss 0.0347 (0.1088)	MaskDICELoss 0.7273 (0.5017)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 206 is less than current step: 499. Dropping entry: {'train/loss': 1.9564354062080382, 'train/ce_loss': 0.1908203125, 'train/seg_cls_loss': 0.0213134765625, 'train/kl_loss': 0.3619140625, 'train/mask_bce_loss': 0.12444118689745665, 'train/mask_dice_loss': 0.7348800152540207, 'train/mask_loss': 0.8593212008476258, 'metrics/total_secs_per_batch': 7.342483997344971, 'metrics/data_secs_per_batch': 3.4436028957366944, '_timestamp': 1740976297.2526758}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 206 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976297.2529533}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 207 is less than current step: 499. Dropping entry: {'train/loss': 1.9159492373466491, 'train/ce_loss': 0.3515625, 'train/seg_cls_loss': 0.0137451171875, 'train/kl_loss': 0.3271484375, 'train/mask_bce_loss': 0.2047695189714432, 'train/mask_dice_loss': 0.5576972812414169, 'train/mask_loss': 0.7624668002128601, 'metrics/total_secs_per_batch': 6.001004457473755, 'metrics/data_secs_per_batch': 2.8256517887115478, '_timestamp': 1740976303.2538908}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 207 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976303.254323}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 208 is less than current step: 499. Dropping entry: {'train/loss': 1.659278154373169, 'train/ce_loss': 0.4072265625, 'train/seg_cls_loss': 0.010687255859375, 'train/kl_loss': 0.2533203125, 'train/mask_bce_loss': 0.10884720794856548, 'train/mask_dice_loss': 0.5016512453556061, 'train/mask_loss': 0.6104984492063522, 'metrics/total_secs_per_batch': 6.297580003738403, 'metrics/data_secs_per_batch': 2.658175754547119, '_timestamp': 1740976309.5513403}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 208 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976309.5516858}).
[2025-03-02 22:31:56,088] [INFO] [logging.py:96:log_dist] [Rank 0] step=3710, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:31:56,093] [INFO] [timer.py:215:stop] epoch=0/micro_step=37100/global_step=3710, RunningAvgSamplesPerSec=1.459865419571762, CurrSamplesPerSec=1.52879361921816, MemAllocated=31.1GB, MaxMemAllocated=37.23GB
Epoch: [7][210/500]	Time  6.543 ( 6.543)	Loss 1.5440 (1.8058)	CeLoss 0.2363 (0.2812)	SegCLSLoss 0.0101 (0.0147)	KLLoss 0.3652 (0.3316)	MaskLoss 0.6333 (0.7420)	MaskBCELoss 0.2007 (0.1842)	MaskDICELoss 0.4327 (0.5578)
Epoch: [7][211/500]	Time  6.395 ( 6.395)	Loss 2.2644 (1.7891)	CeLoss 0.1943 (0.1901)	SegCLSLoss 0.0220 (0.0193)	KLLoss 0.3691 (0.3312)	MaskLoss 1.0111 (0.7781)	MaskBCELoss 0.1932 (0.1406)	MaskDICELoss 0.8179 (0.6376)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 209 is less than current step: 499. Dropping entry: {'train/loss': 1.8058016538619994, 'train/ce_loss': 0.28125, 'train/seg_cls_loss': 0.014703369140625, 'train/kl_loss': 0.331640625, 'train/mask_bce_loss': 0.18421086631715297, 'train/mask_dice_loss': 0.5578012853860855, 'train/mask_loss': 0.7420121550559997, 'metrics/total_secs_per_batch': 6.542808771133423, 'metrics/data_secs_per_batch': 2.8491363286972047, '_timestamp': 1740976316.0939038}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 209 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976316.0941815}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 210 is less than current step: 499. Dropping entry: {'train/loss': 1.7890772819519043, 'train/ce_loss': 0.190087890625, 'train/seg_cls_loss': 0.019268798828125, 'train/kl_loss': 0.33125, 'train/mask_bce_loss': 0.14055810328572987, 'train/mask_dice_loss': 0.6375742971897125, 'train/mask_loss': 0.7781323909759521, 'metrics/total_secs_per_batch': 6.394869565963745, 'metrics/data_secs_per_batch': 2.773256945610046, '_timestamp': 1740976322.4892511}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 210 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976322.4896173}).
Epoch: [7][212/500]	Time  5.491 ( 5.491)	Loss 0.9888 (1.5734)	CeLoss 0.2715 (0.3352)	SegCLSLoss 0.0098 (0.0133)	KLLoss 0.3613 (0.2539)	MaskLoss 0.3382 (0.6031)	MaskBCELoss 0.0571 (0.1482)	MaskDICELoss 0.2811 (0.4549)
Epoch: [7][213/500]	Time  5.759 ( 5.759)	Loss 1.7935 (1.4723)	CeLoss 0.3418 (0.3335)	SegCLSLoss 0.0211 (0.0144)	KLLoss 0.3750 (0.2924)	MaskLoss 0.7024 (0.5512)	MaskBCELoss 0.2248 (0.1577)	MaskDICELoss 0.4775 (0.3935)
Epoch: [7][214/500]	Time  5.248 ( 5.248)	Loss 1.1964 (1.5685)	CeLoss 0.1963 (0.5412)	SegCLSLoss 0.0102 (0.0099)	KLLoss 0.3633 (0.2543)	MaskLoss 0.4795 (0.4985)	MaskBCELoss 0.0768 (0.1122)	MaskDICELoss 0.4027 (0.3862)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 211 is less than current step: 499. Dropping entry: {'train/loss': 1.5733603477478026, 'train/ce_loss': 0.335205078125, 'train/seg_cls_loss': 0.013330078125, 'train/kl_loss': 0.25390625, 'train/mask_bce_loss': 0.14818547442555427, 'train/mask_dice_loss': 0.4548765361309052, 'train/mask_loss': 0.6030620098114013, 'metrics/total_secs_per_batch': 5.490747928619385, 'metrics/data_secs_per_batch': 2.448255515098572, '_timestamp': 1740976327.9797368}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 211 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976327.980013}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 212 is less than current step: 499. Dropping entry: {'train/loss': 1.4722918808460235, 'train/ce_loss': 0.33349609375, 'train/seg_cls_loss': 0.014398193359375, 'train/kl_loss': 0.2923828125, 'train/mask_bce_loss': 0.15773023813962936, 'train/mask_dice_loss': 0.39345476031303406, 'train/mask_loss': 0.5511850021779537, 'metrics/total_secs_per_batch': 5.758788824081421, 'metrics/data_secs_per_batch': 2.580838513374329, '_timestamp': 1740976333.7387338}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 212 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976333.7390966}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 213 is less than current step: 499. Dropping entry: {'train/loss': 1.5685279011726379, 'train/ce_loss': 0.5412109375, 'train/seg_cls_loss': 0.00985107421875, 'train/kl_loss': 0.254296875, 'train/mask_bce_loss': 0.11224129423499107, 'train/mask_dice_loss': 0.38623163253068926, 'train/mask_loss': 0.49847293049097063, 'metrics/total_secs_per_batch': 5.247854709625244, 'metrics/data_secs_per_batch': 2.1162159919738768, '_timestamp': 1740976338.986381}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 213 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976338.98666}).
Epoch: [7][215/500]	Time  6.134 ( 6.134)	Loss 1.4415 (1.5244)	CeLoss 0.2285 (0.4493)	SegCLSLoss 0.0099 (0.0118)	KLLoss 0.3555 (0.2510)	MaskLoss 0.5860 (0.5221)	MaskBCELoss 0.0616 (0.0775)	MaskDICELoss 0.5244 (0.4445)
Epoch: [7][216/500]	Time  5.721 ( 5.721)	Loss 0.0806 (1.6283)	CeLoss 0.0806 (0.4109)	SegCLSLoss 0.0000 (0.0123)	KLLoss 0.0000 (0.2562)	MaskLoss 0.0000 (0.5928)	MaskBCELoss 0.0000 (0.1302)	MaskDICELoss 0.0000 (0.4626)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 214 is less than current step: 499. Dropping entry: {'train/loss': 1.524373745918274, 'train/ce_loss': 0.44931640625, 'train/seg_cls_loss': 0.01180419921875, 'train/kl_loss': 0.2509765625, 'train/mask_bce_loss': 0.07754153516143561, 'train/mask_dice_loss': 0.4445086270570755, 'train/mask_loss': 0.5220501571893692, 'metrics/total_secs_per_batch': 6.134176969528198, 'metrics/data_secs_per_batch': 2.427539825439453, '_timestamp': 1740976345.1205664}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 214 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976345.1208642}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 215 is less than current step: 499. Dropping entry: {'train/loss': 1.6283286690711976, 'train/ce_loss': 0.410888671875, 'train/seg_cls_loss': 0.012274169921875, 'train/kl_loss': 0.25625, 'train/mask_bce_loss': 0.13019619658589363, 'train/mask_dice_loss': 0.4625570058822632, 'train/mask_loss': 0.5927532017230988, 'metrics/total_secs_per_batch': 5.720827579498291, 'metrics/data_secs_per_batch': 2.5592312574386598, '_timestamp': 1740976350.8415768}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 215 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976350.8420408}).
Epoch: [7][217/500]	Time  7.311 ( 7.311)	Loss 1.6746 (1.7455)	CeLoss 0.2363 (0.3686)	SegCLSLoss 0.0120 (0.0120)	KLLoss 0.3613 (0.2908)	MaskLoss 0.6986 (0.6709)	MaskBCELoss 0.1483 (0.1880)	MaskDICELoss 0.5503 (0.4829)
Epoch: [7][218/500]	Time  6.531 ( 6.531)	Loss 2.8046 (1.4338)	CeLoss 0.2715 (0.1911)	SegCLSLoss 0.0102 (0.0100)	KLLoss 0.3691 (0.2551)	MaskLoss 1.2451 (0.6060)	MaskBCELoss 0.5620 (0.1088)	MaskDICELoss 0.6831 (0.4973)
Epoch: [7][219/500]	Time  5.780 ( 5.780)	Loss 1.3323 (1.6131)	CeLoss 0.2178 (0.3092)	SegCLSLoss 0.0143 (0.0141)	KLLoss 0.3613 (0.3262)	MaskLoss 0.5353 (0.6321)	MaskBCELoss 0.0697 (0.1347)	MaskDICELoss 0.4656 (0.4973)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 216 is less than current step: 499. Dropping entry: {'train/loss': 1.745549237728119, 'train/ce_loss': 0.3685546875, 'train/seg_cls_loss': 0.011993408203125, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.18803796656429766, 'train/mask_dice_loss': 0.48288117349147797, 'train/mask_loss': 0.670919144153595, 'metrics/total_secs_per_batch': 7.311067342758179, 'metrics/data_secs_per_batch': 2.936190700531006, '_timestamp': 1740976358.1524355}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 216 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976358.1527443}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 217 is less than current step: 499. Dropping entry: {'train/loss': 1.4337819695472718, 'train/ce_loss': 0.191064453125, 'train/seg_cls_loss': 0.010009765625, 'train/kl_loss': 0.255078125, 'train/mask_bce_loss': 0.10876379627734423, 'train/mask_dice_loss': 0.49726292192935945, 'train/mask_loss': 0.6060267210006713, 'metrics/total_secs_per_batch': 6.530935764312744, 'metrics/data_secs_per_batch': 3.0018608570098877, '_timestamp': 1740976364.6836085}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 217 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976364.6839666}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 218 is less than current step: 499. Dropping entry: {'train/loss': 1.6131255686283112, 'train/ce_loss': 0.3091796875, 'train/seg_cls_loss': 0.01412353515625, 'train/kl_loss': 0.326171875, 'train/mask_bce_loss': 0.13471318185329437, 'train/mask_dice_loss': 0.4973378926515579, 'train/mask_loss': 0.6320510655641556, 'metrics/total_secs_per_batch': 5.7795844078063965, 'metrics/data_secs_per_batch': 2.548860502243042, '_timestamp': 1740976370.463063}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 218 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976370.463359}).
[2025-03-02 22:32:56,125] [INFO] [logging.py:96:log_dist] [Rank 0] step=3720, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:32:56,131] [INFO] [timer.py:215:stop] epoch=0/micro_step=37200/global_step=3720, RunningAvgSamplesPerSec=1.4603513765875333, CurrSamplesPerSec=1.764471116662253, MemAllocated=31.23GB, MaxMemAllocated=37.23GB
Epoch: [7][220/500]	Time  5.669 ( 5.669)	Loss 1.2303 (1.5763)	CeLoss 0.3242 (0.4690)	SegCLSLoss 0.0111 (0.0111)	KLLoss 0.3633 (0.2549)	MaskLoss 0.4316 (0.5381)	MaskBCELoss 0.0774 (0.1436)	MaskDICELoss 0.3541 (0.3945)
Epoch: [7][221/500]	Time  5.766 ( 5.766)	Loss 0.9492 (1.7870)	CeLoss 0.9492 (0.4328)	SegCLSLoss 0.0000 (0.0125)	KLLoss 0.0000 (0.2928)	MaskLoss 0.0000 (0.6594)	MaskBCELoss 0.0000 (0.1947)	MaskDICELoss 0.0000 (0.4647)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 219 is less than current step: 499. Dropping entry: {'train/loss': 1.576340425014496, 'train/ce_loss': 0.46904296875, 'train/seg_cls_loss': 0.0111083984375, 'train/kl_loss': 0.2548828125, 'train/mask_bce_loss': 0.1436055302619934, 'train/mask_dice_loss': 0.3945158451795578, 'train/mask_loss': 0.5381213873624802, 'metrics/total_secs_per_batch': 5.669121026992798, 'metrics/data_secs_per_batch': 2.410747265815735, '_timestamp': 1740976376.1319137}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 219 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976376.1321952}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 220 is less than current step: 499. Dropping entry: {'train/loss': 1.7869903922080994, 'train/ce_loss': 0.4328125, 'train/seg_cls_loss': 0.012548828125, 'train/kl_loss': 0.2927734375, 'train/mask_bce_loss': 0.19473232384771108, 'train/mask_dice_loss': 0.46468085050582886, 'train/mask_loss': 0.6594131678342819, 'metrics/total_secs_per_batch': 5.766148328781128, 'metrics/data_secs_per_batch': 2.482509398460388, '_timestamp': 1740976381.8984714}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 220 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976381.8988218}).
Epoch: [7][222/500]	Time  6.149 ( 6.149)	Loss 2.4355 (1.7352)	CeLoss 0.1289 (0.3972)	SegCLSLoss 0.0289 (0.0155)	KLLoss 0.3789 (0.2895)	MaskLoss 1.1269 (0.6507)	MaskBCELoss 0.2783 (0.0720)	MaskDICELoss 0.8486 (0.5787)
Epoch: [7][223/500]	Time  5.284 ( 5.284)	Loss 2.2055 (1.2117)	CeLoss 0.2061 (0.6094)	SegCLSLoss 0.0210 (0.0063)	KLLoss 0.3516 (0.1449)	MaskLoss 0.9768 (0.2924)	MaskBCELoss 0.0187 (0.0294)	MaskDICELoss 0.9580 (0.2630)
Epoch: [7][224/500]	Time  4.704 ( 4.704)	Loss 1.9755 (1.4512)	CeLoss 0.1904 (0.8514)	SegCLSLoss 0.0110 (0.0059)	KLLoss 0.3555 (0.1441)	MaskLoss 0.8716 (0.2912)	MaskBCELoss 0.1747 (0.0491)	MaskDICELoss 0.6969 (0.2421)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 221 is less than current step: 499. Dropping entry: {'train/loss': 1.7352208256721497, 'train/ce_loss': 0.39716796875, 'train/seg_cls_loss': 0.015478515625, 'train/kl_loss': 0.289453125, 'train/mask_bce_loss': 0.07202153112739325, 'train/mask_dice_loss': 0.578694349527359, 'train/mask_loss': 0.6507158696651458, 'metrics/total_secs_per_batch': 6.148930788040161, 'metrics/data_secs_per_batch': 2.6324899196624756, '_timestamp': 1740976388.0471742}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 221 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976388.047442}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 222 is less than current step: 499. Dropping entry: {'train/loss': 1.2116522431373595, 'train/ce_loss': 0.609423828125, 'train/seg_cls_loss': 0.0062744140625, 'train/kl_loss': 0.144921875, 'train/mask_bce_loss': 0.029426576010882853, 'train/mask_dice_loss': 0.2629962146282196, 'train/mask_loss': 0.29242279529571535, 'metrics/total_secs_per_batch': 5.284282684326172, 'metrics/data_secs_per_batch': 2.078575944900513, '_timestamp': 1740976393.3317053}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 222 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976393.3320634}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 223 is less than current step: 499. Dropping entry: {'train/loss': 1.4512482166290284, 'train/ce_loss': 0.8513671875, 'train/seg_cls_loss': 0.0059326171875, 'train/kl_loss': 0.144140625, 'train/mask_bce_loss': 0.049081570282578466, 'train/mask_dice_loss': 0.24206987917423248, 'train/mask_loss': 0.29115145206451415, 'metrics/total_secs_per_batch': 4.703873157501221, 'metrics/data_secs_per_batch': 1.9493682861328125, '_timestamp': 1740976398.0353308}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 223 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976398.0356917}).
Epoch: [7][225/500]	Time  4.968 ( 4.968)	Loss 1.5038 (1.5255)	CeLoss 0.3438 (0.4943)	SegCLSLoss 0.0102 (0.0112)	KLLoss 0.3613 (0.2205)	MaskLoss 0.5585 (0.5016)	MaskBCELoss 0.1233 (0.0794)	MaskDICELoss 0.4352 (0.4222)
Epoch: [7][226/500]	Time  5.470 ( 5.470)	Loss 1.6716 (1.4551)	CeLoss 0.2578 (0.3755)	SegCLSLoss 0.0175 (0.0142)	KLLoss 0.3555 (0.2916)	MaskLoss 0.6854 (0.5218)	MaskBCELoss 0.0183 (0.1256)	MaskDICELoss 0.6671 (0.3962)
Epoch: [7][227/500]	Time  6.153 ( 6.153)	Loss 0.5938 (1.1168)	CeLoss 0.5938 (0.3518)	SegCLSLoss 0.0000 (0.0104)	KLLoss 0.0000 (0.2525)	MaskLoss 0.0000 (0.3671)	MaskBCELoss 0.0000 (0.0478)	MaskDICELoss 0.0000 (0.3193)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 224 is less than current step: 499. Dropping entry: {'train/loss': 1.5254645586013793, 'train/ce_loss': 0.494287109375, 'train/seg_cls_loss': 0.01116943359375, 'train/kl_loss': 0.2205078125, 'train/mask_bce_loss': 0.07942054686136543, 'train/mask_dice_loss': 0.422203341126442, 'train/mask_loss': 0.5016238868236542, 'metrics/total_secs_per_batch': 4.967625856399536, 'metrics/data_secs_per_batch': 2.2651826620101927, '_timestamp': 1740976403.0029678}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 224 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976403.0032973}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 225 is less than current step: 499. Dropping entry: {'train/loss': 1.455133330821991, 'train/ce_loss': 0.37548828125, 'train/seg_cls_loss': 0.014239501953125, 'train/kl_loss': 0.2916015625, 'train/mask_bce_loss': 0.1256024593487382, 'train/mask_dice_loss': 0.3962024822831154, 'train/mask_loss': 0.5218049377202988, 'metrics/total_secs_per_batch': 5.469826936721802, 'metrics/data_secs_per_batch': 2.5775723457336426, '_timestamp': 1740976408.4730313}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 225 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976408.4734633}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 226 is less than current step: 499. Dropping entry: {'train/loss': 1.1168107628822326, 'train/ce_loss': 0.3517578125, 'train/seg_cls_loss': 0.01041259765625, 'train/kl_loss': 0.2525390625, 'train/mask_bce_loss': 0.04783678110688925, 'train/mask_dice_loss': 0.31930882632732394, 'train/mask_loss': 0.36714561432600024, 'metrics/total_secs_per_batch': 6.152859687805176, 'metrics/data_secs_per_batch': 2.8458505868911743, '_timestamp': 1740976414.62574}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 226 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976414.626054}).
Epoch: [7][228/500]	Time  4.841 ( 4.841)	Loss 1.4453 (1.7819)	CeLoss 1.4453 (0.7291)	SegCLSLoss 0.0000 (0.0116)	KLLoss 0.0000 (0.2227)	MaskLoss 0.0000 (0.5122)	MaskBCELoss 0.0000 (0.1330)	MaskDICELoss 0.0000 (0.3792)
Epoch: [7][229/500]	Time  6.555 ( 6.555)	Loss 0.3906 (1.2627)	CeLoss 0.3906 (0.3359)	SegCLSLoss 0.0000 (0.0113)	KLLoss 0.0000 (0.2508)	MaskLoss 0.0000 (0.4479)	MaskBCELoss 0.0000 (0.0796)	MaskDICELoss 0.0000 (0.3684)
[2025-03-02 22:33:51,656] [INFO] [logging.py:96:log_dist] [Rank 0] step=3730, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:33:51,663] [INFO] [timer.py:215:stop] epoch=0/micro_step=37300/global_step=3730, RunningAvgSamplesPerSec=1.4610930497061057, CurrSamplesPerSec=1.7730175336357503, MemAllocated=31.44GB, MaxMemAllocated=37.23GB
Epoch: [7][230/500]	Time  5.642 ( 5.642)	Loss 1.8135 (1.3252)	CeLoss 0.2910 (0.5604)	SegCLSLoss 0.0098 (0.0105)	KLLoss 0.3652 (0.2555)	MaskLoss 0.7398 (0.3667)	MaskBCELoss 0.0117 (0.0703)	MaskDICELoss 0.7281 (0.2964)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 227 is less than current step: 499. Dropping entry: {'train/loss': 1.7818625330924989, 'train/ce_loss': 0.7291015625, 'train/seg_cls_loss': 0.011566162109375, 'train/kl_loss': 0.22265625, 'train/mask_bce_loss': 0.13296272233128548, 'train/mask_dice_loss': 0.37920877933502195, 'train/mask_loss': 0.5121714979410171, 'metrics/total_secs_per_batch': 4.840795993804932, 'metrics/data_secs_per_batch': 2.4458161354064942, '_timestamp': 1740976419.4664586}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 227 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976419.4667428}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 228 is less than current step: 499. Dropping entry: {'train/loss': 1.2626906991004945, 'train/ce_loss': 0.3359375, 'train/seg_cls_loss': 0.01129150390625, 'train/kl_loss': 0.25078125, 'train/mask_bce_loss': 0.07957199057564139, 'train/mask_dice_loss': 0.36837492138147354, 'train/mask_loss': 0.44794690757989886, 'metrics/total_secs_per_batch': 6.555267095565796, 'metrics/data_secs_per_batch': 3.4579842329025268, '_timestamp': 1740976426.0219994}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 228 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976426.022367}).
Epoch: [7][231/500]	Time  6.188 ( 6.188)	Loss 1.3239 (2.0740)	CeLoss 0.2412 (0.2235)	SegCLSLoss 0.0100 (0.0158)	KLLoss 0.3633 (0.3660)	MaskLoss 0.5203 (0.9031)	MaskBCELoss 0.1694 (0.2900)	MaskDICELoss 0.3509 (0.6131)
Epoch: [7][232/500]	Time  6.213 ( 6.213)	Loss 2.0291 (2.0067)	CeLoss 0.2617 (0.3314)	SegCLSLoss 0.0114 (0.0158)	KLLoss 0.3652 (0.2932)	MaskLoss 0.8622 (0.8190)	MaskBCELoss 0.1420 (0.1680)	MaskDICELoss 0.7202 (0.6510)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 229 is less than current step: 499. Dropping entry: {'train/loss': 1.3252452313899994, 'train/ce_loss': 0.5603515625, 'train/seg_cls_loss': 0.010467529296875, 'train/kl_loss': 0.25546875, 'train/mask_bce_loss': 0.07033480005338788, 'train/mask_dice_loss': 0.2963893786072731, 'train/mask_loss': 0.36672417968511584, 'metrics/total_secs_per_batch': 5.64198899269104, 'metrics/data_secs_per_batch': 2.4429089307785032, '_timestamp': 1740976431.663528}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 229 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976431.663713}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 230 is less than current step: 499. Dropping entry: {'train/loss': 2.073998010158539, 'train/ce_loss': 0.22353515625, 'train/seg_cls_loss': 0.015771484375, 'train/kl_loss': 0.366015625, 'train/mask_bce_loss': 0.2900229368824512, 'train/mask_dice_loss': 0.6130893766880036, 'train/mask_loss': 0.9031122922897339, 'metrics/total_secs_per_batch': 6.187573671340942, 'metrics/data_secs_per_batch': 2.8016408681869507, '_timestamp': 1740976437.8513823}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 230 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976437.8516085}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 231 is less than current step: 499. Dropping entry: {'train/loss': 2.0067416071891784, 'train/ce_loss': 0.331396484375, 'train/seg_cls_loss': 0.015838623046875, 'train/kl_loss': 0.2931640625, 'train/mask_bce_loss': 0.16801455495879053, 'train/mask_dice_loss': 0.6509812474250793, 'train/mask_loss': 0.8189957976341248, 'metrics/total_secs_per_batch': 6.213082313537598, 'metrics/data_secs_per_batch': 2.899531435966492, '_timestamp': 1740976444.0646372}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 231 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976444.0649986}).
Epoch: [7][233/500]	Time  5.875 ( 5.875)	Loss 1.3300 (1.3325)	CeLoss 0.2393 (0.5480)	SegCLSLoss 0.0102 (0.0086)	KLLoss 0.3594 (0.2154)	MaskLoss 0.5254 (0.3793)	MaskBCELoss 0.0756 (0.0426)	MaskDICELoss 0.4497 (0.3367)
Epoch: [7][234/500]	Time  6.616 ( 6.616)	Loss 1.2670 (1.2876)	CeLoss 0.2422 (0.3172)	SegCLSLoss 0.0125 (0.0097)	KLLoss 0.3574 (0.2146)	MaskLoss 0.4919 (0.4722)	MaskBCELoss 0.0735 (0.0623)	MaskDICELoss 0.4184 (0.4099)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 232 is less than current step: 499. Dropping entry: {'train/loss': 1.3324930131435395, 'train/ce_loss': 0.548046875, 'train/seg_cls_loss': 0.00858154296875, 'train/kl_loss': 0.2154296875, 'train/mask_bce_loss': 0.04262115005403757, 'train/mask_dice_loss': 0.336662457883358, 'train/mask_loss': 0.37928360849618914, 'metrics/total_secs_per_batch': 5.874511480331421, 'metrics/data_secs_per_batch': 2.474635887145996, '_timestamp': 1740976449.9389136}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 232 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976449.939187}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 233 is less than current step: 499. Dropping entry: {'train/loss': 1.287606120109558, 'train/ce_loss': 0.3172119140625, 'train/seg_cls_loss': 0.009674072265625, 'train/kl_loss': 0.2146484375, 'train/mask_bce_loss': 0.062305116280913356, 'train/mask_dice_loss': 0.40985487401485443, 'train/mask_loss': 0.47215999364852906, 'metrics/total_secs_per_batch': 6.616382360458374, 'metrics/data_secs_per_batch': 2.8781051635742188, '_timestamp': 1740976456.5553102}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 233 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976456.5556834}).
Epoch: [7][235/500]	Time  6.839 ( 6.839)	Loss 1.8647 (1.8153)	CeLoss 0.2031 (0.2398)	SegCLSLoss 0.0244 (0.0163)	KLLoss 0.3633 (0.3295)	MaskLoss 0.8064 (0.7670)	MaskBCELoss 0.0259 (0.1666)	MaskDICELoss 0.7805 (0.6004)
Epoch: [7][236/500]	Time  6.532 ( 6.532)	Loss 0.0654 (1.5431)	CeLoss 0.0654 (0.3916)	SegCLSLoss 0.0000 (0.0106)	KLLoss 0.0000 (0.2545)	MaskLoss 0.0000 (0.5604)	MaskBCELoss 0.0000 (0.1116)	MaskDICELoss 0.0000 (0.4488)
Epoch: [7][237/500]	Time  5.937 ( 5.937)	Loss 1.7002 (1.5106)	CeLoss 0.2148 (0.4518)	SegCLSLoss 0.0157 (0.0114)	KLLoss 0.3633 (0.2541)	MaskLoss 0.7202 (0.5138)	MaskBCELoss 0.0613 (0.0885)	MaskDICELoss 0.6589 (0.4253)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 234 is less than current step: 499. Dropping entry: {'train/loss': 1.8153439998626708, 'train/ce_loss': 0.23984375, 'train/seg_cls_loss': 0.0163330078125, 'train/kl_loss': 0.3294921875, 'train/mask_bce_loss': 0.1666207816451788, 'train/mask_dice_loss': 0.6004262089729309, 'train/mask_loss': 0.7670469969511032, 'metrics/total_secs_per_batch': 6.8390679359436035, 'metrics/data_secs_per_batch': 3.028875637054443, '_timestamp': 1740976463.394534}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 234 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976463.3948956}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 235 is less than current step: 499. Dropping entry: {'train/loss': 1.5431019425392152, 'train/ce_loss': 0.3916015625, 'train/seg_cls_loss': 0.010626220703125, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.11158108133822679, 'train/mask_dice_loss': 0.44878824055194855, 'train/mask_loss': 0.5603693306446076, 'metrics/total_secs_per_batch': 6.532339096069336, 'metrics/data_secs_per_batch': 3.0217345476150514, '_timestamp': 1740976469.9267118}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 235 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976469.9270031}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 236 is less than current step: 499. Dropping entry: {'train/loss': 1.5105910122394561, 'train/ce_loss': 0.4517578125, 'train/seg_cls_loss': 0.01136474609375, 'train/kl_loss': 0.2541015625, 'train/mask_bce_loss': 0.08851696848869324, 'train/mask_dice_loss': 0.4252746358513832, 'train/mask_loss': 0.5137915953993797, 'metrics/total_secs_per_batch': 5.936787128448486, 'metrics/data_secs_per_batch': 2.4906198024749755, '_timestamp': 1740976475.8637717}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 236 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976475.8641505}).
Epoch: [7][238/500]	Time  5.096 ( 5.096)	Loss 0.9062 (1.7785)	CeLoss 0.9062 (0.5017)	SegCLSLoss 0.0000 (0.0099)	KLLoss 0.0000 (0.2521)	MaskLoss 0.0000 (0.6233)	MaskBCELoss 0.0000 (0.1491)	MaskDICELoss 0.0000 (0.4741)
Epoch: [7][239/500]	Time  5.730 ( 5.730)	Loss 1.7472 (1.8496)	CeLoss 0.2305 (0.3684)	SegCLSLoss 0.0184 (0.0140)	KLLoss 0.3672 (0.3281)	MaskLoss 0.7359 (0.7208)	MaskBCELoss 0.0256 (0.1819)	MaskDICELoss 0.7103 (0.5389)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 237 is less than current step: 499. Dropping entry: {'train/loss': 1.7785468220710754, 'train/ce_loss': 0.50166015625, 'train/seg_cls_loss': 0.009893798828125, 'train/kl_loss': 0.2521484375, 'train/mask_bce_loss': 0.14913253784179686, 'train/mask_dice_loss': 0.47412524819374086, 'train/mask_loss': 0.6232577860355377, 'metrics/total_secs_per_batch': 5.096012592315674, 'metrics/data_secs_per_batch': 2.2080519676208494, '_timestamp': 1740976480.9595513}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 237 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976480.9598527}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 238 is less than current step: 499. Dropping entry: {'train/loss': 1.849644160270691, 'train/ce_loss': 0.368359375, 'train/seg_cls_loss': 0.013983154296875, 'train/kl_loss': 0.328125, 'train/mask_bce_loss': 0.18189077097922562, 'train/mask_dice_loss': 0.5389274060726166, 'train/mask_loss': 0.7208181649446488, 'metrics/total_secs_per_batch': 5.729668378829956, 'metrics/data_secs_per_batch': 2.582217049598694, '_timestamp': 1740976486.6894128}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 238 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976486.689776}).
[2025-03-02 22:34:52,295] [INFO] [logging.py:96:log_dist] [Rank 0] step=3740, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:34:52,301] [INFO] [timer.py:215:stop] epoch=0/micro_step=37400/global_step=3740, RunningAvgSamplesPerSec=1.4615396056672527, CurrSamplesPerSec=1.7821345841532725, MemAllocated=30.81GB, MaxMemAllocated=37.23GB
Epoch: [7][240/500]	Time  5.613 ( 5.613)	Loss 2.2510 (2.1886)	CeLoss 0.1543 (0.5110)	SegCLSLoss 0.0310 (0.0146)	KLLoss 0.3711 (0.2562)	MaskLoss 1.0220 (0.8225)	MaskBCELoss 0.0338 (0.1961)	MaskDICELoss 0.9882 (0.6264)
Epoch: [7][241/500]	Time  6.264 ( 6.264)	Loss 0.6328 (1.4658)	CeLoss 0.6328 (0.3562)	SegCLSLoss 0.0000 (0.0103)	KLLoss 0.0000 (0.2527)	MaskLoss 0.0000 (0.5396)	MaskBCELoss 0.0000 (0.0926)	MaskDICELoss 0.0000 (0.4470)
Epoch: [7][242/500]	Time  6.210 ( 6.210)	Loss 1.8071 (1.9017)	CeLoss 0.2715 (0.3271)	SegCLSLoss 0.0125 (0.0136)	KLLoss 0.3594 (0.3260)	MaskLoss 0.7473 (0.7677)	MaskBCELoss 0.0270 (0.1740)	MaskDICELoss 0.7203 (0.5937)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 239 is less than current step: 499. Dropping entry: {'train/loss': 2.1885628700256348, 'train/ce_loss': 0.51103515625, 'train/seg_cls_loss': 0.0145751953125, 'train/kl_loss': 0.25625, 'train/mask_bce_loss': 0.1961084071546793, 'train/mask_dice_loss': 0.6263956844806671, 'train/mask_loss': 0.8225041031837463, 'metrics/total_secs_per_batch': 5.613113164901733, 'metrics/data_secs_per_batch': 2.9549112558364867, '_timestamp': 1740976492.3021274}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 239 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976492.3023913}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 240 is less than current step: 499. Dropping entry: {'train/loss': 1.4657635807991027, 'train/ce_loss': 0.35615234375, 'train/seg_cls_loss': 0.010296630859375, 'train/kl_loss': 0.252734375, 'train/mask_bce_loss': 0.09263582695275545, 'train/mask_dice_loss': 0.44698424339294435, 'train/mask_loss': 0.5396200627088547, 'metrics/total_secs_per_batch': 6.263867139816284, 'metrics/data_secs_per_batch': 2.7882800817489626, '_timestamp': 1740976498.5664713}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 240 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976498.566853}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 241 is less than current step: 499. Dropping entry: {'train/loss': 1.9016886472702026, 'train/ce_loss': 0.32705078125, 'train/seg_cls_loss': 0.013616943359375, 'train/kl_loss': 0.3259765625, 'train/mask_bce_loss': 0.1740242602303624, 'train/mask_dice_loss': 0.5936657533049583, 'train/mask_loss': 0.7676900267601013, 'metrics/total_secs_per_batch': 6.210029125213623, 'metrics/data_secs_per_batch': 2.498361086845398, '_timestamp': 1740976504.7762182}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 241 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976504.776501}).
Epoch: [7][243/500]	Time  6.287 ( 6.287)	Loss 2.5320 (1.8756)	CeLoss 0.1162 (0.4397)	SegCLSLoss 0.0356 (0.0144)	KLLoss 0.3750 (0.2902)	MaskLoss 1.1801 (0.6999)	MaskBCELoss 0.3349 (0.1643)	MaskDICELoss 0.8451 (0.5356)
Epoch: [7][244/500]	Time  5.366 ( 5.366)	Loss 0.5586 (1.8000)	CeLoss 0.5586 (0.5217)	SegCLSLoss 0.0000 (0.0112)	KLLoss 0.0000 (0.2174)	MaskLoss 0.0000 (0.6256)	MaskBCELoss 0.0000 (0.2116)	MaskDICELoss 0.0000 (0.4140)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 242 is less than current step: 499. Dropping entry: {'train/loss': 1.8756346821784973, 'train/ce_loss': 0.43974609375, 'train/seg_cls_loss': 0.0143798828125, 'train/kl_loss': 0.290234375, 'train/mask_bce_loss': 0.16431185677647592, 'train/mask_dice_loss': 0.5355660319328308, 'train/mask_loss': 0.6998778879642487, 'metrics/total_secs_per_batch': 6.2873382568359375, 'metrics/data_secs_per_batch': 2.9438159704208373, '_timestamp': 1740976511.0636346}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 242 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976511.0639207}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 243 is less than current step: 499. Dropping entry: {'train/loss': 1.800034725666046, 'train/ce_loss': 0.5216796875, 'train/seg_cls_loss': 0.011163330078125, 'train/kl_loss': 0.2173828125, 'train/mask_bce_loss': 0.21162755209952594, 'train/mask_dice_loss': 0.4139757424592972, 'train/mask_loss': 0.625603300333023, 'metrics/total_secs_per_batch': 5.366007566452026, 'metrics/data_secs_per_batch': 2.0595519065856935, '_timestamp': 1740976516.4295955}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 243 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976516.4299102}).
Epoch: [7][245/500]	Time  6.749 ( 6.749)	Loss 2.3985 (1.4082)	CeLoss 0.1982 (0.2004)	SegCLSLoss 0.0164 (0.0118)	KLLoss 0.3652 (0.2926)	MaskLoss 1.0782 (0.5863)	MaskBCELoss 0.2325 (0.1679)	MaskDICELoss 0.8456 (0.4185)
Epoch: [7][246/500]	Time  5.234 ( 5.234)	Loss 1.7605 (1.6303)	CeLoss 0.1943 (0.5654)	SegCLSLoss 0.0271 (0.0121)	KLLoss 0.3594 (0.2148)	MaskLoss 0.7582 (0.5187)	MaskBCELoss 0.0617 (0.0723)	MaskDICELoss 0.6965 (0.4464)
Epoch: [7][247/500]	Time  6.158 ( 6.158)	Loss 1.2307 (1.3951)	CeLoss 0.2500 (0.3098)	SegCLSLoss 0.0140 (0.0120)	KLLoss 0.3594 (0.2885)	MaskLoss 0.4689 (0.5252)	MaskBCELoss 0.0586 (0.0698)	MaskDICELoss 0.4103 (0.4554)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 244 is less than current step: 499. Dropping entry: {'train/loss': 1.408218216896057, 'train/ce_loss': 0.200390625, 'train/seg_cls_loss': 0.011834716796875, 'train/kl_loss': 0.292578125, 'train/mask_bce_loss': 0.16786073371767998, 'train/mask_dice_loss': 0.418474942445755, 'train/mask_loss': 0.5863356724381447, 'metrics/total_secs_per_batch': 6.749495267868042, 'metrics/data_secs_per_batch': 2.9644377708435057, '_timestamp': 1740976523.1791685}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 244 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976523.1794655}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 245 is less than current step: 499. Dropping entry: {'train/loss': 1.6303033709526062, 'train/ce_loss': 0.5654296875, 'train/seg_cls_loss': 0.012115478515625, 'train/kl_loss': 0.21484375, 'train/mask_bce_loss': 0.07230141609907151, 'train/mask_dice_loss': 0.4463658809661865, 'train/mask_loss': 0.5186672925949096, 'metrics/total_secs_per_batch': 5.233846664428711, 'metrics/data_secs_per_batch': 2.333777165412903, '_timestamp': 1740976528.4129264}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 245 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976528.413286}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 246 is less than current step: 499. Dropping entry: {'train/loss': 1.3951490879058839, 'train/ce_loss': 0.309765625, 'train/seg_cls_loss': 0.011968994140625, 'train/kl_loss': 0.2884765625, 'train/mask_bce_loss': 0.06977431457489729, 'train/mask_dice_loss': 0.45538810789585116, 'train/mask_loss': 0.5251624286174774, 'metrics/total_secs_per_batch': 6.157963037490845, 'metrics/data_secs_per_batch': 2.7670787811279296, '_timestamp': 1740976534.5708904}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 246 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976534.571163}).
Epoch: [7][248/500]	Time  5.294 ( 5.294)	Loss 1.2344 (1.4747)	CeLoss 1.2344 (0.6125)	SegCLSLoss 0.0000 (0.0107)	KLLoss 0.0000 (0.2162)	MaskLoss 0.0000 (0.4176)	MaskBCELoss 0.0000 (0.0401)	MaskDICELoss 0.0000 (0.3775)
Epoch: [7][249/500]	Time  6.083 ( 6.083)	Loss 2.4574 (1.6640)	CeLoss 0.1836 (0.3677)	SegCLSLoss 0.0193 (0.0121)	KLLoss 0.3789 (0.2543)	MaskLoss 1.1130 (0.6324)	MaskBCELoss 0.1996 (0.0999)	MaskDICELoss 0.9134 (0.5325)
[2025-03-02 22:35:51,801] [INFO] [logging.py:96:log_dist] [Rank 0] step=3750, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:35:51,806] [INFO] [timer.py:215:stop] epoch=0/micro_step=37500/global_step=3750, RunningAvgSamplesPerSec=1.462048641002917, CurrSamplesPerSec=1.7071520129848115, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [7][250/500]	Time  5.859 ( 5.859)	Loss 1.2569 (1.9600)	CeLoss 0.2500 (0.4362)	SegCLSLoss 0.0095 (0.0141)	KLLoss 0.3594 (0.3236)	MaskLoss 0.4839 (0.7423)	MaskBCELoss 0.0517 (0.1400)	MaskDICELoss 0.4322 (0.6022)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 247 is less than current step: 499. Dropping entry: {'train/loss': 1.47467440366745, 'train/ce_loss': 0.6125, 'train/seg_cls_loss': 0.01072998046875, 'train/kl_loss': 0.2162109375, 'train/mask_bce_loss': 0.040122528187930585, 'train/mask_dice_loss': 0.377488112449646, 'train/mask_loss': 0.417610639333725, 'metrics/total_secs_per_batch': 5.294024467468262, 'metrics/data_secs_per_batch': 2.4131067514419557, '_timestamp': 1740976539.864899}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 247 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976539.865163}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 248 is less than current step: 499. Dropping entry: {'train/loss': 1.6639896750450134, 'train/ce_loss': 0.36767578125, 'train/seg_cls_loss': 0.0121337890625, 'train/kl_loss': 0.254296875, 'train/mask_bce_loss': 0.09993275376036763, 'train/mask_dice_loss': 0.5324526965618134, 'train/mask_loss': 0.6323854446411132, 'metrics/total_secs_per_batch': 6.083441972732544, 'metrics/data_secs_per_batch': 2.6291852951049806, '_timestamp': 1740976545.9483395}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 248 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976545.9486275}).
Epoch: [7][251/500]	Time  6.426 ( 6.426)	Loss 0.0659 (1.2218)	CeLoss 0.0659 (0.3997)	SegCLSLoss 0.0000 (0.0098)	KLLoss 0.0000 (0.2180)	MaskLoss 0.0000 (0.3977)	MaskBCELoss 0.0000 (0.1335)	MaskDICELoss 0.0000 (0.2642)
Epoch: [7][252/500]	Time  5.251 ( 5.251)	Loss 0.0596 (1.4603)	CeLoss 0.0596 (0.5815)	SegCLSLoss 0.0000 (0.0092)	KLLoss 0.0000 (0.1838)	MaskLoss 0.0000 (0.4281)	MaskBCELoss 0.0000 (0.0791)	MaskDICELoss 0.0000 (0.3490)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 249 is less than current step: 499. Dropping entry: {'train/loss': 1.9600301146507264, 'train/ce_loss': 0.43623046875, 'train/seg_cls_loss': 0.014080810546875, 'train/kl_loss': 0.3236328125, 'train/mask_bce_loss': 0.14002326931804418, 'train/mask_dice_loss': 0.602247628569603, 'train/mask_loss': 0.7422709047794342, 'metrics/total_secs_per_batch': 5.859280586242676, 'metrics/data_secs_per_batch': 2.5705435037612916, '_timestamp': 1740976551.8074346}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 249 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976551.8076336}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 250 is less than current step: 499. Dropping entry: {'train/loss': 1.2217527210712433, 'train/ce_loss': 0.399658203125, 'train/seg_cls_loss': 0.00975341796875, 'train/kl_loss': 0.21796875, 'train/mask_bce_loss': 0.13346854485571386, 'train/mask_dice_loss': 0.2642486453056335, 'train/mask_loss': 0.39771717712283133, 'metrics/total_secs_per_batch': 6.426079988479614, 'metrics/data_secs_per_batch': 2.8984747886657716, '_timestamp': 1740976558.2342145}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 250 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976558.2347124}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 251 is less than current step: 499. Dropping entry: {'train/loss': 1.4602858424186707, 'train/ce_loss': 0.581494140625, 'train/seg_cls_loss': 0.009246826171875, 'train/kl_loss': 0.1837890625, 'train/mask_bce_loss': 0.07911585234105586, 'train/mask_dice_loss': 0.3489518702030182, 'train/mask_loss': 0.4280677199363708, 'metrics/total_secs_per_batch': 5.2506444454193115, 'metrics/data_secs_per_batch': 2.2176425457000732, '_timestamp': 1740976563.4844193}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 251 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976563.4847193}).
Epoch: [7][253/500]	Time  6.587 ( 6.587)	Loss 3.1910 (1.7186)	CeLoss 0.1924 (0.3404)	SegCLSLoss 0.0171 (0.0126)	KLLoss 0.3711 (0.2934)	MaskLoss 1.4764 (0.6713)	MaskBCELoss 0.6919 (0.1395)	MaskDICELoss 0.7844 (0.5318)
Epoch: [7][254/500]	Time  4.874 ( 4.874)	Loss 2.4153 (1.6398)	CeLoss 0.2207 (0.5889)	SegCLSLoss 0.0173 (0.0088)	KLLoss 0.3750 (0.1832)	MaskLoss 1.0739 (0.5140)	MaskBCELoss 0.2866 (0.0981)	MaskDICELoss 0.7873 (0.4160)
Epoch: [7][255/500]	Time  5.595 ( 5.595)	Loss 1.4635 (1.6191)	CeLoss 0.3438 (0.5588)	SegCLSLoss 0.0104 (0.0113)	KLLoss 0.3633 (0.2533)	MaskLoss 0.5384 (0.5146)	MaskBCELoss 0.1222 (0.0869)	MaskDICELoss 0.4161 (0.4277)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 252 is less than current step: 499. Dropping entry: {'train/loss': 1.7186468839645386, 'train/ce_loss': 0.340380859375, 'train/seg_cls_loss': 0.012640380859375, 'train/kl_loss': 0.293359375, 'train/mask_bce_loss': 0.13948284089565277, 'train/mask_dice_loss': 0.5317790746688843, 'train/mask_loss': 0.6712619185447692, 'metrics/total_secs_per_batch': 6.58749532699585, 'metrics/data_secs_per_batch': 2.728345584869385, '_timestamp': 1740976570.0720038}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 252 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976570.0723197}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 253 is less than current step: 499. Dropping entry: {'train/loss': 1.6397831797599793, 'train/ce_loss': 0.5888671875, 'train/seg_cls_loss': 0.008770751953125, 'train/kl_loss': 0.183203125, 'train/mask_bce_loss': 0.09805662650614977, 'train/mask_dice_loss': 0.41597559452056887, 'train/mask_loss': 0.5140322208404541, 'metrics/total_secs_per_batch': 4.874194383621216, 'metrics/data_secs_per_batch': 2.2529610872268675, '_timestamp': 1740976574.9462235}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 253 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976574.9466176}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 254 is less than current step: 499. Dropping entry: {'train/loss': 1.61910560131073, 'train/ce_loss': 0.5587890625, 'train/seg_cls_loss': 0.011297607421875, 'train/kl_loss': 0.2533203125, 'train/mask_bce_loss': 0.08691965006291866, 'train/mask_dice_loss': 0.4277112692594528, 'train/mask_loss': 0.5146309196949005, 'metrics/total_secs_per_batch': 5.594564437866211, 'metrics/data_secs_per_batch': 2.4742836475372316, '_timestamp': 1740976580.5406537}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 254 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976580.540933}).
Epoch: [7][256/500]	Time  5.889 ( 5.889)	Loss 1.4666 (1.5747)	CeLoss 0.1973 (0.4526)	SegCLSLoss 0.0225 (0.0134)	KLLoss 0.3613 (0.2910)	MaskLoss 0.6107 (0.5432)	MaskBCELoss 0.0162 (0.0796)	MaskDICELoss 0.5946 (0.4636)
Epoch: [7][257/500]	Time  6.214 ( 6.214)	Loss 1.1714 (1.3710)	CeLoss 0.2559 (0.4444)	SegCLSLoss 0.0140 (0.0108)	KLLoss 0.3613 (0.2191)	MaskLoss 0.4353 (0.4497)	MaskBCELoss 0.0343 (0.1103)	MaskDICELoss 0.4010 (0.3394)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 255 is less than current step: 499. Dropping entry: {'train/loss': 1.574727725982666, 'train/ce_loss': 0.45263671875, 'train/seg_cls_loss': 0.01341552734375, 'train/kl_loss': 0.291015625, 'train/mask_bce_loss': 0.07955032903701068, 'train/mask_dice_loss': 0.4636240750551224, 'train/mask_loss': 0.5431743949651718, 'metrics/total_secs_per_batch': 5.888680696487427, 'metrics/data_secs_per_batch': 2.389703464508057, '_timestamp': 1740976586.4294186}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 255 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976586.4296267}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 256 is less than current step: 499. Dropping entry: {'train/loss': 1.3710166811943054, 'train/ce_loss': 0.44443359375, 'train/seg_cls_loss': 0.010772705078125, 'train/kl_loss': 0.219140625, 'train/mask_bce_loss': 0.11027423739433288, 'train/mask_dice_loss': 0.33939424753189085, 'train/mask_loss': 0.44966848492622374, 'metrics/total_secs_per_batch': 6.213581323623657, 'metrics/data_secs_per_batch': 2.789299488067627, '_timestamp': 1740976592.643099}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 256 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976592.6435459}).
Epoch: [7][258/500]	Time  6.522 ( 6.522)	Loss 1.4360 (1.3398)	CeLoss 0.1807 (0.3038)	SegCLSLoss 0.0289 (0.0136)	KLLoss 0.3555 (0.2551)	MaskLoss 0.6028 (0.5018)	MaskBCELoss 0.0916 (0.1085)	MaskDICELoss 0.5112 (0.3933)
Epoch: [7][259/500]	Time  5.619 ( 5.619)	Loss 2.1746 (1.6947)	CeLoss 0.2412 (0.4658)	SegCLSLoss 0.0154 (0.0114)	KLLoss 0.3652 (0.2547)	MaskLoss 0.9447 (0.5988)	MaskBCELoss 0.0123 (0.1254)	MaskDICELoss 0.9324 (0.4735)
[2025-03-02 22:36:51,166] [INFO] [logging.py:96:log_dist] [Rank 0] step=3760, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:36:51,172] [INFO] [timer.py:215:stop] epoch=0/micro_step=37600/global_step=3760, RunningAvgSamplesPerSec=1.4625633703243734, CurrSamplesPerSec=1.5657447932866446, MemAllocated=31.27GB, MaxMemAllocated=37.23GB
Epoch: [7][260/500]	Time  6.389 ( 6.389)	Loss 1.0347 (1.4460)	CeLoss 0.1875 (0.2251)	SegCLSLoss 0.0222 (0.0124)	KLLoss 0.3750 (0.3275)	MaskLoss 0.3992 (0.5909)	MaskBCELoss 0.1280 (0.2026)	MaskDICELoss 0.2712 (0.3883)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 257 is less than current step: 499. Dropping entry: {'train/loss': 1.3397662103176118, 'train/ce_loss': 0.303759765625, 'train/seg_cls_loss': 0.013604736328125, 'train/kl_loss': 0.255078125, 'train/mask_bce_loss': 0.1084888068959117, 'train/mask_dice_loss': 0.3933034747838974, 'train/mask_loss': 0.5017922818660736, 'metrics/total_secs_per_batch': 6.522196531295776, 'metrics/data_secs_per_batch': 2.7731077909469604, '_timestamp': 1740976599.1651227}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 257 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976599.1654687}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 258 is less than current step: 499. Dropping entry: {'train/loss': 1.6947052359580994, 'train/ce_loss': 0.4658203125, 'train/seg_cls_loss': 0.01138916015625, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.12535842899233102, 'train/mask_dice_loss': 0.4734590291976929, 'train/mask_loss': 0.5988174617290497, 'metrics/total_secs_per_batch': 5.619225740432739, 'metrics/data_secs_per_batch': 2.329801416397095, '_timestamp': 1740976604.7845788}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 258 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976604.784947}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 259 is less than current step: 499. Dropping entry: {'train/loss': 1.4460100829601288, 'train/ce_loss': 0.225146484375, 'train/seg_cls_loss': 0.012384033203125, 'train/kl_loss': 0.3275390625, 'train/mask_bce_loss': 0.2025559326633811, 'train/mask_dice_loss': 0.3882957801222801, 'train/mask_loss': 0.5908517122268677, 'metrics/total_secs_per_batch': 6.388642311096191, 'metrics/data_secs_per_batch': 3.0117712020874023, '_timestamp': 1740976611.172795}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 259 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976611.1730785}).
Epoch: [7][261/500]	Time  6.261 ( 6.261)	Loss 2.0696 (1.6630)	CeLoss 0.2793 (0.3344)	SegCLSLoss 0.0136 (0.0148)	KLLoss 0.3633 (0.2893)	MaskLoss 0.8736 (0.6463)	MaskBCELoss 0.0279 (0.1403)	MaskDICELoss 0.8457 (0.5060)
Epoch: [7][262/500]	Time  7.210 ( 7.210)	Loss 2.1293 (1.5552)	CeLoss 0.2197 (0.2898)	SegCLSLoss 0.0219 (0.0136)	KLLoss 0.3535 (0.2887)	MaskLoss 0.9318 (0.6149)	MaskBCELoss 0.0492 (0.0706)	MaskDICELoss 0.8827 (0.5443)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 260 is less than current step: 499. Dropping entry: {'train/loss': 1.663016563653946, 'train/ce_loss': 0.334375, 'train/seg_cls_loss': 0.014788818359375, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.14030096263159067, 'train/mask_dice_loss': 0.506002226471901, 'train/mask_loss': 0.646303191781044, 'metrics/total_secs_per_batch': 6.26085090637207, 'metrics/data_secs_per_batch': 2.887698245048523, '_timestamp': 1740976617.4338317}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 260 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976617.4341207}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 261 is less than current step: 499. Dropping entry: {'train/loss': 1.5551501870155335, 'train/ce_loss': 0.28984375, 'train/seg_cls_loss': 0.013616943359375, 'train/kl_loss': 0.288671875, 'train/mask_bce_loss': 0.07059607058763503, 'train/mask_dice_loss': 0.5442837119102478, 'train/mask_loss': 0.6148797810077667, 'metrics/total_secs_per_batch': 7.210054874420166, 'metrics/data_secs_per_batch': 3.3073413610458373, '_timestamp': 1740976624.643968}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 261 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976624.6442833}).
Epoch: [7][263/500]	Time  6.060 ( 6.060)	Loss 2.6459 (1.3838)	CeLoss 0.1260 (0.3546)	SegCLSLoss 0.0293 (0.0117)	KLLoss 0.3789 (0.2180)	MaskLoss 1.2336 (0.5008)	MaskBCELoss 0.4451 (0.1130)	MaskDICELoss 0.7885 (0.3878)
Epoch: [7][264/500]	Time  6.259 ( 6.259)	Loss 1.4455 (1.1918)	CeLoss 0.2217 (0.3168)	SegCLSLoss 0.0209 (0.0115)	KLLoss 0.3672 (0.2527)	MaskLoss 0.5880 (0.4221)	MaskBCELoss 0.0220 (0.0729)	MaskDICELoss 0.5660 (0.3492)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 262 is less than current step: 499. Dropping entry: {'train/loss': 1.3837525486946105, 'train/ce_loss': 0.35458984375, 'train/seg_cls_loss': 0.011737060546875, 'train/kl_loss': 0.21796875, 'train/mask_bce_loss': 0.11298458911478519, 'train/mask_dice_loss': 0.3878272444009781, 'train/mask_loss': 0.5008118331432343, 'metrics/total_secs_per_batch': 6.059711217880249, 'metrics/data_secs_per_batch': 2.628866958618164, '_timestamp': 1740976630.7036386}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 262 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976630.703926}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 263 is less than current step: 499. Dropping entry: {'train/loss': 1.1917792677879333, 'train/ce_loss': 0.316796875, 'train/seg_cls_loss': 0.01148681640625, 'train/kl_loss': 0.252734375, 'train/mask_bce_loss': 0.07287108097225428, 'train/mask_dice_loss': 0.34923926219344137, 'train/mask_loss': 0.42211034148931503, 'metrics/total_secs_per_batch': 6.258669853210449, 'metrics/data_secs_per_batch': 2.9184229135513307, '_timestamp': 1740976636.962281}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 263 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976636.9626331}).
Epoch: [7][265/500]	Time  6.156 ( 6.156)	Loss 1.3750 (1.8477)	CeLoss 1.3750 (0.3421)	SegCLSLoss 0.0000 (0.0152)	KLLoss 0.0000 (0.2961)	MaskLoss 0.0000 (0.7343)	MaskBCELoss 0.0000 (0.1614)	MaskDICELoss 0.0000 (0.5729)
Epoch: [7][266/500]	Time  5.072 ( 5.072)	Loss 1.5938 (1.4464)	CeLoss 1.5938 (0.5984)	SegCLSLoss 0.0000 (0.0078)	KLLoss 0.0000 (0.2197)	MaskLoss 0.0000 (0.4110)	MaskBCELoss 0.0000 (0.0954)	MaskDICELoss 0.0000 (0.3156)
Epoch: [7][267/500]	Time  6.020 ( 6.020)	Loss 1.7371 (1.0514)	CeLoss 0.2520 (0.3666)	SegCLSLoss 0.0165 (0.0097)	KLLoss 0.3594 (0.2178)	MaskLoss 0.7211 (0.3292)	MaskBCELoss 0.0529 (0.0536)	MaskDICELoss 0.6682 (0.2756)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 264 is less than current step: 499. Dropping entry: {'train/loss': 1.8477073311805725, 'train/ce_loss': 0.34208984375, 'train/seg_cls_loss': 0.015216064453125, 'train/kl_loss': 0.29609375, 'train/mask_bce_loss': 0.1613783234730363, 'train/mask_dice_loss': 0.572875726222992, 'train/mask_loss': 0.7342540502548218, 'metrics/total_secs_per_batch': 6.156088352203369, 'metrics/data_secs_per_batch': 2.9684693574905396, '_timestamp': 1740976643.1184208}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 264 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976643.1187093}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 265 is less than current step: 499. Dropping entry: {'train/loss': 1.4464013695716857, 'train/ce_loss': 0.5984375, 'train/seg_cls_loss': 0.0077880859375, 'train/kl_loss': 0.2197265625, 'train/mask_bce_loss': 0.09537249840795994, 'train/mask_dice_loss': 0.3156211569905281, 'train/mask_loss': 0.4109936594963074, 'metrics/total_secs_per_batch': 5.071963787078857, 'metrics/data_secs_per_batch': 2.29507462978363, '_timestamp': 1740976648.1905437}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 265 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976648.1908946}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 266 is less than current step: 499. Dropping entry: {'train/loss': 1.051385796070099, 'train/ce_loss': 0.366552734375, 'train/seg_cls_loss': 0.009661865234375, 'train/kl_loss': 0.2177734375, 'train/mask_bce_loss': 0.05362552460283041, 'train/mask_dice_loss': 0.275558577477932, 'train/mask_loss': 0.32918410152196886, 'metrics/total_secs_per_batch': 6.019959211349487, 'metrics/data_secs_per_batch': 2.4850591659545898, '_timestamp': 1740976654.210446}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 266 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976654.2107565}).
Epoch: [7][268/500]	Time  6.075 ( 6.075)	Loss 1.7894 (1.8749)	CeLoss 0.2559 (0.4860)	SegCLSLoss 0.0128 (0.0116)	KLLoss 0.3711 (0.2924)	MaskLoss 0.7453 (0.6770)	MaskBCELoss 0.1529 (0.1312)	MaskDICELoss 0.5923 (0.5458)
Epoch: [7][269/500]	Time  4.907 ( 4.907)	Loss 2.3962 (1.4820)	CeLoss 0.1426 (0.7622)	SegCLSLoss 0.0244 (0.0074)	KLLoss 0.3867 (0.1838)	MaskLoss 1.1014 (0.3487)	MaskBCELoss 0.2018 (0.0829)	MaskDICELoss 0.8997 (0.2659)
[2025-03-02 22:37:50,640] [INFO] [logging.py:96:log_dist] [Rank 0] step=3770, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:37:50,647] [INFO] [timer.py:215:stop] epoch=0/micro_step=37700/global_step=3770, RunningAvgSamplesPerSec=1.4630694628712375, CurrSamplesPerSec=1.83342852424422, MemAllocated=30.77GB, MaxMemAllocated=37.23GB
Epoch: [7][270/500]	Time  5.456 ( 5.456)	Loss 0.9180 (1.5602)	CeLoss 0.9180 (0.5441)	SegCLSLoss 0.0000 (0.0102)	KLLoss 0.0000 (0.2529)	MaskLoss 0.0000 (0.4928)	MaskBCELoss 0.0000 (0.0634)	MaskDICELoss 0.0000 (0.4294)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 267 is less than current step: 499. Dropping entry: {'train/loss': 1.8748509526252746, 'train/ce_loss': 0.48603515625, 'train/seg_cls_loss': 0.011639404296875, 'train/kl_loss': 0.2923828125, 'train/mask_bce_loss': 0.13119115568697454, 'train/mask_dice_loss': 0.5457851111888885, 'train/mask_loss': 0.6769762575626374, 'metrics/total_secs_per_batch': 6.074575185775757, 'metrics/data_secs_per_batch': 2.652639317512512, '_timestamp': 1740976660.2852101}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 267 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976660.2855747}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 268 is less than current step: 499. Dropping entry: {'train/loss': 1.482005912065506, 'train/ce_loss': 0.76220703125, 'train/seg_cls_loss': 0.007373046875, 'train/kl_loss': 0.1837890625, 'train/mask_bce_loss': 0.08286032006144524, 'train/mask_dice_loss': 0.26585748195648196, 'train/mask_loss': 0.34871779978275297, 'metrics/total_secs_per_batch': 4.906742095947266, 'metrics/data_secs_per_batch': 2.0313929319381714, '_timestamp': 1740976665.1917129}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 268 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976665.1920958}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 269 is less than current step: 499. Dropping entry: {'train/loss': 1.5602106690406798, 'train/ce_loss': 0.544140625, 'train/seg_cls_loss': 0.010247802734375, 'train/kl_loss': 0.2529296875, 'train/mask_bce_loss': 0.06341437892988325, 'train/mask_dice_loss': 0.42938626408576963, 'train/mask_loss': 0.4928006410598755, 'metrics/total_secs_per_batch': 5.4560370445251465, 'metrics/data_secs_per_batch': 2.6204144954681396, '_timestamp': 1740976670.6475308}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 269 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976670.6478481}).
Epoch: [7][271/500]	Time  4.850 ( 4.850)	Loss 1.2969 (1.5884)	CeLoss 1.2969 (0.7106)	SegCLSLoss 0.0000 (0.0089)	KLLoss 0.0000 (0.1799)	MaskLoss 0.0000 (0.4277)	MaskBCELoss 0.0000 (0.0573)	MaskDICELoss 0.0000 (0.3704)
Epoch: [7][272/500]	Time  7.102 ( 7.102)	Loss 0.8579 (1.6169)	CeLoss 0.2578 (0.2099)	SegCLSLoss 0.0089 (0.0117)	KLLoss 0.3574 (0.2883)	MaskLoss 0.2805 (0.6861)	MaskBCELoss 0.1325 (0.0889)	MaskDICELoss 0.1480 (0.5972)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 270 is less than current step: 499. Dropping entry: {'train/loss': 1.5884084463119508, 'train/ce_loss': 0.71064453125, 'train/seg_cls_loss': 0.00888671875, 'train/kl_loss': 0.1798828125, 'train/mask_bce_loss': 0.057329267263412476, 'train/mask_dice_loss': 0.37037104964256284, 'train/mask_loss': 0.42770032286643983, 'metrics/total_secs_per_batch': 4.850116014480591, 'metrics/data_secs_per_batch': 2.132703495025635, '_timestamp': 1740976675.4981537}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 270 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976675.4985447}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 271 is less than current step: 499. Dropping entry: {'train/loss': 1.6168698787689209, 'train/ce_loss': 0.209912109375, 'train/seg_cls_loss': 0.011676025390625, 'train/kl_loss': 0.28828125, 'train/mask_bce_loss': 0.08893676549196243, 'train/mask_dice_loss': 0.5972081393003463, 'train/mask_loss': 0.6861449003219604, 'metrics/total_secs_per_batch': 7.1017608642578125, 'metrics/data_secs_per_batch': 3.339707088470459, '_timestamp': 1740976682.599588}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 271 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976682.5998814}).
Epoch: [7][273/500]	Time  6.794 ( 6.794)	Loss 0.4764 (1.6391)	CeLoss 0.2539 (0.2264)	SegCLSLoss 0.0125 (0.0130)	KLLoss 0.3789 (0.3299)	MaskLoss 0.0898 (0.6866)	MaskBCELoss 0.0568 (0.1746)	MaskDICELoss 0.0330 (0.5120)
Epoch: [7][274/500]	Time  6.275 ( 6.275)	Loss 2.4592 (1.9638)	CeLoss 0.2773 (0.4967)	SegCLSLoss 0.0161 (0.0146)	KLLoss 0.3652 (0.2898)	MaskLoss 1.0685 (0.7155)	MaskBCELoss 0.4475 (0.1253)	MaskDICELoss 0.6210 (0.5902)
Epoch: [7][275/500]	Time  5.813 ( 5.813)	Loss 0.5459 (1.7333)	CeLoss 0.2715 (0.4058)	SegCLSLoss 0.0139 (0.0137)	KLLoss 0.3574 (0.2900)	MaskLoss 0.1157 (0.6459)	MaskBCELoss 0.0104 (0.1074)	MaskDICELoss 0.1053 (0.5384)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 272 is less than current step: 499. Dropping entry: {'train/loss': 1.639148658514023, 'train/ce_loss': 0.226416015625, 'train/seg_cls_loss': 0.01298828125, 'train/kl_loss': 0.3298828125, 'train/mask_bce_loss': 0.17456087083555757, 'train/mask_dice_loss': 0.5120300427079201, 'train/mask_loss': 0.6865909181535244, 'metrics/total_secs_per_batch': 6.793520212173462, 'metrics/data_secs_per_batch': 3.290440797805786, '_timestamp': 1740976689.3932018}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 272 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976689.3934093}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 273 is less than current step: 499. Dropping entry: {'train/loss': 1.9638280272483826, 'train/ce_loss': 0.4966796875, 'train/seg_cls_loss': 0.01461181640625, 'train/kl_loss': 0.28984375, 'train/mask_bce_loss': 0.12530673891305924, 'train/mask_dice_loss': 0.590152183175087, 'train/mask_loss': 0.7154589235782624, 'metrics/total_secs_per_batch': 6.275034189224243, 'metrics/data_secs_per_batch': 2.5268872022628783, '_timestamp': 1740976695.6682491}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 273 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976695.668573}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 274 is less than current step: 499. Dropping entry: {'train/loss': 1.7333093225955962, 'train/ce_loss': 0.40576171875, 'train/seg_cls_loss': 0.013653564453125, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.10742872739210725, 'train/mask_dice_loss': 0.5384251415729523, 'train/mask_loss': 0.6458538800477982, 'metrics/total_secs_per_batch': 5.812706470489502, 'metrics/data_secs_per_batch': 2.7153639793395996, '_timestamp': 1740976701.4808495}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 274 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976701.4811492}).
Epoch: [7][276/500]	Time  5.688 ( 5.688)	Loss 2.4637 (1.7281)	CeLoss 0.1777 (0.3786)	SegCLSLoss 0.0217 (0.0142)	KLLoss 0.3574 (0.2887)	MaskLoss 1.1196 (0.6567)	MaskBCELoss 0.3740 (0.1363)	MaskDICELoss 0.7455 (0.5204)
Epoch: [7][277/500]	Time  5.159 ( 5.159)	Loss 0.9752 (1.5152)	CeLoss 0.2539 (0.5031)	SegCLSLoss 0.0103 (0.0111)	KLLoss 0.3633 (0.2541)	MaskLoss 0.3392 (0.4905)	MaskBCELoss 0.1537 (0.0969)	MaskDICELoss 0.1855 (0.3937)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 275 is less than current step: 499. Dropping entry: {'train/loss': 1.7280505895614624, 'train/ce_loss': 0.37861328125, 'train/seg_cls_loss': 0.014239501953125, 'train/kl_loss': 0.288671875, 'train/mask_bce_loss': 0.13627289598807693, 'train/mask_dice_loss': 0.5203793585300446, 'train/mask_loss': 0.6566522479057312, 'metrics/total_secs_per_batch': 5.688493490219116, 'metrics/data_secs_per_batch': 2.4826564311981203, '_timestamp': 1740976707.1693404}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 275 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976707.1696186}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 276 is less than current step: 499. Dropping entry: {'train/loss': 1.5152356266975402, 'train/ce_loss': 0.503125, 'train/seg_cls_loss': 0.0111083984375, 'train/kl_loss': 0.2541015625, 'train/mask_bce_loss': 0.09685046132653952, 'train/mask_dice_loss': 0.39367750734090806, 'train/mask_loss': 0.4905279725790024, 'metrics/total_secs_per_batch': 5.15913987159729, 'metrics/data_secs_per_batch': 2.339588189125061, '_timestamp': 1740976712.3284657}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 276 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976712.3288074}).
Epoch: [7][278/500]	Time  5.096 ( 5.096)	Loss 0.8323 (1.4550)	CeLoss 0.2793 (0.4267)	SegCLSLoss 0.0129 (0.0137)	KLLoss 0.3633 (0.2580)	MaskLoss 0.2550 (0.4977)	MaskBCELoss 0.1129 (0.0975)	MaskDICELoss 0.1421 (0.4002)
Epoch: [7][279/500]	Time  5.554 ( 5.554)	Loss 2.5042 (1.5369)	CeLoss 0.2949 (0.4371)	SegCLSLoss 0.0165 (0.0125)	KLLoss 0.3750 (0.2895)	MaskLoss 1.0822 (0.5323)	MaskBCELoss 0.3308 (0.1161)	MaskDICELoss 0.7514 (0.4163)
[2025-03-02 22:38:49,220] [INFO] [logging.py:96:log_dist] [Rank 0] step=3780, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:38:49,225] [INFO] [timer.py:215:stop] epoch=0/micro_step=37800/global_step=3780, RunningAvgSamplesPerSec=1.46362403291668, CurrSamplesPerSec=1.6009340777099066, MemAllocated=31.26GB, MaxMemAllocated=37.23GB
Epoch: [7][280/500]	Time  6.248 ( 6.248)	Loss 2.5414 (1.5197)	CeLoss 0.2021 (0.3011)	SegCLSLoss 0.0183 (0.0128)	KLLoss 0.3633 (0.2887)	MaskLoss 1.1472 (0.5916)	MaskBCELoss 0.2822 (0.0638)	MaskDICELoss 0.8650 (0.5278)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 277 is less than current step: 499. Dropping entry: {'train/loss': 1.4549781501293182, 'train/ce_loss': 0.42666015625, 'train/seg_cls_loss': 0.013714599609375, 'train/kl_loss': 0.2580078125, 'train/mask_bce_loss': 0.09751751441508531, 'train/mask_dice_loss': 0.40018639266490935, 'train/mask_loss': 0.49770391136407854, 'metrics/total_secs_per_batch': 5.09558629989624, 'metrics/data_secs_per_batch': 2.394264030456543, '_timestamp': 1740976717.4241276}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 277 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976717.4245284}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 278 is less than current step: 499. Dropping entry: {'train/loss': 1.5368602991104126, 'train/ce_loss': 0.437109375, 'train/seg_cls_loss': 0.01248779296875, 'train/kl_loss': 0.289453125, 'train/mask_bce_loss': 0.11606048420071602, 'train/mask_dice_loss': 0.4162856966257095, 'train/mask_loss': 0.5323461785912513, 'metrics/total_secs_per_batch': 5.554084539413452, 'metrics/data_secs_per_batch': 2.884403705596924, '_timestamp': 1740976722.9782913}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 278 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976722.9786}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 279 is less than current step: 499. Dropping entry: {'train/loss': 1.5197474837303162, 'train/ce_loss': 0.30107421875, 'train/seg_cls_loss': 0.01278076171875, 'train/kl_loss': 0.288671875, 'train/mask_bce_loss': 0.06377776795998216, 'train/mask_dice_loss': 0.5278342485427856, 'train/mask_loss': 0.591612023115158, 'metrics/total_secs_per_batch': 6.248116731643677, 'metrics/data_secs_per_batch': 2.6180627822875975, '_timestamp': 1740976729.226135}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 279 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976729.226433}).
Epoch: [7][281/500]	Time  5.822 ( 5.822)	Loss 0.2236 (1.3800)	CeLoss 0.2236 (0.4301)	SegCLSLoss 0.0000 (0.0096)	KLLoss 0.0000 (0.2188)	MaskLoss 0.0000 (0.4616)	MaskBCELoss 0.0000 (0.0716)	MaskDICELoss 0.0000 (0.3899)
Epoch: [7][282/500]	Time  4.978 ( 4.978)	Loss 2.2181 (1.5452)	CeLoss 0.1777 (0.4292)	SegCLSLoss 0.0254 (0.0130)	KLLoss 0.3750 (0.2547)	MaskLoss 0.9948 (0.5419)	MaskBCELoss 0.0608 (0.0685)	MaskDICELoss 0.9340 (0.4734)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 280 is less than current step: 499. Dropping entry: {'train/loss': 1.3799956798553468, 'train/ce_loss': 0.430078125, 'train/seg_cls_loss': 0.0095947265625, 'train/kl_loss': 0.21875, 'train/mask_bce_loss': 0.07164121121168136, 'train/mask_dice_loss': 0.38993866443634034, 'train/mask_loss': 0.4615798756480217, 'metrics/total_secs_per_batch': 5.821956396102905, 'metrics/data_secs_per_batch': 2.466020655632019, '_timestamp': 1740976735.048227}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 280 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976735.04842}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 281 is less than current step: 499. Dropping entry: {'train/loss': 1.5451695322990417, 'train/ce_loss': 0.42919921875, 'train/seg_cls_loss': 0.013037109375, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.06851336411200463, 'train/mask_dice_loss': 0.4733585178852081, 'train/mask_loss': 0.5418718919157982, 'metrics/total_secs_per_batch': 4.97828221321106, 'metrics/data_secs_per_batch': 2.4799206495285033, '_timestamp': 1740976740.0265083}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 281 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976740.0267859}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 282 is less than current step: 499. Dropping entry: {'train/loss': 1.8496773838996887, 'train/ce_loss': 0.40986328125, 'train/seg_cls_loss': 0.014202880859375, 'train/kl_loss': 0.292578125, 'train/mask_bce_loss': 0.1410694859921932, 'train/mask_dice_loss': 0.5606246799230575, 'train/mask_loss': 0.7016941606998444, 'metrics/total_secs_per_batch': 6.0878987312316895, 'metrics/data_secs_per_batch': 2.583020806312561, '_timestamp': 1740976746.1144097}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 282 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976746.114695}).
Epoch: [7][283/500]	Time  6.088 ( 6.088)	Loss 2.3622 (1.8497)	CeLoss 0.1992 (0.4099)	SegCLSLoss 0.0184 (0.0142)	KLLoss 0.3789 (0.2926)	MaskLoss 1.0581 (0.7017)	MaskBCELoss 0.1760 (0.1411)	MaskDICELoss 0.8821 (0.5606)
Epoch: [7][284/500]	Time  6.312 ( 6.312)	Loss 1.8178 (1.6295)	CeLoss 0.2168 (0.6170)	SegCLSLoss 0.0220 (0.0104)	KLLoss 0.3496 (0.2150)	MaskLoss 0.7781 (0.4929)	MaskBCELoss 0.0347 (0.0603)	MaskDICELoss 0.7434 (0.4326)
Epoch: [7][285/500]	Time  5.090 ( 5.090)	Loss 2.2910 (1.4965)	CeLoss 0.2119 (0.6935)	SegCLSLoss 0.0168 (0.0080)	KLLoss 0.3613 (0.2168)	MaskLoss 1.0166 (0.3887)	MaskBCELoss 0.0208 (0.0426)	MaskDICELoss 0.9958 (0.3461)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 283 is less than current step: 499. Dropping entry: {'train/loss': 1.6295467495918274, 'train/ce_loss': 0.6169921875, 'train/seg_cls_loss': 0.01038818359375, 'train/kl_loss': 0.2150390625, 'train/mask_bce_loss': 0.06031672861427069, 'train/mask_dice_loss': 0.4325816482305527, 'train/mask_loss': 0.4928983747959137, 'metrics/total_secs_per_batch': 6.312198162078857, 'metrics/data_secs_per_batch': 2.2606236934661865, '_timestamp': 1740976752.4266102}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 283 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976752.426978}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 284 is less than current step: 499. Dropping entry: {'train/loss': 1.4964998364448547, 'train/ce_loss': 0.69345703125, 'train/seg_cls_loss': 0.007989501953125, 'train/kl_loss': 0.216796875, 'train/mask_bce_loss': 0.04257489377632737, 'train/mask_dice_loss': 0.34610470533370974, 'train/mask_loss': 0.38867959976196287, 'metrics/total_secs_per_batch': 5.089776515960693, 'metrics/data_secs_per_batch': 2.1276896953582765, '_timestamp': 1740976757.5164032}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 284 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976757.5166898}).
Epoch: [7][286/500]	Time  5.396 ( 5.396)	Loss 3.1988 (1.7797)	CeLoss 0.1523 (0.4721)	SegCLSLoss 0.0188 (0.0135)	KLLoss 0.3750 (0.2906)	MaskLoss 1.4998 (0.6358)	MaskBCELoss 0.6738 (0.1601)	MaskDICELoss 0.8260 (0.4757)
Epoch: [7][287/500]	Time  4.315 ( 4.315)	Loss 1.4609 (1.3985)	CeLoss 1.4609 (0.5649)	SegCLSLoss 0.0000 (0.0087)	KLLoss 0.0000 (0.1828)	MaskLoss 0.0000 (0.4055)	MaskBCELoss 0.0000 (0.1713)	MaskDICELoss 0.0000 (0.2342)
Epoch: [7][288/500]	Time  6.869 ( 6.869)	Loss 1.3227 (1.9660)	CeLoss 0.3145 (0.2224)	SegCLSLoss 0.0106 (0.0180)	KLLoss 0.3652 (0.3615)	MaskLoss 0.4827 (0.8490)	MaskBCELoss 0.0710 (0.1557)	MaskDICELoss 0.4117 (0.6934)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 285 is less than current step: 499. Dropping entry: {'train/loss': 1.7797219395637511, 'train/ce_loss': 0.4720703125, 'train/seg_cls_loss': 0.01353759765625, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.16009287685155868, 'train/mask_dice_loss': 0.4756665199995041, 'train/mask_loss': 0.6357593923807144, 'metrics/total_secs_per_batch': 5.395941734313965, 'metrics/data_secs_per_batch': 2.6451494455337525, '_timestamp': 1740976762.9124022}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 285 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976762.9127936}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 286 is less than current step: 499. Dropping entry: {'train/loss': 1.3985157251358031, 'train/ce_loss': 0.56494140625, 'train/seg_cls_loss': 0.00869140625, 'train/kl_loss': 0.1828125, 'train/mask_bce_loss': 0.17131571769714354, 'train/mask_dice_loss': 0.23419214487075807, 'train/mask_loss': 0.4055078625679016, 'metrics/total_secs_per_batch': 4.315109491348267, 'metrics/data_secs_per_batch': 2.0337676048278808, '_timestamp': 1740976767.227575}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 286 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976767.227897}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 287 is less than current step: 499. Dropping entry: {'train/loss': 1.966041910648346, 'train/ce_loss': 0.22236328125, 'train/seg_cls_loss': 0.018048095703125, 'train/kl_loss': 0.3615234375, 'train/mask_bce_loss': 0.15567360538989305, 'train/mask_dice_loss': 0.6933629900217056, 'train/mask_loss': 0.8490365862846374, 'metrics/total_secs_per_batch': 6.868916749954224, 'metrics/data_secs_per_batch': 2.956306719779968, '_timestamp': 1740976774.0964062}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 287 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976774.0966964}).
Epoch: [7][289/500]	Time  5.894 ( 5.894)	Loss 2.2845 (1.8856)	CeLoss 0.1738 (0.5034)	SegCLSLoss 0.0216 (0.0152)	KLLoss 0.3516 (0.2900)	MaskLoss 1.0324 (0.6728)	MaskBCELoss 0.2890 (0.0970)	MaskDICELoss 0.7434 (0.5758)
[2025-03-02 22:39:46,225] [INFO] [logging.py:96:log_dist] [Rank 0] step=3790, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:39:46,231] [INFO] [timer.py:215:stop] epoch=0/micro_step=37900/global_step=3790, RunningAvgSamplesPerSec=1.4642650281019818, CurrSamplesPerSec=1.6023753473662816, MemAllocated=30.71GB, MaxMemAllocated=37.23GB
Epoch: [7][290/500]	Time  6.242 ( 6.242)	Loss 1.0938 (1.4144)	CeLoss 1.0938 (0.3442)	SegCLSLoss 0.0000 (0.0129)	KLLoss 0.0000 (0.3256)	MaskLoss 0.0000 (0.5155)	MaskBCELoss 0.0000 (0.0843)	MaskDICELoss 0.0000 (0.4312)
Epoch: [7][291/500]	Time  5.791 ( 5.791)	Loss 2.0607 (1.7465)	CeLoss 0.1953 (0.3466)	SegCLSLoss 0.0172 (0.0145)	KLLoss 0.3633 (0.2924)	MaskLoss 0.9102 (0.6817)	MaskBCELoss 0.0359 (0.0815)	MaskDICELoss 0.8743 (0.6002)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 288 is less than current step: 499. Dropping entry: {'train/loss': 1.8855972170829773, 'train/ce_loss': 0.50341796875, 'train/seg_cls_loss': 0.015185546875, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.09704633792862297, 'train/mask_dice_loss': 0.5757815688848495, 'train/mask_loss': 0.6728279113769531, 'metrics/total_secs_per_batch': 5.893881797790527, 'metrics/data_secs_per_batch': 2.486774444580078, '_timestamp': 1740976779.9902873}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 288 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976779.9905713}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 289 is less than current step: 499. Dropping entry: {'train/loss': 1.4144192159175872, 'train/ce_loss': 0.34423828125, 'train/seg_cls_loss': 0.01292724609375, 'train/kl_loss': 0.3255859375, 'train/mask_bce_loss': 0.0842990767210722, 'train/mask_dice_loss': 0.43121132217347624, 'train/mask_loss': 0.5155103892087937, 'metrics/total_secs_per_batch': 6.242310047149658, 'metrics/data_secs_per_batch': 2.890135335922241, '_timestamp': 1740976786.232369}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 289 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976786.232633}).
Epoch: [7][292/500]	Time  5.852 ( 5.852)	Loss 1.8093 (1.4796)	CeLoss 0.2832 (0.2908)	SegCLSLoss 0.0138 (0.0129)	KLLoss 0.3633 (0.2934)	MaskLoss 0.7406 (0.5765)	MaskBCELoss 0.0173 (0.0833)	MaskDICELoss 0.7233 (0.4932)
Epoch: [7][293/500]	Time  6.316 ( 6.316)	Loss 1.8262 (1.5252)	CeLoss 0.3105 (0.4328)	SegCLSLoss 0.0249 (0.0129)	KLLoss 0.3691 (0.2930)	MaskLoss 0.7334 (0.5284)	MaskBCELoss 0.1287 (0.0814)	MaskDICELoss 0.6047 (0.4471)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 290 is less than current step: 499. Dropping entry: {'train/loss': 1.7464664578437805, 'train/ce_loss': 0.34658203125, 'train/seg_cls_loss': 0.01446533203125, 'train/kl_loss': 0.2923828125, 'train/mask_bce_loss': 0.08151519894599915, 'train/mask_dice_loss': 0.6002141118049622, 'train/mask_loss': 0.6817293167114258, 'metrics/total_secs_per_batch': 5.79149317741394, 'metrics/data_secs_per_batch': 2.339673089981079, '_timestamp': 1740976792.024053}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 290 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976792.024333}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 291 is less than current step: 499. Dropping entry: {'train/loss': 1.4796165645122528, 'train/ce_loss': 0.2908203125, 'train/seg_cls_loss': 0.012933349609375, 'train/kl_loss': 0.293359375, 'train/mask_bce_loss': 0.08331740275025368, 'train/mask_dice_loss': 0.493209633231163, 'train/mask_loss': 0.5765270411968231, 'metrics/total_secs_per_batch': 5.851822137832642, 'metrics/data_secs_per_batch': 2.713427925109863, '_timestamp': 1740976797.8759096}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 291 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976797.8761935}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 292 is less than current step: 499. Dropping entry: {'train/loss': 1.5252448797225953, 'train/ce_loss': 0.4328125, 'train/seg_cls_loss': 0.0128662109375, 'train/kl_loss': 0.29296875, 'train/mask_bce_loss': 0.0813549142330885, 'train/mask_dice_loss': 0.447087836265564, 'train/mask_loss': 0.5284427404403687, 'metrics/total_secs_per_batch': 6.316157341003418, 'metrics/data_secs_per_batch': 2.963711667060852, '_timestamp': 1740976804.1920455}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 292 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976804.192325}).
Epoch: [7][294/500]	Time  6.276 ( 6.276)	Loss 1.6547 (1.5209)	CeLoss 0.2031 (0.3405)	SegCLSLoss 0.0226 (0.0162)	KLLoss 0.3516 (0.3303)	MaskLoss 0.7023 (0.5697)	MaskBCELoss 0.0159 (0.1426)	MaskDICELoss 0.6865 (0.4272)
Epoch: [7][295/500]	Time  5.076 ( 5.076)	Loss 1.7489 (1.7519)	CeLoss 0.2363 (0.7018)	SegCLSLoss 0.0116 (0.0088)	KLLoss 0.3633 (0.2176)	MaskLoss 0.7348 (0.5119)	MaskBCELoss 0.1026 (0.1331)	MaskDICELoss 0.6322 (0.3788)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 293 is less than current step: 499. Dropping entry: {'train/loss': 1.520898586511612, 'train/ce_loss': 0.34052734375, 'train/seg_cls_loss': 0.0161865234375, 'train/kl_loss': 0.3302734375, 'train/mask_bce_loss': 0.1425528088817373, 'train/mask_dice_loss': 0.4271738275885582, 'train/mask_loss': 0.5697266355156898, 'metrics/total_secs_per_batch': 6.2762651443481445, 'metrics/data_secs_per_batch': 3.103239107131958, '_timestamp': 1740976810.4683557}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 293 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976810.4686394}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 294 is less than current step: 499. Dropping entry: {'train/loss': 1.7519391894340515, 'train/ce_loss': 0.7017578125, 'train/seg_cls_loss': 0.00875244140625, 'train/kl_loss': 0.217578125, 'train/mask_bce_loss': 0.13310567513108254, 'train/mask_dice_loss': 0.37880141139030454, 'train/mask_loss': 0.5119070827960968, 'metrics/total_secs_per_batch': 5.075747728347778, 'metrics/data_secs_per_batch': 2.4218798875808716, '_timestamp': 1740976815.5440996}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 294 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976815.5443797}).
Epoch: [7][296/500]	Time  7.021 ( 7.021)	Loss 2.4578 (1.6574)	CeLoss 0.1328 (0.2935)	SegCLSLoss 0.0322 (0.0158)	KLLoss 0.3809 (0.2926)	MaskLoss 1.1357 (0.6633)	MaskBCELoss 0.2577 (0.1265)	MaskDICELoss 0.8779 (0.5369)
Epoch: [7][297/500]	Time  6.826 ( 6.826)	Loss 0.1738 (1.5110)	CeLoss 0.1738 (0.2437)	SegCLSLoss 0.0000 (0.0126)	KLLoss 0.0000 (0.2512)	MaskLoss 0.0000 (0.6181)	MaskBCELoss 0.0000 (0.1429)	MaskDICELoss 0.0000 (0.4752)
Epoch: [7][298/500]	Time  4.458 ( 4.458)	Loss 1.1172 (1.4618)	CeLoss 1.1172 (0.8004)	SegCLSLoss 0.0000 (0.0072)	KLLoss 0.0000 (0.1451)	MaskLoss 0.0000 (0.3216)	MaskBCELoss 0.0000 (0.0606)	MaskDICELoss 0.0000 (0.2610)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 295 is less than current step: 499. Dropping entry: {'train/loss': 1.6574398875236511, 'train/ce_loss': 0.29345703125, 'train/seg_cls_loss': 0.01578369140625, 'train/kl_loss': 0.292578125, 'train/mask_bce_loss': 0.1264774540439248, 'train/mask_dice_loss': 0.5368616372346878, 'train/mask_loss': 0.6633390843868255, 'metrics/total_secs_per_batch': 7.021023750305176, 'metrics/data_secs_per_batch': 3.4734644174575804, '_timestamp': 1740976822.5651295}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 295 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976822.5654225}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 296 is less than current step: 499. Dropping entry: {'train/loss': 1.5109947800636292, 'train/ce_loss': 0.24365234375, 'train/seg_cls_loss': 0.012591552734375, 'train/kl_loss': 0.251171875, 'train/mask_bce_loss': 0.1429238948971033, 'train/mask_dice_loss': 0.47517114579677583, 'train/mask_loss': 0.618095052242279, 'metrics/total_secs_per_batch': 6.826182126998901, 'metrics/data_secs_per_batch': 3.0070605754852293, '_timestamp': 1740976829.3913229}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 296 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976829.391618}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 297 is less than current step: 499. Dropping entry: {'train/loss': 1.4617765069007873, 'train/ce_loss': 0.800390625, 'train/seg_cls_loss': 0.007177734375, 'train/kl_loss': 0.1451171875, 'train/mask_bce_loss': 0.06057716719806194, 'train/mask_dice_loss': 0.2610337436199188, 'train/mask_loss': 0.3216109097003937, 'metrics/total_secs_per_batch': 4.457703113555908, 'metrics/data_secs_per_batch': 2.119232153892517, '_timestamp': 1740976833.8491893}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 297 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976833.849536}).
Epoch: [7][299/500]	Time  5.313 ( 5.313)	Loss 2.0007 (1.5181)	CeLoss 0.2383 (0.5263)	SegCLSLoss 0.0200 (0.0126)	KLLoss 0.3555 (0.2523)	MaskLoss 0.8587 (0.4802)	MaskBCELoss 0.0582 (0.0977)	MaskDICELoss 0.8006 (0.3825)
[2025-03-02 22:40:45,225] [INFO] [logging.py:96:log_dist] [Rank 0] step=3800, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:40:45,231] [INFO] [timer.py:215:stop] epoch=0/micro_step=38000/global_step=3800, RunningAvgSamplesPerSec=1.464790615423089, CurrSamplesPerSec=1.6479180139666056, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [7][300/500]	Time  6.070 ( 6.070)	Loss 1.0547 (1.6987)	CeLoss 0.2793 (0.3682)	SegCLSLoss 0.0101 (0.0113)	KLLoss 0.3652 (0.2918)	MaskLoss 0.3662 (0.6477)	MaskBCELoss 0.0594 (0.1823)	MaskDICELoss 0.3068 (0.4654)
Epoch: [7][301/500]	Time  4.511 ( 4.511)	Loss 1.5819 (1.5412)	CeLoss 0.2285 (0.4956)	SegCLSLoss 0.0145 (0.0109)	KLLoss 0.3594 (0.2189)	MaskLoss 0.6552 (0.5092)	MaskBCELoss 0.2587 (0.0899)	MaskDICELoss 0.3965 (0.4193)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 298 is less than current step: 499. Dropping entry: {'train/loss': 1.5181037664413453, 'train/ce_loss': 0.52626953125, 'train/seg_cls_loss': 0.012640380859375, 'train/kl_loss': 0.25234375, 'train/mask_bce_loss': 0.09771787002682686, 'train/mask_dice_loss': 0.38252541720867156, 'train/mask_loss': 0.48024327754974366, 'metrics/total_secs_per_batch': 5.31275486946106, 'metrics/data_secs_per_batch': 2.1589559078216554, '_timestamp': 1740976839.1617975}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 298 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976839.1620927}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 299 is less than current step: 499. Dropping entry: {'train/loss': 1.6987095117568969, 'train/ce_loss': 0.368212890625, 'train/seg_cls_loss': 0.011309814453125, 'train/kl_loss': 0.291796875, 'train/mask_bce_loss': 0.18232263550162314, 'train/mask_dice_loss': 0.4653963878750801, 'train/mask_loss': 0.6477190151810646, 'metrics/total_secs_per_batch': 6.069916009902954, 'metrics/data_secs_per_batch': 2.693638253211975, '_timestamp': 1740976845.2315662}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 299 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976845.23191}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 300 is less than current step: 499. Dropping entry: {'train/loss': 1.541211998462677, 'train/ce_loss': 0.49560546875, 'train/seg_cls_loss': 0.0108642578125, 'train/kl_loss': 0.2189453125, 'train/mask_bce_loss': 0.08988829795271158, 'train/mask_dice_loss': 0.41934074759483336, 'train/mask_loss': 0.509229040145874, 'metrics/total_secs_per_batch': 4.510598659515381, 'metrics/data_secs_per_batch': 2.2103707075119017, '_timestamp': 1740976849.7423143}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 300 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976849.7425902}).
Epoch: [7][302/500]	Time  5.336 ( 5.336)	Loss 2.0736 (1.6129)	CeLoss 0.2295 (0.2440)	SegCLSLoss 0.0199 (0.0126)	KLLoss 0.3535 (0.3279)	MaskLoss 0.8991 (0.6650)	MaskBCELoss 0.3643 (0.1519)	MaskDICELoss 0.5348 (0.5131)
Epoch: [7][303/500]	Time  4.786 ( 4.786)	Loss 1.3203 (1.6208)	CeLoss 1.3203 (0.5620)	SegCLSLoss 0.0000 (0.0108)	KLLoss 0.0000 (0.1889)	MaskLoss 0.0000 (0.5173)	MaskBCELoss 0.0000 (0.1240)	MaskDICELoss 0.0000 (0.3933)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 301 is less than current step: 499. Dropping entry: {'train/loss': 1.612881100177765, 'train/ce_loss': 0.243994140625, 'train/seg_cls_loss': 0.0126220703125, 'train/kl_loss': 0.3279296875, 'train/mask_bce_loss': 0.15188254145905375, 'train/mask_dice_loss': 0.5131273463368415, 'train/mask_loss': 0.6650098919868469, 'metrics/total_secs_per_batch': 5.336233139038086, 'metrics/data_secs_per_batch': 2.206311249732971, '_timestamp': 1740976855.078538}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 301 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976855.0788403}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 302 is less than current step: 499. Dropping entry: {'train/loss': 1.6207582473754882, 'train/ce_loss': 0.5620361328125, 'train/seg_cls_loss': 0.010784912109375, 'train/kl_loss': 0.1888671875, 'train/mask_bce_loss': 0.12400218658149242, 'train/mask_dice_loss': 0.39329832792282104, 'train/mask_loss': 0.5173005104064942, 'metrics/total_secs_per_batch': 4.785982131958008, 'metrics/data_secs_per_batch': 2.259339427947998, '_timestamp': 1740976859.8644962}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 302 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976859.8647833}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 303 is less than current step: 499. Dropping entry: {'train/loss': 1.661418628692627, 'train/ce_loss': 0.3873046875, 'train/seg_cls_loss': 0.01131591796875, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.10456528086215258, 'train/mask_dice_loss': 0.5150600373744965, 'train/mask_loss': 0.6196253210306167, 'metrics/total_secs_per_batch': 6.252022743225098, 'metrics/data_secs_per_batch': 2.944906735420227, '_timestamp': 1740976866.1167421}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 303 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976866.1171062}).
Epoch: [7][304/500]	Time  6.252 ( 6.252)	Loss 1.6172 (1.6614)	CeLoss 1.6172 (0.3873)	SegCLSLoss 0.0000 (0.0113)	KLLoss 0.0000 (0.2908)	MaskLoss 0.0000 (0.6196)	MaskBCELoss 0.0000 (0.1046)	MaskDICELoss 0.0000 (0.5151)
Epoch: [7][305/500]	Time  5.266 ( 5.266)	Loss 0.9336 (1.2426)	CeLoss 0.9336 (0.5376)	SegCLSLoss 0.0000 (0.0069)	KLLoss 0.0000 (0.1529)	MaskLoss 0.0000 (0.3430)	MaskBCELoss 0.0000 (0.0887)	MaskDICELoss 0.0000 (0.2543)
Epoch: [7][306/500]	Time  6.327 ( 6.327)	Loss 1.2266 (1.5791)	CeLoss 1.2266 (0.3000)	SegCLSLoss 0.0000 (0.0147)	KLLoss 0.0000 (0.2873)	MaskLoss 0.0000 (0.6214)	MaskBCELoss 0.0000 (0.0576)	MaskDICELoss 0.0000 (0.5638)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 304 is less than current step: 499. Dropping entry: {'train/loss': 1.2426351189613343, 'train/ce_loss': 0.537646484375, 'train/seg_cls_loss': 0.006939697265625, 'train/kl_loss': 0.1529296875, 'train/mask_bce_loss': 0.08874161392450333, 'train/mask_dice_loss': 0.25428004562854767, 'train/mask_loss': 0.3430216610431671, 'metrics/total_secs_per_batch': 5.266366958618164, 'metrics/data_secs_per_batch': 2.254136896133423, '_timestamp': 1740976871.3828998}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 304 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976871.3830988}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 305 is less than current step: 499. Dropping entry: {'train/loss': 1.5790560007095338, 'train/ce_loss': 0.300048828125, 'train/seg_cls_loss': 0.014739990234375, 'train/kl_loss': 0.2873046875, 'train/mask_bce_loss': 0.05763369891792536, 'train/mask_dice_loss': 0.5638034909963607, 'train/mask_loss': 0.6214371889829635, 'metrics/total_secs_per_batch': 6.327305793762207, 'metrics/data_secs_per_batch': 2.8149054527282713, '_timestamp': 1740976877.7102137}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 305 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976877.7104914}).
Epoch: [7][307/500]	Time  6.560 ( 6.560)	Loss 2.4465 (2.0496)	CeLoss 0.2695 (0.4461)	SegCLSLoss 0.0143 (0.0127)	KLLoss 0.3652 (0.2904)	MaskLoss 1.0660 (0.7840)	MaskBCELoss 0.0769 (0.1470)	MaskDICELoss 0.9892 (0.6371)
Epoch: [7][308/500]	Time  6.060 ( 6.060)	Loss 2.6204 (1.7205)	CeLoss 0.1777 (0.2464)	SegCLSLoss 0.0253 (0.0157)	KLLoss 0.3750 (0.2916)	MaskLoss 1.1959 (0.7185)	MaskBCELoss 0.2812 (0.1418)	MaskDICELoss 0.9148 (0.5767)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 306 is less than current step: 499. Dropping entry: {'train/loss': 2.049626386165619, 'train/ce_loss': 0.44609375, 'train/seg_cls_loss': 0.0126708984375, 'train/kl_loss': 0.2904296875, 'train/mask_bce_loss': 0.1469689017161727, 'train/mask_dice_loss': 0.6370728045701981, 'train/mask_loss': 0.7840417087078094, 'metrics/total_secs_per_batch': 6.559983253479004, 'metrics/data_secs_per_batch': 2.9366397619247437, '_timestamp': 1740976884.2701879}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 306 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976884.270463}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 307 is less than current step: 499. Dropping entry: {'train/loss': 1.7204849600791932, 'train/ce_loss': 0.24638671875, 'train/seg_cls_loss': 0.015704345703125, 'train/kl_loss': 0.2916015625, 'train/mask_bce_loss': 0.14181335568428038, 'train/mask_dice_loss': 0.5766810655593873, 'train/mask_loss': 0.718494427204132, 'metrics/total_secs_per_batch': 6.059579133987427, 'metrics/data_secs_per_batch': 2.6613825082778932, '_timestamp': 1740976890.329905}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 307 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976890.3303306}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 308 is less than current step: 499. Dropping entry: {'train/loss': 1.4784623384475708, 'train/ce_loss': 0.34189453125, 'train/seg_cls_loss': 0.010516357421875, 'train/kl_loss': 0.290234375, 'train/mask_bce_loss': 0.14835527893155814, 'train/mask_dice_loss': 0.4026922911405563, 'train/mask_loss': 0.5510475814342499, 'metrics/total_secs_per_batch': 6.262102842330933, 'metrics/data_secs_per_batch': 2.4548502922058106, '_timestamp': 1740976896.5920455}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 308 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976896.5923772}).
Epoch: [7][309/500]	Time  6.262 ( 6.262)	Loss 1.0275 (1.4785)	CeLoss 0.3574 (0.3419)	SegCLSLoss 0.0108 (0.0105)	KLLoss 0.3555 (0.2902)	MaskLoss 0.3145 (0.5510)	MaskBCELoss 0.0233 (0.1484)	MaskDICELoss 0.2912 (0.4027)
[2025-03-02 22:41:42,955] [INFO] [logging.py:96:log_dist] [Rank 0] step=3810, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:41:42,960] [INFO] [timer.py:215:stop] epoch=0/micro_step=38100/global_step=3810, RunningAvgSamplesPerSec=1.4653854186707829, CurrSamplesPerSec=1.5703803845325972, MemAllocated=31.55GB, MaxMemAllocated=37.23GB
Epoch: [7][310/500]	Time  6.370 ( 6.370)	Loss 1.3220 (1.4792)	CeLoss 0.2539 (0.3578)	SegCLSLoss 0.0177 (0.0126)	KLLoss 0.3574 (0.2902)	MaskLoss 0.5116 (0.5430)	MaskBCELoss 0.0658 (0.1153)	MaskDICELoss 0.4458 (0.4277)
Epoch: [7][311/500]	Time  6.220 ( 6.220)	Loss 2.6016 (1.5058)	CeLoss 0.1758 (0.3136)	SegCLSLoss 0.0168 (0.0119)	KLLoss 0.3711 (0.2914)	MaskLoss 1.1900 (0.5785)	MaskBCELoss 0.4673 (0.1224)	MaskDICELoss 0.7226 (0.4562)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 309 is less than current step: 499. Dropping entry: {'train/loss': 1.479222333431244, 'train/ce_loss': 0.3578125, 'train/seg_cls_loss': 0.012567138671875, 'train/kl_loss': 0.290234375, 'train/mask_bce_loss': 0.11529821725562214, 'train/mask_dice_loss': 0.4277309149503708, 'train/mask_loss': 0.5430291295051575, 'metrics/total_secs_per_batch': 6.369679689407349, 'metrics/data_secs_per_batch': 2.877137470245361, '_timestamp': 1740976902.9613945}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 309 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976902.9616706}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 310 is less than current step: 499. Dropping entry: {'train/loss': 1.5058205485343934, 'train/ce_loss': 0.31357421875, 'train/seg_cls_loss': 0.011932373046875, 'train/kl_loss': 0.29140625, 'train/mask_bce_loss': 0.12235774695873261, 'train/mask_dice_loss': 0.4561872988939285, 'train/mask_loss': 0.5785450339317322, 'metrics/total_secs_per_batch': 6.219862222671509, 'metrics/data_secs_per_batch': 2.6162019968032837, '_timestamp': 1740976909.1814847}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 310 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976909.1817756}).
Epoch: [7][312/500]	Time  5.514 ( 5.514)	Loss 2.5896 (1.8607)	CeLoss 0.1973 (0.4610)	SegCLSLoss 0.0184 (0.0133)	KLLoss 0.3633 (0.2902)	MaskLoss 1.1737 (0.6821)	MaskBCELoss 0.3441 (0.1479)	MaskDICELoss 0.8296 (0.5342)
Epoch: [7][313/500]	Time  5.621 ( 5.621)	Loss 2.2460 (1.7855)	CeLoss 0.2061 (0.2172)	SegCLSLoss 0.0173 (0.0129)	KLLoss 0.3535 (0.3238)	MaskLoss 0.9980 (0.7649)	MaskBCELoss 0.0443 (0.2026)	MaskDICELoss 0.9538 (0.5623)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 311 is less than current step: 499. Dropping entry: {'train/loss': 1.8606566786766052, 'train/ce_loss': 0.46103515625, 'train/seg_cls_loss': 0.013336181640625, 'train/kl_loss': 0.290234375, 'train/mask_bce_loss': 0.14786318121477962, 'train/mask_dice_loss': 0.5342229753732681, 'train/mask_loss': 0.6820861518383026, 'metrics/total_secs_per_batch': 5.5135862827301025, 'metrics/data_secs_per_batch': 2.5690673351287843, '_timestamp': 1740976914.6950245}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 311 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976914.6952147}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 312 is less than current step: 499. Dropping entry: {'train/loss': 1.78554048538208, 'train/ce_loss': 0.217236328125, 'train/seg_cls_loss': 0.0128662109375, 'train/kl_loss': 0.323828125, 'train/mask_bce_loss': 0.20264750570058823, 'train/mask_dice_loss': 0.5622663043439389, 'train/mask_loss': 0.7649138063192368, 'metrics/total_secs_per_batch': 5.621398687362671, 'metrics/data_secs_per_batch': 2.4268120765686034, '_timestamp': 1740976920.3164558}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 312 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976920.316742}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 313 is less than current step: 499. Dropping entry: {'train/loss': 1.529472279548645, 'train/ce_loss': 0.40537109375, 'train/seg_cls_loss': 0.013531494140625, 'train/kl_loss': 0.2533203125, 'train/mask_bce_loss': 0.1157064113765955, 'train/mask_dice_loss': 0.4303041473031044, 'train/mask_loss': 0.546010547876358, 'metrics/total_secs_per_batch': 5.60634708404541, 'metrics/data_secs_per_batch': 2.2715588569641114, '_timestamp': 1740976925.9227772}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 313 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976925.923064}).
Epoch: [7][314/500]	Time  5.606 ( 5.606)	Loss 1.6843 (1.5295)	CeLoss 0.2656 (0.4054)	SegCLSLoss 0.0159 (0.0135)	KLLoss 0.3633 (0.2533)	MaskLoss 0.6869 (0.5460)	MaskBCELoss 0.1325 (0.1157)	MaskDICELoss 0.5544 (0.4303)
Epoch: [7][315/500]	Time  5.984 ( 5.984)	Loss 0.6719 (1.3593)	CeLoss 0.6719 (0.3027)	SegCLSLoss 0.0000 (0.0136)	KLLoss 0.0000 (0.2947)	MaskLoss 0.0000 (0.5104)	MaskBCELoss 0.0000 (0.1065)	MaskDICELoss 0.0000 (0.4039)
Epoch: [7][316/500]	Time  5.934 ( 5.934)	Loss 0.5352 (1.6864)	CeLoss 0.5352 (0.3395)	SegCLSLoss 0.0000 (0.0118)	KLLoss 0.0000 (0.2525)	MaskLoss 0.0000 (0.6580)	MaskBCELoss 0.0000 (0.2223)	MaskDICELoss 0.0000 (0.4357)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 314 is less than current step: 499. Dropping entry: {'train/loss': 1.3592603504657745, 'train/ce_loss': 0.302685546875, 'train/seg_cls_loss': 0.01361083984375, 'train/kl_loss': 0.2947265625, 'train/mask_bce_loss': 0.106454548984766, 'train/mask_dice_loss': 0.40391294062137606, 'train/mask_loss': 0.510367488861084, 'metrics/total_secs_per_batch': 5.983594179153442, 'metrics/data_secs_per_batch': 2.862699198722839, '_timestamp': 1740976931.9066548}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 314 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976931.9070396}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 315 is less than current step: 499. Dropping entry: {'train/loss': 1.686391258239746, 'train/ce_loss': 0.339453125, 'train/seg_cls_loss': 0.01181640625, 'train/kl_loss': 0.2525390625, 'train/mask_bce_loss': 0.22231917325407266, 'train/mask_dice_loss': 0.4356713742017746, 'train/mask_loss': 0.6579905569553375, 'metrics/total_secs_per_batch': 5.934086799621582, 'metrics/data_secs_per_batch': 2.471831512451172, '_timestamp': 1740976937.84059}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 315 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976937.8409317}).
Epoch: [7][317/500]	Time  5.856 ( 5.856)	Loss 2.2636 (1.5697)	CeLoss 0.2275 (0.4120)	SegCLSLoss 0.0195 (0.0130)	KLLoss 0.3457 (0.2146)	MaskLoss 0.9960 (0.5648)	MaskBCELoss 0.0101 (0.0657)	MaskDICELoss 0.9859 (0.4991)
Epoch: [7][318/500]	Time  6.308 ( 6.308)	Loss 1.7597 (1.5127)	CeLoss 0.1982 (0.5246)	SegCLSLoss 0.0179 (0.0101)	KLLoss 0.3613 (0.2186)	MaskLoss 0.7583 (0.4806)	MaskBCELoss 0.0284 (0.0637)	MaskDICELoss 0.7298 (0.4169)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 316 is less than current step: 499. Dropping entry: {'train/loss': 1.56973876953125, 'train/ce_loss': 0.41201171875, 'train/seg_cls_loss': 0.012957763671875, 'train/kl_loss': 0.2146484375, 'train/mask_bce_loss': 0.06569405421614646, 'train/mask_dice_loss': 0.4990581512451172, 'train/mask_loss': 0.564752209186554, 'metrics/total_secs_per_batch': 5.855528116226196, 'metrics/data_secs_per_batch': 2.484236478805542, '_timestamp': 1740976943.6960607}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 316 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976943.6963792}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 317 is less than current step: 499. Dropping entry: {'train/loss': 1.5126647472381591, 'train/ce_loss': 0.524609375, 'train/seg_cls_loss': 0.01005859375, 'train/kl_loss': 0.2185546875, 'train/mask_bce_loss': 0.06367722512222826, 'train/mask_dice_loss': 0.4168738931417465, 'train/mask_loss': 0.4805511236190796, 'metrics/total_secs_per_batch': 6.307793855667114, 'metrics/data_secs_per_batch': 2.5135560035705566, '_timestamp': 1740976950.00387}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 317 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976950.0041656}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 318 is less than current step: 499. Dropping entry: {'train/loss': 1.883093774318695, 'train/ce_loss': 0.208984375, 'train/seg_cls_loss': 0.018017578125, 'train/kl_loss': 0.3650390625, 'train/mask_bce_loss': 0.1470342766493559, 'train/mask_dice_loss': 0.6671200171113014, 'train/mask_loss': 0.8141542822122574, 'metrics/total_secs_per_batch': 6.709152698516846, 'metrics/data_secs_per_batch': 3.008827781677246, '_timestamp': 1740976956.7129486}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 318 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976956.713229}).
Epoch: [7][319/500]	Time  6.709 ( 6.709)	Loss 2.5485 (1.8831)	CeLoss 0.1279 (0.2090)	SegCLSLoss 0.0256 (0.0180)	KLLoss 0.3789 (0.3650)	MaskLoss 1.1849 (0.8142)	MaskBCELoss 0.2863 (0.1470)	MaskDICELoss 0.8986 (0.6671)
[2025-03-02 22:42:42,237] [INFO] [logging.py:96:log_dist] [Rank 0] step=3820, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:42:42,243] [INFO] [timer.py:215:stop] epoch=0/micro_step=38200/global_step=3820, RunningAvgSamplesPerSec=1.4658901992219107, CurrSamplesPerSec=1.808521624345197, MemAllocated=30.69GB, MaxMemAllocated=37.23GB
Epoch: [7][320/500]	Time  5.531 ( 5.531)	Loss 1.3359 (1.2988)	CeLoss 1.3359 (0.5283)	SegCLSLoss 0.0000 (0.0078)	KLLoss 0.0000 (0.1805)	MaskLoss 0.0000 (0.3744)	MaskBCELoss 0.0000 (0.0677)	MaskDICELoss 0.0000 (0.3067)
Epoch: [7][321/500]	Time  5.024 ( 5.024)	Loss 1.2812 (1.5332)	CeLoss 1.2812 (0.5606)	SegCLSLoss 0.0000 (0.0059)	KLLoss 0.0000 (0.1822)	MaskLoss 0.0000 (0.4758)	MaskBCELoss 0.0000 (0.1702)	MaskDICELoss 0.0000 (0.3056)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 319 is less than current step: 499. Dropping entry: {'train/loss': 1.2988052129745484, 'train/ce_loss': 0.5283203125, 'train/seg_cls_loss': 0.007763671875, 'train/kl_loss': 0.18046875, 'train/mask_bce_loss': 0.06770619032904505, 'train/mask_dice_loss': 0.30669640004634857, 'train/mask_loss': 0.3744025945663452, 'metrics/total_secs_per_batch': 5.530935764312744, 'metrics/data_secs_per_batch': 2.382773017883301, '_timestamp': 1740976962.2437904}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 319 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976962.2441382}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 320 is less than current step: 499. Dropping entry: {'train/loss': 1.5332440376281737, 'train/ce_loss': 0.56064453125, 'train/seg_cls_loss': 0.005865478515625, 'train/kl_loss': 0.1822265625, 'train/mask_bce_loss': 0.17018721103668213, 'train/mask_dice_loss': 0.30561449825763704, 'train/mask_loss': 0.47580170035362246, 'metrics/total_secs_per_batch': 5.023740291595459, 'metrics/data_secs_per_batch': 2.573519206047058, '_timestamp': 1740976967.267647}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 320 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976967.2679293}).
Epoch: [7][322/500]	Time  6.320 ( 6.320)	Loss 1.5408 (2.0152)	CeLoss 0.2617 (0.3405)	SegCLSLoss 0.0203 (0.0171)	KLLoss 0.3555 (0.3256)	MaskLoss 0.6171 (0.8168)	MaskBCELoss 0.0290 (0.1533)	MaskDICELoss 0.5881 (0.6636)
Epoch: [7][323/500]	Time  5.141 ( 5.141)	Loss 1.1319 (1.5701)	CeLoss 0.1982 (0.6271)	SegCLSLoss 0.0209 (0.0100)	KLLoss 0.3594 (0.2170)	MaskLoss 0.4434 (0.4580)	MaskBCELoss 0.0818 (0.0879)	MaskDICELoss 0.3615 (0.3701)
Epoch: [7][324/500]	Time  6.499 ( 6.499)	Loss 0.5508 (1.9076)	CeLoss 0.5508 (0.3856)	SegCLSLoss 0.0000 (0.0127)	KLLoss 0.0000 (0.2605)	MaskLoss 0.0000 (0.7446)	MaskBCELoss 0.0000 (0.1874)	MaskDICELoss 0.0000 (0.5572)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 321 is less than current step: 499. Dropping entry: {'train/loss': 2.015230929851532, 'train/ce_loss': 0.34052734375, 'train/seg_cls_loss': 0.017144775390625, 'train/kl_loss': 0.3255859375, 'train/mask_bce_loss': 0.15328215919435023, 'train/mask_dice_loss': 0.6635618299245835, 'train/mask_loss': 0.8168439865112305, 'metrics/total_secs_per_batch': 6.320279836654663, 'metrics/data_secs_per_batch': 2.7657920837402346, '_timestamp': 1740976973.5880263}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 321 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976973.5882542}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 322 is less than current step: 499. Dropping entry: {'train/loss': 1.570084822177887, 'train/ce_loss': 0.6271484375, 'train/seg_cls_loss': 0.010009765625, 'train/kl_loss': 0.2169921875, 'train/mask_bce_loss': 0.08791827689856291, 'train/mask_dice_loss': 0.3701221853494644, 'train/mask_loss': 0.458040452003479, 'metrics/total_secs_per_batch': 5.141289949417114, 'metrics/data_secs_per_batch': 2.230760645866394, '_timestamp': 1740976978.729218}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 322 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976978.7295809}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 323 is less than current step: 499. Dropping entry: {'train/loss': 1.9076209306716918, 'train/ce_loss': 0.38564453125, 'train/seg_cls_loss': 0.01265869140625, 'train/kl_loss': 0.260546875, 'train/mask_bce_loss': 0.1874201141297817, 'train/mask_dice_loss': 0.5572106599807739, 'train/mask_loss': 0.744630777835846, 'metrics/total_secs_per_batch': 6.4988837242126465, 'metrics/data_secs_per_batch': 2.8404483795166016, '_timestamp': 1740976985.2283258}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 323 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976985.2286851}).
Epoch: [7][325/500]	Time  5.595 ( 5.595)	Loss 1.1562 (1.5436)	CeLoss 1.1562 (0.5575)	SegCLSLoss 0.0000 (0.0123)	KLLoss 0.0000 (0.2543)	MaskLoss 0.0000 (0.4774)	MaskBCELoss 0.0000 (0.0956)	MaskDICELoss 0.0000 (0.3818)
Epoch: [7][326/500]	Time  5.876 ( 5.876)	Loss 2.7818 (1.6313)	CeLoss 0.2393 (0.3074)	SegCLSLoss 0.0156 (0.0134)	KLLoss 0.3633 (0.2893)	MaskLoss 1.2493 (0.6440)	MaskBCELoss 0.6662 (0.2038)	MaskDICELoss 0.5831 (0.4403)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 324 is less than current step: 499. Dropping entry: {'train/loss': 1.5436427652835847, 'train/ce_loss': 0.55751953125, 'train/seg_cls_loss': 0.012286376953125, 'train/kl_loss': 0.254296875, 'train/mask_bce_loss': 0.09561692290008068, 'train/mask_dice_loss': 0.3817708671092987, 'train/mask_loss': 0.4773877948522568, 'metrics/total_secs_per_batch': 5.59473443031311, 'metrics/data_secs_per_batch': 2.4290488243103026, '_timestamp': 1740976990.8228924}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 324 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976990.8231761}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 325 is less than current step: 499. Dropping entry: {'train/loss': 1.631304717063904, 'train/ce_loss': 0.3074462890625, 'train/seg_cls_loss': 0.013421630859375, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.2037528025917709, 'train/mask_dice_loss': 0.4402564957737923, 'train/mask_loss': 0.6440092921257019, 'metrics/total_secs_per_batch': 5.875833511352539, 'metrics/data_secs_per_batch': 2.559332990646362, '_timestamp': 1740976996.6987565}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 325 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740976996.699063}).
Epoch: [7][327/500]	Time  5.944 ( 5.944)	Loss 1.7472 (1.3788)	CeLoss 0.2070 (0.3417)	SegCLSLoss 0.0188 (0.0093)	KLLoss 0.3574 (0.2184)	MaskLoss 0.7476 (0.5053)	MaskBCELoss 0.0162 (0.0838)	MaskDICELoss 0.7315 (0.4216)
Epoch: [7][328/500]	Time  5.634 ( 5.634)	Loss 2.1966 (1.8395)	CeLoss 0.1455 (0.4099)	SegCLSLoss 0.0288 (0.0156)	KLLoss 0.3711 (0.2955)	MaskLoss 0.9997 (0.6962)	MaskBCELoss 0.3887 (0.2603)	MaskDICELoss 0.6110 (0.4359)
Epoch: [7][329/500]	Time  5.036 ( 5.036)	Loss 1.0234 (1.6128)	CeLoss 1.0234 (0.6090)	SegCLSLoss 0.0000 (0.0111)	KLLoss 0.0000 (0.2164)	MaskLoss 0.0000 (0.4883)	MaskBCELoss 0.0000 (0.0846)	MaskDICELoss 0.0000 (0.4037)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 326 is less than current step: 499. Dropping entry: {'train/loss': 1.3788379549980163, 'train/ce_loss': 0.341650390625, 'train/seg_cls_loss': 0.009326171875, 'train/kl_loss': 0.218359375, 'train/mask_bce_loss': 0.08375638220459222, 'train/mask_dice_loss': 0.42155615985393524, 'train/mask_loss': 0.5053125262260437, 'metrics/total_secs_per_batch': 5.944462776184082, 'metrics/data_secs_per_batch': 2.513869786262512, '_timestamp': 1740977002.6432056}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 326 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977002.6435695}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 327 is less than current step: 499. Dropping entry: {'train/loss': 1.8395166158676148, 'train/ce_loss': 0.40986328125, 'train/seg_cls_loss': 0.015557861328125, 'train/kl_loss': 0.2955078125, 'train/mask_bce_loss': 0.26029295697808263, 'train/mask_dice_loss': 0.43593019247055054, 'train/mask_loss': 0.6962231516838073, 'metrics/total_secs_per_batch': 5.633936643600464, 'metrics/data_secs_per_batch': 2.830342197418213, '_timestamp': 1740977008.2770703}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 327 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977008.277338}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 328 is less than current step: 499. Dropping entry: {'train/loss': 1.6127555608749389, 'train/ce_loss': 0.608984375, 'train/seg_cls_loss': 0.011138916015625, 'train/kl_loss': 0.21640625, 'train/mask_bce_loss': 0.0846048207487911, 'train/mask_dice_loss': 0.4037065625190735, 'train/mask_loss': 0.4883113861083984, 'metrics/total_secs_per_batch': 5.036224365234375, 'metrics/data_secs_per_batch': 2.4435134172439574, '_timestamp': 1740977013.3132963}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 328 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977013.313574}).
[2025-03-02 22:43:39,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=3830, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:43:39,747] [INFO] [timer.py:215:stop] epoch=0/micro_step=38300/global_step=3830, RunningAvgSamplesPerSec=1.466492595152958, CurrSamplesPerSec=1.5544724534757584, MemAllocated=31.09GB, MaxMemAllocated=37.23GB
Epoch: [7][330/500]	Time  6.435 ( 6.435)	Loss 1.3612 (1.7990)	CeLoss 0.2158 (0.4029)	SegCLSLoss 0.0107 (0.0116)	KLLoss 0.3691 (0.2561)	MaskLoss 0.5517 (0.6824)	MaskBCELoss 0.0504 (0.1031)	MaskDICELoss 0.5013 (0.5793)
Epoch: [7][331/500]	Time  6.287 ( 6.287)	Loss 0.8059 (1.6101)	CeLoss 0.1895 (0.3208)	SegCLSLoss 0.0118 (0.0114)	KLLoss 0.3594 (0.2895)	MaskLoss 0.2872 (0.6272)	MaskBCELoss 0.1278 (0.1560)	MaskDICELoss 0.1594 (0.4712)
Epoch: [7][332/500]	Time  5.618 ( 5.618)	Loss 0.4547 (1.4332)	CeLoss 0.1797 (0.2883)	SegCLSLoss 0.0231 (0.0149)	KLLoss 0.3555 (0.3277)	MaskLoss 0.1141 (0.5523)	MaskBCELoss 0.0198 (0.0953)	MaskDICELoss 0.0942 (0.4570)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 329 is less than current step: 499. Dropping entry: {'train/loss': 1.798987364768982, 'train/ce_loss': 0.4029296875, 'train/seg_cls_loss': 0.011602783203125, 'train/kl_loss': 0.2560546875, 'train/mask_bce_loss': 0.10309884455054999, 'train/mask_dice_loss': 0.5792561769485474, 'train/mask_loss': 0.6823550164699554, 'metrics/total_secs_per_batch': 6.434627294540405, 'metrics/data_secs_per_batch': 2.868968296051025, '_timestamp': 1740977019.7477982}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 329 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977019.748127}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 330 is less than current step: 499. Dropping entry: {'train/loss': 1.6101409912109375, 'train/ce_loss': 0.32080078125, 'train/seg_cls_loss': 0.01138916015625, 'train/kl_loss': 0.289453125, 'train/mask_bce_loss': 0.15595985781401395, 'train/mask_dice_loss': 0.4712297722697258, 'train/mask_loss': 0.6271896302700043, 'metrics/total_secs_per_batch': 6.287242650985718, 'metrics/data_secs_per_batch': 2.8566258907318116, '_timestamp': 1740977026.0352898}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 330 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977026.035596}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 331 is less than current step: 499. Dropping entry: {'train/loss': 1.4332259744405746, 'train/ce_loss': 0.28828125, 'train/seg_cls_loss': 0.014947509765625, 'train/kl_loss': 0.327734375, 'train/mask_bce_loss': 0.0953004838898778, 'train/mask_dice_loss': 0.4570058546960354, 'train/mask_loss': 0.552306343615055, 'metrics/total_secs_per_batch': 5.6177897453308105, 'metrics/data_secs_per_batch': 2.7924845695495604, '_timestamp': 1740977031.6530318}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 331 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977031.653326}).
Epoch: [7][333/500]	Time  5.130 ( 5.130)	Loss 2.5512 (1.8444)	CeLoss 0.2168 (0.6034)	SegCLSLoss 0.0228 (0.0106)	KLLoss 0.3672 (0.2541)	MaskLoss 1.1428 (0.6051)	MaskBCELoss 0.2998 (0.2039)	MaskDICELoss 0.8430 (0.4013)
Epoch: [7][334/500]	Time  5.854 ( 5.854)	Loss 1.3175 (1.4047)	CeLoss 0.2812 (0.3189)	SegCLSLoss 0.0099 (0.0136)	KLLoss 0.3652 (0.2891)	MaskLoss 0.4966 (0.5248)	MaskBCELoss 0.1824 (0.0997)	MaskDICELoss 0.3142 (0.4251)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 332 is less than current step: 499. Dropping entry: {'train/loss': 1.8444487810134889, 'train/ce_loss': 0.60341796875, 'train/seg_cls_loss': 0.01060791015625, 'train/kl_loss': 0.2541015625, 'train/mask_bce_loss': 0.20386390537023544, 'train/mask_dice_loss': 0.40127063989639283, 'train/mask_loss': 0.6051345467567444, 'metrics/total_secs_per_batch': 5.1301045417785645, 'metrics/data_secs_per_batch': 2.5251112461090086, '_timestamp': 1740977036.7830818}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 332 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977036.7833521}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 333 is less than current step: 499. Dropping entry: {'train/loss': 1.4046977937221528, 'train/ce_loss': 0.3189453125, 'train/seg_cls_loss': 0.013641357421875, 'train/kl_loss': 0.2890625, 'train/mask_bce_loss': 0.09969955533742905, 'train/mask_dice_loss': 0.4251102775335312, 'train/mask_loss': 0.5248098351061344, 'metrics/total_secs_per_batch': 5.853856563568115, 'metrics/data_secs_per_batch': 2.5161558628082275, '_timestamp': 1740977042.6369328}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 333 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977042.6372893}).
Epoch: [7][335/500]	Time  6.281 ( 6.281)	Loss 1.9193 (1.6607)	CeLoss 0.1738 (0.3248)	SegCLSLoss 0.0242 (0.0156)	KLLoss 0.3535 (0.2879)	MaskLoss 0.8493 (0.6497)	MaskBCELoss 0.1134 (0.1199)	MaskDICELoss 0.7359 (0.5298)
Epoch: [7][336/500]	Time  4.740 ( 4.740)	Loss 2.3804 (1.4739)	CeLoss 0.2070 (0.6866)	SegCLSLoss 0.0236 (0.0084)	KLLoss 0.3691 (0.1461)	MaskLoss 1.0623 (0.3841)	MaskBCELoss 0.2591 (0.0806)	MaskDICELoss 0.8032 (0.3036)
Epoch: [7][337/500]	Time  5.125 ( 5.125)	Loss 2.5832 (1.5519)	CeLoss 0.2734 (0.4386)	SegCLSLoss 0.0156 (0.0107)	KLLoss 0.3711 (0.2521)	MaskLoss 1.1324 (0.5415)	MaskBCELoss 0.5638 (0.1549)	MaskDICELoss 0.5686 (0.3866)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 334 is less than current step: 499. Dropping entry: {'train/loss': 1.6606942892074585, 'train/ce_loss': 0.3248291015625, 'train/seg_cls_loss': 0.015576171875, 'train/kl_loss': 0.287890625, 'train/mask_bce_loss': 0.11994163179770112, 'train/mask_dice_loss': 0.5297780714929103, 'train/mask_loss': 0.6497196942567826, 'metrics/total_secs_per_batch': 6.2805609703063965, 'metrics/data_secs_per_batch': 2.8705251216888428, '_timestamp': 1740977048.917723}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 334 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977048.918086}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 335 is less than current step: 499. Dropping entry: {'train/loss': 1.4739236235618591, 'train/ce_loss': 0.68662109375, 'train/seg_cls_loss': 0.0083740234375, 'train/kl_loss': 0.14609375, 'train/mask_bce_loss': 0.08055338859558106, 'train/mask_dice_loss': 0.3035763829946518, 'train/mask_loss': 0.3841297686100006, 'metrics/total_secs_per_batch': 4.740161418914795, 'metrics/data_secs_per_batch': 2.113659954071045, '_timestamp': 1740977053.657652}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 335 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977053.657935}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 336 is less than current step: 499. Dropping entry: {'train/loss': 1.5519364178180695, 'train/ce_loss': 0.43857421875, 'train/seg_cls_loss': 0.0106689453125, 'train/kl_loss': 0.2521484375, 'train/mask_bce_loss': 0.15494046211242676, 'train/mask_dice_loss': 0.3865550935268402, 'train/mask_loss': 0.5414955526590347, 'metrics/total_secs_per_batch': 5.124696969985962, 'metrics/data_secs_per_batch': 2.3482329845428467, '_timestamp': 1740977058.7823732}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 336 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977058.7826672}).
Epoch: [7][338/500]	Time  6.432 ( 6.432)	Loss 2.2319 (1.4974)	CeLoss 0.1875 (0.3971)	SegCLSLoss 0.0157 (0.0108)	KLLoss 0.3613 (0.2527)	MaskLoss 1.0003 (0.5349)	MaskBCELoss 0.2851 (0.1096)	MaskDICELoss 0.7151 (0.4252)
Epoch: [7][339/500]	Time  6.594 ( 6.594)	Loss 1.6989 (1.8253)	CeLoss 0.2139 (0.2430)	SegCLSLoss 0.0137 (0.0170)	KLLoss 0.3594 (0.3271)	MaskLoss 0.7215 (0.7705)	MaskBCELoss 0.0262 (0.1373)	MaskDICELoss 0.6954 (0.6332)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 337 is less than current step: 499. Dropping entry: {'train/loss': 1.4974423170089721, 'train/ce_loss': 0.3970703125, 'train/seg_cls_loss': 0.010791015625, 'train/kl_loss': 0.252734375, 'train/mask_bce_loss': 0.10962500935420394, 'train/mask_dice_loss': 0.4252289652824402, 'train/mask_loss': 0.5348539650440216, 'metrics/total_secs_per_batch': 6.43248438835144, 'metrics/data_secs_per_batch': 2.6611101150512697, '_timestamp': 1740977065.2149627}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 337 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977065.2152984}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 338 is less than current step: 499. Dropping entry: {'train/loss': 1.8252771139144897, 'train/ce_loss': 0.24296875, 'train/seg_cls_loss': 0.01700439453125, 'train/kl_loss': 0.3271484375, 'train/mask_bce_loss': 0.13725016172975302, 'train/mask_dice_loss': 0.6332497149705887, 'train/mask_loss': 0.7704998880624772, 'metrics/total_secs_per_batch': 6.594444990158081, 'metrics/data_secs_per_batch': 2.7852440595626833, '_timestamp': 1740977071.809452}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 338 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977071.809776}).
[2025-03-02 22:44:37,653] [INFO] [logging.py:96:log_dist] [Rank 0] step=3840, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:44:37,659] [INFO] [timer.py:215:stop] epoch=0/micro_step=38400/global_step=3840, RunningAvgSamplesPerSec=1.4670694400935205, CurrSamplesPerSec=1.7096166876168526, MemAllocated=31.47GB, MaxMemAllocated=37.23GB
Epoch: [7][340/500]	Time  5.851 ( 5.851)	Loss 1.9074 (1.8549)	CeLoss 0.1660 (0.2902)	SegCLSLoss 0.0332 (0.0159)	KLLoss 0.3672 (0.3301)	MaskLoss 0.8443 (0.7619)	MaskBCELoss 0.0261 (0.1449)	MaskDICELoss 0.8182 (0.6170)
Epoch: [7][341/500]	Time  7.216 ( 7.216)	Loss 1.8828 (1.3794)	CeLoss 1.8828 (0.4164)	SegCLSLoss 0.0000 (0.0103)	KLLoss 0.0000 (0.2160)	MaskLoss 0.0000 (0.4683)	MaskBCELoss 0.0000 (0.0763)	MaskDICELoss 0.0000 (0.3919)
Epoch: [7][342/500]	Time  6.022 ( 6.022)	Loss 2.5598 (1.8423)	CeLoss 0.1816 (0.3249)	SegCLSLoss 0.0192 (0.0154)	KLLoss 0.3613 (0.3262)	MaskLoss 1.1661 (0.7384)	MaskBCELoss 0.3417 (0.1098)	MaskDICELoss 0.8244 (0.6286)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 339 is less than current step: 499. Dropping entry: {'train/loss': 1.8548588395118712, 'train/ce_loss': 0.290234375, 'train/seg_cls_loss': 0.0159423828125, 'train/kl_loss': 0.330078125, 'train/mask_bce_loss': 0.14494214477017522, 'train/mask_dice_loss': 0.6169599309563637, 'train/mask_loss': 0.7619020789861679, 'metrics/total_secs_per_batch': 5.851053476333618, 'metrics/data_secs_per_batch': 2.636699390411377, '_timestamp': 1740977077.660204}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 339 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977077.6604934}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 340 is less than current step: 499. Dropping entry: {'train/loss': 1.3793729305267335, 'train/ce_loss': 0.416357421875, 'train/seg_cls_loss': 0.010296630859375, 'train/kl_loss': 0.216015625, 'train/mask_bce_loss': 0.0763466253876686, 'train/mask_dice_loss': 0.3919287085533142, 'train/mask_loss': 0.4682753264904022, 'metrics/total_secs_per_batch': 7.2162559032440186, 'metrics/data_secs_per_batch': 3.633713150024414, '_timestamp': 1740977084.8768947}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 340 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977084.8772602}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 341 is less than current step: 499. Dropping entry: {'train/loss': 1.8422680616378784, 'train/ce_loss': 0.32490234375, 'train/seg_cls_loss': 0.015362548828125, 'train/kl_loss': 0.326171875, 'train/mask_bce_loss': 0.10979606406763195, 'train/mask_dice_loss': 0.6286231279373169, 'train/mask_loss': 0.7384191930294037, 'metrics/total_secs_per_batch': 6.021687269210815, 'metrics/data_secs_per_batch': 2.6165093898773195, '_timestamp': 1740977090.8983338}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 341 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977090.8986094}).
Epoch: [7][343/500]	Time  5.333 ( 5.333)	Loss 0.9414 (1.4997)	CeLoss 0.9414 (0.5165)	SegCLSLoss 0.0000 (0.0099)	KLLoss 0.0000 (0.1828)	MaskLoss 0.0000 (0.4800)	MaskBCELoss 0.0000 (0.0620)	MaskDICELoss 0.0000 (0.4180)
Epoch: [7][344/500]	Time  6.526 ( 6.526)	Loss 1.0473 (1.7648)	CeLoss 0.5352 (0.3691)	SegCLSLoss 0.0137 (0.0137)	KLLoss 0.3730 (0.3283)	MaskLoss 0.2326 (0.6777)	MaskBCELoss 0.0908 (0.1234)	MaskDICELoss 0.1418 (0.5543)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 342 is less than current step: 499. Dropping entry: {'train/loss': 1.4996861338615417, 'train/ce_loss': 0.516455078125, 'train/seg_cls_loss': 0.009918212890625, 'train/kl_loss': 0.1828125, 'train/mask_bce_loss': 0.06201077308505774, 'train/mask_dice_loss': 0.41803247928619386, 'train/mask_loss': 0.4800432562828064, 'metrics/total_secs_per_batch': 5.332792043685913, 'metrics/data_secs_per_batch': 2.553832983970642, '_timestamp': 1740977096.23111}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 342 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977096.2313955}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 343 is less than current step: 499. Dropping entry: {'train/loss': 1.7648152470588685, 'train/ce_loss': 0.369140625, 'train/seg_cls_loss': 0.01365966796875, 'train/kl_loss': 0.3283203125, 'train/mask_bce_loss': 0.12339119799435139, 'train/mask_dice_loss': 0.5543289214372635, 'train/mask_loss': 0.6777201220393181, 'metrics/total_secs_per_batch': 6.5260093212127686, 'metrics/data_secs_per_batch': 2.870352101325989, '_timestamp': 1740977102.7573643}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 343 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977102.7577221}).
Epoch: [7][345/500]	Time  6.205 ( 6.205)	Loss 1.7553 (1.4626)	CeLoss 0.2832 (0.3261)	SegCLSLoss 0.0124 (0.0110)	KLLoss 0.3633 (0.2561)	MaskLoss 0.7146 (0.5527)	MaskBCELoss 0.4298 (0.1326)	MaskDICELoss 0.2848 (0.4200)
Epoch: [7][346/500]	Time  6.257 ( 6.257)	Loss 2.5151 (1.8680)	CeLoss 0.2236 (0.3303)	SegCLSLoss 0.0145 (0.0165)	KLLoss 0.3633 (0.3260)	MaskLoss 1.1238 (0.7483)	MaskBCELoss 0.2819 (0.1581)	MaskDICELoss 0.8418 (0.5902)
Epoch: [7][347/500]	Time  6.299 ( 6.299)	Loss 2.1199 (1.5472)	CeLoss 0.3672 (0.2846)	SegCLSLoss 0.0135 (0.0105)	KLLoss 0.3770 (0.2533)	MaskLoss 0.8549 (0.6160)	MaskBCELoss 0.0035 (0.1057)	MaskDICELoss 0.8514 (0.5103)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 344 is less than current step: 499. Dropping entry: {'train/loss': 1.46263028383255, 'train/ce_loss': 0.32607421875, 'train/seg_cls_loss': 0.010955810546875, 'train/kl_loss': 0.2560546875, 'train/mask_bce_loss': 0.1326348189264536, 'train/mask_dice_loss': 0.42001820504665377, 'train/mask_loss': 0.5526530265808105, 'metrics/total_secs_per_batch': 6.204747676849365, 'metrics/data_secs_per_batch': 2.805459403991699, '_timestamp': 1740977108.9620824}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 344 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977108.96247}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 345 is less than current step: 499. Dropping entry: {'train/loss': 1.8679799556732177, 'train/ce_loss': 0.3302734375, 'train/seg_cls_loss': 0.0164794921875, 'train/kl_loss': 0.3259765625, 'train/mask_bce_loss': 0.15810822658240795, 'train/mask_dice_loss': 0.5902372181415558, 'train/mask_loss': 0.7483454465866088, 'metrics/total_secs_per_batch': 6.257168292999268, 'metrics/data_secs_per_batch': 2.620128798484802, '_timestamp': 1740977115.2191834}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 345 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977115.2195137}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 346 is less than current step: 499. Dropping entry: {'train/loss': 1.5472195148468018, 'train/ce_loss': 0.2846435546875, 'train/seg_cls_loss': 0.01048583984375, 'train/kl_loss': 0.2533203125, 'train/mask_bce_loss': 0.10567911758553236, 'train/mask_dice_loss': 0.5103256583213807, 'train/mask_loss': 0.6160047769546508, 'metrics/total_secs_per_batch': 6.298890113830566, 'metrics/data_secs_per_batch': 2.9710857391357424, '_timestamp': 1740977121.5180821}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 346 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977121.5183911}).
Epoch: [7][348/500]	Time  5.885 ( 5.885)	Loss 1.4683 (1.3952)	CeLoss 0.2480 (0.3682)	SegCLSLoss 0.0090 (0.0114)	KLLoss 0.3633 (0.2904)	MaskLoss 0.5896 (0.4961)	MaskBCELoss 0.2211 (0.1103)	MaskDICELoss 0.3685 (0.3858)
Epoch: [7][349/500]	Time  5.782 ( 5.782)	Loss 0.7539 (1.9906)	CeLoss 0.7539 (0.6136)	SegCLSLoss 0.0000 (0.0116)	KLLoss 0.0000 (0.2170)	MaskLoss 0.0000 (0.6746)	MaskBCELoss 0.0000 (0.2386)	MaskDICELoss 0.0000 (0.4360)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 347 is less than current step: 499. Dropping entry: {'train/loss': 1.3951607286930083, 'train/ce_loss': 0.3681640625, 'train/seg_cls_loss': 0.011407470703125, 'train/kl_loss': 0.2904296875, 'train/mask_bce_loss': 0.11027938276529312, 'train/mask_dice_loss': 0.38583612591028216, 'train/mask_loss': 0.49611551463603976, 'metrics/total_secs_per_batch': 5.884796142578125, 'metrics/data_secs_per_batch': 2.630075716972351, '_timestamp': 1740977127.4028199}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 347 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977127.4031146}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 348 is less than current step: 499. Dropping entry: {'train/loss': 1.990644598007202, 'train/ce_loss': 0.61357421875, 'train/seg_cls_loss': 0.01162109375, 'train/kl_loss': 0.2169921875, 'train/mask_bce_loss': 0.2386119093745947, 'train/mask_dice_loss': 0.4360072612762451, 'train/mask_loss': 0.6746191680431366, 'metrics/total_secs_per_batch': 5.782407999038696, 'metrics/data_secs_per_batch': 3.0383596897125242, '_timestamp': 1740977133.1852455}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 348 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977133.1855764}).
[2025-03-02 22:45:39,262] [INFO] [logging.py:96:log_dist] [Rank 0] step=3850, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:45:39,268] [INFO] [timer.py:215:stop] epoch=0/micro_step=38500/global_step=3850, RunningAvgSamplesPerSec=1.4674369133435277, CurrSamplesPerSec=1.6441727805482036, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [7][350/500]	Time  6.084 ( 6.084)	Loss 1.1202 (1.9544)	CeLoss 0.2119 (0.3130)	SegCLSLoss 0.0103 (0.0154)	KLLoss 0.3652 (0.3285)	MaskLoss 0.4332 (0.8005)	MaskBCELoss 0.1088 (0.1467)	MaskDICELoss 0.3243 (0.6538)
Epoch: [7][351/500]	Time  5.248 ( 5.248)	Loss 2.7614 (1.9514)	CeLoss 0.2891 (0.4211)	SegCLSLoss 0.0118 (0.0141)	KLLoss 0.3613 (0.2939)	MaskLoss 1.2147 (0.7468)	MaskBCELoss 0.5546 (0.2056)	MaskDICELoss 0.6601 (0.5412)
Epoch: [7][352/500]	Time  6.787 ( 6.787)	Loss 2.3315 (1.6473)	CeLoss 0.1846 (0.3731)	SegCLSLoss 0.0214 (0.0183)	KLLoss 0.3672 (0.2920)	MaskLoss 1.0496 (0.6177)	MaskBCELoss 0.2797 (0.0754)	MaskDICELoss 0.7698 (0.5424)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 349 is less than current step: 499. Dropping entry: {'train/loss': 1.9543504953384399, 'train/ce_loss': 0.31298828125, 'train/seg_cls_loss': 0.015435791015625, 'train/kl_loss': 0.328515625, 'train/mask_bce_loss': 0.14669369542971253, 'train/mask_dice_loss': 0.6537725627422333, 'train/mask_loss': 0.8004662394523621, 'metrics/total_secs_per_batch': 6.083814859390259, 'metrics/data_secs_per_batch': 2.9441211938858034, '_timestamp': 1740977139.2688107}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 349 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977139.2691228}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 350 is less than current step: 499. Dropping entry: {'train/loss': 1.951439416408539, 'train/ce_loss': 0.42109375, 'train/seg_cls_loss': 0.014117431640625, 'train/kl_loss': 0.2939453125, 'train/mask_bce_loss': 0.20558731481432915, 'train/mask_dice_loss': 0.5412261366844178, 'train/mask_loss': 0.7468134552240372, 'metrics/total_secs_per_batch': 5.248406171798706, 'metrics/data_secs_per_batch': 2.1518835067749023, '_timestamp': 1740977144.5174103}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 350 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977144.5177035}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 351 is less than current step: 499. Dropping entry: {'train/loss': 1.6472831130027772, 'train/ce_loss': 0.37314453125, 'train/seg_cls_loss': 0.018280029296875, 'train/kl_loss': 0.2919921875, 'train/mask_bce_loss': 0.07537148837000132, 'train/mask_dice_loss': 0.5423618793487549, 'train/mask_loss': 0.6177333652973175, 'metrics/total_secs_per_batch': 6.7871668338775635, 'metrics/data_secs_per_batch': 3.3964032888412476, '_timestamp': 1740977151.3048258}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 351 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977151.3051815}).
Epoch: [7][353/500]	Time  6.020 ( 6.020)	Loss 2.7431 (1.8523)	CeLoss 0.2969 (0.4146)	SegCLSLoss 0.0152 (0.0137)	KLLoss 0.3633 (0.2908)	MaskLoss 1.2006 (0.7009)	MaskBCELoss 0.3444 (0.1594)	MaskDICELoss 0.8562 (0.5416)
Epoch: [7][354/500]	Time  6.449 ( 6.449)	Loss 1.6637 (1.2878)	CeLoss 0.1543 (0.2500)	SegCLSLoss 0.0287 (0.0136)	KLLoss 0.3750 (0.2545)	MaskLoss 0.7283 (0.5027)	MaskBCELoss 0.0589 (0.0736)	MaskDICELoss 0.6695 (0.4292)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 352 is less than current step: 499. Dropping entry: {'train/loss': 1.8522616624832153, 'train/ce_loss': 0.4146484375, 'train/seg_cls_loss': 0.01373291015625, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.15935887396335602, 'train/mask_dice_loss': 0.5415766388177872, 'train/mask_loss': 0.7009355187416076, 'metrics/total_secs_per_batch': 6.019937992095947, 'metrics/data_secs_per_batch': 2.7156664371490478, '_timestamp': 1740977157.3245163}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 352 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977157.324803}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 353 is less than current step: 499. Dropping entry: {'train/loss': 1.287759292125702, 'train/ce_loss': 0.249951171875, 'train/seg_cls_loss': 0.01358642578125, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.07358979037962854, 'train/mask_dice_loss': 0.4291521668434143, 'train/mask_loss': 0.502741950750351, 'metrics/total_secs_per_batch': 6.449238538742065, 'metrics/data_secs_per_batch': 2.6530611753463744, '_timestamp': 1740977163.773733}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 353 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977163.774013}).
Epoch: [7][355/500]	Time  6.376 ( 6.376)	Loss 1.7365 (1.3869)	CeLoss 0.1943 (0.4219)	SegCLSLoss 0.0193 (0.0096)	KLLoss 0.3613 (0.2545)	MaskLoss 0.7481 (0.4674)	MaskBCELoss 0.0219 (0.0829)	MaskDICELoss 0.7263 (0.3845)
Epoch: [7][356/500]	Time  5.405 ( 5.405)	Loss 1.7563 (1.3998)	CeLoss 0.2178 (0.4420)	SegCLSLoss 0.0239 (0.0093)	KLLoss 0.3965 (0.2201)	MaskLoss 0.7434 (0.4655)	MaskBCELoss 0.3223 (0.1148)	MaskDICELoss 0.4210 (0.3507)
Epoch: [7][357/500]	Time  5.961 ( 5.961)	Loss 2.1104 (1.4499)	CeLoss 0.2471 (0.3141)	SegCLSLoss 0.0179 (0.0135)	KLLoss 0.3496 (0.2924)	MaskLoss 0.9097 (0.5500)	MaskBCELoss 0.0262 (0.0889)	MaskDICELoss 0.8835 (0.4611)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 354 is less than current step: 499. Dropping entry: {'train/loss': 1.3868651866912842, 'train/ce_loss': 0.421875, 'train/seg_cls_loss': 0.0096435546875, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.08292271606624127, 'train/mask_dice_loss': 0.38448447585105894, 'train/mask_loss': 0.4674071967601776, 'metrics/total_secs_per_batch': 6.3757264614105225, 'metrics/data_secs_per_batch': 2.7824204683303835, '_timestamp': 1740977170.1494818}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 354 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977170.1497684}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 355 is less than current step: 499. Dropping entry: {'train/loss': 1.3997838258743287, 'train/ce_loss': 0.4419921875, 'train/seg_cls_loss': 0.00927734375, 'train/kl_loss': 0.2201171875, 'train/mask_bce_loss': 0.11478578057140112, 'train/mask_dice_loss': 0.35068229734897616, 'train/mask_loss': 0.4654680758714676, 'metrics/total_secs_per_batch': 5.404721021652222, 'metrics/data_secs_per_batch': 2.6542813301086428, '_timestamp': 1740977175.5544872}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 355 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977175.554837}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 356 is less than current step: 499. Dropping entry: {'train/loss': 1.4498829126358033, 'train/ce_loss': 0.3140625, 'train/seg_cls_loss': 0.01346435546875, 'train/kl_loss': 0.2923828125, 'train/mask_bce_loss': 0.08891169149428606, 'train/mask_dice_loss': 0.46107860319316385, 'train/mask_loss': 0.5499903008341789, 'metrics/total_secs_per_batch': 5.961340665817261, 'metrics/data_secs_per_batch': 2.6086376190185545, '_timestamp': 1740977181.5155313}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 356 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977181.5158145}).
Epoch: [7][358/500]	Time  5.286 ( 5.286)	Loss 2.1425 (1.5165)	CeLoss 0.2891 (0.3753)	SegCLSLoss 0.0143 (0.0129)	KLLoss 0.3633 (0.2535)	MaskLoss 0.9043 (0.5544)	MaskBCELoss 0.2908 (0.1275)	MaskDICELoss 0.6135 (0.4270)
Epoch: [7][359/500]	Time  5.470 ( 5.470)	Loss 1.1219 (1.0693)	CeLoss 0.2363 (0.4904)	SegCLSLoss 0.0156 (0.0073)	KLLoss 0.3691 (0.1436)	MaskLoss 0.4203 (0.2804)	MaskBCELoss 0.1107 (0.0387)	MaskDICELoss 0.3097 (0.2418)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 357 is less than current step: 499. Dropping entry: {'train/loss': 1.5164916038513183, 'train/ce_loss': 0.37529296875, 'train/seg_cls_loss': 0.012939453125, 'train/kl_loss': 0.253515625, 'train/mask_bce_loss': 0.12745449915528298, 'train/mask_dice_loss': 0.4269827023148537, 'train/mask_loss': 0.5544372051954269, 'metrics/total_secs_per_batch': 5.285967111587524, 'metrics/data_secs_per_batch': 2.2492644071578978, '_timestamp': 1740977186.8015432}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 357 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977186.801837}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 358 is less than current step: 499. Dropping entry: {'train/loss': 1.0693249821662902, 'train/ce_loss': 0.490380859375, 'train/seg_cls_loss': 0.00731201171875, 'train/kl_loss': 0.1435546875, 'train/mask_bce_loss': 0.03865498956292868, 'train/mask_dice_loss': 0.24178386628627777, 'train/mask_loss': 0.28043885827064513, 'metrics/total_secs_per_batch': 5.47039270401001, 'metrics/data_secs_per_batch': 2.5016760349273683, '_timestamp': 1740977192.271924}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 358 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977192.2723193}).
[2025-03-02 22:46:38,736] [INFO] [logging.py:96:log_dist] [Rank 0] step=3860, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:46:38,741] [INFO] [timer.py:215:stop] epoch=0/micro_step=38600/global_step=3860, RunningAvgSamplesPerSec=1.4679218875211306, CurrSamplesPerSec=1.5459710414242662, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [7][360/500]	Time  6.470 ( 6.470)	Loss 2.2688 (1.5694)	CeLoss 0.2578 (0.3814)	SegCLSLoss 0.0195 (0.0125)	KLLoss 0.3594 (0.2918)	MaskLoss 0.9831 (0.5762)	MaskBCELoss 0.0173 (0.0887)	MaskDICELoss 0.9657 (0.4875)
Epoch: [7][361/500]	Time  5.428 ( 5.428)	Loss 2.2005 (1.7249)	CeLoss 0.2793 (0.4713)	SegCLSLoss 0.0204 (0.0135)	KLLoss 0.3535 (0.2525)	MaskLoss 0.9381 (0.6109)	MaskBCELoss 0.0198 (0.1338)	MaskDICELoss 0.9183 (0.4771)
Epoch: [7][362/500]	Time  5.278 ( 5.278)	Loss 1.5309 (1.2366)	CeLoss 0.2129 (0.5289)	SegCLSLoss 0.0098 (0.0090)	KLLoss 0.3652 (0.2543)	MaskLoss 0.6385 (0.3388)	MaskBCELoss 0.0687 (0.0636)	MaskDICELoss 0.5698 (0.2753)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 359 is less than current step: 499. Dropping entry: {'train/loss': 1.569399243593216, 'train/ce_loss': 0.3814208984375, 'train/seg_cls_loss': 0.01253662109375, 'train/kl_loss': 0.291796875, 'train/mask_bce_loss': 0.0886600274592638, 'train/mask_dice_loss': 0.4875068664550781, 'train/mask_loss': 0.576166895031929, 'metrics/total_secs_per_batch': 6.470184803009033, 'metrics/data_secs_per_batch': 2.903102445602417, '_timestamp': 1740977198.742014}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 359 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977198.7423885}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 360 is less than current step: 499. Dropping entry: {'train/loss': 1.7249109029769898, 'train/ce_loss': 0.4712890625, 'train/seg_cls_loss': 0.01353759765625, 'train/kl_loss': 0.2525390625, 'train/mask_bce_loss': 0.13376945136114954, 'train/mask_dice_loss': 0.47712350487709043, 'train/mask_loss': 0.6108929485082626, 'metrics/total_secs_per_batch': 5.427857398986816, 'metrics/data_secs_per_batch': 2.1763816118240356, '_timestamp': 1740977204.1700056}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 360 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977204.1703033}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 361 is less than current step: 499. Dropping entry: {'train/loss': 1.236627572774887, 'train/ce_loss': 0.52890625, 'train/seg_cls_loss': 0.0090087890625, 'train/kl_loss': 0.254296875, 'train/mask_bce_loss': 0.0635604603216052, 'train/mask_dice_loss': 0.27526113390922546, 'train/mask_loss': 0.3388215996325016, 'metrics/total_secs_per_batch': 5.277741432189941, 'metrics/data_secs_per_batch': 2.125268030166626, '_timestamp': 1740977209.4476824}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 361 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977209.4479616}).
Epoch: [7][363/500]	Time  6.726 ( 6.726)	Loss 1.1156 (1.9335)	CeLoss 0.2676 (0.3143)	SegCLSLoss 0.0104 (0.0176)	KLLoss 0.3594 (0.3248)	MaskLoss 0.4035 (0.7890)	MaskBCELoss 0.0457 (0.1092)	MaskDICELoss 0.3578 (0.6798)
Epoch: [7][364/500]	Time  6.954 ( 6.954)	Loss 2.5663 (1.9026)	CeLoss 0.1777 (0.2836)	SegCLSLoss 0.0221 (0.0130)	KLLoss 0.3750 (0.3248)	MaskLoss 1.1699 (0.7901)	MaskBCELoss 0.2906 (0.1722)	MaskDICELoss 0.8793 (0.6179)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 362 is less than current step: 499. Dropping entry: {'train/loss': 1.933547604084015, 'train/ce_loss': 0.3142578125, 'train/seg_cls_loss': 0.0176025390625, 'train/kl_loss': 0.3248046875, 'train/mask_bce_loss': 0.10923979999497532, 'train/mask_dice_loss': 0.6797507852315903, 'train/mask_loss': 0.7889905869960785, 'metrics/total_secs_per_batch': 6.72578763961792, 'metrics/data_secs_per_batch': 3.2698874950408934, '_timestamp': 1740977216.1735015}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 362 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977216.173854}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 363 is less than current step: 499. Dropping entry: {'train/loss': 1.9026036143302918, 'train/ce_loss': 0.28359375, 'train/seg_cls_loss': 0.0130126953125, 'train/kl_loss': 0.3248046875, 'train/mask_bce_loss': 0.17215739600360394, 'train/mask_dice_loss': 0.6179139241576195, 'train/mask_loss': 0.7900713190436364, 'metrics/total_secs_per_batch': 6.954347610473633, 'metrics/data_secs_per_batch': 3.131620907783508, '_timestamp': 1740977223.128048}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 363 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977223.1283996}).
Epoch: [7][365/500]	Time  6.272 ( 6.272)	Loss 2.2065 (1.7059)	CeLoss 0.1758 (0.3412)	SegCLSLoss 0.0231 (0.0120)	KLLoss 0.3770 (0.2910)	MaskLoss 0.9910 (0.6648)	MaskBCELoss 0.3841 (0.1394)	MaskDICELoss 0.6069 (0.5255)
Epoch: [7][366/500]	Time  5.493 ( 5.493)	Loss 1.5922 (1.5680)	CeLoss 0.2412 (0.3364)	SegCLSLoss 0.0111 (0.0132)	KLLoss 0.3633 (0.3258)	MaskLoss 0.6545 (0.5961)	MaskBCELoss 0.1650 (0.1306)	MaskDICELoss 0.4895 (0.4655)
Epoch: [7][367/500]	Time  5.903 ( 5.903)	Loss 2.2078 (1.6656)	CeLoss 0.2285 (0.4458)	SegCLSLoss 0.0261 (0.0131)	KLLoss 0.3574 (0.2578)	MaskLoss 0.9652 (0.5937)	MaskBCELoss 0.0466 (0.0796)	MaskDICELoss 0.9186 (0.5141)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 364 is less than current step: 499. Dropping entry: {'train/loss': 1.7059123992919922, 'train/ce_loss': 0.341162109375, 'train/seg_cls_loss': 0.01197509765625, 'train/kl_loss': 0.291015625, 'train/mask_bce_loss': 0.13936659172177315, 'train/mask_dice_loss': 0.5254792600870133, 'train/mask_loss': 0.6648458540439606, 'metrics/total_secs_per_batch': 6.272094249725342, 'metrics/data_secs_per_batch': 2.910589504241943, '_timestamp': 1740977229.3999624}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 364 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977229.400164}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 365 is less than current step: 499. Dropping entry: {'train/loss': 1.5680191576480866, 'train/ce_loss': 0.33642578125, 'train/seg_cls_loss': 0.013214111328125, 'train/kl_loss': 0.32578125, 'train/mask_bce_loss': 0.13063986082561313, 'train/mask_dice_loss': 0.4654790945351124, 'train/mask_loss': 0.5961189568042755, 'metrics/total_secs_per_batch': 5.493465423583984, 'metrics/data_secs_per_batch': 2.244431233406067, '_timestamp': 1740977234.8935213}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 365 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977234.8938513}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 366 is less than current step: 499. Dropping entry: {'train/loss': 1.6656134486198426, 'train/ce_loss': 0.44580078125, 'train/seg_cls_loss': 0.01312255859375, 'train/kl_loss': 0.2578125, 'train/mask_bce_loss': 0.07963581820949912, 'train/mask_dice_loss': 0.5141083925962449, 'train/mask_loss': 0.5937442183494568, 'metrics/total_secs_per_batch': 5.903021335601807, 'metrics/data_secs_per_batch': 2.701174235343933, '_timestamp': 1740977240.7966878}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 366 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977240.7971482}).
Epoch: [7][368/500]	Time  5.477 ( 5.477)	Loss 0.9760 (1.2074)	CeLoss 0.2100 (0.4220)	SegCLSLoss 0.0135 (0.0097)	KLLoss 0.3633 (0.1818)	MaskLoss 0.3610 (0.3811)	MaskBCELoss 0.0314 (0.0491)	MaskDICELoss 0.3297 (0.3320)
Epoch: [7][369/500]	Time  5.689 ( 5.689)	Loss 1.0703 (1.5811)	CeLoss 1.0703 (0.5223)	SegCLSLoss 0.0000 (0.0093)	KLLoss 0.0000 (0.2523)	MaskLoss 0.0000 (0.5144)	MaskBCELoss 0.0000 (0.0526)	MaskDICELoss 0.0000 (0.4618)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 367 is less than current step: 499. Dropping entry: {'train/loss': 1.2074024438858033, 'train/ce_loss': 0.4219970703125, 'train/seg_cls_loss': 0.009698486328125, 'train/kl_loss': 0.1818359375, 'train/mask_bce_loss': 0.04912145407870412, 'train/mask_dice_loss': 0.33198456168174745, 'train/mask_loss': 0.38110601007938383, 'metrics/total_secs_per_batch': 5.477334022521973, 'metrics/data_secs_per_batch': 2.2388980627059936, '_timestamp': 1740977246.273795}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 367 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977246.2741792}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 368 is less than current step: 499. Dropping entry: {'train/loss': 1.5810878038406373, 'train/ce_loss': 0.522265625, 'train/seg_cls_loss': 0.0092529296875, 'train/kl_loss': 0.25234375, 'train/mask_bce_loss': 0.05261147916316986, 'train/mask_dice_loss': 0.46176054179668424, 'train/mask_loss': 0.5143720209598541, 'metrics/total_secs_per_batch': 5.6887195110321045, 'metrics/data_secs_per_batch': 2.5848113775253294, '_timestamp': 1740977251.9625201}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 368 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977251.9628081}).
[2025-03-02 22:47:37,626] [INFO] [logging.py:96:log_dist] [Rank 0] step=3870, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:47:37,632] [INFO] [timer.py:215:stop] epoch=0/micro_step=38700/global_step=3870, RunningAvgSamplesPerSec=1.468437148120418, CurrSamplesPerSec=1.7639961837526867, MemAllocated=31.31GB, MaxMemAllocated=37.23GB
Epoch: [7][370/500]	Time  5.671 ( 5.671)	Loss 1.6406 (1.5958)	CeLoss 0.2012 (0.5466)	SegCLSLoss 0.0188 (0.0107)	KLLoss 0.3594 (0.2186)	MaskLoss 0.6968 (0.5111)	MaskBCELoss 0.0224 (0.0609)	MaskDICELoss 0.6743 (0.4501)
Epoch: [7][371/500]	Time  6.439 ( 6.439)	Loss 1.6860 (1.6942)	CeLoss 0.2031 (0.4724)	SegCLSLoss 0.0188 (0.0121)	KLLoss 0.3652 (0.2893)	MaskLoss 0.7180 (0.5934)	MaskBCELoss 0.0565 (0.1175)	MaskDICELoss 0.6615 (0.4758)
Epoch: [7][372/500]	Time  5.532 ( 5.532)	Loss 0.9258 (1.5935)	CeLoss 0.9258 (0.6004)	SegCLSLoss 0.0000 (0.0099)	KLLoss 0.0000 (0.2139)	MaskLoss 0.0000 (0.4833)	MaskBCELoss 0.0000 (0.0835)	MaskDICELoss 0.0000 (0.3999)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 369 is less than current step: 499. Dropping entry: {'train/loss': 1.5958403587341308, 'train/ce_loss': 0.54658203125, 'train/seg_cls_loss': 0.010687255859375, 'train/kl_loss': 0.2185546875, 'train/mask_bce_loss': 0.06094096750020981, 'train/mask_dice_loss': 0.4501139707863331, 'train/mask_loss': 0.5110549338161945, 'metrics/total_secs_per_batch': 5.670561790466309, 'metrics/data_secs_per_batch': 2.4908799648284914, '_timestamp': 1740977257.6328633}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 369 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977257.6331515}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 370 is less than current step: 499. Dropping entry: {'train/loss': 1.6941714406013488, 'train/ce_loss': 0.47236328125, 'train/seg_cls_loss': 0.012060546875, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.11754646971821785, 'train/mask_dice_loss': 0.47582830786705016, 'train/mask_loss': 0.59337477684021, 'metrics/total_secs_per_batch': 6.43928337097168, 'metrics/data_secs_per_batch': 2.7055492401123047, '_timestamp': 1740977264.0725129}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 370 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977264.0728514}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 371 is less than current step: 499. Dropping entry: {'train/loss': 1.59345583319664, 'train/ce_loss': 0.600390625, 'train/seg_cls_loss': 0.0098876953125, 'train/kl_loss': 0.2138671875, 'train/mask_bce_loss': 0.08346096947789192, 'train/mask_dice_loss': 0.39988804757595064, 'train/mask_loss': 0.48334901034832, 'metrics/total_secs_per_batch': 5.532191038131714, 'metrics/data_secs_per_batch': 2.5027833461761473, '_timestamp': 1740977269.6045434}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 371 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977269.6048326}).
Epoch: [7][373/500]	Time  5.919 ( 5.919)	Loss 2.0656 (1.4272)	CeLoss 0.1289 (0.4018)	SegCLSLoss 0.0275 (0.0123)	KLLoss 0.3848 (0.2535)	MaskLoss 0.9420 (0.4970)	MaskBCELoss 0.2407 (0.0875)	MaskDICELoss 0.7013 (0.4096)
Epoch: [7][374/500]	Time  7.201 ( 7.201)	Loss 0.7351 (1.4263)	CeLoss 0.2305 (0.2365)	SegCLSLoss 0.0098 (0.0140)	KLLoss 0.3711 (0.3277)	MaskLoss 0.2308 (0.5748)	MaskBCELoss 0.0421 (0.0849)	MaskDICELoss 0.1888 (0.4899)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 372 is less than current step: 499. Dropping entry: {'train/loss': 1.4272212982177734, 'train/ce_loss': 0.401806640625, 'train/seg_cls_loss': 0.01226806640625, 'train/kl_loss': 0.253515625, 'train/mask_bce_loss': 0.08746745325624943, 'train/mask_dice_loss': 0.4095660477876663, 'train/mask_loss': 0.49703350067138674, 'metrics/total_secs_per_batch': 5.9185471534729, 'metrics/data_secs_per_batch': 2.7060996294021606, '_timestamp': 1740977275.523081}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 372 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977275.5233676}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 373 is less than current step: 499. Dropping entry: {'train/loss': 1.4262633085250855, 'train/ce_loss': 0.2365234375, 'train/seg_cls_loss': 0.013970947265625, 'train/kl_loss': 0.327734375, 'train/mask_bce_loss': 0.08485680324956775, 'train/mask_dice_loss': 0.48989594392478464, 'train/mask_loss': 0.5747527427971363, 'metrics/total_secs_per_batch': 7.201190948486328, 'metrics/data_secs_per_batch': 3.519789719581604, '_timestamp': 1740977282.7242815}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 373 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977282.7245746}).
Epoch: [7][375/500]	Time  4.201 ( 4.201)	Loss 2.2967 (1.6990)	CeLoss 0.1289 (0.5806)	SegCLSLoss 0.0300 (0.0156)	KLLoss 0.3828 (0.2180)	MaskLoss 1.0575 (0.5445)	MaskBCELoss 0.2220 (0.0909)	MaskDICELoss 0.8355 (0.4536)
Epoch: [7][376/500]	Time  5.488 ( 5.488)	Loss 1.8750 (1.3614)	CeLoss 1.8750 (0.6426)	SegCLSLoss 0.0000 (0.0077)	KLLoss 0.0000 (0.1799)	MaskLoss 0.0000 (0.3485)	MaskBCELoss 0.0000 (0.0695)	MaskDICELoss 0.0000 (0.2790)
Epoch: [7][377/500]	Time  5.963 ( 5.963)	Loss 2.7246 (1.8715)	CeLoss 0.1875 (0.3529)	SegCLSLoss 0.0253 (0.0153)	KLLoss 0.3965 (0.3340)	MaskLoss 1.2422 (0.7386)	MaskBCELoss 0.4342 (0.2102)	MaskDICELoss 0.8079 (0.5284)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 374 is less than current step: 499. Dropping entry: {'train/loss': 1.6989733576774597, 'train/ce_loss': 0.580615234375, 'train/seg_cls_loss': 0.015625, 'train/kl_loss': 0.21796875, 'train/mask_bce_loss': 0.09093755185604095, 'train/mask_dice_loss': 0.45356867015361785, 'train/mask_loss': 0.5445062145590782, 'metrics/total_secs_per_batch': 4.200646162033081, 'metrics/data_secs_per_batch': 1.7454201936721803, '_timestamp': 1740977286.9249756}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 374 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977286.9252932}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 375 is less than current step: 499. Dropping entry: {'train/loss': 1.361440110206604, 'train/ce_loss': 0.642578125, 'train/seg_cls_loss': 0.007684326171875, 'train/kl_loss': 0.1798828125, 'train/mask_bce_loss': 0.06949258260428906, 'train/mask_dice_loss': 0.2790497332811356, 'train/mask_loss': 0.348542320728302, 'metrics/total_secs_per_batch': 5.488433599472046, 'metrics/data_secs_per_batch': 2.6073819160461427, '_timestamp': 1740977292.4134874}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 375 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977292.4138086}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 376 is less than current step: 499. Dropping entry: {'train/loss': 1.8714967012405395, 'train/ce_loss': 0.3529296875, 'train/seg_cls_loss': 0.015252685546875, 'train/kl_loss': 0.333984375, 'train/mask_bce_loss': 0.2101763605605811, 'train/mask_dice_loss': 0.5284040123224258, 'train/mask_loss': 0.7385803699493408, 'metrics/total_secs_per_batch': 5.962579727172852, 'metrics/data_secs_per_batch': 2.8040422916412355, '_timestamp': 1740977298.375972}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 376 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977298.3762553}).
Epoch: [7][378/500]	Time  5.947 ( 5.947)	Loss 1.7324 (2.1985)	CeLoss 0.2061 (0.3915)	SegCLSLoss 0.0106 (0.0128)	KLLoss 0.3633 (0.3277)	MaskLoss 0.7427 (0.8839)	MaskBCELoss 0.1483 (0.2899)	MaskDICELoss 0.5944 (0.5940)
Epoch: [7][379/500]	Time  7.162 ( 7.162)	Loss 1.6086 (1.6788)	CeLoss 0.2578 (0.2222)	SegCLSLoss 0.0255 (0.0175)	KLLoss 0.3633 (0.3238)	MaskLoss 0.6500 (0.7074)	MaskBCELoss 0.2266 (0.0821)	MaskDICELoss 0.4234 (0.6254)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 377 is less than current step: 499. Dropping entry: {'train/loss': 2.1985217332839966, 'train/ce_loss': 0.39150390625, 'train/seg_cls_loss': 0.012774658203125, 'train/kl_loss': 0.327734375, 'train/mask_bce_loss': 0.2899284388870001, 'train/mask_dice_loss': 0.5940004199743271, 'train/mask_loss': 0.8839288353919983, 'metrics/total_secs_per_batch': 5.946787595748901, 'metrics/data_secs_per_batch': 2.568883752822876, '_timestamp': 1740977304.3227203}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 377 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977304.322913}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 378 is less than current step: 499. Dropping entry: {'train/loss': 1.678831547498703, 'train/ce_loss': 0.22216796875, 'train/seg_cls_loss': 0.017529296875, 'train/kl_loss': 0.323828125, 'train/mask_bce_loss': 0.08207781389355659, 'train/mask_dice_loss': 0.6253555610775947, 'train/mask_loss': 0.7074333667755127, 'metrics/total_secs_per_batch': 7.1618146896362305, 'metrics/data_secs_per_batch': 3.130272960662842, '_timestamp': 1740977311.4845524}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 378 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977311.4848328}).
[2025-03-02 22:48:36,904] [INFO] [logging.py:96:log_dist] [Rank 0] step=3880, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:48:36,910] [INFO] [timer.py:215:stop] epoch=0/micro_step=38800/global_step=3880, RunningAvgSamplesPerSec=1.4689285149700058, CurrSamplesPerSec=1.843349259085759, MemAllocated=30.68GB, MaxMemAllocated=37.23GB
Epoch: [7][380/500]	Time  5.426 ( 5.426)	Loss 1.3359 (1.6928)	CeLoss 1.3359 (0.4500)	SegCLSLoss 0.0000 (0.0139)	KLLoss 0.0000 (0.2541)	MaskLoss 0.0000 (0.6052)	MaskBCELoss 0.0000 (0.0926)	MaskDICELoss 0.0000 (0.5127)
Epoch: [7][381/500]	Time  6.159 ( 6.159)	Loss 1.0583 (1.6107)	CeLoss 0.1885 (0.4561)	SegCLSLoss 0.0164 (0.0115)	KLLoss 0.3633 (0.2531)	MaskLoss 0.4129 (0.5617)	MaskBCELoss 0.0249 (0.1174)	MaskDICELoss 0.3880 (0.4443)
Epoch: [7][382/500]	Time  6.297 ( 6.297)	Loss 1.8427 (1.4991)	CeLoss 0.3594 (0.3275)	SegCLSLoss 0.0092 (0.0127)	KLLoss 0.3574 (0.3244)	MaskLoss 0.7221 (0.5665)	MaskBCELoss 0.0698 (0.0635)	MaskDICELoss 0.6523 (0.5029)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 379 is less than current step: 499. Dropping entry: {'train/loss': 1.6928207397460937, 'train/ce_loss': 0.45, 'train/seg_cls_loss': 0.013861083984375, 'train/kl_loss': 0.2541015625, 'train/mask_bce_loss': 0.09258984485641122, 'train/mask_dice_loss': 0.5126584053039551, 'train/mask_loss': 0.6052482485771179, 'metrics/total_secs_per_batch': 5.426453590393066, 'metrics/data_secs_per_batch': 2.5427082777023315, '_timestamp': 1740977316.910821}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 379 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977316.9110904}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 380 is less than current step: 499. Dropping entry: {'train/loss': 1.6107038855552673, 'train/ce_loss': 0.4560546875, 'train/seg_cls_loss': 0.011505126953125, 'train/kl_loss': 0.253125, 'train/mask_bce_loss': 0.11738624703139067, 'train/mask_dice_loss': 0.4443133473396301, 'train/mask_loss': 0.5616995990276337, 'metrics/total_secs_per_batch': 6.158801078796387, 'metrics/data_secs_per_batch': 2.807202935218811, '_timestamp': 1740977323.0698578}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 380 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977323.0701685}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 381 is less than current step: 499. Dropping entry: {'train/loss': 1.4991223216056824, 'train/ce_loss': 0.3275390625, 'train/seg_cls_loss': 0.01268310546875, 'train/kl_loss': 0.3244140625, 'train/mask_bce_loss': 0.06354583147913218, 'train/mask_dice_loss': 0.5029098451137543, 'train/mask_loss': 0.5664556801319123, 'metrics/total_secs_per_batch': 6.2967143058776855, 'metrics/data_secs_per_batch': 2.6728324413299562, '_timestamp': 1740977329.366744}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 381 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977329.3670769}).
Epoch: [7][383/500]	Time  6.323 ( 6.323)	Loss 0.0859 (1.8121)	CeLoss 0.0859 (0.2198)	SegCLSLoss 0.0000 (0.0149)	KLLoss 0.0000 (0.3234)	MaskLoss 0.0000 (0.7764)	MaskBCELoss 0.0000 (0.1642)	MaskDICELoss 0.0000 (0.6121)
Epoch: [7][384/500]	Time  4.826 ( 4.826)	Loss 1.6826 (1.2630)	CeLoss 0.1816 (0.4989)	SegCLSLoss 0.0242 (0.0118)	KLLoss 0.3711 (0.2211)	MaskLoss 0.7261 (0.3681)	MaskBCELoss 0.1378 (0.1067)	MaskDICELoss 0.5883 (0.2614)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 382 is less than current step: 499. Dropping entry: {'train/loss': 1.8121052980422974, 'train/ce_loss': 0.21982421875, 'train/seg_cls_loss': 0.014892578125, 'train/kl_loss': 0.3234375, 'train/mask_bce_loss': 0.16421788660809397, 'train/mask_dice_loss': 0.6121472746133805, 'train/mask_loss': 0.7763651482760906, 'metrics/total_secs_per_batch': 6.322790622711182, 'metrics/data_secs_per_batch': 2.7557771682739256, '_timestamp': 1740977335.6893716}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 382 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977335.6896698}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 383 is less than current step: 499. Dropping entry: {'train/loss': 1.2629513502120973, 'train/ce_loss': 0.498876953125, 'train/seg_cls_loss': 0.0117919921875, 'train/kl_loss': 0.22109375, 'train/mask_bce_loss': 0.10669785067439079, 'train/mask_dice_loss': 0.26137449964880943, 'train/mask_loss': 0.3680723547935486, 'metrics/total_secs_per_batch': 4.825892925262451, 'metrics/data_secs_per_batch': 1.8832333564758301, '_timestamp': 1740977340.5154314}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 383 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977340.5157871}).
Epoch: [7][385/500]	Time  7.289 ( 7.289)	Loss 1.0572 (1.6599)	CeLoss 0.2471 (0.2180)	SegCLSLoss 0.0114 (0.0137)	KLLoss 0.3633 (0.2893)	MaskLoss 0.3841 (0.7031)	MaskBCELoss 0.1101 (0.1358)	MaskDICELoss 0.2740 (0.5672)
Epoch: [7][386/500]	Time  5.510 ( 5.510)	Loss 2.6671 (1.6464)	CeLoss 0.2090 (0.4277)	SegCLSLoss 0.0184 (0.0100)	KLLoss 0.3633 (0.2559)	MaskLoss 1.2066 (0.5941)	MaskBCELoss 0.2581 (0.0748)	MaskDICELoss 0.9485 (0.5193)
Epoch: [7][387/500]	Time  6.031 ( 6.031)	Loss 1.1406 (1.7201)	CeLoss 1.1406 (0.3854)	SegCLSLoss 0.0000 (0.0124)	KLLoss 0.0000 (0.2561)	MaskLoss 0.0000 (0.6514)	MaskBCELoss 0.0000 (0.0954)	MaskDICELoss 0.0000 (0.5560)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 384 is less than current step: 499. Dropping entry: {'train/loss': 1.6598894953727723, 'train/ce_loss': 0.218017578125, 'train/seg_cls_loss': 0.013690185546875, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.1358279554406181, 'train/mask_dice_loss': 0.5672368943691254, 'train/mask_loss': 0.7030648529529572, 'metrics/total_secs_per_batch': 7.288651943206787, 'metrics/data_secs_per_batch': 3.6588713407516478, '_timestamp': 1740977347.8039076}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 384 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977347.8042004}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 385 is less than current step: 499. Dropping entry: {'train/loss': 1.6464357018470763, 'train/ce_loss': 0.42769775390625, 'train/seg_cls_loss': 0.009967041015625, 'train/kl_loss': 0.255859375, 'train/mask_bce_loss': 0.07478530965745449, 'train/mask_dice_loss': 0.5193004608154297, 'train/mask_loss': 0.5940857768058777, 'metrics/total_secs_per_batch': 5.51041841506958, 'metrics/data_secs_per_batch': 2.40131413936615, '_timestamp': 1740977353.3143728}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 385 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977353.3146758}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 386 is less than current step: 499. Dropping entry: {'train/loss': 1.7201404809951781, 'train/ce_loss': 0.3853515625, 'train/seg_cls_loss': 0.012432861328125, 'train/kl_loss': 0.2560546875, 'train/mask_bce_loss': 0.0954451959580183, 'train/mask_dice_loss': 0.5559824585914612, 'train/mask_loss': 0.6514276623725891, 'metrics/total_secs_per_batch': 6.031112432479858, 'metrics/data_secs_per_batch': 2.7406566619873045, '_timestamp': 1740977359.3456483}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 386 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977359.3460057}).
Epoch: [7][388/500]	Time  6.135 ( 6.135)	Loss 2.8945 (2.2279)	CeLoss 0.8242 (0.2948)	SegCLSLoss 0.0123 (0.0177)	KLLoss 0.3613 (0.3689)	MaskLoss 1.0136 (0.9436)	MaskBCELoss 0.0138 (0.2920)	MaskDICELoss 0.9998 (0.6516)
Epoch: [7][389/500]	Time  5.098 ( 5.098)	Loss 0.9570 (1.4495)	CeLoss 0.9570 (0.4788)	SegCLSLoss 0.0000 (0.0106)	KLLoss 0.0000 (0.2555)	MaskLoss 0.0000 (0.4699)	MaskBCELoss 0.0000 (0.0979)	MaskDICELoss 0.0000 (0.3719)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 387 is less than current step: 499. Dropping entry: {'train/loss': 2.227861225605011, 'train/ce_loss': 0.29482421875, 'train/seg_cls_loss': 0.0177001953125, 'train/kl_loss': 0.3689453125, 'train/mask_bce_loss': 0.29201859440654515, 'train/mask_dice_loss': 0.6515995174646377, 'train/mask_loss': 0.9436181068420411, 'metrics/total_secs_per_batch': 6.134945631027222, 'metrics/data_secs_per_batch': 2.82227303981781, '_timestamp': 1740977365.4803824}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 387 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977365.4806578}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 388 is less than current step: 499. Dropping entry: {'train/loss': 1.449502170085907, 'train/ce_loss': 0.47880859375, 'train/seg_cls_loss': 0.010638427734375, 'train/kl_loss': 0.25546875, 'train/mask_bce_loss': 0.09793830625712871, 'train/mask_dice_loss': 0.3719299703836441, 'train/mask_loss': 0.4698682755231857, 'metrics/total_secs_per_batch': 5.098246335983276, 'metrics/data_secs_per_batch': 2.4443100452423097, '_timestamp': 1740977370.5785902}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 388 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977370.5788589}).
[2025-03-02 22:49:36,904] [INFO] [logging.py:96:log_dist] [Rank 0] step=3890, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:49:36,909] [INFO] [timer.py:215:stop] epoch=0/micro_step=38900/global_step=3890, RunningAvgSamplesPerSec=1.4693776616349346, CurrSamplesPerSec=1.5797632605939103, MemAllocated=31.65GB, MaxMemAllocated=37.23GB
Epoch: [7][390/500]	Time  6.332 ( 6.332)	Loss 0.4043 (1.2744)	CeLoss 0.4043 (0.3561)	SegCLSLoss 0.0000 (0.0098)	KLLoss 0.0000 (0.2539)	MaskLoss 0.0000 (0.4440)	MaskBCELoss 0.0000 (0.0488)	MaskDICELoss 0.0000 (0.3952)
Epoch: [7][391/500]	Time  5.969 ( 5.969)	Loss 1.1177 (1.7643)	CeLoss 0.2852 (0.3890)	SegCLSLoss 0.0129 (0.0140)	KLLoss 0.3594 (0.2926)	MaskLoss 0.3957 (0.6695)	MaskBCELoss 0.0503 (0.1173)	MaskDICELoss 0.3455 (0.5522)
Epoch: [7][392/500]	Time  6.101 ( 6.101)	Loss 1.2319 (1.5634)	CeLoss 0.2490 (0.4315)	SegCLSLoss 0.0182 (0.0130)	KLLoss 0.3574 (0.2896)	MaskLoss 0.4695 (0.5481)	MaskBCELoss 0.0101 (0.0667)	MaskDICELoss 0.4594 (0.4814)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 389 is less than current step: 499. Dropping entry: {'train/loss': 1.2744439482688903, 'train/ce_loss': 0.3560546875, 'train/seg_cls_loss': 0.009765625, 'train/kl_loss': 0.25390625, 'train/mask_bce_loss': 0.048802854400128125, 'train/mask_dice_loss': 0.39515740126371385, 'train/mask_loss': 0.44396025240421294, 'metrics/total_secs_per_batch': 6.331589221954346, 'metrics/data_secs_per_batch': 3.109218144416809, '_timestamp': 1740977376.9100175}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 389 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977376.9102917}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 390 is less than current step: 499. Dropping entry: {'train/loss': 1.7643299102783203, 'train/ce_loss': 0.38896484375, 'train/seg_cls_loss': 0.01396484375, 'train/kl_loss': 0.292578125, 'train/mask_bce_loss': 0.11726795393042266, 'train/mask_dice_loss': 0.5522016912698746, 'train/mask_loss': 0.6694696396589279, 'metrics/total_secs_per_batch': 5.968768119812012, 'metrics/data_secs_per_batch': 2.7505778551101683, '_timestamp': 1740977382.8791757}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 390 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977382.8794088}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 391 is less than current step: 499. Dropping entry: {'train/loss': 1.5633771538734436, 'train/ce_loss': 0.43154296875, 'train/seg_cls_loss': 0.013043212890625, 'train/kl_loss': 0.2896484375, 'train/mask_bce_loss': 0.06665007723495364, 'train/mask_dice_loss': 0.4814447432756424, 'train/mask_loss': 0.5480948209762573, 'metrics/total_secs_per_batch': 6.1008405685424805, 'metrics/data_secs_per_batch': 2.838431167602539, '_timestamp': 1740977388.9798727}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 391 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977388.980211}).
Epoch: [7][393/500]	Time  6.265 ( 6.265)	Loss 1.0683 (1.4840)	CeLoss 0.2051 (0.4325)	SegCLSLoss 0.0162 (0.0113)	KLLoss 0.3633 (0.2941)	MaskLoss 0.4096 (0.5082)	MaskBCELoss 0.0315 (0.0605)	MaskDICELoss 0.3781 (0.4477)
Epoch: [7][394/500]	Time  5.909 ( 5.909)	Loss 1.3125 (1.4920)	CeLoss 1.3125 (0.4976)	SegCLSLoss 0.0000 (0.0102)	KLLoss 0.0000 (0.2531)	MaskLoss 0.0000 (0.4822)	MaskBCELoss 0.0000 (0.0742)	MaskDICELoss 0.0000 (0.4080)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 392 is less than current step: 499. Dropping entry: {'train/loss': 1.4840276956558227, 'train/ce_loss': 0.43251953125, 'train/seg_cls_loss': 0.011346435546875, 'train/kl_loss': 0.294140625, 'train/mask_bce_loss': 0.060521926172077654, 'train/mask_dice_loss': 0.447702856361866, 'train/mask_loss': 0.5082247883081437, 'metrics/total_secs_per_batch': 6.265277862548828, 'metrics/data_secs_per_batch': 2.5021640062332153, '_timestamp': 1740977395.2451217}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 392 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977395.2454016}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 393 is less than current step: 499. Dropping entry: {'train/loss': 1.4920204997062683, 'train/ce_loss': 0.49755859375, 'train/seg_cls_loss': 0.010162353515625, 'train/kl_loss': 0.253125, 'train/mask_bce_loss': 0.07418662495911121, 'train/mask_dice_loss': 0.4080052673816681, 'train/mask_loss': 0.48219189047813416, 'metrics/total_secs_per_batch': 5.90864634513855, 'metrics/data_secs_per_batch': 2.9521597385406495, '_timestamp': 1740977401.154064}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 393 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977401.1544309}).
Epoch: [7][395/500]	Time  6.503 ( 6.503)	Loss 2.5860 (1.9298)	CeLoss 0.1885 (0.2910)	SegCLSLoss 0.0139 (0.0163)	KLLoss 0.3652 (0.2877)	MaskLoss 1.1768 (0.8011)	MaskBCELoss 0.5067 (0.1724)	MaskDICELoss 0.6701 (0.6287)
Epoch: [7][396/500]	Time  5.135 ( 5.135)	Loss 2.2542 (1.3523)	CeLoss 0.2236 (0.4660)	SegCLSLoss 0.0215 (0.0101)	KLLoss 0.3555 (0.2191)	MaskLoss 0.9923 (0.4298)	MaskBCELoss 0.0122 (0.1031)	MaskDICELoss 0.9801 (0.3266)
Epoch: [7][397/500]	Time  6.390 ( 6.390)	Loss 1.1840 (1.8739)	CeLoss 0.1934 (0.1974)	SegCLSLoss 0.0195 (0.0182)	KLLoss 0.3594 (0.3299)	MaskLoss 0.4724 (0.8173)	MaskBCELoss 0.0847 (0.1864)	MaskDICELoss 0.3877 (0.6309)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 394 is less than current step: 499. Dropping entry: {'train/loss': 1.929839563369751, 'train/ce_loss': 0.290966796875, 'train/seg_cls_loss': 0.0162841796875, 'train/kl_loss': 0.2876953125, 'train/mask_bce_loss': 0.1723938936367631, 'train/mask_dice_loss': 0.6286831140518189, 'train/mask_loss': 0.8010770082473755, 'metrics/total_secs_per_batch': 6.502575635910034, 'metrics/data_secs_per_batch': 3.005044937133789, '_timestamp': 1740977407.6563349}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 394 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977407.6566212}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 395 is less than current step: 499. Dropping entry: {'train/loss': 1.3523260712623597, 'train/ce_loss': 0.466015625, 'train/seg_cls_loss': 0.010076904296875, 'train/kl_loss': 0.219140625, 'train/mask_bce_loss': 0.10314924959093333, 'train/mask_dice_loss': 0.3266270518302917, 'train/mask_loss': 0.4297763049602509, 'metrics/total_secs_per_batch': 5.13480544090271, 'metrics/data_secs_per_batch': 2.2984952211380003, '_timestamp': 1740977412.7913566}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 395 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977412.7917757}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 396 is less than current step: 499. Dropping entry: {'train/loss': 1.873874843120575, 'train/ce_loss': 0.19736328125, 'train/seg_cls_loss': 0.01815185546875, 'train/kl_loss': 0.3298828125, 'train/mask_bce_loss': 0.18637617193162442, 'train/mask_dice_loss': 0.6309323400259018, 'train/mask_loss': 0.817308509349823, 'metrics/total_secs_per_batch': 6.389958381652832, 'metrics/data_secs_per_batch': 2.822726678848267, '_timestamp': 1740977419.1810892}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 396 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977419.1813662}).
Epoch: [7][398/500]	Time  5.858 ( 5.858)	Loss 2.7376 (1.7725)	CeLoss 0.1641 (0.3603)	SegCLSLoss 0.0242 (0.0158)	KLLoss 0.3633 (0.2938)	MaskLoss 1.2629 (0.6876)	MaskBCELoss 0.3482 (0.1552)	MaskDICELoss 0.9147 (0.5324)
Epoch: [7][399/500]	Time  5.577 ( 5.577)	Loss 1.4072 (1.4408)	CeLoss 0.2422 (0.4352)	SegCLSLoss 0.0187 (0.0122)	KLLoss 0.3555 (0.2926)	MaskLoss 0.5600 (0.4850)	MaskBCELoss 0.1039 (0.1086)	MaskDICELoss 0.4562 (0.3764)
[2025-03-02 22:50:36,404] [INFO] [logging.py:96:log_dist] [Rank 0] step=3900, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:50:36,410] [INFO] [timer.py:215:stop] epoch=0/micro_step=39000/global_step=3900, RunningAvgSamplesPerSec=1.4698524403294113, CurrSamplesPerSec=1.7261315105761927, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [7][400/500]	Time  5.795 ( 5.795)	Loss 1.9862 (1.6204)	CeLoss 0.2178 (0.4352)	SegCLSLoss 0.0190 (0.0139)	KLLoss 0.3535 (0.2545)	MaskLoss 0.8613 (0.5764)	MaskBCELoss 0.0997 (0.0762)	MaskDICELoss 0.7615 (0.5002)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 397 is less than current step: 499. Dropping entry: {'train/loss': 1.7724759697914123, 'train/ce_loss': 0.360302734375, 'train/seg_cls_loss': 0.01575927734375, 'train/kl_loss': 0.29375, 'train/mask_bce_loss': 0.15520917065441608, 'train/mask_dice_loss': 0.5323959946632385, 'train/mask_loss': 0.6876051694154739, 'metrics/total_secs_per_batch': 5.857560396194458, 'metrics/data_secs_per_batch': 2.6703343629837035, '_timestamp': 1740977425.038664}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 397 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977425.038858}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 398 is less than current step: 499. Dropping entry: {'train/loss': 1.4407613694667816, 'train/ce_loss': 0.43515625, 'train/seg_cls_loss': 0.012176513671875, 'train/kl_loss': 0.292578125, 'train/mask_bce_loss': 0.10860639549791813, 'train/mask_dice_loss': 0.37642272040247915, 'train/mask_loss': 0.48502911627292633, 'metrics/total_secs_per_batch': 5.576783657073975, 'metrics/data_secs_per_batch': 2.440145969390869, '_timestamp': 1740977430.6156857}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 398 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977430.6160479}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 399 is less than current step: 499. Dropping entry: {'train/loss': 1.6203798234462738, 'train/ce_loss': 0.43515625, 'train/seg_cls_loss': 0.013873291015625, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.07618936719372868, 'train/mask_dice_loss': 0.5002114862203598, 'train/mask_loss': 0.5764008551836014, 'metrics/total_secs_per_batch': 5.795231819152832, 'metrics/data_secs_per_batch': 2.438743162155151, '_timestamp': 1740977436.410505}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 399 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977436.4107869}).
Epoch: [7][401/500]	Time  5.690 ( 5.690)	Loss 1.2893 (1.1548)	CeLoss 0.1875 (0.5302)	SegCLSLoss 0.0120 (0.0062)	KLLoss 0.3633 (0.1809)	MaskLoss 0.5299 (0.3017)	MaskBCELoss 0.1398 (0.0880)	MaskDICELoss 0.3901 (0.2138)
Epoch: [7][402/500]	Time  5.666 ( 5.666)	Loss 0.7539 (1.7265)	CeLoss 0.7539 (0.5056)	SegCLSLoss 0.0000 (0.0123)	KLLoss 0.0000 (0.2557)	MaskLoss 0.0000 (0.5945)	MaskBCELoss 0.0000 (0.1082)	MaskDICELoss 0.0000 (0.4863)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 400 is less than current step: 499. Dropping entry: {'train/loss': 1.1548248887062074, 'train/ce_loss': 0.53017578125, 'train/seg_cls_loss': 0.006158447265625, 'train/kl_loss': 0.180859375, 'train/mask_bce_loss': 0.08795089907944202, 'train/mask_dice_loss': 0.21377794891595842, 'train/mask_loss': 0.3017288476228714, 'metrics/total_secs_per_batch': 5.690026521682739, 'metrics/data_secs_per_batch': 2.486876034736633, '_timestamp': 1740977442.100745}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 400 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977442.101117}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 401 is less than current step: 499. Dropping entry: {'train/loss': 1.7264605164527893, 'train/ce_loss': 0.50556640625, 'train/seg_cls_loss': 0.012274169921875, 'train/kl_loss': 0.2556640625, 'train/mask_bce_loss': 0.10815377263352274, 'train/mask_dice_loss': 0.48632648289203645, 'train/mask_loss': 0.5944802522659302, 'metrics/total_secs_per_batch': 5.666193723678589, 'metrics/data_secs_per_batch': 2.627200889587402, '_timestamp': 1740977447.7671041}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 401 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977447.7674558}).
Epoch: [7][403/500]	Time  5.683 ( 5.683)	Loss 2.0689 (1.5558)	CeLoss 0.2236 (0.3896)	SegCLSLoss 0.0162 (0.0119)	KLLoss 0.3574 (0.2883)	MaskLoss 0.9006 (0.5657)	MaskBCELoss 0.1782 (0.1350)	MaskDICELoss 0.7225 (0.4307)
Epoch: [7][404/500]	Time  5.756 ( 5.756)	Loss 2.6870 (1.7175)	CeLoss 0.2910 (0.4055)	SegCLSLoss 0.0156 (0.0143)	KLLoss 0.3711 (0.2557)	MaskLoss 1.1755 (0.6397)	MaskBCELoss 0.4126 (0.1412)	MaskDICELoss 0.7629 (0.4985)
Epoch: [7][405/500]	Time  6.084 ( 6.084)	Loss 2.3362 (1.9426)	CeLoss 0.1934 (0.3586)	SegCLSLoss 0.0219 (0.0154)	KLLoss 0.3711 (0.3260)	MaskLoss 1.0475 (0.7717)	MaskBCELoss 0.3356 (0.2063)	MaskDICELoss 0.7119 (0.5654)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 402 is less than current step: 499. Dropping entry: {'train/loss': 1.5557635366916656, 'train/ce_loss': 0.38955078125, 'train/seg_cls_loss': 0.011895751953125, 'train/kl_loss': 0.28828125, 'train/mask_bce_loss': 0.13500721950549632, 'train/mask_dice_loss': 0.43066752180457113, 'train/mask_loss': 0.5656747400760651, 'metrics/total_secs_per_batch': 5.683444023132324, 'metrics/data_secs_per_batch': 2.460318422317505, '_timestamp': 1740977453.4503665}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 402 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977453.4506593}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 403 is less than current step: 499. Dropping entry: {'train/loss': 1.7175095558166504, 'train/ce_loss': 0.40546875, 'train/seg_cls_loss': 0.0142822265625, 'train/kl_loss': 0.2556640625, 'train/mask_bce_loss': 0.14123233463615179, 'train/mask_dice_loss': 0.4984550416469574, 'train/mask_loss': 0.6396873831748963, 'metrics/total_secs_per_batch': 5.755804777145386, 'metrics/data_secs_per_batch': 2.5270381450653074, '_timestamp': 1740977459.20642}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 403 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977459.2067854}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 404 is less than current step: 499. Dropping entry: {'train/loss': 1.9425999522209167, 'train/ce_loss': 0.35859375, 'train/seg_cls_loss': 0.015380859375, 'train/kl_loss': 0.3259765625, 'train/mask_bce_loss': 0.2063343171030283, 'train/mask_dice_loss': 0.5654051065444946, 'train/mask_loss': 0.7717394292354584, 'metrics/total_secs_per_batch': 6.083683013916016, 'metrics/data_secs_per_batch': 2.5448567628860475, '_timestamp': 1740977465.289928}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 404 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977465.2902188}).
Epoch: [7][406/500]	Time  6.333 ( 6.333)	Loss 1.8562 (1.6453)	CeLoss 0.2422 (0.2986)	SegCLSLoss 0.0122 (0.0148)	KLLoss 0.3613 (0.3279)	MaskLoss 0.7855 (0.6532)	MaskBCELoss 0.1138 (0.0896)	MaskDICELoss 0.6717 (0.5636)
Epoch: [7][407/500]	Time  6.751 ( 6.751)	Loss 1.1719 (0.9925)	CeLoss 1.1719 (0.3750)	SegCLSLoss 0.0000 (0.0064)	KLLoss 0.0000 (0.1828)	MaskLoss 0.0000 (0.2979)	MaskBCELoss 0.0000 (0.0498)	MaskDICELoss 0.0000 (0.2481)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 405 is less than current step: 499. Dropping entry: {'train/loss': 1.645272421836853, 'train/ce_loss': 0.2986328125, 'train/seg_cls_loss': 0.014788818359375, 'train/kl_loss': 0.3279296875, 'train/mask_bce_loss': 0.08955799918621779, 'train/mask_dice_loss': 0.5635957822203637, 'train/mask_loss': 0.6531537741422653, 'metrics/total_secs_per_batch': 6.332508087158203, 'metrics/data_secs_per_batch': 2.5504764795303343, '_timestamp': 1740977471.62246}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 405 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977471.6227593}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 406 is less than current step: 499. Dropping entry: {'train/loss': 0.9925076007843018, 'train/ce_loss': 0.375, 'train/seg_cls_loss': 0.0064208984375, 'train/kl_loss': 0.1828125, 'train/mask_bce_loss': 0.049779553711414334, 'train/mask_dice_loss': 0.24813440665602685, 'train/mask_loss': 0.2979139626026154, 'metrics/total_secs_per_batch': 6.750518321990967, 'metrics/data_secs_per_batch': 3.3244660615921022, '_timestamp': 1740977478.373106}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 406 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977478.3734717}).
Epoch: [7][408/500]	Time  6.068 ( 6.068)	Loss 1.6608 (1.5223)	CeLoss 0.1924 (0.3797)	SegCLSLoss 0.0222 (0.0119)	KLLoss 0.3613 (0.2521)	MaskLoss 0.7108 (0.5558)	MaskBCELoss 0.0161 (0.1053)	MaskDICELoss 0.6947 (0.4505)
Epoch: [7][409/500]	Time  5.298 ( 5.298)	Loss 2.3079 (1.3930)	CeLoss 0.2363 (0.3998)	SegCLSLoss 0.0130 (0.0111)	KLLoss 0.3633 (0.2191)	MaskLoss 1.0143 (0.4828)	MaskBCELoss 0.0146 (0.0791)	MaskDICELoss 0.9997 (0.4037)
[2025-03-02 22:51:35,312] [INFO] [logging.py:96:log_dist] [Rank 0] step=3910, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:51:35,318] [INFO] [timer.py:215:stop] epoch=0/micro_step=39100/global_step=3910, RunningAvgSamplesPerSec=1.4703578516273192, CurrSamplesPerSec=1.792915411132642, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [7][410/500]	Time  5.579 ( 5.579)	Loss 1.8230 (1.7301)	CeLoss 0.2314 (0.4373)	SegCLSLoss 0.0122 (0.0127)	KLLoss 0.3672 (0.2559)	MaskLoss 0.7748 (0.6305)	MaskBCELoss 0.0802 (0.1367)	MaskDICELoss 0.6946 (0.4938)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 407 is less than current step: 499. Dropping entry: {'train/loss': 1.5222512006759643, 'train/ce_loss': 0.3796875, 'train/seg_cls_loss': 0.011932373046875, 'train/kl_loss': 0.2521484375, 'train/mask_bce_loss': 0.1052778597921133, 'train/mask_dice_loss': 0.4504766404628754, 'train/mask_loss': 0.5557545065879822, 'metrics/total_secs_per_batch': 6.068410158157349, 'metrics/data_secs_per_batch': 2.4867299079895018, '_timestamp': 1740977484.4413412}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 407 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977484.441636}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 408 is less than current step: 499. Dropping entry: {'train/loss': 1.3929987668991088, 'train/ce_loss': 0.399755859375, 'train/seg_cls_loss': 0.01112060546875, 'train/kl_loss': 0.219140625, 'train/mask_bce_loss': 0.07908958038315177, 'train/mask_dice_loss': 0.40371350944042206, 'train/mask_loss': 0.48280308842659, 'metrics/total_secs_per_batch': 5.298463582992554, 'metrics/data_secs_per_batch': 2.410059380531311, '_timestamp': 1740977489.7400508}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 408 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977489.7404199}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 409 is less than current step: 499. Dropping entry: {'train/loss': 1.73005553483963, 'train/ce_loss': 0.4373046875, 'train/seg_cls_loss': 0.01270751953125, 'train/kl_loss': 0.255859375, 'train/mask_bce_loss': 0.13670506961643697, 'train/mask_dice_loss': 0.4938011974096298, 'train/mask_loss': 0.6305062741041183, 'metrics/total_secs_per_batch': 5.579446077346802, 'metrics/data_secs_per_batch': 2.53111617565155, '_timestamp': 1740977495.3190289}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 409 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977495.319308}).
Epoch: [7][411/500]	Time  5.120 ( 5.120)	Loss 0.7773 (1.0965)	CeLoss 0.7773 (0.5127)	SegCLSLoss 0.0000 (0.0061)	KLLoss 0.0000 (0.1457)	MaskLoss 0.0000 (0.2830)	MaskBCELoss 0.0000 (0.0491)	MaskDICELoss 0.0000 (0.2339)
Epoch: [7][412/500]	Time  6.032 ( 6.032)	Loss 1.9600 (1.3026)	CeLoss 0.1709 (0.3262)	SegCLSLoss 0.0347 (0.0127)	KLLoss 0.3574 (0.2537)	MaskLoss 0.8677 (0.4722)	MaskBCELoss 0.2716 (0.0940)	MaskDICELoss 0.5961 (0.3782)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 410 is less than current step: 499. Dropping entry: {'train/loss': 1.0964627027511598, 'train/ce_loss': 0.5126953125, 'train/seg_cls_loss': 0.006097412109375, 'train/kl_loss': 0.145703125, 'train/mask_bce_loss': 0.04912927635014057, 'train/mask_dice_loss': 0.23386770486831665, 'train/mask_loss': 0.28299697637557986, 'metrics/total_secs_per_batch': 5.1200878620147705, 'metrics/data_secs_per_batch': 2.190813422203064, '_timestamp': 1740977500.4392922}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 410 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977500.4394875}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 411 is less than current step: 499. Dropping entry: {'train/loss': 1.302576744556427, 'train/ce_loss': 0.326220703125, 'train/seg_cls_loss': 0.012744140625, 'train/kl_loss': 0.2537109375, 'train/mask_bce_loss': 0.094046188890934, 'train/mask_dice_loss': 0.3781650304794312, 'train/mask_loss': 0.472211217880249, 'metrics/total_secs_per_batch': 6.032323122024536, 'metrics/data_secs_per_batch': 2.2834449768066407, '_timestamp': 1740977506.4718392}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 411 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977506.4722123}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 412 is less than current step: 499. Dropping entry: {'train/loss': 1.463220489025116, 'train/ce_loss': 0.53505859375, 'train/seg_cls_loss': 0.009881591796875, 'train/kl_loss': 0.217578125, 'train/mask_bce_loss': 0.09956584125757217, 'train/mask_dice_loss': 0.35113619565963744, 'train/mask_loss': 0.45070203244686124, 'metrics/total_secs_per_batch': 5.369226455688477, 'metrics/data_secs_per_batch': 2.216817092895508, '_timestamp': 1740977511.8408706}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 412 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977511.8411698}).
Epoch: [7][413/500]	Time  5.369 ( 5.369)	Loss 1.1797 (1.4632)	CeLoss 1.1797 (0.5351)	SegCLSLoss 0.0000 (0.0099)	KLLoss 0.0000 (0.2176)	MaskLoss 0.0000 (0.4507)	MaskBCELoss 0.0000 (0.0996)	MaskDICELoss 0.0000 (0.3511)
Epoch: [7][414/500]	Time  5.443 ( 5.443)	Loss 0.1709 (1.3889)	CeLoss 0.1709 (0.6391)	SegCLSLoss 0.0000 (0.0081)	KLLoss 0.0000 (0.1807)	MaskLoss 0.0000 (0.3639)	MaskBCELoss 0.0000 (0.0326)	MaskDICELoss 0.0000 (0.3312)
Epoch: [7][415/500]	Time  6.430 ( 6.430)	Loss 0.5861 (1.7689)	CeLoss 0.3008 (0.2033)	SegCLSLoss 0.0102 (0.0170)	KLLoss 0.3652 (0.3646)	MaskLoss 0.1212 (0.7603)	MaskBCELoss 0.0758 (0.1178)	MaskDICELoss 0.0454 (0.6425)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 413 is less than current step: 499. Dropping entry: {'train/loss': 1.3888935923576355, 'train/ce_loss': 0.6390625, 'train/seg_cls_loss': 0.00806884765625, 'train/kl_loss': 0.1806640625, 'train/mask_bce_loss': 0.03264800179749727, 'train/mask_dice_loss': 0.3312323808670044, 'train/mask_loss': 0.36388038992881777, 'metrics/total_secs_per_batch': 5.44342827796936, 'metrics/data_secs_per_batch': 2.0089654207229612, '_timestamp': 1740977517.28439}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 413 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977517.2845902}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 414 is less than current step: 499. Dropping entry: {'train/loss': 1.7689177215099334, 'train/ce_loss': 0.203271484375, 'train/seg_cls_loss': 0.01697998046875, 'train/kl_loss': 0.3646484375, 'train/mask_bce_loss': 0.11783921844325959, 'train/mask_dice_loss': 0.6424985364079475, 'train/mask_loss': 0.7603377506136895, 'metrics/total_secs_per_batch': 6.429562091827393, 'metrics/data_secs_per_batch': 2.904860734939575, '_timestamp': 1740977523.7140324}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 414 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977523.7143545}).
Epoch: [7][416/500]	Time  5.587 ( 5.587)	Loss 1.7211 (1.4893)	CeLoss 0.2910 (0.5356)	SegCLSLoss 0.0106 (0.0091)	KLLoss 0.3711 (0.2557)	MaskLoss 0.6935 (0.4618)	MaskBCELoss 0.1399 (0.1039)	MaskDICELoss 0.5536 (0.3579)
Epoch: [7][417/500]	Time  5.986 ( 5.986)	Loss 1.6953 (1.5777)	CeLoss 0.2236 (0.4220)	SegCLSLoss 0.0107 (0.0102)	KLLoss 0.3691 (0.2926)	MaskLoss 0.7148 (0.5607)	MaskBCELoss 0.1130 (0.1460)	MaskDICELoss 0.6019 (0.4147)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 415 is less than current step: 499. Dropping entry: {'train/loss': 1.4892691314220428, 'train/ce_loss': 0.53564453125, 'train/seg_cls_loss': 0.009051513671875, 'train/kl_loss': 0.2556640625, 'train/mask_bce_loss': 0.10387477576732636, 'train/mask_dice_loss': 0.35794727355241773, 'train/mask_loss': 0.4618220508098602, 'metrics/total_secs_per_batch': 5.587205410003662, 'metrics/data_secs_per_batch': 2.34513373374939, '_timestamp': 1740977529.3010547}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 415 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977529.3013535}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 416 is less than current step: 499. Dropping entry: {'train/loss': 1.5777495741844176, 'train/ce_loss': 0.42197265625, 'train/seg_cls_loss': 0.01016845703125, 'train/kl_loss': 0.292578125, 'train/mask_bce_loss': 0.14599236585199832, 'train/mask_dice_loss': 0.4146597653627396, 'train/mask_loss': 0.5606521308422089, 'metrics/total_secs_per_batch': 5.985931158065796, 'metrics/data_secs_per_batch': 2.581438398361206, '_timestamp': 1740977535.2872007}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 416 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977535.2875772}).
Epoch: [7][418/500]	Time  7.004 ( 7.004)	Loss 1.6836 (1.7832)	CeLoss 0.2285 (0.4037)	SegCLSLoss 0.0116 (0.0118)	KLLoss 0.3691 (0.2912)	MaskLoss 0.7061 (0.6724)	MaskBCELoss 0.1084 (0.1402)	MaskDICELoss 0.5977 (0.5322)
Epoch: [7][419/500]	Time  5.758 ( 5.758)	Loss 1.1172 (1.3118)	CeLoss 1.1172 (0.3563)	SegCLSLoss 0.0000 (0.0092)	KLLoss 0.0000 (0.1822)	MaskLoss 0.0000 (0.4664)	MaskBCELoss 0.0000 (0.1120)	MaskDICELoss 0.0000 (0.3544)
[2025-03-02 22:52:33,799] [INFO] [logging.py:96:log_dist] [Rank 0] step=3920, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:52:33,805] [INFO] [timer.py:215:stop] epoch=0/micro_step=39200/global_step=3920, RunningAvgSamplesPerSec=1.4708842660834833, CurrSamplesPerSec=1.7374755382293756, MemAllocated=30.8GB, MaxMemAllocated=37.23GB
Epoch: [7][420/500]	Time  5.757 ( 5.757)	Loss 0.8638 (1.6816)	CeLoss 0.3301 (0.4186)	SegCLSLoss 0.0115 (0.0141)	KLLoss 0.3652 (0.2926)	MaskLoss 0.2454 (0.6135)	MaskBCELoss 0.1602 (0.1614)	MaskDICELoss 0.0852 (0.4520)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 417 is less than current step: 499. Dropping entry: {'train/loss': 1.7832375645637513, 'train/ce_loss': 0.4037109375, 'train/seg_cls_loss': 0.011773681640625, 'train/kl_loss': 0.2912109375, 'train/mask_bce_loss': 0.14016649238765239, 'train/mask_dice_loss': 0.5322139918804168, 'train/mask_loss': 0.6723804861307144, 'metrics/total_secs_per_batch': 7.004320859909058, 'metrics/data_secs_per_batch': 3.425126481056213, '_timestamp': 1740977542.2913423}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 417 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977542.2917063}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 418 is less than current step: 499. Dropping entry: {'train/loss': 1.3118064403533936, 'train/ce_loss': 0.356298828125, 'train/seg_cls_loss': 0.0091796875, 'train/kl_loss': 0.1822265625, 'train/mask_bce_loss': 0.11197191178798675, 'train/mask_dice_loss': 0.35440493524074557, 'train/mask_loss': 0.4663768410682678, 'metrics/total_secs_per_batch': 5.75779128074646, 'metrics/data_secs_per_batch': 2.6243710041046144, '_timestamp': 1740977548.0492983}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 418 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977548.0496416}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 419 is less than current step: 499. Dropping entry: {'train/loss': 1.6816026091575622, 'train/ce_loss': 0.4185546875, 'train/seg_cls_loss': 0.01407470703125, 'train/kl_loss': 0.292578125, 'train/mask_bce_loss': 0.16142418459057808, 'train/mask_dice_loss': 0.4520333558320999, 'train/mask_loss': 0.6134575515985489, 'metrics/total_secs_per_batch': 5.757248163223267, 'metrics/data_secs_per_batch': 2.6625523567199707, '_timestamp': 1740977553.8061855}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 419 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977553.8064754}).
Epoch: [7][421/500]	Time  5.610 ( 5.610)	Loss 0.8906 (1.5716)	CeLoss 0.8906 (0.4010)	SegCLSLoss 0.0000 (0.0128)	KLLoss 0.0000 (0.2920)	MaskLoss 0.0000 (0.5674)	MaskBCELoss 0.0000 (0.1220)	MaskDICELoss 0.0000 (0.4455)
Epoch: [7][422/500]	Time  6.775 ( 6.775)	Loss 2.2487 (1.9751)	CeLoss 0.1992 (0.2241)	SegCLSLoss 0.0240 (0.0169)	KLLoss 0.3613 (0.3248)	MaskLoss 1.0008 (0.8550)	MaskBCELoss 0.1363 (0.1450)	MaskDICELoss 0.8645 (0.7099)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 420 is less than current step: 499. Dropping entry: {'train/loss': 1.5716176509857178, 'train/ce_loss': 0.4009765625, 'train/seg_cls_loss': 0.012774658203125, 'train/kl_loss': 0.2919921875, 'train/mask_bce_loss': 0.12199058346450328, 'train/mask_dice_loss': 0.4454588532447815, 'train/mask_loss': 0.5674494326114654, 'metrics/total_secs_per_batch': 5.609729290008545, 'metrics/data_secs_per_batch': 2.425568461418152, '_timestamp': 1740977559.4161022}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 420 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977559.4163783}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 421 is less than current step: 499. Dropping entry: {'train/loss': 1.9751486659049988, 'train/ce_loss': 0.22412109375, 'train/seg_cls_loss': 0.016943359375, 'train/kl_loss': 0.3248046875, 'train/mask_bce_loss': 0.14502990124747156, 'train/mask_dice_loss': 0.7099272310733795, 'train/mask_loss': 0.8549571335315704, 'metrics/total_secs_per_batch': 6.774730682373047, 'metrics/data_secs_per_batch': 2.9470484018325807, '_timestamp': 1740977566.1908443}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 421 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977566.1911283}).
Epoch: [7][423/500]	Time  6.307 ( 6.307)	Loss 1.5335 (1.8951)	CeLoss 0.2734 (0.3434)	SegCLSLoss 0.0137 (0.0153)	KLLoss 0.3711 (0.3316)	MaskLoss 0.6076 (0.7554)	MaskBCELoss 0.1303 (0.1222)	MaskDICELoss 0.4773 (0.6332)
Epoch: [7][424/500]	Time  5.495 ( 5.495)	Loss 1.3826 (1.4306)	CeLoss 0.2910 (0.4179)	SegCLSLoss 0.0089 (0.0099)	KLLoss 0.3574 (0.2525)	MaskLoss 0.5263 (0.4913)	MaskBCELoss 0.3513 (0.1443)	MaskDICELoss 0.1749 (0.3470)
Epoch: [7][425/500]	Time  6.565 ( 6.565)	Loss 1.3515 (1.6482)	CeLoss 0.2188 (0.3978)	SegCLSLoss 0.0173 (0.0128)	KLLoss 0.3613 (0.2553)	MaskLoss 0.5439 (0.6092)	MaskBCELoss 0.0309 (0.1101)	MaskDICELoss 0.5130 (0.4992)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 422 is less than current step: 499. Dropping entry: {'train/loss': 1.895076823234558, 'train/ce_loss': 0.343408203125, 'train/seg_cls_loss': 0.01531982421875, 'train/kl_loss': 0.331640625, 'train/mask_bce_loss': 0.1222284303046763, 'train/mask_dice_loss': 0.6332201316952706, 'train/mask_loss': 0.755448566377163, 'metrics/total_secs_per_batch': 6.307390451431274, 'metrics/data_secs_per_batch': 2.958561658859253, '_timestamp': 1740977572.4983726}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 422 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977572.4987228}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 423 is less than current step: 499. Dropping entry: {'train/loss': 1.4305848479270935, 'train/ce_loss': 0.41787109375, 'train/seg_cls_loss': 0.009918212890625, 'train/kl_loss': 0.2525390625, 'train/mask_bce_loss': 0.14428028613328933, 'train/mask_dice_loss': 0.34703753143548965, 'train/mask_loss': 0.49131782054901124, 'metrics/total_secs_per_batch': 5.494797229766846, 'metrics/data_secs_per_batch': 2.6226497650146485, '_timestamp': 1740977577.9930742}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 423 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977577.9933655}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 424 is less than current step: 499. Dropping entry: {'train/loss': 1.64817134141922, 'train/ce_loss': 0.39775390625, 'train/seg_cls_loss': 0.012786865234375, 'train/kl_loss': 0.2552734375, 'train/mask_bce_loss': 0.11007364634424448, 'train/mask_dice_loss': 0.49916826784610746, 'train/mask_loss': 0.609241908788681, 'metrics/total_secs_per_batch': 6.565381288528442, 'metrics/data_secs_per_batch': 2.9012613534927367, '_timestamp': 1740977584.5586102}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 424 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977584.5589638}).
Epoch: [7][426/500]	Time  6.062 ( 6.062)	Loss 0.5117 (1.5667)	CeLoss 0.5117 (0.2430)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.3287)	MaskLoss 0.0000 (0.6419)	MaskBCELoss 0.0000 (0.1281)	MaskDICELoss 0.0000 (0.5138)
Epoch: [7][427/500]	Time  5.193 ( 5.193)	Loss 2.0878 (1.6563)	CeLoss 0.2520 (0.4512)	SegCLSLoss 0.0194 (0.0103)	KLLoss 0.3535 (0.2533)	MaskLoss 0.8955 (0.5873)	MaskBCELoss 0.0174 (0.1222)	MaskDICELoss 0.8781 (0.4651)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 425 is less than current step: 499. Dropping entry: {'train/loss': 1.5667067766189575, 'train/ce_loss': 0.24296875, 'train/seg_cls_loss': 0.014154052734375, 'train/kl_loss': 0.3287109375, 'train/mask_bce_loss': 0.1281165864318609, 'train/mask_dice_loss': 0.5138305604457856, 'train/mask_loss': 0.6419471383094788, 'metrics/total_secs_per_batch': 6.062448740005493, 'metrics/data_secs_per_batch': 2.647329568862915, '_timestamp': 1740977590.6209362}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 425 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977590.6212313}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 426 is less than current step: 499. Dropping entry: {'train/loss': 1.656338143348694, 'train/ce_loss': 0.451171875, 'train/seg_cls_loss': 0.01025390625, 'train/kl_loss': 0.2533203125, 'train/mask_bce_loss': 0.12220051176846028, 'train/mask_dice_loss': 0.46514824628829954, 'train/mask_loss': 0.5873487532138825, 'metrics/total_secs_per_batch': 5.192937850952148, 'metrics/data_secs_per_batch': 2.4682815074920654, '_timestamp': 1740977595.8140213}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 426 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977595.8143694}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 427 is less than current step: 499. Dropping entry: {'train/loss': 2.015777790546417, 'train/ce_loss': 0.371484375, 'train/seg_cls_loss': 0.0133056640625, 'train/kl_loss': 0.326953125, 'train/mask_bce_loss': 0.1933993510901928, 'train/mask_dice_loss': 0.6090696334838868, 'train/mask_loss': 0.8024689793586731, 'metrics/total_secs_per_batch': 5.888369083404541, 'metrics/data_secs_per_batch': 2.4371344327926634, '_timestamp': 1740977601.7022233}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 427 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977601.702534}).
Epoch: [7][428/500]	Time  5.888 ( 5.888)	Loss 2.6292 (2.0158)	CeLoss 0.2197 (0.3715)	SegCLSLoss 0.0160 (0.0133)	KLLoss 0.3711 (0.3270)	MaskLoss 1.1818 (0.8025)	MaskBCELoss 0.3458 (0.1934)	MaskDICELoss 0.8360 (0.6091)
Epoch: [7][429/500]	Time  6.524 ( 6.524)	Loss 1.6631 (1.3952)	CeLoss 0.2197 (0.4364)	SegCLSLoss 0.0214 (0.0098)	KLLoss 0.3594 (0.2180)	MaskLoss 0.6988 (0.4660)	MaskBCELoss 0.0319 (0.0906)	MaskDICELoss 0.6668 (0.3754)
[2025-03-02 22:53:34,424] [INFO] [logging.py:96:log_dist] [Rank 0] step=3930, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:53:34,430] [INFO] [timer.py:215:stop] epoch=0/micro_step=39300/global_step=3930, RunningAvgSamplesPerSec=1.4712905509682483, CurrSamplesPerSec=1.6120575935592474, MemAllocated=31.15GB, MaxMemAllocated=37.23GB
Epoch: [7][430/500]	Time  6.205 ( 6.205)	Loss 2.2255 (1.7217)	CeLoss 0.1162 (0.3274)	SegCLSLoss 0.0317 (0.0153)	KLLoss 0.3672 (0.2883)	MaskLoss 1.0288 (0.6788)	MaskBCELoss 0.3386 (0.1518)	MaskDICELoss 0.6902 (0.5271)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 428 is less than current step: 499. Dropping entry: {'train/loss': 1.3951528787612915, 'train/ce_loss': 0.436376953125, 'train/seg_cls_loss': 0.0098388671875, 'train/kl_loss': 0.21796875, 'train/mask_bce_loss': 0.0905905518680811, 'train/mask_dice_loss': 0.37541850507259367, 'train/mask_loss': 0.4660090506076813, 'metrics/total_secs_per_batch': 6.5241522789001465, 'metrics/data_secs_per_batch': 2.9522499322891234, '_timestamp': 1740977608.2264047}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 428 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977608.2266157}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 429 is less than current step: 499. Dropping entry: {'train/loss': 1.7216617822647096, 'train/ce_loss': 0.32744140625, 'train/seg_cls_loss': 0.015338134765625, 'train/kl_loss': 0.28828125, 'train/mask_bce_loss': 0.1517783861607313, 'train/mask_dice_loss': 0.5270700842142105, 'train/mask_loss': 0.6788484692573548, 'metrics/total_secs_per_batch': 6.204916477203369, 'metrics/data_secs_per_batch': 2.9097010850906373, '_timestamp': 1740977614.4311504}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 429 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977614.4314885}).
Epoch: [7][431/500]	Time  5.857 ( 5.857)	Loss 1.3148 (1.8298)	CeLoss 0.1992 (0.3860)	SegCLSLoss 0.0181 (0.0128)	KLLoss 0.3535 (0.2900)	MaskLoss 0.5358 (0.7041)	MaskBCELoss 0.0406 (0.1619)	MaskDICELoss 0.4952 (0.5422)
Epoch: [7][432/500]	Time  5.302 ( 5.302)	Loss 0.2002 (1.0078)	CeLoss 0.2002 (0.3276)	SegCLSLoss 0.0000 (0.0109)	KLLoss 0.0000 (0.2256)	MaskLoss 0.0000 (0.3260)	MaskBCELoss 0.0000 (0.0707)	MaskDICELoss 0.0000 (0.2553)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 430 is less than current step: 499. Dropping entry: {'train/loss': 1.82977796792984, 'train/ce_loss': 0.38603515625, 'train/seg_cls_loss': 0.012847900390625, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.1619139291346073, 'train/mask_dice_loss': 0.5421840250492096, 'train/mask_loss': 0.7040979623794555, 'metrics/total_secs_per_batch': 5.856914758682251, 'metrics/data_secs_per_batch': 2.402622127532959, '_timestamp': 1740977620.2882135}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 430 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977620.2885046}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 431 is less than current step: 499. Dropping entry: {'train/loss': 1.007845789194107, 'train/ce_loss': 0.327587890625, 'train/seg_cls_loss': 0.010858154296875, 'train/kl_loss': 0.2255859375, 'train/mask_bce_loss': 0.07072232440114021, 'train/mask_dice_loss': 0.25529530048370364, 'train/mask_loss': 0.3260176211595535, 'metrics/total_secs_per_batch': 5.302079439163208, 'metrics/data_secs_per_batch': 2.1353577852249144, '_timestamp': 1740977625.590312}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 431 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977625.5906084}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 432 is less than current step: 499. Dropping entry: {'train/loss': 1.3003710150718688, 'train/ce_loss': 0.443408203125, 'train/seg_cls_loss': 0.009521484375, 'train/kl_loss': 0.1806640625, 'train/mask_bce_loss': 0.05437165778130293, 'train/mask_dice_loss': 0.36278162002563474, 'train/mask_loss': 0.41715327501296995, 'metrics/total_secs_per_batch': 5.2558112144470215, 'metrics/data_secs_per_batch': 2.4046921730041504, '_timestamp': 1740977630.8462799}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 432 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977630.8466256}).
Epoch: [7][433/500]	Time  5.256 ( 5.256)	Loss 1.1875 (1.3004)	CeLoss 1.1875 (0.4434)	SegCLSLoss 0.0000 (0.0095)	KLLoss 0.0000 (0.1807)	MaskLoss 0.0000 (0.4172)	MaskBCELoss 0.0000 (0.0544)	MaskDICELoss 0.0000 (0.3628)
Epoch: [7][434/500]	Time  5.974 ( 5.974)	Loss 2.7575 (1.7338)	CeLoss 0.1729 (0.2874)	SegCLSLoss 0.0219 (0.0164)	KLLoss 0.3691 (0.3307)	MaskLoss 1.2684 (0.7024)	MaskBCELoss 0.2691 (0.1762)	MaskDICELoss 0.9993 (0.5262)
Epoch: [7][435/500]	Time  5.080 ( 5.080)	Loss 1.8450 (1.3000)	CeLoss 0.2617 (0.5163)	SegCLSLoss 0.0126 (0.0082)	KLLoss 0.3652 (0.1814)	MaskLoss 0.7701 (0.3808)	MaskBCELoss 0.1932 (0.1127)	MaskDICELoss 0.5769 (0.2681)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 433 is less than current step: 499. Dropping entry: {'train/loss': 1.7337918698787689, 'train/ce_loss': 0.28740234375, 'train/seg_cls_loss': 0.01640625, 'train/kl_loss': 0.3306640625, 'train/mask_bce_loss': 0.17621729271486403, 'train/mask_dice_loss': 0.5262255117297172, 'train/mask_loss': 0.7024428099393845, 'metrics/total_secs_per_batch': 5.973799467086792, 'metrics/data_secs_per_batch': 2.755702328681946, '_timestamp': 1740977636.8199027}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 433 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977636.820197}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 434 is less than current step: 499. Dropping entry: {'train/loss': 1.300041949748993, 'train/ce_loss': 0.51630859375, 'train/seg_cls_loss': 0.008221435546875, 'train/kl_loss': 0.1814453125, 'train/mask_bce_loss': 0.11269332244992256, 'train/mask_dice_loss': 0.2680893748998642, 'train/mask_loss': 0.38078269362449646, 'metrics/total_secs_per_batch': 5.0799880027771, 'metrics/data_secs_per_batch': 2.2686301469802856, '_timestamp': 1740977641.899955}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 434 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977641.900345}).
Epoch: [7][436/500]	Time  5.832 ( 5.832)	Loss 0.6244 (1.2745)	CeLoss 0.2393 (0.4047)	SegCLSLoss 0.0106 (0.0120)	KLLoss 0.3652 (0.2184)	MaskLoss 0.1716 (0.4210)	MaskBCELoss 0.0771 (0.0526)	MaskDICELoss 0.0945 (0.3684)
Epoch: [7][437/500]	Time  6.411 ( 6.411)	Loss 0.0530 (1.3587)	CeLoss 0.0530 (0.3114)	SegCLSLoss 0.0000 (0.0112)	KLLoss 0.0000 (0.2520)	MaskLoss 0.0000 (0.5084)	MaskBCELoss 0.0000 (0.0458)	MaskDICELoss 0.0000 (0.4626)
Epoch: [7][438/500]	Time  4.606 ( 4.606)	Loss 0.5356 (2.0178)	CeLoss 0.1953 (0.4144)	SegCLSLoss 0.0098 (0.0126)	KLLoss 0.3652 (0.2938)	MaskLoss 0.1497 (0.7837)	MaskBCELoss 0.0774 (0.2168)	MaskDICELoss 0.0723 (0.5670)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 435 is less than current step: 499. Dropping entry: {'train/loss': 1.2745451152324676, 'train/ce_loss': 0.4046875, 'train/seg_cls_loss': 0.012017822265625, 'train/kl_loss': 0.218359375, 'train/mask_bce_loss': 0.052599521819502115, 'train/mask_dice_loss': 0.36836444661021234, 'train/mask_loss': 0.42096397280693054, 'metrics/total_secs_per_batch': 5.8321754932403564, 'metrics/data_secs_per_batch': 2.676353359222412, '_timestamp': 1740977647.7320948}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 435 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977647.7324069}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 436 is less than current step: 499. Dropping entry: {'train/loss': 1.358720028400421, 'train/ce_loss': 0.3114013671875, 'train/seg_cls_loss': 0.0111572265625, 'train/kl_loss': 0.251953125, 'train/mask_bce_loss': 0.045776179805397985, 'train/mask_dice_loss': 0.4625999480485916, 'train/mask_loss': 0.5083761274814605, 'metrics/total_secs_per_batch': 6.411043643951416, 'metrics/data_secs_per_batch': 3.0299084186553955, '_timestamp': 1740977654.1431065}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 436 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977654.1433933}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 437 is less than current step: 499. Dropping entry: {'train/loss': 2.01777406334877, 'train/ce_loss': 0.41435546875, 'train/seg_cls_loss': 0.012603759765625, 'train/kl_loss': 0.29375, 'train/mask_bce_loss': 0.2167794734239578, 'train/mask_dice_loss': 0.5669610768556594, 'train/mask_loss': 0.783740547299385, 'metrics/total_secs_per_batch': 4.60612678527832, 'metrics/data_secs_per_batch': 1.9256128311157226, '_timestamp': 1740977658.7492313}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 437 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977658.7495155}).
Epoch: [7][439/500]	Time  6.327 ( 6.327)	Loss 1.6421 (1.6813)	CeLoss 0.2334 (0.3680)	SegCLSLoss 0.0160 (0.0134)	KLLoss 0.3691 (0.3273)	MaskLoss 0.6824 (0.6372)	MaskBCELoss 0.0490 (0.1179)	MaskDICELoss 0.6334 (0.5192)
[2025-03-02 22:54:31,961] [INFO] [logging.py:96:log_dist] [Rank 0] step=3940, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:54:31,967] [INFO] [timer.py:215:stop] epoch=0/micro_step=39400/global_step=3940, RunningAvgSamplesPerSec=1.4718648423820782, CurrSamplesPerSec=1.4513880288405956, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [7][440/500]	Time  6.892 ( 6.892)	Loss 1.4669 (1.5945)	CeLoss 0.2393 (0.4184)	SegCLSLoss 0.0197 (0.0129)	KLLoss 0.3555 (0.2893)	MaskLoss 0.5909 (0.5703)	MaskBCELoss 0.0221 (0.0591)	MaskDICELoss 0.5688 (0.5112)
Epoch: [7][441/500]	Time  4.788 ( 4.788)	Loss 3.3067 (1.4594)	CeLoss 0.1289 (0.6298)	SegCLSLoss 0.0262 (0.0078)	KLLoss 0.3809 (0.1826)	MaskLoss 1.5635 (0.4037)	MaskBCELoss 0.8608 (0.1250)	MaskDICELoss 0.7027 (0.2787)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 438 is less than current step: 499. Dropping entry: {'train/loss': 1.6813416242599488, 'train/ce_loss': 0.36796875, 'train/seg_cls_loss': 0.013385009765625, 'train/kl_loss': 0.32734375, 'train/mask_bce_loss': 0.11793679874390364, 'train/mask_dice_loss': 0.519218385219574, 'train/mask_loss': 0.6371551781892777, 'metrics/total_secs_per_batch': 6.327305316925049, 'metrics/data_secs_per_batch': 2.848057270050049, '_timestamp': 1740977665.0767784}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 438 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977665.0770202}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 439 is less than current step: 499. Dropping entry: {'train/loss': 1.5944515705108642, 'train/ce_loss': 0.418359375, 'train/seg_cls_loss': 0.0129150390625, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.0590916340239346, 'train/mask_dice_loss': 0.5112298637628555, 'train/mask_loss': 0.5703215003013611, 'metrics/total_secs_per_batch': 6.891820907592773, 'metrics/data_secs_per_batch': 2.8542293310165405, '_timestamp': 1740977671.968173}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 439 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977671.9684484}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 440 is less than current step: 499. Dropping entry: {'train/loss': 1.4594016790390014, 'train/ce_loss': 0.62978515625, 'train/seg_cls_loss': 0.007818603515625, 'train/kl_loss': 0.1826171875, 'train/mask_bce_loss': 0.12495357133448123, 'train/mask_dice_loss': 0.27872187793254855, 'train/mask_loss': 0.4036754444241524, 'metrics/total_secs_per_batch': 4.788373947143555, 'metrics/data_secs_per_batch': 2.099680495262146, '_timestamp': 1740977676.756757}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 440 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977676.7570498}).
Epoch: [7][442/500]	Time  3.882 ( 3.882)	Loss 2.0517 (1.4713)	CeLoss 0.2832 (0.6331)	SegCLSLoss 0.0093 (0.0070)	KLLoss 0.3574 (0.1830)	MaskLoss 0.8647 (0.4084)	MaskBCELoss 0.3861 (0.1093)	MaskDICELoss 0.4786 (0.2991)
Epoch: [7][443/500]	Time  6.317 ( 6.317)	Loss 1.1094 (1.4246)	CeLoss 0.2314 (0.2765)	SegCLSLoss 0.0111 (0.0150)	KLLoss 0.3613 (0.2941)	MaskLoss 0.4180 (0.5556)	MaskBCELoss 0.1170 (0.0888)	MaskDICELoss 0.3010 (0.4668)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 441 is less than current step: 499. Dropping entry: {'train/loss': 1.471281313896179, 'train/ce_loss': 0.63310546875, 'train/seg_cls_loss': 0.0070068359375, 'train/kl_loss': 0.1830078125, 'train/mask_bce_loss': 0.10932015534490347, 'train/mask_dice_loss': 0.2990744099020958, 'train/mask_loss': 0.40839456021785736, 'metrics/total_secs_per_batch': 3.8815598487854004, 'metrics/data_secs_per_batch': 1.8364660739898682, '_timestamp': 1740977680.6383123}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 441 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977680.6385932}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 442 is less than current step: 499. Dropping entry: {'train/loss': 1.4246398985385895, 'train/ce_loss': 0.27646484375, 'train/seg_cls_loss': 0.0149658203125, 'train/kl_loss': 0.294140625, 'train/mask_bce_loss': 0.08875639578327536, 'train/mask_dice_loss': 0.466825270652771, 'train/mask_loss': 0.5555816620588303, 'metrics/total_secs_per_batch': 6.317128419876099, 'metrics/data_secs_per_batch': 2.767745852470398, '_timestamp': 1740977686.9554298}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 442 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977686.9557064}).
Epoch: [7][444/500]	Time  6.104 ( 6.104)	Loss 1.4141 (1.9123)	CeLoss 1.4141 (0.4837)	SegCLSLoss 0.0000 (0.0127)	KLLoss 0.0000 (0.2904)	MaskLoss 0.0000 (0.6966)	MaskBCELoss 0.0000 (0.1289)	MaskDICELoss 0.0000 (0.5677)
Epoch: [7][445/500]	Time  5.052 ( 5.052)	Loss 1.0156 (1.7691)	CeLoss 1.0156 (0.3995)	SegCLSLoss 0.0000 (0.0145)	KLLoss 0.0000 (0.2576)	MaskLoss 0.0000 (0.6682)	MaskBCELoss 0.0000 (0.1434)	MaskDICELoss 0.0000 (0.5248)
Epoch: [7][446/500]	Time  6.502 ( 6.502)	Loss 2.1508 (1.9873)	CeLoss 0.1895 (0.3382)	SegCLSLoss 0.0197 (0.0180)	KLLoss 0.3574 (0.3240)	MaskLoss 0.9582 (0.8037)	MaskBCELoss 0.0384 (0.1298)	MaskDICELoss 0.9198 (0.6739)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 443 is less than current step: 499. Dropping entry: {'train/loss': 1.912317728996277, 'train/ce_loss': 0.48369140625, 'train/seg_cls_loss': 0.012664794921875, 'train/kl_loss': 0.2904296875, 'train/mask_bce_loss': 0.12891850546002387, 'train/mask_dice_loss': 0.5676700502634049, 'train/mask_loss': 0.6965885490179062, 'metrics/total_secs_per_batch': 6.104171514511108, 'metrics/data_secs_per_batch': 2.8051590442657472, '_timestamp': 1740977693.059671}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 443 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977693.059964}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 444 is less than current step: 499. Dropping entry: {'train/loss': 1.7691015720367431, 'train/ce_loss': 0.39951171875, 'train/seg_cls_loss': 0.014459228515625, 'train/kl_loss': 0.2576171875, 'train/mask_bce_loss': 0.14339381400495768, 'train/mask_dice_loss': 0.5248483926057815, 'train/mask_loss': 0.6682422041893006, 'metrics/total_secs_per_batch': 5.052278280258179, 'metrics/data_secs_per_batch': 2.4215734720230104, '_timestamp': 1740977698.1121118}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 444 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977698.1124659}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 445 is less than current step: 499. Dropping entry: {'train/loss': 1.9873298406600952, 'train/ce_loss': 0.33818359375, 'train/seg_cls_loss': 0.01800537109375, 'train/kl_loss': 0.3240234375, 'train/mask_bce_loss': 0.12980749211274087, 'train/mask_dice_loss': 0.6738671779632568, 'train/mask_loss': 0.8036746740341186, 'metrics/total_secs_per_batch': 6.502182245254517, 'metrics/data_secs_per_batch': 2.8006199836730956, '_timestamp': 1740977704.6141078}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 445 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977704.6143923}).
Epoch: [7][447/500]	Time  5.768 ( 5.768)	Loss 2.5372 (1.7630)	CeLoss 0.1650 (0.4263)	SegCLSLoss 0.0162 (0.0151)	KLLoss 0.3633 (0.2920)	MaskLoss 1.1641 (0.6499)	MaskBCELoss 0.3759 (0.1439)	MaskDICELoss 0.7882 (0.5060)
Epoch: [7][448/500]	Time  5.568 ( 5.568)	Loss 2.1231 (1.8783)	CeLoss 0.2217 (0.5297)	SegCLSLoss 0.0090 (0.0126)	KLLoss 0.3594 (0.2906)	MaskLoss 0.9307 (0.6565)	MaskBCELoss 0.3034 (0.1371)	MaskDICELoss 0.6273 (0.5194)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 446 is less than current step: 499. Dropping entry: {'train/loss': 1.762956428527832, 'train/ce_loss': 0.42626953125, 'train/seg_cls_loss': 0.015087890625, 'train/kl_loss': 0.2919921875, 'train/mask_bce_loss': 0.14392857486382127, 'train/mask_dice_loss': 0.5059578329324722, 'train/mask_loss': 0.6498864173889161, 'metrics/total_secs_per_batch': 5.767841577529907, 'metrics/data_secs_per_batch': 2.6351686477661134, '_timestamp': 1740977710.3819146}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 446 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977710.382178}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 447 is less than current step: 499. Dropping entry: {'train/loss': 1.878325843811035, 'train/ce_loss': 0.5296875, 'train/seg_cls_loss': 0.012615966796875, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.13714631982147693, 'train/mask_dice_loss': 0.5193994164466857, 'train/mask_loss': 0.6565457344055176, 'metrics/total_secs_per_batch': 5.56823468208313, 'metrics/data_secs_per_batch': 2.200062704086304, '_timestamp': 1740977715.9501588}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 447 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977715.9504294}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 448 is less than current step: 499. Dropping entry: {'train/loss': 1.6207794785499572, 'train/ce_loss': 0.46611328125, 'train/seg_cls_loss': 0.014215087890625, 'train/kl_loss': 0.2904296875, 'train/mask_bce_loss': 0.12140538692474365, 'train/mask_dice_loss': 0.437617164850235, 'train/mask_loss': 0.5590225517749786, 'metrics/total_secs_per_batch': 5.9206812381744385, 'metrics/data_secs_per_batch': 2.626432681083679, '_timestamp': 1740977721.8709552}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 448 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977721.871274}).
Epoch: [7][449/500]	Time  5.921 ( 5.921)	Loss 1.2248 (1.6208)	CeLoss 0.2754 (0.4661)	SegCLSLoss 0.0105 (0.0142)	KLLoss 0.3691 (0.2904)	MaskLoss 0.4532 (0.5590)	MaskBCELoss 0.1100 (0.1214)	MaskDICELoss 0.3432 (0.4376)
[2025-03-02 22:55:27,677] [INFO] [logging.py:96:log_dist] [Rank 0] step=3950, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:55:27,682] [INFO] [timer.py:215:stop] epoch=0/micro_step=39500/global_step=3950, RunningAvgSamplesPerSec=1.4725367197646797, CurrSamplesPerSec=1.7209983052636861, MemAllocated=31.58GB, MaxMemAllocated=37.23GB
Epoch: [7][450/500]	Time  5.812 ( 5.812)	Loss 1.8442 (1.6943)	CeLoss 0.2246 (0.4140)	SegCLSLoss 0.0146 (0.0146)	KLLoss 0.3633 (0.2568)	MaskLoss 0.7883 (0.6237)	MaskBCELoss 0.0145 (0.1697)	MaskDICELoss 0.7738 (0.4539)
Epoch: [7][451/500]	Time  6.020 ( 6.020)	Loss 0.4551 (1.3401)	CeLoss 0.4551 (0.3983)	SegCLSLoss 0.0000 (0.0118)	KLLoss 0.0000 (0.2588)	MaskLoss 0.0000 (0.4551)	MaskBCELoss 0.0000 (0.0857)	MaskDICELoss 0.0000 (0.3694)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 449 is less than current step: 499. Dropping entry: {'train/loss': 1.6943084359169007, 'train/ce_loss': 0.414013671875, 'train/seg_cls_loss': 0.014581298828125, 'train/kl_loss': 0.2568359375, 'train/mask_bce_loss': 0.16972708338871598, 'train/mask_dice_loss': 0.45394080877304077, 'train/mask_loss': 0.6236678779125213, 'metrics/total_secs_per_batch': 5.812310457229614, 'metrics/data_secs_per_batch': 2.7073734998703003, '_timestamp': 1740977727.6830275}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 449 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977727.6833427}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 450 is less than current step: 499. Dropping entry: {'train/loss': 1.3400773525238037, 'train/ce_loss': 0.39833984375, 'train/seg_cls_loss': 0.01182861328125, 'train/kl_loss': 0.2587890625, 'train/mask_bce_loss': 0.08572874274104833, 'train/mask_dice_loss': 0.3693685233592987, 'train/mask_loss': 0.45509726256132127, 'metrics/total_secs_per_batch': 6.019826889038086, 'metrics/data_secs_per_batch': 2.1830266237258913, '_timestamp': 1740977733.7029805}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 450 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977733.7032545}).
Epoch: [7][452/500]	Time  5.441 ( 5.441)	Loss 1.5830 (1.5234)	CeLoss 0.2295 (0.4371)	SegCLSLoss 0.0120 (0.0144)	KLLoss 0.3672 (0.2889)	MaskLoss 0.6558 (0.5250)	MaskBCELoss 0.1097 (0.0824)	MaskDICELoss 0.5461 (0.4426)
Epoch: [7][453/500]	Time  6.019 ( 6.019)	Loss 1.5448 (1.9271)	CeLoss 0.1943 (0.1944)	SegCLSLoss 0.0139 (0.0199)	KLLoss 0.3613 (0.3664)	MaskLoss 0.6537 (0.8432)	MaskBCELoss 0.0080 (0.1037)	MaskDICELoss 0.6458 (0.7395)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 451 is less than current step: 499. Dropping entry: {'train/loss': 1.5233906686306, 'train/ce_loss': 0.437109375, 'train/seg_cls_loss': 0.014373779296875, 'train/kl_loss': 0.2888671875, 'train/mask_bce_loss': 0.08241231497377158, 'train/mask_dice_loss': 0.4425642855465412, 'train/mask_loss': 0.5249765977263451, 'metrics/total_secs_per_batch': 5.440972805023193, 'metrics/data_secs_per_batch': 2.1624482393264772, '_timestamp': 1740977739.1440182}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 451 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977739.1443365}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 452 is less than current step: 499. Dropping entry: {'train/loss': 1.9271369278430939, 'train/ce_loss': 0.19443359375, 'train/seg_cls_loss': 0.019873046875, 'train/kl_loss': 0.36640625, 'train/mask_bce_loss': 0.10367888156324626, 'train/mask_dice_loss': 0.7394794151186943, 'train/mask_loss': 0.8431583061814308, 'metrics/total_secs_per_batch': 6.018881797790527, 'metrics/data_secs_per_batch': 2.4372545003890993, '_timestamp': 1740977745.163066}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 452 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977745.1634088}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 453 is less than current step: 499. Dropping entry: {'train/loss': 1.6060849189758302, 'train/ce_loss': 0.345947265625, 'train/seg_cls_loss': 0.012042236328125, 'train/kl_loss': 0.292578125, 'train/mask_bce_loss': 0.12279786011204123, 'train/mask_dice_loss': 0.4894486844539642, 'train/mask_loss': 0.6122465491294861, 'metrics/total_secs_per_batch': 6.533077239990234, 'metrics/data_secs_per_batch': 2.9450889348983766, '_timestamp': 1740977751.6960695}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 453 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977751.6963737}).
Epoch: [7][454/500]	Time  6.533 ( 6.533)	Loss 1.1007 (1.6061)	CeLoss 0.2520 (0.3459)	SegCLSLoss 0.0182 (0.0120)	KLLoss 0.3633 (0.2926)	MaskLoss 0.4009 (0.6122)	MaskBCELoss 0.0097 (0.1228)	MaskDICELoss 0.3912 (0.4894)
Epoch: [7][455/500]	Time  5.447 ( 5.447)	Loss 1.0156 (1.1810)	CeLoss 1.0156 (0.4967)	SegCLSLoss 0.0000 (0.0076)	KLLoss 0.0000 (0.1801)	MaskLoss 0.0000 (0.3312)	MaskBCELoss 0.0000 (0.0529)	MaskDICELoss 0.0000 (0.2783)
Epoch: [7][456/500]	Time  6.692 ( 6.692)	Loss 0.0674 (1.8820)	CeLoss 0.0674 (0.2232)	SegCLSLoss 0.0000 (0.0136)	KLLoss 0.0000 (0.3289)	MaskLoss 0.0000 (0.8096)	MaskBCELoss 0.0000 (0.1821)	MaskDICELoss 0.0000 (0.6275)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 454 is less than current step: 499. Dropping entry: {'train/loss': 1.1809513211250304, 'train/ce_loss': 0.4967041015625, 'train/seg_cls_loss': 0.007586669921875, 'train/kl_loss': 0.180078125, 'train/mask_bce_loss': 0.05291171036660671, 'train/mask_dice_loss': 0.27832322642207147, 'train/mask_loss': 0.331234934926033, 'metrics/total_secs_per_batch': 5.446763753890991, 'metrics/data_secs_per_batch': 2.346659708023071, '_timestamp': 1740977757.1426923}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 454 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977757.142974}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 455 is less than current step: 499. Dropping entry: {'train/loss': 1.8819812774658202, 'train/ce_loss': 0.2232421875, 'train/seg_cls_loss': 0.013555908203125, 'train/kl_loss': 0.32890625, 'train/mask_bce_loss': 0.1820607952773571, 'train/mask_dice_loss': 0.6275333672761917, 'train/mask_loss': 0.8095941543579102, 'metrics/total_secs_per_batch': 6.6916303634643555, 'metrics/data_secs_per_batch': 3.0379798650741576, '_timestamp': 1740977763.8343654}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 455 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977763.8346424}).
Epoch: [7][457/500]	Time  6.181 ( 6.181)	Loss 1.4840 (1.4975)	CeLoss 0.2471 (0.2749)	SegCLSLoss 0.0131 (0.0162)	KLLoss 0.3633 (0.3273)	MaskLoss 0.5965 (0.5909)	MaskBCELoss 0.0832 (0.1081)	MaskDICELoss 0.5133 (0.4828)
Epoch: [7][458/500]	Time  5.627 ( 5.627)	Loss 1.7908 (1.6311)	CeLoss 0.2129 (0.4877)	SegCLSLoss 0.0226 (0.0124)	KLLoss 0.3574 (0.2545)	MaskLoss 0.7655 (0.5557)	MaskBCELoss 0.0073 (0.0781)	MaskDICELoss 0.7582 (0.4776)
Epoch: [7][459/500]	Time  5.134 ( 5.134)	Loss 0.8049 (1.3676)	CeLoss 0.2520 (0.5001)	SegCLSLoss 0.0114 (0.0096)	KLLoss 0.3691 (0.2564)	MaskLoss 0.2550 (0.4185)	MaskBCELoss 0.0319 (0.0642)	MaskDICELoss 0.2231 (0.3543)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 456 is less than current step: 499. Dropping entry: {'train/loss': 1.497526603937149, 'train/ce_loss': 0.27490234375, 'train/seg_cls_loss': 0.01617431640625, 'train/kl_loss': 0.32734375, 'train/mask_bce_loss': 0.10805484219454228, 'train/mask_dice_loss': 0.48284712731838225, 'train/mask_loss': 0.5909019619226455, 'metrics/total_secs_per_batch': 6.181458234786987, 'metrics/data_secs_per_batch': 2.9536699533462523, '_timestamp': 1740977770.0157855}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 456 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977770.0159829}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 457 is less than current step: 499. Dropping entry: {'train/loss': 1.6310733199119567, 'train/ce_loss': 0.487744140625, 'train/seg_cls_loss': 0.012445068359375, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.07809085622429848, 'train/mask_dice_loss': 0.47758251130580903, 'train/mask_loss': 0.5556733787059784, 'metrics/total_secs_per_batch': 5.626996755599976, 'metrics/data_secs_per_batch': 2.265278387069702, '_timestamp': 1740977775.6428576}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 457 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977775.6431692}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 458 is less than current step: 499. Dropping entry: {'train/loss': 1.3676141321659088, 'train/ce_loss': 0.50009765625, 'train/seg_cls_loss': 0.00960693359375, 'train/kl_loss': 0.2564453125, 'train/mask_bce_loss': 0.06422533877193928, 'train/mask_dice_loss': 0.35429851710796356, 'train/mask_loss': 0.41852385401725767, 'metrics/total_secs_per_batch': 5.133817195892334, 'metrics/data_secs_per_batch': 2.178227353096008, '_timestamp': 1740977780.7767448}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 458 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977780.7770543}).
[2025-03-02 22:56:27,277] [INFO] [logging.py:96:log_dist] [Rank 0] step=3960, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:56:27,282] [INFO] [timer.py:215:stop] epoch=0/micro_step=39600/global_step=3960, RunningAvgSamplesPerSec=1.4729928003239856, CurrSamplesPerSec=1.537269185352565, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [7][460/500]	Time  6.507 ( 6.507)	Loss 1.7699 (1.7414)	CeLoss 0.1992 (0.3396)	SegCLSLoss 0.0114 (0.0134)	KLLoss 0.3574 (0.3240)	MaskLoss 0.7648 (0.6813)	MaskBCELoss 0.2078 (0.1359)	MaskDICELoss 0.5570 (0.5454)
Epoch: [7][461/500]	Time  6.159 ( 6.159)	Loss 2.4108 (1.4624)	CeLoss 0.1924 (0.3712)	SegCLSLoss 0.0175 (0.0109)	KLLoss 0.3555 (0.2170)	MaskLoss 1.0872 (0.5321)	MaskBCELoss 0.1369 (0.0966)	MaskDICELoss 0.9503 (0.4355)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 459 is less than current step: 499. Dropping entry: {'train/loss': 1.7414360165596008, 'train/ce_loss': 0.33955078125, 'train/seg_cls_loss': 0.013372802734375, 'train/kl_loss': 0.3240234375, 'train/mask_bce_loss': 0.13591139032505453, 'train/mask_dice_loss': 0.5453534930944443, 'train/mask_loss': 0.6812648802995682, 'metrics/total_secs_per_batch': 6.506750583648682, 'metrics/data_secs_per_batch': 3.1883645057678223, '_timestamp': 1740977787.2831924}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 459 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977787.2834778}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 460 is less than current step: 499. Dropping entry: {'train/loss': 1.462385207414627, 'train/ce_loss': 0.3712158203125, 'train/seg_cls_loss': 0.01090087890625, 'train/kl_loss': 0.2169921875, 'train/mask_bce_loss': 0.09664603341370821, 'train/mask_dice_loss': 0.43546208888292315, 'train/mask_loss': 0.5321081221103668, 'metrics/total_secs_per_batch': 6.159412145614624, 'metrics/data_secs_per_batch': 2.702623891830444, '_timestamp': 1740977793.4428048}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 460 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977793.4430928}).
Epoch: [7][462/500]	Time  6.846 ( 6.846)	Loss 2.3257 (1.3428)	CeLoss 0.2197 (0.2777)	SegCLSLoss 0.0172 (0.0130)	KLLoss 0.3535 (0.2891)	MaskLoss 1.0310 (0.5148)	MaskBCELoss 0.0341 (0.0768)	MaskDICELoss 0.9969 (0.4381)
Epoch: [7][463/500]	Time  6.319 ( 6.319)	Loss 2.3844 (1.6370)	CeLoss 0.2139 (0.3750)	SegCLSLoss 0.0156 (0.0117)	KLLoss 0.3633 (0.2533)	MaskLoss 1.0633 (0.6154)	MaskBCELoss 0.0743 (0.0834)	MaskDICELoss 0.9890 (0.5320)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 461 is less than current step: 499. Dropping entry: {'train/loss': 1.3428361594676972, 'train/ce_loss': 0.277734375, 'train/seg_cls_loss': 0.0130126953125, 'train/kl_loss': 0.2890625, 'train/mask_bce_loss': 0.07676850007846951, 'train/mask_dice_loss': 0.43805778324604033, 'train/mask_loss': 0.5148262798786163, 'metrics/total_secs_per_batch': 6.846042633056641, 'metrics/data_secs_per_batch': 2.9651727199554445, '_timestamp': 1740977800.288863}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 461 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977800.2892375}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 462 is less than current step: 499. Dropping entry: {'train/loss': 1.6369908213615418, 'train/ce_loss': 0.374951171875, 'train/seg_cls_loss': 0.011712646484375, 'train/kl_loss': 0.2533203125, 'train/mask_bce_loss': 0.083425884693861, 'train/mask_dice_loss': 0.5320177674293518, 'train/mask_loss': 0.6154436409473419, 'metrics/total_secs_per_batch': 6.319077491760254, 'metrics/data_secs_per_batch': 2.645349311828613, '_timestamp': 1740977806.6079416}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 462 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977806.6082284}).
Epoch: [7][464/500]	Time  6.414 ( 6.414)	Loss 1.6134 (1.7017)	CeLoss 0.2256 (0.3021)	SegCLSLoss 0.0156 (0.0138)	KLLoss 0.3574 (0.3289)	MaskLoss 0.6719 (0.6797)	MaskBCELoss 0.0169 (0.1043)	MaskDICELoss 0.6551 (0.5754)
Epoch: [7][465/500]	Time  5.638 ( 5.638)	Loss 2.4835 (1.7205)	CeLoss 0.2480 (0.5082)	SegCLSLoss 0.0109 (0.0110)	KLLoss 0.3633 (0.2553)	MaskLoss 1.0962 (0.5906)	MaskBCELoss 0.3955 (0.1524)	MaskDICELoss 0.7007 (0.4382)
Epoch: [7][466/500]	Time  6.809 ( 6.809)	Loss 1.4109 (1.5119)	CeLoss 0.3047 (0.2652)	SegCLSLoss 0.0122 (0.0118)	KLLoss 0.3613 (0.2918)	MaskLoss 0.5316 (0.6059)	MaskBCELoss 0.3167 (0.1403)	MaskDICELoss 0.2149 (0.4656)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 463 is less than current step: 499. Dropping entry: {'train/loss': 1.70173636674881, 'train/ce_loss': 0.3021484375, 'train/seg_cls_loss': 0.01378173828125, 'train/kl_loss': 0.32890625, 'train/mask_bce_loss': 0.10431440048851073, 'train/mask_dice_loss': 0.5754112094640732, 'train/mask_loss': 0.6797256156802177, 'metrics/total_secs_per_batch': 6.414348363876343, 'metrics/data_secs_per_batch': 2.9754382371902466, '_timestamp': 1740977813.0225513}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 463 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977813.0229967}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 464 is less than current step: 499. Dropping entry: {'train/loss': 1.7204951524734498, 'train/ce_loss': 0.508203125, 'train/seg_cls_loss': 0.011029052734375, 'train/kl_loss': 0.2552734375, 'train/mask_bce_loss': 0.1523999072611332, 'train/mask_dice_loss': 0.43816993832588197, 'train/mask_loss': 0.5905698418617249, 'metrics/total_secs_per_batch': 5.638169765472412, 'metrics/data_secs_per_batch': 2.347973132133484, '_timestamp': 1740977818.6604733}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 464 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977818.6607652}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 465 is less than current step: 499. Dropping entry: {'train/loss': 1.5118733167648315, 'train/ce_loss': 0.265234375, 'train/seg_cls_loss': 0.011773681640625, 'train/kl_loss': 0.291796875, 'train/mask_bce_loss': 0.14033352490514517, 'train/mask_dice_loss': 0.4655543088912964, 'train/mask_loss': 0.6058878302574158, 'metrics/total_secs_per_batch': 6.809499979019165, 'metrics/data_secs_per_batch': 3.1910022497177124, '_timestamp': 1740977825.4699361}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 465 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977825.4702287}).
Epoch: [7][467/500]	Time  5.265 ( 5.265)	Loss 1.8222 (1.5919)	CeLoss 0.2178 (0.4381)	SegCLSLoss 0.0135 (0.0118)	KLLoss 0.3652 (0.2537)	MaskLoss 0.7802 (0.5613)	MaskBCELoss 0.3714 (0.1055)	MaskDICELoss 0.4089 (0.4558)
Epoch: [7][468/500]	Time  6.973 ( 6.973)	Loss 1.9738 (1.8612)	CeLoss 0.2402 (0.2075)	SegCLSLoss 0.0205 (0.0167)	KLLoss 0.3496 (0.3242)	MaskLoss 0.8443 (0.8064)	MaskBCELoss 0.0719 (0.1711)	MaskDICELoss 0.7725 (0.6353)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 466 is less than current step: 499. Dropping entry: {'train/loss': 1.591906201839447, 'train/ce_loss': 0.4380859375, 'train/seg_cls_loss': 0.011761474609375, 'train/kl_loss': 0.2537109375, 'train/mask_bce_loss': 0.10549213141202926, 'train/mask_dice_loss': 0.4557929962873459, 'train/mask_loss': 0.5612851321697235, 'metrics/total_secs_per_batch': 5.265099287033081, 'metrics/data_secs_per_batch': 2.414660382270813, '_timestamp': 1740977830.7351582}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 466 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977830.735574}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 467 is less than current step: 499. Dropping entry: {'train/loss': 1.8611940145492554, 'train/ce_loss': 0.20751953125, 'train/seg_cls_loss': 0.01671142578125, 'train/kl_loss': 0.32421875, 'train/mask_bce_loss': 0.17111544143408536, 'train/mask_dice_loss': 0.6353116303682327, 'train/mask_loss': 0.8064270734786987, 'metrics/total_secs_per_batch': 6.972823619842529, 'metrics/data_secs_per_batch': 3.1483312606811524, '_timestamp': 1740977837.7079966}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 467 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977837.7083397}).
Epoch: [7][469/500]	Time  6.296 ( 6.296)	Loss 2.1494 (1.9202)	CeLoss 0.2139 (0.3161)	SegCLSLoss 0.0177 (0.0164)	KLLoss 0.3535 (0.3227)	MaskLoss 0.9458 (0.7818)	MaskBCELoss 0.0134 (0.1045)	MaskDICELoss 0.9323 (0.6773)
[2025-03-02 22:57:30,682] [INFO] [logging.py:96:log_dist] [Rank 0] step=3970, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:57:30,688] [INFO] [timer.py:215:stop] epoch=0/micro_step=39700/global_step=3970, RunningAvgSamplesPerSec=1.4732387122045298, CurrSamplesPerSec=1.4963137838683236, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [7][470/500]	Time  6.685 ( 6.685)	Loss 1.1730 (1.7359)	CeLoss 0.2393 (0.2265)	SegCLSLoss 0.0095 (0.0187)	KLLoss 0.3711 (0.3615)	MaskLoss 0.4459 (0.7319)	MaskBCELoss 0.0723 (0.0846)	MaskDICELoss 0.3736 (0.6473)
Epoch: [7][471/500]	Time  5.694 ( 5.694)	Loss 0.0986 (1.5243)	CeLoss 0.0986 (0.3287)	SegCLSLoss 0.0000 (0.0099)	KLLoss 0.0000 (0.2541)	MaskLoss 0.0000 (0.5824)	MaskBCELoss 0.0000 (0.1149)	MaskDICELoss 0.0000 (0.4675)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 468 is less than current step: 499. Dropping entry: {'train/loss': 1.9201505064964295, 'train/ce_loss': 0.31611328125, 'train/seg_cls_loss': 0.016412353515625, 'train/kl_loss': 0.32265625, 'train/mask_bce_loss': 0.10446089217439294, 'train/mask_dice_loss': 0.6772940456867218, 'train/mask_loss': 0.7817549407482147, 'metrics/total_secs_per_batch': 6.296316623687744, 'metrics/data_secs_per_batch': 2.7887890577316283, '_timestamp': 1740977844.004227}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 468 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977844.0045285}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 469 is less than current step: 499. Dropping entry: {'train/loss': 1.735942041873932, 'train/ce_loss': 0.22646484375, 'train/seg_cls_loss': 0.018670654296875, 'train/kl_loss': 0.3615234375, 'train/mask_bce_loss': 0.08464251114055514, 'train/mask_dice_loss': 0.6472933411598205, 'train/mask_loss': 0.7319358497858047, 'metrics/total_secs_per_batch': 6.68469500541687, 'metrics/data_secs_per_batch': 3.1068403244018556, '_timestamp': 1740977850.688713}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 469 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977850.688991}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 470 is less than current step: 499. Dropping entry: {'train/loss': 1.5243457794189452, 'train/ce_loss': 0.3287109375, 'train/seg_cls_loss': 0.009918212890625, 'train/kl_loss': 0.2541015625, 'train/mask_bce_loss': 0.11491811573505402, 'train/mask_dice_loss': 0.4675184369087219, 'train/mask_loss': 0.5824365556240082, 'metrics/total_secs_per_batch': 5.694070339202881, 'metrics/data_secs_per_batch': 2.3616568088531493, '_timestamp': 1740977856.3830519}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 470 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977856.3833616}).
Epoch: [7][472/500]	Time  6.077 ( 6.077)	Loss 1.0880 (1.7654)	CeLoss 0.2930 (0.2900)	SegCLSLoss 0.0134 (0.0137)	KLLoss 0.3613 (0.2924)	MaskLoss 0.3760 (0.7196)	MaskBCELoss 0.0703 (0.1334)	MaskDICELoss 0.3057 (0.5862)
Epoch: [7][473/500]	Time  7.269 ( 7.269)	Loss 1.5518 (1.8958)	CeLoss 0.3203 (0.2458)	SegCLSLoss 0.0105 (0.0160)	KLLoss 0.3652 (0.3678)	MaskLoss 0.5943 (0.8026)	MaskBCELoss 0.0287 (0.2217)	MaskDICELoss 0.5655 (0.5809)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 471 is less than current step: 499. Dropping entry: {'train/loss': 1.7654385447502137, 'train/ce_loss': 0.289990234375, 'train/seg_cls_loss': 0.013739013671875, 'train/kl_loss': 0.2923828125, 'train/mask_bce_loss': 0.1333750242367387, 'train/mask_dice_loss': 0.5861850678920746, 'train/mask_loss': 0.7195600986480712, 'metrics/total_secs_per_batch': 6.0767982006073, 'metrics/data_secs_per_batch': 2.7469063282012938, '_timestamp': 1740977862.4599097}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 471 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977862.460303}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 472 is less than current step: 499. Dropping entry: {'train/loss': 1.895784741640091, 'train/ce_loss': 0.24580078125, 'train/seg_cls_loss': 0.015997314453125, 'train/kl_loss': 0.3677734375, 'train/mask_bce_loss': 0.22173551423475146, 'train/mask_dice_loss': 0.5808931715786457, 'train/mask_loss': 0.8026286944746971, 'metrics/total_secs_per_batch': 7.268913269042969, 'metrics/data_secs_per_batch': 3.5314319849014284, '_timestamp': 1740977869.7286913}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 472 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977869.728983}).
Epoch: [7][474/500]	Time  6.004 ( 6.004)	Loss 2.2918 (1.8708)	CeLoss 0.1816 (0.1954)	SegCLSLoss 0.0320 (0.0173)	KLLoss 0.3594 (0.3295)	MaskLoss 1.0292 (0.8169)	MaskBCELoss 0.0293 (0.1813)	MaskDICELoss 0.9999 (0.6356)
Epoch: [7][475/500]	Time  6.397 ( 6.397)	Loss 1.1132 (1.8980)	CeLoss 0.2109 (0.2370)	SegCLSLoss 0.0194 (0.0151)	KLLoss 0.3555 (0.3295)	MaskLoss 0.4287 (0.8104)	MaskBCELoss 0.0217 (0.2173)	MaskDICELoss 0.4070 (0.5931)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 473 is less than current step: 499. Dropping entry: {'train/loss': 1.870830738544464, 'train/ce_loss': 0.19541015625, 'train/seg_cls_loss': 0.017266845703125, 'train/kl_loss': 0.3294921875, 'train/mask_bce_loss': 0.18128258772194386, 'train/mask_dice_loss': 0.6355780899524689, 'train/mask_loss': 0.8168606787919999, 'metrics/total_secs_per_batch': 6.004246473312378, 'metrics/data_secs_per_batch': 2.515496850013733, '_timestamp': 1740977875.7331362}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 473 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977875.7334871}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 474 is less than current step: 499. Dropping entry: {'train/loss': 1.8980453372001649, 'train/ce_loss': 0.23701171875, 'train/seg_cls_loss': 0.0150634765625, 'train/kl_loss': 0.3294921875, 'train/mask_bce_loss': 0.21729893172159792, 'train/mask_dice_loss': 0.5931495055556297, 'train/mask_loss': 0.8104484438896179, 'metrics/total_secs_per_batch': 6.396784543991089, 'metrics/data_secs_per_batch': 2.8265597581863404, '_timestamp': 1740977882.1297207}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 474 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977882.1300132}).
Epoch: [7][476/500]	Time  5.530 ( 5.530)	Loss 0.0630 (1.4585)	CeLoss 0.0630 (0.4460)	SegCLSLoss 0.0000 (0.0091)	KLLoss 0.0000 (0.2172)	MaskLoss 0.0000 (0.4930)	MaskBCELoss 0.0000 (0.0748)	MaskDICELoss 0.0000 (0.4182)
Epoch: [7][477/500]	Time  5.492 ( 5.492)	Loss 1.9958 (1.9916)	CeLoss 0.2969 (0.4271)	SegCLSLoss 0.0146 (0.0156)	KLLoss 0.3613 (0.2941)	MaskLoss 0.8280 (0.7636)	MaskBCELoss 0.0268 (0.2027)	MaskDICELoss 0.8012 (0.5609)
Epoch: [7][478/500]	Time  6.755 ( 6.755)	Loss 2.4537 (1.7089)	CeLoss 0.2715 (0.2402)	SegCLSLoss 0.0206 (0.0173)	KLLoss 0.3594 (0.3611)	MaskLoss 1.0687 (0.7119)	MaskBCELoss 0.0926 (0.0932)	MaskDICELoss 0.9761 (0.6187)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 475 is less than current step: 499. Dropping entry: {'train/loss': 1.4584680676460267, 'train/ce_loss': 0.446044921875, 'train/seg_cls_loss': 0.00909423828125, 'train/kl_loss': 0.2171875, 'train/mask_bce_loss': 0.07479323204606772, 'train/mask_dice_loss': 0.41818591952323914, 'train/mask_loss': 0.4929791480302811, 'metrics/total_secs_per_batch': 5.530150651931763, 'metrics/data_secs_per_batch': 2.2149250745773315, '_timestamp': 1740977887.6598804}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 475 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977887.660166}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 476 is less than current step: 499. Dropping entry: {'train/loss': 1.9916395604610444, 'train/ce_loss': 0.4271484375, 'train/seg_cls_loss': 0.0156005859375, 'train/kl_loss': 0.294140625, 'train/mask_bce_loss': 0.20269809793680907, 'train/mask_dice_loss': 0.5608951032161713, 'train/mask_loss': 0.7635932177305221, 'metrics/total_secs_per_batch': 5.49157977104187, 'metrics/data_secs_per_batch': 2.4882016658782957, '_timestamp': 1740977893.1517327}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 476 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977893.152095}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 477 is less than current step: 499. Dropping entry: {'train/loss': 1.7088704824447631, 'train/ce_loss': 0.240234375, 'train/seg_cls_loss': 0.017340087890625, 'train/kl_loss': 0.3611328125, 'train/mask_bce_loss': 0.09315324891358615, 'train/mask_dice_loss': 0.6187038451433182, 'train/mask_loss': 0.7118570953607559, 'metrics/total_secs_per_batch': 6.754878520965576, 'metrics/data_secs_per_batch': 3.0598405599594116, '_timestamp': 1740977899.906368}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 477 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977899.9066458}).
Epoch: [7][479/500]	Time  6.700 ( 6.700)	Loss 2.2148 (2.0851)	CeLoss 0.1992 (0.3812)	SegCLSLoss 0.0192 (0.0152)	KLLoss 0.3555 (0.3246)	MaskLoss 0.9853 (0.8320)	MaskBCELoss 0.0690 (0.1449)	MaskDICELoss 0.9163 (0.6870)
[2025-03-02 22:58:33,074] [INFO] [logging.py:96:log_dist] [Rank 0] step=3980, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:58:33,080] [INFO] [timer.py:215:stop] epoch=0/micro_step=39800/global_step=3980, RunningAvgSamplesPerSec=1.4735388041665496, CurrSamplesPerSec=1.5448636931558806, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [7][480/500]	Time  6.475 ( 6.475)	Loss 1.6739 (1.6977)	CeLoss 0.3223 (0.4030)	SegCLSLoss 0.0101 (0.0145)	KLLoss 0.3594 (0.3279)	MaskLoss 0.6553 (0.6274)	MaskBCELoss 0.1629 (0.1511)	MaskDICELoss 0.4924 (0.4763)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 478 is less than current step: 499. Dropping entry: {'train/loss': 2.0850986003875733, 'train/ce_loss': 0.38115234375, 'train/seg_cls_loss': 0.015228271484375, 'train/kl_loss': 0.324609375, 'train/mask_bce_loss': 0.14492436880245804, 'train/mask_dice_loss': 0.6870292246341705, 'train/mask_loss': 0.8319535851478577, 'metrics/total_secs_per_batch': 6.699507713317871, 'metrics/data_secs_per_batch': 2.8959485292434692, '_timestamp': 1740977906.6059232}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 478 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977906.6062517}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 479 is less than current step: 499. Dropping entry: {'train/loss': 1.6976881206035614, 'train/ce_loss': 0.40302734375, 'train/seg_cls_loss': 0.014520263671875, 'train/kl_loss': 0.3279296875, 'train/mask_bce_loss': 0.15110343974083662, 'train/mask_dice_loss': 0.4763050630688667, 'train/mask_loss': 0.6274085074663163, 'metrics/total_secs_per_batch': 6.474884986877441, 'metrics/data_secs_per_batch': 2.979803037643433, '_timestamp': 1740977913.0805979}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 479 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977913.080955}).
Epoch: [7][481/500]	Time  6.079 ( 6.079)	Loss 2.4931 (1.4858)	CeLoss 0.1670 (0.3847)	SegCLSLoss 0.0215 (0.0112)	KLLoss 0.3711 (0.2551)	MaskLoss 1.1391 (0.5349)	MaskBCELoss 0.2863 (0.0646)	MaskDICELoss 0.8528 (0.4703)
Epoch: [7][482/500]	Time  6.245 ( 6.245)	Loss 2.2056 (1.7755)	CeLoss 0.2168 (0.4319)	SegCLSLoss 0.0245 (0.0150)	KLLoss 0.3555 (0.2877)	MaskLoss 0.9700 (0.6534)	MaskBCELoss 0.0433 (0.0673)	MaskDICELoss 0.9267 (0.5861)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 480 is less than current step: 499. Dropping entry: {'train/loss': 1.485802948474884, 'train/ce_loss': 0.38466796875, 'train/seg_cls_loss': 0.01124267578125, 'train/kl_loss': 0.255078125, 'train/mask_bce_loss': 0.06455259076319635, 'train/mask_dice_loss': 0.4703410714864731, 'train/mask_loss': 0.534893661737442, 'metrics/total_secs_per_batch': 6.078527927398682, 'metrics/data_secs_per_batch': 2.832011604309082, '_timestamp': 1740977919.1594374}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 480 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977919.1597872}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 481 is less than current step: 499. Dropping entry: {'train/loss': 1.7754853248596192, 'train/ce_loss': 0.43193359375, 'train/seg_cls_loss': 0.01502685546875, 'train/kl_loss': 0.2876953125, 'train/mask_bce_loss': 0.06726935934275388, 'train/mask_dice_loss': 0.5861471354961395, 'train/mask_loss': 0.6534164905548095, 'metrics/total_secs_per_batch': 6.245002746582031, 'metrics/data_secs_per_batch': 2.5838034629821776, '_timestamp': 1740977925.4042916}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 481 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977925.404592}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 482 is less than current step: 499. Dropping entry: {'train/loss': 1.8273208737373352, 'train/ce_loss': 0.52099609375, 'train/seg_cls_loss': 0.0140380859375, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.15461217984557152, 'train/mask_dice_loss': 0.4822904348373413, 'train/mask_loss': 0.6369026243686676, 'metrics/total_secs_per_batch': 5.949543237686157, 'metrics/data_secs_per_batch': 2.6101378440856933, '_timestamp': 1740977931.354114}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 482 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977931.3544877}).
Epoch: [7][483/500]	Time  5.950 ( 5.950)	Loss 1.8103 (1.8273)	CeLoss 0.1562 (0.5210)	SegCLSLoss 0.0221 (0.0140)	KLLoss 0.3633 (0.2547)	MaskLoss 0.8036 (0.6369)	MaskBCELoss 0.0879 (0.1546)	MaskDICELoss 0.7157 (0.4823)
Epoch: [7][484/500]	Time  6.888 ( 6.888)	Loss 0.0669 (1.2195)	CeLoss 0.0669 (0.1823)	SegCLSLoss 0.0000 (0.0106)	KLLoss 0.0000 (0.2555)	MaskLoss 0.0000 (0.5032)	MaskBCELoss 0.0000 (0.1025)	MaskDICELoss 0.0000 (0.4007)
Epoch: [7][485/500]	Time  5.387 ( 5.387)	Loss 1.1406 (1.8965)	CeLoss 1.1406 (0.4397)	SegCLSLoss 0.0000 (0.0137)	KLLoss 0.0000 (0.2561)	MaskLoss 0.0000 (0.7122)	MaskBCELoss 0.0000 (0.1698)	MaskDICELoss 0.0000 (0.5424)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 483 is less than current step: 499. Dropping entry: {'train/loss': 1.2195252418518066, 'train/ce_loss': 0.1822509765625, 'train/seg_cls_loss': 0.010589599609375, 'train/kl_loss': 0.25546875, 'train/mask_bce_loss': 0.10253164414316415, 'train/mask_dice_loss': 0.4006757974624634, 'train/mask_loss': 0.5032074391841889, 'metrics/total_secs_per_batch': 6.887832403182983, 'metrics/data_secs_per_batch': 3.1293763637542726, '_timestamp': 1740977938.241667}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 483 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977938.2419581}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 484 is less than current step: 499. Dropping entry: {'train/loss': 1.8965081572532654, 'train/ce_loss': 0.43974609375, 'train/seg_cls_loss': 0.01368408203125, 'train/kl_loss': 0.2560546875, 'train/mask_bce_loss': 0.16978661394678057, 'train/mask_dice_loss': 0.5424323141574859, 'train/mask_loss': 0.7122189342975617, 'metrics/total_secs_per_batch': 5.38710618019104, 'metrics/data_secs_per_batch': 2.4140358686447145, '_timestamp': 1740977943.6287854}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 484 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977943.629059}).
Epoch: [7][486/500]	Time  6.294 ( 6.294)	Loss 1.4744 (1.7825)	CeLoss 0.2539 (0.2909)	SegCLSLoss 0.0209 (0.0180)	KLLoss 0.3672 (0.3285)	MaskLoss 0.5868 (0.7250)	MaskBCELoss 0.0282 (0.1953)	MaskDICELoss 0.5587 (0.5297)
Epoch: [7][487/500]	Time  5.883 ( 5.883)	Loss 2.4810 (1.4323)	CeLoss 0.1484 (0.1896)	SegCLSLoss 0.0212 (0.0145)	KLLoss 0.3691 (0.2906)	MaskLoss 1.1424 (0.6032)	MaskBCELoss 0.2589 (0.1495)	MaskDICELoss 0.8834 (0.4537)
Epoch: [7][488/500]	Time  5.278 ( 5.278)	Loss 2.6800 (1.6280)	CeLoss 0.1562 (0.6421)	SegCLSLoss 0.0231 (0.0110)	KLLoss 0.3848 (0.2207)	MaskLoss 1.2365 (0.4791)	MaskBCELoss 0.3590 (0.0618)	MaskDICELoss 0.8775 (0.4173)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 485 is less than current step: 499. Dropping entry: {'train/loss': 1.7825327217578888, 'train/ce_loss': 0.290869140625, 'train/seg_cls_loss': 0.018035888671875, 'train/kl_loss': 0.328515625, 'train/mask_bce_loss': 0.19530764142982662, 'train/mask_dice_loss': 0.5296501278877258, 'train/mask_loss': 0.7249577671289444, 'metrics/total_secs_per_batch': 6.294111013412476, 'metrics/data_secs_per_batch': 2.6926968336105346, '_timestamp': 1740977949.9230392}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 485 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977949.923394}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 486 is less than current step: 499. Dropping entry: {'train/loss': 1.4322510719299317, 'train/ce_loss': 0.18955078125, 'train/seg_cls_loss': 0.01446533203125, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.14949354799464346, 'train/mask_dice_loss': 0.4537413567304611, 'train/mask_loss': 0.6032349161803723, 'metrics/total_secs_per_batch': 5.882670640945435, 'metrics/data_secs_per_batch': 2.59850914478302, '_timestamp': 1740977955.8055537}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 486 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977955.805899}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 487 is less than current step: 499. Dropping entry: {'train/loss': 1.627990484237671, 'train/ce_loss': 0.64208984375, 'train/seg_cls_loss': 0.011004638671875, 'train/kl_loss': 0.220703125, 'train/mask_bce_loss': 0.06182475397363305, 'train/mask_dice_loss': 0.4172583818435669, 'train/mask_loss': 0.47908313274383546, 'metrics/total_secs_per_batch': 5.277963876724243, 'metrics/data_secs_per_batch': 2.2963826656341553, '_timestamp': 1740977961.0835483}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 487 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977961.0839057}).
Epoch: [7][489/500]	Time  5.255 ( 5.255)	Loss 2.5935 (1.4799)	CeLoss 0.1992 (0.5188)	SegCLSLoss 0.0154 (0.0097)	KLLoss 0.3750 (0.1826)	MaskLoss 1.1747 (0.4691)	MaskBCELoss 0.2944 (0.0919)	MaskDICELoss 0.8802 (0.3772)
[2025-03-02 22:59:31,349] [INFO] [logging.py:96:log_dist] [Rank 0] step=3990, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 22:59:31,355] [INFO] [timer.py:215:stop] epoch=0/micro_step=39900/global_step=3990, RunningAvgSamplesPerSec=1.47406176575441, CurrSamplesPerSec=1.9935401433001467, MemAllocated=31.4GB, MaxMemAllocated=37.23GB
Epoch: [7][490/500]	Time  5.018 ( 5.018)	Loss 1.6728 (1.5185)	CeLoss 0.2158 (0.6698)	SegCLSLoss 0.0266 (0.0086)	KLLoss 0.3574 (0.1797)	MaskLoss 0.7036 (0.4132)	MaskBCELoss 0.0359 (0.0849)	MaskDICELoss 0.6677 (0.3282)
Epoch: [7][491/500]	Time  5.502 ( 5.502)	Loss 2.7109 (1.5491)	CeLoss 0.2305 (0.4111)	SegCLSLoss 0.0206 (0.0118)	KLLoss 0.3750 (0.2588)	MaskLoss 1.2158 (0.5531)	MaskBCELoss 0.2514 (0.1078)	MaskDICELoss 0.9644 (0.4453)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 488 is less than current step: 499. Dropping entry: {'train/loss': 1.4799089670181274, 'train/ce_loss': 0.518798828125, 'train/seg_cls_loss': 0.009716796875, 'train/kl_loss': 0.1826171875, 'train/mask_bce_loss': 0.09193540345877409, 'train/mask_dice_loss': 0.37719388008117677, 'train/mask_loss': 0.46912928223609923, 'metrics/total_secs_per_batch': 5.2547197341918945, 'metrics/data_secs_per_batch': 2.0731697559356688, '_timestamp': 1740977966.3382885}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 488 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977966.3385847}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 489 is less than current step: 499. Dropping entry: {'train/loss': 1.518509316444397, 'train/ce_loss': 0.66982421875, 'train/seg_cls_loss': 0.008587646484375, 'train/kl_loss': 0.1796875, 'train/mask_bce_loss': 0.08494403883814812, 'train/mask_dice_loss': 0.32821686267852784, 'train/mask_loss': 0.4131609082221985, 'metrics/total_secs_per_batch': 5.017842531204224, 'metrics/data_secs_per_batch': 2.0895493984222413, '_timestamp': 1740977971.355995}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 489 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977971.356336}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 490 is less than current step: 499. Dropping entry: {'train/loss': 1.5491347312927246, 'train/ce_loss': 0.411083984375, 'train/seg_cls_loss': 0.011822509765625, 'train/kl_loss': 0.2587890625, 'train/mask_bce_loss': 0.10779695063829423, 'train/mask_dice_loss': 0.44526161551475524, 'train/mask_loss': 0.5530585706233978, 'metrics/total_secs_per_batch': 5.501725196838379, 'metrics/data_secs_per_batch': 2.8400620222091675, '_timestamp': 1740977976.8578458}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 490 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977976.8581727}).
Epoch: [7][492/500]	Time  5.965 ( 5.965)	Loss 1.5742 (1.7619)	CeLoss 0.1914 (0.3573)	SegCLSLoss 0.0136 (0.0157)	KLLoss 0.3633 (0.2904)	MaskLoss 0.6699 (0.6838)	MaskBCELoss 0.1430 (0.1281)	MaskDICELoss 0.5269 (0.5557)
Epoch: [7][493/500]	Time  6.404 ( 6.404)	Loss 0.1406 (1.7509)	CeLoss 0.1406 (0.3636)	SegCLSLoss 0.0000 (0.0121)	KLLoss 0.0000 (0.2572)	MaskLoss 0.0000 (0.6777)	MaskBCELoss 0.0000 (0.1443)	MaskDICELoss 0.0000 (0.5334)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 491 is less than current step: 499. Dropping entry: {'train/loss': 1.761887264251709, 'train/ce_loss': 0.357275390625, 'train/seg_cls_loss': 0.01573486328125, 'train/kl_loss': 0.2904296875, 'train/mask_bce_loss': 0.12807700429111718, 'train/mask_dice_loss': 0.5556986600160598, 'train/mask_loss': 0.6837756663560868, 'metrics/total_secs_per_batch': 5.965188026428223, 'metrics/data_secs_per_batch': 2.834142065048218, '_timestamp': 1740977982.8230343}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 491 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977982.823386}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 492 is less than current step: 499. Dropping entry: {'train/loss': 1.750885808467865, 'train/ce_loss': 0.36357421875, 'train/seg_cls_loss': 0.01212158203125, 'train/kl_loss': 0.2572265625, 'train/mask_bce_loss': 0.14429101385176182, 'train/mask_dice_loss': 0.5334467947483063, 'train/mask_loss': 0.6777378201484681, 'metrics/total_secs_per_batch': 6.4038543701171875, 'metrics/data_secs_per_batch': 2.727412533760071, '_timestamp': 1740977989.226904}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 492 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977989.2271905}).
Epoch: [7][494/500]	Time  7.021 ( 7.021)	Loss 2.4987 (2.0183)	CeLoss 0.1875 (0.2034)	SegCLSLoss 0.0172 (0.0202)	KLLoss 0.3555 (0.3629)	MaskLoss 1.1331 (0.8843)	MaskBCELoss 0.2743 (0.2220)	MaskDICELoss 0.8588 (0.6623)
Epoch: [7][495/500]	Time  6.979 ( 6.979)	Loss 1.5695 (1.5260)	CeLoss 0.1904 (0.2132)	SegCLSLoss 0.0275 (0.0161)	KLLoss 0.3516 (0.2865)	MaskLoss 0.6651 (0.6380)	MaskBCELoss 0.0694 (0.0702)	MaskDICELoss 0.5957 (0.5678)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 493 is less than current step: 499. Dropping entry: {'train/loss': 2.0182673215866087, 'train/ce_loss': 0.20341796875, 'train/seg_cls_loss': 0.02015380859375, 'train/kl_loss': 0.362890625, 'train/mask_bce_loss': 0.22203422263264655, 'train/mask_dice_loss': 0.6622947484254837, 'train/mask_loss': 0.8843289732933044, 'metrics/total_secs_per_batch': 7.020528078079224, 'metrics/data_secs_per_batch': 2.9107666015625, '_timestamp': 1740977996.2474453}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 493 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740977996.247643}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 494 is less than current step: 499. Dropping entry: {'train/loss': 1.5260050415992736, 'train/ce_loss': 0.21318359375, 'train/seg_cls_loss': 0.01607666015625, 'train/kl_loss': 0.2865234375, 'train/mask_bce_loss': 0.07016256228089332, 'train/mask_dice_loss': 0.5678399562835693, 'train/mask_loss': 0.6380025088787079, 'metrics/total_secs_per_batch': 6.979281663894653, 'metrics/data_secs_per_batch': 3.1659110307693483, '_timestamp': 1740978003.2267098}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 494 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978003.2269983}).
Epoch: [7][496/500]	Time  6.317 ( 6.317)	Loss 1.6746 (1.6405)	CeLoss 0.2129 (0.3328)	SegCLSLoss 0.0219 (0.0124)	KLLoss 0.3535 (0.2535)	MaskLoss 0.7074 (0.6381)	MaskBCELoss 0.0245 (0.1117)	MaskDICELoss 0.6829 (0.5264)
Epoch: [7][497/500]	Time  6.668 ( 6.668)	Loss 2.3142 (1.7634)	CeLoss 0.1406 (0.2180)	SegCLSLoss 0.0376 (0.0157)	KLLoss 0.3711 (0.3299)	MaskLoss 1.0590 (0.7522)	MaskBCELoss 0.2610 (0.1463)	MaskDICELoss 0.7980 (0.6059)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 495 is less than current step: 499. Dropping entry: {'train/loss': 1.6405431747436523, 'train/ce_loss': 0.3328125, 'train/seg_cls_loss': 0.012432861328125, 'train/kl_loss': 0.253515625, 'train/mask_bce_loss': 0.11169070247560739, 'train/mask_dice_loss': 0.526403135061264, 'train/mask_loss': 0.6380938351154327, 'metrics/total_secs_per_batch': 6.316523790359497, 'metrics/data_secs_per_batch': 2.941326546669006, '_timestamp': 1740978009.5434628}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 495 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978009.5438116}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 496 is less than current step: 499. Dropping entry: {'train/loss': 1.763444709777832, 'train/ce_loss': 0.218017578125, 'train/seg_cls_loss': 0.015716552734375, 'train/kl_loss': 0.3298828125, 'train/mask_bce_loss': 0.1462968222796917, 'train/mask_dice_loss': 0.6058600962162017, 'train/mask_loss': 0.7521569162607193, 'metrics/total_secs_per_batch': 6.66815710067749, 'metrics/data_secs_per_batch': 3.185431718826294, '_timestamp': 1740978016.2114754}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 496 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978016.211788}).
Epoch: [7][498/500]	Time  6.291 ( 6.291)	Loss 2.1261 (1.8849)	CeLoss 0.1719 (0.3774)	SegCLSLoss 0.0186 (0.0150)	KLLoss 0.3633 (0.3242)	MaskLoss 0.9542 (0.7336)	MaskBCELoss 0.0340 (0.0923)	MaskDICELoss 0.9202 (0.6413)
Epoch: [7][499/500]	Time  5.475 ( 5.475)	Loss 0.6801 (1.2612)	CeLoss 0.2334 (0.3080)	SegCLSLoss 0.0099 (0.0101)	KLLoss 0.3672 (0.2541)	MaskLoss 0.2024 (0.4613)	MaskBCELoss 0.0517 (0.0768)	MaskDICELoss 0.1507 (0.3845)
  0%|                                                                                                                                                              | 0/200 [00:00<?, ?it/s]
[2025-03-02 23:00:34,669] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:00:34,675] [INFO] [timer.py:215:stop] epoch=0/micro_step=40000/global_step=4000, RunningAvgSamplesPerSec=1.4743081400814568, CurrSamplesPerSec=1.4930745619929, MemAllocated=30.81GB, MaxMemAllocated=37.23GB

  4%|██████▊                                                                                                                                               | 9/200 [00:02<00:28,  6.60it/s][34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 497 is less than current step: 499. Dropping entry: {'train/loss': 1.884882640838623, 'train/ce_loss': 0.37744140625, 'train/seg_cls_loss': 0.015020751953125, 'train/kl_loss': 0.32421875, 'train/mask_bce_loss': 0.09228522991761565, 'train/mask_dice_loss': 0.6413181960582733, 'train/mask_loss': 0.7336034119129181, 'metrics/total_secs_per_batch': 6.2909324169158936, 'metrics/data_secs_per_batch': 2.7776515245437623, '_timestamp': 1740978022.5023222}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 497 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978022.502594}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 498 is less than current step: 499. Dropping entry: {'train/loss': 1.2611941158771516, 'train/ce_loss': 0.307958984375, 'train/seg_cls_loss': 0.010125732421875, 'train/kl_loss': 0.2541015625, 'train/mask_bce_loss': 0.07676207944750786, 'train/mask_dice_loss': 0.38452346324920655, 'train/mask_loss': 0.4612855359911919, 'metrics/total_secs_per_batch': 5.474532842636108, 'metrics/data_secs_per_batch': 2.4958702325820923, '_timestamp': 1740978027.976889}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 498 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978027.9771907}).













100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:29<00:00,  6.85it/s]
giou: 0.1879, ciou: 0.1759
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 7 is less than current step: 499. Dropping entry: {'val/giou': 0.1878635585308075, 'val/ciou': 0.17587274312973022, '_timestamp': 1740978063.9035106}).
Epoch: [8][  1/500]	Time  6.553 ( 6.553)	Loss 3.3939 (2.0224)	CeLoss 0.2520 (0.2879)	SegCLSLoss 0.0182 (0.0168)	KLLoss 0.3633 (0.2941)	MaskLoss 1.5475 (0.8483)	MaskBCELoss 0.8483 (0.2076)	MaskDICELoss 0.6992 (0.6407)
Epoch: [8][  2/500]	Time  6.706 ( 6.706)	Loss 1.7627 (1.8752)	CeLoss 0.2129 (0.3092)	SegCLSLoss 0.0205 (0.0164)	KLLoss 0.3555 (0.2895)	MaskLoss 0.7524 (0.7645)	MaskBCELoss 0.0852 (0.1140)	MaskDICELoss 0.6673 (0.6505)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 499. Dropping entry: {'train/loss': 2.022368848323822, 'train/ce_loss': 0.2878662109375, 'train/seg_cls_loss': 0.016796875, 'train/kl_loss': 0.294140625, 'train/mask_bce_loss': 0.2075724832713604, 'train/mask_dice_loss': 0.640684711933136, 'train/mask_loss': 0.8482571840286255, 'metrics/total_secs_per_batch': 6.5528059005737305, 'metrics/data_secs_per_batch': 3.054011344909668, '_timestamp': 1740978070.4637024}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 1 is less than current step: 499. Dropping entry: {'train/loss': 1.8752374291419982, 'train/ce_loss': 0.3091796875, 'train/seg_cls_loss': 0.01636962890625, 'train/kl_loss': 0.289453125, 'train/mask_bce_loss': 0.11399287395179272, 'train/mask_dice_loss': 0.6505301415920257, 'train/mask_loss': 0.7645230174064637, 'metrics/total_secs_per_batch': 6.705805063247681, 'metrics/data_secs_per_batch': 3.0942704916000365, '_timestamp': 1740978077.1697767}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 1 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978077.170175}).
Epoch: [8][  3/500]	Time  5.908 ( 5.908)	Loss 1.4609 (1.9846)	CeLoss 1.4609 (0.4553)	SegCLSLoss 0.0000 (0.0129)	KLLoss 0.0000 (0.2898)	MaskLoss 0.0000 (0.7469)	MaskBCELoss 0.0000 (0.2148)	MaskDICELoss 0.0000 (0.5321)
Epoch: [8][  4/500]	Time  6.200 ( 6.200)	Loss 1.6715 (1.6904)	CeLoss 0.2695 (0.3968)	SegCLSLoss 0.0119 (0.0136)	KLLoss 0.3633 (0.2920)	MaskLoss 0.6795 (0.6286)	MaskBCELoss 0.0538 (0.0653)	MaskDICELoss 0.6257 (0.5633)
Epoch: [8][  5/500]	Time  6.665 ( 6.665)	Loss 2.2817 (1.5006)	CeLoss 0.3496 (0.3509)	SegCLSLoss 0.0157 (0.0110)	KLLoss 0.3594 (0.2531)	MaskLoss 0.9446 (0.5594)	MaskBCELoss 0.0162 (0.0842)	MaskDICELoss 0.9284 (0.4752)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 2 is less than current step: 499. Dropping entry: {'train/loss': 1.9845763921737671, 'train/ce_loss': 0.4552734375, 'train/seg_cls_loss': 0.0128662109375, 'train/kl_loss': 0.28984375, 'train/mask_bce_loss': 0.21478458382189275, 'train/mask_dice_loss': 0.5320934444665909, 'train/mask_loss': 0.746878033876419, 'metrics/total_secs_per_batch': 5.908196687698364, 'metrics/data_secs_per_batch': 2.668357491493225, '_timestamp': 1740978083.0776982}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 2 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978083.077987}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 3 is less than current step: 499. Dropping entry: {'train/loss': 1.6904321312904358, 'train/ce_loss': 0.39677734375, 'train/seg_cls_loss': 0.01363525390625, 'train/kl_loss': 0.2919921875, 'train/mask_bce_loss': 0.06530339114833623, 'train/mask_dice_loss': 0.563311105966568, 'train/mask_loss': 0.6286144971847534, 'metrics/total_secs_per_batch': 6.199657440185547, 'metrics/data_secs_per_batch': 2.5745911836624145, '_timestamp': 1740978089.2775848}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 3 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978089.2779596}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 4 is less than current step: 499. Dropping entry: {'train/loss': 1.5005639314651489, 'train/ce_loss': 0.350927734375, 'train/seg_cls_loss': 0.010986328125, 'train/kl_loss': 0.253125, 'train/mask_bce_loss': 0.08419291190803051, 'train/mask_dice_loss': 0.4752443164587021, 'train/mask_loss': 0.55943723320961, 'metrics/total_secs_per_batch': 6.664573907852173, 'metrics/data_secs_per_batch': 3.04749276638031, '_timestamp': 1740978095.9421976}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 4 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978095.9425743}).
Epoch: [8][  6/500]	Time  4.888 ( 4.888)	Loss 1.1016 (1.5551)	CeLoss 1.1016 (0.6534)	SegCLSLoss 0.0000 (0.0096)	KLLoss 0.0000 (0.2188)	MaskLoss 0.0000 (0.4375)	MaskBCELoss 0.0000 (0.1010)	MaskDICELoss 0.0000 (0.3365)
Epoch: [8][  7/500]	Time  5.801 ( 5.801)	Loss 0.8008 (1.7345)	CeLoss 0.8008 (0.3773)	SegCLSLoss 0.0000 (0.0123)	KLLoss 0.0000 (0.2904)	MaskLoss 0.0000 (0.6609)	MaskBCELoss 0.0000 (0.0817)	MaskDICELoss 0.0000 (0.5792)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 5 is less than current step: 499. Dropping entry: {'train/loss': 1.5551477730274201, 'train/ce_loss': 0.65341796875, 'train/seg_cls_loss': 0.009600830078125, 'train/kl_loss': 0.21875, 'train/mask_bce_loss': 0.10102357380092145, 'train/mask_dice_loss': 0.33651125356554984, 'train/mask_loss': 0.4375348210334778, 'metrics/total_secs_per_batch': 4.888018846511841, 'metrics/data_secs_per_batch': 2.1368563413619994, '_timestamp': 1740978100.8300526}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 5 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978100.8302658}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 6 is less than current step: 499. Dropping entry: {'train/loss': 1.7345414578914642, 'train/ce_loss': 0.37734375, 'train/seg_cls_loss': 0.012310791015625, 'train/kl_loss': 0.2904296875, 'train/mask_bce_loss': 0.08167334143072366, 'train/mask_dice_loss': 0.5792497217655181, 'train/mask_loss': 0.6609230548143387, 'metrics/total_secs_per_batch': 5.801142692565918, 'metrics/data_secs_per_batch': 2.7186632633209227, '_timestamp': 1740978106.6313126}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 6 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978106.6316676}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 7 is less than current step: 499. Dropping entry: {'train/loss': 1.4802112698554992, 'train/ce_loss': 0.6515625, 'train/seg_cls_loss': 0.009381103515625, 'train/kl_loss': 0.2185546875, 'train/mask_bce_loss': 0.12170121818780899, 'train/mask_dice_loss': 0.2792930856347084, 'train/mask_loss': 0.4009942948818207, 'metrics/total_secs_per_batch': 5.312377452850342, 'metrics/data_secs_per_batch': 2.2774771451950073, '_timestamp': 1740978111.9434955}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 7 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978111.9437928}).
Epoch: [8][  8/500]	Time  5.312 ( 5.312)	Loss 1.2422 (1.4802)	CeLoss 1.2422 (0.6516)	SegCLSLoss 0.0000 (0.0094)	KLLoss 0.0000 (0.2186)	MaskLoss 0.0000 (0.4010)	MaskBCELoss 0.0000 (0.1217)	MaskDICELoss 0.0000 (0.2793)
Epoch: [8][  9/500]	Time  6.587 ( 6.587)	Loss 2.1293 (1.9224)	CeLoss 0.2930 (0.2922)	SegCLSLoss 0.0129 (0.0140)	KLLoss 0.3633 (0.3289)	MaskLoss 0.8967 (0.7950)	MaskBCELoss 0.1174 (0.1976)	MaskDICELoss 0.7793 (0.5973)
[2025-03-02 23:02:05,824] [INFO] [logging.py:96:log_dist] [Rank 0] step=4010, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:02:05,830] [INFO] [timer.py:215:stop] epoch=0/micro_step=40100/global_step=4010, RunningAvgSamplesPerSec=1.4746293093194163, CurrSamplesPerSec=1.3700437059302293, MemAllocated=30.95GB, MaxMemAllocated=37.23GB
Epoch: [8][ 10/500]	Time  7.301 ( 7.301)	Loss 2.5986 (1.5811)	CeLoss 0.2500 (0.2272)	SegCLSLoss 0.0178 (0.0174)	KLLoss 0.3672 (0.3650)	MaskLoss 1.1509 (0.6543)	MaskBCELoss 0.2501 (0.1033)	MaskDICELoss 0.9007 (0.5510)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 8 is less than current step: 499. Dropping entry: {'train/loss': 1.9223550319671632, 'train/ce_loss': 0.2921875, 'train/seg_cls_loss': 0.013995361328125, 'train/kl_loss': 0.32890625, 'train/mask_bce_loss': 0.19763146415352822, 'train/mask_dice_loss': 0.5973350971937179, 'train/mask_loss': 0.7949665784835815, 'metrics/total_secs_per_batch': 6.58735203742981, 'metrics/data_secs_per_batch': 3.109568238258362, '_timestamp': 1740978118.5309186}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 8 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978118.5312243}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 9 is less than current step: 499. Dropping entry: {'train/loss': 1.5811111152172088, 'train/ce_loss': 0.22724609375, 'train/seg_cls_loss': 0.017388916015625, 'train/kl_loss': 0.3650390625, 'train/mask_bce_loss': 0.10331547781825065, 'train/mask_dice_loss': 0.5510096095502377, 'train/mask_loss': 0.6543250888586044, 'metrics/total_secs_per_batch': 7.300788640975952, 'metrics/data_secs_per_batch': 3.116076445579529, '_timestamp': 1740978125.8315132}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 9 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978125.831891}).
Epoch: [8][ 11/500]	Time  6.794 ( 6.794)	Loss 1.7240 (2.1628)	CeLoss 0.1963 (0.2098)	SegCLSLoss 0.0175 (0.0177)	KLLoss 0.3672 (0.3650)	MaskLoss 0.7409 (0.9537)	MaskBCELoss 0.0279 (0.1672)	MaskDICELoss 0.7130 (0.7865)
Epoch: [8][ 12/500]	Time  5.306 ( 5.306)	Loss 2.8598 (1.8427)	CeLoss 0.2412 (0.5921)	SegCLSLoss 0.0104 (0.0111)	KLLoss 0.3652 (0.2551)	MaskLoss 1.2883 (0.6097)	MaskBCELoss 0.3306 (0.1463)	MaskDICELoss 0.9578 (0.4634)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 10 is less than current step: 499. Dropping entry: {'train/loss': 2.1627597570419312, 'train/ce_loss': 0.209765625, 'train/seg_cls_loss': 0.01768798828125, 'train/kl_loss': 0.3650390625, 'train/mask_bce_loss': 0.1672049928456545, 'train/mask_dice_loss': 0.7864893257617951, 'train/mask_loss': 0.9536943197250366, 'metrics/total_secs_per_batch': 6.794221878051758, 'metrics/data_secs_per_batch': 2.834219288825989, '_timestamp': 1740978132.6258712}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 10 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978132.6261709}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 11 is less than current step: 499. Dropping entry: {'train/loss': 1.842652940750122, 'train/ce_loss': 0.59208984375, 'train/seg_cls_loss': 0.01112060546875, 'train/kl_loss': 0.255078125, 'train/mask_bce_loss': 0.14626844311133028, 'train/mask_dice_loss': 0.4634369254112244, 'train/mask_loss': 0.6097053766250611, 'metrics/total_secs_per_batch': 5.3064916133880615, 'metrics/data_secs_per_batch': 2.090591287612915, '_timestamp': 1740978137.9323676}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 11 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978137.9326491}).
Epoch: [8][ 13/500]	Time  5.012 ( 5.012)	Loss 0.9805 (1.1931)	CeLoss 0.9805 (0.3858)	SegCLSLoss 0.0000 (0.0084)	KLLoss 0.0000 (0.2193)	MaskLoss 0.0000 (0.3904)	MaskBCELoss 0.0000 (0.0691)	MaskDICELoss 0.0000 (0.3213)
Epoch: [8][ 14/500]	Time  5.544 ( 5.544)	Loss 0.9628 (1.3525)	CeLoss 0.2305 (0.5409)	SegCLSLoss 0.0132 (0.0092)	KLLoss 0.3574 (0.1838)	MaskLoss 0.3447 (0.3942)	MaskBCELoss 0.0362 (0.0510)	MaskDICELoss 0.3085 (0.3432)
Epoch: [8][ 15/500]	Time  5.099 ( 5.099)	Loss 1.1250 (1.5693)	CeLoss 1.1250 (0.5398)	SegCLSLoss 0.0000 (0.0092)	KLLoss 0.0000 (0.2178)	MaskLoss 0.0000 (0.5014)	MaskBCELoss 0.0000 (0.1150)	MaskDICELoss 0.0000 (0.3864)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 12 is less than current step: 499. Dropping entry: {'train/loss': 1.1931271076202392, 'train/ce_loss': 0.3857666015625, 'train/seg_cls_loss': 0.0084228515625, 'train/kl_loss': 0.2193359375, 'train/mask_bce_loss': 0.0691230583935976, 'train/mask_dice_loss': 0.3213247776031494, 'train/mask_loss': 0.3904478311538696, 'metrics/total_secs_per_batch': 5.012117862701416, 'metrics/data_secs_per_batch': 2.236173319816589, '_timestamp': 1740978142.944501}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 12 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978142.944789}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 13 is less than current step: 499. Dropping entry: {'train/loss': 1.3525495767593383, 'train/ce_loss': 0.54091796875, 'train/seg_cls_loss': 0.00921630859375, 'train/kl_loss': 0.1837890625, 'train/mask_bce_loss': 0.0510427625849843, 'train/mask_dice_loss': 0.3431519389152527, 'train/mask_loss': 0.3941947102546692, 'metrics/total_secs_per_batch': 5.543774366378784, 'metrics/data_secs_per_batch': 2.527196669578552, '_timestamp': 1740978148.4884925}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 13 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978148.4889407}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 14 is less than current step: 499. Dropping entry: {'train/loss': 1.5692670702934266, 'train/ce_loss': 0.53984375, 'train/seg_cls_loss': 0.0091552734375, 'train/kl_loss': 0.2177734375, 'train/mask_bce_loss': 0.11499812118709088, 'train/mask_dice_loss': 0.3863834530115128, 'train/mask_loss': 0.5013815820217132, 'metrics/total_secs_per_batch': 5.099440097808838, 'metrics/data_secs_per_batch': 2.188319230079651, '_timestamp': 1740978153.5877411}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 14 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978153.588058}).
Epoch: [8][ 16/500]	Time  6.512 ( 6.512)	Loss 1.0284 (1.4321)	CeLoss 0.2451 (0.2856)	SegCLSLoss 0.0132 (0.0113)	KLLoss 0.3613 (0.2900)	MaskLoss 0.3697 (0.5557)	MaskBCELoss 0.1224 (0.1099)	MaskDICELoss 0.2473 (0.4458)
Epoch: [8][ 17/500]	Time  5.524 ( 5.524)	Loss 1.8002 (1.2231)	CeLoss 0.2324 (0.4484)	SegCLSLoss 0.0101 (0.0085)	KLLoss 0.3691 (0.2529)	MaskLoss 0.7634 (0.3727)	MaskBCELoss 0.1197 (0.0837)	MaskDICELoss 0.6436 (0.2890)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 15 is less than current step: 499. Dropping entry: {'train/loss': 1.4320595622062684, 'train/ce_loss': 0.28564453125, 'train/seg_cls_loss': 0.011322021484375, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.10990085564553738, 'train/mask_dice_loss': 0.44582618623971937, 'train/mask_loss': 0.5557270437479019, 'metrics/total_secs_per_batch': 6.512090444564819, 'metrics/data_secs_per_batch': 2.6612823009490967, '_timestamp': 1740978160.0998006}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 15 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978160.1001096}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 16 is less than current step: 499. Dropping entry: {'train/loss': 1.2231426537036896, 'train/ce_loss': 0.448388671875, 'train/seg_cls_loss': 0.0085205078125, 'train/kl_loss': 0.2529296875, 'train/mask_bce_loss': 0.08368502077646553, 'train/mask_dice_loss': 0.28899471163749696, 'train/mask_loss': 0.37267972677946093, 'metrics/total_secs_per_batch': 5.523611783981323, 'metrics/data_secs_per_batch': 2.5096585512161256, '_timestamp': 1740978165.6235142}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 16 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978165.6238832}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 17 is less than current step: 499. Dropping entry: {'train/loss': 1.599384081363678, 'train/ce_loss': 0.3386962890625, 'train/seg_cls_loss': 0.011370849609375, 'train/kl_loss': 0.25390625, 'train/mask_bce_loss': 0.09804008081555367, 'train/mask_dice_loss': 0.5170206040143966, 'train/mask_loss': 0.6150606811046601, 'metrics/total_secs_per_batch': 5.9363110065460205, 'metrics/data_secs_per_batch': 2.808052825927734, '_timestamp': 1740978171.5598557}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 17 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978171.5602045}).
Epoch: [8][ 18/500]	Time  5.936 ( 5.936)	Loss 0.0598 (1.5994)	CeLoss 0.0598 (0.3387)	SegCLSLoss 0.0000 (0.0114)	KLLoss 0.0000 (0.2539)	MaskLoss 0.0000 (0.6151)	MaskBCELoss 0.0000 (0.0980)	MaskDICELoss 0.0000 (0.5170)
Epoch: [8][ 19/500]	Time  5.653 ( 5.653)	Loss 1.0295 (1.3410)	CeLoss 0.2168 (0.4153)	SegCLSLoss 0.0140 (0.0106)	KLLoss 0.3691 (0.2535)	MaskLoss 0.3849 (0.4476)	MaskBCELoss 0.1413 (0.0862)	MaskDICELoss 0.2436 (0.3613)
[2025-03-02 23:03:03,391] [INFO] [logging.py:96:log_dist] [Rank 0] step=4020, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:03:03,398] [INFO] [timer.py:215:stop] epoch=0/micro_step=40200/global_step=4020, RunningAvgSamplesPerSec=1.4751847850145214, CurrSamplesPerSec=1.616952202787348, MemAllocated=30.72GB, MaxMemAllocated=37.23GB
Epoch: [8][ 20/500]	Time  6.186 ( 6.186)	Loss 0.9648 (1.4751)	CeLoss 0.9648 (0.3369)	SegCLSLoss 0.0000 (0.0098)	KLLoss 0.0000 (0.2154)	MaskLoss 0.0000 (0.5559)	MaskBCELoss 0.0000 (0.0986)	MaskDICELoss 0.0000 (0.4574)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 18 is less than current step: 499. Dropping entry: {'train/loss': 1.3410334825515746, 'train/ce_loss': 0.41533203125, 'train/seg_cls_loss': 0.01063232421875, 'train/kl_loss': 0.253515625, 'train/mask_bce_loss': 0.08623719774186611, 'train/mask_dice_loss': 0.3613303117454052, 'train/mask_loss': 0.44756751507520676, 'metrics/total_secs_per_batch': 5.652821779251099, 'metrics/data_secs_per_batch': 2.2894293785095217, '_timestamp': 1740978177.2126455}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 18 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978177.2129822}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 19 is less than current step: 499. Dropping entry: {'train/loss': 1.4751047849655152, 'train/ce_loss': 0.336865234375, 'train/seg_cls_loss': 0.009771728515625, 'train/kl_loss': 0.2154296875, 'train/mask_bce_loss': 0.09855559403076768, 'train/mask_dice_loss': 0.45738059282302856, 'train/mask_loss': 0.5559361815452576, 'metrics/total_secs_per_batch': 6.186265468597412, 'metrics/data_secs_per_batch': 2.99367938041687, '_timestamp': 1740978183.3986888}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 19 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978183.3990426}).
Epoch: [8][ 21/500]	Time  6.955 ( 6.955)	Loss 1.8764 (1.6155)	CeLoss 0.2275 (0.3046)	SegCLSLoss 0.0195 (0.0161)	KLLoss 0.3711 (0.3258)	MaskLoss 0.8015 (0.6352)	MaskBCELoss 0.0331 (0.0853)	MaskDICELoss 0.7683 (0.5498)
Epoch: [8][ 22/500]	Time  6.053 ( 6.053)	Loss 3.0168 (1.5326)	CeLoss 0.2695 (0.3931)	SegCLSLoss 0.0176 (0.0110)	KLLoss 0.3691 (0.2566)	MaskLoss 1.3512 (0.5540)	MaskBCELoss 0.4334 (0.1291)	MaskDICELoss 0.9177 (0.4249)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 20 is less than current step: 499. Dropping entry: {'train/loss': 1.6154598593711853, 'train/ce_loss': 0.30458984375, 'train/seg_cls_loss': 0.01612548828125, 'train/kl_loss': 0.32578125, 'train/mask_bce_loss': 0.08532703686505556, 'train/mask_dice_loss': 0.5498442888259888, 'train/mask_loss': 0.6351713359355926, 'metrics/total_secs_per_batch': 6.95469331741333, 'metrics/data_secs_per_batch': 2.643548583984375, '_timestamp': 1740978190.3535428}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 20 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978190.353746}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 21 is less than current step: 499. Dropping entry: {'train/loss': 1.532551920413971, 'train/ce_loss': 0.393115234375, 'train/seg_cls_loss': 0.010980224609375, 'train/kl_loss': 0.256640625, 'train/mask_bce_loss': 0.12914946172386407, 'train/mask_dice_loss': 0.42489506006240846, 'train/mask_loss': 0.554044508934021, 'metrics/total_secs_per_batch': 6.05292010307312, 'metrics/data_secs_per_batch': 2.5825785875320433, '_timestamp': 1740978196.4066844}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 21 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978196.4070327}).
Epoch: [8][ 23/500]	Time  6.230 ( 6.230)	Loss 1.7094 (1.7148)	CeLoss 0.1982 (0.4901)	SegCLSLoss 0.0188 (0.0103)	KLLoss 0.3555 (0.2539)	MaskLoss 0.7326 (0.5972)	MaskBCELoss 0.0781 (0.1601)	MaskDICELoss 0.6546 (0.4370)
Epoch: [8][ 24/500]	Time  4.994 ( 4.994)	Loss 1.5312 (1.5574)	CeLoss 1.5312 (0.5440)	SegCLSLoss 0.0000 (0.0129)	KLLoss 0.0000 (0.2543)	MaskLoss 0.0000 (0.4908)	MaskBCELoss 0.0000 (0.0334)	MaskDICELoss 0.0000 (0.4574)
Epoch: [8][ 25/500]	Time  5.758 ( 5.758)	Loss 1.4445 (1.4503)	CeLoss 0.2012 (0.3846)	SegCLSLoss 0.0217 (0.0121)	KLLoss 0.3555 (0.2920)	MaskLoss 0.5982 (0.5151)	MaskBCELoss 0.0842 (0.1141)	MaskDICELoss 0.5140 (0.4010)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 22 is less than current step: 499. Dropping entry: {'train/loss': 1.7147895216941833, 'train/ce_loss': 0.49013671875, 'train/seg_cls_loss': 0.01031494140625, 'train/kl_loss': 0.25390625, 'train/mask_bce_loss': 0.160149684548378, 'train/mask_dice_loss': 0.43703998625278473, 'train/mask_loss': 0.5971896708011627, 'metrics/total_secs_per_batch': 6.2297446727752686, 'metrics/data_secs_per_batch': 2.966476082801819, '_timestamp': 1740978202.6361752}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 22 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978202.636521}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 23 is less than current step: 499. Dropping entry: {'train/loss': 1.5573733389377593, 'train/ce_loss': 0.54404296875, 'train/seg_cls_loss': 0.01292724609375, 'train/kl_loss': 0.254296875, 'train/mask_bce_loss': 0.033400048362091185, 'train/mask_dice_loss': 0.45739601254463197, 'train/mask_loss': 0.49079605713486674, 'metrics/total_secs_per_batch': 4.994497299194336, 'metrics/data_secs_per_batch': 2.3536645412445067, '_timestamp': 1740978207.6306872}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 23 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978207.6309795}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 24 is less than current step: 499. Dropping entry: {'train/loss': 1.4503389239311217, 'train/ce_loss': 0.3845703125, 'train/seg_cls_loss': 0.012109375, 'train/kl_loss': 0.2919921875, 'train/mask_bce_loss': 0.11413346324115992, 'train/mask_dice_loss': 0.4009774014353752, 'train/mask_loss': 0.5151108637452125, 'metrics/total_secs_per_batch': 5.758051872253418, 'metrics/data_secs_per_batch': 2.4386701583862305, '_timestamp': 1740978213.3888066}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 24 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978213.3891194}).
Epoch: [8][ 26/500]	Time  6.363 ( 6.363)	Loss 1.4219 (1.9152)	CeLoss 1.4219 (0.3413)	SegCLSLoss 0.0000 (0.0167)	KLLoss 0.0000 (0.3270)	MaskLoss 0.0000 (0.7665)	MaskBCELoss 0.0000 (0.1319)	MaskDICELoss 0.0000 (0.6346)
Epoch: [8][ 27/500]	Time  6.122 ( 6.122)	Loss 2.0243 (2.0558)	CeLoss 0.1729 (0.2999)	SegCLSLoss 0.0198 (0.0185)	KLLoss 0.3555 (0.3273)	MaskLoss 0.9028 (0.8570)	MaskBCELoss 0.0360 (0.1386)	MaskDICELoss 0.8668 (0.7184)
Epoch: [8][ 28/500]	Time  5.486 ( 5.486)	Loss 1.5547 (1.4490)	CeLoss 1.5547 (0.5793)	SegCLSLoss 0.0000 (0.0113)	KLLoss 0.0000 (0.2160)	MaskLoss 0.0000 (0.4212)	MaskBCELoss 0.0000 (0.0473)	MaskDICELoss 0.0000 (0.3739)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 25 is less than current step: 499. Dropping entry: {'train/loss': 1.915175849199295, 'train/ce_loss': 0.34130859375, 'train/seg_cls_loss': 0.016748046875, 'train/kl_loss': 0.326953125, 'train/mask_bce_loss': 0.1319465517066419, 'train/mask_dice_loss': 0.634576889872551, 'train/mask_loss': 0.7665234416723251, 'metrics/total_secs_per_batch': 6.363420009613037, 'metrics/data_secs_per_batch': 3.1077285766601563, '_timestamp': 1740978219.7522194}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 25 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978219.7525325}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 26 is less than current step: 499. Dropping entry: {'train/loss': 2.055766594409943, 'train/ce_loss': 0.29990234375, 'train/seg_cls_loss': 0.01851806640625, 'train/kl_loss': 0.32734375, 'train/mask_bce_loss': 0.13859718460589648, 'train/mask_dice_loss': 0.7183876752853393, 'train/mask_loss': 0.8569848597049713, 'metrics/total_secs_per_batch': 6.121906995773315, 'metrics/data_secs_per_batch': 2.9262513637542726, '_timestamp': 1740978225.8740668}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 26 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978225.874353}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 27 is less than current step: 499. Dropping entry: {'train/loss': 1.4490343093872071, 'train/ce_loss': 0.579296875, 'train/seg_cls_loss': 0.011260986328125, 'train/kl_loss': 0.216015625, 'train/mask_bce_loss': 0.04728635307401419, 'train/mask_dice_loss': 0.3739104926586151, 'train/mask_loss': 0.42119684517383577, 'metrics/total_secs_per_batch': 5.486233234405518, 'metrics/data_secs_per_batch': 2.673433470726013, '_timestamp': 1740978231.3605711}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 27 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978231.360948}).
Epoch: [8][ 29/500]	Time  5.197 ( 5.197)	Loss 2.5695 (1.7448)	CeLoss 0.2227 (0.5394)	SegCLSLoss 0.0184 (0.0119)	KLLoss 0.3652 (0.2160)	MaskLoss 1.1509 (0.5890)	MaskBCELoss 0.4134 (0.1364)	MaskDICELoss 0.7376 (0.4526)
[2025-03-02 23:04:02,548] [INFO] [logging.py:96:log_dist] [Rank 0] step=4030, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:04:02,553] [INFO] [timer.py:215:stop] epoch=0/micro_step=40300/global_step=4030, RunningAvgSamplesPerSec=1.4756520280138155, CurrSamplesPerSec=1.6679636125552846, MemAllocated=31.31GB, MaxMemAllocated=37.23GB
Epoch: [8][ 30/500]	Time  5.997 ( 5.997)	Loss 1.3517 (1.6352)	CeLoss 0.2168 (0.2496)	SegCLSLoss 0.0186 (0.0142)	KLLoss 0.3672 (0.2912)	MaskLoss 0.5440 (0.6747)	MaskBCELoss 0.0593 (0.1932)	MaskDICELoss 0.4847 (0.4815)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 28 is less than current step: 499. Dropping entry: {'train/loss': 1.7447510957717896, 'train/ce_loss': 0.53935546875, 'train/seg_cls_loss': 0.01192626953125, 'train/kl_loss': 0.216015625, 'train/mask_bce_loss': 0.13642652183771134, 'train/mask_dice_loss': 0.45259941816329957, 'train/mask_loss': 0.5890259325504303, 'metrics/total_secs_per_batch': 5.197110176086426, 'metrics/data_secs_per_batch': 2.1829382181167603, '_timestamp': 1740978236.5574222}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 28 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978236.557708}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 29 is less than current step: 499. Dropping entry: {'train/loss': 1.6352468967437743, 'train/ce_loss': 0.2495849609375, 'train/seg_cls_loss': 0.01416015625, 'train/kl_loss': 0.2912109375, 'train/mask_bce_loss': 0.193206069432199, 'train/mask_dice_loss': 0.48146083652973176, 'train/mask_loss': 0.674666902422905, 'metrics/total_secs_per_batch': 5.996915817260742, 'metrics/data_secs_per_batch': 2.7192959070205687, '_timestamp': 1740978242.5541563}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 29 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978242.5544248}).
Epoch: [8][ 31/500]	Time  5.201 ( 5.201)	Loss 1.6562 (1.4416)	CeLoss 0.2256 (0.5425)	SegCLSLoss 0.0134 (0.0096)	KLLoss 0.3594 (0.2197)	MaskLoss 0.6943 (0.4363)	MaskBCELoss 0.0358 (0.0832)	MaskDICELoss 0.6585 (0.3530)
Epoch: [8][ 32/500]	Time  8.189 ( 8.189)	Loss 0.8713 (1.1816)	CeLoss 0.2754 (0.4418)	SegCLSLoss 0.0133 (0.0115)	KLLoss 0.3633 (0.2541)	MaskLoss 0.2765 (0.3543)	MaskBCELoss 0.0726 (0.0714)	MaskDICELoss 0.2039 (0.2829)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 30 is less than current step: 499. Dropping entry: {'train/loss': 1.441644936800003, 'train/ce_loss': 0.54248046875, 'train/seg_cls_loss': 0.009613037109375, 'train/kl_loss': 0.2197265625, 'train/mask_bce_loss': 0.08321010768413543, 'train/mask_dice_loss': 0.3530420422554016, 'train/mask_loss': 0.43625215590000155, 'metrics/total_secs_per_batch': 5.200648546218872, 'metrics/data_secs_per_batch': 2.1506170272827148, '_timestamp': 1740978247.7552588}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 30 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978247.755489}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 31 is less than current step: 499. Dropping entry: {'train/loss': 1.1816230475902558, 'train/ce_loss': 0.441796875, 'train/seg_cls_loss': 0.0115478515625, 'train/kl_loss': 0.2541015625, 'train/mask_bce_loss': 0.07142938040196896, 'train/mask_dice_loss': 0.2828586995601654, 'train/mask_loss': 0.35428807884454727, 'metrics/total_secs_per_batch': 8.188649654388428, 'metrics/data_secs_per_batch': 3.844345712661743, '_timestamp': 1740978255.9440022}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 31 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978255.944461}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 32 is less than current step: 499. Dropping entry: {'train/loss': 1.528470379114151, 'train/ce_loss': 0.315625, 'train/seg_cls_loss': 0.013385009765625, 'train/kl_loss': 0.288671875, 'train/mask_bce_loss': 0.10038347467780113, 'train/mask_dice_loss': 0.48841225206851957, 'train/mask_loss': 0.5887957334518432, 'metrics/total_secs_per_batch': 6.140998840332031, 'metrics/data_secs_per_batch': 3.015966510772705, '_timestamp': 1740978262.0846467}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 32 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978262.0849192}).
Epoch: [8][ 33/500]	Time  6.141 ( 6.141)	Loss 0.6758 (1.5285)	CeLoss 0.6758 (0.3156)	SegCLSLoss 0.0000 (0.0134)	KLLoss 0.0000 (0.2887)	MaskLoss 0.0000 (0.5888)	MaskBCELoss 0.0000 (0.1004)	MaskDICELoss 0.0000 (0.4884)
Epoch: [8][ 34/500]	Time  5.467 ( 5.467)	Loss 1.9139 (1.3440)	CeLoss 0.2793 (0.4324)	SegCLSLoss 0.0121 (0.0107)	KLLoss 0.3613 (0.2904)	MaskLoss 0.7968 (0.4386)	MaskBCELoss 0.4749 (0.1105)	MaskDICELoss 0.3219 (0.3281)
Epoch: [8][ 35/500]	Time  6.480 ( 6.480)	Loss 0.0986 (1.3347)	CeLoss 0.0986 (0.2529)	SegCLSLoss 0.0000 (0.0115)	KLLoss 0.0000 (0.2186)	MaskLoss 0.0000 (0.5270)	MaskBCELoss 0.0000 (0.1031)	MaskDICELoss 0.0000 (0.4239)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 33 is less than current step: 499. Dropping entry: {'train/loss': 1.3439968198537826, 'train/ce_loss': 0.432421875, 'train/seg_cls_loss': 0.01070556640625, 'train/kl_loss': 0.2904296875, 'train/mask_bce_loss': 0.11046316102147102, 'train/mask_dice_loss': 0.32813681811094286, 'train/mask_loss': 0.4385999783873558, 'metrics/total_secs_per_batch': 5.466644525527954, 'metrics/data_secs_per_batch': 2.2790897607803347, '_timestamp': 1740978267.551289}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 33 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978267.5515869}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 34 is less than current step: 499. Dropping entry: {'train/loss': 1.3346586108207703, 'train/ce_loss': 0.2529296875, 'train/seg_cls_loss': 0.011529541015625, 'train/kl_loss': 0.2185546875, 'train/mask_bce_loss': 0.10314995460212231, 'train/mask_dice_loss': 0.42389613687992095, 'train/mask_loss': 0.5270460963249206, 'metrics/total_secs_per_batch': 6.480462074279785, 'metrics/data_secs_per_batch': 2.7168346881866454, '_timestamp': 1740978274.0321665}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 34 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978274.0325992}).
Epoch: [8][ 36/500]	Time  5.316 ( 5.316)	Loss 1.6551 (1.7470)	CeLoss 0.3164 (0.5512)	SegCLSLoss 0.0148 (0.0129)	KLLoss 0.3652 (0.2551)	MaskLoss 0.6469 (0.5819)	MaskBCELoss 0.1401 (0.1220)	MaskDICELoss 0.5068 (0.4599)
Epoch: [8][ 37/500]	Time  5.999 ( 5.999)	Loss 2.4768 (1.8113)	CeLoss 0.1992 (0.3904)	SegCLSLoss 0.0211 (0.0124)	KLLoss 0.3730 (0.2900)	MaskLoss 1.1149 (0.6928)	MaskBCELoss 0.2995 (0.1880)	MaskDICELoss 0.8153 (0.5048)
Epoch: [8][ 38/500]	Time  6.023 ( 6.023)	Loss 1.2386 (1.5206)	CeLoss 0.2344 (0.3100)	SegCLSLoss 0.0123 (0.0116)	KLLoss 0.3906 (0.2930)	MaskLoss 0.4797 (0.5878)	MaskBCELoss 0.0350 (0.0640)	MaskDICELoss 0.4447 (0.5238)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 35 is less than current step: 499. Dropping entry: {'train/loss': 1.746973955631256, 'train/ce_loss': 0.551171875, 'train/seg_cls_loss': 0.0129150390625, 'train/kl_loss': 0.255078125, 'train/mask_bce_loss': 0.12201252207159996, 'train/mask_dice_loss': 0.4598728835582733, 'train/mask_loss': 0.5818854033946991, 'metrics/total_secs_per_batch': 5.3160223960876465, 'metrics/data_secs_per_batch': 2.361280846595764, '_timestamp': 1740978279.3477693}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 35 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978279.3481352}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 36 is less than current step: 499. Dropping entry: {'train/loss': 1.8113396763801575, 'train/ce_loss': 0.3904296875, 'train/seg_cls_loss': 0.012359619140625, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.18802729062736034, 'train/mask_dice_loss': 0.5048007488250732, 'train/mask_loss': 0.6928280383348465, 'metrics/total_secs_per_batch': 5.998686790466309, 'metrics/data_secs_per_batch': 2.9540554523468017, '_timestamp': 1740978285.3464954}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 36 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978285.3468797}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 37 is less than current step: 499. Dropping entry: {'train/loss': 1.5206104338169097, 'train/ce_loss': 0.310009765625, 'train/seg_cls_loss': 0.011602783203125, 'train/kl_loss': 0.29296875, 'train/mask_bce_loss': 0.06397729071322829, 'train/mask_dice_loss': 0.5237937331199646, 'train/mask_loss': 0.5877710193395614, 'metrics/total_secs_per_batch': 6.022703647613525, 'metrics/data_secs_per_batch': 2.700572371482849, '_timestamp': 1740978291.3691938}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 37 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978291.3694763}).
Epoch: [8][ 39/500]	Time  5.067 ( 5.067)	Loss 2.4839 (1.5891)	CeLoss 0.3223 (0.5918)	SegCLSLoss 0.0146 (0.0102)	KLLoss 0.3633 (0.2195)	MaskLoss 1.0584 (0.4849)	MaskBCELoss 0.2784 (0.1249)	MaskDICELoss 0.7799 (0.3600)
[2025-03-02 23:05:02,444] [INFO] [logging.py:96:log_dist] [Rank 0] step=4040, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:05:02,450] [INFO] [timer.py:215:stop] epoch=0/micro_step=40400/global_step=4040, RunningAvgSamplesPerSec=1.4761009525416071, CurrSamplesPerSec=1.6629354432131263, MemAllocated=31.26GB, MaxMemAllocated=37.23GB
Epoch: [8][ 40/500]	Time  6.015 ( 6.015)	Loss 2.2819 (1.7091)	CeLoss 0.1904 (0.2276)	SegCLSLoss 0.0159 (0.0158)	KLLoss 0.3555 (0.3645)	MaskLoss 1.0238 (0.7184)	MaskBCELoss 0.3215 (0.1749)	MaskDICELoss 0.7023 (0.5435)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 38 is less than current step: 499. Dropping entry: {'train/loss': 1.5891309797763824, 'train/ce_loss': 0.591796875, 'train/seg_cls_loss': 0.01015625, 'train/kl_loss': 0.21953125, 'train/mask_bce_loss': 0.12493266575038434, 'train/mask_dice_loss': 0.3600136786699295, 'train/mask_loss': 0.4849463552236557, 'metrics/total_secs_per_batch': 5.066837310791016, 'metrics/data_secs_per_batch': 2.124246668815613, '_timestamp': 1740978296.4360225}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 38 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978296.4363174}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 39 is less than current step: 499. Dropping entry: {'train/loss': 1.7091031670570374, 'train/ce_loss': 0.22763671875, 'train/seg_cls_loss': 0.01575927734375, 'train/kl_loss': 0.364453125, 'train/mask_bce_loss': 0.17488264283165336, 'train/mask_dice_loss': 0.5435361266136169, 'train/mask_loss': 0.7184187650680542, 'metrics/total_secs_per_batch': 6.015105962753296, 'metrics/data_secs_per_batch': 2.533376145362854, '_timestamp': 1740978302.451031}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 39 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978302.4513626}).
Epoch: [8][ 41/500]	Time  5.175 ( 5.175)	Loss 0.0620 (1.3571)	CeLoss 0.0620 (0.3889)	SegCLSLoss 0.0000 (0.0091)	KLLoss 0.0000 (0.2176)	MaskLoss 0.0000 (0.4709)	MaskBCELoss 0.0000 (0.0886)	MaskDICELoss 0.0000 (0.3823)
Epoch: [8][ 42/500]	Time  4.286 ( 4.286)	Loss 0.8789 (1.6933)	CeLoss 0.8789 (0.6345)	SegCLSLoss 0.0000 (0.0108)	KLLoss 0.0000 (0.1832)	MaskLoss 0.0000 (0.5175)	MaskBCELoss 0.0000 (0.0636)	MaskDICELoss 0.0000 (0.4539)
Epoch: [8][ 43/500]	Time  6.930 ( 6.930)	Loss 1.5261 (1.8655)	CeLoss 0.2520 (0.2392)	SegCLSLoss 0.0118 (0.0162)	KLLoss 0.3672 (0.3654)	MaskLoss 0.6156 (0.7908)	MaskBCELoss 0.1303 (0.1329)	MaskDICELoss 0.4853 (0.6579)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 40 is less than current step: 499. Dropping entry: {'train/loss': 1.357147991657257, 'train/ce_loss': 0.3888671875, 'train/seg_cls_loss': 0.009051513671875, 'train/kl_loss': 0.217578125, 'train/mask_bce_loss': 0.0886200338602066, 'train/mask_dice_loss': 0.38228794634342195, 'train/mask_loss': 0.4709079772233963, 'metrics/total_secs_per_batch': 5.175206184387207, 'metrics/data_secs_per_batch': 2.4742104530334474, '_timestamp': 1740978307.6263568}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 40 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978307.626685}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 41 is less than current step: 499. Dropping entry: {'train/loss': 1.6933319807052611, 'train/ce_loss': 0.634521484375, 'train/seg_cls_loss': 0.010833740234375, 'train/kl_loss': 0.183203125, 'train/mask_bce_loss': 0.06358566931448877, 'train/mask_dice_loss': 0.45388108491897583, 'train/mask_loss': 0.5174667656421661, 'metrics/total_secs_per_batch': 4.285700559616089, 'metrics/data_secs_per_batch': 2.140591788291931, '_timestamp': 1740978311.9120362}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 41 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978311.912322}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 42 is less than current step: 499. Dropping entry: {'train/loss': 1.8654941201210022, 'train/ce_loss': 0.23916015625, 'train/seg_cls_loss': 0.016156005859375, 'train/kl_loss': 0.3654296875, 'train/mask_bce_loss': 0.13288276996463538, 'train/mask_dice_loss': 0.6579209268093109, 'train/mask_loss': 0.7908036917448044, 'metrics/total_secs_per_batch': 6.9302449226379395, 'metrics/data_secs_per_batch': 3.0959586620330812, '_timestamp': 1740978318.8423092}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 42 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978318.842585}).
Epoch: [8][ 44/500]	Time  5.721 ( 5.721)	Loss 1.1719 (1.6623)	CeLoss 1.1719 (0.5703)	SegCLSLoss 0.0000 (0.0102)	KLLoss 0.0000 (0.1820)	MaskLoss 0.0000 (0.5344)	MaskBCELoss 0.0000 (0.1065)	MaskDICELoss 0.0000 (0.4278)
Epoch: [8][ 45/500]	Time  5.040 ( 5.040)	Loss 0.7461 (1.4392)	CeLoss 0.7461 (0.5375)	SegCLSLoss 0.0000 (0.0086)	KLLoss 0.0000 (0.1797)	MaskLoss 0.0000 (0.4397)	MaskBCELoss 0.0000 (0.0207)	MaskDICELoss 0.0000 (0.4190)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 43 is less than current step: 499. Dropping entry: {'train/loss': 1.6622913479804993, 'train/ce_loss': 0.5703125, 'train/seg_cls_loss': 0.010235595703125, 'train/kl_loss': 0.18203125, 'train/mask_bce_loss': 0.10653148097917438, 'train/mask_dice_loss': 0.4278368532657623, 'train/mask_loss': 0.5343683302402497, 'metrics/total_secs_per_batch': 5.721310377120972, 'metrics/data_secs_per_batch': 2.678736090660095, '_timestamp': 1740978324.5635793}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 43 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978324.5637643}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 44 is less than current step: 499. Dropping entry: {'train/loss': 1.4392170906066895, 'train/ce_loss': 0.5375, 'train/seg_cls_loss': 0.008642578125, 'train/kl_loss': 0.1796875, 'train/mask_bce_loss': 0.020691221114248037, 'train/mask_dice_loss': 0.4190345168113708, 'train/mask_loss': 0.43972572684288025, 'metrics/total_secs_per_batch': 5.039915561676025, 'metrics/data_secs_per_batch': 2.164369821548462, '_timestamp': 1740978329.6034973}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 44 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978329.6037655}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 45 is less than current step: 499. Dropping entry: {'train/loss': 1.8665155529975892, 'train/ce_loss': 0.30673828125, 'train/seg_cls_loss': 0.016400146484375, 'train/kl_loss': 0.3244140625, 'train/mask_bce_loss': 0.08551839888095855, 'train/mask_dice_loss': 0.674106565117836, 'train/mask_loss': 0.7596249580383301, 'metrics/total_secs_per_batch': 6.7043540477752686, 'metrics/data_secs_per_batch': 3.2862812995910646, '_timestamp': 1740978336.3078809}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 45 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978336.3082373}).
Epoch: [8][ 46/500]	Time  6.704 ( 6.704)	Loss 2.1626 (1.8665)	CeLoss 0.1826 (0.3067)	SegCLSLoss 0.0208 (0.0164)	KLLoss 0.3594 (0.3244)	MaskLoss 0.9666 (0.7596)	MaskBCELoss 0.0283 (0.0855)	MaskDICELoss 0.9383 (0.6741)
Epoch: [8][ 47/500]	Time  5.148 ( 5.148)	Loss 1.4062 (1.5893)	CeLoss 1.4062 (0.5424)	SegCLSLoss 0.0000 (0.0127)	KLLoss 0.0000 (0.2191)	MaskLoss 0.0000 (0.5094)	MaskBCELoss 0.0000 (0.0746)	MaskDICELoss 0.0000 (0.4348)
Epoch: [8][ 48/500]	Time  5.560 ( 5.560)	Loss 2.3870 (1.5902)	CeLoss 0.4160 (0.3900)	SegCLSLoss 0.0090 (0.0121)	KLLoss 0.3574 (0.2561)	MaskLoss 0.9660 (0.5844)	MaskBCELoss 0.3023 (0.1089)	MaskDICELoss 0.6637 (0.4755)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 46 is less than current step: 499. Dropping entry: {'train/loss': 1.5893366932868958, 'train/ce_loss': 0.5423828125, 'train/seg_cls_loss': 0.012689208984375, 'train/kl_loss': 0.219140625, 'train/mask_bce_loss': 0.07458394644781947, 'train/mask_dice_loss': 0.43478165864944457, 'train/mask_loss': 0.5093656063079834, 'metrics/total_secs_per_batch': 5.148157358169556, 'metrics/data_secs_per_batch': 2.3914469957351683, '_timestamp': 1740978341.456063}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 46 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978341.456257}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 47 is less than current step: 499. Dropping entry: {'train/loss': 1.590188467502594, 'train/ce_loss': 0.3900390625, 'train/seg_cls_loss': 0.012078857421875, 'train/kl_loss': 0.2560546875, 'train/mask_bce_loss': 0.1089177674613893, 'train/mask_dice_loss': 0.47548310458660126, 'train/mask_loss': 0.584400886297226, 'metrics/total_secs_per_batch': 5.559745788574219, 'metrics/data_secs_per_batch': 2.448032021522522, '_timestamp': 1740978347.015975}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 47 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978347.0163202}).
Epoch: [8][ 49/500]	Time  5.364 ( 5.364)	Loss 1.2503 (1.6917)	CeLoss 0.2139 (0.5222)	SegCLSLoss 0.0143 (0.0132)	KLLoss 0.3633 (0.2547)	MaskLoss 0.4963 (0.5687)	MaskBCELoss 0.0664 (0.0762)	MaskDICELoss 0.4299 (0.4925)
[2025-03-02 23:05:57,039] [INFO] [logging.py:96:log_dist] [Rank 0] step=4050, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:05:57,044] [INFO] [timer.py:215:stop] epoch=0/micro_step=40500/global_step=4050, RunningAvgSamplesPerSec=1.4768098574242339, CurrSamplesPerSec=2.1440085566015314, MemAllocated=30.69GB, MaxMemAllocated=37.23GB
Epoch: [8][ 50/500]	Time  4.666 ( 4.666)	Loss 0.9492 (1.6919)	CeLoss 0.9492 (0.3729)	SegCLSLoss 0.0000 (0.0143)	KLLoss 0.0000 (0.2891)	MaskLoss 0.0000 (0.6415)	MaskBCELoss 0.0000 (0.1191)	MaskDICELoss 0.0000 (0.5224)
Epoch: [8][ 51/500]	Time  5.849 ( 5.849)	Loss 1.5521 (1.3695)	CeLoss 0.2129 (0.4100)	SegCLSLoss 0.0232 (0.0100)	KLLoss 0.3652 (0.2195)	MaskLoss 0.6452 (0.4662)	MaskBCELoss 0.0067 (0.0513)	MaskDICELoss 0.6385 (0.4148)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 48 is less than current step: 499. Dropping entry: {'train/loss': 1.6916668117046356, 'train/ce_loss': 0.52216796875, 'train/seg_cls_loss': 0.01324462890625, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.07616536971181631, 'train/mask_dice_loss': 0.4925195902585983, 'train/mask_loss': 0.5686849623918533, 'metrics/total_secs_per_batch': 5.363889694213867, 'metrics/data_secs_per_batch': 2.5111512184143066, '_timestamp': 1740978352.3796198}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 48 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978352.3798862}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 49 is less than current step: 499. Dropping entry: {'train/loss': 1.6919076442718506, 'train/ce_loss': 0.3728515625, 'train/seg_cls_loss': 0.0143310546875, 'train/kl_loss': 0.2890625, 'train/mask_bce_loss': 0.11914784926921129, 'train/mask_dice_loss': 0.522362619638443, 'train/mask_loss': 0.6415104627609253, 'metrics/total_secs_per_batch': 4.6656413078308105, 'metrics/data_secs_per_batch': 2.2771899938583373, '_timestamp': 1740978357.0451198}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 49 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978357.045376}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 50 is less than current step: 499. Dropping entry: {'train/loss': 1.3695253491401673, 'train/ce_loss': 0.4099609375, 'train/seg_cls_loss': 0.00997314453125, 'train/kl_loss': 0.21953125, 'train/mask_bce_loss': 0.05133683113381267, 'train/mask_dice_loss': 0.4148223400115967, 'train/mask_loss': 0.4661591649055481, 'metrics/total_secs_per_batch': 5.849072217941284, 'metrics/data_secs_per_batch': 2.667674350738525, '_timestamp': 1740978362.894448}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 50 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978362.8947937}).
Epoch: [8][ 52/500]	Time  6.506 ( 6.506)	Loss 1.3926 (1.6782)	CeLoss 0.2773 (0.4060)	SegCLSLoss 0.0170 (0.0135)	KLLoss 0.3516 (0.2865)	MaskLoss 0.5361 (0.6184)	MaskBCELoss 0.1082 (0.0665)	MaskDICELoss 0.4279 (0.5518)
Epoch: [8][ 53/500]	Time  6.218 ( 6.218)	Loss 2.5629 (1.9263)	CeLoss 0.2930 (0.3281)	SegCLSLoss 0.0119 (0.0157)	KLLoss 0.3633 (0.3254)	MaskLoss 1.1135 (0.7788)	MaskBCELoss 0.1912 (0.1354)	MaskDICELoss 0.9223 (0.6434)
Epoch: [8][ 54/500]	Time  5.504 ( 5.504)	Loss 0.7734 (1.6114)	CeLoss 0.7734 (0.5415)	SegCLSLoss 0.0000 (0.0107)	KLLoss 0.0000 (0.2195)	MaskLoss 0.0000 (0.5214)	MaskBCELoss 0.0000 (0.0914)	MaskDICELoss 0.0000 (0.4299)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 51 is less than current step: 499. Dropping entry: {'train/loss': 1.6782400131225585, 'train/ce_loss': 0.40595703125, 'train/seg_cls_loss': 0.013531494140625, 'train/kl_loss': 0.2865234375, 'train/mask_bce_loss': 0.06652911575511097, 'train/mask_dice_loss': 0.551838943362236, 'train/mask_loss': 0.6183680593967438, 'metrics/total_secs_per_batch': 6.5063745975494385, 'metrics/data_secs_per_batch': 2.685885500907898, '_timestamp': 1740978369.400924}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 51 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978369.4012501}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 52 is less than current step: 499. Dropping entry: {'train/loss': 1.9263159215450287, 'train/ce_loss': 0.328125, 'train/seg_cls_loss': 0.015673828125, 'train/kl_loss': 0.325390625, 'train/mask_bce_loss': 0.13542171847075224, 'train/mask_dice_loss': 0.6433612503111362, 'train/mask_loss': 0.7787829622626304, 'metrics/total_secs_per_batch': 6.217600584030151, 'metrics/data_secs_per_batch': 2.718254494667053, '_timestamp': 1740978375.6186512}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 52 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978375.6190171}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 53 is less than current step: 499. Dropping entry: {'train/loss': 1.611372208595276, 'train/ce_loss': 0.54150390625, 'train/seg_cls_loss': 0.0107177734375, 'train/kl_loss': 0.21953125, 'train/mask_bce_loss': 0.09143233001232147, 'train/mask_dice_loss': 0.4299276113510132, 'train/mask_loss': 0.5213599443435669, 'metrics/total_secs_per_batch': 5.503767251968384, 'metrics/data_secs_per_batch': 2.569990038871765, '_timestamp': 1740978381.122265}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 53 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978381.1225555}).
Epoch: [8][ 55/500]	Time  6.928 ( 6.928)	Loss 1.4258 (1.6768)	CeLoss 0.2344 (0.2655)	SegCLSLoss 0.0135 (0.0157)	KLLoss 0.3633 (0.2965)	MaskLoss 0.5742 (0.6869)	MaskBCELoss 0.0474 (0.1347)	MaskDICELoss 0.5268 (0.5522)
Epoch: [8][ 56/500]	Time  6.106 ( 6.106)	Loss 0.0825 (1.6346)	CeLoss 0.0825 (0.2504)	SegCLSLoss 0.0000 (0.0126)	KLLoss 0.0000 (0.2549)	MaskLoss 0.0000 (0.6762)	MaskBCELoss 0.0000 (0.1245)	MaskDICELoss 0.0000 (0.5517)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 54 is less than current step: 499. Dropping entry: {'train/loss': 1.6767532110214234, 'train/ce_loss': 0.26552734375, 'train/seg_cls_loss': 0.015728759765625, 'train/kl_loss': 0.296484375, 'train/mask_bce_loss': 0.13465991597622634, 'train/mask_dice_loss': 0.5522030144929886, 'train/mask_loss': 0.6868629336357117, 'metrics/total_secs_per_batch': 6.928373336791992, 'metrics/data_secs_per_batch': 2.76250901222229, '_timestamp': 1740978388.0505266}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 54 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978388.0507238}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 55 is less than current step: 499. Dropping entry: {'train/loss': 1.6346498608589173, 'train/ce_loss': 0.250390625, 'train/seg_cls_loss': 0.012646484375, 'train/kl_loss': 0.2548828125, 'train/mask_bce_loss': 0.1244960630312562, 'train/mask_dice_loss': 0.5517155885696411, 'train/mask_loss': 0.6762116491794586, 'metrics/total_secs_per_batch': 6.106250762939453, 'metrics/data_secs_per_batch': 2.779070830345154, '_timestamp': 1740978394.1570241}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 55 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978394.1573806}).
Epoch: [8][ 57/500]	Time  6.346 ( 6.346)	Loss 1.5760 (1.9258)	CeLoss 0.2930 (0.3105)	SegCLSLoss 0.0121 (0.0135)	KLLoss 0.3594 (0.2908)	MaskLoss 0.6210 (0.7897)	MaskBCELoss 0.0454 (0.2091)	MaskDICELoss 0.5756 (0.5806)
Epoch: [8][ 58/500]	Time  5.749 ( 5.749)	Loss 2.3151 (1.8043)	CeLoss 0.2490 (0.5052)	SegCLSLoss 0.0145 (0.0098)	KLLoss 0.3691 (0.2553)	MaskLoss 1.0111 (0.6344)	MaskBCELoss 0.4046 (0.1327)	MaskDICELoss 0.6065 (0.5017)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 56 is less than current step: 499. Dropping entry: {'train/loss': 1.9257829308509826, 'train/ce_loss': 0.310546875, 'train/seg_cls_loss': 0.013494873046875, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.2091239864937961, 'train/mask_dice_loss': 0.5805741339921952, 'train/mask_loss': 0.7896981120109559, 'metrics/total_secs_per_batch': 6.346197128295898, 'metrics/data_secs_per_batch': 2.746000790596008, '_timestamp': 1740978400.5030096}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 56 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978400.5032856}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 57 is less than current step: 499. Dropping entry: {'train/loss': 1.804254311323166, 'train/ce_loss': 0.505224609375, 'train/seg_cls_loss': 0.009759521484375, 'train/kl_loss': 0.2552734375, 'train/mask_bce_loss': 0.13267751317471266, 'train/mask_dice_loss': 0.5017494320869446, 'train/mask_loss': 0.634426960349083, 'metrics/total_secs_per_batch': 5.748990774154663, 'metrics/data_secs_per_batch': 2.2469556093215943, '_timestamp': 1740978406.252031}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 57 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978406.2523446}).
Epoch: [8][ 59/500]	Time  6.155 ( 6.155)	Loss 1.7241 (1.9248)	CeLoss 0.2402 (0.3315)	SegCLSLoss 0.0139 (0.0141)	KLLoss 0.3633 (0.3240)	MaskLoss 0.7204 (0.7770)	MaskBCELoss 0.0304 (0.2220)	MaskDICELoss 0.6901 (0.5550)
[2025-03-02 23:06:57,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=4060, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:06:57,617] [INFO] [timer.py:215:stop] epoch=0/micro_step=40600/global_step=4060, RunningAvgSamplesPerSec=1.4771944306717755, CurrSamplesPerSec=1.9194504307879134, MemAllocated=31.43GB, MaxMemAllocated=37.23GB
Epoch: [8][ 60/500]	Time  5.211 ( 5.211)	Loss 0.0698 (1.4786)	CeLoss 0.0698 (0.5287)	SegCLSLoss 0.0000 (0.0079)	KLLoss 0.0000 (0.1832)	MaskLoss 0.0000 (0.4639)	MaskBCELoss 0.0000 (0.0888)	MaskDICELoss 0.0000 (0.3751)
Epoch: [8][ 61/500]	Time  6.480 ( 6.480)	Loss 1.4600 (1.6869)	CeLoss 0.2363 (0.3246)	SegCLSLoss 0.0172 (0.0140)	KLLoss 0.3594 (0.3283)	MaskLoss 0.5894 (0.6614)	MaskBCELoss 0.0265 (0.0883)	MaskDICELoss 0.5628 (0.5732)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 58 is less than current step: 499. Dropping entry: {'train/loss': 1.9248476862907409, 'train/ce_loss': 0.33154296875, 'train/seg_cls_loss': 0.01407470703125, 'train/kl_loss': 0.3240234375, 'train/mask_bce_loss': 0.22196034397929906, 'train/mask_dice_loss': 0.5550142824649811, 'train/mask_loss': 0.776974618434906, 'metrics/total_secs_per_batch': 6.154957294464111, 'metrics/data_secs_per_batch': 2.735090398788452, '_timestamp': 1740978412.4069018}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 58 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978412.4070947}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 59 is less than current step: 499. Dropping entry: {'train/loss': 1.478605318069458, 'train/ce_loss': 0.528668212890625, 'train/seg_cls_loss': 0.007940673828125, 'train/kl_loss': 0.183203125, 'train/mask_bce_loss': 0.0887807327322662, 'train/mask_dice_loss': 0.3751038283109665, 'train/mask_loss': 0.4638845682144165, 'metrics/total_secs_per_batch': 5.211341381072998, 'metrics/data_secs_per_batch': 2.171485161781311, '_timestamp': 1740978417.6181839}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 59 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978417.61853}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 60 is less than current step: 499. Dropping entry: {'train/loss': 1.6869159817695618, 'train/ce_loss': 0.324609375, 'train/seg_cls_loss': 0.01395263671875, 'train/kl_loss': 0.3283203125, 'train/mask_bce_loss': 0.0882664205506444, 'train/mask_dice_loss': 0.5731603115797043, 'train/mask_loss': 0.6614267401397228, 'metrics/total_secs_per_batch': 6.480452299118042, 'metrics/data_secs_per_batch': 2.8467269420623778, '_timestamp': 1740978424.0987823}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 60 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978424.0990672}).
Epoch: [8][ 62/500]	Time  6.197 ( 6.197)	Loss 1.2218 (1.5925)	CeLoss 0.3223 (0.2804)	SegCLSLoss 0.0116 (0.0162)	KLLoss 0.3672 (0.3291)	MaskLoss 0.4283 (0.6355)	MaskBCELoss 0.0233 (0.0731)	MaskDICELoss 0.4050 (0.5624)
Epoch: [8][ 63/500]	Time  5.600 ( 5.600)	Loss 2.4023 (1.5628)	CeLoss 0.2363 (0.4618)	SegCLSLoss 0.0194 (0.0120)	KLLoss 0.3535 (0.2539)	MaskLoss 1.0605 (0.5347)	MaskBCELoss 0.1711 (0.0961)	MaskDICELoss 0.8895 (0.4386)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 61 is less than current step: 499. Dropping entry: {'train/loss': 1.5925332009792328, 'train/ce_loss': 0.28037109375, 'train/seg_cls_loss': 0.016180419921875, 'train/kl_loss': 0.3291015625, 'train/mask_bce_loss': 0.07307816324755549, 'train/mask_dice_loss': 0.562446242570877, 'train/mask_loss': 0.6355244010686875, 'metrics/total_secs_per_batch': 6.196579217910767, 'metrics/data_secs_per_batch': 2.539681816101074, '_timestamp': 1740978430.2953253}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 61 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978430.295608}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 62 is less than current step: 499. Dropping entry: {'train/loss': 1.5628469467163086, 'train/ce_loss': 0.46181640625, 'train/seg_cls_loss': 0.01199951171875, 'train/kl_loss': 0.25390625, 'train/mask_bce_loss': 0.09610815159976482, 'train/mask_dice_loss': 0.43863562047481536, 'train/mask_loss': 0.5347437739372254, 'metrics/total_secs_per_batch': 5.599694013595581, 'metrics/data_secs_per_batch': 2.340592861175537, '_timestamp': 1740978435.8952315}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 62 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978435.895575}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 63 is less than current step: 499. Dropping entry: {'train/loss': 1.4528785228729248, 'train/ce_loss': 0.4044921875, 'train/seg_cls_loss': 0.01356201171875, 'train/kl_loss': 0.221484375, 'train/mask_bce_loss': 0.05639161244034767, 'train/mask_dice_loss': 0.453446102142334, 'train/mask_loss': 0.5098377048969269, 'metrics/total_secs_per_batch': 5.845987319946289, 'metrics/data_secs_per_batch': 2.625735855102539, '_timestamp': 1740978441.740999}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 63 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978441.7413313}).
Epoch: [8][ 64/500]	Time  5.846 ( 5.846)	Loss 2.0138 (1.4529)	CeLoss 0.2617 (0.4045)	SegCLSLoss 0.0189 (0.0136)	KLLoss 0.3633 (0.2215)	MaskLoss 0.8526 (0.5098)	MaskBCELoss 0.0572 (0.0564)	MaskDICELoss 0.7954 (0.4534)
Epoch: [8][ 65/500]	Time  6.078 ( 6.078)	Loss 1.0838 (1.3617)	CeLoss 0.2070 (0.3578)	SegCLSLoss 0.0099 (0.0079)	KLLoss 0.3633 (0.2193)	MaskLoss 0.4179 (0.4889)	MaskBCELoss 0.1158 (0.0937)	MaskDICELoss 0.3021 (0.3952)
Epoch: [8][ 66/500]	Time  6.355 ( 6.355)	Loss 0.9648 (1.8228)	CeLoss 0.9648 (0.2805)	SegCLSLoss 0.0000 (0.0187)	KLLoss 0.0000 (0.3256)	MaskLoss 0.0000 (0.7502)	MaskBCELoss 0.0000 (0.1995)	MaskDICELoss 0.0000 (0.5507)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 64 is less than current step: 499. Dropping entry: {'train/loss': 1.3616522431373597, 'train/ce_loss': 0.3577880859375, 'train/seg_cls_loss': 0.007940673828125, 'train/kl_loss': 0.2193359375, 'train/mask_bce_loss': 0.09369963183999061, 'train/mask_dice_loss': 0.3951953262090683, 'train/mask_loss': 0.4888949662446976, 'metrics/total_secs_per_batch': 6.077871322631836, 'metrics/data_secs_per_batch': 2.63751482963562, '_timestamp': 1740978447.8189096}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 64 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978447.8191981}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 65 is less than current step: 499. Dropping entry: {'train/loss': 1.8228090584278107, 'train/ce_loss': 0.28046875, 'train/seg_cls_loss': 0.018743896484375, 'train/kl_loss': 0.3255859375, 'train/mask_bce_loss': 0.19951883852481841, 'train/mask_dice_loss': 0.5507040277123452, 'train/mask_loss': 0.7502228736877441, 'metrics/total_secs_per_batch': 6.355044364929199, 'metrics/data_secs_per_batch': 2.864034414291382, '_timestamp': 1740978454.1740763}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 65 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978454.1743972}).
Epoch: [8][ 67/500]	Time  6.758 ( 6.758)	Loss 2.6751 (1.9180)	CeLoss 0.1816 (0.3164)	SegCLSLoss 0.0182 (0.0167)	KLLoss 0.3594 (0.3270)	MaskLoss 1.2243 (0.7802)	MaskBCELoss 0.3514 (0.1485)	MaskDICELoss 0.8728 (0.6317)
Epoch: [8][ 68/500]	Time  5.460 ( 5.460)	Loss 0.9766 (1.5994)	CeLoss 0.9766 (0.3745)	SegCLSLoss 0.0000 (0.0136)	KLLoss 0.0000 (0.2928)	MaskLoss 0.0000 (0.5944)	MaskBCELoss 0.0000 (0.1624)	MaskDICELoss 0.0000 (0.4319)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 66 is less than current step: 499. Dropping entry: {'train/loss': 1.9179871201515197, 'train/ce_loss': 0.31640625, 'train/seg_cls_loss': 0.01669921875, 'train/kl_loss': 0.326953125, 'train/mask_bce_loss': 0.1485273465514183, 'train/mask_dice_loss': 0.6317064449191093, 'train/mask_loss': 0.7802337884902955, 'metrics/total_secs_per_batch': 6.75829291343689, 'metrics/data_secs_per_batch': 3.191755270957947, '_timestamp': 1740978460.9323456}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 66 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978460.932667}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 67 is less than current step: 499. Dropping entry: {'train/loss': 1.5994150757789611, 'train/ce_loss': 0.37451171875, 'train/seg_cls_loss': 0.013568115234375, 'train/kl_loss': 0.2927734375, 'train/mask_bce_loss': 0.16244509136304258, 'train/mask_dice_loss': 0.4319401726126671, 'train/mask_loss': 0.5943852752447129, 'metrics/total_secs_per_batch': 5.4598493576049805, 'metrics/data_secs_per_batch': 2.5519495010375977, '_timestamp': 1740978466.3920999}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 67 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978466.392404}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 68 is less than current step: 499. Dropping entry: {'train/loss': 1.2406359016895294, 'train/ce_loss': 0.52314453125, 'train/seg_cls_loss': 0.010968017578125, 'train/kl_loss': 0.2166015625, 'train/mask_bce_loss': 0.07013184949755669, 'train/mask_dice_loss': 0.2749907895922661, 'train/mask_loss': 0.34512264281511307, 'metrics/total_secs_per_batch': 5.500640153884888, 'metrics/data_secs_per_batch': 2.67882239818573, '_timestamp': 1740978471.8929298}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 68 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978471.893278}).
Epoch: [8][ 69/500]	Time  5.501 ( 5.501)	Loss 1.7629 (1.2406)	CeLoss 0.1807 (0.5231)	SegCLSLoss 0.0295 (0.0110)	KLLoss 0.3535 (0.2166)	MaskLoss 0.7662 (0.3451)	MaskBCELoss 0.2100 (0.0701)	MaskDICELoss 0.5562 (0.2750)
[2025-03-02 23:07:57,924] [INFO] [logging.py:96:log_dist] [Rank 0] step=4070, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:07:57,930] [INFO] [timer.py:215:stop] epoch=0/micro_step=40700/global_step=4070, RunningAvgSamplesPerSec=1.4775912465439258, CurrSamplesPerSec=1.6564479058036796, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [8][ 70/500]	Time  6.039 ( 6.039)	Loss 2.1098 (1.6342)	CeLoss 0.2656 (0.3084)	SegCLSLoss 0.0153 (0.0143)	KLLoss 0.3652 (0.2912)	MaskLoss 0.8997 (0.6448)	MaskBCELoss 0.0329 (0.0864)	MaskDICELoss 0.8668 (0.5584)
Epoch: [8][ 71/500]	Time  6.283 ( 6.283)	Loss 1.5318 (2.0201)	CeLoss 0.2129 (0.2820)	SegCLSLoss 0.0121 (0.0163)	KLLoss 0.3594 (0.3279)	MaskLoss 0.6389 (0.8485)	MaskBCELoss 0.0443 (0.2564)	MaskDICELoss 0.5947 (0.5921)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 69 is less than current step: 499. Dropping entry: {'train/loss': 1.6342405557632447, 'train/ce_loss': 0.3083984375, 'train/seg_cls_loss': 0.01434326171875, 'train/kl_loss': 0.2912109375, 'train/mask_bce_loss': 0.0863778087310493, 'train/mask_dice_loss': 0.5583791822195053, 'train/mask_loss': 0.6447569787502289, 'metrics/total_secs_per_batch': 6.038857460021973, 'metrics/data_secs_per_batch': 2.6916332483291625, '_timestamp': 1740978477.9313948}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 69 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978477.9316597}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 70 is less than current step: 499. Dropping entry: {'train/loss': 2.020121765136719, 'train/ce_loss': 0.28203125, 'train/seg_cls_loss': 0.016290283203125, 'train/kl_loss': 0.3279296875, 'train/mask_bce_loss': 0.25640891268849375, 'train/mask_dice_loss': 0.5920797020196915, 'train/mask_loss': 0.8484886169433594, 'metrics/total_secs_per_batch': 6.283101797103882, 'metrics/data_secs_per_batch': 2.8820649147033692, '_timestamp': 1740978484.2148852}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 70 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978484.2152336}).
Epoch: [8][ 72/500]	Time  6.061 ( 6.061)	Loss 2.0019 (1.8942)	CeLoss 0.2031 (0.3456)	SegCLSLoss 0.0177 (0.0140)	KLLoss 0.3633 (0.3270)	MaskLoss 0.8769 (0.7544)	MaskBCELoss 0.0331 (0.1690)	MaskDICELoss 0.8438 (0.5854)
Epoch: [8][ 73/500]	Time  5.118 ( 5.118)	Loss 1.2891 (1.6764)	CeLoss 1.2891 (0.5273)	SegCLSLoss 0.0000 (0.0107)	KLLoss 0.0000 (0.2211)	MaskLoss 0.0000 (0.5608)	MaskBCELoss 0.0000 (0.1019)	MaskDICELoss 0.0000 (0.4589)
Epoch: [8][ 74/500]	Time  5.854 ( 5.854)	Loss 0.7227 (1.2483)	CeLoss 0.7227 (0.3444)	SegCLSLoss 0.0000 (0.0088)	KLLoss 0.0000 (0.2199)	MaskLoss 0.0000 (0.4387)	MaskBCELoss 0.0000 (0.1071)	MaskDICELoss 0.0000 (0.3316)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 71 is less than current step: 499. Dropping entry: {'train/loss': 1.894172763824463, 'train/ce_loss': 0.34560546875, 'train/seg_cls_loss': 0.014044189453125, 'train/kl_loss': 0.326953125, 'train/mask_bce_loss': 0.16896505765616893, 'train/mask_dice_loss': 0.5854455351829528, 'train/mask_loss': 0.7544105887413025, 'metrics/total_secs_per_batch': 6.0612688064575195, 'metrics/data_secs_per_batch': 2.6873430728912355, '_timestamp': 1740978490.275992}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 71 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978490.2762897}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 72 is less than current step: 499. Dropping entry: {'train/loss': 1.6764324069023133, 'train/ce_loss': 0.527294921875, 'train/seg_cls_loss': 0.01072998046875, 'train/kl_loss': 0.22109375, 'train/mask_bce_loss': 0.1018980024382472, 'train/mask_dice_loss': 0.4588523805141449, 'train/mask_loss': 0.5607503831386567, 'metrics/total_secs_per_batch': 5.117818832397461, 'metrics/data_secs_per_batch': 2.7182010412216187, '_timestamp': 1740978495.3939908}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 72 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978495.3943424}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 73 is less than current step: 499. Dropping entry: {'train/loss': 1.2483348786830901, 'train/ce_loss': 0.3444091796875, 'train/seg_cls_loss': 0.0087646484375, 'train/kl_loss': 0.219921875, 'train/mask_bce_loss': 0.10714129563421011, 'train/mask_dice_loss': 0.331589138507843, 'train/mask_loss': 0.4387304276227951, 'metrics/total_secs_per_batch': 5.854164361953735, 'metrics/data_secs_per_batch': 2.5737364053726197, '_timestamp': 1740978501.2479858}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 73 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978501.2482882}).
Epoch: [8][ 75/500]	Time  6.293 ( 6.293)	Loss 1.6912 (1.5058)	CeLoss 0.2539 (0.3495)	SegCLSLoss 0.0143 (0.0114)	KLLoss 0.3633 (0.2887)	MaskLoss 0.6962 (0.5608)	MaskBCELoss 0.0444 (0.1577)	MaskDICELoss 0.6518 (0.4031)
Epoch: [8][ 76/500]	Time  6.135 ( 6.135)	Loss 1.0078 (1.7938)	CeLoss 1.0078 (0.3865)	SegCLSLoss 0.0000 (0.0121)	KLLoss 0.0000 (0.2889)	MaskLoss 0.0000 (0.6861)	MaskBCELoss 0.0000 (0.1074)	MaskDICELoss 0.0000 (0.5787)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 74 is less than current step: 499. Dropping entry: {'train/loss': 1.5058290004730224, 'train/ce_loss': 0.349462890625, 'train/seg_cls_loss': 0.011383056640625, 'train/kl_loss': 0.288671875, 'train/mask_bce_loss': 0.1576947309076786, 'train/mask_dice_loss': 0.4030566722154617, 'train/mask_loss': 0.5607514083385468, 'metrics/total_secs_per_batch': 6.292717456817627, 'metrics/data_secs_per_batch': 2.587565207481384, '_timestamp': 1740978507.5406773}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 74 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978507.540949}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 75 is less than current step: 499. Dropping entry: {'train/loss': 1.7937503457069397, 'train/ce_loss': 0.3865234375, 'train/seg_cls_loss': 0.01212158203125, 'train/kl_loss': 0.2888671875, 'train/mask_bce_loss': 0.10740904696285725, 'train/mask_dice_loss': 0.57872394323349, 'train/mask_loss': 0.6861329913139343, 'metrics/total_secs_per_batch': 6.135096311569214, 'metrics/data_secs_per_batch': 2.8500279426574706, '_timestamp': 1740978513.6759825}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 75 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978513.676338}).
Epoch: [8][ 77/500]	Time  6.799 ( 6.799)	Loss 1.9243 (1.1878)	CeLoss 0.2412 (0.3365)	SegCLSLoss 0.0160 (0.0095)	KLLoss 0.3652 (0.2180)	MaskLoss 0.8196 (0.4125)	MaskBCELoss 0.0508 (0.0598)	MaskDICELoss 0.7688 (0.3527)
Epoch: [8][ 78/500]	Time  5.601 ( 5.601)	Loss 1.0859 (1.4226)	CeLoss 1.0859 (0.5604)	SegCLSLoss 0.0000 (0.0099)	KLLoss 0.0000 (0.1789)	MaskLoss 0.0000 (0.4196)	MaskBCELoss 0.0000 (0.0387)	MaskDICELoss 0.0000 (0.3809)
Epoch: [8][ 79/500]	Time  5.405 ( 5.405)	Loss 2.1192 (2.0249)	CeLoss 0.2432 (0.4860)	SegCLSLoss 0.0170 (0.0140)	KLLoss 0.3594 (0.2545)	MaskLoss 0.9160 (0.7532)	MaskBCELoss 0.0288 (0.2249)	MaskDICELoss 0.8872 (0.5283)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 76 is less than current step: 499. Dropping entry: {'train/loss': 1.187841546535492, 'train/ce_loss': 0.3364990234375, 'train/seg_cls_loss': 0.009454345703125, 'train/kl_loss': 0.21796875, 'train/mask_bce_loss': 0.059773122891783714, 'train/mask_dice_loss': 0.35271453857421875, 'train/mask_loss': 0.41248766481876376, 'metrics/total_secs_per_batch': 6.798644304275513, 'metrics/data_secs_per_batch': 3.165519857406616, '_timestamp': 1740978520.4744308}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 76 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978520.4747133}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 77 is less than current step: 499. Dropping entry: {'train/loss': 1.4225755095481873, 'train/ce_loss': 0.56044921875, 'train/seg_cls_loss': 0.00986328125, 'train/kl_loss': 0.17890625, 'train/mask_bce_loss': 0.038705844152718785, 'train/mask_dice_loss': 0.3809315264225006, 'train/mask_loss': 0.41963736414909364, 'metrics/total_secs_per_batch': 5.601099967956543, 'metrics/data_secs_per_batch': 2.4134021282196043, '_timestamp': 1740978526.075546}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 77 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978526.0758467}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 78 is less than current step: 499. Dropping entry: {'train/loss': 2.0248629212379456, 'train/ce_loss': 0.48603515625, 'train/seg_cls_loss': 0.014013671875, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.22486305963248016, 'train/mask_dice_loss': 0.5282910645008088, 'train/mask_loss': 0.7531541168689728, 'metrics/total_secs_per_batch': 5.405061960220337, 'metrics/data_secs_per_batch': 2.510759162902832, '_timestamp': 1740978531.4807498}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 78 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978531.4811566}).
[2025-03-02 23:08:57,566] [INFO] [logging.py:96:log_dist] [Rank 0] step=4080, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:08:57,572] [INFO] [timer.py:215:stop] epoch=0/micro_step=40800/global_step=4080, RunningAvgSamplesPerSec=1.4780223105128434, CurrSamplesPerSec=1.6417967877681305, MemAllocated=30.71GB, MaxMemAllocated=37.23GB
Epoch: [8][ 80/500]	Time  6.093 ( 6.093)	Loss 1.4922 (1.6470)	CeLoss 1.4922 (0.4631)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.2934)	MaskLoss 0.0000 (0.5736)	MaskBCELoss 0.0000 (0.1004)	MaskDICELoss 0.0000 (0.4732)
Epoch: [8][ 81/500]	Time  6.057 ( 6.057)	Loss 1.2846 (1.9914)	CeLoss 0.2373 (0.3559)	SegCLSLoss 0.0092 (0.0161)	KLLoss 0.3613 (0.3260)	MaskLoss 0.5036 (0.7974)	MaskBCELoss 0.2766 (0.2352)	MaskDICELoss 0.2270 (0.5623)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 79 is less than current step: 499. Dropping entry: {'train/loss': 1.647026687860489, 'train/ce_loss': 0.4630859375, 'train/seg_cls_loss': 0.014227294921875, 'train/kl_loss': 0.293359375, 'train/mask_bce_loss': 0.10042983889579774, 'train/mask_dice_loss': 0.47318115681409834, 'train/mask_loss': 0.573610995709896, 'metrics/total_secs_per_batch': 6.092717885971069, 'metrics/data_secs_per_batch': 2.7959388732910155, '_timestamp': 1740978537.5731163}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 79 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978537.573385}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 80 is less than current step: 499. Dropping entry: {'train/loss': 1.9913576602935792, 'train/ce_loss': 0.355859375, 'train/seg_cls_loss': 0.01612548828125, 'train/kl_loss': 0.3259765625, 'train/mask_bce_loss': 0.23517561443150042, 'train/mask_dice_loss': 0.562261027097702, 'train/mask_loss': 0.7974366426467896, 'metrics/total_secs_per_batch': 6.057467699050903, 'metrics/data_secs_per_batch': 2.906146740913391, '_timestamp': 1740978543.6308014}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 80 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978543.6310925}).
Epoch: [8][ 82/500]	Time  6.232 ( 6.232)	Loss 2.8224 (1.6620)	CeLoss 0.2197 (0.2039)	SegCLSLoss 0.0227 (0.0149)	KLLoss 0.3555 (0.3299)	MaskLoss 1.2774 (0.7087)	MaskBCELoss 0.4594 (0.1556)	MaskDICELoss 0.8181 (0.5531)
Epoch: [8][ 83/500]	Time  6.421 ( 6.421)	Loss 2.4586 (1.9851)	CeLoss 0.3516 (0.3283)	SegCLSLoss 0.0091 (0.0149)	KLLoss 0.3633 (0.3236)	MaskLoss 1.0330 (0.8084)	MaskBCELoss 0.4984 (0.1465)	MaskDICELoss 0.5346 (0.6619)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 81 is less than current step: 499. Dropping entry: {'train/loss': 1.6620018243789674, 'train/ce_loss': 0.20390625, 'train/seg_cls_loss': 0.014947509765625, 'train/kl_loss': 0.3298828125, 'train/mask_bce_loss': 0.15561353228986263, 'train/mask_dice_loss': 0.5531217485666275, 'train/mask_loss': 0.7087352961301804, 'metrics/total_secs_per_batch': 6.23167085647583, 'metrics/data_secs_per_batch': 2.697550106048584, '_timestamp': 1740978549.862494}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 81 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978549.862792}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 82 is less than current step: 499. Dropping entry: {'train/loss': 1.9850742101669312, 'train/ce_loss': 0.3283203125, 'train/seg_cls_loss': 0.01490478515625, 'train/kl_loss': 0.3236328125, 'train/mask_bce_loss': 0.14646264826878905, 'train/mask_dice_loss': 0.6619435966014862, 'train/mask_loss': 0.8084062516689301, 'metrics/total_secs_per_batch': 6.420853853225708, 'metrics/data_secs_per_batch': 2.9888992309570312, '_timestamp': 1740978556.2835004}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 82 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978556.2838347}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 83 is less than current step: 499. Dropping entry: {'train/loss': 1.449767404794693, 'train/ce_loss': 0.449560546875, 'train/seg_cls_loss': 0.009478759765625, 'train/kl_loss': 0.21484375, 'train/mask_bce_loss': 0.028405375266447663, 'train/mask_dice_loss': 0.45880741626024246, 'train/mask_loss': 0.4872127890586853, 'metrics/total_secs_per_batch': 5.71093487739563, 'metrics/data_secs_per_batch': 2.355443024635315, '_timestamp': 1740978561.9942784}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 83 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978561.9946299}).
Epoch: [8][ 84/500]	Time  5.711 ( 5.711)	Loss 1.3281 (1.4498)	CeLoss 0.2041 (0.4496)	SegCLSLoss 0.0170 (0.0095)	KLLoss 0.3594 (0.2148)	MaskLoss 0.5395 (0.4872)	MaskBCELoss 0.0980 (0.0284)	MaskDICELoss 0.4415 (0.4588)
Epoch: [8][ 85/500]	Time  5.967 ( 5.967)	Loss 1.3594 (1.6407)	CeLoss 1.3594 (0.3184)	SegCLSLoss 0.0000 (0.0161)	KLLoss 0.0000 (0.2898)	MaskLoss 0.0000 (0.6428)	MaskBCELoss 0.0000 (0.1736)	MaskDICELoss 0.0000 (0.4692)
Epoch: [8][ 86/500]	Time  6.984 ( 6.984)	Loss 1.3159 (1.8867)	CeLoss 0.2734 (0.2411)	SegCLSLoss 0.0106 (0.0154)	KLLoss 0.3691 (0.3621)	MaskLoss 0.4997 (0.8008)	MaskBCELoss 0.1308 (0.1706)	MaskDICELoss 0.3690 (0.6302)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 84 is less than current step: 499. Dropping entry: {'train/loss': 1.6407249927520753, 'train/ce_loss': 0.318359375, 'train/seg_cls_loss': 0.016094970703125, 'train/kl_loss': 0.28984375, 'train/mask_bce_loss': 0.1735548462718725, 'train/mask_dice_loss': 0.4692197620868683, 'train/mask_loss': 0.6427746057510376, 'metrics/total_secs_per_batch': 5.96704888343811, 'metrics/data_secs_per_batch': 2.6772593021392823, '_timestamp': 1740978567.961524}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 84 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978567.961874}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 85 is less than current step: 499. Dropping entry: {'train/loss': 1.886748719215393, 'train/ce_loss': 0.24111328125, 'train/seg_cls_loss': 0.015380859375, 'train/kl_loss': 0.362109375, 'train/mask_bce_loss': 0.170619997382164, 'train/mask_dice_loss': 0.6301762282848358, 'train/mask_loss': 0.8007962286472321, 'metrics/total_secs_per_batch': 6.984318971633911, 'metrics/data_secs_per_batch': 3.1053430318832396, '_timestamp': 1740978574.945649}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 85 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978574.9459395}).
Epoch: [8][ 87/500]	Time  7.443 ( 7.443)	Loss 1.3147 (1.4357)	CeLoss 0.2734 (0.3444)	SegCLSLoss 0.0103 (0.0108)	KLLoss 0.3672 (0.2564)	MaskLoss 0.4991 (0.5301)	MaskBCELoss 0.1193 (0.0998)	MaskDICELoss 0.3799 (0.4303)
Epoch: [8][ 88/500]	Time  5.742 ( 5.742)	Loss 1.3381 (1.5643)	CeLoss 0.2891 (0.5725)	SegCLSLoss 0.0107 (0.0116)	KLLoss 0.3633 (0.2512)	MaskLoss 0.5030 (0.4804)	MaskBCELoss 0.0527 (0.0268)	MaskDICELoss 0.4503 (0.4536)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 86 is less than current step: 499. Dropping entry: {'train/loss': 1.4356542468070983, 'train/ce_loss': 0.34443359375, 'train/seg_cls_loss': 0.0108154296875, 'train/kl_loss': 0.2564453125, 'train/mask_bce_loss': 0.09975929129868746, 'train/mask_dice_loss': 0.43032369315624236, 'train/mask_loss': 0.5300829827785491, 'metrics/total_secs_per_batch': 7.442652463912964, 'metrics/data_secs_per_batch': 3.193740653991699, '_timestamp': 1740978582.3882768}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 86 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978582.3886278}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 87 is less than current step: 499. Dropping entry: {'train/loss': 1.5642951488494874, 'train/ce_loss': 0.5724609375, 'train/seg_cls_loss': 0.01158447265625, 'train/kl_loss': 0.251171875, 'train/mask_bce_loss': 0.0268202249892056, 'train/mask_dice_loss': 0.4535695374011993, 'train/mask_loss': 0.48038976192474364, 'metrics/total_secs_per_batch': 5.741810083389282, 'metrics/data_secs_per_batch': 2.5669765710830688, '_timestamp': 1740978588.1302907}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 87 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978588.1307046}).
Epoch: [8][ 89/500]	Time  5.389 ( 5.389)	Loss 2.2700 (1.7022)	CeLoss 0.2012 (0.6486)	SegCLSLoss 0.0178 (0.0100)	KLLoss 0.3496 (0.2176)	MaskLoss 1.0124 (0.5134)	MaskBCELoss 0.0128 (0.1273)	MaskDICELoss 0.9996 (0.3861)
[2025-03-02 23:09:58,818] [INFO] [logging.py:96:log_dist] [Rank 0] step=4090, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:09:58,824] [INFO] [timer.py:215:stop] epoch=0/micro_step=40900/global_step=4090, RunningAvgSamplesPerSec=1.478365427872733, CurrSamplesPerSec=1.8854269443045062, MemAllocated=31.26GB, MaxMemAllocated=37.23GB
Epoch: [8][ 90/500]	Time  5.305 ( 5.305)	Loss 1.7806 (1.6178)	CeLoss 0.2080 (0.4898)	SegCLSLoss 0.0215 (0.0144)	KLLoss 0.3535 (0.2574)	MaskLoss 0.7634 (0.5475)	MaskBCELoss 0.0242 (0.1084)	MaskDICELoss 0.7392 (0.4391)
Epoch: [8][ 91/500]	Time  5.649 ( 5.649)	Loss 1.1550 (1.1958)	CeLoss 0.2891 (0.4654)	SegCLSLoss 0.0101 (0.0074)	KLLoss 0.3691 (0.2188)	MaskLoss 0.4115 (0.3523)	MaskBCELoss 0.1130 (0.1134)	MaskDICELoss 0.2984 (0.2389)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 88 is less than current step: 499. Dropping entry: {'train/loss': 1.7021847367286682, 'train/ce_loss': 0.648583984375, 'train/seg_cls_loss': 0.01004638671875, 'train/kl_loss': 0.217578125, 'train/mask_bce_loss': 0.12732314029708505, 'train/mask_dice_loss': 0.3860983312129974, 'train/mask_loss': 0.5134214699268341, 'metrics/total_secs_per_batch': 5.389481067657471, 'metrics/data_secs_per_batch': 1.9117518186569213, '_timestamp': 1740978593.519574}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 88 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978593.5198607}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 89 is less than current step: 499. Dropping entry: {'train/loss': 1.617763090133667, 'train/ce_loss': 0.489794921875, 'train/seg_cls_loss': 0.01441650390625, 'train/kl_loss': 0.257421875, 'train/mask_bce_loss': 0.108378273434937, 'train/mask_dice_loss': 0.4391263246536255, 'train/mask_loss': 0.5475045919418335, 'metrics/total_secs_per_batch': 5.305423259735107, 'metrics/data_secs_per_batch': 2.312712025642395, '_timestamp': 1740978598.824824}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 89 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978598.8251162}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 90 is less than current step: 499. Dropping entry: {'train/loss': 1.1958273887634276, 'train/ce_loss': 0.4654052734375, 'train/seg_cls_loss': 0.007427978515625, 'train/kl_loss': 0.21875, 'train/mask_bce_loss': 0.11336599662899971, 'train/mask_dice_loss': 0.23890560865402222, 'train/mask_loss': 0.3522716075181961, 'metrics/total_secs_per_batch': 5.64942479133606, 'metrics/data_secs_per_batch': 2.39376814365387, '_timestamp': 1740978604.4744196}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 90 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978604.4747047}).
Epoch: [8][ 92/500]	Time  6.082 ( 6.082)	Loss 2.0593 (1.6990)	CeLoss 0.2578 (0.4632)	SegCLSLoss 0.0151 (0.0115)	KLLoss 0.3535 (0.2514)	MaskLoss 0.8793 (0.6024)	MaskBCELoss 0.0270 (0.0644)	MaskDICELoss 0.8523 (0.5380)
Epoch: [8][ 93/500]	Time  6.344 ( 6.344)	Loss 1.4405 (1.7319)	CeLoss 0.2441 (0.3723)	SegCLSLoss 0.0092 (0.0138)	KLLoss 0.3652 (0.3252)	MaskLoss 0.5776 (0.6601)	MaskBCELoss 0.2618 (0.1771)	MaskDICELoss 0.3158 (0.4829)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 91 is less than current step: 499. Dropping entry: {'train/loss': 1.698994505405426, 'train/ce_loss': 0.46318359375, 'train/seg_cls_loss': 0.011492919921875, 'train/kl_loss': 0.2513671875, 'train/mask_bce_loss': 0.06437926832586527, 'train/mask_dice_loss': 0.5380476891994477, 'train/mask_loss': 0.6024269610643387, 'metrics/total_secs_per_batch': 6.081722021102905, 'metrics/data_secs_per_batch': 2.8731138467788697, '_timestamp': 1740978610.5563536}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 91 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978610.5566993}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 92 is less than current step: 499. Dropping entry: {'train/loss': 1.7318607568740845, 'train/ce_loss': 0.372265625, 'train/seg_cls_loss': 0.013836669921875, 'train/kl_loss': 0.3251953125, 'train/mask_bce_loss': 0.1771483538672328, 'train/mask_dice_loss': 0.48292264342308044, 'train/mask_loss': 0.6600709974765777, 'metrics/total_secs_per_batch': 6.343603610992432, 'metrics/data_secs_per_batch': 3.066871738433838, '_timestamp': 1740978616.899799}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 92 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978616.9000936}).
Epoch: [8][ 94/500]	Time  6.303 ( 6.303)	Loss 1.2349 (1.8998)	CeLoss 0.3008 (0.3161)	SegCLSLoss 0.0118 (0.0150)	KLLoss 0.3613 (0.3285)	MaskLoss 0.4456 (0.7717)	MaskBCELoss 0.0874 (0.1381)	MaskDICELoss 0.3581 (0.6336)
Epoch: [8][ 95/500]	Time  6.684 ( 6.684)	Loss 1.7212 (1.6134)	CeLoss 0.2393 (0.2116)	SegCLSLoss 0.0200 (0.0156)	KLLoss 0.3535 (0.3250)	MaskLoss 0.7180 (0.6807)	MaskBCELoss 0.0574 (0.0492)	MaskDICELoss 0.6606 (0.6316)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 93 is less than current step: 499. Dropping entry: {'train/loss': 1.899830412864685, 'train/ce_loss': 0.31611328125, 'train/seg_cls_loss': 0.01502685546875, 'train/kl_loss': 0.328515625, 'train/mask_bce_loss': 0.13811909086070956, 'train/mask_dice_loss': 0.6336222738027573, 'train/mask_loss': 0.771741372346878, 'metrics/total_secs_per_batch': 6.303369522094727, 'metrics/data_secs_per_batch': 2.985449457168579, '_timestamp': 1740978623.2031786}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 93 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978623.203479}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 94 is less than current step: 499. Dropping entry: {'train/loss': 1.6134352684020996, 'train/ce_loss': 0.21162109375, 'train/seg_cls_loss': 0.01558837890625, 'train/kl_loss': 0.325, 'train/mask_bce_loss': 0.04917450910434127, 'train/mask_dice_loss': 0.6315665543079376, 'train/mask_loss': 0.6807410717010498, 'metrics/total_secs_per_batch': 6.684340000152588, 'metrics/data_secs_per_batch': 2.915812087059021, '_timestamp': 1740978629.8877165}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 94 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978629.888093}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 95 is less than current step: 499. Dropping entry: {'train/loss': 1.663951575756073, 'train/ce_loss': 0.43505859375, 'train/seg_cls_loss': 0.012738037109375, 'train/kl_loss': 0.25390625, 'train/mask_bce_loss': 0.12058108188211918, 'train/mask_dice_loss': 0.4779962718486786, 'train/mask_loss': 0.5985773503780365, 'metrics/total_secs_per_batch': 5.989964246749878, 'metrics/data_secs_per_batch': 2.799082541465759, '_timestamp': 1740978635.8774412}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 95 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978635.8777905}).
Epoch: [8][ 96/500]	Time  5.990 ( 5.990)	Loss 2.0420 (1.6640)	CeLoss 0.2324 (0.4351)	SegCLSLoss 0.0162 (0.0127)	KLLoss 0.3613 (0.2539)	MaskLoss 0.8823 (0.5986)	MaskBCELoss 0.0572 (0.1206)	MaskDICELoss 0.8251 (0.4780)
Epoch: [8][ 97/500]	Time  6.982 ( 6.982)	Loss 1.6463 (1.4547)	CeLoss 0.2168 (0.2083)	SegCLSLoss 0.0322 (0.0139)	KLLoss 0.3711 (0.2549)	MaskLoss 0.6884 (0.6070)	MaskBCELoss 0.2267 (0.1063)	MaskDICELoss 0.4616 (0.5006)
Epoch: [8][ 98/500]	Time  7.472 ( 7.472)	Loss 2.4705 (1.5804)	CeLoss 0.1221 (0.2257)	SegCLSLoss 0.0299 (0.0145)	KLLoss 0.3984 (0.3326)	MaskLoss 1.1469 (0.6571)	MaskBCELoss 0.2683 (0.1254)	MaskDICELoss 0.8786 (0.5316)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 96 is less than current step: 499. Dropping entry: {'train/loss': 1.4547435283660888, 'train/ce_loss': 0.20830078125, 'train/seg_cls_loss': 0.01385498046875, 'train/kl_loss': 0.2548828125, 'train/mask_bce_loss': 0.10633740331977606, 'train/mask_dice_loss': 0.5006242156028747, 'train/mask_loss': 0.6069616198539733, 'metrics/total_secs_per_batch': 6.982177019119263, 'metrics/data_secs_per_batch': 2.980080556869507, '_timestamp': 1740978642.8597713}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 96 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978642.8601074}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 97 is less than current step: 499. Dropping entry: {'train/loss': 1.5803738296031953, 'train/ce_loss': 0.225732421875, 'train/seg_cls_loss': 0.014471435546875, 'train/kl_loss': 0.3326171875, 'train/mask_bce_loss': 0.12544404957443475, 'train/mask_dice_loss': 0.5316129952669144, 'train/mask_loss': 0.657057037949562, 'metrics/total_secs_per_batch': 7.472164154052734, 'metrics/data_secs_per_batch': 3.3947906494140625, '_timestamp': 1740978650.3319795}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 97 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978650.3323195}).
Epoch: [8][ 99/500]	Time  6.409 ( 6.409)	Loss 1.5327 (1.7165)	CeLoss 0.1641 (0.3153)	SegCLSLoss 0.0254 (0.0167)	KLLoss 0.3535 (0.3260)	MaskLoss 0.6604 (0.6800)	MaskBCELoss 0.1099 (0.1367)	MaskDICELoss 0.5505 (0.5433)
[2025-03-02 23:11:02,327] [INFO] [logging.py:96:log_dist] [Rank 0] step=4100, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:11:02,333] [INFO] [timer.py:215:stop] epoch=0/micro_step=41000/global_step=4100, RunningAvgSamplesPerSec=1.4785866117203792, CurrSamplesPerSec=1.7884633203335887, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [8][100/500]	Time  5.593 ( 5.593)	Loss 1.2644 (1.5402)	CeLoss 0.3320 (0.3886)	SegCLSLoss 0.0129 (0.0129)	KLLoss 0.3633 (0.2945)	MaskLoss 0.4447 (0.5579)	MaskBCELoss 0.1068 (0.1235)	MaskDICELoss 0.3379 (0.4344)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 98 is less than current step: 499. Dropping entry: {'train/loss': 1.7164811372756958, 'train/ce_loss': 0.31533203125, 'train/seg_cls_loss': 0.016705322265625, 'train/kl_loss': 0.3259765625, 'train/mask_bce_loss': 0.13666227404028178, 'train/mask_dice_loss': 0.5433068066835404, 'train/mask_loss': 0.6799690753221512, 'metrics/total_secs_per_batch': 6.408961057662964, 'metrics/data_secs_per_batch': 3.11293363571167, '_timestamp': 1740978656.7407858}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 98 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978656.7410724}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 99 is less than current step: 499. Dropping entry: {'train/loss': 1.5401564240455627, 'train/ce_loss': 0.38857421875, 'train/seg_cls_loss': 0.012890625, 'train/kl_loss': 0.29453125, 'train/mask_bce_loss': 0.12351973112672568, 'train/mask_dice_loss': 0.4343514382839203, 'train/mask_loss': 0.5578711777925491, 'metrics/total_secs_per_batch': 5.593028783798218, 'metrics/data_secs_per_batch': 2.415381574630737, '_timestamp': 1740978662.3336434}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 99 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978662.3339596}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 100 is less than current step: 499. Dropping entry: {'train/loss': 1.5760360479354858, 'train/ce_loss': 0.6443359375, 'train/seg_cls_loss': 0.0125, 'train/kl_loss': 0.219140625, 'train/mask_bce_loss': 0.07667182628065347, 'train/mask_dice_loss': 0.37496923059225085, 'train/mask_loss': 0.45164105892181394, 'metrics/total_secs_per_batch': 4.834635257720947, 'metrics/data_secs_per_batch': 2.161406683921814, '_timestamp': 1740978667.1685894}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 100 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978667.1689162}).
Epoch: [8][101/500]	Time  4.835 ( 4.835)	Loss 2.0728 (1.5760)	CeLoss 0.2246 (0.6443)	SegCLSLoss 0.0212 (0.0125)	KLLoss 0.3574 (0.2191)	MaskLoss 0.9007 (0.4516)	MaskBCELoss 0.0241 (0.0767)	MaskDICELoss 0.8765 (0.3750)
Epoch: [8][102/500]	Time  5.625 ( 5.625)	Loss 2.2591 (2.3242)	CeLoss 0.1895 (0.4604)	SegCLSLoss 0.0264 (0.0130)	KLLoss 0.3516 (0.2535)	MaskLoss 1.0104 (0.9160)	MaskBCELoss 0.0265 (0.3891)	MaskDICELoss 0.9839 (0.5269)
Epoch: [8][103/500]	Time  5.328 ( 5.328)	Loss 1.5280 (1.7551)	CeLoss 0.2754 (0.4175)	SegCLSLoss 0.0098 (0.0117)	KLLoss 0.3652 (0.2549)	MaskLoss 0.6048 (0.6531)	MaskBCELoss 0.0885 (0.2069)	MaskDICELoss 0.5163 (0.4462)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 101 is less than current step: 499. Dropping entry: {'train/loss': 2.324159848690033, 'train/ce_loss': 0.46044921875, 'train/seg_cls_loss': 0.012969970703125, 'train/kl_loss': 0.253515625, 'train/mask_bce_loss': 0.3890939552336931, 'train/mask_dice_loss': 0.526892215013504, 'train/mask_loss': 0.915986168384552, 'metrics/total_secs_per_batch': 5.625137805938721, 'metrics/data_secs_per_batch': 2.4425447940826417, '_timestamp': 1740978672.793629}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 101 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978672.7940087}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 102 is less than current step: 499. Dropping entry: {'train/loss': 1.7551344871520995, 'train/ce_loss': 0.41748046875, 'train/seg_cls_loss': 0.011676025390625, 'train/kl_loss': 0.2548828125, 'train/mask_bce_loss': 0.20685549285262822, 'train/mask_dice_loss': 0.44620003551244736, 'train/mask_loss': 0.6530555248260498, 'metrics/total_secs_per_batch': 5.328461647033691, 'metrics/data_secs_per_batch': 2.3177680015563964, '_timestamp': 1740978678.1222177}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 102 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978678.1225595}).
Epoch: [8][104/500]	Time  5.623 ( 5.623)	Loss 1.0728 (1.5924)	CeLoss 0.2217 (0.4329)	SegCLSLoss 0.0156 (0.0115)	KLLoss 0.3555 (0.2539)	MaskLoss 0.4036 (0.5641)	MaskBCELoss 0.0033 (0.0617)	MaskDICELoss 0.4003 (0.5024)
Epoch: [8][105/500]	Time  5.783 ( 5.783)	Loss 1.9194 (1.8295)	CeLoss 0.2119 (0.5264)	SegCLSLoss 0.0167 (0.0122)	KLLoss 0.3652 (0.2533)	MaskLoss 0.8308 (0.6359)	MaskBCELoss 0.0555 (0.1105)	MaskDICELoss 0.7753 (0.5254)
Epoch: [8][106/500]	Time  6.071 ( 6.071)	Loss 0.6638 (1.5878)	CeLoss 0.2910 (0.4968)	SegCLSLoss 0.0107 (0.0118)	KLLoss 0.3672 (0.2512)	MaskLoss 0.1649 (0.5298)	MaskBCELoss 0.0537 (0.0995)	MaskDICELoss 0.1113 (0.4303)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 103 is less than current step: 499. Dropping entry: {'train/loss': 1.592410385608673, 'train/ce_loss': 0.43291015625, 'train/seg_cls_loss': 0.011468505859375, 'train/kl_loss': 0.25390625, 'train/mask_bce_loss': 0.061675473139621316, 'train/mask_dice_loss': 0.502449631690979, 'train/mask_loss': 0.5641250997781754, 'metrics/total_secs_per_batch': 5.623277902603149, 'metrics/data_secs_per_batch': 2.6035394191741945, '_timestamp': 1740978683.745319}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 103 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978683.745512}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 104 is less than current step: 499. Dropping entry: {'train/loss': 1.829512858390808, 'train/ce_loss': 0.5263671875, 'train/seg_cls_loss': 0.0121826171875, 'train/kl_loss': 0.2533203125, 'train/mask_bce_loss': 0.1104895208030939, 'train/mask_dice_loss': 0.5253606498241424, 'train/mask_loss': 0.6358501732349395, 'metrics/total_secs_per_batch': 5.782684087753296, 'metrics/data_secs_per_batch': 2.703313207626343, '_timestamp': 1740978689.5280561}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 104 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978689.5283449}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 105 is less than current step: 499. Dropping entry: {'train/loss': 1.5878295063972474, 'train/ce_loss': 0.496826171875, 'train/seg_cls_loss': 0.0117919921875, 'train/kl_loss': 0.251171875, 'train/mask_bce_loss': 0.09949821457266808, 'train/mask_dice_loss': 0.43032963275909425, 'train/mask_loss': 0.5298278510570527, 'metrics/total_secs_per_batch': 6.071152210235596, 'metrics/data_secs_per_batch': 2.9163498401641847, '_timestamp': 1740978695.5994234}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 105 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978695.5997803}).
Epoch: [8][107/500]	Time  6.444 ( 6.444)	Loss 1.3631 (1.9879)	CeLoss 0.2324 (0.4018)	SegCLSLoss 0.0214 (0.0169)	KLLoss 0.3574 (0.2900)	MaskLoss 0.5419 (0.7743)	MaskBCELoss 0.1818 (0.1964)	MaskDICELoss 0.3601 (0.5779)
Epoch: [8][108/500]	Time  4.658 ( 4.658)	Loss 2.0255 (1.6386)	CeLoss 0.2129 (0.6121)	SegCLSLoss 0.0164 (0.0100)	KLLoss 0.3633 (0.2182)	MaskLoss 0.8839 (0.4998)	MaskBCELoss 0.0692 (0.0609)	MaskDICELoss 0.8146 (0.4389)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 106 is less than current step: 499. Dropping entry: {'train/loss': 1.9879441142082215, 'train/ce_loss': 0.4017578125, 'train/seg_cls_loss': 0.016888427734375, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.1964149534702301, 'train/mask_dice_loss': 0.5778793692588806, 'train/mask_loss': 0.7742943227291107, 'metrics/total_secs_per_batch': 6.444147109985352, 'metrics/data_secs_per_batch': 3.1114206075668336, '_timestamp': 1740978702.0433588}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 106 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978702.0436485}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 107 is less than current step: 499. Dropping entry: {'train/loss': 1.638568603992462, 'train/ce_loss': 0.612109375, 'train/seg_cls_loss': 0.0100341796875, 'train/kl_loss': 0.2181640625, 'train/mask_bce_loss': 0.06089629512280226, 'train/mask_dice_loss': 0.43885675966739657, 'train/mask_loss': 0.4997530519962311, 'metrics/total_secs_per_batch': 4.658254384994507, 'metrics/data_secs_per_batch': 2.17823121547699, '_timestamp': 1740978706.7016246}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 107 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978706.7020068}).
Epoch: [8][109/500]	Time  6.277 ( 6.277)	Loss 1.6484 (1.8843)	CeLoss 1.6484 (0.7174)	SegCLSLoss 0.0000 (0.0103)	KLLoss 0.0000 (0.2156)	MaskLoss 0.0000 (0.5701)	MaskBCELoss 0.0000 (0.0672)	MaskDICELoss 0.0000 (0.5028)
[2025-03-02 23:11:59,160] [INFO] [logging.py:96:log_dist] [Rank 0] step=4110, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:11:59,167] [INFO] [timer.py:215:stop] epoch=0/micro_step=41100/global_step=4110, RunningAvgSamplesPerSec=1.4791622066573888, CurrSamplesPerSec=1.6162976972201732, MemAllocated=31.23GB, MaxMemAllocated=37.23GB
Epoch: [8][110/500]	Time  6.189 ( 6.189)	Loss 1.1837 (1.7200)	CeLoss 0.2832 (0.4097)	SegCLSLoss 0.0111 (0.0139)	KLLoss 0.3691 (0.2922)	MaskLoss 0.4288 (0.6369)	MaskBCELoss 0.1289 (0.1113)	MaskDICELoss 0.2998 (0.5256)
Epoch: [8][111/500]	Time  4.995 ( 4.995)	Loss 1.2188 (1.7240)	CeLoss 1.2188 (0.5856)	SegCLSLoss 0.0000 (0.0102)	KLLoss 0.0000 (0.1816)	MaskLoss 0.0000 (0.5574)	MaskBCELoss 0.0000 (0.1385)	MaskDICELoss 0.0000 (0.4189)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 108 is less than current step: 499. Dropping entry: {'train/loss': 1.8842800617218018, 'train/ce_loss': 0.7173828125, 'train/seg_cls_loss': 0.01033935546875, 'train/kl_loss': 0.215625, 'train/mask_bce_loss': 0.06722920462489128, 'train/mask_dice_loss': 0.5028404951095581, 'train/mask_loss': 0.5700697064399719, 'metrics/total_secs_per_batch': 6.277484655380249, 'metrics/data_secs_per_batch': 2.796217203140259, '_timestamp': 1740978712.9791043}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 108 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978712.9794145}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 109 is less than current step: 499. Dropping entry: {'train/loss': 1.719953143596649, 'train/ce_loss': 0.40966796875, 'train/seg_cls_loss': 0.013922119140625, 'train/kl_loss': 0.2921875, 'train/mask_bce_loss': 0.11128727439790964, 'train/mask_dice_loss': 0.5255936026573181, 'train/mask_loss': 0.6368808746337891, 'metrics/total_secs_per_batch': 6.188632249832153, 'metrics/data_secs_per_batch': 2.7622013807296755, '_timestamp': 1740978719.1675427}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 109 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978719.1678584}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 110 is less than current step: 499. Dropping entry: {'train/loss': 1.723993182182312, 'train/ce_loss': 0.58564453125, 'train/seg_cls_loss': 0.0101806640625, 'train/kl_loss': 0.181640625, 'train/mask_bce_loss': 0.13850064910948276, 'train/mask_dice_loss': 0.4188572645187378, 'train/mask_loss': 0.557357919216156, 'metrics/total_secs_per_batch': 4.9951722621917725, 'metrics/data_secs_per_batch': 2.0361367225646974, '_timestamp': 1740978724.1628926}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 110 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978724.1631742}).
Epoch: [8][112/500]	Time  5.946 ( 5.946)	Loss 2.2199 (1.2772)	CeLoss 0.2354 (0.3897)	SegCLSLoss 0.0186 (0.0114)	KLLoss 0.3652 (0.2178)	MaskLoss 0.9693 (0.4300)	MaskBCELoss 0.0286 (0.0403)	MaskDICELoss 0.9408 (0.3898)
Epoch: [8][113/500]	Time  5.455 ( 5.455)	Loss 2.4350 (1.7296)	CeLoss 0.2383 (0.4456)	SegCLSLoss 0.0140 (0.0117)	KLLoss 0.3574 (0.2896)	MaskLoss 1.0769 (0.6247)	MaskBCELoss 0.3821 (0.1462)	MaskDICELoss 0.6948 (0.4784)
Epoch: [8][114/500]	Time  5.727 ( 5.727)	Loss 1.5999 (1.5801)	CeLoss 0.1865 (0.3071)	SegCLSLoss 0.0217 (0.0179)	KLLoss 0.3594 (0.3295)	MaskLoss 0.6833 (0.6154)	MaskBCELoss 0.0987 (0.1405)	MaskDICELoss 0.5846 (0.4749)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 111 is less than current step: 499. Dropping entry: {'train/loss': 1.2772366166114808, 'train/ce_loss': 0.38974609375, 'train/seg_cls_loss': 0.011376953125, 'train/kl_loss': 0.2177734375, 'train/mask_bce_loss': 0.04026833008974791, 'train/mask_dice_loss': 0.38975624144077303, 'train/mask_loss': 0.4300245761871338, 'metrics/total_secs_per_batch': 5.9458441734313965, 'metrics/data_secs_per_batch': 2.3649869203567504, '_timestamp': 1740978730.108763}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 111 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978730.1090634}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 112 is less than current step: 499. Dropping entry: {'train/loss': 1.729597806930542, 'train/ce_loss': 0.44560546875, 'train/seg_cls_loss': 0.011724853515625, 'train/kl_loss': 0.2896484375, 'train/mask_bce_loss': 0.1462400084361434, 'train/mask_dice_loss': 0.478422175347805, 'train/mask_loss': 0.624662184715271, 'metrics/total_secs_per_batch': 5.45473575592041, 'metrics/data_secs_per_batch': 2.2570316076278685, '_timestamp': 1740978735.5635753}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 112 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978735.5638912}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 113 is less than current step: 499. Dropping entry: {'train/loss': 1.5800965130329132, 'train/ce_loss': 0.30712890625, 'train/seg_cls_loss': 0.017889404296875, 'train/kl_loss': 0.3294921875, 'train/mask_bce_loss': 0.14045853298157454, 'train/mask_dice_loss': 0.47493151724338534, 'train/mask_loss': 0.6153900429606438, 'metrics/total_secs_per_batch': 5.727279186248779, 'metrics/data_secs_per_batch': 2.658368730545044, '_timestamp': 1740978741.290876}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 113 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978741.2912061}).
Epoch: [8][115/500]	Time  4.869 ( 4.869)	Loss 0.4665 (1.2805)	CeLoss 0.2197 (0.6001)	SegCLSLoss 0.0090 (0.0091)	KLLoss 0.3672 (0.2176)	MaskLoss 0.1024 (0.3270)	MaskBCELoss 0.0374 (0.0619)	MaskDICELoss 0.0650 (0.2651)
Epoch: [8][116/500]	Time  5.031 ( 5.031)	Loss 1.0834 (1.8007)	CeLoss 0.3008 (0.6161)	SegCLSLoss 0.0107 (0.0116)	KLLoss 0.3691 (0.2203)	MaskLoss 0.3698 (0.5785)	MaskBCELoss 0.1422 (0.1367)	MaskDICELoss 0.2276 (0.4418)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 114 is less than current step: 499. Dropping entry: {'train/loss': 1.2804960906505585, 'train/ce_loss': 0.60009765625, 'train/seg_cls_loss': 0.009088134765625, 'train/kl_loss': 0.217578125, 'train/mask_bce_loss': 0.06191619858145714, 'train/mask_dice_loss': 0.2650505922734737, 'train/mask_loss': 0.32696678638458254, 'metrics/total_secs_per_batch': 4.869304656982422, 'metrics/data_secs_per_batch': 1.9399339437484742, '_timestamp': 1740978746.1601093}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 114 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978746.160413}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 115 is less than current step: 499. Dropping entry: {'train/loss': 1.8006804823875426, 'train/ce_loss': 0.61611328125, 'train/seg_cls_loss': 0.011614990234375, 'train/kl_loss': 0.2203125, 'train/mask_bce_loss': 0.13665037639439107, 'train/mask_dice_loss': 0.4418148577213287, 'train/mask_loss': 0.5784652411937714, 'metrics/total_secs_per_batch': 5.031296730041504, 'metrics/data_secs_per_batch': 2.1815773010253907, '_timestamp': 1740978751.191376}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 115 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978751.1916986}).
Epoch: [8][117/500]	Time  6.808 ( 6.808)	Loss 2.0377 (1.6714)	CeLoss 0.2578 (0.2491)	SegCLSLoss 0.0159 (0.0149)	KLLoss 0.3613 (0.3645)	MaskLoss 0.8685 (0.6890)	MaskBCELoss 0.0325 (0.1000)	MaskDICELoss 0.8359 (0.5890)
Epoch: [8][118/500]	Time  5.297 ( 5.297)	Loss 1.6562 (1.8032)	CeLoss 1.6562 (0.4381)	SegCLSLoss 0.0000 (0.0131)	KLLoss 0.0000 (0.2545)	MaskLoss 0.0000 (0.6666)	MaskBCELoss 0.0000 (0.1002)	MaskDICELoss 0.0000 (0.5664)
Epoch: [8][119/500]	Time  6.415 ( 6.415)	Loss 1.6449 (1.6380)	CeLoss 0.2383 (0.3267)	SegCLSLoss 0.0092 (0.0134)	KLLoss 0.3613 (0.2891)	MaskLoss 0.6828 (0.6378)	MaskBCELoss 0.1770 (0.1200)	MaskDICELoss 0.5058 (0.5177)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 116 is less than current step: 499. Dropping entry: {'train/loss': 1.671358871459961, 'train/ce_loss': 0.24912109375, 'train/seg_cls_loss': 0.01494140625, 'train/kl_loss': 0.364453125, 'train/mask_bce_loss': 0.1000456009991467, 'train/mask_dice_loss': 0.588954159617424, 'train/mask_loss': 0.6889997601509095, 'metrics/total_secs_per_batch': 6.8082849979400635, 'metrics/data_secs_per_batch': 2.9266474723815916, '_timestamp': 1740978757.999855}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 116 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978758.0002146}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 117 is less than current step: 499. Dropping entry: {'train/loss': 1.8032462477684021, 'train/ce_loss': 0.4381103515625, 'train/seg_cls_loss': 0.013079833984375, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.10015683583915233, 'train/mask_dice_loss': 0.5663954734802246, 'train/mask_loss': 0.6665523052215576, 'metrics/total_secs_per_batch': 5.297421216964722, 'metrics/data_secs_per_batch': 2.648173117637634, '_timestamp': 1740978763.2971094}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 117 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978763.2973971}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 118 is less than current step: 499. Dropping entry: {'train/loss': 1.6380033850669862, 'train/ce_loss': 0.326708984375, 'train/seg_cls_loss': 0.013446044921875, 'train/kl_loss': 0.2890625, 'train/mask_bce_loss': 0.12003915766254067, 'train/mask_dice_loss': 0.5177369445562363, 'train/mask_loss': 0.6377761006355286, 'metrics/total_secs_per_batch': 6.41548752784729, 'metrics/data_secs_per_batch': 2.7439752578735352, '_timestamp': 1740978769.7125683}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 118 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978769.7128444}).
[2025-03-02 23:12:55,811] [INFO] [logging.py:96:log_dist] [Rank 0] step=4120, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:12:55,817] [INFO] [timer.py:215:stop] epoch=0/micro_step=41200/global_step=4120, RunningAvgSamplesPerSec=1.4797452189965776, CurrSamplesPerSec=1.6384048000140625, MemAllocated=31.73GB, MaxMemAllocated=37.23GB
Epoch: [8][120/500]	Time  6.105 ( 6.105)	Loss 0.0654 (1.3560)	CeLoss 0.0654 (0.3003)	SegCLSLoss 0.0000 (0.0106)	KLLoss 0.0000 (0.2588)	MaskLoss 0.0000 (0.5122)	MaskBCELoss 0.0000 (0.0997)	MaskDICELoss 0.0000 (0.4125)
Epoch: [8][121/500]	Time  6.867 ( 6.867)	Loss 2.5257 (1.8031)	CeLoss 0.2461 (0.4306)	SegCLSLoss 0.0167 (0.0127)	KLLoss 0.3633 (0.2568)	MaskLoss 1.1173 (0.6704)	MaskBCELoss 0.4306 (0.1282)	MaskDICELoss 0.6868 (0.5422)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 119 is less than current step: 499. Dropping entry: {'train/loss': 1.3560009956359864, 'train/ce_loss': 0.300341796875, 'train/seg_cls_loss': 0.0105712890625, 'train/kl_loss': 0.2587890625, 'train/mask_bce_loss': 0.0996986074373126, 'train/mask_dice_loss': 0.41248156875371933, 'train/mask_loss': 0.5121801882982254, 'metrics/total_secs_per_batch': 6.1051366329193115, 'metrics/data_secs_per_batch': 2.5750324964523315, '_timestamp': 1740978775.8175073}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 119 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978775.817805}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 120 is less than current step: 499. Dropping entry: {'train/loss': 1.8031418204307557, 'train/ce_loss': 0.43056640625, 'train/seg_cls_loss': 0.012738037109375, 'train/kl_loss': 0.2568359375, 'train/mask_bce_loss': 0.12817153073847293, 'train/mask_dice_loss': 0.5421982109546661, 'train/mask_loss': 0.6703697323799134, 'metrics/total_secs_per_batch': 6.866569757461548, 'metrics/data_secs_per_batch': 3.190651774406433, '_timestamp': 1740978782.684994}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 120 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978782.6855836}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 121 is less than current step: 499. Dropping entry: {'train/loss': 1.5911493062973023, 'train/ce_loss': 0.63271484375, 'train/seg_cls_loss': 0.0111083984375, 'train/kl_loss': 0.1853515625, 'train/mask_bce_loss': 0.09224545937031507, 'train/mask_dice_loss': 0.3749600529670715, 'train/mask_loss': 0.46720550358295443, 'metrics/total_secs_per_batch': 4.429696798324585, 'metrics/data_secs_per_batch': 2.047720265388489, '_timestamp': 1740978787.1140416}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 121 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978787.1144092}).
Epoch: [8][122/500]	Time  4.430 ( 4.430)	Loss 1.1007 (1.5911)	CeLoss 0.2520 (0.6327)	SegCLSLoss 0.0115 (0.0111)	KLLoss 0.3574 (0.1854)	MaskLoss 0.4039 (0.4672)	MaskBCELoss 0.0599 (0.0922)	MaskDICELoss 0.3440 (0.3750)
Epoch: [8][123/500]	Time  5.997 ( 5.997)	Loss 1.7288 (1.5042)	CeLoss 0.2100 (0.3528)	SegCLSLoss 0.0114 (0.0120)	KLLoss 0.3594 (0.2531)	MaskLoss 0.7384 (0.5600)	MaskBCELoss 0.1339 (0.0773)	MaskDICELoss 0.6045 (0.4827)
Epoch: [8][124/500]	Time  4.694 ( 4.694)	Loss 2.3842 (1.4168)	CeLoss 0.1777 (0.5095)	SegCLSLoss 0.0277 (0.0103)	KLLoss 0.3613 (0.2178)	MaskLoss 1.0783 (0.4402)	MaskBCELoss 0.4216 (0.1378)	MaskDICELoss 0.6567 (0.3024)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 122 is less than current step: 499. Dropping entry: {'train/loss': 1.5042299509048462, 'train/ce_loss': 0.35283203125, 'train/seg_cls_loss': 0.012030029296875, 'train/kl_loss': 0.253125, 'train/mask_bce_loss': 0.07728867642581463, 'train/mask_dice_loss': 0.4827364593744278, 'train/mask_loss': 0.5600251317024231, 'metrics/total_secs_per_batch': 5.997220754623413, 'metrics/data_secs_per_batch': 2.7979310274124147, '_timestamp': 1740978793.111174}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 122 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978793.1114159}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 123 is less than current step: 499. Dropping entry: {'train/loss': 1.4168118834495544, 'train/ce_loss': 0.50947265625, 'train/seg_cls_loss': 0.010260009765625, 'train/kl_loss': 0.2177734375, 'train/mask_bce_loss': 0.1378008395433426, 'train/mask_dice_loss': 0.30244103372097014, 'train/mask_loss': 0.44024187326431274, 'metrics/total_secs_per_batch': 4.694171190261841, 'metrics/data_secs_per_batch': 2.1706027746200562, '_timestamp': 1740978797.8054872}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 123 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978797.8057137}).
Epoch: [8][125/500]	Time  4.577 ( 4.577)	Loss 1.2656 (1.6843)	CeLoss 1.2656 (0.7286)	SegCLSLoss 0.0000 (0.0106)	KLLoss 0.0000 (0.1504)	MaskLoss 0.0000 (0.4676)	MaskBCELoss 0.0000 (0.1275)	MaskDICELoss 0.0000 (0.3401)
Epoch: [8][126/500]	Time  5.924 ( 5.924)	Loss 1.1910 (1.2642)	CeLoss 0.1348 (0.3633)	SegCLSLoss 0.0298 (0.0122)	KLLoss 0.3672 (0.2539)	MaskLoss 0.5027 (0.4347)	MaskBCELoss 0.0030 (0.0590)	MaskDICELoss 0.4997 (0.3757)
Epoch: [8][127/500]	Time  5.873 ( 5.873)	Loss 0.7730 (1.5428)	CeLoss 0.2852 (0.3212)	SegCLSLoss 0.0097 (0.0108)	KLLoss 0.3613 (0.3268)	MaskLoss 0.2244 (0.5918)	MaskBCELoss 0.0386 (0.1487)	MaskDICELoss 0.1858 (0.4431)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 124 is less than current step: 499. Dropping entry: {'train/loss': 1.6843018531799316, 'train/ce_loss': 0.7285888671875, 'train/seg_cls_loss': 0.01064453125, 'train/kl_loss': 0.150390625, 'train/mask_bce_loss': 0.12749996427446603, 'train/mask_dice_loss': 0.3401026129722595, 'train/mask_loss': 0.46760258078575134, 'metrics/total_secs_per_batch': 4.576757907867432, 'metrics/data_secs_per_batch': 2.011636829376221, '_timestamp': 1740978802.382167}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 124 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978802.3824894}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 125 is less than current step: 499. Dropping entry: {'train/loss': 1.2641780018806457, 'train/ce_loss': 0.36328125, 'train/seg_cls_loss': 0.012213134765625, 'train/kl_loss': 0.25390625, 'train/mask_bce_loss': 0.05899834637530148, 'train/mask_dice_loss': 0.37572736442089083, 'train/mask_loss': 0.43472571671009064, 'metrics/total_secs_per_batch': 5.924421072006226, 'metrics/data_secs_per_batch': 2.6199256420135497, '_timestamp': 1740978808.3066657}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 125 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978808.3069775}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 126 is less than current step: 499. Dropping entry: {'train/loss': 1.542769968509674, 'train/ce_loss': 0.32119140625, 'train/seg_cls_loss': 0.0108154296875, 'train/kl_loss': 0.3267578125, 'train/mask_bce_loss': 0.14867856483906508, 'train/mask_dice_loss': 0.443116569519043, 'train/mask_loss': 0.5917951375246048, 'metrics/total_secs_per_batch': 5.872865200042725, 'metrics/data_secs_per_batch': 2.899296689033508, '_timestamp': 1740978814.1794538}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 126 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978814.1797442}).
Epoch: [8][128/500]	Time  6.404 ( 6.404)	Loss 1.6826 (1.6559)	CeLoss 0.2832 (0.2449)	SegCLSLoss 0.0114 (0.0165)	KLLoss 0.3574 (0.3250)	MaskLoss 0.6792 (0.6853)	MaskBCELoss 0.0886 (0.1028)	MaskDICELoss 0.5906 (0.5825)
Epoch: [8][129/500]	Time  5.833 ( 5.833)	Loss 1.6248 (1.4389)	CeLoss 0.3242 (0.4001)	SegCLSLoss 0.0115 (0.0102)	KLLoss 0.3613 (0.2525)	MaskLoss 0.6288 (0.5042)	MaskBCELoss 0.1489 (0.1231)	MaskDICELoss 0.4799 (0.3811)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 127 is less than current step: 499. Dropping entry: {'train/loss': 1.655915379524231, 'train/ce_loss': 0.244873046875, 'train/seg_cls_loss': 0.016497802734375, 'train/kl_loss': 0.325, 'train/mask_bce_loss': 0.10280427827965469, 'train/mask_dice_loss': 0.5824532300233841, 'train/mask_loss': 0.6852575093507767, 'metrics/total_secs_per_batch': 6.40371036529541, 'metrics/data_secs_per_batch': 2.832535147666931, '_timestamp': 1740978820.5834064}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 127 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978820.5836399}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 128 is less than current step: 499. Dropping entry: {'train/loss': 1.4389045476913451, 'train/ce_loss': 0.4001220703125, 'train/seg_cls_loss': 0.0102294921875, 'train/kl_loss': 0.2525390625, 'train/mask_bce_loss': 0.12308210264891387, 'train/mask_dice_loss': 0.38112359046936034, 'train/mask_loss': 0.5042056918144227, 'metrics/total_secs_per_batch': 5.833344221115112, 'metrics/data_secs_per_batch': 2.635466980934143, '_timestamp': 1740978826.416565}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 128 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978826.4168632}).
[2025-03-02 23:13:52,415] [INFO] [logging.py:96:log_dist] [Rank 0] step=4130, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:13:52,421] [INFO] [timer.py:215:stop] epoch=0/micro_step=41300/global_step=4130, RunningAvgSamplesPerSec=1.4803283226595612, CurrSamplesPerSec=1.6654379293904704, MemAllocated=31.26GB, MaxMemAllocated=37.23GB
Epoch: [8][130/500]	Time  6.006 ( 6.006)	Loss 2.6947 (1.5785)	CeLoss 0.2295 (0.3725)	SegCLSLoss 0.0186 (0.0134)	KLLoss 0.3906 (0.3297)	MaskLoss 1.2087 (0.5833)	MaskBCELoss 0.2882 (0.1184)	MaskDICELoss 0.9205 (0.4649)
Epoch: [8][131/500]	Time  5.919 ( 5.919)	Loss 2.4776 (1.8712)	CeLoss 0.2432 (0.4508)	SegCLSLoss 0.0266 (0.0147)	KLLoss 0.3750 (0.2568)	MaskLoss 1.0923 (0.6936)	MaskBCELoss 0.2826 (0.1603)	MaskDICELoss 0.8097 (0.5332)
Epoch: [8][132/500]	Time  6.986 ( 6.986)	Loss 0.0845 (1.6943)	CeLoss 0.0845 (0.4315)	SegCLSLoss 0.0000 (0.0110)	KLLoss 0.0000 (0.2561)	MaskLoss 0.0000 (0.6159)	MaskBCELoss 0.0000 (0.1252)	MaskDICELoss 0.0000 (0.4908)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 129 is less than current step: 499. Dropping entry: {'train/loss': 1.5785391330718994, 'train/ce_loss': 0.3724609375, 'train/seg_cls_loss': 0.013446044921875, 'train/kl_loss': 0.3296875, 'train/mask_bce_loss': 0.11843302408233285, 'train/mask_dice_loss': 0.4648795172572136, 'train/mask_loss': 0.5833125382661819, 'metrics/total_secs_per_batch': 6.006080150604248, 'metrics/data_secs_per_batch': 2.663158822059631, '_timestamp': 1740978832.4223769}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 129 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978832.4226494}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 130 is less than current step: 499. Dropping entry: {'train/loss': 1.8711827456951142, 'train/ce_loss': 0.450830078125, 'train/seg_cls_loss': 0.01466064453125, 'train/kl_loss': 0.2568359375, 'train/mask_bce_loss': 0.160308182798326, 'train/mask_dice_loss': 0.5332421690225602, 'train/mask_loss': 0.6935503512620926, 'metrics/total_secs_per_batch': 5.9189980030059814, 'metrics/data_secs_per_batch': 2.5551828861236574, '_timestamp': 1740978838.3416615}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 130 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978838.3418658}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 131 is less than current step: 499. Dropping entry: {'train/loss': 1.6943115592002869, 'train/ce_loss': 0.431494140625, 'train/seg_cls_loss': 0.01097412109375, 'train/kl_loss': 0.2560546875, 'train/mask_bce_loss': 0.125163983553648, 'train/mask_dice_loss': 0.4907661974430084, 'train/mask_loss': 0.6159301906824112, 'metrics/total_secs_per_batch': 6.986054182052612, 'metrics/data_secs_per_batch': 2.7766648054122927, '_timestamp': 1740978845.3277686}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 131 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978845.328092}).
Epoch: [8][133/500]	Time  6.931 ( 6.931)	Loss 0.9710 (1.5950)	CeLoss 0.2393 (0.2472)	SegCLSLoss 0.0103 (0.0151)	KLLoss 0.3613 (0.3617)	MaskLoss 0.3449 (0.6519)	MaskBCELoss 0.0971 (0.1820)	MaskDICELoss 0.2478 (0.4699)
Epoch: [8][134/500]	Time  6.399 ( 6.399)	Loss 0.9175 (1.4740)	CeLoss 0.2832 (0.2970)	SegCLSLoss 0.0106 (0.0132)	KLLoss 0.3691 (0.2914)	MaskLoss 0.2957 (0.5706)	MaskBCELoss 0.1105 (0.0916)	MaskDICELoss 0.1852 (0.4790)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 132 is less than current step: 499. Dropping entry: {'train/loss': 1.5950148969888687, 'train/ce_loss': 0.24716796875, 'train/seg_cls_loss': 0.01512451171875, 'train/kl_loss': 0.36171875, 'train/mask_bce_loss': 0.18197606652975082, 'train/mask_dice_loss': 0.46992591917514803, 'train/mask_loss': 0.6519019827246666, 'metrics/total_secs_per_batch': 6.931178092956543, 'metrics/data_secs_per_batch': 3.1103941440582275, '_timestamp': 1740978852.2588034}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 132 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978852.2589962}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 133 is less than current step: 499. Dropping entry: {'train/loss': 1.4740419745445252, 'train/ce_loss': 0.29697265625, 'train/seg_cls_loss': 0.01322021484375, 'train/kl_loss': 0.29140625, 'train/mask_bce_loss': 0.09160595349967479, 'train/mask_dice_loss': 0.479008786380291, 'train/mask_loss': 0.5706147342920304, 'metrics/total_secs_per_batch': 6.398714303970337, 'metrics/data_secs_per_batch': 2.770072078704834, '_timestamp': 1740978858.657523}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 133 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978858.6578083}).
Epoch: [8][135/500]	Time  5.655 ( 5.655)	Loss 2.3060 (1.6343)	CeLoss 0.2480 (0.4152)	SegCLSLoss 0.0117 (0.0122)	KLLoss 0.3594 (0.2533)	MaskLoss 1.0085 (0.5937)	MaskBCELoss 0.0097 (0.0480)	MaskDICELoss 0.9988 (0.5457)
Epoch: [8][136/500]	Time  5.275 ( 5.275)	Loss 1.3828 (1.6721)	CeLoss 1.3828 (0.5778)	SegCLSLoss 0.0000 (0.0110)	KLLoss 0.0000 (0.2186)	MaskLoss 0.0000 (0.5333)	MaskBCELoss 0.0000 (0.1224)	MaskDICELoss 0.0000 (0.4109)
Epoch: [8][137/500]	Time  5.614 ( 5.614)	Loss 2.0530 (1.8073)	CeLoss 0.2051 (0.4534)	SegCLSLoss 0.0210 (0.0131)	KLLoss 0.3535 (0.2949)	MaskLoss 0.9005 (0.6588)	MaskBCELoss 0.0800 (0.2107)	MaskDICELoss 0.8205 (0.4481)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 134 is less than current step: 499. Dropping entry: {'train/loss': 1.634340023994446, 'train/ce_loss': 0.415234375, 'train/seg_cls_loss': 0.012249755859375, 'train/kl_loss': 0.2533203125, 'train/mask_bce_loss': 0.04802681035362184, 'train/mask_dice_loss': 0.5457057088613511, 'train/mask_loss': 0.5937325239181519, 'metrics/total_secs_per_batch': 5.65461277961731, 'metrics/data_secs_per_batch': 2.6086086511611937, '_timestamp': 1740978864.312131}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 134 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978864.3123233}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 135 is less than current step: 499. Dropping entry: {'train/loss': 1.6720630764961242, 'train/ce_loss': 0.57783203125, 'train/seg_cls_loss': 0.01104736328125, 'train/kl_loss': 0.2185546875, 'train/mask_bce_loss': 0.12243548221886158, 'train/mask_dice_loss': 0.41086167097091675, 'train/mask_loss': 0.5332971513271332, 'metrics/total_secs_per_batch': 5.275200605392456, 'metrics/data_secs_per_batch': 2.3527835607528687, '_timestamp': 1740978869.5874283}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 135 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978869.587764}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 136 is less than current step: 499. Dropping entry: {'train/loss': 1.8072516918182373, 'train/ce_loss': 0.45341796875, 'train/seg_cls_loss': 0.013079833984375, 'train/kl_loss': 0.294921875, 'train/mask_bce_loss': 0.2106845460832119, 'train/mask_dice_loss': 0.4480682611465454, 'train/mask_loss': 0.6587528049945831, 'metrics/total_secs_per_batch': 5.613641738891602, 'metrics/data_secs_per_batch': 2.479061317443848, '_timestamp': 1740978875.2013}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 136 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978875.2016442}).
Epoch: [8][138/500]	Time  6.466 ( 6.466)	Loss 2.3411 (1.3778)	CeLoss 0.2256 (0.2300)	SegCLSLoss 0.0093 (0.0126)	KLLoss 0.3711 (0.2891)	MaskLoss 1.0368 (0.5562)	MaskBCELoss 0.0602 (0.0879)	MaskDICELoss 0.9765 (0.4683)
Epoch: [8][139/500]	Time  6.450 ( 6.450)	Loss 1.3472 (1.8812)	CeLoss 0.3320 (0.3031)	SegCLSLoss 0.0115 (0.0145)	KLLoss 0.3672 (0.3260)	MaskLoss 0.4861 (0.7691)	MaskBCELoss 0.1777 (0.1418)	MaskDICELoss 0.3084 (0.6272)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 137 is less than current step: 499. Dropping entry: {'train/loss': 1.3778203547000885, 'train/ce_loss': 0.22998046875, 'train/seg_cls_loss': 0.01259765625, 'train/kl_loss': 0.2890625, 'train/mask_bce_loss': 0.08789686521049589, 'train/mask_dice_loss': 0.4682984799146652, 'train/mask_loss': 0.5561953485012054, 'metrics/total_secs_per_batch': 6.466050624847412, 'metrics/data_secs_per_batch': 2.854475259780884, '_timestamp': 1740978881.6671085}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 137 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978881.667393}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 138 is less than current step: 499. Dropping entry: {'train/loss': 1.8812299847602845, 'train/ce_loss': 0.303125, 'train/seg_cls_loss': 0.014544677734375, 'train/kl_loss': 0.3259765625, 'train/mask_bce_loss': 0.14183555077761412, 'train/mask_dice_loss': 0.6272462397813797, 'train/mask_loss': 0.7690817892551423, 'metrics/total_secs_per_batch': 6.45048713684082, 'metrics/data_secs_per_batch': 3.2679030656814576, '_timestamp': 1740978888.1175797}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 138 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978888.1179183}).
[2025-03-02 23:14:53,588] [INFO] [logging.py:96:log_dist] [Rank 0] step=4140, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:14:53,593] [INFO] [timer.py:215:stop] epoch=0/micro_step=41400/global_step=4140, RunningAvgSamplesPerSec=1.4806669699244837, CurrSamplesPerSec=1.8263491644644512, MemAllocated=31.29GB, MaxMemAllocated=37.23GB
Epoch: [8][140/500]	Time  5.477 ( 5.477)	Loss 1.5556 (1.8289)	CeLoss 0.2334 (0.4058)	SegCLSLoss 0.0149 (0.0141)	KLLoss 0.3652 (0.3297)	MaskLoss 0.6391 (0.6917)	MaskBCELoss 0.0279 (0.1315)	MaskDICELoss 0.6113 (0.5602)
Epoch: [8][141/500]	Time  6.809 ( 6.809)	Loss 2.0793 (1.5320)	CeLoss 0.1982 (0.2185)	SegCLSLoss 0.0177 (0.0154)	KLLoss 0.3770 (0.3311)	MaskLoss 0.9176 (0.6362)	MaskBCELoss 0.0161 (0.1070)	MaskDICELoss 0.9015 (0.5292)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 139 is less than current step: 499. Dropping entry: {'train/loss': 1.8289315462112428, 'train/ce_loss': 0.40576171875, 'train/seg_cls_loss': 0.014141845703125, 'train/kl_loss': 0.3296875, 'train/mask_bce_loss': 0.13145634839311243, 'train/mask_dice_loss': 0.5602066934108734, 'train/mask_loss': 0.6916630357503891, 'metrics/total_secs_per_batch': 5.477051019668579, 'metrics/data_secs_per_batch': 2.6688605070114138, '_timestamp': 1740978893.5943942}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 139 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978893.5945866}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 140 is less than current step: 499. Dropping entry: {'train/loss': 1.5320374846458436, 'train/ce_loss': 0.21845703125, 'train/seg_cls_loss': 0.015362548828125, 'train/kl_loss': 0.3310546875, 'train/mask_bce_loss': 0.10698642842471599, 'train/mask_dice_loss': 0.5292471736669541, 'train/mask_loss': 0.636233600974083, 'metrics/total_secs_per_batch': 6.809192895889282, 'metrics/data_secs_per_batch': 2.974297022819519, '_timestamp': 1740978900.403832}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 140 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978900.4041185}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 141 is less than current step: 499. Dropping entry: {'train/loss': 1.5262254774570465, 'train/ce_loss': 0.3423828125, 'train/seg_cls_loss': 0.0146240234375, 'train/kl_loss': 0.290234375, 'train/mask_bce_loss': 0.07686956627294421, 'train/mask_dice_loss': 0.4969853609800339, 'train/mask_loss': 0.5738549262285233, 'metrics/total_secs_per_batch': 5.893342971801758, 'metrics/data_secs_per_batch': 2.5941282510757446, '_timestamp': 1740978906.2971325}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 141 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978906.2974172}).
Epoch: [8][142/500]	Time  5.893 ( 5.893)	Loss 0.7645 (1.5262)	CeLoss 0.3438 (0.3424)	SegCLSLoss 0.0104 (0.0146)	KLLoss 0.3711 (0.2902)	MaskLoss 0.1889 (0.5739)	MaskBCELoss 0.0587 (0.0769)	MaskDICELoss 0.1302 (0.4970)
Epoch: [8][143/500]	Time  6.242 ( 6.242)	Loss 2.3853 (1.9738)	CeLoss 0.2236 (0.4645)	SegCLSLoss 0.0217 (0.0130)	KLLoss 0.3672 (0.2900)	MaskLoss 1.0569 (0.7369)	MaskBCELoss 0.2807 (0.1775)	MaskDICELoss 0.7762 (0.5594)
Epoch: [8][144/500]	Time  6.172 ( 6.172)	Loss 1.8186 (1.7163)	CeLoss 0.2363 (0.3917)	SegCLSLoss 0.0121 (0.0120)	KLLoss 0.3633 (0.2895)	MaskLoss 0.7697 (0.6447)	MaskBCELoss 0.0842 (0.1004)	MaskDICELoss 0.6855 (0.5443)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 142 is less than current step: 499. Dropping entry: {'train/loss': 1.973811113834381, 'train/ce_loss': 0.464453125, 'train/seg_cls_loss': 0.01302490234375, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.17748182509094476, 'train/mask_dice_loss': 0.5594237267971038, 'train/mask_loss': 0.7369055569171905, 'metrics/total_secs_per_batch': 6.24230170249939, 'metrics/data_secs_per_batch': 3.008484125137329, '_timestamp': 1740978912.5395446}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 142 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978912.5397475}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 143 is less than current step: 499. Dropping entry: {'train/loss': 1.7162905216217041, 'train/ce_loss': 0.39169921875, 'train/seg_cls_loss': 0.0120361328125, 'train/kl_loss': 0.289453125, 'train/mask_bce_loss': 0.10041843801736831, 'train/mask_dice_loss': 0.5442502588033676, 'train/mask_loss': 0.6446686953306198, 'metrics/total_secs_per_batch': 6.172447681427002, 'metrics/data_secs_per_batch': 2.511513352394104, '_timestamp': 1740978918.7120504}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 143 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978918.7123759}).
Epoch: [8][145/500]	Time  6.716 ( 6.716)	Loss 1.9003 (1.4197)	CeLoss 0.2051 (0.2441)	SegCLSLoss 0.0201 (0.0128)	KLLoss 0.3711 (0.2916)	MaskLoss 0.8242 (0.5700)	MaskBCELoss 0.0641 (0.1201)	MaskDICELoss 0.7601 (0.4498)
Epoch: [8][146/500]	Time  6.742 ( 6.742)	Loss 2.1432 (1.9182)	CeLoss 0.2227 (0.2113)	SegCLSLoss 0.0187 (0.0187)	KLLoss 0.3672 (0.3666)	MaskLoss 0.9368 (0.8303)	MaskBCELoss 0.0431 (0.1665)	MaskDICELoss 0.8937 (0.6638)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 144 is less than current step: 499. Dropping entry: {'train/loss': 1.4197215974330901, 'train/ce_loss': 0.244140625, 'train/seg_cls_loss': 0.0128173828125, 'train/kl_loss': 0.2916015625, 'train/mask_bce_loss': 0.12012997195124626, 'train/mask_dice_loss': 0.4498382568359375, 'train/mask_loss': 0.5699682205915451, 'metrics/total_secs_per_batch': 6.716327428817749, 'metrics/data_secs_per_batch': 3.0332256317138673, '_timestamp': 1740978925.428237}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 144 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978925.4284294}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 145 is less than current step: 499. Dropping entry: {'train/loss': 1.9182432889938354, 'train/ce_loss': 0.211328125, 'train/seg_cls_loss': 0.018695068359375, 'train/kl_loss': 0.3666015625, 'train/mask_bce_loss': 0.1665210982784629, 'train/mask_dice_loss': 0.6637919455766678, 'train/mask_loss': 0.8303130388259887, 'metrics/total_secs_per_batch': 6.742075443267822, 'metrics/data_secs_per_batch': 3.0335384368896485, '_timestamp': 1740978932.170329}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 145 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978932.1706288}).
Epoch: [8][147/500]	Time  6.514 ( 6.514)	Loss 1.3566 (1.7558)	CeLoss 0.2109 (0.2382)	SegCLSLoss 0.0204 (0.0169)	KLLoss 0.3672 (0.3639)	MaskLoss 0.5494 (0.7365)	MaskBCELoss 0.1948 (0.1561)	MaskDICELoss 0.3546 (0.5804)
Epoch: [8][148/500]	Time  7.087 ( 7.087)	Loss 3.0416 (2.0877)	CeLoss 0.1865 (0.1938)	SegCLSLoss 0.0210 (0.0203)	KLLoss 0.3730 (0.3627)	MaskLoss 1.4036 (0.9239)	MaskBCELoss 0.6039 (0.1778)	MaskDICELoss 0.7997 (0.7461)
Epoch: [8][149/500]	Time  5.475 ( 5.475)	Loss 0.8047 (1.6412)	CeLoss 0.8047 (0.5582)	SegCLSLoss 0.0000 (0.0132)	KLLoss 0.0000 (0.2191)	MaskLoss 0.0000 (0.5273)	MaskBCELoss 0.0000 (0.1035)	MaskDICELoss 0.0000 (0.4238)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 146 is less than current step: 499. Dropping entry: {'train/loss': 1.7558409690856933, 'train/ce_loss': 0.23818359375, 'train/seg_cls_loss': 0.016888427734375, 'train/kl_loss': 0.3638671875, 'train/mask_bce_loss': 0.15607119370251893, 'train/mask_dice_loss': 0.5804430291056633, 'train/mask_loss': 0.7365142226219177, 'metrics/total_secs_per_batch': 6.513797283172607, 'metrics/data_secs_per_batch': 3.1818093776702883, '_timestamp': 1740978938.6841037}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 146 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978938.68438}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 147 is less than current step: 499. Dropping entry: {'train/loss': 2.087704026699066, 'train/ce_loss': 0.19384765625, 'train/seg_cls_loss': 0.02025146484375, 'train/kl_loss': 0.3626953125, 'train/mask_bce_loss': 0.1777696148492396, 'train/mask_dice_loss': 0.7461116909980774, 'train/mask_loss': 0.9238813042640686, 'metrics/total_secs_per_batch': 7.086696147918701, 'metrics/data_secs_per_batch': 3.1980740785598756, '_timestamp': 1740978945.7708542}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 147 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978945.771137}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 148 is less than current step: 499. Dropping entry: {'train/loss': 1.641234290599823, 'train/ce_loss': 0.558203125, 'train/seg_cls_loss': 0.0131591796875, 'train/kl_loss': 0.219140625, 'train/mask_bce_loss': 0.10348151130601764, 'train/mask_dice_loss': 0.4238250911235809, 'train/mask_loss': 0.5273065984249115, 'metrics/total_secs_per_batch': 5.47461199760437, 'metrics/data_secs_per_batch': 2.606710410118103, '_timestamp': 1740978951.2455087}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 148 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978951.2458208}).
[2025-03-02 23:15:56,247] [INFO] [logging.py:96:log_dist] [Rank 0] step=4150, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:15:56,253] [INFO] [timer.py:215:stop] epoch=0/micro_step=41500/global_step=4150, RunningAvgSamplesPerSec=1.4809254876476636, CurrSamplesPerSec=1.9973658016518552, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [8][150/500]	Time  5.008 ( 5.008)	Loss 2.2440 (1.7224)	CeLoss 0.2021 (0.5200)	SegCLSLoss 0.0101 (0.0120)	KLLoss 0.3652 (0.2604)	MaskLoss 0.9999 (0.5852)	MaskBCELoss 0.1043 (0.0896)	MaskDICELoss 0.8956 (0.4956)
Epoch: [8][151/500]	Time  5.877 ( 5.877)	Loss 2.5156 (1.2471)	CeLoss 0.2295 (0.3497)	SegCLSLoss 0.0190 (0.0089)	KLLoss 0.3594 (0.2164)	MaskLoss 1.1201 (0.4358)	MaskBCELoss 0.2905 (0.1122)	MaskDICELoss 0.8296 (0.3236)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 149 is less than current step: 499. Dropping entry: {'train/loss': 1.722379469871521, 'train/ce_loss': 0.52001953125, 'train/seg_cls_loss': 0.011962890625, 'train/kl_loss': 0.2603515625, 'train/mask_bce_loss': 0.08959125056862831, 'train/mask_dice_loss': 0.49557308852672577, 'train/mask_loss': 0.5851643443107605, 'metrics/total_secs_per_batch': 5.008280515670776, 'metrics/data_secs_per_batch': 2.0766990184783936, '_timestamp': 1740978956.2535586}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 149 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978956.253858}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 150 is less than current step: 499. Dropping entry: {'train/loss': 1.2470862030982972, 'train/ce_loss': 0.349658203125, 'train/seg_cls_loss': 0.0088623046875, 'train/kl_loss': 0.21640625, 'train/mask_bce_loss': 0.11219655498862266, 'train/mask_dice_loss': 0.3235779896378517, 'train/mask_loss': 0.43577454090118406, 'metrics/total_secs_per_batch': 5.877323865890503, 'metrics/data_secs_per_batch': 2.699581837654114, '_timestamp': 1740978962.1310318}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 150 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978962.1313102}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 151 is less than current step: 499. Dropping entry: {'train/loss': 1.3519446164369584, 'train/ce_loss': 0.437890625, 'train/seg_cls_loss': 0.01181640625, 'train/kl_loss': 0.255078125, 'train/mask_bce_loss': 0.10612163506448269, 'train/mask_dice_loss': 0.33513387441635134, 'train/mask_loss': 0.44125550873577596, 'metrics/total_secs_per_batch': 4.827494144439697, 'metrics/data_secs_per_batch': 2.129407596588135, '_timestamp': 1740978966.9585133}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 151 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978966.9587853}).
Epoch: [8][152/500]	Time  4.827 ( 4.827)	Loss 0.3983 (1.3519)	CeLoss 0.2559 (0.4379)	SegCLSLoss 0.0083 (0.0118)	KLLoss 0.3711 (0.2551)	MaskLoss 0.0507 (0.4413)	MaskBCELoss 0.0270 (0.1061)	MaskDICELoss 0.0237 (0.3351)
Epoch: [8][153/500]	Time  7.041 ( 7.041)	Loss 0.7578 (1.5385)	CeLoss 0.7578 (0.2953)	SegCLSLoss 0.0000 (0.0146)	KLLoss 0.0000 (0.3279)	MaskLoss 0.0000 (0.6015)	MaskBCELoss 0.0000 (0.0879)	MaskDICELoss 0.0000 (0.5135)
Epoch: [8][154/500]	Time  6.743 ( 6.743)	Loss 1.0234 (1.4817)	CeLoss 1.0234 (0.3450)	SegCLSLoss 0.0000 (0.0147)	KLLoss 0.0000 (0.3281)	MaskLoss 0.0000 (0.5482)	MaskBCELoss 0.0000 (0.1095)	MaskDICELoss 0.0000 (0.4388)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 152 is less than current step: 499. Dropping entry: {'train/loss': 1.5385079860687256, 'train/ce_loss': 0.2953125, 'train/seg_cls_loss': 0.01458740234375, 'train/kl_loss': 0.3279296875, 'train/mask_bce_loss': 0.08793181609362363, 'train/mask_dice_loss': 0.5135487288236618, 'train/mask_loss': 0.6014805555343627, 'metrics/total_secs_per_batch': 7.040505647659302, 'metrics/data_secs_per_batch': 3.0964773178100584, '_timestamp': 1740978973.9992754}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 152 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978973.9996367}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 153 is less than current step: 499. Dropping entry: {'train/loss': 1.4816910743713378, 'train/ce_loss': 0.34501953125, 'train/seg_cls_loss': 0.014739990234375, 'train/kl_loss': 0.328125, 'train/mask_bce_loss': 0.10946246068924666, 'train/mask_dice_loss': 0.43875612169504163, 'train/mask_loss': 0.5482185810804368, 'metrics/total_secs_per_batch': 6.742783308029175, 'metrics/data_secs_per_batch': 3.2824907541275024, '_timestamp': 1740978980.741818}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 153 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978980.7420979}).
Epoch: [8][155/500]	Time  6.390 ( 6.390)	Loss 2.1685 (1.3580)	CeLoss 0.2363 (0.2471)	SegCLSLoss 0.0135 (0.0101)	KLLoss 0.3652 (0.2213)	MaskLoss 0.9446 (0.5419)	MaskBCELoss 0.0213 (0.1317)	MaskDICELoss 0.9234 (0.4102)
Epoch: [8][156/500]	Time  6.479 ( 6.479)	Loss 1.5539 (1.9630)	CeLoss 0.2227 (0.2805)	SegCLSLoss 0.0210 (0.0155)	KLLoss 0.3633 (0.3266)	MaskLoss 0.6422 (0.8209)	MaskBCELoss 0.1763 (0.2014)	MaskDICELoss 0.4658 (0.6194)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 154 is less than current step: 499. Dropping entry: {'train/loss': 1.3579924345016479, 'train/ce_loss': 0.2470947265625, 'train/seg_cls_loss': 0.010076904296875, 'train/kl_loss': 0.2212890625, 'train/mask_bce_loss': 0.1316813912242651, 'train/mask_dice_loss': 0.4101932570338249, 'train/mask_loss': 0.5418746411800385, 'metrics/total_secs_per_batch': 6.389909505844116, 'metrics/data_secs_per_batch': 2.8202053546905517, '_timestamp': 1740978987.1317995}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 154 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978987.132076}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 155 is less than current step: 499. Dropping entry: {'train/loss': 1.9629604458808898, 'train/ce_loss': 0.280517578125, 'train/seg_cls_loss': 0.015460205078125, 'train/kl_loss': 0.3265625, 'train/mask_bce_loss': 0.2014355662278831, 'train/mask_dice_loss': 0.6194489359855652, 'train/mask_loss': 0.8208845019340515, 'metrics/total_secs_per_batch': 6.478771209716797, 'metrics/data_secs_per_batch': 3.1082056045532225, '_timestamp': 1740978993.610632}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 155 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978993.6110427}).
Epoch: [8][157/500]	Time  6.058 ( 6.058)	Loss 2.0340 (1.4716)	CeLoss 0.2285 (0.3474)	SegCLSLoss 0.0187 (0.0116)	KLLoss 0.3613 (0.2543)	MaskLoss 0.8803 (0.5465)	MaskBCELoss 0.0323 (0.0897)	MaskDICELoss 0.8479 (0.4568)
Epoch: [8][158/500]	Time  6.845 ( 6.845)	Loss 2.7487 (2.2701)	CeLoss 0.1680 (0.2093)	SegCLSLoss 0.0234 (0.0184)	KLLoss 0.3730 (0.3646)	MaskLoss 1.2660 (1.0075)	MaskBCELoss 0.3623 (0.2515)	MaskDICELoss 0.9036 (0.7561)
Epoch: [8][159/500]	Time  4.643 ( 4.643)	Loss 1.0781 (1.2929)	CeLoss 1.0781 (0.5887)	SegCLSLoss 0.0000 (0.0097)	KLLoss 0.0000 (0.2156)	MaskLoss 0.0000 (0.3390)	MaskBCELoss 0.0000 (0.0171)	MaskDICELoss 0.0000 (0.3219)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 156 is less than current step: 499. Dropping entry: {'train/loss': 1.4715648591518402, 'train/ce_loss': 0.347412109375, 'train/seg_cls_loss': 0.01160888671875, 'train/kl_loss': 0.254296875, 'train/mask_bce_loss': 0.08966826777905226, 'train/mask_dice_loss': 0.45678310096263885, 'train/mask_loss': 0.5464513748884201, 'metrics/total_secs_per_batch': 6.058490753173828, 'metrics/data_secs_per_batch': 2.766266202926636, '_timestamp': 1740978999.6690805}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 156 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740978999.6693974}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 157 is less than current step: 499. Dropping entry: {'train/loss': 2.270126384496689, 'train/ce_loss': 0.20927734375, 'train/seg_cls_loss': 0.018365478515625, 'train/kl_loss': 0.3646484375, 'train/mask_bce_loss': 0.25146085061132906, 'train/mask_dice_loss': 0.7560632735490799, 'train/mask_loss': 1.0075241297483444, 'metrics/total_secs_per_batch': 6.845475673675537, 'metrics/data_secs_per_batch': 2.9910403966903685, '_timestamp': 1740979006.5144963}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 157 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979006.5147755}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 158 is less than current step: 499. Dropping entry: {'train/loss': 1.292940229177475, 'train/ce_loss': 0.588671875, 'train/seg_cls_loss': 0.009722900390625, 'train/kl_loss': 0.215625, 'train/mask_bce_loss': 0.017086763121187686, 'train/mask_dice_loss': 0.3219126522541046, 'train/mask_loss': 0.33899941146373747, 'metrics/total_secs_per_batch': 4.64348030090332, 'metrics/data_secs_per_batch': 2.215988278388977, '_timestamp': 1740979011.1579645}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 158 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979011.1582294}).
[2025-03-02 23:16:56,918] [INFO] [logging.py:96:log_dist] [Rank 0] step=4160, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:16:56,924] [INFO] [timer.py:215:stop] epoch=0/micro_step=41600/global_step=4160, RunningAvgSamplesPerSec=1.481287729824876, CurrSamplesPerSec=1.7342801524451599, MemAllocated=30.99GB, MaxMemAllocated=37.23GB
Epoch: [8][160/500]	Time  5.768 ( 5.768)	Loss 0.8516 (1.6742)	CeLoss 0.8516 (0.5134)	SegCLSLoss 0.0000 (0.0137)	KLLoss 0.0000 (0.2176)	MaskLoss 0.0000 (0.5661)	MaskBCELoss 0.0000 (0.0960)	MaskDICELoss 0.0000 (0.4701)
Epoch: [8][161/500]	Time  6.858 ( 6.858)	Loss 1.7889 (1.6878)	CeLoss 0.1924 (0.2145)	SegCLSLoss 0.0251 (0.0166)	KLLoss 0.3652 (0.3271)	MaskLoss 0.7734 (0.7160)	MaskBCELoss 0.0290 (0.1264)	MaskDICELoss 0.7444 (0.5897)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 159 is less than current step: 499. Dropping entry: {'train/loss': 1.6742488622665406, 'train/ce_loss': 0.51337890625, 'train/seg_cls_loss': 0.0137451171875, 'train/kl_loss': 0.217578125, 'train/mask_bce_loss': 0.09600409958511591, 'train/mask_dice_loss': 0.47012423574924467, 'train/mask_loss': 0.5661283314228058, 'metrics/total_secs_per_batch': 5.767583608627319, 'metrics/data_secs_per_batch': 2.7594796895980833, '_timestamp': 1740979016.925352}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 159 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979016.925539}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 160 is less than current step: 499. Dropping entry: {'train/loss': 1.687798523902893, 'train/ce_loss': 0.214501953125, 'train/seg_cls_loss': 0.016552734375, 'train/kl_loss': 0.3271484375, 'train/mask_bce_loss': 0.12636082535609602, 'train/mask_dice_loss': 0.5896819859743119, 'train/mask_loss': 0.7160428136587143, 'metrics/total_secs_per_batch': 6.858333349227905, 'metrics/data_secs_per_batch': 3.043515753746033, '_timestamp': 1740979023.783894}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 160 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979023.7841804}).
Epoch: [8][162/500]	Time  6.623 ( 6.623)	Loss 2.1816 (1.9000)	CeLoss 0.1885 (0.3520)	SegCLSLoss 0.0219 (0.0142)	KLLoss 0.3594 (0.2912)	MaskLoss 0.9731 (0.7559)	MaskBCELoss 0.0304 (0.1057)	MaskDICELoss 0.9428 (0.6501)
Epoch: [8][163/500]	Time  6.090 ( 6.090)	Loss 0.6719 (1.7430)	CeLoss 0.6719 (0.2663)	SegCLSLoss 0.0000 (0.0155)	KLLoss 0.0000 (0.3236)	MaskLoss 0.0000 (0.7184)	MaskBCELoss 0.0000 (0.1173)	MaskDICELoss 0.0000 (0.6011)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 161 is less than current step: 499. Dropping entry: {'train/loss': 1.8999824404716492, 'train/ce_loss': 0.351953125, 'train/seg_cls_loss': 0.014208984375, 'train/kl_loss': 0.2912109375, 'train/mask_bce_loss': 0.1057372098788619, 'train/mask_dice_loss': 0.6501133680343628, 'train/mask_loss': 0.7558505952358245, 'metrics/total_secs_per_batch': 6.623002052307129, 'metrics/data_secs_per_batch': 3.1059503078460695, '_timestamp': 1740979030.4069786}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 161 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979030.4073648}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 162 is less than current step: 499. Dropping entry: {'train/loss': 1.7430428266525269, 'train/ce_loss': 0.26630859375, 'train/seg_cls_loss': 0.015484619140625, 'train/kl_loss': 0.3236328125, 'train/mask_bce_loss': 0.11732964534312487, 'train/mask_dice_loss': 0.6010667771100998, 'train/mask_loss': 0.7183964192867279, 'metrics/total_secs_per_batch': 6.089517593383789, 'metrics/data_secs_per_batch': 2.775058937072754, '_timestamp': 1740979036.496542}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 162 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979036.496852}).
Epoch: [8][164/500]	Time  6.374 ( 6.374)	Loss 1.0469 (1.4012)	CeLoss 1.0469 (0.4008)	SegCLSLoss 0.0000 (0.0102)	KLLoss 0.0000 (0.2186)	MaskLoss 0.0000 (0.4868)	MaskBCELoss 0.0000 (0.0750)	MaskDICELoss 0.0000 (0.4118)
Epoch: [8][165/500]	Time  5.957 ( 5.957)	Loss 1.3574 (1.6819)	CeLoss 0.2852 (0.2941)	SegCLSLoss 0.0104 (0.0139)	KLLoss 0.3633 (0.3273)	MaskLoss 0.5146 (0.6740)	MaskBCELoss 0.0306 (0.0708)	MaskDICELoss 0.4841 (0.6032)
Epoch: [8][166/500]	Time  4.923 ( 4.923)	Loss 2.1573 (1.7354)	CeLoss 0.2051 (0.3997)	SegCLSLoss 0.0269 (0.0130)	KLLoss 0.3789 (0.2584)	MaskLoss 0.9507 (0.6516)	MaskBCELoss 0.2505 (0.1491)	MaskDICELoss 0.7003 (0.5025)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 163 is less than current step: 499. Dropping entry: {'train/loss': 1.401225709915161, 'train/ce_loss': 0.40078125, 'train/seg_cls_loss': 0.010174560546875, 'train/kl_loss': 0.2185546875, 'train/mask_bce_loss': 0.07499380800873041, 'train/mask_dice_loss': 0.411800691485405, 'train/mask_loss': 0.4867944955825806, 'metrics/total_secs_per_batch': 6.374035835266113, 'metrics/data_secs_per_batch': 3.0004884004592896, '_timestamp': 1740979042.8704584}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 163 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979042.8707364}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 164 is less than current step: 499. Dropping entry: {'train/loss': 1.6819289326667786, 'train/ce_loss': 0.294140625, 'train/seg_cls_loss': 0.01387939453125, 'train/kl_loss': 0.32734375, 'train/mask_bce_loss': 0.07075933832675219, 'train/mask_dice_loss': 0.6032129280269146, 'train/mask_loss': 0.6739722639322281, 'metrics/total_secs_per_batch': 5.9566004276275635, 'metrics/data_secs_per_batch': 2.65302996635437, '_timestamp': 1740979048.8272784}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 164 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979048.8276644}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 165 is less than current step: 499. Dropping entry: {'train/loss': 1.7353811740875245, 'train/ce_loss': 0.399658203125, 'train/seg_cls_loss': 0.012982177734375, 'train/kl_loss': 0.2583984375, 'train/mask_bce_loss': 0.14909848682582377, 'train/mask_dice_loss': 0.5025032341480256, 'train/mask_loss': 0.6516017228364944, 'metrics/total_secs_per_batch': 4.922924757003784, 'metrics/data_secs_per_batch': 2.54302122592926, '_timestamp': 1740979053.7499874}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 165 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979053.7502594}).
Epoch: [8][167/500]	Time  4.918 ( 4.918)	Loss 0.9844 (1.1914)	CeLoss 0.9844 (0.5778)	SegCLSLoss 0.0000 (0.0078)	KLLoss 0.0000 (0.1434)	MaskLoss 0.0000 (0.2976)	MaskBCELoss 0.0000 (0.0511)	MaskDICELoss 0.0000 (0.2465)
Epoch: [8][168/500]	Time  5.866 ( 5.866)	Loss 2.1296 (1.7498)	CeLoss 0.1602 (0.6004)	SegCLSLoss 0.0280 (0.0122)	KLLoss 0.3633 (0.2529)	MaskLoss 0.9598 (0.5591)	MaskBCELoss 0.0156 (0.0826)	MaskDICELoss 0.9442 (0.4765)
Epoch: [8][169/500]	Time  6.831 ( 6.831)	Loss 1.5153 (1.9181)	CeLoss 0.2812 (0.2448)	SegCLSLoss 0.0110 (0.0160)	KLLoss 0.3652 (0.3623)	MaskLoss 0.5955 (0.8146)	MaskBCELoss 0.1155 (0.1313)	MaskDICELoss 0.4801 (0.6833)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 166 is less than current step: 499. Dropping entry: {'train/loss': 1.1913631081581115, 'train/ce_loss': 0.577783203125, 'train/seg_cls_loss': 0.007794189453125, 'train/kl_loss': 0.143359375, 'train/mask_bce_loss': 0.05114369727671146, 'train/mask_dice_loss': 0.24646658301353455, 'train/mask_loss': 0.297610279917717, 'metrics/total_secs_per_batch': 4.918442487716675, 'metrics/data_secs_per_batch': 2.2266563892364504, '_timestamp': 1740979058.6684291}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 166 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979058.6686153}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 167 is less than current step: 499. Dropping entry: {'train/loss': 1.749770176410675, 'train/ce_loss': 0.600390625, 'train/seg_cls_loss': 0.012152099609375, 'train/kl_loss': 0.2529296875, 'train/mask_bce_loss': 0.08263952555134893, 'train/mask_dice_loss': 0.47647408545017245, 'train/mask_loss': 0.5591136038303375, 'metrics/total_secs_per_batch': 5.865715980529785, 'metrics/data_secs_per_batch': 2.5915184736251833, '_timestamp': 1740979064.5341685}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 167 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979064.5345223}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 168 is less than current step: 499. Dropping entry: {'train/loss': 1.9181057691574097, 'train/ce_loss': 0.24482421875, 'train/seg_cls_loss': 0.0159912109375, 'train/kl_loss': 0.3623046875, 'train/mask_bce_loss': 0.13126724138855933, 'train/mask_dice_loss': 0.6833032205700874, 'train/mask_loss': 0.8145704537630081, 'metrics/total_secs_per_batch': 6.830731630325317, 'metrics/data_secs_per_batch': 3.1094852924346923, '_timestamp': 1740979071.3651316}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 168 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979071.3654952}).
[2025-03-02 23:17:58,328] [INFO] [logging.py:96:log_dist] [Rank 0] step=4170, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:17:58,334] [INFO] [timer.py:215:stop] epoch=0/micro_step=41700/global_step=4170, RunningAvgSamplesPerSec=1.4816095711949548, CurrSamplesPerSec=1.4350780813732942, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [8][170/500]	Time  6.970 ( 6.970)	Loss 1.2892 (1.6931)	CeLoss 0.2363 (0.2257)	SegCLSLoss 0.0162 (0.0161)	KLLoss 0.3691 (0.3641)	MaskLoss 0.5040 (0.7114)	MaskBCELoss 0.1788 (0.1104)	MaskDICELoss 0.3252 (0.6010)
Epoch: [8][171/500]	Time  6.115 ( 6.115)	Loss 2.6065 (1.6865)	CeLoss 0.3340 (0.3144)	SegCLSLoss 0.0104 (0.0115)	KLLoss 0.3535 (0.2914)	MaskLoss 1.1157 (0.6685)	MaskBCELoss 0.1846 (0.1213)	MaskDICELoss 0.9311 (0.5472)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 169 is less than current step: 499. Dropping entry: {'train/loss': 1.693131250143051, 'train/ce_loss': 0.22568359375, 'train/seg_cls_loss': 0.016082763671875, 'train/kl_loss': 0.3640625, 'train/mask_bce_loss': 0.11043284330517053, 'train/mask_dice_loss': 0.6009765177965164, 'train/mask_loss': 0.7114093631505967, 'metrics/total_secs_per_batch': 6.970160484313965, 'metrics/data_secs_per_batch': 3.153021502494812, '_timestamp': 1740979078.3348641}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 169 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979078.3351367}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 170 is less than current step: 499. Dropping entry: {'train/loss': 1.6864533424377441, 'train/ce_loss': 0.31435546875, 'train/seg_cls_loss': 0.011468505859375, 'train/kl_loss': 0.29140625, 'train/mask_bce_loss': 0.12134038750082254, 'train/mask_dice_loss': 0.5471792295575142, 'train/mask_loss': 0.6685196131467819, 'metrics/total_secs_per_batch': 6.114651441574097, 'metrics/data_secs_per_batch': 2.8705845355987547, '_timestamp': 1740979084.4497128}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 170 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979084.4499931}).
Epoch: [8][172/500]	Time  5.412 ( 5.412)	Loss 0.5625 (1.4475)	CeLoss 0.5625 (0.2221)	SegCLSLoss 0.0000 (0.0100)	KLLoss 0.0000 (0.2535)	MaskLoss 0.0000 (0.5974)	MaskBCELoss 0.0000 (0.1785)	MaskDICELoss 0.0000 (0.4189)
Epoch: [8][173/500]	Time  7.080 ( 7.080)	Loss 1.6224 (1.5949)	CeLoss 0.2559 (0.2374)	SegCLSLoss 0.0162 (0.0142)	KLLoss 0.3652 (0.3273)	MaskLoss 0.6608 (0.6587)	MaskBCELoss 0.0890 (0.0958)	MaskDICELoss 0.5718 (0.5629)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 171 is less than current step: 499. Dropping entry: {'train/loss': 1.4475233912467957, 'train/ce_loss': 0.222119140625, 'train/seg_cls_loss': 0.010028076171875, 'train/kl_loss': 0.253515625, 'train/mask_bce_loss': 0.17853604070842266, 'train/mask_dice_loss': 0.41888288110494615, 'train/mask_loss': 0.5974189192056656, 'metrics/total_secs_per_batch': 5.411799430847168, 'metrics/data_secs_per_batch': 2.5003816366195677, '_timestamp': 1740979089.8615234}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 171 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979089.861811}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 172 is less than current step: 499. Dropping entry: {'train/loss': 1.594900870323181, 'train/ce_loss': 0.23740234375, 'train/seg_cls_loss': 0.01419677734375, 'train/kl_loss': 0.32734375, 'train/mask_bce_loss': 0.09580821800045669, 'train/mask_dice_loss': 0.5628726989030838, 'train/mask_loss': 0.6586809098720551, 'metrics/total_secs_per_batch': 7.080377817153931, 'metrics/data_secs_per_batch': 3.0930094003677366, '_timestamp': 1740979096.9419687}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 172 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979096.9422855}).
Epoch: [8][174/500]	Time  5.738 ( 5.738)	Loss 2.4292 (1.8339)	CeLoss 0.2217 (0.2535)	SegCLSLoss 0.0170 (0.0179)	KLLoss 0.3809 (0.2949)	MaskLoss 1.0798 (0.7707)	MaskBCELoss 0.2400 (0.1891)	MaskDICELoss 0.8398 (0.5816)
Epoch: [8][175/500]	Time  6.178 ( 6.178)	Loss 2.4791 (1.6769)	CeLoss 0.1299 (0.2658)	SegCLSLoss 0.0282 (0.0160)	KLLoss 0.3848 (0.2895)	MaskLoss 1.1487 (0.6871)	MaskBCELoss 0.2108 (0.1847)	MaskDICELoss 0.9380 (0.5024)
Epoch: [8][176/500]	Time  7.106 ( 7.106)	Loss 2.6930 (1.6043)	CeLoss 0.1768 (0.1929)	SegCLSLoss 0.0248 (0.0138)	KLLoss 0.3633 (0.2914)	MaskLoss 1.2337 (0.6875)	MaskBCELoss 0.3573 (0.1547)	MaskDICELoss 0.8764 (0.5328)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 173 is less than current step: 499. Dropping entry: {'train/loss': 1.8338800311088561, 'train/ce_loss': 0.253466796875, 'train/seg_cls_loss': 0.01793212890625, 'train/kl_loss': 0.294921875, 'train/mask_bce_loss': 0.18912951350212098, 'train/mask_dice_loss': 0.5816190987825394, 'train/mask_loss': 0.7707486093044281, 'metrics/total_secs_per_batch': 5.7384161949157715, 'metrics/data_secs_per_batch': 2.4096989154815676, '_timestamp': 1740979102.6803517}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 173 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979102.680669}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 174 is less than current step: 499. Dropping entry: {'train/loss': 1.6769367337226868, 'train/ce_loss': 0.2658203125, 'train/seg_cls_loss': 0.01602783203125, 'train/kl_loss': 0.289453125, 'train/mask_bce_loss': 0.18471312522888184, 'train/mask_dice_loss': 0.502388060092926, 'train/mask_loss': 0.6871011853218079, 'metrics/total_secs_per_batch': 6.1776838302612305, 'metrics/data_secs_per_batch': 3.122190022468567, '_timestamp': 1740979108.8580332}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 174 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979108.858314}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 175 is less than current step: 499. Dropping entry: {'train/loss': 1.6042698502540589, 'train/ce_loss': 0.192919921875, 'train/seg_cls_loss': 0.013812255859375, 'train/kl_loss': 0.29140625, 'train/mask_bce_loss': 0.15468623116612434, 'train/mask_dice_loss': 0.5328246861696243, 'train/mask_loss': 0.6875109046697616, 'metrics/total_secs_per_batch': 7.105583906173706, 'metrics/data_secs_per_batch': 3.2311182498931883, '_timestamp': 1740979115.9636745}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 175 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979115.9639943}).
Epoch: [8][177/500]	Time  6.689 ( 6.689)	Loss 2.4669 (1.7766)	CeLoss 0.2061 (0.2395)	SegCLSLoss 0.0139 (0.0136)	KLLoss 0.3652 (0.3232)	MaskLoss 1.1085 (0.7492)	MaskBCELoss 0.2513 (0.0882)	MaskDICELoss 0.8571 (0.6611)
Epoch: [8][178/500]	Time  6.704 ( 6.704)	Loss 2.0705 (1.6062)	CeLoss 0.2422 (0.2048)	SegCLSLoss 0.0115 (0.0136)	KLLoss 0.3594 (0.3293)	MaskLoss 0.8936 (0.6807)	MaskBCELoss 0.0147 (0.1448)	MaskDICELoss 0.8789 (0.5359)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 176 is less than current step: 499. Dropping entry: {'train/loss': 1.7765572309494018, 'train/ce_loss': 0.239453125, 'train/seg_cls_loss': 0.01361083984375, 'train/kl_loss': 0.3232421875, 'train/mask_bce_loss': 0.08815095545724035, 'train/mask_dice_loss': 0.6610651582479476, 'train/mask_loss': 0.7492161214351654, 'metrics/total_secs_per_batch': 6.689203262329102, 'metrics/data_secs_per_batch': 3.0153834342956545, '_timestamp': 1740979122.6529055}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 176 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979122.6532445}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 177 is less than current step: 499. Dropping entry: {'train/loss': 1.606204241514206, 'train/ce_loss': 0.204833984375, 'train/seg_cls_loss': 0.013555908203125, 'train/kl_loss': 0.329296875, 'train/mask_bce_loss': 0.14479271061718463, 'train/mask_dice_loss': 0.5359217181801796, 'train/mask_loss': 0.6807144254446029, 'metrics/total_secs_per_batch': 6.703850746154785, 'metrics/data_secs_per_batch': 3.012217378616333, '_timestamp': 1740979129.3566759}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 177 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979129.3569782}).
Epoch: [8][179/500]	Time  5.671 ( 5.671)	Loss 2.7090 (1.7037)	CeLoss 0.1836 (0.5840)	SegCLSLoss 0.0166 (0.0110)	KLLoss 0.3770 (0.2549)	MaskLoss 1.2403 (0.5443)	MaskBCELoss 0.5079 (0.1562)	MaskDICELoss 0.7323 (0.3882)
[2025-03-02 23:18:59,862] [INFO] [logging.py:96:log_dist] [Rank 0] step=4180, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:18:59,867] [INFO] [timer.py:215:stop] epoch=0/micro_step=41800/global_step=4180, RunningAvgSamplesPerSec=1.4819235309939567, CurrSamplesPerSec=2.067070746258006, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [8][180/500]	Time  4.840 ( 4.840)	Loss 0.8280 (1.3357)	CeLoss 0.2207 (0.6001)	SegCLSLoss 0.0133 (0.0079)	KLLoss 0.3652 (0.2178)	MaskLoss 0.2822 (0.3550)	MaskBCELoss 0.0744 (0.0758)	MaskDICELoss 0.2077 (0.2791)
Epoch: [8][181/500]	Time  6.459 ( 6.459)	Loss 2.3359 (1.6975)	CeLoss 0.2041 (0.3104)	SegCLSLoss 0.0166 (0.0128)	KLLoss 0.3789 (0.2568)	MaskLoss 1.0430 (0.6775)	MaskBCELoss 0.2702 (0.1247)	MaskDICELoss 0.7728 (0.5528)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 178 is less than current step: 499. Dropping entry: {'train/loss': 1.7036718249320983, 'train/ce_loss': 0.583984375, 'train/seg_cls_loss': 0.010968017578125, 'train/kl_loss': 0.2548828125, 'train/mask_bce_loss': 0.15616272613406182, 'train/mask_dice_loss': 0.3881536602973938, 'train/mask_loss': 0.5443163871765136, 'metrics/total_secs_per_batch': 5.671409606933594, 'metrics/data_secs_per_batch': 2.3334394693374634, '_timestamp': 1740979135.0282652}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 178 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979135.0286129}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 179 is less than current step: 499. Dropping entry: {'train/loss': 1.3356829166412354, 'train/ce_loss': 0.60009765625, 'train/seg_cls_loss': 0.007879638671875, 'train/kl_loss': 0.2177734375, 'train/mask_bce_loss': 0.07584525905549526, 'train/mask_dice_loss': 0.2791055738925934, 'train/mask_loss': 0.35495083332061766, 'metrics/total_secs_per_batch': 4.83966064453125, 'metrics/data_secs_per_batch': 1.9187037229537964, '_timestamp': 1740979139.867552}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 179 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979139.8678536}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 180 is less than current step: 499. Dropping entry: {'train/loss': 1.697456705570221, 'train/ce_loss': 0.3104248046875, 'train/seg_cls_loss': 0.012750244140625, 'train/kl_loss': 0.2568359375, 'train/mask_bce_loss': 0.12466249130666256, 'train/mask_dice_loss': 0.5527889847755432, 'train/mask_loss': 0.6774514794349671, 'metrics/total_secs_per_batch': 6.459374904632568, 'metrics/data_secs_per_batch': 2.7592127323150635, '_timestamp': 1740979146.3271415}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 180 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979146.3275244}).
Epoch: [8][182/500]	Time  5.348 ( 5.348)	Loss 1.5979 (1.2208)	CeLoss 0.2031 (0.3974)	SegCLSLoss 0.0159 (0.0101)	KLLoss 0.3633 (0.2174)	MaskLoss 0.6754 (0.3983)	MaskBCELoss 0.0388 (0.0798)	MaskDICELoss 0.6366 (0.3185)
Epoch: [8][183/500]	Time  5.199 ( 5.199)	Loss 2.2486 (1.7125)	CeLoss 0.2002 (0.4394)	SegCLSLoss 0.0130 (0.0115)	KLLoss 0.3574 (0.2578)	MaskLoss 1.0032 (0.6207)	MaskBCELoss 0.0422 (0.1284)	MaskDICELoss 0.9610 (0.4924)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 181 is less than current step: 499. Dropping entry: {'train/loss': 1.2207778155803681, 'train/ce_loss': 0.39736328125, 'train/seg_cls_loss': 0.010064697265625, 'train/kl_loss': 0.2173828125, 'train/mask_bce_loss': 0.07981384918093681, 'train/mask_dice_loss': 0.31846569031476973, 'train/mask_loss': 0.398279532790184, 'metrics/total_secs_per_batch': 5.348012208938599, 'metrics/data_secs_per_batch': 2.2980695247650145, '_timestamp': 1740979151.6751263}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 181 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979151.675328}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 182 is less than current step: 499. Dropping entry: {'train/loss': 1.7124945521354675, 'train/ce_loss': 0.439404296875, 'train/seg_cls_loss': 0.01153564453125, 'train/kl_loss': 0.2578125, 'train/mask_bce_loss': 0.1283553797751665, 'train/mask_dice_loss': 0.4923694431781769, 'train/mask_loss': 0.620724818110466, 'metrics/total_secs_per_batch': 5.199171781539917, 'metrics/data_secs_per_batch': 2.673252272605896, '_timestamp': 1740979156.8744953}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 182 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979156.8749273}).
Epoch: [8][184/500]	Time  6.182 ( 6.182)	Loss 0.4395 (1.4341)	CeLoss 0.4395 (0.4520)	SegCLSLoss 0.0000 (0.0097)	KLLoss 0.0000 (0.2164)	MaskLoss 0.0000 (0.4779)	MaskBCELoss 0.0000 (0.0594)	MaskDICELoss 0.0000 (0.4185)
Epoch: [8][185/500]	Time  5.346 ( 5.346)	Loss 1.4297 (1.2232)	CeLoss 1.4297 (0.6041)	SegCLSLoss 0.0000 (0.0067)	KLLoss 0.0000 (0.1832)	MaskLoss 0.0000 (0.2986)	MaskBCELoss 0.0000 (0.0359)	MaskDICELoss 0.0000 (0.2627)
Epoch: [8][186/500]	Time  6.924 ( 6.924)	Loss 2.0078 (1.8037)	CeLoss 0.2129 (0.3316)	SegCLSLoss 0.0232 (0.0141)	KLLoss 0.3516 (0.3244)	MaskLoss 0.8740 (0.7163)	MaskBCELoss 0.1115 (0.1304)	MaskDICELoss 0.7625 (0.5859)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 183 is less than current step: 499. Dropping entry: {'train/loss': 1.4340918421745301, 'train/ce_loss': 0.451953125, 'train/seg_cls_loss': 0.009722900390625, 'train/kl_loss': 0.21640625, 'train/mask_bce_loss': 0.05935272481292486, 'train/mask_dice_loss': 0.4185330420732498, 'train/mask_loss': 0.477885764837265, 'metrics/total_secs_per_batch': 6.182095766067505, 'metrics/data_secs_per_batch': 2.915649747848511, '_timestamp': 1740979163.0564187}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 183 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979163.0567102}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 184 is less than current step: 499. Dropping entry: {'train/loss': 1.2232381224632263, 'train/ce_loss': 0.6041015625, 'train/seg_cls_loss': 0.00672607421875, 'train/kl_loss': 0.183203125, 'train/mask_bce_loss': 0.035921050049364564, 'train/mask_dice_loss': 0.2626609027385712, 'train/mask_loss': 0.29858195334672927, 'metrics/total_secs_per_batch': 5.345588445663452, 'metrics/data_secs_per_batch': 2.6463045835494996, '_timestamp': 1740979168.4020038}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 184 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979168.402293}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 185 is less than current step: 499. Dropping entry: {'train/loss': 1.803669774532318, 'train/ce_loss': 0.331640625, 'train/seg_cls_loss': 0.014068603515625, 'train/kl_loss': 0.3244140625, 'train/mask_bce_loss': 0.13038773108273743, 'train/mask_dice_loss': 0.58590027987957, 'train/mask_loss': 0.7162880092859268, 'metrics/total_secs_per_batch': 6.924043655395508, 'metrics/data_secs_per_batch': 3.1725852489471436, '_timestamp': 1740979175.326124}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 185 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979175.3264375}).
Epoch: [8][187/500]	Time  6.165 ( 6.165)	Loss 2.3195 (1.8843)	CeLoss 0.1660 (0.4127)	SegCLSLoss 0.0200 (0.0136)	KLLoss 0.3770 (0.2889)	MaskLoss 1.0533 (0.7180)	MaskBCELoss 0.2122 (0.1306)	MaskDICELoss 0.8411 (0.5874)
Epoch: [8][188/500]	Time  5.187 ( 5.187)	Loss 1.8204 (1.9409)	CeLoss 0.4121 (0.5574)	SegCLSLoss 0.0260 (0.0151)	KLLoss 0.3711 (0.2578)	MaskLoss 0.6787 (0.6750)	MaskBCELoss 0.2086 (0.1554)	MaskDICELoss 0.4702 (0.5197)
Epoch: [8][189/500]	Time  5.411 ( 5.411)	Loss 1.8516 (1.3507)	CeLoss 1.8516 (0.5031)	SegCLSLoss 0.0000 (0.0114)	KLLoss 0.0000 (0.2184)	MaskLoss 0.0000 (0.4098)	MaskBCELoss 0.0000 (0.0829)	MaskDICELoss 0.0000 (0.3269)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 186 is less than current step: 499. Dropping entry: {'train/loss': 1.8842577457427978, 'train/ce_loss': 0.4126953125, 'train/seg_cls_loss': 0.01357421875, 'train/kl_loss': 0.2888671875, 'train/mask_bce_loss': 0.1305527061223984, 'train/mask_dice_loss': 0.5874062508344651, 'train/mask_loss': 0.7179589450359345, 'metrics/total_secs_per_batch': 6.165490627288818, 'metrics/data_secs_per_batch': 2.5754539728164674, '_timestamp': 1740979181.4916363}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 186 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979181.4919631}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 187 is less than current step: 499. Dropping entry: {'train/loss': 1.9408682703971862, 'train/ce_loss': 0.557373046875, 'train/seg_cls_loss': 0.015130615234375, 'train/kl_loss': 0.2578125, 'train/mask_bce_loss': 0.15537065491080285, 'train/mask_dice_loss': 0.5196533262729645, 'train/mask_loss': 0.6750239908695221, 'metrics/total_secs_per_batch': 5.1873533725738525, 'metrics/data_secs_per_batch': 2.105479431152344, '_timestamp': 1740979186.6788945}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 187 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979186.6791832}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 188 is less than current step: 499. Dropping entry: {'train/loss': 1.3506516933441162, 'train/ce_loss': 0.503125, 'train/seg_cls_loss': 0.011383056640625, 'train/kl_loss': 0.218359375, 'train/mask_bce_loss': 0.08290253952145576, 'train/mask_dice_loss': 0.3269447922706604, 'train/mask_loss': 0.4098473280668259, 'metrics/total_secs_per_batch': 5.4114298820495605, 'metrics/data_secs_per_batch': 2.7411953449249267, '_timestamp': 1740979192.090505}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 188 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979192.0908709}).
[2025-03-02 23:19:56,907] [INFO] [logging.py:96:log_dist] [Rank 0] step=4190, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:19:56,913] [INFO] [timer.py:215:stop] epoch=0/micro_step=41900/global_step=4190, RunningAvgSamplesPerSec=1.4824715079731392, CurrSamplesPerSec=2.0736912935504175, MemAllocated=31.31GB, MaxMemAllocated=37.23GB
Epoch: [8][190/500]	Time  4.824 ( 4.824)	Loss 1.6444 (1.7212)	CeLoss 0.2090 (0.4974)	SegCLSLoss 0.0205 (0.0140)	KLLoss 0.3633 (0.2549)	MaskLoss 0.6943 (0.5957)	MaskBCELoss 0.0351 (0.1405)	MaskDICELoss 0.6591 (0.4551)
Epoch: [8][191/500]	Time  6.399 ( 6.399)	Loss 0.7608 (1.4123)	CeLoss 0.1982 (0.3788)	SegCLSLoss 0.0112 (0.0111)	KLLoss 0.3613 (0.2535)	MaskLoss 0.2603 (0.5014)	MaskBCELoss 0.0197 (0.0659)	MaskDICELoss 0.2406 (0.4355)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 189 is less than current step: 499. Dropping entry: {'train/loss': 1.7211894631385802, 'train/ce_loss': 0.49736328125, 'train/seg_cls_loss': 0.013983154296875, 'train/kl_loss': 0.2548828125, 'train/mask_bce_loss': 0.1405236966907978, 'train/mask_dice_loss': 0.45512962341308594, 'train/mask_loss': 0.5956533312797546, 'metrics/total_secs_per_batch': 4.824166297912598, 'metrics/data_secs_per_batch': 2.1842657327651978, '_timestamp': 1740979196.9142995}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 189 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979196.9145818}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 190 is less than current step: 499. Dropping entry: {'train/loss': 1.4122584939002991, 'train/ce_loss': 0.378759765625, 'train/seg_cls_loss': 0.0111083984375, 'train/kl_loss': 0.253515625, 'train/mask_bce_loss': 0.06588953789323568, 'train/mask_dice_loss': 0.4354789838194847, 'train/mask_loss': 0.5013685166835785, 'metrics/total_secs_per_batch': 6.399303436279297, 'metrics/data_secs_per_batch': 2.9106210470199585, '_timestamp': 1740979203.3137965}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 190 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979203.314083}).
Epoch: [8][192/500]	Time  5.440 ( 5.440)	Loss 1.6455 (1.7932)	CeLoss 0.2090 (0.5512)	SegCLSLoss 0.0152 (0.0129)	KLLoss 0.3672 (0.2535)	MaskLoss 0.6958 (0.6050)	MaskBCELoss 0.0096 (0.1233)	MaskDICELoss 0.6862 (0.4817)
Epoch: [8][193/500]	Time  5.650 ( 5.650)	Loss 0.5547 (1.3310)	CeLoss 0.5547 (0.5396)	SegCLSLoss 0.0000 (0.0091)	KLLoss 0.0000 (0.2172)	MaskLoss 0.0000 (0.3825)	MaskBCELoss 0.0000 (0.0938)	MaskDICELoss 0.0000 (0.2887)
Epoch: [8][194/500]	Time  6.265 ( 6.265)	Loss 1.1577 (1.7775)	CeLoss 0.2520 (0.3023)	SegCLSLoss 0.0102 (0.0182)	KLLoss 0.3594 (0.3273)	MaskLoss 0.4323 (0.7165)	MaskBCELoss 0.0704 (0.1219)	MaskDICELoss 0.3620 (0.5947)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 191 is less than current step: 499. Dropping entry: {'train/loss': 1.7932254672050476, 'train/ce_loss': 0.551171875, 'train/seg_cls_loss': 0.01285400390625, 'train/kl_loss': 0.253515625, 'train/mask_bce_loss': 0.1232693674042821, 'train/mask_dice_loss': 0.48174180686473844, 'train/mask_loss': 0.6050111770629882, 'metrics/total_secs_per_batch': 5.440350294113159, 'metrics/data_secs_per_batch': 2.258251953125, '_timestamp': 1740979208.7542217}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 191 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979208.7545626}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 192 is less than current step: 499. Dropping entry: {'train/loss': 1.331009578704834, 'train/ce_loss': 0.53955078125, 'train/seg_cls_loss': 0.009051513671875, 'train/kl_loss': 0.2171875, 'train/mask_bce_loss': 0.09376094564795494, 'train/mask_dice_loss': 0.28873603492975236, 'train/mask_loss': 0.38249698281288147, 'metrics/total_secs_per_batch': 5.650447130203247, 'metrics/data_secs_per_batch': 2.7040630340576173, '_timestamp': 1740979214.404633}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 192 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979214.404928}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 193 is less than current step: 499. Dropping entry: {'train/loss': 1.7774728536605835, 'train/ce_loss': 0.30234375, 'train/seg_cls_loss': 0.018231201171875, 'train/kl_loss': 0.32734375, 'train/mask_bce_loss': 0.12185015100985766, 'train/mask_dice_loss': 0.5946694761514664, 'train/mask_loss': 0.716519632935524, 'metrics/total_secs_per_batch': 6.265207529067993, 'metrics/data_secs_per_batch': 2.86633460521698, '_timestamp': 1740979220.6698635}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 193 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979220.6702585}).
Epoch: [8][195/500]	Time  5.375 ( 5.375)	Loss 1.1611 (1.5211)	CeLoss 0.2754 (0.5616)	SegCLSLoss 0.0092 (0.0101)	KLLoss 0.3613 (0.2541)	MaskLoss 0.4223 (0.4643)	MaskBCELoss 0.0909 (0.1022)	MaskDICELoss 0.3314 (0.3621)
Epoch: [8][196/500]	Time  5.498 ( 5.498)	Loss 1.9054 (1.7313)	CeLoss 0.1953 (0.4426)	SegCLSLoss 0.0216 (0.0135)	KLLoss 0.3516 (0.2877)	MaskLoss 0.8321 (0.6267)	MaskBCELoss 0.1038 (0.0679)	MaskDICELoss 0.7283 (0.5588)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 194 is less than current step: 499. Dropping entry: {'train/loss': 1.5210724830627442, 'train/ce_loss': 0.56162109375, 'train/seg_cls_loss': 0.010113525390625, 'train/kl_loss': 0.2541015625, 'train/mask_bce_loss': 0.10223737098276615, 'train/mask_dice_loss': 0.36210745573043823, 'train/mask_loss': 0.46434484124183656, 'metrics/total_secs_per_batch': 5.375236988067627, 'metrics/data_secs_per_batch': 2.4828721046447755, '_timestamp': 1740979226.0451314}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 194 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979226.045555}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 195 is less than current step: 499. Dropping entry: {'train/loss': 1.7313117265701294, 'train/ce_loss': 0.442578125, 'train/seg_cls_loss': 0.0135498046875, 'train/kl_loss': 0.2876953125, 'train/mask_bce_loss': 0.0678505931980908, 'train/mask_dice_loss': 0.558840411901474, 'train/mask_loss': 0.6266910046339035, 'metrics/total_secs_per_batch': 5.497803449630737, 'metrics/data_secs_per_batch': 2.14124481678009, '_timestamp': 1740979231.5429766}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 195 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979231.5433152}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 196 is less than current step: 499. Dropping entry: {'train/loss': 1.5969093918800354, 'train/ce_loss': 0.407177734375, 'train/seg_cls_loss': 0.011883544921875, 'train/kl_loss': 0.253515625, 'train/mask_bce_loss': 0.12534974883310496, 'train/mask_dice_loss': 0.4538422405719757, 'train/mask_loss': 0.5791920021176338, 'metrics/total_secs_per_batch': 5.762381076812744, 'metrics/data_secs_per_batch': 2.7958841562271117, '_timestamp': 1740979237.3052456}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 196 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979237.3055894}).
Epoch: [8][197/500]	Time  5.762 ( 5.762)	Loss 2.5320 (1.5969)	CeLoss 0.2031 (0.4072)	SegCLSLoss 0.0228 (0.0119)	KLLoss 0.3633 (0.2535)	MaskLoss 1.1400 (0.5792)	MaskBCELoss 0.3424 (0.1253)	MaskDICELoss 0.7976 (0.4538)
Epoch: [8][198/500]	Time  5.223 ( 5.223)	Loss 1.8490 (1.7449)	CeLoss 0.2129 (0.5107)	SegCLSLoss 0.0310 (0.0131)	KLLoss 0.3594 (0.2195)	MaskLoss 0.7927 (0.6027)	MaskBCELoss 0.0880 (0.1370)	MaskDICELoss 0.7046 (0.4657)
Epoch: [8][199/500]	Time  6.167 ( 6.167)	Loss 2.1243 (1.8839)	CeLoss 0.2490 (0.3844)	SegCLSLoss 0.0110 (0.0144)	KLLoss 0.3633 (0.2898)	MaskLoss 0.9167 (0.7315)	MaskBCELoss 0.2237 (0.2374)	MaskDICELoss 0.6930 (0.4942)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 197 is less than current step: 499. Dropping entry: {'train/loss': 1.744876539707184, 'train/ce_loss': 0.5107421875, 'train/seg_cls_loss': 0.01309814453125, 'train/kl_loss': 0.21953125, 'train/mask_bce_loss': 0.13702501431107522, 'train/mask_dice_loss': 0.46568668484687803, 'train/mask_loss': 0.6027117073535919, 'metrics/total_secs_per_batch': 5.2232348918914795, 'metrics/data_secs_per_batch': 2.2515517234802247, '_timestamp': 1740979242.5285573}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 197 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979242.5288944}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 198 is less than current step: 499. Dropping entry: {'train/loss': 1.8839279174804688, 'train/ce_loss': 0.384375, 'train/seg_cls_loss': 0.014422607421875, 'train/kl_loss': 0.28984375, 'train/mask_bce_loss': 0.23735585510730745, 'train/mask_dice_loss': 0.49415889084339143, 'train/mask_loss': 0.7315147340297699, 'metrics/total_secs_per_batch': 6.166639089584351, 'metrics/data_secs_per_batch': 2.6347779989242555, '_timestamp': 1740979248.6952639}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 198 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979248.6955955}).
[2025-03-02 23:20:53,600] [INFO] [logging.py:96:log_dist] [Rank 0] step=4200, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:20:53,606] [INFO] [timer.py:215:stop] epoch=0/micro_step=42000/global_step=4200, RunningAvgSamplesPerSec=1.483035847829054, CurrSamplesPerSec=2.036723928581921, MemAllocated=30.95GB, MaxMemAllocated=37.23GB
Epoch: [8][200/500]	Time  4.912 ( 4.912)	Loss 1.6454 (1.5281)	CeLoss 0.2041 (0.4853)	SegCLSLoss 0.0233 (0.0124)	KLLoss 0.3750 (0.2539)	MaskLoss 0.6958 (0.5058)	MaskBCELoss 0.0465 (0.1081)	MaskDICELoss 0.6493 (0.3977)
Epoch: [8][201/500]	Time  6.773 ( 6.773)	Loss 0.0806 (1.2678)	CeLoss 0.0806 (0.2841)	SegCLSLoss 0.0000 (0.0090)	KLLoss 0.0000 (0.2521)	MaskLoss 0.0000 (0.4770)	MaskBCELoss 0.0000 (0.0579)	MaskDICELoss 0.0000 (0.4191)
Epoch: [8][202/500]	Time  5.749 ( 5.749)	Loss 1.0351 (1.4453)	CeLoss 0.1973 (0.4273)	SegCLSLoss 0.0211 (0.0143)	KLLoss 0.3652 (0.2893)	MaskLoss 0.3955 (0.4910)	MaskBCELoss 0.1885 (0.0841)	MaskDICELoss 0.2069 (0.4069)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 199 is less than current step: 499. Dropping entry: {'train/loss': 1.528105878829956, 'train/ce_loss': 0.48525390625, 'train/seg_cls_loss': 0.012396240234375, 'train/kl_loss': 0.25390625, 'train/mask_bce_loss': 0.10805862955749035, 'train/mask_dice_loss': 0.3976935356855392, 'train/mask_loss': 0.5057521626353264, 'metrics/total_secs_per_batch': 4.9116370677948, 'metrics/data_secs_per_batch': 1.9271057844161987, '_timestamp': 1740979253.6065862}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 199 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979253.6068826}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 200 is less than current step: 499. Dropping entry: {'train/loss': 1.2677527070045471, 'train/ce_loss': 0.284130859375, 'train/seg_cls_loss': 0.008990478515625, 'train/kl_loss': 0.2521484375, 'train/mask_bce_loss': 0.05789048485457897, 'train/mask_dice_loss': 0.41912551820278166, 'train/mask_loss': 0.47701599895954133, 'metrics/total_secs_per_batch': 6.7727508544921875, 'metrics/data_secs_per_batch': 3.1083290576934814, '_timestamp': 1740979260.3795595}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 200 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979260.3798828}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 201 is less than current step: 499. Dropping entry: {'train/loss': 1.4453106462955474, 'train/ce_loss': 0.42734375, 'train/seg_cls_loss': 0.01431884765625, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.08410720471292735, 'train/mask_dice_loss': 0.4069074928760529, 'train/mask_loss': 0.49101469814777376, 'metrics/total_secs_per_batch': 5.748754978179932, 'metrics/data_secs_per_batch': 2.5277425050735474, '_timestamp': 1740979266.1284559}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 201 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979266.1288111}).
Epoch: [8][203/500]	Time  6.756 ( 6.756)	Loss 1.8731 (1.6670)	CeLoss 0.2188 (0.2362)	SegCLSLoss 0.0157 (0.0148)	KLLoss 0.3555 (0.3621)	MaskLoss 0.8057 (0.6937)	MaskBCELoss 0.0728 (0.1313)	MaskDICELoss 0.7329 (0.5624)
Epoch: [8][204/500]	Time  6.219 ( 6.219)	Loss 1.2738 (1.4877)	CeLoss 0.2441 (0.4190)	SegCLSLoss 0.0113 (0.0118)	KLLoss 0.3652 (0.2523)	MaskLoss 0.4934 (0.5187)	MaskBCELoss 0.0560 (0.0484)	MaskDICELoss 0.4373 (0.4704)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 202 is less than current step: 499. Dropping entry: {'train/loss': 1.666957861185074, 'train/ce_loss': 0.23623046875, 'train/seg_cls_loss': 0.014788818359375, 'train/kl_loss': 0.362109375, 'train/mask_bce_loss': 0.13132118806242943, 'train/mask_dice_loss': 0.5624116428196431, 'train/mask_loss': 0.6937328428030014, 'metrics/total_secs_per_batch': 6.75565242767334, 'metrics/data_secs_per_batch': 3.021212100982666, '_timestamp': 1740979272.8840938}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 202 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979272.8844197}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 203 is less than current step: 499. Dropping entry: {'train/loss': 1.4877304434776306, 'train/ce_loss': 0.418994140625, 'train/seg_cls_loss': 0.011822509765625, 'train/kl_loss': 0.25234375, 'train/mask_bce_loss': 0.048377071134746076, 'train/mask_dice_loss': 0.47036605775356294, 'train/mask_loss': 0.5187431275844574, 'metrics/total_secs_per_batch': 6.219196557998657, 'metrics/data_secs_per_batch': 2.6682959318161013, '_timestamp': 1740979279.103143}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 203 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979279.1034186}).
Epoch: [8][205/500]	Time  6.142 ( 6.142)	Loss 1.6721 (1.4469)	CeLoss 0.2266 (0.4133)	SegCLSLoss 0.0128 (0.0093)	KLLoss 0.3633 (0.2531)	MaskLoss 0.7013 (0.5016)	MaskBCELoss 0.2289 (0.1022)	MaskDICELoss 0.4724 (0.3994)
Epoch: [8][206/500]	Time  5.620 ( 5.620)	Loss 0.1699 (1.8581)	CeLoss 0.1699 (0.4247)	SegCLSLoss 0.0000 (0.0122)	KLLoss 0.0000 (0.2551)	MaskLoss 0.0000 (0.7008)	MaskBCELoss 0.0000 (0.2547)	MaskDICELoss 0.0000 (0.4461)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 204 is less than current step: 499. Dropping entry: {'train/loss': 1.4469439506530761, 'train/ce_loss': 0.41328125, 'train/seg_cls_loss': 0.009271240234375, 'train/kl_loss': 0.253125, 'train/mask_bce_loss': 0.10223099328577519, 'train/mask_dice_loss': 0.39941480904817583, 'train/mask_loss': 0.5016458064317704, 'metrics/total_secs_per_batch': 6.141888380050659, 'metrics/data_secs_per_batch': 2.676788401603699, '_timestamp': 1740979285.245016}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 204 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979285.245209}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 205 is less than current step: 499. Dropping entry: {'train/loss': 1.8581187844276428, 'train/ce_loss': 0.42470703125, 'train/seg_cls_loss': 0.01217041015625, 'train/kl_loss': 0.255078125, 'train/mask_bce_loss': 0.25471351919695734, 'train/mask_dice_loss': 0.4461232155561447, 'train/mask_loss': 0.7008367300033569, 'metrics/total_secs_per_batch': 5.619609594345093, 'metrics/data_secs_per_batch': 2.332369303703308, '_timestamp': 1740979290.8646579}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 205 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979290.8649328}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 206 is less than current step: 499. Dropping entry: {'train/loss': 1.7104504585266114, 'train/ce_loss': 0.275830078125, 'train/seg_cls_loss': 0.01094970703125, 'train/kl_loss': 0.2904296875, 'train/mask_bce_loss': 0.15568180046975613, 'train/mask_dice_loss': 0.5441479086875916, 'train/mask_loss': 0.6998297154903412, 'metrics/total_secs_per_batch': 6.358499765396118, 'metrics/data_secs_per_batch': 2.9112114906311035, '_timestamp': 1740979297.223165}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 206 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979297.223453}).
Epoch: [8][207/500]	Time  6.358 ( 6.358)	Loss 0.6797 (1.7105)	CeLoss 0.6797 (0.2758)	SegCLSLoss 0.0000 (0.0109)	KLLoss 0.0000 (0.2904)	MaskLoss 0.0000 (0.6998)	MaskBCELoss 0.0000 (0.1557)	MaskDICELoss 0.0000 (0.5441)
Epoch: [8][208/500]	Time  6.079 ( 6.079)	Loss 2.5310 (1.7983)	CeLoss 0.2275 (0.4857)	SegCLSLoss 0.0221 (0.0140)	KLLoss 0.3691 (0.2559)	MaskLoss 1.1278 (0.6400)	MaskBCELoss 0.4251 (0.1628)	MaskDICELoss 0.7028 (0.4772)
Epoch: [8][209/500]	Time  5.807 ( 5.807)	Loss 0.7907 (1.5525)	CeLoss 0.2734 (0.3984)	SegCLSLoss 0.0195 (0.0130)	KLLoss 0.3613 (0.2912)	MaskLoss 0.2352 (0.5593)	MaskBCELoss 0.1171 (0.1213)	MaskDICELoss 0.1181 (0.4380)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 207 is less than current step: 499. Dropping entry: {'train/loss': 1.798270010948181, 'train/ce_loss': 0.4857421875, 'train/seg_cls_loss': 0.01397705078125, 'train/kl_loss': 0.255859375, 'train/mask_bce_loss': 0.16276112496852874, 'train/mask_dice_loss': 0.4772430181503296, 'train/mask_loss': 0.640004140138626, 'metrics/total_secs_per_batch': 6.0792763233184814, 'metrics/data_secs_per_batch': 2.887847137451172, '_timestamp': 1740979303.3026204}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 207 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979303.302844}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 208 is less than current step: 499. Dropping entry: {'train/loss': 1.5524598002433776, 'train/ce_loss': 0.398388671875, 'train/seg_cls_loss': 0.01297607421875, 'train/kl_loss': 0.2912109375, 'train/mask_bce_loss': 0.12130749970674515, 'train/mask_dice_loss': 0.4380034670233727, 'train/mask_loss': 0.5593109607696534, 'metrics/total_secs_per_batch': 5.806723356246948, 'metrics/data_secs_per_batch': 2.690866708755493, '_timestamp': 1740979309.109165}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 208 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979309.1094422}).
[2025-03-02 23:21:55,126] [INFO] [logging.py:96:log_dist] [Rank 0] step=4210, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:21:55,133] [INFO] [timer.py:215:stop] epoch=0/micro_step=42100/global_step=4210, RunningAvgSamplesPerSec=1.4833450665688817, CurrSamplesPerSec=1.6602821950569848, MemAllocated=31.61GB, MaxMemAllocated=37.23GB
Epoch: [8][210/500]	Time  6.025 ( 6.025)	Loss 0.4883 (1.6204)	CeLoss 0.4883 (0.3374)	SegCLSLoss 0.0000 (0.0140)	KLLoss 0.0000 (0.2520)	MaskLoss 0.0000 (0.6254)	MaskBCELoss 0.0000 (0.1162)	MaskDICELoss 0.0000 (0.5092)
Epoch: [8][211/500]	Time  6.166 ( 6.166)	Loss 2.0174 (1.7209)	CeLoss 0.2793 (0.3770)	SegCLSLoss 0.0126 (0.0114)	KLLoss 0.3711 (0.2543)	MaskLoss 0.8475 (0.6563)	MaskBCELoss 0.1419 (0.1339)	MaskDICELoss 0.7057 (0.5224)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 209 is less than current step: 499. Dropping entry: {'train/loss': 1.6203898429870605, 'train/ce_loss': 0.33740234375, 'train/seg_cls_loss': 0.01402587890625, 'train/kl_loss': 0.251953125, 'train/mask_bce_loss': 0.11620983127504587, 'train/mask_dice_loss': 0.509219479560852, 'train/mask_loss': 0.6254293024539948, 'metrics/total_secs_per_batch': 6.024637222290039, 'metrics/data_secs_per_batch': 2.700964426994324, '_timestamp': 1740979315.1335964}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 209 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979315.133868}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 210 is less than current step: 499. Dropping entry: {'train/loss': 1.7208673477172851, 'train/ce_loss': 0.377001953125, 'train/seg_cls_loss': 0.0114013671875, 'train/kl_loss': 0.254296875, 'train/mask_bce_loss': 0.13386247893795372, 'train/mask_dice_loss': 0.5224452137947082, 'train/mask_loss': 0.6563076972961426, 'metrics/total_secs_per_batch': 6.165999412536621, 'metrics/data_secs_per_batch': 2.767124056816101, '_timestamp': 1740979321.2998004}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 210 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979321.3000836}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 211 is less than current step: 499. Dropping entry: {'train/loss': 1.2767390131950378, 'train/ce_loss': 0.3171875, 'train/seg_cls_loss': 0.011260986328125, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.1361113093793392, 'train/mask_dice_loss': 0.32628162726759913, 'train/mask_loss': 0.4623929411172867, 'metrics/total_secs_per_batch': 5.607899904251099, 'metrics/data_secs_per_batch': 2.521688771247864, '_timestamp': 1740979326.907709}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 211 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979326.9079933}).
Epoch: [8][212/500]	Time  5.608 ( 5.608)	Loss 1.2718 (1.2767)	CeLoss 0.2559 (0.3172)	SegCLSLoss 0.0118 (0.0113)	KLLoss 0.3613 (0.2900)	MaskLoss 0.4875 (0.4624)	MaskBCELoss 0.0479 (0.1361)	MaskDICELoss 0.4396 (0.3263)
Epoch: [8][213/500]	Time  5.725 ( 5.725)	Loss 1.0234 (1.6740)	CeLoss 1.0234 (0.4386)	SegCLSLoss 0.0000 (0.0130)	KLLoss 0.0000 (0.2178)	MaskLoss 0.0000 (0.6035)	MaskBCELoss 0.0000 (0.0956)	MaskDICELoss 0.0000 (0.5079)
Epoch: [8][214/500]	Time  6.308 ( 6.308)	Loss 2.5082 (1.8567)	CeLoss 0.1611 (0.2563)	SegCLSLoss 0.0239 (0.0157)	KLLoss 0.3750 (0.3627)	MaskLoss 1.1486 (0.7782)	MaskBCELoss 0.2398 (0.1936)	MaskDICELoss 0.9088 (0.5846)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 212 is less than current step: 499. Dropping entry: {'train/loss': 1.674028491973877, 'train/ce_loss': 0.438623046875, 'train/seg_cls_loss': 0.012957763671875, 'train/kl_loss': 0.2177734375, 'train/mask_bce_loss': 0.09563185265287757, 'train/mask_dice_loss': 0.5079107224941254, 'train/mask_loss': 0.6035425722599029, 'metrics/total_secs_per_batch': 5.7245612144470215, 'metrics/data_secs_per_batch': 2.7280948638916014, '_timestamp': 1740979332.6322541}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 212 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979332.6324527}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 213 is less than current step: 499. Dropping entry: {'train/loss': 1.8566906869411468, 'train/ce_loss': 0.25634765625, 'train/seg_cls_loss': 0.015655517578125, 'train/kl_loss': 0.3626953125, 'train/mask_bce_loss': 0.19358883234672247, 'train/mask_dice_loss': 0.5845611914992332, 'train/mask_loss': 0.7781500279903412, 'metrics/total_secs_per_batch': 6.308343887329102, 'metrics/data_secs_per_batch': 2.781997561454773, '_timestamp': 1740979338.9406648}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 213 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979338.9409602}).
Epoch: [8][215/500]	Time  5.465 ( 5.465)	Loss 1.7052 (1.5297)	CeLoss 0.1797 (0.4104)	SegCLSLoss 0.0236 (0.0117)	KLLoss 0.3555 (0.2545)	MaskLoss 0.7393 (0.5441)	MaskBCELoss 0.0391 (0.1098)	MaskDICELoss 0.7002 (0.4342)
Epoch: [8][216/500]	Time  5.315 ( 5.315)	Loss 1.9936 (1.5424)	CeLoss 0.2402 (0.3017)	SegCLSLoss 0.0228 (0.0155)	KLLoss 0.3613 (0.2537)	MaskLoss 0.8523 (0.6038)	MaskBCELoss 0.2915 (0.1356)	MaskDICELoss 0.5608 (0.4681)
Epoch: [8][217/500]	Time  5.460 ( 5.460)	Loss 0.0664 (1.1549)	CeLoss 0.0664 (0.3156)	SegCLSLoss 0.0000 (0.0118)	KLLoss 0.0000 (0.2559)	MaskLoss 0.0000 (0.4039)	MaskBCELoss 0.0000 (0.0968)	MaskDICELoss 0.0000 (0.3071)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 214 is less than current step: 499. Dropping entry: {'train/loss': 1.5296642303466796, 'train/ce_loss': 0.4103515625, 'train/seg_cls_loss': 0.011663818359375, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.1098332928493619, 'train/mask_dice_loss': 0.4342468619346619, 'train/mask_loss': 0.5440801560878754, 'metrics/total_secs_per_batch': 5.464866876602173, 'metrics/data_secs_per_batch': 2.7696645259857178, '_timestamp': 1740979344.4056156}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 214 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979344.4059567}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 215 is less than current step: 499. Dropping entry: {'train/loss': 1.542385184764862, 'train/ce_loss': 0.30166015625, 'train/seg_cls_loss': 0.015521240234375, 'train/kl_loss': 0.2537109375, 'train/mask_bce_loss': 0.13561583328992127, 'train/mask_dice_loss': 0.4681451261043549, 'train/mask_loss': 0.603760951757431, 'metrics/total_secs_per_batch': 5.314562797546387, 'metrics/data_secs_per_batch': 2.4927504539489744, '_timestamp': 1740979349.7201018}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 215 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979349.7203867}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 216 is less than current step: 499. Dropping entry: {'train/loss': 1.1548879981040954, 'train/ce_loss': 0.315625, 'train/seg_cls_loss': 0.011785888671875, 'train/kl_loss': 0.255859375, 'train/mask_bce_loss': 0.09677481241524219, 'train/mask_dice_loss': 0.30713402703404424, 'train/mask_loss': 0.40390883609652517, 'metrics/total_secs_per_batch': 5.4604833126068115, 'metrics/data_secs_per_batch': 2.2895402193069456, '_timestamp': 1740979355.1805475}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 216 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979355.1808236}).
Epoch: [8][218/500]	Time  6.048 ( 6.048)	Loss 0.9955 (1.8047)	CeLoss 0.2812 (0.4604)	SegCLSLoss 0.0116 (0.0138)	KLLoss 0.3594 (0.2893)	MaskLoss 0.3366 (0.6543)	MaskBCELoss 0.0607 (0.0834)	MaskDICELoss 0.2760 (0.5709)
Epoch: [8][219/500]	Time  7.277 ( 7.277)	Loss 2.0007 (1.5760)	CeLoss 0.3418 (0.1935)	SegCLSLoss 0.0127 (0.0127)	KLLoss 0.3613 (0.2938)	MaskLoss 0.8090 (0.6735)	MaskBCELoss 0.2380 (0.1398)	MaskDICELoss 0.5710 (0.5337)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 217 is less than current step: 499. Dropping entry: {'train/loss': 1.8046918034553527, 'train/ce_loss': 0.46044921875, 'train/seg_cls_loss': 0.013824462890625, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.08340417174622416, 'train/mask_dice_loss': 0.5709436535835266, 'train/mask_loss': 0.6543478310108185, 'metrics/total_secs_per_batch': 6.048189401626587, 'metrics/data_secs_per_batch': 2.4093930959701537, '_timestamp': 1740979361.2287326}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 217 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979361.2290146}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 218 is less than current step: 499. Dropping entry: {'train/loss': 1.576045286655426, 'train/ce_loss': 0.19345703125, 'train/seg_cls_loss': 0.012713623046875, 'train/kl_loss': 0.29375, 'train/mask_bce_loss': 0.13977624289691448, 'train/mask_dice_loss': 0.5336956232786179, 'train/mask_loss': 0.673471873998642, 'metrics/total_secs_per_batch': 7.276979446411133, 'metrics/data_secs_per_batch': 3.322098803520203, '_timestamp': 1740979368.5057049}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 218 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979368.5059988}).
[2025-03-02 23:22:55,645] [INFO] [logging.py:96:log_dist] [Rank 0] step=4220, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:22:55,651] [INFO] [timer.py:215:stop] epoch=0/micro_step=42200/global_step=4220, RunningAvgSamplesPerSec=1.4837055545002298, CurrSamplesPerSec=1.3995748056887625, MemAllocated=31.47GB, MaxMemAllocated=37.23GB
Epoch: [8][220/500]	Time  7.147 ( 7.147)	Loss 1.5179 (1.5503)	CeLoss 0.1836 (0.2537)	SegCLSLoss 0.0154 (0.0132)	KLLoss 0.3613 (0.3279)	MaskLoss 0.6452 (0.6285)	MaskBCELoss 0.0559 (0.0938)	MaskDICELoss 0.5893 (0.5347)
Epoch: [8][221/500]	Time  5.327 ( 5.327)	Loss 1.5837 (1.3181)	CeLoss 0.2158 (0.4516)	SegCLSLoss 0.0215 (0.0152)	KLLoss 0.3652 (0.2533)	MaskLoss 0.6600 (0.4168)	MaskBCELoss 0.1814 (0.0830)	MaskDICELoss 0.4786 (0.3338)
Epoch: [8][222/500]	Time  5.271 ( 5.271)	Loss 1.1327 (1.2541)	CeLoss 0.1816 (0.4905)	SegCLSLoss 0.0231 (0.0088)	KLLoss 0.3613 (0.2166)	MaskLoss 0.4516 (0.3688)	MaskBCELoss 0.2748 (0.1148)	MaskDICELoss 0.1768 (0.2540)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 219 is less than current step: 499. Dropping entry: {'train/loss': 1.55027779340744, 'train/ce_loss': 0.2537109375, 'train/seg_cls_loss': 0.013189697265625, 'train/kl_loss': 0.3279296875, 'train/mask_bce_loss': 0.09380823522806167, 'train/mask_dice_loss': 0.5346509739756584, 'train/mask_loss': 0.6284592151641846, 'metrics/total_secs_per_batch': 7.146594285964966, 'metrics/data_secs_per_batch': 3.1583004236221313, '_timestamp': 1740979375.6521547}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 219 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979375.652472}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 220 is less than current step: 499. Dropping entry: {'train/loss': 1.3181257665157318, 'train/ce_loss': 0.4515625, 'train/seg_cls_loss': 0.01522216796875, 'train/kl_loss': 0.2533203125, 'train/mask_bce_loss': 0.08301317188888788, 'train/mask_dice_loss': 0.3337645441293716, 'train/mask_loss': 0.41677771508693695, 'metrics/total_secs_per_batch': 5.3265697956085205, 'metrics/data_secs_per_batch': 2.3331527709960938, '_timestamp': 1740979380.9789941}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 220 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979380.9793053}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 221 is less than current step: 499. Dropping entry: {'train/loss': 1.2541346549987793, 'train/ce_loss': 0.49052734375, 'train/seg_cls_loss': 0.0087890625, 'train/kl_loss': 0.2166015625, 'train/mask_bce_loss': 0.11480755284428597, 'train/mask_dice_loss': 0.2540078297257423, 'train/mask_loss': 0.3688153803348541, 'metrics/total_secs_per_batch': 5.271378993988037, 'metrics/data_secs_per_batch': 2.2747815370559694, '_timestamp': 1740979386.2503219}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 221 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979386.2506156}).
Epoch: [8][223/500]	Time  5.681 ( 5.681)	Loss 2.0315 (1.8072)	CeLoss 0.2148 (0.4202)	SegCLSLoss 0.0149 (0.0148)	KLLoss 0.3574 (0.2893)	MaskLoss 0.8868 (0.6754)	MaskBCELoss 0.0580 (0.1336)	MaskDICELoss 0.8288 (0.5418)
Epoch: [8][224/500]	Time  5.723 ( 5.723)	Loss 2.1835 (2.0468)	CeLoss 0.2285 (0.3343)	SegCLSLoss 0.0194 (0.0143)	KLLoss 0.3555 (0.3285)	MaskLoss 0.9550 (0.8362)	MaskBCELoss 0.0797 (0.2910)	MaskDICELoss 0.8754 (0.5452)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 222 is less than current step: 499. Dropping entry: {'train/loss': 1.8072232961654664, 'train/ce_loss': 0.42021484375, 'train/seg_cls_loss': 0.01478271484375, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.1335959923453629, 'train/mask_dice_loss': 0.5417929977178574, 'train/mask_loss': 0.6753889858722687, 'metrics/total_secs_per_batch': 5.680964469909668, 'metrics/data_secs_per_batch': 2.5156095743179323, '_timestamp': 1740979391.9312413}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 222 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979391.9314454}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 223 is less than current step: 499. Dropping entry: {'train/loss': 2.0468371391296385, 'train/ce_loss': 0.33427734375, 'train/seg_cls_loss': 0.0142822265625, 'train/kl_loss': 0.328515625, 'train/mask_bce_loss': 0.2909624338150024, 'train/mask_dice_loss': 0.5452491104602813, 'train/mask_loss': 0.8362115532159805, 'metrics/total_secs_per_batch': 5.7232725620269775, 'metrics/data_secs_per_batch': 2.57803590297699, '_timestamp': 1740979397.654713}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 223 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979397.6550627}).
Epoch: [8][225/500]	Time  6.163 ( 6.163)	Loss 0.1719 (1.8802)	CeLoss 0.1719 (0.2777)	SegCLSLoss 0.0000 (0.0128)	KLLoss 0.0000 (0.2916)	MaskLoss 0.0000 (0.7835)	MaskBCELoss 0.0000 (0.2091)	MaskDICELoss 0.0000 (0.5744)
Epoch: [8][226/500]	Time  5.028 ( 5.028)	Loss 2.1189 (1.6931)	CeLoss 0.1348 (0.5330)	SegCLSLoss 0.0298 (0.0135)	KLLoss 0.3633 (0.2561)	MaskLoss 0.9667 (0.5639)	MaskBCELoss 0.0510 (0.1303)	MaskDICELoss 0.9157 (0.4336)
Epoch: [8][227/500]	Time  4.897 ( 4.897)	Loss 2.2335 (1.8021)	CeLoss 0.1934 (0.7160)	SegCLSLoss 0.0161 (0.0109)	KLLoss 0.3691 (0.2191)	MaskLoss 0.9976 (0.5293)	MaskBCELoss 0.0788 (0.0847)	MaskDICELoss 0.9188 (0.4446)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 224 is less than current step: 499. Dropping entry: {'train/loss': 1.8802078008651733, 'train/ce_loss': 0.277734375, 'train/seg_cls_loss': 0.012786865234375, 'train/kl_loss': 0.2916015625, 'train/mask_bce_loss': 0.20907898545265197, 'train/mask_dice_loss': 0.5743842869997025, 'train/mask_loss': 0.7834632813930511, 'metrics/total_secs_per_batch': 6.162560701370239, 'metrics/data_secs_per_batch': 2.633288311958313, '_timestamp': 1740979403.8171225}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 224 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979403.8174202}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 225 is less than current step: 499. Dropping entry: {'train/loss': 1.6931311011314392, 'train/ce_loss': 0.5330078125, 'train/seg_cls_loss': 0.0135009765625, 'train/kl_loss': 0.2560546875, 'train/mask_bce_loss': 0.1303172251675278, 'train/mask_dice_loss': 0.43363115191459656, 'train/mask_loss': 0.5639483690261841, 'metrics/total_secs_per_batch': 5.027721166610718, 'metrics/data_secs_per_batch': 2.1199751138687133, '_timestamp': 1740979408.8448274}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 225 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979408.8451307}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 226 is less than current step: 499. Dropping entry: {'train/loss': 1.8021240234375, 'train/ce_loss': 0.716015625, 'train/seg_cls_loss': 0.010888671875, 'train/kl_loss': 0.219140625, 'train/mask_bce_loss': 0.0847083929926157, 'train/mask_dice_loss': 0.4446251094341278, 'train/mask_loss': 0.5293335020542145, 'metrics/total_secs_per_batch': 4.897168397903442, 'metrics/data_secs_per_batch': 2.1617499351501466, '_timestamp': 1740979413.742146}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 226 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979413.742487}).
Epoch: [8][228/500]	Time  6.182 ( 6.182)	Loss 1.6671 (1.3481)	CeLoss 0.2109 (0.2561)	SegCLSLoss 0.0143 (0.0121)	KLLoss 0.3594 (0.2531)	MaskLoss 0.7066 (0.5302)	MaskBCELoss 0.0966 (0.0701)	MaskDICELoss 0.6099 (0.4601)
Epoch: [8][229/500]	Time  7.018 ( 7.018)	Loss 1.4250 (1.8757)	CeLoss 0.2383 (0.2244)	SegCLSLoss 0.0110 (0.0155)	KLLoss 0.3555 (0.3645)	MaskLoss 0.5729 (0.8035)	MaskBCELoss 0.0693 (0.1397)	MaskDICELoss 0.5036 (0.6639)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 227 is less than current step: 499. Dropping entry: {'train/loss': 1.3480880379676818, 'train/ce_loss': 0.256103515625, 'train/seg_cls_loss': 0.012078857421875, 'train/kl_loss': 0.253125, 'train/mask_bce_loss': 0.07011451637372375, 'train/mask_dice_loss': 0.46010625809431077, 'train/mask_loss': 0.5302207812666893, 'metrics/total_secs_per_batch': 6.181698560714722, 'metrics/data_secs_per_batch': 2.925693392753601, '_timestamp': 1740979419.92371}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 227 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979419.9240043}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 228 is less than current step: 499. Dropping entry: {'train/loss': 1.875675129890442, 'train/ce_loss': 0.2244140625, 'train/seg_cls_loss': 0.015509033203125, 'train/kl_loss': 0.364453125, 'train/mask_bce_loss': 0.13965786956250667, 'train/mask_dice_loss': 0.6638535112142563, 'train/mask_loss': 0.8035113722085953, 'metrics/total_secs_per_batch': 7.017831563949585, 'metrics/data_secs_per_batch': 3.1555705785751345, '_timestamp': 1740979426.941488}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 228 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979426.9417768}).
[2025-03-02 23:23:53,522] [INFO] [logging.py:96:log_dist] [Rank 0] step=4230, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:23:53,527] [INFO] [timer.py:215:stop] epoch=0/micro_step=42300/global_step=4230, RunningAvgSamplesPerSec=1.4842021988874738, CurrSamplesPerSec=1.5184729628042721, MemAllocated=31.26GB, MaxMemAllocated=37.23GB
Epoch: [8][230/500]	Time  6.587 ( 6.587)	Loss 2.6658 (1.3749)	CeLoss 0.2070 (0.2936)	SegCLSLoss 0.0253 (0.0104)	KLLoss 0.3711 (0.2920)	MaskLoss 1.2040 (0.5233)	MaskBCELoss 0.2907 (0.1183)	MaskDICELoss 0.9133 (0.4050)
Epoch: [8][231/500]	Time  5.407 ( 5.407)	Loss 2.7100 (1.8112)	CeLoss 0.2637 (0.4856)	SegCLSLoss 0.0098 (0.0137)	KLLoss 0.3613 (0.2918)	MaskLoss 1.2027 (0.6447)	MaskBCELoss 0.6089 (0.1135)	MaskDICELoss 0.5938 (0.5312)
Epoch: [8][232/500]	Time  6.108 ( 6.108)	Loss 2.0326 (2.0956)	CeLoss 0.1855 (0.3947)	SegCLSLoss 0.0210 (0.0131)	KLLoss 0.3555 (0.2904)	MaskLoss 0.9001 (0.8325)	MaskBCELoss 0.1056 (0.1878)	MaskDICELoss 0.7945 (0.6447)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 229 is less than current step: 499. Dropping entry: {'train/loss': 1.3748875379562377, 'train/ce_loss': 0.2935546875, 'train/seg_cls_loss': 0.01041259765625, 'train/kl_loss': 0.2919921875, 'train/mask_bce_loss': 0.11829681545495987, 'train/mask_dice_loss': 0.404986796528101, 'train/mask_loss': 0.5232836008071899, 'metrics/total_secs_per_batch': 6.587136268615723, 'metrics/data_secs_per_batch': 2.675619912147522, '_timestamp': 1740979433.5285096}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 229 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979433.5288236}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 230 is less than current step: 499. Dropping entry: {'train/loss': 1.811189067363739, 'train/ce_loss': 0.48564453125, 'train/seg_cls_loss': 0.013714599609375, 'train/kl_loss': 0.291796875, 'train/mask_bce_loss': 0.11349934823811055, 'train/mask_dice_loss': 0.5312065124511719, 'train/mask_loss': 0.6447058588266372, 'metrics/total_secs_per_batch': 5.4071269035339355, 'metrics/data_secs_per_batch': 2.517381191253662, '_timestamp': 1740979438.9359577}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 230 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979438.9363718}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 231 is less than current step: 499. Dropping entry: {'train/loss': 2.0955840349197388, 'train/ce_loss': 0.3947265625, 'train/seg_cls_loss': 0.0130859375, 'train/kl_loss': 0.2904296875, 'train/mask_bce_loss': 0.18779940567910672, 'train/mask_dice_loss': 0.644660571217537, 'train/mask_loss': 0.8324599742889405, 'metrics/total_secs_per_batch': 6.107956647872925, 'metrics/data_secs_per_batch': 2.8625742197036743, '_timestamp': 1740979445.043754}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 231 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979445.0440876}).
Epoch: [8][233/500]	Time  5.804 ( 5.804)	Loss 2.5516 (1.7669)	CeLoss 0.1245 (0.4068)	SegCLSLoss 0.0281 (0.0163)	KLLoss 0.3848 (0.2930)	MaskLoss 1.1869 (0.6612)	MaskBCELoss 0.2666 (0.1185)	MaskDICELoss 0.9203 (0.5428)
Epoch: [8][234/500]	Time  6.401 ( 6.401)	Loss 1.4778 (1.9758)	CeLoss 0.2949 (0.3660)	SegCLSLoss 0.0096 (0.0155)	KLLoss 0.3652 (0.3271)	MaskLoss 0.5709 (0.7847)	MaskBCELoss 0.1985 (0.1490)	MaskDICELoss 0.3725 (0.6357)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 232 is less than current step: 499. Dropping entry: {'train/loss': 1.7668969273567199, 'train/ce_loss': 0.406787109375, 'train/seg_cls_loss': 0.016314697265625, 'train/kl_loss': 0.29296875, 'train/mask_bce_loss': 0.11845672577619552, 'train/mask_dice_loss': 0.5427749395370484, 'train/mask_loss': 0.6612316727638244, 'metrics/total_secs_per_batch': 5.804480791091919, 'metrics/data_secs_per_batch': 2.610047698020935, '_timestamp': 1740979450.8482268}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 232 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979450.8484995}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 233 is less than current step: 499. Dropping entry: {'train/loss': 1.9758338689804078, 'train/ce_loss': 0.366015625, 'train/seg_cls_loss': 0.015484619140625, 'train/kl_loss': 0.3271484375, 'train/mask_bce_loss': 0.14904107013717294, 'train/mask_dice_loss': 0.6357020080089569, 'train/mask_loss': 0.7847430825233459, 'metrics/total_secs_per_batch': 6.400782823562622, 'metrics/data_secs_per_batch': 2.7383804321289062, '_timestamp': 1740979457.249047}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 233 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979457.249339}).
Epoch: [8][235/500]	Time  6.090 ( 6.090)	Loss 0.4082 (1.3707)	CeLoss 0.4082 (0.4624)	SegCLSLoss 0.0000 (0.0103)	KLLoss 0.0000 (0.2152)	MaskLoss 0.0000 (0.4408)	MaskBCELoss 0.0000 (0.0445)	MaskDICELoss 0.0000 (0.3963)
Epoch: [8][236/500]	Time  5.157 ( 5.157)	Loss 1.3906 (1.3233)	CeLoss 1.3906 (0.5361)	SegCLSLoss 0.0000 (0.0076)	KLLoss 0.0000 (0.1457)	MaskLoss 0.0000 (0.3843)	MaskBCELoss 0.0000 (0.0623)	MaskDICELoss 0.0000 (0.3219)
Epoch: [8][237/500]	Time  5.596 ( 5.596)	Loss 1.3171 (1.5065)	CeLoss 0.2480 (0.4308)	SegCLSLoss 0.0167 (0.0126)	KLLoss 0.3574 (0.2537)	MaskLoss 0.5130 (0.5222)	MaskBCELoss 0.1614 (0.1266)	MaskDICELoss 0.3516 (0.3956)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 234 is less than current step: 499. Dropping entry: {'train/loss': 1.3706613600254058, 'train/ce_loss': 0.46240234375, 'train/seg_cls_loss': 0.010260009765625, 'train/kl_loss': 0.215234375, 'train/mask_bce_loss': 0.04446678925305605, 'train/mask_dice_loss': 0.39628381133079527, 'train/mask_loss': 0.44075060188770293, 'metrics/total_secs_per_batch': 6.090017795562744, 'metrics/data_secs_per_batch': 2.7902291297912596, '_timestamp': 1740979463.3390198}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 234 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979463.3393135}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 235 is less than current step: 499. Dropping entry: {'train/loss': 1.3232535243034362, 'train/ce_loss': 0.5361328125, 'train/seg_cls_loss': 0.007623291015625, 'train/kl_loss': 0.145703125, 'train/mask_bce_loss': 0.06233878098428249, 'train/mask_dice_loss': 0.3219442248344421, 'train/mask_loss': 0.38428300619125366, 'metrics/total_secs_per_batch': 5.156952142715454, 'metrics/data_secs_per_batch': 2.4046634912490843, '_timestamp': 1740979468.4961748}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 235 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979468.4965203}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 236 is less than current step: 499. Dropping entry: {'train/loss': 1.506511890888214, 'train/ce_loss': 0.43076171875, 'train/seg_cls_loss': 0.0126220703125, 'train/kl_loss': 0.2537109375, 'train/mask_bce_loss': 0.12659018896520138, 'train/mask_dice_loss': 0.3955622285604477, 'train/mask_loss': 0.5221524208784103, 'metrics/total_secs_per_batch': 5.59617280960083, 'metrics/data_secs_per_batch': 2.2731604099273683, '_timestamp': 1740979474.092176}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 236 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979474.0924618}).
Epoch: [8][238/500]	Time  6.594 ( 6.594)	Loss 0.7772 (1.7552)	CeLoss 0.2334 (0.2405)	SegCLSLoss 0.0110 (0.0150)	KLLoss 0.3613 (0.3629)	MaskLoss 0.2509 (0.7353)	MaskBCELoss 0.0661 (0.1876)	MaskDICELoss 0.1848 (0.5477)
Epoch: [8][239/500]	Time  5.699 ( 5.699)	Loss 0.9903 (1.4860)	CeLoss 0.2500 (0.4673)	SegCLSLoss 0.0178 (0.0140)	KLLoss 0.3594 (0.2555)	MaskLoss 0.3477 (0.4931)	MaskBCELoss 0.0468 (0.1148)	MaskDICELoss 0.3009 (0.3783)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 237 is less than current step: 499. Dropping entry: {'train/loss': 1.755165410041809, 'train/ce_loss': 0.24052734375, 'train/seg_cls_loss': 0.0149658203125, 'train/kl_loss': 0.362890625, 'train/mask_bce_loss': 0.18757408987730742, 'train/mask_dice_loss': 0.547723463177681, 'train/mask_loss': 0.7352975487709046, 'metrics/total_secs_per_batch': 6.594091892242432, 'metrics/data_secs_per_batch': 2.898259902000427, '_timestamp': 1740979480.686409}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 237 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979480.6867652}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 238 is less than current step: 499. Dropping entry: {'train/loss': 1.486049234867096, 'train/ce_loss': 0.46728515625, 'train/seg_cls_loss': 0.01402587890625, 'train/kl_loss': 0.25546875, 'train/mask_bce_loss': 0.11480599455535412, 'train/mask_dice_loss': 0.3782674580812454, 'train/mask_loss': 0.493073445558548, 'metrics/total_secs_per_batch': 5.698807239532471, 'metrics/data_secs_per_batch': 2.6652842283248903, '_timestamp': 1740979486.3850522}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 238 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979486.3853252}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 239 is less than current step: 499. Dropping entry: {'train/loss': 1.7086939692497254, 'train/ce_loss': 0.77841796875, 'train/seg_cls_loss': 0.0091064453125, 'train/kl_loss': 0.1814453125, 'train/mask_bce_loss': 0.11036503724753857, 'train/mask_dice_loss': 0.3433960109949112, 'train/mask_loss': 0.45376105308532716, 'metrics/total_secs_per_batch': 4.810574293136597, 'metrics/data_secs_per_batch': 1.9214749336242676, '_timestamp': 1740979491.1954532}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 239 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979491.195738}).
[2025-03-02 23:24:51,189] [INFO] [logging.py:96:log_dist] [Rank 0] step=4240, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:24:51,194] [INFO] [timer.py:215:stop] epoch=0/micro_step=42400/global_step=4240, RunningAvgSamplesPerSec=1.4847077199813083, CurrSamplesPerSec=2.0794367583960347, MemAllocated=30.7GB, MaxMemAllocated=37.23GB
Epoch: [8][240/500]	Time  4.811 ( 4.811)	Loss 1.4453 (1.7087)	CeLoss 1.4453 (0.7784)	SegCLSLoss 0.0000 (0.0091)	KLLoss 0.0000 (0.1814)	MaskLoss 0.0000 (0.4538)	MaskBCELoss 0.0000 (0.1104)	MaskDICELoss 0.0000 (0.3434)
Epoch: [8][241/500]	Time  7.489 ( 7.489)	Loss 1.6577 (1.9665)	CeLoss 0.3672 (0.2381)	SegCLSLoss 0.0123 (0.0176)	KLLoss 0.3594 (0.3646)	MaskLoss 0.6247 (0.8415)	MaskBCELoss 0.2896 (0.1487)	MaskDICELoss 0.3351 (0.6928)
Epoch: [8][242/500]	Time  5.825 ( 5.825)	Loss 2.3279 (1.5087)	CeLoss 0.1992 (0.3631)	SegCLSLoss 0.0214 (0.0146)	KLLoss 0.3594 (0.2553)	MaskLoss 1.0409 (0.5564)	MaskBCELoss 0.0786 (0.1022)	MaskDICELoss 0.9623 (0.4541)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 240 is less than current step: 499. Dropping entry: {'train/loss': 1.9665117383003234, 'train/ce_loss': 0.2380859375, 'train/seg_cls_loss': 0.017626953125, 'train/kl_loss': 0.3646484375, 'train/mask_bce_loss': 0.14870145544409752, 'train/mask_dice_loss': 0.6927575290203094, 'train/mask_loss': 0.8414589822292328, 'metrics/total_secs_per_batch': 7.489423990249634, 'metrics/data_secs_per_batch': 3.3770013809204102, '_timestamp': 1740979498.6850684}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 240 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979498.68534}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 241 is less than current step: 499. Dropping entry: {'train/loss': 1.508716595172882, 'train/ce_loss': 0.3630859375, 'train/seg_cls_loss': 0.01463623046875, 'train/kl_loss': 0.2552734375, 'train/mask_bce_loss': 0.10224034115672112, 'train/mask_dice_loss': 0.4541199058294296, 'train/mask_loss': 0.556360250711441, 'metrics/total_secs_per_batch': 5.82519793510437, 'metrics/data_secs_per_batch': 2.7317075967788695, '_timestamp': 1740979504.5102541}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 241 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979504.5105298}).
Epoch: [8][243/500]	Time  4.941 ( 4.941)	Loss 0.8125 (1.4653)	CeLoss 0.8125 (0.3928)	SegCLSLoss 0.0000 (0.0136)	KLLoss 0.0000 (0.2891)	MaskLoss 0.0000 (0.5183)	MaskBCELoss 0.0000 (0.1486)	MaskDICELoss 0.0000 (0.3697)
Epoch: [8][244/500]	Time  6.201 ( 6.201)	Loss 1.2521 (1.6948)	CeLoss 0.2539 (0.5324)	SegCLSLoss 0.0138 (0.0104)	KLLoss 0.3633 (0.2523)	MaskLoss 0.4766 (0.5659)	MaskBCELoss 0.0347 (0.0848)	MaskDICELoss 0.4419 (0.4811)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 242 is less than current step: 499. Dropping entry: {'train/loss': 1.4653123319149017, 'train/ce_loss': 0.392822265625, 'train/seg_cls_loss': 0.0135986328125, 'train/kl_loss': 0.2890625, 'train/mask_bce_loss': 0.14862810969352722, 'train/mask_dice_loss': 0.36972139924764635, 'train/mask_loss': 0.5183495134115219, 'metrics/total_secs_per_batch': 4.94110631942749, 'metrics/data_secs_per_batch': 2.451288604736328, '_timestamp': 1740979509.4513683}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 242 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979509.4516487}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 243 is less than current step: 499. Dropping entry: {'train/loss': 1.6947674870491027, 'train/ce_loss': 0.532421875, 'train/seg_cls_loss': 0.01044921875, 'train/kl_loss': 0.25234375, 'train/mask_bce_loss': 0.08476624535396696, 'train/mask_dice_loss': 0.48112336099147796, 'train/mask_loss': 0.5658896058797837, 'metrics/total_secs_per_batch': 6.201392650604248, 'metrics/data_secs_per_batch': 2.9972805261611937, '_timestamp': 1740979515.6529853}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 243 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979515.653413}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 244 is less than current step: 499. Dropping entry: {'train/loss': 1.7728715181350707, 'train/ce_loss': 0.32109375, 'train/seg_cls_loss': 0.0192138671875, 'train/kl_loss': 0.2927734375, 'train/mask_bce_loss': 0.13058597296476365, 'train/mask_dice_loss': 0.5758204877376556, 'train/mask_loss': 0.7064064741134644, 'metrics/total_secs_per_batch': 5.509573698043823, 'metrics/data_secs_per_batch': 2.5714917898178102, '_timestamp': 1740979521.162375}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 244 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979521.1626527}).
Epoch: [8][245/500]	Time  5.510 ( 5.510)	Loss 0.3789 (1.7729)	CeLoss 0.3789 (0.3211)	SegCLSLoss 0.0000 (0.0192)	KLLoss 0.0000 (0.2928)	MaskLoss 0.0000 (0.7064)	MaskBCELoss 0.0000 (0.1306)	MaskDICELoss 0.0000 (0.5758)
Epoch: [8][246/500]	Time  6.224 ( 6.224)	Loss 1.0945 (1.8595)	CeLoss 0.2334 (0.3258)	SegCLSLoss 0.0193 (0.0179)	KLLoss 0.3633 (0.3289)	MaskLoss 0.4076 (0.7459)	MaskBCELoss 0.1024 (0.1476)	MaskDICELoss 0.3052 (0.5982)
Epoch: [8][247/500]	Time  5.866 ( 5.866)	Loss 1.1980 (1.5892)	CeLoss 0.2812 (0.2931)	SegCLSLoss 0.0120 (0.0131)	KLLoss 0.3652 (0.3293)	MaskLoss 0.4369 (0.6283)	MaskBCELoss 0.0598 (0.2285)	MaskDICELoss 0.3770 (0.3998)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 245 is less than current step: 499. Dropping entry: {'train/loss': 1.8594871163368225, 'train/ce_loss': 0.32578125, 'train/seg_cls_loss': 0.017864990234375, 'train/kl_loss': 0.32890625, 'train/mask_bce_loss': 0.14764537252485752, 'train/mask_dice_loss': 0.5982114553451539, 'train/mask_loss': 0.7458568334579467, 'metrics/total_secs_per_batch': 6.224299907684326, 'metrics/data_secs_per_batch': 2.812870955467224, '_timestamp': 1740979527.3866937}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 245 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979527.3869863}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 246 is less than current step: 499. Dropping entry: {'train/loss': 1.589239662885666, 'train/ce_loss': 0.29306640625, 'train/seg_cls_loss': 0.01314697265625, 'train/kl_loss': 0.329296875, 'train/mask_bce_loss': 0.22845828868448734, 'train/mask_dice_loss': 0.3998041279613972, 'train/mask_loss': 0.628262422978878, 'metrics/total_secs_per_batch': 5.866361856460571, 'metrics/data_secs_per_batch': 2.6573856830596925, '_timestamp': 1740979533.253021}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 246 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979533.2533095}).
Epoch: [8][248/500]	Time  5.971 ( 5.971)	Loss 1.1016 (1.8433)	CeLoss 1.1016 (0.4338)	SegCLSLoss 0.0000 (0.0134)	KLLoss 0.0000 (0.2879)	MaskLoss 0.0000 (0.6870)	MaskBCELoss 0.0000 (0.1427)	MaskDICELoss 0.0000 (0.5443)
Epoch: [8][249/500]	Time  5.831 ( 5.831)	Loss 0.0811 (1.1425)	CeLoss 0.0811 (0.5066)	SegCLSLoss 0.0000 (0.0077)	KLLoss 0.0000 (0.1814)	MaskLoss 0.0000 (0.3069)	MaskBCELoss 0.0000 (0.0394)	MaskDICELoss 0.0000 (0.2674)
[2025-03-02 23:25:50,887] [INFO] [logging.py:96:log_dist] [Rank 0] step=4250, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:25:50,894] [INFO] [timer.py:215:stop] epoch=0/micro_step=42500/global_step=4250, RunningAvgSamplesPerSec=1.4851056375319431, CurrSamplesPerSec=1.7128514991060657, MemAllocated=30.64GB, MaxMemAllocated=37.23GB
Epoch: [8][250/500]	Time  5.840 ( 5.840)	Loss 1.8588 (1.8742)	CeLoss 0.8750 (0.2972)	SegCLSLoss 0.0088 (0.0159)	KLLoss 0.3574 (0.3643)	MaskLoss 0.4723 (0.7662)	MaskBCELoss 0.1979 (0.0998)	MaskDICELoss 0.2744 (0.6664)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 247 is less than current step: 499. Dropping entry: {'train/loss': 1.84331756234169, 'train/ce_loss': 0.4337890625, 'train/seg_cls_loss': 0.01337890625, 'train/kl_loss': 0.287890625, 'train/mask_bce_loss': 0.1427360052242875, 'train/mask_dice_loss': 0.5443036168813705, 'train/mask_loss': 0.6870396286249161, 'metrics/total_secs_per_batch': 5.970835208892822, 'metrics/data_secs_per_batch': 2.8005435943603514, '_timestamp': 1740979539.2238605}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 247 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979539.2240584}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 248 is less than current step: 499. Dropping entry: {'train/loss': 1.142470943927765, 'train/ce_loss': 0.506640625, 'train/seg_cls_loss': 0.00765380859375, 'train/kl_loss': 0.1814453125, 'train/mask_bce_loss': 0.039432155713438985, 'train/mask_dice_loss': 0.26744784861803056, 'train/mask_loss': 0.3068800002336502, 'metrics/total_secs_per_batch': 5.831352710723877, 'metrics/data_secs_per_batch': 2.6604159832000733, '_timestamp': 1740979545.0552294}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 248 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979545.0555074}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 249 is less than current step: 499. Dropping entry: {'train/loss': 1.8742143213748932, 'train/ce_loss': 0.29716796875, 'train/seg_cls_loss': 0.015948486328125, 'train/kl_loss': 0.3642578125, 'train/mask_bce_loss': 0.09982628803700208, 'train/mask_dice_loss': 0.6663824371993542, 'train/mask_loss': 0.766208729147911, 'metrics/total_secs_per_batch': 5.839822053909302, 'metrics/data_secs_per_batch': 2.791813516616821, '_timestamp': 1740979550.894837}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 249 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979550.8951075}).
Epoch: [8][251/500]	Time  4.802 ( 4.802)	Loss 1.6816 (1.4776)	CeLoss 0.1816 (0.6109)	SegCLSLoss 0.0209 (0.0102)	KLLoss 0.3516 (0.1824)	MaskLoss 0.7270 (0.4217)	MaskBCELoss 0.0630 (0.0612)	MaskDICELoss 0.6640 (0.3605)
Epoch: [8][252/500]	Time  6.949 ( 6.949)	Loss 1.6941 (1.6507)	CeLoss 0.2061 (0.4118)	SegCLSLoss 0.0142 (0.0117)	KLLoss 0.3613 (0.2547)	MaskLoss 0.7225 (0.6038)	MaskBCELoss 0.0307 (0.1327)	MaskDICELoss 0.6919 (0.4711)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 250 is less than current step: 499. Dropping entry: {'train/loss': 1.4776337146759033, 'train/ce_loss': 0.610888671875, 'train/seg_cls_loss': 0.010205078125, 'train/kl_loss': 0.182421875, 'train/mask_bce_loss': 0.06120747160166502, 'train/mask_dice_loss': 0.3605195343494415, 'train/mask_loss': 0.42172701358795167, 'metrics/total_secs_per_batch': 4.802064895629883, 'metrics/data_secs_per_batch': 1.9777860641479492, '_timestamp': 1740979555.697216}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 250 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979555.697481}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 251 is less than current step: 499. Dropping entry: {'train/loss': 1.65070298910141, 'train/ce_loss': 0.411767578125, 'train/seg_cls_loss': 0.011712646484375, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.13272941280156375, 'train/mask_dice_loss': 0.4710644595324993, 'train/mask_loss': 0.6037938714027404, 'metrics/total_secs_per_batch': 6.948791027069092, 'metrics/data_secs_per_batch': 2.8745126724243164, '_timestamp': 1740979562.6460497}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 251 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979562.646368}).
Epoch: [8][253/500]	Time  6.037 ( 6.037)	Loss 2.2204 (1.7556)	CeLoss 0.2266 (0.4417)	SegCLSLoss 0.0159 (0.0149)	KLLoss 0.3789 (0.2906)	MaskLoss 0.9745 (0.6387)	MaskBCELoss 0.1743 (0.1424)	MaskDICELoss 0.8002 (0.4962)
Epoch: [8][254/500]	Time  6.616 ( 6.616)	Loss 1.3932 (1.8013)	CeLoss 0.2129 (0.2365)	SegCLSLoss 0.0136 (0.0155)	KLLoss 0.3652 (0.3617)	MaskLoss 0.5687 (0.7604)	MaskBCELoss 0.1260 (0.1508)	MaskDICELoss 0.4426 (0.6096)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 252 is less than current step: 499. Dropping entry: {'train/loss': 1.755584865808487, 'train/ce_loss': 0.44169921875, 'train/seg_cls_loss': 0.014910888671875, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.14243481457233428, 'train/mask_dice_loss': 0.4962462991476059, 'train/mask_loss': 0.6386811226606369, 'metrics/total_secs_per_batch': 6.037495374679565, 'metrics/data_secs_per_batch': 2.813553214073181, '_timestamp': 1740979568.683382}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 252 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979568.6835718}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 253 is less than current step: 499. Dropping entry: {'train/loss': 1.8013481199741364, 'train/ce_loss': 0.2365234375, 'train/seg_cls_loss': 0.01552734375, 'train/kl_loss': 0.36171875, 'train/mask_bce_loss': 0.15082743354141712, 'train/mask_dice_loss': 0.6095634222030639, 'train/mask_loss': 0.7603908628225327, 'metrics/total_secs_per_batch': 6.615993976593018, 'metrics/data_secs_per_batch': 2.9362587213516234, '_timestamp': 1740979575.2996361}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 253 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979575.3000064}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 254 is less than current step: 499. Dropping entry: {'train/loss': 1.2107468724250794, 'train/ce_loss': 0.5207763671875, 'train/seg_cls_loss': 0.009136962890625, 'train/kl_loss': 0.1470703125, 'train/mask_bce_loss': 0.0556377137079835, 'train/mask_dice_loss': 0.2797772288322449, 'train/mask_loss': 0.33541494607925415, 'metrics/total_secs_per_batch': 6.1965858936309814, 'metrics/data_secs_per_batch': 2.567513346672058, '_timestamp': 1740979581.4959888}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 254 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979581.4962773}).
Epoch: [8][255/500]	Time  6.197 ( 6.197)	Loss 1.8650 (1.2107)	CeLoss 0.1904 (0.5208)	SegCLSLoss 0.0262 (0.0091)	KLLoss 0.3711 (0.1471)	MaskLoss 0.8124 (0.3354)	MaskBCELoss 0.0374 (0.0556)	MaskDICELoss 0.7750 (0.2798)
Epoch: [8][256/500]	Time  5.863 ( 5.863)	Loss 1.9813 (1.6497)	CeLoss 0.2373 (0.3650)	SegCLSLoss 0.0117 (0.0125)	KLLoss 0.3633 (0.2516)	MaskLoss 0.8510 (0.6265)	MaskBCELoss 0.0915 (0.0644)	MaskDICELoss 0.7595 (0.5622)
Epoch: [8][257/500]	Time  6.019 ( 6.019)	Loss 1.5201 (1.9075)	CeLoss 0.2041 (0.4721)	SegCLSLoss 0.0144 (0.0147)	KLLoss 0.3652 (0.2928)	MaskLoss 0.6360 (0.6996)	MaskBCELoss 0.0299 (0.0875)	MaskDICELoss 0.6061 (0.6121)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 255 is less than current step: 499. Dropping entry: {'train/loss': 1.6497013449668885, 'train/ce_loss': 0.364990234375, 'train/seg_cls_loss': 0.01253662109375, 'train/kl_loss': 0.2515625, 'train/mask_bce_loss': 0.06437921011820436, 'train/mask_dice_loss': 0.5621560335159301, 'train/mask_loss': 0.6265352427959442, 'metrics/total_secs_per_batch': 5.863117456436157, 'metrics/data_secs_per_batch': 2.4565454721450806, '_timestamp': 1740979587.3590987}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 255 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979587.3592951}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 256 is less than current step: 499. Dropping entry: {'train/loss': 1.9075404167175294, 'train/ce_loss': 0.4720703125, 'train/seg_cls_loss': 0.0146728515625, 'train/kl_loss': 0.2927734375, 'train/mask_bce_loss': 0.08745582746341825, 'train/mask_dice_loss': 0.6121151626110077, 'train/mask_loss': 0.6995709896087646, 'metrics/total_secs_per_batch': 6.0188891887664795, 'metrics/data_secs_per_batch': 2.532692003250122, '_timestamp': 1740979593.3782132}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 256 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979593.3785672}).
Epoch: [8][258/500]	Time  5.874 ( 5.874)	Loss 2.4910 (2.1645)	CeLoss 0.2246 (0.3302)	SegCLSLoss 0.0160 (0.0144)	KLLoss 0.3633 (0.3293)	MaskLoss 1.1108 (0.8971)	MaskBCELoss 0.2580 (0.3015)	MaskDICELoss 0.8527 (0.5957)
Epoch: [8][259/500]	Time  6.342 ( 6.342)	Loss 1.6673 (1.9332)	CeLoss 0.3047 (0.3421)	SegCLSLoss 0.0120 (0.0147)	KLLoss 0.3633 (0.3266)	MaskLoss 0.6598 (0.7755)	MaskBCELoss 0.1692 (0.1745)	MaskDICELoss 0.4906 (0.6010)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 257 is less than current step: 499. Dropping entry: {'train/loss': 2.1645361185073853, 'train/ce_loss': 0.33017578125, 'train/seg_cls_loss': 0.014373779296875, 'train/kl_loss': 0.329296875, 'train/mask_bce_loss': 0.3014578379690647, 'train/mask_dice_loss': 0.5956539630889892, 'train/mask_loss': 0.8971118062734604, 'metrics/total_secs_per_batch': 5.874429702758789, 'metrics/data_secs_per_batch': 2.4383814096450807, '_timestamp': 1740979599.2524164}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 257 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979599.2526171}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 258 is less than current step: 499. Dropping entry: {'train/loss': 1.9331547737121582, 'train/ce_loss': 0.34208984375, 'train/seg_cls_loss': 0.0146728515625, 'train/kl_loss': 0.3265625, 'train/mask_bce_loss': 0.17447839360684156, 'train/mask_dice_loss': 0.6009857058525085, 'train/mask_loss': 0.7754640936851501, 'metrics/total_secs_per_batch': 6.341609716415405, 'metrics/data_secs_per_batch': 2.7905550718307497, '_timestamp': 1740979605.594055}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 258 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979605.5943322}).
[2025-03-02 23:26:52,402] [INFO] [logging.py:96:log_dist] [Rank 0] step=4260, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:26:52,408] [INFO] [timer.py:215:stop] epoch=0/micro_step=42600/global_step=4260, RunningAvgSamplesPerSec=1.485407874071249, CurrSamplesPerSec=1.4676464450328912, MemAllocated=31.16GB, MaxMemAllocated=37.23GB
Epoch: [8][260/500]	Time  6.815 ( 6.815)	Loss 2.5753 (2.1423)	CeLoss 0.1270 (0.2024)	SegCLSLoss 0.0388 (0.0202)	KLLoss 0.3750 (0.3660)	MaskLoss 1.1958 (0.9466)	MaskBCELoss 0.4791 (0.2647)	MaskDICELoss 0.7167 (0.6820)
Epoch: [8][261/500]	Time  5.279 ( 5.279)	Loss 1.4688 (1.6398)	CeLoss 1.4688 (0.6088)	SegCLSLoss 0.0000 (0.0093)	KLLoss 0.0000 (0.2240)	MaskLoss 0.0000 (0.5020)	MaskBCELoss 0.0000 (0.1468)	MaskDICELoss 0.0000 (0.3552)
Epoch: [8][262/500]	Time  5.956 ( 5.956)	Loss 0.8315 (1.7502)	CeLoss 0.2285 (0.2826)	SegCLSLoss 0.0250 (0.0162)	KLLoss 0.3613 (0.3258)	MaskLoss 0.2771 (0.7137)	MaskBCELoss 0.0040 (0.1154)	MaskDICELoss 0.2731 (0.5982)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 259 is less than current step: 499. Dropping entry: {'train/loss': 2.1422562837600707, 'train/ce_loss': 0.20244140625, 'train/seg_cls_loss': 0.02022705078125, 'train/kl_loss': 0.366015625, 'train/mask_bce_loss': 0.2646623499691486, 'train/mask_dice_loss': 0.68195406422019, 'train/mask_loss': 0.9466164231300354, 'metrics/total_secs_per_batch': 6.815206050872803, 'metrics/data_secs_per_batch': 3.099437379837036, '_timestamp': 1740979612.4091365}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 259 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979612.4094462}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 260 is less than current step: 499. Dropping entry: {'train/loss': 1.6397558569908142, 'train/ce_loss': 0.6087890625, 'train/seg_cls_loss': 0.009344482421875, 'train/kl_loss': 0.2240234375, 'train/mask_bce_loss': 0.14676289176568388, 'train/mask_dice_loss': 0.35519510954618455, 'train/mask_loss': 0.501958005130291, 'metrics/total_secs_per_batch': 5.279332876205444, 'metrics/data_secs_per_batch': 2.3105899810791017, '_timestamp': 1740979617.6885724}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 260 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979617.6887717}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 261 is less than current step: 499. Dropping entry: {'train/loss': 1.7501518726348877, 'train/ce_loss': 0.282568359375, 'train/seg_cls_loss': 0.016180419921875, 'train/kl_loss': 0.32578125, 'train/mask_bce_loss': 0.11541462750174106, 'train/mask_dice_loss': 0.5982355356216431, 'train/mask_loss': 0.7136501610279083, 'metrics/total_secs_per_batch': 5.955695629119873, 'metrics/data_secs_per_batch': 2.5968575716018676, '_timestamp': 1740979623.6442955}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 261 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979623.6446102}).
Epoch: [8][263/500]	Time  6.194 ( 6.194)	Loss 1.8131 (1.6926)	CeLoss 0.2305 (0.5416)	SegCLSLoss 0.0161 (0.0138)	KLLoss 0.3574 (0.2547)	MaskLoss 0.7699 (0.5593)	MaskBCELoss 0.0291 (0.0785)	MaskDICELoss 0.7407 (0.4808)
Epoch: [8][264/500]	Time  6.842 ( 6.842)	Loss 0.3281 (1.5013)	CeLoss 0.3281 (0.2407)	SegCLSLoss 0.0000 (0.0139)	KLLoss 0.0000 (0.3266)	MaskLoss 0.0000 (0.6106)	MaskBCELoss 0.0000 (0.1147)	MaskDICELoss 0.0000 (0.4959)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 262 is less than current step: 499. Dropping entry: {'train/loss': 1.692607843875885, 'train/ce_loss': 0.5416015625, 'train/seg_cls_loss': 0.013836669921875, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.07850478328764439, 'train/mask_dice_loss': 0.48083623945713044, 'train/mask_loss': 0.5593410313129425, 'metrics/total_secs_per_batch': 6.19360089302063, 'metrics/data_secs_per_batch': 2.78918251991272, '_timestamp': 1740979629.8380334}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 262 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979629.838478}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 263 is less than current step: 499. Dropping entry: {'train/loss': 1.5013111710548401, 'train/ce_loss': 0.24072265625, 'train/seg_cls_loss': 0.013934326171875, 'train/kl_loss': 0.3265625, 'train/mask_bce_loss': 0.11467016153037549, 'train/mask_dice_loss': 0.4958975434303284, 'train/mask_loss': 0.61056769490242, 'metrics/total_secs_per_batch': 6.84150505065918, 'metrics/data_secs_per_batch': 3.0351401805877685, '_timestamp': 1740979636.6794214}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 263 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979636.679717}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 264 is less than current step: 499. Dropping entry: {'train/loss': 1.4748288810253143, 'train/ce_loss': 0.31875, 'train/seg_cls_loss': 0.011822509765625, 'train/kl_loss': 0.2884765625, 'train/mask_bce_loss': 0.0704916825518012, 'train/mask_dice_loss': 0.4901649370789528, 'train/mask_loss': 0.5606566160917282, 'metrics/total_secs_per_batch': 5.503355264663696, 'metrics/data_secs_per_batch': 2.365765690803528, '_timestamp': 1740979642.1827614}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 264 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979642.1830351}).
Epoch: [8][265/500]	Time  5.503 ( 5.503)	Loss 0.2891 (1.4748)	CeLoss 0.2891 (0.3187)	SegCLSLoss 0.0000 (0.0118)	KLLoss 0.0000 (0.2885)	MaskLoss 0.0000 (0.5607)	MaskBCELoss 0.0000 (0.0705)	MaskDICELoss 0.0000 (0.4902)
Epoch: [8][266/500]	Time  6.560 ( 6.560)	Loss 1.9251 (1.4504)	CeLoss 0.2197 (0.2493)	SegCLSLoss 0.0206 (0.0134)	KLLoss 0.3691 (0.3242)	MaskLoss 0.8288 (0.5808)	MaskBCELoss 0.0070 (0.0669)	MaskDICELoss 0.8218 (0.5140)
Epoch: [8][267/500]	Time  5.156 ( 5.156)	Loss 1.9082 (1.5204)	CeLoss 0.2656 (0.5350)	SegCLSLoss 0.0136 (0.0086)	KLLoss 0.3574 (0.2525)	MaskLoss 0.8008 (0.4782)	MaskBCELoss 0.0490 (0.1134)	MaskDICELoss 0.7518 (0.3647)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 265 is less than current step: 499. Dropping entry: {'train/loss': 1.450368046760559, 'train/ce_loss': 0.24931640625, 'train/seg_cls_loss': 0.013360595703125, 'train/kl_loss': 0.32421875, 'train/mask_bce_loss': 0.06686760913580655, 'train/mask_dice_loss': 0.513980483263731, 'train/mask_loss': 0.5808480888605118, 'metrics/total_secs_per_batch': 6.560463905334473, 'metrics/data_secs_per_batch': 2.920839476585388, '_timestamp': 1740979648.7433043}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 265 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979648.7436235}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 266 is less than current step: 499. Dropping entry: {'train/loss': 1.5203959345817566, 'train/ce_loss': 0.5349609375, 'train/seg_cls_loss': 0.008575439453125, 'train/kl_loss': 0.2525390625, 'train/mask_bce_loss': 0.11341949850320816, 'train/mask_dice_loss': 0.3647472277283669, 'train/mask_loss': 0.4781667247414589, 'metrics/total_secs_per_batch': 5.156232595443726, 'metrics/data_secs_per_batch': 2.345295476913452, '_timestamp': 1740979653.8996465}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 266 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979653.9000924}).
Epoch: [8][268/500]	Time  5.836 ( 5.836)	Loss 1.9261 (1.6208)	CeLoss 0.2295 (0.4332)	SegCLSLoss 0.0220 (0.0137)	KLLoss 0.3555 (0.2941)	MaskLoss 0.8253 (0.5759)	MaskBCELoss 0.1286 (0.1058)	MaskDICELoss 0.6968 (0.4700)
Epoch: [8][269/500]	Time  6.076 ( 6.076)	Loss 1.7624 (1.3914)	CeLoss 0.2168 (0.3370)	SegCLSLoss 0.0162 (0.0128)	KLLoss 0.3633 (0.2893)	MaskLoss 0.7503 (0.5094)	MaskBCELoss 0.0079 (0.0864)	MaskDICELoss 0.7424 (0.4230)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 267 is less than current step: 499. Dropping entry: {'train/loss': 1.6207605123519897, 'train/ce_loss': 0.433203125, 'train/seg_cls_loss': 0.01365966796875, 'train/kl_loss': 0.294140625, 'train/mask_bce_loss': 0.10584603073075413, 'train/mask_dice_loss': 0.47001273930072784, 'train/mask_loss': 0.5758587688207626, 'metrics/total_secs_per_batch': 5.835769414901733, 'metrics/data_secs_per_batch': 2.2970059633255007, '_timestamp': 1740979659.7352102}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 267 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979659.735405}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 268 is less than current step: 499. Dropping entry: {'train/loss': 1.3914362072944642, 'train/ce_loss': 0.33701171875, 'train/seg_cls_loss': 0.012750244140625, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.08637924836948514, 'train/mask_dice_loss': 0.4230107247829437, 'train/mask_loss': 0.509389977157116, 'metrics/total_secs_per_batch': 6.07609486579895, 'metrics/data_secs_per_batch': 2.7501597166061402, '_timestamp': 1740979665.8115513}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 268 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979665.8119054}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 269 is less than current step: 499. Dropping entry: {'train/loss': 1.6898956298828125, 'train/ce_loss': 0.47197265625, 'train/seg_cls_loss': 0.011212158203125, 'train/kl_loss': 0.255859375, 'train/mask_bce_loss': 0.14145399257540703, 'train/mask_dice_loss': 0.45193131268024445, 'train/mask_loss': 0.5933853030204773, 'metrics/total_secs_per_batch': 5.399955749511719, 'metrics/data_secs_per_batch': 2.2741771936416626, '_timestamp': 1740979671.2112594}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 269 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979671.2117095}).
[2025-03-02 23:27:51,206] [INFO] [logging.py:96:log_dist] [Rank 0] step=4270, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:27:51,210] [INFO] [timer.py:215:stop] epoch=0/micro_step=42700/global_step=4270, RunningAvgSamplesPerSec=1.4858491275189079, CurrSamplesPerSec=1.8525694786874547, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [8][270/500]	Time  5.400 ( 5.400)	Loss 2.3164 (1.6899)	CeLoss 0.2695 (0.4720)	SegCLSLoss 0.0125 (0.0112)	KLLoss 0.3594 (0.2559)	MaskLoss 1.0029 (0.5934)	MaskBCELoss 0.2261 (0.1415)	MaskDICELoss 0.7769 (0.4519)
Epoch: [8][271/500]	Time  6.015 ( 6.015)	Loss 1.6533 (1.5911)	CeLoss 0.2773 (0.3103)	SegCLSLoss 0.0113 (0.0145)	KLLoss 0.3574 (0.2855)	MaskLoss 0.6675 (0.6226)	MaskBCELoss 0.0303 (0.0907)	MaskDICELoss 0.6372 (0.5319)
Epoch: [8][272/500]	Time  5.712 ( 5.712)	Loss 1.8510 (1.7077)	CeLoss 0.2178 (0.5891)	SegCLSLoss 0.0240 (0.0129)	KLLoss 0.3496 (0.2527)	MaskLoss 0.7937 (0.5435)	MaskBCELoss 0.1068 (0.0945)	MaskDICELoss 0.6868 (0.4489)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 270 is less than current step: 499. Dropping entry: {'train/loss': 1.5911449790000916, 'train/ce_loss': 0.31025390625, 'train/seg_cls_loss': 0.014508056640625, 'train/kl_loss': 0.285546875, 'train/mask_bce_loss': 0.09071019687689841, 'train/mask_dice_loss': 0.531913086771965, 'train/mask_loss': 0.6226232826709748, 'metrics/total_secs_per_batch': 6.015341281890869, 'metrics/data_secs_per_batch': 2.7420994520187376, '_timestamp': 1740979677.226919}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 270 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979677.2273774}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 271 is less than current step: 499. Dropping entry: {'train/loss': 1.7076613843441009, 'train/ce_loss': 0.5890625, 'train/seg_cls_loss': 0.01290283203125, 'train/kl_loss': 0.252734375, 'train/mask_bce_loss': 0.09454435035586357, 'train/mask_dice_loss': 0.44893476366996765, 'train/mask_loss': 0.5434791177511216, 'metrics/total_secs_per_batch': 5.7118754386901855, 'metrics/data_secs_per_batch': 2.4446680784225463, '_timestamp': 1740979682.9385614}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 271 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979682.9388628}).
Epoch: [8][273/500]	Time  5.831 ( 5.831)	Loss 2.7723 (1.8485)	CeLoss 0.2324 (0.2558)	SegCLSLoss 0.0177 (0.0158)	KLLoss 0.3555 (0.2922)	MaskLoss 1.2475 (0.7777)	MaskBCELoss 0.4836 (0.1849)	MaskDICELoss 0.7639 (0.5928)
Epoch: [8][274/500]	Time  7.266 ( 7.266)	Loss 2.3333 (1.5615)	CeLoss 0.2012 (0.2744)	SegCLSLoss 0.0177 (0.0132)	KLLoss 0.3652 (0.2523)	MaskLoss 1.0436 (0.6277)	MaskBCELoss 0.1059 (0.0942)	MaskDICELoss 0.9377 (0.5334)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 272 is less than current step: 499. Dropping entry: {'train/loss': 1.848484754562378, 'train/ce_loss': 0.255810546875, 'train/seg_cls_loss': 0.015765380859375, 'train/kl_loss': 0.2921875, 'train/mask_bce_loss': 0.18486416968517005, 'train/mask_dice_loss': 0.5928205996751785, 'train/mask_loss': 0.7776847600936889, 'metrics/total_secs_per_batch': 5.83114767074585, 'metrics/data_secs_per_batch': 2.6588003873825072, '_timestamp': 1740979688.7699342}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 272 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979688.7703145}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 273 is less than current step: 499. Dropping entry: {'train/loss': 1.5614627122879028, 'train/ce_loss': 0.274365234375, 'train/seg_cls_loss': 0.013232421875, 'train/kl_loss': 0.25234375, 'train/mask_bce_loss': 0.09423199519515038, 'train/mask_dice_loss': 0.5334476053714752, 'train/mask_loss': 0.6276796102523804, 'metrics/total_secs_per_batch': 7.2664244174957275, 'metrics/data_secs_per_batch': 3.148989224433899, '_timestamp': 1740979696.0362701}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 273 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979696.0366127}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 274 is less than current step: 499. Dropping entry: {'train/loss': 1.4208062052726746, 'train/ce_loss': 0.52109375, 'train/seg_cls_loss': 0.011907958984375, 'train/kl_loss': 0.2154296875, 'train/mask_bce_loss': 0.0649375194683671, 'train/mask_dice_loss': 0.37110034823417665, 'train/mask_loss': 0.4360378682613373, 'metrics/total_secs_per_batch': 5.594832181930542, 'metrics/data_secs_per_batch': 2.499929976463318, '_timestamp': 1740979701.631162}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 274 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979701.6317956}).
Epoch: [8][275/500]	Time  5.595 ( 5.595)	Loss 1.5448 (1.4208)	CeLoss 0.2236 (0.5211)	SegCLSLoss 0.0208 (0.0119)	KLLoss 0.3750 (0.2154)	MaskLoss 0.6366 (0.4360)	MaskBCELoss 0.3151 (0.0649)	MaskDICELoss 0.3216 (0.3711)
Epoch: [8][276/500]	Time  5.601 ( 5.601)	Loss 2.2622 (1.8192)	CeLoss 0.3066 (0.3467)	SegCLSLoss 0.0126 (0.0135)	KLLoss 0.3633 (0.2904)	MaskLoss 0.9563 (0.7183)	MaskBCELoss 0.0942 (0.1377)	MaskDICELoss 0.8621 (0.5806)
Epoch: [8][277/500]	Time  6.270 ( 6.270)	Loss 1.6044 (2.0472)	CeLoss 0.2500 (0.3834)	SegCLSLoss 0.0143 (0.0157)	KLLoss 0.3574 (0.2902)	MaskLoss 0.6557 (0.8135)	MaskBCELoss 0.0140 (0.1967)	MaskDICELoss 0.6417 (0.6168)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 275 is less than current step: 499. Dropping entry: {'train/loss': 1.8191746473312378, 'train/ce_loss': 0.3466796875, 'train/seg_cls_loss': 0.01351318359375, 'train/kl_loss': 0.2904296875, 'train/mask_bce_loss': 0.13766825157217683, 'train/mask_dice_loss': 0.580610477924347, 'train/mask_loss': 0.7182787239551545, 'metrics/total_secs_per_batch': 5.600613832473755, 'metrics/data_secs_per_batch': 2.500074338912964, '_timestamp': 1740979707.2317271}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 275 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979707.2320387}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 276 is less than current step: 499. Dropping entry: {'train/loss': 2.047215986251831, 'train/ce_loss': 0.3833984375, 'train/seg_cls_loss': 0.015673828125, 'train/kl_loss': 0.290234375, 'train/mask_bce_loss': 0.19673456167802214, 'train/mask_dice_loss': 0.6168148279190063, 'train/mask_loss': 0.8135493934154511, 'metrics/total_secs_per_batch': 6.269888639450073, 'metrics/data_secs_per_batch': 2.8317983865737917, '_timestamp': 1740979713.5014706}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 276 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979713.5018206}).
Epoch: [8][278/500]	Time  4.906 ( 4.906)	Loss 0.9336 (1.4330)	CeLoss 0.9336 (0.4591)	SegCLSLoss 0.0000 (0.0111)	KLLoss 0.0000 (0.2566)	MaskLoss 0.0000 (0.4713)	MaskBCELoss 0.0000 (0.1149)	MaskDICELoss 0.0000 (0.3563)
Epoch: [8][279/500]	Time  6.211 ( 6.211)	Loss 3.2419 (1.9458)	CeLoss 0.3672 (0.3059)	SegCLSLoss 0.0210 (0.0171)	KLLoss 0.3770 (0.3289)	MaskLoss 1.4139 (0.7994)	MaskBCELoss 1.0341 (0.1546)	MaskDICELoss 0.3798 (0.6448)
[2025-03-02 23:28:49,384] [INFO] [logging.py:96:log_dist] [Rank 0] step=4280, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:28:49,390] [INFO] [timer.py:215:stop] epoch=0/micro_step=42800/global_step=4280, RunningAvgSamplesPerSec=1.4863207838420387, CurrSamplesPerSec=2.0961610398684023, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [8][280/500]	Time  4.772 ( 4.772)	Loss 2.0223 (1.6701)	CeLoss 0.2314 (0.4672)	SegCLSLoss 0.0198 (0.0139)	KLLoss 0.3555 (0.2541)	MaskLoss 0.8725 (0.5852)	MaskBCELoss 0.0735 (0.0867)	MaskDICELoss 0.7990 (0.4985)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 277 is less than current step: 499. Dropping entry: {'train/loss': 1.4329546093940735, 'train/ce_loss': 0.45908203125, 'train/seg_cls_loss': 0.011126708984375, 'train/kl_loss': 0.256640625, 'train/mask_bce_loss': 0.11492860205471515, 'train/mask_dice_loss': 0.3563338674604893, 'train/mask_loss': 0.4712624683976173, 'metrics/total_secs_per_batch': 4.906368970870972, 'metrics/data_secs_per_batch': 2.204565000534058, '_timestamp': 1740979718.407833}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 277 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979718.4081085}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 278 is less than current step: 499. Dropping entry: {'train/loss': 1.9457863569259644, 'train/ce_loss': 0.305859375, 'train/seg_cls_loss': 0.01712646484375, 'train/kl_loss': 0.32890625, 'train/mask_bce_loss': 0.15458074472844602, 'train/mask_dice_loss': 0.6447772949934005, 'train/mask_loss': 0.7993580281734467, 'metrics/total_secs_per_batch': 6.210739374160767, 'metrics/data_secs_per_batch': 2.988062930107117, '_timestamp': 1740979724.6188006}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 278 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979724.6191492}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 279 is less than current step: 499. Dropping entry: {'train/loss': 1.670106625556946, 'train/ce_loss': 0.4671875, 'train/seg_cls_loss': 0.013897705078125, 'train/kl_loss': 0.2541015625, 'train/mask_bce_loss': 0.08667677310295403, 'train/mask_dice_loss': 0.49852301776409147, 'train/mask_loss': 0.5851998031139374, 'metrics/total_secs_per_batch': 4.772497177124023, 'metrics/data_secs_per_batch': 1.8931963205337525, '_timestamp': 1740979729.3908925}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 279 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979729.391164}).
Epoch: [8][281/500]	Time  5.430 ( 5.430)	Loss 1.2713 (1.5314)	CeLoss 0.2695 (0.5507)	SegCLSLoss 0.0150 (0.0113)	KLLoss 0.3809 (0.2240)	MaskLoss 0.4775 (0.4763)	MaskBCELoss 0.1682 (0.1109)	MaskDICELoss 0.3093 (0.3654)
Epoch: [8][282/500]	Time  5.689 ( 5.689)	Loss 1.1016 (1.4136)	CeLoss 1.1016 (0.4597)	SegCLSLoss 0.0000 (0.0089)	KLLoss 0.0000 (0.2174)	MaskLoss 0.0000 (0.4637)	MaskBCELoss 0.0000 (0.0715)	MaskDICELoss 0.0000 (0.3922)
Epoch: [8][283/500]	Time  5.278 ( 5.278)	Loss 1.1582 (1.6788)	CeLoss 0.2275 (0.5607)	SegCLSLoss 0.0110 (0.0111)	KLLoss 0.3672 (0.2564)	MaskLoss 0.4443 (0.5433)	MaskBCELoss 0.1445 (0.0870)	MaskDICELoss 0.2998 (0.4563)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 280 is less than current step: 499. Dropping entry: {'train/loss': 1.531431210041046, 'train/ce_loss': 0.55068359375, 'train/seg_cls_loss': 0.011297607421875, 'train/kl_loss': 0.2240234375, 'train/mask_bce_loss': 0.11089441515505313, 'train/mask_dice_loss': 0.3654168739914894, 'train/mask_loss': 0.47631129920482634, 'metrics/total_secs_per_batch': 5.4296958446502686, 'metrics/data_secs_per_batch': 2.4797457695007323, '_timestamp': 1740979734.8207781}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 280 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979734.8210573}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 281 is less than current step: 499. Dropping entry: {'train/loss': 1.4136369585990907, 'train/ce_loss': 0.45966796875, 'train/seg_cls_loss': 0.00885009765625, 'train/kl_loss': 0.2173828125, 'train/mask_bce_loss': 0.07147273421287537, 'train/mask_dice_loss': 0.3921816810965538, 'train/mask_loss': 0.4636544167995453, 'metrics/total_secs_per_batch': 5.689434289932251, 'metrics/data_secs_per_batch': 2.852363109588623, '_timestamp': 1740979740.5102174}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 281 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979740.510513}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 282 is less than current step: 499. Dropping entry: {'train/loss': 1.6787555694580079, 'train/ce_loss': 0.5607421875, 'train/seg_cls_loss': 0.011090087890625, 'train/kl_loss': 0.2564453125, 'train/mask_bce_loss': 0.0869552280753851, 'train/mask_dice_loss': 0.45632881224155425, 'train/mask_loss': 0.5432840406894683, 'metrics/total_secs_per_batch': 5.277722120285034, 'metrics/data_secs_per_batch': 2.665720248222351, '_timestamp': 1740979745.7879672}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 282 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979745.7882628}).
Epoch: [8][284/500]	Time  5.236 ( 5.236)	Loss 1.4453 (1.8960)	CeLoss 1.4453 (0.5487)	SegCLSLoss 0.0000 (0.0140)	KLLoss 0.0000 (0.2531)	MaskLoss 0.0000 (0.6576)	MaskBCELoss 0.0000 (0.1653)	MaskDICELoss 0.0000 (0.4923)
Epoch: [8][285/500]	Time  7.005 ( 7.005)	Loss 2.3151 (1.5490)	CeLoss 0.4395 (0.2567)	SegCLSLoss 0.0125 (0.0112)	KLLoss 0.3672 (0.2902)	MaskLoss 0.9163 (0.6289)	MaskBCELoss 0.3812 (0.1823)	MaskDICELoss 0.5352 (0.4466)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 283 is less than current step: 499. Dropping entry: {'train/loss': 1.8960494160652162, 'train/ce_loss': 0.54873046875, 'train/seg_cls_loss': 0.01397705078125, 'train/kl_loss': 0.253125, 'train/mask_bce_loss': 0.1652725201100111, 'train/mask_dice_loss': 0.49232250452041626, 'train/mask_loss': 0.657595020532608, 'metrics/total_secs_per_batch': 5.235670566558838, 'metrics/data_secs_per_batch': 2.527907156944275, '_timestamp': 1740979751.023724}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 283 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979751.0240645}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 284 is less than current step: 499. Dropping entry: {'train/loss': 1.548983633518219, 'train/ce_loss': 0.256689453125, 'train/seg_cls_loss': 0.011151123046875, 'train/kl_loss': 0.290234375, 'train/mask_bce_loss': 0.18228942221030592, 'train/mask_dice_loss': 0.44657250940799714, 'train/mask_loss': 0.6288619339466095, 'metrics/total_secs_per_batch': 7.0049450397491455, 'metrics/data_secs_per_batch': 2.923711657524109, '_timestamp': 1740979758.0285609}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 284 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979758.0288363}).
Epoch: [8][286/500]	Time  5.519 ( 5.519)	Loss 1.1662 (1.4440)	CeLoss 0.2246 (0.3366)	SegCLSLoss 0.0093 (0.0098)	KLLoss 0.3652 (0.2572)	MaskLoss 0.4503 (0.5384)	MaskBCELoss 0.1806 (0.1652)	MaskDICELoss 0.2697 (0.3732)
Epoch: [8][287/500]	Time  5.582 ( 5.582)	Loss 2.0145 (1.8020)	CeLoss 0.2334 (0.6124)	SegCLSLoss 0.0159 (0.0105)	KLLoss 0.3633 (0.2178)	MaskLoss 0.8686 (0.5813)	MaskBCELoss 0.1341 (0.1031)	MaskDICELoss 0.7345 (0.4782)
Epoch: [8][288/500]	Time  4.631 ( 4.631)	Loss 1.2101 (1.5301)	CeLoss 0.2578 (0.4566)	SegCLSLoss 0.0113 (0.0100)	KLLoss 0.3652 (0.2559)	MaskLoss 0.4547 (0.5214)	MaskBCELoss 0.0771 (0.1047)	MaskDICELoss 0.3775 (0.4167)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 285 is less than current step: 499. Dropping entry: {'train/loss': 1.443975216150284, 'train/ce_loss': 0.336572265625, 'train/seg_cls_loss': 0.009765625, 'train/kl_loss': 0.2572265625, 'train/mask_bce_loss': 0.16518843472003936, 'train/mask_dice_loss': 0.37318103611469267, 'train/mask_loss': 0.5383694589138031, 'metrics/total_secs_per_batch': 5.519429445266724, 'metrics/data_secs_per_batch': 2.3809828042984007, '_timestamp': 1740979763.548145}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 285 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979763.5484898}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 286 is less than current step: 499. Dropping entry: {'train/loss': 1.8020463347434998, 'train/ce_loss': 0.61240234375, 'train/seg_cls_loss': 0.010540771484375, 'train/kl_loss': 0.2177734375, 'train/mask_bce_loss': 0.10310838157311082, 'train/mask_dice_loss': 0.47818822860717775, 'train/mask_loss': 0.5812966227531433, 'metrics/total_secs_per_batch': 5.582461595535278, 'metrics/data_secs_per_batch': 2.568616008758545, '_timestamp': 1740979769.1304822}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 286 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979769.1308312}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 287 is less than current step: 499. Dropping entry: {'train/loss': 1.5301090955734253, 'train/ce_loss': 0.456640625, 'train/seg_cls_loss': 0.00997314453125, 'train/kl_loss': 0.255859375, 'train/mask_bce_loss': 0.10472949810791761, 'train/mask_dice_loss': 0.41667269468307494, 'train/mask_loss': 0.5214021950960159, 'metrics/total_secs_per_batch': 4.630782604217529, 'metrics/data_secs_per_batch': 1.9887804508209228, '_timestamp': 1740979773.7612386}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 287 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979773.761519}).
Epoch: [8][289/500]	Time  4.896 ( 4.896)	Loss 0.4219 (1.4272)	CeLoss 0.4219 (0.6163)	SegCLSLoss 0.0000 (0.0076)	KLLoss 0.0000 (0.1461)	MaskLoss 0.0000 (0.3963)	MaskBCELoss 0.0000 (0.1051)	MaskDICELoss 0.0000 (0.2912)
[2025-03-02 23:29:43,494] [INFO] [logging.py:96:log_dist] [Rank 0] step=4290, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:29:43,500] [INFO] [timer.py:215:stop] epoch=0/micro_step=42900/global_step=4290, RunningAvgSamplesPerSec=1.487000236036631, CurrSamplesPerSec=2.0648932058909684, MemAllocated=30.93GB, MaxMemAllocated=37.23GB
Epoch: [8][290/500]	Time  4.844 ( 4.844)	Loss 1.1328 (1.4556)	CeLoss 1.1328 (0.4554)	SegCLSLoss 0.0000 (0.0121)	KLLoss 0.0000 (0.2545)	MaskLoss 0.0000 (0.4842)	MaskBCELoss 0.0000 (0.0394)	MaskDICELoss 0.0000 (0.4449)
Epoch: [8][291/500]	Time  5.651 ( 5.651)	Loss 2.6814 (1.4350)	CeLoss 0.1182 (0.5362)	SegCLSLoss 0.0306 (0.0093)	KLLoss 0.3730 (0.2188)	MaskLoss 1.2552 (0.4360)	MaskBCELoss 0.4649 (0.1288)	MaskDICELoss 0.7903 (0.3072)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 288 is less than current step: 499. Dropping entry: {'train/loss': 1.427229928970337, 'train/ce_loss': 0.61630859375, 'train/seg_cls_loss': 0.007623291015625, 'train/kl_loss': 0.14609375, 'train/mask_bce_loss': 0.10512935593724251, 'train/mask_dice_loss': 0.29120045304298403, 'train/mask_loss': 0.39632980823516845, 'metrics/total_secs_per_batch': 4.89581823348999, 'metrics/data_secs_per_batch': 2.146317791938782, '_timestamp': 1740979778.657108}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 288 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979778.6573925}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 289 is less than current step: 499. Dropping entry: {'train/loss': 1.4555731177330018, 'train/ce_loss': 0.45537109375, 'train/seg_cls_loss': 0.012139892578125, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.039364932011812924, 'train/mask_dice_loss': 0.4448669523000717, 'train/mask_loss': 0.48423188328742983, 'metrics/total_secs_per_batch': 4.84448766708374, 'metrics/data_secs_per_batch': 2.159294319152832, '_timestamp': 1740979783.5013585}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 289 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979783.5016408}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 290 is less than current step: 499. Dropping entry: {'train/loss': 1.4349795937538148, 'train/ce_loss': 0.536181640625, 'train/seg_cls_loss': 0.00927734375, 'train/kl_loss': 0.21875, 'train/mask_bce_loss': 0.12880879752337931, 'train/mask_dice_loss': 0.3072112753987312, 'train/mask_loss': 0.4360200628638268, 'metrics/total_secs_per_batch': 5.65129542350769, 'metrics/data_secs_per_batch': 2.4322284936904905, '_timestamp': 1740979789.153073}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 290 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979789.1534307}).
Epoch: [8][292/500]	Time  5.093 ( 5.093)	Loss 2.3584 (1.4670)	CeLoss 0.2168 (0.4806)	SegCLSLoss 0.0141 (0.0098)	KLLoss 0.3965 (0.2193)	MaskLoss 1.0474 (0.4798)	MaskBCELoss 0.3109 (0.0918)	MaskDICELoss 0.7365 (0.3879)
Epoch: [8][293/500]	Time  5.928 ( 5.928)	Loss 1.8304 (2.1569)	CeLoss 0.2129 (0.2656)	SegCLSLoss 0.0204 (0.0187)	KLLoss 0.3555 (0.3312)	MaskLoss 0.7863 (0.9244)	MaskBCELoss 0.0979 (0.2134)	MaskDICELoss 0.6884 (0.7110)
Epoch: [8][294/500]	Time  5.319 ( 5.319)	Loss 1.8410 (1.7465)	CeLoss 0.2734 (0.5107)	SegCLSLoss 0.0151 (0.0130)	KLLoss 0.3691 (0.2557)	MaskLoss 0.7613 (0.6019)	MaskBCELoss 0.1868 (0.1324)	MaskDICELoss 0.5745 (0.4694)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 291 is less than current step: 499. Dropping entry: {'train/loss': 1.46697895526886, 'train/ce_loss': 0.48056640625, 'train/seg_cls_loss': 0.009820556640625, 'train/kl_loss': 0.2193359375, 'train/mask_bce_loss': 0.09184644408524037, 'train/mask_dice_loss': 0.3879320859909058, 'train/mask_loss': 0.47977853417396543, 'metrics/total_secs_per_batch': 5.0930421352386475, 'metrics/data_secs_per_batch': 2.1983999490737913, '_timestamp': 1740979794.245889}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 291 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979794.2460809}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 292 is less than current step: 499. Dropping entry: {'train/loss': 2.1568813800811766, 'train/ce_loss': 0.265625, 'train/seg_cls_loss': 0.01866455078125, 'train/kl_loss': 0.33125, 'train/mask_bce_loss': 0.21343097537755967, 'train/mask_dice_loss': 0.7110058099031449, 'train/mask_loss': 0.9244367808103562, 'metrics/total_secs_per_batch': 5.927976846694946, 'metrics/data_secs_per_batch': 2.7591632127761843, '_timestamp': 1740979800.1757767}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 292 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979800.1763043}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 293 is less than current step: 499. Dropping entry: {'train/loss': 1.7464527249336244, 'train/ce_loss': 0.5107421875, 'train/seg_cls_loss': 0.0130126953125, 'train/kl_loss': 0.2556640625, 'train/mask_bce_loss': 0.13244427870959044, 'train/mask_dice_loss': 0.4694441944360733, 'train/mask_loss': 0.6018884658813477, 'metrics/total_secs_per_batch': 5.318676471710205, 'metrics/data_secs_per_batch': 2.371748518943787, '_timestamp': 1740979805.4927588}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 293 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979805.4930844}).
Epoch: [8][295/500]	Time  6.087 ( 6.087)	Loss 1.5502 (1.4443)	CeLoss 0.3047 (0.4067)	SegCLSLoss 0.0162 (0.0116)	KLLoss 0.3633 (0.2166)	MaskLoss 0.6003 (0.5050)	MaskBCELoss 0.0163 (0.0973)	MaskDICELoss 0.5840 (0.4077)
Epoch: [8][296/500]	Time  5.081 ( 5.081)	Loss 0.8516 (2.0199)	CeLoss 0.8516 (0.4346)	SegCLSLoss 0.0000 (0.0151)	KLLoss 0.0000 (0.2906)	MaskLoss 0.0000 (0.7742)	MaskBCELoss 0.0000 (0.2117)	MaskDICELoss 0.0000 (0.5625)
Epoch: [8][297/500]	Time  4.792 ( 4.792)	Loss 0.8867 (1.7184)	CeLoss 0.8867 (0.7273)	SegCLSLoss 0.0000 (0.0094)	KLLoss 0.0000 (0.1826)	MaskLoss 0.0000 (0.4840)	MaskBCELoss 0.0000 (0.1036)	MaskDICELoss 0.0000 (0.3805)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 294 is less than current step: 499. Dropping entry: {'train/loss': 1.4443474769592286, 'train/ce_loss': 0.40673828125, 'train/seg_cls_loss': 0.01160888671875, 'train/kl_loss': 0.2166015625, 'train/mask_bce_loss': 0.09730531312525273, 'train/mask_dice_loss': 0.40772975981235504, 'train/mask_loss': 0.5050350725650787, 'metrics/total_secs_per_batch': 6.087245941162109, 'metrics/data_secs_per_batch': 2.5684620141983032, '_timestamp': 1740979811.5798867}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 294 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979811.5801907}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 295 is less than current step: 499. Dropping entry: {'train/loss': 2.019942116737366, 'train/ce_loss': 0.4345703125, 'train/seg_cls_loss': 0.0150634765625, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.21170028112828732, 'train/mask_dice_loss': 0.5624797582626343, 'train/mask_loss': 0.7741800308227539, 'metrics/total_secs_per_batch': 5.080765962600708, 'metrics/data_secs_per_batch': 2.421769714355469, '_timestamp': 1740979816.6608167}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 295 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979816.66116}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 296 is less than current step: 499. Dropping entry: {'train/loss': 1.7183706879615783, 'train/ce_loss': 0.72734375, 'train/seg_cls_loss': 0.009381103515625, 'train/kl_loss': 0.1826171875, 'train/mask_bce_loss': 0.1035605838522315, 'train/mask_dice_loss': 0.3804782867431641, 'train/mask_loss': 0.4840388596057892, 'metrics/total_secs_per_batch': 4.79184627532959, 'metrics/data_secs_per_batch': 1.9953128576278687, '_timestamp': 1740979821.4524934}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 296 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979821.4527853}).
Epoch: [8][298/500]	Time  6.525 ( 6.525)	Loss 1.9523 (1.4259)	CeLoss 0.1836 (0.4285)	SegCLSLoss 0.0284 (0.0128)	KLLoss 0.3574 (0.2139)	MaskLoss 0.8590 (0.4849)	MaskBCELoss 0.0170 (0.0692)	MaskDICELoss 0.8420 (0.4157)
Epoch: [8][299/500]	Time  4.737 ( 4.737)	Loss 2.3763 (1.7407)	CeLoss 0.1152 (0.6003)	SegCLSLoss 0.0391 (0.0115)	KLLoss 0.3789 (0.2184)	MaskLoss 1.1017 (0.5563)	MaskBCELoss 0.4334 (0.1123)	MaskDICELoss 0.6683 (0.4440)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 297 is less than current step: 499. Dropping entry: {'train/loss': 1.4259086966514587, 'train/ce_loss': 0.428466796875, 'train/seg_cls_loss': 0.01279296875, 'train/kl_loss': 0.2138671875, 'train/mask_bce_loss': 0.06919705756008625, 'train/mask_dice_loss': 0.41565670967102053, 'train/mask_loss': 0.48485376834869387, 'metrics/total_secs_per_batch': 6.524548292160034, 'metrics/data_secs_per_batch': 2.816433000564575, '_timestamp': 1740979827.9770596}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 297 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979827.9772627}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 298 is less than current step: 499. Dropping entry: {'train/loss': 1.7407055020332336, 'train/ce_loss': 0.60029296875, 'train/seg_cls_loss': 0.01146240234375, 'train/kl_loss': 0.218359375, 'train/mask_bce_loss': 0.11234840676188469, 'train/mask_dice_loss': 0.44399067759513855, 'train/mask_loss': 0.5563390851020813, 'metrics/total_secs_per_batch': 4.737130403518677, 'metrics/data_secs_per_batch': 2.046090340614319, '_timestamp': 1740979832.7142816}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 298 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979832.7147248}).
[2025-03-02 23:30:38,744] [INFO] [logging.py:96:log_dist] [Rank 0] step=4300, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:30:38,750] [INFO] [timer.py:215:stop] epoch=0/micro_step=43000/global_step=4300, RunningAvgSamplesPerSec=1.4876186313151505, CurrSamplesPerSec=1.6569098185333302, MemAllocated=31.09GB, MaxMemAllocated=37.23GB
Epoch: [8][300/500]	Time  6.037 ( 6.037)	Loss 2.2353 (1.8610)	CeLoss 0.2617 (0.4629)	SegCLSLoss 0.0193 (0.0140)	KLLoss 0.3691 (0.2916)	MaskLoss 0.9633 (0.6807)	MaskBCELoss 0.4413 (0.1596)	MaskDICELoss 0.5220 (0.5212)
Epoch: [8][301/500]	Time  6.163 ( 6.163)	Loss 1.4181 (1.7816)	CeLoss 0.2061 (0.5074)	SegCLSLoss 0.0173 (0.0119)	KLLoss 0.3594 (0.2529)	MaskLoss 0.5841 (0.6216)	MaskBCELoss 0.0080 (0.0969)	MaskDICELoss 0.5761 (0.5247)
Epoch: [8][302/500]	Time  5.375 ( 5.375)	Loss 2.5803 (1.6159)	CeLoss 0.2197 (0.6206)	SegCLSLoss 0.0127 (0.0078)	KLLoss 0.3633 (0.2207)	MaskLoss 1.1583 (0.4845)	MaskBCELoss 0.4449 (0.1268)	MaskDICELoss 0.7134 (0.3577)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 299 is less than current step: 499. Dropping entry: {'train/loss': 1.8609660148620606, 'train/ce_loss': 0.462890625, 'train/seg_cls_loss': 0.0139892578125, 'train/kl_loss': 0.2916015625, 'train/mask_bce_loss': 0.15955518800765275, 'train/mask_dice_loss': 0.5211719542741775, 'train/mask_loss': 0.6807271510362625, 'metrics/total_secs_per_batch': 6.037252187728882, 'metrics/data_secs_per_batch': 2.838114595413208, '_timestamp': 1740979838.7512267}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 299 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979838.7515366}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 300 is less than current step: 499. Dropping entry: {'train/loss': 1.781634521484375, 'train/ce_loss': 0.507421875, 'train/seg_cls_loss': 0.01192626953125, 'train/kl_loss': 0.2529296875, 'train/mask_bce_loss': 0.09692544741556049, 'train/mask_dice_loss': 0.5246535211801528, 'train/mask_loss': 0.6215789705514908, 'metrics/total_secs_per_batch': 6.163463354110718, 'metrics/data_secs_per_batch': 2.8507591009140016, '_timestamp': 1740979844.9148715}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 300 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979844.9151561}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 301 is less than current step: 499. Dropping entry: {'train/loss': 1.6159423410892486, 'train/ce_loss': 0.62060546875, 'train/seg_cls_loss': 0.007794189453125, 'train/kl_loss': 0.220703125, 'train/mask_bce_loss': 0.12683957032859325, 'train/mask_dice_loss': 0.3576941043138504, 'train/mask_loss': 0.4845336705446243, 'metrics/total_secs_per_batch': 5.3750622272491455, 'metrics/data_secs_per_batch': 2.370926260948181, '_timestamp': 1740979850.2899294}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 301 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979850.2903008}).
Epoch: [8][303/500]	Time  6.146 ( 6.146)	Loss 2.4736 (1.6778)	CeLoss 0.2793 (0.3228)	SegCLSLoss 0.0130 (0.0139)	KLLoss 0.3652 (0.2973)	MaskLoss 1.0756 (0.6591)	MaskBCELoss 0.5117 (0.1952)	MaskDICELoss 0.5640 (0.4639)
Epoch: [8][304/500]	Time  6.027 ( 6.027)	Loss 1.8274 (1.5861)	CeLoss 0.2910 (0.3321)	SegCLSLoss 0.0101 (0.0107)	KLLoss 0.3652 (0.2604)	MaskLoss 0.7467 (0.6111)	MaskBCELoss 0.1145 (0.1515)	MaskDICELoss 0.6322 (0.4596)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 302 is less than current step: 499. Dropping entry: {'train/loss': 1.677841603755951, 'train/ce_loss': 0.32275390625, 'train/seg_cls_loss': 0.01392822265625, 'train/kl_loss': 0.297265625, 'train/mask_bce_loss': 0.19515732750296594, 'train/mask_dice_loss': 0.46392949521541593, 'train/mask_loss': 0.6590868234634399, 'metrics/total_secs_per_batch': 6.1463117599487305, 'metrics/data_secs_per_batch': 2.900276017189026, '_timestamp': 1740979856.4362526}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 302 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979856.4366171}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 303 is less than current step: 499. Dropping entry: {'train/loss': 1.5861160516738892, 'train/ce_loss': 0.33212890625, 'train/seg_cls_loss': 0.01072998046875, 'train/kl_loss': 0.2603515625, 'train/mask_bce_loss': 0.15150362122803926, 'train/mask_dice_loss': 0.45957196056842803, 'train/mask_loss': 0.6110755920410156, 'metrics/total_secs_per_batch': 6.027212381362915, 'metrics/data_secs_per_batch': 2.79532527923584, '_timestamp': 1740979862.4637363}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 303 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979862.4641013}).
Epoch: [8][305/500]	Time  5.900 ( 5.900)	Loss 1.5139 (1.6173)	CeLoss 0.1982 (0.4035)	SegCLSLoss 0.0212 (0.0127)	KLLoss 0.3750 (0.2895)	MaskLoss 0.6339 (0.5893)	MaskBCELoss 0.1602 (0.1277)	MaskDICELoss 0.4737 (0.4616)
Epoch: [8][306/500]	Time  5.953 ( 5.953)	Loss 1.8327 (1.5624)	CeLoss 0.1953 (0.4620)	SegCLSLoss 0.0138 (0.0116)	KLLoss 0.3613 (0.2523)	MaskLoss 0.7972 (0.5345)	MaskBCELoss 0.0167 (0.0747)	MaskDICELoss 0.7805 (0.4598)
Epoch: [8][307/500]	Time  6.604 ( 6.604)	Loss 2.1025 (1.8135)	CeLoss 0.2695 (0.3368)	SegCLSLoss 0.0146 (0.0170)	KLLoss 0.3633 (0.3250)	MaskLoss 0.8940 (0.7177)	MaskBCELoss 0.1860 (0.0844)	MaskDICELoss 0.7080 (0.6333)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 304 is less than current step: 499. Dropping entry: {'train/loss': 1.6173006296157837, 'train/ce_loss': 0.403515625, 'train/seg_cls_loss': 0.012744140625, 'train/kl_loss': 0.289453125, 'train/mask_bce_loss': 0.1277146078646183, 'train/mask_dice_loss': 0.46155093759298327, 'train/mask_loss': 0.5892655581235886, 'metrics/total_secs_per_batch': 5.900159597396851, 'metrics/data_secs_per_batch': 2.7539572715759277, '_timestamp': 1740979868.363605}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 304 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979868.3638866}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 305 is less than current step: 499. Dropping entry: {'train/loss': 1.5623915195465088, 'train/ce_loss': 0.46201171875, 'train/seg_cls_loss': 0.011614990234375, 'train/kl_loss': 0.25234375, 'train/mask_bce_loss': 0.07470742892473936, 'train/mask_dice_loss': 0.4597598135471344, 'train/mask_loss': 0.5344672501087189, 'metrics/total_secs_per_batch': 5.953379392623901, 'metrics/data_secs_per_batch': 2.602119493484497, '_timestamp': 1740979874.3173542}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 305 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979874.3177679}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 306 is less than current step: 499. Dropping entry: {'train/loss': 1.8134647905826569, 'train/ce_loss': 0.33681640625, 'train/seg_cls_loss': 0.01700439453125, 'train/kl_loss': 0.325, 'train/mask_bce_loss': 0.08437088560312986, 'train/mask_dice_loss': 0.6332989990711212, 'train/mask_loss': 0.7176698833703995, 'metrics/total_secs_per_batch': 6.604489326477051, 'metrics/data_secs_per_batch': 3.2808567762374876, '_timestamp': 1740979880.9215364}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 306 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979880.9219618}).
Epoch: [8][308/500]	Time  7.619 ( 7.619)	Loss 3.1331 (1.6615)	CeLoss 0.3203 (0.2869)	SegCLSLoss 0.0124 (0.0129)	KLLoss 0.3574 (0.2891)	MaskLoss 1.3859 (0.6696)	MaskBCELoss 0.7011 (0.1660)	MaskDICELoss 0.6848 (0.5036)
Epoch: [8][309/500]	Time  6.419 ( 6.419)	Loss 2.0374 (1.8262)	CeLoss 0.3027 (0.2054)	SegCLSLoss 0.0149 (0.0167)	KLLoss 0.3594 (0.3266)	MaskLoss 0.8459 (0.7900)	MaskBCELoss 0.0430 (0.1350)	MaskDICELoss 0.8028 (0.6550)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 307 is less than current step: 499. Dropping entry: {'train/loss': 1.6615188837051391, 'train/ce_loss': 0.2869140625, 'train/seg_cls_loss': 0.01287841796875, 'train/kl_loss': 0.2890625, 'train/mask_bce_loss': 0.16602956373244523, 'train/mask_dice_loss': 0.5035970687866211, 'train/mask_loss': 0.6696266293525696, 'metrics/total_secs_per_batch': 7.618959665298462, 'metrics/data_secs_per_batch': 3.3444348573684692, '_timestamp': 1740979888.5408142}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 307 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979888.5412312}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 308 is less than current step: 499. Dropping entry: {'train/loss': 1.8261685848236084, 'train/ce_loss': 0.205419921875, 'train/seg_cls_loss': 0.016680908203125, 'train/kl_loss': 0.3265625, 'train/mask_bce_loss': 0.1349942935630679, 'train/mask_dice_loss': 0.6550187110900879, 'train/mask_loss': 0.7900130033493042, 'metrics/total_secs_per_batch': 6.419389009475708, 'metrics/data_secs_per_batch': 2.793782138824463, '_timestamp': 1740979894.9601455}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 308 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979894.9605432}).
[2025-03-02 23:31:41,350] [INFO] [logging.py:96:log_dist] [Rank 0] step=4310, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:31:41,355] [INFO] [timer.py:215:stop] epoch=0/micro_step=43100/global_step=4310, RunningAvgSamplesPerSec=1.4878565726396884, CurrSamplesPerSec=1.5638307467183095, MemAllocated=31.26GB, MaxMemAllocated=37.23GB
Epoch: [8][310/500]	Time  6.397 ( 6.397)	Loss 1.6256 (1.7242)	CeLoss 0.1670 (0.3164)	SegCLSLoss 0.0244 (0.0159)	KLLoss 0.3555 (0.3271)	MaskLoss 0.7054 (0.6836)	MaskBCELoss 0.0259 (0.1315)	MaskDICELoss 0.6795 (0.5521)
Epoch: [8][311/500]	Time  6.807 ( 6.807)	Loss 1.9746 (1.7795)	CeLoss 0.1924 (0.2967)	SegCLSLoss 0.0217 (0.0123)	KLLoss 0.3516 (0.3256)	MaskLoss 0.8681 (0.7221)	MaskBCELoss 0.0116 (0.1393)	MaskDICELoss 0.8565 (0.5828)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 309 is less than current step: 499. Dropping entry: {'train/loss': 1.7242367744445801, 'train/ce_loss': 0.31640625, 'train/seg_cls_loss': 0.01588134765625, 'train/kl_loss': 0.3271484375, 'train/mask_bce_loss': 0.13152616769075393, 'train/mask_dice_loss': 0.5520765990018844, 'train/mask_loss': 0.6836027592420578, 'metrics/total_secs_per_batch': 6.396697044372559, 'metrics/data_secs_per_batch': 2.8849376440048218, '_timestamp': 1740979901.3564131}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 309 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979901.3567674}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 310 is less than current step: 499. Dropping entry: {'train/loss': 1.7795356869697572, 'train/ce_loss': 0.2966796875, 'train/seg_cls_loss': 0.01231689453125, 'train/kl_loss': 0.3255859375, 'train/mask_bce_loss': 0.13932256223633885, 'train/mask_dice_loss': 0.5828183352947235, 'train/mask_loss': 0.7221408873796463, 'metrics/total_secs_per_batch': 6.8070502281188965, 'metrics/data_secs_per_batch': 2.947205376625061, '_timestamp': 1740979908.1640155}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 310 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979908.1644537}).
Epoch: [8][312/500]	Time  6.188 ( 6.188)	Loss 2.0040 (1.7869)	CeLoss 0.2812 (0.5040)	SegCLSLoss 0.0093 (0.0125)	KLLoss 0.3691 (0.2545)	MaskLoss 0.8409 (0.6256)	MaskBCELoss 0.4097 (0.1844)	MaskDICELoss 0.4312 (0.4412)
Epoch: [8][313/500]	Time  6.760 ( 6.760)	Loss 1.9366 (1.9615)	CeLoss 0.2988 (0.3974)	SegCLSLoss 0.0118 (0.0167)	KLLoss 0.3633 (0.2902)	MaskLoss 0.7974 (0.7634)	MaskBCELoss 0.1232 (0.1408)	MaskDICELoss 0.6742 (0.6226)
Epoch: [8][314/500]	Time  6.278 ( 6.278)	Loss 2.3598 (1.7076)	CeLoss 0.1865 (0.2749)	SegCLSLoss 0.0243 (0.0128)	KLLoss 0.3633 (0.2932)	MaskLoss 1.0627 (0.6985)	MaskBCELoss 0.0629 (0.1146)	MaskDICELoss 0.9998 (0.5838)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 311 is less than current step: 499. Dropping entry: {'train/loss': 1.7868676900863647, 'train/ce_loss': 0.50400390625, 'train/seg_cls_loss': 0.012506103515625, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.18437376990914345, 'train/mask_dice_loss': 0.4412378013134003, 'train/mask_loss': 0.6256115704774856, 'metrics/total_secs_per_batch': 6.187808275222778, 'metrics/data_secs_per_batch': 2.5875700950622558, '_timestamp': 1740979914.354399}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 311 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979914.355432}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 312 is less than current step: 499. Dropping entry: {'train/loss': 1.9614574670791627, 'train/ce_loss': 0.39736328125, 'train/seg_cls_loss': 0.0166748046875, 'train/kl_loss': 0.290234375, 'train/mask_bce_loss': 0.14081264156848192, 'train/mask_dice_loss': 0.6225821197032928, 'train/mask_loss': 0.7633947610855103, 'metrics/total_secs_per_batch': 6.760030269622803, 'metrics/data_secs_per_batch': 3.0868372440338137, '_timestamp': 1740979921.1115456}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 312 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979921.1118786}).
Epoch: [8][315/500]	Time  5.599 ( 5.599)	Loss 2.5477 (1.4687)	CeLoss 0.2139 (0.3757)	SegCLSLoss 0.0182 (0.0120)	KLLoss 0.3594 (0.2201)	MaskLoss 1.1450 (0.5325)	MaskBCELoss 0.2872 (0.1744)	MaskDICELoss 0.8577 (0.3582)
Epoch: [8][316/500]	Time  5.985 ( 5.985)	Loss 2.1286 (1.8087)	CeLoss 0.2432 (0.3848)	SegCLSLoss 0.0110 (0.0144)	KLLoss 0.3633 (0.2891)	MaskLoss 0.9217 (0.6938)	MaskBCELoss 0.1939 (0.1334)	MaskDICELoss 0.7278 (0.5604)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 313 is less than current step: 499. Dropping entry: {'train/loss': 1.7075778603553773, 'train/ce_loss': 0.27490234375, 'train/seg_cls_loss': 0.012823486328125, 'train/kl_loss': 0.2931640625, 'train/mask_bce_loss': 0.11463310588151217, 'train/mask_dice_loss': 0.5838335514068603, 'train/mask_loss': 0.6984666645526886, 'metrics/total_secs_per_batch': 6.277513742446899, 'metrics/data_secs_per_batch': 2.7132620096206663, '_timestamp': 1740979927.3891103}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 313 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979927.389515}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 314 is less than current step: 499. Dropping entry: {'train/loss': 1.468719446659088, 'train/ce_loss': 0.375732421875, 'train/seg_cls_loss': 0.011962890625, 'train/kl_loss': 0.2201171875, 'train/mask_bce_loss': 0.17436747141182424, 'train/mask_dice_loss': 0.3581611976027489, 'train/mask_loss': 0.5325286701321602, 'metrics/total_secs_per_batch': 5.5987770557403564, 'metrics/data_secs_per_batch': 2.393943929672241, '_timestamp': 1740979932.987874}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 314 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979932.9881926}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 315 is less than current step: 499. Dropping entry: {'train/loss': 1.8086705327033996, 'train/ce_loss': 0.384765625, 'train/seg_cls_loss': 0.01444091796875, 'train/kl_loss': 0.2890625, 'train/mask_bce_loss': 0.13341317744925618, 'train/mask_dice_loss': 0.5604240626096726, 'train/mask_loss': 0.6938372313976288, 'metrics/total_secs_per_batch': 5.985199928283691, 'metrics/data_secs_per_batch': 2.27262806892395, '_timestamp': 1740979938.9733627}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 315 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979938.9737833}).
Epoch: [8][317/500]	Time  6.506 ( 6.506)	Loss 2.5342 (1.5563)	CeLoss 0.1943 (0.1846)	SegCLSLoss 0.0209 (0.0168)	KLLoss 0.3691 (0.2906)	MaskLoss 1.1460 (0.6672)	MaskBCELoss 0.4070 (0.1800)	MaskDICELoss 0.7390 (0.4872)
Epoch: [8][318/500]	Time  6.746 ( 6.746)	Loss 1.6409 (1.9354)	CeLoss 0.2539 (0.3605)	SegCLSLoss 0.0142 (0.0174)	KLLoss 0.3633 (0.2914)	MaskLoss 0.6710 (0.7684)	MaskBCELoss 0.0188 (0.1475)	MaskDICELoss 0.6523 (0.6210)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 316 is less than current step: 499. Dropping entry: {'train/loss': 1.5563208729028701, 'train/ce_loss': 0.1845703125, 'train/seg_cls_loss': 0.01676025390625, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.17997364137554542, 'train/mask_dice_loss': 0.4872004553675652, 'train/mask_loss': 0.6671741023659706, 'metrics/total_secs_per_batch': 6.50634503364563, 'metrics/data_secs_per_batch': 3.0197530031204223, '_timestamp': 1740979945.4793305}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 316 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979945.4796138}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 317 is less than current step: 499. Dropping entry: {'train/loss': 1.9354222893714905, 'train/ce_loss': 0.360546875, 'train/seg_cls_loss': 0.01744384765625, 'train/kl_loss': 0.29140625, 'train/mask_bce_loss': 0.14746506623923777, 'train/mask_dice_loss': 0.6209784731268883, 'train/mask_loss': 0.7684435307979584, 'metrics/total_secs_per_batch': 6.745709419250488, 'metrics/data_secs_per_batch': 2.778132939338684, '_timestamp': 1740979952.2263837}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 317 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979952.226893}).
Epoch: [8][319/500]	Time  6.681 ( 6.681)	Loss 1.0000 (1.6202)	CeLoss 1.0000 (0.3094)	SegCLSLoss 0.0000 (0.0140)	KLLoss 0.0000 (0.2553)	MaskLoss 0.0000 (0.6391)	MaskBCELoss 0.0000 (0.1389)	MaskDICELoss 0.0000 (0.5001)
[2025-03-02 23:32:44,110] [INFO] [logging.py:96:log_dist] [Rank 0] step=4320, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:32:44,115] [INFO] [timer.py:215:stop] epoch=0/micro_step=43200/global_step=4320, RunningAvgSamplesPerSec=1.4880858413824194, CurrSamplesPerSec=1.9199657525640146, MemAllocated=31.56GB, MaxMemAllocated=37.23GB
Epoch: [8][320/500]	Time  5.210 ( 5.210)	Loss 1.8749 (2.2051)	CeLoss 0.2051 (0.4566)	SegCLSLoss 0.0184 (0.0168)	KLLoss 0.3613 (0.2936)	MaskLoss 0.8124 (0.8554)	MaskBCELoss 0.0286 (0.2291)	MaskDICELoss 0.7838 (0.6263)
Epoch: [8][321/500]	Time  5.370 ( 5.370)	Loss 0.0845 (1.5535)	CeLoss 0.0845 (0.3949)	SegCLSLoss 0.0000 (0.0103)	KLLoss 0.0000 (0.2197)	MaskLoss 0.0000 (0.5657)	MaskBCELoss 0.0000 (0.1020)	MaskDICELoss 0.0000 (0.4637)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 318 is less than current step: 499. Dropping entry: {'train/loss': 1.6202454328536988, 'train/ce_loss': 0.309423828125, 'train/seg_cls_loss': 0.01400146484375, 'train/kl_loss': 0.2552734375, 'train/mask_bce_loss': 0.13890385292470456, 'train/mask_dice_loss': 0.5001495182514191, 'train/mask_loss': 0.6390533745288849, 'metrics/total_secs_per_batch': 6.680912017822266, 'metrics/data_secs_per_batch': 3.6445902824401855, '_timestamp': 1740979958.9060066}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 318 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979958.9062235}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 319 is less than current step: 499. Dropping entry: {'train/loss': 2.205114471912384, 'train/ce_loss': 0.456640625, 'train/seg_cls_loss': 0.0168212890625, 'train/kl_loss': 0.2935546875, 'train/mask_bce_loss': 0.22912145368754863, 'train/mask_dice_loss': 0.626267820596695, 'train/mask_loss': 0.8553892791271209, 'metrics/total_secs_per_batch': 5.210108518600464, 'metrics/data_secs_per_batch': 2.1590692520141603, '_timestamp': 1740979964.1158872}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 319 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979964.1161785}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 320 is less than current step: 499. Dropping entry: {'train/loss': 1.553489339351654, 'train/ce_loss': 0.394873046875, 'train/seg_cls_loss': 0.010308837890625, 'train/kl_loss': 0.2197265625, 'train/mask_bce_loss': 0.10202820776030422, 'train/mask_dice_loss': 0.46365689039230346, 'train/mask_loss': 0.5656850934028625, 'metrics/total_secs_per_batch': 5.370349645614624, 'metrics/data_secs_per_batch': 2.4479301452636717, '_timestamp': 1740979969.4865956}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 320 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979969.4869564}).
Epoch: [8][322/500]	Time  5.519 ( 5.519)	Loss 2.0906 (1.7684)	CeLoss 0.3418 (0.4439)	SegCLSLoss 0.0131 (0.0131)	KLLoss 0.3574 (0.2891)	MaskLoss 0.8539 (0.6444)	MaskBCELoss 0.0235 (0.1270)	MaskDICELoss 0.8304 (0.5175)
Epoch: [8][323/500]	Time  6.136 ( 6.136)	Loss 1.2109 (1.7993)	CeLoss 1.2109 (0.4333)	SegCLSLoss 0.0000 (0.0116)	KLLoss 0.0000 (0.2889)	MaskLoss 0.0000 (0.6656)	MaskBCELoss 0.0000 (0.1267)	MaskDICELoss 0.0000 (0.5389)
Epoch: [8][324/500]	Time  6.400 ( 6.400)	Loss 2.3274 (1.9448)	CeLoss 0.1924 (0.2907)	SegCLSLoss 0.0222 (0.0188)	KLLoss 0.3848 (0.3307)	MaskLoss 1.0426 (0.8059)	MaskBCELoss 0.2598 (0.1770)	MaskDICELoss 0.7828 (0.6289)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 321 is less than current step: 499. Dropping entry: {'train/loss': 1.768358826637268, 'train/ce_loss': 0.4439453125, 'train/seg_cls_loss': 0.01314697265625, 'train/kl_loss': 0.2890625, 'train/mask_bce_loss': 0.1269827350974083, 'train/mask_dice_loss': 0.517450574040413, 'train/mask_loss': 0.6444333076477051, 'metrics/total_secs_per_batch': 5.519301414489746, 'metrics/data_secs_per_batch': 2.204838180541992, '_timestamp': 1740979975.0057044}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 321 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979975.005987}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 322 is less than current step: 499. Dropping entry: {'train/loss': 1.7992651224136353, 'train/ce_loss': 0.43330078125, 'train/seg_cls_loss': 0.011614990234375, 'train/kl_loss': 0.2888671875, 'train/mask_bce_loss': 0.126712928339839, 'train/mask_dice_loss': 0.5388864070177078, 'train/mask_loss': 0.6655993312597275, 'metrics/total_secs_per_batch': 6.135509014129639, 'metrics/data_secs_per_batch': 2.981471800804138, '_timestamp': 1740979981.1413732}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 322 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979981.1417158}).
Epoch: [8][325/500]	Time  5.462 ( 5.462)	Loss 1.6246 (1.7710)	CeLoss 0.3184 (0.5514)	SegCLSLoss 0.0139 (0.0107)	KLLoss 0.3672 (0.2547)	MaskLoss 0.6307 (0.5943)	MaskBCELoss 0.2549 (0.1369)	MaskDICELoss 0.3757 (0.4574)
Epoch: [8][326/500]	Time  6.319 ( 6.319)	Loss 1.2315 (1.5249)	CeLoss 0.2188 (0.4111)	SegCLSLoss 0.0157 (0.0094)	KLLoss 0.3574 (0.2531)	MaskLoss 0.4849 (0.5419)	MaskBCELoss 0.1396 (0.1652)	MaskDICELoss 0.3453 (0.3767)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 323 is less than current step: 499. Dropping entry: {'train/loss': 1.94480459690094, 'train/ce_loss': 0.29072265625, 'train/seg_cls_loss': 0.018792724609375, 'train/kl_loss': 0.3306640625, 'train/mask_bce_loss': 0.17702827649191022, 'train/mask_dice_loss': 0.6288701176643372, 'train/mask_loss': 0.8058983832597733, 'metrics/total_secs_per_batch': 6.3995256423950195, 'metrics/data_secs_per_batch': 2.8095516204833983, '_timestamp': 1740979987.5407205}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 323 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979987.54093}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 324 is less than current step: 499. Dropping entry: {'train/loss': 1.7709805727005006, 'train/ce_loss': 0.5513671875, 'train/seg_cls_loss': 0.01068115234375, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.13685862943530083, 'train/mask_dice_loss': 0.45742071270942686, 'train/mask_loss': 0.5942793309688568, 'metrics/total_secs_per_batch': 5.462019681930542, 'metrics/data_secs_per_batch': 2.4056215047836305, '_timestamp': 1740979993.0027854}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 324 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979993.0031824}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 325 is less than current step: 499. Dropping entry: {'train/loss': 1.5248639464378357, 'train/ce_loss': 0.411083984375, 'train/seg_cls_loss': 0.009393310546875, 'train/kl_loss': 0.253125, 'train/mask_bce_loss': 0.16517141535878183, 'train/mask_dice_loss': 0.3767283335328102, 'train/mask_loss': 0.5418997496366501, 'metrics/total_secs_per_batch': 6.319383144378662, 'metrics/data_secs_per_batch': 3.051028275489807, '_timestamp': 1740979999.3224404}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 325 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740979999.3228164}).
Epoch: [8][327/500]	Time  6.215 ( 6.215)	Loss 1.4082 (1.4076)	CeLoss 0.1641 (0.2860)	SegCLSLoss 0.0322 (0.0126)	KLLoss 0.3691 (0.2902)	MaskLoss 0.5957 (0.5431)	MaskBCELoss 0.0309 (0.0825)	MaskDICELoss 0.5648 (0.4606)
Epoch: [8][328/500]	Time  6.383 ( 6.383)	Loss 0.1123 (1.5235)	CeLoss 0.1123 (0.4440)	SegCLSLoss 0.0000 (0.0114)	KLLoss 0.0000 (0.2500)	MaskLoss 0.0000 (0.5245)	MaskBCELoss 0.0000 (0.0324)	MaskDICELoss 0.0000 (0.4921)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 326 is less than current step: 499. Dropping entry: {'train/loss': 1.4075763046741485, 'train/ce_loss': 0.285986328125, 'train/seg_cls_loss': 0.01258544921875, 'train/kl_loss': 0.290234375, 'train/mask_bce_loss': 0.08250540383160114, 'train/mask_dice_loss': 0.4606138102710247, 'train/mask_loss': 0.5431192070245743, 'metrics/total_secs_per_batch': 6.215013265609741, 'metrics/data_secs_per_batch': 2.786495804786682, '_timestamp': 1740980005.5371778}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 326 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980005.537456}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 327 is less than current step: 499. Dropping entry: {'train/loss': 1.5235423922538758, 'train/ce_loss': 0.44404296875, 'train/seg_cls_loss': 0.011407470703125, 'train/kl_loss': 0.25, 'train/mask_bce_loss': 0.032394667621701956, 'train/mask_dice_loss': 0.4920718505978584, 'train/mask_loss': 0.5244665205478668, 'metrics/total_secs_per_batch': 6.383242845535278, 'metrics/data_secs_per_batch': 2.502441573143005, '_timestamp': 1740980011.9205353}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 327 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980011.920869}).
Epoch: [8][329/500]	Time  6.443 ( 6.443)	Loss 0.4020 (1.6344)	CeLoss 0.2139 (0.2288)	SegCLSLoss 0.0188 (0.0146)	KLLoss 0.3594 (0.3270)	MaskLoss 0.0711 (0.6827)	MaskBCELoss 0.0414 (0.1488)	MaskDICELoss 0.0297 (0.5339)
[2025-03-02 23:33:43,166] [INFO] [logging.py:96:log_dist] [Rank 0] step=4330, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:33:43,172] [INFO] [timer.py:215:stop] epoch=0/micro_step=43300/global_step=4330, RunningAvgSamplesPerSec=1.4885032968134588, CurrSamplesPerSec=2.0796940086444344, MemAllocated=30.69GB, MaxMemAllocated=37.23GB
Epoch: [8][330/500]	Time  4.810 ( 4.810)	Loss 1.3125 (1.6666)	CeLoss 1.3125 (0.5104)	SegCLSLoss 0.0000 (0.0129)	KLLoss 0.0000 (0.2531)	MaskLoss 0.0000 (0.5623)	MaskBCELoss 0.0000 (0.1204)	MaskDICELoss 0.0000 (0.4419)
Epoch: [8][331/500]	Time  5.534 ( 5.534)	Loss 0.1167 (1.4734)	CeLoss 0.1167 (0.4303)	SegCLSLoss 0.0000 (0.0100)	KLLoss 0.0000 (0.1842)	MaskLoss 0.0000 (0.5098)	MaskBCELoss 0.0000 (0.1024)	MaskDICELoss 0.0000 (0.4074)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 328 is less than current step: 499. Dropping entry: {'train/loss': 1.6343959718942642, 'train/ce_loss': 0.22880859375, 'train/seg_cls_loss': 0.014599609375, 'train/kl_loss': 0.326953125, 'train/mask_bce_loss': 0.14880757499486208, 'train/mask_dice_loss': 0.533917760848999, 'train/mask_loss': 0.6827253356575966, 'metrics/total_secs_per_batch': 6.442565202713013, 'metrics/data_secs_per_batch': 2.8604277610778808, '_timestamp': 1740980018.3630333}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 328 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980018.363257}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 329 is less than current step: 499. Dropping entry: {'train/loss': 1.6665666222572326, 'train/ce_loss': 0.5103515625, 'train/seg_cls_loss': 0.01285400390625, 'train/kl_loss': 0.253125, 'train/mask_bce_loss': 0.12043092735111713, 'train/mask_dice_loss': 0.44185630828142164, 'train/mask_loss': 0.5622872292995453, 'metrics/total_secs_per_batch': 4.810057640075684, 'metrics/data_secs_per_batch': 2.1069604396820067, '_timestamp': 1740980023.1728501}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 329 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980023.173126}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 330 is less than current step: 499. Dropping entry: {'train/loss': 1.4733652949333191, 'train/ce_loss': 0.4302734375, 'train/seg_cls_loss': 0.010040283203125, 'train/kl_loss': 0.1841796875, 'train/mask_bce_loss': 0.10237979050725698, 'train/mask_dice_loss': 0.40739854872226716, 'train/mask_loss': 0.5097783386707306, 'metrics/total_secs_per_batch': 5.534356117248535, 'metrics/data_secs_per_batch': 2.34415717124939, '_timestamp': 1740980028.7074232}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 330 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980028.707717}).
Epoch: [8][332/500]	Time  6.091 ( 6.091)	Loss 2.1067 (1.5728)	CeLoss 0.2393 (0.3704)	SegCLSLoss 0.0138 (0.0100)	KLLoss 0.3652 (0.2184)	MaskLoss 0.9117 (0.5878)	MaskBCELoss 0.0415 (0.1657)	MaskDICELoss 0.8702 (0.4221)
Epoch: [8][333/500]	Time  5.160 ( 5.160)	Loss 0.0588 (1.1081)	CeLoss 0.0588 (0.5312)	SegCLSLoss 0.0000 (0.0063)	KLLoss 0.0000 (0.1447)	MaskLoss 0.0000 (0.2798)	MaskBCELoss 0.0000 (0.1057)	MaskDICELoss 0.0000 (0.1741)
Epoch: [8][334/500]	Time  6.286 ( 6.286)	Loss 2.1632 (1.4778)	CeLoss 0.2197 (0.2649)	SegCLSLoss 0.0155 (0.0129)	KLLoss 0.3652 (0.2891)	MaskLoss 0.9498 (0.5887)	MaskBCELoss 0.0849 (0.0688)	MaskDICELoss 0.8649 (0.5199)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 331 is less than current step: 499. Dropping entry: {'train/loss': 1.5728341102600099, 'train/ce_loss': 0.37041015625, 'train/seg_cls_loss': 0.01002197265625, 'train/kl_loss': 0.218359375, 'train/mask_bce_loss': 0.16565525569021702, 'train/mask_dice_loss': 0.42212897539138794, 'train/mask_loss': 0.5877842366695404, 'metrics/total_secs_per_batch': 6.090512752532959, 'metrics/data_secs_per_batch': 2.7235415458679197, '_timestamp': 1740980034.7979815}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 331 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980034.7983575}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 332 is less than current step: 499. Dropping entry: {'train/loss': 1.1081113457679748, 'train/ce_loss': 0.5311767578125, 'train/seg_cls_loss': 0.00626220703125, 'train/kl_loss': 0.1447265625, 'train/mask_bce_loss': 0.10568718761205673, 'train/mask_dice_loss': 0.174088691174984, 'train/mask_loss': 0.2797758877277374, 'metrics/total_secs_per_batch': 5.160008192062378, 'metrics/data_secs_per_batch': 2.4866583585739135, '_timestamp': 1740980039.9581182}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 332 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980039.9584677}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 333 is less than current step: 499. Dropping entry: {'train/loss': 1.477776539325714, 'train/ce_loss': 0.2649169921875, 'train/seg_cls_loss': 0.012939453125, 'train/kl_loss': 0.2890625, 'train/mask_bce_loss': 0.06882033180445432, 'train/mask_dice_loss': 0.5198848217725753, 'train/mask_loss': 0.588705164194107, 'metrics/total_secs_per_batch': 6.285712003707886, 'metrics/data_secs_per_batch': 2.7076325178146363, '_timestamp': 1740980046.2436588}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 333 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980046.2439451}).
Epoch: [8][335/500]	Time  5.915 ( 5.915)	Loss 0.0742 (1.3782)	CeLoss 0.0742 (0.3593)	SegCLSLoss 0.0000 (0.0105)	KLLoss 0.0000 (0.2193)	MaskLoss 0.0000 (0.4957)	MaskBCELoss 0.0000 (0.0963)	MaskDICELoss 0.0000 (0.3994)
Epoch: [8][336/500]	Time  5.015 ( 5.015)	Loss 1.2756 (1.4178)	CeLoss 0.2695 (0.6477)	SegCLSLoss 0.0110 (0.0091)	KLLoss 0.3652 (0.1832)	MaskLoss 0.4816 (0.3736)	MaskBCELoss 0.0629 (0.0724)	MaskDICELoss 0.4186 (0.3012)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 334 is less than current step: 499. Dropping entry: {'train/loss': 1.3781944990158081, 'train/ce_loss': 0.35927734375, 'train/seg_cls_loss': 0.01046142578125, 'train/kl_loss': 0.2193359375, 'train/mask_bce_loss': 0.09628146421164274, 'train/mask_dice_loss': 0.39940758645534513, 'train/mask_loss': 0.49568904638290406, 'metrics/total_secs_per_batch': 5.914658308029175, 'metrics/data_secs_per_batch': 2.753806805610657, '_timestamp': 1740980052.1583438}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 334 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980052.158537}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 335 is less than current step: 499. Dropping entry: {'train/loss': 1.4177743315696716, 'train/ce_loss': 0.64765625, 'train/seg_cls_loss': 0.0091064453125, 'train/kl_loss': 0.183203125, 'train/mask_bce_loss': 0.07243490107357502, 'train/mask_dice_loss': 0.30119836032390596, 'train/mask_loss': 0.37363325655460355, 'metrics/total_secs_per_batch': 5.015326261520386, 'metrics/data_secs_per_batch': 2.234299063682556, '_timestamp': 1740980057.1737075}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 335 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980057.1740072}).
Epoch: [8][337/500]	Time  6.976 ( 6.976)	Loss 2.4519 (1.8290)	CeLoss 0.1777 (0.3506)	SegCLSLoss 0.0178 (0.0150)	KLLoss 0.3691 (0.3281)	MaskLoss 1.1141 (0.7191)	MaskBCELoss 0.2516 (0.1323)	MaskDICELoss 0.8625 (0.5868)
Epoch: [8][338/500]	Time  5.345 ( 5.345)	Loss 2.0128 (1.7594)	CeLoss 0.1875 (0.6099)	SegCLSLoss 0.0237 (0.0106)	KLLoss 0.3535 (0.2156)	MaskLoss 0.8892 (0.5614)	MaskBCELoss 0.0393 (0.1438)	MaskDICELoss 0.8499 (0.4176)
Epoch: [8][339/500]	Time  5.163 ( 5.163)	Loss 0.6992 (1.3347)	CeLoss 0.6992 (0.6518)	SegCLSLoss 0.0000 (0.0092)	KLLoss 0.0000 (0.1799)	MaskLoss 0.0000 (0.3301)	MaskBCELoss 0.0000 (0.0318)	MaskDICELoss 0.0000 (0.2983)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 336 is less than current step: 499. Dropping entry: {'train/loss': 1.8290203332901, 'train/ce_loss': 0.3505859375, 'train/seg_cls_loss': 0.01495361328125, 'train/kl_loss': 0.328125, 'train/mask_bce_loss': 0.1322705391794443, 'train/mask_dice_loss': 0.5867806315422058, 'train/mask_loss': 0.7190511763095856, 'metrics/total_secs_per_batch': 6.9762749671936035, 'metrics/data_secs_per_batch': 3.0732956409454344, '_timestamp': 1740980064.1499078}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 336 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980064.1502144}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 337 is less than current step: 499. Dropping entry: {'train/loss': 1.7594055533409119, 'train/ce_loss': 0.60986328125, 'train/seg_cls_loss': 0.010589599609375, 'train/kl_loss': 0.215625, 'train/mask_bce_loss': 0.1437974702566862, 'train/mask_dice_loss': 0.4175947517156601, 'train/mask_loss': 0.5613922297954559, 'metrics/total_secs_per_batch': 5.344573259353638, 'metrics/data_secs_per_batch': 2.338597822189331, '_timestamp': 1740980069.4947464}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 337 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980069.4951081}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 338 is less than current step: 499. Dropping entry: {'train/loss': 1.334712064266205, 'train/ce_loss': 0.6517578125, 'train/seg_cls_loss': 0.00919189453125, 'train/kl_loss': 0.1798828125, 'train/mask_bce_loss': 0.031798708438873294, 'train/mask_dice_loss': 0.29825263321399687, 'train/mask_loss': 0.33005134463310243, 'metrics/total_secs_per_batch': 5.1629321575164795, 'metrics/data_secs_per_batch': 2.342977023124695, '_timestamp': 1740980074.6574314}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 338 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980074.6577163}).
[2025-03-02 23:34:39,594] [INFO] [logging.py:96:log_dist] [Rank 0] step=4340, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:34:39,600] [INFO] [timer.py:215:stop] epoch=0/micro_step=43400/global_step=4340, RunningAvgSamplesPerSec=1.4890533696679773, CurrSamplesPerSec=2.023252597706153, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [8][340/500]	Time  4.944 ( 4.944)	Loss 1.8958 (1.6845)	CeLoss 0.2412 (0.4537)	SegCLSLoss 0.0177 (0.0138)	KLLoss 0.3691 (0.2572)	MaskLoss 0.8044 (0.5990)	MaskBCELoss 0.0912 (0.1488)	MaskDICELoss 0.7132 (0.4502)
Epoch: [8][341/500]	Time  5.385 ( 5.385)	Loss 1.8577 (1.6077)	CeLoss 0.2061 (0.6751)	SegCLSLoss 0.0170 (0.0091)	KLLoss 0.3516 (0.1828)	MaskLoss 0.8039 (0.4548)	MaskBCELoss 0.0422 (0.0702)	MaskDICELoss 0.7617 (0.3846)
Epoch: [8][342/500]	Time  5.580 ( 5.580)	Loss 2.6345 (2.0786)	CeLoss 0.1650 (0.3479)	SegCLSLoss 0.0183 (0.0202)	KLLoss 0.3945 (0.3305)	MaskLoss 1.2108 (0.8439)	MaskBCELoss 0.3233 (0.2257)	MaskDICELoss 0.8875 (0.6182)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 339 is less than current step: 499. Dropping entry: {'train/loss': 1.6844667196273804, 'train/ce_loss': 0.4537109375, 'train/seg_cls_loss': 0.013848876953125, 'train/kl_loss': 0.2572265625, 'train/mask_bce_loss': 0.14881847575306892, 'train/mask_dice_loss': 0.4502019867300987, 'train/mask_loss': 0.5990204572677612, 'metrics/total_secs_per_batch': 4.944103956222534, 'metrics/data_secs_per_batch': 2.1085590362548827, '_timestamp': 1740980079.6013374}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 339 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980079.6016037}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 340 is less than current step: 499. Dropping entry: {'train/loss': 1.607665979862213, 'train/ce_loss': 0.67509765625, 'train/seg_cls_loss': 0.009136962890625, 'train/kl_loss': 0.1828125, 'train/mask_bce_loss': 0.07017798973247409, 'train/mask_dice_loss': 0.3846315562725067, 'train/mask_loss': 0.45480955243110655, 'metrics/total_secs_per_batch': 5.384736061096191, 'metrics/data_secs_per_batch': 2.012536954879761, '_timestamp': 1740980084.986303}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 340 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980084.9866629}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 341 is less than current step: 499. Dropping entry: {'train/loss': 2.078557825088501, 'train/ce_loss': 0.3478515625, 'train/seg_cls_loss': 0.020159912109375, 'train/kl_loss': 0.33046875, 'train/mask_bce_loss': 0.22570125125348567, 'train/mask_dice_loss': 0.6182163238525391, 'train/mask_loss': 0.8439175844192505, 'metrics/total_secs_per_batch': 5.579596042633057, 'metrics/data_secs_per_batch': 2.2313318729400633, '_timestamp': 1740980090.566081}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 341 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980090.5664291}).
Epoch: [8][343/500]	Time  5.596 ( 5.596)	Loss 2.5435 (1.9290)	CeLoss 0.2559 (0.3569)	SegCLSLoss 0.0108 (0.0141)	KLLoss 0.3652 (0.3307)	MaskLoss 1.1223 (0.7661)	MaskBCELoss 0.3498 (0.1650)	MaskDICELoss 0.7725 (0.6011)
Epoch: [8][344/500]	Time  5.559 ( 5.559)	Loss 1.9987 (1.5685)	CeLoss 0.2334 (0.4408)	SegCLSLoss 0.0188 (0.0107)	KLLoss 0.3555 (0.2520)	MaskLoss 0.8597 (0.5485)	MaskBCELoss 0.0123 (0.1388)	MaskDICELoss 0.8474 (0.4097)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 342 is less than current step: 499. Dropping entry: {'train/loss': 1.9290373265743255, 'train/ce_loss': 0.35693359375, 'train/seg_cls_loss': 0.014080810546875, 'train/kl_loss': 0.3306640625, 'train/mask_bce_loss': 0.16498433849774302, 'train/mask_dice_loss': 0.6010968387126923, 'train/mask_loss': 0.7660811811685562, 'metrics/total_secs_per_batch': 5.595768213272095, 'metrics/data_secs_per_batch': 2.6146477699279784, '_timestamp': 1740980096.1616724}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 342 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980096.1619747}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 343 is less than current step: 499. Dropping entry: {'train/loss': 1.5684756636619568, 'train/ce_loss': 0.440771484375, 'train/seg_cls_loss': 0.010662841796875, 'train/kl_loss': 0.251953125, 'train/mask_bce_loss': 0.13878737818449735, 'train/mask_dice_loss': 0.409683857858181, 'train/mask_loss': 0.5484712302684784, 'metrics/total_secs_per_batch': 5.558515548706055, 'metrics/data_secs_per_batch': 2.5996461153030395, '_timestamp': 1740980101.720181}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 343 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980101.7205584}).
Epoch: [8][345/500]	Time  6.607 ( 6.607)	Loss 1.9876 (2.2261)	CeLoss 0.2324 (0.2121)	SegCLSLoss 0.0181 (0.0173)	KLLoss 0.3652 (0.3713)	MaskLoss 0.8551 (0.9843)	MaskBCELoss 0.0400 (0.2723)	MaskDICELoss 0.8151 (0.7120)
Epoch: [8][346/500]	Time  5.955 ( 5.955)	Loss 3.3576 (1.7882)	CeLoss 0.2871 (0.1943)	SegCLSLoss 0.0140 (0.0155)	KLLoss 0.3633 (0.2930)	MaskLoss 1.5128 (0.7783)	MaskBCELoss 0.9250 (0.2266)	MaskDICELoss 0.5878 (0.5517)
Epoch: [8][347/500]	Time  6.868 ( 6.868)	Loss 1.7349 (1.9494)	CeLoss 0.2305 (0.3046)	SegCLSLoss 0.0194 (0.0174)	KLLoss 0.3594 (0.3289)	MaskLoss 0.7297 (0.8017)	MaskBCELoss 0.0455 (0.1853)	MaskDICELoss 0.6843 (0.6164)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 344 is less than current step: 499. Dropping entry: {'train/loss': 2.226118564605713, 'train/ce_loss': 0.212109375, 'train/seg_cls_loss': 0.017279052734375, 'train/kl_loss': 0.3712890625, 'train/mask_bce_loss': 0.27231356212869284, 'train/mask_dice_loss': 0.7120347887277603, 'train/mask_loss': 0.984348350763321, 'metrics/total_secs_per_batch': 6.6074981689453125, 'metrics/data_secs_per_batch': 2.9424354076385497, '_timestamp': 1740980108.3276744}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 344 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980108.3279517}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 345 is less than current step: 499. Dropping entry: {'train/loss': 1.7881529331207275, 'train/ce_loss': 0.194287109375, 'train/seg_cls_loss': 0.0154541015625, 'train/kl_loss': 0.29296875, 'train/mask_bce_loss': 0.22658322639763356, 'train/mask_dice_loss': 0.5517461568117141, 'train/mask_loss': 0.7783293843269348, 'metrics/total_secs_per_batch': 5.95478630065918, 'metrics/data_secs_per_batch': 2.5971185684204103, '_timestamp': 1740980114.2824621}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 345 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980114.2827551}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 346 is less than current step: 499. Dropping entry: {'train/loss': 1.9494417548179626, 'train/ce_loss': 0.30458984375, 'train/seg_cls_loss': 0.0173828125, 'train/kl_loss': 0.32890625, 'train/mask_bce_loss': 0.18529741317033768, 'train/mask_dice_loss': 0.6164254307746887, 'train/mask_loss': 0.8017228305339813, 'metrics/total_secs_per_batch': 6.867528915405273, 'metrics/data_secs_per_batch': 3.313759970664978, '_timestamp': 1740980121.1501653}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 346 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980121.150579}).
Epoch: [8][348/500]	Time  5.856 ( 5.856)	Loss 1.4326 (1.5049)	CeLoss 0.2656 (0.3924)	SegCLSLoss 0.0124 (0.0126)	KLLoss 0.3633 (0.2562)	MaskLoss 0.5620 (0.5403)	MaskBCELoss 0.2038 (0.0847)	MaskDICELoss 0.3582 (0.4556)
Epoch: [8][349/500]	Time  5.627 ( 5.627)	Loss 1.1172 (1.4315)	CeLoss 1.1172 (0.4636)	SegCLSLoss 0.0000 (0.0095)	KLLoss 0.0000 (0.2174)	MaskLoss 0.0000 (0.4706)	MaskBCELoss 0.0000 (0.0686)	MaskDICELoss 0.0000 (0.4020)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 347 is less than current step: 499. Dropping entry: {'train/loss': 1.5048726439476012, 'train/ce_loss': 0.3923828125, 'train/seg_cls_loss': 0.012628173828125, 'train/kl_loss': 0.25625, 'train/mask_bce_loss': 0.08466323465108871, 'train/mask_dice_loss': 0.4555904626846313, 'train/mask_loss': 0.5402537047863006, 'metrics/total_secs_per_batch': 5.856045484542847, 'metrics/data_secs_per_batch': 2.7736822843551634, '_timestamp': 1740980127.0060947}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 347 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980127.0063918}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 348 is less than current step: 499. Dropping entry: {'train/loss': 1.4315107345581055, 'train/ce_loss': 0.4636474609375, 'train/seg_cls_loss': 0.00946044921875, 'train/kl_loss': 0.2173828125, 'train/mask_bce_loss': 0.06858135126531124, 'train/mask_dice_loss': 0.40197136998176575, 'train/mask_loss': 0.47055272459983827, 'metrics/total_secs_per_batch': 5.6265881061553955, 'metrics/data_secs_per_batch': 2.7478561878204344, '_timestamp': 1740980132.6327116}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 348 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980132.6330175}).
[2025-03-02 23:35:39,113] [INFO] [logging.py:96:log_dist] [Rank 0] step=4350, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:35:39,120] [INFO] [timer.py:215:stop] epoch=0/micro_step=43500/global_step=4350, RunningAvgSamplesPerSec=1.4894436027475275, CurrSamplesPerSec=1.5416477527720265, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [8][350/500]	Time  6.488 ( 6.488)	Loss 1.8372 (1.6644)	CeLoss 0.2754 (0.3340)	SegCLSLoss 0.0166 (0.0140)	KLLoss 0.3594 (0.3293)	MaskLoss 0.7594 (0.6453)	MaskBCELoss 0.0369 (0.0720)	MaskDICELoss 0.7225 (0.5734)
Epoch: [8][351/500]	Time  6.108 ( 6.108)	Loss 2.4545 (1.8489)	CeLoss 0.1836 (0.2654)	SegCLSLoss 0.0229 (0.0152)	KLLoss 0.3672 (0.3266)	MaskLoss 1.1110 (0.7713)	MaskBCELoss 0.1114 (0.1727)	MaskDICELoss 0.9997 (0.5986)
Epoch: [8][352/500]	Time  6.475 ( 6.475)	Loss 1.8164 (1.8671)	CeLoss 0.1982 (0.5320)	SegCLSLoss 0.0197 (0.0152)	KLLoss 0.3496 (0.2863)	MaskLoss 0.7866 (0.6495)	MaskBCELoss 0.0092 (0.1207)	MaskDICELoss 0.7775 (0.5288)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 349 is less than current step: 499. Dropping entry: {'train/loss': 1.6644193172454833, 'train/ce_loss': 0.333984375, 'train/seg_cls_loss': 0.013970947265625, 'train/kl_loss': 0.329296875, 'train/mask_bce_loss': 0.07198159694671631, 'train/mask_dice_loss': 0.5733628153800965, 'train/mask_loss': 0.645344415307045, 'metrics/total_secs_per_batch': 6.488222122192383, 'metrics/data_secs_per_batch': 2.9906970262527466, '_timestamp': 1740980139.120661}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 349 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980139.1209323}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 350 is less than current step: 499. Dropping entry: {'train/loss': 1.8488516628742218, 'train/ce_loss': 0.2654296875, 'train/seg_cls_loss': 0.015234375, 'train/kl_loss': 0.3265625, 'train/mask_bce_loss': 0.17270783819258212, 'train/mask_dice_loss': 0.5985929891467094, 'train/mask_loss': 0.7713008314371109, 'metrics/total_secs_per_batch': 6.107793807983398, 'metrics/data_secs_per_batch': 2.57806282043457, '_timestamp': 1740980145.228647}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 350 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980145.2289221}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 351 is less than current step: 499. Dropping entry: {'train/loss': 1.867068588733673, 'train/ce_loss': 0.53203125, 'train/seg_cls_loss': 0.015203857421875, 'train/kl_loss': 0.286328125, 'train/mask_bce_loss': 0.12071984773501754, 'train/mask_dice_loss': 0.5287812411785126, 'train/mask_loss': 0.6495010882616044, 'metrics/total_secs_per_batch': 6.475334167480469, 'metrics/data_secs_per_batch': 2.4408897876739504, '_timestamp': 1740980151.7040079}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 351 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980151.7042942}).
Epoch: [8][353/500]	Time  6.325 ( 6.325)	Loss 0.4102 (1.6666)	CeLoss 0.4102 (0.4834)	SegCLSLoss 0.0000 (0.0119)	KLLoss 0.0000 (0.2537)	MaskLoss 0.0000 (0.5758)	MaskBCELoss 0.0000 (0.1130)	MaskDICELoss 0.0000 (0.4629)
Epoch: [8][354/500]	Time  5.739 ( 5.739)	Loss 1.5369 (1.5230)	CeLoss 0.2178 (0.4292)	SegCLSLoss 0.0123 (0.0110)	KLLoss 0.3633 (0.2916)	MaskLoss 0.6386 (0.5297)	MaskBCELoss 0.0287 (0.1306)	MaskDICELoss 0.6099 (0.3991)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 352 is less than current step: 499. Dropping entry: {'train/loss': 1.6665921330451965, 'train/ce_loss': 0.4833984375, 'train/seg_cls_loss': 0.011920166015625, 'train/kl_loss': 0.2537109375, 'train/mask_bce_loss': 0.1129652488976717, 'train/mask_dice_loss': 0.4628600999712944, 'train/mask_loss': 0.5758253574371338, 'metrics/total_secs_per_batch': 6.32474160194397, 'metrics/data_secs_per_batch': 3.086618971824646, '_timestamp': 1740980158.028893}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 352 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980158.029224}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 353 is less than current step: 499. Dropping entry: {'train/loss': 1.523025244474411, 'train/ce_loss': 0.42919921875, 'train/seg_cls_loss': 0.01104736328125, 'train/kl_loss': 0.2916015625, 'train/mask_bce_loss': 0.13056825269013644, 'train/mask_dice_loss': 0.39910842925310136, 'train/mask_loss': 0.5296766817569732, 'metrics/total_secs_per_batch': 5.739177227020264, 'metrics/data_secs_per_batch': 2.410635471343994, '_timestamp': 1740980163.767981}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 353 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980163.7682762}).
Epoch: [8][355/500]	Time  5.658 ( 5.658)	Loss 1.2031 (1.5974)	CeLoss 1.2031 (0.5309)	SegCLSLoss 0.0000 (0.0130)	KLLoss 0.0000 (0.2574)	MaskLoss 0.0000 (0.5171)	MaskBCELoss 0.0000 (0.1049)	MaskDICELoss 0.0000 (0.4122)
Epoch: [8][356/500]	Time  6.003 ( 6.003)	Loss 2.3762 (1.8728)	CeLoss 0.2021 (0.4053)	SegCLSLoss 0.0173 (0.0137)	KLLoss 0.3633 (0.2928)	MaskLoss 1.0646 (0.7156)	MaskBCELoss 0.6779 (0.2071)	MaskDICELoss 0.3867 (0.5085)
Epoch: [8][357/500]	Time  6.446 ( 6.446)	Loss 1.7564 (1.8535)	CeLoss 0.1953 (0.2033)	SegCLSLoss 0.0183 (0.0183)	KLLoss 0.3633 (0.3668)	MaskLoss 0.7581 (0.8022)	MaskBCELoss 0.0279 (0.1596)	MaskDICELoss 0.7302 (0.6425)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 354 is less than current step: 499. Dropping entry: {'train/loss': 1.5973510503768922, 'train/ce_loss': 0.530859375, 'train/seg_cls_loss': 0.01295166015625, 'train/kl_loss': 0.257421875, 'train/mask_bce_loss': 0.10489045158028602, 'train/mask_dice_loss': 0.41224210560321806, 'train/mask_loss': 0.5171325474977493, 'metrics/total_secs_per_batch': 5.658263921737671, 'metrics/data_secs_per_batch': 2.7401870012283327, '_timestamp': 1740980169.4262428}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 354 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980169.426521}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 355 is less than current step: 499. Dropping entry: {'train/loss': 1.8727547883987428, 'train/ce_loss': 0.4052734375, 'train/seg_cls_loss': 0.01365966796875, 'train/kl_loss': 0.2927734375, 'train/mask_bce_loss': 0.20714574102312328, 'train/mask_dice_loss': 0.5084797084331513, 'train/mask_loss': 0.7156254470348358, 'metrics/total_secs_per_batch': 6.002667427062988, 'metrics/data_secs_per_batch': 2.519080948829651, '_timestamp': 1740980175.4292247}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 355 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980175.4296377}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 356 is less than current step: 499. Dropping entry: {'train/loss': 1.8534963488578797, 'train/ce_loss': 0.2033203125, 'train/seg_cls_loss': 0.018328857421875, 'train/kl_loss': 0.366796875, 'train/mask_bce_loss': 0.15964713040739298, 'train/mask_dice_loss': 0.6425404965877533, 'train/mask_loss': 0.8021876215934753, 'metrics/total_secs_per_batch': 6.445976734161377, 'metrics/data_secs_per_batch': 2.9650356769561768, '_timestamp': 1740980181.8749642}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 356 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980181.8752928}).
Epoch: [8][358/500]	Time  5.962 ( 5.962)	Loss 1.2812 (1.9825)	CeLoss 1.2812 (0.5225)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.2562)	MaskLoss 0.0000 (0.7137)	MaskBCELoss 0.0000 (0.0957)	MaskDICELoss 0.0000 (0.6180)
Epoch: [8][359/500]	Time  5.663 ( 5.663)	Loss 1.8720 (1.7110)	CeLoss 0.2852 (0.3372)	SegCLSLoss 0.0134 (0.0169)	KLLoss 0.3594 (0.2842)	MaskLoss 0.7729 (0.6685)	MaskBCELoss 0.0453 (0.0498)	MaskDICELoss 0.7277 (0.6188)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 357 is less than current step: 499. Dropping entry: {'train/loss': 1.982512903213501, 'train/ce_loss': 0.5224609375, 'train/seg_cls_loss': 0.014178466796875, 'train/kl_loss': 0.25625, 'train/mask_bce_loss': 0.09574614372104406, 'train/mask_dice_loss': 0.6179712355136872, 'train/mask_loss': 0.7137173712253571, 'metrics/total_secs_per_batch': 5.96204948425293, 'metrics/data_secs_per_batch': 2.978225517272949, '_timestamp': 1740980187.837859}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 357 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980187.8384984}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 358 is less than current step: 499. Dropping entry: {'train/loss': 1.710999321937561, 'train/ce_loss': 0.33720703125, 'train/seg_cls_loss': 0.01689453125, 'train/kl_loss': 0.2841796875, 'train/mask_bce_loss': 0.04975561643950641, 'train/mask_dice_loss': 0.6187811464071273, 'train/mask_loss': 0.6685367614030838, 'metrics/total_secs_per_batch': 5.662681341171265, 'metrics/data_secs_per_batch': 2.27644305229187, '_timestamp': 1740980193.499959}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 358 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980193.5004382}).
[2025-03-02 23:36:39,143] [INFO] [logging.py:96:log_dist] [Rank 0] step=4360, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:36:39,148] [INFO] [timer.py:215:stop] epoch=0/micro_step=43600/global_step=4360, RunningAvgSamplesPerSec=1.489806428441951, CurrSamplesPerSec=1.770634961342461, MemAllocated=31.23GB, MaxMemAllocated=37.23GB
Epoch: [8][360/500]	Time  5.650 ( 5.650)	Loss 1.3712 (1.6381)	CeLoss 0.2852 (0.6109)	SegCLSLoss 0.0099 (0.0076)	KLLoss 0.3613 (0.2555)	MaskLoss 0.5216 (0.4987)	MaskBCELoss 0.0962 (0.1069)	MaskDICELoss 0.4254 (0.3918)
Epoch: [8][361/500]	Time  6.141 ( 6.141)	Loss 2.3155 (1.6492)	CeLoss 0.1992 (0.3268)	SegCLSLoss 0.0176 (0.0154)	KLLoss 0.3613 (0.3311)	MaskLoss 1.0357 (0.6407)	MaskBCELoss 0.2210 (0.1460)	MaskDICELoss 0.8147 (0.4947)
Epoch: [8][362/500]	Time  6.272 ( 6.272)	Loss 1.2500 (1.2366)	CeLoss 1.2500 (0.3112)	SegCLSLoss 0.0000 (0.0139)	KLLoss 0.0000 (0.2906)	MaskLoss 0.0000 (0.4447)	MaskBCELoss 0.0000 (0.0727)	MaskDICELoss 0.0000 (0.3720)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 359 is less than current step: 499. Dropping entry: {'train/loss': 1.6381066679954528, 'train/ce_loss': 0.6109375, 'train/seg_cls_loss': 0.00758056640625, 'train/kl_loss': 0.25546875, 'train/mask_bce_loss': 0.10694055831991137, 'train/mask_dice_loss': 0.3918002635240555, 'train/mask_loss': 0.49874082803726194, 'metrics/total_secs_per_batch': 5.649942398071289, 'metrics/data_secs_per_batch': 2.210138201713562, '_timestamp': 1740980199.1493425}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 359 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980199.1495438}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 360 is less than current step: 499. Dropping entry: {'train/loss': 1.6492215037345885, 'train/ce_loss': 0.3267578125, 'train/seg_cls_loss': 0.01544189453125, 'train/kl_loss': 0.3310546875, 'train/mask_bce_loss': 0.14599588625133036, 'train/mask_dice_loss': 0.4947281539440155, 'train/mask_loss': 0.6407240271568299, 'metrics/total_secs_per_batch': 6.141128063201904, 'metrics/data_secs_per_batch': 2.4735990524291993, '_timestamp': 1740980205.2912734}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 360 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980205.291919}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 361 is less than current step: 499. Dropping entry: {'train/loss': 1.2366442143917085, 'train/ce_loss': 0.31123046875, 'train/seg_cls_loss': 0.013909912109375, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.07266671983525157, 'train/mask_dice_loss': 0.3720225729048252, 'train/mask_loss': 0.44468929097056387, 'metrics/total_secs_per_batch': 6.2721710205078125, 'metrics/data_secs_per_batch': 3.124027156829834, '_timestamp': 1740980211.5629175}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 361 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980211.5632803}).
Epoch: [8][363/500]	Time  7.091 ( 7.091)	Loss 2.2412 (1.4905)	CeLoss 0.1758 (0.2928)	SegCLSLoss 0.0187 (0.0111)	KLLoss 0.3711 (0.2900)	MaskLoss 1.0093 (0.5815)	MaskBCELoss 0.3095 (0.1484)	MaskDICELoss 0.6998 (0.4332)
Epoch: [8][364/500]	Time  6.695 ( 6.695)	Loss 1.4779 (2.0587)	CeLoss 0.2266 (0.2797)	SegCLSLoss 0.0126 (0.0168)	KLLoss 0.3672 (0.3287)	MaskLoss 0.6042 (0.8687)	MaskBCELoss 0.0303 (0.2461)	MaskDICELoss 0.5739 (0.6226)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 362 is less than current step: 499. Dropping entry: {'train/loss': 1.490510928630829, 'train/ce_loss': 0.2927734375, 'train/seg_cls_loss': 0.011126708984375, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.1483680946752429, 'train/mask_dice_loss': 0.43316667079925536, 'train/mask_loss': 0.5815347641706466, 'metrics/total_secs_per_batch': 7.09069299697876, 'metrics/data_secs_per_batch': 3.135926032066345, '_timestamp': 1740980218.6537602}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 362 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980218.6542096}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 363 is less than current step: 499. Dropping entry: {'train/loss': 2.058730363845825, 'train/ce_loss': 0.279736328125, 'train/seg_cls_loss': 0.016778564453125, 'train/kl_loss': 0.3287109375, 'train/mask_bce_loss': 0.2460999885573983, 'train/mask_dice_loss': 0.6226206541061401, 'train/mask_loss': 0.8687206387519837, 'metrics/total_secs_per_batch': 6.694965600967407, 'metrics/data_secs_per_batch': 3.1765586614608763, '_timestamp': 1740980225.349242}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 363 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980225.350278}).
Epoch: [8][365/500]	Time  5.698 ( 5.698)	Loss 2.9233 (1.6016)	CeLoss 0.6680 (0.4302)	SegCLSLoss 0.0134 (0.0130)	KLLoss 0.3535 (0.2531)	MaskLoss 1.1062 (0.5697)	MaskBCELoss 0.1487 (0.0951)	MaskDICELoss 0.9575 (0.4746)
Epoch: [8][366/500]	Time  6.184 ( 6.184)	Loss 1.7328 (1.6466)	CeLoss 0.2412 (0.4079)	SegCLSLoss 0.0211 (0.0131)	KLLoss 0.3594 (0.2586)	MaskLoss 0.7229 (0.6032)	MaskBCELoss 0.0222 (0.0896)	MaskDICELoss 0.7006 (0.5135)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 364 is less than current step: 499. Dropping entry: {'train/loss': 1.601617169380188, 'train/ce_loss': 0.43017578125, 'train/seg_cls_loss': 0.01304931640625, 'train/kl_loss': 0.253125, 'train/mask_bce_loss': 0.09514671349897981, 'train/mask_dice_loss': 0.4745583444833755, 'train/mask_loss': 0.5697050601243973, 'metrics/total_secs_per_batch': 5.697591543197632, 'metrics/data_secs_per_batch': 2.5928729295730593, '_timestamp': 1740980231.0461235}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 364 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980231.0464065}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 365 is less than current step: 499. Dropping entry: {'train/loss': 1.646565568447113, 'train/ce_loss': 0.4079345703125, 'train/seg_cls_loss': 0.0131103515625, 'train/kl_loss': 0.25859375, 'train/mask_bce_loss': 0.08964374586939812, 'train/mask_dice_loss': 0.5135096430778503, 'train/mask_loss': 0.6031533896923065, 'metrics/total_secs_per_batch': 6.183979272842407, 'metrics/data_secs_per_batch': 2.7171219110488893, '_timestamp': 1740980237.2300763}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 365 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980237.2303956}).
Epoch: [8][367/500]	Time  6.062 ( 6.062)	Loss 1.4853 (1.7634)	CeLoss 0.1895 (0.4619)	SegCLSLoss 0.0200 (0.0130)	KLLoss 0.3555 (0.2883)	MaskLoss 0.6255 (0.6331)	MaskBCELoss 0.1955 (0.0845)	MaskDICELoss 0.4299 (0.5486)
Epoch: [8][368/500]	Time  4.727 ( 4.727)	Loss 0.0679 (1.3629)	CeLoss 0.0679 (0.6552)	SegCLSLoss 0.0000 (0.0071)	KLLoss 0.0000 (0.1422)	MaskLoss 0.0000 (0.3449)	MaskBCELoss 0.0000 (0.0346)	MaskDICELoss 0.0000 (0.3103)
Epoch: [8][369/500]	Time  6.213 ( 6.213)	Loss 1.1610 (1.5933)	CeLoss 0.2295 (0.4453)	SegCLSLoss 0.0107 (0.0130)	KLLoss 0.3750 (0.2900)	MaskLoss 0.4448 (0.5562)	MaskBCELoss 0.2274 (0.1296)	MaskDICELoss 0.2174 (0.4266)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 366 is less than current step: 499. Dropping entry: {'train/loss': 1.7633921265602113, 'train/ce_loss': 0.4619140625, 'train/seg_cls_loss': 0.013031005859375, 'train/kl_loss': 0.28828125, 'train/mask_bce_loss': 0.08446628488600254, 'train/mask_dice_loss': 0.5485969543457031, 'train/mask_loss': 0.6330632507801056, 'metrics/total_secs_per_batch': 6.06242823600769, 'metrics/data_secs_per_batch': 2.6048809051513673, '_timestamp': 1740980243.2928567}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 366 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980243.2932918}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 367 is less than current step: 499. Dropping entry: {'train/loss': 1.3628639936447144, 'train/ce_loss': 0.655224609375, 'train/seg_cls_loss': 0.007098388671875, 'train/kl_loss': 0.1421875, 'train/mask_bce_loss': 0.0346010496839881, 'train/mask_dice_loss': 0.310331916809082, 'train/mask_loss': 0.3449329733848572, 'metrics/total_secs_per_batch': 4.727183818817139, 'metrics/data_secs_per_batch': 1.9823989629745484, '_timestamp': 1740980248.0197124}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 367 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980248.0199928}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 368 is less than current step: 499. Dropping entry: {'train/loss': 1.5932607650756836, 'train/ce_loss': 0.4453125, 'train/seg_cls_loss': 0.01302490234375, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.12963126990944146, 'train/mask_dice_loss': 0.42656942903995515, 'train/mask_loss': 0.5562006950378418, 'metrics/total_secs_per_batch': 6.212744474411011, 'metrics/data_secs_per_batch': 2.8764655590057373, '_timestamp': 1740980254.2334228}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 368 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980254.2342722}).
[2025-03-02 23:37:42,092] [INFO] [logging.py:96:log_dist] [Rank 0] step=4370, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:37:42,097] [INFO] [timer.py:215:stop] epoch=0/micro_step=43700/global_step=4370, RunningAvgSamplesPerSec=1.490019469983682, CurrSamplesPerSec=1.2718621086036328, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [8][370/500]	Time  7.866 ( 7.866)	Loss 1.8437 (1.7092)	CeLoss 0.2188 (0.2040)	SegCLSLoss 0.0141 (0.0136)	KLLoss 0.3574 (0.3293)	MaskLoss 0.7910 (0.7326)	MaskBCELoss 0.0257 (0.1813)	MaskDICELoss 0.7654 (0.5513)
Epoch: [8][371/500]	Time  6.393 ( 6.393)	Loss 2.7173 (1.4964)	CeLoss 0.1924 (0.3185)	SegCLSLoss 0.0164 (0.0114)	KLLoss 0.3691 (0.2533)	MaskLoss 1.2400 (0.5733)	MaskBCELoss 0.4328 (0.0920)	MaskDICELoss 0.8072 (0.4814)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 369 is less than current step: 499. Dropping entry: {'train/loss': 1.7092483580112456, 'train/ce_loss': 0.20400390625, 'train/seg_cls_loss': 0.0135986328125, 'train/kl_loss': 0.329296875, 'train/mask_bce_loss': 0.1813289536163211, 'train/mask_dice_loss': 0.5512737259268761, 'train/mask_loss': 0.7326026886701584, 'metrics/total_secs_per_batch': 7.8658447265625, 'metrics/data_secs_per_batch': 3.6749829292297362, '_timestamp': 1740980262.0981576}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 369 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980262.0984657}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 370 is less than current step: 499. Dropping entry: {'train/loss': 1.4964130818843842, 'train/ce_loss': 0.31845703125, 'train/seg_cls_loss': 0.011383056640625, 'train/kl_loss': 0.2533203125, 'train/mask_bce_loss': 0.09195382865145803, 'train/mask_dice_loss': 0.48135036826133726, 'train/mask_loss': 0.573304197192192, 'metrics/total_secs_per_batch': 6.393366813659668, 'metrics/data_secs_per_batch': 2.549074101448059, '_timestamp': 1740980268.491697}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 370 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980268.4920175}).
Epoch: [8][372/500]	Time  7.096 ( 7.096)	Loss 1.9404 (1.7221)	CeLoss 0.2295 (0.2258)	SegCLSLoss 0.0168 (0.0140)	KLLoss 0.3613 (0.3252)	MaskLoss 0.8335 (0.7283)	MaskBCELoss 0.0250 (0.0715)	MaskDICELoss 0.8085 (0.6568)
Epoch: [8][373/500]	Time  6.383 ( 6.383)	Loss 2.4485 (1.5658)	CeLoss 0.1108 (0.3624)	SegCLSLoss 0.0262 (0.0146)	KLLoss 0.4102 (0.2582)	MaskLoss 1.1418 (0.5850)	MaskBCELoss 0.2863 (0.1203)	MaskDICELoss 0.8555 (0.4648)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 371 is less than current step: 499. Dropping entry: {'train/loss': 1.7221342206001282, 'train/ce_loss': 0.22578125, 'train/seg_cls_loss': 0.0140380859375, 'train/kl_loss': 0.3251953125, 'train/mask_bce_loss': 0.07146493140608072, 'train/mask_dice_loss': 0.6567896664142608, 'train/mask_loss': 0.7282545983791351, 'metrics/total_secs_per_batch': 7.095515727996826, 'metrics/data_secs_per_batch': 3.213115382194519, '_timestamp': 1740980275.5873003}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 371 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980275.587525}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 372 is less than current step: 499. Dropping entry: {'train/loss': 1.5657950282096862, 'train/ce_loss': 0.362353515625, 'train/seg_cls_loss': 0.014581298828125, 'train/kl_loss': 0.258203125, 'train/mask_bce_loss': 0.12025015717372298, 'train/mask_dice_loss': 0.4647958010435104, 'train/mask_loss': 0.5850459456443786, 'metrics/total_secs_per_batch': 6.382874488830566, 'metrics/data_secs_per_batch': 3.0722195625305178, '_timestamp': 1740980281.9701512}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 372 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980281.9704468}).
Epoch: [8][374/500]	Time  5.793 ( 5.793)	Loss 1.2656 (0.8098)	CeLoss 1.2656 (0.4180)	SegCLSLoss 0.0000 (0.0051)	KLLoss 0.0000 (0.1459)	MaskLoss 0.0000 (0.1873)	MaskBCELoss 0.0000 (0.0474)	MaskDICELoss 0.0000 (0.1399)
Epoch: [8][375/500]	Time  7.128 ( 7.128)	Loss 1.6408 (1.8251)	CeLoss 0.1973 (0.2409)	SegCLSLoss 0.0255 (0.0158)	KLLoss 0.3555 (0.3646)	MaskLoss 0.6974 (0.7700)	MaskBCELoss 0.0485 (0.1891)	MaskDICELoss 0.6488 (0.5809)
Epoch: [8][376/500]	Time  5.314 ( 5.314)	Loss 1.5668 (1.7388)	CeLoss 0.2305 (0.4730)	SegCLSLoss 0.0212 (0.0107)	KLLoss 0.3555 (0.2529)	MaskLoss 0.6447 (0.6175)	MaskBCELoss 0.1263 (0.1900)	MaskDICELoss 0.5185 (0.4275)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 373 is less than current step: 499. Dropping entry: {'train/loss': 0.8098280787467956, 'train/ce_loss': 0.41796875, 'train/seg_cls_loss': 0.00513916015625, 'train/kl_loss': 0.1458984375, 'train/mask_bce_loss': 0.04739534743130207, 'train/mask_dice_loss': 0.13989173248410225, 'train/mask_loss': 0.18728707283735274, 'metrics/total_secs_per_batch': 5.793084621429443, 'metrics/data_secs_per_batch': 2.772596764564514, '_timestamp': 1740980287.7632046}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 373 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980287.7634046}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 374 is less than current step: 499. Dropping entry: {'train/loss': 1.8250540018081665, 'train/ce_loss': 0.24091796875, 'train/seg_cls_loss': 0.015802001953125, 'train/kl_loss': 0.3646484375, 'train/mask_bce_loss': 0.18910439349710942, 'train/mask_dice_loss': 0.5808932960033417, 'train/mask_loss': 0.769997701048851, 'metrics/total_secs_per_batch': 7.128388404846191, 'metrics/data_secs_per_batch': 3.1433722734451295, '_timestamp': 1740980294.8917365}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 374 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980294.8920653}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 375 is less than current step: 499. Dropping entry: {'train/loss': 1.7388309836387634, 'train/ce_loss': 0.473046875, 'train/seg_cls_loss': 0.010723876953125, 'train/kl_loss': 0.2529296875, 'train/mask_bce_loss': 0.18997787758708, 'train/mask_dice_loss': 0.42748447358608244, 'train/mask_loss': 0.6174623548984528, 'metrics/total_secs_per_batch': 5.314198732376099, 'metrics/data_secs_per_batch': 2.577145051956177, '_timestamp': 1740980300.2057633}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 375 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980300.206123}).
Epoch: [8][377/500]	Time  6.158 ( 6.158)	Loss 1.6126 (1.7003)	CeLoss 0.2266 (0.3659)	SegCLSLoss 0.0188 (0.0126)	KLLoss 0.3535 (0.3273)	MaskLoss 0.6706 (0.6477)	MaskBCELoss 0.0868 (0.0975)	MaskDICELoss 0.5837 (0.5502)
Epoch: [8][378/500]	Time  6.059 ( 6.059)	Loss 0.8828 (1.5575)	CeLoss 0.2031 (0.3173)	SegCLSLoss 0.0120 (0.0139)	KLLoss 0.3613 (0.2900)	MaskLoss 0.3188 (0.6020)	MaskBCELoss 0.1141 (0.0785)	MaskDICELoss 0.2047 (0.5236)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 376 is less than current step: 499. Dropping entry: {'train/loss': 1.7002871870994567, 'train/ce_loss': 0.36591796875, 'train/seg_cls_loss': 0.01259765625, 'train/kl_loss': 0.32734375, 'train/mask_bce_loss': 0.09747833637520671, 'train/mask_dice_loss': 0.550223845243454, 'train/mask_loss': 0.6477021753787995, 'metrics/total_secs_per_batch': 6.158284425735474, 'metrics/data_secs_per_batch': 2.9233895540237427, '_timestamp': 1740980306.3640935}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 376 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980306.3643749}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 377 is less than current step: 499. Dropping entry: {'train/loss': 1.5575429022312164, 'train/ce_loss': 0.31728515625, 'train/seg_cls_loss': 0.0138671875, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.0784568072296679, 'train/mask_dice_loss': 0.5235568434000015, 'train/mask_loss': 0.6020136505365372, 'metrics/total_secs_per_batch': 6.059450387954712, 'metrics/data_secs_per_batch': 2.6976575374603273, '_timestamp': 1740980312.423608}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 377 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980312.423923}).
Epoch: [8][379/500]	Time  5.497 ( 5.497)	Loss 1.1641 (1.7896)	CeLoss 1.1641 (0.3821)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.2932)	MaskLoss 0.0000 (0.6856)	MaskBCELoss 0.0000 (0.1306)	MaskDICELoss 0.0000 (0.5549)
[2025-03-02 23:38:44,574] [INFO] [logging.py:96:log_dist] [Rank 0] step=4380, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:38:44,580] [INFO] [timer.py:215:stop] epoch=0/micro_step=43800/global_step=4380, RunningAvgSamplesPerSec=1.4902549691806093, CurrSamplesPerSec=1.5016716790963298, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [8][380/500]	Time  6.661 ( 6.661)	Loss 2.3042 (1.8041)	CeLoss 0.2256 (0.3015)	SegCLSLoss 0.0147 (0.0159)	KLLoss 0.3633 (0.3289)	MaskLoss 1.0174 (0.7310)	MaskBCELoss 0.1440 (0.1563)	MaskDICELoss 0.8733 (0.5746)
Epoch: [8][381/500]	Time  5.952 ( 5.952)	Loss 2.2249 (2.2257)	CeLoss 0.2637 (0.3426)	SegCLSLoss 0.0112 (0.0145)	KLLoss 0.3672 (0.3285)	MaskLoss 0.9591 (0.9214)	MaskBCELoss 0.1002 (0.3052)	MaskDICELoss 0.8589 (0.6162)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 378 is less than current step: 499. Dropping entry: {'train/loss': 1.7895718455314635, 'train/ce_loss': 0.38212890625, 'train/seg_cls_loss': 0.01417236328125, 'train/kl_loss': 0.2931640625, 'train/mask_bce_loss': 0.1306392803788185, 'train/mask_dice_loss': 0.5549181178212166, 'train/mask_loss': 0.6855573952198029, 'metrics/total_secs_per_batch': 5.49726676940918, 'metrics/data_secs_per_batch': 2.8264151573181153, '_timestamp': 1740980317.9207213}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 378 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980317.9209912}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 379 is less than current step: 499. Dropping entry: {'train/loss': 1.8041350603103639, 'train/ce_loss': 0.30146484375, 'train/seg_cls_loss': 0.015911865234375, 'train/kl_loss': 0.32890625, 'train/mask_bce_loss': 0.15633947420865296, 'train/mask_dice_loss': 0.5746343076229096, 'train/mask_loss': 0.7309737861156463, 'metrics/total_secs_per_batch': 6.660800218582153, 'metrics/data_secs_per_batch': 3.1544392108917236, '_timestamp': 1740980324.5813613}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 379 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980324.5816364}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 380 is less than current step: 499. Dropping entry: {'train/loss': 2.2257066130638123, 'train/ce_loss': 0.342578125, 'train/seg_cls_loss': 0.01446533203125, 'train/kl_loss': 0.328515625, 'train/mask_bce_loss': 0.30523068690672517, 'train/mask_dice_loss': 0.6162163555622101, 'train/mask_loss': 0.9214470475912094, 'metrics/total_secs_per_batch': 5.9517741203308105, 'metrics/data_secs_per_batch': 2.5873836040496827, '_timestamp': 1740980330.5333335}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 380 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980330.5336092}).
Epoch: [8][382/500]	Time  4.893 ( 4.893)	Loss 1.2891 (1.4390)	CeLoss 1.2891 (0.7771)	SegCLSLoss 0.0000 (0.0094)	KLLoss 0.0000 (0.1830)	MaskLoss 0.0000 (0.3194)	MaskBCELoss 0.0000 (0.0596)	MaskDICELoss 0.0000 (0.2598)
Epoch: [8][383/500]	Time  6.924 ( 6.924)	Loss 1.4817 (1.5233)	CeLoss 0.2178 (0.2748)	SegCLSLoss 0.0181 (0.0138)	KLLoss 0.3594 (0.2934)	MaskLoss 0.6100 (0.6061)	MaskBCELoss 0.0082 (0.1749)	MaskDICELoss 0.6018 (0.4312)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 381 is less than current step: 499. Dropping entry: {'train/loss': 1.4390235364437103, 'train/ce_loss': 0.77705078125, 'train/seg_cls_loss': 0.009417724609375, 'train/kl_loss': 0.1830078125, 'train/mask_bce_loss': 0.059583242237567904, 'train/mask_dice_loss': 0.25983085483312607, 'train/mask_loss': 0.31941409707069396, 'metrics/total_secs_per_batch': 4.893442630767822, 'metrics/data_secs_per_batch': 2.4611394882202147, '_timestamp': 1740980335.4269407}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 381 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980335.4272783}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 382 is less than current step: 499. Dropping entry: {'train/loss': 1.5233014583587647, 'train/ce_loss': 0.274755859375, 'train/seg_cls_loss': 0.013836669921875, 'train/kl_loss': 0.293359375, 'train/mask_bce_loss': 0.1749038103967905, 'train/mask_dice_loss': 0.4312049105763435, 'train/mask_loss': 0.6061087280511857, 'metrics/total_secs_per_batch': 6.923994541168213, 'metrics/data_secs_per_batch': 3.094000816345215, '_timestamp': 1740980342.3508565}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 382 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980342.3511496}).
Epoch: [8][384/500]	Time  6.642 ( 6.642)	Loss 2.4617 (1.9973)	CeLoss 0.1562 (0.3361)	SegCLSLoss 0.0208 (0.0174)	KLLoss 0.3750 (0.2992)	MaskLoss 1.1283 (0.8111)	MaskBCELoss 0.2339 (0.1676)	MaskDICELoss 0.8944 (0.6435)
Epoch: [8][385/500]	Time  5.967 ( 5.967)	Loss 0.8359 (1.3139)	CeLoss 0.8359 (0.3751)	SegCLSLoss 0.0000 (0.0084)	KLLoss 0.0000 (0.2174)	MaskLoss 0.0000 (0.4565)	MaskBCELoss 0.0000 (0.1245)	MaskDICELoss 0.0000 (0.3320)
Epoch: [8][386/500]	Time  5.858 ( 5.858)	Loss 1.5312 (1.3554)	CeLoss 1.5312 (0.4624)	SegCLSLoss 0.0000 (0.0100)	KLLoss 0.0000 (0.2553)	MaskLoss 0.0000 (0.4312)	MaskBCELoss 0.0000 (0.1203)	MaskDICELoss 0.0000 (0.3108)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 383 is less than current step: 499. Dropping entry: {'train/loss': 1.9972588896751404, 'train/ce_loss': 0.3361328125, 'train/seg_cls_loss': 0.017401123046875, 'train/kl_loss': 0.29921875, 'train/mask_bce_loss': 0.16757550956681372, 'train/mask_dice_loss': 0.6435051023960113, 'train/mask_loss': 0.8110806167125701, 'metrics/total_secs_per_batch': 6.642273187637329, 'metrics/data_secs_per_batch': 3.3157506227493285, '_timestamp': 1740980348.9930732}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 383 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980348.993425}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 384 is less than current step: 499. Dropping entry: {'train/loss': 1.3139374017715455, 'train/ce_loss': 0.37509765625, 'train/seg_cls_loss': 0.0083984375, 'train/kl_loss': 0.2173828125, 'train/mask_bce_loss': 0.12449469901621342, 'train/mask_dice_loss': 0.3320345550775528, 'train/mask_loss': 0.4565292477607727, 'metrics/total_secs_per_batch': 5.966925382614136, 'metrics/data_secs_per_batch': 2.672160530090332, '_timestamp': 1740980354.9601333}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 384 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980354.9604623}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 385 is less than current step: 499. Dropping entry: {'train/loss': 1.3553534209728242, 'train/ce_loss': 0.462353515625, 'train/seg_cls_loss': 0.010040283203125, 'train/kl_loss': 0.2552734375, 'train/mask_bce_loss': 0.12033811807632447, 'train/mask_dice_loss': 0.3108298070728779, 'train/mask_loss': 0.431167921423912, 'metrics/total_secs_per_batch': 5.857582330703735, 'metrics/data_secs_per_batch': 3.0325013160705567, '_timestamp': 1740980360.8177392}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 385 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980360.8180764}).
Epoch: [8][387/500]	Time  6.056 ( 6.056)	Loss 2.5169 (1.6930)	CeLoss 0.1973 (0.4438)	SegCLSLoss 0.0178 (0.0134)	KLLoss 0.3652 (0.2879)	MaskLoss 1.1373 (0.6069)	MaskBCELoss 0.2872 (0.1014)	MaskDICELoss 0.8502 (0.5056)
Epoch: [8][388/500]	Time  5.387 ( 5.387)	Loss 1.3281 (1.7222)	CeLoss 1.3281 (0.5875)	SegCLSLoss 0.0000 (0.0147)	KLLoss 0.0000 (0.2219)	MaskLoss 0.0000 (0.5526)	MaskBCELoss 0.0000 (0.1338)	MaskDICELoss 0.0000 (0.4188)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 386 is less than current step: 499. Dropping entry: {'train/loss': 1.6929977416992188, 'train/ce_loss': 0.44384765625, 'train/seg_cls_loss': 0.01337890625, 'train/kl_loss': 0.287890625, 'train/mask_bce_loss': 0.1013629900291562, 'train/mask_dice_loss': 0.5055850833654404, 'train/mask_loss': 0.6069480776786804, 'metrics/total_secs_per_batch': 6.056292295455933, 'metrics/data_secs_per_batch': 2.933165192604065, '_timestamp': 1740980366.8742256}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 386 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980366.8747344}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 387 is less than current step: 499. Dropping entry: {'train/loss': 1.7221866369247436, 'train/ce_loss': 0.5875, 'train/seg_cls_loss': 0.014678955078125, 'train/kl_loss': 0.221875, 'train/mask_bce_loss': 0.13378992825746536, 'train/mask_dice_loss': 0.41880728900432584, 'train/mask_loss': 0.5525972127914429, 'metrics/total_secs_per_batch': 5.3866026401519775, 'metrics/data_secs_per_batch': 2.5757232904434204, '_timestamp': 1740980372.2606194}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 387 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980372.2609189}).
Epoch: [8][389/500]	Time  5.537 ( 5.537)	Loss 1.4844 (1.4861)	CeLoss 1.4844 (0.4364)	SegCLSLoss 0.0000 (0.0119)	KLLoss 0.0000 (0.2561)	MaskLoss 0.0000 (0.5091)	MaskBCELoss 0.0000 (0.0764)	MaskDICELoss 0.0000 (0.4327)
[2025-03-02 23:39:43,164] [INFO] [logging.py:96:log_dist] [Rank 0] step=4390, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:39:43,170] [INFO] [timer.py:215:stop] epoch=0/micro_step=43900/global_step=4390, RunningAvgSamplesPerSec=1.490686667652834, CurrSamplesPerSec=1.8617746291180066, MemAllocated=31.44GB, MaxMemAllocated=37.23GB
Epoch: [8][390/500]	Time  5.373 ( 5.373)	Loss 1.4811 (1.2065)	CeLoss 0.2812 (0.4211)	SegCLSLoss 0.0114 (0.0083)	KLLoss 0.3633 (0.2543)	MaskLoss 0.5784 (0.3778)	MaskBCELoss 0.0278 (0.1163)	MaskDICELoss 0.5506 (0.2615)
Epoch: [8][391/500]	Time  6.469 ( 6.469)	Loss 0.8610 (1.5578)	CeLoss 0.2363 (0.2752)	SegCLSLoss 0.0136 (0.0121)	KLLoss 0.3672 (0.2932)	MaskLoss 0.2909 (0.6236)	MaskBCELoss 0.0541 (0.1610)	MaskDICELoss 0.2368 (0.4626)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 388 is less than current step: 499. Dropping entry: {'train/loss': 1.486121791601181, 'train/ce_loss': 0.43642578125, 'train/seg_cls_loss': 0.011865234375, 'train/kl_loss': 0.2560546875, 'train/mask_bce_loss': 0.07638583071529866, 'train/mask_dice_loss': 0.43269068747758865, 'train/mask_loss': 0.5090765208005905, 'metrics/total_secs_per_batch': 5.537405490875244, 'metrics/data_secs_per_batch': 2.4890818357467652, '_timestamp': 1740980377.7979472}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 388 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980377.7982814}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 389 is less than current step: 499. Dropping entry: {'train/loss': 1.2065119743347168, 'train/ce_loss': 0.4210693359375, 'train/seg_cls_loss': 0.008306884765625, 'train/kl_loss': 0.254296875, 'train/mask_bce_loss': 0.11634560702368617, 'train/mask_dice_loss': 0.2614831291139126, 'train/mask_loss': 0.37782873809337614, 'metrics/total_secs_per_batch': 5.372980833053589, 'metrics/data_secs_per_batch': 2.2807347536087037, '_timestamp': 1740980383.1707413}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 389 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980383.1711}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 390 is less than current step: 499. Dropping entry: {'train/loss': 1.5578148782253265, 'train/ce_loss': 0.275244140625, 'train/seg_cls_loss': 0.012127685546875, 'train/kl_loss': 0.2931640625, 'train/mask_bce_loss': 0.16102008447051047, 'train/mask_dice_loss': 0.4625894919037819, 'train/mask_loss': 0.6236095905303956, 'metrics/total_secs_per_batch': 6.468683958053589, 'metrics/data_secs_per_batch': 2.927704620361328, '_timestamp': 1740980389.6400013}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 390 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980389.6404567}).
Epoch: [8][392/500]	Time  5.618 ( 5.618)	Loss 1.7792 (1.4222)	CeLoss 0.2354 (0.6087)	SegCLSLoss 0.0111 (0.0096)	KLLoss 0.3633 (0.2191)	MaskLoss 0.7509 (0.3934)	MaskBCELoss 0.1401 (0.1183)	MaskDICELoss 0.6108 (0.2751)
Epoch: [8][393/500]	Time  6.588 ( 6.588)	Loss 2.0997 (1.6236)	CeLoss 0.2715 (0.4281)	SegCLSLoss 0.0220 (0.0147)	KLLoss 0.3633 (0.2900)	MaskLoss 0.8897 (0.5794)	MaskBCELoss 0.0961 (0.1165)	MaskDICELoss 0.7936 (0.4629)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 391 is less than current step: 499. Dropping entry: {'train/loss': 1.4222380876541139, 'train/ce_loss': 0.60869140625, 'train/seg_cls_loss': 0.00963134765625, 'train/kl_loss': 0.219140625, 'train/mask_bce_loss': 0.11830298230051994, 'train/mask_dice_loss': 0.2751402832567692, 'train/mask_loss': 0.3934432610869408, 'metrics/total_secs_per_batch': 5.617668628692627, 'metrics/data_secs_per_batch': 2.558943033218384, '_timestamp': 1740980395.2578537}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 391 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980395.258534}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 392 is less than current step: 499. Dropping entry: {'train/loss': 1.6236334323883057, 'train/ce_loss': 0.428125, 'train/seg_cls_loss': 0.01468505859375, 'train/kl_loss': 0.2900390625, 'train/mask_bce_loss': 0.11650906149297953, 'train/mask_dice_loss': 0.46288577914237977, 'train/mask_loss': 0.5793948471546173, 'metrics/total_secs_per_batch': 6.588257789611816, 'metrics/data_secs_per_batch': 3.016403818130493, '_timestamp': 1740980401.8455455}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 392 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980401.845907}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 393 is less than current step: 499. Dropping entry: {'train/loss': 1.4820184350013732, 'train/ce_loss': 0.373828125, 'train/seg_cls_loss': 0.01236572265625, 'train/kl_loss': 0.2560546875, 'train/mask_bce_loss': 0.10945047847926617, 'train/mask_dice_loss': 0.42872670888900755, 'train/mask_loss': 0.5381771922111511, 'metrics/total_secs_per_batch': 5.665889263153076, 'metrics/data_secs_per_batch': 2.6690845012664797, '_timestamp': 1740980407.5116956}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 393 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980407.5121849}).
Epoch: [8][394/500]	Time  5.666 ( 5.666)	Loss 1.3920 (1.4820)	CeLoss 0.2285 (0.3738)	SegCLSLoss 0.0107 (0.0124)	KLLoss 0.3574 (0.2561)	MaskLoss 0.5612 (0.5382)	MaskBCELoss 0.0876 (0.1095)	MaskDICELoss 0.4736 (0.4287)
Epoch: [8][395/500]	Time  6.869 ( 6.869)	Loss 2.2731 (1.7070)	CeLoss 0.2207 (0.2530)	SegCLSLoss 0.0276 (0.0150)	KLLoss 0.3438 (0.3270)	MaskLoss 1.0018 (0.7068)	MaskBCELoss 0.0175 (0.1557)	MaskDICELoss 0.9843 (0.5511)
Epoch: [8][396/500]	Time  6.263 ( 6.263)	Loss 1.2157 (1.7249)	CeLoss 0.2148 (0.3918)	SegCLSLoss 0.0099 (0.0118)	KLLoss 0.3633 (0.2893)	MaskLoss 0.4799 (0.6490)	MaskBCELoss 0.0464 (0.1682)	MaskDICELoss 0.4335 (0.4808)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 394 is less than current step: 499. Dropping entry: {'train/loss': 1.7069815278053284, 'train/ce_loss': 0.25302734375, 'train/seg_cls_loss': 0.0150146484375, 'train/kl_loss': 0.326953125, 'train/mask_bce_loss': 0.15567286852747203, 'train/mask_dice_loss': 0.5511382102966309, 'train/mask_loss': 0.7068110853433609, 'metrics/total_secs_per_batch': 6.869482517242432, 'metrics/data_secs_per_batch': 3.267201614379883, '_timestamp': 1740980414.3810637}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 394 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980414.3814564}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 395 is less than current step: 499. Dropping entry: {'train/loss': 1.7249208450317384, 'train/ce_loss': 0.391796875, 'train/seg_cls_loss': 0.011761474609375, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.16819069348275661, 'train/mask_dice_loss': 0.48084198832511904, 'train/mask_loss': 0.6490326732397079, 'metrics/total_secs_per_batch': 6.263466835021973, 'metrics/data_secs_per_batch': 2.5989104509353638, '_timestamp': 1740980420.6450038}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 395 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980420.6455288}).
Epoch: [8][397/500]	Time  5.747 ( 5.747)	Loss 0.9492 (1.6228)	CeLoss 0.9492 (0.5264)	SegCLSLoss 0.0000 (0.0109)	KLLoss 0.0000 (0.2174)	MaskLoss 0.0000 (0.5348)	MaskBCELoss 0.0000 (0.0842)	MaskDICELoss 0.0000 (0.4506)
Epoch: [8][398/500]	Time  5.914 ( 5.914)	Loss 2.0879 (1.8539)	CeLoss 0.3281 (0.4159)	SegCLSLoss 0.0118 (0.0137)	KLLoss 0.3633 (0.2920)	MaskLoss 0.8584 (0.7011)	MaskBCELoss 0.0834 (0.1221)	MaskDICELoss 0.7750 (0.5790)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 396 is less than current step: 499. Dropping entry: {'train/loss': 1.6228456616401672, 'train/ce_loss': 0.5263671875, 'train/seg_cls_loss': 0.010882568359375, 'train/kl_loss': 0.2173828125, 'train/mask_bce_loss': 0.08423668956384063, 'train/mask_dice_loss': 0.4505748152732849, 'train/mask_loss': 0.5348115026950836, 'metrics/total_secs_per_batch': 5.746610403060913, 'metrics/data_secs_per_batch': 2.751127505302429, '_timestamp': 1740980426.3910065}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 396 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980426.3912168}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 397 is less than current step: 499. Dropping entry: {'train/loss': 1.8538854122161865, 'train/ce_loss': 0.41591796875, 'train/seg_cls_loss': 0.013739013671875, 'train/kl_loss': 0.2919921875, 'train/mask_bce_loss': 0.12208625562489032, 'train/mask_dice_loss': 0.5790263742208481, 'train/mask_loss': 0.7011126220226288, 'metrics/total_secs_per_batch': 5.913524866104126, 'metrics/data_secs_per_batch': 2.362202215194702, '_timestamp': 1740980432.305069}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 397 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980432.305556}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 398 is less than current step: 499. Dropping entry: {'train/loss': 1.377202820777893, 'train/ce_loss': 0.57744140625, 'train/seg_cls_loss': 0.008148193359375, 'train/kl_loss': 0.1822265625, 'train/mask_bce_loss': 0.05803255378268659, 'train/mask_dice_loss': 0.3307153433561325, 'train/mask_loss': 0.38874788880348204, 'metrics/total_secs_per_batch': 5.197126865386963, 'metrics/data_secs_per_batch': 2.65181622505188, '_timestamp': 1740980437.5016549}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 398 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980437.5019605}).
Epoch: [8][399/500]	Time  5.197 ( 5.197)	Loss 0.8828 (1.3772)	CeLoss 0.8828 (0.5774)	SegCLSLoss 0.0000 (0.0081)	KLLoss 0.0000 (0.1822)	MaskLoss 0.0000 (0.3887)	MaskBCELoss 0.0000 (0.0580)	MaskDICELoss 0.0000 (0.3307)
[2025-03-02 23:40:43,163] [INFO] [logging.py:96:log_dist] [Rank 0] step=4400, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:40:43,169] [INFO] [timer.py:215:stop] epoch=0/micro_step=44000/global_step=4400, RunningAvgSamplesPerSec=1.491045560130213, CurrSamplesPerSec=1.7648039446610386, MemAllocated=31.1GB, MaxMemAllocated=37.23GB
Epoch: [8][400/500]	Time  5.668 ( 5.668)	Loss 1.1277 (1.4351)	CeLoss 0.1836 (0.3665)	SegCLSLoss 0.0115 (0.0112)	KLLoss 0.3633 (0.2559)	MaskLoss 0.4511 (0.5186)	MaskBCELoss 0.1215 (0.1668)	MaskDICELoss 0.3296 (0.3519)
Epoch: [8][401/500]	Time  5.068 ( 5.068)	Loss 2.7148 (1.4892)	CeLoss 0.3086 (0.6568)	SegCLSLoss 0.0114 (0.0076)	KLLoss 0.3613 (0.1814)	MaskLoss 1.1816 (0.4051)	MaskBCELoss 0.4077 (0.1060)	MaskDICELoss 0.7739 (0.2991)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 399 is less than current step: 499. Dropping entry: {'train/loss': 1.4350900113582612, 'train/ce_loss': 0.366455078125, 'train/seg_cls_loss': 0.01123046875, 'train/kl_loss': 0.255859375, 'train/mask_bce_loss': 0.16676090648397804, 'train/mask_dice_loss': 0.3518827348947525, 'train/mask_loss': 0.5186436384916305, 'metrics/total_secs_per_batch': 5.6681482791900635, 'metrics/data_secs_per_batch': 2.5089218854904174, '_timestamp': 1740980443.1697285}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 399 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980443.1702354}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 400 is less than current step: 499. Dropping entry: {'train/loss': 1.4891643047332763, 'train/ce_loss': 0.6568359375, 'train/seg_cls_loss': 0.007647705078125, 'train/kl_loss': 0.1814453125, 'train/mask_bce_loss': 0.10599224083125591, 'train/mask_dice_loss': 0.2990879476070404, 'train/mask_loss': 0.40508019626140596, 'metrics/total_secs_per_batch': 5.0680155754089355, 'metrics/data_secs_per_batch': 2.1829253435134888, '_timestamp': 1740980448.237887}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 400 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980448.2382443}).
Epoch: [8][402/500]	Time  6.058 ( 6.058)	Loss 2.0507 (1.7833)	CeLoss 0.1826 (0.2542)	SegCLSLoss 0.0237 (0.0160)	KLLoss 0.3535 (0.3279)	MaskLoss 0.9106 (0.7441)	MaskBCELoss 0.0325 (0.1530)	MaskDICELoss 0.8781 (0.5911)
Epoch: [8][403/500]	Time  5.939 ( 5.939)	Loss 1.0196 (1.7464)	CeLoss 0.2383 (0.2756)	SegCLSLoss 0.0197 (0.0167)	KLLoss 0.3691 (0.2951)	MaskLoss 0.3672 (0.7164)	MaskBCELoss 0.1436 (0.1795)	MaskDICELoss 0.2236 (0.5369)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 401 is less than current step: 499. Dropping entry: {'train/loss': 1.7832772493362428, 'train/ce_loss': 0.25419921875, 'train/seg_cls_loss': 0.016021728515625, 'train/kl_loss': 0.3279296875, 'train/mask_bce_loss': 0.1529990140348673, 'train/mask_dice_loss': 0.5911298394203186, 'train/mask_loss': 0.744128841161728, 'metrics/total_secs_per_batch': 6.058027744293213, 'metrics/data_secs_per_batch': 2.8439600229263307, '_timestamp': 1740980454.2960007}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 401 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980454.2964776}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 402 is less than current step: 499. Dropping entry: {'train/loss': 1.7463908672332764, 'train/ce_loss': 0.275634765625, 'train/seg_cls_loss': 0.01673583984375, 'train/kl_loss': 0.2951171875, 'train/mask_bce_loss': 0.1794892585836351, 'train/mask_dice_loss': 0.5368702441453934, 'train/mask_loss': 0.7163595020771026, 'metrics/total_secs_per_batch': 5.938599348068237, 'metrics/data_secs_per_batch': 2.9299388885498048, '_timestamp': 1740980460.2351012}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 402 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980460.235685}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 403 is less than current step: 499. Dropping entry: {'train/loss': 1.4948061227798461, 'train/ce_loss': 0.46171875, 'train/seg_cls_loss': 0.010711669921875, 'train/kl_loss': 0.291015625, 'train/mask_bce_loss': 0.07820443585515022, 'train/mask_dice_loss': 0.4210540883243084, 'train/mask_loss': 0.49925852119922637, 'metrics/total_secs_per_batch': 6.8306968212127686, 'metrics/data_secs_per_batch': 3.395820736885071, '_timestamp': 1740980467.065334}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 403 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980467.06576}).
Epoch: [8][404/500]	Time  6.831 ( 6.831)	Loss 1.0223 (1.4948)	CeLoss 0.3320 (0.4617)	SegCLSLoss 0.0098 (0.0107)	KLLoss 0.3633 (0.2910)	MaskLoss 0.3247 (0.4993)	MaskBCELoss 0.1499 (0.0782)	MaskDICELoss 0.1748 (0.4211)
Epoch: [8][405/500]	Time  5.576 ( 5.576)	Loss 1.2071 (1.8580)	CeLoss 0.3750 (0.4112)	SegCLSLoss 0.0106 (0.0137)	KLLoss 0.3633 (0.2943)	MaskLoss 0.3946 (0.7051)	MaskBCELoss 0.1372 (0.1876)	MaskDICELoss 0.2574 (0.5175)
Epoch: [8][406/500]	Time  5.796 ( 5.796)	Loss 1.2734 (1.2759)	CeLoss 1.2734 (0.5442)	SegCLSLoss 0.0000 (0.0100)	KLLoss 0.0000 (0.1803)	MaskLoss 0.0000 (0.3543)	MaskBCELoss 0.0000 (0.0152)	MaskDICELoss 0.0000 (0.3392)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 404 is less than current step: 499. Dropping entry: {'train/loss': 1.85804340839386, 'train/ce_loss': 0.41123046875, 'train/seg_cls_loss': 0.01373291015625, 'train/kl_loss': 0.2943359375, 'train/mask_bce_loss': 0.18762152828276157, 'train/mask_dice_loss': 0.5175232231616974, 'train/mask_loss': 0.7051447480916977, 'metrics/total_secs_per_batch': 5.576382160186768, 'metrics/data_secs_per_batch': 2.434544587135315, '_timestamp': 1740980472.641768}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 404 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980472.6421227}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 405 is less than current step: 499. Dropping entry: {'train/loss': 1.275861483812332, 'train/ce_loss': 0.544189453125, 'train/seg_cls_loss': 0.010040283203125, 'train/kl_loss': 0.1802734375, 'train/mask_bce_loss': 0.015155535377562045, 'train/mask_dice_loss': 0.3391570493578911, 'train/mask_loss': 0.35431258380413055, 'metrics/total_secs_per_batch': 5.796070575714111, 'metrics/data_secs_per_batch': 2.6711830615997316, '_timestamp': 1740980478.437627}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 405 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980478.4379387}).
Epoch: [8][407/500]	Time  5.024 ( 5.024)	Loss 1.1250 (1.8497)	CeLoss 1.1250 (0.5475)	SegCLSLoss 0.0000 (0.0155)	KLLoss 0.0000 (0.2557)	MaskLoss 0.0000 (0.6345)	MaskBCELoss 0.0000 (0.0815)	MaskDICELoss 0.0000 (0.5530)
Epoch: [8][408/500]	Time  5.997 ( 5.997)	Loss 0.9651 (1.6090)	CeLoss 0.2559 (0.4265)	SegCLSLoss 0.0161 (0.0128)	KLLoss 0.3672 (0.2887)	MaskLoss 0.3322 (0.5736)	MaskBCELoss 0.0719 (0.1236)	MaskDICELoss 0.2603 (0.4500)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 406 is less than current step: 499. Dropping entry: {'train/loss': 1.8497421145439148, 'train/ce_loss': 0.547509765625, 'train/seg_cls_loss': 0.01553955078125, 'train/kl_loss': 0.2556640625, 'train/mask_bce_loss': 0.08150041811168193, 'train/mask_dice_loss': 0.5530386060476303, 'train/mask_loss': 0.6345390141010284, 'metrics/total_secs_per_batch': 5.024168968200684, 'metrics/data_secs_per_batch': 2.3661494731903074, '_timestamp': 1740980483.4617736}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 406 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980483.4620743}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 407 is less than current step: 499. Dropping entry: {'train/loss': 1.6089553773403167, 'train/ce_loss': 0.42646484375, 'train/seg_cls_loss': 0.01279296875, 'train/kl_loss': 0.288671875, 'train/mask_bce_loss': 0.12360431849956513, 'train/mask_dice_loss': 0.4500139862298965, 'train/mask_loss': 0.5736183196306228, 'metrics/total_secs_per_batch': 5.996975421905518, 'metrics/data_secs_per_batch': 2.575982928276062, '_timestamp': 1740980489.4589787}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 407 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980489.459327}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 408 is less than current step: 499. Dropping entry: {'train/loss': 1.5692540526390075, 'train/ce_loss': 0.3873046875, 'train/seg_cls_loss': 0.015020751953125, 'train/kl_loss': 0.2890625, 'train/mask_bce_loss': 0.05639447830617428, 'train/mask_dice_loss': 0.5164161555469036, 'train/mask_loss': 0.5728106208145618, 'metrics/total_secs_per_batch': 6.678218126296997, 'metrics/data_secs_per_batch': 2.8591197729110718, '_timestamp': 1740980496.1369777}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 408 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980496.1372902}).
Epoch: [8][409/500]	Time  6.678 ( 6.678)	Loss 1.5370 (1.5693)	CeLoss 0.2432 (0.3873)	SegCLSLoss 0.0227 (0.0150)	KLLoss 0.3535 (0.2891)	MaskLoss 0.6240 (0.5728)	MaskBCELoss 0.0302 (0.0564)	MaskDICELoss 0.5938 (0.5164)
[2025-03-02 23:41:42,379] [INFO] [logging.py:96:log_dist] [Rank 0] step=4410, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:41:42,385] [INFO] [timer.py:215:stop] epoch=0/micro_step=44100/global_step=4410, RunningAvgSamplesPerSec=1.4914424085109992, CurrSamplesPerSec=1.6007090543189526, MemAllocated=30.71GB, MaxMemAllocated=37.23GB
Epoch: [8][410/500]	Time  6.249 ( 6.249)	Loss 1.0312 (1.6282)	CeLoss 1.0312 (0.3902)	SegCLSLoss 0.0000 (0.0136)	KLLoss 0.0000 (0.2893)	MaskLoss 0.0000 (0.6012)	MaskBCELoss 0.0000 (0.1006)	MaskDICELoss 0.0000 (0.5006)
Epoch: [8][411/500]	Time  5.939 ( 5.939)	Loss 3.3726 (2.0281)	CeLoss 0.2090 (0.3177)	SegCLSLoss 0.0302 (0.0183)	KLLoss 0.3613 (0.3254)	MaskLoss 1.5564 (0.8344)	MaskBCELoss 0.6301 (0.1651)	MaskDICELoss 0.9263 (0.6693)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 409 is less than current step: 499. Dropping entry: {'train/loss': 1.6281843423843383, 'train/ce_loss': 0.390234375, 'train/seg_cls_loss': 0.013641357421875, 'train/kl_loss': 0.2892578125, 'train/mask_bce_loss': 0.10061937421560288, 'train/mask_dice_loss': 0.5005821704864502, 'train/mask_loss': 0.6012015521526337, 'metrics/total_secs_per_batch': 6.24891209602356, 'metrics/data_secs_per_batch': 3.031537175178528, '_timestamp': 1740980502.3856962}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 409 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980502.3859768}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 410 is less than current step: 499. Dropping entry: {'train/loss': 2.028127634525299, 'train/ce_loss': 0.31767578125, 'train/seg_cls_loss': 0.01827392578125, 'train/kl_loss': 0.325390625, 'train/mask_bce_loss': 0.16512199975550174, 'train/mask_dice_loss': 0.6692543119192124, 'train/mask_loss': 0.8343763172626495, 'metrics/total_secs_per_batch': 5.938513278961182, 'metrics/data_secs_per_batch': 2.645294189453125, '_timestamp': 1740980508.3246326}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 410 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980508.3249974}).
Epoch: [8][412/500]	Time  5.106 ( 5.106)	Loss 1.0391 (1.4691)	CeLoss 1.0391 (0.6102)	SegCLSLoss 0.0000 (0.0101)	KLLoss 0.0000 (0.2172)	MaskLoss 0.0000 (0.4161)	MaskBCELoss 0.0000 (0.0642)	MaskDICELoss 0.0000 (0.3519)
Epoch: [8][413/500]	Time  6.441 ( 6.441)	Loss 1.7783 (2.0441)	CeLoss 0.2285 (0.2216)	SegCLSLoss 0.0188 (0.0199)	KLLoss 0.3574 (0.3682)	MaskLoss 0.7525 (0.8879)	MaskBCELoss 0.0851 (0.2377)	MaskDICELoss 0.6674 (0.6502)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 411 is less than current step: 499. Dropping entry: {'train/loss': 1.4690751552581787, 'train/ce_loss': 0.61015625, 'train/seg_cls_loss': 0.010137939453125, 'train/kl_loss': 0.2171875, 'train/mask_bce_loss': 0.06420051231980324, 'train/mask_dice_loss': 0.3518800362944603, 'train/mask_loss': 0.41608054339885714, 'metrics/total_secs_per_batch': 5.106046199798584, 'metrics/data_secs_per_batch': 2.298157787322998, '_timestamp': 1740980513.4304638}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 411 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980513.4306638}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 412 is less than current step: 499. Dropping entry: {'train/loss': 2.0440946221351624, 'train/ce_loss': 0.22158203125, 'train/seg_cls_loss': 0.01993408203125, 'train/kl_loss': 0.3681640625, 'train/mask_bce_loss': 0.23769028000533582, 'train/mask_dice_loss': 0.6501773416996002, 'train/mask_loss': 0.8878676205873489, 'metrics/total_secs_per_batch': 6.44082236289978, 'metrics/data_secs_per_batch': 2.907555866241455, '_timestamp': 1740980519.8714418}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 412 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980519.8717842}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 413 is less than current step: 499. Dropping entry: {'train/loss': 1.6588553488254547, 'train/ce_loss': 0.29970703125, 'train/seg_cls_loss': 0.015185546875, 'train/kl_loss': 0.3296875, 'train/mask_bce_loss': 0.16055146530270575, 'train/mask_dice_loss': 0.49866136759519575, 'train/mask_loss': 0.6592128247022628, 'metrics/total_secs_per_batch': 6.288526296615601, 'metrics/data_secs_per_batch': 2.9961177349090575, '_timestamp': 1740980526.1598418}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 413 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980526.1601276}).
Epoch: [8][414/500]	Time  6.289 ( 6.289)	Loss 0.8477 (1.6589)	CeLoss 0.8477 (0.2997)	SegCLSLoss 0.0000 (0.0152)	KLLoss 0.0000 (0.3297)	MaskLoss 0.0000 (0.6592)	MaskBCELoss 0.0000 (0.1606)	MaskDICELoss 0.0000 (0.4987)
Epoch: [8][415/500]	Time  5.878 ( 5.878)	Loss 0.7891 (1.8616)	CeLoss 0.7891 (0.4138)	SegCLSLoss 0.0000 (0.0143)	KLLoss 0.0000 (0.2904)	MaskLoss 0.0000 (0.7058)	MaskBCELoss 0.0000 (0.2029)	MaskDICELoss 0.0000 (0.5029)
Epoch: [8][416/500]	Time  7.120 ( 7.120)	Loss 2.2335 (1.7197)	CeLoss 0.1885 (0.2871)	SegCLSLoss 0.0201 (0.0112)	KLLoss 0.3555 (0.2898)	MaskLoss 0.9995 (0.6990)	MaskBCELoss 0.0159 (0.1394)	MaskDICELoss 0.9837 (0.5596)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 414 is less than current step: 499. Dropping entry: {'train/loss': 1.8615829229354859, 'train/ce_loss': 0.41376953125, 'train/seg_cls_loss': 0.01431884765625, 'train/kl_loss': 0.2904296875, 'train/mask_bce_loss': 0.20288898684084417, 'train/mask_dice_loss': 0.5029024690389633, 'train/mask_loss': 0.7057914584875107, 'metrics/total_secs_per_batch': 5.877625226974487, 'metrics/data_secs_per_batch': 2.694517493247986, '_timestamp': 1740980532.0374699}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 414 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980532.0377593}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 415 is less than current step: 499. Dropping entry: {'train/loss': 1.7196736931800842, 'train/ce_loss': 0.287109375, 'train/seg_cls_loss': 0.011187744140625, 'train/kl_loss': 0.28984375, 'train/mask_bce_loss': 0.1394129663705826, 'train/mask_dice_loss': 0.5595840185880661, 'train/mask_loss': 0.6989969968795776, 'metrics/total_secs_per_batch': 7.119686841964722, 'metrics/data_secs_per_batch': 2.951714015007019, '_timestamp': 1740980539.1573029}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 415 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980539.1576555}).
Epoch: [8][417/500]	Time  6.048 ( 6.048)	Loss 2.5013 (1.9252)	CeLoss 0.2988 (0.4765)	SegCLSLoss 0.0139 (0.0105)	KLLoss 0.3535 (0.2512)	MaskLoss 1.0798 (0.7092)	MaskBCELoss 0.3274 (0.1826)	MaskDICELoss 0.7524 (0.5266)
Epoch: [8][418/500]	Time  6.405 ( 6.405)	Loss 1.5387 (1.4719)	CeLoss 0.2490 (0.3032)	SegCLSLoss 0.0175 (0.0141)	KLLoss 0.3555 (0.2912)	MaskLoss 0.6229 (0.5662)	MaskBCELoss 0.1463 (0.1259)	MaskDICELoss 0.4765 (0.4404)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 416 is less than current step: 499. Dropping entry: {'train/loss': 1.9251894235610962, 'train/ce_loss': 0.47646484375, 'train/seg_cls_loss': 0.010479736328125, 'train/kl_loss': 0.251171875, 'train/mask_bce_loss': 0.18256913218647242, 'train/mask_dice_loss': 0.5266075998544693, 'train/mask_loss': 0.7091767370700837, 'metrics/total_secs_per_batch': 6.048391103744507, 'metrics/data_secs_per_batch': 2.9049956798553467, '_timestamp': 1740980545.2055054}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 416 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980545.2057984}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 417 is less than current step: 499. Dropping entry: {'train/loss': 1.4719424277544022, 'train/ce_loss': 0.30322265625, 'train/seg_cls_loss': 0.01409912109375, 'train/kl_loss': 0.2912109375, 'train/mask_bce_loss': 0.12586711794137956, 'train/mask_dice_loss': 0.44037752524018287, 'train/mask_loss': 0.5662446528673172, 'metrics/total_secs_per_batch': 6.404560089111328, 'metrics/data_secs_per_batch': 2.5727319955825805, '_timestamp': 1740980551.6101387}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 417 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980551.6104176}).
Epoch: [8][419/500]	Time  6.319 ( 6.319)	Loss 0.6971 (1.5239)	CeLoss 0.1797 (0.2552)	SegCLSLoss 0.0245 (0.0154)	KLLoss 0.3945 (0.2934)	MaskLoss 0.2323 (0.6158)	MaskBCELoss 0.0446 (0.0966)	MaskDICELoss 0.1877 (0.5191)
[2025-03-02 23:42:43,820] [INFO] [logging.py:96:log_dist] [Rank 0] step=4420, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:42:43,826] [INFO] [timer.py:215:stop] epoch=0/micro_step=44200/global_step=4420, RunningAvgSamplesPerSec=1.491725478795087, CurrSamplesPerSec=1.695996779376233, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [8][420/500]	Time  5.898 ( 5.898)	Loss 2.0822 (1.7112)	CeLoss 0.2217 (0.5092)	SegCLSLoss 0.0159 (0.0107)	KLLoss 0.3633 (0.2521)	MaskLoss 0.9083 (0.5858)	MaskBCELoss 0.0275 (0.0841)	MaskDICELoss 0.8808 (0.5017)
Epoch: [8][421/500]	Time  6.299 ( 6.299)	Loss 1.2301 (1.3847)	CeLoss 0.2656 (0.3760)	SegCLSLoss 0.0123 (0.0136)	KLLoss 0.3594 (0.2516)	MaskLoss 0.4617 (0.4884)	MaskBCELoss 0.1304 (0.0478)	MaskDICELoss 0.3314 (0.4405)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 418 is less than current step: 499. Dropping entry: {'train/loss': 1.5238983392715455, 'train/ce_loss': 0.25517578125, 'train/seg_cls_loss': 0.015362548828125, 'train/kl_loss': 0.293359375, 'train/mask_bce_loss': 0.0966129700653255, 'train/mask_dice_loss': 0.5191447898745537, 'train/mask_loss': 0.6157577753067016, 'metrics/total_secs_per_batch': 6.31898045539856, 'metrics/data_secs_per_batch': 2.992399311065674, '_timestamp': 1740980557.9291377}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 418 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980557.929434}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 419 is less than current step: 499. Dropping entry: {'train/loss': 1.711194109916687, 'train/ce_loss': 0.5091796875, 'train/seg_cls_loss': 0.010650634765625, 'train/kl_loss': 0.2521484375, 'train/mask_bce_loss': 0.08412426011636853, 'train/mask_dice_loss': 0.5016973912715912, 'train/mask_loss': 0.5858216464519501, 'metrics/total_secs_per_batch': 5.897913694381714, 'metrics/data_secs_per_batch': 2.283550834655762, '_timestamp': 1740980563.8268332}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 419 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980563.8271434}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 420 is less than current step: 499. Dropping entry: {'train/loss': 1.3846998929977417, 'train/ce_loss': 0.3760009765625, 'train/seg_cls_loss': 0.013604736328125, 'train/kl_loss': 0.2515625, 'train/mask_bce_loss': 0.04784150030463934, 'train/mask_dice_loss': 0.44054116010665895, 'train/mask_loss': 0.48838266134262087, 'metrics/total_secs_per_batch': 6.298532247543335, 'metrics/data_secs_per_batch': 2.9654335975646973, '_timestamp': 1740980570.125649}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 420 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980570.1259775}).
Epoch: [8][422/500]	Time  5.700 ( 5.700)	Loss 0.6462 (1.0730)	CeLoss 0.2236 (0.3963)	SegCLSLoss 0.0089 (0.0078)	KLLoss 0.3672 (0.2176)	MaskLoss 0.1903 (0.3254)	MaskBCELoss 0.0912 (0.0814)	MaskDICELoss 0.0991 (0.2440)
Epoch: [8][423/500]	Time  5.191 ( 5.191)	Loss 1.3984 (1.3705)	CeLoss 1.3984 (0.4558)	SegCLSLoss 0.0000 (0.0076)	KLLoss 0.0000 (0.2180)	MaskLoss 0.0000 (0.4446)	MaskBCELoss 0.0000 (0.1286)	MaskDICELoss 0.0000 (0.3160)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 421 is less than current step: 499. Dropping entry: {'train/loss': 1.0729993820190429, 'train/ce_loss': 0.3962890625, 'train/seg_cls_loss': 0.007763671875, 'train/kl_loss': 0.217578125, 'train/mask_bce_loss': 0.08142141681164503, 'train/mask_dice_loss': 0.24399428740143775, 'train/mask_loss': 0.32541570365428923, 'metrics/total_secs_per_batch': 5.69967794418335, 'metrics/data_secs_per_batch': 2.6857091665267943, '_timestamp': 1740980575.8252144}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 421 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980575.8254936}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 422 is less than current step: 499. Dropping entry: {'train/loss': 1.3705279350280761, 'train/ce_loss': 0.45576171875, 'train/seg_cls_loss': 0.007598876953125, 'train/kl_loss': 0.21796875, 'train/mask_bce_loss': 0.12862388156354426, 'train/mask_dice_loss': 0.3159662544727325, 'train/mask_loss': 0.4445901393890381, 'metrics/total_secs_per_batch': 5.190814971923828, 'metrics/data_secs_per_batch': 2.641760015487671, '_timestamp': 1740980581.0160253}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 422 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980581.0163126}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 423 is less than current step: 499. Dropping entry: {'train/loss': 1.482187533378601, 'train/ce_loss': 0.448193359375, 'train/seg_cls_loss': 0.011456298828125, 'train/kl_loss': 0.218359375, 'train/mask_bce_loss': 0.12866204585880042, 'train/mask_dice_loss': 0.37437020391225817, 'train/mask_loss': 0.5030322432518005, 'metrics/total_secs_per_batch': 5.327991962432861, 'metrics/data_secs_per_batch': 2.2135664939880373, '_timestamp': 1740980586.3440173}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 423 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980586.3442888}).
Epoch: [8][424/500]	Time  5.328 ( 5.328)	Loss 1.0816 (1.4822)	CeLoss 0.3887 (0.4482)	SegCLSLoss 0.0107 (0.0115)	KLLoss 0.3652 (0.2184)	MaskLoss 0.3250 (0.5030)	MaskBCELoss 0.1238 (0.1287)	MaskDICELoss 0.2012 (0.3744)
Epoch: [8][425/500]	Time  6.051 ( 6.051)	Loss 1.3681 (1.7200)	CeLoss 0.2217 (0.3234)	SegCLSLoss 0.0165 (0.0131)	KLLoss 0.3672 (0.2918)	MaskLoss 0.5512 (0.6803)	MaskBCELoss 0.0417 (0.0699)	MaskDICELoss 0.5095 (0.6105)
Epoch: [8][426/500]	Time  5.654 ( 5.654)	Loss 1.1484 (1.3448)	CeLoss 1.1484 (0.3776)	SegCLSLoss 0.0000 (0.0112)	KLLoss 0.0000 (0.2541)	MaskLoss 0.0000 (0.4682)	MaskBCELoss 0.0000 (0.0909)	MaskDICELoss 0.0000 (0.3773)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 424 is less than current step: 499. Dropping entry: {'train/loss': 1.7199573755264281, 'train/ce_loss': 0.323388671875, 'train/seg_cls_loss': 0.013079833984375, 'train/kl_loss': 0.291796875, 'train/mask_bce_loss': 0.06986045497469603, 'train/mask_dice_loss': 0.6104551553726196, 'train/mask_loss': 0.6803156018257142, 'metrics/total_secs_per_batch': 6.050861358642578, 'metrics/data_secs_per_batch': 2.7767461776733398, '_timestamp': 1740980592.394943}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 424 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980592.3952122}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 425 is less than current step: 499. Dropping entry: {'train/loss': 1.3448021113872528, 'train/ce_loss': 0.37763671875, 'train/seg_cls_loss': 0.011151123046875, 'train/kl_loss': 0.2541015625, 'train/mask_bce_loss': 0.09092339067719876, 'train/mask_dice_loss': 0.3772784411907196, 'train/mask_loss': 0.46820183396339415, 'metrics/total_secs_per_batch': 5.654220342636108, 'metrics/data_secs_per_batch': 2.647032046318054, '_timestamp': 1740980598.0491972}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 425 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980598.0495222}).
Epoch: [8][427/500]	Time  5.660 ( 5.660)	Loss 2.3236 (1.5960)	CeLoss 0.2197 (0.4535)	SegCLSLoss 0.0148 (0.0107)	KLLoss 0.3594 (0.2908)	MaskLoss 1.0300 (0.5539)	MaskBCELoss 0.2357 (0.1123)	MaskDICELoss 0.7942 (0.4417)
Epoch: [8][428/500]	Time  6.375 ( 6.375)	Loss 1.8682 (1.9173)	CeLoss 0.1904 (0.2009)	SegCLSLoss 0.0186 (0.0192)	KLLoss 0.3574 (0.3301)	MaskLoss 0.8169 (0.8370)	MaskBCELoss 0.0599 (0.1140)	MaskDICELoss 0.7570 (0.7230)
Epoch: [8][429/500]	Time  5.594 ( 5.594)	Loss 2.0633 (1.5169)	CeLoss 0.2207 (0.3483)	SegCLSLoss 0.0106 (0.0132)	KLLoss 0.3633 (0.2908)	MaskLoss 0.9008 (0.5665)	MaskBCELoss 0.2256 (0.1207)	MaskDICELoss 0.6752 (0.4458)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 426 is less than current step: 499. Dropping entry: {'train/loss': 1.5959661900997162, 'train/ce_loss': 0.453515625, 'train/seg_cls_loss': 0.010723876953125, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.11227809805423021, 'train/mask_dice_loss': 0.441662023961544, 'train/mask_loss': 0.553940124809742, 'metrics/total_secs_per_batch': 5.660398244857788, 'metrics/data_secs_per_batch': 2.439994478225708, '_timestamp': 1740980603.7095742}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 426 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980603.7098813}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 427 is less than current step: 499. Dropping entry: {'train/loss': 1.9172874450683595, 'train/ce_loss': 0.200927734375, 'train/seg_cls_loss': 0.019219970703125, 'train/kl_loss': 0.330078125, 'train/mask_bce_loss': 0.11396832717582583, 'train/mask_dice_loss': 0.7230201035737991, 'train/mask_loss': 0.8369884312152862, 'metrics/total_secs_per_batch': 6.375200510025024, 'metrics/data_secs_per_batch': 2.946072745323181, '_timestamp': 1740980610.0847354}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 427 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980610.0850253}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 428 is less than current step: 499. Dropping entry: {'train/loss': 1.516916561126709, 'train/ce_loss': 0.34833984375, 'train/seg_cls_loss': 0.0132080078125, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.12066930215805768, 'train/mask_dice_loss': 0.44579678028821945, 'train/mask_loss': 0.5664660856127739, 'metrics/total_secs_per_batch': 5.594028949737549, 'metrics/data_secs_per_batch': 2.4744659900665282, '_timestamp': 1740980615.6787646}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 428 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980615.6790497}).
[2025-03-02 23:43:41,183] [INFO] [logging.py:96:log_dist] [Rank 0] step=4430, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:43:41,189] [INFO] [timer.py:215:stop] epoch=0/micro_step=44300/global_step=4430, RunningAvgSamplesPerSec=1.4922124131179997, CurrSamplesPerSec=1.8150955060947347, MemAllocated=31.38GB, MaxMemAllocated=37.23GB
Epoch: [8][430/500]	Time  5.511 ( 5.511)	Loss 0.0698 (1.3864)	CeLoss 0.0698 (0.4439)	SegCLSLoss 0.0000 (0.0107)	KLLoss 0.0000 (0.2166)	MaskLoss 0.0000 (0.4578)	MaskBCELoss 0.0000 (0.0741)	MaskDICELoss 0.0000 (0.3836)
Epoch: [8][431/500]	Time  4.726 ( 4.726)	Loss 0.9922 (1.6307)	CeLoss 0.9922 (0.6355)	SegCLSLoss 0.0000 (0.0088)	KLLoss 0.0000 (0.1814)	MaskLoss 0.0000 (0.4864)	MaskBCELoss 0.0000 (0.1464)	MaskDICELoss 0.0000 (0.3399)
Epoch: [8][432/500]	Time  6.156 ( 6.156)	Loss 1.1947 (2.0397)	CeLoss 0.2539 (0.3846)	SegCLSLoss 0.0171 (0.0142)	KLLoss 0.3613 (0.2912)	MaskLoss 0.4489 (0.8096)	MaskBCELoss 0.0237 (0.2075)	MaskDICELoss 0.4252 (0.6020)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 429 is less than current step: 499. Dropping entry: {'train/loss': 1.3863561749458313, 'train/ce_loss': 0.443896484375, 'train/seg_cls_loss': 0.010711669921875, 'train/kl_loss': 0.2166015625, 'train/mask_bce_loss': 0.07414385126903653, 'train/mask_dice_loss': 0.38360942602157594, 'train/mask_loss': 0.45775328278541566, 'metrics/total_secs_per_batch': 5.510946273803711, 'metrics/data_secs_per_batch': 2.188957953453064, '_timestamp': 1740980621.1894872}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 429 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980621.1896727}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 430 is less than current step: 499. Dropping entry: {'train/loss': 1.630749273300171, 'train/ce_loss': 0.635546875, 'train/seg_cls_loss': 0.008758544921875, 'train/kl_loss': 0.1814453125, 'train/mask_bce_loss': 0.146434872969985, 'train/mask_dice_loss': 0.33993584513664243, 'train/mask_loss': 0.48637073040008544, 'metrics/total_secs_per_batch': 4.726190090179443, 'metrics/data_secs_per_batch': 2.0719576597213747, '_timestamp': 1740980625.9159153}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 430 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980625.916197}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 431 is less than current step: 499. Dropping entry: {'train/loss': 2.0396663427352903, 'train/ce_loss': 0.3845703125, 'train/seg_cls_loss': 0.014208984375, 'train/kl_loss': 0.2912109375, 'train/mask_bce_loss': 0.20754338949918746, 'train/mask_dice_loss': 0.6020358711481094, 'train/mask_loss': 0.8095792531967163, 'metrics/total_secs_per_batch': 6.1564435958862305, 'metrics/data_secs_per_batch': 2.6910705089569094, '_timestamp': 1740980632.07234}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 431 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980632.0726266}).
Epoch: [8][433/500]	Time  4.533 ( 4.533)	Loss 0.7031 (1.6121)	CeLoss 0.7031 (0.6027)	SegCLSLoss 0.0000 (0.0102)	KLLoss 0.0000 (0.2193)	MaskLoss 0.0000 (0.4910)	MaskBCELoss 0.0000 (0.1229)	MaskDICELoss 0.0000 (0.3682)
Epoch: [8][434/500]	Time  5.070 ( 5.070)	Loss 0.5273 (1.5086)	CeLoss 0.5273 (0.4853)	SegCLSLoss 0.0000 (0.0134)	KLLoss 0.0000 (0.2188)	MaskLoss 0.0000 (0.4974)	MaskBCELoss 0.0000 (0.1004)	MaskDICELoss 0.0000 (0.3970)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 432 is less than current step: 499. Dropping entry: {'train/loss': 1.612051510810852, 'train/ce_loss': 0.602734375, 'train/seg_cls_loss': 0.010205078125, 'train/kl_loss': 0.2193359375, 'train/mask_bce_loss': 0.12286240160465241, 'train/mask_dice_loss': 0.3681731097400188, 'train/mask_loss': 0.49103550612926483, 'metrics/total_secs_per_batch': 4.532617568969727, 'metrics/data_secs_per_batch': 1.8681175470352174, '_timestamp': 1740980636.6049469}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 432 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980636.6051354}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 433 is less than current step: 499. Dropping entry: {'train/loss': 1.5085667610168456, 'train/ce_loss': 0.48525390625, 'train/seg_cls_loss': 0.013427734375, 'train/kl_loss': 0.21875, 'train/mask_bce_loss': 0.10036047995090484, 'train/mask_dice_loss': 0.39703812301158903, 'train/mask_loss': 0.4973986119031906, 'metrics/total_secs_per_batch': 5.069957733154297, 'metrics/data_secs_per_batch': 2.5249375104904175, '_timestamp': 1740980641.6749628}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 433 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980641.6752546}).
Epoch: [8][435/500]	Time  6.213 ( 6.213)	Loss 2.1674 (1.7803)	CeLoss 0.2188 (0.3016)	SegCLSLoss 0.0193 (0.0164)	KLLoss 0.3574 (0.3219)	MaskLoss 0.9519 (0.7192)	MaskBCELoss 0.0208 (0.0589)	MaskDICELoss 0.9310 (0.6603)
Epoch: [8][436/500]	Time  6.389 ( 6.389)	Loss 1.5381 (1.3831)	CeLoss 0.1631 (0.2898)	SegCLSLoss 0.0206 (0.0149)	KLLoss 0.3555 (0.2910)	MaskLoss 0.6646 (0.5284)	MaskBCELoss 0.0067 (0.0963)	MaskDICELoss 0.6579 (0.4321)
Epoch: [8][437/500]	Time  6.150 ( 6.150)	Loss 1.7248 (2.0627)	CeLoss 0.1914 (0.3488)	SegCLSLoss 0.0258 (0.0183)	KLLoss 0.3652 (0.3275)	MaskLoss 0.7423 (0.8361)	MaskBCELoss 0.0062 (0.1385)	MaskDICELoss 0.7361 (0.6975)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 434 is less than current step: 499. Dropping entry: {'train/loss': 1.7802967190742494, 'train/ce_loss': 0.3015625, 'train/seg_cls_loss': 0.016448974609375, 'train/kl_loss': 0.321875, 'train/mask_bce_loss': 0.058930404111742975, 'train/mask_dice_loss': 0.6603195190429687, 'train/mask_loss': 0.7192499175667763, 'metrics/total_secs_per_batch': 6.21252703666687, 'metrics/data_secs_per_batch': 2.7557600498199464, '_timestamp': 1740980647.8874912}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 434 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980647.8877802}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 435 is less than current step: 499. Dropping entry: {'train/loss': 1.383131980895996, 'train/ce_loss': 0.28984375, 'train/seg_cls_loss': 0.014892578125, 'train/kl_loss': 0.291015625, 'train/mask_bce_loss': 0.09626092216931284, 'train/mask_dice_loss': 0.4321214765310287, 'train/mask_loss': 0.5283824026584625, 'metrics/total_secs_per_batch': 6.3885369300842285, 'metrics/data_secs_per_batch': 2.8721529722213743, '_timestamp': 1740980654.2760024}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 435 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980654.2762873}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 436 is less than current step: 499. Dropping entry: {'train/loss': 2.062726366519928, 'train/ce_loss': 0.348828125, 'train/seg_cls_loss': 0.0183349609375, 'train/kl_loss': 0.3275390625, 'train/mask_bce_loss': 0.13853465327993036, 'train/mask_dice_loss': 0.6975160419940949, 'train/mask_loss': 0.8360506951808929, 'metrics/total_secs_per_batch': 6.149894714355469, 'metrics/data_secs_per_batch': 2.6336233854293822, '_timestamp': 1740980660.4260762}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 436 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980660.4264188}).
Epoch: [8][438/500]	Time  5.778 ( 5.778)	Loss 2.5235 (1.4056)	CeLoss 0.2188 (0.2520)	SegCLSLoss 0.0167 (0.0137)	KLLoss 0.3555 (0.2541)	MaskLoss 1.1299 (0.5605)	MaskBCELoss 0.3481 (0.1157)	MaskDICELoss 0.7818 (0.4448)
Epoch: [8][439/500]	Time  6.757 ( 6.757)	Loss 1.4531 (1.6353)	CeLoss 1.4531 (0.3384)	SegCLSLoss 0.0000 (0.0150)	KLLoss 0.0000 (0.2889)	MaskLoss 0.0000 (0.6302)	MaskBCELoss 0.0000 (0.1115)	MaskDICELoss 0.0000 (0.5187)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 437 is less than current step: 499. Dropping entry: {'train/loss': 1.4056297183036803, 'train/ce_loss': 0.2519775390625, 'train/seg_cls_loss': 0.01373291015625, 'train/kl_loss': 0.2541015625, 'train/mask_bce_loss': 0.11566653139889241, 'train/mask_dice_loss': 0.44482655823230743, 'train/mask_loss': 0.5604930847883225, 'metrics/total_secs_per_batch': 5.777997732162476, 'metrics/data_secs_per_batch': 2.6877365589141844, '_timestamp': 1740980666.2038705}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 437 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980666.2040727}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 438 is less than current step: 499. Dropping entry: {'train/loss': 1.6353171229362489, 'train/ce_loss': 0.33837890625, 'train/seg_cls_loss': 0.015032958984375, 'train/kl_loss': 0.2888671875, 'train/mask_bce_loss': 0.1115409281104803, 'train/mask_dice_loss': 0.5186664491891861, 'train/mask_loss': 0.6302073895931244, 'metrics/total_secs_per_batch': 6.756774425506592, 'metrics/data_secs_per_batch': 2.8123961687088013, '_timestamp': 1740980672.9606674}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 438 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980672.9609423}).
[2025-03-02 23:44:38,479] [INFO] [logging.py:96:log_dist] [Rank 0] step=4440, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:44:38,485] [INFO] [timer.py:215:stop] epoch=0/micro_step=44400/global_step=4440, RunningAvgSamplesPerSec=1.4927007878516207, CurrSamplesPerSec=1.810371659110402, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [8][440/500]	Time  5.525 ( 5.525)	Loss 2.3422 (1.7036)	CeLoss 0.2109 (0.3084)	SegCLSLoss 0.0143 (0.0148)	KLLoss 0.3633 (0.3273)	MaskLoss 1.0442 (0.6775)	MaskBCELoss 0.4061 (0.1565)	MaskDICELoss 0.6381 (0.5210)
Epoch: [8][441/500]	Time  5.851 ( 5.851)	Loss 1.5696 (1.5794)	CeLoss 0.2422 (0.4059)	SegCLSLoss 0.0109 (0.0126)	KLLoss 0.3652 (0.2887)	MaskLoss 0.6422 (0.5692)	MaskBCELoss 0.0819 (0.0785)	MaskDICELoss 0.5603 (0.4907)
Epoch: [8][442/500]	Time  5.767 ( 5.767)	Loss 1.3993 (1.7599)	CeLoss 0.2168 (0.4115)	SegCLSLoss 0.0173 (0.0136)	KLLoss 0.3633 (0.2895)	MaskLoss 0.5688 (0.6562)	MaskBCELoss 0.0592 (0.1452)	MaskDICELoss 0.5095 (0.5110)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 439 is less than current step: 499. Dropping entry: {'train/loss': 1.703572291135788, 'train/ce_loss': 0.3083984375, 'train/seg_cls_loss': 0.0148193359375, 'train/kl_loss': 0.32734375, 'train/mask_bce_loss': 0.15650635715574027, 'train/mask_dice_loss': 0.521012207865715, 'train/mask_loss': 0.677518567442894, 'metrics/total_secs_per_batch': 5.525372505187988, 'metrics/data_secs_per_batch': 2.4998636484146117, '_timestamp': 1740980678.4858544}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 439 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980678.4861553}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 440 is less than current step: 499. Dropping entry: {'train/loss': 1.5793639719486237, 'train/ce_loss': 0.405859375, 'train/seg_cls_loss': 0.012628173828125, 'train/kl_loss': 0.288671875, 'train/mask_bce_loss': 0.07848406750708818, 'train/mask_dice_loss': 0.49069008976221085, 'train/mask_loss': 0.5691741615533829, 'metrics/total_secs_per_batch': 5.850680351257324, 'metrics/data_secs_per_batch': 2.4764501094818114, '_timestamp': 1740980684.3367097}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 440 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980684.3369825}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 441 is less than current step: 499. Dropping entry: {'train/loss': 1.759868037700653, 'train/ce_loss': 0.4115234375, 'train/seg_cls_loss': 0.01363525390625, 'train/kl_loss': 0.289453125, 'train/mask_bce_loss': 0.1452302996069193, 'train/mask_dice_loss': 0.5109732508659363, 'train/mask_loss': 0.6562035501003265, 'metrics/total_secs_per_batch': 5.767199754714966, 'metrics/data_secs_per_batch': 2.229253125190735, '_timestamp': 1740980690.1039212}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 441 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980690.1041899}).
Epoch: [8][443/500]	Time  5.407 ( 5.407)	Loss 2.1073 (1.6446)	CeLoss 0.2461 (0.4436)	SegCLSLoss 0.0107 (0.0126)	KLLoss 0.3691 (0.2539)	MaskLoss 0.9091 (0.5846)	MaskBCELoss 0.2766 (0.1495)	MaskDICELoss 0.6325 (0.4351)
Epoch: [8][444/500]	Time  5.589 ( 5.589)	Loss 1.4730 (1.3996)	CeLoss 0.2109 (0.6266)	SegCLSLoss 0.0134 (0.0081)	KLLoss 0.3633 (0.2172)	MaskLoss 0.6095 (0.3737)	MaskBCELoss 0.0714 (0.0453)	MaskDICELoss 0.5382 (0.3283)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 442 is less than current step: 499. Dropping entry: {'train/loss': 1.6445534467697143, 'train/ce_loss': 0.4435546875, 'train/seg_cls_loss': 0.012640380859375, 'train/kl_loss': 0.25390625, 'train/mask_bce_loss': 0.14953060681000352, 'train/mask_dice_loss': 0.4350508004426956, 'train/mask_loss': 0.5845814108848572, 'metrics/total_secs_per_batch': 5.406523942947388, 'metrics/data_secs_per_batch': 2.0462795734405517, '_timestamp': 1740980695.5106335}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 442 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980695.5109756}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 443 is less than current step: 499. Dropping entry: {'train/loss': 1.3996482968330384, 'train/ce_loss': 0.6265625, 'train/seg_cls_loss': 0.00811767578125, 'train/kl_loss': 0.2171875, 'train/mask_bce_loss': 0.04533615242689848, 'train/mask_dice_loss': 0.32831611186265947, 'train/mask_loss': 0.3736522704362869, 'metrics/total_secs_per_batch': 5.5885231494903564, 'metrics/data_secs_per_batch': 2.4266348838806153, '_timestamp': 1740980701.0990334}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 443 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980701.0994036}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 444 is less than current step: 499. Dropping entry: {'train/loss': 2.06707124710083, 'train/ce_loss': 0.294140625, 'train/seg_cls_loss': 0.0170654296875, 'train/kl_loss': 0.3240234375, 'train/mask_bce_loss': 0.15874708481132985, 'train/mask_dice_loss': 0.7072592407464982, 'train/mask_loss': 0.8660063087940216, 'metrics/total_secs_per_batch': 6.106350421905518, 'metrics/data_secs_per_batch': 2.8643420457839968, '_timestamp': 1740980707.2054656}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 444 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980707.20579}).
Epoch: [8][445/500]	Time  6.106 ( 6.106)	Loss 2.5430 (2.0671)	CeLoss 0.1270 (0.2941)	SegCLSLoss 0.0259 (0.0171)	KLLoss 0.3789 (0.3240)	MaskLoss 1.1826 (0.8660)	MaskBCELoss 0.2466 (0.1587)	MaskDICELoss 0.9360 (0.7073)
Epoch: [8][446/500]	Time  5.406 ( 5.406)	Loss 1.6165 (1.5220)	CeLoss 0.2295 (0.2925)	SegCLSLoss 0.0107 (0.0162)	KLLoss 0.3633 (0.3301)	MaskLoss 0.6725 (0.5941)	MaskBCELoss 0.0745 (0.1193)	MaskDICELoss 0.5980 (0.4748)
Epoch: [8][447/500]	Time  6.578 ( 6.578)	Loss 1.6779 (1.5973)	CeLoss 0.2227 (0.2715)	SegCLSLoss 0.0183 (0.0139)	KLLoss 0.3594 (0.2896)	MaskLoss 0.7052 (0.6450)	MaskBCELoss 0.0703 (0.0974)	MaskDICELoss 0.6348 (0.5476)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 445 is less than current step: 499. Dropping entry: {'train/loss': 1.5220038414001464, 'train/ce_loss': 0.292529296875, 'train/seg_cls_loss': 0.016204833984375, 'train/kl_loss': 0.330078125, 'train/mask_bce_loss': 0.11929653491824865, 'train/mask_dice_loss': 0.4748108580708504, 'train/mask_loss': 0.594107386469841, 'metrics/total_secs_per_batch': 5.406224250793457, 'metrics/data_secs_per_batch': 2.4196561574935913, '_timestamp': 1740980712.6117077}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 445 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980712.612042}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 446 is less than current step: 499. Dropping entry: {'train/loss': 1.5973300695419312, 'train/ce_loss': 0.271533203125, 'train/seg_cls_loss': 0.013934326171875, 'train/kl_loss': 0.2896484375, 'train/mask_bce_loss': 0.09736882783472538, 'train/mask_dice_loss': 0.5476096987724304, 'train/mask_loss': 0.6449785113334656, 'metrics/total_secs_per_batch': 6.577566146850586, 'metrics/data_secs_per_batch': 2.8094167709350586, '_timestamp': 1740980719.1891487}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 446 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980719.1894908}).
Epoch: [8][448/500]	Time  5.325 ( 5.325)	Loss 2.8439 (1.7905)	CeLoss 0.1973 (0.4563)	SegCLSLoss 0.0161 (0.0132)	KLLoss 0.3613 (0.2541)	MaskLoss 1.3013 (0.6509)	MaskBCELoss 0.5392 (0.1549)	MaskDICELoss 0.7621 (0.4960)
Epoch: [8][449/500]	Time  6.045 ( 6.045)	Loss 1.2638 (1.7907)	CeLoss 0.2363 (0.2807)	SegCLSLoss 0.0215 (0.0168)	KLLoss 0.3730 (0.2928)	MaskLoss 0.4893 (0.7361)	MaskBCELoss 0.1241 (0.1090)	MaskDICELoss 0.3653 (0.6271)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 447 is less than current step: 499. Dropping entry: {'train/loss': 1.790459108352661, 'train/ce_loss': 0.45634765625, 'train/seg_cls_loss': 0.013189697265625, 'train/kl_loss': 0.2541015625, 'train/mask_bce_loss': 0.15489616971462966, 'train/mask_dice_loss': 0.4960462719202042, 'train/mask_loss': 0.6509424388408661, 'metrics/total_secs_per_batch': 5.3245015144348145, 'metrics/data_secs_per_batch': 2.1761363744735718, '_timestamp': 1740980724.513651}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 447 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980724.5139308}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 448 is less than current step: 499. Dropping entry: {'train/loss': 1.7906710386276246, 'train/ce_loss': 0.2806640625, 'train/seg_cls_loss': 0.01676025390625, 'train/kl_loss': 0.2927734375, 'train/mask_bce_loss': 0.10901529369875788, 'train/mask_dice_loss': 0.6270917117595672, 'train/mask_loss': 0.7361070096492768, 'metrics/total_secs_per_batch': 6.045062065124512, 'metrics/data_secs_per_batch': 2.612474465370178, '_timestamp': 1740980730.558688}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 448 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980730.5589538}).
[2025-03-02 23:45:38,189] [INFO] [logging.py:96:log_dist] [Rank 0] step=4450, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:45:38,194] [INFO] [timer.py:215:stop] epoch=0/micro_step=44500/global_step=4450, RunningAvgSamplesPerSec=1.4930663677165092, CurrSamplesPerSec=1.3097510707157172, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [8][450/500]	Time  7.637 ( 7.637)	Loss 2.0980 (1.8527)	CeLoss 0.2207 (0.2331)	SegCLSLoss 0.0245 (0.0153)	KLLoss 0.3516 (0.3600)	MaskLoss 0.9143 (0.7881)	MaskBCELoss 0.0378 (0.1087)	MaskDICELoss 0.8765 (0.6794)
Epoch: [8][451/500]	Time  5.911 ( 5.911)	Loss 1.1641 (1.6247)	CeLoss 1.1641 (0.4561)	SegCLSLoss 0.0000 (0.0094)	KLLoss 0.0000 (0.2191)	MaskLoss 0.0000 (0.5711)	MaskBCELoss 0.0000 (0.1562)	MaskDICELoss 0.0000 (0.4148)
Epoch: [8][452/500]	Time  5.837 ( 5.837)	Loss 1.8107 (1.3140)	CeLoss 0.2051 (0.2522)	SegCLSLoss 0.0223 (0.0132)	KLLoss 0.3535 (0.2908)	MaskLoss 0.7794 (0.5132)	MaskBCELoss 0.0311 (0.0708)	MaskDICELoss 0.7482 (0.4424)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 449 is less than current step: 499. Dropping entry: {'train/loss': 1.852660596370697, 'train/ce_loss': 0.23310546875, 'train/seg_cls_loss': 0.015283203125, 'train/kl_loss': 0.3599609375, 'train/mask_bce_loss': 0.1087394137866795, 'train/mask_dice_loss': 0.6793584674596786, 'train/mask_loss': 0.7880978792905807, 'metrics/total_secs_per_batch': 7.636600017547607, 'metrics/data_secs_per_batch': 3.2311994314193724, '_timestamp': 1740980738.195205}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 449 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980738.1955662}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 450 is less than current step: 499. Dropping entry: {'train/loss': 1.6246649384498597, 'train/ce_loss': 0.4560546875, 'train/seg_cls_loss': 0.00938720703125, 'train/kl_loss': 0.219140625, 'train/mask_bce_loss': 0.15624217577278615, 'train/mask_dice_loss': 0.4148305177688599, 'train/mask_loss': 0.5710726976394653, 'metrics/total_secs_per_batch': 5.911484956741333, 'metrics/data_secs_per_batch': 2.638690638542175, '_timestamp': 1740980744.106835}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 450 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980744.1071367}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 451 is less than current step: 499. Dropping entry: {'train/loss': 1.3140019476413727, 'train/ce_loss': 0.252197265625, 'train/seg_cls_loss': 0.0131591796875, 'train/kl_loss': 0.2908203125, 'train/mask_bce_loss': 0.07076866459101439, 'train/mask_dice_loss': 0.44240906834602356, 'train/mask_loss': 0.5131777375936508, 'metrics/total_secs_per_batch': 5.837484359741211, 'metrics/data_secs_per_batch': 2.432949924468994, '_timestamp': 1740980749.9442985}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 451 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980749.9446015}).
Epoch: [8][453/500]	Time  6.049 ( 6.049)	Loss 1.7769 (1.4486)	CeLoss 0.2285 (0.5504)	SegCLSLoss 0.0154 (0.0114)	KLLoss 0.3574 (0.2166)	MaskLoss 0.7527 (0.4356)	MaskBCELoss 0.0878 (0.0689)	MaskDICELoss 0.6649 (0.3667)
Epoch: [8][454/500]	Time  6.682 ( 6.682)	Loss 1.1953 (1.9722)	CeLoss 1.1953 (0.3156)	SegCLSLoss 0.0000 (0.0177)	KLLoss 0.0000 (0.3299)	MaskLoss 0.0000 (0.8074)	MaskBCELoss 0.0000 (0.1943)	MaskDICELoss 0.0000 (0.6131)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 452 is less than current step: 499. Dropping entry: {'train/loss': 1.4486361265182495, 'train/ce_loss': 0.550390625, 'train/seg_cls_loss': 0.011395263671875, 'train/kl_loss': 0.2166015625, 'train/mask_bce_loss': 0.06891051027923822, 'train/mask_dice_loss': 0.3667356789112091, 'train/mask_loss': 0.43564618825912477, 'metrics/total_secs_per_batch': 6.049350738525391, 'metrics/data_secs_per_batch': 3.0279000759124757, '_timestamp': 1740980755.9937491}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 452 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980755.9940734}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 453 is less than current step: 499. Dropping entry: {'train/loss': 1.9722102284431458, 'train/ce_loss': 0.315625, 'train/seg_cls_loss': 0.017657470703125, 'train/kl_loss': 0.3298828125, 'train/mask_bce_loss': 0.19429893838241696, 'train/mask_dice_loss': 0.6130952268838883, 'train/mask_loss': 0.8073941707611084, 'metrics/total_secs_per_batch': 6.681929111480713, 'metrics/data_secs_per_batch': 3.1018010854721068, '_timestamp': 1740980762.6757038}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 453 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980762.676009}).
Epoch: [8][455/500]	Time  5.602 ( 5.602)	Loss 3.2933 (1.6808)	CeLoss 0.2207 (0.2742)	SegCLSLoss 0.0144 (0.0150)	KLLoss 0.3594 (0.3285)	MaskLoss 1.5148 (0.6831)	MaskBCELoss 0.8133 (0.1720)	MaskDICELoss 0.7015 (0.5110)
Epoch: [8][456/500]	Time  5.087 ( 5.087)	Loss 1.5429 (1.5144)	CeLoss 0.2910 (0.3145)	SegCLSLoss 0.0123 (0.0128)	KLLoss 0.3633 (0.2531)	MaskLoss 0.6045 (0.5840)	MaskBCELoss 0.2475 (0.0986)	MaskDICELoss 0.3570 (0.4854)
Epoch: [8][457/500]	Time  5.818 ( 5.818)	Loss 1.0312 (1.4635)	CeLoss 1.0312 (0.4097)	SegCLSLoss 0.0000 (0.0153)	KLLoss 0.0000 (0.2885)	MaskLoss 0.0000 (0.5087)	MaskBCELoss 0.0000 (0.0980)	MaskDICELoss 0.0000 (0.4107)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 454 is less than current step: 499. Dropping entry: {'train/loss': 1.6807696223258972, 'train/ce_loss': 0.27421875, 'train/seg_cls_loss': 0.0150390625, 'train/kl_loss': 0.328515625, 'train/mask_bce_loss': 0.1720475791953504, 'train/mask_dice_loss': 0.5110130198299885, 'train/mask_loss': 0.6830605879426003, 'metrics/total_secs_per_batch': 5.602339029312134, 'metrics/data_secs_per_batch': 2.5063580513000487, '_timestamp': 1740980768.2779162}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 454 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980768.2782037}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 455 is less than current step: 499. Dropping entry: {'train/loss': 1.5144125938415527, 'train/ce_loss': 0.314501953125, 'train/seg_cls_loss': 0.0128173828125, 'train/kl_loss': 0.253125, 'train/mask_bce_loss': 0.09860407710075378, 'train/mask_dice_loss': 0.4854332715272903, 'train/mask_loss': 0.5840373635292053, 'metrics/total_secs_per_batch': 5.086705684661865, 'metrics/data_secs_per_batch': 2.3153717517852783, '_timestamp': 1740980773.3646328}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 455 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980773.3649228}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 456 is less than current step: 499. Dropping entry: {'train/loss': 1.4634639501571656, 'train/ce_loss': 0.40966796875, 'train/seg_cls_loss': 0.0153076171875, 'train/kl_loss': 0.2884765625, 'train/mask_bce_loss': 0.09802617342211306, 'train/mask_dice_loss': 0.4106589287519455, 'train/mask_loss': 0.5086850970983505, 'metrics/total_secs_per_batch': 5.818339586257935, 'metrics/data_secs_per_batch': 2.7140999317169188, '_timestamp': 1740980779.1831534}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 456 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980779.183499}).
Epoch: [8][458/500]	Time  6.072 ( 6.072)	Loss 1.3359 (1.1446)	CeLoss 1.3359 (0.4245)	SegCLSLoss 0.0000 (0.0107)	KLLoss 0.0000 (0.2559)	MaskLoss 0.0000 (0.3446)	MaskBCELoss 0.0000 (0.0812)	MaskDICELoss 0.0000 (0.2634)
Epoch: [8][459/500]	Time  6.071 ( 6.071)	Loss 1.4967 (1.8698)	CeLoss 0.2363 (0.3402)	SegCLSLoss 0.0115 (0.0146)	KLLoss 0.3652 (0.3295)	MaskLoss 0.6087 (0.7445)	MaskBCELoss 0.1919 (0.1135)	MaskDICELoss 0.4168 (0.6310)
[2025-03-02 23:46:37,055] [INFO] [logging.py:96:log_dist] [Rank 0] step=4460, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:46:37,061] [INFO] [timer.py:215:stop] epoch=0/micro_step=44600/global_step=4460, RunningAvgSamplesPerSec=1.493472629148307, CurrSamplesPerSec=1.743627366696587, MemAllocated=30.79GB, MaxMemAllocated=37.23GB
Epoch: [8][460/500]	Time  5.737 ( 5.737)	Loss 0.7584 (1.7617)	CeLoss 0.3047 (0.4185)	SegCLSLoss 0.0141 (0.0143)	KLLoss 0.3750 (0.2576)	MaskLoss 0.2044 (0.6551)	MaskBCELoss 0.1082 (0.1773)	MaskDICELoss 0.0962 (0.4778)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 457 is less than current step: 499. Dropping entry: {'train/loss': 1.144584384560585, 'train/ce_loss': 0.42451171875, 'train/seg_cls_loss': 0.0106689453125, 'train/kl_loss': 0.255859375, 'train/mask_bce_loss': 0.08121844474226236, 'train/mask_dice_loss': 0.263388192653656, 'train/mask_loss': 0.34460663869977, 'metrics/total_secs_per_batch': 6.071687936782837, 'metrics/data_secs_per_batch': 2.9595105409622193, '_timestamp': 1740980785.254641}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 457 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980785.2548404}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 458 is less than current step: 499. Dropping entry: {'train/loss': 1.8697785198688508, 'train/ce_loss': 0.340234375, 'train/seg_cls_loss': 0.014593505859375, 'train/kl_loss': 0.3294921875, 'train/mask_bce_loss': 0.1135067829862237, 'train/mask_dice_loss': 0.6310016140341759, 'train/mask_loss': 0.7445084005594254, 'metrics/total_secs_per_batch': 6.070911884307861, 'metrics/data_secs_per_batch': 2.616197681427002, '_timestamp': 1740980791.3255844}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 458 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980791.325875}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 459 is less than current step: 499. Dropping entry: {'train/loss': 1.7617438793182374, 'train/ce_loss': 0.41845703125, 'train/seg_cls_loss': 0.01429443359375, 'train/kl_loss': 0.2576171875, 'train/mask_bce_loss': 0.17733894637785852, 'train/mask_dice_loss': 0.47780055850744246, 'train/mask_loss': 0.655139508843422, 'metrics/total_secs_per_batch': 5.736752271652222, 'metrics/data_secs_per_batch': 2.590200638771057, '_timestamp': 1740980797.0621312}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 459 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980797.0623918}).
Epoch: [8][461/500]	Time  5.865 ( 5.865)	Loss 1.8416 (1.4347)	CeLoss 0.2100 (0.5004)	SegCLSLoss 0.0240 (0.0089)	KLLoss 0.3652 (0.2176)	MaskLoss 0.7919 (0.4542)	MaskBCELoss 0.0088 (0.0589)	MaskDICELoss 0.7831 (0.3953)
Epoch: [8][462/500]	Time  6.231 ( 6.231)	Loss 0.8602 (1.5969)	CeLoss 0.2471 (0.3201)	SegCLSLoss 0.0146 (0.0143)	KLLoss 0.3789 (0.3303)	MaskLoss 0.2846 (0.6184)	MaskBCELoss 0.1606 (0.1511)	MaskDICELoss 0.1240 (0.4673)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 460 is less than current step: 499. Dropping entry: {'train/loss': 1.4346892356872558, 'train/ce_loss': 0.500390625, 'train/seg_cls_loss': 0.008941650390625, 'train/kl_loss': 0.217578125, 'train/mask_bce_loss': 0.05888809934258461, 'train/mask_dice_loss': 0.395272921025753, 'train/mask_loss': 0.4541610151529312, 'metrics/total_secs_per_batch': 5.864593982696533, 'metrics/data_secs_per_batch': 2.3558109045028686, '_timestamp': 1740980802.9270098}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 460 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980802.9274154}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 461 is less than current step: 499. Dropping entry: {'train/loss': 1.5969194531440736, 'train/ce_loss': 0.3201171875, 'train/seg_cls_loss': 0.01434326171875, 'train/kl_loss': 0.3302734375, 'train/mask_bce_loss': 0.15109903533011676, 'train/mask_dice_loss': 0.4673314020037651, 'train/mask_loss': 0.6184304356575012, 'metrics/total_secs_per_batch': 6.230689287185669, 'metrics/data_secs_per_batch': 2.7462273836135864, '_timestamp': 1740980809.1577737}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 461 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980809.158113}).
Epoch: [8][463/500]	Time  6.156 ( 6.156)	Loss 0.1514 (1.7172)	CeLoss 0.1514 (0.3472)	SegCLSLoss 0.0000 (0.0107)	KLLoss 0.0000 (0.2537)	MaskLoss 0.0000 (0.6696)	MaskBCELoss 0.0000 (0.1377)	MaskDICELoss 0.0000 (0.5319)
Epoch: [8][464/500]	Time  6.964 ( 6.964)	Loss 2.1406 (1.5260)	CeLoss 0.1943 (0.2798)	SegCLSLoss 0.0200 (0.0134)	KLLoss 0.3555 (0.2549)	MaskLoss 0.9502 (0.6069)	MaskBCELoss 0.0147 (0.1468)	MaskDICELoss 0.9355 (0.4601)
Epoch: [8][465/500]	Time  4.734 ( 4.734)	Loss 2.1992 (1.5942)	CeLoss 0.2422 (0.5797)	SegCLSLoss 0.0134 (0.0127)	KLLoss 0.3672 (0.2188)	MaskLoss 0.9570 (0.4931)	MaskBCELoss 0.3912 (0.1429)	MaskDICELoss 0.5658 (0.3502)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 462 is less than current step: 499. Dropping entry: {'train/loss': 1.7171649575233459, 'train/ce_loss': 0.34716796875, 'train/seg_cls_loss': 0.01070556640625, 'train/kl_loss': 0.2537109375, 'train/mask_bce_loss': 0.13768341280519963, 'train/mask_dice_loss': 0.5318854093551636, 'train/mask_loss': 0.6695688188076019, 'metrics/total_secs_per_batch': 6.156456232070923, 'metrics/data_secs_per_batch': 2.5944893598556518, '_timestamp': 1740980815.3140981}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 462 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980815.3143919}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 463 is less than current step: 499. Dropping entry: {'train/loss': 1.5260022580623627, 'train/ce_loss': 0.27978515625, 'train/seg_cls_loss': 0.013421630859375, 'train/kl_loss': 0.2548828125, 'train/mask_bce_loss': 0.14681542748585344, 'train/mask_dice_loss': 0.46008220613002776, 'train/mask_loss': 0.6068976193666458, 'metrics/total_secs_per_batch': 6.963581085205078, 'metrics/data_secs_per_batch': 3.0657004594802855, '_timestamp': 1740980822.2776845}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 463 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980822.277997}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 464 is less than current step: 499. Dropping entry: {'train/loss': 1.5942054271697998, 'train/ce_loss': 0.5796875, 'train/seg_cls_loss': 0.012725830078125, 'train/kl_loss': 0.21875, 'train/mask_bce_loss': 0.14290100736543537, 'train/mask_dice_loss': 0.3502466380596161, 'train/mask_loss': 0.49314764738082884, 'metrics/total_secs_per_batch': 4.734368085861206, 'metrics/data_secs_per_batch': 2.0429133653640745, '_timestamp': 1740980827.0122063}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 464 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980827.0126264}).
Epoch: [8][466/500]	Time  5.527 ( 5.527)	Loss 1.4403 (1.4104)	CeLoss 0.2461 (0.5399)	SegCLSLoss 0.0146 (0.0086)	KLLoss 0.3633 (0.2170)	MaskLoss 0.5746 (0.4221)	MaskBCELoss 0.0685 (0.0631)	MaskDICELoss 0.5062 (0.3590)
Epoch: [8][467/500]	Time  4.929 ( 4.929)	Loss 1.9008 (1.7198)	CeLoss 0.3301 (0.7513)	SegCLSLoss 0.0142 (0.0097)	KLLoss 0.3535 (0.2150)	MaskLoss 0.7639 (0.4711)	MaskBCELoss 0.0939 (0.0739)	MaskDICELoss 0.6700 (0.3972)
Epoch: [8][468/500]	Time  5.642 ( 5.642)	Loss 2.6686 (1.6416)	CeLoss 0.1621 (0.4084)	SegCLSLoss 0.0332 (0.0132)	KLLoss 0.3750 (0.2537)	MaskLoss 1.2259 (0.6005)	MaskBCELoss 0.3010 (0.1091)	MaskDICELoss 0.9249 (0.4914)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 465 is less than current step: 499. Dropping entry: {'train/loss': 1.4103757977485656, 'train/ce_loss': 0.53994140625, 'train/seg_cls_loss': 0.008563232421875, 'train/kl_loss': 0.2169921875, 'train/mask_bce_loss': 0.06309116664342582, 'train/mask_dice_loss': 0.3590400844812393, 'train/mask_loss': 0.42213125228881837, 'metrics/total_secs_per_batch': 5.527053356170654, 'metrics/data_secs_per_batch': 2.31663064956665, '_timestamp': 1740980832.5391133}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 465 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980832.5393934}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 466 is less than current step: 499. Dropping entry: {'train/loss': 1.7198395252227783, 'train/ce_loss': 0.75126953125, 'train/seg_cls_loss': 0.009686279296875, 'train/kl_loss': 0.2150390625, 'train/mask_bce_loss': 0.07387264305725694, 'train/mask_dice_loss': 0.3971799373626709, 'train/mask_loss': 0.47105257511138915, 'metrics/total_secs_per_batch': 4.928845405578613, 'metrics/data_secs_per_batch': 2.310593104362488, '_timestamp': 1740980837.4680748}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 466 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980837.4683995}).
Epoch: [8][469/500]	Time  6.039 ( 6.039)	Loss 1.5674 (1.6198)	CeLoss 0.2227 (0.2053)	SegCLSLoss 0.0187 (0.0172)	KLLoss 0.3535 (0.3256)	MaskLoss 0.6499 (0.6867)	MaskBCELoss 0.0400 (0.1177)	MaskDICELoss 0.6099 (0.5690)
[2025-03-02 23:47:35,925] [INFO] [logging.py:96:log_dist] [Rank 0] step=4470, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:47:35,931] [INFO] [timer.py:215:stop] epoch=0/micro_step=44700/global_step=4470, RunningAvgSamplesPerSec=1.4938771567109317, CurrSamplesPerSec=1.4745700282281997, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [8][470/500]	Time  6.783 ( 6.783)	Loss 2.5780 (1.7945)	CeLoss 0.2148 (0.2977)	SegCLSLoss 0.0203 (0.0179)	KLLoss 0.3730 (0.3273)	MaskLoss 1.1581 (0.7274)	MaskBCELoss 0.3153 (0.1435)	MaskDICELoss 0.8428 (0.5839)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 467 is less than current step: 499. Dropping entry: {'train/loss': 1.6415528178215026, 'train/ce_loss': 0.408447265625, 'train/seg_cls_loss': 0.013165283203125, 'train/kl_loss': 0.2537109375, 'train/mask_bce_loss': 0.10905243046581745, 'train/mask_dice_loss': 0.4914358913898468, 'train/mask_loss': 0.6004883229732514, 'metrics/total_secs_per_batch': 5.642415285110474, 'metrics/data_secs_per_batch': 2.2029321670532225, '_timestamp': 1740980843.1104236}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 467 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980843.1106405}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 468 is less than current step: 499. Dropping entry: {'train/loss': 1.6197993636131287, 'train/ce_loss': 0.205322265625, 'train/seg_cls_loss': 0.017193603515625, 'train/kl_loss': 0.3255859375, 'train/mask_bce_loss': 0.11771760433912277, 'train/mask_dice_loss': 0.5690131314098835, 'train/mask_loss': 0.6867307305335999, 'metrics/total_secs_per_batch': 6.038622617721558, 'metrics/data_secs_per_batch': 2.7093722820281982, '_timestamp': 1740980849.1490376}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 468 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980849.149326}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 469 is less than current step: 499. Dropping entry: {'train/loss': 1.7945085287094116, 'train/ce_loss': 0.29765625, 'train/seg_cls_loss': 0.017919921875, 'train/kl_loss': 0.32734375, 'train/mask_bce_loss': 0.1434901174157858, 'train/mask_dice_loss': 0.583891099691391, 'train/mask_loss': 0.7273812174797059, 'metrics/total_secs_per_batch': 6.783288955688477, 'metrics/data_secs_per_batch': 2.9255674839019776, '_timestamp': 1740980855.93208}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 469 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980855.9323559}).
Epoch: [8][471/500]	Time  5.344 ( 5.344)	Loss 2.3100 (1.2515)	CeLoss 0.3008 (0.5429)	SegCLSLoss 0.0120 (0.0060)	KLLoss 0.3594 (0.1814)	MaskLoss 0.9841 (0.3438)	MaskBCELoss 0.1284 (0.0557)	MaskDICELoss 0.8558 (0.2881)
Epoch: [8][472/500]	Time  5.598 ( 5.598)	Loss 1.6185 (1.3884)	CeLoss 0.2412 (0.4727)	SegCLSLoss 0.0099 (0.0101)	KLLoss 0.3633 (0.2525)	MaskLoss 0.6676 (0.4426)	MaskBCELoss 0.1066 (0.0829)	MaskDICELoss 0.5611 (0.3597)
Epoch: [8][473/500]	Time  5.298 ( 5.298)	Loss 2.3607 (2.0192)	CeLoss 0.2031 (0.3332)	SegCLSLoss 0.0168 (0.0173)	KLLoss 0.3652 (0.2955)	MaskLoss 1.0563 (0.8240)	MaskBCELoss 0.3253 (0.1920)	MaskDICELoss 0.7310 (0.6320)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 470 is less than current step: 499. Dropping entry: {'train/loss': 1.2515068173408508, 'train/ce_loss': 0.54287109375, 'train/seg_cls_loss': 0.00604248046875, 'train/kl_loss': 0.1814453125, 'train/mask_bce_loss': 0.05572506468743086, 'train/mask_dice_loss': 0.2880947545170784, 'train/mask_loss': 0.3438198208808899, 'metrics/total_secs_per_batch': 5.344237804412842, 'metrics/data_secs_per_batch': 2.432978868484497, '_timestamp': 1740980861.2766354}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 470 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980861.2769752}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 471 is less than current step: 499. Dropping entry: {'train/loss': 1.3883500814437866, 'train/ce_loss': 0.472705078125, 'train/seg_cls_loss': 0.010076904296875, 'train/kl_loss': 0.2525390625, 'train/mask_bce_loss': 0.08289963304996491, 'train/mask_dice_loss': 0.35973732769489286, 'train/mask_loss': 0.4426369577646255, 'metrics/total_secs_per_batch': 5.597819566726685, 'metrics/data_secs_per_batch': 2.5981589555740356, '_timestamp': 1740980866.8743474}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 471 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980866.8747053}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 472 is less than current step: 499. Dropping entry: {'train/loss': 2.0191757917404174, 'train/ce_loss': 0.333203125, 'train/seg_cls_loss': 0.017340087890625, 'train/kl_loss': 0.2955078125, 'train/mask_bce_loss': 0.19202588964253664, 'train/mask_dice_loss': 0.6319662958383561, 'train/mask_loss': 0.8239921808242798, 'metrics/total_secs_per_batch': 5.297628164291382, 'metrics/data_secs_per_batch': 2.390029501914978, '_timestamp': 1740980872.171995}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 472 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980872.1722739}).
Epoch: [8][474/500]	Time  7.216 ( 7.216)	Loss 1.5657 (1.4827)	CeLoss 0.2188 (0.2223)	SegCLSLoss 0.0187 (0.0120)	KLLoss 0.3555 (0.3262)	MaskLoss 0.6510 (0.6111)	MaskBCELoss 0.1062 (0.1495)	MaskDICELoss 0.5448 (0.4616)
Epoch: [8][475/500]	Time  5.487 ( 5.487)	Loss 2.3168 (1.3830)	CeLoss 0.2178 (0.5299)	SegCLSLoss 0.0188 (0.0083)	KLLoss 0.3555 (0.2189)	MaskLoss 1.0266 (0.4135)	MaskBCELoss 0.1277 (0.0661)	MaskDICELoss 0.8989 (0.3475)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 473 is less than current step: 499. Dropping entry: {'train/loss': 1.4827091455459596, 'train/ce_loss': 0.222265625, 'train/seg_cls_loss': 0.01199951171875, 'train/kl_loss': 0.326171875, 'train/mask_bce_loss': 0.14950382467359305, 'train/mask_dice_loss': 0.46157731115818024, 'train/mask_loss': 0.6110811293125152, 'metrics/total_secs_per_batch': 7.215730428695679, 'metrics/data_secs_per_batch': 3.2432538270950317, '_timestamp': 1740980879.3878849}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 473 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980879.388232}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 474 is less than current step: 499. Dropping entry: {'train/loss': 1.3829582989215852, 'train/ce_loss': 0.5298828125, 'train/seg_cls_loss': 0.008343505859375, 'train/kl_loss': 0.2189453125, 'train/mask_bce_loss': 0.066072917310521, 'train/mask_dice_loss': 0.34747654795646665, 'train/mask_loss': 0.4135494604706764, 'metrics/total_secs_per_batch': 5.4869606494903564, 'metrics/data_secs_per_batch': 2.1602359771728517, '_timestamp': 1740980884.8747008}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 474 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980884.8749962}).
Epoch: [8][476/500]	Time  5.658 ( 5.658)	Loss 1.4098 (1.5684)	CeLoss 0.3496 (0.5011)	SegCLSLoss 0.0104 (0.0093)	KLLoss 0.3652 (0.1812)	MaskLoss 0.5086 (0.5222)	MaskBCELoss 0.1183 (0.1359)	MaskDICELoss 0.3903 (0.3863)
Epoch: [8][477/500]	Time  5.863 ( 5.863)	Loss 2.7670 (1.4587)	CeLoss 0.3086 (0.3344)	SegCLSLoss 0.0109 (0.0143)	KLLoss 0.3672 (0.2547)	MaskLoss 1.2077 (0.5458)	MaskBCELoss 0.3963 (0.1072)	MaskDICELoss 0.8115 (0.4386)
Epoch: [8][478/500]	Time  5.690 ( 5.690)	Loss 2.3071 (1.7750)	CeLoss 0.3008 (0.4051)	SegCLSLoss 0.0165 (0.0144)	KLLoss 0.3613 (0.2896)	MaskLoss 0.9817 (0.6669)	MaskBCELoss 0.0187 (0.0877)	MaskDICELoss 0.9629 (0.5792)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 475 is less than current step: 499. Dropping entry: {'train/loss': 1.56844402551651, 'train/ce_loss': 0.501123046875, 'train/seg_cls_loss': 0.009307861328125, 'train/kl_loss': 0.18125, 'train/mask_bce_loss': 0.13591569866985082, 'train/mask_dice_loss': 0.38627017438411715, 'train/mask_loss': 0.522185879945755, 'metrics/total_secs_per_batch': 5.657598972320557, 'metrics/data_secs_per_batch': 2.1305469036102296, '_timestamp': 1740980890.5322514}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 475 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980890.532597}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 476 is less than current step: 499. Dropping entry: {'train/loss': 1.4587105095386506, 'train/ce_loss': 0.334423828125, 'train/seg_cls_loss': 0.014306640625, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.10720751397311687, 'train/mask_dice_loss': 0.4385783955454826, 'train/mask_loss': 0.5457859188318253, 'metrics/total_secs_per_batch': 5.863496541976929, 'metrics/data_secs_per_batch': 2.5393356323242187, '_timestamp': 1740980896.3958044}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 476 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980896.3961167}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 477 is less than current step: 499. Dropping entry: {'train/loss': 1.7750378787517547, 'train/ce_loss': 0.405078125, 'train/seg_cls_loss': 0.014404296875, 'train/kl_loss': 0.2896484375, 'train/mask_bce_loss': 0.08768970770761371, 'train/mask_dice_loss': 0.5792237512767315, 'train/mask_loss': 0.6669134631752968, 'metrics/total_secs_per_batch': 5.690249919891357, 'metrics/data_secs_per_batch': 2.324708127975464, '_timestamp': 1740980902.0863385}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 477 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980902.0867176}).
Epoch: [8][479/500]	Time  5.797 ( 5.797)	Loss 2.2646 (1.7413)	CeLoss 0.2451 (0.4891)	SegCLSLoss 0.0168 (0.0122)	KLLoss 0.3496 (0.2883)	MaskLoss 0.9877 (0.6085)	MaskBCELoss 0.0216 (0.1234)	MaskDICELoss 0.9662 (0.4851)
[2025-03-02 23:48:33,253] [INFO] [logging.py:96:log_dist] [Rank 0] step=4480, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:48:33,259] [INFO] [timer.py:215:stop] epoch=0/micro_step=44800/global_step=4480, RunningAvgSamplesPerSec=1.494356980263142, CurrSamplesPerSec=1.860341172664494, MemAllocated=30.8GB, MaxMemAllocated=37.23GB
Epoch: [8][480/500]	Time  5.377 ( 5.377)	Loss 1.7993 (2.0338)	CeLoss 0.1484 (0.2795)	SegCLSLoss 0.0225 (0.0179)	KLLoss 0.3633 (0.3264)	MaskLoss 0.8015 (0.8562)	MaskBCELoss 0.0257 (0.2269)	MaskDICELoss 0.7758 (0.6293)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 478 is less than current step: 499. Dropping entry: {'train/loss': 1.7412935376167298, 'train/ce_loss': 0.4890625, 'train/seg_cls_loss': 0.012249755859375, 'train/kl_loss': 0.28828125, 'train/mask_bce_loss': 0.12344804182648658, 'train/mask_dice_loss': 0.48508935868740083, 'train/mask_loss': 0.6085374087095261, 'metrics/total_secs_per_batch': 5.797338962554932, 'metrics/data_secs_per_batch': 2.697031545639038, '_timestamp': 1740980907.883385}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 478 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980907.88376}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 479 is less than current step: 499. Dropping entry: {'train/loss': 2.0338096380233766, 'train/ce_loss': 0.2794921875, 'train/seg_cls_loss': 0.017938232421875, 'train/kl_loss': 0.3263671875, 'train/mask_bce_loss': 0.2269190328195691, 'train/mask_dice_loss': 0.6292924404144287, 'train/mask_loss': 0.8562114715576172, 'metrics/total_secs_per_batch': 5.377062797546387, 'metrics/data_secs_per_batch': 2.582404851913452, '_timestamp': 1740980913.2603242}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 479 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980913.2606637}).
Epoch: [8][481/500]	Time  5.155 ( 5.155)	Loss 2.0667 (1.7665)	CeLoss 0.2617 (0.7189)	SegCLSLoss 0.0113 (0.0089)	KLLoss 0.3594 (0.2176)	MaskLoss 0.8820 (0.5106)	MaskBCELoss 0.0809 (0.1227)	MaskDICELoss 0.8011 (0.3879)
Epoch: [8][482/500]	Time  5.144 ( 5.144)	Loss 0.9922 (1.6075)	CeLoss 0.9922 (0.5241)	SegCLSLoss 0.0000 (0.0116)	KLLoss 0.0000 (0.2156)	MaskLoss 0.0000 (0.5281)	MaskBCELoss 0.0000 (0.0605)	MaskDICELoss 0.0000 (0.4676)
Epoch: [8][483/500]	Time  7.276 ( 7.276)	Loss 2.0421 (1.9060)	CeLoss 0.2168 (0.2437)	SegCLSLoss 0.0203 (0.0158)	KLLoss 0.3555 (0.3604)	MaskLoss 0.8902 (0.8091)	MaskBCELoss 0.0279 (0.1501)	MaskDICELoss 0.8623 (0.6591)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 480 is less than current step: 499. Dropping entry: {'train/loss': 1.7664890527725219, 'train/ce_loss': 0.7189453125, 'train/seg_cls_loss': 0.008935546875, 'train/kl_loss': 0.217578125, 'train/mask_bce_loss': 0.12269208319485188, 'train/mask_dice_loss': 0.3878961980342865, 'train/mask_loss': 0.510588276386261, 'metrics/total_secs_per_batch': 5.155207633972168, 'metrics/data_secs_per_batch': 2.034160923957825, '_timestamp': 1740980918.4156778}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 480 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980918.4159787}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 481 is less than current step: 499. Dropping entry: {'train/loss': 1.6074579119682313, 'train/ce_loss': 0.52412109375, 'train/seg_cls_loss': 0.011572265625, 'train/kl_loss': 0.215625, 'train/mask_bce_loss': 0.060549875069409606, 'train/mask_dice_loss': 0.467593115568161, 'train/mask_loss': 0.5281429946422577, 'metrics/total_secs_per_batch': 5.143618106842041, 'metrics/data_secs_per_batch': 2.690687394142151, '_timestamp': 1740980923.559306}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 481 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980923.559592}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 482 is less than current step: 499. Dropping entry: {'train/loss': 1.9059931278228759, 'train/ce_loss': 0.24365234375, 'train/seg_cls_loss': 0.01575927734375, 'train/kl_loss': 0.3603515625, 'train/mask_bce_loss': 0.15007316041737795, 'train/mask_dice_loss': 0.6590757220983505, 'train/mask_loss': 0.8091488897800445, 'metrics/total_secs_per_batch': 7.27643346786499, 'metrics/data_secs_per_batch': 3.3621764183044434, '_timestamp': 1740980930.835878}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 482 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980930.8362179}).
Epoch: [8][484/500]	Time  5.219 ( 5.219)	Loss 1.6406 (1.5610)	CeLoss 1.6406 (0.6956)	SegCLSLoss 0.0000 (0.0101)	KLLoss 0.0000 (0.2186)	MaskLoss 0.0000 (0.4192)	MaskBCELoss 0.0000 (0.0751)	MaskDICELoss 0.0000 (0.3441)
Epoch: [8][485/500]	Time  6.429 ( 6.429)	Loss 1.4766 (1.8884)	CeLoss 1.4766 (0.4072)	SegCLSLoss 0.0000 (0.0141)	KLLoss 0.0000 (0.3264)	MaskLoss 0.0000 (0.7207)	MaskBCELoss 0.0000 (0.1751)	MaskDICELoss 0.0000 (0.5456)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 483 is less than current step: 499. Dropping entry: {'train/loss': 1.5610330045223235, 'train/ce_loss': 0.69560546875, 'train/seg_cls_loss': 0.010089111328125, 'train/kl_loss': 0.2185546875, 'train/mask_bce_loss': 0.075112871453166, 'train/mask_dice_loss': 0.3440755054354668, 'train/mask_loss': 0.4191883698105812, 'metrics/total_secs_per_batch': 5.218888521194458, 'metrics/data_secs_per_batch': 2.3719656467437744, '_timestamp': 1740980936.054617}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 483 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980936.054902}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 484 is less than current step: 499. Dropping entry: {'train/loss': 1.8883911728858949, 'train/ce_loss': 0.4072265625, 'train/seg_cls_loss': 0.014068603515625, 'train/kl_loss': 0.3263671875, 'train/mask_bce_loss': 0.1750750621780753, 'train/mask_dice_loss': 0.5456342041492462, 'train/mask_loss': 0.720709252357483, 'metrics/total_secs_per_batch': 6.428852558135986, 'metrics/data_secs_per_batch': 3.18309690952301, '_timestamp': 1740980942.4834785}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 484 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980942.4837592}).
Epoch: [8][486/500]	Time  7.222 ( 7.222)	Loss 1.8003 (2.0403)	CeLoss 0.2324 (0.3983)	SegCLSLoss 0.0113 (0.0162)	KLLoss 0.3574 (0.3258)	MaskLoss 0.7634 (0.8005)	MaskBCELoss 0.0741 (0.1279)	MaskDICELoss 0.6894 (0.6726)
Epoch: [8][487/500]	Time  5.776 ( 5.776)	Loss 1.6406 (1.4508)	CeLoss 1.6406 (0.5111)	SegCLSLoss 0.0000 (0.0108)	KLLoss 0.0000 (0.2166)	MaskLoss 0.0000 (0.4562)	MaskBCELoss 0.0000 (0.0697)	MaskDICELoss 0.0000 (0.3865)
Epoch: [8][488/500]	Time  5.749 ( 5.749)	Loss 0.0469 (1.4981)	CeLoss 0.0469 (0.4133)	SegCLSLoss 0.0000 (0.0117)	KLLoss 0.0000 (0.2547)	MaskLoss 0.0000 (0.5267)	MaskBCELoss 0.0000 (0.0980)	MaskDICELoss 0.0000 (0.4287)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 485 is less than current step: 499. Dropping entry: {'train/loss': 2.040277349948883, 'train/ce_loss': 0.39833984375, 'train/seg_cls_loss': 0.016156005859375, 'train/kl_loss': 0.32578125, 'train/mask_bce_loss': 0.12793466141447424, 'train/mask_dice_loss': 0.6725750893354416, 'train/mask_loss': 0.8005097508430481, 'metrics/total_secs_per_batch': 7.221885919570923, 'metrics/data_secs_per_batch': 3.0002036094665527, '_timestamp': 1740980949.70537}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 485 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980949.7056673}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 486 is less than current step: 499. Dropping entry: {'train/loss': 1.4507561087608338, 'train/ce_loss': 0.511083984375, 'train/seg_cls_loss': 0.010772705078125, 'train/kl_loss': 0.2166015625, 'train/mask_bce_loss': 0.06967283952981233, 'train/mask_dice_loss': 0.3865401715040207, 'train/mask_loss': 0.4562130033969879, 'metrics/total_secs_per_batch': 5.775695562362671, 'metrics/data_secs_per_batch': 2.6184649229049684, '_timestamp': 1740980955.4810767}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 486 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980955.481383}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 487 is less than current step: 499. Dropping entry: {'train/loss': 1.4981490612030028, 'train/ce_loss': 0.41328125, 'train/seg_cls_loss': 0.01171875, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.09804213959723711, 'train/mask_dice_loss': 0.4286691054701805, 'train/mask_loss': 0.5267112404108047, 'metrics/total_secs_per_batch': 5.748711824417114, 'metrics/data_secs_per_batch': 2.4781755924224855, '_timestamp': 1740980961.2299137}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 487 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980961.2302558}).
Epoch: [8][489/500]	Time  6.518 ( 6.518)	Loss 2.0342 (1.5508)	CeLoss 0.2539 (0.3373)	SegCLSLoss 0.0146 (0.0113)	KLLoss 0.3633 (0.2545)	MaskLoss 0.8677 (0.5912)	MaskBCELoss 0.0627 (0.1165)	MaskDICELoss 0.8050 (0.4748)
[2025-03-02 23:49:32,584] [INFO] [logging.py:96:log_dist] [Rank 0] step=4490, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:49:32,591] [INFO] [timer.py:215:stop] epoch=0/micro_step=44900/global_step=4490, RunningAvgSamplesPerSec=1.4947352157310383, CurrSamplesPerSec=2.0650073724872824, MemAllocated=30.7GB, MaxMemAllocated=37.23GB
Epoch: [8][490/500]	Time  4.844 ( 4.844)	Loss 1.0000 (1.2923)	CeLoss 1.0000 (0.4745)	SegCLSLoss 0.0000 (0.0077)	KLLoss 0.0000 (0.1795)	MaskLoss 0.0000 (0.3980)	MaskBCELoss 0.0000 (0.0777)	MaskDICELoss 0.0000 (0.3203)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 488 is less than current step: 499. Dropping entry: {'train/loss': 1.5508427977561952, 'train/ce_loss': 0.3373046875, 'train/seg_cls_loss': 0.011297607421875, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.11647969409823418, 'train/mask_dice_loss': 0.47476201355457304, 'train/mask_loss': 0.5912417113780976, 'metrics/total_secs_per_batch': 6.517913579940796, 'metrics/data_secs_per_batch': 3.0060439825057985, '_timestamp': 1740980967.7476466}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 488 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980967.7479086}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 489 is less than current step: 499. Dropping entry: {'train/loss': 1.2923123359680175, 'train/ce_loss': 0.4744873046875, 'train/seg_cls_loss': 0.00767822265625, 'train/kl_loss': 0.1794921875, 'train/mask_bce_loss': 0.0777138628065586, 'train/mask_dice_loss': 0.3203099682927132, 'train/mask_loss': 0.3980238318443298, 'metrics/total_secs_per_batch': 4.844186067581177, 'metrics/data_secs_per_batch': 2.1057987451553344, '_timestamp': 1740980972.5916646}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 489 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980972.5919416}).
Epoch: [8][491/500]	Time  5.947 ( 5.947)	Loss 1.7149 (1.2474)	CeLoss 0.1777 (0.4484)	SegCLSLoss 0.0262 (0.0088)	KLLoss 0.3633 (0.1811)	MaskLoss 0.7442 (0.3883)	MaskBCELoss 0.0277 (0.0605)	MaskDICELoss 0.7164 (0.3278)
Epoch: [8][492/500]	Time  6.492 ( 6.492)	Loss 1.7974 (1.7121)	CeLoss 0.1592 (0.2736)	SegCLSLoss 0.0317 (0.0167)	KLLoss 0.3613 (0.3307)	MaskLoss 0.7933 (0.6984)	MaskBCELoss 0.1016 (0.1205)	MaskDICELoss 0.6916 (0.5780)
Epoch: [8][493/500]	Time  5.929 ( 5.929)	Loss 0.9883 (1.7393)	CeLoss 0.9883 (0.4619)	SegCLSLoss 0.0000 (0.0117)	KLLoss 0.0000 (0.2551)	MaskLoss 0.0000 (0.6229)	MaskBCELoss 0.0000 (0.1299)	MaskDICELoss 0.0000 (0.4930)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 490 is less than current step: 499. Dropping entry: {'train/loss': 1.2474146485328674, 'train/ce_loss': 0.4484375, 'train/seg_cls_loss': 0.0088134765625, 'train/kl_loss': 0.1810546875, 'train/mask_bce_loss': 0.060496820323169234, 'train/mask_dice_loss': 0.32781011164188384, 'train/mask_loss': 0.3883069336414337, 'metrics/total_secs_per_batch': 5.946674346923828, 'metrics/data_secs_per_batch': 2.63056538105011, '_timestamp': 1740980978.5386183}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 490 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980978.5390248}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 491 is less than current step: 499. Dropping entry: {'train/loss': 1.7120722651481628, 'train/ce_loss': 0.2736328125, 'train/seg_cls_loss': 0.01669921875, 'train/kl_loss': 0.3306640625, 'train/mask_bce_loss': 0.12046698476187885, 'train/mask_dice_loss': 0.5779519528150558, 'train/mask_loss': 0.6984189450740814, 'metrics/total_secs_per_batch': 6.491518259048462, 'metrics/data_secs_per_batch': 2.97000138759613, '_timestamp': 1740980985.030087}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 491 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980985.0303712}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 492 is less than current step: 499. Dropping entry: {'train/loss': 1.7392750978469849, 'train/ce_loss': 0.4619140625, 'train/seg_cls_loss': 0.01171875, 'train/kl_loss': 0.255078125, 'train/mask_bce_loss': 0.12986002452671527, 'train/mask_dice_loss': 0.4930001705884933, 'train/mask_loss': 0.6228601992130279, 'metrics/total_secs_per_batch': 5.92949104309082, 'metrics/data_secs_per_batch': 2.679964709281921, '_timestamp': 1740980990.9595766}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 492 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980990.9598432}).
Epoch: [8][494/500]	Time  5.880 ( 5.880)	Loss 1.9682 (1.6274)	CeLoss 0.1904 (0.4159)	SegCLSLoss 0.0214 (0.0137)	KLLoss 0.3535 (0.2861)	MaskLoss 0.8659 (0.5880)	MaskBCELoss 0.1415 (0.0913)	MaskDICELoss 0.7245 (0.4967)
Epoch: [8][495/500]	Time  6.474 ( 6.474)	Loss 2.4517 (1.4631)	CeLoss 0.2393 (0.3569)	SegCLSLoss 0.0115 (0.0095)	KLLoss 0.3613 (0.2531)	MaskLoss 1.0852 (0.5379)	MaskBCELoss 0.3202 (0.1757)	MaskDICELoss 0.7650 (0.3622)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 493 is less than current step: 499. Dropping entry: {'train/loss': 1.6273577392101288, 'train/ce_loss': 0.41591796875, 'train/seg_cls_loss': 0.013720703125, 'train/kl_loss': 0.2861328125, 'train/mask_bce_loss': 0.09133559614419937, 'train/mask_dice_loss': 0.49665968865156174, 'train/mask_loss': 0.5879952773451805, 'metrics/total_secs_per_batch': 5.880247354507446, 'metrics/data_secs_per_batch': 2.7064496994018556, '_timestamp': 1740980996.840149}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 493 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740980996.8405545}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 494 is less than current step: 499. Dropping entry: {'train/loss': 1.4631035208702088, 'train/ce_loss': 0.35693359375, 'train/seg_cls_loss': 0.0095458984375, 'train/kl_loss': 0.253125, 'train/mask_bce_loss': 0.17573840785771608, 'train/mask_dice_loss': 0.36220984160900116, 'train/mask_loss': 0.5379482418298721, 'metrics/total_secs_per_batch': 6.473935842514038, 'metrics/data_secs_per_batch': 2.810280466079712, '_timestamp': 1740981003.31378}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 494 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981003.3141}).
Epoch: [8][496/500]	Time  6.317 ( 6.317)	Loss 1.4615 (1.7739)	CeLoss 0.2617 (0.3676)	SegCLSLoss 0.0135 (0.0156)	KLLoss 0.3633 (0.2955)	MaskLoss 0.5784 (0.6846)	MaskBCELoss 0.0798 (0.2262)	MaskDICELoss 0.4986 (0.4584)
Epoch: [8][497/500]	Time  6.941 ( 6.941)	Loss 0.0635 (1.6411)	CeLoss 0.0635 (0.2021)	SegCLSLoss 0.0000 (0.0151)	KLLoss 0.0000 (0.2947)	MaskLoss 0.0000 (0.7010)	MaskBCELoss 0.0000 (0.1446)	MaskDICELoss 0.0000 (0.5564)
Epoch: [8][498/500]	Time  4.495 ( 4.495)	Loss 1.2569 (1.5427)	CeLoss 0.2197 (0.6703)	SegCLSLoss 0.0103 (0.0101)	KLLoss 0.3652 (0.1832)	MaskLoss 0.4976 (0.4245)	MaskBCELoss 0.0589 (0.1245)	MaskDICELoss 0.4387 (0.3000)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 495 is less than current step: 499. Dropping entry: {'train/loss': 1.7739237070083618, 'train/ce_loss': 0.367578125, 'train/seg_cls_loss': 0.0156005859375, 'train/kl_loss': 0.2955078125, 'train/mask_bce_loss': 0.22620881721377373, 'train/mask_dice_loss': 0.4583604484796524, 'train/mask_loss': 0.6845692664384841, 'metrics/total_secs_per_batch': 6.317176103591919, 'metrics/data_secs_per_batch': 2.6116021394729616, '_timestamp': 1740981009.631141}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 495 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981009.6315243}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 496 is less than current step: 499. Dropping entry: {'train/loss': 1.6410796165466308, 'train/ce_loss': 0.20205078125, 'train/seg_cls_loss': 0.015057373046875, 'train/kl_loss': 0.2947265625, 'train/mask_bce_loss': 0.144579048268497, 'train/mask_dice_loss': 0.5563806861639022, 'train/mask_loss': 0.7009597301483155, 'metrics/total_secs_per_batch': 6.941067457199097, 'metrics/data_secs_per_batch': 3.2112019777297975, '_timestamp': 1740981016.5719962}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 496 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981016.5722666}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 497 is less than current step: 499. Dropping entry: {'train/loss': 1.542712664604187, 'train/ce_loss': 0.6703125, 'train/seg_cls_loss': 0.01005859375, 'train/kl_loss': 0.183203125, 'train/mask_bce_loss': 0.1244934480637312, 'train/mask_dice_loss': 0.2999878764152527, 'train/mask_loss': 0.4244813323020935, 'metrics/total_secs_per_batch': 4.495161294937134, 'metrics/data_secs_per_batch': 1.6574945211410523, '_timestamp': 1740981021.0672557}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 497 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981021.0675812}).
Epoch: [8][499/500]	Time  6.775 ( 6.775)	Loss 2.4134 (1.4976)	CeLoss 0.2021 (0.2095)	SegCLSLoss 0.0221 (0.0150)	KLLoss 0.3574 (0.3256)	MaskLoss 1.0827 (0.6239)	MaskBCELoss 0.2376 (0.1505)	MaskDICELoss 0.8451 (0.4734)
[2025-03-02 23:50:31,945] [INFO] [logging.py:96:log_dist] [Rank 0] step=4500, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:50:31,950] [INFO] [timer.py:215:stop] epoch=0/micro_step=45000/global_step=4500, RunningAvgSamplesPerSec=1.495110667389803, CurrSamplesPerSec=2.4356467358021203, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [8][500/500]	Time  4.109 ( 4.109)	Loss 1.9564 (1.2244)	CeLoss 0.3203 (0.7738)	SegCLSLoss 0.0114 (0.0057)	KLLoss 0.3633 (0.1471)	MaskLoss 0.7966 (0.2164)	MaskBCELoss 0.1356 (0.0536)	MaskDICELoss 0.6610 (0.1628)


 16%|███████████████████████▌                                                                                                                           | 32/200 [00:05<00:21,  7.81it/s][34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 498 is less than current step: 499. Dropping entry: {'train/loss': 1.4976344347000121, 'train/ce_loss': 0.20947265625, 'train/seg_cls_loss': 0.015045166015625, 'train/kl_loss': 0.3255859375, 'train/mask_bce_loss': 0.15053610373288392, 'train/mask_dice_loss': 0.47337876036763193, 'train/mask_loss': 0.6239148691296578, 'metrics/total_secs_per_batch': 6.775470018386841, 'metrics/data_secs_per_batch': 3.251042866706848, '_timestamp': 1740981027.8434017}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 498 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981027.8441541}).











100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:28<00:00,  8.00it/s]

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:29<00:00,  6.89it/s]
Epoch: [9][  1/500]	Time  6.810 ( 6.810)	Loss 1.4362 (1.4659)	CeLoss 0.2617 (0.3104)	SegCLSLoss 0.0098 (0.0117)	KLLoss 0.3574 (0.2555)	MaskLoss 0.5667 (0.5619)	MaskBCELoss 0.0894 (0.0926)	MaskDICELoss 0.4773 (0.4692)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 8 is less than current step: 499. Dropping entry: {'val/giou': 0.1878635585308075, 'val/ciou': 0.17587274312973022, '_timestamp': 1740981060.9755528}).
Epoch: [9][  2/500]	Time  5.518 ( 5.518)	Loss 0.9414 (1.1814)	CeLoss 0.9414 (0.5250)	SegCLSLoss 0.0000 (0.0082)	KLLoss 0.0000 (0.1822)	MaskLoss 0.0000 (0.3171)	MaskBCELoss 0.0000 (0.0537)	MaskDICELoss 0.0000 (0.2634)
Epoch: [9][  3/500]	Time  6.846 ( 6.846)	Loss 1.1371 (1.4814)	CeLoss 0.2197 (0.3179)	SegCLSLoss 0.0171 (0.0145)	KLLoss 0.3594 (0.3256)	MaskLoss 0.4367 (0.5617)	MaskBCELoss 0.0643 (0.0642)	MaskDICELoss 0.3724 (0.4975)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 499. Dropping entry: {'train/loss': 1.465868389606476, 'train/ce_loss': 0.310400390625, 'train/seg_cls_loss': 0.0116943359375, 'train/kl_loss': 0.25546875, 'train/mask_bce_loss': 0.09262365307658911, 'train/mask_dice_loss': 0.4692412167787552, 'train/mask_loss': 0.5618648648262023, 'metrics/total_secs_per_batch': 6.810161113739014, 'metrics/data_secs_per_batch': 3.0702306509017943, '_timestamp': 1740981067.7926722}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 1 is less than current step: 499. Dropping entry: {'train/loss': 1.1813614010810851, 'train/ce_loss': 0.525048828125, 'train/seg_cls_loss': 0.00821533203125, 'train/kl_loss': 0.1822265625, 'train/mask_bce_loss': 0.053728125244379046, 'train/mask_dice_loss': 0.26339299976825714, 'train/mask_loss': 0.3171211302280426, 'metrics/total_secs_per_batch': 5.517989873886108, 'metrics/data_secs_per_batch': 2.399899220466614, '_timestamp': 1740981073.3106}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 1 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981073.3109078}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 2 is less than current step: 499. Dropping entry: {'train/loss': 1.481404334306717, 'train/ce_loss': 0.31787109375, 'train/seg_cls_loss': 0.014544677734375, 'train/kl_loss': 0.3255859375, 'train/mask_bce_loss': 0.06418015789240598, 'train/mask_dice_loss': 0.49751811027526854, 'train/mask_loss': 0.5616982638835907, 'metrics/total_secs_per_batch': 6.845835208892822, 'metrics/data_secs_per_batch': 3.1582618236541746, '_timestamp': 1740981080.1566844}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 2 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981080.157054}).
Epoch: [9][  4/500]	Time  5.776 ( 5.776)	Loss 1.2266 (1.7824)	CeLoss 1.2266 (0.4050)	SegCLSLoss 0.0000 (0.0141)	KLLoss 0.0000 (0.2914)	MaskLoss 0.0000 (0.6706)	MaskBCELoss 0.0000 (0.1459)	MaskDICELoss 0.0000 (0.5246)
Epoch: [9][  5/500]	Time  5.439 ( 5.439)	Loss 1.3047 (1.3135)	CeLoss 1.3047 (0.5664)	SegCLSLoss 0.0000 (0.0079)	KLLoss 0.0000 (0.2543)	MaskLoss 0.0000 (0.3592)	MaskBCELoss 0.0000 (0.0895)	MaskDICELoss 0.0000 (0.2697)
Epoch: [9][  6/500]	Time  4.351 ( 4.351)	Loss 0.7626 (1.5116)	CeLoss 0.3301 (0.9841)	SegCLSLoss 0.0103 (0.0065)	KLLoss 0.3633 (0.1434)	MaskLoss 0.1948 (0.2549)	MaskBCELoss 0.0484 (0.0301)	MaskDICELoss 0.1464 (0.2248)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 3 is less than current step: 499. Dropping entry: {'train/loss': 1.7823562383651734, 'train/ce_loss': 0.40498046875, 'train/seg_cls_loss': 0.014093017578125, 'train/kl_loss': 0.29140625, 'train/mask_bce_loss': 0.1459401397034526, 'train/mask_dice_loss': 0.5246325016021729, 'train/mask_loss': 0.6705726444721222, 'metrics/total_secs_per_batch': 5.775899887084961, 'metrics/data_secs_per_batch': 2.6540791034698485, '_timestamp': 1740981085.932365}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 3 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981085.9326503}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 4 is less than current step: 499. Dropping entry: {'train/loss': 1.3134690761566161, 'train/ce_loss': 0.56640625, 'train/seg_cls_loss': 0.00792236328125, 'train/kl_loss': 0.254296875, 'train/mask_bce_loss': 0.08948298171162605, 'train/mask_dice_loss': 0.2696929633617401, 'train/mask_loss': 0.35917594730854036, 'metrics/total_secs_per_batch': 5.439216613769531, 'metrics/data_secs_per_batch': 2.623174214363098, '_timestamp': 1740981091.3717854}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 4 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981091.3721492}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 5 is less than current step: 499. Dropping entry: {'train/loss': 1.5115804970264435, 'train/ce_loss': 0.98408203125, 'train/seg_cls_loss': 0.00648193359375, 'train/kl_loss': 0.143359375, 'train/mask_bce_loss': 0.030117168463766573, 'train/mask_dice_loss': 0.2247941702604294, 'train/mask_loss': 0.2549113303422928, 'metrics/total_secs_per_batch': 4.350969314575195, 'metrics/data_secs_per_batch': 1.8862985372543335, '_timestamp': 1740981095.722632}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 5 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981095.7229311}).
Epoch: [9][  7/500]	Time  5.631 ( 5.631)	Loss 2.4995 (1.5468)	CeLoss 0.2256 (0.4186)	SegCLSLoss 0.0146 (0.0097)	KLLoss 0.3691 (0.2555)	MaskLoss 1.1150 (0.5488)	MaskBCELoss 0.2936 (0.1469)	MaskDICELoss 0.8214 (0.4020)
Epoch: [9][  8/500]	Time  6.044 ( 6.044)	Loss 2.2590 (1.8018)	CeLoss 0.1924 (0.4721)	SegCLSLoss 0.0160 (0.0126)	KLLoss 0.3555 (0.2871)	MaskLoss 1.0114 (0.6474)	MaskBCELoss 0.0480 (0.1581)	MaskDICELoss 0.9634 (0.4893)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 6 is less than current step: 499. Dropping entry: {'train/loss': 1.5467530012130737, 'train/ce_loss': 0.418603515625, 'train/seg_cls_loss': 0.009747314453125, 'train/kl_loss': 0.25546875, 'train/mask_bce_loss': 0.1468734011054039, 'train/mask_dice_loss': 0.401966980099678, 'train/mask_loss': 0.5488403737545013, 'metrics/total_secs_per_batch': 5.630517959594727, 'metrics/data_secs_per_batch': 2.4684978485107423, '_timestamp': 1740981101.353095}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 6 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981101.3533776}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 7 is less than current step: 499. Dropping entry: {'train/loss': 1.8017866015434265, 'train/ce_loss': 0.4720703125, 'train/seg_cls_loss': 0.01259765625, 'train/kl_loss': 0.287109375, 'train/mask_bce_loss': 0.15806794110685587, 'train/mask_dice_loss': 0.4893097221851349, 'train/mask_loss': 0.6473776757717132, 'metrics/total_secs_per_batch': 6.043722867965698, 'metrics/data_secs_per_batch': 2.8586270093917845, '_timestamp': 1740981107.3967838}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 7 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981107.3971548}).
Epoch: [9][  9/500]	Time  5.887 ( 5.887)	Loss 0.0967 (1.9120)	CeLoss 0.0967 (0.4468)	SegCLSLoss 0.0000 (0.0122)	KLLoss 0.0000 (0.2525)	MaskLoss 0.0000 (0.7170)	MaskBCELoss 0.0000 (0.1983)	MaskDICELoss 0.0000 (0.5187)
[2025-03-02 23:51:57,549] [INFO] [logging.py:96:log_dist] [Rank 0] step=4510, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:51:57,554] [INFO] [timer.py:215:stop] epoch=0/micro_step=45100/global_step=4510, RunningAvgSamplesPerSec=1.495622797475554, CurrSamplesPerSec=2.3422856627492723, MemAllocated=30.72GB, MaxMemAllocated=37.23GB
Epoch: [9][ 10/500]	Time  4.271 ( 4.271)	Loss 0.8164 (1.4196)	CeLoss 0.8164 (0.9252)	SegCLSLoss 0.0000 (0.0063)	KLLoss 0.0000 (0.1090)	MaskLoss 0.0000 (0.2402)	MaskBCELoss 0.0000 (0.0472)	MaskDICELoss 0.0000 (0.1930)
Epoch: [9][ 11/500]	Time  6.176 ( 6.176)	Loss 0.8062 (1.6005)	CeLoss 0.2197 (0.3453)	SegCLSLoss 0.0101 (0.0139)	KLLoss 0.3594 (0.2920)	MaskLoss 0.2732 (0.6096)	MaskBCELoss 0.1405 (0.1456)	MaskDICELoss 0.1327 (0.4641)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 8 is less than current step: 499. Dropping entry: {'train/loss': 1.911991822719574, 'train/ce_loss': 0.44677734375, 'train/seg_cls_loss': 0.01221923828125, 'train/kl_loss': 0.2525390625, 'train/mask_bce_loss': 0.19827484488487243, 'train/mask_dice_loss': 0.5187073945999146, 'train/mask_loss': 0.716982239484787, 'metrics/total_secs_per_batch': 5.887444734573364, 'metrics/data_secs_per_batch': 2.703848433494568, '_timestamp': 1740981113.2842495}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 8 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981113.2845411}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 9 is less than current step: 499. Dropping entry: {'train/loss': 1.419608724117279, 'train/ce_loss': 0.925244140625, 'train/seg_cls_loss': 0.006280517578125, 'train/kl_loss': 0.108984375, 'train/mask_bce_loss': 0.04719136487692595, 'train/mask_dice_loss': 0.19300850331783295, 'train/mask_loss': 0.24019986987113953, 'metrics/total_secs_per_batch': 4.270964860916138, 'metrics/data_secs_per_batch': 2.0164302587509155, '_timestamp': 1740981117.5550718}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 9 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981117.5554113}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 10 is less than current step: 499. Dropping entry: {'train/loss': 1.6005267381668091, 'train/ce_loss': 0.3453125, 'train/seg_cls_loss': 0.0139404296875, 'train/kl_loss': 0.2919921875, 'train/mask_bce_loss': 0.14557894486933948, 'train/mask_dice_loss': 0.4640594094991684, 'train/mask_loss': 0.6096383661031723, 'metrics/total_secs_per_batch': 6.175724029541016, 'metrics/data_secs_per_batch': 2.8268208503723145, '_timestamp': 1740981123.7309823}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 10 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981123.7312875}).
Epoch: [9][ 12/500]	Time  6.714 ( 6.714)	Loss 0.1318 (1.6420)	CeLoss 0.1318 (0.2689)	SegCLSLoss 0.0000 (0.0160)	KLLoss 0.0000 (0.2904)	MaskLoss 0.0000 (0.6679)	MaskBCELoss 0.0000 (0.1006)	MaskDICELoss 0.0000 (0.5673)
Epoch: [9][ 13/500]	Time  5.070 ( 5.070)	Loss 1.1484 (1.4970)	CeLoss 1.1484 (0.6755)	SegCLSLoss 0.0000 (0.0092)	KLLoss 0.0000 (0.1812)	MaskLoss 0.0000 (0.3994)	MaskBCELoss 0.0000 (0.0961)	MaskDICELoss 0.0000 (0.3033)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 11 is less than current step: 499. Dropping entry: {'train/loss': 1.64203839302063, 'train/ce_loss': 0.2689453125, 'train/seg_cls_loss': 0.01602783203125, 'train/kl_loss': 0.2904296875, 'train/mask_bce_loss': 0.10061705969274044, 'train/mask_dice_loss': 0.5673259526491166, 'train/mask_loss': 0.6679430216550827, 'metrics/total_secs_per_batch': 6.713502645492554, 'metrics/data_secs_per_batch': 2.963562798500061, '_timestamp': 1740981130.4444675}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 11 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981130.4446628}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 12 is less than current step: 499. Dropping entry: {'train/loss': 1.4970420002937317, 'train/ce_loss': 0.67548828125, 'train/seg_cls_loss': 0.00916748046875, 'train/kl_loss': 0.18125, 'train/mask_bce_loss': 0.09609879031777382, 'train/mask_dice_loss': 0.30330111384391784, 'train/mask_loss': 0.3993999004364014, 'metrics/total_secs_per_batch': 5.0701000690460205, 'metrics/data_secs_per_batch': 2.3668974876403808, '_timestamp': 1740981135.5145175}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 12 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981135.5147927}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 13 is less than current step: 499. Dropping entry: {'train/loss': 1.9296347498893738, 'train/ce_loss': 0.216796875, 'train/seg_cls_loss': 0.0180908203125, 'train/kl_loss': 0.36171875, 'train/mask_bce_loss': 0.1311330390162766, 'train/mask_dice_loss': 0.7025808274745942, 'train/mask_loss': 0.8337138533592224, 'metrics/total_secs_per_batch': 6.919098854064941, 'metrics/data_secs_per_batch': 3.062592887878418, '_timestamp': 1740981142.4336479}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 13 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981142.4339464}).
Epoch: [9][ 14/500]	Time  6.919 ( 6.919)	Loss 1.7831 (1.9296)	CeLoss 0.2158 (0.2168)	SegCLSLoss 0.0198 (0.0181)	KLLoss 0.3633 (0.3617)	MaskLoss 0.7607 (0.8337)	MaskBCELoss 0.0422 (0.1311)	MaskDICELoss 0.7184 (0.7026)
Epoch: [9][ 15/500]	Time  6.164 ( 6.164)	Loss 2.5007 (1.6711)	CeLoss 0.2188 (0.3149)	SegCLSLoss 0.0198 (0.0138)	KLLoss 0.3789 (0.3293)	MaskLoss 1.1175 (0.6581)	MaskBCELoss 0.3610 (0.1944)	MaskDICELoss 0.7565 (0.4637)
Epoch: [9][ 16/500]	Time  5.158 ( 5.158)	Loss 0.9688 (1.7276)	CeLoss 0.9688 (0.6897)	SegCLSLoss 0.0000 (0.0100)	KLLoss 0.0000 (0.2162)	MaskLoss 0.0000 (0.5055)	MaskBCELoss 0.0000 (0.1396)	MaskDICELoss 0.0000 (0.3659)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 14 is less than current step: 499. Dropping entry: {'train/loss': 1.6710563957691194, 'train/ce_loss': 0.31494140625, 'train/seg_cls_loss': 0.013751220703125, 'train/kl_loss': 0.329296875, 'train/mask_bce_loss': 0.1944268023595214, 'train/mask_dice_loss': 0.463708820939064, 'train/mask_loss': 0.6581356137990951, 'metrics/total_secs_per_batch': 6.164346694946289, 'metrics/data_secs_per_batch': 3.0078484296798704, '_timestamp': 1740981148.5979755}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 14 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981148.5982594}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 15 is less than current step: 499. Dropping entry: {'train/loss': 1.7275977849960327, 'train/ce_loss': 0.68974609375, 'train/seg_cls_loss': 0.01002197265625, 'train/kl_loss': 0.2162109375, 'train/mask_bce_loss': 0.13961666598916053, 'train/mask_dice_loss': 0.3659302741289139, 'train/mask_loss': 0.5055469274520874, 'metrics/total_secs_per_batch': 5.158113241195679, 'metrics/data_secs_per_batch': 2.459500288963318, '_timestamp': 1740981153.7562923}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 15 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981153.7566578}).
Epoch: [9][ 17/500]	Time  4.928 ( 4.928)	Loss 1.3203 (1.3474)	CeLoss 1.3203 (0.5137)	SegCLSLoss 0.0000 (0.0103)	KLLoss 0.0000 (0.2527)	MaskLoss 0.0000 (0.4016)	MaskBCELoss 0.0000 (0.0808)	MaskDICELoss 0.0000 (0.3208)
Epoch: [9][ 18/500]	Time  6.220 ( 6.220)	Loss 2.0476 (1.7044)	CeLoss 0.2578 (0.2068)	SegCLSLoss 0.0128 (0.0156)	KLLoss 0.3555 (0.3299)	MaskLoss 0.8744 (0.7284)	MaskBCELoss 0.0050 (0.1002)	MaskDICELoss 0.8694 (0.6282)
Epoch: [9][ 19/500]	Time  6.298 ( 6.298)	Loss 1.5830 (1.4309)	CeLoss 0.2500 (0.3671)	SegCLSLoss 0.0119 (0.0101)	KLLoss 0.3613 (0.2510)	MaskLoss 0.6450 (0.5169)	MaskBCELoss 0.0767 (0.0682)	MaskDICELoss 0.5683 (0.4487)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 16 is less than current step: 499. Dropping entry: {'train/loss': 1.3473943173885345, 'train/ce_loss': 0.513671875, 'train/seg_cls_loss': 0.010272216796875, 'train/kl_loss': 0.252734375, 'train/mask_bce_loss': 0.0808314684778452, 'train/mask_dice_loss': 0.32079536765813826, 'train/mask_loss': 0.4016268402338028, 'metrics/total_secs_per_batch': 4.928468227386475, 'metrics/data_secs_per_batch': 2.4978796005249024, '_timestamp': 1740981158.6845891}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 16 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981158.684877}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 17 is less than current step: 499. Dropping entry: {'train/loss': 1.7043916225433349, 'train/ce_loss': 0.206787109375, 'train/seg_cls_loss': 0.0156494140625, 'train/kl_loss': 0.3298828125, 'train/mask_bce_loss': 0.10021079112775624, 'train/mask_dice_loss': 0.6281813099980355, 'train/mask_loss': 0.7283920913934707, 'metrics/total_secs_per_batch': 6.219743490219116, 'metrics/data_secs_per_batch': 2.628815174102783, '_timestamp': 1740981164.9043267}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 17 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981164.904614}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 18 is less than current step: 499. Dropping entry: {'train/loss': 1.4308899879455566, 'train/ce_loss': 0.36708984375, 'train/seg_cls_loss': 0.010125732421875, 'train/kl_loss': 0.2509765625, 'train/mask_bce_loss': 0.06819310188293456, 'train/mask_dice_loss': 0.4486679196357727, 'train/mask_loss': 0.5168610095977784, 'metrics/total_secs_per_batch': 6.298275470733643, 'metrics/data_secs_per_batch': 2.6852914094924927, '_timestamp': 1740981171.2028232}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 18 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981171.2031798}).
[2025-03-02 23:52:57,468] [INFO] [logging.py:96:log_dist] [Rank 0] step=4520, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:52:57,473] [INFO] [timer.py:215:stop] epoch=0/micro_step=45200/global_step=4520, RunningAvgSamplesPerSec=1.4959672349896966, CurrSamplesPerSec=1.5948697132655671, MemAllocated=31.11GB, MaxMemAllocated=37.23GB
Epoch: [9][ 20/500]	Time  6.272 ( 6.272)	Loss 2.5516 (1.6203)	CeLoss 0.1885 (0.3422)	SegCLSLoss 0.0201 (0.0118)	KLLoss 0.3809 (0.2895)	MaskLoss 1.1576 (0.6216)	MaskBCELoss 0.2335 (0.0651)	MaskDICELoss 0.9241 (0.5565)
Epoch: [9][ 21/500]	Time  6.013 ( 6.013)	Loss 1.6571 (1.5809)	CeLoss 0.2070 (0.4573)	SegCLSLoss 0.0327 (0.0136)	KLLoss 0.3594 (0.2916)	MaskLoss 0.6987 (0.5440)	MaskBCELoss 0.0538 (0.0753)	MaskDICELoss 0.6449 (0.4687)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 19 is less than current step: 499. Dropping entry: {'train/loss': 1.6203337848186492, 'train/ce_loss': 0.3421875, 'train/seg_cls_loss': 0.0118408203125, 'train/kl_loss': 0.289453125, 'train/mask_bce_loss': 0.06511192452162504, 'train/mask_dice_loss': 0.5564807504415512, 'train/mask_loss': 0.6215926796197891, 'metrics/total_secs_per_batch': 6.272021293640137, 'metrics/data_secs_per_batch': 2.6470755338668823, '_timestamp': 1740981177.4744368}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 19 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981177.4746482}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 20 is less than current step: 499. Dropping entry: {'train/loss': 1.5809039294719696, 'train/ce_loss': 0.45732421875, 'train/seg_cls_loss': 0.0135986328125, 'train/kl_loss': 0.2916015625, 'train/mask_bce_loss': 0.07525948043912649, 'train/mask_dice_loss': 0.46870809942483904, 'train/mask_loss': 0.5439675897359848, 'metrics/total_secs_per_batch': 6.012811183929443, 'metrics/data_secs_per_batch': 2.908704400062561, '_timestamp': 1740981183.4874086}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 20 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981183.4877896}).
Epoch: [9][ 22/500]	Time  4.877 ( 4.877)	Loss 1.2188 (1.4815)	CeLoss 1.2188 (0.6884)	SegCLSLoss 0.0000 (0.0085)	KLLoss 0.0000 (0.1848)	MaskLoss 0.0000 (0.3852)	MaskBCELoss 0.0000 (0.0884)	MaskDICELoss 0.0000 (0.2968)
Epoch: [9][ 23/500]	Time  6.145 ( 6.145)	Loss 1.1057 (1.2068)	CeLoss 0.2275 (0.4356)	SegCLSLoss 0.0122 (0.0101)	KLLoss 0.3652 (0.2146)	MaskLoss 0.4181 (0.3724)	MaskBCELoss 0.1041 (0.0252)	MaskDICELoss 0.3140 (0.3472)
Epoch: [9][ 24/500]	Time  5.298 ( 5.298)	Loss 2.3937 (1.6464)	CeLoss 0.1641 (0.4847)	SegCLSLoss 0.0259 (0.0113)	KLLoss 0.3730 (0.2535)	MaskLoss 1.0899 (0.5654)	MaskBCELoss 0.2492 (0.1125)	MaskDICELoss 0.8407 (0.4529)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 21 is less than current step: 499. Dropping entry: {'train/loss': 1.4814877390861512, 'train/ce_loss': 0.68837890625, 'train/seg_cls_loss': 0.00848388671875, 'train/kl_loss': 0.184765625, 'train/mask_bce_loss': 0.08843105789273978, 'train/mask_dice_loss': 0.2967952251434326, 'train/mask_loss': 0.38522628843784334, 'metrics/total_secs_per_batch': 4.877338647842407, 'metrics/data_secs_per_batch': 2.1146472692489624, '_timestamp': 1740981188.3647764}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 21 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981188.365075}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 22 is less than current step: 499. Dropping entry: {'train/loss': 1.206796145439148, 'train/ce_loss': 0.4356201171875, 'train/seg_cls_loss': 0.010107421875, 'train/kl_loss': 0.2146484375, 'train/mask_bce_loss': 0.025167527375742793, 'train/mask_dice_loss': 0.3472368836402893, 'train/mask_loss': 0.3724044144153595, 'metrics/total_secs_per_batch': 6.145148515701294, 'metrics/data_secs_per_batch': 2.662911224365234, '_timestamp': 1740981194.5101871}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 22 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981194.5105696}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 23 is less than current step: 499. Dropping entry: {'train/loss': 1.6463972926139832, 'train/ce_loss': 0.48466796875, 'train/seg_cls_loss': 0.01126708984375, 'train/kl_loss': 0.253515625, 'train/mask_bce_loss': 0.11251801941543818, 'train/mask_dice_loss': 0.4529169648885727, 'train/mask_loss': 0.5654349803924561, 'metrics/total_secs_per_batch': 5.298449277877808, 'metrics/data_secs_per_batch': 2.104374146461487, '_timestamp': 1740981199.80841}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 23 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981199.8087053}).
Epoch: [9][ 25/500]	Time  5.644 ( 5.644)	Loss 2.5785 (1.5991)	CeLoss 0.1992 (0.3970)	SegCLSLoss 0.0154 (0.0097)	KLLoss 0.3594 (0.2531)	MaskLoss 1.1676 (0.5860)	MaskBCELoss 0.2579 (0.1848)	MaskDICELoss 0.9098 (0.4011)
Epoch: [9][ 26/500]	Time  5.987 ( 5.987)	Loss 1.0938 (1.4393)	CeLoss 1.0938 (0.6979)	SegCLSLoss 0.0000 (0.0078)	KLLoss 0.0000 (0.2172)	MaskLoss 0.0000 (0.3578)	MaskBCELoss 0.0000 (0.0421)	MaskDICELoss 0.0000 (0.3157)
Epoch: [9][ 27/500]	Time  6.153 ( 6.153)	Loss 2.9055 (1.3490)	CeLoss 0.2295 (0.3076)	SegCLSLoss 0.0157 (0.0124)	KLLoss 0.3652 (0.2514)	MaskLoss 1.3160 (0.5051)	MaskBCELoss 0.4276 (0.0812)	MaskDICELoss 0.8884 (0.4239)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 24 is less than current step: 499. Dropping entry: {'train/loss': 1.5991458415985107, 'train/ce_loss': 0.397021484375, 'train/seg_cls_loss': 0.009674072265625, 'train/kl_loss': 0.253125, 'train/mask_bce_loss': 0.1848314255475998, 'train/mask_dice_loss': 0.4011428654193878, 'train/mask_loss': 0.5859742879867553, 'metrics/total_secs_per_batch': 5.644230842590332, 'metrics/data_secs_per_batch': 2.735330510139465, '_timestamp': 1740981205.4526503}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 24 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981205.4529495}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 25 is less than current step: 499. Dropping entry: {'train/loss': 1.4392872333526612, 'train/ce_loss': 0.69794921875, 'train/seg_cls_loss': 0.007843017578125, 'train/kl_loss': 0.2171875, 'train/mask_bce_loss': 0.042081902362406255, 'train/mask_dice_loss': 0.3157453089952469, 'train/mask_loss': 0.3578272104263306, 'metrics/total_secs_per_batch': 5.986565113067627, 'metrics/data_secs_per_batch': 2.4741658210754394, '_timestamp': 1740981211.4391792}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 25 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981211.439448}).
Epoch: [9][ 28/500]	Time  6.635 ( 6.635)	Loss 1.1286 (1.2820)	CeLoss 0.3828 (0.2201)	SegCLSLoss 0.0093 (0.0121)	KLLoss 0.3613 (0.2906)	MaskLoss 0.3534 (0.5134)	MaskBCELoss 0.1162 (0.1132)	MaskDICELoss 0.2372 (0.4002)
Epoch: [9][ 29/500]	Time  4.878 ( 4.878)	Loss 0.2158 (1.6303)	CeLoss 0.2158 (0.5973)	SegCLSLoss 0.0000 (0.0098)	KLLoss 0.0000 (0.1852)	MaskLoss 0.0000 (0.5049)	MaskBCELoss 0.0000 (0.1524)	MaskDICELoss 0.0000 (0.3525)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 26 is less than current step: 499. Dropping entry: {'train/loss': 1.349010705947876, 'train/ce_loss': 0.3076416015625, 'train/seg_cls_loss': 0.01240234375, 'train/kl_loss': 0.2513671875, 'train/mask_bce_loss': 0.08116848934441805, 'train/mask_dice_loss': 0.4239398822188377, 'train/mask_loss': 0.505108368396759, 'metrics/total_secs_per_batch': 6.153197526931763, 'metrics/data_secs_per_batch': 2.8105048656463625, '_timestamp': 1740981217.5924993}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 26 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981217.5929556}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 27 is less than current step: 499. Dropping entry: {'train/loss': 1.2820072233676911, 'train/ce_loss': 0.2201171875, 'train/seg_cls_loss': 0.012139892578125, 'train/kl_loss': 0.290625, 'train/mask_bce_loss': 0.1132034195587039, 'train/mask_dice_loss': 0.40016346573829653, 'train/mask_loss': 0.5133668869733811, 'metrics/total_secs_per_batch': 6.634908437728882, 'metrics/data_secs_per_batch': 3.0074299812316894, '_timestamp': 1740981224.2273297}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 27 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981224.2275326}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 28 is less than current step: 499. Dropping entry: {'train/loss': 1.630269432067871, 'train/ce_loss': 0.597265625, 'train/seg_cls_loss': 0.009783935546875, 'train/kl_loss': 0.18515625, 'train/mask_bce_loss': 0.1523578055202961, 'train/mask_dice_loss': 0.35252299904823303, 'train/mask_loss': 0.5048808097839356, 'metrics/total_secs_per_batch': 4.878057479858398, 'metrics/data_secs_per_batch': 2.2443606853485107, '_timestamp': 1740981229.105389}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 28 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981229.1056738}).
[2025-03-02 23:53:55,281] [INFO] [logging.py:96:log_dist] [Rank 0] step=4530, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:53:55,286] [INFO] [timer.py:215:stop] epoch=0/micro_step=45300/global_step=4530, RunningAvgSamplesPerSec=1.4964145212013795, CurrSamplesPerSec=1.618165410428864, MemAllocated=31.55GB, MaxMemAllocated=37.23GB
Epoch: [9][ 30/500]	Time  6.181 ( 6.181)	Loss 1.8216 (1.5286)	CeLoss 0.2363 (0.3556)	SegCLSLoss 0.0206 (0.0130)	KLLoss 0.3555 (0.2885)	MaskLoss 0.7692 (0.5686)	MaskBCELoss 0.0497 (0.0751)	MaskDICELoss 0.7195 (0.4935)
Exception in thread Thread-15 (_pin_memory_loop):
Traceback (most recent call last):
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 54, in _pin_memory_loop
    do_one_step()
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 31, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/multiprocessing/reductions.py", line 495, in rebuild_storage_fd
    fd = df.detach()
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/connection.py", line 502, in Client
    c = SocketClient(address)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/connection.py", line 630, in SocketClient
    s.connect(address)
FileNotFoundError: [Errno 2] No such file or directory
Epoch: [9][ 31/500]	Time  6.375 ( 6.375)	Loss 2.3066 (1.9680)	CeLoss 0.1885 (0.4034)	SegCLSLoss 0.0164 (0.0151)	KLLoss 0.3711 (0.2934)	MaskLoss 1.0366 (0.7637)	MaskBCELoss 0.2727 (0.2195)	MaskDICELoss 0.7639 (0.5443)
Epoch: [9][ 32/500]	Time  6.053 ( 6.053)	Loss 1.3150 (1.8530)	CeLoss 0.2246 (0.3055)	SegCLSLoss 0.0109 (0.0154)	KLLoss 0.3633 (0.3295)	MaskLoss 0.5237 (0.7535)	MaskBCELoss 0.0669 (0.1449)	MaskDICELoss 0.4568 (0.6086)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 29 is less than current step: 499. Dropping entry: {'train/loss': 1.5285568237304688, 'train/ce_loss': 0.35556640625, 'train/seg_cls_loss': 0.01295166015625, 'train/kl_loss': 0.2884765625, 'train/mask_bce_loss': 0.07508919350802898, 'train/mask_dice_loss': 0.4935349225997925, 'train/mask_loss': 0.5686241209506988, 'metrics/total_secs_per_batch': 6.181482315063477, 'metrics/data_secs_per_batch': 2.5412845373153687, '_timestamp': 1740981235.2866423}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 29 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981235.2868452}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 30 is less than current step: 499. Dropping entry: {'train/loss': 1.967987585067749, 'train/ce_loss': 0.40341796875, 'train/seg_cls_loss': 0.015142822265625, 'train/kl_loss': 0.293359375, 'train/mask_bce_loss': 0.21945285461843014, 'train/mask_dice_loss': 0.5442772686481476, 'train/mask_loss': 0.7637301325798035, 'metrics/total_secs_per_batch': 6.37544322013855, 'metrics/data_secs_per_batch': 3.0958467721939087, '_timestamp': 1740981241.6626785}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 30 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981241.6631463}).
Epoch: [9][ 33/500]	Time  5.494 ( 5.494)	Loss 1.9672 (1.5668)	CeLoss 0.2148 (0.4256)	SegCLSLoss 0.0189 (0.0113)	KLLoss 0.3555 (0.2510)	MaskLoss 0.8537 (0.5552)	MaskBCELoss 0.0458 (0.0830)	MaskDICELoss 0.8079 (0.4721)
Epoch: [9][ 34/500]	Time  5.884 ( 5.884)	Loss 1.1719 (1.9853)	CeLoss 1.1719 (0.5464)	SegCLSLoss 0.0000 (0.0115)	KLLoss 0.0000 (0.2547)	MaskLoss 0.0000 (0.7039)	MaskBCELoss 0.0000 (0.1453)	MaskDICELoss 0.0000 (0.5586)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 31 is less than current step: 499. Dropping entry: {'train/loss': 1.8530010342597962, 'train/ce_loss': 0.30546875, 'train/seg_cls_loss': 0.0154052734375, 'train/kl_loss': 0.3294921875, 'train/mask_bce_loss': 0.1449135766364634, 'train/mask_dice_loss': 0.608588907122612, 'train/mask_loss': 0.753502470254898, 'metrics/total_secs_per_batch': 6.053164958953857, 'metrics/data_secs_per_batch': 2.8632909774780275, '_timestamp': 1740981247.715581}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 31 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981247.7159464}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 32 is less than current step: 499. Dropping entry: {'train/loss': 1.5668105363845826, 'train/ce_loss': 0.4255859375, 'train/seg_cls_loss': 0.01129150390625, 'train/kl_loss': 0.2509765625, 'train/mask_bce_loss': 0.08303284458816051, 'train/mask_dice_loss': 0.47214976847171786, 'train/mask_loss': 0.5551826164126397, 'metrics/total_secs_per_batch': 5.494251489639282, 'metrics/data_secs_per_batch': 2.47909255027771, '_timestamp': 1740981253.2097452}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 32 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981253.2100198}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 33 is less than current step: 499. Dropping entry: {'train/loss': 1.9853103399276733, 'train/ce_loss': 0.54638671875, 'train/seg_cls_loss': 0.011480712890625, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.14532926697283982, 'train/mask_dice_loss': 0.5585563868284226, 'train/mask_loss': 0.7038856446743011, 'metrics/total_secs_per_batch': 5.884183168411255, 'metrics/data_secs_per_batch': 2.772697639465332, '_timestamp': 1740981259.094104}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 33 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981259.0944448}).
Epoch: [9][ 35/500]	Time  6.786 ( 6.786)	Loss 1.8424 (1.7946)	CeLoss 0.2988 (0.3461)	SegCLSLoss 0.0140 (0.0149)	KLLoss 0.3613 (0.3299)	MaskLoss 0.7493 (0.7038)	MaskBCELoss 0.0298 (0.1983)	MaskDICELoss 0.7196 (0.5055)
Epoch: [9][ 36/500]	Time  6.083 ( 6.083)	Loss 1.8203 (1.3423)	CeLoss 0.2119 (0.3849)	SegCLSLoss 0.0167 (0.0110)	KLLoss 0.3613 (0.2176)	MaskLoss 0.7812 (0.4650)	MaskBCELoss 0.0709 (0.0847)	MaskDICELoss 0.7103 (0.3803)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 34 is less than current step: 499. Dropping entry: {'train/loss': 1.7945705652236938, 'train/ce_loss': 0.34609375, 'train/seg_cls_loss': 0.01492919921875, 'train/kl_loss': 0.3298828125, 'train/mask_bce_loss': 0.19826391469687224, 'train/mask_dice_loss': 0.5055155023932457, 'train/mask_loss': 0.7037794142961502, 'metrics/total_secs_per_batch': 6.785916566848755, 'metrics/data_secs_per_batch': 3.097729969024658, '_timestamp': 1740981265.879855}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 34 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981265.880136}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 35 is less than current step: 499. Dropping entry: {'train/loss': 1.342266845703125, 'train/ce_loss': 0.38486328125, 'train/seg_cls_loss': 0.01104736328125, 'train/kl_loss': 0.217578125, 'train/mask_bce_loss': 0.08470814451575279, 'train/mask_dice_loss': 0.38027293384075167, 'train/mask_loss': 0.4649810761213303, 'metrics/total_secs_per_batch': 6.083016395568848, 'metrics/data_secs_per_batch': 2.8011497259140015, '_timestamp': 1740981271.962851}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 35 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981271.9632351}).
Epoch: [9][ 37/500]	Time  6.466 ( 6.466)	Loss 1.5254 (1.5223)	CeLoss 0.1768 (0.2919)	SegCLSLoss 0.0227 (0.0137)	KLLoss 0.3555 (0.2568)	MaskLoss 0.6504 (0.5988)	MaskBCELoss 0.0854 (0.1694)	MaskDICELoss 0.5650 (0.4294)
Epoch: [9][ 38/500]	Time  4.908 ( 4.908)	Loss 1.1953 (1.6889)	CeLoss 1.1953 (0.4977)	SegCLSLoss 0.0000 (0.0143)	KLLoss 0.0000 (0.2238)	MaskLoss 0.0000 (0.5808)	MaskBCELoss 0.0000 (0.1399)	MaskDICELoss 0.0000 (0.4410)
Epoch: [9][ 39/500]	Time  6.373 ( 6.373)	Loss 2.5136 (1.6376)	CeLoss 0.2812 (0.2860)	SegCLSLoss 0.0123 (0.0128)	KLLoss 0.3613 (0.2930)	MaskLoss 1.0947 (0.6578)	MaskBCELoss 0.3942 (0.1534)	MaskDICELoss 0.7005 (0.5045)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 36 is less than current step: 499. Dropping entry: {'train/loss': 1.5222905158996582, 'train/ce_loss': 0.291943359375, 'train/seg_cls_loss': 0.013671875, 'train/kl_loss': 0.2568359375, 'train/mask_bce_loss': 0.16937408596277237, 'train/mask_dice_loss': 0.4293932348489761, 'train/mask_loss': 0.5987673401832581, 'metrics/total_secs_per_batch': 6.466243028640747, 'metrics/data_secs_per_batch': 2.966877055168152, '_timestamp': 1740981278.4290838}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 36 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981278.4293528}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 37 is less than current step: 499. Dropping entry: {'train/loss': 1.6889259696006775, 'train/ce_loss': 0.49765625, 'train/seg_cls_loss': 0.014324951171875, 'train/kl_loss': 0.223828125, 'train/mask_bce_loss': 0.1398814738728106, 'train/mask_dice_loss': 0.44095846116542814, 'train/mask_loss': 0.5808399319648743, 'metrics/total_secs_per_batch': 4.907557249069214, 'metrics/data_secs_per_batch': 2.352388834953308, '_timestamp': 1740981283.3368719}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 37 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981283.3372254}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 38 is less than current step: 499. Dropping entry: {'train/loss': 1.6376397490501404, 'train/ce_loss': 0.28603515625, 'train/seg_cls_loss': 0.012786865234375, 'train/kl_loss': 0.29296875, 'train/mask_bce_loss': 0.15338315144181253, 'train/mask_dice_loss': 0.5044503957033157, 'train/mask_loss': 0.6578335464000702, 'metrics/total_secs_per_batch': 6.372537136077881, 'metrics/data_secs_per_batch': 2.764852237701416, '_timestamp': 1740981289.709242}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 38 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981289.7095351}).
[2025-03-02 23:54:55,306] [INFO] [logging.py:96:log_dist] [Rank 0] step=4540, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:54:55,312] [INFO] [timer.py:215:stop] epoch=0/micro_step=45400/global_step=4540, RunningAvgSamplesPerSec=1.4967647716942167, CurrSamplesPerSec=1.7850362667204154, MemAllocated=31.25GB, MaxMemAllocated=37.23GB
Epoch: [9][ 40/500]	Time  5.604 ( 5.604)	Loss 2.7284 (1.6100)	CeLoss 0.2324 (0.5194)	SegCLSLoss 0.0168 (0.0088)	KLLoss 0.3574 (0.2176)	MaskLoss 1.2255 (0.5321)	MaskBCELoss 0.2968 (0.1203)	MaskDICELoss 0.9288 (0.4118)
Epoch: [9][ 41/500]	Time  6.112 ( 6.112)	Loss 0.6094 (1.8089)	CeLoss 0.6094 (0.3834)	SegCLSLoss 0.0000 (0.0136)	KLLoss 0.0000 (0.2932)	MaskLoss 0.0000 (0.6946)	MaskBCELoss 0.0000 (0.2522)	MaskDICELoss 0.0000 (0.4424)
Epoch: [9][ 42/500]	Time  5.237 ( 5.237)	Loss 0.0718 (1.2715)	CeLoss 0.0718 (0.3569)	SegCLSLoss 0.0000 (0.0097)	KLLoss 0.0000 (0.2182)	MaskLoss 0.0000 (0.4439)	MaskBCELoss 0.0000 (0.0865)	MaskDICELoss 0.0000 (0.3574)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 39 is less than current step: 499. Dropping entry: {'train/loss': 1.609997034072876, 'train/ce_loss': 0.51943359375, 'train/seg_cls_loss': 0.008770751953125, 'train/kl_loss': 0.217578125, 'train/mask_bce_loss': 0.12026725430041552, 'train/mask_dice_loss': 0.411830872297287, 'train/mask_loss': 0.5320981264114379, 'metrics/total_secs_per_batch': 5.603802680969238, 'metrics/data_secs_per_batch': 2.721267557144165, '_timestamp': 1740981295.3127785}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 39 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981295.3129632}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 40 is less than current step: 499. Dropping entry: {'train/loss': 1.8089494585990906, 'train/ce_loss': 0.3833984375, 'train/seg_cls_loss': 0.013616943359375, 'train/kl_loss': 0.2931640625, 'train/mask_bce_loss': 0.2521937720477581, 'train/mask_dice_loss': 0.4423688441514969, 'train/mask_loss': 0.6945626199245453, 'metrics/total_secs_per_batch': 6.112457752227783, 'metrics/data_secs_per_batch': 2.6571091175079347, '_timestamp': 1740981301.4256313}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 40 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981301.4259772}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 41 is less than current step: 499. Dropping entry: {'train/loss': 1.2714829295873642, 'train/ce_loss': 0.35693359375, 'train/seg_cls_loss': 0.009661865234375, 'train/kl_loss': 0.2181640625, 'train/mask_bce_loss': 0.08646325916051864, 'train/mask_dice_loss': 0.35743249505758284, 'train/mask_loss': 0.44389574937522414, 'metrics/total_secs_per_batch': 5.2369701862335205, 'metrics/data_secs_per_batch': 2.428815746307373, '_timestamp': 1740981306.66247}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 41 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981306.6627617}).
Epoch: [9][ 43/500]	Time  4.635 ( 4.635)	Loss 0.3730 (1.1106)	CeLoss 0.3730 (0.5979)	SegCLSLoss 0.0000 (0.0043)	KLLoss 0.0000 (0.1082)	MaskLoss 0.0000 (0.2499)	MaskBCELoss 0.0000 (0.0416)	MaskDICELoss 0.0000 (0.2083)
Epoch: [9][ 44/500]	Time  5.868 ( 5.868)	Loss 2.4466 (1.7318)	CeLoss 0.1953 (0.3812)	SegCLSLoss 0.0242 (0.0158)	KLLoss 0.3535 (0.2914)	MaskLoss 1.1022 (0.6569)	MaskBCELoss 0.2561 (0.1157)	MaskDICELoss 0.8461 (0.5411)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 42 is less than current step: 499. Dropping entry: {'train/loss': 1.1106415271759034, 'train/ce_loss': 0.5978515625, 'train/seg_cls_loss': 0.00428466796875, 'train/kl_loss': 0.108203125, 'train/mask_bce_loss': 0.04161341693252325, 'train/mask_dice_loss': 0.2083362489938736, 'train/mask_loss': 0.24994966387748718, 'metrics/total_secs_per_batch': 4.634833574295044, 'metrics/data_secs_per_batch': 1.849514102935791, '_timestamp': 1740981311.2972598}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 42 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981311.2974544}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 43 is less than current step: 499. Dropping entry: {'train/loss': 1.7318008065223693, 'train/ce_loss': 0.38125, 'train/seg_cls_loss': 0.015814208984375, 'train/kl_loss': 0.29140625, 'train/mask_bce_loss': 0.1157470541074872, 'train/mask_dice_loss': 0.5411201491951942, 'train/mask_loss': 0.6568671897053718, 'metrics/total_secs_per_batch': 5.868053197860718, 'metrics/data_secs_per_batch': 2.5385505676269533, '_timestamp': 1740981317.1655474}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 43 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981317.1658885}).
Epoch: [9][ 45/500]	Time  6.067 ( 6.067)	Loss 2.3871 (1.8215)	CeLoss 0.1123 (0.2248)	SegCLSLoss 0.0288 (0.0158)	KLLoss 0.3828 (0.3635)	MaskLoss 1.1110 (0.7761)	MaskBCELoss 0.1677 (0.1564)	MaskDICELoss 0.9433 (0.6197)
Epoch: [9][ 46/500]	Time  6.388 ( 6.388)	Loss 1.3967 (1.6869)	CeLoss 0.1943 (0.3075)	SegCLSLoss 0.0229 (0.0140)	KLLoss 0.3594 (0.3270)	MaskLoss 0.5773 (0.6699)	MaskBCELoss 0.0913 (0.1402)	MaskDICELoss 0.4860 (0.5297)
Epoch: [9][ 47/500]	Time  5.569 ( 5.569)	Loss 1.8864 (1.2703)	CeLoss 0.2021 (0.5449)	SegCLSLoss 0.0215 (0.0076)	KLLoss 0.3750 (0.1506)	MaskLoss 0.8182 (0.3532)	MaskBCELoss 0.0682 (0.0366)	MaskDICELoss 0.7500 (0.3166)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 44 is less than current step: 499. Dropping entry: {'train/loss': 1.8215230077505111, 'train/ce_loss': 0.2248046875, 'train/seg_cls_loss': 0.01578369140625, 'train/kl_loss': 0.3634765625, 'train/mask_bce_loss': 0.15638539120554923, 'train/mask_dice_loss': 0.6197081476449966, 'train/mask_loss': 0.7760935351252556, 'metrics/total_secs_per_batch': 6.0665552616119385, 'metrics/data_secs_per_batch': 2.8882481813430787, '_timestamp': 1740981323.2318559}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 44 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981323.2321243}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 45 is less than current step: 499. Dropping entry: {'train/loss': 1.686919629573822, 'train/ce_loss': 0.30751953125, 'train/seg_cls_loss': 0.0139892578125, 'train/kl_loss': 0.326953125, 'train/mask_bce_loss': 0.14024554807692766, 'train/mask_dice_loss': 0.5296791180968284, 'train/mask_loss': 0.66992467045784, 'metrics/total_secs_per_batch': 6.388214349746704, 'metrics/data_secs_per_batch': 2.8051519870758055, '_timestamp': 1740981329.6205964}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 45 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981329.621089}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 46 is less than current step: 499. Dropping entry: {'train/loss': 1.270348858833313, 'train/ce_loss': 0.544921875, 'train/seg_cls_loss': 0.00755615234375, 'train/kl_loss': 0.1505859375, 'train/mask_bce_loss': 0.03661769265308976, 'train/mask_dice_loss': 0.3165743052959442, 'train/mask_loss': 0.353192001581192, 'metrics/total_secs_per_batch': 5.56894326210022, 'metrics/data_secs_per_batch': 2.656555938720703, '_timestamp': 1740981335.1890817}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 46 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981335.1893806}).
Epoch: [9][ 48/500]	Time  7.622 ( 7.622)	Loss 2.0061 (1.6340)	CeLoss 0.3359 (0.2215)	SegCLSLoss 0.0099 (0.0141)	KLLoss 0.3633 (0.3275)	MaskLoss 0.8136 (0.6861)	MaskBCELoss 0.1464 (0.1365)	MaskDICELoss 0.6672 (0.5496)
Epoch: [9][ 49/500]	Time  6.038 ( 6.038)	Loss 0.7170 (1.5976)	CeLoss 0.2090 (0.3643)	SegCLSLoss 0.0131 (0.0168)	KLLoss 0.3809 (0.2934)	MaskLoss 0.2316 (0.5978)	MaskBCELoss 0.0904 (0.1073)	MaskDICELoss 0.1412 (0.4905)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 47 is less than current step: 499. Dropping entry: {'train/loss': 1.6340372920036317, 'train/ce_loss': 0.221533203125, 'train/seg_cls_loss': 0.014117431640625, 'train/kl_loss': 0.3275390625, 'train/mask_bce_loss': 0.13650712594389916, 'train/mask_dice_loss': 0.5496277332305908, 'train/mask_loss': 0.6861348628997803, 'metrics/total_secs_per_batch': 7.622372388839722, 'metrics/data_secs_per_batch': 3.340913987159729, '_timestamp': 1740981342.811409}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 47 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981342.8117785}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 48 is less than current step: 499. Dropping entry: {'train/loss': 1.5975603580474853, 'train/ce_loss': 0.3642578125, 'train/seg_cls_loss': 0.016827392578125, 'train/kl_loss': 0.293359375, 'train/mask_bce_loss': 0.1072550724260509, 'train/mask_dice_loss': 0.49054854512214663, 'train/mask_loss': 0.5978036165237427, 'metrics/total_secs_per_batch': 6.037900686264038, 'metrics/data_secs_per_batch': 2.8266796350479124, '_timestamp': 1740981348.8495536}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 48 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981348.849915}).
[2025-03-02 23:55:54,594] [INFO] [logging.py:96:log_dist] [Rank 0] step=4550, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:55:54,600] [INFO] [timer.py:215:stop] epoch=0/micro_step=45500/global_step=4550, RunningAvgSamplesPerSec=1.497136097427527, CurrSamplesPerSec=1.7391406746729894, MemAllocated=31.42GB, MaxMemAllocated=37.23GB
Epoch: [9][ 50/500]	Time  5.752 ( 5.752)	Loss 0.0811 (1.3182)	CeLoss 0.0811 (0.4438)	SegCLSLoss 0.0000 (0.0109)	KLLoss 0.0000 (0.1863)	MaskLoss 0.0000 (0.4253)	MaskBCELoss 0.0000 (0.1314)	MaskDICELoss 0.0000 (0.2939)
Epoch: [9][ 51/500]	Time  6.696 ( 6.696)	Loss 1.3084 (1.9005)	CeLoss 0.2197 (0.2099)	SegCLSLoss 0.0099 (0.0158)	KLLoss 0.3672 (0.3293)	MaskLoss 0.5234 (0.8248)	MaskBCELoss 0.0811 (0.2195)	MaskDICELoss 0.4422 (0.6053)
Epoch: [9][ 52/500]	Time  6.454 ( 6.454)	Loss 2.4595 (1.8806)	CeLoss 0.2256 (0.2142)	SegCLSLoss 0.0189 (0.0189)	KLLoss 0.3652 (0.3633)	MaskLoss 1.0940 (0.8103)	MaskBCELoss 0.1181 (0.1642)	MaskDICELoss 0.9759 (0.6461)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 49 is less than current step: 499. Dropping entry: {'train/loss': 1.3181836485862732, 'train/ce_loss': 0.44384765625, 'train/seg_cls_loss': 0.010919189453125, 'train/kl_loss': 0.186328125, 'train/mask_bce_loss': 0.13140005599707366, 'train/mask_dice_loss': 0.2938538685441017, 'train/mask_loss': 0.4252539277076721, 'metrics/total_secs_per_batch': 5.751853704452515, 'metrics/data_secs_per_batch': 2.4563724994659424, '_timestamp': 1740981354.6009812}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 49 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981354.601259}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 50 is less than current step: 499. Dropping entry: {'train/loss': 1.90053950548172, 'train/ce_loss': 0.20986328125, 'train/seg_cls_loss': 0.01583251953125, 'train/kl_loss': 0.329296875, 'train/mask_bce_loss': 0.21949841938912867, 'train/mask_dice_loss': 0.6053318679332733, 'train/mask_loss': 0.824830287694931, 'metrics/total_secs_per_batch': 6.696311712265015, 'metrics/data_secs_per_batch': 2.917717623710632, '_timestamp': 1740981361.297535}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 50 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981361.297827}).
Epoch: [9][ 53/500]	Time  6.543 ( 6.543)	Loss 0.1338 (1.5776)	CeLoss 0.1338 (0.2282)	SegCLSLoss 0.0000 (0.0139)	KLLoss 0.0000 (0.3260)	MaskLoss 0.0000 (0.6550)	MaskBCELoss 0.0000 (0.0780)	MaskDICELoss 0.0000 (0.5771)
Epoch: [9][ 54/500]	Time  6.503 ( 6.503)	Loss 1.4139 (1.7305)	CeLoss 0.2598 (0.3272)	SegCLSLoss 0.0182 (0.0156)	KLLoss 0.3535 (0.3254)	MaskLoss 0.5546 (0.6814)	MaskBCELoss 0.0671 (0.0806)	MaskDICELoss 0.4876 (0.6008)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 51 is less than current step: 499. Dropping entry: {'train/loss': 1.880628478527069, 'train/ce_loss': 0.21416015625, 'train/seg_cls_loss': 0.01888427734375, 'train/kl_loss': 0.36328125, 'train/mask_bce_loss': 0.16421422623097898, 'train/mask_dice_loss': 0.646070709824562, 'train/mask_loss': 0.8102849274873734, 'metrics/total_secs_per_batch': 6.453865051269531, 'metrics/data_secs_per_batch': 2.7321566104888917, '_timestamp': 1740981367.7513514}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 51 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981367.7516372}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 52 is less than current step: 499. Dropping entry: {'train/loss': 1.5775526583194732, 'train/ce_loss': 0.22822265625, 'train/seg_cls_loss': 0.0139404296875, 'train/kl_loss': 0.3259765625, 'train/mask_bce_loss': 0.07798364562913776, 'train/mask_dice_loss': 0.5770524397492409, 'train/mask_loss': 0.6550360888242721, 'metrics/total_secs_per_batch': 6.543197393417358, 'metrics/data_secs_per_batch': 2.9577630758285522, '_timestamp': 1740981374.2947683}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 52 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981374.2951183}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 53 is less than current step: 499. Dropping entry: {'train/loss': 1.73053081035614, 'train/ce_loss': 0.32724609375, 'train/seg_cls_loss': 0.015643310546875, 'train/kl_loss': 0.325390625, 'train/mask_bce_loss': 0.080640022829175, 'train/mask_dice_loss': 0.6007874876260757, 'train/mask_loss': 0.6814275085926056, 'metrics/total_secs_per_batch': 6.503441095352173, 'metrics/data_secs_per_batch': 3.063170146942139, '_timestamp': 1740981380.7980087}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 53 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981380.7982845}).
Epoch: [9][ 55/500]	Time  5.272 ( 5.272)	Loss 1.7946 (1.4032)	CeLoss 0.1729 (0.4856)	SegCLSLoss 0.0250 (0.0100)	KLLoss 0.3496 (0.2166)	MaskLoss 0.7870 (0.4455)	MaskBCELoss 0.0896 (0.0671)	MaskDICELoss 0.6973 (0.3783)
Epoch: [9][ 56/500]	Time  4.363 ( 4.363)	Loss 1.1719 (1.4187)	CeLoss 1.1719 (0.7332)	SegCLSLoss 0.0000 (0.0066)	KLLoss 0.0000 (0.1822)	MaskLoss 0.0000 (0.3318)	MaskBCELoss 0.0000 (0.0435)	MaskDICELoss 0.0000 (0.2884)
Epoch: [9][ 57/500]	Time  5.638 ( 5.638)	Loss 1.8556 (1.5526)	CeLoss 0.2559 (0.3734)	SegCLSLoss 0.0244 (0.0126)	KLLoss 0.3691 (0.2951)	MaskLoss 0.7755 (0.5715)	MaskBCELoss 0.2673 (0.1900)	MaskDICELoss 0.5082 (0.3814)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 54 is less than current step: 499. Dropping entry: {'train/loss': 1.4032097578048706, 'train/ce_loss': 0.48564453125, 'train/seg_cls_loss': 0.009954833984375, 'train/kl_loss': 0.2166015625, 'train/mask_bce_loss': 0.06711894031614066, 'train/mask_dice_loss': 0.37833358347415924, 'train/mask_loss': 0.44545252025127413, 'metrics/total_secs_per_batch': 5.272290229797363, 'metrics/data_secs_per_batch': 2.2918092966079713, '_timestamp': 1740981386.0702932}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 54 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981386.0706367}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 55 is less than current step: 499. Dropping entry: {'train/loss': 1.4187224745750426, 'train/ce_loss': 0.733203125, 'train/seg_cls_loss': 0.006597900390625, 'train/kl_loss': 0.1822265625, 'train/mask_bce_loss': 0.04346357239410281, 'train/mask_dice_loss': 0.28835860192775725, 'train/mask_loss': 0.3318221777677536, 'metrics/total_secs_per_batch': 4.363447189331055, 'metrics/data_secs_per_batch': 2.0900057554244995, '_timestamp': 1740981390.43377}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 55 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981390.4340622}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 56 is less than current step: 499. Dropping entry: {'train/loss': 1.5526106119155885, 'train/ce_loss': 0.373388671875, 'train/seg_cls_loss': 0.012640380859375, 'train/kl_loss': 0.2951171875, 'train/mask_bce_loss': 0.19004986621439457, 'train/mask_dice_loss': 0.3814458504319191, 'train/mask_loss': 0.5714957296848298, 'metrics/total_secs_per_batch': 5.638026237487793, 'metrics/data_secs_per_batch': 2.1616353511810305, '_timestamp': 1740981396.0717793}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 56 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981396.0720522}).
Epoch: [9][ 58/500]	Time  5.533 ( 5.533)	Loss 1.0664 (1.3129)	CeLoss 0.3105 (0.3852)	SegCLSLoss 0.0111 (0.0109)	KLLoss 0.3574 (0.2887)	MaskLoss 0.3574 (0.4467)	MaskBCELoss 0.0734 (0.0876)	MaskDICELoss 0.2840 (0.3591)
Epoch: [9][ 59/500]	Time  6.539 ( 6.539)	Loss 1.8851 (1.5241)	CeLoss 0.2090 (0.5550)	SegCLSLoss 0.0190 (0.0085)	KLLoss 0.3555 (0.1811)	MaskLoss 0.8156 (0.4732)	MaskBCELoss 0.0483 (0.1268)	MaskDICELoss 0.7672 (0.3465)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 57 is less than current step: 499. Dropping entry: {'train/loss': 1.312935507297516, 'train/ce_loss': 0.38515625, 'train/seg_cls_loss': 0.01090087890625, 'train/kl_loss': 0.288671875, 'train/mask_bce_loss': 0.08757285848259926, 'train/mask_dice_loss': 0.3591292664408684, 'train/mask_loss': 0.4467021316289902, 'metrics/total_secs_per_batch': 5.532927751541138, 'metrics/data_secs_per_batch': 2.670670580863953, '_timestamp': 1740981401.6047466}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 57 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981401.6050477}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 58 is less than current step: 499. Dropping entry: {'train/loss': 1.5240826606750488, 'train/ce_loss': 0.555029296875, 'train/seg_cls_loss': 0.008544921875, 'train/kl_loss': 0.1810546875, 'train/mask_bce_loss': 0.12679254300892354, 'train/mask_dice_loss': 0.3464548408985138, 'train/mask_loss': 0.4732473850250244, 'metrics/total_secs_per_batch': 6.539407968521118, 'metrics/data_secs_per_batch': 2.946630668640137, '_timestamp': 1740981408.1443295}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 58 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981408.1447601}).
[2025-03-02 23:56:52,694] [INFO] [logging.py:96:log_dist] [Rank 0] step=4560, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:56:52,699] [INFO] [timer.py:215:stop] epoch=0/micro_step=45600/global_step=4560, RunningAvgSamplesPerSec=1.497564413490296, CurrSamplesPerSec=2.1955967394663096, MemAllocated=30.78GB, MaxMemAllocated=37.23GB
Epoch: [9][ 60/500]	Time  4.556 ( 4.556)	Loss 0.9609 (1.6559)	CeLoss 0.9609 (0.6125)	SegCLSLoss 0.0000 (0.0087)	KLLoss 0.0000 (0.2164)	MaskLoss 0.0000 (0.5086)	MaskBCELoss 0.0000 (0.1216)	MaskDICELoss 0.0000 (0.3870)
Epoch: [9][ 61/500]	Time  5.702 ( 5.702)	Loss 0.8750 (1.5762)	CeLoss 0.2773 (0.5061)	SegCLSLoss 0.0139 (0.0095)	KLLoss 0.3652 (0.2545)	MaskLoss 0.2764 (0.5200)	MaskBCELoss 0.1880 (0.1101)	MaskDICELoss 0.0884 (0.4099)
Epoch: [9][ 62/500]	Time  7.019 ( 7.019)	Loss 1.9689 (2.0458)	CeLoss 0.1875 (0.2225)	SegCLSLoss 0.0215 (0.0185)	KLLoss 0.3516 (0.3643)	MaskLoss 0.8677 (0.8888)	MaskBCELoss 0.0513 (0.1727)	MaskDICELoss 0.8165 (0.7161)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 59 is less than current step: 499. Dropping entry: {'train/loss': 1.6558501243591308, 'train/ce_loss': 0.6125, 'train/seg_cls_loss': 0.008673095703125, 'train/kl_loss': 0.21640625, 'train/mask_bce_loss': 0.12160539161413908, 'train/mask_dice_loss': 0.387032562494278, 'train/mask_loss': 0.5086379587650299, 'metrics/total_secs_per_batch': 4.556490421295166, 'metrics/data_secs_per_batch': 1.9032568454742431, '_timestamp': 1740981412.7004144}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 59 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981412.7006824}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 60 is less than current step: 499. Dropping entry: {'train/loss': 1.5762446343898773, 'train/ce_loss': 0.5060546875, 'train/seg_cls_loss': 0.009454345703125, 'train/kl_loss': 0.2544921875, 'train/mask_bce_loss': 0.11014131437987089, 'train/mask_dice_loss': 0.4098657608032227, 'train/mask_loss': 0.5200070768594742, 'metrics/total_secs_per_batch': 5.702327728271484, 'metrics/data_secs_per_batch': 2.8521119594573974, '_timestamp': 1740981418.4029596}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 60 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981418.403259}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 61 is less than current step: 499. Dropping entry: {'train/loss': 2.0457801938056948, 'train/ce_loss': 0.2224609375, 'train/seg_cls_loss': 0.01854248046875, 'train/kl_loss': 0.3642578125, 'train/mask_bce_loss': 0.17268601767718791, 'train/mask_dice_loss': 0.716122055053711, 'train/mask_loss': 0.8888080656528473, 'metrics/total_secs_per_batch': 7.018550634384155, 'metrics/data_secs_per_batch': 3.0460565090179443, '_timestamp': 1740981425.4216893}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 61 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981425.4220474}).
Epoch: [9][ 63/500]	Time  5.292 ( 5.292)	Loss 1.7445 (1.2894)	CeLoss 0.2910 (0.2932)	SegCLSLoss 0.0112 (0.0115)	KLLoss 0.3613 (0.2895)	MaskLoss 0.7062 (0.4807)	MaskBCELoss 0.4442 (0.0933)	MaskDICELoss 0.2620 (0.3875)
Epoch: [9][ 64/500]	Time  5.447 ( 5.447)	Loss 1.5562 (1.5809)	CeLoss 0.2490 (0.5866)	SegCLSLoss 0.0101 (0.0129)	KLLoss 0.3633 (0.2543)	MaskLoss 0.6326 (0.4813)	MaskBCELoss 0.1042 (0.0818)	MaskDICELoss 0.5284 (0.3995)
Epoch: [9][ 65/500]	Time  6.287 ( 6.287)	Loss 2.3246 (1.8110)	CeLoss 0.2334 (0.3499)	SegCLSLoss 0.0129 (0.0183)	KLLoss 0.3613 (0.3252)	MaskLoss 1.0236 (0.7095)	MaskBCELoss 0.0935 (0.0981)	MaskDICELoss 0.9301 (0.6114)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 62 is less than current step: 499. Dropping entry: {'train/loss': 1.289422744512558, 'train/ce_loss': 0.2931640625, 'train/seg_cls_loss': 0.0114501953125, 'train/kl_loss': 0.289453125, 'train/mask_bce_loss': 0.09325675731524825, 'train/mask_dice_loss': 0.3874897606670856, 'train/mask_loss': 0.4807465150952339, 'metrics/total_secs_per_batch': 5.291542053222656, 'metrics/data_secs_per_batch': 2.5115613460540773, '_timestamp': 1740981430.71304}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 62 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981430.7133121}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 63 is less than current step: 499. Dropping entry: {'train/loss': 1.5809031844139099, 'train/ce_loss': 0.58662109375, 'train/seg_cls_loss': 0.012939453125, 'train/kl_loss': 0.254296875, 'train/mask_bce_loss': 0.08184736873954535, 'train/mask_dice_loss': 0.39947336316108706, 'train/mask_loss': 0.4813207373023033, 'metrics/total_secs_per_batch': 5.4471354484558105, 'metrics/data_secs_per_batch': 2.6755442142486574, '_timestamp': 1740981436.1601868}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 63 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981436.1605356}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 64 is less than current step: 499. Dropping entry: {'train/loss': 1.8109651386737824, 'train/ce_loss': 0.34990234375, 'train/seg_cls_loss': 0.0183349609375, 'train/kl_loss': 0.3251953125, 'train/mask_bce_loss': 0.09812117014080286, 'train/mask_dice_loss': 0.6114141285419464, 'train/mask_loss': 0.7095352917909622, 'metrics/total_secs_per_batch': 6.28685188293457, 'metrics/data_secs_per_batch': 2.7102853059768677, '_timestamp': 1740981442.4471827}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 64 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981442.4475083}).
Epoch: [9][ 66/500]	Time  6.190 ( 6.190)	Loss 0.7812 (1.5187)	CeLoss 0.7812 (0.3242)	SegCLSLoss 0.0000 (0.0123)	KLLoss 0.0000 (0.2533)	MaskLoss 0.0000 (0.5814)	MaskBCELoss 0.0000 (0.1109)	MaskDICELoss 0.0000 (0.4705)
Epoch: [9][ 67/500]	Time  6.411 ( 6.411)	Loss 2.2739 (1.7242)	CeLoss 0.2021 (0.3543)	SegCLSLoss 0.0212 (0.0143)	KLLoss 0.3633 (0.2889)	MaskLoss 1.0124 (0.6670)	MaskBCELoss 0.0136 (0.0927)	MaskDICELoss 0.9988 (0.5743)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 65 is less than current step: 499. Dropping entry: {'train/loss': 1.5187434434890748, 'train/ce_loss': 0.32421875, 'train/seg_cls_loss': 0.01234130859375, 'train/kl_loss': 0.2533203125, 'train/mask_bce_loss': 0.11091574188321829, 'train/mask_dice_loss': 0.4705262929201126, 'train/mask_loss': 0.5814420282840729, 'metrics/total_secs_per_batch': 6.18973708152771, 'metrics/data_secs_per_batch': 2.9119135618209837, '_timestamp': 1740981448.6369364}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 65 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981448.6371682}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 66 is less than current step: 499. Dropping entry: {'train/loss': 1.7242431044578552, 'train/ce_loss': 0.354345703125, 'train/seg_cls_loss': 0.014263916015625, 'train/kl_loss': 0.2888671875, 'train/mask_bce_loss': 0.09272953141480685, 'train/mask_dice_loss': 0.5742504179477692, 'train/mask_loss': 0.6669799447059631, 'metrics/total_secs_per_batch': 6.411144495010376, 'metrics/data_secs_per_batch': 2.9463362455368043, '_timestamp': 1740981455.0479355}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 66 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981455.048225}).
Epoch: [9][ 68/500]	Time  5.898 ( 5.898)	Loss 1.4133 (1.5103)	CeLoss 0.2578 (0.3995)	SegCLSLoss 0.0101 (0.0129)	KLLoss 0.3652 (0.2871)	MaskLoss 0.5562 (0.5379)	MaskBCELoss 0.0911 (0.0753)	MaskDICELoss 0.4651 (0.4626)
Epoch: [9][ 69/500]	Time  6.264 ( 6.264)	Loss 2.4078 (1.7237)	CeLoss 0.1914 (0.3817)	SegCLSLoss 0.0194 (0.0148)	KLLoss 0.3711 (0.3258)	MaskLoss 1.0848 (0.6509)	MaskBCELoss 0.3091 (0.0860)	MaskDICELoss 0.7757 (0.5649)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 67 is less than current step: 499. Dropping entry: {'train/loss': 1.5102712631225585, 'train/ce_loss': 0.39951171875, 'train/seg_cls_loss': 0.0129150390625, 'train/kl_loss': 0.287109375, 'train/mask_bce_loss': 0.07526621613651514, 'train/mask_dice_loss': 0.4625842452049255, 'train/mask_loss': 0.5378504693508148, 'metrics/total_secs_per_batch': 5.897565126419067, 'metrics/data_secs_per_batch': 2.6644570589065553, '_timestamp': 1740981460.9456606}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 67 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981460.9460201}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 68 is less than current step: 499. Dropping entry: {'train/loss': 1.723656553030014, 'train/ce_loss': 0.38173828125, 'train/seg_cls_loss': 0.014849853515625, 'train/kl_loss': 0.32578125, 'train/mask_bce_loss': 0.08597014648839832, 'train/mask_dice_loss': 0.5649206236004829, 'train/mask_loss': 0.650890764594078, 'metrics/total_secs_per_batch': 6.264059782028198, 'metrics/data_secs_per_batch': 2.715299153327942, '_timestamp': 1740981467.2095928}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 68 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981467.2098985}).
[2025-03-02 23:57:52,730] [INFO] [logging.py:96:log_dist] [Rank 0] step=4570, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:57:52,736] [INFO] [timer.py:215:stop] epoch=0/micro_step=45700/global_step=4570, RunningAvgSamplesPerSec=1.497895933888295, CurrSamplesPerSec=1.8094739588157511, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [9][ 70/500]	Time  5.528 ( 5.528)	Loss 1.5961 (1.6972)	CeLoss 0.2559 (0.6165)	SegCLSLoss 0.0121 (0.0116)	KLLoss 0.3633 (0.2182)	MaskLoss 0.6486 (0.5266)	MaskBCELoss 0.1805 (0.0869)	MaskDICELoss 0.4681 (0.4397)
Epoch: [9][ 71/500]	Time  5.212 ( 5.212)	Loss 0.5859 (1.9845)	CeLoss 0.5859 (0.3308)	SegCLSLoss 0.0000 (0.0137)	KLLoss 0.0000 (0.2945)	MaskLoss 0.0000 (0.8087)	MaskBCELoss 0.0000 (0.2400)	MaskDICELoss 0.0000 (0.5687)
Epoch: [9][ 72/500]	Time  5.751 ( 5.751)	Loss 1.0668 (1.6948)	CeLoss 0.2324 (0.4230)	SegCLSLoss 0.0110 (0.0148)	KLLoss 0.3652 (0.2881)	MaskLoss 0.3957 (0.6177)	MaskBCELoss 0.2030 (0.0535)	MaskDICELoss 0.1927 (0.5643)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 69 is less than current step: 499. Dropping entry: {'train/loss': 1.6971883177757263, 'train/ce_loss': 0.61650390625, 'train/seg_cls_loss': 0.01160888671875, 'train/kl_loss': 0.2181640625, 'train/mask_bce_loss': 0.08689759224653244, 'train/mask_dice_loss': 0.43972390592098237, 'train/mask_loss': 0.5266215085983277, 'metrics/total_secs_per_batch': 5.528062105178833, 'metrics/data_secs_per_batch': 2.3165504217147825, '_timestamp': 1740981472.737427}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 69 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981472.7377014}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 70 is less than current step: 499. Dropping entry: {'train/loss': 1.9845179557800292, 'train/ce_loss': 0.33076171875, 'train/seg_cls_loss': 0.01365966796875, 'train/kl_loss': 0.29453125, 'train/mask_bce_loss': 0.2400031592696905, 'train/mask_dice_loss': 0.5686620771884918, 'train/mask_loss': 0.8086652278900146, 'metrics/total_secs_per_batch': 5.211966037750244, 'metrics/data_secs_per_batch': 2.364079737663269, '_timestamp': 1740981477.949592}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 70 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981477.9498806}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 71 is less than current step: 499. Dropping entry: {'train/loss': 1.6948186993598937, 'train/ce_loss': 0.423046875, 'train/seg_cls_loss': 0.014849853515625, 'train/kl_loss': 0.2880859375, 'train/mask_bce_loss': 0.05346707459539175, 'train/mask_dice_loss': 0.564254766702652, 'train/mask_loss': 0.6177218377590179, 'metrics/total_secs_per_batch': 5.7507240772247314, 'metrics/data_secs_per_batch': 2.7188633918762206, '_timestamp': 1740981483.7005813}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 71 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981483.7009614}).
Epoch: [9][ 73/500]	Time  5.497 ( 5.497)	Loss 1.6231 (1.5031)	CeLoss 0.2578 (0.5068)	SegCLSLoss 0.0112 (0.0096)	KLLoss 0.3613 (0.2523)	MaskLoss 0.6612 (0.4829)	MaskBCELoss 0.1705 (0.1143)	MaskDICELoss 0.4906 (0.3686)
Epoch: [9][ 74/500]	Time  6.109 ( 6.109)	Loss 2.2761 (1.5136)	CeLoss 0.2012 (0.3087)	SegCLSLoss 0.0167 (0.0111)	KLLoss 0.3691 (0.2535)	MaskLoss 1.0145 (0.5871)	MaskBCELoss 0.0147 (0.0966)	MaskDICELoss 0.9998 (0.4905)
Epoch: [9][ 75/500]	Time  5.270 ( 5.270)	Loss 1.9542 (1.0667)	CeLoss 0.2734 (0.4226)	SegCLSLoss 0.0115 (0.0077)	KLLoss 0.3594 (0.1805)	MaskLoss 0.8199 (0.3112)	MaskBCELoss 0.1314 (0.0465)	MaskDICELoss 0.6884 (0.2646)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 72 is less than current step: 499. Dropping entry: {'train/loss': 1.503147703409195, 'train/ce_loss': 0.5068359375, 'train/seg_cls_loss': 0.009625244140625, 'train/kl_loss': 0.25234375, 'train/mask_bce_loss': 0.11433451771736144, 'train/mask_dice_loss': 0.368586990237236, 'train/mask_loss': 0.4829215109348297, 'metrics/total_secs_per_batch': 5.496608018875122, 'metrics/data_secs_per_batch': 2.1844127655029295, '_timestamp': 1740981489.1969435}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 72 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981489.1972306}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 73 is less than current step: 499. Dropping entry: {'train/loss': 1.5135540127754212, 'train/ce_loss': 0.3086669921875, 'train/seg_cls_loss': 0.011083984375, 'train/kl_loss': 0.253515625, 'train/mask_bce_loss': 0.09658276345580816, 'train/mask_dice_loss': 0.4904798775911331, 'train/mask_loss': 0.5870626509189606, 'metrics/total_secs_per_batch': 6.108615398406982, 'metrics/data_secs_per_batch': 2.4881752729415894, '_timestamp': 1740981495.3055372}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 73 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981495.305823}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 74 is less than current step: 499. Dropping entry: {'train/loss': 1.0666885256767273, 'train/ce_loss': 0.422607421875, 'train/seg_cls_loss': 0.00767822265625, 'train/kl_loss': 0.18046875, 'train/mask_bce_loss': 0.04653233122080565, 'train/mask_dice_loss': 0.264619542658329, 'train/mask_loss': 0.3111518740653992, 'metrics/total_secs_per_batch': 5.269925594329834, 'metrics/data_secs_per_batch': 2.2370946407318115, '_timestamp': 1740981500.5757222}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 74 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981500.5760925}).
Epoch: [9][ 76/500]	Time  6.641 ( 6.641)	Loss 2.0674 (1.3580)	CeLoss 0.2363 (0.2127)	SegCLSLoss 0.0226 (0.0154)	KLLoss 0.3535 (0.3234)	MaskLoss 0.8921 (0.5526)	MaskBCELoss 0.2339 (0.0764)	MaskDICELoss 0.6582 (0.4762)
Epoch: [9][ 77/500]	Time  5.552 ( 5.552)	Loss 1.7269 (1.5091)	CeLoss 0.3320 (0.4021)	SegCLSLoss 0.0093 (0.0102)	KLLoss 0.3730 (0.2547)	MaskLoss 0.6769 (0.5382)	MaskBCELoss 0.3327 (0.1403)	MaskDICELoss 0.3443 (0.3979)
Epoch: [9][ 78/500]	Time  4.363 ( 4.363)	Loss 1.3047 (1.9459)	CeLoss 1.3047 (0.5198)	SegCLSLoss 0.0000 (0.0116)	KLLoss 0.0000 (0.2607)	MaskLoss 0.0000 (0.6970)	MaskBCELoss 0.0000 (0.2035)	MaskDICELoss 0.0000 (0.4934)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 75 is less than current step: 499. Dropping entry: {'train/loss': 1.3580271542072295, 'train/ce_loss': 0.212744140625, 'train/seg_cls_loss': 0.015447998046875, 'train/kl_loss': 0.3234375, 'train/mask_bce_loss': 0.076355523429811, 'train/mask_dice_loss': 0.47621761411428454, 'train/mask_loss': 0.5525731384754181, 'metrics/total_secs_per_batch': 6.641235113143921, 'metrics/data_secs_per_batch': 3.065054678916931, '_timestamp': 1740981507.216726}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 75 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981507.2170293}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 76 is less than current step: 499. Dropping entry: {'train/loss': 1.5091120719909668, 'train/ce_loss': 0.4021484375, 'train/seg_cls_loss': 0.0101806640625, 'train/kl_loss': 0.2546875, 'train/mask_bce_loss': 0.14033311437815427, 'train/mask_dice_loss': 0.39791432619094846, 'train/mask_loss': 0.5382474362850189, 'metrics/total_secs_per_batch': 5.551515340805054, 'metrics/data_secs_per_batch': 2.716661739349365, '_timestamp': 1740981512.768266}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 76 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981512.76856}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 77 is less than current step: 499. Dropping entry: {'train/loss': 1.9458563446998596, 'train/ce_loss': 0.51982421875, 'train/seg_cls_loss': 0.01160888671875, 'train/kl_loss': 0.2607421875, 'train/mask_bce_loss': 0.20351556241512297, 'train/mask_dice_loss': 0.49343605935573576, 'train/mask_loss': 0.6969516038894653, 'metrics/total_secs_per_batch': 4.36333441734314, 'metrics/data_secs_per_batch': 2.2282338380813598, '_timestamp': 1740981517.1315677}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 77 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981517.1318533}).
Epoch: [9][ 79/500]	Time  6.551 ( 6.551)	Loss 1.3473 (1.6564)	CeLoss 0.2490 (0.3216)	SegCLSLoss 0.0091 (0.0118)	KLLoss 0.3633 (0.2898)	MaskLoss 0.5282 (0.6498)	MaskBCELoss 0.2449 (0.1203)	MaskDICELoss 0.2833 (0.5295)
[2025-03-02 23:58:49,843] [INFO] [logging.py:96:log_dist] [Rank 0] step=4580, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]
[2025-03-02 23:58:49,848] [INFO] [timer.py:215:stop] epoch=0/micro_step=45800/global_step=4580, RunningAvgSamplesPerSec=1.4983695919400462, CurrSamplesPerSec=1.6221533735792104, MemAllocated=31.24GB, MaxMemAllocated=37.23GB
Epoch: [9][ 80/500]	Time  6.167 ( 6.167)	Loss 1.6135 (1.8670)	CeLoss 0.2373 (0.3312)	SegCLSLoss 0.0097 (0.0127)	KLLoss 0.3633 (0.3252)	MaskLoss 0.6671 (0.7483)	MaskBCELoss 0.2168 (0.1721)	MaskDICELoss 0.4503 (0.5763)
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 78 is less than current step: 499. Dropping entry: {'train/loss': 1.6563607335090638, 'train/ce_loss': 0.32158203125, 'train/seg_cls_loss': 0.011785888671875, 'train/kl_loss': 0.28984375, 'train/mask_bce_loss': 0.1202622439712286, 'train/mask_dice_loss': 0.5295001596212388, 'train/mask_loss': 0.6497623920440674, 'metrics/total_secs_per_batch': 6.55140495300293, 'metrics/data_secs_per_batch': 3.052980089187622, '_timestamp': 1740981523.6831937}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 78 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981523.683569}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 79 is less than current step: 499. Dropping entry: {'train/loss': 1.8669751644134522, 'train/ce_loss': 0.33125, 'train/seg_cls_loss': 0.01273193359375, 'train/kl_loss': 0.3251953125, 'train/mask_bce_loss': 0.17207808550447226, 'train/mask_dice_loss': 0.5762532353401184, 'train/mask_loss': 0.7483313322067261, 'metrics/total_secs_per_batch': 6.166512489318848, 'metrics/data_secs_per_batch': 2.7333504915237428, '_timestamp': 1740981529.8493214}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 79 is less than current step: 499. Dropping entry: {'train/lr': 0.0, '_timestamp': 1740981529.8496308}).
Exception in thread Thread-1:
Traceback (most recent call last):
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/tensorboard/summary/writer/event_file_writer.py", line 244, in run
    self._run()
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/tensorboard/summary/writer/event_file_writer.py", line 275, in _run
    self._record_writer.write(data)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/tensorboard/summary/writer/record_writer.py", line 40, in write
    self._writer.write(header + header_crc + data + footer_crc)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 773, in write
    self.fs.append(self.filename, file_content, self.binary_mode)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 167, in append
    self._write(filename, file_content, "ab" if binary_mode else "a")
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 171, in _write
    with io.open(filename, mode, encoding=encoding) as f:
FileNotFoundError: [Errno 2] No such file or directory: b'./runs/plum-13b_kld_0.1_dice_2.0/events.out.tfevents.1740950207.blender15.cs.illinois.edu.293127.0'
Epoch: [9][ 81/500]	Time  7.173 ( 7.173)	Loss 2.6921 (1.6916)	CeLoss 0.1943 (0.2920)	SegCLSLoss 0.0190 (0.0130)	KLLoss 0.3594 (0.2895)	MaskLoss 1.2260 (0.6821)	MaskBCELoss 0.3241 (0.1419)	MaskDICELoss 0.9019 (0.5402)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 656, in <module>
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 398, in main
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 533, in train
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/tensorboard/writer.py", line 396, in add_scalar
[rank0]:     self._get_file_writer().add_summary(summary, global_step, walltime)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/tensorboard/writer.py", line 114, in add_summary
[rank0]:     self.add_event(event, global_step, walltime)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/tensorboard/writer.py", line 98, in add_event
[rank0]:     self.event_writer.add_event(event)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/tensorboard/summary/writer/event_file_writer.py", line 117, in add_event
[rank0]:     self._async_writer.write(event.SerializeToString())
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/tensorboard/summary/writer/event_file_writer.py", line 171, in write
[rank0]:     self._check_worker_status()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/tensorboard/summary/writer/event_file_writer.py", line 212, in _check_worker_status
[rank0]:     raise exception
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
[rank0]:     self.run()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/tensorboard/summary/writer/event_file_writer.py", line 244, in run
[rank0]:     self._run()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/tensorboard/summary/writer/event_file_writer.py", line 275, in _run
[rank0]:     self._record_writer.write(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/tensorboard/summary/writer/record_writer.py", line 40, in write
[rank0]:     self._writer.write(header + header_crc + data + footer_crc)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 773, in write
[rank0]:     self.fs.append(self.filename, file_content, self.binary_mode)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 167, in append
[rank0]:     self._write(filename, file_content, "ab" if binary_mode else "a")
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 171, in _write
[rank0]:     with io.open(filename, mode, encoding=encoding) as f:
[rank0]: FileNotFoundError: [Errno 2] No such file or directory: b'./runs/plum-13b_kld_0.1_dice_2.0/events.out.tfevents.1740950207.blender15.cs.illinois.edu.293127.0'
Epoch: [9][ 82/500]	Time  6.140 ( 6.140)	Loss 1.9625 (1.5468)	CeLoss 0.2031 (0.3435)	SegCLSLoss 0.0177 (0.0121)	KLLoss 0.3555 (0.2533)	MaskLoss 0.8572 (0.5859)	MaskBCELoss 0.0435 (0.0471)	MaskDICELoss 0.8138 (0.5388)