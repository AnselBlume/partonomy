
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565



Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [02:20<00:00, 46.77s/it]
trainable params: 6,553,600 || all params: 14,027,161,511 || trainable%: 0.04672078520562206
>> model.config.train_mask_prompt_encoder:  True
n:  base_model.model.model.embed_tokens.weight p.shape:  torch.Size([32002, 5120])
n:  base_model.model.model.visual_model.prompt_encoder.point_embeddings.0.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.prompt_encoder.point_embeddings.1.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.prompt_encoder.point_embeddings.2.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.prompt_encoder.point_embeddings.3.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.prompt_encoder.not_a_point_embed.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.0.weight p.shape:  torch.Size([4, 1, 2, 2])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.0.bias p.shape:  torch.Size([4])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.1.weight p.shape:  torch.Size([4])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.1.bias p.shape:  torch.Size([4])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.3.weight p.shape:  torch.Size([16, 4, 2, 2])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.3.bias p.shape:  torch.Size([16])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.4.weight p.shape:  torch.Size([16])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.4.bias p.shape:  torch.Size([16])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.6.weight p.shape:  torch.Size([256, 16, 1, 1])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.6.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.prompt_encoder.no_mask_embed.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_token.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.mask_decoder.mask_tokens.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.weight p.shape:  torch.Size([256, 64, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.weight p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.weight p.shape:  torch.Size([64, 32, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.bias p.shape:  torch.Size([4])
n:  base_model.model.model.text_hidden_fcs.0.0.weight p.shape:  torch.Size([5120, 5120])
n:  base_model.model.model.text_hidden_fcs.0.0.bias p.shape:  torch.Size([5120])
n:  base_model.model.model.text_hidden_fcs.0.2.weight p.shape:  torch.Size([256, 5120])
n:  base_model.model.model.text_hidden_fcs.0.2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.token_to_mask_fcs.0.0.weight p.shape:  torch.Size([5120, 5120])
n:  base_model.model.model.token_to_mask_fcs.0.0.bias p.shape:  torch.Size([5120])
n:  base_model.model.model.token_to_mask_fcs.0.2.weight p.shape:  torch.Size([3, 5120])
n:  base_model.model.model.token_to_mask_fcs.0.2.bias p.shape:  torch.Size([3])
n:  base_model.model.lm_head.weight p.shape:  torch.Size([32002, 5120])
n:  base_model.model.mask_pooler.queries p.shape:  torch.Size([4096, 1, 256])
n:  base_model.model.mask_pooler.prompt_encoder.point_embeddings.0.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.mask_pooler.prompt_encoder.point_embeddings.1.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.mask_pooler.prompt_encoder.point_embeddings.2.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.mask_pooler.prompt_encoder.point_embeddings.3.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.mask_pooler.prompt_encoder.not_a_point_embed.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.mask_pooler.prompt_encoder.mask_downscaling.0.weight p.shape:  torch.Size([4, 1, 2, 2])
n:  base_model.model.mask_pooler.prompt_encoder.mask_downscaling.0.bias p.shape:  torch.Size([4])
n:  base_model.model.mask_pooler.prompt_encoder.mask_downscaling.1.weight p.shape:  torch.Size([4])
n:  base_model.model.mask_pooler.prompt_encoder.mask_downscaling.1.bias p.shape:  torch.Size([4])
n:  base_model.model.mask_pooler.prompt_encoder.mask_downscaling.3.weight p.shape:  torch.Size([16, 4, 2, 2])
n:  base_model.model.mask_pooler.prompt_encoder.mask_downscaling.3.bias p.shape:  torch.Size([16])
n:  base_model.model.mask_pooler.prompt_encoder.mask_downscaling.4.weight p.shape:  torch.Size([16])
n:  base_model.model.mask_pooler.prompt_encoder.mask_downscaling.4.bias p.shape:  torch.Size([16])
n:  base_model.model.mask_pooler.prompt_encoder.mask_downscaling.6.weight p.shape:  torch.Size([256, 16, 1, 1])
n:  base_model.model.mask_pooler.prompt_encoder.mask_downscaling.6.bias p.shape:  torch.Size([256])
n:  base_model.model.mask_pooler.prompt_encoder.no_mask_embed.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.mask_pooler.mask_modulator.projections.0.weight p.shape:  torch.Size([8, 256])
n:  base_model.model.mask_pooler.mask_modulator.projections.0.bias p.shape:  torch.Size([8])
n:  base_model.model.mask_pooler.mask_modulator.projections.1.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.mask_pooler.mask_modulator.projections.1.bias p.shape:  torch.Size([32])
n:  base_model.model.mask_pooler.mask_modulator.projections.2.weight p.shape:  torch.Size([512, 256])
n:  base_model.model.mask_pooler.mask_modulator.projections.2.bias p.shape:  torch.Size([512])
n:  base_model.model.mask_pooler.attn.in_proj_weight p.shape:  torch.Size([768, 256])
n:  base_model.model.mask_pooler.attn.in_proj_bias p.shape:  torch.Size([768])
n:  base_model.model.mask_pooler.attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.mask_pooler.attn.out_proj.bias p.shape:  torch.Size([256])
ade20k:  20210
cocostuff:  118287
loading annotations into memory...
Done (t=1.08s)
creating index...
index created!
pascal_part:  4366
loading annotations into memory...
Done (t=9.66s)
creating index...
index created!
paco_lvis:  45790
mapillary:  18000
loading dataset refclef into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refclef/refs(unc).p
creating index...
index created.
DONE (t=4.16s)
dataset refclef (refs unc) (train split) has 17978 images and 99523 annotations.
loading dataset refcoco into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco/refs(unc).p
creating index...
index created.
DONE (t=5.93s)
dataset refcoco (refs unc) (train split) has 16994 images and 196771 annotations.
loading dataset refcoco+ into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco+/refs(unc).p
creating index...
index created.
DONE (t=7.84s)
dataset refcoco+ (refs unc) (train split) has 16992 images and 196737 annotations.
loading dataset refcocog into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcocog/refs(umd).p
creating index...
index created.
DONE (t=8.58s)
dataset refcocog (refs umd) (train split) has 21899 images and 208960 annotations.
vqa_data:  157712
number of reason_seg samples:  239
len(self.img_to_explanation):  239
Training with 50000 examples.
Validating with 200 examples.
[2025-07-26 01:16:21,585] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5, git-hash=unknown, git-branch=unknown
[2025-07-26 01:16:21,585] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2025-07-26 01:16:21,585] [INFO] [comm.py:594:init_distributed] cdb=None
[2025-07-26 01:16:21,585] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Some weights of PLUMForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-llama-2-13b-chat-lightning-preview and are newly initialized: ['model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.6.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.0.mlp.lin1.weight', 'model.visual_model.image_encoder.blocks.30.norm2.weight', 'model.visual_model.image_encoder.blocks.23.attn.qkv.weight', 'model.visual_model.image_encoder.blocks.29.attn.proj.weight', 'model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.bias', 'model.visual_model.image_encoder.blocks.14.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.17.norm2.weight', 'model.visual_model.image_encoder.blocks.20.attn.proj.bias', 'model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.bias', 'model.visual_model.mask_decoder.iou_prediction_head.layers.1.bias', 'model.visual_model.image_encoder.blocks.28.mlp.lin1.weight', 'model.visual_model.image_encoder.blocks.29.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.1.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.5.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.26.attn.proj.weight', 'model.visual_model.image_encoder.blocks.15.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.22.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.11.mlp.lin1.weight', 'model.visual_model.image_encoder.blocks.9.mlp.lin1.bias', 'model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.bias', 'model.visual_model.image_encoder.blocks.25.norm2.bias', 'model.visual_model.image_encoder.blocks.31.attn.proj.weight', 'model.visual_model.image_encoder.blocks.10.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.14.attn.proj.bias', 'model.token_to_mask_fcs.0.0.weight', 'mask_pooler.mask_modulator.projections.1.weight', 'model.visual_model.image_encoder.blocks.25.attn.qkv.weight', 'model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias', 'model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.weight', 'model.token_to_mask_fcs.0.0.bias', 'model.visual_model.image_encoder.blocks.16.norm2.weight', 'model.visual_model.image_encoder.blocks.21.attn.rel_pos_h', 'model.visual_model.image_encoder.blocks.12.norm1.weight', 'model.visual_model.image_encoder.blocks.27.attn.proj.bias', 'model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.bias', 'model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight', 'model.visual_model.image_encoder.blocks.7.norm2.bias', 'model.visual_model.image_encoder.blocks.4.attn.proj.weight', 'model.visual_model.image_encoder.blocks.13.attn.qkv.weight', 'model.visual_model.image_encoder.blocks.29.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.31.attn.rel_pos_h', 'model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.bias', 'model.visual_model.image_encoder.blocks.23.attn.qkv.bias', 'model.visual_model.mask_decoder.transformer.norm_final_attn.bias', 'model.visual_model.image_encoder.blocks.14.attn.rel_pos_h', 'model.visual_model.image_encoder.blocks.18.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.15.mlp.lin1.bias', 'model.visual_model.image_encoder.blocks.24.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.26.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.25.mlp.lin1.bias', 'model.visual_model.image_encoder.blocks.23.norm2.weight', 'model.visual_model.image_encoder.blocks.16.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.28.mlp.lin1.bias', 'mask_pooler.prompt_encoder.mask_downscaling.6.bias', 'model.visual_model.image_encoder.blocks.21.norm1.bias', 'model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.29.norm1.weight', 'model.visual_model.image_encoder.blocks.30.mlp.lin1.bias', 'model.visual_model.image_encoder.blocks.10.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.26.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.17.mlp.lin1.weight', 'model.visual_model.image_encoder.blocks.15.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.3.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.20.attn.rel_pos_h', 'model.visual_model.image_encoder.blocks.31.mlp.lin1.bias', 'model.visual_model.image_encoder.blocks.10.attn.rel_pos_h', 'model.visual_model.image_encoder.blocks.8.attn.rel_pos_h', 'model.visual_model.image_encoder.blocks.14.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.21.mlp.lin2.weight', 'model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias', 'model.visual_model.image_encoder.blocks.18.attn.proj.bias', 'model.visual_model.image_encoder.blocks.29.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.23.attn.rel_pos_w', 'model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.bias', 'model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias', 'model.visual_model.image_encoder.blocks.18.mlp.lin1.bias', 'model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias', 'model.visual_model.image_encoder.blocks.16.norm1.bias', 'model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.weight', 'model.visual_model.image_encoder.blocks.19.attn.proj.weight', 'model.visual_model.image_encoder.blocks.4.attn.qkv.weight', 'model.visual_model.prompt_encoder.no_mask_embed.weight', 'model.visual_model.image_encoder.blocks.21.attn.proj.bias', 'model.visual_model.image_encoder.blocks.14.mlp.lin1.bias', 'model.visual_model.image_encoder.blocks.12.attn.proj.bias', 'model.visual_model.image_encoder.blocks.13.mlp.lin1.weight', 'model.visual_model.image_encoder.blocks.22.mlp.lin1.bias', 'model.visual_model.mask_decoder.iou_prediction_head.layers.0.bias', 'model.visual_model.image_encoder.blocks.19.mlp.lin1.bias', 'model.visual_model.image_encoder.blocks.27.norm2.bias', 'model.visual_model.image_encoder.blocks.28.norm1.bias', 'model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight', 'model.visual_model.image_encoder.blocks.15.attn.proj.bias', 'model.visual_model.image_encoder.blocks.16.norm1.weight', 'model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight', 'model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.bias', 'mask_pooler.prompt_encoder.mask_downscaling.3.weight', 'model.visual_model.image_encoder.blocks.27.mlp.lin1.weight', 'model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.bias', 'model.visual_model.mask_decoder.output_upscaling.1.weight', 'model.visual_model.image_encoder.blocks.6.mlp.lin1.bias', 'model.visual_model.prompt_encoder.mask_downscaling.0.weight', 'model.visual_model.image_encoder.blocks.3.attn.proj.weight', 'model.visual_model.image_encoder.blocks.6.norm1.weight', 'model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.weight', 'model.visual_model.image_encoder.blocks.1.norm1.bias', 'model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.bias', 'model.visual_model.image_encoder.blocks.4.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.2.attn.qkv.weight', 'model.text_hidden_fcs.0.2.weight', 'model.visual_model.image_encoder.blocks.18.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.4.norm1.bias', 'model.visual_model.image_encoder.blocks.5.attn.proj.bias', 'model.visual_model.image_encoder.blocks.9.attn.qkv.weight', 'model.visual_model.image_encoder.blocks.22.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.2.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.7.attn.proj.weight', 'model.visual_model.image_encoder.blocks.22.norm1.weight', 'model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.bias', 'model.visual_model.image_encoder.blocks.2.attn.rel_pos_h', 'model.visual_model.image_encoder.blocks.25.mlp.lin1.weight', 'model.visual_model.image_encoder.blocks.12.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.24.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.17.attn.proj.weight', 'model.visual_model.image_encoder.blocks.2.norm1.weight', 'model.visual_model.image_encoder.blocks.8.attn.qkv.weight', 'model.visual_model.image_encoder.blocks.1.attn.qkv.weight', 'model.visual_model.prompt_encoder.mask_downscaling.4.bias', 'model.visual_model.image_encoder.blocks.0.attn.qkv.weight', 'model.visual_model.image_encoder.blocks.12.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.22.norm2.bias', 'model.visual_model.image_encoder.blocks.27.mlp.lin1.bias', 'model.visual_model.image_encoder.blocks.12.norm2.weight', 'model.visual_model.image_encoder.blocks.30.attn.proj.bias', 'model.visual_model.mask_decoder.iou_prediction_head.layers.2.bias', 'model.visual_model.image_encoder.blocks.17.norm1.bias', 'model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.weight', 'model.visual_model.image_encoder.blocks.21.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.31.attn.proj.bias', 'model.visual_model.image_encoder.blocks.20.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.1.mlp.lin2.weight', 'model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.weight', 'model.visual_model.image_encoder.blocks.10.norm2.weight', 'model.visual_model.image_encoder.blocks.7.norm1.bias', 'model.visual_model.image_encoder.blocks.12.attn.rel_pos_w', 'model.visual_model.mask_decoder.transformer.layers.0.norm4.bias', 'model.visual_model.image_encoder.blocks.13.mlp.lin1.bias', 'model.visual_model.image_encoder.blocks.16.norm2.bias', 'model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight', 'model.visual_model.mask_decoder.mask_tokens.weight', 'model.visual_model.image_encoder.blocks.13.attn.proj.weight', 'model.visual_model.image_encoder.blocks.15.attn.qkv.weight', 'model.visual_model.image_encoder.blocks.13.attn.proj.bias', 'model.visual_model.mask_decoder.output_upscaling.3.weight', 'model.visual_model.image_encoder.blocks.30.norm2.bias', 'model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.bias', 'model.visual_model.image_encoder.patch_embed.proj.bias', 'model.visual_model.image_encoder.blocks.9.mlp.lin1.weight', 'model.visual_model.image_encoder.blocks.16.mlp.lin1.bias', 'model.visual_model.image_encoder.blocks.29.attn.proj.bias', 'mask_pooler.mask_modulator.projections.2.weight', 'model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.bias', 'model.visual_model.image_encoder.blocks.4.mlp.lin1.weight', 'model.visual_model.image_encoder.blocks.28.norm2.bias', 'model.visual_model.image_encoder.blocks.9.mlp.lin2.weight', 'model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight', 'model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight', 'model.visual_model.image_encoder.blocks.31.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.16.attn.qkv.bias', 'model.visual_model.prompt_encoder.mask_downscaling.6.bias', 'model.visual_model.image_encoder.blocks.8.mlp.lin1.weight', 'mask_pooler.prompt_encoder.mask_downscaling.0.weight', 'model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias', 'model.visual_model.image_encoder.blocks.29.mlp.lin1.bias', 'model.visual_model.image_encoder.blocks.4.attn.rel_pos_h', 'model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.bias', 'model.visual_model.image_encoder.blocks.19.attn.rel_pos_h', 'model.visual_model.image_encoder.blocks.21.norm1.weight', 'model.visual_model.image_encoder.blocks.2.mlp.lin1.bias', 'model.visual_model.image_encoder.blocks.5.norm1.bias', 'model.visual_model.image_encoder.blocks.16.attn.proj.bias', 'model.visual_model.image_encoder.blocks.12.mlp.lin1.weight', 'model.visual_model.image_encoder.blocks.17.attn.qkv.bias', 'model.visual_model.mask_decoder.output_upscaling.0.bias', 'model.visual_model.image_encoder.blocks.10.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.3.norm1.weight', 'mask_pooler.attn.out_proj.weight', 'model.visual_model.image_encoder.blocks.2.attn.proj.weight', 'model.visual_model.image_encoder.blocks.9.attn.rel_pos_h', 'model.visual_model.image_encoder.blocks.5.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.25.attn.rel_pos_h', 'model.visual_model.image_encoder.blocks.3.attn.rel_pos_h', 'mask_pooler.mask_modulator.projections.0.weight', 'model.visual_model.image_encoder.blocks.4.norm2.bias', 'model.visual_model.image_encoder.blocks.25.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.27.attn.qkv.weight', 'model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.bias', 'model.visual_model.image_encoder.blocks.12.attn.proj.weight', 'model.visual_model.image_encoder.blocks.0.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.2.mlp.lin1.weight', 'model.visual_model.image_encoder.blocks.28.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.27.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.19.norm1.bias', 'model.visual_model.image_encoder.blocks.19.attn.qkv.weight', 'model.visual_model.image_encoder.blocks.23.mlp.lin1.weight', 'model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight', 'model.visual_model.mask_decoder.transformer.layers.0.norm4.weight', 'model.visual_model.image_encoder.blocks.9.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.22.norm2.weight', 'model.visual_model.image_encoder.blocks.26.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.18.norm2.weight', 'model.visual_model.image_encoder.blocks.30.norm1.weight', 'model.visual_model.mask_decoder.transformer.layers.0.norm1.weight', 'model.visual_model.image_encoder.blocks.4.mlp.lin1.bias', 'model.visual_model.image_encoder.blocks.11.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.11.attn.rel_pos_h', 'model.visual_model.image_encoder.blocks.6.attn.rel_pos_h', 'model.visual_model.image_encoder.blocks.0.mlp.lin1.bias', 'model.visual_model.image_encoder.blocks.2.norm2.bias', 'model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight', 'model.visual_model.image_encoder.blocks.31.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.10.norm1.weight', 'model.visual_model.image_encoder.blocks.29.attn.rel_pos_h', 'model.visual_model.mask_decoder.transformer.layers.1.norm3.weight', 'mask_pooler.attn.out_proj.bias', 'model.visual_model.image_encoder.blocks.18.attn.qkv.weight', 'model.visual_model.image_encoder.blocks.0.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.15.norm1.weight', 'model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight', 'model.visual_model.image_encoder.blocks.17.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.21.mlp.lin1.weight', 'model.visual_model.image_encoder.blocks.29.mlp.lin1.weight', 'model.visual_model.image_encoder.blocks.6.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.21.norm2.weight', 'model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.weight', 'model.visual_model.image_encoder.blocks.13.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.25.attn.proj.bias', 'model.visual_model.image_encoder.blocks.20.norm1.bias', 'model.visual_model.image_encoder.blocks.15.norm2.bias', 'model.visual_model.image_encoder.blocks.8.mlp.lin2.bias', 'model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.weight', 'model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.17.norm1.weight', 'model.visual_model.image_encoder.blocks.12.norm2.bias', 'mask_pooler.prompt_encoder.point_embeddings.0.weight', 'model.visual_model.image_encoder.blocks.24.attn.proj.weight', 'model.visual_model.image_encoder.neck.3.weight', 'model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.bias', 'model.visual_model.image_encoder.blocks.11.norm2.weight', 'model.visual_model.image_encoder.blocks.12.mlp.lin1.bias', 'model.visual_model.image_encoder.blocks.20.mlp.lin2.weight', 'model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight', 'model.visual_model.image_encoder.blocks.31.norm2.bias', 'model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias', 'model.visual_model.image_encoder.blocks.7.attn.rel_pos_h', 'model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.bias', 'model.visual_model.image_encoder.pos_embed', 'model.visual_model.prompt_encoder.point_embeddings.1.weight', 'model.visual_model.image_encoder.blocks.6.attn.proj.bias', 'model.visual_model.image_encoder.blocks.19.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.18.norm1.bias', 'mask_pooler.block2.0.3.weight', 'model.visual_model.image_encoder.blocks.24.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.23.norm1.weight', 'model.visual_model.image_encoder.blocks.14.norm1.bias', 'model.visual_model.image_encoder.blocks.9.attn.proj.weight', 'model.visual_model.image_encoder.blocks.31.norm2.weight', 'model.visual_model.image_encoder.blocks.19.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.25.norm1.weight', 'model.visual_model.image_encoder.blocks.9.norm2.bias', 'model.visual_model.image_encoder.blocks.1.attn.rel_pos_w', 'model.visual_model.prompt_encoder.mask_downscaling.3.weight', 'model.visual_model.image_encoder.blocks.1.norm1.weight', 'model.visual_model.image_encoder.blocks.7.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.19.norm2.bias', 'model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.weight', 'model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias', 'mask_pooler.prompt_encoder.not_a_point_embed.weight', 'model.visual_model.image_encoder.blocks.28.norm2.weight', 'mask_pooler.prompt_encoder.point_embeddings.1.weight', 'model.visual_model.image_encoder.blocks.6.norm1.bias', 'model.visual_model.mask_decoder.transformer.layers.1.norm1.bias', 'model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias', 'model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.weight', 'model.visual_model.image_encoder.blocks.26.attn.qkv.weight', 'model.visual_model.image_encoder.blocks.28.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.17.norm2.bias', 'model.visual_model.image_encoder.blocks.21.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.11.norm1.bias', 'model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.weight', 'model.visual_model.image_encoder.blocks.10.norm2.bias', 'model.visual_model.image_encoder.blocks.8.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.20.norm2.bias', 'model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.bias', 'model.visual_model.image_encoder.blocks.31.norm1.weight', 'model.visual_model.image_encoder.blocks.13.norm2.weight', 'model.visual_model.image_encoder.blocks.7.mlp.lin1.weight', 'model.visual_model.image_encoder.neck.3.bias', 'model.visual_model.prompt_encoder.mask_downscaling.3.bias', 'model.visual_model.image_encoder.blocks.30.norm1.bias', 'model.visual_model.image_encoder.blocks.21.attn.proj.weight', 'model.visual_model.image_encoder.blocks.6.norm2.bias', 'model.visual_model.mask_decoder.output_upscaling.0.weight', 'model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias', 'mask_pooler.mask_modulator.projections.1.bias', 'mask_pooler.block2.0.4.weight', 'model.visual_model.image_encoder.blocks.15.norm1.bias', 'model.visual_model.prompt_encoder.point_embeddings.0.weight', 'model.visual_model.image_encoder.blocks.13.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.6.attn.proj.weight', 'model.visual_model.prompt_encoder.mask_downscaling.6.weight', 'model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.weight', 'model.visual_model.image_encoder.blocks.24.mlp.lin2.weight', 'mask_pooler.prompt_encoder.mask_downscaling.4.weight', 'model.visual_model.image_encoder.blocks.20.attn.proj.weight', 'model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.bias', 'model.visual_model.image_encoder.blocks.7.norm2.weight', 'model.visual_model.image_encoder.blocks.22.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.30.attn.qkv.weight', 'model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias', 'model.visual_model.image_encoder.blocks.14.attn.proj.weight', 'model.visual_model.image_encoder.blocks.26.norm2.weight', 'model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.bias', 'model.visual_model.image_encoder.blocks.8.mlp.lin1.bias', 'mask_pooler.queries', 'model.visual_model.image_encoder.blocks.27.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.27.norm1.bias', 'model.visual_model.image_encoder.blocks.7.norm1.weight', 'mask_pooler.final_conv.bias', 'model.visual_model.image_encoder.blocks.10.mlp.lin1.bias', 'model.visual_model.image_encoder.blocks.24.attn.proj.bias', 'model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight', 'model.visual_model.image_encoder.blocks.7.mlp.lin1.bias', 'mask_pooler.block1.0.1.weight', 'model.visual_model.image_encoder.blocks.24.norm2.weight', 'model.visual_model.image_encoder.blocks.4.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.18.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.3.attn.qkv.weight', 'model.visual_model.image_encoder.blocks.30.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.23.attn.proj.bias', 'model.visual_model.image_encoder.blocks.19.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.6.mlp.lin2.bias', 'model.visual_model.mask_decoder.iou_token.weight', 'model.visual_model.prompt_encoder.mask_downscaling.0.bias', 'model.visual_model.image_encoder.blocks.12.attn.rel_pos_h', 'model.visual_model.image_encoder.blocks.14.attn.qkv.bias', 'model.visual_model.mask_decoder.transformer.layers.1.norm2.bias', 'model.visual_model.image_encoder.blocks.25.norm2.weight', 'model.visual_model.image_encoder.blocks.8.attn.proj.weight', 'model.visual_model.image_encoder.blocks.21.mlp.lin1.bias', 'model.visual_model.mask_decoder.transformer.layers.0.norm2.bias', 'model.visual_model.image_encoder.blocks.18.mlp.lin1.weight', 'model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.weight', 'model.visual_model.image_encoder.blocks.19.mlp.lin1.weight', 'model.visual_model.image_encoder.blocks.26.mlp.lin1.bias', 'model.visual_model.image_encoder.blocks.6.norm2.weight', 'model.visual_model.image_encoder.blocks.7.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.8.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.16.mlp.lin1.weight', 'mask_pooler.prompt_encoder.point_embeddings.2.weight', 'model.visual_model.image_encoder.blocks.11.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.14.attn.qkv.weight', 'model.visual_model.image_encoder.blocks.11.attn.qkv.weight', 'model.visual_model.image_encoder.blocks.3.norm1.bias', 'model.visual_model.image_encoder.blocks.26.norm2.bias', 'model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight', 'model.visual_model.image_encoder.blocks.5.mlp.lin2.weight', 'model.visual_model.image_encoder.patch_embed.proj.weight', 'model.visual_model.image_encoder.blocks.12.attn.qkv.bias', 'model.visual_model.prompt_encoder.not_a_point_embed.weight', 'model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.bias', 'model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.weight', 'model.visual_model.image_encoder.blocks.17.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.17.mlp.lin1.bias', 'model.visual_model.image_encoder.blocks.3.norm2.bias', 'model.visual_model.image_encoder.blocks.0.norm2.bias', 'model.visual_model.image_encoder.blocks.29.norm1.bias', 'model.visual_model.image_encoder.blocks.9.norm1.weight', 'model.visual_model.image_encoder.blocks.19.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.20.mlp.lin1.bias', 'model.visual_model.image_encoder.blocks.0.attn.proj.bias', 'model.visual_model.image_encoder.blocks.22.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.5.attn.proj.weight', 'model.visual_model.mask_decoder.iou_prediction_head.layers.0.weight', 'model.visual_model.image_encoder.blocks.24.norm1.weight', 'model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.bias', 'model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias', 'model.visual_model.image_encoder.blocks.13.norm1.bias', 'model.visual_model.image_encoder.blocks.7.attn.qkv.weight', 'model.visual_model.image_encoder.blocks.8.norm2.bias', 'model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.bias', 'model.visual_model.image_encoder.blocks.25.norm1.bias', 'model.visual_model.prompt_encoder.mask_downscaling.4.weight', 'model.visual_model.image_encoder.blocks.28.norm1.weight', 'model.visual_model.image_encoder.blocks.18.norm1.weight', 'mask_pooler.prompt_encoder.mask_downscaling.6.weight', 'model.visual_model.image_encoder.blocks.18.norm2.bias', 'model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.bias', 'model.visual_model.image_encoder.neck.0.weight', 'model.visual_model.prompt_encoder.point_embeddings.2.weight', 'model.visual_model.mask_decoder.transformer.layers.1.norm3.bias', 'model.visual_model.image_encoder.blocks.13.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.11.mlp.lin1.bias', 'model.visual_model.image_encoder.blocks.5.mlp.lin1.weight', 'model.visual_model.image_encoder.blocks.24.norm1.bias', 'model.visual_model.image_encoder.blocks.5.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.3.norm2.weight', 'model.visual_model.image_encoder.blocks.27.norm1.weight', 'model.visual_model.image_encoder.blocks.13.norm1.weight', 'model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.bias', 'model.text_hidden_fcs.0.0.weight', 'model.visual_model.image_encoder.blocks.24.norm2.bias', 'model.visual_model.image_encoder.blocks.26.norm1.weight', 'model.visual_model.image_encoder.blocks.2.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.3.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.30.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.16.attn.qkv.weight', 'model.visual_model.image_encoder.blocks.23.attn.rel_pos_h', 'model.visual_model.image_encoder.blocks.26.norm1.bias', 'model.visual_model.mask_decoder.transformer.layers.0.norm3.bias', 'model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.bias', 'model.visual_model.mask_decoder.transformer.layers.1.norm1.weight', 'model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight', 'model.visual_model.image_encoder.blocks.20.attn.rel_pos_w', 'model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.weight', 'model.visual_model.image_encoder.blocks.18.attn.proj.weight', 'model.visual_model.prompt_encoder.mask_downscaling.1.bias', 'model.visual_model.image_encoder.blocks.17.attn.rel_pos_h', 'model.visual_model.image_encoder.blocks.29.norm2.weight', 'mask_pooler.block1.0.1.bias', 'model.visual_model.image_encoder.blocks.29.attn.qkv.weight', 'model.visual_model.image_encoder.neck.2.weight', 'model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.weight', 'mask_pooler.prompt_encoder.mask_downscaling.4.bias', 'model.visual_model.image_encoder.blocks.24.mlp.lin1.bias', 'model.visual_model.image_encoder.blocks.22.attn.proj.bias', 'model.visual_model.image_encoder.blocks.0.norm2.weight', 'model.visual_model.mask_decoder.transformer.layers.0.norm2.weight', 'model.visual_model.image_encoder.blocks.28.attn.rel_pos_h', 'model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.weight', 'model.visual_model.image_encoder.blocks.11.attn.proj.bias', 'model.visual_model.image_encoder.blocks.10.norm1.bias', 'model.visual_model.image_encoder.blocks.24.attn.qkv.weight', 'model.visual_model.image_encoder.blocks.28.attn.proj.bias', 'model.visual_model.image_encoder.blocks.3.mlp.lin1.bias', 'model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.weight', 'model.visual_model.image_encoder.blocks.8.norm1.bias', 'model.visual_model.image_encoder.blocks.14.norm2.weight', 'model.visual_model.mask_decoder.iou_prediction_head.layers.1.weight', 'model.visual_model.image_encoder.blocks.28.attn.qkv.weight', 'model.visual_model.image_encoder.blocks.17.attn.rel_pos_w', 'model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.weight', 'mask_pooler.prompt_encoder.mask_downscaling.0.bias', 'model.visual_model.image_encoder.blocks.26.mlp.lin1.weight', 'model.visual_model.image_encoder.blocks.0.attn.rel_pos_h', 'model.visual_model.image_encoder.blocks.19.norm1.weight', 'model.visual_model.image_encoder.blocks.11.attn.proj.weight', 'model.visual_model.image_encoder.blocks.29.norm2.bias', 'model.visual_model.image_encoder.blocks.3.attn.proj.bias', 'mask_pooler.prompt_encoder.point_embeddings.3.weight', 'mask_pooler.prompt_encoder.no_mask_embed.weight', 'model.visual_model.image_encoder.blocks.28.mlp.lin2.weight', 'mask_pooler.attn.in_proj_weight', 'model.visual_model.image_encoder.blocks.10.mlp.lin1.weight', 'model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.weight', 'model.visual_model.image_encoder.blocks.25.mlp.lin2.bias', 'model.visual_model.mask_decoder.iou_prediction_head.layers.2.weight', 'model.visual_model.image_encoder.blocks.4.attn.proj.bias', 'model.visual_model.image_encoder.blocks.22.mlp.lin1.weight', 'model.visual_model.image_encoder.blocks.26.attn.proj.bias', 'mask_pooler.block1.0.0.bias', 'model.visual_model.image_encoder.blocks.7.attn.proj.bias', 'mask_pooler.mask_modulator.projections.2.bias', 'model.visual_model.mask_decoder.transformer.norm_final_attn.weight', 'model.visual_model.image_encoder.blocks.31.attn.qkv.bias', 'model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight', 'model.visual_model.image_encoder.blocks.11.norm1.weight', 'model.visual_model.image_encoder.blocks.14.norm2.bias', 'model.visual_model.image_encoder.blocks.16.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.23.mlp.lin1.bias', 'mask_pooler.attn.in_proj_bias', 'model.visual_model.image_encoder.blocks.0.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.15.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.1.attn.proj.bias', 'model.visual_model.image_encoder.blocks.9.mlp.lin2.bias', 'model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias', 'model.visual_model.image_encoder.blocks.15.norm2.weight', 'model.visual_model.image_encoder.blocks.25.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.4.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.30.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.17.attn.proj.bias', 'model.visual_model.mask_decoder.transformer.layers.0.norm3.weight', 'model.visual_model.image_encoder.blocks.7.mlp.lin2.bias', 'model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight', 'mask_pooler.mask_modulator.projections.0.bias', 'model.visual_model.image_encoder.blocks.4.norm1.weight', 'model.visual_model.image_encoder.blocks.30.attn.rel_pos_w', 'model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias', 'model.visual_model.image_encoder.blocks.14.mlp.lin1.weight', 'model.visual_model.image_encoder.blocks.4.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.22.attn.rel_pos_h', 'model.visual_model.image_encoder.blocks.23.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.12.attn.qkv.weight', 'model.visual_model.image_encoder.blocks.27.attn.proj.weight', 'model.visual_model.image_encoder.blocks.23.norm2.bias', 'model.visual_model.mask_decoder.output_upscaling.1.bias', 'model.visual_model.image_encoder.blocks.11.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.13.attn.rel_pos_h', 'model.visual_model.image_encoder.blocks.17.attn.qkv.weight', 'model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.weight', 'model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias', 'model.visual_model.image_encoder.blocks.8.norm2.weight', 'model.visual_model.image_encoder.blocks.20.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.22.attn.qkv.weight', 'model.visual_model.mask_decoder.transformer.layers.0.norm1.bias', 'model.visual_model.image_encoder.neck.1.weight', 'model.visual_model.image_encoder.blocks.2.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.1.mlp.lin1.weight', 'model.visual_model.image_encoder.blocks.15.attn.proj.weight', 'model.visual_model.image_encoder.blocks.13.norm2.bias', 'model.visual_model.image_encoder.blocks.23.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.4.norm2.weight', 'model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.weight', 'model.visual_model.image_encoder.blocks.5.norm2.bias', 'model.visual_model.image_encoder.blocks.5.attn.rel_pos_h', 'model.visual_model.image_encoder.blocks.1.attn.proj.weight', 'model.visual_model.image_encoder.blocks.25.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.7.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.30.attn.rel_pos_h', 'model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.weight', 'model.visual_model.image_encoder.blocks.2.attn.proj.bias', 'model.visual_model.image_encoder.blocks.10.attn.qkv.weight', 'model.visual_model.image_encoder.blocks.25.attn.proj.weight', 'model.visual_model.image_encoder.blocks.27.attn.qkv.bias', 'model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias', 'model.visual_model.image_encoder.blocks.26.attn.rel_pos_h', 'model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.weight', 'mask_pooler.block1.0.0.weight', 'mask_pooler.prompt_encoder.mask_downscaling.3.bias', 'model.visual_model.image_encoder.blocks.9.norm2.weight', 'model.visual_model.image_encoder.blocks.11.norm2.bias', 'model.text_hidden_fcs.0.0.bias', 'model.visual_model.image_encoder.blocks.1.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.2.norm1.bias', 'model.visual_model.image_encoder.blocks.31.mlp.lin1.weight', 'model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight', 'model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.weight', 'model.visual_model.image_encoder.blocks.3.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.14.norm1.weight', 'model.visual_model.image_encoder.blocks.20.norm2.weight', 'model.visual_model.image_encoder.blocks.27.norm2.weight', 'model.visual_model.image_encoder.blocks.20.norm1.weight', 'model.visual_model.image_encoder.blocks.11.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.21.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.23.norm1.bias', 'model.visual_model.image_encoder.blocks.16.attn.proj.weight', 'model.visual_model.image_encoder.blocks.23.attn.proj.weight', 'model.visual_model.image_encoder.blocks.8.attn.proj.bias', 'model.visual_model.image_encoder.blocks.6.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.20.attn.qkv.weight', 'model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.weight', 'model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.bias', 'model.token_to_mask_fcs.0.2.bias', 'model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias', 'model.visual_model.image_encoder.blocks.21.norm2.bias', 'model.visual_model.image_encoder.blocks.10.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.24.attn.rel_pos_h', 'model.visual_model.image_encoder.blocks.14.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.15.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.27.attn.rel_pos_h', 'model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.weight', 'model.visual_model.image_encoder.blocks.18.attn.rel_pos_h', 'model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.bias', 'model.visual_model.image_encoder.blocks.5.mlp.lin1.bias', 'model.visual_model.image_encoder.blocks.9.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.21.attn.qkv.weight', 'model.visual_model.image_encoder.blocks.27.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.5.norm2.weight', 'model.token_to_mask_fcs.0.2.weight', 'model.visual_model.image_encoder.blocks.6.attn.qkv.weight', 'mask_pooler.prompt_encoder.mask_downscaling.1.bias', 'model.visual_model.image_encoder.blocks.0.attn.proj.weight', 'model.visual_model.mask_decoder.output_upscaling.3.bias', 'model.visual_model.image_encoder.blocks.15.attn.rel_pos_h', 'model.visual_model.image_encoder.blocks.3.mlp.lin1.weight', 'model.visual_model.image_encoder.blocks.9.norm1.bias', 'model.visual_model.image_encoder.neck.1.bias', 'model.visual_model.mask_decoder.transformer.layers.1.norm4.bias', 'model.visual_model.image_encoder.blocks.10.attn.proj.weight', 'model.visual_model.image_encoder.blocks.0.norm1.weight', 'model.visual_model.mask_decoder.transformer.layers.1.norm2.weight', 'mask_pooler.final_conv.weight', 'model.visual_model.image_encoder.blocks.16.attn.rel_pos_h', 'model.visual_model.image_encoder.blocks.18.mlp.lin2.bias', 'mask_pooler.prompt_encoder.mask_downscaling.1.weight', 'model.visual_model.image_encoder.blocks.30.mlp.lin1.weight', 'model.visual_model.image_encoder.blocks.31.norm1.bias', 'model.visual_model.image_encoder.blocks.31.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.1.attn.rel_pos_h', 'model.visual_model.image_encoder.blocks.8.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.24.mlp.lin1.weight', 'model.visual_model.prompt_encoder.pe_layer.positional_encoding_gaussian_matrix', 'model.text_hidden_fcs.0.2.bias', 'model.visual_model.prompt_encoder.point_embeddings.3.weight', 'model.visual_model.image_encoder.blocks.12.norm1.bias', 'model.visual_model.image_encoder.blocks.20.mlp.lin1.weight', 'model.visual_model.image_encoder.blocks.28.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.22.norm1.bias', 'model.visual_model.image_encoder.blocks.6.mlp.lin1.weight', 'model.visual_model.image_encoder.blocks.13.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.22.attn.proj.weight', 'model.visual_model.image_encoder.blocks.0.norm1.bias', 'model.visual_model.image_encoder.blocks.1.mlp.lin1.bias', 'model.visual_model.image_encoder.blocks.31.attn.qkv.weight', 'model.visual_model.image_encoder.blocks.1.norm2.weight', 'model.visual_model.image_encoder.blocks.16.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.26.mlp.lin2.weight', 'model.visual_model.prompt_encoder.mask_downscaling.1.weight', 'model.visual_model.image_encoder.blocks.28.attn.proj.weight', 'model.visual_model.image_encoder.blocks.1.norm2.bias', 'model.visual_model.image_encoder.blocks.0.attn.rel_pos_w', 'model.visual_model.image_encoder.blocks.19.attn.proj.bias', 'model.visual_model.image_encoder.blocks.15.mlp.lin1.weight', 'model.visual_model.image_encoder.blocks.5.attn.qkv.weight', 'mask_pooler.prompt_encoder.pe_layer.positional_encoding_gaussian_matrix', 'model.visual_model.image_encoder.blocks.29.mlp.lin2.bias', 'model.visual_model.image_encoder.blocks.2.norm2.weight', 'model.visual_model.image_encoder.blocks.5.norm1.weight', 'model.visual_model.mask_decoder.transformer.layers.1.norm4.weight', 'model.visual_model.image_encoder.blocks.10.attn.proj.bias', 'model.visual_model.image_encoder.blocks.3.mlp.lin2.weight', 'model.visual_model.image_encoder.blocks.9.attn.proj.bias', 'mask_pooler.block2.0.3.bias', 'mask_pooler.block2.0.4.bias', 'model.visual_model.image_encoder.blocks.19.norm2.weight', 'model.visual_model.image_encoder.blocks.30.attn.proj.weight', 'model.visual_model.image_encoder.blocks.2.attn.qkv.bias', 'model.visual_model.image_encoder.blocks.8.norm1.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[2025-07-26 01:16:32,577] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Loading extension module fused_adam...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/tensor/python_tensor.cpp:78.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
Time to load fused_adam op: 34.563669204711914 seconds
[2025-07-26 01:17:07,851] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-07-26 01:17:08,030] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-07-26 01:17:08,030] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-07-26 01:17:08,031] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-07-26 01:17:08,031] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500000000
[2025-07-26 01:17:08,031] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500000000
[2025-07-26 01:17:08,031] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2025-07-26 01:17:08,031] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(393537628, False)]
[2025-07-26 01:17:12,041] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2025-07-26 01:17:12,042] [INFO] [utils.py:786:see_memory_usage] MA 27.79 GB         Max_MA 28.52 GB         CA 28.67 GB         Max_CA 29 GB
[2025-07-26 01:17:12,042] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.5 GB, percent = 5.0%
[2025-07-26 01:17:15,116] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2025-07-26 01:17:15,116] [INFO] [utils.py:786:see_memory_usage] MA 30.73 GB         Max_MA 32.19 GB         CA 33.07 GB         Max_CA 33 GB
[2025-07-26 01:17:15,117] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.46 GB, percent = 5.0%
[2025-07-26 01:17:15,117] [INFO] [stage_1_and_2.py:488:__init__] optimizer state initialized
  0%|                                                                                                                                                               | 0/200 [00:00<?, ?it/s]
[2025-07-26 01:17:18,135] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2025-07-26 01:17:18,136] [INFO] [utils.py:786:see_memory_usage] MA 30.73 GB         Max_MA 30.73 GB         CA 33.07 GB         Max_CA 33 GB
[2025-07-26 01:17:18,136] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.53 GB, percent = 5.0%
[2025-07-26 01:17:18,142] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-07-26 01:17:18,142] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2025-07-26 01:17:18,142] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7f1bdf7e1bd0>
[2025-07-26 01:17:18,142] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
[2025-07-26 01:17:18,146] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2025-07-26 01:17:18,146] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-07-26 01:17:18,146] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-07-26 01:17:18,146] [INFO] [config.py:964:print]   amp_enabled .................. False
[2025-07-26 01:17:18,146] [INFO] [config.py:964:print]   amp_params ................... False
[2025-07-26 01:17:18,146] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-07-26 01:17:18,147] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2025-07-26 01:17:18,147] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2025-07-26 01:17:18,147] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2025-07-26 01:17:18,147] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2025-07-26 01:17:18,147] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1bdf7e25c0>
[2025-07-26 01:17:18,147] [INFO] [config.py:964:print]   communication_data_type ...... None
[2025-07-26 01:17:18,147] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-07-26 01:17:18,147] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2025-07-26 01:17:18,147] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2025-07-26 01:17:18,147] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-07-26 01:17:18,147] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2025-07-26 01:17:18,147] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2025-07-26 01:17:18,147] [INFO] [config.py:964:print]   disable_allgather ............ False
[2025-07-26 01:17:18,147] [INFO] [config.py:964:print]   dump_state ................... False
[2025-07-26 01:17:18,147] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2025-07-26 01:17:18,147] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2025-07-26 01:17:18,147] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2025-07-26 01:17:18,147] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-07-26 01:17:18,147] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2025-07-26 01:17:18,148] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2025-07-26 01:17:18,148] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2025-07-26 01:17:18,148] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2025-07-26 01:17:18,148] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2025-07-26 01:17:18,148] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2025-07-26 01:17:18,148] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-07-26 01:17:18,148] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2025-07-26 01:17:18,148] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2025-07-26 01:17:18,148] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2025-07-26 01:17:18,148] [INFO] [config.py:964:print]   global_rank .................. 0
[2025-07-26 01:17:18,148] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2025-07-26 01:17:18,148] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 10
[2025-07-26 01:17:18,148] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0
[2025-07-26 01:17:18,148] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2025-07-26 01:17:18,148] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-07-26 01:17:18,148] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2025-07-26 01:17:18,148] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2025-07-26 01:17:18,148] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2025-07-26 01:17:18,148] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2025-07-26 01:17:18,148] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2025-07-26 01:17:18,148] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2025-07-26 01:17:18,149] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-07-26 01:17:18,149] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-07-26 01:17:18,149] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2025-07-26 01:17:18,149] [INFO] [config.py:964:print]   optimizer_name ............... adamw
[2025-07-26 01:17:18,149] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.0003, 'weight_decay': 0.0, 'betas': (0.9, 0.95)}
[2025-07-26 01:17:18,149] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2025-07-26 01:17:18,149] [INFO] [config.py:964:print]   pld_enabled .................. False
[2025-07-26 01:17:18,149] [INFO] [config.py:964:print]   pld_params ................... False
[2025-07-26 01:17:18,149] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2025-07-26 01:17:18,149] [INFO] [config.py:964:print]   scheduler_name ............... WarmupDecayLR
[2025-07-26 01:17:18,149] [INFO] [config.py:964:print]   scheduler_params ............. {'total_num_steps': 12500, 'warmup_min_lr': 0, 'warmup_max_lr': 0.0003, 'warmup_num_steps': 100, 'warmup_type': 'linear'}
[2025-07-26 01:17:18,149] [INFO] [config.py:964:print]   sparse_attention ............. None
[2025-07-26 01:17:18,149] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2025-07-26 01:17:18,149] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2025-07-26 01:17:18,149] [INFO] [config.py:964:print]   train_batch_size ............. 100
[2025-07-26 01:17:18,149] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  10
[2025-07-26 01:17:18,149] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2025-07-26 01:17:18,149] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2025-07-26 01:17:18,149] [INFO] [config.py:964:print]   world_size ................... 1
[2025-07-26 01:17:18,149] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2025-07-26 01:17:18,150] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2025-07-26 01:17:18,150] [INFO] [config.py:964:print]   zero_enabled ................. True
[2025-07-26 01:17:18,150] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2025-07-26 01:17:18,150] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2025-07-26 01:17:18,150] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 10,
    "gradient_accumulation_steps": 10,
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 0.0003,
            "weight_decay": 0.0,
            "betas": [0.9, 0.95]
        }
    },
    "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "total_num_steps": 1.250000e+04,
            "warmup_min_lr": 0,
            "warmup_max_lr": 0.0003,
            "warmup_num_steps": 100,
            "warmup_type": "linear"
        }
    },
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "gradient_clipping": 1.0,
    "zero_optimization": {
        "stage": 2,
        "contiguous_gradients": true,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 5.000000e+08,
        "allgather_bucket_size": 5.000000e+08
    }
}
(train) >> AFTER DEEPSPEED
>> (train) Auto-resume from:  ./runs/plum-13b_kld_0.1_focal_tversky_8_v1_0shot_w_reasonseg/plum-13b_kld_0.1_focal_tversky_8_v1_0shot_w_reasonseg_maxlen512_epochs25_kld_loss_0_dice_loss_8_feedback_loop_train_prompt_enc_srates_9_5_5_1_ckpt_model
  0%|                                                                                                                                                               | 0/200 [00:00<?, ?it/s]/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)
  return F.conv2d(input, weight, bias, self.stride,
>> (ValDataset) sampled_sents:  something showing that the man is playing sports
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py:952: UserWarning: Plan failed with a CuDNNError: cuDNN error: CUDNN_STATUS_BAD_PARAM
Exception raised from run_conv_plan at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:374 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f21f2978897 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe1640b (0x7f2191dfb40b in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x108f133 (0x7f2192074133 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0x1091043 (0x7f2192076043 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x109176b (0x7f219207676b in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x1075c7d (0x7f219205ac7d in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x107656a (0x7f219205b56a in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #7: at::native::cudnn_convolution_transpose(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, bool) + 0xa4 (0x7f219205b714 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #8: <unknown function> + 0x32f2cc2 (0x7f21942d7cc2 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #9: <unknown function> + 0x32ff147 (0x7f21942e4147 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #10: at::_ops::cudnn_convolution_transpose::call(at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool) + 0x2fb (0x7f21e6c3c2fb in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #11: at::native::_convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long, bool, bool, bool, bool) + 0x166d (0x7f21e636348d in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #12: <unknown function> + 0x2a8d27f (0x7f21e74f127f in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #13: <unknown function> + 0x2a93bdc (0x7f21e74f7bdc in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #14: at::_ops::_convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt, bool, bool, bool, bool) + 0x344 (0x7f21e6c3a0d4 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #15: at::native::convolution(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, bool, c10::ArrayRef<long>, long) + 0x3b8 (0x7f21e6356868 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x2a8cb1c (0x7f21e74f0b1c in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x2a93a48 (0x7f21e74f7a48 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #18: at::_ops::convolution::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt) + 0x17b (0x7f21e6bf7d6b in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0x41902e1 (0x7f21e8bf42e1 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #20: <unknown function> + 0x4191259 (0x7f21e8bf5259 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #21: at::_ops::convolution::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, bool, c10::ArrayRef<c10::SymInt>, c10::SymInt) + 0x2d4 (0x7f21e6c38ed4 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #22: <unknown function> + 0x164a2e0 (0x7f21e60ae2e0 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #23: at::native::conv_transpose2d_symint(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, c10::ArrayRef<c10::SymInt>) + 0x125 (0x7f21e635bb05 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #24: <unknown function> + 0x2c879c9 (0x7f21e76eb9c9 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #25: <unknown function> + 0x2c87b03 (0x7f21e76ebb03 in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #26: at::_ops::conv_transpose2d_input::call(at::Tensor const&, at::Tensor const&, std::optional<at::Tensor> const&, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>, c10::SymInt, c10::ArrayRef<c10::SymInt>) + 0x2cb (0x7f21e6f9509b in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #27: <unknown function> + 0x61476f (0x7f21f1a1c76f in /shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #28: /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python() [0x4fc697]
frame #29: _PyObject_MakeTpCall + 0x25b (0x4f614b in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x53d6 (0x4f2376 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #31: /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python() [0x508006]
frame #32: _PyEval_EvalFrameDefault + 0x2b79 (0x4efb19 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #33: /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python() [0x508006]
frame #34: _PyEval_EvalFrameDefault + 0x2b79 (0x4efb19 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #35: _PyObject_FastCallDictTstate + 0xcd (0x4f561d in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #36: _PyObject_Call_Prepend + 0x66 (0x506596 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #37: /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python() [0x5cc323]
frame #38: _PyObject_MakeTpCall + 0x25b (0x4f614b in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #39: _PyEval_EvalFrameDefault + 0x4dde (0x4f1d7e in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #40: /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python() [0x508006]
frame #41: _PyEval_EvalFrameDefault + 0x2b79 (0x4efb19 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #42: /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python() [0x508006]
frame #43: _PyEval_EvalFrameDefault + 0x2b79 (0x4efb19 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #44: _PyObject_FastCallDictTstate + 0xcd (0x4f561d in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #45: _PyObject_Call_Prepend + 0x66 (0x506596 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #46: /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python() [0x5cc323]
frame #47: _PyObject_MakeTpCall + 0x25b (0x4f614b in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #48: _PyEval_EvalFrameDefault + 0x53d6 (0x4f2376 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #49: /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python() [0x507eae]
frame #50: _PyEval_EvalFrameDefault + 0x13b3 (0x4ee353 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #51: /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python() [0x507eae]
frame #52: PyObject_Call + 0xb8 (0x508858 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #53: _PyEval_EvalFrameDefault + 0x2b79 (0x4efb19 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #54: /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python() [0x507eae]
frame #55: PyObject_Call + 0xb8 (0x508858 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #56: _PyEval_EvalFrameDefault + 0x2b79 (0x4efb19 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #57: _PyFunction_Vectorcall + 0x6f (0x4fcadf in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #58: _PyObject_FastCallDictTstate + 0x17d (0x4f56cd in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #59: _PyObject_Call_Prepend + 0x66 (0x506596 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #60: /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python() [0x5cc323]
frame #61: _PyObject_MakeTpCall + 0x25b (0x4f614b in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #62: _PyEval_EvalFrameDefault + 0x5757 (0x4f26f7 in /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python)
frame #63: /shared/nas/data/m1/jk100/.conda/envs/llava/bin/python() [0x507eae]
 (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:921.)
  return F.conv_transpose2d(
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py:952: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv_transpose2d(
>> (ValDataset) sampled_sents:  Horses can be trained to follow commands and be controlled while being ridden. What object in the picture is used for controlling and guiding a horse?
  3%|████▌                                                                                                                                                  | 6/200 [00:06<01:49,  1.77it/s]
>> (ValDataset) sampled_sents:  something that allows people to drink without touching the rim of the cup
>> (ValDataset) sampled_sents:  In the picture, there seems to be a symbiotic relationship between two species, where one provides protection for the other. What animal in the picture is known to seek refuge in the tentacles of another creature for safety?
>> (ValDataset) sampled_sents:  the slogan for welcoming
>> (ValDataset) sampled_sents:  For the safety of newborns, they are often placed in a secure and comfortable space when they sleep. What furniture in the picture is commonly used to provide a safe sleeping environment for babies?

  6%|█████████▊                                                                                                                                            | 13/200 [00:08<00:58,  3.21it/s]
>> (ValDataset) sampled_sents:  Birds have various ways of searching for food. What part of their body helps them to grab and pick up food from the ground in the picture?
>> (ValDataset) sampled_sents:  the sour food
>> (ValDataset) sampled_sents:  something that protects the woman's eyes from getting wet
>> (ValDataset) sampled_sents:  When participating in water activities such as kayaking, it is important to ensure personal safety. What item in the picture is designed to provide buoyancy and keep the person afloat in case of an accident?
>> (ValDataset) sampled_sents:  the sewage outlet
>> (ValDataset) sampled_sents:  In some circus performances, gymnasts perform aerial acrobatics using long pieces of fabric that hang from the ceiling. What in the picture could be used for this type of performance?

 10%|███████████████                                                                                                                                       | 20/200 [00:09<00:46,  3.91it/s]
>> (ValDataset) sampled_sents:  If a person wanted to charge their electric car while parked in the street, what object in the picture could they use to connect the car to an electrical power source?
>> (ValDataset) sampled_sents:  Where do people put their dirty hygiene products to keep the bathroom clean?
>> (ValDataset) sampled_sents:  When the wind blows, small white objects are blown away and scattered in the air. What in the picture is responsible for this phenomenon?
>> (ValDataset) sampled_sents:  something that the dog's food should be put into
>> (ValDataset) sampled_sents:  the food that makes people feel spicy or hot
>> (ValDataset) sampled_sents:  the object used to protect the ears

 14%|████████████████████▎                                                                                                                                 | 27/200 [00:11<00:46,  3.70it/s]
>> (ValDataset) sampled_sents:  the person who is shooting a free throw
>> (ValDataset) sampled_sents:  When soldiers gather for a group photo, what object in the picture is usually held by one of them to represent their unit or country?
>> (ValDataset) sampled_sents:  Sometimes, water leaks from faulty plumbing or faucets. What part of the plumbing system in the picture can be a potential source of the water leak?
>> (ValDataset) sampled_sents:  the part of the persons that is above the water
>> (ValDataset) sampled_sents:  When we need to access or store things above our reach, what would be helpful to stand on?
>> (ValDataset) sampled_sents:  What structure can be used to drain the water from the bathroom floor and prevent the accumulation of wastewater?

 18%|██████████████████████████▎                                                                                                                           | 35/200 [00:13<00:37,  4.42it/s]
>> (ValDataset) sampled_sents:  In a rural landscape, what objects in the picture could provide shade and shelter for animals or humans?
>> (ValDataset) sampled_sents:  something used for playing videos or music
>> (ValDataset) sampled_sents:  When celebrating birthdays, it is common to have a cake with decorations. What part of the cake in the picture is typically used to write birthday greetings or the name of the person celebrating?
>> (ValDataset) sampled_sents:  something that allow pedestrians to cross the canyon
>> (ValDataset) sampled_sents:  the place where the dishes and meals should be put for eating
>> (ValDataset) sampled_sents:  something that the persons use to cross the water
>> (ValDataset) sampled_sents:  Driving at night can be very dangerous due to poor visibility, which can lead to accidents. What part of the car needs to be turned on when driving at night?

 20%|██████████████████████████████▋                                                                                                                       | 41/200 [00:16<00:56,  2.79it/s]
>> (ValDataset) sampled_sents:  The fastest sinking ship
>> (ValDataset) sampled_sents:  In some regions, people raise certain animals for their milk, meat, and skin. What animal in the picture could be domesticated for such purposes?
>> (ValDataset) sampled_sents:  the place where the child is about to slip/fall off
>> (ValDataset) sampled_sents:  In order to facilitate transportation and connect different regions, what structure in the picture was built across the water?
>> (ValDataset) sampled_sents:  the cesspit

 24%|████████████████████████████████████                                                                                                                  | 48/200 [00:17<00:40,  3.74it/s]
>> (ValDataset) sampled_sents:  the objects that can protect the snail and prevent it from getting injured
>> (ValDataset) sampled_sents:  If a person wants to watch TV or a movie, which furniture is the most suitable for them to sit and watch?
>> (ValDataset) sampled_sents:  In historical buildings, there are often signs or symbols displayed on the walls or floors to represent a specific meaning or identity. What in the picture could be used to display such signs or symbols?
>> (ValDataset) sampled_sents:  Insects can help pollinate flowers effectively, but some insects also collect pollen and nectar to produce honey. What in the picture does not make honey?
>> (ValDataset) sampled_sents:  Flowers are often used to decorate tables during special occasions or events. What item in the picture can be used to hold the flowers and keep them fresh by providing water to them?
>> (ValDataset) sampled_sents:  Bamboo is very hard, and its sharp edges can easily scratch people. What tool in the picture can I use to split the bamboo and cross this bamboo forest?

 26%|███████████████████████████████████████▊                                                                                                              | 53/200 [00:19<00:40,  3.65it/s]
>> (ValDataset) sampled_sents:  Seafood dishes often include a tangy condiment that enhances the flavor. What item in the picture can be squeezed onto the seafood as a tangy flavor enhancer?
>> (ValDataset) sampled_sents:  Dogs often like to find a comfortable place to rest. What object in the picture can offer a soft and comfortable surface for the dog to lie on?
>> (ValDataset) sampled_sents:  the area that people can walk on
>> (ValDataset) sampled_sents:  When admiring a beautiful sunset, what part of the picture would we most likely focus on?
>> (ValDataset) sampled_sents:  As a speaker, relying solely on verbal explanations may not effectively convey the intended message to the audience. What area of the picture should be used to project the key content and make it more understandable for the audience during the presentation?

 30%|█████████████████████████████████████████████                                                                                                         | 60/200 [00:21<00:46,  3.01it/s]
>> (ValDataset) sampled_sents:  In a zoo, there are separate areas designated for different animals. What structure in the picture is used to confine and display animals?
>> (ValDataset) sampled_sents:  something that tastes spicy
>> (ValDataset) sampled_sents:  When going fishing on a calm sea, what type of boat shown in the picture would be an ideal choice for a peaceful fishing experience?
>> (ValDataset) sampled_sents:  Insects have various ways to protect themselves from predators. What characteristics can a moth use to deter potential threats?
>> (ValDataset) sampled_sents:  Fishing is a popular activity for relaxation and leisure. What tool is the man in the picture using to catch fish?

 33%|█████████████████████████████████████████████████▌                                                                                                    | 66/200 [00:23<00:34,  3.90it/s]
>> (ValDataset) sampled_sents:  the shadow of the red car
>> (ValDataset) sampled_sents:  the part of this animal's body that comes into contact with the air
>> (ValDataset) sampled_sents:  Many people use bags to carry their belongings when they go out. What part of the bag in the picture can be used to carry the bag comfortably over the shoulder?
>> (ValDataset) sampled_sents:  During a meal, people typically use utensils to bring food to their mouths. What tool in the picture can be used to fulfill this need?
>> (ValDataset) sampled_sents:  During the move, what object can be used to store and transport various sundries and small household items, which is sturdy and relatively easy to carry?

 37%|███████████████████████████████████████████████████████▌                                                                                              | 74/200 [00:25<00:30,  4.10it/s]
>> (ValDataset) sampled_sents:  something used by a Jedi warrior to attack enemies
>> (ValDataset) sampled_sents:  What object in the picture should be used with water to create foam and effectively remove oil and dirt when your hands are very dirty?
>> (ValDataset) sampled_sents:  In a concert or public speaking event, what object in the picture would be used by the speaker or performer to amplify their voice?
>> (ValDataset) sampled_sents:  In the living room, people often sit on the sofa to watch TV or chat. What object can they use to adjust the TV screen or change channels?
>> (ValDataset) sampled_sents:  something that produces pollen
>> (ValDataset) sampled_sents:  the part that can transmit traction and braking torque, coming into contact with the ground
>> (ValDataset) sampled_sents:  the area where people can stand without getting wet

 40%|███████████████████████████████████████████████████████████▎                                                                                          | 79/200 [00:27<00:41,  2.94it/s]
>> (ValDataset) sampled_sents:  the region showing current time
>> (ValDataset) sampled_sents:  the reflection of the object
>> (ValDataset) sampled_sents:  Dogs use their mouths to perform various tasks, including eating and vocalizing. What part of the dog's body is primarily responsible for these actions?
>> (ValDataset) sampled_sents:  something that protects the persons' heads

 42%|███████████████████████████████████████████████████████████████▊                                                                                      | 85/200 [00:29<00:33,  3.47it/s]
>> (ValDataset) sampled_sents:  When constructing a house, what part of the building process involves assembling a sturdy base and framework?
>> (ValDataset) sampled_sents:  The office is a place where people focus on their work. What structure in the room can help block noise and reduces Interruptions from the outside environment?
>> (ValDataset) sampled_sents:  the camera lens that is more suitable for photographing nearby objects
>> (ValDataset) sampled_sents:  the part of the house that can be opened
>> (ValDataset) sampled_sents:  the source of power for the ship
>> (ValDataset) sampled_sents:  When hanging laundry outside to dry, what material in the picture would be suitable for holding the clothes securely in place?

 46%|█████████████████████████████████████████████████████████████████████▊                                                                                | 93/200 [00:31<00:26,  4.11it/s]
>> (ValDataset) sampled_sents:  Dogs have keen sense of smell, which is why they can be used as drug-sniffing dogs. Which part in the picture gives dogs this characteristic?
>> (ValDataset) sampled_sents:  the place where the patient lies down to receive examination
>> (ValDataset) sampled_sents:  In an intense dragon boat race. What object in the picture should be struck to boost the morale of the competing team and cheer them on?
>> (ValDataset) sampled_sents:  the equipment for sweeping away rain on rainy days
>> (ValDataset) sampled_sents:  Soldiers are often equipped with various tools and weapons to carry out their duties. What item in the picture can be classified as a weapon?
>> (ValDataset) sampled_sents:  What object is used to cover the entrance of the bathroom and ensure privacy?

 50%|██████████████████████████████████████████████████████████████████████████▌                                                                          | 100/200 [00:33<00:24,  4.14it/s]
>> (ValDataset) sampled_sents:  a place where bees can suck nectar from flowers
>> (ValDataset) sampled_sents:  I want to take a trip around the world, but I need some transportation to help me cross the oceans. What type of transportation in the picture can fulfill this requirement?
>> (ValDataset) sampled_sents:  where we can see the speed of the car
>> (ValDataset) sampled_sents:  When a photographer wants to adjust the focus of a camera lens, what part in the picture could they manipulate to achieve this?
>> (ValDataset) sampled_sents:  unopened flower bud
>> (ValDataset) sampled_sents:  This image depicts a forest. Which of the animals in the picture pose a threat to human safety?

 54%|███████████████████████████████████████████████████████████████████████████████▋                                                                     | 107/200 [00:35<00:23,  3.89it/s]
>> (ValDataset) sampled_sents:  When visiting a library or bookstore, people often browse through the shelves to find interesting books to read. Which area in the picture could provide a variety of reading materials for visitors?
>> (ValDataset) sampled_sents:  the person who appears to have already won in the battle
>> (ValDataset) sampled_sents:  In a healthy meal, vegetables are often included to provide essential nutrients. What in the picture can be used to eat the vegetables?
>> (ValDataset) sampled_sents:  something that prevents people from getting into the building
>> (ValDataset) sampled_sents:  something that the monkey uses to pierce its food
>> (ValDataset) sampled_sents:  something that avoids falling down
>> (ValDataset) sampled_sents:  the container that can be used to hold soup currently

>> (ValDataset) sampled_sents:  When you look closely into a person's eye, what is the part that surrounds the dark-colored iris and helps protect the eye from dust and debris?
>> (ValDataset) sampled_sents:  the object that reaches the sky
>> (ValDataset) sampled_sents:  In an orchestra, musicians play different instruments. What object in the picture is commonly played with a bow to produce sound?
>> (ValDataset) sampled_sents:  What object in a typical modern kitchen can be used to quickly heat up leftovers, boil water, or cook frozen food?
>> (ValDataset) sampled_sents:  When plants grow excessively in unwanted areas, it is necessary to remove them to maintain the aesthetics of the surroundings. What part of the plants in the picture may need to be removed in this situation?
>> (ValDataset) sampled_sents:  In a television studio, various equipment is used to capture and record video footage. What in the picture could be used to stabilize and hold the camera steady during filming?

 60%|████████████████████████████████████████████████████████████████████████████████████████▋                                                            | 119/200 [00:39<00:30,  2.68it/s]
>> (ValDataset) sampled_sents:  Insects are often found on or near trees, where they can find shelter and food. What part of the tree in this picture could insects commonly be found on or around?
>> (ValDataset) sampled_sents:  If the person in the picture is the owner of the blue car and wants to drive it, which car door is least likely to be the one he enters based on the picture shown?
>> (ValDataset) sampled_sents:  the keys on a piano used to play notes of half-steps or semitones
>> (ValDataset) sampled_sents:  The general steps for brewing tea are to put tea leaves into a teacup and then pour hot water over them. What container in the picture is most likely to be used next for pouring hot water to make tea?

 64%|███████████████████████████████████████████████████████████████████████████████████████████████▎                                                     | 128/200 [00:42<00:14,  4.81it/s]
>> (ValDataset) sampled_sents:  When the weather is very cold outside, what equipment do we need to use to increase the indoor temperature in the room?
>> (ValDataset) sampled_sents:  If we wanted to identify the specific vehicle shown in the picture, what part of the car should we look at?
>> (ValDataset) sampled_sents:  something hot and light
>> (ValDataset) sampled_sents:  objects that can help women appear taller
>> (ValDataset) sampled_sents:  What container in the picture is used for arranging the flowers to make them look more beautiful?
>> (ValDataset) sampled_sents:  the places where the driver can observe the speed
>> (ValDataset) sampled_sents:  something indicating that someone is celerating the birthday
>> (ValDataset) sampled_sents:  something that we use to control computer games

 68%|██████████████████████████████████████████████████████████████████████████████████████████████████████                                               | 137/200 [00:44<00:14,  4.37it/s]
>> (ValDataset) sampled_sents:  the container that contains the vegetable
>> (ValDataset) sampled_sents:  In the animal kingdom, the males of certain species have a distinctive feature that sets them apart from the females. What part of the lion in this picture is a defining characteristic of male lions?
>> (ValDataset) sampled_sents:  During times of war, armored vehicles are commonly used to protect soldiers and engage in combat. What object in the picture can provide such protection?
>> (ValDataset) sampled_sents:  It is common for some bird species to live near bodies of water and rely on them as their primary habitat. What in the picture can be considered as the habitat for the birds mentioned?
>> (ValDataset) sampled_sents:  In historical architecture, a building often has a grand entrance that consists of a large opening with a curved or pointed top. What feature in the picture resembles such an entrance?
>> (ValDataset) sampled_sents:  the people who are about to get on the vehicle
>> (ValDataset) sampled_sents:  something indicating the identity of the bus
>> (ValDataset) sampled_sents:  the object that can be used by the owner to lead the dog

 72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                         | 144/200 [00:46<00:15,  3.64it/s]
>> (ValDataset) sampled_sents:  What musical instrument in the picture is usually played with both hands on a keyboard?
>> (ValDataset) sampled_sents:  When eating scrambled eggs for breakfast, people often add a side dish made of potatoes. What item in the picture can be used to serve the potatoes?
>> (ValDataset) sampled_sents:  something that can control the fan speed
>> (ValDataset) sampled_sents:  What structure separates two areas in the room and is commonly used to hold onto for support when going up and down?
>> (ValDataset) sampled_sents:  In urban areas, there are designated areas for bicycles to ride safely. What area in the picture would a cyclist use to navigate through the city?
>> (ValDataset) sampled_sents:  In a mechanical workshop, there are various machines and tools used for different purposes. What in the picture could be used to rotate or spin other parts or objects?

 74%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████                                      | 149/200 [00:48<00:19,  2.61it/s]
>> (ValDataset) sampled_sents:  the stronger Mario
>> (ValDataset) sampled_sents:  where to wash hands
>> (ValDataset) sampled_sents:  the food that the bird likes to eat

 77%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                  | 154/200 [00:49<00:14,  3.25it/s]
>> (ValDataset) sampled_sents:  In order to shape and carve hard materials, such as wood or stone, what part of the tool in the picture would be used to provide precise control and force?
>> (ValDataset) sampled_sents:  In the picture, there is a legal requirement for vehicles to display identifying information. What part of the car is used to display this information?
>> (ValDataset) sampled_sents:  the ship that is most likely to carry a fleet commander
>> (ValDataset) sampled_sents:  In outdoor recreational activities, people can experience the thrill of flying in the sky with the help of equipment. What object in the picture can provide this experience?
>> (ValDataset) sampled_sents:  We are currently watching a game and it's halftime. Who are the cheerleaders who come out to liven up the atmosphere?
>> (ValDataset) sampled_sents:  In an educational setting, children often use different materials to learn about letters, numbers, and words. What object in the picture could be used as a visual aid for learning about letters and words?

 81%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                            | 162/200 [00:52<00:10,  3.78it/s]
>> (ValDataset) sampled_sents:  If you want to play table tennis indoors, what furniture in the picture should be used as the playing surface?
>> (ValDataset) sampled_sents:  In a military airfield, what area is specifically designed for aircraft to take off and land?
>> (ValDataset) sampled_sents:  Generally speaking, dogs do not have horns on their heads, only a pair of ears. What part of the dog's head in this picture looks strange?
>> (ValDataset) sampled_sents:  In an organized workspace, one might have a designated area to store important documents and files. What piece of furniture in the picture could be used for this purpose?
>> (ValDataset) sampled_sents:  something that ensures the person to land safely
>> (ValDataset) sampled_sents:  A bride and groom often walk together down the aisle during a wedding ceremony. What object in the picture is the bride most likely holding during this moment?

 84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                       | 168/200 [00:53<00:09,  3.29it/s]
>> (ValDataset) sampled_sents:  the object that might reflect this person's marital status
>> (ValDataset) sampled_sents:  knobs without screws in the center
>> (ValDataset) sampled_sents:  where the garbage should be put
>> (ValDataset) sampled_sents:  Dogs are faithful companions to humans, and humans often play fetch games with them. What object will the dog likely retrieve and bring back to the human for the next round of fetch in the picture?
>> (ValDataset) sampled_sents:  People often add gas to water to improve its taste. What part of the picture will be consumed as a beverage?
>> (ValDataset) sampled_sents:  When people want to take their pets to the park and enjoy some outdoor activities, they often want to make sure their pets are safe and cannot run away. What in the picture could help with this?

 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                  | 176/200 [00:56<00:06,  3.83it/s]
>> (ValDataset) sampled_sents:  If we were to have a friendly competition to see who can paddle the fastest in the water, what would be the most suitable vessels for this activity in the picture?
>> (ValDataset) sampled_sents:  the man who seems to lose
>> (ValDataset) sampled_sents:  In a dark cave, there is no natural light source. What object in the picture can be used to provide light to navigate and explore the cave?
>> (ValDataset) sampled_sents:  the car that may need repair
>> (ValDataset) sampled_sents:  In ancient times, people used different methods to measure time during the day. What object in the picture could have been used as a timekeeping device based on the position of the sun?
>> (ValDataset) sampled_sents:  In a busy and bustling city, it is important for businesses to have eye-catching signs to attract customers. What feature in the picture can serve this purpose?

 92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████            | 184/200 [00:58<00:03,  4.01it/s]
>> (ValDataset) sampled_sents:  Birds often need a place to rest or observe their surroundings. What part of a tree in the picture offers a suitable spot for birds to do so?
>> (ValDataset) sampled_sents:  something that we can sit on
>> (ValDataset) sampled_sents:  To keep bread fresh and protected, it is often placed in a protective covering. What item in the picture is commonly used for this purpose?
>> (ValDataset) sampled_sents:  Sometimes, when a car is parked on the side of the road, the driver may accidentally leave the car door open. What part in the picture can indicate that the car door is open?
>> (ValDataset) sampled_sents:  something that produces pollen
>> (ValDataset) sampled_sents:  When purchasing meat from a grocery store, it is often stored and sold in a certain type of container. What object in the picture could be used for this purpose?
>> (ValDataset) sampled_sents:  What object in the picture could be used for defense and firepower in an ancient fort?

 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████      | 192/200 [01:00<00:01,  4.42it/s]
>> (ValDataset) sampled_sents:  Dogs have keen sense of smell, which is why they can be used as drug-sniffing dogs. Which part in the picture gives dogs this characteristic?
>> (ValDataset) sampled_sents:  Some birds have long bills that they use to catch food from the water. What part of the bird's body in the picture may have this characteristic?
>> (ValDataset) sampled_sents:  After cooking, consuming food, and preparing for food, where can we throw away the rest of the food and scraps?
>> (ValDataset) sampled_sents:  the person who is most likely to be the girl's trainer
>> (ValDataset) sampled_sents:  At a car show, visitors can get close to the displayed vehicles to admire their design and features. What part of the car in this picture is open, allowing viewers to see the engine compartment?
>> (ValDataset) sampled_sents:  When snorkeling or scuba diving, it is important to wear certain equipment to protect the eyes and aid in vision. What object in the picture can be used for this purpose?
>> (ValDataset) sampled_sents:  the places for further exploration

 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 197/200 [01:02<00:01,  2.47it/s]
>> (ValDataset) sampled_sents:  the person who is speaking currently
>> (ValDataset) sampled_sents:  the ball that can only be hit into the hole at last
>> (ValDataset) sampled_sents:  the reflection of the camera in the mirror
>> (ValDataset) sampled_sents:  In some rural areas, horse-drawn carts are still used for transportation and carrying goods. What is the main source of power that drives the cart in the picture?
>> (ValDataset) sampled_sents:  In case of a fire, it is important to have access to fire safety equipment. What object in the picture is specifically designed to store and release fire extinguishing substances?
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [01:02<00:00,  3.18it/s]
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
>> (ValDataset) sampled_sents:  something used for contacting other people
giou: 0.0760, ciou: 0.0721 | BIO per cls acc: O=0.2424, B=0.2300, I=0.1585
[2025-07-26 01:18:21,305] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step0 is about to be saved!
[2025-07-26 01:18:35,776] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./runs/plum-13b_kld_0.1_focal_tversky_8_v1_0shot_w_reasonseg/plum-13b_kld_0.1_focal_tversky_8_v1_0shot_w_reasonseg_maxlen512_epochs25_kld_loss_0_dice_loss_8_feedback_loop_train_prompt_enc_srates_9_5_5_1_ckpt_model/global_step0/mp_rank_00_model_states.pt
