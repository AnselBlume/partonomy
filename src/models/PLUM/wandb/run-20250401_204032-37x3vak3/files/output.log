/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
>> name : layers.0.self_attn.in_proj_bias
>> name : layers.0.self_attn.out_proj.bias
>> name : layers.0.linear1.bias
>> name : layers.0.linear2.bias
>> name : layers.0.norm1.weight
>> name : layers.0.norm1.bias
>> name : layers.0.norm2.weight
>> name : layers.0.norm2.bias
> /shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py(233)__init__()
-> for name, module in self.encoder.named_modules():
Parameter containing:
tensor(..., device='meta', size=(2048, 4096), requires_grad=True)
torch.Size([2048, 4096])
tensor(..., device='meta', size=(4096,), grad_fn=<SelectBackward0>)
tensor(..., device='meta', size=(10,), grad_fn=<SliceBackward0>)
Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)
Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)
tensor(..., device='meta', size=(), grad_fn=<SelectBackward0>)
*** NotImplementedError: aten::_local_scalar_dense: attempted to run this operator with Meta tensors, but there was no abstract impl or Meta kernel registered. You may have run into this message while using an operator with PT2 compilation APIs (torch.compile/torch.export); in order to use this operator with those APIs you'll need to add an abstract impl.Please see the following doc for next steps: https://docs.google.com/document/d/1_W62p8WJOQQUzPsJYa7s701JXt0qf2OfLub2sbkHOaU/edit
Parameter containing:
tensor(..., device='meta', size=(4096,), requires_grad=True)
Traceback (most recent call last):
  File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 795, in <module>
    main(sys.argv[1:])
  File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 226, in main
    model = PLUMForCausalLM.from_pretrained(
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2700, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py", line 452, in __init__
    d_model=config.hidden_size,
  File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py", line 233, in __init__
    if isinstance(module, nn.LayerNorm):
  File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py", line 233, in __init__
    if isinstance(module, nn.LayerNorm):
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
