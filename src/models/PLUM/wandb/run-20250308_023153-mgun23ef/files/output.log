You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565



Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.57s/it]
trainable params: 6,553,600 || all params: 14,025,701,683 || trainable%: 0.0467256480147682
n:  base_model.model.model.embed_tokens.weight p.shape:  torch.Size([32002, 5120])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_token.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.mask_decoder.mask_tokens.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.weight p.shape:  torch.Size([256, 64, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.weight p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.weight p.shape:  torch.Size([64, 32, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.bias p.shape:  torch.Size([4])
n:  base_model.model.model.text_hidden_fcs.0.0.weight p.shape:  torch.Size([5120, 5120])
n:  base_model.model.model.text_hidden_fcs.0.0.bias p.shape:  torch.Size([5120])
n:  base_model.model.model.text_hidden_fcs.0.2.weight p.shape:  torch.Size([256, 5120])
n:  base_model.model.model.text_hidden_fcs.0.2.bias p.shape:  torch.Size([256])
n:  base_model.model.lm_head.weight p.shape:  torch.Size([32002, 5120])
>> (PLUM.py) Initializing teacher LLM...
>> (PLUM.py) Teacher LLM initialized.
ade20k:  20210
cocostuff:  118287
loading annotations into memory...
Done (t=0.53s)
creating index...
index created!
pascal_part:  4366
loading annotations into memory...
Done (t=8.40s)
creating index...
index created!
paco_lvis:  45790
mapillary:  18000
loading dataset refclef into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refclef/refs(unc).p
creating index...
index created.
DONE (t=2.66s)
dataset refclef (refs unc) (train split) has 17978 images and 99523 annotations.
loading dataset refcoco into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco/refs(unc).p
creating index...
index created.
DONE (t=5.21s)
dataset refcoco (refs unc) (train split) has 16994 images and 196771 annotations.
loading dataset refcoco+ into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco+/refs(unc).p
creating index...
index created.
DONE (t=5.86s)
dataset refcoco+ (refs unc) (train split) has 16992 images and 196737 annotations.
loading dataset refcocog into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcocog/refs(umd).p
creating index...
index created.
DONE (t=5.96s)
dataset refcocog (refs umd) (train split) has 21899 images and 208960 annotations.
vqa_data:  157712
number of reason_seg samples:  239
len(self.img_to_explanation):  239
Training with 20000 examples and validating with 200 examples.
[2025-03-08 02:33:30,634] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5, git-hash=unknown, git-branch=unknown
[2025-03-08 02:33:30,634] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2025-03-08 02:33:30,634] [INFO] [comm.py:594:init_distributed] cdb=None
[2025-03-08 02:33:30,634] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-03-08 02:34:19,963] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Time to load fused_adam op: 0.7002172470092773 seconds
[2025-03-08 02:34:20,860] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-03-08 02:34:21,662] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-03-08 02:34:21,662] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-03-08 02:34:21,662] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-03-08 02:34:21,662] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500000000
[2025-03-08 02:34:21,662] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500000000
[2025-03-08 02:34:21,663] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2025-03-08 02:34:21,663] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Loading extension module fused_adam...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/tensor/python_tensor.cpp:78.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
Rank: 0 partition count [1] and sizes[(365842916, False)]
[2025-03-08 02:34:26,861] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2025-03-08 02:34:26,862] [INFO] [utils.py:786:see_memory_usage] MA 53.71 GB         Max_MA 54.39 GB         CA 54.6 GB         Max_CA 55 GB
[2025-03-08 02:34:26,863] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 220.17 GB, percent = 21.9%
[2025-03-08 02:34:29,950] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2025-03-08 02:34:29,951] [INFO] [utils.py:786:see_memory_usage] MA 56.44 GB         Max_MA 57.8 GB         CA 58.69 GB         Max_CA 59 GB
[2025-03-08 02:34:29,952] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 220.39 GB, percent = 21.9%
[2025-03-08 02:34:29,952] [INFO] [stage_1_and_2.py:488:__init__] optimizer state initialized
[2025-03-08 02:34:33,324] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2025-03-08 02:34:33,325] [INFO] [utils.py:786:see_memory_usage] MA 56.44 GB         Max_MA 56.44 GB         CA 58.69 GB         Max_CA 59 GB
[2025-03-08 02:34:33,325] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 220.21 GB, percent = 21.9%
[2025-03-08 02:34:33,331] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-03-08 02:34:33,332] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2025-03-08 02:34:33,332] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7fe70d3b22f0>
[2025-03-08 02:34:33,332] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
[2025-03-08 02:34:33,342] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2025-03-08 02:34:33,342] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-03-08 02:34:33,342] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-03-08 02:34:33,342] [INFO] [config.py:964:print]   amp_enabled .................. False
[2025-03-08 02:34:33,342] [INFO] [config.py:964:print]   amp_params ................... False
[2025-03-08 02:34:33,343] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-08 02:34:33,343] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2025-03-08 02:34:33,343] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2025-03-08 02:34:33,343] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2025-03-08 02:34:33,343] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2025-03-08 02:34:33,343] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd235749930>
[2025-03-08 02:34:33,343] [INFO] [config.py:964:print]   communication_data_type ...... None
[2025-03-08 02:34:33,343] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-08 02:34:33,343] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2025-03-08 02:34:33,343] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2025-03-08 02:34:33,343] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-08 02:34:33,343] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2025-03-08 02:34:33,343] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2025-03-08 02:34:33,343] [INFO] [config.py:964:print]   disable_allgather ............ False
[2025-03-08 02:34:33,343] [INFO] [config.py:964:print]   dump_state ................... False
[2025-03-08 02:34:33,343] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2025-03-08 02:34:33,344] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2025-03-08 02:34:33,344] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-08 02:34:33,344] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-08 02:34:33,344] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2025-03-08 02:34:33,344] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2025-03-08 02:34:33,344] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2025-03-08 02:34:33,344] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2025-03-08 02:34:33,344] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2025-03-08 02:34:33,344] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2025-03-08 02:34:33,344] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-03-08 02:34:33,344] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2025-03-08 02:34:33,344] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2025-03-08 02:34:33,344] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2025-03-08 02:34:33,345] [INFO] [config.py:964:print]   global_rank .................. 0
[2025-03-08 02:34:33,345] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2025-03-08 02:34:33,345] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 10
[2025-03-08 02:34:33,345] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0
[2025-03-08 02:34:33,345] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2025-03-08 02:34:33,345] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-08 02:34:33,345] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2025-03-08 02:34:33,345] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2025-03-08 02:34:33,345] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2025-03-08 02:34:33,345] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2025-03-08 02:34:33,345] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2025-03-08 02:34:33,345] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2025-03-08 02:34:33,345] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-03-08 02:34:33,345] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-03-08 02:34:33,345] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2025-03-08 02:34:33,346] [INFO] [config.py:964:print]   optimizer_name ............... adamw
[2025-03-08 02:34:33,346] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.0003, 'weight_decay': 0.0, 'betas': (0.9, 0.95)}
[2025-03-08 02:34:33,346] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2025-03-08 02:34:33,346] [INFO] [config.py:964:print]   pld_enabled .................. False
[2025-03-08 02:34:33,346] [INFO] [config.py:964:print]   pld_params ................... False
[2025-03-08 02:34:33,346] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2025-03-08 02:34:33,346] [INFO] [config.py:964:print]   scheduler_name ............... WarmupDecayLR
[2025-03-08 02:34:33,346] [INFO] [config.py:964:print]   scheduler_params ............. {'total_num_steps': 25000, 'warmup_min_lr': 0, 'warmup_max_lr': 0.0003, 'warmup_num_steps': 100, 'warmup_type': 'linear'}
[2025-03-08 02:34:33,346] [INFO] [config.py:964:print]   sparse_attention ............. None
[2025-03-08 02:34:33,346] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2025-03-08 02:34:33,346] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2025-03-08 02:34:33,346] [INFO] [config.py:964:print]   train_batch_size ............. 40
[2025-03-08 02:34:33,346] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  4
[2025-03-08 02:34:33,346] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2025-03-08 02:34:33,346] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2025-03-08 02:34:33,346] [INFO] [config.py:964:print]   world_size ................... 1
[2025-03-08 02:34:33,346] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2025-03-08 02:34:33,346] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2025-03-08 02:34:33,347] [INFO] [config.py:964:print]   zero_enabled ................. True
[2025-03-08 02:34:33,347] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-08 02:34:33,347] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2025-03-08 02:34:33,347] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 4,
    "gradient_accumulation_steps": 10,
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 0.0003,
            "weight_decay": 0.0,
            "betas": [0.9, 0.95]
        }
    },
    "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "total_num_steps": 2.500000e+04,
            "warmup_min_lr": 0,
            "warmup_max_lr": 0.0003,
            "warmup_num_steps": 100,
            "warmup_type": "linear"
        }
    },
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "gradient_clipping": 1.0,
    "zero_optimization": {
        "stage": 2,
        "contiguous_gradients": true,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 5.000000e+08,
        "allgather_bucket_size": 5.000000e+08
    }
}
(train) >> AFTER DEEPSPEED
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)
  return F.conv2d(input, weight, bias, self.stride,
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([2, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
>> last_hidden_states.shape:  torch.Size([12, 508, 5120])
>> per_token_labels.shape:  torch.Size([12, 508])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([5, 478, 640])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 508, 5120])
>> per_token_labels.shape:  torch.Size([12, 508])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([5, 478, 640])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([5, 640, 480])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([5, 640, 480])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([5, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([1, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
>> last_hidden_states.shape:  torch.Size([15, 411, 5120])
>> per_token_labels.shape:  torch.Size([15, 411])
>> masks_list[0].shape:  torch.Size([4, 640, 426])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 640, 426])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 411, 5120])
>> per_token_labels.shape:  torch.Size([15, 411])
>> masks_list[0].shape:  torch.Size([4, 640, 426])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 640, 426])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 465, 5120])
>> per_token_labels.shape:  torch.Size([12, 465])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 465, 5120])
>> per_token_labels.shape:  torch.Size([12, 465])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 342, 5120])
>> per_token_labels.shape:  torch.Size([17, 342])
>> masks_list[0].shape:  torch.Size([5, 553, 640])
>> masks_list[1].shape:  torch.Size([5, 427, 640])
>> gt_mask.shape:  torch.Size([1, 553, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 342, 5120])
>> per_token_labels.shape:  torch.Size([17, 342])
>> masks_list[0].shape:  torch.Size([5, 553, 640])
>> masks_list[1].shape:  torch.Size([5, 427, 640])
>> gt_mask.shape:  torch.Size([1, 553, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 342, 5120])
>> per_token_labels.shape:  torch.Size([17, 342])
>> masks_list[0].shape:  torch.Size([5, 553, 640])
>> masks_list[1].shape:  torch.Size([5, 427, 640])
>> gt_mask.shape:  torch.Size([1, 427, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 342, 5120])
>> per_token_labels.shape:  torch.Size([17, 342])
>> masks_list[0].shape:  torch.Size([5, 553, 640])
>> masks_list[1].shape:  torch.Size([5, 427, 640])
>> gt_mask.shape:  torch.Size([1, 427, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 417, 5120])
>> per_token_labels.shape:  torch.Size([20, 417])
>> masks_list[0].shape:  torch.Size([5, 256, 256])
>> masks_list[1].shape:  torch.Size([5, 640, 640])
>> gt_mask.shape:  torch.Size([1, 256, 256])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 417, 5120])
>> per_token_labels.shape:  torch.Size([20, 417])
>> masks_list[0].shape:  torch.Size([5, 256, 256])
>> masks_list[1].shape:  torch.Size([5, 640, 640])
>> gt_mask.shape:  torch.Size([1, 256, 256])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 417, 5120])
>> per_token_labels.shape:  torch.Size([20, 417])
>> masks_list[0].shape:  torch.Size([5, 256, 256])
>> masks_list[1].shape:  torch.Size([5, 640, 640])
>> gt_mask.shape:  torch.Size([1, 360, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 417, 5120])
>> per_token_labels.shape:  torch.Size([20, 417])
>> masks_list[0].shape:  torch.Size([5, 256, 256])
>> masks_list[1].shape:  torch.Size([5, 640, 640])
>> gt_mask.shape:  torch.Size([1, 360, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 332, 5120])
>> per_token_labels.shape:  torch.Size([19, 332])
>> masks_list[0].shape:  torch.Size([4, 640, 425])
>> masks_list[1].shape:  torch.Size([5, 427, 640])
>> gt_mask.shape:  torch.Size([1, 640, 425])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 332, 5120])
>> per_token_labels.shape:  torch.Size([19, 332])
>> masks_list[0].shape:  torch.Size([4, 640, 425])
>> masks_list[1].shape:  torch.Size([5, 427, 640])
>> gt_mask.shape:  torch.Size([1, 640, 425])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 332, 5120])
>> per_token_labels.shape:  torch.Size([19, 332])
>> masks_list[0].shape:  torch.Size([4, 640, 425])
>> masks_list[1].shape:  torch.Size([5, 427, 640])
>> gt_mask.shape:  torch.Size([1, 640, 425])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 332, 5120])
>> per_token_labels.shape:  torch.Size([19, 332])
>> masks_list[0].shape:  torch.Size([4, 640, 425])
>> masks_list[1].shape:  torch.Size([5, 427, 640])
>> gt_mask.shape:  torch.Size([1, 640, 425])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 332, 5120])
>> per_token_labels.shape:  torch.Size([19, 332])
>> masks_list[0].shape:  torch.Size([4, 640, 425])
>> masks_list[1].shape:  torch.Size([5, 427, 640])
>> gt_mask.shape:  torch.Size([1, 640, 425])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 332, 5120])
>> per_token_labels.shape:  torch.Size([19, 332])
>> masks_list[0].shape:  torch.Size([4, 640, 425])
>> masks_list[1].shape:  torch.Size([5, 427, 640])
>> gt_mask.shape:  torch.Size([1, 640, 425])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 332, 5120])
>> per_token_labels.shape:  torch.Size([19, 332])
>> masks_list[0].shape:  torch.Size([4, 640, 425])
>> masks_list[1].shape:  torch.Size([5, 427, 640])
>> gt_mask.shape:  torch.Size([1, 640, 425])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 332, 5120])
>> per_token_labels.shape:  torch.Size([19, 332])
>> masks_list[0].shape:  torch.Size([4, 640, 425])
>> masks_list[1].shape:  torch.Size([5, 427, 640])
>> gt_mask.shape:  torch.Size([1, 640, 425])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([9, 512, 5120])
>> per_token_labels.shape:  torch.Size([9, 512])
>> masks_list[0].shape:  torch.Size([0, 246, 326])
>> masks_list[1].shape:  torch.Size([2, 427, 640])
>> gt_mask.shape:  torch.Size([1, 427, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([9, 512, 5120])
>> per_token_labels.shape:  torch.Size([9, 512])
>> masks_list[0].shape:  torch.Size([0, 246, 326])
>> masks_list[1].shape:  torch.Size([2, 427, 640])
>> gt_mask.shape:  torch.Size([1, 427, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 432, 5120])
>> per_token_labels.shape:  torch.Size([16, 432])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 206, 275])
>> gt_mask.shape:  torch.Size([1, 206, 275])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 432, 5120])
>> per_token_labels.shape:  torch.Size([16, 432])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 206, 275])
>> gt_mask.shape:  torch.Size([1, 206, 275])
>> image_emb:  torch.Size([1, 256, 64, 64])
Epoch: [0][  1/500]	Time 28.945 (28.945)	Loss 0.6755 (0.5479)	CeLoss 0.2793 (0.2300)	SegCLSLoss 0.1016 (0.0903)	KLLoss 0.0000 (0.0056)	MaskLoss 0.1721 (0.1349)	MaskBCELoss 0.0888 (0.0553)	MaskDICELoss 0.0833 (0.0796)
>> last_hidden_states.shape:  torch.Size([16, 463, 5120])
>> per_token_labels.shape:  torch.Size([16, 463])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([0, 640, 480])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 463, 5120])
>> per_token_labels.shape:  torch.Size([16, 463])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([0, 640, 480])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 442, 5120])
>> per_token_labels.shape:  torch.Size([14, 442])
>> masks_list[0].shape:  torch.Size([4, 250, 464])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 250, 464])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 442, 5120])
>> per_token_labels.shape:  torch.Size([14, 442])
>> masks_list[0].shape:  torch.Size([4, 250, 464])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 250, 464])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 493, 5120])
>> per_token_labels.shape:  torch.Size([16, 493])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 493, 5120])
>> per_token_labels.shape:  torch.Size([16, 493])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 330, 5120])
>> per_token_labels.shape:  torch.Size([20, 330])
>> masks_list[0].shape:  torch.Size([5, 512, 683])
>> masks_list[1].shape:  torch.Size([5, 640, 480])
>> gt_mask.shape:  torch.Size([1, 512, 683])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 330, 5120])
>> per_token_labels.shape:  torch.Size([20, 330])
>> masks_list[0].shape:  torch.Size([5, 512, 683])
>> masks_list[1].shape:  torch.Size([5, 640, 480])
>> gt_mask.shape:  torch.Size([1, 512, 683])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 350, 5120])
>> per_token_labels.shape:  torch.Size([16, 350])
>> masks_list[0].shape:  torch.Size([3, 425, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 425, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 350, 5120])
>> per_token_labels.shape:  torch.Size([16, 350])
>> masks_list[0].shape:  torch.Size([3, 425, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 425, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([6, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
>> last_hidden_states.shape:  torch.Size([15, 512, 5120])
>> per_token_labels.shape:  torch.Size([15, 512])
>> masks_list[0].shape:  torch.Size([4, 375, 500])
>> masks_list[1].shape:  torch.Size([5, 512, 683])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 512, 5120])
>> per_token_labels.shape:  torch.Size([15, 512])
>> masks_list[0].shape:  torch.Size([4, 375, 500])
>> masks_list[1].shape:  torch.Size([5, 512, 683])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([7, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([0, 427, 640])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 426, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([0, 427, 640])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 426, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 335, 5120])
>> per_token_labels.shape:  torch.Size([18, 335])
>> masks_list[0].shape:  torch.Size([5, 480, 360])
>> masks_list[1].shape:  torch.Size([3, 427, 640])
>> gt_mask.shape:  torch.Size([1, 480, 360])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 335, 5120])
>> per_token_labels.shape:  torch.Size([18, 335])
>> masks_list[0].shape:  torch.Size([5, 480, 360])
>> masks_list[1].shape:  torch.Size([3, 427, 640])
>> gt_mask.shape:  torch.Size([1, 480, 360])
>> image_emb:  torch.Size([1, 256, 64, 64])
Epoch: [0][  2/500]	Time 25.678 (25.678)	Loss 0.4460 (0.7401)	CeLoss 0.3398 (0.2426)	SegCLSLoss 0.0947 (0.0803)	KLLoss 0.0000 (0.0065)	MaskLoss 0.0292 (0.2270)	MaskBCELoss 0.0139 (0.1601)	MaskDICELoss 0.0153 (0.0669)
>> last_hidden_states.shape:  torch.Size([9, 512, 5120])
>> per_token_labels.shape:  torch.Size([9, 512])
>> masks_list[0].shape:  torch.Size([5, 366, 640])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 366, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([9, 512, 5120])
>> per_token_labels.shape:  torch.Size([9, 512])
>> masks_list[0].shape:  torch.Size([5, 366, 640])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 366, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([9, 512, 5120])
>> per_token_labels.shape:  torch.Size([9, 512])
>> masks_list[0].shape:  torch.Size([5, 366, 640])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 366, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([9, 512, 5120])
>> per_token_labels.shape:  torch.Size([9, 512])
>> masks_list[0].shape:  torch.Size([5, 366, 640])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 366, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([9, 512, 5120])
>> per_token_labels.shape:  torch.Size([9, 512])
>> masks_list[0].shape:  torch.Size([5, 366, 640])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 366, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([9, 512, 5120])
>> per_token_labels.shape:  torch.Size([9, 512])
>> masks_list[0].shape:  torch.Size([5, 366, 640])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 366, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 337, 5120])
>> per_token_labels.shape:  torch.Size([18, 337])
>> masks_list[0].shape:  torch.Size([5, 428, 640])
>> masks_list[1].shape:  torch.Size([5, 3120, 4160])
>> gt_mask.shape:  torch.Size([1, 428, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 337, 5120])
>> per_token_labels.shape:  torch.Size([18, 337])
>> masks_list[0].shape:  torch.Size([5, 428, 640])
>> masks_list[1].shape:  torch.Size([5, 3120, 4160])
>> gt_mask.shape:  torch.Size([1, 428, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 512, 683])
>> masks_list[1].shape:  torch.Size([0, 427, 640])
>> gt_mask.shape:  torch.Size([1, 512, 683])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 512, 683])
>> masks_list[1].shape:  torch.Size([0, 427, 640])
>> gt_mask.shape:  torch.Size([1, 512, 683])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 337, 5120])
>> per_token_labels.shape:  torch.Size([20, 337])
>> masks_list[0].shape:  torch.Size([5, 3024, 4032])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 3024, 4032])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 337, 5120])
>> per_token_labels.shape:  torch.Size([20, 337])
>> masks_list[0].shape:  torch.Size([5, 3024, 4032])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 3024, 4032])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 334, 5120])
>> per_token_labels.shape:  torch.Size([20, 334])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([5, 512, 672])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 334, 5120])
>> per_token_labels.shape:  torch.Size([20, 334])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([5, 512, 672])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 334, 5120])
>> per_token_labels.shape:  torch.Size([20, 334])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([5, 512, 672])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 334, 5120])
>> per_token_labels.shape:  torch.Size([20, 334])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([5, 512, 672])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 460, 5120])
>> per_token_labels.shape:  torch.Size([16, 460])
>> masks_list[0].shape:  torch.Size([5, 333, 500])
>> masks_list[1].shape:  torch.Size([0, 640, 480])
>> gt_mask.shape:  torch.Size([1, 333, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 460, 5120])
>> per_token_labels.shape:  torch.Size([16, 460])
>> masks_list[0].shape:  torch.Size([5, 333, 500])
>> masks_list[1].shape:  torch.Size([0, 640, 480])
>> gt_mask.shape:  torch.Size([1, 333, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 342, 5120])
>> per_token_labels.shape:  torch.Size([19, 342])
>> masks_list[0].shape:  torch.Size([5, 480, 360])
>> masks_list[1].shape:  torch.Size([4, 640, 479])
>> gt_mask.shape:  torch.Size([1, 480, 360])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 342, 5120])
>> per_token_labels.shape:  torch.Size([19, 342])
>> masks_list[0].shape:  torch.Size([5, 480, 360])
>> masks_list[1].shape:  torch.Size([4, 640, 479])
>> gt_mask.shape:  torch.Size([1, 480, 360])
>> image_emb:  torch.Size([1, 256, 64, 64])
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([3, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
>> last_hidden_states.shape:  torch.Size([9, 512, 5120])
>> per_token_labels.shape:  torch.Size([9, 512])
>> masks_list[0].shape:  torch.Size([0, 375, 500])
>> masks_list[1].shape:  torch.Size([0, 433, 640])
>> gt_mask.shape:  torch.Size([1, 429, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([9, 512, 5120])
>> per_token_labels.shape:  torch.Size([9, 512])
>> masks_list[0].shape:  torch.Size([0, 375, 500])
>> masks_list[1].shape:  torch.Size([0, 433, 640])
>> gt_mask.shape:  torch.Size([1, 429, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([16, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
>> last_hidden_states.shape:  torch.Size([18, 389, 5120])
>> per_token_labels.shape:  torch.Size([18, 389])
>> masks_list[0].shape:  torch.Size([3, 333, 500])
>> masks_list[1].shape:  torch.Size([5, 2736, 3648])
>> gt_mask.shape:  torch.Size([1, 333, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 389, 5120])
>> per_token_labels.shape:  torch.Size([18, 389])
>> masks_list[0].shape:  torch.Size([3, 333, 500])
>> masks_list[1].shape:  torch.Size([5, 2736, 3648])
>> gt_mask.shape:  torch.Size([1, 333, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([9, 512, 5120])
>> per_token_labels.shape:  torch.Size([9, 512])
>> masks_list[0].shape:  torch.Size([2, 429, 640])
>> masks_list[1].shape:  torch.Size([0, 337, 500])
>> gt_mask.shape:  torch.Size([1, 429, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([9, 512, 5120])
>> per_token_labels.shape:  torch.Size([9, 512])
>> masks_list[0].shape:  torch.Size([2, 429, 640])
>> masks_list[1].shape:  torch.Size([0, 337, 500])
>> gt_mask.shape:  torch.Size([1, 429, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
Epoch: [0][  3/500]	Time 25.479 (25.479)	Loss 3.1302 (1.3894)	CeLoss 0.1357 (0.2503)	SegCLSLoss 0.1040 (0.0992)	KLLoss 0.0249 (0.0131)	MaskLoss 1.4651 (0.5414)	MaskBCELoss 1.3720 (0.4471)	MaskDICELoss 0.0931 (0.0942)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([13, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
>> last_hidden_states.shape:  torch.Size([12, 462, 5120])
>> per_token_labels.shape:  torch.Size([12, 462])
>> masks_list[0].shape:  torch.Size([0, 427, 640])
>> masks_list[1].shape:  torch.Size([4, 486, 640])
>> gt_mask.shape:  torch.Size([1, 486, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 462, 5120])
>> per_token_labels.shape:  torch.Size([12, 462])
>> masks_list[0].shape:  torch.Size([0, 427, 640])
>> masks_list[1].shape:  torch.Size([4, 486, 640])
>> gt_mask.shape:  torch.Size([1, 486, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 332, 5120])
>> per_token_labels.shape:  torch.Size([17, 332])
>> masks_list[0].shape:  torch.Size([5, 360, 640])
>> masks_list[1].shape:  torch.Size([5, 375, 500])
>> gt_mask.shape:  torch.Size([1, 360, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 332, 5120])
>> per_token_labels.shape:  torch.Size([17, 332])
>> masks_list[0].shape:  torch.Size([5, 360, 640])
>> masks_list[1].shape:  torch.Size([5, 375, 500])
>> gt_mask.shape:  torch.Size([1, 360, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 332, 5120])
>> per_token_labels.shape:  torch.Size([17, 332])
>> masks_list[0].shape:  torch.Size([5, 360, 640])
>> masks_list[1].shape:  torch.Size([5, 375, 500])
>> gt_mask.shape:  torch.Size([1, 360, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 332, 5120])
>> per_token_labels.shape:  torch.Size([17, 332])
>> masks_list[0].shape:  torch.Size([5, 360, 640])
>> masks_list[1].shape:  torch.Size([5, 375, 500])
>> gt_mask.shape:  torch.Size([1, 360, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([8, 512, 5120])
>> per_token_labels.shape:  torch.Size([8, 512])
>> masks_list[0].shape:  torch.Size([2, 488, 640])
>> masks_list[1].shape:  torch.Size([0, 640, 480])
>> gt_mask.shape:  torch.Size([1, 488, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([8, 512, 5120])
>> per_token_labels.shape:  torch.Size([8, 512])
>> masks_list[0].shape:  torch.Size([2, 488, 640])
>> masks_list[1].shape:  torch.Size([0, 640, 480])
>> gt_mask.shape:  torch.Size([1, 488, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([10, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
>> last_hidden_states.shape:  torch.Size([20, 402, 5120])
>> per_token_labels.shape:  torch.Size([20, 402])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 427, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 402, 5120])
>> per_token_labels.shape:  torch.Size([20, 402])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 427, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 402, 5120])
>> per_token_labels.shape:  torch.Size([20, 402])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 427, 640])
>> gt_mask.shape:  torch.Size([1, 427, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 402, 5120])
>> per_token_labels.shape:  torch.Size([20, 402])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 427, 640])
>> gt_mask.shape:  torch.Size([1, 427, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([9, 512, 5120])
>> per_token_labels.shape:  torch.Size([9, 512])
>> masks_list[0].shape:  torch.Size([2, 428, 640])
>> masks_list[1].shape:  torch.Size([0, 427, 640])
>> gt_mask.shape:  torch.Size([1, 428, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([9, 512, 5120])
>> per_token_labels.shape:  torch.Size([9, 512])
>> masks_list[0].shape:  torch.Size([2, 428, 640])
>> masks_list[1].shape:  torch.Size([0, 427, 640])
>> gt_mask.shape:  torch.Size([1, 428, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 390, 5120])
>> per_token_labels.shape:  torch.Size([19, 390])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 2848, 4272])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 390, 5120])
>> per_token_labels.shape:  torch.Size([19, 390])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 2848, 4272])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 462, 5120])
>> per_token_labels.shape:  torch.Size([15, 462])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([4, 480, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 462, 5120])
>> per_token_labels.shape:  torch.Size([15, 462])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([4, 480, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 332, 5120])
>> per_token_labels.shape:  torch.Size([17, 332])
>> masks_list[0].shape:  torch.Size([2, 509, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 509, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 332, 5120])
>> per_token_labels.shape:  torch.Size([17, 332])
>> masks_list[0].shape:  torch.Size([2, 509, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 509, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
Epoch: [0][  4/500]	Time 27.620 (27.620)	Loss 0.7019 (1.2521)	CeLoss 0.3555 (0.2248)	SegCLSLoss 0.0957 (0.0798)	KLLoss 0.0000 (0.0108)	MaskLoss 0.1494 (0.4909)	MaskBCELoss 0.0788 (0.4218)	MaskDICELoss 0.0706 (0.0690)
>> last_hidden_states.shape:  torch.Size([18, 335, 5120])
>> per_token_labels.shape:  torch.Size([18, 335])
>> masks_list[0].shape:  torch.Size([5, 360, 480])
>> masks_list[1].shape:  torch.Size([3, 640, 478])
>> gt_mask.shape:  torch.Size([1, 360, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 335, 5120])
>> per_token_labels.shape:  torch.Size([18, 335])
>> masks_list[0].shape:  torch.Size([5, 360, 480])
>> masks_list[1].shape:  torch.Size([3, 640, 478])
>> gt_mask.shape:  torch.Size([1, 360, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 340, 5120])
>> per_token_labels.shape:  torch.Size([17, 340])
>> masks_list[0].shape:  torch.Size([2, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 375, 500])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 340, 5120])
>> per_token_labels.shape:  torch.Size([17, 340])
>> masks_list[0].shape:  torch.Size([2, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 375, 500])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 336, 5120])
>> per_token_labels.shape:  torch.Size([19, 336])
>> masks_list[0].shape:  torch.Size([4, 640, 426])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 640, 426])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 336, 5120])
>> per_token_labels.shape:  torch.Size([19, 336])
>> masks_list[0].shape:  torch.Size([4, 640, 426])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 640, 426])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 499, 5120])
>> per_token_labels.shape:  torch.Size([12, 499])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([0, 390, 640])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 499, 5120])
>> per_token_labels.shape:  torch.Size([12, 499])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([0, 390, 640])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 499, 5120])
>> per_token_labels.shape:  torch.Size([12, 499])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([0, 390, 640])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 499, 5120])
>> per_token_labels.shape:  torch.Size([12, 499])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([0, 390, 640])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([11, 512, 5120])
>> per_token_labels.shape:  torch.Size([11, 512])
>> masks_list[0].shape:  torch.Size([0, 427, 640])
>> masks_list[1].shape:  torch.Size([0, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 427, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([11, 512, 5120])
>> per_token_labels.shape:  torch.Size([11, 512])
>> masks_list[0].shape:  torch.Size([0, 427, 640])
>> masks_list[1].shape:  torch.Size([0, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 427, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 2592, 4608])
>> masks_list[1].shape:  torch.Size([0, 429, 640])
>> gt_mask.shape:  torch.Size([1, 2592, 4608])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 2592, 4608])
>> masks_list[1].shape:  torch.Size([0, 429, 640])
>> gt_mask.shape:  torch.Size([1, 2592, 4608])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 388, 5120])
>> per_token_labels.shape:  torch.Size([20, 388])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 427, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 388, 5120])
>> per_token_labels.shape:  torch.Size([20, 388])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 427, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 478, 640])
>> masks_list[1].shape:  torch.Size([0, 426, 640])
>> gt_mask.shape:  torch.Size([1, 478, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 478, 640])
>> masks_list[1].shape:  torch.Size([0, 426, 640])
>> gt_mask.shape:  torch.Size([1, 478, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([4, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][  5/500]	Time 28.419 (28.419)	Loss 2.9412 (1.5093)	CeLoss 0.2324 (0.2354)	SegCLSLoss 0.1030 (0.0892)	KLLoss 0.0176 (0.0140)	MaskLoss 1.3237 (0.6112)	MaskBCELoss 1.2477 (0.5350)	MaskDICELoss 0.0761 (0.0762)
>> last_hidden_states.shape:  torch.Size([16, 418, 5120])
>> per_token_labels.shape:  torch.Size([16, 418])
>> masks_list[0].shape:  torch.Size([5, 426, 640])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 426, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 418, 5120])
>> per_token_labels.shape:  torch.Size([16, 418])
>> masks_list[0].shape:  torch.Size([5, 426, 640])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 426, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 437, 5120])
>> per_token_labels.shape:  torch.Size([15, 437])
>> masks_list[0].shape:  torch.Size([5, 374, 500])
>> masks_list[1].shape:  torch.Size([0, 500, 375])
>> gt_mask.shape:  torch.Size([1, 374, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 437, 5120])
>> per_token_labels.shape:  torch.Size([15, 437])
>> masks_list[0].shape:  torch.Size([5, 374, 500])
>> masks_list[1].shape:  torch.Size([0, 500, 375])
>> gt_mask.shape:  torch.Size([1, 374, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 469, 5120])
>> per_token_labels.shape:  torch.Size([16, 469])
>> masks_list[0].shape:  torch.Size([5, 512, 637])
>> masks_list[1].shape:  torch.Size([5, 500, 332])
>> gt_mask.shape:  torch.Size([1, 512, 637])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 469, 5120])
>> per_token_labels.shape:  torch.Size([16, 469])
>> masks_list[0].shape:  torch.Size([5, 512, 637])
>> masks_list[1].shape:  torch.Size([5, 500, 332])
>> gt_mask.shape:  torch.Size([1, 512, 637])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 457, 5120])
>> per_token_labels.shape:  torch.Size([16, 457])
>> masks_list[0].shape:  torch.Size([5, 640, 480])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 640, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 457, 5120])
>> per_token_labels.shape:  torch.Size([16, 457])
>> masks_list[0].shape:  torch.Size([5, 640, 480])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 640, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 457, 5120])
>> per_token_labels.shape:  torch.Size([16, 457])
>> masks_list[0].shape:  torch.Size([5, 640, 480])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 640, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 457, 5120])
>> per_token_labels.shape:  torch.Size([16, 457])
>> masks_list[0].shape:  torch.Size([5, 640, 480])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 640, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 512, 5120])
>> per_token_labels.shape:  torch.Size([14, 512])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 425, 640])
>> gt_mask.shape:  torch.Size([1, 425, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 512, 5120])
>> per_token_labels.shape:  torch.Size([14, 512])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 425, 640])
>> gt_mask.shape:  torch.Size([1, 425, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([11, 456, 5120])
>> per_token_labels.shape:  torch.Size([11, 456])
>> masks_list[0].shape:  torch.Size([0, 427, 640])
>> masks_list[1].shape:  torch.Size([5, 640, 426])
>> gt_mask.shape:  torch.Size([1, 640, 426])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([11, 456, 5120])
>> per_token_labels.shape:  torch.Size([11, 456])
>> masks_list[0].shape:  torch.Size([0, 427, 640])
>> masks_list[1].shape:  torch.Size([5, 640, 426])
>> gt_mask.shape:  torch.Size([1, 640, 426])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 356, 5120])
>> per_token_labels.shape:  torch.Size([17, 356])
>> masks_list[0].shape:  torch.Size([5, 512, 683])
>> masks_list[1].shape:  torch.Size([5, 1080, 1920])
>> gt_mask.shape:  torch.Size([1, 512, 683])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 356, 5120])
>> per_token_labels.shape:  torch.Size([17, 356])
>> masks_list[0].shape:  torch.Size([5, 512, 683])
>> masks_list[1].shape:  torch.Size([5, 1080, 1920])
>> gt_mask.shape:  torch.Size([1, 512, 683])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 470, 5120])
>> per_token_labels.shape:  torch.Size([13, 470])
>> masks_list[0].shape:  torch.Size([0, 640, 458])
>> masks_list[1].shape:  torch.Size([5, 500, 333])
>> gt_mask.shape:  torch.Size([1, 500, 333])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 470, 5120])
>> per_token_labels.shape:  torch.Size([13, 470])
>> masks_list[0].shape:  torch.Size([0, 640, 458])
>> masks_list[1].shape:  torch.Size([5, 500, 333])
>> gt_mask.shape:  torch.Size([1, 500, 333])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 470, 5120])
>> per_token_labels.shape:  torch.Size([13, 470])
>> masks_list[0].shape:  torch.Size([0, 640, 458])
>> masks_list[1].shape:  torch.Size([5, 500, 333])
>> gt_mask.shape:  torch.Size([1, 500, 333])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 470, 5120])
>> per_token_labels.shape:  torch.Size([13, 470])
>> masks_list[0].shape:  torch.Size([0, 640, 458])
>> masks_list[1].shape:  torch.Size([5, 500, 333])
>> gt_mask.shape:  torch.Size([1, 500, 333])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 470, 5120])
>> per_token_labels.shape:  torch.Size([13, 470])
>> masks_list[0].shape:  torch.Size([0, 640, 458])
>> masks_list[1].shape:  torch.Size([5, 500, 333])
>> gt_mask.shape:  torch.Size([1, 500, 333])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 470, 5120])
>> per_token_labels.shape:  torch.Size([13, 470])
>> masks_list[0].shape:  torch.Size([0, 640, 458])
>> masks_list[1].shape:  torch.Size([5, 500, 333])
>> gt_mask.shape:  torch.Size([1, 500, 333])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 470, 5120])
>> per_token_labels.shape:  torch.Size([13, 470])
>> masks_list[0].shape:  torch.Size([0, 640, 458])
>> masks_list[1].shape:  torch.Size([5, 500, 333])
>> gt_mask.shape:  torch.Size([1, 500, 333])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 470, 5120])
>> per_token_labels.shape:  torch.Size([13, 470])
>> masks_list[0].shape:  torch.Size([0, 640, 458])
>> masks_list[1].shape:  torch.Size([5, 500, 333])
>> gt_mask.shape:  torch.Size([1, 500, 333])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 470, 5120])
>> per_token_labels.shape:  torch.Size([13, 470])
>> masks_list[0].shape:  torch.Size([0, 640, 458])
>> masks_list[1].shape:  torch.Size([5, 500, 333])
>> gt_mask.shape:  torch.Size([1, 500, 333])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 470, 5120])
>> per_token_labels.shape:  torch.Size([13, 470])
>> masks_list[0].shape:  torch.Size([0, 640, 458])
>> masks_list[1].shape:  torch.Size([5, 500, 333])
>> gt_mask.shape:  torch.Size([1, 500, 333])
>> image_emb:  torch.Size([1, 256, 64, 64])
Epoch: [0][  6/500]	Time 29.116 (29.116)	Loss 0.5354 (0.8788)	CeLoss 0.2295 (0.2316)	SegCLSLoss 0.1030 (0.0918)	KLLoss 0.0000 (0.0100)	MaskLoss 0.1271 (0.2980)	MaskBCELoss 0.0285 (0.2209)	MaskDICELoss 0.0986 (0.0770)
>> last_hidden_states.shape:  torch.Size([16, 500, 5120])
>> per_token_labels.shape:  torch.Size([16, 500])
>> masks_list[0].shape:  torch.Size([5, 3024, 4032])
>> masks_list[1].shape:  torch.Size([0, 427, 640])
>> gt_mask.shape:  torch.Size([1, 3024, 4032])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 500, 5120])
>> per_token_labels.shape:  torch.Size([16, 500])
>> masks_list[0].shape:  torch.Size([5, 3024, 4032])
>> masks_list[1].shape:  torch.Size([0, 427, 640])
>> gt_mask.shape:  torch.Size([1, 3024, 4032])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 640, 480])
>> masks_list[1].shape:  torch.Size([5, 360, 480])
>> gt_mask.shape:  torch.Size([1, 640, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 640, 480])
>> masks_list[1].shape:  torch.Size([5, 360, 480])
>> gt_mask.shape:  torch.Size([1, 640, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 640, 480])
>> masks_list[1].shape:  torch.Size([5, 360, 480])
>> gt_mask.shape:  torch.Size([1, 640, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 640, 480])
>> masks_list[1].shape:  torch.Size([5, 360, 480])
>> gt_mask.shape:  torch.Size([1, 640, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 448, 5120])
>> per_token_labels.shape:  torch.Size([16, 448])
>> masks_list[0].shape:  torch.Size([5, 204, 640])
>> masks_list[1].shape:  torch.Size([5, 640, 480])
>> gt_mask.shape:  torch.Size([1, 204, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 448, 5120])
>> per_token_labels.shape:  torch.Size([16, 448])
>> masks_list[0].shape:  torch.Size([5, 204, 640])
>> masks_list[1].shape:  torch.Size([5, 640, 480])
>> gt_mask.shape:  torch.Size([1, 204, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([0, 427, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([0, 427, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([11, 476, 5120])
>> per_token_labels.shape:  torch.Size([11, 476])
>> masks_list[0].shape:  torch.Size([5, 3024, 4032])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 3024, 4032])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([11, 476, 5120])
>> per_token_labels.shape:  torch.Size([11, 476])
>> masks_list[0].shape:  torch.Size([5, 3024, 4032])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 3024, 4032])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 440, 5120])
>> per_token_labels.shape:  torch.Size([12, 440])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([0, 413, 640])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 440, 5120])
>> per_token_labels.shape:  torch.Size([12, 440])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([0, 413, 640])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 450, 5120])
>> per_token_labels.shape:  torch.Size([19, 450])
>> masks_list[0].shape:  torch.Size([5, 500, 281])
>> masks_list[1].shape:  torch.Size([4, 333, 500])
>> gt_mask.shape:  torch.Size([1, 500, 281])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 450, 5120])
>> per_token_labels.shape:  torch.Size([19, 450])
>> masks_list[0].shape:  torch.Size([5, 500, 281])
>> masks_list[1].shape:  torch.Size([4, 333, 500])
>> gt_mask.shape:  torch.Size([1, 500, 281])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 331, 5120])
>> per_token_labels.shape:  torch.Size([18, 331])
>> masks_list[0].shape:  torch.Size([5, 333, 500])
>> masks_list[1].shape:  torch.Size([4, 320, 480])
>> gt_mask.shape:  torch.Size([1, 333, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 331, 5120])
>> per_token_labels.shape:  torch.Size([18, 331])
>> masks_list[0].shape:  torch.Size([5, 333, 500])
>> masks_list[1].shape:  torch.Size([4, 320, 480])
>> gt_mask.shape:  torch.Size([1, 333, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 336, 5120])
>> per_token_labels.shape:  torch.Size([20, 336])
>> masks_list[0].shape:  torch.Size([5, 428, 640])
>> masks_list[1].shape:  torch.Size([5, 478, 640])
>> gt_mask.shape:  torch.Size([1, 428, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 336, 5120])
>> per_token_labels.shape:  torch.Size([20, 336])
>> masks_list[0].shape:  torch.Size([5, 428, 640])
>> masks_list[1].shape:  torch.Size([5, 478, 640])
>> gt_mask.shape:  torch.Size([1, 428, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 394, 5120])
>> per_token_labels.shape:  torch.Size([20, 394])
>> masks_list[0].shape:  torch.Size([5, 512, 690])
>> masks_list[1].shape:  torch.Size([5, 375, 500])
>> gt_mask.shape:  torch.Size([1, 512, 690])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 394, 5120])
>> per_token_labels.shape:  torch.Size([20, 394])
>> masks_list[0].shape:  torch.Size([5, 512, 690])
>> masks_list[1].shape:  torch.Size([5, 375, 500])
>> gt_mask.shape:  torch.Size([1, 512, 690])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 394, 5120])
>> per_token_labels.shape:  torch.Size([20, 394])
>> masks_list[0].shape:  torch.Size([5, 512, 690])
>> masks_list[1].shape:  torch.Size([5, 375, 500])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 394, 5120])
>> per_token_labels.shape:  torch.Size([20, 394])
>> masks_list[0].shape:  torch.Size([5, 512, 690])
>> masks_list[1].shape:  torch.Size([5, 375, 500])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
Epoch: [0][  7/500]	Time 28.941 (28.941)	Loss 0.5219 (0.9850)	CeLoss 0.2471 (0.2363)	SegCLSLoss 0.1016 (0.1000)	KLLoss 0.0000 (0.0083)	MaskLoss 0.1125 (0.3472)	MaskBCELoss 0.0201 (0.2528)	MaskDICELoss 0.0924 (0.0944)
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([0, 612, 612])
>> masks_list[1].shape:  torch.Size([5, 523, 640])
>> gt_mask.shape:  torch.Size([1, 523, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([0, 612, 612])
>> masks_list[1].shape:  torch.Size([5, 523, 640])
>> gt_mask.shape:  torch.Size([1, 523, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([0, 612, 612])
>> masks_list[1].shape:  torch.Size([5, 523, 640])
>> gt_mask.shape:  torch.Size([1, 523, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([0, 612, 612])
>> masks_list[1].shape:  torch.Size([5, 523, 640])
>> gt_mask.shape:  torch.Size([1, 523, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([0, 612, 612])
>> masks_list[1].shape:  torch.Size([5, 523, 640])
>> gt_mask.shape:  torch.Size([1, 523, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([0, 612, 612])
>> masks_list[1].shape:  torch.Size([5, 523, 640])
>> gt_mask.shape:  torch.Size([1, 523, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([0, 612, 612])
>> masks_list[1].shape:  torch.Size([5, 523, 640])
>> gt_mask.shape:  torch.Size([1, 523, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([0, 612, 612])
>> masks_list[1].shape:  torch.Size([5, 523, 640])
>> gt_mask.shape:  torch.Size([1, 523, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([0, 612, 612])
>> masks_list[1].shape:  torch.Size([5, 523, 640])
>> gt_mask.shape:  torch.Size([1, 523, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([0, 612, 612])
>> masks_list[1].shape:  torch.Size([5, 523, 640])
>> gt_mask.shape:  torch.Size([1, 523, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 468, 5120])
>> per_token_labels.shape:  torch.Size([16, 468])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 360, 480])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 468, 5120])
>> per_token_labels.shape:  torch.Size([16, 468])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 360, 480])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([0, 427, 640])
>> masks_list[1].shape:  torch.Size([5, 395, 490])
>> gt_mask.shape:  torch.Size([1, 395, 490])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([0, 427, 640])
>> masks_list[1].shape:  torch.Size([5, 395, 490])
>> gt_mask.shape:  torch.Size([1, 395, 490])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 335, 5120])
>> per_token_labels.shape:  torch.Size([18, 335])
>> masks_list[0].shape:  torch.Size([3, 512, 683])
>> masks_list[1].shape:  torch.Size([5, 500, 375])
>> gt_mask.shape:  torch.Size([1, 512, 683])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 335, 5120])
>> per_token_labels.shape:  torch.Size([18, 335])
>> masks_list[0].shape:  torch.Size([3, 512, 683])
>> masks_list[1].shape:  torch.Size([5, 500, 375])
>> gt_mask.shape:  torch.Size([1, 512, 683])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 512, 5120])
>> per_token_labels.shape:  torch.Size([15, 512])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([4, 373, 500])
>> gt_mask.shape:  torch.Size([1, 373, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 512, 5120])
>> per_token_labels.shape:  torch.Size([15, 512])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([4, 373, 500])
>> gt_mask.shape:  torch.Size([1, 373, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 512, 5120])
>> per_token_labels.shape:  torch.Size([15, 512])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([4, 373, 500])
>> gt_mask.shape:  torch.Size([1, 373, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 512, 5120])
>> per_token_labels.shape:  torch.Size([15, 512])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([4, 373, 500])
>> gt_mask.shape:  torch.Size([1, 373, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 512, 5120])
>> per_token_labels.shape:  torch.Size([15, 512])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([4, 373, 500])
>> gt_mask.shape:  torch.Size([1, 373, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 512, 5120])
>> per_token_labels.shape:  torch.Size([15, 512])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([4, 373, 500])
>> gt_mask.shape:  torch.Size([1, 373, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 512, 5120])
>> per_token_labels.shape:  torch.Size([15, 512])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([4, 373, 500])
>> gt_mask.shape:  torch.Size([1, 373, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 512, 5120])
>> per_token_labels.shape:  torch.Size([15, 512])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([4, 373, 500])
>> gt_mask.shape:  torch.Size([1, 373, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 471, 5120])
>> per_token_labels.shape:  torch.Size([16, 471])
>> masks_list[0].shape:  torch.Size([2, 375, 500])
>> masks_list[1].shape:  torch.Size([0, 1920, 2560])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 471, 5120])
>> per_token_labels.shape:  torch.Size([16, 471])
>> masks_list[0].shape:  torch.Size([2, 375, 500])
>> masks_list[1].shape:  torch.Size([0, 1920, 2560])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 471, 5120])
>> per_token_labels.shape:  torch.Size([16, 471])
>> masks_list[0].shape:  torch.Size([2, 375, 500])
>> masks_list[1].shape:  torch.Size([0, 1920, 2560])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 471, 5120])
>> per_token_labels.shape:  torch.Size([16, 471])
>> masks_list[0].shape:  torch.Size([2, 375, 500])
>> masks_list[1].shape:  torch.Size([0, 1920, 2560])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([9, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
>> last_hidden_states.shape:  torch.Size([20, 459, 5120])
>> per_token_labels.shape:  torch.Size([20, 459])
>> masks_list[0].shape:  torch.Size([5, 1524, 2704])
>> masks_list[1].shape:  torch.Size([0, 1181, 1754])
>> gt_mask.shape:  torch.Size([1, 1524, 2704])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 459, 5120])
>> per_token_labels.shape:  torch.Size([20, 459])
>> masks_list[0].shape:  torch.Size([5, 1524, 2704])
>> masks_list[1].shape:  torch.Size([0, 1181, 1754])
>> gt_mask.shape:  torch.Size([1, 1524, 2704])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 458, 5120])
>> per_token_labels.shape:  torch.Size([16, 458])
>> masks_list[0].shape:  torch.Size([5, 625, 640])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 625, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 458, 5120])
>> per_token_labels.shape:  torch.Size([16, 458])
>> masks_list[0].shape:  torch.Size([5, 625, 640])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 625, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([0, 640, 536])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([0, 640, 536])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
Epoch: [0][  8/500]	Time 32.504 (32.504)	Loss 0.5821 (0.6245)	CeLoss 0.2520 (0.2082)	SegCLSLoss 0.1025 (0.0912)	KLLoss 0.0222 (0.0086)	MaskLoss 0.1340 (0.1833)	MaskBCELoss 0.0345 (0.1012)	MaskDICELoss 0.0995 (0.0821)
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 360, 480])
>> masks_list[1].shape:  torch.Size([5, 360, 480])
>> gt_mask.shape:  torch.Size([1, 360, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 360, 480])
>> masks_list[1].shape:  torch.Size([5, 360, 480])
>> gt_mask.shape:  torch.Size([1, 360, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 512, 5120])
>> per_token_labels.shape:  torch.Size([15, 512])
>> masks_list[0].shape:  torch.Size([0, 360, 640])
>> masks_list[1].shape:  torch.Size([5, 333, 500])
>> gt_mask.shape:  torch.Size([1, 333, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 512, 5120])
>> per_token_labels.shape:  torch.Size([15, 512])
>> masks_list[0].shape:  torch.Size([0, 360, 640])
>> masks_list[1].shape:  torch.Size([5, 333, 500])
>> gt_mask.shape:  torch.Size([1, 333, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([9, 512, 5120])
>> per_token_labels.shape:  torch.Size([9, 512])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([2, 640, 428])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([9, 512, 5120])
>> per_token_labels.shape:  torch.Size([9, 512])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([2, 640, 428])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([0, 428, 640])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([0, 428, 640])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 343, 5120])
>> per_token_labels.shape:  torch.Size([20, 343])
>> masks_list[0].shape:  torch.Size([5, 3024, 4032])
>> masks_list[1].shape:  torch.Size([5, 473, 640])
>> gt_mask.shape:  torch.Size([1, 3024, 4032])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 343, 5120])
>> per_token_labels.shape:  torch.Size([20, 343])
>> masks_list[0].shape:  torch.Size([5, 3024, 4032])
>> masks_list[1].shape:  torch.Size([5, 473, 640])
>> gt_mask.shape:  torch.Size([1, 3024, 4032])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 330, 5120])
>> per_token_labels.shape:  torch.Size([20, 330])
>> masks_list[0].shape:  torch.Size([5, 229, 304])
>> masks_list[1].shape:  torch.Size([5, 512, 811])
>> gt_mask.shape:  torch.Size([1, 229, 304])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 330, 5120])
>> per_token_labels.shape:  torch.Size([20, 330])
>> masks_list[0].shape:  torch.Size([5, 229, 304])
>> masks_list[1].shape:  torch.Size([5, 512, 811])
>> gt_mask.shape:  torch.Size([1, 229, 304])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 500, 333])
>> masks_list[1].shape:  torch.Size([0, 375, 500])
>> gt_mask.shape:  torch.Size([1, 500, 333])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 500, 333])
>> masks_list[1].shape:  torch.Size([0, 375, 500])
>> gt_mask.shape:  torch.Size([1, 500, 333])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 392, 5120])
>> per_token_labels.shape:  torch.Size([16, 392])
>> masks_list[0].shape:  torch.Size([2, 640, 640])
>> masks_list[1].shape:  torch.Size([5, 425, 640])
>> gt_mask.shape:  torch.Size([1, 640, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 392, 5120])
>> per_token_labels.shape:  torch.Size([16, 392])
>> masks_list[0].shape:  torch.Size([2, 640, 640])
>> masks_list[1].shape:  torch.Size([5, 425, 640])
>> gt_mask.shape:  torch.Size([1, 640, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([12, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([5, 332, 500])
>> masks_list[1].shape:  torch.Size([0, 427, 640])
>> gt_mask.shape:  torch.Size([1, 332, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([5, 332, 500])
>> masks_list[1].shape:  torch.Size([0, 427, 640])
>> gt_mask.shape:  torch.Size([1, 332, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 335, 5120])
>> per_token_labels.shape:  torch.Size([17, 335])
>> masks_list[0].shape:  torch.Size([5, 640, 480])
>> masks_list[1].shape:  torch.Size([5, 640, 640])
>> gt_mask.shape:  torch.Size([1, 640, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 335, 5120])
>> per_token_labels.shape:  torch.Size([17, 335])
>> masks_list[0].shape:  torch.Size([5, 640, 480])
>> masks_list[1].shape:  torch.Size([5, 640, 640])
>> gt_mask.shape:  torch.Size([1, 640, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
Epoch: [0][  9/500]	Time 27.793 (27.793)	Loss 0.6501 (0.7326)	CeLoss 0.3652 (0.2391)	SegCLSLoss 0.0947 (0.0989)	KLLoss 0.0000 (0.0132)	MaskLoss 0.1188 (0.2188)	MaskBCELoss 0.0260 (0.1314)	MaskDICELoss 0.0928 (0.0874)
>> last_hidden_states.shape:  torch.Size([16, 497, 5120])
>> per_token_labels.shape:  torch.Size([16, 497])
>> masks_list[0].shape:  torch.Size([5, 320, 480])
>> masks_list[1].shape:  torch.Size([5, 333, 500])
>> gt_mask.shape:  torch.Size([1, 320, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 497, 5120])
>> per_token_labels.shape:  torch.Size([16, 497])
>> masks_list[0].shape:  torch.Size([5, 320, 480])
>> masks_list[1].shape:  torch.Size([5, 333, 500])
>> gt_mask.shape:  torch.Size([1, 320, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 464, 5120])
>> per_token_labels.shape:  torch.Size([16, 464])
>> masks_list[0].shape:  torch.Size([5, 3024, 4032])
>> masks_list[1].shape:  torch.Size([5, 427, 640])
>> gt_mask.shape:  torch.Size([1, 3024, 4032])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 464, 5120])
>> per_token_labels.shape:  torch.Size([16, 464])
>> masks_list[0].shape:  torch.Size([5, 3024, 4032])
>> masks_list[1].shape:  torch.Size([5, 427, 640])
>> gt_mask.shape:  torch.Size([1, 3024, 4032])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 334, 5120])
>> per_token_labels.shape:  torch.Size([17, 334])
>> masks_list[0].shape:  torch.Size([4, 290, 500])
>> masks_list[1].shape:  torch.Size([3, 375, 500])
>> gt_mask.shape:  torch.Size([1, 290, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 334, 5120])
>> per_token_labels.shape:  torch.Size([17, 334])
>> masks_list[0].shape:  torch.Size([4, 290, 500])
>> masks_list[1].shape:  torch.Size([3, 375, 500])
>> gt_mask.shape:  torch.Size([1, 290, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 409, 5120])
>> per_token_labels.shape:  torch.Size([18, 409])
>> masks_list[0].shape:  torch.Size([3, 426, 640])
>> masks_list[1].shape:  torch.Size([5, 1704, 1372])
>> gt_mask.shape:  torch.Size([1, 426, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 409, 5120])
>> per_token_labels.shape:  torch.Size([18, 409])
>> masks_list[0].shape:  torch.Size([3, 426, 640])
>> masks_list[1].shape:  torch.Size([5, 1704, 1372])
>> gt_mask.shape:  torch.Size([1, 426, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 335, 5120])
>> per_token_labels.shape:  torch.Size([20, 335])
>> masks_list[0].shape:  torch.Size([5, 2988, 3984])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 2988, 3984])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 335, 5120])
>> per_token_labels.shape:  torch.Size([20, 335])
>> masks_list[0].shape:  torch.Size([5, 2988, 3984])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 2988, 3984])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 335, 5120])
>> per_token_labels.shape:  torch.Size([20, 335])
>> masks_list[0].shape:  torch.Size([5, 2988, 3984])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 2988, 3984])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 335, 5120])
>> per_token_labels.shape:  torch.Size([20, 335])
>> masks_list[0].shape:  torch.Size([5, 2988, 3984])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 2988, 3984])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 391, 5120])
>> per_token_labels.shape:  torch.Size([20, 391])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 3456, 2592])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 391, 5120])
>> per_token_labels.shape:  torch.Size([20, 391])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 3456, 2592])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 391, 5120])
>> per_token_labels.shape:  torch.Size([20, 391])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 3456, 2592])
>> gt_mask.shape:  torch.Size([1, 333, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 391, 5120])
>> per_token_labels.shape:  torch.Size([20, 391])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 3456, 2592])
>> gt_mask.shape:  torch.Size([1, 333, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 391, 5120])
>> per_token_labels.shape:  torch.Size([20, 391])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 3456, 2592])
>> gt_mask.shape:  torch.Size([1, 333, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 391, 5120])
>> per_token_labels.shape:  torch.Size([20, 391])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 3456, 2592])
>> gt_mask.shape:  torch.Size([1, 333, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 391, 5120])
>> per_token_labels.shape:  torch.Size([20, 391])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 3456, 2592])
>> gt_mask.shape:  torch.Size([1, 333, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 391, 5120])
>> per_token_labels.shape:  torch.Size([20, 391])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 3456, 2592])
>> gt_mask.shape:  torch.Size([1, 333, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 391, 5120])
>> per_token_labels.shape:  torch.Size([20, 391])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 3456, 2592])
>> gt_mask.shape:  torch.Size([1, 333, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 391, 5120])
>> per_token_labels.shape:  torch.Size([20, 391])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 3456, 2592])
>> gt_mask.shape:  torch.Size([1, 333, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 391, 5120])
>> per_token_labels.shape:  torch.Size([20, 391])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 3456, 2592])
>> gt_mask.shape:  torch.Size([1, 333, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 391, 5120])
>> per_token_labels.shape:  torch.Size([20, 391])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 3456, 2592])
>> gt_mask.shape:  torch.Size([1, 333, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 442, 5120])
>> per_token_labels.shape:  torch.Size([16, 442])
>> masks_list[0].shape:  torch.Size([5, 512, 619])
>> masks_list[1].shape:  torch.Size([5, 640, 566])
>> gt_mask.shape:  torch.Size([1, 512, 619])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 442, 5120])
>> per_token_labels.shape:  torch.Size([16, 442])
>> masks_list[0].shape:  torch.Size([5, 512, 619])
>> masks_list[1].shape:  torch.Size([5, 640, 566])
>> gt_mask.shape:  torch.Size([1, 512, 619])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 337, 5120])
>> per_token_labels.shape:  torch.Size([17, 337])
>> masks_list[0].shape:  torch.Size([5, 500, 375])
>> masks_list[1].shape:  torch.Size([3, 640, 640])
>> gt_mask.shape:  torch.Size([1, 500, 375])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 337, 5120])
>> per_token_labels.shape:  torch.Size([17, 337])
>> masks_list[0].shape:  torch.Size([5, 500, 375])
>> masks_list[1].shape:  torch.Size([3, 640, 640])
>> gt_mask.shape:  torch.Size([1, 500, 375])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 451, 5120])
>> per_token_labels.shape:  torch.Size([12, 451])
>> masks_list[0].shape:  torch.Size([5, 428, 640])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 428, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 451, 5120])
>> per_token_labels.shape:  torch.Size([12, 451])
>> masks_list[0].shape:  torch.Size([5, 428, 640])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 428, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
Epoch: [0][ 10/500]	Time 28.220 (28.220)	Loss 2.7846 (0.7522)	CeLoss 0.2012 (0.2422)	SegCLSLoss 0.1016 (0.0891)	KLLoss 0.0217 (0.0059)	MaskLoss 1.2603 (0.2313)	MaskBCELoss 1.1914 (0.1590)	MaskDICELoss 0.0689 (0.0723)
>> last_hidden_states.shape:  torch.Size([19, 483, 5120])
>> per_token_labels.shape:  torch.Size([19, 483])
>> masks_list[0].shape:  torch.Size([5, 406, 500])
>> masks_list[1].shape:  torch.Size([4, 480, 640])
>> gt_mask.shape:  torch.Size([1, 406, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 483, 5120])
>> per_token_labels.shape:  torch.Size([19, 483])
>> masks_list[0].shape:  torch.Size([5, 406, 500])
>> masks_list[1].shape:  torch.Size([4, 480, 640])
>> gt_mask.shape:  torch.Size([1, 406, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 335, 5120])
>> per_token_labels.shape:  torch.Size([18, 335])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 416, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 335, 5120])
>> per_token_labels.shape:  torch.Size([18, 335])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 416, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 370, 5120])
>> per_token_labels.shape:  torch.Size([19, 370])
>> masks_list[0].shape:  torch.Size([5, 235, 312])
>> masks_list[1].shape:  torch.Size([0, 3456, 5184])
>> gt_mask.shape:  torch.Size([1, 235, 312])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 370, 5120])
>> per_token_labels.shape:  torch.Size([19, 370])
>> masks_list[0].shape:  torch.Size([5, 235, 312])
>> masks_list[1].shape:  torch.Size([0, 3456, 5184])
>> gt_mask.shape:  torch.Size([1, 235, 312])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 350, 5120])
>> per_token_labels.shape:  torch.Size([17, 350])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 350, 5120])
>> per_token_labels.shape:  torch.Size([17, 350])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 336, 5120])
>> per_token_labels.shape:  torch.Size([17, 336])
>> masks_list[0].shape:  torch.Size([5, 242, 500])
>> masks_list[1].shape:  torch.Size([5, 375, 500])
>> gt_mask.shape:  torch.Size([1, 242, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 336, 5120])
>> per_token_labels.shape:  torch.Size([17, 336])
>> masks_list[0].shape:  torch.Size([5, 242, 500])
>> masks_list[1].shape:  torch.Size([5, 375, 500])
>> gt_mask.shape:  torch.Size([1, 242, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 378, 5120])
>> per_token_labels.shape:  torch.Size([17, 378])
>> masks_list[0].shape:  torch.Size([2, 360, 480])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 360, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 378, 5120])
>> per_token_labels.shape:  torch.Size([17, 378])
>> masks_list[0].shape:  torch.Size([2, 360, 480])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 360, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([9, 512, 5120])
>> per_token_labels.shape:  torch.Size([9, 512])
>> masks_list[0].shape:  torch.Size([2, 640, 440])
>> masks_list[1].shape:  torch.Size([5, 375, 500])
>> gt_mask.shape:  torch.Size([1, 640, 440])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([9, 512, 5120])
>> per_token_labels.shape:  torch.Size([9, 512])
>> masks_list[0].shape:  torch.Size([2, 640, 440])
>> masks_list[1].shape:  torch.Size([5, 375, 500])
>> gt_mask.shape:  torch.Size([1, 640, 440])
>> image_emb:  torch.Size([1, 256, 64, 64])
Epoch: [0][ 11/500]	Time 27.561 (27.561)	Loss 0.4601 (0.5001)	CeLoss 0.1660 (0.2323)	SegCLSLoss 0.1016 (0.0683)	KLLoss 0.0000 (0.0056)	MaskLoss 0.1222 (0.1156)	MaskBCELoss 0.0290 (0.0525)	MaskDICELoss 0.0932 (0.0630)
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 360, 480])
>> masks_list[1].shape:  torch.Size([5, 640, 430])
>> gt_mask.shape:  torch.Size([1, 360, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 360, 480])
>> masks_list[1].shape:  torch.Size([5, 640, 430])
>> gt_mask.shape:  torch.Size([1, 360, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([10, 512, 5120])
>> per_token_labels.shape:  torch.Size([10, 512])
>> masks_list[0].shape:  torch.Size([0, 360, 640])
>> masks_list[1].shape:  torch.Size([3, 427, 640])
>> gt_mask.shape:  torch.Size([1, 427, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([10, 512, 5120])
>> per_token_labels.shape:  torch.Size([10, 512])
>> masks_list[0].shape:  torch.Size([0, 360, 640])
>> masks_list[1].shape:  torch.Size([3, 427, 640])
>> gt_mask.shape:  torch.Size([1, 427, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 342, 5120])
>> per_token_labels.shape:  torch.Size([17, 342])
>> masks_list[0].shape:  torch.Size([5, 427, 640])
>> masks_list[1].shape:  torch.Size([2, 480, 640])
>> gt_mask.shape:  torch.Size([1, 427, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 342, 5120])
>> per_token_labels.shape:  torch.Size([17, 342])
>> masks_list[0].shape:  torch.Size([5, 427, 640])
>> masks_list[1].shape:  torch.Size([2, 480, 640])
>> gt_mask.shape:  torch.Size([1, 427, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 334, 5120])
>> per_token_labels.shape:  torch.Size([17, 334])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([5, 480, 360])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 334, 5120])
>> per_token_labels.shape:  torch.Size([17, 334])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([5, 480, 360])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 339, 5120])
>> per_token_labels.shape:  torch.Size([18, 339])
>> masks_list[0].shape:  torch.Size([5, 500, 500])
>> masks_list[1].shape:  torch.Size([3, 368, 500])
>> gt_mask.shape:  torch.Size([1, 500, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 339, 5120])
>> per_token_labels.shape:  torch.Size([18, 339])
>> masks_list[0].shape:  torch.Size([5, 500, 500])
>> masks_list[1].shape:  torch.Size([3, 368, 500])
>> gt_mask.shape:  torch.Size([1, 500, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([0, 427, 640])
>> masks_list[1].shape:  torch.Size([5, 640, 427])
>> gt_mask.shape:  torch.Size([1, 640, 427])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([0, 427, 640])
>> masks_list[1].shape:  torch.Size([5, 640, 427])
>> gt_mask.shape:  torch.Size([1, 640, 427])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 640, 428])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 640, 428])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 640, 428])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 640, 428])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 346, 5120])
>> per_token_labels.shape:  torch.Size([14, 346])
>> masks_list[0].shape:  torch.Size([2, 473, 640])
>> masks_list[1].shape:  torch.Size([5, 429, 640])
>> gt_mask.shape:  torch.Size([1, 473, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 346, 5120])
>> per_token_labels.shape:  torch.Size([14, 346])
>> masks_list[0].shape:  torch.Size([2, 473, 640])
>> masks_list[1].shape:  torch.Size([5, 429, 640])
>> gt_mask.shape:  torch.Size([1, 473, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
Epoch: [0][ 12/500]	Time 24.256 (24.256)	Loss 0.6573 (0.5798)	CeLoss 0.2910 (0.2374)	SegCLSLoss 0.0957 (0.0792)	KLLoss 0.0205 (0.0072)	MaskLoss 0.1536 (0.1495)	MaskBCELoss 0.0913 (0.0767)	MaskDICELoss 0.0624 (0.0728)
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 360, 480])
>> gt_mask.shape:  torch.Size([1, 360, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 360, 480])
>> gt_mask.shape:  torch.Size([1, 360, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 412, 5120])
>> per_token_labels.shape:  torch.Size([14, 412])
>> masks_list[0].shape:  torch.Size([0, 500, 325])
>> masks_list[1].shape:  torch.Size([5, 2592, 4608])
>> gt_mask.shape:  torch.Size([1, 2592, 4608])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 412, 5120])
>> per_token_labels.shape:  torch.Size([14, 412])
>> masks_list[0].shape:  torch.Size([0, 500, 325])
>> masks_list[1].shape:  torch.Size([5, 2592, 4608])
>> gt_mask.shape:  torch.Size([1, 2592, 4608])
>> image_emb:  torch.Size([1, 256, 64, 64])
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([11, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 485, 5120])
>> per_token_labels.shape:  torch.Size([16, 485])
>> masks_list[0].shape:  torch.Size([5, 240, 320])
>> masks_list[1].shape:  torch.Size([5, 375, 500])
>> gt_mask.shape:  torch.Size([1, 240, 320])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 485, 5120])
>> per_token_labels.shape:  torch.Size([16, 485])
>> masks_list[0].shape:  torch.Size([5, 240, 320])
>> masks_list[1].shape:  torch.Size([5, 375, 500])
>> gt_mask.shape:  torch.Size([1, 240, 320])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 351, 5120])
>> per_token_labels.shape:  torch.Size([20, 351])
>> masks_list[0].shape:  torch.Size([5, 398, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 398, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 351, 5120])
>> per_token_labels.shape:  torch.Size([20, 351])
>> masks_list[0].shape:  torch.Size([5, 398, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 398, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 351, 5120])
>> per_token_labels.shape:  torch.Size([20, 351])
>> masks_list[0].shape:  torch.Size([5, 398, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 398, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 351, 5120])
>> per_token_labels.shape:  torch.Size([20, 351])
>> masks_list[0].shape:  torch.Size([5, 398, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 398, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 351, 5120])
>> per_token_labels.shape:  torch.Size([20, 351])
>> masks_list[0].shape:  torch.Size([5, 398, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 398, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 351, 5120])
>> per_token_labels.shape:  torch.Size([20, 351])
>> masks_list[0].shape:  torch.Size([5, 398, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 398, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 351, 5120])
>> per_token_labels.shape:  torch.Size([20, 351])
>> masks_list[0].shape:  torch.Size([5, 398, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 398, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 351, 5120])
>> per_token_labels.shape:  torch.Size([20, 351])
>> masks_list[0].shape:  torch.Size([5, 398, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 398, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 351, 5120])
>> per_token_labels.shape:  torch.Size([20, 351])
>> masks_list[0].shape:  torch.Size([5, 398, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 398, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 351, 5120])
>> per_token_labels.shape:  torch.Size([20, 351])
>> masks_list[0].shape:  torch.Size([5, 398, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 398, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([11, 512, 5120])
>> per_token_labels.shape:  torch.Size([11, 512])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 376, 500])
>> gt_mask.shape:  torch.Size([1, 376, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([11, 512, 5120])
>> per_token_labels.shape:  torch.Size([11, 512])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 376, 500])
>> gt_mask.shape:  torch.Size([1, 376, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 456, 5120])
>> per_token_labels.shape:  torch.Size([16, 456])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 640, 480])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 456, 5120])
>> per_token_labels.shape:  torch.Size([16, 456])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 640, 480])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 407, 5120])
>> per_token_labels.shape:  torch.Size([20, 407])
>> masks_list[0].shape:  torch.Size([5, 427, 640])
>> masks_list[1].shape:  torch.Size([5, 426, 640])
>> gt_mask.shape:  torch.Size([1, 427, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 407, 5120])
>> per_token_labels.shape:  torch.Size([20, 407])
>> masks_list[0].shape:  torch.Size([5, 427, 640])
>> masks_list[1].shape:  torch.Size([5, 426, 640])
>> gt_mask.shape:  torch.Size([1, 427, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
Epoch: [0][ 13/500]	Time 28.816 (28.816)	Loss 0.6484 (0.4588)	CeLoss 0.2109 (0.2104)	SegCLSLoss 0.0991 (0.0815)	KLLoss 0.0000 (0.0106)	MaskLoss 0.1937 (0.1013)	MaskBCELoss 0.0940 (0.0221)	MaskDICELoss 0.0997 (0.0792)
>> last_hidden_states.shape:  torch.Size([17, 337, 5120])
>> per_token_labels.shape:  torch.Size([17, 337])
>> masks_list[0].shape:  torch.Size([2, 427, 640])
>> masks_list[1].shape:  torch.Size([5, 333, 500])
>> gt_mask.shape:  torch.Size([1, 427, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 337, 5120])
>> per_token_labels.shape:  torch.Size([17, 337])
>> masks_list[0].shape:  torch.Size([2, 427, 640])
>> masks_list[1].shape:  torch.Size([5, 333, 500])
>> gt_mask.shape:  torch.Size([1, 427, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 470, 5120])
>> per_token_labels.shape:  torch.Size([12, 470])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 500, 436])
>> gt_mask.shape:  torch.Size([1, 500, 436])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 470, 5120])
>> per_token_labels.shape:  torch.Size([12, 470])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 500, 436])
>> gt_mask.shape:  torch.Size([1, 500, 436])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 331, 5120])
>> per_token_labels.shape:  torch.Size([18, 331])
>> masks_list[0].shape:  torch.Size([3, 375, 500])
>> masks_list[1].shape:  torch.Size([5, 435, 640])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 331, 5120])
>> per_token_labels.shape:  torch.Size([18, 331])
>> masks_list[0].shape:  torch.Size([3, 375, 500])
>> masks_list[1].shape:  torch.Size([5, 435, 640])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 355, 5120])
>> per_token_labels.shape:  torch.Size([17, 355])
>> masks_list[0].shape:  torch.Size([5, 1920, 2560])
>> masks_list[1].shape:  torch.Size([5, 256, 256])
>> gt_mask.shape:  torch.Size([1, 1920, 2560])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 355, 5120])
>> per_token_labels.shape:  torch.Size([17, 355])
>> masks_list[0].shape:  torch.Size([5, 1920, 2560])
>> masks_list[1].shape:  torch.Size([5, 256, 256])
>> gt_mask.shape:  torch.Size([1, 1920, 2560])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 446, 5120])
>> per_token_labels.shape:  torch.Size([16, 446])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 446, 5120])
>> per_token_labels.shape:  torch.Size([16, 446])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 446, 5120])
>> per_token_labels.shape:  torch.Size([16, 446])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 446, 5120])
>> per_token_labels.shape:  torch.Size([16, 446])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 458, 5120])
>> per_token_labels.shape:  torch.Size([17, 458])
>> masks_list[0].shape:  torch.Size([3, 640, 444])
>> masks_list[1].shape:  torch.Size([5, 1952, 2592])
>> gt_mask.shape:  torch.Size([1, 640, 444])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 458, 5120])
>> per_token_labels.shape:  torch.Size([17, 458])
>> masks_list[0].shape:  torch.Size([3, 640, 444])
>> masks_list[1].shape:  torch.Size([5, 1952, 2592])
>> gt_mask.shape:  torch.Size([1, 640, 444])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 458, 5120])
>> per_token_labels.shape:  torch.Size([17, 458])
>> masks_list[0].shape:  torch.Size([3, 640, 444])
>> masks_list[1].shape:  torch.Size([5, 1952, 2592])
>> gt_mask.shape:  torch.Size([1, 640, 444])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 458, 5120])
>> per_token_labels.shape:  torch.Size([17, 458])
>> masks_list[0].shape:  torch.Size([3, 640, 444])
>> masks_list[1].shape:  torch.Size([5, 1952, 2592])
>> gt_mask.shape:  torch.Size([1, 640, 444])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 458, 5120])
>> per_token_labels.shape:  torch.Size([17, 458])
>> masks_list[0].shape:  torch.Size([3, 640, 444])
>> masks_list[1].shape:  torch.Size([5, 1952, 2592])
>> gt_mask.shape:  torch.Size([1, 640, 444])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 458, 5120])
>> per_token_labels.shape:  torch.Size([17, 458])
>> masks_list[0].shape:  torch.Size([3, 640, 444])
>> masks_list[1].shape:  torch.Size([5, 1952, 2592])
>> gt_mask.shape:  torch.Size([1, 640, 444])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 335, 5120])
>> per_token_labels.shape:  torch.Size([17, 335])
>> masks_list[0].shape:  torch.Size([5, 425, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 425, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 335, 5120])
>> per_token_labels.shape:  torch.Size([17, 335])
>> masks_list[0].shape:  torch.Size([5, 425, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 425, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 438, 5120])
>> per_token_labels.shape:  torch.Size([16, 438])
>> masks_list[0].shape:  torch.Size([5, 426, 640])
>> masks_list[1].shape:  torch.Size([5, 640, 480])
>> gt_mask.shape:  torch.Size([1, 426, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 438, 5120])
>> per_token_labels.shape:  torch.Size([16, 438])
>> masks_list[0].shape:  torch.Size([5, 426, 640])
>> masks_list[1].shape:  torch.Size([5, 640, 480])
>> gt_mask.shape:  torch.Size([1, 426, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 438, 5120])
>> per_token_labels.shape:  torch.Size([16, 438])
>> masks_list[0].shape:  torch.Size([5, 426, 640])
>> masks_list[1].shape:  torch.Size([5, 640, 480])
>> gt_mask.shape:  torch.Size([1, 426, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 438, 5120])
>> per_token_labels.shape:  torch.Size([16, 438])
>> masks_list[0].shape:  torch.Size([5, 426, 640])
>> masks_list[1].shape:  torch.Size([5, 640, 480])
>> gt_mask.shape:  torch.Size([1, 426, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 438, 5120])
>> per_token_labels.shape:  torch.Size([16, 438])
>> masks_list[0].shape:  torch.Size([5, 426, 640])
>> masks_list[1].shape:  torch.Size([5, 640, 480])
>> gt_mask.shape:  torch.Size([1, 426, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 438, 5120])
>> per_token_labels.shape:  torch.Size([16, 438])
>> masks_list[0].shape:  torch.Size([5, 426, 640])
>> masks_list[1].shape:  torch.Size([5, 640, 480])
>> gt_mask.shape:  torch.Size([1, 426, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 438, 5120])
>> per_token_labels.shape:  torch.Size([16, 438])
>> masks_list[0].shape:  torch.Size([5, 426, 640])
>> masks_list[1].shape:  torch.Size([5, 640, 480])
>> gt_mask.shape:  torch.Size([1, 426, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 438, 5120])
>> per_token_labels.shape:  torch.Size([16, 438])
>> masks_list[0].shape:  torch.Size([5, 426, 640])
>> masks_list[1].shape:  torch.Size([5, 640, 480])
>> gt_mask.shape:  torch.Size([1, 426, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 334, 5120])
>> per_token_labels.shape:  torch.Size([17, 334])
>> masks_list[0].shape:  torch.Size([4, 682, 512])
>> masks_list[1].shape:  torch.Size([3, 225, 300])
>> gt_mask.shape:  torch.Size([1, 682, 512])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 334, 5120])
>> per_token_labels.shape:  torch.Size([17, 334])
>> masks_list[0].shape:  torch.Size([4, 682, 512])
>> masks_list[1].shape:  torch.Size([3, 225, 300])
>> gt_mask.shape:  torch.Size([1, 682, 512])
>> image_emb:  torch.Size([1, 256, 64, 64])
Epoch: [0][ 14/500]	Time 27.291 (27.291)	Loss 0.7787 (0.5609)	CeLoss 0.3027 (0.2635)	SegCLSLoss 0.0928 (0.0878)	KLLoss 0.0000 (0.0045)	MaskLoss 0.2144 (0.1255)	MaskBCELoss 0.1162 (0.0376)	MaskDICELoss 0.0982 (0.0879)
>> last_hidden_states.shape:  torch.Size([15, 336, 5120])
>> per_token_labels.shape:  torch.Size([15, 336])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([2, 480, 640])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 336, 5120])
>> per_token_labels.shape:  torch.Size([15, 336])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([2, 480, 640])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([0, 640, 640])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([0, 640, 640])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 443, 5120])
>> per_token_labels.shape:  torch.Size([16, 443])
>> masks_list[0].shape:  torch.Size([5, 360, 480])
>> masks_list[1].shape:  torch.Size([5, 512, 768])
>> gt_mask.shape:  torch.Size([1, 360, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 443, 5120])
>> per_token_labels.shape:  torch.Size([16, 443])
>> masks_list[0].shape:  torch.Size([5, 360, 480])
>> masks_list[1].shape:  torch.Size([5, 512, 768])
>> gt_mask.shape:  torch.Size([1, 360, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 440, 5120])
>> per_token_labels.shape:  torch.Size([13, 440])
>> masks_list[0].shape:  torch.Size([0, 427, 640])
>> masks_list[1].shape:  torch.Size([5, 375, 500])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 440, 5120])
>> per_token_labels.shape:  torch.Size([13, 440])
>> masks_list[0].shape:  torch.Size([0, 427, 640])
>> masks_list[1].shape:  torch.Size([5, 375, 500])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 460, 5120])
>> per_token_labels.shape:  torch.Size([14, 460])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([3, 360, 480])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 460, 5120])
>> per_token_labels.shape:  torch.Size([14, 460])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([3, 360, 480])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 339, 5120])
>> per_token_labels.shape:  torch.Size([19, 339])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([4, 480, 360])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 339, 5120])
>> per_token_labels.shape:  torch.Size([19, 339])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([4, 480, 360])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 512, 5120])
>> per_token_labels.shape:  torch.Size([13, 512])
>> masks_list[0].shape:  torch.Size([0, 500, 500])
>> masks_list[1].shape:  torch.Size([5, 428, 640])
>> gt_mask.shape:  torch.Size([1, 428, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 512, 5120])
>> per_token_labels.shape:  torch.Size([13, 512])
>> masks_list[0].shape:  torch.Size([0, 500, 500])
>> masks_list[1].shape:  torch.Size([5, 428, 640])
>> gt_mask.shape:  torch.Size([1, 428, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 459, 5120])
>> per_token_labels.shape:  torch.Size([14, 459])
>> masks_list[0].shape:  torch.Size([5, 427, 640])
>> masks_list[1].shape:  torch.Size([0, 608, 640])
>> gt_mask.shape:  torch.Size([1, 427, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 459, 5120])
>> per_token_labels.shape:  torch.Size([14, 459])
>> masks_list[0].shape:  torch.Size([5, 427, 640])
>> masks_list[1].shape:  torch.Size([0, 608, 640])
>> gt_mask.shape:  torch.Size([1, 427, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 459, 5120])
>> per_token_labels.shape:  torch.Size([14, 459])
>> masks_list[0].shape:  torch.Size([5, 427, 640])
>> masks_list[1].shape:  torch.Size([0, 608, 640])
>> gt_mask.shape:  torch.Size([1, 427, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 459, 5120])
>> per_token_labels.shape:  torch.Size([14, 459])
>> masks_list[0].shape:  torch.Size([5, 427, 640])
>> masks_list[1].shape:  torch.Size([0, 608, 640])
>> gt_mask.shape:  torch.Size([1, 427, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([5, 429, 640])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 429, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([5, 429, 640])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 429, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([5, 429, 640])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 429, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([5, 429, 640])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 429, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 405, 5120])
>> per_token_labels.shape:  torch.Size([15, 405])
>> masks_list[0].shape:  torch.Size([5, 512, 768])
>> masks_list[1].shape:  torch.Size([4, 360, 640])
>> gt_mask.shape:  torch.Size([1, 512, 768])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 405, 5120])
>> per_token_labels.shape:  torch.Size([15, 405])
>> masks_list[0].shape:  torch.Size([5, 512, 768])
>> masks_list[1].shape:  torch.Size([4, 360, 640])
>> gt_mask.shape:  torch.Size([1, 512, 768])
>> image_emb:  torch.Size([1, 256, 64, 64])
Epoch: [0][ 15/500]	Time 26.109 (26.109)	Loss 0.7315 (0.6023)	CeLoss 0.2773 (0.2517)	SegCLSLoss 0.1025 (0.1001)	KLLoss 0.0000 (0.0135)	MaskLoss 0.2017 (0.1469)	MaskBCELoss 0.1197 (0.0518)	MaskDICELoss 0.0820 (0.0951)
>> last_hidden_states.shape:  torch.Size([13, 512, 5120])
>> per_token_labels.shape:  torch.Size([13, 512])
>> masks_list[0].shape:  torch.Size([5, 424, 640])
>> masks_list[1].shape:  torch.Size([2, 480, 640])
>> gt_mask.shape:  torch.Size([1, 424, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 512, 5120])
>> per_token_labels.shape:  torch.Size([13, 512])
>> masks_list[0].shape:  torch.Size([5, 424, 640])
>> masks_list[1].shape:  torch.Size([2, 480, 640])
>> gt_mask.shape:  torch.Size([1, 424, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 512, 5120])
>> per_token_labels.shape:  torch.Size([13, 512])
>> masks_list[0].shape:  torch.Size([5, 424, 640])
>> masks_list[1].shape:  torch.Size([2, 480, 640])
>> gt_mask.shape:  torch.Size([1, 424, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 512, 5120])
>> per_token_labels.shape:  torch.Size([13, 512])
>> masks_list[0].shape:  torch.Size([5, 424, 640])
>> masks_list[1].shape:  torch.Size([2, 480, 640])
>> gt_mask.shape:  torch.Size([1, 424, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 329, 5120])
>> per_token_labels.shape:  torch.Size([13, 329])
>> masks_list[0].shape:  torch.Size([2, 256, 256])
>> masks_list[1].shape:  torch.Size([5, 512, 683])
>> gt_mask.shape:  torch.Size([1, 256, 256])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 329, 5120])
>> per_token_labels.shape:  torch.Size([13, 329])
>> masks_list[0].shape:  torch.Size([2, 256, 256])
>> masks_list[1].shape:  torch.Size([5, 512, 683])
>> gt_mask.shape:  torch.Size([1, 256, 256])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 488, 5120])
>> per_token_labels.shape:  torch.Size([13, 488])
>> masks_list[0].shape:  torch.Size([0, 472, 640])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 488, 5120])
>> per_token_labels.shape:  torch.Size([13, 488])
>> masks_list[0].shape:  torch.Size([0, 472, 640])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 479, 5120])
>> per_token_labels.shape:  torch.Size([14, 479])
>> masks_list[0].shape:  torch.Size([5, 360, 480])
>> masks_list[1].shape:  torch.Size([4, 480, 640])
>> gt_mask.shape:  torch.Size([1, 360, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 479, 5120])
>> per_token_labels.shape:  torch.Size([14, 479])
>> masks_list[0].shape:  torch.Size([5, 360, 480])
>> masks_list[1].shape:  torch.Size([4, 480, 640])
>> gt_mask.shape:  torch.Size([1, 360, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 336, 5120])
>> per_token_labels.shape:  torch.Size([20, 336])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([5, 481, 640])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 336, 5120])
>> per_token_labels.shape:  torch.Size([20, 336])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([5, 481, 640])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 331, 5120])
>> per_token_labels.shape:  torch.Size([20, 331])
>> masks_list[0].shape:  torch.Size([5, 500, 489])
>> masks_list[1].shape:  torch.Size([5, 428, 640])
>> gt_mask.shape:  torch.Size([1, 500, 489])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 331, 5120])
>> per_token_labels.shape:  torch.Size([20, 331])
>> masks_list[0].shape:  torch.Size([5, 500, 489])
>> masks_list[1].shape:  torch.Size([5, 428, 640])
>> gt_mask.shape:  torch.Size([1, 500, 489])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 2988, 3984])
>> masks_list[1].shape:  torch.Size([0, 427, 640])
>> gt_mask.shape:  torch.Size([1, 2988, 3984])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 2988, 3984])
>> masks_list[1].shape:  torch.Size([0, 427, 640])
>> gt_mask.shape:  torch.Size([1, 2988, 3984])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 339, 5120])
>> per_token_labels.shape:  torch.Size([20, 339])
>> masks_list[0].shape:  torch.Size([5, 426, 640])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 426, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 339, 5120])
>> per_token_labels.shape:  torch.Size([20, 339])
>> masks_list[0].shape:  torch.Size([5, 426, 640])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 426, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 339, 5120])
>> per_token_labels.shape:  torch.Size([20, 339])
>> masks_list[0].shape:  torch.Size([5, 426, 640])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 426, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 339, 5120])
>> per_token_labels.shape:  torch.Size([20, 339])
>> masks_list[0].shape:  torch.Size([5, 426, 640])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 426, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 336, 5120])
>> per_token_labels.shape:  torch.Size([20, 336])
>> masks_list[0].shape:  torch.Size([5, 428, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 428, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 336, 5120])
>> per_token_labels.shape:  torch.Size([20, 336])
>> masks_list[0].shape:  torch.Size([5, 428, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 428, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
Epoch: [0][ 16/500]	Time 24.937 (24.937)	Loss 0.5955 (0.6733)	CeLoss 0.3262 (0.2809)	SegCLSLoss 0.0947 (0.0884)	KLLoss 0.0160 (0.0093)	MaskLoss 0.1071 (0.1716)	MaskBCELoss 0.0072 (0.0877)	MaskDICELoss 0.0999 (0.0839)
>> last_hidden_states.shape:  torch.Size([19, 335, 5120])
>> per_token_labels.shape:  torch.Size([19, 335])
>> masks_list[0].shape:  torch.Size([5, 512, 771])
>> masks_list[1].shape:  torch.Size([5, 640, 404])
>> gt_mask.shape:  torch.Size([1, 512, 771])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 335, 5120])
>> per_token_labels.shape:  torch.Size([19, 335])
>> masks_list[0].shape:  torch.Size([5, 512, 771])
>> masks_list[1].shape:  torch.Size([5, 640, 404])
>> gt_mask.shape:  torch.Size([1, 512, 771])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 483, 5120])
>> per_token_labels.shape:  torch.Size([12, 483])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 483, 5120])
>> per_token_labels.shape:  torch.Size([12, 483])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 483, 5120])
>> per_token_labels.shape:  torch.Size([12, 483])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 483, 5120])
>> per_token_labels.shape:  torch.Size([12, 483])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 346, 5120])
>> per_token_labels.shape:  torch.Size([18, 346])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([3, 512, 684])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 346, 5120])
>> per_token_labels.shape:  torch.Size([18, 346])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([3, 512, 684])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 329, 5120])
>> per_token_labels.shape:  torch.Size([19, 329])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([4, 480, 360])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 329, 5120])
>> per_token_labels.shape:  torch.Size([19, 329])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([4, 480, 360])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 331, 5120])
>> per_token_labels.shape:  torch.Size([16, 331])
>> masks_list[0].shape:  torch.Size([5, 500, 413])
>> masks_list[1].shape:  torch.Size([3, 360, 480])
>> gt_mask.shape:  torch.Size([1, 500, 413])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 331, 5120])
>> per_token_labels.shape:  torch.Size([16, 331])
>> masks_list[0].shape:  torch.Size([5, 500, 413])
>> masks_list[1].shape:  torch.Size([3, 360, 480])
>> gt_mask.shape:  torch.Size([1, 500, 413])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([5, 480, 360])
>> masks_list[1].shape:  torch.Size([0, 424, 640])
>> gt_mask.shape:  torch.Size([1, 480, 360])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([5, 480, 360])
>> masks_list[1].shape:  torch.Size([0, 424, 640])
>> gt_mask.shape:  torch.Size([1, 480, 360])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 443, 5120])
>> per_token_labels.shape:  torch.Size([13, 443])
>> masks_list[0].shape:  torch.Size([0, 427, 640])
>> masks_list[1].shape:  torch.Size([5, 2400, 3200])
>> gt_mask.shape:  torch.Size([1, 2400, 3200])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 443, 5120])
>> per_token_labels.shape:  torch.Size([13, 443])
>> masks_list[0].shape:  torch.Size([0, 427, 640])
>> masks_list[1].shape:  torch.Size([5, 2400, 3200])
>> gt_mask.shape:  torch.Size([1, 2400, 3200])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 343, 5120])
>> per_token_labels.shape:  torch.Size([20, 343])
>> masks_list[0].shape:  torch.Size([5, 512, 683])
>> masks_list[1].shape:  torch.Size([5, 2592, 4608])
>> gt_mask.shape:  torch.Size([1, 512, 683])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 343, 5120])
>> per_token_labels.shape:  torch.Size([20, 343])
>> masks_list[0].shape:  torch.Size([5, 512, 683])
>> masks_list[1].shape:  torch.Size([5, 2592, 4608])
>> gt_mask.shape:  torch.Size([1, 512, 683])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 342, 5120])
>> per_token_labels.shape:  torch.Size([16, 342])
>> masks_list[0].shape:  torch.Size([5, 291, 250])
>> masks_list[1].shape:  torch.Size([2, 362, 640])
>> gt_mask.shape:  torch.Size([1, 291, 250])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 342, 5120])
>> per_token_labels.shape:  torch.Size([16, 342])
>> masks_list[0].shape:  torch.Size([5, 291, 250])
>> masks_list[1].shape:  torch.Size([2, 362, 640])
>> gt_mask.shape:  torch.Size([1, 291, 250])
>> image_emb:  torch.Size([1, 256, 64, 64])
Epoch: [0][ 17/500]	Time 22.917 (22.917)	Loss 0.8389 (0.5956)	CeLoss 0.3789 (0.2838)	SegCLSLoss 0.0947 (0.0878)	KLLoss 0.0000 (0.0066)	MaskLoss 0.2070 (0.1324)	MaskBCELoss 0.1232 (0.0459)	MaskDICELoss 0.0838 (0.0865)
>> last_hidden_states.shape:  torch.Size([16, 332, 5120])
>> per_token_labels.shape:  torch.Size([16, 332])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([2, 333, 500])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 332, 5120])
>> per_token_labels.shape:  torch.Size([16, 332])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([2, 333, 500])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 425, 5120])
>> per_token_labels.shape:  torch.Size([16, 425])
>> masks_list[0].shape:  torch.Size([5, 2988, 3984])
>> masks_list[1].shape:  torch.Size([0, 433, 640])
>> gt_mask.shape:  torch.Size([1, 2988, 3984])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 425, 5120])
>> per_token_labels.shape:  torch.Size([16, 425])
>> masks_list[0].shape:  torch.Size([5, 2988, 3984])
>> masks_list[1].shape:  torch.Size([0, 433, 640])
>> gt_mask.shape:  torch.Size([1, 2988, 3984])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([0, 427, 640])
>> masks_list[1].shape:  torch.Size([5, 500, 375])
>> gt_mask.shape:  torch.Size([1, 500, 375])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([0, 427, 640])
>> masks_list[1].shape:  torch.Size([5, 500, 375])
>> gt_mask.shape:  torch.Size([1, 500, 375])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 432, 5120])
>> per_token_labels.shape:  torch.Size([12, 432])
>> masks_list[0].shape:  torch.Size([0, 427, 640])
>> masks_list[1].shape:  torch.Size([5, 512, 683])
>> gt_mask.shape:  torch.Size([1, 512, 683])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 432, 5120])
>> per_token_labels.shape:  torch.Size([12, 432])
>> masks_list[0].shape:  torch.Size([0, 427, 640])
>> masks_list[1].shape:  torch.Size([5, 512, 683])
>> gt_mask.shape:  torch.Size([1, 512, 683])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([0, 427, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([0, 427, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 339, 5120])
>> per_token_labels.shape:  torch.Size([16, 339])
>> masks_list[0].shape:  torch.Size([5, 2988, 3984])
>> masks_list[1].shape:  torch.Size([5, 394, 640])
>> gt_mask.shape:  torch.Size([1, 2988, 3984])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 339, 5120])
>> per_token_labels.shape:  torch.Size([16, 339])
>> masks_list[0].shape:  torch.Size([5, 2988, 3984])
>> masks_list[1].shape:  torch.Size([5, 394, 640])
>> gt_mask.shape:  torch.Size([1, 2988, 3984])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 334, 5120])
>> per_token_labels.shape:  torch.Size([20, 334])
>> masks_list[0].shape:  torch.Size([5, 419, 599])
>> masks_list[1].shape:  torch.Size([5, 512, 768])
>> gt_mask.shape:  torch.Size([1, 419, 599])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 334, 5120])
>> per_token_labels.shape:  torch.Size([20, 334])
>> masks_list[0].shape:  torch.Size([5, 419, 599])
>> masks_list[1].shape:  torch.Size([5, 512, 768])
>> gt_mask.shape:  torch.Size([1, 419, 599])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 333, 5120])
>> per_token_labels.shape:  torch.Size([20, 333])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([5, 640, 640])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 333, 5120])
>> per_token_labels.shape:  torch.Size([20, 333])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([5, 640, 640])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([0, 333, 500])
>> masks_list[1].shape:  torch.Size([5, 425, 640])
>> gt_mask.shape:  torch.Size([1, 425, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([0, 333, 500])
>> masks_list[1].shape:  torch.Size([5, 425, 640])
>> gt_mask.shape:  torch.Size([1, 425, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([10, 512, 5120])
>> per_token_labels.shape:  torch.Size([10, 512])
>> masks_list[0].shape:  torch.Size([2, 612, 612])
>> masks_list[1].shape:  torch.Size([4, 480, 640])
>> gt_mask.shape:  torch.Size([1, 612, 612])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([10, 512, 5120])
>> per_token_labels.shape:  torch.Size([10, 512])
>> masks_list[0].shape:  torch.Size([2, 612, 612])
>> masks_list[1].shape:  torch.Size([4, 480, 640])
>> gt_mask.shape:  torch.Size([1, 612, 612])
>> image_emb:  torch.Size([1, 256, 64, 64])
Epoch: [0][ 18/500]	Time 24.191 (24.191)	Loss 1.3082 (0.6656)	CeLoss 0.1562 (0.2538)	SegCLSLoss 0.1030 (0.0988)	KLLoss 0.0232 (0.0150)	MaskLoss 0.5447 (0.1776)	MaskBCELoss 0.4547 (0.0822)	MaskDICELoss 0.0900 (0.0954)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([8, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
>> last_hidden_states.shape:  torch.Size([13, 512, 5120])
>> per_token_labels.shape:  torch.Size([13, 512])
>> masks_list[0].shape:  torch.Size([5, 427, 640])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 427, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 512, 5120])
>> per_token_labels.shape:  torch.Size([13, 512])
>> masks_list[0].shape:  torch.Size([5, 427, 640])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 427, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 439, 5120])
>> per_token_labels.shape:  torch.Size([16, 439])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 375, 500])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 439, 5120])
>> per_token_labels.shape:  torch.Size([16, 439])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 375, 500])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 409, 5120])
>> per_token_labels.shape:  torch.Size([18, 409])
>> masks_list[0].shape:  torch.Size([5, 275, 250])
>> masks_list[1].shape:  torch.Size([3, 640, 427])
>> gt_mask.shape:  torch.Size([1, 275, 250])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 409, 5120])
>> per_token_labels.shape:  torch.Size([18, 409])
>> masks_list[0].shape:  torch.Size([5, 275, 250])
>> masks_list[1].shape:  torch.Size([3, 640, 427])
>> gt_mask.shape:  torch.Size([1, 275, 250])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 329, 5120])
>> per_token_labels.shape:  torch.Size([19, 329])
>> masks_list[0].shape:  torch.Size([5, 512, 778])
>> masks_list[1].shape:  torch.Size([5, 512, 768])
>> gt_mask.shape:  torch.Size([1, 512, 778])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 329, 5120])
>> per_token_labels.shape:  torch.Size([19, 329])
>> masks_list[0].shape:  torch.Size([5, 512, 778])
>> masks_list[1].shape:  torch.Size([5, 512, 768])
>> gt_mask.shape:  torch.Size([1, 512, 778])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 336, 5120])
>> per_token_labels.shape:  torch.Size([16, 336])
>> masks_list[0].shape:  torch.Size([4, 512, 683])
>> masks_list[1].shape:  torch.Size([2, 320, 480])
>> gt_mask.shape:  torch.Size([1, 512, 683])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 336, 5120])
>> per_token_labels.shape:  torch.Size([16, 336])
>> masks_list[0].shape:  torch.Size([4, 512, 683])
>> masks_list[1].shape:  torch.Size([2, 320, 480])
>> gt_mask.shape:  torch.Size([1, 512, 683])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 447, 5120])
>> per_token_labels.shape:  torch.Size([20, 447])
>> masks_list[0].shape:  torch.Size([0, 525, 200])
>> masks_list[1].shape:  torch.Size([5, 500, 493])
>> gt_mask.shape:  torch.Size([1, 500, 493])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 447, 5120])
>> per_token_labels.shape:  torch.Size([20, 447])
>> masks_list[0].shape:  torch.Size([0, 525, 200])
>> masks_list[1].shape:  torch.Size([5, 500, 493])
>> gt_mask.shape:  torch.Size([1, 500, 493])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 512, 5120])
>> per_token_labels.shape:  torch.Size([13, 512])
>> masks_list[0].shape:  torch.Size([5, 683, 512])
>> masks_list[1].shape:  torch.Size([2, 480, 640])
>> gt_mask.shape:  torch.Size([1, 683, 512])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 512, 5120])
>> per_token_labels.shape:  torch.Size([13, 512])
>> masks_list[0].shape:  torch.Size([5, 683, 512])
>> masks_list[1].shape:  torch.Size([2, 480, 640])
>> gt_mask.shape:  torch.Size([1, 683, 512])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 508, 5120])
>> per_token_labels.shape:  torch.Size([13, 508])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([0, 590, 640])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 508, 5120])
>> per_token_labels.shape:  torch.Size([13, 508])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([0, 590, 640])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 508, 5120])
>> per_token_labels.shape:  torch.Size([13, 508])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([0, 590, 640])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 508, 5120])
>> per_token_labels.shape:  torch.Size([13, 508])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([0, 590, 640])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 344, 5120])
>> per_token_labels.shape:  torch.Size([17, 344])
>> masks_list[0].shape:  torch.Size([5, 634, 640])
>> masks_list[1].shape:  torch.Size([5, 3120, 4160])
>> gt_mask.shape:  torch.Size([1, 634, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 344, 5120])
>> per_token_labels.shape:  torch.Size([17, 344])
>> masks_list[0].shape:  torch.Size([5, 634, 640])
>> masks_list[1].shape:  torch.Size([5, 3120, 4160])
>> gt_mask.shape:  torch.Size([1, 634, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 412, 5120])
>> per_token_labels.shape:  torch.Size([12, 412])
>> masks_list[0].shape:  torch.Size([5, 640, 427])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 640, 427])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 412, 5120])
>> per_token_labels.shape:  torch.Size([12, 412])
>> masks_list[0].shape:  torch.Size([5, 640, 427])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 640, 427])
>> image_emb:  torch.Size([1, 256, 64, 64])
Epoch: [0][ 19/500]	Time 26.037 (26.037)	Loss 0.4973 (0.6697)	CeLoss 0.2061 (0.2509)	SegCLSLoss 0.1030 (0.1001)	KLLoss 0.0132 (0.0097)	MaskLoss 0.1166 (0.1818)	MaskBCELoss 0.0274 (0.0890)	MaskDICELoss 0.0892 (0.0928)
>> last_hidden_states.shape:  torch.Size([16, 508, 5120])
>> per_token_labels.shape:  torch.Size([16, 508])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 508, 5120])
>> per_token_labels.shape:  torch.Size([16, 508])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 419, 5120])
>> per_token_labels.shape:  torch.Size([16, 419])
>> masks_list[0].shape:  torch.Size([5, 500, 333])
>> masks_list[1].shape:  torch.Size([5, 549, 640])
>> gt_mask.shape:  torch.Size([1, 500, 333])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 419, 5120])
>> per_token_labels.shape:  torch.Size([16, 419])
>> masks_list[0].shape:  torch.Size([5, 500, 333])
>> masks_list[1].shape:  torch.Size([5, 549, 640])
>> gt_mask.shape:  torch.Size([1, 500, 333])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 495, 5120])
>> per_token_labels.shape:  torch.Size([12, 495])
>> masks_list[0].shape:  torch.Size([0, 360, 640])
>> masks_list[1].shape:  torch.Size([5, 402, 500])
>> gt_mask.shape:  torch.Size([1, 402, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 495, 5120])
>> per_token_labels.shape:  torch.Size([12, 495])
>> masks_list[0].shape:  torch.Size([0, 360, 640])
>> masks_list[1].shape:  torch.Size([5, 402, 500])
>> gt_mask.shape:  torch.Size([1, 402, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([11, 512, 5120])
>> per_token_labels.shape:  torch.Size([11, 512])
>> masks_list[0].shape:  torch.Size([5, 425, 640])
>> masks_list[1].shape:  torch.Size([4, 640, 426])
>> gt_mask.shape:  torch.Size([1, 425, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([11, 512, 5120])
>> per_token_labels.shape:  torch.Size([11, 512])
>> masks_list[0].shape:  torch.Size([5, 425, 640])
>> masks_list[1].shape:  torch.Size([4, 640, 426])
>> gt_mask.shape:  torch.Size([1, 425, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 335, 5120])
>> per_token_labels.shape:  torch.Size([20, 335])
>> masks_list[0].shape:  torch.Size([5, 425, 640])
>> masks_list[1].shape:  torch.Size([5, 224, 300])
>> gt_mask.shape:  torch.Size([1, 425, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 335, 5120])
>> per_token_labels.shape:  torch.Size([20, 335])
>> masks_list[0].shape:  torch.Size([5, 425, 640])
>> masks_list[1].shape:  torch.Size([5, 224, 300])
>> gt_mask.shape:  torch.Size([1, 425, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 396, 5120])
>> per_token_labels.shape:  torch.Size([16, 396])
>> masks_list[0].shape:  torch.Size([1, 2144, 3216])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 396, 5120])
>> per_token_labels.shape:  torch.Size([16, 396])
>> masks_list[0].shape:  torch.Size([1, 2144, 3216])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 493, 5120])
>> per_token_labels.shape:  torch.Size([16, 493])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 493, 5120])
>> per_token_labels.shape:  torch.Size([16, 493])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 512, 5120])
>> per_token_labels.shape:  torch.Size([13, 512])
>> masks_list[0].shape:  torch.Size([5, 512, 683])
>> masks_list[1].shape:  torch.Size([4, 640, 463])
>> gt_mask.shape:  torch.Size([1, 512, 683])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 512, 5120])
>> per_token_labels.shape:  torch.Size([13, 512])
>> masks_list[0].shape:  torch.Size([5, 512, 683])
>> masks_list[1].shape:  torch.Size([4, 640, 463])
>> gt_mask.shape:  torch.Size([1, 512, 683])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 506, 5120])
>> per_token_labels.shape:  torch.Size([20, 506])
>> masks_list[0].shape:  torch.Size([5, 640, 597])
>> masks_list[1].shape:  torch.Size([5, 3000, 2400])
>> gt_mask.shape:  torch.Size([1, 640, 597])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 506, 5120])
>> per_token_labels.shape:  torch.Size([20, 506])
>> masks_list[0].shape:  torch.Size([5, 640, 597])
>> masks_list[1].shape:  torch.Size([5, 3000, 2400])
>> gt_mask.shape:  torch.Size([1, 640, 597])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 416, 5120])
>> per_token_labels.shape:  torch.Size([13, 416])
>> masks_list[0].shape:  torch.Size([2, 360, 480])
>> masks_list[1].shape:  torch.Size([5, 375, 500])
>> gt_mask.shape:  torch.Size([1, 360, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 416, 5120])
>> per_token_labels.shape:  torch.Size([13, 416])
>> masks_list[0].shape:  torch.Size([2, 360, 480])
>> masks_list[1].shape:  torch.Size([5, 375, 500])
>> gt_mask.shape:  torch.Size([1, 360, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
Epoch: [0][ 20/500]	Time 28.499 (28.499)	Loss 1.1653 (0.6835)	CeLoss 0.2539 (0.2209)	SegCLSLoss 0.1016 (0.1012)	KLLoss 0.0000 (0.0140)	MaskLoss 0.4295 (0.2025)	MaskBCELoss 0.3298 (0.1103)	MaskDICELoss 0.0997 (0.0922)
>> last_hidden_states.shape:  torch.Size([20, 337, 5120])
>> per_token_labels.shape:  torch.Size([20, 337])
>> masks_list[0].shape:  torch.Size([5, 426, 640])
>> masks_list[1].shape:  torch.Size([5, 300, 452])
>> gt_mask.shape:  torch.Size([1, 426, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 337, 5120])
>> per_token_labels.shape:  torch.Size([20, 337])
>> masks_list[0].shape:  torch.Size([5, 426, 640])
>> masks_list[1].shape:  torch.Size([5, 300, 452])
>> gt_mask.shape:  torch.Size([1, 426, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 512, 5120])
>> per_token_labels.shape:  torch.Size([15, 512])
>> masks_list[0].shape:  torch.Size([4, 266, 267])
>> masks_list[1].shape:  torch.Size([5, 332, 500])
>> gt_mask.shape:  torch.Size([1, 266, 267])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 512, 5120])
>> per_token_labels.shape:  torch.Size([15, 512])
>> masks_list[0].shape:  torch.Size([4, 266, 267])
>> masks_list[1].shape:  torch.Size([5, 332, 500])
>> gt_mask.shape:  torch.Size([1, 266, 267])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([6, 501, 5120])
>> per_token_labels.shape:  torch.Size([6, 501])
>> masks_list[0].shape:  torch.Size([3, 222, 250])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 222, 250])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([6, 501, 5120])
>> per_token_labels.shape:  torch.Size([6, 501])
>> masks_list[0].shape:  torch.Size([3, 222, 250])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 222, 250])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 333, 5120])
>> per_token_labels.shape:  torch.Size([20, 333])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([5, 500, 375])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 333, 5120])
>> per_token_labels.shape:  torch.Size([20, 333])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([5, 500, 375])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 333, 5120])
>> per_token_labels.shape:  torch.Size([20, 333])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([5, 500, 375])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 333, 5120])
>> per_token_labels.shape:  torch.Size([20, 333])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([5, 500, 375])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 333, 5120])
>> per_token_labels.shape:  torch.Size([20, 333])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([5, 500, 375])
>> gt_mask.shape:  torch.Size([1, 500, 334])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 333, 5120])
>> per_token_labels.shape:  torch.Size([20, 333])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([5, 500, 375])
>> gt_mask.shape:  torch.Size([1, 500, 334])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([11, 512, 5120])
>> per_token_labels.shape:  torch.Size([11, 512])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([0, 427, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([11, 512, 5120])
>> per_token_labels.shape:  torch.Size([11, 512])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([0, 427, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([11, 512, 5120])
>> per_token_labels.shape:  torch.Size([11, 512])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([0, 427, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([11, 512, 5120])
>> per_token_labels.shape:  torch.Size([11, 512])
>> masks_list[0].shape:  torch.Size([0, 480, 640])
>> masks_list[1].shape:  torch.Size([0, 427, 640])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([0, 424, 640])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 512, 5120])
>> per_token_labels.shape:  torch.Size([12, 512])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([0, 424, 640])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 337, 5120])
>> per_token_labels.shape:  torch.Size([20, 337])
>> masks_list[0].shape:  torch.Size([5, 521, 640])
>> masks_list[1].shape:  torch.Size([5, 334, 500])
>> gt_mask.shape:  torch.Size([1, 521, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 337, 5120])
>> per_token_labels.shape:  torch.Size([20, 337])
>> masks_list[0].shape:  torch.Size([5, 521, 640])
>> masks_list[1].shape:  torch.Size([5, 334, 500])
>> gt_mask.shape:  torch.Size([1, 521, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 481, 5120])
>> per_token_labels.shape:  torch.Size([16, 481])
>> masks_list[0].shape:  torch.Size([5, 428, 640])
>> masks_list[1].shape:  torch.Size([5, 262, 350])
>> gt_mask.shape:  torch.Size([1, 428, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 481, 5120])
>> per_token_labels.shape:  torch.Size([16, 481])
>> masks_list[0].shape:  torch.Size([5, 428, 640])
>> masks_list[1].shape:  torch.Size([5, 262, 350])
>> gt_mask.shape:  torch.Size([1, 428, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 512, 5120])
>> per_token_labels.shape:  torch.Size([15, 512])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([3, 332, 500])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 512, 5120])
>> per_token_labels.shape:  torch.Size([15, 512])
>> masks_list[0].shape:  torch.Size([5, 375, 500])
>> masks_list[1].shape:  torch.Size([3, 332, 500])
>> gt_mask.shape:  torch.Size([1, 375, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([0, 429, 640])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([0, 429, 640])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
Epoch: [0][ 21/500]	Time 27.515 (27.515)	Loss 0.4683 (0.5162)	CeLoss 0.2158 (0.2186)	SegCLSLoss 0.1001 (0.0992)	KLLoss 0.0000 (0.0085)	MaskLoss 0.1013 (0.1216)	MaskBCELoss 0.0016 (0.0299)	MaskDICELoss 0.0997 (0.0918)
>> last_hidden_states.shape:  torch.Size([19, 476, 5120])
>> per_token_labels.shape:  torch.Size([19, 476])
>> masks_list[0].shape:  torch.Size([5, 2988, 5312])
>> masks_list[1].shape:  torch.Size([5, 2048, 1536])
>> gt_mask.shape:  torch.Size([1, 2988, 5312])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 476, 5120])
>> per_token_labels.shape:  torch.Size([19, 476])
>> masks_list[0].shape:  torch.Size([5, 2988, 5312])
>> masks_list[1].shape:  torch.Size([5, 2048, 1536])
>> gt_mask.shape:  torch.Size([1, 2988, 5312])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 512, 5120])
>> per_token_labels.shape:  torch.Size([15, 512])
>> masks_list[0].shape:  torch.Size([4, 256, 384])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 256, 384])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 512, 5120])
>> per_token_labels.shape:  torch.Size([15, 512])
>> masks_list[0].shape:  torch.Size([4, 256, 384])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 256, 384])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 360, 5120])
>> per_token_labels.shape:  torch.Size([19, 360])
>> masks_list[0].shape:  torch.Size([5, 600, 400])
>> masks_list[1].shape:  torch.Size([5, 512, 768])
>> gt_mask.shape:  torch.Size([1, 600, 400])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 360, 5120])
>> per_token_labels.shape:  torch.Size([19, 360])
>> masks_list[0].shape:  torch.Size([5, 600, 400])
>> masks_list[1].shape:  torch.Size([5, 512, 768])
>> gt_mask.shape:  torch.Size([1, 600, 400])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 349, 5120])
>> per_token_labels.shape:  torch.Size([19, 349])
>> masks_list[0].shape:  torch.Size([5, 3456, 4608])
>> masks_list[1].shape:  torch.Size([5, 2448, 3264])
>> gt_mask.shape:  torch.Size([1, 3456, 4608])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 424, 5120])
>> per_token_labels.shape:  torch.Size([15, 424])
>> masks_list[0].shape:  torch.Size([0, 427, 640])
>> masks_list[1].shape:  torch.Size([4, 426, 640])
>> gt_mask.shape:  torch.Size([1, 426, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 424, 5120])
>> per_token_labels.shape:  torch.Size([15, 424])
>> masks_list[0].shape:  torch.Size([0, 427, 640])
>> masks_list[1].shape:  torch.Size([4, 426, 640])
>> gt_mask.shape:  torch.Size([1, 426, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 497, 5120])
>> per_token_labels.shape:  torch.Size([15, 497])
>> masks_list[0].shape:  torch.Size([4, 640, 480])
>> masks_list[1].shape:  torch.Size([0, 424, 640])
>> gt_mask.shape:  torch.Size([1, 640, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([15, 497, 5120])
>> per_token_labels.shape:  torch.Size([15, 497])
>> masks_list[0].shape:  torch.Size([4, 640, 480])
>> masks_list[1].shape:  torch.Size([0, 424, 640])
>> gt_mask.shape:  torch.Size([1, 640, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([8, 512, 5120])
>> per_token_labels.shape:  torch.Size([8, 512])
>> masks_list[0].shape:  torch.Size([0, 500, 375])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 612, 612])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([8, 512, 5120])
>> per_token_labels.shape:  torch.Size([8, 512])
>> masks_list[0].shape:  torch.Size([0, 500, 375])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 612, 612])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 439, 5120])
>> per_token_labels.shape:  torch.Size([16, 439])
>> masks_list[0].shape:  torch.Size([5, 339, 500])
>> masks_list[1].shape:  torch.Size([5, 512, 768])
>> gt_mask.shape:  torch.Size([1, 339, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 439, 5120])
>> per_token_labels.shape:  torch.Size([16, 439])
>> masks_list[0].shape:  torch.Size([5, 339, 500])
>> masks_list[1].shape:  torch.Size([5, 512, 768])
>> gt_mask.shape:  torch.Size([1, 339, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
Epoch: [0][ 22/500]	Time 28.449 (28.449)	Loss 0.5253 (0.4475)	CeLoss 0.2598 (0.2013)	SegCLSLoss 0.1016 (0.0803)	KLLoss 0.0082 (0.0106)	MaskLoss 0.1056 (0.1005)	MaskBCELoss 0.0063 (0.0238)	MaskDICELoss 0.0994 (0.0766)
>> last_hidden_states.shape:  torch.Size([12, 497, 5120])
>> per_token_labels.shape:  torch.Size([12, 497])
>> masks_list[0].shape:  torch.Size([5, 608, 640])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 608, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([12, 497, 5120])
>> per_token_labels.shape:  torch.Size([12, 497])
>> masks_list[0].shape:  torch.Size([5, 608, 640])
>> masks_list[1].shape:  torch.Size([0, 480, 640])
>> gt_mask.shape:  torch.Size([1, 608, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 454, 5120])
>> per_token_labels.shape:  torch.Size([19, 454])
>> masks_list[0].shape:  torch.Size([0, 2057, 3284])
>> masks_list[1].shape:  torch.Size([5, 480, 360])
>> gt_mask.shape:  torch.Size([1, 480, 360])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([19, 454, 5120])
>> per_token_labels.shape:  torch.Size([19, 454])
>> masks_list[0].shape:  torch.Size([0, 2057, 3284])
>> masks_list[1].shape:  torch.Size([5, 480, 360])
>> gt_mask.shape:  torch.Size([1, 480, 360])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([8, 512, 5120])
>> per_token_labels.shape:  torch.Size([8, 512])
>> masks_list[0].shape:  torch.Size([5, 640, 427])
>> masks_list[1].shape:  torch.Size([0, 427, 640])
>> gt_mask.shape:  torch.Size([1, 640, 427])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([8, 512, 5120])
>> per_token_labels.shape:  torch.Size([8, 512])
>> masks_list[0].shape:  torch.Size([5, 640, 427])
>> masks_list[1].shape:  torch.Size([0, 427, 640])
>> gt_mask.shape:  torch.Size([1, 640, 427])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([8, 512, 5120])
>> per_token_labels.shape:  torch.Size([8, 512])
>> masks_list[0].shape:  torch.Size([5, 640, 427])
>> masks_list[1].shape:  torch.Size([0, 427, 640])
>> gt_mask.shape:  torch.Size([1, 640, 427])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([8, 512, 5120])
>> per_token_labels.shape:  torch.Size([8, 512])
>> masks_list[0].shape:  torch.Size([5, 640, 427])
>> masks_list[1].shape:  torch.Size([0, 427, 640])
>> gt_mask.shape:  torch.Size([1, 640, 427])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([20, 340, 5120])
>> per_token_labels.shape:  torch.Size([20, 340])
>> masks_list[0].shape:  torch.Size([5, 2448, 3264])
>> masks_list[1].shape:  torch.Size([5, 376, 640])
>> gt_mask.shape:  torch.Size([1, 2448, 3264])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 509, 5120])
>> per_token_labels.shape:  torch.Size([14, 509])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 512, 683])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 509, 5120])
>> per_token_labels.shape:  torch.Size([14, 509])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 512, 683])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 509, 5120])
>> per_token_labels.shape:  torch.Size([14, 509])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 512, 683])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 509, 5120])
>> per_token_labels.shape:  torch.Size([14, 509])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 512, 683])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 509, 5120])
>> per_token_labels.shape:  torch.Size([14, 509])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 512, 683])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 509, 5120])
>> per_token_labels.shape:  torch.Size([14, 509])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 512, 683])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 509, 5120])
>> per_token_labels.shape:  torch.Size([14, 509])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 512, 683])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 509, 5120])
>> per_token_labels.shape:  torch.Size([14, 509])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 512, 683])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 509, 5120])
>> per_token_labels.shape:  torch.Size([14, 509])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 512, 683])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 509, 5120])
>> per_token_labels.shape:  torch.Size([14, 509])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([5, 512, 683])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 512, 5120])
>> per_token_labels.shape:  torch.Size([13, 512])
>> masks_list[0].shape:  torch.Size([0, 458, 640])
>> masks_list[1].shape:  torch.Size([5, 1920, 2560])
>> gt_mask.shape:  torch.Size([1, 1920, 2560])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([13, 512, 5120])
>> per_token_labels.shape:  torch.Size([13, 512])
>> masks_list[0].shape:  torch.Size([0, 458, 640])
>> masks_list[1].shape:  torch.Size([5, 1920, 2560])
>> gt_mask.shape:  torch.Size([1, 1920, 2560])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 343, 5120])
>> per_token_labels.shape:  torch.Size([18, 343])
>> masks_list[0].shape:  torch.Size([5, 333, 500])
>> masks_list[1].shape:  torch.Size([3, 240, 320])
>> gt_mask.shape:  torch.Size([1, 333, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 343, 5120])
>> per_token_labels.shape:  torch.Size([18, 343])
>> masks_list[0].shape:  torch.Size([5, 333, 500])
>> masks_list[1].shape:  torch.Size([3, 240, 320])
>> gt_mask.shape:  torch.Size([1, 333, 500])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 457, 5120])
>> per_token_labels.shape:  torch.Size([14, 457])
>> masks_list[0].shape:  torch.Size([0, 522, 640])
>> masks_list[1].shape:  torch.Size([5, 640, 480])
>> gt_mask.shape:  torch.Size([1, 640, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([14, 457, 5120])
>> per_token_labels.shape:  torch.Size([14, 457])
>> masks_list[0].shape:  torch.Size([0, 522, 640])
>> masks_list[1].shape:  torch.Size([5, 640, 480])
>> gt_mask.shape:  torch.Size([1, 640, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 338, 5120])
>> per_token_labels.shape:  torch.Size([17, 338])
>> masks_list[0].shape:  torch.Size([2, 427, 640])
>> masks_list[1].shape:  torch.Size([5, 3024, 4032])
>> gt_mask.shape:  torch.Size([1, 427, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([17, 338, 5120])
>> per_token_labels.shape:  torch.Size([17, 338])
>> masks_list[0].shape:  torch.Size([2, 427, 640])
>> masks_list[1].shape:  torch.Size([5, 3024, 4032])
>> gt_mask.shape:  torch.Size([1, 427, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 640, 480])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 640, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 640, 480])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 640, 480])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 640, 480])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 500, 333])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 640, 480])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 500, 333])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 640, 480])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 500, 333])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([16, 512, 5120])
>> per_token_labels.shape:  torch.Size([16, 512])
>> masks_list[0].shape:  torch.Size([5, 640, 480])
>> masks_list[1].shape:  torch.Size([5, 480, 640])
>> gt_mask.shape:  torch.Size([1, 500, 333])
>> image_emb:  torch.Size([1, 256, 64, 64])
Epoch: [0][ 23/500]	Time 28.552 (28.552)	Loss 0.4955 (0.5161)	CeLoss 0.2324 (0.2251)	SegCLSLoss 0.1016 (0.0996)	KLLoss 0.0000 (0.0148)	MaskLoss 0.1056 (0.1170)	MaskBCELoss 0.0062 (0.0182)	MaskDICELoss 0.0994 (0.0988)
>> last_hidden_states.shape:  torch.Size([18, 482, 5120])
>> per_token_labels.shape:  torch.Size([18, 482])
>> masks_list[0].shape:  torch.Size([5, 640, 427])
>> masks_list[1].shape:  torch.Size([5, 2507, 4458])
>> gt_mask.shape:  torch.Size([1, 640, 427])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 482, 5120])
>> per_token_labels.shape:  torch.Size([18, 482])
>> masks_list[0].shape:  torch.Size([5, 640, 427])
>> masks_list[1].shape:  torch.Size([5, 2507, 4458])
>> gt_mask.shape:  torch.Size([1, 640, 427])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 332, 5120])
>> per_token_labels.shape:  torch.Size([18, 332])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([3, 698, 512])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([18, 332, 5120])
>> per_token_labels.shape:  torch.Size([18, 332])
>> masks_list[0].shape:  torch.Size([5, 480, 640])
>> masks_list[1].shape:  torch.Size([3, 698, 512])
>> gt_mask.shape:  torch.Size([1, 480, 640])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([10, 512, 5120])
>> per_token_labels.shape:  torch.Size([10, 512])
>> masks_list[0].shape:  torch.Size([4, 480, 360])
>> masks_list[1].shape:  torch.Size([4, 480, 360])
>> gt_mask.shape:  torch.Size([1, 480, 360])
>> image_emb:  torch.Size([1, 256, 64, 64])
>> last_hidden_states.shape:  torch.Size([10, 512, 5120])
>> per_token_labels.shape:  torch.Size([10, 512])
>> masks_list[0].shape:  torch.Size([4, 480, 360])
>> masks_list[1].shape:  torch.Size([4, 480, 360])
>> gt_mask.shape:  torch.Size([1, 480, 360])
