
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")


Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 3/3 [02:57<00:00, 59.18s/it]
Some weights of PLUMForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-llama-2-13b-chat-lightning-preview and are newly initialized: ['model.vision_tower.visual.transformer.resblocks.22.mlp.c_proj.weight', 'model.vision_tower.transformer.resblocks.10.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.1.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.10.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.14.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.23.attn.in_proj_weight', 'model.vision_tower.transformer.resblocks.4.mlp.c_fc.bias', 'model.vision_tower.transformer.resblocks.6.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.15.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.13.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.9.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.8.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.6.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.10.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.16.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.4.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.11.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.13.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.21.ln_1.bias', 'model.vision_tower.transformer.resblocks.3.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.8.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.8.mlp.c_proj.bias', 'model.vision_tower.visual.ln_pre.weight', 'model.vision_tower.transformer.resblocks.5.mlp.c_fc.bias', 'model.vision_tower.transformer.resblocks.9.ln_2.bias', 'model.vision_tower.transformer.resblocks.0.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.19.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.23.ln_1.weight', 'model.vision_tower.transformer.resblocks.5.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.10.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.3.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.4.mlp.c_fc.weight', 'model.vision_tower.transformer.resblocks.8.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.3.attn.in_proj_weight', 'model.vision_tower.transformer.resblocks.1.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.2.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.10.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.19.attn.in_proj_weight', 'model.vision_tower.transformer.resblocks.7.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.2.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.20.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.11.mlp.c_fc.weight', 'bio_encoder.encoder.layers.0.linear2.bias', 'model.vision_tower.visual.transformer.resblocks.3.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.23.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.17.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.10.mlp.c_proj.weight', 'model.vision_tower.ln_final.bias', 'model.vision_tower.transformer.resblocks.3.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.17.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.21.attn.in_proj_bias', 'bio_encoder.encoder.layers.0.linear1.weight', 'model.vision_tower.transformer.resblocks.11.ln_2.bias', 'model.vision_tower.transformer.resblocks.9.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.6.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.4.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.5.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.2.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.8.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.8.ln_1.weight', 'model.vision_tower.transformer.resblocks.4.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.7.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.2.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.18.mlp.c_proj.weight', 'model.vision_tower.visual.ln_post.weight', 'model.vision_tower.visual.transformer.resblocks.2.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.0.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.23.attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.7.mlp.c_fc.bias', 'model.vision_tower.transformer.resblocks.1.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.20.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.5.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.13.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.21.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.5.attn.in_proj_weight', 'model.vision_tower.transformer.resblocks.0.mlp.c_fc.bias', 'bio_encoder.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.5.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.13.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.20.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.3.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.14.mlp.c_fc.weight', 'bio_encoder.encoder.layers.0.norm1.weight', 'model.vision_tower.visual.transformer.resblocks.2.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.20.attn.in_proj_weight', 'model.vision_tower.visual.proj', 'model.vision_tower.visual.transformer.resblocks.5.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.16.mlp.c_proj.bias', 'model.vision_tower.transformer.resblocks.7.ln_1.weight', 'model.vision_tower.transformer.resblocks.11.ln_1.bias', 'model.vision_tower.transformer.resblocks.9.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.23.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.4.attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.0.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.10.attn.in_proj_weight', 'bio_encoder.encoder.layers.0.self_attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.20.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.1.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.10.mlp.c_fc.bias', 'model.vision_tower.transformer.resblocks.0.attn.in_proj_weight', 'bio_encoder.encoder.layers.0.norm2.bias', 'model.vision_tower.visual.transformer.resblocks.16.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.15.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.10.mlp.c_fc.bias', 'model.vision_tower.transformer.resblocks.3.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.15.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.7.mlp.c_proj.bias', 'model.vision_tower.transformer.resblocks.3.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.11.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.9.mlp.c_fc.bias', 'model.vision_tower.transformer.resblocks.3.attn.in_proj_weight', 'model.vision_tower.visual.ln_pre.bias', 'model.vision_tower.visual.transformer.resblocks.12.attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.1.ln_2.bias', 'model.vision_tower.transformer.resblocks.9.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.0.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.8.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.12.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.7.mlp.c_proj.weight', 'model.vision_tower.transformer.resblocks.2.ln_1.bias', 'model.vision_tower.transformer.resblocks.10.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.21.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.13.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.17.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.9.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.16.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.22.ln_2.bias', 'model.vision_tower.transformer.resblocks.8.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.4.ln_1.weight', 'model.vision_tower.transformer.resblocks.0.ln_2.bias', 'model.vision_tower.transformer.resblocks.4.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.7.mlp.c_fc.bias', 'model.vision_tower.transformer.resblocks.9.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.22.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.11.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.12.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.11.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.2.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.1.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.22.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.19.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.20.ln_2.bias', 'model.vision_tower.text_projection', 'model.vision_tower.visual.transformer.resblocks.7.mlp.c_fc.weight', 'model.vision_tower.transformer.resblocks.8.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.11.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.8.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.14.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.0.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.0.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.4.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.4.mlp.c_proj.bias', 'model.vision_tower.transformer.resblocks.2.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.1.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.9.attn.in_proj_weight', 'model.vision_tower.transformer.resblocks.5.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.6.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.5.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.5.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.17.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.0.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.6.ln_1.weight', 'model.vision_tower.transformer.resblocks.10.attn.in_proj_weight', 'model.vision_tower.transformer.resblocks.3.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.18.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.1.ln_2.weight', 'model.vision_tower.transformer.resblocks.1.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.11.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.6.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.11.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.3.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.17.mlp.c_fc.weight', 'model.vision_tower.transformer.resblocks.6.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.18.mlp.c_proj.bias', 'model.vision_tower.transformer.resblocks.4.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.14.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.17.mlp.c_fc.bias', 'model.vision_tower.transformer.resblocks.8.ln_2.bias', 'model.vision_tower.transformer.resblocks.1.attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.8.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.7.ln_2.weight', 'model.vision_tower.transformer.resblocks.1.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.17.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.14.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.10.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.6.mlp.c_fc.weight', 'model.vision_tower.transformer.resblocks.6.attn.out_proj.bias', 'bio_encoder.encoder.layers.0.norm2.weight', 'model.vision_tower.transformer.resblocks.11.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.18.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.13.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.21.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.13.attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.11.ln_1.weight', 'model.vision_tower.transformer.resblocks.4.mlp.c_proj.bias', 'model.vision_tower.transformer.resblocks.5.mlp.c_fc.weight', 'model.vision_tower.transformer.resblocks.3.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.2.ln_2.weight', 'model.vision_tower.transformer.resblocks.11.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.11.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.0.ln_2.bias', 'model.vision_tower.transformer.resblocks.3.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.4.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.23.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.4.attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.1.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.9.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.4.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.8.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.13.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.6.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.3.ln_1.weight', 'model.vision_tower.transformer.resblocks.5.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.2.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.11.ln_2.bias', 'model.vision_tower.transformer.resblocks.11.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.5.attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.11.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.9.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.5.ln_1.bias', 'model.vision_tower.transformer.resblocks.11.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.0.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.5.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.12.mlp.c_proj.bias', 'model.vision_tower.transformer.resblocks.2.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.21.mlp.c_fc.bias', 'model.vision_tower.transformer.resblocks.5.mlp.c_proj.bias', 'model.vision_tower.transformer.resblocks.8.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.0.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.20.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.20.ln_2.weight', 'model.vision_tower.transformer.resblocks.7.mlp.c_proj.weight', 'model.vision_tower.transformer.resblocks.8.attn.in_proj_weight', 'model.vision_tower.transformer.resblocks.10.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.16.mlp.c_fc.weight', 'model.vision_tower.transformer.resblocks.6.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.1.attn.in_proj_bias', 'model.vision_tower.visual.ln_post.bias', 'model.vision_tower.visual.transformer.resblocks.22.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.18.ln_1.weight', 'model.vision_tower.transformer.resblocks.5.ln_2.bias', 'model.vision_tower.transformer.resblocks.6.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.19.ln_1.bias', 'model.vision_tower.visual.conv1.weight', 'model.vision_tower.transformer.resblocks.0.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.6.mlp.c_fc.bias', 'model.vision_tower.transformer.resblocks.2.mlp.c_proj.weight', 'model.vision_tower.transformer.resblocks.5.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.17.ln_1.bias', 'model.vision_tower.transformer.resblocks.10.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.9.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.20.mlp.c_proj.bias', 'model.vision_tower.transformer.resblocks.11.attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.2.mlp.c_fc.bias', 'model.vision_tower.positional_embedding', 'model.vision_tower.visual.transformer.resblocks.1.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.16.attn.in_proj_bias', 'model.vision_tower.token_embedding.weight', 'model.vision_tower.visual.transformer.resblocks.22.mlp.c_proj.bias', 'model.vision_tower.transformer.resblocks.7.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.7.ln_2.bias', 'model.vision_tower.transformer.resblocks.8.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.23.ln_2.bias', 'model.vision_tower.transformer.resblocks.8.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.1.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.13.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.5.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.9.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.14.ln_1.bias', 'model.vision_tower.transformer.resblocks.0.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.18.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.23.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.1.ln_1.weight', 'model.vision_tower.transformer.resblocks.10.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.15.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.11.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.15.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.3.mlp.c_proj.bias', 'model.vision_tower.transformer.resblocks.4.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.0.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.15.ln_1.weight', 'model.vision_tower.transformer.resblocks.3.attn.in_proj_bias', 'model.vision_tower.logit_scale', 'model.vision_tower.visual.transformer.resblocks.16.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.1.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.0.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.16.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.7.attn.in_proj_weight', 'bio_encoder.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.6.ln_1.weight', 'model.vision_tower.transformer.resblocks.11.mlp.c_proj.bias', 'model.vision_tower.transformer.resblocks.2.attn.in_proj_weight', 'model.vision_tower.transformer.resblocks.10.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.21.attn.in_proj_weight', 'model.vision_tower.transformer.resblocks.6.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.2.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.18.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.1.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.3.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.14.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.5.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.22.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.23.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.9.mlp.c_fc.weight', 'model.vision_tower.transformer.resblocks.1.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.0.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.22.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.2.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.0.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.13.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.0.mlp.c_proj.bias', 'model.vision_tower.transformer.resblocks.11.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.21.mlp.c_fc.weight', 'model.vision_tower.transformer.resblocks.4.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.5.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.21.attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.9.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.12.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.22.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.18.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.5.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.3.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.15.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.15.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.6.mlp.c_proj.weight', 'model.vision_tower.transformer.resblocks.9.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.21.mlp.c_proj.weight', 'model.vision_tower.transformer.resblocks.4.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.0.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.7.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.9.ln_1.bias', 'model.vision_tower.transformer.resblocks.5.ln_2.weight', 'model.vision_tower.transformer.resblocks.8.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.19.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.23.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.10.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.6.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.20.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.6.ln_2.bias', 'model.vision_tower.transformer.resblocks.10.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.18.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.5.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.10.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.7.ln_2.bias', 'model.vision_tower.transformer.resblocks.0.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.7.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.17.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.2.attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.10.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.3.mlp.c_proj.weight', 'model.vision_tower.transformer.resblocks.9.mlp.c_proj.bias', 'model.vision_tower.transformer.resblocks.0.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.14.ln_2.bias', 'model.vision_tower.visual.positional_embedding', 'bio_encoder.encoder.layers.0.linear2.weight', 'model.vision_tower.visual.transformer.resblocks.1.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.9.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.11.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.4.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.10.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.23.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.6.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.12.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.13.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.17.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.9.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.17.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.21.ln_2.bias', 'model.vision_tower.visual.transformer.resblocks.19.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.1.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.7.ln_1.weight', 'bio_encoder.encoder.layers.0.norm1.bias', 'model.vision_tower.visual.transformer.resblocks.21.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.20.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.16.ln_1.weight', 'model.vision_tower.ln_final.weight', 'model.vision_tower.visual.transformer.resblocks.18.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.9.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.3.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.19.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.3.attn.out_proj.bias', 'model.vision_tower.transformer.resblocks.7.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.18.mlp.c_fc.bias', 'model.vision_tower.transformer.resblocks.7.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.1.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.16.ln_2.weight', 'model.vision_tower.transformer.resblocks.6.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.12.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.17.mlp.c_proj.bias', 'model.vision_tower.visual.transformer.resblocks.19.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.10.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.22.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.15.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.2.attn.in_proj_bias', 'model.vision_tower.transformer.resblocks.10.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.19.ln_1.weight', 'model.vision_tower.transformer.resblocks.9.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.20.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.2.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.8.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.2.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.14.mlp.c_fc.bias', 'model.vision_tower.transformer.resblocks.7.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.8.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.12.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.2.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.12.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.14.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.15.attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.4.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.9.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.16.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.15.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.19.mlp.c_proj.weight', 'model.vision_tower.transformer.resblocks.7.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.12.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.22.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.4.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.18.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.4.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.15.mlp.c_fc.weight', 'model.vision_tower.transformer.resblocks.4.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.12.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.22.ln_1.bias', 'model.vision_tower.visual.class_embedding', 'model.vision_tower.transformer.resblocks.2.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.9.attn.out_proj.weight', 'bio_encoder.encoder.layers.0.self_attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.4.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.6.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.8.mlp.c_proj.weight', 'model.vision_tower.visual.transformer.resblocks.7.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.19.ln_2.weight', 'model.vision_tower.transformer.resblocks.10.ln_2.bias', 'model.vision_tower.transformer.resblocks.6.mlp.c_proj.weight', 'model.vision_tower.transformer.resblocks.1.mlp.c_fc.bias', 'bio_encoder.encoder.layers.0.linear1.bias', 'model.vision_tower.transformer.resblocks.7.mlp.c_fc.weight', 'model.vision_tower.visual.transformer.resblocks.16.attn.in_proj_weight', 'model.vision_tower.transformer.resblocks.6.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.0.mlp.c_fc.weight', 'model.vision_tower.transformer.resblocks.8.attn.out_proj.weight', 'model.vision_tower.transformer.resblocks.6.attn.in_proj_bias', 'model.vision_tower.visual.transformer.resblocks.14.attn.out_proj.bias', 'model.vision_tower.visual.transformer.resblocks.19.ln_2.bias', 'model.vision_tower.transformer.resblocks.1.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.12.attn.in_proj_weight', 'model.vision_tower.visual.transformer.resblocks.8.ln_2.bias', 'model.vision_tower.transformer.resblocks.3.ln_1.bias', 'model.vision_tower.transformer.resblocks.2.ln_1.weight', 'model.vision_tower.visual.transformer.resblocks.23.ln_2.weight', 'model.vision_tower.visual.transformer.resblocks.11.ln_2.weight', 'model.vision_tower.transformer.resblocks.3.ln_2.weight', 'model.vision_tower.transformer.resblocks.7.attn.out_proj.weight', 'model.vision_tower.visual.transformer.resblocks.3.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.13.mlp.c_fc.bias', 'model.vision_tower.visual.transformer.resblocks.8.ln_1.bias', 'model.vision_tower.visual.transformer.resblocks.14.mlp.c_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 6,553,600 || all params: 14,276,015,668 || trainable%: 0.045906365980600855
>> model.config.train_mask_prompt_encoder:  True
n:  base_model.model.model.embed_tokens.weight p.shape:  torch.Size([32002, 5120])
n:  base_model.model.model.visual_model.prompt_encoder.point_embeddings.0.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.prompt_encoder.point_embeddings.1.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.prompt_encoder.point_embeddings.2.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.prompt_encoder.point_embeddings.3.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.prompt_encoder.not_a_point_embed.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.0.weight p.shape:  torch.Size([4, 1, 2, 2])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.0.bias p.shape:  torch.Size([4])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.1.weight p.shape:  torch.Size([4])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.1.bias p.shape:  torch.Size([4])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.3.weight p.shape:  torch.Size([16, 4, 2, 2])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.3.bias p.shape:  torch.Size([16])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.4.weight p.shape:  torch.Size([16])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.4.bias p.shape:  torch.Size([16])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.6.weight p.shape:  torch.Size([256, 16, 1, 1])
n:  base_model.model.model.visual_model.prompt_encoder.mask_downscaling.6.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.prompt_encoder.no_mask_embed.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_token.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.mask_decoder.mask_tokens.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.weight p.shape:  torch.Size([256, 64, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.weight p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.weight p.shape:  torch.Size([64, 32, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.bias p.shape:  torch.Size([4])
n:  base_model.model.model.text_hidden_fcs.0.0.weight p.shape:  torch.Size([5120, 5120])
n:  base_model.model.model.text_hidden_fcs.0.0.bias p.shape:  torch.Size([5120])
n:  base_model.model.model.text_hidden_fcs.0.2.weight p.shape:  torch.Size([256, 5120])
n:  base_model.model.model.text_hidden_fcs.0.2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.token_to_mask_fcs.0.0.weight p.shape:  torch.Size([5120, 5120])
n:  base_model.model.model.token_to_mask_fcs.0.0.bias p.shape:  torch.Size([5120])
n:  base_model.model.model.token_to_mask_fcs.0.2.weight p.shape:  torch.Size([3, 5120])
n:  base_model.model.model.token_to_mask_fcs.0.2.bias p.shape:  torch.Size([3])
n:  base_model.model.lm_head.weight p.shape:  torch.Size([32002, 5120])
n:  base_model.model.bio_encoder.encoder.layers.0.self_attn.in_proj_weight p.shape:  torch.Size([15360, 5120])
n:  base_model.model.bio_encoder.encoder.layers.0.self_attn.in_proj_bias p.shape:  torch.Size([15360])
n:  base_model.model.bio_encoder.encoder.layers.0.self_attn.out_proj.weight p.shape:  torch.Size([5120, 5120])
n:  base_model.model.bio_encoder.encoder.layers.0.self_attn.out_proj.bias p.shape:  torch.Size([5120])
n:  base_model.model.bio_encoder.encoder.layers.0.linear1.weight p.shape:  torch.Size([2048, 5120])
n:  base_model.model.bio_encoder.encoder.layers.0.linear1.bias p.shape:  torch.Size([2048])
n:  base_model.model.bio_encoder.encoder.layers.0.linear2.weight p.shape:  torch.Size([5120, 2048])
n:  base_model.model.bio_encoder.encoder.layers.0.linear2.bias p.shape:  torch.Size([5120])
n:  base_model.model.bio_encoder.encoder.layers.0.norm1.weight p.shape:  torch.Size([5120])
n:  base_model.model.bio_encoder.encoder.layers.0.norm1.bias p.shape:  torch.Size([5120])
n:  base_model.model.bio_encoder.encoder.layers.0.norm2.weight p.shape:  torch.Size([5120])
n:  base_model.model.bio_encoder.encoder.layers.0.norm2.bias p.shape:  torch.Size([5120])
ade20k:  20210
cocostuff:  118287
loading annotations into memory...
Done (t=0.73s)
creating index...
index created!
pascal_part:  4366
loading annotations into memory...
Done (t=9.47s)
creating index...
index created!
paco_lvis:  45790
mapillary:  18000
loading dataset refclef into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refclef/refs(unc).p
creating index...
index created.
DONE (t=3.36s)
dataset refclef (refs unc) (train split) has 17978 images and 99523 annotations.
loading dataset refcoco into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco/refs(unc).p
creating index...
index created.
DONE (t=6.11s)
dataset refcoco (refs unc) (train split) has 16994 images and 196771 annotations.
loading dataset refcoco+ into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco+/refs(unc).p
creating index...
index created.
DONE (t=6.97s)
dataset refcoco+ (refs unc) (train split) has 16992 images and 196737 annotations.
loading dataset refcocog into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcocog/refs(umd).p
creating index...
index created.
DONE (t=4.00s)
dataset refcocog (refs umd) (train split) has 21899 images and 208960 annotations.
vqa_data:  157712
number of reason_seg samples:  239
len(self.img_to_explanation):  239
Training with 20000 examples.
Validating with 200 examples.
[2025-04-28 21:18:06,722] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5, git-hash=unknown, git-branch=unknown
[2025-04-28 21:18:06,723] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2025-04-28 21:18:06,723] [INFO] [comm.py:594:init_distributed] cdb=None
[2025-04-28 21:18:06,723] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Using /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
[2025-04-28 21:18:26,807] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Time to load fused_adam op: 0.9760956764221191 seconds
[2025-04-28 21:18:28,057] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-28 21:18:28,332] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-28 21:18:28,334] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-28 21:18:28,334] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-04-28 21:18:28,335] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500000000
[2025-04-28 21:18:28,335] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500000000
[2025-04-28 21:18:28,335] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2025-04-28 21:18:28,336] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Detected CUDA files, patching ldflags
Emitting ninja build file /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module fused_adam...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/tensor/python_tensor.cpp:78.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
Rank: 0 partition count [1] and sizes[(517961268, False)]
[2025-04-28 21:18:36,508] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2025-04-28 21:18:36,509] [INFO] [utils.py:786:see_memory_usage] MA 28.73 GB         Max_MA 29.69 GB         CA 29.91 GB         Max_CA 30 GB
[2025-04-28 21:18:36,509] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 80.66 GB, percent = 8.0%
[2025-04-28 21:18:41,648] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2025-04-28 21:18:41,649] [INFO] [utils.py:786:see_memory_usage] MA 32.59 GB         Max_MA 34.52 GB         CA 35.7 GB         Max_CA 36 GB
[2025-04-28 21:18:41,650] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 80.65 GB, percent = 8.0%
[2025-04-28 21:18:41,650] [INFO] [stage_1_and_2.py:488:__init__] optimizer state initialized
[2025-04-28 21:18:45,318] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2025-04-28 21:18:45,319] [INFO] [utils.py:786:see_memory_usage] MA 32.59 GB         Max_MA 32.59 GB         CA 35.7 GB         Max_CA 36 GB
[2025-04-28 21:18:45,319] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 80.69 GB, percent = 8.0%
[2025-04-28 21:18:45,326] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-04-28 21:18:45,326] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2025-04-28 21:18:45,326] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7fecc383ee60>
[2025-04-28 21:18:45,326] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
[2025-04-28 21:18:45,329] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2025-04-28 21:18:45,330] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-04-28 21:18:45,330] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-04-28 21:18:45,330] [INFO] [config.py:964:print]   amp_enabled .................. False
[2025-04-28 21:18:45,330] [INFO] [config.py:964:print]   amp_params ................... False
[2025-04-28 21:18:45,330] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-28 21:18:45,331] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2025-04-28 21:18:45,331] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2025-04-28 21:18:45,331] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2025-04-28 21:18:45,331] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2025-04-28 21:18:45,331] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fef2d88bdc0>
[2025-04-28 21:18:45,331] [INFO] [config.py:964:print]   communication_data_type ...... None
[2025-04-28 21:18:45,331] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-28 21:18:45,331] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2025-04-28 21:18:45,331] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2025-04-28 21:18:45,331] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-28 21:18:45,331] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2025-04-28 21:18:45,331] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2025-04-28 21:18:45,331] [INFO] [config.py:964:print]   disable_allgather ............ False
[2025-04-28 21:18:45,331] [INFO] [config.py:964:print]   dump_state ................... False
[2025-04-28 21:18:45,332] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2025-04-28 21:18:45,332] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2025-04-28 21:18:45,332] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-28 21:18:45,332] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-28 21:18:45,332] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2025-04-28 21:18:45,332] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2025-04-28 21:18:45,332] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2025-04-28 21:18:45,332] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2025-04-28 21:18:45,332] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2025-04-28 21:18:45,332] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2025-04-28 21:18:45,332] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-04-28 21:18:45,332] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2025-04-28 21:18:45,332] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2025-04-28 21:18:45,332] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2025-04-28 21:18:45,333] [INFO] [config.py:964:print]   global_rank .................. 0
[2025-04-28 21:18:45,333] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2025-04-28 21:18:45,333] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 10
[2025-04-28 21:18:45,333] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0
[2025-04-28 21:18:45,333] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2025-04-28 21:18:45,333] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-28 21:18:45,333] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2025-04-28 21:18:45,333] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2025-04-28 21:18:45,333] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2025-04-28 21:18:45,333] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2025-04-28 21:18:45,333] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2025-04-28 21:18:45,333] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2025-04-28 21:18:45,333] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-04-28 21:18:45,334] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-04-28 21:18:45,334] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2025-04-28 21:18:45,334] [INFO] [config.py:964:print]   optimizer_name ............... adamw
[2025-04-28 21:18:45,334] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.0003, 'weight_decay': 0.0, 'betas': (0.9, 0.95)}
[2025-04-28 21:18:45,334] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2025-04-28 21:18:45,334] [INFO] [config.py:964:print]   pld_enabled .................. False
[2025-04-28 21:18:45,334] [INFO] [config.py:964:print]   pld_params ................... False
[2025-04-28 21:18:45,334] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2025-04-28 21:18:45,334] [INFO] [config.py:964:print]   scheduler_name ............... WarmupDecayLR
[2025-04-28 21:18:45,334] [INFO] [config.py:964:print]   scheduler_params ............. {'total_num_steps': 12500, 'warmup_min_lr': 0, 'warmup_max_lr': 0.0003, 'warmup_num_steps': 100, 'warmup_type': 'linear'}
[2025-04-28 21:18:45,334] [INFO] [config.py:964:print]   sparse_attention ............. None
[2025-04-28 21:18:45,334] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2025-04-28 21:18:45,334] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2025-04-28 21:18:45,334] [INFO] [config.py:964:print]   train_batch_size ............. 40
[2025-04-28 21:18:45,334] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  4
[2025-04-28 21:18:45,335] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2025-04-28 21:18:45,335] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2025-04-28 21:18:45,335] [INFO] [config.py:964:print]   world_size ................... 1
[2025-04-28 21:18:45,335] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2025-04-28 21:18:45,335] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2025-04-28 21:18:45,335] [INFO] [config.py:964:print]   zero_enabled ................. True
[2025-04-28 21:18:45,335] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-28 21:18:45,335] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2025-04-28 21:18:45,335] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 4,
    "gradient_accumulation_steps": 10,
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 0.0003,
            "weight_decay": 0.0,
            "betas": [0.9, 0.95]
        }
    },
    "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "total_num_steps": 1.250000e+04,
            "warmup_min_lr": 0,
            "warmup_max_lr": 0.0003,
            "warmup_num_steps": 100,
            "warmup_type": "linear"
        }
    },
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "gradient_clipping": 1.0,
    "zero_optimization": {
        "stage": 2,
        "contiguous_gradients": true,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 5.000000e+08,
        "allgather_bucket_size": 5.000000e+08
    }
}
(train) >> AFTER DEEPSPEED
>> (train) Auto-resume from:  ./runs/plum-13b_kld_0_dice_0_v1_0shot/plum-13b_kld_0_dice_0_v1_0shot_bidirbio_2048_accum_10_maxlen512_epochs25_segloss_0_bce_loss_2_kld_loss_0_dice_loss_0_bidir_bio_train_prompt_enc_naclip_ckpt_model
>> (train) resume exists:  False
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)
  return F.conv2d(input, weight, bias, self.stride,
> /shared/nas2/jk100/partonomy_private/src/models/PLUM/model/llava/model/multimodal_encoder/NACLIP/clip/model.py(325)forward()
-> image_features = self.encode_image(image)
tensor([[[[-1.2344e+00, -1.2500e+00, -1.1641e+00,  ..., -6.6797e-01,
           -1.0781e+00, -1.2656e+00],
          [-1.1641e+00, -1.2266e+00, -1.1641e+00,  ..., -1.2793e-01,
           -8.0078e-01, -1.3828e+00],
          [-1.0078e+00, -1.1172e+00, -1.2812e+00,  ..., -1.8652e-01,
           -1.1172e+00, -1.2500e+00],
          ...,
          [ 4.2578e-01,  3.3984e-01,  4.4141e-01,  ...,  2.2266e-01,
            1.6406e-01,  2.8125e-01],
          [ 1.9336e-01,  2.9492e-01,  9.0820e-02,  ...,  4.7119e-02,
            1.7871e-01,  3.1055e-01],
          [ 6.1768e-02,  1.2012e-01, -2.5879e-02,  ...,  4.7119e-02,
            7.6172e-02, -1.1328e-01]],
         [[-9.7266e-01, -1.0156e+00, -9.4141e-01,  ..., -4.9219e-01,
           -8.5156e-01, -1.0625e+00],
          [-9.1016e-01, -1.0156e+00, -9.4141e-01,  ..., -1.1169e-02,
           -5.8203e-01, -1.1484e+00],
          [-8.0469e-01, -8.9844e-01, -1.0469e+00,  ..., -8.6426e-02,
           -9.1016e-01, -9.8828e-01],
          ...,
          [ 8.0078e-01,  6.9531e-01,  8.4375e-01,  ...,  5.4297e-01,
            4.8438e-01,  5.8984e-01],
          [ 6.1719e-01,  7.1094e-01,  4.3945e-01,  ...,  4.6875e-01,
            6.1719e-01,  7.1094e-01],
          [ 5.2734e-01,  5.7422e-01,  3.7891e-01,  ...,  4.8438e-01,
            5.1562e-01,  3.6328e-01]],
         [[-9.1016e-01, -9.6875e-01, -9.2578e-01,  ..., -5.0000e-01,
           -8.1250e-01, -9.9609e-01],
          [-8.8281e-01, -9.5312e-01, -9.1016e-01,  ..., -1.3351e-03,
           -5.2734e-01, -1.0547e+00],
          [-7.9688e-01, -8.6719e-01, -1.0078e+00,  ..., -1.0107e-01,
           -8.8281e-01, -9.9609e-01],
          ...,
          [-2.7148e-01, -3.2812e-01, -3.5742e-01,  ..., -5.0000e-01,
           -5.8594e-01, -5.0000e-01],
          [-5.9766e-01, -4.5703e-01, -5.1172e-01,  ..., -6.9922e-01,
           -5.4297e-01, -3.5742e-01],
          [-6.9922e-01, -6.2891e-01, -6.2891e-01,  ..., -5.8594e-01,
           -4.0039e-01, -7.6953e-01]]],
        [[[-1.2656e+00, -1.1328e+00, -1.0078e+00,  ..., -1.6719e+00,
           -1.6172e+00, -1.5859e+00],
          [-9.2969e-01, -1.1328e+00, -1.0625e+00,  ..., -1.6719e+00,
           -1.6172e+00, -1.6016e+00],
          [-1.0312e+00, -1.1953e+00, -1.1484e+00,  ..., -1.5625e+00,
           -1.5859e+00, -1.6016e+00],
          ...,
          [ 1.1250e+00,  1.0859e+00,  9.0820e-02,  ..., -1.1230e-02,
           -2.1582e-01,  2.3730e-01],
          [ 1.3594e+00,  1.0391e+00,  3.5352e-01,  ..., -1.1328e-01,
           -4.0527e-02,  2.8125e-01],
          [ 9.2188e-01,  1.1562e+00,  3.6914e-01,  ..., -1.8652e-01,
           -3.6133e-01, -2.8906e-01]],
         [[-1.1953e+00, -1.1094e+00, -9.8828e-01,  ..., -1.6172e+00,
           -1.5703e+00, -1.5547e+00],
          [-8.5156e-01, -1.0625e+00, -9.4141e-01,  ..., -1.6172e+00,
           -1.5859e+00, -1.5703e+00],
          [-9.4141e-01, -1.0938e+00, -9.5703e-01,  ..., -1.5391e+00,
           -1.5547e+00, -1.5703e+00],
          ...,
          [ 1.1562e+00,  1.2344e+00,  7.5391e-01,  ..., -2.6245e-02,
           -5.6152e-02,  3.0469e-01],
          [ 1.3281e+00,  1.3984e+00,  1.0859e+00,  ..., -1.9141e-01,
            1.2402e-01,  4.0820e-01],
          [ 1.2500e+00,  1.4609e+00,  9.0234e-01,  ..., -1.0107e-01,
           -1.6113e-01,  4.8828e-02]],
         [[-1.2109e+00, -1.1641e+00, -1.1250e+00,  ..., -1.4375e+00,
           -1.3984e+00, -1.3672e+00],
          [-7.9688e-01, -1.0078e+00, -1.0234e+00,  ..., -1.4375e+00,
           -1.3984e+00, -1.3672e+00],
          [-8.5547e-01, -1.0391e+00, -1.0938e+00,  ..., -1.3203e+00,
           -1.3516e+00, -1.3672e+00],
          ...,
          [ 8.7891e-01,  8.9453e-01,  2.7100e-02,  ..., -3.1445e-01,
           -3.5742e-01,  1.4062e-01],
          [ 1.1953e+00,  8.9453e-01,  3.2617e-01,  ..., -3.5742e-01,
           -2.4316e-01,  4.1260e-02],
          [ 7.6562e-01,  1.1484e+00,  1.1230e-01,  ..., -3.2812e-01,
           -4.4141e-01, -2.8516e-01]]],
        [[[-1.2656e+00, -1.1328e+00, -1.0078e+00,  ..., -1.6719e+00,
           -1.6172e+00, -1.5859e+00],
          [-9.2969e-01, -1.1328e+00, -1.0625e+00,  ..., -1.6719e+00,
           -1.6172e+00, -1.6016e+00],
          [-1.0312e+00, -1.1953e+00, -1.1484e+00,  ..., -1.5625e+00,
           -1.5859e+00, -1.6016e+00],
          ...,
          [ 1.1250e+00,  1.0859e+00,  9.0820e-02,  ..., -1.1230e-02,
           -2.1582e-01,  2.3730e-01],
          [ 1.3594e+00,  1.0391e+00,  3.5352e-01,  ..., -1.1328e-01,
           -4.0527e-02,  2.8125e-01],
          [ 9.2188e-01,  1.1562e+00,  3.6914e-01,  ..., -1.8652e-01,
           -3.6133e-01, -2.8906e-01]],
         [[-1.1953e+00, -1.1094e+00, -9.8828e-01,  ..., -1.6172e+00,
           -1.5703e+00, -1.5547e+00],
          [-8.5156e-01, -1.0625e+00, -9.4141e-01,  ..., -1.6172e+00,
           -1.5859e+00, -1.5703e+00],
          [-9.4141e-01, -1.0938e+00, -9.5703e-01,  ..., -1.5391e+00,
           -1.5547e+00, -1.5703e+00],
          ...,
          [ 1.1562e+00,  1.2344e+00,  7.5391e-01,  ..., -2.6245e-02,
           -5.6152e-02,  3.0469e-01],
          [ 1.3281e+00,  1.3984e+00,  1.0859e+00,  ..., -1.9141e-01,
            1.2402e-01,  4.0820e-01],
          [ 1.2500e+00,  1.4609e+00,  9.0234e-01,  ..., -1.0107e-01,
           -1.6113e-01,  4.8828e-02]],
         [[-1.2109e+00, -1.1641e+00, -1.1250e+00,  ..., -1.4375e+00,
           -1.3984e+00, -1.3672e+00],
          [-7.9688e-01, -1.0078e+00, -1.0234e+00,  ..., -1.4375e+00,
           -1.3984e+00, -1.3672e+00],
          [-8.5547e-01, -1.0391e+00, -1.0938e+00,  ..., -1.3203e+00,
           -1.3516e+00, -1.3672e+00],
          ...,
          [ 8.7891e-01,  8.9453e-01,  2.7100e-02,  ..., -3.1445e-01,
           -3.5742e-01,  1.4062e-01],
          [ 1.1953e+00,  8.9453e-01,  3.2617e-01,  ..., -3.5742e-01,
           -2.4316e-01,  4.1260e-02],
          [ 7.6562e-01,  1.1484e+00,  1.1230e-01,  ..., -3.2812e-01,
           -4.4141e-01, -2.8516e-01]]],
        ...,
        [[[ 1.4922e+00,  1.6094e+00,  1.8125e+00,  ..., -1.8652e-01,
           -1.8652e-01, -1.8652e-01],
          [ 1.6250e+00,  1.5625e+00,  1.3203e+00,  ..., -2.0117e-01,
           -2.4512e-01, -2.3047e-01],
          [ 1.1875e+00,  1.5625e+00,  1.6250e+00,  ..., -2.0117e-01,
           -2.4512e-01, -2.3047e-01],
          ...,
          [ 1.7871e-01,  4.7119e-02, -1.2793e-01,  ..., -1.0469e+00,
           -1.0781e+00, -1.1172e+00],
          [-2.7344e-01, -2.5977e-01, -2.8906e-01,  ..., -1.3516e+00,
           -1.3281e+00, -1.2812e+00],
          [-4.1992e-01, -3.9062e-01,  6.1768e-02,  ..., -1.4688e+00,
           -1.4531e+00, -1.4297e+00]],
         [[ 1.6406e+00,  1.7422e+00,  1.9688e+00,  ...,  4.3945e-01,
            4.3945e-01,  4.3945e-01],
          [ 1.7734e+00,  1.7109e+00,  1.4453e+00,  ...,  4.8438e-01,
            5.0000e-01,  5.0000e-01],
          [ 1.3125e+00,  1.7031e+00,  1.7578e+00,  ...,  5.4297e-01,
            5.7422e-01,  5.5859e-01],
          ...,
          [ 9.3750e-02, -7.1289e-02, -2.2168e-01,  ..., -1.1641e+00,
           -1.1797e+00, -1.2422e+00],
          [-4.9219e-01, -4.9219e-01, -5.0781e-01,  ..., -1.5000e+00,
           -1.4688e+00, -1.4219e+00],
          [-7.0312e-01, -7.0312e-01, -2.2168e-01,  ..., -1.6328e+00,
           -1.5859e+00, -1.5703e+00]],
         [[ 1.5234e+00,  1.6328e+00,  1.8203e+00,  ...,  1.3203e+00,
            1.3672e+00,  1.3672e+00],
          [ 1.6797e+00,  1.6172e+00,  1.3672e+00,  ...,  1.1953e+00,
            1.2109e+00,  1.1953e+00],
          [ 1.2891e+00,  1.6641e+00,  1.7188e+00,  ...,  1.2500e+00,
            1.2812e+00,  1.2656e+00],
          ...,
          [-2.9785e-02, -1.8652e-01, -4.2773e-01,  ..., -1.1094e+00,
           -1.1562e+00, -1.1953e+00],
          [-6.4062e-01, -6.6797e-01, -8.1250e-01,  ..., -1.3672e+00,
           -1.3516e+00, -1.2969e+00],
          [-9.1016e-01, -9.2578e-01, -6.2891e-01,  ..., -1.4531e+00,
           -1.4219e+00, -1.3984e+00]]],
        [[[ 1.4922e+00,  1.6094e+00,  1.8125e+00,  ..., -1.8652e-01,
           -1.8652e-01, -1.8652e-01],
          [ 1.6250e+00,  1.5625e+00,  1.3203e+00,  ..., -2.0117e-01,
           -2.4512e-01, -2.3047e-01],
          [ 1.1875e+00,  1.5625e+00,  1.6250e+00,  ..., -2.0117e-01,
           -2.4512e-01, -2.3047e-01],
          ...,
          [ 1.7871e-01,  4.7119e-02, -1.2793e-01,  ..., -1.0469e+00,
           -1.0781e+00, -1.1172e+00],
          [-2.7344e-01, -2.5977e-01, -2.8906e-01,  ..., -1.3516e+00,
           -1.3281e+00, -1.2812e+00],
          [-4.1992e-01, -3.9062e-01,  6.1768e-02,  ..., -1.4688e+00,
           -1.4531e+00, -1.4297e+00]],
         [[ 1.6406e+00,  1.7422e+00,  1.9688e+00,  ...,  4.3945e-01,
            4.3945e-01,  4.3945e-01],
          [ 1.7734e+00,  1.7109e+00,  1.4453e+00,  ...,  4.8438e-01,
            5.0000e-01,  5.0000e-01],
          [ 1.3125e+00,  1.7031e+00,  1.7578e+00,  ...,  5.4297e-01,
            5.7422e-01,  5.5859e-01],
          ...,
          [ 9.3750e-02, -7.1289e-02, -2.2168e-01,  ..., -1.1641e+00,
           -1.1797e+00, -1.2422e+00],
          [-4.9219e-01, -4.9219e-01, -5.0781e-01,  ..., -1.5000e+00,
           -1.4688e+00, -1.4219e+00],
          [-7.0312e-01, -7.0312e-01, -2.2168e-01,  ..., -1.6328e+00,
           -1.5859e+00, -1.5703e+00]],
         [[ 1.5234e+00,  1.6328e+00,  1.8203e+00,  ...,  1.3203e+00,
            1.3672e+00,  1.3672e+00],
          [ 1.6797e+00,  1.6172e+00,  1.3672e+00,  ...,  1.1953e+00,
            1.2109e+00,  1.1953e+00],
          [ 1.2891e+00,  1.6641e+00,  1.7188e+00,  ...,  1.2500e+00,
            1.2812e+00,  1.2656e+00],
          ...,
          [-2.9785e-02, -1.8652e-01, -4.2773e-01,  ..., -1.1094e+00,
           -1.1562e+00, -1.1953e+00],
          [-6.4062e-01, -6.6797e-01, -8.1250e-01,  ..., -1.3672e+00,
           -1.3516e+00, -1.2969e+00],
          [-9.1016e-01, -9.2578e-01, -6.2891e-01,  ..., -1.4531e+00,
           -1.4219e+00, -1.3984e+00]]],
        [[[ 1.4922e+00,  1.6094e+00,  1.8125e+00,  ..., -1.8652e-01,
           -1.8652e-01, -1.8652e-01],
          [ 1.6250e+00,  1.5625e+00,  1.3203e+00,  ..., -2.0117e-01,
           -2.4512e-01, -2.3047e-01],
          [ 1.1875e+00,  1.5625e+00,  1.6250e+00,  ..., -2.0117e-01,
           -2.4512e-01, -2.3047e-01],
          ...,
          [ 1.7871e-01,  4.7119e-02, -1.2793e-01,  ..., -1.0469e+00,
           -1.0781e+00, -1.1172e+00],
          [-2.7344e-01, -2.5977e-01, -2.8906e-01,  ..., -1.3516e+00,
           -1.3281e+00, -1.2812e+00],
          [-4.1992e-01, -3.9062e-01,  6.1768e-02,  ..., -1.4688e+00,
           -1.4531e+00, -1.4297e+00]],
         [[ 1.6406e+00,  1.7422e+00,  1.9688e+00,  ...,  4.3945e-01,
            4.3945e-01,  4.3945e-01],
          [ 1.7734e+00,  1.7109e+00,  1.4453e+00,  ...,  4.8438e-01,
            5.0000e-01,  5.0000e-01],
          [ 1.3125e+00,  1.7031e+00,  1.7578e+00,  ...,  5.4297e-01,
            5.7422e-01,  5.5859e-01],
          ...,
          [ 9.3750e-02, -7.1289e-02, -2.2168e-01,  ..., -1.1641e+00,
           -1.1797e+00, -1.2422e+00],
          [-4.9219e-01, -4.9219e-01, -5.0781e-01,  ..., -1.5000e+00,
           -1.4688e+00, -1.4219e+00],
          [-7.0312e-01, -7.0312e-01, -2.2168e-01,  ..., -1.6328e+00,
           -1.5859e+00, -1.5703e+00]],
         [[ 1.5234e+00,  1.6328e+00,  1.8203e+00,  ...,  1.3203e+00,
            1.3672e+00,  1.3672e+00],
          [ 1.6797e+00,  1.6172e+00,  1.3672e+00,  ...,  1.1953e+00,
            1.2109e+00,  1.1953e+00],
          [ 1.2891e+00,  1.6641e+00,  1.7188e+00,  ...,  1.2500e+00,
            1.2812e+00,  1.2656e+00],
          ...,
          [-2.9785e-02, -1.8652e-01, -4.2773e-01,  ..., -1.1094e+00,
           -1.1562e+00, -1.1953e+00],
          [-6.4062e-01, -6.6797e-01, -8.1250e-01,  ..., -1.3672e+00,
           -1.3516e+00, -1.2969e+00],
          [-9.1016e-01, -9.2578e-01, -6.2891e-01,  ..., -1.4531e+00,
           -1.4219e+00, -1.3984e+00]]]], device='cuda:0', dtype=torch.bfloat16)
torch.Size([16, 3, 224, 224])
torch.bfloat16
torch.bfloat16
*** AttributeError: 'VisionTransformer' object has no attribute 'dtype'
torch.bfloat16
torch.bfloat16
*** RuntimeError: expected scalar type Float but found BFloat16
torch.bfloat16
VisionTransformer(
  (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
  (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (transformer): Transformer(
    (resblocks): Sequential(
      (0): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (12): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (13): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (14): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (15): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (16): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (17): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (18): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (19): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (20): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (21): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (22): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (23): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
*** AttributeError: 'LayerNorm' object has no attribute 'dtype'
VisionTransformer(
  (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
  (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (transformer): Transformer(
    (resblocks): Sequential(
      (0): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (12): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (13): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (14): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (15): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (16): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (17): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (18): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (19): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (20): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (21): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (22): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (23): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Parameter containing:
tensor([[[[ 2.5269e-02,  1.0620e-02,  7.1716e-03,  ...,  2.3438e-02,
            2.1729e-02,  4.8523e-03],
          [ 1.3733e-02, -6.2256e-03, -4.8218e-03,  ...,  1.6357e-02,
            7.0190e-03, -1.3184e-02],
          [ 1.0071e-02,  2.1362e-03,  2.2888e-03,  ...,  5.5542e-03,
            5.0354e-03, -1.2573e-02],
          ...,
          [-1.0132e-02, -2.3842e-04, -5.4626e-03,  ..., -1.9287e-02,
           -2.4048e-02, -2.4780e-02],
          [-3.4790e-03, -1.0986e-02, -1.3611e-02,  ..., -7.6294e-03,
            1.5564e-03, -4.4250e-03],
          [-2.1484e-02, -4.3213e-02, -3.0151e-02,  ..., -5.2185e-03,
            3.7384e-03, -6.8054e-03]],
         [[ 1.5869e-02, -4.9133e-03, -1.2268e-02,  ...,  2.5879e-02,
            2.4048e-02,  5.6458e-03],
          [ 2.1057e-03, -2.4170e-02, -2.6367e-02,  ...,  1.5320e-02,
            5.2490e-03, -1.5137e-02],
          [ 5.1880e-03, -7.2632e-03, -9.4604e-03,  ...,  7.9346e-03,
            5.4626e-03, -1.2390e-02],
          ...,
          [-4.2114e-03,  5.9509e-03, -6.2561e-04,  ...,  3.8452e-03,
            4.2915e-04, -5.4321e-03],
          [-2.8534e-03, -1.1475e-02, -1.3672e-02,  ...,  1.5137e-02,
            2.3315e-02,  1.2878e-02],
          [-2.7710e-02, -4.9561e-02, -3.1128e-02,  ...,  1.2817e-02,
            1.7944e-02,  6.4468e-04]],
         [[ 1.6357e-02, -2.0142e-03, -4.8828e-03,  ...,  1.6113e-02,
            1.1353e-02,  5.2795e-03],
          [ 1.8539e-03, -2.1606e-02, -2.4780e-02,  ...,  6.0120e-03,
           -4.9438e-03, -8.3618e-03],
          [ 4.8523e-03, -1.0803e-02, -1.5015e-02,  ...,  6.0272e-04,
           -2.3651e-04, -7.5989e-03],
          ...,
          [ 2.4109e-03,  2.6703e-03, -8.1787e-03,  ..., -1.0254e-02,
           -1.0254e-02, -7.4158e-03],
          [ 8.2397e-03, -3.1586e-03, -9.0332e-03,  ..., -3.5248e-03,
            6.7444e-03,  5.1880e-03],
          [-1.0498e-02, -2.6978e-02, -1.5625e-02,  ...,  1.6479e-03,
            6.4697e-03,  2.7084e-04]]],
        [[[ 1.5381e-02,  2.6123e-02,  5.8594e-03,  ...,  8.4229e-03,
           -9.0332e-03,  2.0264e-02],
          [-1.8799e-02, -7.4463e-03, -1.6846e-02,  ...,  2.9602e-03,
           -1.9193e-05,  1.5625e-02],
          [-2.4780e-02, -1.2207e-02,  1.7395e-03,  ..., -2.6245e-02,
           -2.8687e-02, -3.6926e-03],
          ...,
          [ 7.4463e-03, -6.8054e-03,  1.4954e-02,  ...,  1.4832e-02,
           -5.6763e-03, -6.2561e-03],
          [ 6.2256e-03, -8.1787e-03,  4.1260e-02,  ..., -5.0664e-06,
           -2.8320e-02, -1.9409e-02],
          [ 1.7090e-02, -1.1108e-02,  6.1279e-02,  ..., -4.5395e-04,
            7.2937e-03,  4.6875e-02]],
         [[ 2.1118e-02,  3.3447e-02,  1.0498e-02,  ...,  6.6223e-03,
           -1.1841e-02,  1.7822e-02],
          [-1.5015e-02, -8.8882e-04, -9.4604e-03,  ...,  4.7607e-03,
            3.3569e-04,  1.4709e-02],
          [-2.3071e-02, -6.3477e-03,  1.1230e-02,  ..., -2.9297e-02,
           -3.2715e-02, -7.0496e-03],
          ...,
          [ 5.5237e-03, -1.1841e-02,  7.7209e-03,  ...,  1.1230e-02,
           -1.1230e-02, -9.8877e-03],
          [ 2.2125e-03, -1.5564e-02,  3.5889e-02,  ..., -4.5776e-03,
           -3.7598e-02, -2.6733e-02],
          [ 1.0376e-02, -2.4048e-02,  5.2979e-02,  ..., -1.1353e-02,
           -3.9978e-03,  4.0283e-02]],
         [[ 1.0498e-02,  2.2339e-02,  8.9264e-04,  ...,  5.2795e-03,
           -1.2939e-02,  1.7456e-02],
          [-2.5024e-02, -1.1597e-02, -1.9043e-02,  ...,  4.4556e-03,
            4.0436e-04,  1.5625e-02],
          [-3.3447e-02, -1.8066e-02, -1.3199e-03,  ..., -2.8564e-02,
           -3.0151e-02, -6.7444e-03],
          ...,
          [ 4.9744e-03, -1.0437e-02,  1.0681e-02,  ...,  1.3428e-02,
           -8.7280e-03, -8.7891e-03],
          [ 2.5635e-03, -1.3245e-02,  4.1260e-02,  ...,  9.7275e-04,
           -3.0762e-02, -2.5391e-02],
          [ 8.9722e-03, -2.1484e-02,  5.3467e-02,  ..., -6.4087e-03,
           -4.1771e-04,  3.9307e-02]]],
        [[[ 7.2327e-03,  3.7231e-03, -2.4719e-03,  ...,  1.0376e-02,
            1.4282e-02,  1.9409e-02],
          [-3.0212e-03, -3.2501e-03, -1.2878e-02,  ...,  9.7656e-04,
            5.2185e-03,  6.8665e-03],
          [-4.3335e-03,  5.6763e-03, -3.1891e-03,  ...,  8.3923e-04,
            2.2278e-03,  7.1411e-03],
          ...,
          [ 4.4556e-03,  3.8757e-03, -2.0027e-04,  ...,  1.5991e-02,
            1.4221e-02,  1.2756e-02],
          [ 2.5146e-02,  1.4771e-02,  5.0964e-03,  ...,  2.2827e-02,
            2.2217e-02,  2.3071e-02],
          [ 3.0762e-02,  2.6001e-02,  1.4282e-02,  ...,  2.5146e-02,
            2.1729e-02,  2.4902e-02]],
         [[ 6.9580e-03,  5.0964e-03, -2.8992e-03,  ...,  1.7822e-02,
            1.9653e-02,  2.9053e-02],
          [-2.8992e-03, -1.2054e-03, -1.1108e-02,  ...,  5.5542e-03,
            5.9509e-03,  1.3428e-02],
          [-9.8267e-03,  4.3640e-03,  2.3842e-04,  ...,  1.1292e-03,
           -8.8120e-04,  9.4604e-03],
          ...,
          [ 6.1340e-03,  5.1880e-03,  2.1057e-03,  ...,  2.5024e-02,
            2.5146e-02,  2.7710e-02],
          [ 3.1128e-02,  2.0142e-02,  8.1177e-03,  ...,  3.3691e-02,
            3.3203e-02,  4.0527e-02],
          [ 3.7598e-02,  3.2227e-02,  1.8311e-02,  ...,  4.2480e-02,
            3.9551e-02,  4.6387e-02]],
         [[ 1.6968e-02,  1.5381e-02,  1.1292e-02,  ...,  2.1362e-02,
            2.1362e-02,  3.0029e-02],
          [ 7.4158e-03,  7.8125e-03,  1.1063e-03,  ...,  7.4768e-03,
            7.7209e-03,  1.2939e-02],
          [-5.4550e-04,  1.1230e-02,  6.4697e-03,  ...,  4.1199e-03,
            3.2501e-03,  9.7046e-03],
          ...,
          [ 3.3569e-03,  2.0313e-04,  1.3733e-03,  ...,  7.8125e-03,
            7.9346e-03,  1.4832e-02],
          [ 1.6602e-02,  2.9144e-03,  4.2152e-04,  ...,  1.1780e-02,
            1.1353e-02,  1.8799e-02],
          [ 1.9775e-02,  1.0681e-02,  2.9297e-03,  ...,  2.0996e-02,
            1.5442e-02,  2.2217e-02]]],
        ...,
        [[[-3.1662e-04, -6.4850e-04, -6.1035e-04,  ...,  6.5327e-05,
            2.8038e-04, -5.1880e-04],
          [-5.2452e-06, -9.8419e-04, -9.5367e-04,  ...,  1.9908e-05,
           -1.0681e-04, -8.2970e-05],
          [-9.5749e-04, -6.4850e-04, -1.2054e-03,  ..., -6.1035e-04,
           -4.2343e-04,  6.3324e-04],
          ...,
          [-7.1716e-04, -6.2180e-04,  1.0252e-04,  ..., -3.2425e-05,
           -7.7820e-04, -7.9346e-04],
          [-9.9182e-04,  6.4373e-05, -1.2589e-03,  ..., -8.0490e-04,
           -1.2970e-03, -1.2054e-03],
          [-2.8038e-04, -5.7983e-04, -2.4223e-04,  ..., -5.1498e-05,
            4.4250e-04,  1.4400e-04]],
         [[ 5.6839e-04,  1.9550e-05,  2.8229e-04,  ...,  1.6975e-04,
            9.6130e-04, -5.6458e-04],
          [ 9.7656e-04,  2.3961e-05,  4.3869e-04,  ...,  3.5667e-04,
            7.8583e-04, -6.2561e-04],
          [-2.5558e-04,  3.8528e-04,  1.7090e-03,  ...,  6.6757e-04,
            6.1798e-04,  7.3624e-04],
          ...,
          [ 5.9509e-04,  4.7684e-04,  4.6921e-04,  ...,  1.0605e-03,
            6.5994e-04,  3.1090e-04],
          [ 8.3542e-04,  4.7684e-04, -1.0071e-03,  ...,  6.4850e-04,
           -2.4128e-04,  3.6430e-04],
          [ 4.7874e-04, -3.4714e-04,  6.3324e-04,  ...,  2.3842e-04,
            1.9455e-04,  4.0627e-04]],
         [[ 2.0599e-04, -7.5817e-05, -6.9046e-04,  ..., -2.5558e-04,
           -5.5313e-04, -5.7220e-04],
          [-9.0122e-05,  3.5286e-04,  2.0027e-04,  ..., -2.6512e-04,
            1.1635e-04,  5.8365e-04],
          [-9.5367e-04, -3.9673e-04, -3.9482e-04,  ...,  2.1648e-04,
            6.0797e-05,  1.7738e-04],
          ...,
          [ 4.8828e-04, -1.6594e-04,  5.3024e-04,  ...,  3.1281e-04,
            9.8705e-05,  5.7602e-04],
          [ 7.5340e-05,  4.1008e-04, -6.9046e-04,  ...,  1.8597e-04,
            1.9646e-04, -3.3760e-04],
          [-1.4305e-04, -2.5558e-04,  2.9564e-04,  ...,  2.1362e-04,
           -2.9755e-04,  4.0245e-04]]],
        [[[ 1.2329e-02,  1.8921e-02,  5.3101e-03,  ...,  1.1597e-02,
            6.5918e-03,  2.7100e-02],
          [ 1.1230e-02,  2.9663e-02,  1.3672e-02,  ..., -8.9722e-03,
           -1.7578e-02, -3.2501e-03],
          [ 2.1118e-02,  3.1250e-02,  1.7700e-02,  ..., -1.1169e-02,
           -2.7466e-02, -5.5237e-03],
          ...,
          [ 3.7354e-02,  2.5757e-02,  6.7749e-03,  ...,  1.8677e-02,
            2.8809e-02,  3.6621e-02],
          [ 1.9531e-02, -5.3711e-03, -2.1240e-02,  ..., -1.6968e-02,
           -2.3682e-02,  4.6387e-03],
          [ 1.3977e-02, -2.7100e-02, -5.1025e-02,  ..., -1.2512e-02,
           -4.0039e-02,  1.3611e-02]],
         [[ 1.7090e-03,  9.3994e-03, -2.4109e-03,  ...,  8.3618e-03,
            3.3112e-03,  2.3071e-02],
          [ 1.1673e-03,  2.3560e-02,  1.1536e-02,  ..., -8.2397e-03,
           -1.8555e-02, -6.4392e-03],
          [ 1.0742e-02,  2.6733e-02,  1.9043e-02,  ..., -1.1780e-02,
           -3.0396e-02, -9.2773e-03],
          ...,
          [ 3.4912e-02,  2.1484e-02,  8.1253e-04,  ...,  1.6235e-02,
            2.6367e-02,  3.4180e-02],
          [ 1.4648e-02, -1.1108e-02, -2.7344e-02,  ..., -2.0630e-02,
           -3.1250e-02, -1.2970e-03],
          [ 5.7983e-03, -3.9062e-02, -6.3477e-02,  ..., -2.1851e-02,
           -5.2246e-02,  4.1809e-03]],
         [[-1.0681e-02, -1.8539e-03, -8.5449e-03,  ...,  4.6997e-03,
           -1.9379e-03,  1.7456e-02],
          [-1.3000e-02,  1.1108e-02,  5.3101e-03,  ..., -8.9722e-03,
           -2.1362e-02, -9.2773e-03],
          [-3.2959e-03,  1.4587e-02,  1.3855e-02,  ..., -1.2329e-02,
           -2.9785e-02, -1.3000e-02],
          ...,
          [ 3.2715e-02,  2.2461e-02,  6.9885e-03,  ...,  1.0681e-02,
            1.9531e-02,  2.4536e-02],
          [ 1.3794e-02, -6.0425e-03, -1.4587e-02,  ..., -2.2461e-02,
           -3.1982e-02, -9.7046e-03],
          [ 5.8594e-03, -3.2227e-02, -5.2002e-02,  ..., -2.4414e-02,
           -5.2979e-02, -3.0975e-03]]],
        [[[ 2.2583e-02, -7.3547e-03, -2.9053e-02,  ..., -2.2827e-02,
            8.4839e-03, -4.8828e-02],
          [ 1.7456e-02, -3.1494e-02, -4.2725e-02,  ..., -6.2561e-03,
            9.4604e-03, -3.8818e-02],
          [ 2.2095e-02, -1.5869e-02, -4.1260e-02,  ...,  4.5898e-02,
            2.4048e-02, -1.1353e-02],
          ...,
          [ 2.8320e-02,  3.8086e-02,  4.1748e-02,  ...,  2.4780e-02,
           -2.1973e-03, -2.0020e-02],
          [-1.5625e-02, -2.0996e-02, -2.9144e-03,  ...,  2.0874e-02,
            7.9956e-03,  1.4160e-02],
          [-3.1586e-03, -9.1553e-03,  7.2937e-03,  ...,  1.6602e-02,
            1.3580e-03,  1.6602e-02]],
         [[ 2.2705e-02, -8.0566e-03, -3.0762e-02,  ..., -2.0020e-02,
            1.6235e-02, -4.3457e-02],
          [ 1.6724e-02, -3.2471e-02, -4.2725e-02,  ..., -6.4468e-04,
            1.8555e-02, -3.2227e-02],
          [ 2.0752e-02, -1.6113e-02, -4.0039e-02,  ...,  5.0537e-02,
            3.0029e-02, -5.6152e-03],
          ...,
          [ 4.2236e-02,  5.0537e-02,  5.0049e-02,  ...,  2.2095e-02,
           -1.8692e-03, -1.5198e-02],
          [-4.8828e-03, -1.0925e-02,  6.5918e-03,  ...,  1.9043e-02,
            7.9346e-03,  2.0020e-02],
          [ 5.0049e-03, -1.7853e-03,  1.5076e-02,  ...,  1.0437e-02,
           -2.7924e-03,  1.9165e-02]],
         [[ 7.0190e-03, -1.8311e-02, -3.5889e-02,  ..., -1.5259e-02,
            2.1729e-02, -3.0396e-02],
          [-1.7357e-04, -4.1504e-02, -4.5654e-02,  ...,  4.7913e-03,
            2.4780e-02, -1.7944e-02],
          [ 2.1057e-03, -2.8687e-02, -4.5654e-02,  ...,  5.0293e-02,
            3.4424e-02,  4.2114e-03],
          ...,
          [ 3.7354e-02,  4.6143e-02,  4.9561e-02,  ...,  2.0142e-02,
            4.0894e-03, -6.3782e-03],
          [-5.8594e-03, -9.4604e-03,  1.2390e-02,  ...,  2.1362e-02,
            1.5747e-02,  2.5879e-02],
          [ 2.3193e-03, -7.0953e-04,  1.7944e-02,  ...,  1.1047e-02,
            2.1515e-03,  2.2217e-02]]]], device='cuda:0', dtype=torch.bfloat16)
torch.bfloat16
*** AttributeError: 'Parameter' object has no attribute 'weight'
Parameter containing:
tensor([[ 0.0019,  0.0479, -0.0150,  ...,  0.0005, -0.0559, -0.0461],
        [ 0.0114, -0.0413,  0.0356,  ...,  0.0271, -0.0312, -0.0383],
        [-0.0026, -0.0339, -0.0006,  ...,  0.0216, -0.0294, -0.0422],
        ...,
        [-0.0038, -0.0349, -0.0048,  ..., -0.0228, -0.0327, -0.0413],
        [-0.0046, -0.0359, -0.0026,  ..., -0.0349, -0.0356, -0.0354],
        [-0.0073, -0.0287, -0.0144,  ..., -0.0203, -0.0272, -0.0359]],
       device='cuda:0', dtype=torch.bfloat16)
Parameter containing:
tensor([[ 0.0225, -0.0139, -0.0072,  ..., -0.0058, -0.0078,  0.0139],
        [ 0.0187,  0.0084,  0.0400,  ..., -0.0150, -0.0240, -0.0003],
        [ 0.0075, -0.0007,  0.0194,  ..., -0.0062, -0.0082,  0.0156],
        ...,
        [ 0.0121, -0.0166, -0.0144,  ..., -0.0067,  0.0088,  0.0027],
        [-0.0165, -0.0099, -0.0053,  ..., -0.0005, -0.0001, -0.0074],
        [ 0.0092,  0.0048,  0.0069,  ...,  0.0054, -0.0162,  0.0262]],
       device='cuda:0', dtype=torch.bfloat16)
LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 913, in <module>
[rank0]:     main(sys.argv[1:])
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 568, in main
[rank0]:     train_iter = train(
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 673, in train
[rank0]:     output_dict = model(**input_dict)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1735, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/peft/peft_model.py", line 922, in forward
[rank0]:     return self.base_model(
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py", line 532, in forward
[rank0]:     return self.model_forward(**kwargs)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py", line 596, in model_forward
[rank0]:     outputs = super().forward(
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/llava/model/language_model/llava_llama.py", line 88, in forward
[rank0]:     ) = self.prepare_inputs_labels_for_multimodal(
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/llava/model/llava_arch.py", line 123, in prepare_inputs_labels_for_multimodal
[rank0]:     image_features = self.encode_images(images)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/llava/model/llava_arch.py", line 94, in encode_images
[rank0]:     image_features = self.get_model().get_vision_tower()(images)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/llava/model/multimodal_encoder/NACLIP/clip/model.py", line 325, in forward
[rank0]:     image_features = self.encode_image(image)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/llava/model/multimodal_encoder/NACLIP/clip/model.py", line 325, in forward
[rank0]:     image_features = self.encode_image(image)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/bdb.py", line 90, in trace_dispatch
[rank0]:     return self.dispatch_line(frame)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/bdb.py", line 115, in dispatch_line
[rank0]:     if self.quitting: raise BdbQuit
[rank0]: bdb.BdbQuit