
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565



Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:16<00:00,  5.58s/it]
trainable params: 6,553,600 || all params: 14,025,701,683 || trainable%: 0.0467256480147682
n:  base_model.model.model.embed_tokens.weight p.shape:  torch.Size([32002, 5120])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_token.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.mask_decoder.mask_tokens.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.weight p.shape:  torch.Size([256, 64, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.weight p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.weight p.shape:  torch.Size([64, 32, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.bias p.shape:  torch.Size([4])
n:  base_model.model.model.text_hidden_fcs.0.0.weight p.shape:  torch.Size([5120, 5120])
n:  base_model.model.model.text_hidden_fcs.0.0.bias p.shape:  torch.Size([5120])
n:  base_model.model.model.text_hidden_fcs.0.2.weight p.shape:  torch.Size([256, 5120])
n:  base_model.model.model.text_hidden_fcs.0.2.bias p.shape:  torch.Size([256])
n:  base_model.model.lm_head.weight p.shape:  torch.Size([32002, 5120])
ade20k:  20210
cocostuff:  118287
loading annotations into memory...
Done (t=0.54s)
creating index...
index created!
pascal_part:  4366
loading annotations into memory...
Done (t=7.65s)
creating index...
index created!
paco_lvis:  45790
mapillary:  18000
loading dataset refclef into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refclef/refs(unc).p
creating index...
index created.
DONE (t=2.68s)
dataset refclef (refs unc) (train split) has 17978 images and 99523 annotations.
loading dataset refcoco into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco/refs(unc).p
creating index...
index created.
DONE (t=4.96s)
dataset refcoco (refs unc) (train split) has 16994 images and 196771 annotations.
loading dataset refcoco+ into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco+/refs(unc).p
creating index...
index created.
DONE (t=5.78s)
dataset refcoco+ (refs unc) (train split) has 16992 images and 196737 annotations.
loading dataset refcocog into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcocog/refs(umd).p
creating index...
index created.
DONE (t=3.33s)
dataset refcocog (refs umd) (train split) has 21899 images and 208960 annotations.
vqa_data:  157712
number of reason_seg samples:  239
len(self.img_to_explanation):  239
Training with 5000 examples and validating with 200 examples.
[2025-03-01 22:23:27,407] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5, git-hash=unknown, git-branch=unknown
[2025-03-01 22:23:27,407] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2025-03-01 22:23:27,407] [INFO] [comm.py:594:init_distributed] cdb=None
[2025-03-01 22:23:27,407] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Using /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[2025-03-01 22:23:40,959] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Time to load fused_adam op: 0.9502115249633789 seconds
[2025-03-01 22:23:42,095] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-03-01 22:23:42,281] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-03-01 22:23:42,282] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-03-01 22:23:42,282] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-03-01 22:23:42,282] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500000000
[2025-03-01 22:23:42,282] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500000000
[2025-03-01 22:23:42,282] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2025-03-01 22:23:42,282] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Loading extension module fused_adam...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/tensor/python_tensor.cpp:78.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
Rank: 0 partition count [1] and sizes[(365842916, False)]
[2025-03-01 22:23:46,576] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2025-03-01 22:23:46,577] [INFO] [utils.py:786:see_memory_usage] MA 27.69 GB         Max_MA 28.37 GB         CA 28.51 GB         Max_CA 29 GB
[2025-03-01 22:23:46,577] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 49.29 GB, percent = 4.9%
[2025-03-01 22:23:49,534] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2025-03-01 22:23:49,535] [INFO] [utils.py:786:see_memory_usage] MA 30.41 GB         Max_MA 31.78 GB         CA 32.6 GB         Max_CA 33 GB
[2025-03-01 22:23:49,535] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 49.3 GB, percent = 4.9%
[2025-03-01 22:23:49,535] [INFO] [stage_1_and_2.py:488:__init__] optimizer state initialized
[2025-03-01 22:23:52,489] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2025-03-01 22:23:52,490] [INFO] [utils.py:786:see_memory_usage] MA 30.41 GB         Max_MA 30.41 GB         CA 32.6 GB         Max_CA 33 GB
[2025-03-01 22:23:52,490] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 49.29 GB, percent = 4.9%
[2025-03-01 22:23:52,495] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-03-01 22:23:52,495] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2025-03-01 22:23:52,495] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7fcaff158310>
[2025-03-01 22:23:52,496] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
[2025-03-01 22:23:52,498] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2025-03-01 22:23:52,499] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-03-01 22:23:52,499] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-03-01 22:23:52,499] [INFO] [config.py:964:print]   amp_enabled .................. False
[2025-03-01 22:23:52,499] [INFO] [config.py:964:print]   amp_params ................... False
[2025-03-01 22:23:52,499] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-01 22:23:52,499] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2025-03-01 22:23:52,499] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2025-03-01 22:23:52,499] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2025-03-01 22:23:52,499] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2025-03-01 22:23:52,499] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fcaff1580d0>
[2025-03-01 22:23:52,500] [INFO] [config.py:964:print]   communication_data_type ...... None
[2025-03-01 22:23:52,500] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-01 22:23:52,500] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2025-03-01 22:23:52,500] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2025-03-01 22:23:52,500] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-01 22:23:52,500] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2025-03-01 22:23:52,500] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2025-03-01 22:23:52,500] [INFO] [config.py:964:print]   disable_allgather ............ False
[2025-03-01 22:23:52,500] [INFO] [config.py:964:print]   dump_state ................... False
[2025-03-01 22:23:52,500] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2025-03-01 22:23:52,500] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2025-03-01 22:23:52,500] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-01 22:23:52,500] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-01 22:23:52,500] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2025-03-01 22:23:52,500] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2025-03-01 22:23:52,500] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2025-03-01 22:23:52,500] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2025-03-01 22:23:52,500] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2025-03-01 22:23:52,501] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2025-03-01 22:23:52,501] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-03-01 22:23:52,501] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2025-03-01 22:23:52,501] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2025-03-01 22:23:52,501] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2025-03-01 22:23:52,501] [INFO] [config.py:964:print]   global_rank .................. 0
[2025-03-01 22:23:52,501] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2025-03-01 22:23:52,501] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 10
[2025-03-01 22:23:52,501] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0
[2025-03-01 22:23:52,501] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2025-03-01 22:23:52,501] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-01 22:23:52,501] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2025-03-01 22:23:52,501] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2025-03-01 22:23:52,501] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2025-03-01 22:23:52,501] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2025-03-01 22:23:52,501] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2025-03-01 22:23:52,501] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2025-03-01 22:23:52,502] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-03-01 22:23:52,502] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-03-01 22:23:52,502] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2025-03-01 22:23:52,502] [INFO] [config.py:964:print]   optimizer_name ............... adamw
[2025-03-01 22:23:52,502] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.0003, 'weight_decay': 0.0, 'betas': (0.9, 0.95)}
[2025-03-01 22:23:52,502] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2025-03-01 22:23:52,502] [INFO] [config.py:964:print]   pld_enabled .................. False
[2025-03-01 22:23:52,502] [INFO] [config.py:964:print]   pld_params ................... False
[2025-03-01 22:23:52,502] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2025-03-01 22:23:52,502] [INFO] [config.py:964:print]   scheduler_name ............... WarmupDecayLR
[2025-03-01 22:23:52,502] [INFO] [config.py:964:print]   scheduler_params ............. {'total_num_steps': 5000, 'warmup_min_lr': 0, 'warmup_max_lr': 0.0003, 'warmup_num_steps': 100, 'warmup_type': 'linear'}
[2025-03-01 22:23:52,502] [INFO] [config.py:964:print]   sparse_attention ............. None
[2025-03-01 22:23:52,502] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2025-03-01 22:23:52,502] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2025-03-01 22:23:52,502] [INFO] [config.py:964:print]   train_batch_size ............. 10
[2025-03-01 22:23:52,502] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2025-03-01 22:23:52,503] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2025-03-01 22:23:52,503] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2025-03-01 22:23:52,503] [INFO] [config.py:964:print]   world_size ................... 1
[2025-03-01 22:23:52,503] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2025-03-01 22:23:52,503] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2025-03-01 22:23:52,503] [INFO] [config.py:964:print]   zero_enabled ................. True
[2025-03-01 22:23:52,503] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-01 22:23:52,503] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2025-03-01 22:23:52,503] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1,
    "gradient_accumulation_steps": 10,
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 0.0003,
            "weight_decay": 0.0,
            "betas": [0.9, 0.95]
        }
    },
    "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "total_num_steps": 5.000000e+03,
            "warmup_min_lr": 0,
            "warmup_max_lr": 0.0003,
            "warmup_num_steps": 100,
            "warmup_type": "linear"
        }
    },
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "gradient_clipping": 1.0,
    "zero_optimization": {
        "stage": 2,
        "contiguous_gradients": true,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 5.000000e+08,
        "allgather_bucket_size": 5.000000e+08
    }
}
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)
  return F.conv2d(input, weight, bias, self.stride,
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:70: UserWarning: Using a target size (torch.Size([450, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch: [0][  1/500]	Time 10.569 (10.569)	Loss 1.5000 (8.2029)	CeLoss 1.5000 (2.4961)	SegCLSLoss 0.0000 (0.7391)	KLLoss 0.0000 (0.3383)	MaskLoss 0.0000 (3.0406)	MaskBCELoss 0.0000 (2.5441)	MaskDICELoss 0.0000 (0.4965)
Epoch: [0][  2/500]	Time  9.685 ( 9.685)	Loss 4.4538 (4.4194)	CeLoss 3.1406 (2.4102)	SegCLSLoss 1.2578 (1.1164)	KLLoss 0.5156 (0.4383)	MaskLoss 1.0929 (1.3983)	MaskBCELoss 0.0945 (0.5015)	MaskDICELoss 0.9983 (0.8968)
Epoch: [0][  3/500]	Time  8.428 ( 8.428)	Loss 4.7908 (3.8373)	CeLoss 2.6094 (2.1695)	SegCLSLoss 1.1875 (0.7273)	KLLoss 0.5586 (0.3352)	MaskLoss 1.5431 (1.1010)	MaskBCELoss 0.5441 (0.5022)	MaskDICELoss 0.9990 (0.5988)
Epoch: [0][  4/500]	Time  8.989 ( 8.989)	Loss 3.6072 (4.1932)	CeLoss 2.4219 (2.4734)	SegCLSLoss 1.2656 (0.8625)	KLLoss 0.4805 (0.3785)	MaskLoss 1.0215 (1.1678)	MaskBCELoss 0.0226 (0.4697)	MaskDICELoss 0.9989 (0.6980)
Epoch: [0][  5/500]	Time  9.999 ( 9.999)	Loss 3.4404 (4.8145)	CeLoss 2.3125 (2.6516)	SegCLSLoss 1.1953 (1.2266)	KLLoss 0.4219 (0.5217)	MaskLoss 1.0163 (1.5233)	MaskBCELoss 0.0173 (0.5269)	MaskDICELoss 0.9990 (0.9964)
Epoch: [0][  6/500]	Time  8.850 ( 8.850)	Loss 1.0625 (3.3264)	CeLoss 1.0625 (2.1414)	SegCLSLoss 0.0000 (0.7430)	KLLoss 0.0000 (0.2855)	MaskLoss 0.0000 (0.8545)	MaskBCELoss 0.0000 (0.2572)	MaskDICELoss 0.0000 (0.5973)
Epoch: [0][  7/500]	Time  8.704 ( 8.704)	Loss 3.1860 (3.6692)	CeLoss 1.5859 (2.4023)	SegCLSLoss 1.2422 (0.9969)	KLLoss 0.4102 (0.3893)	MaskLoss 1.2288 (0.9768)	MaskBCELoss 0.2456 (0.1862)	MaskDICELoss 0.9832 (0.7907)
Epoch: [0][  8/500]	Time  8.706 ( 8.706)	Loss 5.7701 (4.1016)	CeLoss 3.0469 (2.3914)	SegCLSLoss 1.2109 (0.9789)	KLLoss 0.5234 (0.4309)	MaskLoss 1.8004 (1.2045)	MaskBCELoss 0.8091 (0.4126)	MaskDICELoss 0.9913 (0.7919)
Epoch: [0][  9/500]	Time  8.539 ( 8.539)	Loss 4.0758 (4.4130)	CeLoss 2.2188 (2.6172)	SegCLSLoss 1.2734 (0.9883)	KLLoss 0.4316 (0.4639)	MaskLoss 1.3402 (1.2374)	MaskBCELoss 0.3642 (0.4556)	MaskDICELoss 0.9760 (0.7819)
[2025-03-01 22:25:23,354] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[2.6999999999999996e-05], mom=[(0.9, 0.95)]
[2025-03-01 22:25:23,355] [INFO] [timer.py:215:stop] epoch=0/micro_step=100/global_step=10, RunningAvgSamplesPerSec=1.135933845830358, CurrSamplesPerSec=1.2164147477597071, MemAllocated=30.86GB, MaxMemAllocated=35.8GB
Epoch: [0][ 10/500]	Time  8.223 ( 8.223)	Loss 0.8555 (3.0867)	CeLoss 0.8555 (2.1777)	SegCLSLoss 0.0000 (0.7344)	KLLoss 0.0000 (0.3141)	MaskLoss 0.0000 (0.7127)	MaskBCELoss 0.0000 (0.1247)	MaskDICELoss 0.0000 (0.5880)
Epoch: [0][ 11/500]	Time  9.310 ( 9.310)	Loss 4.3304 (3.6367)	CeLoss 3.0000 (2.4203)	SegCLSLoss 1.1875 (1.0898)	KLLoss 0.5625 (0.4832)	MaskLoss 1.1013 (0.9987)	MaskBCELoss 0.1240 (0.1139)	MaskDICELoss 0.9773 (0.8848)
Epoch: [0][ 12/500]	Time  8.380 ( 8.380)	Loss 3.5947 (3.3934)	CeLoss 2.2812 (2.1055)	SegCLSLoss 1.2422 (0.9914)	KLLoss 0.4629 (0.4039)	MaskLoss 1.0820 (0.9493)	MaskBCELoss 0.0983 (0.2104)	MaskDICELoss 0.9837 (0.7389)
Epoch: [0][ 13/500]	Time  8.119 ( 8.119)	Loss 3.6052 (2.8493)	CeLoss 1.6016 (1.7848)	SegCLSLoss 1.2422 (0.7344)	KLLoss 0.4062 (0.2916)	MaskLoss 1.2617 (0.7548)	MaskBCELoss 0.5037 (0.2138)	MaskDICELoss 0.7579 (0.5410)
Epoch: [0][ 14/500]	Time  8.111 ( 8.111)	Loss 0.8594 (3.1542)	CeLoss 0.8594 (2.0547)	SegCLSLoss 0.0000 (0.7453)	KLLoss 0.0000 (0.3332)	MaskLoss 0.0000 (0.7843)	MaskBCELoss 0.0000 (0.2226)	MaskDICELoss 0.0000 (0.5616)
Epoch: [0][ 15/500]	Time  9.682 ( 9.682)	Loss 3.5912 (4.0814)	CeLoss 1.4141 (2.1586)	SegCLSLoss 1.2344 (1.2242)	KLLoss 0.3848 (0.5221)	MaskLoss 1.3108 (1.3182)	MaskBCELoss 0.6030 (0.4336)	MaskDICELoss 0.7078 (0.8845)
Epoch: [0][ 16/500]	Time  9.789 ( 9.789)	Loss 3.3209 (3.8980)	CeLoss 1.5469 (2.1734)	SegCLSLoss 1.2344 (1.2266)	KLLoss 0.3691 (0.5221)	MaskLoss 1.2553 (1.2548)	MaskBCELoss 0.3580 (0.3231)	MaskDICELoss 0.8972 (0.9316)
Epoch: [0][ 17/500]	Time  9.279 ( 9.279)	Loss 3.4619 (3.7797)	CeLoss 2.0000 (2.2414)	SegCLSLoss 1.2344 (1.0820)	KLLoss 0.5391 (0.5152)	MaskLoss 1.1512 (1.1117)	MaskBCELoss 0.1742 (0.2951)	MaskDICELoss 0.9770 (0.8166)
Epoch: [0][ 18/500]	Time  7.780 ( 7.780)	Loss 0.9297 (2.6620)	CeLoss 0.9297 (1.7324)	SegCLSLoss 0.0000 (0.6172)	KLLoss 0.0000 (0.2793)	MaskLoss 0.0000 (0.6405)	MaskBCELoss 0.0000 (0.1995)	MaskDICELoss 0.0000 (0.4411)
Epoch: [0][ 19/500]	Time  8.940 ( 8.940)	Loss 2.7954 (2.7283)	CeLoss 1.5000 (1.5316)	SegCLSLoss 1.2422 (0.9766)	KLLoss 0.4219 (0.3885)	MaskLoss 1.0442 (0.9364)	MaskBCELoss 0.0989 (0.1612)	MaskDICELoss 0.9453 (0.7753)
[2025-03-01 22:26:53,055] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[5.6999999999999996e-05], mom=[(0.9, 0.95)]
[2025-03-01 22:26:53,056] [INFO] [timer.py:215:stop] epoch=0/micro_step=200/global_step=20, RunningAvgSamplesPerSec=1.124208224719208, CurrSamplesPerSec=0.9699956126721458, MemAllocated=31.24GB, MaxMemAllocated=35.84GB
Epoch: [0][ 20/500]	Time 10.311 (10.311)	Loss 3.0385 (3.7643)	CeLoss 1.8281 (1.8086)	SegCLSLoss 1.1953 (1.2164)	KLLoss 0.4531 (0.5232)	MaskLoss 1.0580 (1.3837)	MaskBCELoss 0.0584 (0.4379)	MaskDICELoss 0.9996 (0.9459)
Epoch: [0][ 21/500]	Time  9.445 ( 9.445)	Loss 2.8112 (3.2218)	CeLoss 1.3594 (1.6586)	SegCLSLoss 1.2344 (1.0898)	KLLoss 0.4336 (0.4773)	MaskLoss 1.1297 (1.1496)	MaskBCELoss 0.1798 (0.2959)	MaskDICELoss 0.9499 (0.8537)
Epoch: [0][ 22/500]	Time  8.394 ( 8.394)	Loss 3.0078 (2.9251)	CeLoss 1.6172 (1.4414)	SegCLSLoss 1.1953 (0.9844)	KLLoss 0.4062 (0.4271)	MaskLoss 1.1166 (1.0689)	MaskBCELoss 0.1539 (0.3052)	MaskDICELoss 0.9627 (0.7637)
Epoch: [0][ 23/500]	Time  8.154 ( 8.154)	Loss 3.2767 (2.2433)	CeLoss 1.3281 (1.2066)	SegCLSLoss 1.2266 (0.6125)	KLLoss 0.4492 (0.2555)	MaskLoss 1.3647 (0.7149)	MaskBCELoss 0.4379 (0.2486)	MaskDICELoss 0.9268 (0.4663)
Epoch: [0][ 24/500]	Time  7.896 ( 7.896)	Loss 0.8125 (2.7153)	CeLoss 0.8125 (1.3234)	SegCLSLoss 0.0000 (0.8500)	KLLoss 0.0000 (0.3693)	MaskLoss 0.0000 (0.9686)	MaskBCELoss 0.0000 (0.3207)	MaskDICELoss 0.0000 (0.6480)
Epoch: [0][ 25/500]	Time  8.669 ( 8.669)	Loss 2.6963 (2.5992)	CeLoss 1.2422 (1.2518)	SegCLSLoss 1.2578 (0.9727)	KLLoss 0.5391 (0.4197)	MaskLoss 1.1444 (0.9932)	MaskBCELoss 0.1660 (0.2422)	MaskDICELoss 0.9784 (0.7510)
Epoch: [0][ 26/500]	Time  9.221 ( 9.221)	Loss 3.4598 (2.4871)	CeLoss 1.4219 (1.1469)	SegCLSLoss 1.1953 (0.9688)	KLLoss 0.6250 (0.4199)	MaskLoss 1.3828 (0.9878)	MaskBCELoss 0.5018 (0.2413)	MaskDICELoss 0.8809 (0.7465)
Epoch: [0][ 27/500]	Time  9.956 ( 9.956)	Loss 2.8561 (2.8170)	CeLoss 1.0078 (1.2523)	SegCLSLoss 1.1562 (1.0797)	KLLoss 0.6016 (0.4951)	MaskLoss 1.3096 (1.1229)	MaskBCELoss 0.4102 (0.3084)	MaskDICELoss 0.8994 (0.8145)
Epoch: [0][ 28/500]	Time  9.368 ( 9.368)	Loss 2.4848 (2.5127)	CeLoss 1.0703 (1.1402)	SegCLSLoss 1.1875 (1.0711)	KLLoss 0.5586 (0.4654)	MaskLoss 1.1339 (1.0675)	MaskBCELoss 0.1692 (0.2016)	MaskDICELoss 0.9647 (0.8659)
Epoch: [0][ 29/500]	Time  8.281 ( 8.281)	Loss 2.4263 (2.4825)	CeLoss 1.2031 (1.1227)	SegCLSLoss 1.1562 (0.8281)	KLLoss 0.6250 (0.3937)	MaskLoss 1.0680 (0.9513)	MaskBCELoss 0.0740 (0.3124)	MaskDICELoss 0.9940 (0.6389)
[2025-03-01 22:28:19,553] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[8.699999999999999e-05], mom=[(0.9, 0.95)]
[2025-03-01 22:28:19,553] [INFO] [timer.py:215:stop] epoch=0/micro_step=300/global_step=30, RunningAvgSamplesPerSec=1.1354621178360105, CurrSamplesPerSec=1.4062586176970084, MemAllocated=30.8GB, MaxMemAllocated=35.84GB
Epoch: [0][ 30/500]	Time  7.113 ( 7.113)	Loss 0.9648 (1.6866)	CeLoss 0.9648 (0.9281)	SegCLSLoss 0.0000 (0.4727)	KLLoss 0.0000 (0.2096)	MaskLoss 0.0000 (0.5411)	MaskBCELoss 0.0000 (0.1675)	MaskDICELoss 0.0000 (0.3736)
Epoch: [0][ 31/500]	Time  7.790 ( 7.790)	Loss 0.7734 (2.5757)	CeLoss 0.7734 (0.9648)	SegCLSLoss 0.0000 (1.0711)	KLLoss 0.0000 (0.4498)	MaskLoss 0.0000 (1.1819)	MaskBCELoss 0.0000 (0.3232)	MaskDICELoss 0.0000 (0.8587)
Epoch: [0][ 32/500]	Time  9.667 ( 9.667)	Loss 3.1743 (2.7077)	CeLoss 0.9570 (1.0191)	SegCLSLoss 1.2031 (1.1766)	KLLoss 0.6172 (0.5445)	MaskLoss 1.4916 (1.2590)	MaskBCELoss 0.5773 (0.3136)	MaskDICELoss 0.9142 (0.9453)
Epoch: [0][ 33/500]	Time  7.934 ( 7.934)	Loss 2.1708 (1.9989)	CeLoss 0.9297 (1.0344)	SegCLSLoss 1.1484 (0.6961)	KLLoss 0.4961 (0.3160)	MaskLoss 1.0776 (0.7407)	MaskBCELoss 0.0880 (0.1641)	MaskDICELoss 0.9895 (0.5766)
Epoch: [0][ 34/500]	Time  8.708 ( 8.708)	Loss 2.3669 (2.3938)	CeLoss 0.6719 (0.9141)	SegCLSLoss 1.1797 (1.0336)	KLLoss 0.4492 (0.4127)	MaskLoss 1.2509 (1.1187)	MaskBCELoss 0.3172 (0.2682)	MaskDICELoss 0.9336 (0.8505)
Epoch: [0][ 35/500]	Time  8.814 ( 8.814)	Loss 2.2778 (2.1732)	CeLoss 0.9570 (0.8016)	SegCLSLoss 1.1250 (0.9102)	KLLoss 0.5430 (0.4316)	MaskLoss 1.1083 (0.9988)	MaskBCELoss 0.1387 (0.2786)	MaskDICELoss 0.9696 (0.7202)
Epoch: [0][ 36/500]	Time  8.939 ( 8.939)	Loss 2.5161 (2.6589)	CeLoss 0.9180 (0.9102)	SegCLSLoss 1.1250 (1.0102)	KLLoss 0.4688 (0.4666)	MaskLoss 1.2041 (1.2351)	MaskBCELoss 0.2865 (0.4172)	MaskDICELoss 0.9176 (0.8180)
Epoch: [0][ 37/500]	Time  9.566 ( 9.566)	Loss 2.0908 (2.3119)	CeLoss 0.5781 (0.8766)	SegCLSLoss 1.0781 (0.9898)	KLLoss 0.5820 (0.4658)	MaskLoss 1.2332 (1.1088)	MaskBCELoss 0.2380 (0.2576)	MaskDICELoss 0.9951 (0.8512)
Epoch: [0][ 38/500]	Time  8.261 ( 8.261)	Loss 2.4865 (2.0233)	CeLoss 0.8359 (0.8176)	SegCLSLoss 1.1016 (0.8531)	KLLoss 0.5859 (0.4510)	MaskLoss 1.2278 (0.9599)	MaskBCELoss 0.3213 (0.2000)	MaskDICELoss 0.9065 (0.7599)
Epoch: [0][ 39/500]	Time  8.742 ( 8.742)	Loss 2.2354 (2.1799)	CeLoss 0.6562 (0.8402)	SegCLSLoss 1.0781 (0.9586)	KLLoss 0.5977 (0.4652)	MaskLoss 1.2069 (1.0679)	MaskBCELoss 0.2911 (0.2171)	MaskDICELoss 0.9159 (0.8508)
[2025-03-01 22:29:47,070] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[0.000117], mom=[(0.9, 0.95)]
[2025-03-01 22:29:47,071] [INFO] [timer.py:215:stop] epoch=0/micro_step=400/global_step=40, RunningAvgSamplesPerSec=1.1373869487866166, CurrSamplesPerSec=1.0995666149736538, MemAllocated=31.44GB, MaxMemAllocated=36.24GB
Epoch: [0][ 40/500]	Time  9.096 ( 9.096)	Loss 2.6331 (1.7741)	CeLoss 0.7031 (0.7051)	SegCLSLoss 1.0312 (0.7367)	KLLoss 0.5156 (0.3965)	MaskLoss 1.4371 (0.8543)	MaskBCELoss 0.4639 (0.1826)	MaskDICELoss 0.9731 (0.6717)
Epoch: [0][ 41/500]	Time  7.964 ( 7.964)	Loss 1.8179 (1.7953)	CeLoss 0.7070 (0.7738)	SegCLSLoss 1.0625 (0.6250)	KLLoss 0.4336 (0.3051)	MaskLoss 1.0412 (0.7852)	MaskBCELoss 0.0419 (0.2117)	MaskDICELoss 0.9993 (0.5736)
Epoch: [0][ 42/500]	Time  9.421 ( 9.421)	Loss 1.7318 (1.9281)	CeLoss 0.4355 (0.6031)	SegCLSLoss 1.0078 (0.8953)	KLLoss 0.3945 (0.4475)	MaskLoss 1.1163 (1.0829)	MaskBCELoss 0.1574 (0.2244)	MaskDICELoss 0.9588 (0.8585)
Epoch: [0][ 43/500]	Time  7.993 ( 7.993)	Loss 0.9648 (1.6459)	CeLoss 0.9648 (0.7588)	SegCLSLoss 0.0000 (0.5785)	KLLoss 0.0000 (0.3369)	MaskLoss 0.0000 (0.7356)	MaskBCELoss 0.0000 (0.1531)	MaskDICELoss 0.0000 (0.5825)
Epoch: [0][ 44/500]	Time  7.196 ( 7.196)	Loss 3.4246 (1.6256)	CeLoss 0.5625 (0.7014)	SegCLSLoss 0.9453 (0.5590)	KLLoss 0.5977 (0.3002)	MaskLoss 1.8908 (0.7560)	MaskBCELoss 0.9653 (0.1781)	MaskDICELoss 0.9255 (0.5780)
Epoch: [0][ 45/500]	Time  8.316 ( 8.316)	Loss 0.7188 (1.9285)	CeLoss 0.7188 (0.7994)	SegCLSLoss 0.0000 (0.6992)	KLLoss 0.0000 (0.4461)	MaskLoss 0.0000 (0.9648)	MaskBCELoss 0.0000 (0.1976)	MaskDICELoss 0.0000 (0.7672)
Epoch: [0][ 46/500]	Time  8.974 ( 8.974)	Loss 2.1006 (1.6960)	CeLoss 0.8281 (0.6465)	SegCLSLoss 0.8516 (0.6863)	KLLoss 0.5156 (0.4182)	MaskLoss 1.1662 (0.9244)	MaskBCELoss 0.1783 (0.1629)	MaskDICELoss 0.9879 (0.7615)
Epoch: [0][ 47/500]	Time  8.429 ( 8.429)	Loss 2.4010 (1.7855)	CeLoss 0.5234 (0.5756)	SegCLSLoss 0.7812 (0.6547)	KLLoss 0.6133 (0.4260)	MaskLoss 1.4610 (1.0058)	MaskBCELoss 0.5043 (0.2535)	MaskDICELoss 0.9567 (0.7523)
Epoch: [0][ 48/500]	Time  8.496 ( 8.496)	Loss 2.1168 (1.8996)	CeLoss 0.3711 (0.5689)	SegCLSLoss 0.7617 (0.6789)	KLLoss 0.4180 (0.5037)	MaskLoss 1.3000 (1.1308)	MaskBCELoss 0.4779 (0.2840)	MaskDICELoss 0.8221 (0.8468)
Epoch: [0][ 49/500]	Time  9.064 ( 9.064)	Loss 1.8220 (2.0771)	CeLoss 0.4316 (0.6391)	SegCLSLoss 0.7031 (0.6320)	KLLoss 0.5000 (0.4820)	MaskLoss 1.2162 (1.1833)	MaskBCELoss 0.2884 (0.3536)	MaskDICELoss 0.9278 (0.8297)
[2025-03-01 22:31:12,431] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[0.000147], mom=[(0.9, 0.95)]
[2025-03-01 22:31:12,431] [INFO] [timer.py:215:stop] epoch=0/micro_step=500/global_step=50, RunningAvgSamplesPerSec=1.1443650691749359, CurrSamplesPerSec=1.0519432440255447, MemAllocated=30.71GB, MaxMemAllocated=36.26GB
Epoch: [0][ 50/500]	Time  9.508 ( 9.508)	Loss 1.7344 (1.5405)	CeLoss 1.7344 (0.5854)	SegCLSLoss 0.0000 (0.5242)	KLLoss 0.0000 (0.3705)	MaskLoss 0.0000 (0.9227)	MaskBCELoss 0.0000 (0.1546)	MaskDICELoss 0.0000 (0.7682)
Epoch: [0][ 51/500]	Time  7.829 ( 7.829)	Loss 2.1744 (1.6070)	CeLoss 0.5117 (0.5020)	SegCLSLoss 0.6484 (0.4262)	KLLoss 0.4785 (0.3887)	MaskLoss 1.3090 (0.9302)	MaskBCELoss 0.4560 (0.2842)	MaskDICELoss 0.8530 (0.6460)
Epoch: [0][ 52/500]	Time  8.256 ( 8.256)	Loss 1.5289 (1.6919)	CeLoss 0.3594 (0.6590)	SegCLSLoss 0.5117 (0.4297)	KLLoss 0.5938 (0.4279)	MaskLoss 1.1833 (0.9706)	MaskBCELoss 0.2134 (0.2216)	MaskDICELoss 0.9700 (0.7491)
Epoch: [0][ 53/500]	Time  9.036 ( 9.036)	Loss 0.6250 (1.5905)	CeLoss 0.6250 (0.4809)	SegCLSLoss 0.0000 (0.3797)	KLLoss 0.0000 (0.4494)	MaskLoss 0.0000 (1.0178)	MaskBCELoss 0.0000 (0.2737)	MaskDICELoss 0.0000 (0.7441)
Epoch: [0][ 54/500]	Time  8.143 ( 8.143)	Loss 1.2818 (1.2966)	CeLoss 0.3535 (0.6504)	SegCLSLoss 0.4492 (0.2156)	KLLoss 0.5391 (0.2746)	MaskLoss 1.0908 (0.6221)	MaskBCELoss 0.1056 (0.1518)	MaskDICELoss 0.9852 (0.4703)
Epoch: [0][ 55/500]	Time  9.121 ( 9.121)	Loss 1.2243 (1.6329)	CeLoss 0.3945 (0.4414)	SegCLSLoss 0.3574 (0.3348)	KLLoss 0.5586 (0.5250)	MaskLoss 1.0707 (1.1183)	MaskBCELoss 0.0765 (0.3096)	MaskDICELoss 0.9942 (0.8087)
Epoch: [0][ 56/500]	Time  8.689 ( 8.689)	Loss 1.2471 (1.3808)	CeLoss 0.3008 (0.5865)	SegCLSLoss 0.3242 (0.2352)	KLLoss 0.5312 (0.3930)	MaskLoss 1.1234 (0.8398)	MaskBCELoss 0.1484 (0.1712)	MaskDICELoss 0.9750 (0.6686)
Epoch: [0][ 57/500]	Time  8.272 ( 8.272)	Loss 1.1719 (1.6793)	CeLoss 1.1719 (0.5193)	SegCLSLoss 0.0000 (0.2109)	KLLoss 0.0000 (0.4926)	MaskLoss 0.0000 (1.0791)	MaskBCELoss 0.0000 (0.3430)	MaskDICELoss 0.0000 (0.7361)
Epoch: [0][ 58/500]	Time  9.813 ( 9.813)	Loss 1.2363 (1.2841)	CeLoss 0.3242 (0.4627)	SegCLSLoss 0.2393 (0.1996)	KLLoss 0.5703 (0.4781)	MaskLoss 1.1328 (0.9247)	MaskBCELoss 0.1510 (0.1730)	MaskDICELoss 0.9817 (0.7518)
Epoch: [0][ 59/500]	Time  9.080 ( 9.080)	Loss 1.2890 (1.5777)	CeLoss 0.2051 (0.3369)	SegCLSLoss 0.2754 (0.2126)	KLLoss 0.4902 (0.6229)	MaskLoss 1.1733 (1.2756)	MaskBCELoss 0.2391 (0.3310)	MaskDICELoss 0.9342 (0.9446)
[2025-03-01 22:32:39,545] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[0.00017699999999999997], mom=[(0.9, 0.95)]
[2025-03-01 22:32:39,546] [INFO] [timer.py:215:stop] epoch=0/micro_step=600/global_step=60, RunningAvgSamplesPerSec=1.1450054166881947, CurrSamplesPerSec=1.1268183808422736, MemAllocated=31.27GB, MaxMemAllocated=36.65GB
Epoch: [0][ 60/500]	Time  8.876 ( 8.876)	Loss 1.4203 (1.4749)	CeLoss 0.2559 (0.5352)	SegCLSLoss 0.2598 (0.1775)	KLLoss 0.5078 (0.4744)	MaskLoss 1.1954 (0.9925)	MaskBCELoss 0.2906 (0.2364)	MaskDICELoss 0.9048 (0.7562)
Epoch: [0][ 61/500]	Time  8.950 ( 8.950)	Loss 1.3951 (1.4826)	CeLoss 0.2715 (0.4004)	SegCLSLoss 0.1807 (0.1560)	KLLoss 0.6484 (0.5727)	MaskLoss 1.2355 (1.1337)	MaskBCELoss 0.2774 (0.2917)	MaskDICELoss 0.9581 (0.8419)
Epoch: [0][ 62/500]	Time  9.021 ( 9.021)	Loss 1.5478 (1.2328)	CeLoss 0.4199 (0.3730)	SegCLSLoss 0.1455 (0.1588)	KLLoss 0.6953 (0.4787)	MaskLoss 1.2234 (0.9524)	MaskBCELoss 0.2959 (0.2030)	MaskDICELoss 0.9275 (0.7494)
Epoch: [0][ 63/500]	Time  8.437 ( 8.437)	Loss 1.9586 (1.5229)	CeLoss 0.3887 (0.4041)	SegCLSLoss 0.1299 (0.1564)	KLLoss 0.7695 (0.5859)	MaskLoss 1.3760 (1.1379)	MaskBCELoss 0.5450 (0.3144)	MaskDICELoss 0.8310 (0.8235)
Epoch: [0][ 64/500]	Time  7.532 ( 7.532)	Loss 0.8633 (1.4558)	CeLoss 0.8633 (0.5007)	SegCLSLoss 0.0000 (0.1026)	KLLoss 0.0000 (0.3814)	MaskLoss 0.0000 (0.8532)	MaskBCELoss 0.0000 (0.3180)	MaskDICELoss 0.0000 (0.5352)
Epoch: [0][ 65/500]	Time  9.961 ( 9.961)	Loss 1.6236 (1.3719)	CeLoss 0.3711 (0.3937)	SegCLSLoss 0.1157 (0.1342)	KLLoss 0.7500 (0.5078)	MaskLoss 1.2809 (1.0104)	MaskBCELoss 0.3690 (0.2705)	MaskDICELoss 0.9119 (0.7399)
Epoch: [0][ 66/500]	Time  8.534 ( 8.534)	Loss 1.2544 (1.5065)	CeLoss 0.3125 (0.3496)	SegCLSLoss 0.1641 (0.1465)	KLLoss 0.6797 (0.6094)	MaskLoss 1.1669 (1.1390)	MaskBCELoss 0.1843 (0.3428)	MaskDICELoss 0.9826 (0.7962)
Epoch: [0][ 67/500]	Time  8.626 ( 8.626)	Loss 1.1705 (1.2951)	CeLoss 0.3301 (0.4410)	SegCLSLoss 0.1631 (0.1199)	KLLoss 0.6797 (0.5508)	MaskLoss 1.1267 (0.9599)	MaskBCELoss 0.1300 (0.2095)	MaskDICELoss 0.9967 (0.7504)
Epoch: [0][ 68/500]	Time  7.858 ( 7.858)	Loss 1.1986 (1.5612)	CeLoss 0.2891 (0.4648)	SegCLSLoss 0.1914 (0.1165)	KLLoss 0.6016 (0.5262)	MaskLoss 1.1360 (1.0693)	MaskBCELoss 0.1639 (0.3357)	MaskDICELoss 0.9721 (0.7336)
Epoch: [0][ 69/500]	Time  8.620 ( 8.620)	Loss 1.1091 (1.2571)	CeLoss 0.3672 (0.5679)	SegCLSLoss 0.1318 (0.0900)	KLLoss 0.7422 (0.4992)	MaskLoss 1.0821 (0.8286)	MaskBCELoss 0.0897 (0.1531)	MaskDICELoss 0.9925 (0.6755)
[2025-03-01 22:34:06,129] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[0.00020699999999999996], mom=[(0.9, 0.95)]
[2025-03-01 22:34:06,130] [INFO] [timer.py:215:stop] epoch=0/micro_step=700/global_step=70, RunningAvgSamplesPerSec=1.1464854327706042, CurrSamplesPerSec=1.1058004547637272, MemAllocated=31.23GB, MaxMemAllocated=36.65GB
Epoch: [0][ 70/500]	Time  9.045 ( 9.045)	Loss 1.0091 (1.2153)	CeLoss 0.2490 (0.4424)	SegCLSLoss 0.1387 (0.0891)	KLLoss 0.6328 (0.4738)	MaskLoss 1.0767 (0.8614)	MaskBCELoss 0.1016 (0.1983)	MaskDICELoss 0.9751 (0.6630)
Epoch: [0][ 71/500]	Time  9.264 ( 9.264)	Loss 1.6014 (1.2693)	CeLoss 0.2598 (0.3838)	SegCLSLoss 0.1406 (0.0894)	KLLoss 0.6562 (0.5797)	MaskLoss 1.3329 (0.9965)	MaskBCELoss 0.4033 (0.2283)	MaskDICELoss 0.9297 (0.7682)
Epoch: [0][ 72/500]	Time  8.221 ( 8.221)	Loss 1.5907 (1.8434)	CeLoss 0.3145 (0.3597)	SegCLSLoss 0.0894 (0.1206)	KLLoss 0.8203 (0.6145)	MaskLoss 1.3226 (1.3411)	MaskBCELoss 0.3800 (0.5020)	MaskDICELoss 0.9426 (0.8391)
Epoch: [0][ 73/500]	Time  7.964 ( 7.964)	Loss 1.8359 (1.1826)	CeLoss 1.8359 (0.6428)	SegCLSLoss 0.0000 (0.0802)	KLLoss 0.0000 (0.3211)	MaskLoss 0.0000 (0.5978)	MaskBCELoss 0.0000 (0.1339)	MaskDICELoss 0.0000 (0.4639)
Epoch: [0][ 74/500]	Time  8.063 ( 8.063)	Loss 1.5547 (1.4773)	CeLoss 1.5547 (0.6982)	SegCLSLoss 0.0000 (0.0654)	KLLoss 0.0000 (0.4402)	MaskLoss 0.0000 (0.7789)	MaskBCELoss 0.0000 (0.2379)	MaskDICELoss 0.0000 (0.5410)
Epoch: [0][ 75/500]	Time  7.379 ( 7.379)	Loss 1.2734 (1.5309)	CeLoss 1.2734 (0.7927)	SegCLSLoss 0.0000 (0.0748)	KLLoss 0.0000 (0.4004)	MaskLoss 0.0000 (0.7621)	MaskBCELoss 0.0000 (0.2133)	MaskDICELoss 0.0000 (0.5488)
Epoch: [0][ 76/500]	Time  8.853 ( 8.853)	Loss 1.1618 (1.2741)	CeLoss 0.3105 (0.3593)	SegCLSLoss 0.1475 (0.1047)	KLLoss 0.5781 (0.6074)	MaskLoss 1.1352 (1.0780)	MaskBCELoss 0.1396 (0.2157)	MaskDICELoss 0.9956 (0.8623)
Epoch: [0][ 77/500]	Time  8.875 ( 8.875)	Loss 1.3125 (1.2698)	CeLoss 1.3125 (0.4509)	SegCLSLoss 0.0000 (0.0826)	KLLoss 0.0000 (0.4461)	MaskLoss 0.0000 (0.8707)	MaskBCELoss 0.0000 (0.2282)	MaskDICELoss 0.0000 (0.6425)
Epoch: [0][ 78/500]	Time  9.144 ( 9.144)	Loss 1.0280 (1.5299)	CeLoss 0.3477 (0.3443)	SegCLSLoss 0.1250 (0.0767)	KLLoss 0.5859 (0.5789)	MaskLoss 1.0493 (1.1431)	MaskBCELoss 0.0621 (0.3839)	MaskDICELoss 0.9872 (0.7591)
Epoch: [0][ 79/500]	Time  8.721 ( 8.721)	Loss 0.8711 (0.9985)	CeLoss 0.1953 (0.3750)	SegCLSLoss 0.1187 (0.0902)	KLLoss 0.6055 (0.5012)	MaskLoss 1.0475 (0.8794)	MaskBCELoss 0.0623 (0.0925)	MaskDICELoss 0.9851 (0.7869)
[2025-03-01 22:35:32,014] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[0.000237], mom=[(0.9, 0.95)]
[2025-03-01 22:35:32,015] [INFO] [timer.py:215:stop] epoch=0/micro_step=800/global_step=80, RunningAvgSamplesPerSec=1.1487676901850887, CurrSamplesPerSec=1.0640060292349096, MemAllocated=31.09GB, MaxMemAllocated=36.65GB
Epoch: [0][ 80/500]	Time  9.400 ( 9.400)	Loss 1.9391 (1.2735)	CeLoss 0.3281 (0.4354)	SegCLSLoss 0.1377 (0.0761)	KLLoss 0.5586 (0.4309)	MaskLoss 1.4771 (0.9088)	MaskBCELoss 0.5360 (0.2302)	MaskDICELoss 0.9411 (0.6786)
Epoch: [0][ 81/500]	Time 10.354 (10.354)	Loss 1.1585 (1.3194)	CeLoss 0.2734 (0.2547)	SegCLSLoss 0.1226 (0.1116)	KLLoss 0.5781 (0.6104)	MaskLoss 1.1337 (1.2309)	MaskBCELoss 0.1718 (0.2626)	MaskDICELoss 0.9619 (0.9683)
Epoch: [0][ 82/500]	Time 10.557 (10.557)	Loss 1.9081 (1.3714)	CeLoss 0.3496 (0.3467)	SegCLSLoss 0.0679 (0.0839)	KLLoss 0.7852 (0.5914)	MaskLoss 1.4252 (1.1259)	MaskBCELoss 0.5418 (0.2800)	MaskDICELoss 0.8834 (0.8458)
Epoch: [0][ 83/500]	Time  7.516 ( 7.516)	Loss 1.1638 (1.2963)	CeLoss 0.2988 (0.6830)	SegCLSLoss 0.1045 (0.0603)	KLLoss 0.6016 (0.2826)	MaskLoss 1.1215 (0.6347)	MaskBCELoss 0.1677 (0.1771)	MaskDICELoss 0.9538 (0.4575)
Epoch: [0][ 84/500]	Time  8.972 ( 8.972)	Loss 1.2172 (1.4006)	CeLoss 0.2812 (0.3528)	SegCLSLoss 0.0986 (0.0939)	KLLoss 0.6562 (0.5518)	MaskLoss 1.1910 (1.1317)	MaskBCELoss 0.1945 (0.2900)	MaskDICELoss 0.9965 (0.8417)
Epoch: [0][ 85/500]	Time  7.609 ( 7.609)	Loss 1.0391 (1.1275)	CeLoss 1.0391 (0.6325)	SegCLSLoss 0.0000 (0.0359)	KLLoss 0.0000 (0.2436)	MaskLoss 0.0000 (0.5083)	MaskBCELoss 0.0000 (0.1486)	MaskDICELoss 0.0000 (0.3597)
Epoch: [0][ 86/500]	Time  7.370 ( 7.370)	Loss 0.5664 (1.3913)	CeLoss 0.5664 (0.4271)	SegCLSLoss 0.0000 (0.0774)	KLLoss 0.0000 (0.3900)	MaskLoss 0.0000 (0.9173)	MaskBCELoss 0.0000 (0.3111)	MaskDICELoss 0.0000 (0.6062)
Epoch: [0][ 87/500]	Time  9.352 ( 9.352)	Loss 1.4856 (1.3053)	CeLoss 0.2373 (0.4346)	SegCLSLoss 0.1079 (0.0814)	KLLoss 0.5977 (0.4562)	MaskLoss 1.2492 (0.9736)	MaskBCELoss 0.3800 (0.2290)	MaskDICELoss 0.8692 (0.7446)
Epoch: [0][ 88/500]	Time  8.322 ( 8.322)	Loss 0.6289 (1.3052)	CeLoss 0.6289 (0.4884)	SegCLSLoss 0.0000 (0.0609)	KLLoss 0.0000 (0.3316)	MaskLoss 0.0000 (0.8038)	MaskBCELoss 0.0000 (0.2563)	MaskDICELoss 0.0000 (0.5474)
Epoch: [0][ 89/500]	Time  7.833 ( 7.833)	Loss 1.0429 (1.4065)	CeLoss 0.2520 (0.4428)	SegCLSLoss 0.1123 (0.0731)	KLLoss 0.5391 (0.4643)	MaskLoss 1.1058 (1.0138)	MaskBCELoss 0.1209 (0.2802)	MaskDICELoss 0.9849 (0.7337)
[2025-03-01 22:36:58,241] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[0.000267], mom=[(0.9, 0.95)]
[2025-03-01 22:36:58,242] [INFO] [timer.py:215:stop] epoch=0/micro_step=900/global_step=90, RunningAvgSamplesPerSec=1.1500243502978638, CurrSamplesPerSec=1.1989292514075607, MemAllocated=31.77GB, MaxMemAllocated=36.78GB
Epoch: [0][ 90/500]	Time  8.342 ( 8.342)	Loss 0.8281 (0.9402)	CeLoss 0.8281 (0.5227)	SegCLSLoss 0.0000 (0.0301)	KLLoss 0.0000 (0.2617)	MaskLoss 0.0000 (0.4807)	MaskBCELoss 0.0000 (0.1080)	MaskDICELoss 0.0000 (0.3728)
Epoch: [0][ 91/500]	Time  8.018 ( 8.018)	Loss 1.2304 (1.4614)	CeLoss 0.2969 (0.6827)	SegCLSLoss 0.0615 (0.0579)	KLLoss 0.7461 (0.4258)	MaskLoss 1.1562 (0.8601)	MaskBCELoss 0.2162 (0.2131)	MaskDICELoss 0.9400 (0.6470)
Epoch: [0][ 92/500]	Time  9.135 ( 9.135)	Loss 1.5448 (1.1843)	CeLoss 0.2617 (0.3994)	SegCLSLoss 0.0520 (0.0558)	KLLoss 0.7617 (0.5148)	MaskLoss 1.2995 (0.9425)	MaskBCELoss 0.4053 (0.1903)	MaskDICELoss 0.8942 (0.7522)
Epoch: [0][ 93/500]	Time  7.373 ( 7.373)	Loss 1.3956 (1.1872)	CeLoss 0.2754 (0.6075)	SegCLSLoss 0.1079 (0.0503)	KLLoss 0.5391 (0.2748)	MaskLoss 1.2186 (0.6232)	MaskBCELoss 0.3041 (0.1620)	MaskDICELoss 0.9144 (0.4612)
Epoch: [0][ 94/500]	Time  8.116 ( 8.116)	Loss 1.5423 (0.9351)	CeLoss 0.1953 (0.4494)	SegCLSLoss 0.1177 (0.0427)	KLLoss 0.4883 (0.2740)	MaskLoss 1.3262 (0.5938)	MaskBCELoss 0.4169 (0.1116)	MaskDICELoss 0.9093 (0.4822)
Epoch: [0][ 95/500]	Time  8.782 ( 8.782)	Loss 3.1795 (1.5435)	CeLoss 0.3262 (0.4551)	SegCLSLoss 0.0513 (0.0561)	KLLoss 0.7773 (0.5070)	MaskLoss 2.0805 (1.0954)	MaskBCELoss 1.1918 (0.3420)	MaskDICELoss 0.8888 (0.7534)
Epoch: [0][ 96/500]	Time  8.219 ( 8.219)	Loss 0.3496 (1.0902)	CeLoss 0.3496 (0.4882)	SegCLSLoss 0.0000 (0.0552)	KLLoss 0.0000 (0.3594)	MaskLoss 0.0000 (0.7057)	MaskBCELoss 0.0000 (0.1476)	MaskDICELoss 0.0000 (0.5581)
Epoch: [0][ 97/500]	Time  8.523 ( 8.523)	Loss 0.9727 (1.2645)	CeLoss 0.9727 (0.4370)	SegCLSLoss 0.0000 (0.0626)	KLLoss 0.0000 (0.4672)	MaskLoss 0.0000 (0.9557)	MaskBCELoss 0.0000 (0.2125)	MaskDICELoss 0.0000 (0.7432)
Epoch: [0][ 98/500]	Time  9.385 ( 9.385)	Loss 0.8289 (1.1189)	CeLoss 0.1846 (0.3326)	SegCLSLoss 0.1104 (0.0638)	KLLoss 0.4570 (0.4547)	MaskLoss 1.0401 (0.9352)	MaskBCELoss 0.0457 (0.1912)	MaskDICELoss 0.9944 (0.7440)
Epoch: [0][ 99/500]	Time  8.455 ( 8.455)	Loss 1.0499 (1.1008)	CeLoss 0.2754 (0.3395)	SegCLSLoss 0.0757 (0.0589)	KLLoss 0.5664 (0.3871)	MaskLoss 1.0908 (0.8445)	MaskBCELoss 0.1280 (0.2064)	MaskDICELoss 0.9628 (0.6381)
[2025-03-01 22:38:23,048] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[0.00029699999999999996], mom=[(0.9, 0.95)]
[2025-03-01 22:38:23,049] [INFO] [timer.py:215:stop] epoch=0/micro_step=1000/global_step=100, RunningAvgSamplesPerSec=1.1529480754182742, CurrSamplesPerSec=1.1365950270769938, MemAllocated=31.24GB, MaxMemAllocated=36.78GB
Epoch: [0][100/500]	Time  8.800 ( 8.800)	Loss 0.9814 (1.3068)	CeLoss 0.2832 (0.4947)	SegCLSLoss 0.0830 (0.0625)	KLLoss 0.5625 (0.4654)	MaskLoss 1.0655 (0.9539)	MaskBCELoss 0.0830 (0.2026)	MaskDICELoss 0.9825 (0.7513)
Epoch: [0][101/500]	Time  8.722 ( 8.722)	Loss 1.5911 (1.1030)	CeLoss 0.2949 (0.4250)	SegCLSLoss 0.0510 (0.0443)	KLLoss 0.6875 (0.4414)	MaskLoss 1.3549 (0.8331)	MaskBCELoss 0.3956 (0.1593)	MaskDICELoss 0.9593 (0.6737)
Epoch: [0][102/500]	Time  9.234 ( 9.234)	Loss 0.8005 (1.1645)	CeLoss 0.2051 (0.4233)	SegCLSLoss 0.0874 (0.0695)	KLLoss 0.5234 (0.4479)	MaskLoss 1.0235 (0.9271)	MaskBCELoss 0.0265 (0.1621)	MaskDICELoss 0.9971 (0.7650)
Epoch: [0][103/500]	Time 10.644 (10.644)	Loss 1.3290 (1.4661)	CeLoss 0.2637 (0.2841)	SegCLSLoss 0.0518 (0.0673)	KLLoss 0.6250 (0.5805)	MaskLoss 1.2163 (1.2752)	MaskBCELoss 0.2878 (0.3405)	MaskDICELoss 0.9285 (0.9348)
Epoch: [0][104/500]	Time  7.224 ( 7.224)	Loss 1.2578 (1.3470)	CeLoss 1.2578 (0.7099)	SegCLSLoss 0.0000 (0.0430)	KLLoss 0.0000 (0.2545)	MaskLoss 0.0000 (0.6396)	MaskBCELoss 0.0000 (0.1972)	MaskDICELoss 0.0000 (0.4424)
Epoch: [0][105/500]	Time  9.056 ( 9.056)	Loss 0.9534 (1.3407)	CeLoss 0.2012 (0.2357)	SegCLSLoss 0.0820 (0.0764)	KLLoss 0.5078 (0.5762)	MaskLoss 1.0900 (1.1928)	MaskBCELoss 0.1108 (0.3135)	MaskDICELoss 0.9793 (0.8792)
Epoch: [0][106/500]	Time  9.733 ( 9.733)	Loss 1.2732 (1.3704)	CeLoss 0.3633 (0.2805)	SegCLSLoss 0.0405 (0.0647)	KLLoss 0.7344 (0.6039)	MaskLoss 1.1481 (1.2105)	MaskBCELoss 0.2109 (0.3016)	MaskDICELoss 0.9372 (0.9089)
Epoch: [0][107/500]	Time  7.830 ( 7.830)	Loss 1.3359 (1.0341)	CeLoss 1.3359 (0.5927)	SegCLSLoss 0.0000 (0.0314)	KLLoss 0.0000 (0.2863)	MaskLoss 0.0000 (0.5652)	MaskBCELoss 0.0000 (0.0955)	MaskDICELoss 0.0000 (0.4696)
Epoch: [0][108/500]	Time  9.363 ( 9.363)	Loss 1.0880 (0.9593)	CeLoss 0.3516 (0.3794)	SegCLSLoss 0.0537 (0.0513)	KLLoss 0.6328 (0.4816)	MaskLoss 1.0997 (0.8649)	MaskBCELoss 0.1061 (0.0813)	MaskDICELoss 0.9936 (0.7836)
Epoch: [0][109/500]	Time  8.903 ( 8.903)	Loss 1.5156 (0.9497)	CeLoss 1.5156 (0.3395)	SegCLSLoss 0.0000 (0.0543)	KLLoss 0.0000 (0.3775)	MaskLoss 0.0000 (0.8031)	MaskBCELoss 0.0000 (0.1211)	MaskDICELoss 0.0000 (0.6820)
[2025-03-01 22:39:53,173] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[0.0002994489795918367], mom=[(0.9, 0.95)]
[2025-03-01 22:39:53,174] [INFO] [timer.py:215:stop] epoch=0/micro_step=1100/global_step=110, RunningAvgSamplesPerSec=1.1488078336722054, CurrSamplesPerSec=1.0621708844760416, MemAllocated=31.41GB, MaxMemAllocated=36.78GB
Epoch: [0][110/500]	Time  9.417 ( 9.417)	Loss 0.1611 (1.1074)	CeLoss 0.1611 (0.4299)	SegCLSLoss 0.0000 (0.0533)	KLLoss 0.0000 (0.3713)	MaskLoss 0.0000 (0.8377)	MaskBCELoss 0.0000 (0.1546)	MaskDICELoss 0.0000 (0.6831)
Epoch: [0][111/500]	Time  9.368 ( 9.368)	Loss 0.8438 (1.2439)	CeLoss 0.8438 (0.4077)	SegCLSLoss 0.0000 (0.0486)	KLLoss 0.0000 (0.3895)	MaskLoss 0.0000 (0.9188)	MaskBCELoss 0.0000 (0.2350)	MaskDICELoss 0.0000 (0.6838)
Epoch: [0][112/500]	Time  7.397 ( 7.397)	Loss 1.2235 (1.0874)	CeLoss 0.1611 (0.7279)	SegCLSLoss 0.1099 (0.0309)	KLLoss 0.4336 (0.2137)	MaskLoss 1.1945 (0.4623)	MaskBCELoss 0.2736 (0.0753)	MaskDICELoss 0.9208 (0.3870)
Epoch: [0][113/500]	Time  7.046 ( 7.046)	Loss 1.0625 (1.4314)	CeLoss 1.0625 (0.5976)	SegCLSLoss 0.0000 (0.0355)	KLLoss 0.0000 (0.3010)	MaskLoss 0.0000 (0.7668)	MaskBCELoss 0.0000 (0.2884)	MaskDICELoss 0.0000 (0.4784)
Epoch: [0][114/500]	Time  6.983 ( 6.983)	Loss 1.3834 (1.3437)	CeLoss 0.1836 (0.5579)	SegCLSLoss 0.1289 (0.0539)	KLLoss 0.4316 (0.3793)	MaskLoss 1.2352 (0.8628)	MaskBCELoss 0.3451 (0.2184)	MaskDICELoss 0.8900 (0.6444)
Epoch: [0][115/500]	Time  8.347 ( 8.347)	Loss 2.3775 (1.4330)	CeLoss 0.5391 (0.5839)	SegCLSLoss 0.0474 (0.0424)	KLLoss 0.6055 (0.3979)	MaskLoss 1.4949 (0.8812)	MaskBCELoss 0.7117 (0.2582)	MaskDICELoss 0.7832 (0.6230)
Epoch: [0][116/500]	Time  8.332 ( 8.332)	Loss 1.4871 (1.3331)	CeLoss 0.3418 (0.5768)	SegCLSLoss 0.0427 (0.0441)	KLLoss 0.6992 (0.4008)	MaskLoss 1.2403 (0.8570)	MaskBCELoss 0.3358 (0.2038)	MaskDICELoss 0.9045 (0.6531)
Epoch: [0][117/500]	Time  8.255 ( 8.255)	Loss 1.3044 (1.1520)	CeLoss 0.2500 (0.5953)	SegCLSLoss 0.0479 (0.0396)	KLLoss 0.6992 (0.3289)	MaskLoss 1.2015 (0.6979)	MaskBCELoss 0.2868 (0.1254)	MaskDICELoss 0.9147 (0.5725)
Epoch: [0][118/500]	Time  8.641 ( 8.641)	Loss 1.2346 (1.0367)	CeLoss 0.3828 (0.3438)	SegCLSLoss 0.0449 (0.0414)	KLLoss 0.6758 (0.4262)	MaskLoss 1.1392 (0.8362)	MaskBCELoss 0.1725 (0.1692)	MaskDICELoss 0.9667 (0.6670)
Epoch: [0][119/500]	Time  8.990 ( 8.990)	Loss 1.0468 (1.1506)	CeLoss 0.2637 (0.4521)	SegCLSLoss 0.0645 (0.0398)	KLLoss 0.5391 (0.4145)	MaskLoss 1.0962 (0.8335)	MaskBCELoss 0.1345 (0.1746)	MaskDICELoss 0.9617 (0.6589)
[2025-03-01 22:41:15,408] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[0.00029883673469387754], mom=[(0.9, 0.95)]
[2025-03-01 22:41:15,409] [INFO] [timer.py:215:stop] epoch=0/micro_step=1200/global_step=120, RunningAvgSamplesPerSec=1.1542304159248717, CurrSamplesPerSec=1.1267320806237915, MemAllocated=31.24GB, MaxMemAllocated=36.85GB
Epoch: [0][120/500]	Time  8.877 ( 8.877)	Loss 0.8510 (1.4381)	CeLoss 0.2148 (0.3666)	SegCLSLoss 0.0659 (0.0623)	KLLoss 0.5078 (0.4434)	MaskLoss 1.0491 (1.0821)	MaskBCELoss 0.0523 (0.3331)	MaskDICELoss 0.9968 (0.7490)
Epoch: [0][121/500]	Time  8.897 ( 8.897)	Loss 0.9074 (1.0834)	CeLoss 0.2324 (0.3687)	SegCLSLoss 0.0620 (0.0552)	KLLoss 0.5078 (0.3586)	MaskLoss 1.0652 (0.8365)	MaskBCELoss 0.0741 (0.1794)	MaskDICELoss 0.9911 (0.6571)
Epoch: [0][122/500]	Time  8.965 ( 8.965)	Loss 1.3160 (1.3174)	CeLoss 0.2832 (0.3545)	SegCLSLoss 0.0481 (0.0445)	KLLoss 0.6055 (0.5711)	MaskLoss 1.2304 (1.1185)	MaskBCELoss 0.2627 (0.2543)	MaskDICELoss 0.9677 (0.8642)
Epoch: [0][123/500]	Time  9.156 ( 9.156)	Loss 1.5928 (1.1955)	CeLoss 0.3125 (0.3355)	SegCLSLoss 0.0457 (0.0496)	KLLoss 0.6758 (0.5473)	MaskLoss 1.3242 (1.0657)	MaskBCELoss 0.3965 (0.2015)	MaskDICELoss 0.9277 (0.8642)
Epoch: [0][124/500]	Time  8.600 ( 8.600)	Loss 1.1016 (1.0652)	CeLoss 1.1016 (0.5546)	SegCLSLoss 0.0000 (0.0279)	KLLoss 0.0000 (0.2998)	MaskLoss 0.0000 (0.6088)	MaskBCELoss 0.0000 (0.1283)	MaskDICELoss 0.0000 (0.4805)
Epoch: [0][125/500]	Time  8.037 ( 8.037)	Loss 1.6977 (1.6204)	CeLoss 0.3516 (0.3214)	SegCLSLoss 0.0417 (0.0464)	KLLoss 0.6602 (0.5461)	MaskLoss 1.3353 (1.2396)	MaskBCELoss 0.4380 (0.4373)	MaskDICELoss 0.8973 (0.8023)
Epoch: [0][126/500]	Time  9.926 ( 9.926)	Loss 0.9722 (1.2234)	CeLoss 0.1885 (0.3133)	SegCLSLoss 0.0786 (0.0526)	KLLoss 0.4883 (0.4963)	MaskLoss 1.1069 (1.0759)	MaskBCELoss 0.1275 (0.2307)	MaskDICELoss 0.9794 (0.8452)
Epoch: [0][127/500]	Time  9.261 ( 9.261)	Loss 1.7207 (1.1791)	CeLoss 0.3809 (0.3086)	SegCLSLoss 0.0356 (0.0463)	KLLoss 0.6758 (0.4906)	MaskLoss 1.3074 (1.0570)	MaskBCELoss 0.4457 (0.2127)	MaskDICELoss 0.8617 (0.8443)
Epoch: [0][128/500]	Time  8.429 ( 8.429)	Loss 1.1328 (1.2430)	CeLoss 1.1328 (0.4188)	SegCLSLoss 0.0000 (0.0531)	KLLoss 0.0000 (0.4262)	MaskLoss 0.0000 (0.9536)	MaskBCELoss 0.0000 (0.2140)	MaskDICELoss 0.0000 (0.7395)
Epoch: [0][129/500]	Time  8.220 ( 8.220)	Loss 0.8863 (1.2204)	CeLoss 0.1758 (0.3526)	SegCLSLoss 0.0674 (0.0404)	KLLoss 0.4941 (0.4510)	MaskLoss 1.0741 (0.9962)	MaskBCELoss 0.0935 (0.2331)	MaskDICELoss 0.9806 (0.7630)
[2025-03-01 22:42:43,206] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[0.0002982244897959184], mom=[(0.9, 0.95)]
[2025-03-01 22:42:43,207] [INFO] [timer.py:215:stop] epoch=0/micro_step=1300/global_step=130, RunningAvgSamplesPerSec=1.153038661805606, CurrSamplesPerSec=1.2040509099212144, MemAllocated=31.24GB, MaxMemAllocated=36.85GB
Epoch: [0][130/500]	Time  8.307 ( 8.307)	Loss 1.2050 (1.0886)	CeLoss 0.2305 (0.5055)	SegCLSLoss 0.0640 (0.0381)	KLLoss 0.4746 (0.3744)	MaskLoss 1.1691 (0.7878)	MaskBCELoss 0.2391 (0.1134)	MaskDICELoss 0.9300 (0.6744)
Epoch: [0][131/500]	Time  8.705 ( 8.705)	Loss 1.1875 (1.2396)	CeLoss 1.1875 (0.5133)	SegCLSLoss 0.0000 (0.0335)	KLLoss 0.0000 (0.4018)	MaskLoss 0.0000 (0.8515)	MaskBCELoss 0.0000 (0.1893)	MaskDICELoss 0.0000 (0.6622)
Epoch: [0][132/500]	Time  8.479 ( 8.479)	Loss 1.5482 (1.3336)	CeLoss 0.3066 (0.3084)	SegCLSLoss 0.0237 (0.0439)	KLLoss 0.7227 (0.5301)	MaskLoss 1.3228 (1.1333)	MaskBCELoss 0.3790 (0.2911)	MaskDICELoss 0.9438 (0.8422)
Epoch: [0][133/500]	Time  9.400 ( 9.400)	Loss 1.4562 (1.1698)	CeLoss 0.3477 (0.3955)	SegCLSLoss 0.0299 (0.0383)	KLLoss 0.7031 (0.3684)	MaskLoss 1.2400 (0.8711)	MaskBCELoss 0.3153 (0.2130)	MaskDICELoss 0.9248 (0.6581)
Epoch: [0][134/500]	Time  6.903 ( 6.903)	Loss 2.1816 (1.4024)	CeLoss 0.1270 (0.5161)	SegCLSLoss 0.1016 (0.0521)	KLLoss 0.4219 (0.3389)	MaskLoss 1.6446 (0.9127)	MaskBCELoss 0.7877 (0.2693)	MaskDICELoss 0.8569 (0.6434)
Epoch: [0][135/500]	Time  8.386 ( 8.386)	Loss 2.9581 (1.5080)	CeLoss 0.3066 (0.3531)	SegCLSLoss 0.0245 (0.0380)	KLLoss 0.7656 (0.5596)	MaskLoss 1.8131 (1.1620)	MaskBCELoss 1.1555 (0.3697)	MaskDICELoss 0.6577 (0.7923)
Epoch: [0][136/500]	Time  7.494 ( 7.494)	Loss 1.3825 (1.1924)	CeLoss 0.2480 (0.6652)	SegCLSLoss 0.0317 (0.0318)	KLLoss 0.6875 (0.2660)	MaskLoss 1.2590 (0.5968)	MaskBCELoss 0.3263 (0.1419)	MaskDICELoss 0.9327 (0.4550)
Epoch: [0][137/500]	Time  9.131 ( 9.131)	Loss 0.9738 (1.2446)	CeLoss 0.2002 (0.2932)	SegCLSLoss 0.0515 (0.0494)	KLLoss 0.4902 (0.4828)	MaskLoss 1.1232 (1.0948)	MaskBCELoss 0.1244 (0.2529)	MaskDICELoss 0.9988 (0.8419)
Epoch: [0][138/500]	Time  7.997 ( 7.997)	Loss 1.0743 (1.2607)	CeLoss 0.2676 (0.3822)	SegCLSLoss 0.0337 (0.0362)	KLLoss 0.5820 (0.4500)	MaskLoss 1.1294 (0.9716)	MaskBCELoss 0.1497 (0.2497)	MaskDICELoss 0.9797 (0.7219)
Epoch: [0][139/500]	Time  8.484 ( 8.484)	Loss 1.8579 (0.9786)	CeLoss 0.4062 (0.4328)	SegCLSLoss 0.0286 (0.0250)	KLLoss 0.6953 (0.3596)	MaskLoss 1.3643 (0.6892)	MaskBCELoss 0.5039 (0.1258)	MaskDICELoss 0.8603 (0.5634)
[2025-03-01 22:44:09,654] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[0.00029761224489795917], mom=[(0.9, 0.95)]
[2025-03-01 22:44:09,655] [INFO] [timer.py:215:stop] epoch=0/micro_step=1400/global_step=140, RunningAvgSamplesPerSec=1.1533217725628542, CurrSamplesPerSec=0.8721960456449452, MemAllocated=31.23GB, MaxMemAllocated=36.85GB
Epoch: [0][140/500]	Time 11.467 (11.467)	Loss 0.9115 (1.1287)	CeLoss 0.2119 (0.4685)	SegCLSLoss 0.0483 (0.0264)	KLLoss 0.5234 (0.3648)	MaskLoss 1.0673 (0.7458)	MaskBCELoss 0.0943 (0.1828)	MaskDICELoss 0.9730 (0.5630)
Epoch: [0][141/500]	Time  7.478 ( 7.478)	Loss 0.8423 (1.2206)	CeLoss 0.1973 (0.5833)	SegCLSLoss 0.0527 (0.0297)	KLLoss 0.4668 (0.3309)	MaskLoss 1.0414 (0.7417)	MaskBCELoss 0.0653 (0.1678)	MaskDICELoss 0.9761 (0.5739)
Epoch: [0][142/500]	Time  7.908 ( 7.908)	Loss 1.1447 (1.2396)	CeLoss 0.2539 (0.4811)	SegCLSLoss 0.0571 (0.0398)	KLLoss 0.4746 (0.3625)	MaskLoss 1.1509 (0.8682)	MaskBCELoss 0.1907 (0.2031)	MaskDICELoss 0.9602 (0.6651)
Epoch: [0][143/500]	Time  8.356 ( 8.356)	Loss 1.1562 (1.1519)	CeLoss 1.1562 (0.5442)	SegCLSLoss 0.0000 (0.0253)	KLLoss 0.0000 (0.3490)	MaskLoss 0.0000 (0.7216)	MaskBCELoss 0.0000 (0.1561)	MaskDICELoss 0.0000 (0.5654)
Epoch: [0][144/500]	Time  9.524 ( 9.524)	Loss 1.4119 (1.2974)	CeLoss 0.2617 (0.2671)	SegCLSLoss 0.0452 (0.0484)	KLLoss 0.5625 (0.5395)	MaskLoss 1.2725 (1.2129)	MaskBCELoss 0.3270 (0.2664)	MaskDICELoss 0.9455 (0.9464)
Epoch: [0][145/500]	Time  8.543 ( 8.543)	Loss 1.4049 (1.2425)	CeLoss 0.2715 (0.2895)	SegCLSLoss 0.0579 (0.0570)	KLLoss 0.4707 (0.4578)	MaskLoss 1.1932 (1.0893)	MaskBCELoss 0.3384 (0.2532)	MaskDICELoss 0.8548 (0.8361)
Epoch: [0][146/500]	Time  8.750 ( 8.750)	Loss 1.0195 (1.3680)	CeLoss 0.2471 (0.5171)	SegCLSLoss 0.0469 (0.0355)	KLLoss 0.5859 (0.4541)	MaskLoss 1.0995 (0.9695)	MaskBCELoss 0.1335 (0.2322)	MaskDICELoss 0.9660 (0.7373)
Epoch: [0][147/500]	Time  8.685 ( 8.685)	Loss 1.0395 (1.1892)	CeLoss 0.2461 (0.3841)	SegCLSLoss 0.0496 (0.0405)	KLLoss 0.5742 (0.4410)	MaskLoss 1.1167 (0.9506)	MaskBCELoss 0.1398 (0.2063)	MaskDICELoss 0.9769 (0.7442)
Epoch: [0][148/500]	Time  8.105 ( 8.105)	Loss 0.8945 (1.0506)	CeLoss 0.8945 (0.3614)	SegCLSLoss 0.0000 (0.0276)	KLLoss 0.0000 (0.4533)	MaskLoss 0.0000 (0.8278)	MaskBCELoss 0.0000 (0.1744)	MaskDICELoss 0.0000 (0.6534)
Epoch: [0][149/500]	Time  9.857 ( 9.857)	Loss 1.5156 (1.3683)	CeLoss 1.5156 (0.3831)	SegCLSLoss 0.0000 (0.0423)	KLLoss 0.0000 (0.5020)	MaskLoss 0.0000 (1.1008)	MaskBCELoss 0.0000 (0.2756)	MaskDICELoss 0.0000 (0.8251)
[2025-03-01 22:45:35,993] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[0.00029699999999999996], mom=[(0.9, 0.95)]
[2025-03-01 22:45:35,994] [INFO] [timer.py:215:stop] epoch=0/micro_step=1500/global_step=150, RunningAvgSamplesPerSec=1.1536640949147385, CurrSamplesPerSec=1.0951254288571373, MemAllocated=31.08GB, MaxMemAllocated=36.85GB
Epoch: [0][150/500]	Time  9.133 ( 9.133)	Loss 1.1099 (1.1688)	CeLoss 0.2852 (0.4149)	SegCLSLoss 0.0292 (0.0338)	KLLoss 0.6992 (0.4785)	MaskLoss 1.1187 (0.9362)	MaskBCELoss 0.1678 (0.1794)	MaskDICELoss 0.9509 (0.7568)
Epoch: [0][151/500]	Time  8.962 ( 8.962)	Loss 1.1718 (1.0246)	CeLoss 0.2188 (0.4437)	SegCLSLoss 0.0265 (0.0284)	KLLoss 0.7461 (0.4258)	MaskLoss 1.1732 (0.7864)	MaskBCELoss 0.2352 (0.1157)	MaskDICELoss 0.9380 (0.6707)
Epoch: [0][152/500]	Time  9.573 ( 9.573)	Loss 1.0352 (1.1648)	CeLoss 0.3223 (0.3310)	SegCLSLoss 0.0242 (0.0312)	KLLoss 0.7578 (0.5496)	MaskLoss 1.0833 (1.0591)	MaskBCELoss 0.1064 (0.1926)	MaskDICELoss 0.9769 (0.8666)
Epoch: [0][153/500]	Time  9.883 ( 9.883)	Loss 2.0221 (1.5889)	CeLoss 0.3320 (0.3345)	SegCLSLoss 0.0236 (0.0314)	KLLoss 0.7266 (0.4881)	MaskLoss 1.5264 (1.1665)	MaskBCELoss 0.6101 (0.4370)	MaskDICELoss 0.9163 (0.7295)
Epoch: [0][154/500]	Time  9.423 ( 9.423)	Loss 1.1155 (1.1890)	CeLoss 0.1973 (0.3550)	SegCLSLoss 0.0557 (0.0378)	KLLoss 0.5000 (0.4684)	MaskLoss 1.1510 (0.9696)	MaskBCELoss 0.2103 (0.2201)	MaskDICELoss 0.9407 (0.7494)
Epoch: [0][155/500]	Time  9.587 ( 9.587)	Loss 2.0285 (1.3141)	CeLoss 0.2031 (0.4330)	SegCLSLoss 0.0437 (0.0373)	KLLoss 0.5156 (0.3609)	MaskLoss 1.5592 (0.9091)	MaskBCELoss 0.6829 (0.2719)	MaskDICELoss 0.8763 (0.6372)
Epoch: [0][156/500]	Time  8.674 ( 8.674)	Loss 0.7031 (1.1705)	CeLoss 0.7031 (0.5827)	SegCLSLoss 0.0000 (0.0250)	KLLoss 0.0000 (0.3641)	MaskLoss 0.0000 (0.7047)	MaskBCELoss 0.0000 (0.1487)	MaskDICELoss 0.0000 (0.5560)
Epoch: [0][157/500]	Time  9.845 ( 9.845)	Loss 1.0380 (1.2566)	CeLoss 0.2168 (0.3410)	SegCLSLoss 0.0369 (0.0285)	KLLoss 0.5703 (0.5664)	MaskLoss 1.0976 (1.0641)	MaskBCELoss 0.1693 (0.2462)	MaskDICELoss 0.9283 (0.8180)
Epoch: [0][158/500]	Time  9.130 ( 9.130)	Loss 1.8984 (1.3800)	CeLoss 1.8984 (0.5526)	SegCLSLoss 0.0000 (0.0299)	KLLoss 0.0000 (0.4969)	MaskLoss 0.0000 (0.9517)	MaskBCELoss 0.0000 (0.2244)	MaskDICELoss 0.0000 (0.7273)
Epoch: [0][159/500]	Time  9.172 ( 9.172)	Loss 1.2783 (1.2017)	CeLoss 0.2617 (0.4403)	SegCLSLoss 0.0265 (0.0332)	KLLoss 0.6641 (0.3760)	MaskLoss 1.2127 (0.8446)	MaskBCELoss 0.2644 (0.2151)	MaskDICELoss 0.9483 (0.6295)
[2025-03-01 22:47:10,459] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[0.0002963877551020408], mom=[(0.9, 0.95)]
[2025-03-01 22:47:10,460] [INFO] [timer.py:215:stop] epoch=0/micro_step=1600/global_step=160, RunningAvgSamplesPerSec=1.1471617245186683, CurrSamplesPerSec=0.9790354771932662, MemAllocated=31.24GB, MaxMemAllocated=36.85GB
Epoch: [0][160/500]	Time 10.216 (10.216)	Loss 1.1921 (1.1118)	CeLoss 0.2490 (0.4135)	SegCLSLoss 0.0476 (0.0213)	KLLoss 0.5117 (0.4539)	MaskLoss 1.1588 (0.8241)	MaskBCELoss 0.2262 (0.1836)	MaskDICELoss 0.9326 (0.6404)
Epoch: [0][161/500]	Time  8.778 ( 8.778)	Loss 0.8154 (1.1286)	CeLoss 0.2334 (0.3886)	SegCLSLoss 0.0420 (0.0345)	KLLoss 0.5547 (0.4721)	MaskLoss 1.0268 (0.9353)	MaskBCELoss 0.0321 (0.1701)	MaskDICELoss 0.9947 (0.7652)
Epoch: [0][162/500]	Time  8.950 ( 8.950)	Loss 1.0901 (1.1222)	CeLoss 0.4688 (0.4494)	SegCLSLoss 0.0366 (0.0295)	KLLoss 0.5703 (0.3926)	MaskLoss 1.0300 (0.8133)	MaskBCELoss 0.0592 (0.1678)	MaskDICELoss 0.9708 (0.6455)
Epoch: [0][163/500]	Time  9.116 ( 9.116)	Loss 0.7749 (1.1475)	CeLoss 0.1680 (0.3340)	SegCLSLoss 0.0615 (0.0428)	KLLoss 0.4688 (0.4264)	MaskLoss 1.0207 (0.9565)	MaskBCELoss 0.0435 (0.2093)	MaskDICELoss 0.9772 (0.7472)
Epoch: [0][164/500]	Time  8.362 ( 8.362)	Loss 1.1484 (1.0361)	CeLoss 1.1484 (0.6462)	SegCLSLoss 0.0000 (0.0149)	KLLoss 0.0000 (0.3309)	MaskLoss 0.0000 (0.5531)	MaskBCELoss 0.0000 (0.0706)	MaskDICELoss 0.0000 (0.4825)
Epoch: [0][165/500]	Time  8.842 ( 8.842)	Loss 0.7543 (1.5152)	CeLoss 0.2031 (0.3718)	SegCLSLoss 0.0315 (0.0308)	KLLoss 0.6133 (0.5359)	MaskLoss 1.0151 (1.1883)	MaskBCELoss 0.0187 (0.3559)	MaskDICELoss 0.9964 (0.8323)
Epoch: [0][166/500]	Time  8.777 ( 8.777)	Loss 1.3827 (1.1378)	CeLoss 0.2930 (0.4572)	SegCLSLoss 0.0195 (0.0192)	KLLoss 0.7578 (0.4402)	MaskLoss 1.2313 (0.8203)	MaskBCELoss 0.3095 (0.1739)	MaskDICELoss 0.9218 (0.6465)
Epoch: [0][167/500]	Time  8.836 ( 8.836)	Loss 0.8240 (1.0596)	CeLoss 0.2412 (0.2883)	SegCLSLoss 0.0240 (0.0302)	KLLoss 0.6133 (0.4561)	MaskLoss 1.0217 (0.9295)	MaskBCELoss 0.0395 (0.1942)	MaskDICELoss 0.9823 (0.7353)
Epoch: [0][168/500]	Time  9.035 ( 9.035)	Loss 1.0130 (0.9902)	CeLoss 0.2334 (0.2819)	SegCLSLoss 0.0181 (0.0276)	KLLoss 0.7539 (0.4105)	MaskLoss 1.1033 (0.8256)	MaskBCELoss 0.1461 (0.1878)	MaskDICELoss 0.9572 (0.6378)
Epoch: [0][169/500]	Time  9.536 ( 9.536)	Loss 0.7266 (0.9356)	CeLoss 0.7266 (0.2418)	SegCLSLoss 0.0000 (0.0309)	KLLoss 0.0000 (0.4436)	MaskLoss 0.0000 (0.9015)	MaskBCELoss 0.0000 (0.1517)	MaskDICELoss 0.0000 (0.7498)
[2025-03-01 22:48:40,394] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[0.00029577551020408164], mom=[(0.9, 0.95)]
[2025-03-01 22:48:40,395] [INFO] [timer.py:215:stop] epoch=0/micro_step=1700/global_step=170, RunningAvgSamplesPerSec=1.1450128104273893, CurrSamplesPerSec=1.0307256610880042, MemAllocated=31.23GB, MaxMemAllocated=36.85GB
Epoch: [0][170/500]	Time  9.704 ( 9.704)	Loss 0.8048 (1.0897)	CeLoss 0.2812 (0.3784)	SegCLSLoss 0.0306 (0.0300)	KLLoss 0.5469 (0.5250)	MaskLoss 0.9819 (0.9952)	MaskBCELoss 0.0113 (0.1327)	MaskDICELoss 0.9705 (0.8625)
Epoch: [0][171/500]	Time 10.251 (10.251)	Loss 1.3335 (1.0570)	CeLoss 0.2451 (0.2433)	SegCLSLoss 0.0342 (0.0317)	KLLoss 0.5430 (0.5555)	MaskLoss 1.2075 (1.0236)	MaskBCELoss 0.3120 (0.1910)	MaskDICELoss 0.8955 (0.8327)
Epoch: [0][172/500]	Time  9.689 ( 9.689)	Loss 1.2380 (0.9754)	CeLoss 0.3301 (0.2649)	SegCLSLoss 0.0219 (0.0329)	KLLoss 0.7031 (0.5955)	MaskLoss 1.1331 (1.0723)	MaskBCELoss 0.2198 (0.1052)	MaskDICELoss 0.9133 (0.9671)
Epoch: [0][173/500]	Time  8.485 ( 8.485)	Loss 2.9069 (1.3390)	CeLoss 0.2812 (0.6171)	SegCLSLoss 0.0293 (0.0211)	KLLoss 0.6367 (0.3543)	MaskLoss 1.9837 (0.7802)	MaskBCELoss 1.0788 (0.2142)	MaskDICELoss 0.9050 (0.5660)
Epoch: [0][174/500]	Time  9.104 ( 9.104)	Loss 1.3364 (0.9969)	CeLoss 0.2695 (0.3560)	SegCLSLoss 0.0334 (0.0222)	KLLoss 0.5195 (0.3484)	MaskLoss 1.2003 (0.7186)	MaskBCELoss 0.2994 (0.1803)	MaskDICELoss 0.9008 (0.5383)
Epoch: [0][175/500]	Time  7.917 ( 7.917)	Loss 1.7344 (1.2465)	CeLoss 0.1875 (0.5898)	SegCLSLoss 0.0737 (0.0220)	KLLoss 0.4121 (0.3338)	MaskLoss 1.3557 (0.7331)	MaskBCELoss 0.5546 (0.1862)	MaskDICELoss 0.8011 (0.5469)
Epoch: [0][176/500]	Time  9.135 ( 9.135)	Loss 1.2417 (1.2136)	CeLoss 0.2275 (0.3668)	SegCLSLoss 0.0205 (0.0283)	KLLoss 0.6953 (0.4156)	MaskLoss 1.1775 (0.8724)	MaskBCELoss 0.2765 (0.2645)	MaskDICELoss 0.9010 (0.6079)
Epoch: [0][177/500]	Time  9.233 ( 9.233)	Loss 1.1016 (1.0941)	CeLoss 1.1016 (0.3905)	SegCLSLoss 0.0000 (0.0241)	KLLoss 0.0000 (0.4176)	MaskLoss 0.0000 (0.8283)	MaskBCELoss 0.0000 (0.1850)	MaskDICELoss 0.0000 (0.6433)
Epoch: [0][178/500]	Time  8.034 ( 8.034)	Loss 0.0623 (1.0354)	CeLoss 0.0623 (0.4355)	SegCLSLoss 0.0000 (0.0185)	KLLoss 0.0000 (0.3684)	MaskLoss 0.0000 (0.7112)	MaskBCELoss 0.0000 (0.1567)	MaskDICELoss 0.0000 (0.5545)
Epoch: [0][179/500]	Time  9.669 ( 9.669)	Loss 1.0577 (1.3237)	CeLoss 0.2910 (0.4918)	SegCLSLoss 0.0162 (0.0211)	KLLoss 0.7461 (0.5172)	MaskLoss 1.0903 (0.9461)	MaskBCELoss 0.1425 (0.2323)	MaskDICELoss 0.9478 (0.7138)
[2025-03-01 22:50:10,927] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[0.00029516326530612243], mom=[(0.9, 0.95)]
[2025-03-01 22:50:10,927] [INFO] [timer.py:215:stop] epoch=0/micro_step=1800/global_step=180, RunningAvgSamplesPerSec=1.1426737302322039, CurrSamplesPerSec=1.109376575817497, MemAllocated=30.79GB, MaxMemAllocated=36.85GB
Epoch: [0][180/500]	Time  9.016 ( 9.016)	Loss 0.9718 (1.2562)	CeLoss 0.2734 (0.3496)	SegCLSLoss 0.0581 (0.0304)	KLLoss 0.4531 (0.5316)	MaskLoss 1.0678 (1.0734)	MaskBCELoss 0.0901 (0.2364)	MaskDICELoss 0.9777 (0.8370)
Epoch: [0][181/500]	Time  8.801 ( 8.801)	Loss 0.8565 (1.1840)	CeLoss 0.1797 (0.4320)	SegCLSLoss 0.0308 (0.0239)	KLLoss 0.5547 (0.3937)	MaskLoss 1.0694 (0.8544)	MaskBCELoss 0.0843 (0.2086)	MaskDICELoss 0.9851 (0.6458)
Epoch: [0][182/500]	Time  9.328 ( 9.328)	Loss 1.5120 (1.2635)	CeLoss 0.2656 (0.2894)	SegCLSLoss 0.0212 (0.0257)	KLLoss 0.6992 (0.5852)	MaskLoss 1.2598 (1.0832)	MaskBCELoss 0.4045 (0.2799)	MaskDICELoss 0.8553 (0.8033)
Epoch: [0][183/500]	Time  9.392 ( 9.392)	Loss 1.2128 (1.0699)	CeLoss 0.2324 (0.3712)	SegCLSLoss 0.0630 (0.0237)	KLLoss 0.4434 (0.4803)	MaskLoss 1.1606 (0.9098)	MaskBCELoss 0.2459 (0.1549)	MaskDICELoss 0.9148 (0.7549)
Epoch: [0][184/500]	Time  9.859 ( 9.859)	Loss 1.9232 (1.1244)	CeLoss 0.2754 (0.4420)	SegCLSLoss 0.0273 (0.0244)	KLLoss 0.6289 (0.4617)	MaskLoss 1.4537 (0.9013)	MaskBCELoss 0.6048 (0.1463)	MaskDICELoss 0.8489 (0.7550)
Epoch: [0][185/500]	Time  8.522 ( 8.522)	Loss 1.0547 (1.2083)	CeLoss 1.0547 (0.5000)	SegCLSLoss 0.0000 (0.0168)	KLLoss 0.0000 (0.4471)	MaskLoss 0.0000 (0.8411)	MaskBCELoss 0.0000 (0.1863)	MaskDICELoss 0.0000 (0.6547)
Epoch: [0][186/500]	Time  8.401 ( 8.401)	Loss 1.5223 (1.5740)	CeLoss 0.2451 (0.3004)	SegCLSLoss 0.0173 (0.0245)	KLLoss 0.7305 (0.5609)	MaskLoss 1.2980 (1.2470)	MaskBCELoss 0.4129 (0.4253)	MaskDICELoss 0.8851 (0.8217)
Epoch: [0][187/500]	Time  9.681 ( 9.681)	Loss 1.0233 (0.9260)	CeLoss 0.2090 (0.1882)	SegCLSLoss 0.0344 (0.0244)	KLLoss 0.5742 (0.4828)	MaskLoss 1.1134 (0.9178)	MaskBCELoss 0.1600 (0.1778)	MaskDICELoss 0.9534 (0.7400)
Epoch: [0][188/500]	Time  9.419 ( 9.419)	Loss 1.4851 (1.1389)	CeLoss 0.2178 (0.4283)	SegCLSLoss 0.0278 (0.0233)	KLLoss 0.5586 (0.4719)	MaskLoss 1.2683 (0.9177)	MaskBCELoss 0.4130 (0.1601)	MaskDICELoss 0.8553 (0.7577)
Epoch: [0][189/500]	Time  7.904 ( 7.904)	Loss 1.1875 (1.2202)	CeLoss 1.1875 (0.7573)	SegCLSLoss 0.0000 (0.0126)	KLLoss 0.0000 (0.2916)	MaskLoss 0.0000 (0.5780)	MaskBCELoss 0.0000 (0.1117)	MaskDICELoss 0.0000 (0.4664)
[2025-03-01 22:51:40,961] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[0.0002945510204081632], mom=[(0.9, 0.95)]
[2025-03-01 22:51:40,962] [INFO] [timer.py:215:stop] epoch=0/micro_step=1900/global_step=190, RunningAvgSamplesPerSec=1.1409356569060318, CurrSamplesPerSec=1.146084152001824, MemAllocated=30.94GB, MaxMemAllocated=36.85GB
Epoch: [0][190/500]	Time  8.727 ( 8.727)	Loss 1.2378 (1.1632)	CeLoss 0.2734 (0.4437)	SegCLSLoss 0.0173 (0.0228)	KLLoss 0.6758 (0.4412)	MaskLoss 1.1238 (0.9179)	MaskBCELoss 0.2631 (0.1663)	MaskDICELoss 0.8607 (0.7516)
Epoch: [0][191/500]	Time 10.333 (10.333)	Loss 1.2830 (1.1981)	CeLoss 0.2207 (0.2613)	SegCLSLoss 0.0300 (0.0244)	KLLoss 0.5352 (0.5289)	MaskLoss 1.2014 (1.0746)	MaskBCELoss 0.2980 (0.2582)	MaskDICELoss 0.9034 (0.8164)
Epoch: [0][192/500]	Time  7.913 ( 7.913)	Loss 1.2459 (1.1448)	CeLoss 0.2148 (0.6751)	SegCLSLoss 0.0405 (0.0156)	KLLoss 0.5234 (0.2703)	MaskLoss 1.1895 (0.5736)	MaskBCELoss 0.2772 (0.1168)	MaskDICELoss 0.9123 (0.4568)
Epoch: [0][193/500]	Time  8.043 ( 8.043)	Loss 1.0169 (1.1377)	CeLoss 0.2520 (0.7151)	SegCLSLoss 0.0300 (0.0117)	KLLoss 0.5898 (0.2965)	MaskLoss 1.1151 (0.5630)	MaskBCELoss 0.1278 (0.0901)	MaskDICELoss 0.9873 (0.4728)
Epoch: [0][194/500]	Time  8.845 ( 8.845)	Loss 1.0857 (1.1390)	CeLoss 0.2656 (0.4591)	SegCLSLoss 0.0193 (0.0187)	KLLoss 0.6367 (0.4867)	MaskLoss 1.0717 (0.8891)	MaskBCELoss 0.1830 (0.1506)	MaskDICELoss 0.8887 (0.7385)
Epoch: [0][195/500]	Time  9.088 ( 9.088)	Loss 1.3445 (1.1063)	CeLoss 0.2070 (0.3673)	SegCLSLoss 0.0172 (0.0134)	KLLoss 0.6641 (0.4375)	MaskLoss 1.2220 (0.8438)	MaskBCELoss 0.3451 (0.2070)	MaskDICELoss 0.8769 (0.6368)
Epoch: [0][196/500]	Time  9.288 ( 9.288)	Loss 1.0946 (1.2745)	CeLoss 0.3359 (0.3016)	SegCLSLoss 0.0253 (0.0226)	KLLoss 0.6016 (0.5188)	MaskLoss 1.1149 (1.1201)	MaskBCELoss 0.1264 (0.2678)	MaskDICELoss 0.9885 (0.8523)
Epoch: [0][197/500]	Time  7.899 ( 7.899)	Loss 1.6016 (1.0943)	CeLoss 1.6016 (0.6494)	SegCLSLoss 0.0000 (0.0088)	KLLoss 0.0000 (0.2352)	MaskLoss 0.0000 (0.5001)	MaskBCELoss 0.0000 (0.1271)	MaskDICELoss 0.0000 (0.3730)
Epoch: [0][198/500]	Time  8.851 ( 8.851)	Loss 1.0280 (1.2829)	CeLoss 0.2178 (0.3780)	SegCLSLoss 0.0281 (0.0257)	KLLoss 0.5430 (0.5236)	MaskLoss 1.0962 (1.0612)	MaskBCELoss 0.1656 (0.2409)	MaskDICELoss 0.9306 (0.8203)
Epoch: [0][199/500]	Time  9.844 ( 9.844)	Loss 1.3533 (1.0205)	CeLoss 0.2480 (0.4416)	SegCLSLoss 0.0211 (0.0141)	KLLoss 0.6602 (0.2797)	MaskLoss 1.2149 (0.6168)	MaskBCELoss 0.3254 (0.1758)	MaskDICELoss 0.8895 (0.4410)
[2025-03-01 22:53:09,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[0.00029393877551020406], mom=[(0.9, 0.95)]
[2025-03-01 22:53:09,453] [INFO] [timer.py:215:stop] epoch=0/micro_step=2000/global_step=200, RunningAvgSamplesPerSec=1.1403906842278382, CurrSamplesPerSec=1.192653660821656, MemAllocated=30.7GB, MaxMemAllocated=36.85GB
Epoch: [0][200/500]	Time  8.387 ( 8.387)	Loss 1.2734 (1.1737)	CeLoss 1.2734 (0.5103)	SegCLSLoss 0.0000 (0.0183)	KLLoss 0.0000 (0.4131)	MaskLoss 0.0000 (0.8111)	MaskBCELoss 0.0000 (0.1658)	MaskDICELoss 0.0000 (0.6453)
Epoch: [0][201/500]	Time  8.232 ( 8.232)	Loss 1.3228 (1.2216)	CeLoss 0.2480 (0.4967)	SegCLSLoss 0.0239 (0.0188)	KLLoss 0.6289 (0.4117)	MaskLoss 1.2151 (0.8285)	MaskBCELoss 0.3037 (0.2007)	MaskDICELoss 0.9114 (0.6278)
Epoch: [0][202/500]	Time  8.625 ( 8.625)	Loss 1.6668 (1.3064)	CeLoss 0.2656 (0.4576)	SegCLSLoss 0.0232 (0.0281)	KLLoss 0.6992 (0.4520)	MaskLoss 1.2510 (0.9452)	MaskBCELoss 0.5093 (0.2414)	MaskDICELoss 0.7417 (0.7038)
Epoch: [0][203/500]	Time  9.604 ( 9.604)	Loss 0.1543 (0.9878)	CeLoss 0.1543 (0.2376)	SegCLSLoss 0.0000 (0.0299)	KLLoss 0.0000 (0.4945)	MaskLoss 0.0000 (1.0018)	MaskBCELoss 0.0000 (0.1562)	MaskDICELoss 0.0000 (0.8455)
Epoch: [0][204/500]	Time  9.318 ( 9.318)	Loss 1.1902 (1.1713)	CeLoss 0.2422 (0.2960)	SegCLSLoss 0.0415 (0.0359)	KLLoss 0.4570 (0.4721)	MaskLoss 1.1616 (1.0430)	MaskBCELoss 0.2305 (0.2239)	MaskDICELoss 0.9311 (0.8191)
Epoch: [0][205/500]	Time  8.782 ( 8.782)	Loss 1.4135 (1.0411)	CeLoss 0.2539 (0.5710)	SegCLSLoss 0.0253 (0.0190)	KLLoss 0.6641 (0.3395)	MaskLoss 1.2234 (0.6549)	MaskBCELoss 0.3575 (0.0889)	MaskDICELoss 0.8659 (0.5660)
Epoch: [0][206/500]	Time  9.369 ( 9.369)	Loss 0.7109 (1.0332)	CeLoss 0.7109 (0.3526)	SegCLSLoss 0.0000 (0.0225)	KLLoss 0.0000 (0.3949)	MaskLoss 0.0000 (0.8163)	MaskBCELoss 0.0000 (0.1742)	MaskDICELoss 0.0000 (0.6421)
Epoch: [0][207/500]	Time  8.378 ( 8.378)	Loss 0.9655 (1.0598)	CeLoss 0.2422 (0.4870)	SegCLSLoss 0.0317 (0.0242)	KLLoss 0.5156 (0.3857)	MaskLoss 1.0783 (0.7760)	MaskBCELoss 0.1123 (0.1151)	MaskDICELoss 0.9660 (0.6608)
Epoch: [0][208/500]	Time  8.544 ( 8.544)	Loss 1.1484 (1.1133)	CeLoss 1.1484 (0.7343)	SegCLSLoss 0.0000 (0.0140)	KLLoss 0.0000 (0.2826)	MaskLoss 0.0000 (0.5505)	MaskBCELoss 0.0000 (0.0646)	MaskDICELoss 0.0000 (0.4859)
Epoch: [0][209/500]	Time  8.325 ( 8.325)	Loss 1.1953 (1.2866)	CeLoss 1.1953 (0.4092)	SegCLSLoss 0.0000 (0.0279)	KLLoss 0.0000 (0.4365)	MaskLoss 0.0000 (1.0022)	MaskBCELoss 0.0000 (0.2417)	MaskDICELoss 0.0000 (0.7606)
[2025-03-01 22:54:37,405] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=0, lr=[0.00029332653061224485], mom=[(0.9, 0.95)]
[2025-03-01 22:54:37,406] [INFO] [timer.py:215:stop] epoch=0/micro_step=2100/global_step=210, RunningAvgSamplesPerSec=1.140234647313684, CurrSamplesPerSec=1.1397260774257036, MemAllocated=31.57GB, MaxMemAllocated=36.85GB
Epoch: [0][210/500]	Time  8.776 ( 8.776)	Loss 0.8822 (1.1759)	CeLoss 0.2559 (0.4491)	SegCLSLoss 0.0444 (0.0214)	KLLoss 0.4805 (0.3934)	MaskLoss 1.0411 (0.8434)	MaskBCELoss 0.0562 (0.1962)	MaskDICELoss 0.9849 (0.6472)
Epoch: [0][211/500]	Time  8.755 ( 8.755)	Loss 1.1208 (1.2259)	CeLoss 0.2617 (0.5242)	SegCLSLoss 0.0216 (0.0212)	KLLoss 0.6562 (0.4100)	MaskLoss 1.0968 (0.8129)	MaskBCELoss 0.1993 (0.1897)	MaskDICELoss 0.8976 (0.6232)
Epoch: [0][212/500]	Time  8.620 ( 8.620)	Loss 0.9667 (1.1997)	CeLoss 0.2832 (0.4556)	SegCLSLoss 0.0262 (0.0353)	KLLoss 0.5391 (0.4570)	MaskLoss 1.0713 (0.9059)	MaskBCELoss 0.0894 (0.1824)	MaskDICELoss 0.9818 (0.7235)
Epoch: [0][213/500]	Time  9.612 ( 9.612)	Loss 1.1641 (1.1050)	CeLoss 1.1641 (0.3326)	SegCLSLoss 0.0000 (0.0227)	KLLoss 0.0000 (0.4512)	MaskLoss 0.0000 (0.9296)	MaskBCELoss 0.0000 (0.1975)	MaskDICELoss 0.0000 (0.7321)
Epoch: [0][214/500]	Time  9.761 ( 9.761)	Loss 1.4504 (1.1279)	CeLoss 0.2188 (0.3897)	SegCLSLoss 0.0157 (0.0314)	KLLoss 0.6836 (0.4369)	MaskLoss 1.1149 (0.8944)	MaskBCELoss 0.4442 (0.1835)	MaskDICELoss 0.6707 (0.7109)
Epoch: [0][215/500]	Time  8.416 ( 8.416)	Loss 1.2422 (1.2814)	CeLoss 1.2422 (0.4629)	SegCLSLoss 0.0000 (0.0270)	KLLoss 0.0000 (0.3787)	MaskLoss 0.0000 (0.8498)	MaskBCELoss 0.0000 (0.2533)	MaskDICELoss 0.0000 (0.5965)
Epoch: [0][216/500]	Time  8.823 ( 8.823)	Loss 1.1185 (1.3469)	CeLoss 0.2695 (0.4472)	SegCLSLoss 0.0232 (0.0233)	KLLoss 0.5703 (0.4096)	MaskLoss 1.1391 (0.9785)	MaskBCELoss 0.1785 (0.2659)	MaskDICELoss 0.9606 (0.7126)
Epoch: [0][217/500]	Time  8.079 ( 8.079)	Loss 1.0991 (0.9969)	CeLoss 0.1309 (0.5196)	SegCLSLoss 0.0825 (0.0212)	KLLoss 0.3965 (0.3109)	MaskLoss 1.1712 (0.6617)	MaskBCELoss 0.2277 (0.0906)	MaskDICELoss 0.9435 (0.5711)
Epoch: [0][218/500]	Time  9.420 ( 9.420)	Loss 0.7461 (1.2008)	CeLoss 0.7461 (0.4300)	SegCLSLoss 0.0000 (0.0209)	KLLoss 0.0000 (0.4285)	MaskLoss 0.0000 (0.9319)	MaskBCELoss 0.0000 (0.1964)	MaskDICELoss 0.0000 (0.7355)
Epoch: [0][219/500]	Time  9.956 ( 9.956)	Loss 1.0078 (1.0628)	CeLoss 1.0078 (0.3189)	SegCLSLoss 0.0000 (0.0269)	KLLoss 0.0000 (0.4756)	MaskLoss 0.0000 (1.0012)	MaskBCELoss 0.0000 (0.1531)	MaskDICELoss 0.0000 (0.8481)
[2025-03-01 22:56:07,014] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=0, lr=[0.0002927142857142857], mom=[(0.9, 0.95)]
[2025-03-01 22:56:07,014] [INFO] [timer.py:215:stop] epoch=0/micro_step=2200/global_step=220, RunningAvgSamplesPerSec=1.1391071822263905, CurrSamplesPerSec=1.224977579482476, MemAllocated=31.25GB, MaxMemAllocated=36.85GB
Epoch: [0][220/500]	Time  8.165 ( 8.165)	Loss 1.0878 (1.0524)	CeLoss 0.2197 (0.4963)	SegCLSLoss 0.0356 (0.0189)	KLLoss 0.5234 (0.3150)	MaskLoss 1.1059 (0.6945)	MaskBCELoss 0.1983 (0.1331)	MaskDICELoss 0.9075 (0.5613)
Epoch: [0][221/500]	Time  8.350 ( 8.350)	Loss 0.9005 (1.0292)	CeLoss 0.2178 (0.6346)	SegCLSLoss 0.0294 (0.0163)	KLLoss 0.5430 (0.2570)	MaskLoss 1.0737 (0.5433)	MaskBCELoss 0.0875 (0.0765)	MaskDICELoss 0.9862 (0.4668)
Epoch: [0][222/500]	Time  7.213 ( 7.213)	Loss 1.2734 (1.2504)	CeLoss 1.2734 (0.5302)	SegCLSLoss 0.0000 (0.0221)	KLLoss 0.0000 (0.3154)	MaskLoss 0.0000 (0.7542)	MaskBCELoss 0.0000 (0.2212)	MaskDICELoss 0.0000 (0.5329)
Epoch: [0][223/500]	Time  8.868 ( 8.868)	Loss 0.4570 (1.0351)	CeLoss 0.4570 (0.3964)	SegCLSLoss 0.0000 (0.0241)	KLLoss 0.0000 (0.3803)	MaskLoss 0.0000 (0.8030)	MaskBCELoss 0.0000 (0.1502)	MaskDICELoss 0.0000 (0.6528)
Epoch: [0][224/500]	Time  8.642 ( 8.642)	Loss 0.8111 (1.2112)	CeLoss 0.2461 (0.5367)	SegCLSLoss 0.0311 (0.0251)	KLLoss 0.5820 (0.4434)	MaskLoss 1.0192 (0.8797)	MaskBCELoss 0.0265 (0.1481)	MaskDICELoss 0.9927 (0.7316)
Epoch: [0][225/500]	Time  8.676 ( 8.676)	Loss 1.2680 (1.1615)	CeLoss 0.1738 (0.3409)	SegCLSLoss 0.0381 (0.0357)	KLLoss 0.4785 (0.4580)	MaskLoss 1.2245 (1.0229)	MaskBCELoss 0.3083 (0.1942)	MaskDICELoss 0.9163 (0.8288)
Epoch: [0][226/500]	Time  8.186 ( 8.186)	Loss 1.3663 (1.1512)	CeLoss 0.2832 (0.5007)	SegCLSLoss 0.0215 (0.0219)	KLLoss 0.6758 (0.3750)	MaskLoss 1.1252 (0.7618)	MaskBCELoss 0.3405 (0.1726)	MaskDICELoss 0.7848 (0.5892)
Epoch: [0][227/500]	Time  9.283 ( 9.283)	Loss 0.8711 (1.1601)	CeLoss 0.8711 (0.3082)	SegCLSLoss 0.0000 (0.0252)	KLLoss 0.0000 (0.5244)	MaskLoss 0.0000 (1.0089)	MaskBCELoss 0.0000 (0.2232)	MaskDICELoss 0.0000 (0.7856)
Epoch: [0][228/500]	Time  7.795 ( 7.795)	Loss 1.1797 (1.0241)	CeLoss 1.1797 (0.5879)	SegCLSLoss 0.0000 (0.0174)	KLLoss 0.0000 (0.2588)	MaskLoss 0.0000 (0.5616)	MaskBCELoss 0.0000 (0.0977)	MaskDICELoss 0.0000 (0.4639)
Epoch: [0][229/500]	Time  8.697 ( 8.697)	Loss 1.1826 (1.2438)	CeLoss 0.1260 (0.4765)	SegCLSLoss 0.0654 (0.0249)	KLLoss 0.4023 (0.3504)	MaskLoss 1.2124 (0.8571)	MaskBCELoss 0.2788 (0.2177)	MaskDICELoss 0.9335 (0.6393)
[2025-03-01 22:57:30,348] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=0, lr=[0.00029210204081632653], mom=[(0.9, 0.95)]
[2025-03-01 22:57:30,349] [INFO] [timer.py:215:stop] epoch=0/micro_step=2300/global_step=230, RunningAvgSamplesPerSec=1.1416553893452186, CurrSamplesPerSec=1.3118645198019947, MemAllocated=30.72GB, MaxMemAllocated=36.85GB
Epoch: [0][230/500]	Time  7.624 ( 7.624)	Loss 1.0859 (1.1097)	CeLoss 1.0859 (0.6351)	SegCLSLoss 0.0000 (0.0154)	KLLoss 0.0000 (0.3305)	MaskLoss 0.0000 (0.6508)	MaskBCELoss 0.0000 (0.0944)	MaskDICELoss 0.0000 (0.5565)
Epoch: [0][231/500]	Time  8.351 ( 8.351)	Loss 1.6250 (0.9992)	CeLoss 1.6250 (0.5002)	SegCLSLoss 0.0000 (0.0203)	KLLoss 0.0000 (0.3775)	MaskLoss 0.0000 (0.7550)	MaskBCELoss 0.0000 (0.0741)	MaskDICELoss 0.0000 (0.6809)
Epoch: [0][232/500]	Time  7.340 ( 7.340)	Loss 1.0391 (1.2561)	CeLoss 1.0391 (0.6714)	SegCLSLoss 0.0000 (0.0145)	KLLoss 0.0000 (0.2854)	MaskLoss 0.0000 (0.5801)	MaskBCELoss 0.0000 (0.1916)	MaskDICELoss 0.0000 (0.3885)
Epoch: [0][233/500]	Time  8.842 ( 8.842)	Loss 1.1060 (1.2954)	CeLoss 0.2539 (0.3917)	SegCLSLoss 0.0162 (0.0182)	KLLoss 0.6484 (0.5695)	MaskLoss 1.0454 (1.0406)	MaskBCELoss 0.2143 (0.2494)	MaskDICELoss 0.8311 (0.7912)
Epoch: [0][234/500]	Time  8.494 ( 8.494)	Loss 0.8714 (1.1703)	CeLoss 0.2637 (0.4899)	SegCLSLoss 0.0225 (0.0137)	KLLoss 0.5781 (0.4344)	MaskLoss 1.0156 (0.7865)	MaskBCELoss 0.0588 (0.1869)	MaskDICELoss 0.9568 (0.5996)
Epoch: [0][235/500]	Time  8.528 ( 8.528)	Loss 1.3515 (1.0902)	CeLoss 0.2432 (0.3880)	SegCLSLoss 0.0148 (0.0210)	KLLoss 0.7109 (0.4008)	MaskLoss 1.2242 (0.8225)	MaskBCELoss 0.3263 (0.1870)	MaskDICELoss 0.8979 (0.6355)
Epoch: [0][236/500]	Time  8.829 ( 8.829)	Loss 1.4053 (0.9555)	CeLoss 0.3555 (0.4824)	SegCLSLoss 0.0175 (0.0134)	KLLoss 0.6758 (0.3563)	MaskLoss 1.1537 (0.6329)	MaskBCELoss 0.3101 (0.1001)	MaskDICELoss 0.8436 (0.5329)
Epoch: [0][237/500]	Time  9.767 ( 9.767)	Loss 1.0272 (1.0530)	CeLoss 0.2480 (0.4473)	SegCLSLoss 0.0151 (0.0142)	KLLoss 0.7383 (0.4383)	MaskLoss 1.1021 (0.7624)	MaskBCELoss 0.1469 (0.1450)	MaskDICELoss 0.9552 (0.6173)
Epoch: [0][238/500]	Time  7.987 ( 7.987)	Loss 1.4859 (1.1785)	CeLoss 0.1963 (0.5981)	SegCLSLoss 0.0297 (0.0126)	KLLoss 0.5391 (0.3828)	MaskLoss 1.2938 (0.6737)	MaskBCELoss 0.4187 (0.1583)	MaskDICELoss 0.8751 (0.5154)
Epoch: [0][239/500]	Time  8.671 ( 8.671)	Loss 0.7305 (0.9825)	CeLoss 0.7305 (0.4203)	SegCLSLoss 0.0000 (0.0158)	KLLoss 0.0000 (0.4320)	MaskLoss 0.0000 (0.7649)	MaskBCELoss 0.0000 (0.1145)	MaskDICELoss 0.0000 (0.6505)
[2025-03-01 22:58:56,462] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=0, lr=[0.0002914897959183673], mom=[(0.9, 0.95)]
[2025-03-01 22:58:56,463] [INFO] [timer.py:215:stop] epoch=0/micro_step=2400/global_step=240, RunningAvgSamplesPerSec=1.1424726430934329, CurrSamplesPerSec=1.0747565592745627, MemAllocated=31.26GB, MaxMemAllocated=36.85GB
Epoch: [0][240/500]	Time  9.306 ( 9.306)	Loss 1.2716 (1.0336)	CeLoss 0.2695 (0.2324)	SegCLSLoss 0.0289 (0.0289)	KLLoss 0.5391 (0.4588)	MaskLoss 1.1971 (0.9198)	MaskBCELoss 0.2599 (0.2178)	MaskDICELoss 0.9372 (0.7020)
Epoch: [0][241/500]	Time  8.699 ( 8.699)	Loss 0.7464 (1.1613)	CeLoss 0.1807 (0.4573)	SegCLSLoss 0.0175 (0.0161)	KLLoss 0.6562 (0.4965)	MaskLoss 1.0234 (0.8779)	MaskBCELoss 0.0302 (0.1713)	MaskDICELoss 0.9933 (0.7067)
Epoch: [0][242/500]	Time  8.130 ( 8.130)	Loss 0.0869 (1.0032)	CeLoss 0.0869 (0.5810)	SegCLSLoss 0.0000 (0.0119)	KLLoss 0.0000 (0.2967)	MaskLoss 0.0000 (0.5498)	MaskBCELoss 0.0000 (0.0943)	MaskDICELoss 0.0000 (0.4555)
Epoch: [0][243/500]	Time  8.376 ( 8.376)	Loss 3.0325 (1.2693)	CeLoss 0.2910 (0.3223)	SegCLSLoss 0.0249 (0.0188)	KLLoss 0.5234 (0.4656)	MaskLoss 1.8323 (0.9860)	MaskBCELoss 1.2091 (0.2965)	MaskDICELoss 0.6231 (0.6896)
Epoch: [0][244/500]	Time  9.323 ( 9.323)	Loss 1.4862 (1.1250)	CeLoss 0.2354 (0.5166)	SegCLSLoss 0.0170 (0.0165)	KLLoss 0.6953 (0.4256)	MaskLoss 1.2508 (0.7700)	MaskBCELoss 0.4111 (0.1433)	MaskDICELoss 0.8396 (0.6267)
Epoch: [0][245/500]	Time  8.819 ( 8.819)	Loss 1.3412 (1.1824)	CeLoss 0.1943 (0.3931)	SegCLSLoss 0.0256 (0.0191)	KLLoss 0.5117 (0.3740)	MaskLoss 1.2244 (0.8552)	MaskBCELoss 0.3480 (0.2347)	MaskDICELoss 0.8764 (0.6205)
Epoch: [0][246/500]	Time  9.018 ( 9.018)	Loss 0.7227 (1.0327)	CeLoss 0.7227 (0.4807)	SegCLSLoss 0.0000 (0.0163)	KLLoss 0.0000 (0.3096)	MaskLoss 0.0000 (0.6940)	MaskBCELoss 0.0000 (0.1312)	MaskDICELoss 0.0000 (0.5627)
Epoch: [0][247/500]	Time  8.186 ( 8.186)	Loss 1.0391 (1.1891)	CeLoss 1.0391 (0.7892)	SegCLSLoss 0.0000 (0.0084)	KLLoss 0.0000 (0.2426)	MaskLoss 0.0000 (0.4640)	MaskBCELoss 0.0000 (0.1092)	MaskDICELoss 0.0000 (0.3548)
Epoch: [0][248/500]	Time 10.182 (10.182)	Loss 1.3578 (1.1261)	CeLoss 0.3652 (0.2665)	SegCLSLoss 0.0153 (0.0243)	KLLoss 0.7109 (0.5699)	MaskLoss 1.1549 (1.1115)	MaskBCELoss 0.2715 (0.1944)	MaskDICELoss 0.8834 (0.9171)
Epoch: [0][249/500]	Time  8.740 ( 8.740)	Loss 0.7699 (1.0744)	CeLoss 0.2188 (0.2684)	SegCLSLoss 0.0206 (0.0267)	KLLoss 0.6289 (0.5100)	MaskLoss 1.0152 (1.0196)	MaskBCELoss 0.0219 (0.1886)	MaskDICELoss 0.9933 (0.8309)
[2025-03-01 23:00:25,578] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=0, lr=[0.0002908775510204081], mom=[(0.9, 0.95)]
[2025-03-01 23:00:25,578] [INFO] [timer.py:215:stop] epoch=0/micro_step=2500/global_step=250, RunningAvgSamplesPerSec=1.1416462584031402, CurrSamplesPerSec=1.037180036211132, MemAllocated=31.23GB, MaxMemAllocated=36.85GB
Epoch: [0][250/500]	Time  9.643 ( 9.643)	Loss 1.0439 (1.4125)	CeLoss 0.3086 (0.2447)	SegCLSLoss 0.0156 (0.0281)	KLLoss 0.6797 (0.5998)	MaskLoss 1.0717 (1.2488)	MaskBCELoss 0.1278 (0.3528)	MaskDICELoss 0.9439 (0.8959)
Epoch: [0][251/500]	Time  9.411 ( 9.411)	Loss 0.7812 (0.8080)	CeLoss 0.7812 (0.3187)	SegCLSLoss 0.0000 (0.0138)	KLLoss 0.0000 (0.3582)	MaskLoss 0.0000 (0.6650)	MaskBCELoss 0.0000 (0.1001)	MaskDICELoss 0.0000 (0.5649)
Epoch: [0][252/500]	Time  8.509 ( 8.509)	Loss 0.7727 (1.0174)	CeLoss 0.2578 (0.4271)	SegCLSLoss 0.0282 (0.0174)	KLLoss 0.5391 (0.4094)	MaskLoss 0.9762 (0.7639)	MaskBCELoss 0.0087 (0.1332)	MaskDICELoss 0.9675 (0.6307)
Epoch: [0][253/500]	Time  9.688 ( 9.688)	Loss 1.3541 (1.3112)	CeLoss 0.2637 (0.2505)	SegCLSLoss 0.0137 (0.0225)	KLLoss 0.7578 (0.6234)	MaskLoss 1.0955 (1.1706)	MaskBCELoss 0.3579 (0.3093)	MaskDICELoss 0.7376 (0.8612)
Epoch: [0][254/500]	Time  8.929 ( 8.929)	Loss 1.2234 (1.0499)	CeLoss 0.3457 (0.3908)	SegCLSLoss 0.0211 (0.0243)	KLLoss 0.6719 (0.4654)	MaskLoss 0.9879 (0.8551)	MaskBCELoss 0.2493 (0.1463)	MaskDICELoss 0.7386 (0.7088)
Epoch: [0][255/500]	Time  9.055 ( 9.055)	Loss 2.7519 (1.4091)	CeLoss 0.2617 (0.4511)	SegCLSLoss 0.0159 (0.0177)	KLLoss 0.7070 (0.4951)	MaskLoss 1.8682 (1.0130)	MaskBCELoss 1.0321 (0.2950)	MaskDICELoss 0.8361 (0.7180)
Epoch: [0][256/500]	Time  8.750 ( 8.750)	Loss 0.8438 (0.9947)	CeLoss 0.3008 (0.3776)	SegCLSLoss 0.0209 (0.0166)	KLLoss 0.5703 (0.4262)	MaskLoss 1.0137 (0.7703)	MaskBCELoss 0.0176 (0.1490)	MaskDICELoss 0.9961 (0.6214)
Epoch: [0][257/500]	Time  9.225 ( 9.225)	Loss 1.3975 (1.1312)	CeLoss 0.2676 (0.3616)	SegCLSLoss 0.0172 (0.0199)	KLLoss 0.7109 (0.5033)	MaskLoss 1.1779 (0.9115)	MaskBCELoss 0.3554 (0.2025)	MaskDICELoss 0.8224 (0.7090)
Epoch: [0][258/500]	Time  8.145 ( 8.145)	Loss 1.2656 (1.1243)	CeLoss 1.2656 (0.4548)	SegCLSLoss 0.0000 (0.0195)	KLLoss 0.0000 (0.4287)	MaskLoss 0.0000 (0.7986)	MaskBCELoss 0.0000 (0.1736)	MaskDICELoss 0.0000 (0.6250)
Epoch: [0][259/500]	Time 59.072 (59.072)	Loss 0.9942 (1.2863)	CeLoss 0.2324 (0.4998)	SegCLSLoss 0.0327 (0.0246)	KLLoss 0.5039 (0.3889)	MaskLoss 1.0817 (0.8548)	MaskBCELoss 0.1362 (0.2312)	MaskDICELoss 0.9454 (0.6236)
[2025-03-01 23:03:44,747] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=0, lr=[0.00029026530612244895], mom=[(0.9, 0.95)]
[2025-03-01 23:03:44,748] [INFO] [timer.py:215:stop] epoch=0/micro_step=2600/global_step=260, RunningAvgSamplesPerSec=1.0879388484094663, CurrSamplesPerSec=0.14623214360467424, MemAllocated=31.64GB, MaxMemAllocated=36.85GB
Epoch: [0][260/500]	Time 68.387 (68.387)	Loss 1.1894 (1.2335)	CeLoss 0.2354 (0.5550)	SegCLSLoss 0.0242 (0.0184)	KLLoss 0.5820 (0.4250)	MaskLoss 1.1071 (0.8051)	MaskBCELoss 0.2592 (0.1779)	MaskDICELoss 0.8480 (0.6271)
Epoch: [0][261/500]	Time 62.205 (62.205)	Loss 1.8061 (1.3632)	CeLoss 0.2598 (0.4929)	SegCLSLoss 0.0220 (0.0228)	KLLoss 0.6719 (0.4580)	MaskLoss 1.2647 (0.9699)	MaskBCELoss 0.6015 (0.2493)	MaskDICELoss 0.6631 (0.7205)
Epoch: [0][262/500]	Time 69.322 (69.322)	Loss 1.6034 (1.1902)	CeLoss 0.2168 (0.4039)	SegCLSLoss 0.0247 (0.0239)	KLLoss 0.5586 (0.4391)	MaskLoss 1.2850 (0.9250)	MaskBCELoss 0.4876 (0.2078)	MaskDICELoss 0.7973 (0.7173)
Epoch: [0][263/500]	Time 61.101 (61.101)	Loss 1.1717 (1.3015)	CeLoss 0.2295 (0.5594)	SegCLSLoss 0.0262 (0.0212)	KLLoss 0.5117 (0.2955)	MaskLoss 1.1368 (0.7570)	MaskBCELoss 0.2407 (0.2354)	MaskDICELoss 0.8961 (0.5216)
Epoch: [0][264/500]	Time 65.351 (65.351)	Loss 0.8516 (1.0847)	CeLoss 0.8516 (0.4690)	SegCLSLoss 0.0000 (0.0168)	KLLoss 0.0000 (0.3887)	MaskLoss 0.0000 (0.7721)	MaskBCELoss 0.0000 (0.1474)	MaskDICELoss 0.0000 (0.6247)
Epoch: [0][265/500]	Time 52.723 (52.723)	Loss 1.1680 (0.8839)	CeLoss 0.1230 (0.5303)	SegCLSLoss 0.0588 (0.0106)	KLLoss 0.4258 (0.1527)	MaskLoss 1.1946 (0.3515)	MaskBCELoss 0.2789 (0.1151)	MaskDICELoss 0.9157 (0.2363)
Epoch: [0][266/500]	Time 62.258 (62.258)	Loss 1.3256 (1.1230)	CeLoss 0.3398 (0.4679)	SegCLSLoss 0.0212 (0.0207)	KLLoss 0.5664 (0.3490)	MaskLoss 1.0882 (0.7938)	MaskBCELoss 0.2879 (0.1653)	MaskDICELoss 0.8003 (0.6286)
Epoch: [0][267/500]	Time 59.923 (59.923)	Loss 1.3047 (1.0904)	CeLoss 1.3047 (0.4967)	SegCLSLoss 0.0000 (0.0132)	KLLoss 0.0000 (0.3463)	MaskLoss 0.0000 (0.6991)	MaskBCELoss 0.0000 (0.1584)	MaskDICELoss 0.0000 (0.5406)
Epoch: [0][268/500]	Time 61.957 (61.957)	Loss 0.8242 (1.1785)	CeLoss 0.8242 (0.6118)	SegCLSLoss 0.0000 (0.0186)	KLLoss 0.0000 (0.3375)	MaskLoss 0.0000 (0.6854)	MaskBCELoss 0.0000 (0.1432)	MaskDICELoss 0.0000 (0.5422)
Epoch: [0][269/500]	Time 69.185 (69.185)	Loss 1.4009 (0.9752)	CeLoss 0.3770 (0.2021)	SegCLSLoss 0.0220 (0.0311)	KLLoss 0.5547 (0.4783)	MaskLoss 1.1052 (0.9940)	MaskBCELoss 0.3064 (0.1736)	MaskDICELoss 0.7987 (0.8204)
[2025-03-01 23:14:24,391] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=0, lr=[0.0002896530612244898], mom=[(0.9, 0.95)]
[2025-03-01 23:14:24,392] [INFO] [timer.py:215:stop] epoch=0/micro_step=2700/global_step=270, RunningAvgSamplesPerSec=0.890044027250911, CurrSamplesPerSec=0.13224315446768267, MemAllocated=31.23GB, MaxMemAllocated=36.85GB
Epoch: [0][270/500]	Time 75.620 (75.620)	Loss 1.4631 (1.2297)	CeLoss 0.6055 (0.4020)	SegCLSLoss 0.0199 (0.0218)	KLLoss 0.5781 (0.5406)	MaskLoss 1.1227 (1.0403)	MaskBCELoss 0.1897 (0.1978)	MaskDICELoss 0.9329 (0.8425)
Epoch: [0][271/500]	Time 65.913 (65.913)	Loss 1.2742 (1.1981)	CeLoss 0.2969 (0.3757)	SegCLSLoss 0.0284 (0.0153)	KLLoss 0.5195 (0.4344)	MaskLoss 1.1671 (0.8809)	MaskBCELoss 0.2534 (0.2495)	MaskDICELoss 0.9137 (0.6313)
Epoch: [0][272/500]	Time 69.443 (69.443)	Loss 0.9428 (1.0710)	CeLoss 0.2500 (0.2244)	SegCLSLoss 0.0186 (0.0219)	KLLoss 0.6758 (0.5355)	MaskLoss 1.0826 (1.0387)	MaskBCELoss 0.0945 (0.2110)	MaskDICELoss 0.9881 (0.8277)
Epoch: [0][273/500]	Time 71.923 (71.923)	Loss 1.1436 (1.1380)	CeLoss 0.1689 (0.4186)	SegCLSLoss 0.0388 (0.0170)	KLLoss 0.5078 (0.5066)	MaskLoss 1.2045 (0.9059)	MaskBCELoss 0.2352 (0.1720)	MaskDICELoss 0.9693 (0.7339)
Epoch: [0][274/500]	Time 67.084 (67.084)	Loss 2.0671 (1.1433)	CeLoss 0.2383 (0.4714)	SegCLSLoss 0.0277 (0.0150)	KLLoss 0.5156 (0.4223)	MaskLoss 1.5201 (0.8091)	MaskBCELoss 0.7034 (0.1732)	MaskDICELoss 0.8166 (0.6358)
Epoch: [0][275/500]	Time 68.697 (68.697)	Loss 1.0945 (1.1422)	CeLoss 0.2559 (0.3141)	SegCLSLoss 0.0187 (0.0200)	KLLoss 0.6562 (0.5641)	MaskLoss 1.0676 (1.0061)	MaskBCELoss 0.1967 (0.2099)	MaskDICELoss 0.8709 (0.7962)
Epoch: [0][276/500]	Time 73.107 (73.107)	Loss 0.7891 (1.1545)	CeLoss 0.2217 (0.3571)	SegCLSLoss 0.0276 (0.0197)	KLLoss 0.5312 (0.5402)	MaskLoss 0.9663 (1.0070)	MaskBCELoss 0.0471 (0.1892)	MaskDICELoss 0.9192 (0.8178)
Epoch: [0][277/500]	Time 68.257 (68.257)	Loss 0.7227 (1.0406)	CeLoss 0.2188 (0.3654)	SegCLSLoss 0.0214 (0.0206)	KLLoss 0.5352 (0.4465)	MaskLoss 0.9306 (0.8817)	MaskBCELoss 0.0186 (0.1495)	MaskDICELoss 0.9121 (0.7322)
Epoch: [0][278/500]	Time 75.285 (75.285)	Loss 0.7822 (1.0448)	CeLoss 0.2227 (0.2290)	SegCLSLoss 0.0181 (0.0262)	KLLoss 0.6484 (0.6004)	MaskLoss 1.0132 (1.0901)	MaskBCELoss 0.0294 (0.1719)	MaskDICELoss 0.9837 (0.9182)
Epoch: [0][279/500]	Time 74.325 (74.325)	Loss 1.3739 (1.1270)	CeLoss 0.2275 (0.2102)	SegCLSLoss 0.0211 (0.0298)	KLLoss 0.6641 (0.5723)	MaskLoss 1.2595 (1.1458)	MaskBCELoss 0.3372 (0.2193)	MaskDICELoss 0.9223 (0.9265)
[2025-03-01 23:26:10,577] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=0, lr=[0.0002890408163265306], mom=[(0.9, 0.95)]
[2025-03-01 23:26:10,578] [INFO] [timer.py:215:stop] epoch=0/micro_step=2800/global_step=280, RunningAvgSamplesPerSec=0.7478631750169396, CurrSamplesPerSec=0.13859974178829615, MemAllocated=31.25GB, MaxMemAllocated=36.85GB
Epoch: [0][280/500]	Time 72.152 (72.152)	Loss 1.3738 (1.1780)	CeLoss 0.1943 (0.3939)	SegCLSLoss 0.0237 (0.0228)	KLLoss 0.5547 (0.4619)	MaskLoss 1.2349 (0.9233)	MaskBCELoss 0.3669 (0.2073)	MaskDICELoss 0.8681 (0.7159)
Epoch: [0][281/500]	Time 61.180 (61.180)	Loss 1.3792 (1.6671)	CeLoss 0.1943 (0.5346)	SegCLSLoss 0.0245 (0.0169)	KLLoss 0.5781 (0.4930)	MaskLoss 1.2720 (1.0809)	MaskBCELoss 0.3574 (0.3891)	MaskDICELoss 0.9146 (0.6918)
Epoch: [0][282/500]	Time 63.307 (63.307)	Loss 1.4219 (1.1172)	CeLoss 1.4219 (0.4897)	SegCLSLoss 0.0000 (0.0199)	KLLoss 0.0000 (0.4107)	MaskLoss 0.0000 (0.7732)	MaskBCELoss 0.0000 (0.1539)	MaskDICELoss 0.0000 (0.6193)
Epoch: [0][283/500]	Time 69.113 (69.113)	Loss 0.7148 (1.0005)	CeLoss 0.7148 (0.2914)	SegCLSLoss 0.0000 (0.0179)	KLLoss 0.0000 (0.4994)	MaskLoss 0.0000 (0.8911)	MaskBCELoss 0.0000 (0.1698)	MaskDICELoss 0.0000 (0.7213)
Epoch: [0][284/500]	Time 58.126 (58.126)	Loss 1.8104 (1.2875)	CeLoss 0.2295 (0.5539)	SegCLSLoss 0.0125 (0.0135)	KLLoss 0.7812 (0.3812)	MaskLoss 1.3753 (0.7171)	MaskBCELoss 0.5916 (0.2456)	MaskDICELoss 0.7836 (0.4715)
Epoch: [0][285/500]	Time 81.141 (81.141)	Loss 1.2913 (1.1937)	CeLoss 0.3086 (0.2352)	SegCLSLoss 0.0144 (0.0233)	KLLoss 0.6953 (0.5984)	MaskLoss 1.0397 (1.1259)	MaskBCELoss 0.3034 (0.2559)	MaskDICELoss 0.7363 (0.8700)
Epoch: [0][286/500]	Time 69.451 (69.451)	Loss 1.5971 (1.2342)	CeLoss 0.2969 (0.5674)	SegCLSLoss 0.0154 (0.0138)	KLLoss 0.6680 (0.4191)	MaskLoss 1.2225 (0.8030)	MaskBCELoss 0.4541 (0.1723)	MaskDICELoss 0.7683 (0.6307)
Epoch: [0][287/500]	Time 70.683 (70.683)	Loss 0.7237 (1.1424)	CeLoss 0.2148 (0.5158)	SegCLSLoss 0.0236 (0.0131)	KLLoss 0.5664 (0.4520)	MaskLoss 0.9861 (0.7853)	MaskBCELoss 0.0027 (0.1516)	MaskDICELoss 0.9834 (0.6338)
Epoch: [0][288/500]	Time 66.639 (66.639)	Loss 1.3528 (1.2254)	CeLoss 0.2266 (0.2986)	SegCLSLoss 0.0294 (0.0194)	KLLoss 0.5273 (0.5504)	MaskLoss 1.1825 (1.0552)	MaskBCELoss 0.3469 (0.2596)	MaskDICELoss 0.8357 (0.7957)
Epoch: [0][289/500]	Time 71.682 (71.682)	Loss 1.5411 (1.2688)	CeLoss 0.3027 (0.4249)	SegCLSLoss 0.0150 (0.0193)	KLLoss 0.6797 (0.4736)	MaskLoss 1.2826 (0.9137)	MaskBCELoss 0.3928 (0.2516)	MaskDICELoss 0.8897 (0.6621)
[2025-03-01 23:37:28,614] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=0, lr=[0.0002884285714285714], mom=[(0.9, 0.95)]
[2025-03-01 23:37:28,615] [INFO] [timer.py:215:stop] epoch=0/micro_step=2900/global_step=290, RunningAvgSamplesPerSec=0.6552486960977625, CurrSamplesPerSec=0.14989702706264357, MemAllocated=31.28GB, MaxMemAllocated=36.85GB
Epoch: [0][290/500]	Time 66.715 (66.715)	Loss 0.8287 (0.9709)	CeLoss 0.2539 (0.5100)	SegCLSLoss 0.0186 (0.0128)	KLLoss 0.5898 (0.2771)	MaskLoss 0.9977 (0.5696)	MaskBCELoss 0.0441 (0.1132)	MaskDICELoss 0.9537 (0.4563)
Epoch: [0][291/500]	Time 63.208 (63.208)	Loss 1.2780 (1.0903)	CeLoss 0.1572 (0.4262)	SegCLSLoss 0.0325 (0.0155)	KLLoss 0.5000 (0.4199)	MaskLoss 1.2544 (0.7883)	MaskBCELoss 0.3180 (0.1747)	MaskDICELoss 0.9365 (0.6136)
Epoch: [0][292/500]	Time 76.971 (76.971)	Loss 1.5182 (1.0918)	CeLoss 0.3633 (0.2591)	SegCLSLoss 0.0183 (0.0243)	KLLoss 0.6094 (0.5803)	MaskLoss 1.2417 (1.0918)	MaskBCELoss 0.3496 (0.1831)	MaskDICELoss 0.8921 (0.9087)
Epoch: [0][293/500]	Time 65.485 (65.485)	Loss 0.8945 (0.9455)	CeLoss 0.8945 (0.3975)	SegCLSLoss 0.0000 (0.0147)	KLLoss 0.0000 (0.3443)	MaskLoss 0.0000 (0.6839)	MaskBCELoss 0.0000 (0.1325)	MaskDICELoss 0.0000 (0.5514)
Epoch: [0][294/500]	Time 64.606 (64.606)	Loss 1.0081 (1.0406)	CeLoss 0.2480 (0.5572)	SegCLSLoss 0.0261 (0.0200)	KLLoss 0.5938 (0.3650)	MaskLoss 1.0961 (0.7243)	MaskBCELoss 0.1322 (0.0741)	MaskDICELoss 0.9638 (0.6502)
Epoch: [0][295/500]	Time 67.577 (67.577)	Loss 1.3438 (1.1612)	CeLoss 1.3438 (0.5722)	SegCLSLoss 0.0000 (0.0171)	KLLoss 0.0000 (0.3189)	MaskLoss 0.0000 (0.6927)	MaskBCELoss 0.0000 (0.1561)	MaskDICELoss 0.0000 (0.5366)
Epoch: [0][296/500]	Time 69.960 (69.960)	Loss 0.7385 (1.0650)	CeLoss 0.1875 (0.4432)	SegCLSLoss 0.0315 (0.0172)	KLLoss 0.5508 (0.4855)	MaskLoss 1.0035 (0.8622)	MaskBCELoss 0.0224 (0.1215)	MaskDICELoss 0.9811 (0.7408)
Epoch: [0][297/500]	Time 65.648 (65.648)	Loss 0.9922 (0.9142)	CeLoss 0.9922 (0.3912)	SegCLSLoss 0.0000 (0.0178)	KLLoss 0.0000 (0.3738)	MaskLoss 0.0000 (0.7554)	MaskBCELoss 0.0000 (0.0909)	MaskDICELoss 0.0000 (0.6645)
Epoch: [0][298/500]	Time 71.781 (71.781)	Loss 1.3861 (1.0982)	CeLoss 0.2695 (0.3760)	SegCLSLoss 0.0308 (0.0225)	KLLoss 0.5117 (0.4475)	MaskLoss 1.2196 (0.8954)	MaskBCELoss 0.3274 (0.1755)	MaskDICELoss 0.8921 (0.7199)
Epoch: [0][299/500]	Time 71.191 (71.191)	Loss 1.0022 (1.0030)	CeLoss 0.2129 (0.2427)	SegCLSLoss 0.0165 (0.0234)	KLLoss 0.6484 (0.5656)	MaskLoss 1.0861 (1.0690)	MaskBCELoss 0.1590 (0.1428)	MaskDICELoss 0.9271 (0.9262)
[2025-03-01 23:48:49,342] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=0, lr=[0.0002878163265306122], mom=[(0.9, 0.95)]
[2025-03-01 23:48:49,343] [INFO] [timer.py:215:stop] epoch=0/micro_step=3000/global_step=300, RunningAvgSamplesPerSec=0.5870776019666245, CurrSamplesPerSec=0.15552255936130766, MemAllocated=31.33GB, MaxMemAllocated=36.85GB
Epoch: [0][300/500]	Time 64.301 (64.301)	Loss 0.1055 (0.9028)	CeLoss 0.1055 (0.3992)	SegCLSLoss 0.0000 (0.0155)	KLLoss 0.0000 (0.4168)	MaskLoss 0.0000 (0.7166)	MaskBCELoss 0.0000 (0.0918)	MaskDICELoss 0.0000 (0.6248)
Epoch: [0][301/500]	Time 67.057 (67.057)	Loss 1.2638 (1.1668)	CeLoss 0.2383 (0.4780)	SegCLSLoss 0.0232 (0.0166)	KLLoss 0.6367 (0.4305)	MaskLoss 1.0226 (0.7933)	MaskBCELoss 0.3350 (0.1892)	MaskDICELoss 0.6876 (0.6041)
Epoch: [0][302/500]	Time 63.395 (63.395)	Loss 1.8516 (1.0749)	CeLoss 1.8516 (0.4340)	SegCLSLoss 0.0000 (0.0149)	KLLoss 0.0000 (0.5059)	MaskLoss 0.0000 (0.8669)	MaskBCELoss 0.0000 (0.1334)	MaskDICELoss 0.0000 (0.7335)
Epoch: [0][303/500]	Time 65.546 (65.546)	Loss 0.9338 (0.9957)	CeLoss 0.2559 (0.2756)	SegCLSLoss 0.0171 (0.0187)	KLLoss 0.6602 (0.5334)	MaskLoss 0.9070 (0.9514)	MaskBCELoss 0.1444 (0.1568)	MaskDICELoss 0.7626 (0.7946)
Epoch: [0][304/500]	Time 60.032 (60.032)	Loss 0.8748 (1.0982)	CeLoss 0.2070 (0.6366)	SegCLSLoss 0.0199 (0.0093)	KLLoss 0.5508 (0.3219)	MaskLoss 1.0618 (0.5461)	MaskBCELoss 0.0847 (0.1225)	MaskDICELoss 0.9771 (0.4236)
Epoch: [0][305/500]	Time 57.498 (57.498)	Loss 1.0974 (1.0901)	CeLoss 0.2178 (0.6020)	SegCLSLoss 0.0125 (0.0092)	KLLoss 0.7734 (0.4031)	MaskLoss 0.8343 (0.5735)	MaskBCELoss 0.3044 (0.1313)	MaskDICELoss 0.5298 (0.4422)
Epoch: [0][306/500]	Time 62.061 (62.061)	Loss 0.8032 (1.1830)	CeLoss 0.1953 (0.5076)	SegCLSLoss 0.0190 (0.0132)	KLLoss 0.5938 (0.4188)	MaskLoss 1.0292 (0.7890)	MaskBCELoss 0.0557 (0.1828)	MaskDICELoss 0.9735 (0.6062)
Epoch: [0][307/500]	Time 73.045 (73.045)	Loss 1.4675 (1.2033)	CeLoss 0.2676 (0.4501)	SegCLSLoss 0.0267 (0.0164)	KLLoss 0.5352 (0.4680)	MaskLoss 1.1387 (0.8954)	MaskBCELoss 0.4113 (0.1982)	MaskDICELoss 0.7274 (0.6973)
Epoch: [0][308/500]	Time 49.009 (49.009)	Loss 1.3438 (1.1572)	CeLoss 1.3438 (0.9299)	SegCLSLoss 0.0000 (0.0054)	KLLoss 0.0000 (0.1828)	MaskLoss 0.0000 (0.3241)	MaskBCELoss 0.0000 (0.0418)	MaskDICELoss 0.0000 (0.2823)
Epoch: [0][309/500]	Time 77.670 (77.670)	Loss 0.9785 (1.0073)	CeLoss 0.2520 (0.2229)	SegCLSLoss 0.0153 (0.0210)	KLLoss 0.7148 (0.5877)	MaskLoss 1.0065 (1.0431)	MaskBCELoss 0.1436 (0.1681)	MaskDICELoss 0.8629 (0.8750)
[2025-03-01 23:59:20,334] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=0, lr=[0.00028720408163265305], mom=[(0.9, 0.95)]
[2025-03-01 23:59:20,335] [INFO] [timer.py:215:stop] epoch=0/micro_step=3100/global_step=310, RunningAvgSamplesPerSec=0.5396913980318246, CurrSamplesPerSec=0.17961230902282213, MemAllocated=31.71GB, MaxMemAllocated=36.85GB
Epoch: [0][310/500]	Time 55.677 (55.677)	Loss 0.0481 (0.9783)	CeLoss 0.0481 (0.3726)	SegCLSLoss 0.0000 (0.0182)	KLLoss 0.0000 (0.3863)	MaskLoss 0.0000 (0.7850)	MaskBCELoss 0.0000 (0.1360)	MaskDICELoss 0.0000 (0.6490)
Epoch: [0][311/500]	Time 29.781 (29.781)	Loss 1.2236 (1.0658)	CeLoss 0.1396 (0.2487)	SegCLSLoss 0.0479 (0.0200)	KLLoss 0.4453 (0.4676)	MaskLoss 1.2134 (0.9149)	MaskBCELoss 0.3019 (0.2330)	MaskDICELoss 0.9115 (0.6819)
Epoch: [0][312/500]	Time  9.133 ( 9.133)	Loss 1.4609 (1.3338)	CeLoss 1.4609 (0.4577)	SegCLSLoss 0.0000 (0.0166)	KLLoss 0.0000 (0.4453)	MaskLoss 0.0000 (0.9546)	MaskBCELoss 0.0000 (0.2602)	MaskDICELoss 0.0000 (0.6944)
Epoch: [0][313/500]	Time 10.268 (10.268)	Loss 0.9085 (1.0443)	CeLoss 0.2871 (0.2614)	SegCLSLoss 0.0211 (0.0225)	KLLoss 0.5703 (0.4768)	MaskLoss 1.0198 (0.9808)	MaskBCELoss 0.0678 (0.1875)	MaskDICELoss 0.9520 (0.7933)
Epoch: [0][314/500]	Time  8.228 ( 8.228)	Loss 0.9766 (0.9885)	CeLoss 0.9766 (0.4422)	SegCLSLoss 0.0000 (0.0182)	KLLoss 0.0000 (0.3326)	MaskLoss 0.0000 (0.6711)	MaskBCELoss 0.0000 (0.1344)	MaskDICELoss 0.0000 (0.5367)
Epoch: [0][315/500]	Time  8.999 ( 8.999)	Loss 1.0312 (1.1795)	CeLoss 1.0312 (0.4261)	SegCLSLoss 0.0000 (0.0204)	KLLoss 0.0000 (0.4320)	MaskLoss 0.0000 (0.8728)	MaskBCELoss 0.0000 (0.2046)	MaskDICELoss 0.0000 (0.6682)
Epoch: [0][316/500]	Time  7.802 ( 7.802)	Loss 1.3752 (1.2416)	CeLoss 0.2715 (0.6467)	SegCLSLoss 0.0139 (0.0114)	KLLoss 0.6445 (0.3449)	MaskLoss 1.0640 (0.6530)	MaskBCELoss 0.3759 (0.1752)	MaskDICELoss 0.6881 (0.4778)
Epoch: [0][317/500]	Time  8.872 ( 8.872)	Loss 0.9792 (1.3024)	CeLoss 0.2031 (0.3368)	SegCLSLoss 0.0187 (0.0277)	KLLoss 0.5898 (0.4756)	MaskLoss 1.0985 (1.0446)	MaskBCELoss 0.1447 (0.2862)	MaskDICELoss 0.9539 (0.7585)
Epoch: [0][318/500]	Time  9.389 ( 9.389)	Loss 1.9219 (0.9942)	CeLoss 1.9219 (0.4746)	SegCLSLoss 0.0000 (0.0133)	KLLoss 0.0000 (0.4113)	MaskLoss 0.0000 (0.7114)	MaskBCELoss 0.0000 (0.1047)	MaskDICELoss 0.0000 (0.6066)
Epoch: [0][319/500]	Time  9.206 ( 9.206)	Loss 1.0938 (1.2739)	CeLoss 1.0938 (0.3503)	SegCLSLoss 0.0000 (0.0174)	KLLoss 0.0000 (0.4545)	MaskLoss 0.0000 (0.9369)	MaskBCELoss 0.0000 (0.2978)	MaskDICELoss 0.0000 (0.6391)
[2025-03-02 00:01:10,660] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=0, lr=[0.00028659183673469384], mom=[(0.9, 0.95)]
[2025-03-02 00:01:10,661] [INFO] [timer.py:215:stop] epoch=0/micro_step=3200/global_step=320, RunningAvgSamplesPerSec=0.5466475515758019, CurrSamplesPerSec=1.156520187576515, MemAllocated=31.45GB, MaxMemAllocated=36.85GB
Epoch: [0][320/500]	Time  8.648 ( 8.648)	Loss 0.8317 (1.1579)	CeLoss 0.1904 (0.3482)	SegCLSLoss 0.0211 (0.0159)	KLLoss 0.5430 (0.4566)	MaskLoss 1.0501 (0.8745)	MaskBCELoss 0.0704 (0.2429)	MaskDICELoss 0.9797 (0.6316)
Epoch: [0][321/500]	Time  9.051 ( 9.051)	Loss 0.7188 (0.9207)	CeLoss 0.7188 (0.4665)	SegCLSLoss 0.0000 (0.0153)	KLLoss 0.0000 (0.3311)	MaskLoss 0.0000 (0.6446)	MaskBCELoss 0.0000 (0.0828)	MaskDICELoss 0.0000 (0.5619)
Epoch: [0][322/500]	Time  8.830 ( 8.830)	Loss 0.9290 (0.8925)	CeLoss 0.2197 (0.4232)	SegCLSLoss 0.0239 (0.0184)	KLLoss 0.5039 (0.3676)	MaskLoss 0.9616 (0.7265)	MaskBCELoss 0.1445 (0.0646)	MaskDICELoss 0.8171 (0.6619)
Epoch: [0][323/500]	Time  9.450 ( 9.450)	Loss 1.0874 (1.1374)	CeLoss 0.2520 (0.3984)	SegCLSLoss 0.0170 (0.0230)	KLLoss 0.6719 (0.4219)	MaskLoss 0.8353 (0.8614)	MaskBCELoss 0.2733 (0.1978)	MaskDICELoss 0.5619 (0.6636)
Epoch: [0][324/500]	Time 11.444 (11.444)	Loss 1.0725 (1.1334)	CeLoss 0.2354 (0.6896)	SegCLSLoss 0.0154 (0.0110)	KLLoss 0.6367 (0.3578)	MaskLoss 0.9758 (0.6123)	MaskBCELoss 0.2276 (0.0881)	MaskDICELoss 0.7481 (0.5241)
Epoch: [0][325/500]	Time 68.852 (68.852)	Loss 1.1304 (1.1433)	CeLoss 0.2559 (0.3618)	SegCLSLoss 0.0223 (0.0128)	KLLoss 0.5742 (0.5152)	MaskLoss 1.1489 (0.8694)	MaskBCELoss 0.1922 (0.2268)	MaskDICELoss 0.9566 (0.6426)
Epoch: [0][326/500]	Time 55.719 (55.719)	Loss 0.9741 (1.0609)	CeLoss 0.2402 (0.6676)	SegCLSLoss 0.0226 (0.0096)	KLLoss 0.5195 (0.2934)	MaskLoss 1.0441 (0.5252)	MaskBCELoss 0.1334 (0.0839)	MaskDICELoss 0.9107 (0.4412)
Epoch: [0][327/500]	Time 63.090 (63.090)	Loss 1.3054 (0.9783)	CeLoss 0.3789 (0.4422)	SegCLSLoss 0.0153 (0.0130)	KLLoss 0.6445 (0.4219)	MaskLoss 0.9343 (0.6873)	MaskBCELoss 0.3010 (0.1240)	MaskDICELoss 0.6333 (0.5632)
Epoch: [0][328/500]	Time 66.469 (66.469)	Loss 1.3516 (1.0627)	CeLoss 1.3516 (0.5798)	SegCLSLoss 0.0000 (0.0123)	KLLoss 0.0000 (0.3555)	MaskLoss 0.0000 (0.6340)	MaskBCELoss 0.0000 (0.1063)	MaskDICELoss 0.0000 (0.5277)
Epoch: [0][329/500]	Time 54.337 (54.337)	Loss 0.9874 (1.2817)	CeLoss 0.1934 (0.7224)	SegCLSLoss 0.0193 (0.0136)	KLLoss 0.6016 (0.2590)	MaskLoss 0.8557 (0.5896)	MaskBCELoss 0.2377 (0.1718)	MaskDICELoss 0.6180 (0.4179)
[2025-03-02 00:07:58,584] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=0, lr=[0.0002859795918367347], mom=[(0.9, 0.95)]
[2025-03-02 00:07:58,585] [INFO] [timer.py:215:stop] epoch=0/micro_step=3300/global_step=330, RunningAvgSamplesPerSec=0.5268920940564598, CurrSamplesPerSec=0.1648003903686615, MemAllocated=30.69GB, MaxMemAllocated=36.85GB
Epoch: [0][330/500]	Time 60.682 (60.682)	Loss 1.2656 (1.0032)	CeLoss 1.2656 (0.3947)	SegCLSLoss 0.0000 (0.0160)	KLLoss 0.0000 (0.3793)	MaskLoss 0.0000 (0.7544)	MaskBCELoss 0.0000 (0.1488)	MaskDICELoss 0.0000 (0.6056)
Epoch: [0][331/500]	Time 72.931 (72.931)	Loss 1.2030 (1.1329)	CeLoss 0.2471 (0.2235)	SegCLSLoss 0.0162 (0.0161)	KLLoss 0.6094 (0.4674)	MaskLoss 1.0296 (0.9313)	MaskBCELoss 0.2882 (0.2905)	MaskDICELoss 0.7414 (0.6408)
Epoch: [0][332/500]	Time 63.339 (63.339)	Loss 0.9552 (1.0067)	CeLoss 0.2080 (0.3124)	SegCLSLoss 0.0248 (0.0160)	KLLoss 0.5000 (0.5594)	MaskLoss 1.0525 (0.9035)	MaskBCELoss 0.1388 (0.1565)	MaskDICELoss 0.9137 (0.7470)
Epoch: [0][333/500]	Time 67.995 (67.995)	Loss 0.0530 (1.0167)	CeLoss 0.0530 (0.3327)	SegCLSLoss 0.0000 (0.0132)	KLLoss 0.0000 (0.4146)	MaskLoss 0.0000 (0.7811)	MaskBCELoss 0.0000 (0.1912)	MaskDICELoss 0.0000 (0.5899)
Epoch: [0][334/500]	Time 67.121 (67.121)	Loss 0.9581 (1.1897)	CeLoss 0.3066 (0.4180)	SegCLSLoss 0.0124 (0.0186)	KLLoss 0.7188 (0.4676)	MaskLoss 0.8398 (0.8331)	MaskBCELoss 0.1505 (0.2306)	MaskDICELoss 0.6894 (0.6026)
Epoch: [0][335/500]	Time 66.199 (66.199)	Loss 1.0703 (1.1505)	CeLoss 1.0703 (0.3474)	SegCLSLoss 0.0000 (0.0165)	KLLoss 0.0000 (0.5520)	MaskLoss 0.0000 (0.9785)	MaskBCELoss 0.0000 (0.2037)	MaskDICELoss 0.0000 (0.7748)
Epoch: [0][336/500]	Time 69.678 (69.678)	Loss 0.9647 (0.9084)	CeLoss 0.2148 (0.3131)	SegCLSLoss 0.0272 (0.0157)	KLLoss 0.5000 (0.4641)	MaskLoss 1.0447 (0.8257)	MaskBCELoss 0.1425 (0.1165)	MaskDICELoss 0.9022 (0.7092)
Epoch: [0][337/500]	Time 68.779 (68.779)	Loss 1.0090 (0.6943)	CeLoss 0.1514 (0.2262)	SegCLSLoss 0.0505 (0.0211)	KLLoss 0.4531 (0.3918)	MaskLoss 1.1301 (0.7402)	MaskBCELoss 0.1781 (0.0583)	MaskDICELoss 0.9520 (0.6819)
Epoch: [0][338/500]	Time 61.454 (61.454)	Loss 0.9609 (1.1313)	CeLoss 0.9609 (0.4080)	SegCLSLoss 0.0000 (0.0213)	KLLoss 0.0000 (0.3928)	MaskLoss 0.0000 (0.8140)	MaskBCELoss 0.0000 (0.2039)	MaskDICELoss 0.0000 (0.6101)
Epoch: [0][339/500]	Time 61.752 (61.752)	Loss 1.7906 (1.2712)	CeLoss 0.2539 (0.4806)	SegCLSLoss 0.0154 (0.0155)	KLLoss 0.7188 (0.4871)	MaskLoss 1.3812 (0.9414)	MaskBCELoss 0.5589 (0.2081)	MaskDICELoss 0.8224 (0.7333)
[2025-03-02 00:19:06,710] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=0, lr=[0.00028536734693877547], mom=[(0.9, 0.95)]
[2025-03-02 00:19:06,711] [INFO] [timer.py:215:stop] epoch=0/micro_step=3400/global_step=340, RunningAvgSamplesPerSec=0.49033166955933916, CurrSamplesPerSec=0.1451854142281335, MemAllocated=30.69GB, MaxMemAllocated=36.85GB
Epoch: [0][340/500]	Time 68.879 (68.879)	Loss 1.5391 (0.9680)	CeLoss 1.5391 (0.3803)	SegCLSLoss 0.0000 (0.0158)	KLLoss 0.0000 (0.4066)	MaskLoss 0.0000 (0.7619)	MaskBCELoss 0.0000 (0.1326)	MaskDICELoss 0.0000 (0.6293)
Epoch: [0][341/500]	Time 61.730 (61.730)	Loss 1.4219 (1.0987)	CeLoss 1.4219 (0.3679)	SegCLSLoss 0.0000 (0.0193)	KLLoss 0.0000 (0.5344)	MaskLoss 0.0000 (0.9594)	MaskBCELoss 0.0000 (0.1611)	MaskDICELoss 0.0000 (0.7983)
Epoch: [0][342/500]	Time 73.366 (73.366)	Loss 0.9374 (1.2949)	CeLoss 0.2266 (0.2704)	SegCLSLoss 0.0439 (0.0222)	KLLoss 0.6133 (0.6195)	MaskLoss 1.0692 (1.1429)	MaskBCELoss 0.1032 (0.2947)	MaskDICELoss 0.9661 (0.8482)
Epoch: [0][343/500]	Time 71.351 (71.351)	Loss 1.0247 (1.0803)	CeLoss 0.2539 (0.3916)	SegCLSLoss 0.0136 (0.0196)	KLLoss 0.7461 (0.4615)	MaskLoss 0.9841 (0.8866)	MaskBCELoss 0.1819 (0.1571)	MaskDICELoss 0.8022 (0.7295)
Epoch: [0][344/500]	Time 68.686 (68.686)	Loss 0.7942 (1.2413)	CeLoss 0.2578 (0.3545)	SegCLSLoss 0.0168 (0.0177)	KLLoss 0.6367 (0.5656)	MaskLoss 1.0122 (0.9800)	MaskBCELoss 0.0150 (0.2585)	MaskDICELoss 0.9972 (0.7214)
Epoch: [0][345/500]	Time 53.993 (53.993)	Loss 1.2500 (1.2528)	CeLoss 1.2500 (0.5383)	SegCLSLoss 0.0000 (0.0166)	KLLoss 0.0000 (0.4172)	MaskLoss 0.0000 (0.8067)	MaskBCELoss 0.0000 (0.2019)	MaskDICELoss 0.0000 (0.6048)
Epoch: [0][346/500]	Time 56.612 (56.612)	Loss 1.0156 (0.9726)	CeLoss 1.0156 (0.4704)	SegCLSLoss 0.0000 (0.0178)	KLLoss 0.0000 (0.4043)	MaskLoss 0.0000 (0.7143)	MaskBCELoss 0.0000 (0.0907)	MaskDICELoss 0.0000 (0.6236)
Epoch: [0][347/500]	Time 63.963 (63.963)	Loss 0.0574 (1.1108)	CeLoss 0.0574 (0.4189)	SegCLSLoss 0.0000 (0.0169)	KLLoss 0.0000 (0.4131)	MaskLoss 0.0000 (0.7993)	MaskBCELoss 0.0000 (0.1892)	MaskDICELoss 0.0000 (0.6101)
Epoch: [0][348/500]	Time 61.990 (61.990)	Loss 2.0812 (1.3545)	CeLoss 0.3242 (0.6107)	SegCLSLoss 0.0189 (0.0149)	KLLoss 0.6328 (0.4258)	MaskLoss 1.5836 (0.8374)	MaskBCELoss 0.6369 (0.2118)	MaskDICELoss 0.9466 (0.6256)
Epoch: [0][349/500]	Time 72.500 (72.500)	Loss 1.8356 (1.1817)	CeLoss 0.3398 (0.3985)	SegCLSLoss 0.0164 (0.0185)	KLLoss 0.7188 (0.5844)	MaskLoss 1.2461 (0.9387)	MaskBCELoss 0.5766 (0.2032)	MaskDICELoss 0.6695 (0.7356)
[2025-03-02 00:29:50,936] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=0, lr=[0.00028475510204081626], mom=[(0.9, 0.95)]
[2025-03-02 00:29:50,937] [INFO] [timer.py:215:stop] epoch=0/micro_step=3500/global_step=350, RunningAvgSamplesPerSec=0.46169145506031484, CurrSamplesPerSec=0.16657543344609668, MemAllocated=30.78GB, MaxMemAllocated=36.85GB
Epoch: [0][350/500]	Time 60.035 (60.035)	Loss 1.2677 (1.0689)	CeLoss 0.2754 (0.3534)	SegCLSLoss 0.0153 (0.0183)	KLLoss 0.7422 (0.4805)	MaskLoss 0.9198 (0.8716)	MaskBCELoss 0.3497 (0.1804)	MaskDICELoss 0.5701 (0.6912)
Epoch: [0][351/500]	Time 69.546 (69.546)	Loss 1.2031 (1.2625)	CeLoss 1.2031 (0.4593)	SegCLSLoss 0.0000 (0.0137)	KLLoss 0.0000 (0.5160)	MaskLoss 0.0000 (0.8950)	MaskBCELoss 0.0000 (0.2326)	MaskDICELoss 0.0000 (0.6623)
Epoch: [0][352/500]	Time 67.805 (67.805)	Loss 1.4173 (0.7464)	CeLoss 0.2871 (0.3021)	SegCLSLoss 0.0156 (0.0120)	KLLoss 0.6797 (0.3703)	MaskLoss 1.1697 (0.6271)	MaskBCELoss 0.3583 (0.0832)	MaskDICELoss 0.8114 (0.5439)
Epoch: [0][353/500]	Time 64.075 (64.075)	Loss 1.7521 (0.9742)	CeLoss 0.2793 (0.4603)	SegCLSLoss 0.0332 (0.0136)	KLLoss 0.5430 (0.3611)	MaskLoss 1.3576 (0.6310)	MaskBCELoss 0.5176 (0.1278)	MaskDICELoss 0.8400 (0.5032)
Epoch: [0][354/500]	Time 65.549 (65.549)	Loss 1.1372 (1.2735)	CeLoss 0.2158 (0.5421)	SegCLSLoss 0.0165 (0.0182)	KLLoss 0.6992 (0.3937)	MaskLoss 1.0572 (0.7922)	MaskBCELoss 0.2566 (0.2174)	MaskDICELoss 0.8006 (0.5748)
Epoch: [0][355/500]	Time 64.381 (64.381)	Loss 0.5195 (1.2008)	CeLoss 0.5195 (0.4475)	SegCLSLoss 0.0000 (0.0178)	KLLoss 0.0000 (0.4064)	MaskLoss 0.0000 (0.7978)	MaskBCELoss 0.0000 (0.2304)	MaskDICELoss 0.0000 (0.5674)
Epoch: [0][356/500]	Time 57.575 (57.575)	Loss 1.5469 (1.1706)	CeLoss 1.5469 (0.5261)	SegCLSLoss 0.0000 (0.0164)	KLLoss 0.0000 (0.4152)	MaskLoss 0.0000 (0.7899)	MaskBCELoss 0.0000 (0.1609)	MaskDICELoss 0.0000 (0.6290)
Epoch: [0][357/500]	Time 58.797 (58.797)	Loss 0.7974 (0.9985)	CeLoss 0.1650 (0.4090)	SegCLSLoss 0.0293 (0.0173)	KLLoss 0.5312 (0.3867)	MaskLoss 0.9569 (0.7034)	MaskBCELoss 0.0928 (0.1528)	MaskDICELoss 0.8640 (0.5507)
Epoch: [0][358/500]	Time 68.788 (68.788)	Loss 0.7891 (1.1198)	CeLoss 0.7891 (0.3527)	SegCLSLoss 0.0000 (0.0185)	KLLoss 0.0000 (0.4691)	MaskLoss 0.0000 (0.8932)	MaskBCELoss 0.0000 (0.2075)	MaskDICELoss 0.0000 (0.6857)
Epoch: [0][359/500]	Time 57.410 (57.410)	Loss 1.0648 (1.2580)	CeLoss 0.2373 (0.4237)	SegCLSLoss 0.0173 (0.0194)	KLLoss 0.6445 (0.4801)	MaskLoss 0.6012 (0.8487)	MaskBCELoss 0.3454 (0.2667)	MaskDICELoss 0.2559 (0.5820)
[2025-03-02 00:40:28,473] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=0, lr=[0.0002841428571428571], mom=[(0.9, 0.95)]
[2025-03-02 00:40:28,474] [INFO] [timer.py:215:stop] epoch=0/micro_step=3600/global_step=360, RunningAvgSamplesPerSec=0.4379194779872619, CurrSamplesPerSec=0.15721247299624502, MemAllocated=30.68GB, MaxMemAllocated=36.85GB
Epoch: [0][360/500]	Time 63.611 (63.611)	Loss 1.6016 (0.9534)	CeLoss 1.6016 (0.4117)	SegCLSLoss 0.0000 (0.0175)	KLLoss 0.0000 (0.3971)	MaskLoss 0.0000 (0.7322)	MaskBCELoss 0.0000 (0.1113)	MaskDICELoss 0.0000 (0.6209)
Epoch: [0][361/500]	Time 61.212 (61.212)	Loss 0.7040 (0.8903)	CeLoss 0.2275 (0.3703)	SegCLSLoss 0.0205 (0.0170)	KLLoss 0.5586 (0.4711)	MaskLoss 0.8470 (0.7416)	MaskBCELoss 0.0282 (0.0939)	MaskDICELoss 0.8188 (0.6477)
Epoch: [0][362/500]	Time 66.757 (66.757)	Loss 1.3728 (0.7914)	CeLoss 0.2295 (0.3987)	SegCLSLoss 0.0264 (0.0122)	KLLoss 0.6016 (0.3777)	MaskLoss 1.0556 (0.5948)	MaskBCELoss 0.4019 (0.0594)	MaskDICELoss 0.6537 (0.5354)
Epoch: [0][363/500]	Time 69.699 (69.699)	Loss 0.7591 (1.1540)	CeLoss 0.2197 (0.4355)	SegCLSLoss 0.0217 (0.0184)	KLLoss 0.5703 (0.4734)	MaskLoss 1.0041 (0.8624)	MaskBCELoss 0.0177 (0.1855)	MaskDICELoss 0.9864 (0.6769)
Epoch: [0][364/500]	Time 55.782 (55.782)	Loss 1.6835 (1.1946)	CeLoss 0.2949 (0.5821)	SegCLSLoss 0.0190 (0.0150)	KLLoss 0.5820 (0.3488)	MaskLoss 1.2652 (0.6597)	MaskBCELoss 0.4975 (0.1834)	MaskDICELoss 0.7676 (0.4763)
Epoch: [0][365/500]	Time 53.834 (53.834)	Loss 0.8077 (1.0447)	CeLoss 0.2451 (0.5163)	SegCLSLoss 0.0170 (0.0145)	KLLoss 0.6289 (0.3535)	MaskLoss 1.0258 (0.6511)	MaskBCELoss 0.0273 (0.1305)	MaskDICELoss 0.9985 (0.5206)
Epoch: [0][366/500]	Time 52.974 (52.974)	Loss 1.2113 (1.1827)	CeLoss 0.2295 (0.4626)	SegCLSLoss 0.0172 (0.0156)	KLLoss 0.6602 (0.4980)	MaskLoss 0.9460 (0.8207)	MaskBCELoss 0.3334 (0.2012)	MaskDICELoss 0.6126 (0.6195)
Epoch: [0][367/500]	Time 63.142 (63.142)	Loss 1.5980 (1.1873)	CeLoss 0.2402 (0.3789)	SegCLSLoss 0.0214 (0.0210)	KLLoss 0.5820 (0.4744)	MaskLoss 0.9915 (0.8652)	MaskBCELoss 0.5682 (0.2437)	MaskDICELoss 0.4234 (0.6215)
Epoch: [0][368/500]	Time 54.274 (54.274)	Loss 1.0466 (1.1930)	CeLoss 0.2520 (0.6808)	SegCLSLoss 0.0131 (0.0102)	KLLoss 0.6797 (0.3793)	MaskLoss 0.8770 (0.6346)	MaskBCELoss 0.2335 (0.1266)	MaskDICELoss 0.6435 (0.5080)
Epoch: [0][369/500]	Time 67.993 (67.993)	Loss 0.9236 (1.1707)	CeLoss 0.2754 (0.2338)	SegCLSLoss 0.0128 (0.0216)	KLLoss 0.7305 (0.6170)	MaskLoss 0.8585 (1.0788)	MaskBCELoss 0.1421 (0.2579)	MaskDICELoss 0.7164 (0.8210)
[2025-03-02 00:50:36,596] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=0, lr=[0.00028353061224489795], mom=[(0.9, 0.95)]
[2025-03-02 00:50:36,597] [INFO] [timer.py:215:stop] epoch=0/micro_step=3700/global_step=370, RunningAvgSamplesPerSec=0.41898529056668954, CurrSamplesPerSec=0.1601181994375769, MemAllocated=30.84GB, MaxMemAllocated=36.85GB
Epoch: [0][370/500]	Time 62.456 (62.456)	Loss 1.5391 (1.0884)	CeLoss 1.5391 (0.3558)	SegCLSLoss 0.0000 (0.0202)	KLLoss 0.0000 (0.5488)	MaskLoss 0.0000 (0.9301)	MaskBCELoss 0.0000 (0.1717)	MaskDICELoss 0.0000 (0.7584)
Epoch: [0][371/500]	Time 54.838 (54.838)	Loss 0.8899 (1.2689)	CeLoss 0.2656 (0.3784)	SegCLSLoss 0.0151 (0.0203)	KLLoss 0.6328 (0.4738)	MaskLoss 1.0001 (0.9518)	MaskBCELoss 0.0776 (0.2696)	MaskDICELoss 0.9224 (0.6822)
Epoch: [0][372/500]	Time 55.573 (55.573)	Loss 1.3046 (1.1208)	CeLoss 0.3965 (0.3997)	SegCLSLoss 0.0160 (0.0161)	KLLoss 0.6836 (0.4883)	MaskLoss 1.0176 (0.8168)	MaskBCELoss 0.2610 (0.2031)	MaskDICELoss 0.7566 (0.6137)
Epoch: [0][373/500]	Time 49.214 (49.214)	Loss 1.0988 (1.0760)	CeLoss 0.1787 (0.5853)	SegCLSLoss 0.0417 (0.0144)	KLLoss 0.4531 (0.3566)	MaskLoss 1.1341 (0.6372)	MaskBCELoss 0.2217 (0.1100)	MaskDICELoss 0.9124 (0.5271)
Epoch: [0][374/500]	Time 65.102 (65.102)	Loss 0.0874 (1.1358)	CeLoss 0.0874 (0.2517)	SegCLSLoss 0.0000 (0.0212)	KLLoss 0.0000 (0.5322)	MaskLoss 0.0000 (0.9987)	MaskBCELoss 0.0000 (0.2494)	MaskDICELoss 0.0000 (0.7492)
Epoch: [0][375/500]	Time 50.769 (50.769)	Loss 0.9219 (1.1263)	CeLoss 0.9219 (0.6441)	SegCLSLoss 0.0000 (0.0118)	KLLoss 0.0000 (0.3049)	MaskLoss 0.0000 (0.5660)	MaskBCELoss 0.0000 (0.1289)	MaskDICELoss 0.0000 (0.4371)
Epoch: [0][376/500]	Time 71.651 (71.651)	Loss 0.8723 (1.0527)	CeLoss 0.2852 (0.2374)	SegCLSLoss 0.0194 (0.0177)	KLLoss 0.5742 (0.5535)	MaskLoss 0.9868 (0.9413)	MaskBCELoss 0.0560 (0.2241)	MaskDICELoss 0.9308 (0.7172)
Epoch: [0][377/500]	Time 64.712 (64.712)	Loss 0.8122 (0.9740)	CeLoss 0.2891 (0.4240)	SegCLSLoss 0.0156 (0.0169)	KLLoss 0.8320 (0.5082)	MaskLoss 1.0024 (0.8251)	MaskBCELoss 0.0094 (0.0858)	MaskDICELoss 0.9930 (0.7393)
Epoch: [0][378/500]	Time 70.305 (70.305)	Loss 1.3444 (1.1779)	CeLoss 0.2451 (0.2426)	SegCLSLoss 0.0271 (0.0240)	KLLoss 0.5312 (0.6002)	MaskLoss 1.2153 (1.1332)	MaskBCELoss 0.3193 (0.2378)	MaskDICELoss 0.8960 (0.8954)
Epoch: [0][379/500]	Time 57.790 (57.790)	Loss 2.1902 (1.3378)	CeLoss 0.6719 (0.7121)	SegCLSLoss 0.0142 (0.0166)	KLLoss 0.6406 (0.3396)	MaskLoss 1.2684 (0.7126)	MaskBCELoss 0.5842 (0.1740)	MaskDICELoss 0.6842 (0.5387)
[2025-03-02 01:00:24,973] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=0, lr=[0.00028291836734693873], mom=[(0.9, 0.95)]
[2025-03-02 01:00:24,974] [INFO] [timer.py:215:stop] epoch=0/micro_step=3800/global_step=380, RunningAvgSamplesPerSec=0.4033512809489119, CurrSamplesPerSec=0.20652118059473434, MemAllocated=30.69GB, MaxMemAllocated=36.85GB
Epoch: [0][380/500]	Time 48.423 (48.423)	Loss 0.8047 (0.9822)	CeLoss 0.8047 (0.3086)	SegCLSLoss 0.0000 (0.0175)	KLLoss 0.0000 (0.5742)	MaskLoss 0.0000 (0.9070)	MaskBCELoss 0.0000 (0.1407)	MaskDICELoss 0.0000 (0.7663)
Epoch: [0][381/500]	Time 26.691 (26.691)	Loss 1.2734 (1.2612)	CeLoss 1.2734 (0.5826)	SegCLSLoss 0.0000 (0.0175)	KLLoss 0.0000 (0.3170)	MaskLoss 0.0000 (0.7512)	MaskBCELoss 0.0000 (0.1961)	MaskDICELoss 0.0000 (0.5551)
Epoch: [0][382/500]	Time  9.578 ( 9.578)	Loss 0.0928 (1.0661)	CeLoss 0.0928 (0.4447)	SegCLSLoss 0.0000 (0.0135)	KLLoss 0.0000 (0.4250)	MaskLoss 0.0000 (0.7397)	MaskBCELoss 0.0000 (0.1630)	MaskDICELoss 0.0000 (0.5767)
Epoch: [0][383/500]	Time  8.653 ( 8.653)	Loss 0.1001 (0.8760)	CeLoss 0.1001 (0.4941)	SegCLSLoss 0.0000 (0.0098)	KLLoss 0.0000 (0.2973)	MaskLoss 0.0000 (0.5185)	MaskBCELoss 0.0000 (0.0785)	MaskDICELoss 0.0000 (0.4400)
Epoch: [0][384/500]	Time  8.647 ( 8.647)	Loss 1.2316 (1.0675)	CeLoss 0.2949 (0.3395)	SegCLSLoss 0.0166 (0.0190)	KLLoss 0.6328 (0.5371)	MaskLoss 1.1435 (0.9610)	MaskBCELoss 0.2381 (0.1586)	MaskDICELoss 0.9054 (0.8023)
Epoch: [0][385/500]	Time  8.178 ( 8.178)	Loss 0.9781 (1.1740)	CeLoss 0.1650 (0.6769)	SegCLSLoss 0.0315 (0.0137)	KLLoss 0.5195 (0.3465)	MaskLoss 0.9968 (0.6349)	MaskBCELoss 0.1993 (0.1152)	MaskDICELoss 0.7974 (0.5197)
Epoch: [0][386/500]	Time  9.831 ( 9.831)	Loss 0.8613 (1.0809)	CeLoss 0.2988 (0.2993)	SegCLSLoss 0.0182 (0.0177)	KLLoss 0.5742 (0.5398)	MaskLoss 0.9202 (0.9479)	MaskBCELoss 0.0617 (0.1991)	MaskDICELoss 0.8584 (0.7488)
Epoch: [0][387/500]	Time  9.600 ( 9.600)	Loss 0.8315 (1.0486)	CeLoss 0.1982 (0.3537)	SegCLSLoss 0.0121 (0.0160)	KLLoss 0.7109 (0.5559)	MaskLoss 1.0621 (0.9187)	MaskBCELoss 0.0642 (0.1517)	MaskDICELoss 0.9979 (0.7670)
Epoch: [0][388/500]	Time  8.926 ( 8.926)	Loss 1.1507 (0.9990)	CeLoss 0.0957 (0.5005)	SegCLSLoss 0.0693 (0.0179)	KLLoss 0.4434 (0.4428)	MaskLoss 1.1920 (0.6638)	MaskBCELoss 0.2826 (0.1051)	MaskDICELoss 0.9094 (0.5587)
Epoch: [0][389/500]	Time  8.745 ( 8.745)	Loss 1.2727 (1.1903)	CeLoss 0.1768 (0.4456)	SegCLSLoss 0.0415 (0.0197)	KLLoss 0.4844 (0.4633)	MaskLoss 1.1565 (0.8919)	MaskBCELoss 0.3315 (0.1927)	MaskDICELoss 0.8250 (0.6993)
[2025-03-02 01:02:12,732] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=0, lr=[0.0002823061224489796], mom=[(0.9, 0.95)]
[2025-03-02 01:02:12,733] [INFO] [timer.py:215:stop] epoch=0/micro_step=3900/global_step=390, RunningAvgSamplesPerSec=0.4093159673743323, CurrSamplesPerSec=1.122650771255232, MemAllocated=31.24GB, MaxMemAllocated=36.85GB
Epoch: [0][390/500]	Time  8.909 ( 8.909)	Loss 1.3983 (0.9666)	CeLoss 0.2324 (0.3354)	SegCLSLoss 0.0212 (0.0152)	KLLoss 0.6250 (0.4895)	MaskLoss 1.2821 (0.8534)	MaskBCELoss 0.3427 (0.1315)	MaskDICELoss 0.9393 (0.7220)
Epoch: [0][391/500]	Time  8.705 ( 8.705)	Loss 1.2896 (1.0963)	CeLoss 0.1768 (0.3514)	SegCLSLoss 0.0371 (0.0187)	KLLoss 0.4863 (0.5486)	MaskLoss 1.1948 (0.9770)	MaskBCELoss 0.3313 (0.1647)	MaskDICELoss 0.8635 (0.8123)
Epoch: [0][392/500]	Time  8.981 ( 8.981)	Loss 1.0053 (0.9509)	CeLoss 0.3770 (0.5226)	SegCLSLoss 0.0327 (0.0139)	KLLoss 0.7070 (0.3793)	MaskLoss 0.9320 (0.6325)	MaskBCELoss 0.0978 (0.0702)	MaskDICELoss 0.8342 (0.5623)
Epoch: [0][393/500]	Time  8.800 ( 8.800)	Loss 0.8492 (1.2501)	CeLoss 0.2891 (0.2301)	SegCLSLoss 0.0166 (0.0210)	KLLoss 0.6914 (0.5949)	MaskLoss 0.8930 (1.1982)	MaskBCELoss 0.0705 (0.2736)	MaskDICELoss 0.8225 (0.9246)
Epoch: [0][394/500]	Time 27.862 (27.862)	Loss 1.2918 (0.9482)	CeLoss 0.3809 (0.4197)	SegCLSLoss 0.0133 (0.0131)	KLLoss 0.7109 (0.4705)	MaskLoss 1.1029 (0.6814)	MaskBCELoss 0.2358 (0.1208)	MaskDICELoss 0.8671 (0.5606)
Epoch: [0][395/500]	Time 57.338 (57.338)	Loss 1.3529 (1.0372)	CeLoss 0.1904 (0.5714)	SegCLSLoss 0.0267 (0.0109)	KLLoss 0.5391 (0.3832)	MaskLoss 1.2367 (0.6261)	MaskBCELoss 0.3536 (0.0982)	MaskDICELoss 0.8831 (0.5280)
Epoch: [0][396/500]	Time 61.358 (61.358)	Loss 1.2761 (1.1005)	CeLoss 0.1533 (0.3145)	SegCLSLoss 0.0267 (0.0172)	KLLoss 0.5039 (0.5797)	MaskLoss 1.2181 (0.9580)	MaskBCELoss 0.3334 (0.1990)	MaskDICELoss 0.8848 (0.7590)
Epoch: [0][397/500]	Time 64.689 (64.689)	Loss 1.1361 (1.2940)	CeLoss 0.2109 (0.3513)	SegCLSLoss 0.0165 (0.0165)	KLLoss 0.6797 (0.5586)	MaskLoss 0.9075 (1.0277)	MaskBCELoss 0.3091 (0.2805)	MaskDICELoss 0.5984 (0.7472)
Epoch: [0][398/500]	Time 53.971 (53.971)	Loss 1.0199 (1.4581)	CeLoss 0.2051 (0.3523)	SegCLSLoss 0.0262 (0.0161)	KLLoss 0.5781 (0.5305)	MaskLoss 1.1210 (1.0146)	MaskBCELoss 0.1611 (0.3938)	MaskDICELoss 0.9599 (0.6209)
Epoch: [0][399/500]	Time 59.187 (59.187)	Loss 1.5204 (1.1795)	CeLoss 0.1602 (0.2830)	SegCLSLoss 0.0266 (0.0218)	KLLoss 0.5430 (0.5420)	MaskLoss 1.2809 (1.0306)	MaskBCELoss 0.4707 (0.2468)	MaskDICELoss 0.8102 (0.7838)
[2025-03-02 01:09:05,327] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=0, lr=[0.0002816938775510204], mom=[(0.9, 0.95)]
[2025-03-02 01:09:05,328] [INFO] [timer.py:215:stop] epoch=0/micro_step=4000/global_step=400, RunningAvgSamplesPerSec=0.40235305439789465, CurrSamplesPerSec=0.16207022292078502, MemAllocated=30.88GB, MaxMemAllocated=36.85GB
Epoch: [0][400/500]	Time 61.704 (61.704)	Loss 0.8477 (1.0463)	CeLoss 0.8477 (0.5048)	SegCLSLoss 0.0000 (0.0104)	KLLoss 0.0000 (0.3863)	MaskLoss 0.0000 (0.6553)	MaskBCELoss 0.0000 (0.1390)	MaskDICELoss 0.0000 (0.5163)
Epoch: [0][401/500]	Time 59.223 (59.223)	Loss 1.1406 (1.0803)	CeLoss 1.1406 (0.5493)	SegCLSLoss 0.0000 (0.0116)	KLLoss 0.0000 (0.3805)	MaskLoss 0.0000 (0.6655)	MaskBCELoss 0.0000 (0.1281)	MaskDICELoss 0.0000 (0.5374)
Epoch: [0][402/500]	Time 71.328 (71.328)	Loss 1.4984 (1.2652)	CeLoss 0.2061 (0.2357)	SegCLSLoss 0.0187 (0.0196)	KLLoss 0.5781 (0.6211)	MaskLoss 1.3823 (1.1559)	MaskBCELoss 0.3943 (0.2944)	MaskDICELoss 0.9880 (0.8615)
Epoch: [0][403/500]	Time 70.109 (70.109)	Loss 2.5204 (1.0649)	CeLoss 0.2432 (0.3644)	SegCLSLoss 0.0221 (0.0126)	KLLoss 0.5742 (0.4363)	MaskLoss 1.7833 (0.8265)	MaskBCELoss 0.9165 (0.1874)	MaskDICELoss 0.8668 (0.6391)
Epoch: [0][404/500]	Time 66.359 (66.359)	Loss 1.0224 (1.0333)	CeLoss 0.3184 (0.3610)	SegCLSLoss 0.0173 (0.0174)	KLLoss 0.6523 (0.5578)	MaskLoss 1.0210 (0.9358)	MaskBCELoss 0.1238 (0.1306)	MaskDICELoss 0.8971 (0.8052)
Epoch: [0][405/500]	Time 63.710 (63.710)	Loss 1.5563 (0.9910)	CeLoss 0.3828 (0.3726)	SegCLSLoss 0.0134 (0.0153)	KLLoss 0.7812 (0.5145)	MaskLoss 0.9907 (0.8347)	MaskBCELoss 0.4482 (0.1289)	MaskDICELoss 0.5425 (0.7058)
Epoch: [0][406/500]	Time 66.505 (66.505)	Loss 1.1409 (1.1594)	CeLoss 0.1758 (0.4766)	SegCLSLoss 0.0242 (0.0136)	KLLoss 0.5820 (0.5074)	MaskLoss 1.1489 (0.8598)	MaskBCELoss 0.2526 (0.1642)	MaskDICELoss 0.8963 (0.6957)
Epoch: [0][407/500]	Time 62.035 (62.035)	Loss 1.3516 (1.3923)	CeLoss 1.3516 (0.5229)	SegCLSLoss 0.0000 (0.0113)	KLLoss 0.0000 (0.4570)	MaskLoss 0.0000 (0.8724)	MaskBCELoss 0.0000 (0.2849)	MaskDICELoss 0.0000 (0.5875)
Epoch: [0][408/500]	Time 54.942 (54.942)	Loss 1.4837 (1.3905)	CeLoss 0.2832 (0.5049)	SegCLSLoss 0.0140 (0.0125)	KLLoss 0.6641 (0.5203)	MaskLoss 1.1538 (0.9504)	MaskBCELoss 0.4106 (0.2695)	MaskDICELoss 0.7432 (0.6809)
Epoch: [0][409/500]	Time 35.515 (35.515)	Loss 1.2656 (1.1963)	CeLoss 1.2656 (0.6174)	SegCLSLoss 0.0000 (0.0086)	KLLoss 0.0000 (0.4125)	MaskLoss 0.0000 (0.6283)	MaskBCELoss 0.0000 (0.1738)	MaskDICELoss 0.0000 (0.4545)
[2025-03-02 01:19:27,668] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=0, lr=[0.0002810816326530612], mom=[(0.9, 0.95)]
[2025-03-02 01:19:27,669] [INFO] [timer.py:215:stop] epoch=0/micro_step=4100/global_step=410, RunningAvgSamplesPerSec=0.3880490082847666, CurrSamplesPerSec=0.13771533888681026, MemAllocated=31.25GB, MaxMemAllocated=36.85GB
Epoch: [0][410/500]	Time 72.615 (72.615)	Loss 0.8745 (1.0243)	CeLoss 0.1768 (0.2854)	SegCLSLoss 0.0164 (0.0217)	KLLoss 0.5547 (0.4848)	MaskLoss 1.0511 (0.9860)	MaskBCELoss 0.1096 (0.1568)	MaskDICELoss 0.9414 (0.8292)
Epoch: [0][411/500]	Time 57.434 (57.434)	Loss 1.4843 (1.4354)	CeLoss 0.2295 (0.8545)	SegCLSLoss 0.0244 (0.0110)	KLLoss 0.5586 (0.3648)	MaskLoss 1.2662 (0.6916)	MaskBCELoss 0.4060 (0.1531)	MaskDICELoss 0.8602 (0.5385)
Epoch: [0][412/500]	Time 55.531 (55.531)	Loss 0.3867 (0.8570)	CeLoss 0.3867 (0.4622)	SegCLSLoss 0.0000 (0.0114)	KLLoss 0.0000 (0.3568)	MaskLoss 0.0000 (0.5681)	MaskBCELoss 0.0000 (0.0700)	MaskDICELoss 0.0000 (0.4981)
Epoch: [0][413/500]	Time 54.961 (54.961)	Loss 1.9609 (1.2986)	CeLoss 1.9609 (0.5855)	SegCLSLoss 0.0000 (0.0132)	KLLoss 0.0000 (0.4166)	MaskLoss 0.0000 (0.7695)	MaskBCELoss 0.0000 (0.2143)	MaskDICELoss 0.0000 (0.5552)
Epoch: [0][414/500]	Time 60.501 (60.501)	Loss 0.9718 (1.0831)	CeLoss 0.2500 (0.3328)	SegCLSLoss 0.0166 (0.0159)	KLLoss 0.5547 (0.5281)	MaskLoss 1.0549 (0.9759)	MaskBCELoss 0.1243 (0.1696)	MaskDICELoss 0.9306 (0.8063)
Epoch: [0][415/500]	Time 66.499 (66.499)	Loss 1.1094 (1.2615)	CeLoss 0.1562 (0.4877)	SegCLSLoss 0.0304 (0.0156)	KLLoss 0.5312 (0.4082)	MaskLoss 1.1127 (0.8536)	MaskBCELoss 0.2542 (0.2261)	MaskDICELoss 0.8585 (0.6275)
Epoch: [0][416/500]	Time 64.079 (64.079)	Loss 1.0455 (1.0104)	CeLoss 0.1846 (0.3021)	SegCLSLoss 0.0287 (0.0186)	KLLoss 0.5078 (0.4699)	MaskLoss 1.1061 (0.8481)	MaskBCELoss 0.1955 (0.1833)	MaskDICELoss 0.9106 (0.6647)
Epoch: [0][417/500]	Time 60.998 (60.998)	Loss 1.0391 (1.1800)	CeLoss 1.0391 (0.4740)	SegCLSLoss 0.0000 (0.0183)	KLLoss 0.0000 (0.3799)	MaskLoss 0.0000 (0.8086)	MaskBCELoss 0.0000 (0.1949)	MaskDICELoss 0.0000 (0.6137)
Epoch: [0][418/500]	Time 62.526 (62.526)	Loss 1.4984 (1.1449)	CeLoss 0.3574 (0.2571)	SegCLSLoss 0.0170 (0.0209)	KLLoss 0.5664 (0.5762)	MaskLoss 1.2140 (1.0877)	MaskBCELoss 0.3508 (0.2223)	MaskDICELoss 0.8633 (0.8653)
Epoch: [0][419/500]	Time 61.792 (61.792)	Loss 1.3359 (1.0689)	CeLoss 1.3359 (0.4624)	SegCLSLoss 0.0000 (0.0125)	KLLoss 0.0000 (0.4188)	MaskLoss 0.0000 (0.7549)	MaskBCELoss 0.0000 (0.1485)	MaskDICELoss 0.0000 (0.6064)
[2025-03-02 01:29:23,050] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=0, lr=[0.000280469387755102], mom=[(0.9, 0.95)]
[2025-03-02 01:29:23,051] [INFO] [timer.py:215:stop] epoch=0/micro_step=4200/global_step=420, RunningAvgSamplesPerSec=0.3762546032757871, CurrSamplesPerSec=0.19585089277743287, MemAllocated=31.25GB, MaxMemAllocated=36.85GB
Epoch: [0][420/500]	Time 51.061 (51.061)	Loss 0.8784 (1.1276)	CeLoss 0.1748 (0.7933)	SegCLSLoss 0.0236 (0.0126)	KLLoss 0.5078 (0.2109)	MaskLoss 1.0234 (0.4314)	MaskBCELoss 0.1201 (0.0750)	MaskDICELoss 0.9033 (0.3564)
Epoch: [0][421/500]	Time 56.469 (56.469)	Loss 1.2529 (1.3474)	CeLoss 0.1963 (0.7264)	SegCLSLoss 0.0259 (0.0120)	KLLoss 0.5000 (0.2711)	MaskLoss 1.1705 (0.6126)	MaskBCELoss 0.3058 (0.2059)	MaskDICELoss 0.8647 (0.4067)
Epoch: [0][422/500]	Time 75.945 (75.945)	Loss 1.2602 (0.9703)	CeLoss 0.2422 (0.2639)	SegCLSLoss 0.0302 (0.0198)	KLLoss 0.5234 (0.6434)	MaskLoss 1.1948 (1.0233)	MaskBCELoss 0.2700 (0.1232)	MaskDICELoss 0.9248 (0.9002)
Epoch: [0][423/500]	Time 68.317 (68.317)	Loss 0.9187 (1.1016)	CeLoss 0.2471 (0.5291)	SegCLSLoss 0.0183 (0.0168)	KLLoss 0.5430 (0.4680)	MaskLoss 1.0623 (0.8126)	MaskBCELoss 0.0878 (0.1051)	MaskDICELoss 0.9746 (0.7075)
Epoch: [0][424/500]	Time 60.549 (60.549)	Loss 0.8372 (1.1417)	CeLoss 0.1953 (0.5098)	SegCLSLoss 0.0160 (0.0137)	KLLoss 0.6172 (0.4137)	MaskLoss 1.0507 (0.7787)	MaskBCELoss 0.0725 (0.1571)	MaskDICELoss 0.9782 (0.6216)
Epoch: [0][425/500]	Time 76.791 (76.791)	Loss 0.4727 (1.0504)	CeLoss 0.4727 (0.2746)	SegCLSLoss 0.0000 (0.0171)	KLLoss 0.0000 (0.5691)	MaskLoss 0.0000 (0.9510)	MaskBCELoss 0.0000 (0.1945)	MaskDICELoss 0.0000 (0.7565)
Epoch: [0][426/500]	Time 57.177 (57.177)	Loss 1.1825 (1.2112)	CeLoss 0.2246 (0.5266)	SegCLSLoss 0.0129 (0.0155)	KLLoss 0.7031 (0.4211)	MaskLoss 0.9849 (0.7799)	MaskBCELoss 0.3057 (0.1913)	MaskDICELoss 0.6792 (0.5886)
Epoch: [0][427/500]	Time 52.720 (52.720)	Loss 1.3662 (1.0636)	CeLoss 0.2793 (0.6213)	SegCLSLoss 0.0150 (0.0081)	KLLoss 0.6992 (0.3449)	MaskLoss 1.0716 (0.5006)	MaskBCELoss 0.3622 (0.1254)	MaskDICELoss 0.7093 (0.3752)
Epoch: [0][428/500]	Time 63.141 (63.141)	Loss 0.9444 (0.8306)	CeLoss 0.2432 (0.5400)	SegCLSLoss 0.0271 (0.0083)	KLLoss 0.5156 (0.2297)	MaskLoss 1.0929 (0.4098)	MaskBCELoss 0.0947 (0.0545)	MaskDICELoss 0.9982 (0.3554)
Epoch: [0][429/500]	Time 56.598 (56.598)	Loss 0.0547 (1.1409)	CeLoss 0.0547 (0.4119)	SegCLSLoss 0.0000 (0.0122)	KLLoss 0.0000 (0.3809)	MaskLoss 0.0000 (0.7491)	MaskBCELoss 0.0000 (0.2324)	MaskDICELoss 0.0000 (0.5167)
[2025-03-02 01:39:57,589] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=0, lr=[0.00027985714285714284], mom=[(0.9, 0.95)]
[2025-03-02 01:39:57,590] [INFO] [timer.py:215:stop] epoch=0/micro_step=4300/global_step=430, RunningAvgSamplesPerSec=0.36444076396509845, CurrSamplesPerSec=0.14963490000409307, MemAllocated=31.23GB, MaxMemAllocated=36.85GB
Epoch: [0][430/500]	Time 66.831 (66.831)	Loss 1.0933 (1.1432)	CeLoss 0.2441 (0.3299)	SegCLSLoss 0.0128 (0.0181)	KLLoss 0.7656 (0.5430)	MaskLoss 1.0583 (0.9602)	MaskBCELoss 0.2094 (0.2162)	MaskDICELoss 0.8489 (0.7440)
Epoch: [0][431/500]	Time 57.866 (57.866)	Loss 0.9967 (1.1471)	CeLoss 0.2715 (0.6397)	SegCLSLoss 0.0118 (0.0132)	KLLoss 0.7227 (0.3430)	MaskLoss 0.9000 (0.6177)	MaskBCELoss 0.1796 (0.1279)	MaskDICELoss 0.7204 (0.4898)
Epoch: [0][432/500]	Time 61.176 (61.176)	Loss 1.3047 (1.0866)	CeLoss 1.3047 (0.4608)	SegCLSLoss 0.0000 (0.0232)	KLLoss 0.0000 (0.3563)	MaskLoss 0.0000 (0.7583)	MaskBCELoss 0.0000 (0.1568)	MaskDICELoss 0.0000 (0.6016)
Epoch: [0][433/500]	Time 61.632 (61.632)	Loss 0.8179 (1.1013)	CeLoss 0.1699 (0.4332)	SegCLSLoss 0.0232 (0.0168)	KLLoss 0.5938 (0.4826)	MaskLoss 1.0086 (0.8507)	MaskBCELoss 0.0880 (0.1562)	MaskDICELoss 0.9207 (0.6944)
Epoch: [0][434/500]	Time 58.438 (58.438)	Loss 1.0547 (0.9592)	CeLoss 1.0547 (0.4653)	SegCLSLoss 0.0000 (0.0130)	KLLoss 0.0000 (0.4266)	MaskLoss 0.0000 (0.6567)	MaskBCELoss 0.0000 (0.1061)	MaskDICELoss 0.0000 (0.5506)
Epoch: [0][435/500]	Time 56.226 (56.226)	Loss 0.9182 (1.0914)	CeLoss 0.2148 (0.4817)	SegCLSLoss 0.0305 (0.0129)	KLLoss 0.4883 (0.3578)	MaskLoss 1.0309 (0.6661)	MaskBCELoss 0.1149 (0.1803)	MaskDICELoss 0.9160 (0.4859)
Epoch: [0][436/500]	Time 56.081 (56.081)	Loss 1.3359 (1.2050)	CeLoss 1.3359 (0.6238)	SegCLSLoss 0.0000 (0.0137)	KLLoss 0.0000 (0.3617)	MaskLoss 0.0000 (0.6531)	MaskBCELoss 0.0000 (0.1651)	MaskDICELoss 0.0000 (0.4879)
Epoch: [0][437/500]	Time 63.483 (63.483)	Loss 0.9569 (1.0671)	CeLoss 0.2871 (0.4880)	SegCLSLoss 0.0157 (0.0167)	KLLoss 0.6953 (0.4104)	MaskLoss 1.0376 (0.7449)	MaskBCELoss 0.0954 (0.1322)	MaskDICELoss 0.9422 (0.6127)
Epoch: [0][438/500]	Time 72.117 (72.117)	Loss 0.9922 (1.0690)	CeLoss 0.9922 (0.3887)	SegCLSLoss 0.0000 (0.0170)	KLLoss 0.0000 (0.4627)	MaskLoss 0.0000 (0.8382)	MaskBCELoss 0.0000 (0.1684)	MaskDICELoss 0.0000 (0.6698)
Epoch: [0][439/500]	Time 68.444 (68.444)	Loss 1.1513 (1.1177)	CeLoss 0.2324 (0.4586)	SegCLSLoss 0.0146 (0.0124)	KLLoss 0.6875 (0.4375)	MaskLoss 0.8559 (0.7370)	MaskBCELoss 0.3227 (0.1897)	MaskDICELoss 0.5331 (0.5473)
[2025-03-02 01:50:19,087] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=0, lr=[0.0002792448979591836], mom=[(0.9, 0.95)]
[2025-03-02 01:50:19,088] [INFO] [timer.py:215:stop] epoch=0/micro_step=4400/global_step=440, RunningAvgSamplesPerSec=0.3542112316516904, CurrSamplesPerSec=0.15143750223064378, MemAllocated=31.24GB, MaxMemAllocated=36.85GB
Epoch: [0][440/500]	Time 66.036 (66.036)	Loss 1.5135 (1.1653)	CeLoss 0.1934 (0.4425)	SegCLSLoss 0.0233 (0.0170)	KLLoss 0.5273 (0.4520)	MaskLoss 1.2813 (0.8691)	MaskBCELoss 0.4452 (0.1865)	MaskDICELoss 0.8361 (0.6826)
Epoch: [0][441/500]	Time 65.819 (65.819)	Loss 0.0752 (1.0334)	CeLoss 0.0752 (0.3934)	SegCLSLoss 0.0000 (0.0127)	KLLoss 0.0000 (0.4098)	MaskLoss 0.0000 (0.7509)	MaskBCELoss 0.0000 (0.1722)	MaskDICELoss 0.0000 (0.5787)
Epoch: [0][442/500]	Time 57.760 (57.760)	Loss 1.2344 (1.0663)	CeLoss 1.2344 (0.4492)	SegCLSLoss 0.0000 (0.0188)	KLLoss 0.0000 (0.3600)	MaskLoss 0.0000 (0.7856)	MaskBCELoss 0.0000 (0.1433)	MaskDICELoss 0.0000 (0.6423)
Epoch: [0][443/500]	Time 66.101 (66.101)	Loss 1.2344 (1.1794)	CeLoss 1.2344 (0.4617)	SegCLSLoss 0.0000 (0.0161)	KLLoss 0.0000 (0.4828)	MaskLoss 0.0000 (0.8630)	MaskBCELoss 0.0000 (0.1854)	MaskDICELoss 0.0000 (0.6777)
Epoch: [0][444/500]	Time 65.859 (65.859)	Loss 1.3604 (1.2073)	CeLoss 0.1904 (0.3534)	SegCLSLoss 0.0135 (0.0170)	KLLoss 0.7383 (0.5410)	MaskLoss 1.2262 (0.9801)	MaskBCELoss 0.3667 (0.2369)	MaskDICELoss 0.8595 (0.7432)
Epoch: [0][445/500]	Time 61.649 (61.649)	Loss 0.7383 (1.1450)	CeLoss 0.7383 (0.6049)	SegCLSLoss 0.0000 (0.0103)	KLLoss 0.0000 (0.2891)	MaskLoss 0.0000 (0.5990)	MaskBCELoss 0.0000 (0.1570)	MaskDICELoss 0.0000 (0.4420)
Epoch: [0][446/500]	Time 68.372 (68.372)	Loss 0.7923 (1.1028)	CeLoss 0.1934 (0.3230)	SegCLSLoss 0.0220 (0.0167)	KLLoss 0.5117 (0.5371)	MaskLoss 0.9995 (0.9645)	MaskBCELoss 0.0590 (0.1928)	MaskDICELoss 0.9406 (0.7717)
Epoch: [0][447/500]	Time 64.155 (64.155)	Loss 1.1875 (1.0836)	CeLoss 1.1875 (0.4713)	SegCLSLoss 0.0000 (0.0155)	KLLoss 0.0000 (0.3996)	MaskLoss 0.0000 (0.7364)	MaskBCELoss 0.0000 (0.1576)	MaskDICELoss 0.0000 (0.5788)
Epoch: [0][448/500]	Time 66.672 (66.672)	Loss 1.2936 (1.1214)	CeLoss 0.0923 (0.4033)	SegCLSLoss 0.0537 (0.0228)	KLLoss 0.4297 (0.4381)	MaskLoss 1.1680 (0.8530)	MaskBCELoss 0.3936 (0.1867)	MaskDICELoss 0.7743 (0.6663)
Epoch: [0][449/500]	Time 64.585 (64.585)	Loss 1.1587 (0.8841)	CeLoss 0.2471 (0.2546)	SegCLSLoss 0.0143 (0.0166)	KLLoss 0.6875 (0.4629)	MaskLoss 0.9649 (0.8374)	MaskBCELoss 0.2816 (0.1350)	MaskDICELoss 0.6834 (0.7024)
[2025-03-02 02:01:03,031] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=0, lr=[0.00027863265306122447], mom=[(0.9, 0.95)]
[2025-03-02 02:01:03,032] [INFO] [timer.py:215:stop] epoch=0/micro_step=4500/global_step=450, RunningAvgSamplesPerSec=0.34436553671462317, CurrSamplesPerSec=0.15880729869061475, MemAllocated=30.78GB, MaxMemAllocated=36.85GB
Epoch: [0][450/500]	Time 62.971 (62.971)	Loss 0.9727 (1.0726)	CeLoss 0.9727 (0.4485)	SegCLSLoss 0.0000 (0.0144)	KLLoss 0.0000 (0.4914)	MaskLoss 0.0000 (0.7873)	MaskBCELoss 0.0000 (0.1489)	MaskDICELoss 0.0000 (0.6385)
Epoch: [0][451/500]	Time 21.776 (21.776)	Loss 0.8490 (1.1390)	CeLoss 0.2305 (0.4438)	SegCLSLoss 0.0298 (0.0149)	KLLoss 0.5742 (0.4934)	MaskLoss 1.0352 (0.8478)	MaskBCELoss 0.0575 (0.1759)	MaskDICELoss 0.9776 (0.6719)
Epoch: [0][452/500]	Time  8.214 ( 8.214)	Loss 0.8802 (1.0729)	CeLoss 0.3613 (0.4323)	SegCLSLoss 0.0131 (0.0110)	KLLoss 0.6953 (0.4496)	MaskLoss 0.5934 (0.6444)	MaskBCELoss 0.1442 (0.2086)	MaskDICELoss 0.4492 (0.4358)
Epoch: [0][453/500]	Time  9.090 ( 9.090)	Loss 0.8671 (1.0939)	CeLoss 0.2139 (0.3215)	SegCLSLoss 0.0303 (0.0195)	KLLoss 0.6016 (0.4576)	MaskLoss 0.8850 (0.8957)	MaskBCELoss 0.1307 (0.2099)	MaskDICELoss 0.7542 (0.6858)
Epoch: [0][454/500]	Time  9.232 ( 9.232)	Loss 0.8264 (1.1925)	CeLoss 0.3008 (0.3729)	SegCLSLoss 0.0188 (0.0138)	KLLoss 0.6289 (0.5852)	MaskLoss 0.6206 (0.8567)	MaskBCELoss 0.1370 (0.2561)	MaskDICELoss 0.4836 (0.6007)
Epoch: [0][455/500]	Time  9.017 ( 9.017)	Loss 0.9122 (1.0829)	CeLoss 0.3438 (0.4677)	SegCLSLoss 0.0150 (0.0118)	KLLoss 0.6914 (0.5203)	MaskLoss 0.8169 (0.7818)	MaskBCELoss 0.1015 (0.1456)	MaskDICELoss 0.7154 (0.6362)
Epoch: [0][456/500]	Time  9.507 ( 9.507)	Loss 1.1680 (1.1898)	CeLoss 0.2969 (0.3264)	SegCLSLoss 0.0131 (0.0157)	KLLoss 0.6562 (0.5477)	MaskLoss 0.9586 (0.9690)	MaskBCELoss 0.2573 (0.2473)	MaskDICELoss 0.7013 (0.7218)
Epoch: [0][457/500]	Time  8.584 ( 8.584)	Loss 1.4609 (1.1593)	CeLoss 1.4609 (0.6370)	SegCLSLoss 0.0000 (0.0106)	KLLoss 0.0000 (0.2855)	MaskLoss 0.0000 (0.5676)	MaskBCELoss 0.0000 (0.1555)	MaskDICELoss 0.0000 (0.4121)
Epoch: [0][458/500]	Time  8.031 ( 8.031)	Loss 0.0864 (1.2242)	CeLoss 0.0864 (0.3899)	SegCLSLoss 0.0000 (0.0135)	KLLoss 0.0000 (0.3824)	MaskLoss 0.0000 (0.8692)	MaskBCELoss 0.0000 (0.2619)	MaskDICELoss 0.0000 (0.6072)
Epoch: [0][459/500]	Time  9.118 ( 9.118)	Loss 1.2902 (1.0556)	CeLoss 0.2090 (0.4330)	SegCLSLoss 0.0280 (0.0111)	KLLoss 0.4805 (0.3469)	MaskLoss 1.1851 (0.6872)	MaskBCELoss 0.3167 (0.1823)	MaskDICELoss 0.8684 (0.5048)
[2025-03-02 02:02:43,937] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=0, lr=[0.0002780204081632653], mom=[(0.9, 0.95)]
[2025-03-02 02:02:43,938] [INFO] [timer.py:215:stop] epoch=0/micro_step=4600/global_step=460, RunningAvgSamplesPerSec=0.34934303710424963, CurrSamplesPerSec=1.1999164181755222, MemAllocated=31.23GB, MaxMemAllocated=36.85GB
Epoch: [0][460/500]	Time  8.335 ( 8.335)	Loss 1.0493 (1.3068)	CeLoss 0.2617 (0.4604)	SegCLSLoss 0.0136 (0.0149)	KLLoss 0.6367 (0.5006)	MaskLoss 0.8865 (0.8517)	MaskBCELoss 0.2257 (0.2755)	MaskDICELoss 0.6608 (0.5762)
Epoch: [0][461/500]	Time  9.611 ( 9.611)	Loss 0.6847 (1.1100)	CeLoss 0.2373 (0.2938)	SegCLSLoss 0.0135 (0.0187)	KLLoss 0.6680 (0.5398)	MaskLoss 0.5528 (0.9675)	MaskBCELoss 0.1094 (0.2154)	MaskDICELoss 0.4433 (0.7521)
Epoch: [0][462/500]	Time  9.480 ( 9.480)	Loss 1.4351 (1.1144)	CeLoss 0.2051 (0.3215)	SegCLSLoss 0.0177 (0.0137)	KLLoss 0.6172 (0.5664)	MaskLoss 1.2750 (0.9619)	MaskBCELoss 0.3891 (0.2035)	MaskDICELoss 0.8859 (0.7584)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 640, in <module>
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 387, in main
[rank0]:     )
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 497, in train
[rank0]:     ce_loss = output_dict["ce_loss"]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1861, in backward
[rank0]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1890, in backward
[rank0]:     buf_1 = torch.empty(int(self.reduce_bucket_size),
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 954.00 MiB. GPU