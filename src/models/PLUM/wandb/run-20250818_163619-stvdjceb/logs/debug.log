2025-08-18 16:36:19,332 INFO    MainThread:77925 [wandb_setup.py:_flush():76] Current SDK version is 0.16.0
2025-08-18 16:36:19,332 INFO    MainThread:77925 [wandb_setup.py:_flush():76] Configure stats pid to 77925
2025-08-18 16:36:19,332 INFO    MainThread:77925 [wandb_setup.py:_flush():76] Loading settings from /home/jk100/.config/wandb/settings
2025-08-18 16:36:19,332 INFO    MainThread:77925 [wandb_setup.py:_flush():76] Loading settings from /shared/nas2/jk100/partonomy_private/src/models/PLUM/wandb/settings
2025-08-18 16:36:19,332 INFO    MainThread:77925 [wandb_setup.py:_flush():76] Loading settings from environment variables: {}
2025-08-18 16:36:19,332 INFO    MainThread:77925 [wandb_setup.py:_flush():76] Applying setup settings: {'_disable_service': False}
2025-08-18 16:36:19,332 INFO    MainThread:77925 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program_relpath': 'src/models/PLUM/pixar_train_ds.py', 'program_abspath': '/shared/nas2/jk100/partonomy_private/src/models/PLUM/pixar_train_ds.py', 'program': '/shared/nas2/jk100/partonomy_private/src/models/PLUM/pixar_train_ds.py'}
2025-08-18 16:36:19,332 INFO    MainThread:77925 [wandb_init.py:_log_setup():524] Logging user logs to /shared/nas2/jk100/partonomy_private/src/models/PLUM/wandb/run-20250818_163619-stvdjceb/logs/debug.log
2025-08-18 16:36:19,332 INFO    MainThread:77925 [wandb_init.py:_log_setup():525] Logging internal logs to /shared/nas2/jk100/partonomy_private/src/models/PLUM/wandb/run-20250818_163619-stvdjceb/logs/debug-internal.log
2025-08-18 16:36:19,332 INFO    MainThread:77925 [wandb_init.py:init():564] calling init triggers
2025-08-18 16:36:19,332 INFO    MainThread:77925 [wandb_init.py:init():571] wandb.init called with sweep_config: {}
config: {'local_rank': 0, 'version': 'liuhaotian/llava-llama-2-13b-chat-lightning-preview', 'zero_shot_ckpt_path': '', 'vis_save_path': './vis_output', 'precision': 'fp32', 'image_size': 1024, 'model_max_length': 1536, 'lora_r': 8, 'vision_tower': 'openai/clip-vit-large-patch14', 'load_in_8bit': False, 'load_in_4bit': False, 'dataset': 'sem_seg||refer_seg||vqa||reason_seg', 'sample_rates': '9,3,3,1', 'sem_seg_data': 'ade20k||cocostuff||pascal_part||paco_lvis||mapillary', 'refer_seg_data': 'refclef||refcoco||refcoco+||refcocog', 'explanatory_seg_data': False, 'wandb': False, 'wandb_project_name': 'plum-training', 'vqa_data': 'llava_instruct_150k', 'reason_seg_data': 'ReasonSeg|train', 'val_dataset': 'ade20k|validation', 'dataset_dir': './dataset', 'pixar_data': 'oven||infoseek||aokvqa||okvqa||textvqa||webqa', 'sample_one_question_per_image': False, 'random_seed': 42, 'metrics': Namespace(iou_evaluator_config=Namespace(matching_strategy=<MatchingStrategy.PAIRED: 'paired'>, reduction=<Reduction.MACRO: 'macro'>)), 'predictions_path': '/shared/nas2/jk100/partonomy_private/results/predictions.json', 'output_predictions': False, 'log_base_dir': './runs', 'exp_name': 'plum', 'epochs': 1, 'steps_per_epoch': 1, 'batch_size': 1, 'grad_accumulation_steps': 10, 'val_batch_size': 1, 'workers': 0, 'lr': 0.0003, 'ce_loss_weight': 1.0, 'dice_type': 'dice', 'dice_loss_weight': 0.5, 'dice_scale_factor': 1000.0, 'bce_loss_weight': 2.0, 'kld_loss_weight': 0.5, 'kld_sigma': 1.0, 'seg_cls_loss_weight': 0.5, 'seg_cls_loss_per_cls_weight': [0.1, 1.0, 1.0], 'use_teacher_ref': False, 'use_bidir_bio': False, 'focal_tversky_alpha': 0.7, 'focal_tversky_beta': 0.3, 'limit_batches': None, 'use_feedback_loop': False, 'bidir_nhead': 8, 'bidir_dim_feedforward': 2048, 'lora_alpha': 16, 'lora_dropout': 0.05, 'lora_target_modules': 'q_proj,v_proj', 'explanatory': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'num_classes_per_sample': 5, 'exclude_val': False, 'no_eval': False, 'eval_only': False, 'vision_pretrained': 'PATH_TO_SAM_ViT-H', 'out_dim': 256, 'resume': '', 'print_freq': 1, 'start_epoch': 0, 'gradient_checkpointing': True, 'train_mask_decoder': True, 'train_mask_prompt_encoder': False, 'use_mm_start_end': True, 'auto_resume': False, 'log_dir': './runs/plum', 'conv_type': 'llava_v1', 'debug': True, 'distributed': False}
2025-08-18 16:36:19,332 INFO    MainThread:77925 [wandb_init.py:init():614] starting backend
2025-08-18 16:36:19,332 INFO    MainThread:77925 [wandb_init.py:init():618] setting up manager
2025-08-18 16:36:19,333 INFO    MainThread:77925 [backend.py:_multiprocessing_setup():105] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2025-08-18 16:36:19,335 INFO    MainThread:77925 [wandb_init.py:init():624] backend started and connected
2025-08-18 16:36:19,337 INFO    MainThread:77925 [wandb_init.py:init():716] updated telemetry
2025-08-18 16:36:19,349 INFO    MainThread:77925 [wandb_init.py:init():749] communicating run to backend with 90.0 second timeout
2025-08-18 16:36:19,769 INFO    MainThread:77925 [wandb_run.py:_on_init():2254] communicating current version
2025-08-18 16:36:19,854 INFO    MainThread:77925 [wandb_run.py:_on_init():2263] got version response upgrade_message: "wandb version 0.21.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"

2025-08-18 16:36:19,855 INFO    MainThread:77925 [wandb_init.py:init():800] starting run threads in backend
2025-08-18 16:36:24,857 INFO    MainThread:77925 [wandb_run.py:_console_start():2233] atexit reg
2025-08-18 16:36:24,858 INFO    MainThread:77925 [wandb_run.py:_redirect():2088] redirect: wrap_raw
2025-08-18 16:36:24,858 INFO    MainThread:77925 [wandb_run.py:_redirect():2153] Wrapping output streams.
2025-08-18 16:36:24,858 INFO    MainThread:77925 [wandb_run.py:_redirect():2178] Redirects installed.
2025-08-18 16:36:24,858 INFO    MainThread:77925 [wandb_init.py:init():841] run started, returning control to user process
2025-08-18 16:38:53,785 WARNING MsgRouterThr:77925 [router.py:message_loop():77] message_loop has been closed
