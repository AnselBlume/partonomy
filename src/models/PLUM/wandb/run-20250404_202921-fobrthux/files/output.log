You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")


Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:16<00:00,  5.46s/it]
Some weights of PLUMForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-llama-2-13b-chat-lightning-preview and are newly initialized: ['bio_encoder.encoder.layers.0.self_attn.in_proj_weight', 'bio_encoder.encoder.layers.0.self_attn.out_proj.bias', 'bio_encoder.encoder.layers.0.norm2.weight', 'bio_encoder.encoder.layers.0.self_attn.in_proj_bias', 'bio_encoder.encoder.layers.0.linear1.weight', 'bio_encoder.encoder.layers.0.norm1.bias', 'bio_encoder.encoder.layers.0.linear1.bias', 'bio_encoder.encoder.layers.0.self_attn.out_proj.weight', 'bio_encoder.encoder.layers.0.norm2.bias', 'bio_encoder.encoder.layers.0.linear2.weight', 'bio_encoder.encoder.layers.0.norm1.weight', 'bio_encoder.encoder.layers.0.linear2.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 6,553,600 || all params: 14,151,578,931 || trainable%: 0.0463100268313092
n:  base_model.model.model.embed_tokens.weight p.shape:  torch.Size([32002, 5120])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_token.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.mask_decoder.mask_tokens.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.weight p.shape:  torch.Size([256, 64, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.weight p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.weight p.shape:  torch.Size([64, 32, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.bias p.shape:  torch.Size([4])
n:  base_model.model.model.text_hidden_fcs.0.0.weight p.shape:  torch.Size([5120, 5120])
n:  base_model.model.model.text_hidden_fcs.0.0.bias p.shape:  torch.Size([5120])
n:  base_model.model.model.text_hidden_fcs.0.2.weight p.shape:  torch.Size([256, 5120])
n:  base_model.model.model.text_hidden_fcs.0.2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.token_to_mask_fcs.0.0.weight p.shape:  torch.Size([5120, 5120])
n:  base_model.model.model.token_to_mask_fcs.0.0.bias p.shape:  torch.Size([5120])
n:  base_model.model.model.token_to_mask_fcs.0.2.weight p.shape:  torch.Size([3, 5120])
n:  base_model.model.model.token_to_mask_fcs.0.2.bias p.shape:  torch.Size([3])
n:  base_model.model.lm_head.weight p.shape:  torch.Size([32002, 5120])
n:  base_model.model.bio_encoder.encoder.layers.0.self_attn.in_proj_weight p.shape:  torch.Size([15360, 5120])
n:  base_model.model.bio_encoder.encoder.layers.0.self_attn.in_proj_bias p.shape:  torch.Size([15360])
n:  base_model.model.bio_encoder.encoder.layers.0.self_attn.out_proj.weight p.shape:  torch.Size([5120, 5120])
n:  base_model.model.bio_encoder.encoder.layers.0.self_attn.out_proj.bias p.shape:  torch.Size([5120])
n:  base_model.model.bio_encoder.encoder.layers.0.linear1.weight p.shape:  torch.Size([2048, 5120])
n:  base_model.model.bio_encoder.encoder.layers.0.linear1.bias p.shape:  torch.Size([2048])
n:  base_model.model.bio_encoder.encoder.layers.0.linear2.weight p.shape:  torch.Size([5120, 2048])
n:  base_model.model.bio_encoder.encoder.layers.0.linear2.bias p.shape:  torch.Size([5120])
n:  base_model.model.bio_encoder.encoder.layers.0.norm1.weight p.shape:  torch.Size([5120])
n:  base_model.model.bio_encoder.encoder.layers.0.norm1.bias p.shape:  torch.Size([5120])
n:  base_model.model.bio_encoder.encoder.layers.0.norm2.weight p.shape:  torch.Size([5120])
n:  base_model.model.bio_encoder.encoder.layers.0.norm2.bias p.shape:  torch.Size([5120])
>> (plum_train_ds) loading ExplanatorySegDataset...
>> (plum_train_ds) question_types:  QuestionType.POSITIVE
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/pascal_part/pascal_part_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/pascal_part/pascal_part_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partimagenet/partimagenet_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partimagenet/partimagenet_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/paco_lvis/paco_lvis_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/paco_lvis/paco_lvis_qa_pairs.json
>> (plum_train_ds) question_types:  QuestionType.POSITIVE_WITH_LABEL
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/pascal_part/pascal_part_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/pascal_part/pascal_part_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partimagenet/partimagenet_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partimagenet/partimagenet_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/paco_lvis/paco_lvis_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/paco_lvis/paco_lvis_qa_pairs.json
>> (plum_train_ds) question_types:  QuestionType.NEGATIVE
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/pascal_part/pascal_part_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/pascal_part/pascal_part_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partimagenet/partimagenet_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partimagenet/partimagenet_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/paco_lvis/paco_lvis_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/paco_lvis/paco_lvis_qa_pairs.json
>> (plum_train_ds) question_types:  QuestionType.NEGATIVE_WITH_LABEL
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/pascal_part/pascal_part_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/pascal_part/pascal_part_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partimagenet/partimagenet_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partimagenet/partimagenet_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/paco_lvis/paco_lvis_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/paco_lvis/paco_lvis_qa_pairs.json
>> (plum_train_ds) question_types:  QuestionType.DIFFERENCE
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/pascal_part/pascal_part_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/pascal_part/pascal_part_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partimagenet/partimagenet_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partimagenet/partimagenet_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/paco_lvis/paco_lvis_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/paco_lvis/paco_lvis_qa_pairs.json
>> (plum_train_ds) question_types:  QuestionType.DIFFERENCE_WITH_LABEL
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/pascal_part/pascal_part_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/pascal_part/pascal_part_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partimagenet/partimagenet_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/partimagenet/partimagenet_qa_pairs.json
(explanatory_seg_dataset.py) >>> Loading dataset from: /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/paco_lvis/paco_lvis_qa_pairs.json /shared/nas2/blume5/sp25/partonomy/partonomy_private/data/partonomy_descriptors/paco_lvis/paco_lvis_qa_pairs.json
ade20k:  20210
cocostuff:  118287
loading annotations into memory...
Done (t=0.20s)
creating index...
index created!
pascal_part:  4366
loading annotations into memory...
Done (t=4.49s)
creating index...
index created!
paco_lvis:  45790
mapillary:  18000
loading dataset refclef into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refclef/refs(unc).p
creating index...
index created.
DONE (t=1.68s)
dataset refclef (refs unc) (train split) has 17978 images and 99523 annotations.
loading dataset refcoco into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco/refs(unc).p
creating index...
index created.
DONE (t=10.06s)
dataset refcoco (refs unc) (train split) has 16994 images and 196771 annotations.
loading dataset refcoco+ into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco+/refs(unc).p
creating index...
index created.
DONE (t=3.69s)
dataset refcoco+ (refs unc) (train split) has 16992 images and 196737 annotations.
loading dataset refcocog into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcocog/refs(umd).p
creating index...
index created.
DONE (t=3.32s)
dataset refcocog (refs umd) (train split) has 21899 images and 208960 annotations.
vqa_data:  157712
number of reason_seg samples:  239
len(self.img_to_explanation):  239
Training with 20000 examples and validating with 200 examples.
[2025-04-04 20:34:48,204] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5, git-hash=unknown, git-branch=unknown
[2025-04-04 20:34:48,204] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2025-04-04 20:34:48,204] [INFO] [comm.py:594:init_distributed] cdb=None
[2025-04-04 20:34:48,204] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-04 20:35:23,336] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Time to load fused_adam op: 0.8708095550537109 seconds
[2025-04-04 20:35:33,396] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-04-04 20:35:33,585] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-04 20:35:33,585] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-04-04 20:35:33,585] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-04-04 20:35:33,586] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500000000
[2025-04-04 20:35:33,586] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500000000
[2025-04-04 20:35:33,586] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2025-04-04 20:35:33,586] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Loading extension module fused_adam...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/tensor/python_tensor.cpp:78.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
Rank: 0 partition count [1] and sizes[(517955048, False)]
[2025-04-04 20:35:43,372] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2025-04-04 20:35:43,373] [INFO] [utils.py:786:see_memory_usage] MA 28.49 GB         Max_MA 29.45 GB         CA 29.59 GB         Max_CA 30 GB
[2025-04-04 20:35:43,374] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 105.44 GB, percent = 10.5%
[2025-04-04 20:35:51,611] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2025-04-04 20:35:51,612] [INFO] [utils.py:786:see_memory_usage] MA 32.35 GB         Max_MA 34.28 GB         CA 35.38 GB         Max_CA 35 GB
[2025-04-04 20:35:51,612] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 105.32 GB, percent = 10.5%
[2025-04-04 20:35:51,612] [INFO] [stage_1_and_2.py:488:__init__] optimizer state initialized
[2025-04-04 20:35:59,390] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2025-04-04 20:35:59,391] [INFO] [utils.py:786:see_memory_usage] MA 32.35 GB         Max_MA 32.35 GB         CA 35.38 GB         Max_CA 35 GB
[2025-04-04 20:35:59,391] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 105.31 GB, percent = 10.5%
[2025-04-04 20:35:59,398] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-04-04 20:35:59,398] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2025-04-04 20:35:59,398] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7f31785efb50>
[2025-04-04 20:35:59,398] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
[2025-04-04 20:35:59,401] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2025-04-04 20:35:59,401] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-04-04 20:35:59,401] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-04-04 20:35:59,401] [INFO] [config.py:964:print]   amp_enabled .................. False
[2025-04-04 20:35:59,401] [INFO] [config.py:964:print]   amp_params ................... False
[2025-04-04 20:35:59,402] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-04 20:35:59,402] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2025-04-04 20:35:59,402] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2025-04-04 20:35:59,402] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2025-04-04 20:35:59,402] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2025-04-04 20:35:59,402] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f290bbbc070>
[2025-04-04 20:35:59,402] [INFO] [config.py:964:print]   communication_data_type ...... None
[2025-04-04 20:35:59,402] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-04 20:35:59,402] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2025-04-04 20:35:59,402] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2025-04-04 20:35:59,402] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-04 20:35:59,402] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2025-04-04 20:35:59,402] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2025-04-04 20:35:59,402] [INFO] [config.py:964:print]   disable_allgather ............ False
[2025-04-04 20:35:59,402] [INFO] [config.py:964:print]   dump_state ................... False
[2025-04-04 20:35:59,402] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2025-04-04 20:35:59,403] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2025-04-04 20:35:59,403] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-04 20:35:59,403] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-04 20:35:59,403] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2025-04-04 20:35:59,403] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2025-04-04 20:35:59,403] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2025-04-04 20:35:59,403] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2025-04-04 20:35:59,403] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2025-04-04 20:35:59,403] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2025-04-04 20:35:59,403] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-04-04 20:35:59,403] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2025-04-04 20:35:59,403] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2025-04-04 20:35:59,403] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2025-04-04 20:35:59,403] [INFO] [config.py:964:print]   global_rank .................. 0
[2025-04-04 20:35:59,403] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2025-04-04 20:35:59,403] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 10
[2025-04-04 20:35:59,403] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0
[2025-04-04 20:35:59,403] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2025-04-04 20:35:59,403] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-04 20:35:59,404] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2025-04-04 20:35:59,404] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2025-04-04 20:35:59,404] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2025-04-04 20:35:59,404] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2025-04-04 20:35:59,404] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2025-04-04 20:35:59,404] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2025-04-04 20:35:59,404] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-04-04 20:35:59,404] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-04-04 20:35:59,404] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2025-04-04 20:35:59,404] [INFO] [config.py:964:print]   optimizer_name ............... adamw
[2025-04-04 20:35:59,404] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.0003, 'weight_decay': 0.0, 'betas': (0.9, 0.95)}
[2025-04-04 20:35:59,404] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2025-04-04 20:35:59,404] [INFO] [config.py:964:print]   pld_enabled .................. False
[2025-04-04 20:35:59,404] [INFO] [config.py:964:print]   pld_params ................... False
[2025-04-04 20:35:59,404] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2025-04-04 20:35:59,404] [INFO] [config.py:964:print]   scheduler_name ............... WarmupDecayLR
[2025-04-04 20:35:59,404] [INFO] [config.py:964:print]   scheduler_params ............. {'total_num_steps': 25000, 'warmup_min_lr': 0, 'warmup_max_lr': 0.0003, 'warmup_num_steps': 100, 'warmup_type': 'linear'}
[2025-04-04 20:35:59,404] [INFO] [config.py:964:print]   sparse_attention ............. None
[2025-04-04 20:35:59,405] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2025-04-04 20:35:59,405] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2025-04-04 20:35:59,405] [INFO] [config.py:964:print]   train_batch_size ............. 40
[2025-04-04 20:35:59,405] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  4
[2025-04-04 20:35:59,405] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2025-04-04 20:35:59,405] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2025-04-04 20:35:59,405] [INFO] [config.py:964:print]   world_size ................... 1
[2025-04-04 20:35:59,405] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2025-04-04 20:35:59,405] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2025-04-04 20:35:59,405] [INFO] [config.py:964:print]   zero_enabled ................. True
[2025-04-04 20:35:59,405] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-04 20:35:59,405] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2025-04-04 20:35:59,405] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 4,
    "gradient_accumulation_steps": 10,
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 0.0003,
            "weight_decay": 0.0,
            "betas": [0.9, 0.95]
        }
    },
    "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "total_num_steps": 2.500000e+04,
            "warmup_min_lr": 0,
            "warmup_max_lr": 0.0003,
            "warmup_num_steps": 100,
            "warmup_type": "linear"
        }
    },
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "gradient_clipping": 1.0,
    "zero_optimization": {
        "stage": 2,
        "contiguous_gradients": true,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 5.000000e+08,
        "allgather_bucket_size": 5.000000e+08
    }
}
(train) >> AFTER DEEPSPEED
>> (train) Auto-resume from:  ./runs/plum-13b_kld_0_dice_4_v1_partonomy/plum-13b_kld_0_dice_4_v1_partonomy_accum_10_maxlen512_epochs50_segloss_2_bce_loss_2_kld_loss_0_dice_loss_4_bidir_bio_exp_seg_ckpt_model
>> (train) resume exists:  False
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)
  return F.conv2d(input, weight, bias, self.stride,
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch: [0][  1/500]	Time 34.497 (34.497)	Loss 0.9265 (0.9481)	CeLoss 0.2158 (0.2044)	SegCLSLoss 0.1309 (0.1318)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1347 (0.1458)	MaskBCELoss 0.0437 (0.0514)	MaskDICELoss 0.0910 (0.0944)
Epoch: [0][  2/500]	Time 32.795 (32.795)	Loss 0.9079 (0.9461)	CeLoss 0.2236 (0.2298)	SegCLSLoss 0.1328 (0.1331)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1209 (0.1358)	MaskBCELoss 0.0316 (0.0468)	MaskDICELoss 0.0893 (0.0890)
Epoch: [0][  3/500]	Time 27.181 (27.181)	Loss 0.8943 (0.9257)	CeLoss 0.1543 (0.1875)	SegCLSLoss 0.1309 (0.1310)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1399 (0.1431)	MaskBCELoss 0.0405 (0.0482)	MaskDICELoss 0.0994 (0.0949)
Epoch: [0][  4/500]	Time 31.863 (31.863)	Loss 1.0988 (1.0175)	CeLoss 0.3398 (0.2561)	SegCLSLoss 0.1387 (0.1333)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1505 (0.1544)	MaskBCELoss 0.0594 (0.0613)	MaskDICELoss 0.0911 (0.0931)
Epoch: [0][  5/500]	Time 32.687 (32.687)	Loss 0.9246 (0.9561)	CeLoss 0.1855 (0.2104)	SegCLSLoss 0.1289 (0.1312)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1523 (0.1468)	MaskBCELoss 0.0641 (0.0518)	MaskDICELoss 0.0882 (0.0950)
Epoch: [0][  6/500]	Time 25.821 (25.821)	Loss 1.0250 (1.0321)	CeLoss 0.3008 (0.2799)	SegCLSLoss 0.1367 (0.1347)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1295 (0.1457)	MaskBCELoss 0.0339 (0.0503)	MaskDICELoss 0.0955 (0.0955)
Epoch: [0][  7/500]	Time 21.185 (21.185)	Loss 0.9500 (1.0030)	CeLoss 0.2051 (0.2214)	SegCLSLoss 0.1318 (0.1325)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1415 (0.1619)	MaskBCELoss 0.0424 (0.0651)	MaskDICELoss 0.0991 (0.0967)
Epoch: [0][  8/500]	Time 20.715 (20.715)	Loss 0.9331 (0.9429)	CeLoss 0.1807 (0.1995)	SegCLSLoss 0.1328 (0.1316)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1478 (0.1434)	MaskBCELoss 0.0524 (0.0468)	MaskDICELoss 0.0954 (0.0966)
Epoch: [0][  9/500]	Time 21.184 (21.184)	Loss 1.0204 (0.9917)	CeLoss 0.2773 (0.2538)	SegCLSLoss 0.1367 (0.1333)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1396 (0.1411)	MaskBCELoss 0.0441 (0.0468)	MaskDICELoss 0.0956 (0.0944)
Epoch: [0][ 10/500]	Time 22.063 (22.063)	Loss 0.9035 (1.0081)	CeLoss 0.2314 (0.2340)	SegCLSLoss 0.1328 (0.1321)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1209 (0.1603)	MaskBCELoss 0.0385 (0.0656)	MaskDICELoss 0.0824 (0.0947)
Epoch: [0][ 11/500]	Time 20.396 (20.396)	Loss 0.7456 (0.8271)	CeLoss 0.1187 (0.2281)	SegCLSLoss 0.0918 (0.0796)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1243 (0.1257)	MaskBCELoss 0.0274 (0.0316)	MaskDICELoss 0.0969 (0.0942)
Epoch: [0][ 12/500]	Time 21.328 (21.328)	Loss 0.7964 (0.8275)	CeLoss 0.2266 (0.2400)	SegCLSLoss 0.0742 (0.0775)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1180 (0.1206)	MaskBCELoss 0.0254 (0.0247)	MaskDICELoss 0.0927 (0.0959)
Epoch: [0][ 13/500]	Time 23.694 (23.694)	Loss 0.8758 (0.8140)	CeLoss 0.2930 (0.2166)	SegCLSLoss 0.0815 (0.0794)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1106 (0.1263)	MaskBCELoss 0.0113 (0.0333)	MaskDICELoss 0.0992 (0.0930)
Epoch: [0][ 14/500]	Time 19.890 (19.890)	Loss 0.9212 (0.8842)	CeLoss 0.3320 (0.2742)	SegCLSLoss 0.0811 (0.0804)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1196 (0.1274)	MaskBCELoss 0.0254 (0.0302)	MaskDICELoss 0.0941 (0.0972)
Epoch: [0][ 15/500]	Time 22.292 (22.292)	Loss 0.7702 (0.8237)	CeLoss 0.2012 (0.2340)	SegCLSLoss 0.0732 (0.0781)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1137 (0.1215)	MaskBCELoss 0.0166 (0.0263)	MaskDICELoss 0.0972 (0.0953)
Epoch: [0][ 16/500]	Time 23.730 (23.730)	Loss 0.7240 (0.8020)	CeLoss 0.1631 (0.1931)	SegCLSLoss 0.0747 (0.0791)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1061 (0.1294)	MaskBCELoss 0.0064 (0.0334)	MaskDICELoss 0.0997 (0.0960)
Epoch: [0][ 17/500]	Time 22.772 (22.772)	Loss 0.7798 (0.8125)	CeLoss 0.1660 (0.2186)	SegCLSLoss 0.0732 (0.0783)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1353 (0.1223)	MaskBCELoss 0.0370 (0.0257)	MaskDICELoss 0.0983 (0.0966)
Epoch: [0][ 18/500]	Time 22.335 (22.335)	Loss 0.9264 (0.8570)	CeLoss 0.2080 (0.2549)	SegCLSLoss 0.0889 (0.0808)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1716 (0.1238)	MaskBCELoss 0.0723 (0.0274)	MaskDICELoss 0.0994 (0.0964)
Epoch: [0][ 19/500]	Time 23.492 (23.492)	Loss 0.9046 (0.8452)	CeLoss 0.3262 (0.2537)	SegCLSLoss 0.0791 (0.0786)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1110 (0.1216)	MaskBCELoss 0.0118 (0.0258)	MaskDICELoss 0.0991 (0.0958)
Epoch: [0][ 20/500]	Time 21.355 (21.355)	Loss 0.7823 (0.8116)	CeLoss 0.1777 (0.2104)	SegCLSLoss 0.0791 (0.0806)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1256 (0.1243)	MaskBCELoss 0.0280 (0.0285)	MaskDICELoss 0.0976 (0.0958)
Epoch: [0][ 21/500]	Time 23.962 (23.962)	Loss 0.8421 (0.7788)	CeLoss 0.3027 (0.2260)	SegCLSLoss 0.0479 (0.0524)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1219 (0.1248)	MaskBCELoss 0.0220 (0.0257)	MaskDICELoss 0.0999 (0.0991)
Epoch: [0][ 22/500]	Time 20.544 (20.544)	Loss 0.6799 (0.7724)	CeLoss 0.1670 (0.2099)	SegCLSLoss 0.0520 (0.0380)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1041 (0.1436)	MaskBCELoss 0.0041 (0.0441)	MaskDICELoss 0.0999 (0.0995)
Epoch: [0][ 23/500]	Time 25.325 (25.325)	Loss 0.8218 (0.7581)	CeLoss 0.2754 (0.2236)	SegCLSLoss 0.0471 (0.0464)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1270 (0.1213)	MaskBCELoss 0.0274 (0.0216)	MaskDICELoss 0.0996 (0.0997)
Epoch: [0][ 24/500]	Time 21.085 (21.085)	Loss 0.8705 (0.7960)	CeLoss 0.3164 (0.2317)	SegCLSLoss 0.0728 (0.0710)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1040 (0.1125)	MaskBCELoss 0.0041 (0.0137)	MaskDICELoss 0.0999 (0.0987)
Epoch: [0][ 25/500]	Time 19.690 (19.690)	Loss 0.8488 (0.7915)	CeLoss 0.3379 (0.2544)	SegCLSLoss 0.0515 (0.0454)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1075 (0.1240)	MaskBCELoss 0.0109 (0.0246)	MaskDICELoss 0.0966 (0.0994)
Epoch: [0][ 26/500]	Time 23.898 (23.898)	Loss 0.7053 (0.7421)	CeLoss 0.1816 (0.1916)	SegCLSLoss 0.0271 (0.0489)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1373 (0.1271)	MaskBCELoss 0.0399 (0.0279)	MaskDICELoss 0.0974 (0.0992)
Epoch: [0][ 27/500]	Time 20.746 (20.746)	Loss 0.7960 (0.7426)	CeLoss 0.2695 (0.2203)	SegCLSLoss 0.0273 (0.0331)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1359 (0.1292)	MaskBCELoss 0.0363 (0.0306)	MaskDICELoss 0.0996 (0.0986)
Epoch: [0][ 28/500]	Time 23.779 (23.779)	Loss 0.7523 (0.7507)	CeLoss 0.1318 (0.2206)	SegCLSLoss 0.0850 (0.0513)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1257 (0.1143)	MaskBCELoss 0.0260 (0.0146)	MaskDICELoss 0.0997 (0.0997)
Epoch: [0][ 29/500]	Time 26.511 (26.511)	Loss 0.6611 (0.7327)	CeLoss 0.1543 (0.1747)	SegCLSLoss 0.0500 (0.0576)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1041 (0.1222)	MaskBCELoss 0.0041 (0.0228)	MaskDICELoss 0.0999 (0.0993)
Epoch: [0][ 30/500]	Time 25.162 (25.162)	Loss 0.6764 (0.7888)	CeLoss 0.1621 (0.2201)	SegCLSLoss 0.0540 (0.0587)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1040 (0.1268)	MaskBCELoss 0.0042 (0.0280)	MaskDICELoss 0.0998 (0.0988)
Epoch: [0][ 31/500]	Time 22.832 (22.832)	Loss 0.7251 (0.8228)	CeLoss 0.1621 (0.2172)	SegCLSLoss 0.0457 (0.0785)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1395 (0.1261)	MaskBCELoss 0.0430 (0.0280)	MaskDICELoss 0.0965 (0.0982)
Epoch: [0][ 32/500]	Time 24.706 (24.706)	Loss 0.6933 (0.8175)	CeLoss 0.1328 (0.1967)	SegCLSLoss 0.0781 (0.0924)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1022 (0.1189)	MaskBCELoss 0.0023 (0.0199)	MaskDICELoss 0.0999 (0.0989)
Epoch: [0][ 33/500]	Time 19.988 (19.988)	Loss 0.7623 (0.8153)	CeLoss 0.1992 (0.1904)	SegCLSLoss 0.0405 (0.0940)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1414 (0.1203)	MaskBCELoss 0.0423 (0.0224)	MaskDICELoss 0.0991 (0.0979)
Epoch: [0][ 34/500]	Time 22.314 (22.314)	Loss 0.7181 (0.7976)	CeLoss 0.1699 (0.2160)	SegCLSLoss 0.0447 (0.0769)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1311 (0.1155)	MaskBCELoss 0.0328 (0.0169)	MaskDICELoss 0.0983 (0.0986)
Epoch: [0][ 35/500]	Time 23.807 (23.807)	Loss 0.7650 (0.8053)	CeLoss 0.2617 (0.2041)	SegCLSLoss 0.0471 (0.0869)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1098 (0.1160)	MaskBCELoss 0.0145 (0.0182)	MaskDICELoss 0.0953 (0.0978)
Epoch: [0][ 36/500]	Time 23.736 (23.736)	Loss 0.6693 (0.8355)	CeLoss 0.1279 (0.2042)	SegCLSLoss 0.0610 (0.0954)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1161 (0.1227)	MaskBCELoss 0.0225 (0.0252)	MaskDICELoss 0.0936 (0.0976)
Epoch: [0][ 37/500]	Time 22.849 (22.849)	Loss 0.7889 (0.8305)	CeLoss 0.2471 (0.2214)	SegCLSLoss 0.0613 (0.0737)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1107 (0.1343)	MaskBCELoss 0.0114 (0.0375)	MaskDICELoss 0.0993 (0.0968)
Epoch: [0][ 38/500]	Time 21.037 (21.037)	Loss 0.9019 (0.8430)	CeLoss 0.1875 (0.1858)	SegCLSLoss 0.1377 (0.1056)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1239 (0.1267)	MaskBCELoss 0.0282 (0.0302)	MaskDICELoss 0.0957 (0.0965)
Epoch: [0][ 39/500]	Time 23.673 (23.673)	Loss 0.8000 (0.7846)	CeLoss 0.1650 (0.1859)	SegCLSLoss 0.1123 (0.0788)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1049 (0.1240)	MaskBCELoss 0.0051 (0.0274)	MaskDICELoss 0.0998 (0.0966)
Epoch: [0][ 40/500]	Time 19.838 (19.838)	Loss 0.8091 (0.8067)	CeLoss 0.2812 (0.2083)	SegCLSLoss 0.0579 (0.0899)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1064 (0.1105)	MaskBCELoss 0.0066 (0.0118)	MaskDICELoss 0.0998 (0.0987)
Epoch: [0][ 41/500]	Time 24.941 (24.941)	Loss 0.8688 (0.7399)	CeLoss 0.0854 (0.1803)	SegCLSLoss 0.1816 (0.0703)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1138 (0.1139)	MaskBCELoss 0.0181 (0.0182)	MaskDICELoss 0.0956 (0.0957)
Epoch: [0][ 42/500]	Time 23.212 (23.212)	Loss 0.6356 (0.6913)	CeLoss 0.1562 (0.1733)	SegCLSLoss 0.0261 (0.0412)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1190 (0.1242)	MaskBCELoss 0.0250 (0.0304)	MaskDICELoss 0.0941 (0.0937)
Epoch: [0][ 43/500]	Time 25.910 (25.910)	Loss 0.7943 (0.6965)	CeLoss 0.1494 (0.1692)	SegCLSLoss 0.1040 (0.0478)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1296 (0.1222)	MaskBCELoss 0.0402 (0.0286)	MaskDICELoss 0.0894 (0.0936)
Epoch: [0][ 44/500]	Time 22.678 (22.678)	Loss 0.6660 (0.6926)	CeLoss 0.1475 (0.1645)	SegCLSLoss 0.0571 (0.0514)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1026 (0.1158)	MaskBCELoss 0.0033 (0.0192)	MaskDICELoss 0.0992 (0.0966)
Epoch: [0][ 45/500]	Time 22.657 (22.657)	Loss 0.7170 (0.7301)	CeLoss 0.2480 (0.2092)	SegCLSLoss 0.0342 (0.0466)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1073 (0.1214)	MaskBCELoss 0.0147 (0.0289)	MaskDICELoss 0.0926 (0.0924)
Epoch: [0][ 46/500]	Time 23.332 (23.332)	Loss 0.6494 (0.7290)	CeLoss 0.1738 (0.1832)	SegCLSLoss 0.0267 (0.0585)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1204 (0.1219)	MaskBCELoss 0.0294 (0.0294)	MaskDICELoss 0.0910 (0.0925)
Epoch: [0][ 47/500]	Time 21.257 (21.257)	Loss 0.6472 (0.7664)	CeLoss 0.1426 (0.1673)	SegCLSLoss 0.0508 (0.0856)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1025 (0.1193)	MaskBCELoss 0.0033 (0.0246)	MaskDICELoss 0.0992 (0.0947)
Epoch: [0][ 48/500]	Time 23.976 (23.976)	Loss 0.7356 (0.7370)	CeLoss 0.2578 (0.1954)	SegCLSLoss 0.0283 (0.0570)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1171 (0.1184)	MaskBCELoss 0.0235 (0.0229)	MaskDICELoss 0.0936 (0.0954)
Epoch: [0][ 49/500]	Time 22.168 (22.168)	Loss 0.7280 (0.7066)	CeLoss 0.2402 (0.2049)	SegCLSLoss 0.0256 (0.0417)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1262 (0.1143)	MaskBCELoss 0.0346 (0.0195)	MaskDICELoss 0.0917 (0.0948)
Epoch: [0][ 50/500]	Time 22.146 (22.146)	Loss 0.7431 (0.7561)	CeLoss 0.1748 (0.1824)	SegCLSLoss 0.0732 (0.0770)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1160 (0.1157)	MaskBCELoss 0.0214 (0.0216)	MaskDICELoss 0.0946 (0.0941)
Epoch: [0][ 51/500]	Time 24.678 (24.678)	Loss 0.5875 (0.6098)	CeLoss 0.1172 (0.1410)	SegCLSLoss 0.0192 (0.0225)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1174 (0.1196)	MaskBCELoss 0.0188 (0.0274)	MaskDICELoss 0.0986 (0.0922)
Epoch: [0][ 52/500]	Time 25.911 (25.911)	Loss 0.6130 (0.6165)	CeLoss 0.1270 (0.1390)	SegCLSLoss 0.0204 (0.0266)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1254 (0.1212)	MaskBCELoss 0.0278 (0.0302)	MaskDICELoss 0.0976 (0.0910)
Epoch: [0][ 53/500]	Time 21.899 (21.899)	Loss 0.6367 (0.6078)	CeLoss 0.1621 (0.1329)	SegCLSLoss 0.0256 (0.0235)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1142 (0.1190)	MaskBCELoss 0.0171 (0.0243)	MaskDICELoss 0.0971 (0.0947)
Epoch: [0][ 54/500]	Time 23.828 (23.828)	Loss 0.6431 (0.6333)	CeLoss 0.1396 (0.1558)	SegCLSLoss 0.0266 (0.0232)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1305 (0.1217)	MaskBCELoss 0.0360 (0.0278)	MaskDICELoss 0.0945 (0.0939)
Epoch: [0][ 55/500]	Time 20.379 (20.379)	Loss 0.6029 (0.6018)	CeLoss 0.1504 (0.1423)	SegCLSLoss 0.0171 (0.0210)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1138 (0.1163)	MaskBCELoss 0.0183 (0.0237)	MaskDICELoss 0.0955 (0.0926)
Epoch: [0][ 56/500]	Time 19.434 (19.434)	Loss 0.6056 (0.6189)	CeLoss 0.1069 (0.1408)	SegCLSLoss 0.0277 (0.0209)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1227 (0.1220)	MaskBCELoss 0.0237 (0.0259)	MaskDICELoss 0.0989 (0.0961)
Epoch: [0][ 57/500]	Time 23.800 (23.800)	Loss 0.6400 (0.6136)	CeLoss 0.1621 (0.1381)	SegCLSLoss 0.0238 (0.0219)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1187 (0.1217)	MaskBCELoss 0.0221 (0.0277)	MaskDICELoss 0.0966 (0.0940)
Epoch: [0][ 58/500]	Time 24.940 (24.940)	Loss 0.5630 (0.6089)	CeLoss 0.1030 (0.1325)	SegCLSLoss 0.0156 (0.0219)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1169 (0.1253)	MaskBCELoss 0.0196 (0.0343)	MaskDICELoss 0.0974 (0.0910)
Epoch: [0][ 59/500]	Time 20.307 (20.307)	Loss 0.6326 (0.6141)	CeLoss 0.1660 (0.1409)	SegCLSLoss 0.0261 (0.0236)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1161 (0.1201)	MaskBCELoss 0.0252 (0.0273)	MaskDICELoss 0.0909 (0.0928)
Epoch: [0][ 60/500]	Time 19.794 (19.794)	Loss 0.5821 (0.6146)	CeLoss 0.1436 (0.1399)	SegCLSLoss 0.0143 (0.0242)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1067 (0.1211)	MaskBCELoss 0.0083 (0.0290)	MaskDICELoss 0.0984 (0.0920)
Epoch: [0][ 61/500]	Time 23.382 (23.382)	Loss 0.5418 (0.5693)	CeLoss 0.0854 (0.1099)	SegCLSLoss 0.0211 (0.0242)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1109 (0.1119)	MaskBCELoss 0.0150 (0.0182)	MaskDICELoss 0.0959 (0.0937)
Epoch: [0][ 62/500]	Time 20.161 (20.161)	Loss 0.6106 (0.5749)	CeLoss 0.1475 (0.1127)	SegCLSLoss 0.0312 (0.0236)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1018 (0.1124)	MaskBCELoss 0.0030 (0.0171)	MaskDICELoss 0.0988 (0.0953)
Epoch: [0][ 63/500]	Time 21.205 (21.205)	Loss 0.6137 (0.5852)	CeLoss 0.0898 (0.1140)	SegCLSLoss 0.0520 (0.0264)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1116 (0.1145)	MaskBCELoss 0.0131 (0.0198)	MaskDICELoss 0.0984 (0.0947)
Epoch: [0][ 64/500]	Time 23.451 (23.451)	Loss 0.5692 (0.5764)	CeLoss 0.1094 (0.1076)	SegCLSLoss 0.0330 (0.0260)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1031 (0.1135)	MaskBCELoss 0.0090 (0.0186)	MaskDICELoss 0.0940 (0.0949)
Epoch: [0][ 65/500]	Time 22.790 (22.790)	Loss 0.5534 (0.5756)	CeLoss 0.0991 (0.0979)	SegCLSLoss 0.0271 (0.0309)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1090 (0.1143)	MaskBCELoss 0.0178 (0.0207)	MaskDICELoss 0.0911 (0.0935)
Epoch: [0][ 66/500]	Time 21.112 (21.112)	Loss 0.5923 (0.5795)	CeLoss 0.1396 (0.1108)	SegCLSLoss 0.0243 (0.0275)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1040 (0.1121)	MaskBCELoss 0.0060 (0.0173)	MaskDICELoss 0.0980 (0.0948)
Epoch: [0][ 67/500]	Time 22.163 (22.163)	Loss 0.5769 (0.5711)	CeLoss 0.1084 (0.1074)	SegCLSLoss 0.0297 (0.0254)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1092 (0.1119)	MaskBCELoss 0.0139 (0.0173)	MaskDICELoss 0.0953 (0.0945)
Epoch: [0][ 68/500]	Time 21.203 (21.203)	Loss 0.5418 (0.5785)	CeLoss 0.0815 (0.1106)	SegCLSLoss 0.0219 (0.0262)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1154 (0.1118)	MaskBCELoss 0.0223 (0.0158)	MaskDICELoss 0.0931 (0.0960)
Epoch: [0][ 69/500]	Time 25.422 (25.422)	Loss 0.5623 (0.5672)	CeLoss 0.0938 (0.0937)	SegCLSLoss 0.0254 (0.0300)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1190 (0.1162)	MaskBCELoss 0.0290 (0.0256)	MaskDICELoss 0.0899 (0.0906)
Epoch: [0][ 70/500]	Time 25.632 (25.632)	Loss 0.6124 (0.5735)	CeLoss 0.1118 (0.1004)	SegCLSLoss 0.0552 (0.0297)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1109 (0.1157)	MaskBCELoss 0.0266 (0.0246)	MaskDICELoss 0.0843 (0.0912)
Epoch: [0][ 71/500]	Time 22.509 (22.509)	Loss 0.5435 (0.5704)	CeLoss 0.0679 (0.0730)	SegCLSLoss 0.0327 (0.0430)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1090 (0.1123)	MaskBCELoss 0.0131 (0.0190)	MaskDICELoss 0.0959 (0.0933)
Epoch: [0][ 72/500]	Time 23.139 (23.139)	Loss 0.6113 (0.5816)	CeLoss 0.0830 (0.0694)	SegCLSLoss 0.0552 (0.0444)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1171 (0.1162)	MaskBCELoss 0.0250 (0.0207)	MaskDICELoss 0.0921 (0.0955)
Epoch: [0][ 73/500]	Time 20.850 (20.850)	Loss 0.5639 (0.6076)	CeLoss 0.0659 (0.0752)	SegCLSLoss 0.0435 (0.0589)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1087 (0.1124)	MaskBCELoss 0.0121 (0.0176)	MaskDICELoss 0.0966 (0.0948)
Epoch: [0][ 74/500]	Time 25.579 (25.579)	Loss 0.7798 (0.6342)	CeLoss 0.0535 (0.0741)	SegCLSLoss 0.1582 (0.0716)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1122 (0.1135)	MaskBCELoss 0.0188 (0.0185)	MaskDICELoss 0.0933 (0.0950)
Epoch: [0][ 75/500]	Time 24.219 (24.219)	Loss 0.6102 (0.5599)	CeLoss 0.0830 (0.0788)	SegCLSLoss 0.0481 (0.0321)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1257 (0.1139)	MaskBCELoss 0.0358 (0.0194)	MaskDICELoss 0.0899 (0.0945)
Epoch: [0][ 76/500]	Time 25.405 (25.405)	Loss 0.6843 (0.6310)	CeLoss 0.0654 (0.0747)	SegCLSLoss 0.0815 (0.0645)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1375 (0.1217)	MaskBCELoss 0.0468 (0.0298)	MaskDICELoss 0.0906 (0.0919)
Epoch: [0][ 77/500]	Time 22.296 (22.296)	Loss 0.5647 (0.5836)	CeLoss 0.0488 (0.0782)	SegCLSLoss 0.0347 (0.0427)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1286 (0.1159)	MaskBCELoss 0.0343 (0.0219)	MaskDICELoss 0.0944 (0.0941)
Epoch: [0][ 78/500]	Time 21.519 (21.519)	Loss 0.6849 (0.6180)	CeLoss 0.0566 (0.0713)	SegCLSLoss 0.1094 (0.0686)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1219 (0.1125)	MaskBCELoss 0.0388 (0.0200)	MaskDICELoss 0.0831 (0.0925)
Epoch: [0][ 79/500]	Time 19.397 (19.397)	Loss 0.5301 (0.5643)	CeLoss 0.0811 (0.0794)	SegCLSLoss 0.0201 (0.0369)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1082 (0.1097)	MaskBCELoss 0.0120 (0.0138)	MaskDICELoss 0.0963 (0.0959)
Epoch: [0][ 80/500]	Time 21.049 (21.049)	Loss 0.5601 (0.6087)	CeLoss 0.0864 (0.0905)	SegCLSLoss 0.0347 (0.0549)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1030 (0.1056)	MaskBCELoss 0.0037 (0.0071)	MaskDICELoss 0.0993 (0.0985)
Epoch: [0][ 81/500]	Time 21.915 (21.915)	Loss 0.5423 (0.5651)	CeLoss 0.0237 (0.0586)	SegCLSLoss 0.0493 (0.0490)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1161 (0.1086)	MaskBCELoss 0.0223 (0.0131)	MaskDICELoss 0.0937 (0.0955)
Epoch: [0][ 82/500]	Time 25.641 (25.641)	Loss 0.5437 (0.5606)	CeLoss 0.0566 (0.0569)	SegCLSLoss 0.0359 (0.0441)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1163 (0.1140)	MaskBCELoss 0.0248 (0.0202)	MaskDICELoss 0.0915 (0.0938)
Epoch: [0][ 83/500]	Time 24.004 (24.004)	Loss 0.5156 (0.5562)	CeLoss 0.0381 (0.0578)	SegCLSLoss 0.0310 (0.0408)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1106 (0.1139)	MaskBCELoss 0.0133 (0.0192)	MaskDICELoss 0.0972 (0.0946)
Epoch: [0][ 84/500]	Time 23.719 (23.719)	Loss 0.5465 (0.5494)	CeLoss 0.0280 (0.0453)	SegCLSLoss 0.0547 (0.0457)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1085 (0.1112)	MaskBCELoss 0.0124 (0.0159)	MaskDICELoss 0.0960 (0.0952)
Epoch: [0][ 85/500]	Time 20.981 (20.981)	Loss 0.5410 (0.5661)	CeLoss 0.0938 (0.0633)	SegCLSLoss 0.0240 (0.0403)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1010 (0.1149)	MaskBCELoss 0.0026 (0.0188)	MaskDICELoss 0.0984 (0.0961)
Epoch: [0][ 86/500]	Time 20.258 (20.258)	Loss 0.5869 (0.5679)	CeLoss 0.0427 (0.0632)	SegCLSLoss 0.0503 (0.0420)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1264 (0.1152)	MaskBCELoss 0.0312 (0.0202)	MaskDICELoss 0.0952 (0.0950)
Epoch: [0][ 87/500]	Time 22.341 (22.341)	Loss 0.5437 (0.5549)	CeLoss 0.0295 (0.0509)	SegCLSLoss 0.0464 (0.0437)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1140 (0.1129)	MaskBCELoss 0.0170 (0.0175)	MaskDICELoss 0.0969 (0.0953)
Epoch: [0][ 88/500]	Time 26.556 (26.556)	Loss 0.5613 (0.5536)	CeLoss 0.0461 (0.0534)	SegCLSLoss 0.0496 (0.0404)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1113 (0.1138)	MaskBCELoss 0.0145 (0.0180)	MaskDICELoss 0.0967 (0.0959)
Epoch: [0][ 89/500]	Time 20.725 (20.725)	Loss 0.5765 (0.5628)	CeLoss 0.1069 (0.0558)	SegCLSLoss 0.0312 (0.0424)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1057 (0.1152)	MaskBCELoss 0.0079 (0.0192)	MaskDICELoss 0.0978 (0.0960)
Epoch: [0][ 90/500]	Time 21.912 (21.912)	Loss 0.5404 (0.5571)	CeLoss 0.0796 (0.0617)	SegCLSLoss 0.0334 (0.0416)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1010 (0.1119)	MaskBCELoss 0.0053 (0.0177)	MaskDICELoss 0.0957 (0.0942)
Epoch: [0][ 91/500]	Time 22.503 (22.503)	Loss 0.4943 (0.5419)	CeLoss 0.0732 (0.0480)	SegCLSLoss 0.0178 (0.0451)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1050 (0.1086)	MaskBCELoss 0.0172 (0.0154)	MaskDICELoss 0.0878 (0.0932)
Epoch: [0][ 92/500]	Time 19.930 (19.930)	Loss 0.4652 (0.5049)	CeLoss 0.0297 (0.0488)	SegCLSLoss 0.0181 (0.0197)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1029 (0.1137)	MaskBCELoss 0.0060 (0.0191)	MaskDICELoss 0.0969 (0.0946)
Epoch: [0][ 93/500]	Time 23.425 (23.425)	Loss 0.5143 (0.5119)	CeLoss 0.0273 (0.0526)	SegCLSLoss 0.0325 (0.0236)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1160 (0.1122)	MaskBCELoss 0.0210 (0.0183)	MaskDICELoss 0.0950 (0.0939)
Epoch: [0][ 94/500]	Time 23.220 (23.220)	Loss 0.5024 (0.5014)	CeLoss 0.0310 (0.0443)	SegCLSLoss 0.0378 (0.0263)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1020 (0.1085)	MaskBCELoss 0.0063 (0.0147)	MaskDICELoss 0.0957 (0.0938)
Epoch: [0][ 95/500]	Time 23.469 (23.469)	Loss 0.5139 (0.5080)	CeLoss 0.0228 (0.0501)	SegCLSLoss 0.0332 (0.0232)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1189 (0.1122)	MaskBCELoss 0.0253 (0.0185)	MaskDICELoss 0.0936 (0.0936)
Epoch: [0][ 96/500]	Time 21.234 (21.234)	Loss 0.5045 (0.5162)	CeLoss 0.0242 (0.0447)	SegCLSLoss 0.0244 (0.0292)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1228 (0.1125)	MaskBCELoss 0.0299 (0.0186)	MaskDICELoss 0.0929 (0.0940)
Epoch: [0][ 97/500]	Time 19.353 (19.353)	Loss 0.4615 (0.5122)	CeLoss 0.0242 (0.0653)	SegCLSLoss 0.0133 (0.0183)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1098 (0.1113)	MaskBCELoss 0.0143 (0.0175)	MaskDICELoss 0.0955 (0.0938)
Epoch: [0][ 98/500]	Time 22.198 (22.198)	Loss 0.4907 (0.5108)	CeLoss 0.0237 (0.0568)	SegCLSLoss 0.0210 (0.0180)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1173 (0.1160)	MaskBCELoss 0.0220 (0.0230)	MaskDICELoss 0.0953 (0.0930)
Epoch: [0][ 99/500]	Time 20.327 (20.327)	Loss 0.5363 (0.5236)	CeLoss 0.0708 (0.0539)	SegCLSLoss 0.0109 (0.0332)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1277 (0.1085)	MaskBCELoss 0.0333 (0.0153)	MaskDICELoss 0.0944 (0.0932)
[2025-04-04 21:14:19,131] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[0.00029991566265060235], mom=[(0.9, 0.95)]
[2025-04-04 21:14:19,139] [INFO] [timer.py:215:stop] epoch=0/micro_step=100/global_step=10, RunningAvgSamplesPerSec=1.8063641704113076, CurrSamplesPerSec=1.907019248847781, MemAllocated=33.43GB, MaxMemAllocated=45.56GB
Epoch: [0][100/500]	Time 19.258 (19.258)	Loss 0.5109 (0.5064)	CeLoss 0.1104 (0.0460)	SegCLSLoss 0.0049 (0.0279)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0994 (0.1080)	MaskBCELoss 0.0035 (0.0138)	MaskDICELoss 0.0959 (0.0942)
Epoch: [0][101/500]	Time 22.234 (22.234)	Loss 0.4775 (0.5132)	CeLoss 0.0420 (0.0529)	SegCLSLoss 0.0243 (0.0257)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0986 (0.1111)	MaskBCELoss 0.0038 (0.0179)	MaskDICELoss 0.0948 (0.0932)
Epoch: [0][102/500]	Time 25.299 (25.299)	Loss 0.4821 (0.4956)	CeLoss 0.0488 (0.0516)	SegCLSLoss 0.0096 (0.0152)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1144 (0.1148)	MaskBCELoss 0.0217 (0.0228)	MaskDICELoss 0.0927 (0.0920)
Epoch: [0][103/500]	Time 21.236 (21.236)	Loss 0.5084 (0.4803)	CeLoss 0.0276 (0.0383)	SegCLSLoss 0.0425 (0.0234)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1032 (0.1041)	MaskBCELoss 0.0085 (0.0106)	MaskDICELoss 0.0947 (0.0935)
Epoch: [0][104/500]	Time 25.399 (25.399)	Loss 0.5791 (0.5075)	CeLoss 0.0559 (0.0327)	SegCLSLoss 0.0552 (0.0286)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1204 (0.1155)	MaskBCELoss 0.0341 (0.0222)	MaskDICELoss 0.0863 (0.0933)
Epoch: [0][105/500]	Time 27.232 (27.232)	Loss 0.4519 (0.5300)	CeLoss 0.0256 (0.0387)	SegCLSLoss 0.0081 (0.0433)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1104 (0.1118)	MaskBCELoss 0.0158 (0.0215)	MaskDICELoss 0.0946 (0.0904)
Epoch: [0][106/500]	Time 22.563 (22.563)	Loss 0.4937 (0.5139)	CeLoss 0.0177 (0.0492)	SegCLSLoss 0.0310 (0.0258)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1122 (0.1132)	MaskBCELoss 0.0173 (0.0200)	MaskDICELoss 0.0949 (0.0933)
Epoch: [0][107/500]	Time 23.837 (23.837)	Loss 0.5344 (0.4938)	CeLoss 0.0293 (0.0326)	SegCLSLoss 0.0620 (0.0279)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1016 (0.1105)	MaskBCELoss 0.0125 (0.0183)	MaskDICELoss 0.0891 (0.0922)
Epoch: [0][108/500]	Time 22.737 (22.737)	Loss 0.4927 (0.4897)	CeLoss 0.0698 (0.0426)	SegCLSLoss 0.0100 (0.0180)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1142 (0.1136)	MaskBCELoss 0.0269 (0.0217)	MaskDICELoss 0.0873 (0.0919)
Epoch: [0][109/500]	Time 21.758 (21.758)	Loss 0.4670 (0.5102)	CeLoss 0.0215 (0.0426)	SegCLSLoss 0.0227 (0.0316)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1123 (0.1087)	MaskBCELoss 0.0245 (0.0152)	MaskDICELoss 0.0878 (0.0935)
Epoch: [0][110/500]	Time 20.703 (20.703)	Loss 0.5102 (0.4836)	CeLoss 0.0376 (0.0347)	SegCLSLoss 0.0405 (0.0240)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1052 (0.1075)	MaskBCELoss 0.0146 (0.0145)	MaskDICELoss 0.0906 (0.0930)
Epoch: [0][111/500]	Time 23.781 (23.781)	Loss 0.4539 (0.4758)	CeLoss 0.0217 (0.0430)	SegCLSLoss 0.0205 (0.0203)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1058 (0.1071)	MaskBCELoss 0.0162 (0.0181)	MaskDICELoss 0.0896 (0.0890)
Epoch: [0][112/500]	Time 19.655 (19.655)	Loss 0.6218 (0.4793)	CeLoss 0.0996 (0.0409)	SegCLSLoss 0.0679 (0.0222)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1032 (0.1081)	MaskBCELoss 0.0134 (0.0192)	MaskDICELoss 0.0897 (0.0890)
Epoch: [0][113/500]	Time 22.936 (22.936)	Loss 0.4933 (0.4750)	CeLoss 0.0918 (0.0382)	SegCLSLoss 0.0183 (0.0206)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0995 (0.1090)	MaskBCELoss 0.0165 (0.0202)	MaskDICELoss 0.0830 (0.0888)
Epoch: [0][114/500]	Time 28.041 (28.041)	Loss 0.4991 (0.5107)	CeLoss 0.0693 (0.0653)	SegCLSLoss 0.0116 (0.0244)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1206 (0.1090)	MaskBCELoss 0.0378 (0.0196)	MaskDICELoss 0.0827 (0.0894)
Epoch: [0][115/500]	Time 24.455 (24.455)	Loss 0.4491 (0.4819)	CeLoss 0.0118 (0.0373)	SegCLSLoss 0.0171 (0.0217)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1152 (0.1119)	MaskBCELoss 0.0288 (0.0233)	MaskDICELoss 0.0864 (0.0887)
Epoch: [0][116/500]	Time 20.608 (20.608)	Loss 0.4270 (0.4632)	CeLoss 0.0168 (0.0322)	SegCLSLoss 0.0209 (0.0188)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0985 (0.1075)	MaskBCELoss 0.0128 (0.0184)	MaskDICELoss 0.0857 (0.0892)
Epoch: [0][117/500]	Time 26.914 (26.914)	Loss 0.4962 (0.4786)	CeLoss 0.0659 (0.0398)	SegCLSLoss 0.0126 (0.0241)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1142 (0.1077)	MaskBCELoss 0.0260 (0.0199)	MaskDICELoss 0.0882 (0.0877)
Epoch: [0][118/500]	Time 23.932 (23.932)	Loss 0.5012 (0.4831)	CeLoss 0.0479 (0.0478)	SegCLSLoss 0.0396 (0.0198)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0989 (0.1088)	MaskBCELoss 0.0109 (0.0197)	MaskDICELoss 0.0880 (0.0891)
Epoch: [0][119/500]	Time 23.655 (23.655)	Loss 0.5164 (0.4750)	CeLoss 0.0574 (0.0451)	SegCLSLoss 0.0098 (0.0165)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1303 (0.1081)	MaskBCELoss 0.0408 (0.0178)	MaskDICELoss 0.0894 (0.0904)
Epoch: [0][120/500]	Time 19.840 (19.840)	Loss 0.5486 (0.4744)	CeLoss 0.1069 (0.0411)	SegCLSLoss 0.0098 (0.0191)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1186 (0.1072)	MaskBCELoss 0.0262 (0.0168)	MaskDICELoss 0.0924 (0.0904)
Epoch: [0][121/500]	Time 20.494 (20.494)	Loss 0.5128 (0.4879)	CeLoss 0.0894 (0.0520)	SegCLSLoss 0.0164 (0.0140)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1057 (0.1086)	MaskBCELoss 0.0160 (0.0133)	MaskDICELoss 0.0898 (0.0953)
Epoch: [0][122/500]	Time 26.024 (26.024)	Loss 0.5338 (0.4903)	CeLoss 0.0742 (0.0435)	SegCLSLoss 0.0125 (0.0175)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1203 (0.1126)	MaskBCELoss 0.0234 (0.0192)	MaskDICELoss 0.0970 (0.0933)
Epoch: [0][123/500]	Time 22.216 (22.216)	Loss 0.4555 (0.4681)	CeLoss 0.0038 (0.0297)	SegCLSLoss 0.0170 (0.0157)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1148 (0.1117)	MaskBCELoss 0.0208 (0.0200)	MaskDICELoss 0.0940 (0.0918)
Epoch: [0][124/500]	Time 21.915 (21.915)	Loss 0.4773 (0.4804)	CeLoss 0.0625 (0.0394)	SegCLSLoss 0.0109 (0.0152)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1028 (0.1135)	MaskBCELoss 0.0091 (0.0216)	MaskDICELoss 0.0937 (0.0918)
Epoch: [0][125/500]	Time 23.398 (23.398)	Loss 0.4915 (0.4768)	CeLoss 0.0515 (0.0393)	SegCLSLoss 0.0114 (0.0135)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1121 (0.1128)	MaskBCELoss 0.0156 (0.0202)	MaskDICELoss 0.0965 (0.0925)
Epoch: [0][126/500]	Time 22.998 (22.998)	Loss 0.4394 (0.4788)	CeLoss 0.0127 (0.0383)	SegCLSLoss 0.0121 (0.0147)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1056 (0.1121)	MaskBCELoss 0.0100 (0.0186)	MaskDICELoss 0.0956 (0.0935)
Epoch: [0][127/500]	Time 24.676 (24.676)	Loss 0.5147 (0.4881)	CeLoss 0.0908 (0.0463)	SegCLSLoss 0.0099 (0.0128)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1087 (0.1143)	MaskBCELoss 0.0150 (0.0205)	MaskDICELoss 0.0936 (0.0938)
Epoch: [0][128/500]	Time 20.411 (20.411)	Loss 0.4778 (0.4734)	CeLoss 0.0283 (0.0376)	SegCLSLoss 0.0168 (0.0148)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1102 (0.1081)	MaskBCELoss 0.0126 (0.0132)	MaskDICELoss 0.0976 (0.0949)
Epoch: [0][129/500]	Time 21.697 (21.697)	Loss 0.4758 (0.4935)	CeLoss 0.0037 (0.0463)	SegCLSLoss 0.0165 (0.0167)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1294 (0.1148)	MaskBCELoss 0.0392 (0.0227)	MaskDICELoss 0.0901 (0.0921)
Epoch: [0][130/500]	Time 23.344 (23.344)	Loss 0.4609 (0.4888)	CeLoss 0.0364 (0.0574)	SegCLSLoss 0.0135 (0.0136)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1107 (0.1090)	MaskBCELoss 0.0227 (0.0159)	MaskDICELoss 0.0881 (0.0931)
Epoch: [0][131/500]	Time 20.053 (20.053)	Loss 0.4279 (0.4458)	CeLoss 0.0059 (0.0387)	SegCLSLoss 0.0155 (0.0150)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1065 (0.1031)	MaskBCELoss 0.0176 (0.0178)	MaskDICELoss 0.0889 (0.0853)
Epoch: [0][132/500]	Time 19.557 (19.557)	Loss 0.5173 (0.4396)	CeLoss 0.0483 (0.0278)	SegCLSLoss 0.0107 (0.0166)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1361 (0.1027)	MaskBCELoss 0.0483 (0.0161)	MaskDICELoss 0.0878 (0.0866)
Epoch: [0][133/500]	Time 23.908 (23.908)	Loss 0.4549 (0.4705)	CeLoss 0.0493 (0.0500)	SegCLSLoss 0.0121 (0.0161)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1009 (0.1070)	MaskBCELoss 0.0111 (0.0198)	MaskDICELoss 0.0898 (0.0872)
Epoch: [0][134/500]	Time 21.694 (21.694)	Loss 0.4042 (0.4478)	CeLoss 0.0148 (0.0337)	SegCLSLoss 0.0215 (0.0174)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0877 (0.1032)	MaskBCELoss 0.0022 (0.0167)	MaskDICELoss 0.0855 (0.0865)
Epoch: [0][135/500]	Time 23.823 (23.823)	Loss 0.5265 (0.4901)	CeLoss 0.0669 (0.0668)	SegCLSLoss 0.0120 (0.0147)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1204 (0.1067)	MaskBCELoss 0.0228 (0.0164)	MaskDICELoss 0.0976 (0.0903)
Epoch: [0][136/500]	Time 23.972 (23.972)	Loss 0.4963 (0.4557)	CeLoss 0.0232 (0.0332)	SegCLSLoss 0.0167 (0.0173)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1259 (0.1050)	MaskBCELoss 0.0319 (0.0161)	MaskDICELoss 0.0940 (0.0889)
Epoch: [0][137/500]	Time 21.772 (21.772)	Loss 0.4298 (0.4455)	CeLoss 0.0092 (0.0241)	SegCLSLoss 0.0166 (0.0162)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1090 (0.1075)	MaskBCELoss 0.0243 (0.0206)	MaskDICELoss 0.0847 (0.0869)
Epoch: [0][138/500]	Time 20.402 (20.402)	Loss 0.4798 (0.4671)	CeLoss 0.0688 (0.0478)	SegCLSLoss 0.0104 (0.0157)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1057 (0.1045)	MaskBCELoss 0.0164 (0.0152)	MaskDICELoss 0.0893 (0.0894)
Epoch: [0][139/500]	Time 24.931 (24.931)	Loss 0.4734 (0.4632)	CeLoss 0.0223 (0.0393)	SegCLSLoss 0.0177 (0.0190)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1141 (0.1061)	MaskBCELoss 0.0203 (0.0191)	MaskDICELoss 0.0937 (0.0869)
Epoch: [0][140/500]	Time 23.622 (23.622)	Loss 0.5496 (0.4679)	CeLoss 0.0703 (0.0407)	SegCLSLoss 0.0127 (0.0143)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1359 (0.1122)	MaskBCELoss 0.0448 (0.0251)	MaskDICELoss 0.0911 (0.0871)
Epoch: [0][141/500]	Time 21.530 (21.530)	Loss 0.4789 (0.4841)	CeLoss 0.0248 (0.0508)	SegCLSLoss 0.0166 (0.0118)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1218 (0.1159)	MaskBCELoss 0.0330 (0.0268)	MaskDICELoss 0.0888 (0.0891)
Epoch: [0][142/500]	Time 24.182 (24.182)	Loss 0.3994 (0.4533)	CeLoss 0.0168 (0.0423)	SegCLSLoss 0.0162 (0.0114)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0914 (0.1067)	MaskBCELoss 0.0078 (0.0193)	MaskDICELoss 0.0837 (0.0874)
Epoch: [0][143/500]	Time 24.858 (24.858)	Loss 0.4736 (0.4547)	CeLoss 0.0148 (0.0342)	SegCLSLoss 0.0142 (0.0162)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1306 (0.1075)	MaskBCELoss 0.0460 (0.0210)	MaskDICELoss 0.0846 (0.0865)
Epoch: [0][144/500]	Time 20.773 (20.773)	Loss 0.4492 (0.4690)	CeLoss 0.0072 (0.0424)	SegCLSLoss 0.0126 (0.0172)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1203 (0.1102)	MaskBCELoss 0.0323 (0.0243)	MaskDICELoss 0.0881 (0.0859)
Epoch: [0][145/500]	Time 19.891 (19.891)	Loss 0.4799 (0.4611)	CeLoss 0.0164 (0.0536)	SegCLSLoss 0.0151 (0.0139)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1263 (0.1025)	MaskBCELoss 0.0359 (0.0152)	MaskDICELoss 0.0904 (0.0873)
Epoch: [0][146/500]	Time 22.745 (22.745)	Loss 0.4660 (0.4490)	CeLoss 0.0142 (0.0339)	SegCLSLoss 0.0159 (0.0160)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1135 (0.1052)	MaskBCELoss 0.0170 (0.0189)	MaskDICELoss 0.0966 (0.0863)
Epoch: [0][147/500]	Time 23.477 (23.477)	Loss 0.4356 (0.4554)	CeLoss 0.0026 (0.0406)	SegCLSLoss 0.0087 (0.0129)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1216 (0.1085)	MaskBCELoss 0.0353 (0.0224)	MaskDICELoss 0.0863 (0.0860)
Epoch: [0][148/500]	Time 19.847 (19.847)	Loss 0.4731 (0.4415)	CeLoss 0.0193 (0.0309)	SegCLSLoss 0.0203 (0.0146)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1176 (0.1038)	MaskBCELoss 0.0288 (0.0170)	MaskDICELoss 0.0888 (0.0868)
Epoch: [0][149/500]	Time 19.695 (19.695)	Loss 0.4319 (0.4297)	CeLoss 0.0486 (0.0401)	SegCLSLoss 0.0125 (0.0113)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0930 (0.0987)	MaskBCELoss 0.0068 (0.0140)	MaskDICELoss 0.0862 (0.0847)
Epoch: [0][150/500]	Time 19.885 (19.885)	Loss 0.5045 (0.4554)	CeLoss 0.0437 (0.0576)	SegCLSLoss 0.0086 (0.0120)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1302 (0.1014)	MaskBCELoss 0.0387 (0.0159)	MaskDICELoss 0.0916 (0.0855)
Epoch: [0][151/500]	Time 21.507 (21.507)	Loss 0.3993 (0.4131)	CeLoss 0.0039 (0.0221)	SegCLSLoss 0.0060 (0.0088)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1062 (0.1031)	MaskBCELoss 0.0207 (0.0194)	MaskDICELoss 0.0855 (0.0837)
Epoch: [0][152/500]	Time 20.107 (20.107)	Loss 0.4835 (0.4170)	CeLoss 0.0452 (0.0271)	SegCLSLoss 0.0051 (0.0085)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1254 (0.1026)	MaskBCELoss 0.0367 (0.0187)	MaskDICELoss 0.0886 (0.0839)
Epoch: [0][153/500]	Time 21.733 (21.733)	Loss 0.4271 (0.4404)	CeLoss 0.0586 (0.0437)	SegCLSLoss 0.0072 (0.0091)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0921 (0.1046)	MaskBCELoss 0.0073 (0.0198)	MaskDICELoss 0.0849 (0.0847)
Epoch: [0][154/500]	Time 24.624 (24.624)	Loss 0.3817 (0.4462)	CeLoss 0.0083 (0.0419)	SegCLSLoss 0.0051 (0.0088)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1001 (0.1087)	MaskBCELoss 0.0187 (0.0241)	MaskDICELoss 0.0815 (0.0846)
Epoch: [0][155/500]	Time 23.040 (23.040)	Loss 0.4339 (0.4356)	CeLoss 0.0752 (0.0355)	SegCLSLoss 0.0036 (0.0130)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0928 (0.1037)	MaskBCELoss 0.0097 (0.0203)	MaskDICELoss 0.0831 (0.0834)
Epoch: [0][156/500]	Time 22.145 (22.145)	Loss 0.5285 (0.4253)	CeLoss 0.0845 (0.0328)	SegCLSLoss 0.0070 (0.0089)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1169 (0.1023)	MaskBCELoss 0.0187 (0.0173)	MaskDICELoss 0.0982 (0.0850)
Epoch: [0][157/500]	Time 24.654 (24.654)	Loss 0.3560 (0.4350)	CeLoss 0.0056 (0.0367)	SegCLSLoss 0.0043 (0.0084)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0905 (0.1052)	MaskBCELoss 0.0101 (0.0197)	MaskDICELoss 0.0804 (0.0855)
Epoch: [0][158/500]	Time 24.362 (24.362)	Loss 0.4563 (0.4345)	CeLoss 0.0129 (0.0354)	SegCLSLoss 0.0101 (0.0067)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1211 (0.1062)	MaskBCELoss 0.0306 (0.0197)	MaskDICELoss 0.0905 (0.0866)
Epoch: [0][159/500]	Time 26.459 (26.459)	Loss 0.4744 (0.4376)	CeLoss 0.0186 (0.0384)	SegCLSLoss 0.0192 (0.0109)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1160 (0.1046)	MaskBCELoss 0.0232 (0.0206)	MaskDICELoss 0.0927 (0.0841)
Epoch: [0][160/500]	Time 22.213 (22.213)	Loss 0.5006 (0.4278)	CeLoss 0.0708 (0.0374)	SegCLSLoss 0.0046 (0.0057)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1152 (0.1037)	MaskBCELoss 0.0202 (0.0179)	MaskDICELoss 0.0950 (0.0858)
Epoch: [0][161/500]	Time 22.431 (22.431)	Loss 0.4339 (0.4213)	CeLoss 0.0762 (0.0298)	SegCLSLoss 0.0058 (0.0075)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0978 (0.1045)	MaskBCELoss 0.0225 (0.0208)	MaskDICELoss 0.0754 (0.0837)
Epoch: [0][162/500]	Time 22.931 (22.931)	Loss 0.4692 (0.4173)	CeLoss 0.0610 (0.0395)	SegCLSLoss 0.0175 (0.0097)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0998 (0.0980)	MaskBCELoss 0.0130 (0.0168)	MaskDICELoss 0.0868 (0.0812)
Epoch: [0][163/500]	Time 21.527 (21.527)	Loss 0.4017 (0.4268)	CeLoss 0.0023 (0.0392)	SegCLSLoss 0.0071 (0.0076)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1097 (0.1013)	MaskBCELoss 0.0269 (0.0164)	MaskDICELoss 0.0828 (0.0849)
Epoch: [0][164/500]	Time 22.665 (22.665)	Loss 0.4907 (0.4283)	CeLoss 0.0908 (0.0374)	SegCLSLoss 0.0077 (0.0088)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1018 (0.1014)	MaskBCELoss 0.0115 (0.0161)	MaskDICELoss 0.0904 (0.0853)
Epoch: [0][165/500]	Time 24.250 (24.250)	Loss 0.4520 (0.4348)	CeLoss 0.0811 (0.0364)	SegCLSLoss 0.0048 (0.0087)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1016 (0.1077)	MaskBCELoss 0.0225 (0.0250)	MaskDICELoss 0.0791 (0.0827)
Epoch: [0][166/500]	Time 21.905 (21.905)	Loss 0.4982 (0.4494)	CeLoss 0.1006 (0.0543)	SegCLSLoss 0.0046 (0.0078)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1119 (0.1062)	MaskBCELoss 0.0298 (0.0227)	MaskDICELoss 0.0821 (0.0835)
Epoch: [0][167/500]	Time 22.755 (22.755)	Loss 0.4369 (0.4158)	CeLoss 0.0166 (0.0390)	SegCLSLoss 0.0143 (0.0071)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1067 (0.0988)	MaskBCELoss 0.0174 (0.0163)	MaskDICELoss 0.0892 (0.0825)
Epoch: [0][168/500]	Time 26.500 (26.500)	Loss 0.4360 (0.4378)	CeLoss 0.0139 (0.0336)	SegCLSLoss 0.0177 (0.0099)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1113 (0.1100)	MaskBCELoss 0.0291 (0.0277)	MaskDICELoss 0.0821 (0.0823)
Epoch: [0][169/500]	Time 20.271 (20.271)	Loss 0.4160 (0.4077)	CeLoss 0.0078 (0.0230)	SegCLSLoss 0.0132 (0.0088)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1032 (0.1007)	MaskBCELoss 0.0154 (0.0178)	MaskDICELoss 0.0877 (0.0829)
Epoch: [0][170/500]	Time 24.820 (24.820)	Loss 0.4329 (0.4359)	CeLoss 0.0640 (0.0451)	SegCLSLoss 0.0075 (0.0078)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0954 (0.1033)	MaskBCELoss 0.0138 (0.0189)	MaskDICELoss 0.0816 (0.0844)
Epoch: [0][171/500]	Time 21.742 (21.742)	Loss 0.3561 (0.4263)	CeLoss 0.0065 (0.0461)	SegCLSLoss 0.0065 (0.0072)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0912 (0.1002)	MaskBCELoss 0.0142 (0.0176)	MaskDICELoss 0.0770 (0.0826)
Epoch: [0][172/500]	Time 22.978 (22.978)	Loss 0.4126 (0.4225)	CeLoss 0.0591 (0.0307)	SegCLSLoss 0.0066 (0.0092)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0908 (0.1031)	MaskBCELoss 0.0114 (0.0196)	MaskDICELoss 0.0794 (0.0835)
Epoch: [0][173/500]	Time 24.278 (24.278)	Loss 0.4422 (0.4228)	CeLoss 0.0669 (0.0417)	SegCLSLoss 0.0054 (0.0060)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1016 (0.1009)	MaskBCELoss 0.0208 (0.0171)	MaskDICELoss 0.0808 (0.0838)
Epoch: [0][174/500]	Time 23.645 (23.645)	Loss 0.3622 (0.4254)	CeLoss 0.0021 (0.0403)	SegCLSLoss 0.0049 (0.0066)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0968 (0.1028)	MaskBCELoss 0.0184 (0.0196)	MaskDICELoss 0.0783 (0.0832)
Epoch: [0][175/500]	Time 23.571 (23.571)	Loss 0.4495 (0.4409)	CeLoss 0.1299 (0.0607)	SegCLSLoss 0.0058 (0.0084)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0788 (0.0985)	MaskBCELoss 0.0036 (0.0153)	MaskDICELoss 0.0753 (0.0832)
Epoch: [0][176/500]	Time 22.639 (22.639)	Loss 0.4602 (0.3887)	CeLoss 0.0654 (0.0192)	SegCLSLoss 0.0052 (0.0090)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1050 (0.0948)	MaskBCELoss 0.0177 (0.0137)	MaskDICELoss 0.0874 (0.0810)
Epoch: [0][177/500]	Time 23.086 (23.086)	Loss 0.4610 (0.4472)	CeLoss 0.0928 (0.0651)	SegCLSLoss 0.0062 (0.0076)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1003 (0.1029)	MaskBCELoss 0.0228 (0.0224)	MaskDICELoss 0.0775 (0.0805)
Epoch: [0][178/500]	Time 20.883 (20.883)	Loss 0.4333 (0.4091)	CeLoss 0.0771 (0.0406)	SegCLSLoss 0.0052 (0.0061)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0908 (0.0969)	MaskBCELoss 0.0087 (0.0156)	MaskDICELoss 0.0821 (0.0812)
Epoch: [0][179/500]	Time 22.252 (22.252)	Loss 0.3594 (0.4166)	CeLoss 0.0386 (0.0335)	SegCLSLoss 0.0063 (0.0065)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0788 (0.1029)	MaskBCELoss 0.0036 (0.0207)	MaskDICELoss 0.0753 (0.0822)
Epoch: [0][180/500]	Time 20.798 (20.798)	Loss 0.5314 (0.4164)	CeLoss 0.0762 (0.0324)	SegCLSLoss 0.0187 (0.0077)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1272 (0.1026)	MaskBCELoss 0.0454 (0.0209)	MaskDICELoss 0.0818 (0.0817)
Epoch: [0][181/500]	Time 24.851 (24.851)	Loss 0.3768 (0.4092)	CeLoss 0.0039 (0.0269)	SegCLSLoss 0.0047 (0.0042)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0993 (0.1048)	MaskBCELoss 0.0167 (0.0225)	MaskDICELoss 0.0825 (0.0822)
Epoch: [0][182/500]	Time 22.260 (22.260)	Loss 0.4548 (0.3952)	CeLoss 0.0396 (0.0611)	SegCLSLoss 0.0044 (0.0064)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1173 (0.0863)	MaskBCELoss 0.0314 (0.0119)	MaskDICELoss 0.0859 (0.0744)
Epoch: [0][183/500]	Time 21.720 (21.720)	Loss 0.3541 (0.3909)	CeLoss 0.0049 (0.0247)	SegCLSLoss 0.0048 (0.0045)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0922 (0.0998)	MaskBCELoss 0.0147 (0.0209)	MaskDICELoss 0.0775 (0.0789)
Epoch: [0][184/500]	Time 24.394 (24.394)	Loss 0.4525 (0.3825)	CeLoss 0.0081 (0.0202)	SegCLSLoss 0.0101 (0.0049)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1169 (0.0959)	MaskBCELoss 0.0216 (0.0157)	MaskDICELoss 0.0952 (0.0803)
Epoch: [0][185/500]	Time 22.883 (22.883)	Loss 0.3705 (0.3979)	CeLoss 0.0017 (0.0232)	SegCLSLoss 0.0037 (0.0046)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1024 (0.1016)	MaskBCELoss 0.0241 (0.0205)	MaskDICELoss 0.0783 (0.0811)
Epoch: [0][186/500]	Time 20.458 (20.458)	Loss 0.4104 (0.4502)	CeLoss 0.0598 (0.0469)	SegCLSLoss 0.0044 (0.0054)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0950 (0.1100)	MaskBCELoss 0.0193 (0.0237)	MaskDICELoss 0.0758 (0.0863)
Epoch: [0][187/500]	Time 26.431 (26.431)	Loss 0.3811 (0.4143)	CeLoss 0.0017 (0.0318)	SegCLSLoss 0.0045 (0.0051)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1033 (0.1026)	MaskBCELoss 0.0214 (0.0191)	MaskDICELoss 0.0819 (0.0836)
Epoch: [0][188/500]	Time 21.455 (21.455)	Loss 0.4016 (0.4141)	CeLoss 0.0035 (0.0369)	SegCLSLoss 0.0046 (0.0037)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1098 (0.1031)	MaskBCELoss 0.0252 (0.0213)	MaskDICELoss 0.0846 (0.0818)
Epoch: [0][189/500]	Time 23.304 (23.304)	Loss 0.3689 (0.4150)	CeLoss 0.0046 (0.0381)	SegCLSLoss 0.0050 (0.0048)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0962 (0.1009)	MaskBCELoss 0.0153 (0.0181)	MaskDICELoss 0.0809 (0.0828)
Epoch: [0][190/500]	Time 22.582 (22.582)	Loss 0.4370 (0.4074)	CeLoss 0.1001 (0.0239)	SegCLSLoss 0.0033 (0.0043)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0871 (0.1048)	MaskBCELoss 0.0088 (0.0221)	MaskDICELoss 0.0783 (0.0827)
Epoch: [0][191/500]	Time 21.563 (21.563)	Loss 0.3694 (0.3930)	CeLoss 0.0010 (0.0101)	SegCLSLoss 0.0033 (0.0041)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1008 (0.1042)	MaskBCELoss 0.0206 (0.0210)	MaskDICELoss 0.0802 (0.0832)
Epoch: [0][192/500]	Time 21.606 (21.606)	Loss 0.3374 (0.3979)	CeLoss 0.0032 (0.0183)	SegCLSLoss 0.0033 (0.0039)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0885 (0.1044)	MaskBCELoss 0.0131 (0.0229)	MaskDICELoss 0.0754 (0.0815)
Epoch: [0][193/500]	Time 24.160 (24.160)	Loss 0.4449 (0.3985)	CeLoss 0.0771 (0.0243)	SegCLSLoss 0.0037 (0.0039)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0978 (0.1006)	MaskBCELoss 0.0154 (0.0180)	MaskDICELoss 0.0824 (0.0826)
Epoch: [0][194/500]	Time 19.673 (19.673)	Loss 0.4201 (0.4103)	CeLoss 0.0035 (0.0484)	SegCLSLoss 0.0040 (0.0046)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1181 (0.0974)	MaskBCELoss 0.0318 (0.0184)	MaskDICELoss 0.0862 (0.0790)
Epoch: [0][195/500]	Time 21.007 (21.007)	Loss 0.4392 (0.4056)	CeLoss 0.0703 (0.0412)	SegCLSLoss 0.0035 (0.0034)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1048 (0.0984)	MaskBCELoss 0.0286 (0.0179)	MaskDICELoss 0.0761 (0.0805)
Epoch: [0][196/500]	Time 22.819 (22.819)	Loss 0.4704 (0.4192)	CeLoss 0.0425 (0.0412)	SegCLSLoss 0.0023 (0.0028)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1228 (0.1022)	MaskBCELoss 0.0339 (0.0183)	MaskDICELoss 0.0889 (0.0839)
Epoch: [0][197/500]	Time 23.669 (23.669)	Loss 0.3592 (0.4529)	CeLoss 0.0048 (0.0523)	SegCLSLoss 0.0051 (0.0040)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0948 (0.1122)	MaskBCELoss 0.0175 (0.0282)	MaskDICELoss 0.0773 (0.0840)
Epoch: [0][198/500]	Time 22.242 (22.242)	Loss 0.4225 (0.4024)	CeLoss 0.0047 (0.0266)	SegCLSLoss 0.0045 (0.0040)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1162 (0.1016)	MaskBCELoss 0.0280 (0.0193)	MaskDICELoss 0.0882 (0.0823)
Epoch: [0][199/500]	Time 21.668 (21.668)	Loss 0.4052 (0.3982)	CeLoss 0.0053 (0.0298)	SegCLSLoss 0.0058 (0.0039)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1088 (0.1006)	MaskBCELoss 0.0234 (0.0209)	MaskDICELoss 0.0854 (0.0797)
[2025-04-04 21:52:13,424] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[0.0002985903614457831], mom=[(0.9, 0.95)]
[2025-04-04 21:52:13,433] [INFO] [timer.py:215:stop] epoch=0/micro_step=200/global_step=20, RunningAvgSamplesPerSec=1.794234248947212, CurrSamplesPerSec=1.7510909812867699, MemAllocated=36.06GB, MaxMemAllocated=45.56GB
Epoch: [0][200/500]	Time 24.171 (24.171)	Loss 0.4409 (0.3933)	CeLoss 0.0041 (0.0260)	SegCLSLoss 0.0036 (0.0035)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1179 (0.0993)	MaskBCELoss 0.0210 (0.0185)	MaskDICELoss 0.0969 (0.0808)
Epoch: [0][201/500]	Time 21.886 (21.886)	Loss 0.4424 (0.4099)	CeLoss 0.0033 (0.0287)	SegCLSLoss 0.0030 (0.0028)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1270 (0.1052)	MaskBCELoss 0.0376 (0.0227)	MaskDICELoss 0.0895 (0.0825)
Epoch: [0][202/500]	Time 22.589 (22.589)	Loss 0.3946 (0.3938)	CeLoss 0.0322 (0.0255)	SegCLSLoss 0.0025 (0.0032)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0999 (0.1002)	MaskBCELoss 0.0211 (0.0194)	MaskDICELoss 0.0788 (0.0808)
Epoch: [0][203/500]	Time 25.028 (25.028)	Loss 0.4086 (0.4071)	CeLoss 0.0014 (0.0253)	SegCLSLoss 0.0029 (0.0026)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1126 (0.1046)	MaskBCELoss 0.0244 (0.0209)	MaskDICELoss 0.0882 (0.0837)
Epoch: [0][204/500]	Time 22.703 (22.703)	Loss 0.4050 (0.3933)	CeLoss 0.0032 (0.0354)	SegCLSLoss 0.0035 (0.0023)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1125 (0.0965)	MaskBCELoss 0.0277 (0.0164)	MaskDICELoss 0.0849 (0.0801)
Epoch: [0][205/500]	Time 25.837 (25.837)	Loss 0.3575 (0.4211)	CeLoss 0.0030 (0.0360)	SegCLSLoss 0.0068 (0.0043)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0979 (0.1067)	MaskBCELoss 0.0255 (0.0250)	MaskDICELoss 0.0725 (0.0816)
Epoch: [0][206/500]	Time 23.431 (23.431)	Loss 0.4020 (0.4182)	CeLoss 0.0023 (0.0250)	SegCLSLoss 0.0026 (0.0028)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1142 (0.1089)	MaskBCELoss 0.0312 (0.0240)	MaskDICELoss 0.0830 (0.0849)
Epoch: [0][207/500]	Time 24.264 (24.264)	Loss 0.3945 (0.4268)	CeLoss 0.0520 (0.0557)	SegCLSLoss 0.0039 (0.0040)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0901 (0.1003)	MaskBCELoss 0.0128 (0.0191)	MaskDICELoss 0.0773 (0.0812)
Epoch: [0][208/500]	Time 23.649 (23.649)	Loss 0.3761 (0.4076)	CeLoss 0.0044 (0.0325)	SegCLSLoss 0.0031 (0.0031)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1022 (0.1018)	MaskBCELoss 0.0216 (0.0192)	MaskDICELoss 0.0806 (0.0826)
Epoch: [0][209/500]	Time 19.837 (19.837)	Loss 0.3452 (0.3902)	CeLoss 0.0056 (0.0232)	SegCLSLoss 0.0026 (0.0027)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0897 (0.1003)	MaskBCELoss 0.0123 (0.0197)	MaskDICELoss 0.0774 (0.0805)
Epoch: [0][210/500]	Time 21.678 (21.678)	Loss 0.3998 (0.4045)	CeLoss 0.0019 (0.0342)	SegCLSLoss 0.0035 (0.0031)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1081 (0.0996)	MaskBCELoss 0.0208 (0.0172)	MaskDICELoss 0.0873 (0.0824)
Epoch: [0][211/500]	Time 23.525 (23.525)	Loss 0.4210 (0.4032)	CeLoss 0.0018 (0.0235)	SegCLSLoss 0.0098 (0.0040)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1125 (0.1046)	MaskBCELoss 0.0252 (0.0234)	MaskDICELoss 0.0873 (0.0812)
Epoch: [0][212/500]	Time 22.962 (22.962)	Loss 0.4242 (0.4244)	CeLoss 0.0211 (0.0370)	SegCLSLoss 0.0020 (0.0039)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1174 (0.1054)	MaskBCELoss 0.0352 (0.0209)	MaskDICELoss 0.0822 (0.0845)
Epoch: [0][213/500]	Time 21.863 (21.863)	Loss 0.3968 (0.3924)	CeLoss 0.0383 (0.0417)	SegCLSLoss 0.0023 (0.0025)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0995 (0.0940)	MaskBCELoss 0.0220 (0.0152)	MaskDICELoss 0.0774 (0.0789)
Epoch: [0][214/500]	Time 22.444 (22.444)	Loss 0.3798 (0.3899)	CeLoss 0.0435 (0.0249)	SegCLSLoss 0.0034 (0.0033)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0904 (0.0975)	MaskBCELoss 0.0162 (0.0157)	MaskDICELoss 0.0743 (0.0818)
Epoch: [0][215/500]	Time 19.229 (19.229)	Loss 0.3440 (0.3781)	CeLoss 0.0007 (0.0151)	SegCLSLoss 0.0022 (0.0028)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0937 (0.0975)	MaskBCELoss 0.0180 (0.0164)	MaskDICELoss 0.0757 (0.0812)
Epoch: [0][216/500]	Time 24.666 (24.666)	Loss 0.4272 (0.3940)	CeLoss 0.0732 (0.0215)	SegCLSLoss 0.0016 (0.0033)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0961 (0.0998)	MaskBCELoss 0.0167 (0.0167)	MaskDICELoss 0.0794 (0.0832)
Epoch: [0][217/500]	Time 22.902 (22.902)	Loss 0.3288 (0.4074)	CeLoss 0.0011 (0.0235)	SegCLSLoss 0.0016 (0.0035)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0851 (0.1037)	MaskBCELoss 0.0081 (0.0189)	MaskDICELoss 0.0771 (0.0848)
Epoch: [0][218/500]	Time 20.635 (20.635)	Loss 0.4593 (0.3930)	CeLoss 0.0674 (0.0289)	SegCLSLoss 0.0026 (0.0028)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1083 (0.0982)	MaskBCELoss 0.0232 (0.0172)	MaskDICELoss 0.0851 (0.0811)
Epoch: [0][219/500]	Time 21.228 (21.228)	Loss 0.4150 (0.4088)	CeLoss 0.0016 (0.0290)	SegCLSLoss 0.0045 (0.0041)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1094 (0.1038)	MaskBCELoss 0.0165 (0.0218)	MaskDICELoss 0.0929 (0.0820)
Epoch: [0][220/500]	Time 24.364 (24.364)	Loss 0.3423 (0.4108)	CeLoss 0.0034 (0.0370)	SegCLSLoss 0.0044 (0.0032)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0856 (0.1027)	MaskBCELoss 0.0061 (0.0218)	MaskDICELoss 0.0795 (0.0809)
Epoch: [0][221/500]	Time 22.033 (22.033)	Loss 0.4946 (0.3887)	CeLoss 0.0649 (0.0291)	SegCLSLoss 0.0074 (0.0033)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1198 (0.0968)	MaskBCELoss 0.0322 (0.0171)	MaskDICELoss 0.0876 (0.0797)
Epoch: [0][222/500]	Time 20.497 (20.497)	Loss 0.3956 (0.3934)	CeLoss 0.0493 (0.0409)	SegCLSLoss 0.0032 (0.0023)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0945 (0.0935)	MaskBCELoss 0.0192 (0.0132)	MaskDICELoss 0.0753 (0.0804)
Epoch: [0][223/500]	Time 23.752 (23.752)	Loss 0.4003 (0.3885)	CeLoss 0.0713 (0.0277)	SegCLSLoss 0.0028 (0.0023)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0854 (0.0987)	MaskBCELoss 0.0091 (0.0193)	MaskDICELoss 0.0763 (0.0794)
Epoch: [0][224/500]	Time 19.110 (19.110)	Loss 0.2956 (0.3450)	CeLoss 0.0005 (0.0083)	SegCLSLoss 0.0002 (0.0035)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0749 (0.0876)	MaskBCELoss 0.0026 (0.0104)	MaskDICELoss 0.0724 (0.0773)
Epoch: [0][225/500]	Time 29.271 (29.271)	Loss 0.4356 (0.4382)	CeLoss 0.1104 (0.0437)	SegCLSLoss 0.0011 (0.0025)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0884 (0.1087)	MaskBCELoss 0.0153 (0.0226)	MaskDICELoss 0.0731 (0.0861)
Epoch: [0][226/500]	Time 21.460 (21.460)	Loss 0.3884 (0.3782)	CeLoss 0.0016 (0.0169)	SegCLSLoss 0.0030 (0.0025)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1021 (0.0984)	MaskBCELoss 0.0138 (0.0187)	MaskDICELoss 0.0883 (0.0797)
Epoch: [0][227/500]	Time 23.097 (23.097)	Loss 0.4805 (0.3920)	CeLoss 0.0339 (0.0267)	SegCLSLoss 0.0009 (0.0024)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1394 (0.0987)	MaskBCELoss 0.0564 (0.0172)	MaskDICELoss 0.0830 (0.0815)
Epoch: [0][228/500]	Time 22.098 (22.098)	Loss 0.3708 (0.3940)	CeLoss 0.0028 (0.0208)	SegCLSLoss 0.0021 (0.0024)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1000 (0.1017)	MaskBCELoss 0.0181 (0.0193)	MaskDICELoss 0.0819 (0.0825)
Epoch: [0][229/500]	Time 23.848 (23.848)	Loss 0.4386 (0.3819)	CeLoss 0.0669 (0.0469)	SegCLSLoss 0.0036 (0.0025)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1002 (0.0914)	MaskBCELoss 0.0181 (0.0178)	MaskDICELoss 0.0822 (0.0736)
Epoch: [0][230/500]	Time 22.436 (22.436)	Loss 0.3819 (0.4046)	CeLoss 0.0574 (0.0417)	SegCLSLoss 0.0012 (0.0019)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0894 (0.1004)	MaskBCELoss 0.0177 (0.0212)	MaskDICELoss 0.0717 (0.0792)
Epoch: [0][231/500]	Time 26.010 (26.010)	Loss 0.3863 (0.3934)	CeLoss 0.0005 (0.0140)	SegCLSLoss 0.0026 (0.0022)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1057 (0.1026)	MaskBCELoss 0.0211 (0.0177)	MaskDICELoss 0.0846 (0.0849)
Epoch: [0][232/500]	Time 20.710 (20.710)	Loss 0.4459 (0.4076)	CeLoss 0.0400 (0.0204)	SegCLSLoss 0.0014 (0.0017)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1153 (0.1054)	MaskBCELoss 0.0291 (0.0189)	MaskDICELoss 0.0862 (0.0865)
Epoch: [0][233/500]	Time 24.181 (24.181)	Loss 0.3689 (0.3833)	CeLoss 0.0013 (0.0084)	SegCLSLoss 0.0029 (0.0023)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0974 (0.1008)	MaskBCELoss 0.0140 (0.0164)	MaskDICELoss 0.0835 (0.0844)
Epoch: [0][234/500]	Time 23.338 (23.338)	Loss 0.3536 (0.3929)	CeLoss 0.0005 (0.0291)	SegCLSLoss 0.0014 (0.0017)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0905 (0.0977)	MaskBCELoss 0.0057 (0.0151)	MaskDICELoss 0.0848 (0.0826)
Epoch: [0][235/500]	Time 22.905 (22.905)	Loss 0.3701 (0.4158)	CeLoss 0.0010 (0.0400)	SegCLSLoss 0.0028 (0.0018)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0973 (0.1040)	MaskBCELoss 0.0127 (0.0219)	MaskDICELoss 0.0845 (0.0821)
Epoch: [0][236/500]	Time 23.629 (23.629)	Loss 0.4379 (0.4103)	CeLoss 0.0957 (0.0403)	SegCLSLoss 0.0013 (0.0021)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0903 (0.0982)	MaskBCELoss 0.0108 (0.0137)	MaskDICELoss 0.0795 (0.0846)
Epoch: [0][237/500]	Time 26.668 (26.668)	Loss 0.3941 (0.4356)	CeLoss 0.0007 (0.0481)	SegCLSLoss 0.0028 (0.0027)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1108 (0.1053)	MaskBCELoss 0.0277 (0.0195)	MaskDICELoss 0.0831 (0.0858)
Epoch: [0][238/500]	Time 21.777 (21.777)	Loss 0.4496 (0.4124)	CeLoss 0.0747 (0.0344)	SegCLSLoss 0.0026 (0.0019)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1024 (0.1051)	MaskBCELoss 0.0199 (0.0231)	MaskDICELoss 0.0825 (0.0820)
Epoch: [0][239/500]	Time 23.825 (23.825)	Loss 0.4312 (0.4112)	CeLoss 0.0006 (0.0237)	SegCLSLoss 0.0021 (0.0018)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1196 (0.1056)	MaskBCELoss 0.0260 (0.0191)	MaskDICELoss 0.0936 (0.0864)
Epoch: [0][240/500]	Time 24.055 (24.055)	Loss 0.4242 (0.4146)	CeLoss 0.0010 (0.0302)	SegCLSLoss 0.0021 (0.0020)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1276 (0.1041)	MaskBCELoss 0.0457 (0.0179)	MaskDICELoss 0.0819 (0.0861)
Epoch: [0][241/500]	Time 24.150 (24.150)	Loss 0.3882 (0.3840)	CeLoss 0.0435 (0.0245)	SegCLSLoss 0.0014 (0.0019)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0962 (0.0991)	MaskBCELoss 0.0214 (0.0204)	MaskDICELoss 0.0748 (0.0787)
Epoch: [0][242/500]	Time 18.993 (18.993)	Loss 0.3909 (0.3643)	CeLoss 0.0884 (0.0157)	SegCLSLoss 0.0001 (0.0017)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0780 (0.0942)	MaskBCELoss 0.0049 (0.0159)	MaskDICELoss 0.0731 (0.0783)
Epoch: [0][243/500]	Time 26.995 (26.995)	Loss 0.4059 (0.4144)	CeLoss 0.0674 (0.0349)	SegCLSLoss 0.0009 (0.0018)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0931 (0.1068)	MaskBCELoss 0.0176 (0.0257)	MaskDICELoss 0.0755 (0.0812)
Epoch: [0][244/500]	Time 21.592 (21.592)	Loss 0.4438 (0.3786)	CeLoss 0.0405 (0.0206)	SegCLSLoss 0.0015 (0.0012)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1144 (0.0971)	MaskBCELoss 0.0287 (0.0164)	MaskDICELoss 0.0857 (0.0807)
Epoch: [0][245/500]	Time 24.578 (24.578)	Loss 0.3519 (0.3981)	CeLoss 0.0007 (0.0278)	SegCLSLoss 0.0018 (0.0016)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0921 (0.1020)	MaskBCELoss 0.0103 (0.0204)	MaskDICELoss 0.0817 (0.0816)
Epoch: [0][246/500]	Time 21.945 (21.945)	Loss 0.3335 (0.3657)	CeLoss 0.0002 (0.0103)	SegCLSLoss 0.0012 (0.0029)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0866 (0.0965)	MaskBCELoss 0.0077 (0.0182)	MaskDICELoss 0.0789 (0.0783)
Epoch: [0][247/500]	Time 23.091 (23.091)	Loss 0.3876 (0.3859)	CeLoss 0.0403 (0.0268)	SegCLSLoss 0.0016 (0.0015)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0958 (0.0983)	MaskBCELoss 0.0195 (0.0185)	MaskDICELoss 0.0763 (0.0798)
Epoch: [0][248/500]	Time 22.618 (22.618)	Loss 0.4705 (0.3872)	CeLoss 0.0610 (0.0245)	SegCLSLoss 0.0014 (0.0014)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1151 (0.0992)	MaskBCELoss 0.0268 (0.0186)	MaskDICELoss 0.0883 (0.0807)
Epoch: [0][249/500]	Time 22.276 (22.276)	Loss 0.3934 (0.3936)	CeLoss 0.0251 (0.0268)	SegCLSLoss 0.0005 (0.0017)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1082 (0.0996)	MaskBCELoss 0.0328 (0.0175)	MaskDICELoss 0.0754 (0.0821)
Epoch: [0][250/500]	Time 25.456 (25.456)	Loss 0.4102 (0.4114)	CeLoss 0.0645 (0.0419)	SegCLSLoss 0.0028 (0.0023)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0896 (0.1007)	MaskBCELoss 0.0091 (0.0190)	MaskDICELoss 0.0805 (0.0817)
Epoch: [0][251/500]	Time 25.777 (25.777)	Loss 0.4971 (0.4202)	CeLoss 0.0684 (0.0343)	SegCLSLoss 0.0022 (0.0014)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1217 (0.1090)	MaskBCELoss 0.0312 (0.0264)	MaskDICELoss 0.0905 (0.0826)
Epoch: [0][252/500]	Time 21.633 (21.633)	Loss 0.4044 (0.3931)	CeLoss 0.0023 (0.0205)	SegCLSLoss 0.0020 (0.0017)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1157 (0.1019)	MaskBCELoss 0.0323 (0.0192)	MaskDICELoss 0.0834 (0.0827)
Epoch: [0][253/500]	Time 24.244 (24.244)	Loss 0.4202 (0.4073)	CeLoss 0.0454 (0.0358)	SegCLSLoss 0.0016 (0.0019)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1011 (0.1016)	MaskBCELoss 0.0164 (0.0194)	MaskDICELoss 0.0847 (0.0822)
Epoch: [0][254/500]	Time 24.155 (24.155)	Loss 0.3888 (0.3814)	CeLoss 0.0008 (0.0144)	SegCLSLoss 0.0022 (0.0013)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1062 (0.1031)	MaskBCELoss 0.0208 (0.0240)	MaskDICELoss 0.0855 (0.0791)
Epoch: [0][255/500]	Time 23.381 (23.381)	Loss 0.3563 (0.3768)	CeLoss 0.0002 (0.0153)	SegCLSLoss 0.0015 (0.0012)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0983 (0.0998)	MaskBCELoss 0.0201 (0.0200)	MaskDICELoss 0.0782 (0.0798)
Epoch: [0][256/500]	Time 21.856 (21.856)	Loss 0.3564 (0.3780)	CeLoss 0.0286 (0.0384)	SegCLSLoss 0.0018 (0.0012)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0845 (0.0921)	MaskBCELoss 0.0069 (0.0155)	MaskDICELoss 0.0776 (0.0765)
Epoch: [0][257/500]	Time 23.888 (23.888)	Loss 0.4068 (0.4225)	CeLoss 0.0566 (0.0434)	SegCLSLoss 0.0014 (0.0010)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0958 (0.1044)	MaskBCELoss 0.0178 (0.0203)	MaskDICELoss 0.0780 (0.0841)
Epoch: [0][258/500]	Time 20.205 (20.205)	Loss 0.5123 (0.3828)	CeLoss 0.0554 (0.0227)	SegCLSLoss 0.0021 (0.0013)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1391 (0.1003)	MaskBCELoss 0.0519 (0.0218)	MaskDICELoss 0.0872 (0.0785)
Epoch: [0][259/500]	Time 24.891 (24.891)	Loss 0.3936 (0.3903)	CeLoss 0.0625 (0.0281)	SegCLSLoss 0.0006 (0.0021)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0886 (0.0994)	MaskBCELoss 0.0123 (0.0198)	MaskDICELoss 0.0763 (0.0796)
Epoch: [0][260/500]	Time 24.135 (24.135)	Loss 0.4251 (0.4036)	CeLoss 0.0386 (0.0398)	SegCLSLoss 0.0019 (0.0014)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1056 (0.0980)	MaskBCELoss 0.0197 (0.0155)	MaskDICELoss 0.0859 (0.0825)
Epoch: [0][261/500]	Time 20.204 (20.204)	Loss 0.4018 (0.4037)	CeLoss 0.0850 (0.0508)	SegCLSLoss 0.0004 (0.0008)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0856 (0.0978)	MaskBCELoss 0.0132 (0.0200)	MaskDICELoss 0.0723 (0.0778)
Epoch: [0][262/500]	Time 24.203 (24.203)	Loss 0.4119 (0.3886)	CeLoss 0.0007 (0.0196)	SegCLSLoss 0.0006 (0.0013)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1193 (0.1040)	MaskBCELoss 0.0336 (0.0248)	MaskDICELoss 0.0857 (0.0792)
Epoch: [0][263/500]	Time 21.835 (21.835)	Loss 0.4304 (0.3888)	CeLoss 0.1030 (0.0469)	SegCLSLoss 0.0004 (0.0014)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0865 (0.0943)	MaskBCELoss 0.0099 (0.0190)	MaskDICELoss 0.0767 (0.0753)
Epoch: [0][264/500]	Time 23.661 (23.661)	Loss 0.3583 (0.3842)	CeLoss 0.0001 (0.0368)	SegCLSLoss 0.0003 (0.0010)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1008 (0.0964)	MaskBCELoss 0.0228 (0.0202)	MaskDICELoss 0.0780 (0.0763)
Epoch: [0][265/500]	Time 22.790 (22.790)	Loss 0.3269 (0.3673)	CeLoss 0.0029 (0.0162)	SegCLSLoss 0.0010 (0.0011)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0933 (0.0987)	MaskBCELoss 0.0256 (0.0229)	MaskDICELoss 0.0677 (0.0758)
Epoch: [0][266/500]	Time 20.743 (20.743)	Loss 0.4240 (0.3964)	CeLoss 0.0018 (0.0337)	SegCLSLoss 0.0048 (0.0019)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1154 (0.0976)	MaskBCELoss 0.0244 (0.0158)	MaskDICELoss 0.0909 (0.0819)
Epoch: [0][267/500]	Time 25.498 (25.498)	Loss 0.4267 (0.4071)	CeLoss 0.0581 (0.0263)	SegCLSLoss 0.0025 (0.0013)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0973 (0.1055)	MaskBCELoss 0.0129 (0.0220)	MaskDICELoss 0.0844 (0.0835)
Epoch: [0][268/500]	Time 22.186 (22.186)	Loss 0.3506 (0.3679)	CeLoss 0.0003 (0.0214)	SegCLSLoss 0.0052 (0.0014)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0928 (0.0949)	MaskBCELoss 0.0157 (0.0181)	MaskDICELoss 0.0771 (0.0769)
Epoch: [0][269/500]	Time 23.007 (23.007)	Loss 0.4226 (0.3825)	CeLoss 0.0850 (0.0444)	SegCLSLoss 0.0008 (0.0017)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0944 (0.0929)	MaskBCELoss 0.0207 (0.0184)	MaskDICELoss 0.0737 (0.0745)
Epoch: [0][270/500]	Time 22.578 (22.578)	Loss 0.3346 (0.3616)	CeLoss 0.0011 (0.0123)	SegCLSLoss 0.0016 (0.0017)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0860 (0.0940)	MaskBCELoss 0.0069 (0.0151)	MaskDICELoss 0.0792 (0.0789)
Epoch: [0][271/500]	Time 21.870 (21.870)	Loss 0.3308 (0.3709)	CeLoss 0.0002 (0.0197)	SegCLSLoss 0.0297 (0.0084)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0695 (0.0917)	MaskBCELoss 0.0035 (0.0162)	MaskDICELoss 0.0660 (0.0755)
Epoch: [0][272/500]	Time 21.380 (21.380)	Loss 0.3919 (0.3774)	CeLoss 0.0581 (0.0320)	SegCLSLoss 0.0105 (0.0096)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0864 (0.0889)	MaskBCELoss 0.0164 (0.0148)	MaskDICELoss 0.0699 (0.0742)
Epoch: [0][273/500]	Time 20.217 (20.217)	Loss 0.3419 (0.3847)	CeLoss 0.0016 (0.0311)	SegCLSLoss 0.0084 (0.0072)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0896 (0.0948)	MaskBCELoss 0.0175 (0.0199)	MaskDICELoss 0.0721 (0.0749)
Epoch: [0][274/500]	Time 23.184 (23.184)	Loss 0.3568 (0.3895)	CeLoss 0.0038 (0.0418)	SegCLSLoss 0.0062 (0.0089)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0925 (0.0882)	MaskBCELoss 0.0148 (0.0116)	MaskDICELoss 0.0778 (0.0766)
Epoch: [0][275/500]	Time 19.866 (19.866)	Loss 0.3850 (0.3862)	CeLoss 0.0562 (0.0263)	SegCLSLoss 0.0287 (0.0106)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0695 (0.0951)	MaskBCELoss 0.0035 (0.0209)	MaskDICELoss 0.0660 (0.0742)
Epoch: [0][276/500]	Time 21.223 (21.223)	Loss 0.4120 (0.3955)	CeLoss 0.0002 (0.0354)	SegCLSLoss 0.0011 (0.0068)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1105 (0.0967)	MaskBCELoss 0.0162 (0.0201)	MaskDICELoss 0.0943 (0.0766)
Epoch: [0][277/500]	Time 22.798 (22.798)	Loss 0.3530 (0.3985)	CeLoss 0.0002 (0.0332)	SegCLSLoss 0.0410 (0.0082)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0694 (0.0979)	MaskBCELoss 0.0035 (0.0213)	MaskDICELoss 0.0659 (0.0766)
Epoch: [0][278/500]	Time 22.026 (22.026)	Loss 0.3927 (0.3767)	CeLoss 0.0469 (0.0212)	SegCLSLoss 0.0037 (0.0038)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0964 (0.0974)	MaskBCELoss 0.0235 (0.0208)	MaskDICELoss 0.0729 (0.0766)
Epoch: [0][279/500]	Time 21.610 (21.610)	Loss 0.4540 (0.3840)	CeLoss 0.0327 (0.0388)	SegCLSLoss 0.0054 (0.0157)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1217 (0.0860)	MaskBCELoss 0.0383 (0.0152)	MaskDICELoss 0.0835 (0.0708)
Epoch: [0][280/500]	Time 22.829 (22.829)	Loss 0.3879 (0.3848)	CeLoss 0.0515 (0.0268)	SegCLSLoss 0.0150 (0.0088)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0823 (0.0939)	MaskBCELoss 0.0112 (0.0176)	MaskDICELoss 0.0710 (0.0763)
Epoch: [0][281/500]	Time 21.803 (21.803)	Loss 0.5250 (0.3956)	CeLoss 0.0757 (0.0230)	SegCLSLoss 0.0011 (0.0010)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1259 (0.1052)	MaskBCELoss 0.0283 (0.0252)	MaskDICELoss 0.0975 (0.0800)
Epoch: [0][282/500]	Time 27.966 (27.966)	Loss 0.3418 (0.3843)	CeLoss 0.0002 (0.0209)	SegCLSLoss 0.0007 (0.0013)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0975 (0.1026)	MaskBCELoss 0.0249 (0.0248)	MaskDICELoss 0.0726 (0.0778)
Epoch: [0][283/500]	Time 22.545 (22.545)	Loss 0.4456 (0.4112)	CeLoss 0.0422 (0.0239)	SegCLSLoss 0.0002 (0.0009)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1211 (0.1122)	MaskBCELoss 0.0406 (0.0317)	MaskDICELoss 0.0805 (0.0805)
Epoch: [0][284/500]	Time 20.232 (20.232)	Loss 0.3332 (0.3542)	CeLoss 0.0004 (0.0197)	SegCLSLoss 0.0005 (0.0017)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0885 (0.0909)	MaskBCELoss 0.0110 (0.0163)	MaskDICELoss 0.0774 (0.0746)
Epoch: [0][285/500]	Time 28.854 (28.854)	Loss 0.2888 (0.3748)	CeLoss 0.0010 (0.0308)	SegCLSLoss 0.0002 (0.0011)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0758 (0.0955)	MaskBCELoss 0.0079 (0.0200)	MaskDICELoss 0.0679 (0.0755)
Epoch: [0][286/500]	Time 23.953 (23.953)	Loss 0.3975 (0.3567)	CeLoss 0.0006 (0.0191)	SegCLSLoss 0.0014 (0.0018)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1121 (0.0936)	MaskBCELoss 0.0271 (0.0201)	MaskDICELoss 0.0850 (0.0734)
Epoch: [0][287/500]	Time 22.226 (22.226)	Loss 0.4719 (0.3750)	CeLoss 0.0474 (0.0381)	SegCLSLoss 0.0013 (0.0010)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1184 (0.0916)	MaskBCELoss 0.0258 (0.0158)	MaskDICELoss 0.0926 (0.0758)
Epoch: [0][288/500]	Time 24.872 (24.872)	Loss 0.3396 (0.3657)	CeLoss 0.0005 (0.0182)	SegCLSLoss 0.0002 (0.0008)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0908 (0.0967)	MaskBCELoss 0.0122 (0.0204)	MaskDICELoss 0.0786 (0.0763)
Epoch: [0][289/500]	Time 24.186 (24.186)	Loss 0.4258 (0.3908)	CeLoss 0.0002 (0.0329)	SegCLSLoss 0.0012 (0.0008)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1269 (0.1023)	MaskBCELoss 0.0421 (0.0264)	MaskDICELoss 0.0848 (0.0759)
Epoch: [0][290/500]	Time 25.456 (25.456)	Loss 0.2659 (0.3483)	CeLoss 0.0002 (0.0089)	SegCLSLoss 0.0002 (0.0016)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0682 (0.0934)	MaskBCELoss 0.0037 (0.0187)	MaskDICELoss 0.0645 (0.0747)
Epoch: [0][291/500]	Time 22.304 (22.304)	Loss 0.2899 (0.3775)	CeLoss 0.0002 (0.0310)	SegCLSLoss 0.0021 (0.0019)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0731 (0.0952)	MaskBCELoss 0.0036 (0.0192)	MaskDICELoss 0.0695 (0.0761)
Epoch: [0][292/500]	Time 22.753 (22.753)	Loss 0.4560 (0.3940)	CeLoss 0.0005 (0.0302)	SegCLSLoss 0.0027 (0.0017)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1394 (0.1012)	MaskBCELoss 0.0538 (0.0222)	MaskDICELoss 0.0857 (0.0790)
Epoch: [0][293/500]	Time 19.153 (19.153)	Loss 0.3019 (0.3481)	CeLoss 0.0002 (0.0123)	SegCLSLoss 0.0047 (0.0019)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0752 (0.0905)	MaskBCELoss 0.0043 (0.0150)	MaskDICELoss 0.0709 (0.0755)
Epoch: [0][294/500]	Time 25.633 (25.633)	Loss 0.4194 (0.3875)	CeLoss 0.0732 (0.0343)	SegCLSLoss 0.0008 (0.0030)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0967 (0.0957)	MaskBCELoss 0.0211 (0.0179)	MaskDICELoss 0.0757 (0.0778)
Epoch: [0][295/500]	Time 20.387 (20.387)	Loss 0.3579 (0.3645)	CeLoss 0.0002 (0.0282)	SegCLSLoss 0.0017 (0.0019)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0997 (0.0919)	MaskBCELoss 0.0224 (0.0175)	MaskDICELoss 0.0774 (0.0744)
Epoch: [0][296/500]	Time 22.875 (22.875)	Loss 0.5134 (0.3937)	CeLoss 0.0693 (0.0213)	SegCLSLoss 0.0074 (0.0025)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1239 (0.1024)	MaskBCELoss 0.0333 (0.0213)	MaskDICELoss 0.0906 (0.0812)
Epoch: [0][297/500]	Time 23.037 (23.037)	Loss 0.4070 (0.3899)	CeLoss 0.0312 (0.0289)	SegCLSLoss 0.0011 (0.0019)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1032 (0.0997)	MaskBCELoss 0.0197 (0.0209)	MaskDICELoss 0.0836 (0.0788)
Epoch: [0][298/500]	Time 24.701 (24.701)	Loss 0.3792 (0.4145)	CeLoss 0.0464 (0.0361)	SegCLSLoss 0.0020 (0.0034)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0901 (0.1060)	MaskBCELoss 0.0157 (0.0262)	MaskDICELoss 0.0743 (0.0798)
Epoch: [0][299/500]	Time 25.811 (25.811)	Loss 0.3492 (0.3790)	CeLoss 0.0008 (0.0208)	SegCLSLoss 0.0028 (0.0029)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0900 (0.0973)	MaskBCELoss 0.0086 (0.0185)	MaskDICELoss 0.0814 (0.0788)
[2025-04-04 22:30:32,519] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[0.0002972650602409638], mom=[(0.9, 0.95)]
[2025-04-04 22:30:32,526] [INFO] [timer.py:215:stop] epoch=0/micro_step=300/global_step=30, RunningAvgSamplesPerSec=1.7624705450420302, CurrSamplesPerSec=1.6597864620251594, MemAllocated=34.4GB, MaxMemAllocated=45.56GB
Epoch: [0][300/500]	Time 21.360 (21.360)	Loss 0.3147 (0.3809)	CeLoss 0.0001 (0.0287)	SegCLSLoss 0.0017 (0.0017)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0822 (0.0976)	MaskBCELoss 0.0088 (0.0208)	MaskDICELoss 0.0734 (0.0768)
Epoch: [0][301/500]	Time 24.562 (24.562)	Loss 0.3822 (0.3917)	CeLoss 0.0452 (0.0213)	SegCLSLoss 0.0023 (0.0038)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0922 (0.1019)	MaskBCELoss 0.0181 (0.0226)	MaskDICELoss 0.0741 (0.0794)
Epoch: [0][302/500]	Time 24.448 (24.448)	Loss 0.3179 (0.3841)	CeLoss 0.0004 (0.0162)	SegCLSLoss 0.0027 (0.0017)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0838 (0.1020)	MaskBCELoss 0.0116 (0.0217)	MaskDICELoss 0.0722 (0.0803)
Epoch: [0][303/500]	Time 23.296 (23.296)	Loss 0.4237 (0.4072)	CeLoss 0.0410 (0.0248)	SegCLSLoss 0.0011 (0.0040)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1070 (0.1065)	MaskBCELoss 0.0238 (0.0258)	MaskDICELoss 0.0832 (0.0807)
Epoch: [0][304/500]	Time 21.689 (21.689)	Loss 0.4332 (0.3771)	CeLoss 0.0508 (0.0126)	SegCLSLoss 0.0138 (0.0036)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0984 (0.1003)	MaskBCELoss 0.0193 (0.0219)	MaskDICELoss 0.0791 (0.0784)
Epoch: [0][305/500]	Time 23.120 (23.120)	Loss 0.3844 (0.4068)	CeLoss 0.0003 (0.0218)	SegCLSLoss 0.0017 (0.0016)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1114 (0.1085)	MaskBCELoss 0.0324 (0.0262)	MaskDICELoss 0.0789 (0.0824)
Epoch: [0][306/500]	Time 22.252 (22.252)	Loss 0.3694 (0.3716)	CeLoss 0.0001 (0.0128)	SegCLSLoss 0.0009 (0.0025)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1032 (0.0978)	MaskBCELoss 0.0225 (0.0187)	MaskDICELoss 0.0806 (0.0791)
Epoch: [0][307/500]	Time 23.742 (23.742)	Loss 0.3343 (0.3958)	CeLoss 0.0011 (0.0368)	SegCLSLoss 0.0014 (0.0027)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0898 (0.1002)	MaskBCELoss 0.0144 (0.0235)	MaskDICELoss 0.0754 (0.0766)
Epoch: [0][308/500]	Time 22.476 (22.476)	Loss 0.3427 (0.4133)	CeLoss 0.0002 (0.0318)	SegCLSLoss 0.0028 (0.0017)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0884 (0.1077)	MaskBCELoss 0.0083 (0.0265)	MaskDICELoss 0.0800 (0.0813)
Epoch: [0][309/500]	Time 20.866 (20.866)	Loss 0.4078 (0.4046)	CeLoss 0.0002 (0.0210)	SegCLSLoss 0.0012 (0.0018)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1154 (0.1062)	MaskBCELoss 0.0283 (0.0224)	MaskDICELoss 0.0872 (0.0838)
Epoch: [0][310/500]	Time 21.967 (21.967)	Loss 0.3425 (0.3850)	CeLoss 0.0474 (0.0366)	SegCLSLoss 0.0043 (0.0021)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0822 (0.0950)	MaskBCELoss 0.0211 (0.0179)	MaskDICELoss 0.0611 (0.0771)
Epoch: [0][311/500]	Time 24.105 (24.105)	Loss 0.4093 (0.4165)	CeLoss 0.0005 (0.0275)	SegCLSLoss 0.0009 (0.0013)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1228 (0.1103)	MaskBCELoss 0.0420 (0.0274)	MaskDICELoss 0.0808 (0.0829)
Epoch: [0][312/500]	Time 20.211 (20.211)	Loss 0.4062 (0.3856)	CeLoss 0.0610 (0.0323)	SegCLSLoss 0.0006 (0.0018)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0980 (0.0974)	MaskBCELoss 0.0240 (0.0199)	MaskDICELoss 0.0740 (0.0775)
Epoch: [0][313/500]	Time 20.532 (20.532)	Loss 0.4007 (0.3871)	CeLoss 0.0459 (0.0278)	SegCLSLoss 0.0013 (0.0019)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0963 (0.0993)	MaskBCELoss 0.0165 (0.0207)	MaskDICELoss 0.0798 (0.0785)
Epoch: [0][314/500]	Time 19.671 (19.671)	Loss 0.3749 (0.3807)	CeLoss 0.0002 (0.0256)	SegCLSLoss 0.0014 (0.0018)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1060 (0.0988)	MaskBCELoss 0.0260 (0.0218)	MaskDICELoss 0.0800 (0.0769)
Epoch: [0][315/500]	Time 23.698 (23.698)	Loss 0.5572 (0.4070)	CeLoss 0.0864 (0.0267)	SegCLSLoss 0.0011 (0.0016)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1453 (0.1082)	MaskBCELoss 0.0561 (0.0279)	MaskDICELoss 0.0892 (0.0803)
Epoch: [0][316/500]	Time 20.835 (20.835)	Loss 0.3959 (0.3867)	CeLoss 0.0542 (0.0277)	SegCLSLoss 0.0022 (0.0022)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0981 (0.0997)	MaskBCELoss 0.0275 (0.0221)	MaskDICELoss 0.0706 (0.0776)
Epoch: [0][317/500]	Time 23.605 (23.605)	Loss 0.4619 (0.3996)	CeLoss 0.1016 (0.0227)	SegCLSLoss 0.0014 (0.0016)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1081 (0.1067)	MaskBCELoss 0.0376 (0.0266)	MaskDICELoss 0.0705 (0.0801)
Epoch: [0][318/500]	Time 21.598 (21.598)	Loss 0.4031 (0.3892)	CeLoss 0.0016 (0.0221)	SegCLSLoss 0.0008 (0.0011)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1118 (0.1019)	MaskBCELoss 0.0236 (0.0213)	MaskDICELoss 0.0881 (0.0806)
Epoch: [0][319/500]	Time 20.170 (20.170)	Loss 0.3210 (0.3790)	CeLoss 0.0002 (0.0276)	SegCLSLoss 0.0048 (0.0019)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0829 (0.0960)	MaskBCELoss 0.0102 (0.0182)	MaskDICELoss 0.0727 (0.0778)
Epoch: [0][320/500]	Time 24.128 (24.128)	Loss 0.5917 (0.4172)	CeLoss 0.0554 (0.0130)	SegCLSLoss 0.0012 (0.0015)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1897 (0.1175)	MaskBCELoss 0.1125 (0.0343)	MaskDICELoss 0.0772 (0.0831)
Epoch: [0][321/500]	Time 23.404 (23.404)	Loss 0.3437 (0.3716)	CeLoss 0.0013 (0.0230)	SegCLSLoss 0.0069 (0.0027)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0906 (0.0958)	MaskBCELoss 0.0169 (0.0201)	MaskDICELoss 0.0737 (0.0758)
Epoch: [0][322/500]	Time 19.705 (19.705)	Loss 0.2989 (0.3726)	CeLoss 0.0003 (0.0225)	SegCLSLoss 0.0023 (0.0029)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0767 (0.0956)	MaskBCELoss 0.0064 (0.0190)	MaskDICELoss 0.0703 (0.0766)
Epoch: [0][323/500]	Time 23.478 (23.478)	Loss 0.3157 (0.3627)	CeLoss 0.0002 (0.0263)	SegCLSLoss 0.0029 (0.0030)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0822 (0.0903)	MaskBCELoss 0.0097 (0.0154)	MaskDICELoss 0.0726 (0.0749)
Epoch: [0][324/500]	Time 20.156 (20.156)	Loss 0.2830 (0.3566)	CeLoss 0.0203 (0.0249)	SegCLSLoss 0.0051 (0.0032)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0701 (0.0897)	MaskBCELoss 0.0139 (0.0168)	MaskDICELoss 0.0562 (0.0729)
Epoch: [0][325/500]	Time 24.697 (24.697)	Loss 0.3974 (0.3895)	CeLoss 0.0435 (0.0377)	SegCLSLoss 0.0006 (0.0025)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1044 (0.0972)	MaskBCELoss 0.0325 (0.0210)	MaskDICELoss 0.0719 (0.0762)
Epoch: [0][326/500]	Time 21.239 (21.239)	Loss 0.4106 (0.3637)	CeLoss 0.0005 (0.0153)	SegCLSLoss 0.0023 (0.0037)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1162 (0.0956)	MaskBCELoss 0.0298 (0.0205)	MaskDICELoss 0.0864 (0.0750)
Epoch: [0][327/500]	Time 22.287 (22.287)	Loss 0.3438 (0.3741)	CeLoss 0.0366 (0.0216)	SegCLSLoss 0.0027 (0.0019)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0808 (0.0978)	MaskBCELoss 0.0107 (0.0212)	MaskDICELoss 0.0702 (0.0766)
Epoch: [0][328/500]	Time 23.243 (23.243)	Loss 0.3806 (0.3472)	CeLoss 0.0019 (0.0078)	SegCLSLoss 0.0035 (0.0023)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1058 (0.0917)	MaskBCELoss 0.0258 (0.0161)	MaskDICELoss 0.0800 (0.0756)
Epoch: [0][329/500]	Time 23.367 (23.367)	Loss 0.3585 (0.3781)	CeLoss 0.0006 (0.0303)	SegCLSLoss 0.0022 (0.0021)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0959 (0.0949)	MaskBCELoss 0.0152 (0.0181)	MaskDICELoss 0.0808 (0.0769)
Epoch: [0][330/500]	Time 21.352 (21.352)	Loss 0.3773 (0.3684)	CeLoss 0.1084 (0.0284)	SegCLSLoss 0.0012 (0.0023)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0688 (0.0927)	MaskBCELoss 0.0044 (0.0178)	MaskDICELoss 0.0644 (0.0750)
Epoch: [0][331/500]	Time 25.225 (25.225)	Loss 0.4002 (0.3742)	CeLoss 0.0613 (0.0290)	SegCLSLoss 0.0069 (0.0024)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0867 (0.0942)	MaskBCELoss 0.0108 (0.0183)	MaskDICELoss 0.0759 (0.0760)
Epoch: [0][332/500]	Time 23.373 (23.373)	Loss 0.3003 (0.3763)	CeLoss 0.0002 (0.0178)	SegCLSLoss 0.0026 (0.0014)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0807 (0.0986)	MaskBCELoss 0.0139 (0.0194)	MaskDICELoss 0.0668 (0.0792)
Epoch: [0][333/500]	Time 22.441 (22.441)	Loss 0.3308 (0.3592)	CeLoss 0.0001 (0.0136)	SegCLSLoss 0.0009 (0.0012)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0918 (0.0939)	MaskBCELoss 0.0192 (0.0162)	MaskDICELoss 0.0726 (0.0777)
Epoch: [0][334/500]	Time 24.188 (24.188)	Loss 0.3838 (0.3627)	CeLoss 0.0366 (0.0222)	SegCLSLoss 0.0002 (0.0010)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0920 (0.0922)	MaskBCELoss 0.0106 (0.0151)	MaskDICELoss 0.0814 (0.0771)
Epoch: [0][335/500]	Time 21.423 (21.423)	Loss 0.3553 (0.3683)	CeLoss 0.0002 (0.0302)	SegCLSLoss 0.0004 (0.0010)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1024 (0.0931)	MaskBCELoss 0.0278 (0.0181)	MaskDICELoss 0.0747 (0.0750)
Epoch: [0][336/500]	Time 20.903 (20.903)	Loss 0.4073 (0.3620)	CeLoss 0.0332 (0.0338)	SegCLSLoss 0.0004 (0.0013)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1117 (0.0893)	MaskBCELoss 0.0367 (0.0158)	MaskDICELoss 0.0750 (0.0735)
Epoch: [0][337/500]	Time 20.498 (20.498)	Loss 0.4113 (0.3602)	CeLoss 0.1436 (0.0358)	SegCLSLoss 0.0009 (0.0014)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0684 (0.0882)	MaskBCELoss 0.0037 (0.0156)	MaskDICELoss 0.0646 (0.0726)
Epoch: [0][338/500]	Time 21.335 (21.335)	Loss 0.4304 (0.3523)	CeLoss 0.0222 (0.0181)	SegCLSLoss 0.0017 (0.0014)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1250 (0.0907)	MaskBCELoss 0.0475 (0.0158)	MaskDICELoss 0.0774 (0.0750)
Epoch: [0][339/500]	Time 20.063 (20.063)	Loss 0.3361 (0.3800)	CeLoss 0.0001 (0.0467)	SegCLSLoss 0.0007 (0.0013)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0920 (0.0923)	MaskBCELoss 0.0167 (0.0194)	MaskDICELoss 0.0753 (0.0730)
Epoch: [0][340/500]	Time 22.323 (22.323)	Loss 0.3807 (0.3949)	CeLoss 0.0211 (0.0368)	SegCLSLoss 0.0006 (0.0011)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1038 (0.1009)	MaskBCELoss 0.0282 (0.0238)	MaskDICELoss 0.0755 (0.0771)
Epoch: [0][341/500]	Time 19.964 (19.964)	Loss 0.3777 (0.3603)	CeLoss 0.0003 (0.0323)	SegCLSLoss 0.0006 (0.0007)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1006 (0.0886)	MaskBCELoss 0.0130 (0.0140)	MaskDICELoss 0.0876 (0.0747)
Epoch: [0][342/500]	Time 22.823 (22.823)	Loss 0.4523 (0.3681)	CeLoss 0.0579 (0.0286)	SegCLSLoss 0.0003 (0.0009)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1088 (0.0924)	MaskBCELoss 0.0205 (0.0159)	MaskDICELoss 0.0883 (0.0765)
Epoch: [0][343/500]	Time 24.345 (24.345)	Loss 0.3679 (0.3617)	CeLoss 0.0693 (0.0290)	SegCLSLoss 0.0004 (0.0007)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0780 (0.0906)	MaskBCELoss 0.0072 (0.0155)	MaskDICELoss 0.0708 (0.0751)
Epoch: [0][344/500]	Time 21.966 (21.966)	Loss 0.3763 (0.3525)	CeLoss 0.0356 (0.0341)	SegCLSLoss 0.0002 (0.0010)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0907 (0.0865)	MaskBCELoss 0.0112 (0.0148)	MaskDICELoss 0.0794 (0.0717)
Epoch: [0][345/500]	Time 27.010 (27.010)	Loss 0.3880 (0.3867)	CeLoss 0.0300 (0.0263)	SegCLSLoss 0.0006 (0.0007)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1012 (0.0998)	MaskBCELoss 0.0241 (0.0201)	MaskDICELoss 0.0772 (0.0798)
Epoch: [0][346/500]	Time 24.530 (24.530)	Loss 0.4100 (0.3657)	CeLoss 0.0425 (0.0295)	SegCLSLoss 0.0020 (0.0008)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0990 (0.0931)	MaskBCELoss 0.0164 (0.0189)	MaskDICELoss 0.0826 (0.0742)
Epoch: [0][347/500]	Time 22.197 (22.197)	Loss 0.3855 (0.3568)	CeLoss 0.0013 (0.0200)	SegCLSLoss 0.0005 (0.0006)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1104 (0.0923)	MaskBCELoss 0.0292 (0.0167)	MaskDICELoss 0.0812 (0.0756)
Epoch: [0][348/500]	Time 26.174 (26.174)	Loss 0.2559 (0.3667)	CeLoss 0.0002 (0.0372)	SegCLSLoss 0.0011 (0.0010)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0654 (0.0912)	MaskBCELoss 0.0041 (0.0186)	MaskDICELoss 0.0613 (0.0726)
Epoch: [0][349/500]	Time 31.394 (31.394)	Loss 0.3229 (0.3917)	CeLoss 0.0001 (0.0304)	SegCLSLoss 0.0005 (0.0008)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0846 (0.1020)	MaskBCELoss 0.0083 (0.0242)	MaskDICELoss 0.0763 (0.0778)
Epoch: [0][350/500]	Time 31.241 (31.241)	Loss 0.3164 (0.3724)	CeLoss 0.0454 (0.0242)	SegCLSLoss 0.0022 (0.0008)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0728 (0.0961)	MaskBCELoss 0.0123 (0.0189)	MaskDICELoss 0.0605 (0.0772)
Epoch: [0][351/500]	Time 34.232 (34.232)	Loss 0.4816 (0.3763)	CeLoss 0.0635 (0.0313)	SegCLSLoss 0.0004 (0.0016)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1240 (0.0963)	MaskBCELoss 0.0392 (0.0217)	MaskDICELoss 0.0848 (0.0746)
Epoch: [0][352/500]	Time 30.425 (30.425)	Loss 0.3067 (0.3657)	CeLoss 0.0028 (0.0327)	SegCLSLoss 0.0007 (0.0009)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0793 (0.0911)	MaskBCELoss 0.0073 (0.0167)	MaskDICELoss 0.0720 (0.0744)
Epoch: [0][353/500]	Time 28.480 (28.480)	Loss 0.3424 (0.3417)	CeLoss 0.0003 (0.0168)	SegCLSLoss 0.0018 (0.0029)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0936 (0.0864)	MaskBCELoss 0.0180 (0.0132)	MaskDICELoss 0.0756 (0.0731)
Epoch: [0][354/500]	Time 31.878 (31.878)	Loss 0.3460 (0.3837)	CeLoss 0.0273 (0.0465)	SegCLSLoss 0.0017 (0.0010)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0884 (0.0938)	MaskBCELoss 0.0193 (0.0201)	MaskDICELoss 0.0692 (0.0738)
Epoch: [0][355/500]	Time 33.389 (33.389)	Loss 0.4100 (0.3845)	CeLoss 0.0918 (0.0359)	SegCLSLoss 0.0011 (0.0010)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0839 (0.0957)	MaskBCELoss 0.0096 (0.0181)	MaskDICELoss 0.0743 (0.0776)
Epoch: [0][356/500]	Time 35.144 (35.144)	Loss 0.3212 (0.3495)	CeLoss 0.0005 (0.0160)	SegCLSLoss 0.0018 (0.0027)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0841 (0.0897)	MaskBCELoss 0.0095 (0.0153)	MaskDICELoss 0.0745 (0.0744)
Epoch: [0][357/500]	Time 33.410 (33.410)	Loss 0.3809 (0.3814)	CeLoss 0.0396 (0.0218)	SegCLSLoss 0.0012 (0.0019)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0912 (0.0993)	MaskBCELoss 0.0129 (0.0207)	MaskDICELoss 0.0783 (0.0786)
Epoch: [0][358/500]	Time 33.620 (33.620)	Loss 0.3465 (0.3635)	CeLoss 0.0002 (0.0224)	SegCLSLoss 0.0011 (0.0013)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0948 (0.0949)	MaskBCELoss 0.0175 (0.0205)	MaskDICELoss 0.0773 (0.0744)
Epoch: [0][359/500]	Time 33.180 (33.180)	Loss 0.4362 (0.4001)	CeLoss 0.0483 (0.0553)	SegCLSLoss 0.0047 (0.0019)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1050 (0.0952)	MaskBCELoss 0.0208 (0.0200)	MaskDICELoss 0.0842 (0.0753)
Epoch: [0][360/500]	Time 32.797 (32.797)	Loss 0.4306 (0.3956)	CeLoss 0.0598 (0.0322)	SegCLSLoss 0.0014 (0.0010)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1084 (0.1009)	MaskBCELoss 0.0327 (0.0211)	MaskDICELoss 0.0757 (0.0798)
Epoch: [0][361/500]	Time 31.559 (31.559)	Loss 0.3091 (0.3493)	CeLoss 0.0007 (0.0162)	SegCLSLoss 0.0007 (0.0010)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0835 (0.0924)	MaskBCELoss 0.0136 (0.0192)	MaskDICELoss 0.0699 (0.0732)
Epoch: [0][362/500]	Time 32.161 (32.161)	Loss 0.3247 (0.3370)	CeLoss 0.0005 (0.0147)	SegCLSLoss 0.0012 (0.0012)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0828 (0.0871)	MaskBCELoss 0.0047 (0.0143)	MaskDICELoss 0.0781 (0.0728)
Epoch: [0][363/500]	Time 36.470 (36.470)	Loss 0.3255 (0.3586)	CeLoss 0.0004 (0.0158)	SegCLSLoss 0.0011 (0.0018)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0869 (0.0943)	MaskBCELoss 0.0123 (0.0190)	MaskDICELoss 0.0746 (0.0753)
Epoch: [0][364/500]	Time 32.519 (32.519)	Loss 0.3275 (0.3707)	CeLoss 0.0003 (0.0350)	SegCLSLoss 0.0009 (0.0007)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0895 (0.0918)	MaskBCELoss 0.0164 (0.0164)	MaskDICELoss 0.0731 (0.0754)
Epoch: [0][365/500]	Time 31.979 (31.979)	Loss 0.3145 (0.3486)	CeLoss 0.0008 (0.0169)	SegCLSLoss 0.0007 (0.0011)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0807 (0.0886)	MaskBCELoss 0.0051 (0.0124)	MaskDICELoss 0.0755 (0.0762)
Epoch: [0][366/500]	Time 29.381 (29.381)	Loss 0.4852 (0.3603)	CeLoss 0.0566 (0.0256)	SegCLSLoss 0.0005 (0.0013)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1281 (0.0917)	MaskBCELoss 0.0423 (0.0173)	MaskDICELoss 0.0858 (0.0744)
Epoch: [0][367/500]	Time 30.709 (30.709)	Loss 0.3920 (0.3562)	CeLoss 0.0469 (0.0197)	SegCLSLoss 0.0071 (0.0017)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0872 (0.0921)	MaskBCELoss 0.0088 (0.0176)	MaskDICELoss 0.0784 (0.0745)
Epoch: [0][368/500]	Time 34.030 (34.030)	Loss 0.2889 (0.3358)	CeLoss 0.0002 (0.0052)	SegCLSLoss 0.0012 (0.0013)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0742 (0.0902)	MaskBCELoss 0.0053 (0.0163)	MaskDICELoss 0.0689 (0.0739)
Epoch: [0][369/500]	Time 31.233 (31.233)	Loss 0.4233 (0.3528)	CeLoss 0.0859 (0.0395)	SegCLSLoss 0.0008 (0.0010)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0925 (0.0835)	MaskBCELoss 0.0171 (0.0114)	MaskDICELoss 0.0754 (0.0721)
Epoch: [0][370/500]	Time 32.654 (32.654)	Loss 0.2557 (0.3532)	CeLoss 0.0003 (0.0248)	SegCLSLoss 0.0012 (0.0008)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0649 (0.0900)	MaskBCELoss 0.0032 (0.0166)	MaskDICELoss 0.0617 (0.0734)
Epoch: [0][371/500]	Time 30.531 (30.531)	Loss 0.3850 (0.3678)	CeLoss 0.0598 (0.0243)	SegCLSLoss 0.0006 (0.0006)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0835 (0.0927)	MaskBCELoss 0.0050 (0.0144)	MaskDICELoss 0.0785 (0.0784)
Epoch: [0][372/500]	Time 29.205 (29.205)	Loss 0.3596 (0.3878)	CeLoss 0.0452 (0.0400)	SegCLSLoss 0.0006 (0.0005)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0894 (0.0952)	MaskBCELoss 0.0221 (0.0170)	MaskDICELoss 0.0673 (0.0782)
Epoch: [0][373/500]	Time 31.194 (31.194)	Loss 0.4041 (0.3878)	CeLoss 0.0391 (0.0313)	SegCLSLoss 0.0002 (0.0006)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0996 (0.0953)	MaskBCELoss 0.0168 (0.0130)	MaskDICELoss 0.0827 (0.0823)
Epoch: [0][374/500]	Time 28.905 (28.905)	Loss 0.3827 (0.3858)	CeLoss 0.0002 (0.0374)	SegCLSLoss 0.0004 (0.0006)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1028 (0.0943)	MaskBCELoss 0.0148 (0.0149)	MaskDICELoss 0.0880 (0.0794)
Epoch: [0][375/500]	Time 29.861 (29.861)	Loss 0.3095 (0.3599)	CeLoss 0.0002 (0.0071)	SegCLSLoss 0.0004 (0.0006)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0848 (0.0960)	MaskBCELoss 0.0153 (0.0163)	MaskDICELoss 0.0695 (0.0798)
Epoch: [0][376/500]	Time 31.458 (31.458)	Loss 0.3403 (0.3776)	CeLoss 0.0002 (0.0211)	SegCLSLoss 0.0003 (0.0014)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0917 (0.0990)	MaskBCELoss 0.0136 (0.0212)	MaskDICELoss 0.0781 (0.0778)
Epoch: [0][377/500]	Time 33.374 (33.374)	Loss 0.3222 (0.3453)	CeLoss 0.0010 (0.0113)	SegCLSLoss 0.0005 (0.0006)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0834 (0.0897)	MaskBCELoss 0.0067 (0.0129)	MaskDICELoss 0.0767 (0.0767)
Epoch: [0][378/500]	Time 28.441 (28.441)	Loss 0.3836 (0.4202)	CeLoss 0.0640 (0.0543)	SegCLSLoss 0.0003 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0886 (0.1009)	MaskBCELoss 0.0179 (0.0192)	MaskDICELoss 0.0708 (0.0817)
Epoch: [0][379/500]	Time 30.600 (30.600)	Loss 0.4418 (0.3740)	CeLoss 0.0669 (0.0223)	SegCLSLoss 0.0001 (0.0004)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1026 (0.0954)	MaskBCELoss 0.0179 (0.0155)	MaskDICELoss 0.0847 (0.0800)
Epoch: [0][380/500]	Time 35.170 (35.170)	Loss 0.4497 (0.4040)	CeLoss 0.0464 (0.0368)	SegCLSLoss 0.0005 (0.0012)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1069 (0.1017)	MaskBCELoss 0.0127 (0.0209)	MaskDICELoss 0.0942 (0.0808)
Epoch: [0][381/500]	Time 32.554 (32.554)	Loss 0.3903 (0.3895)	CeLoss 0.0522 (0.0442)	SegCLSLoss 0.0006 (0.0006)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0923 (0.0949)	MaskBCELoss 0.0162 (0.0177)	MaskDICELoss 0.0761 (0.0772)
Epoch: [0][382/500]	Time 30.911 (30.911)	Loss 0.3451 (0.3628)	CeLoss 0.0605 (0.0278)	SegCLSLoss 0.0009 (0.0010)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0804 (0.0920)	MaskBCELoss 0.0196 (0.0176)	MaskDICELoss 0.0609 (0.0744)
Epoch: [0][383/500]	Time 31.174 (31.174)	Loss 0.3246 (0.3533)	CeLoss 0.0010 (0.0283)	SegCLSLoss 0.0004 (0.0005)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0861 (0.0902)	MaskBCELoss 0.0109 (0.0184)	MaskDICELoss 0.0752 (0.0718)
Epoch: [0][384/500]	Time 33.275 (33.275)	Loss 0.3780 (0.3469)	CeLoss 0.0630 (0.0195)	SegCLSLoss 0.0024 (0.0012)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0852 (0.0905)	MaskBCELoss 0.0151 (0.0184)	MaskDICELoss 0.0701 (0.0721)
Epoch: [0][385/500]	Time 31.542 (31.542)	Loss 0.3634 (0.3481)	CeLoss 0.0004 (0.0320)	SegCLSLoss 0.0015 (0.0006)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1007 (0.0858)	MaskBCELoss 0.0213 (0.0141)	MaskDICELoss 0.0793 (0.0716)
Epoch: [0][386/500]	Time 30.332 (30.332)	Loss 0.4774 (0.3704)	CeLoss 0.0654 (0.0496)	SegCLSLoss 0.0005 (0.0005)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1160 (0.0876)	MaskBCELoss 0.0265 (0.0153)	MaskDICELoss 0.0895 (0.0723)
Epoch: [0][387/500]	Time 31.248 (31.248)	Loss 0.2398 (0.3485)	CeLoss 0.0003 (0.0185)	SegCLSLoss 0.0008 (0.0005)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0608 (0.0900)	MaskBCELoss 0.0027 (0.0155)	MaskDICELoss 0.0581 (0.0745)
Epoch: [0][388/500]	Time 31.704 (31.704)	Loss 0.3609 (0.3861)	CeLoss 0.0679 (0.0328)	SegCLSLoss 0.0003 (0.0005)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0758 (0.1000)	MaskBCELoss 0.0054 (0.0240)	MaskDICELoss 0.0703 (0.0760)
Epoch: [0][389/500]	Time 31.155 (31.155)	Loss 0.3695 (0.3404)	CeLoss 0.0008 (0.0187)	SegCLSLoss 0.0007 (0.0006)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1064 (0.0883)	MaskBCELoss 0.0293 (0.0164)	MaskDICELoss 0.0772 (0.0719)
Epoch: [0][390/500]	Time 35.987 (35.987)	Loss 0.3434 (0.3631)	CeLoss 0.0003 (0.0161)	SegCLSLoss 0.0001 (0.0004)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0957 (0.0983)	MaskBCELoss 0.0200 (0.0234)	MaskDICELoss 0.0757 (0.0748)
Epoch: [0][391/500]	Time 30.961 (30.961)	Loss 0.3516 (0.3325)	CeLoss 0.0586 (0.0304)	SegCLSLoss 0.0002 (0.0004)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0751 (0.0811)	MaskBCELoss 0.0040 (0.0115)	MaskDICELoss 0.0712 (0.0696)
Epoch: [0][392/500]	Time 33.896 (33.896)	Loss 0.3054 (0.3314)	CeLoss 0.0004 (0.0186)	SegCLSLoss 0.0005 (0.0008)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0805 (0.0839)	MaskBCELoss 0.0090 (0.0122)	MaskDICELoss 0.0715 (0.0717)
Epoch: [0][393/500]	Time 32.233 (32.233)	Loss 0.3156 (0.3618)	CeLoss 0.0005 (0.0193)	SegCLSLoss 0.0004 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0880 (0.0958)	MaskBCELoss 0.0188 (0.0205)	MaskDICELoss 0.0692 (0.0753)
Epoch: [0][394/500]	Time 29.922 (29.922)	Loss 0.3907 (0.3380)	CeLoss 0.0618 (0.0276)	SegCLSLoss 0.0005 (0.0005)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0882 (0.0844)	MaskBCELoss 0.0125 (0.0141)	MaskDICELoss 0.0757 (0.0703)
Epoch: [0][395/500]	Time 32.859 (32.859)	Loss 0.3185 (0.3342)	CeLoss 0.0003 (0.0214)	SegCLSLoss 0.0002 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0882 (0.0863)	MaskBCELoss 0.0176 (0.0165)	MaskDICELoss 0.0707 (0.0698)
Epoch: [0][396/500]	Time 31.703 (31.703)	Loss 0.3476 (0.3713)	CeLoss 0.0315 (0.0412)	SegCLSLoss 0.0000 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0866 (0.0916)	MaskBCELoss 0.0152 (0.0184)	MaskDICELoss 0.0714 (0.0732)
Epoch: [0][397/500]	Time 26.497 (26.497)	Loss 0.2442 (0.3537)	CeLoss 0.0004 (0.0394)	SegCLSLoss 0.0006 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0622 (0.0870)	MaskBCELoss 0.0031 (0.0171)	MaskDICELoss 0.0591 (0.0699)
Epoch: [0][398/500]	Time 35.955 (35.955)	Loss 0.3331 (0.3605)	CeLoss 0.0003 (0.0183)	SegCLSLoss 0.0001 (0.0004)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0948 (0.0956)	MaskBCELoss 0.0233 (0.0205)	MaskDICELoss 0.0715 (0.0751)
Epoch: [0][399/500]	Time 33.643 (33.643)	Loss 0.3959 (0.3439)	CeLoss 0.0469 (0.0180)	SegCLSLoss 0.0005 (0.0004)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0976 (0.0902)	MaskBCELoss 0.0212 (0.0178)	MaskDICELoss 0.0764 (0.0724)
[2025-04-04 23:16:14,835] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[0.00029593975903614454], mom=[(0.9, 0.95)]
[2025-04-04 23:16:14,843] [INFO] [timer.py:215:stop] epoch=0/micro_step=400/global_step=40, RunningAvgSamplesPerSec=1.666298779398971, CurrSamplesPerSec=1.2507800407150984, MemAllocated=34.36GB, MaxMemAllocated=45.56GB
Epoch: [0][400/500]	Time 33.461 (33.461)	Loss 0.3505 (0.3888)	CeLoss 0.0520 (0.0393)	SegCLSLoss 0.0002 (0.0004)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0809 (0.0965)	MaskBCELoss 0.0127 (0.0186)	MaskDICELoss 0.0682 (0.0779)
Epoch: [0][401/500]	Time 32.911 (32.911)	Loss 0.3217 (0.3572)	CeLoss 0.0003 (0.0232)	SegCLSLoss 0.0001 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0876 (0.0932)	MaskBCELoss 0.0146 (0.0197)	MaskDICELoss 0.0730 (0.0735)
Epoch: [0][402/500]	Time 35.974 (35.974)	Loss 0.3064 (0.3948)	CeLoss 0.0010 (0.0365)	SegCLSLoss 0.0003 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0838 (0.1010)	MaskBCELoss 0.0151 (0.0230)	MaskDICELoss 0.0687 (0.0779)
Epoch: [0][403/500]	Time 36.376 (36.376)	Loss 0.3808 (0.3715)	CeLoss 0.0400 (0.0319)	SegCLSLoss 0.0002 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0939 (0.0940)	MaskBCELoss 0.0175 (0.0185)	MaskDICELoss 0.0763 (0.0755)
Epoch: [0][404/500]	Time 31.971 (31.971)	Loss 0.3298 (0.3348)	CeLoss 0.0004 (0.0299)	SegCLSLoss 0.0003 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0881 (0.0825)	MaskBCELoss 0.0118 (0.0128)	MaskDICELoss 0.0763 (0.0697)
Epoch: [0][405/500]	Time 30.789 (30.789)	Loss 0.2978 (0.3448)	CeLoss 0.0581 (0.0215)	SegCLSLoss 0.0001 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0614 (0.0883)	MaskBCELoss 0.0032 (0.0153)	MaskDICELoss 0.0582 (0.0730)
Epoch: [0][406/500]	Time 32.871 (32.871)	Loss 0.3350 (0.3356)	CeLoss 0.0417 (0.0305)	SegCLSLoss 0.0001 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0771 (0.0817)	MaskBCELoss 0.0078 (0.0109)	MaskDICELoss 0.0694 (0.0708)
Epoch: [0][407/500]	Time 33.287 (33.287)	Loss 0.3689 (0.3560)	CeLoss 0.0003 (0.0206)	SegCLSLoss 0.0002 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1024 (0.0943)	MaskBCELoss 0.0207 (0.0211)	MaskDICELoss 0.0817 (0.0732)
Epoch: [0][408/500]	Time 31.097 (31.097)	Loss 0.5002 (0.3789)	CeLoss 0.0728 (0.0396)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1185 (0.0936)	MaskBCELoss 0.0231 (0.0177)	MaskDICELoss 0.0953 (0.0759)
Epoch: [0][409/500]	Time 32.409 (32.409)	Loss 0.3805 (0.3487)	CeLoss 0.1045 (0.0341)	SegCLSLoss 0.0004 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0710 (0.0856)	MaskBCELoss 0.0044 (0.0142)	MaskDICELoss 0.0665 (0.0714)
Epoch: [0][410/500]	Time 29.124 (29.124)	Loss 0.2793 (0.3271)	CeLoss 0.0003 (0.0240)	SegCLSLoss 0.0003 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0750 (0.0836)	MaskBCELoss 0.0108 (0.0160)	MaskDICELoss 0.0642 (0.0676)
Epoch: [0][411/500]	Time 31.723 (31.723)	Loss 0.4651 (0.3818)	CeLoss 0.0140 (0.0336)	SegCLSLoss 0.0003 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1296 (0.0959)	MaskBCELoss 0.0340 (0.0179)	MaskDICELoss 0.0956 (0.0779)
Epoch: [0][412/500]	Time 35.291 (35.291)	Loss 0.3072 (0.3497)	CeLoss 0.0020 (0.0184)	SegCLSLoss 0.0002 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0805 (0.0923)	MaskBCELoss 0.0086 (0.0192)	MaskDICELoss 0.0719 (0.0731)
Epoch: [0][413/500]	Time 33.230 (33.230)	Loss 0.3812 (0.3715)	CeLoss 0.0645 (0.0499)	SegCLSLoss 0.0012 (0.0004)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0886 (0.0902)	MaskBCELoss 0.0200 (0.0199)	MaskDICELoss 0.0686 (0.0703)
Epoch: [0][414/500]	Time 31.304 (31.304)	Loss 0.3427 (0.3681)	CeLoss 0.0752 (0.0388)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0722 (0.0910)	MaskBCELoss 0.0107 (0.0176)	MaskDICELoss 0.0615 (0.0734)
Epoch: [0][415/500]	Time 32.652 (32.652)	Loss 0.4130 (0.3457)	CeLoss 0.0001 (0.0103)	SegCLSLoss 0.0003 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1158 (0.0926)	MaskBCELoss 0.0254 (0.0178)	MaskDICELoss 0.0904 (0.0749)
Epoch: [0][416/500]	Time 30.857 (30.857)	Loss 0.4363 (0.3975)	CeLoss 0.0854 (0.0445)	SegCLSLoss 0.0001 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1004 (0.0992)	MaskBCELoss 0.0255 (0.0221)	MaskDICELoss 0.0749 (0.0771)
Epoch: [0][417/500]	Time 32.187 (32.187)	Loss 0.4051 (0.3948)	CeLoss 0.0825 (0.0429)	SegCLSLoss 0.0000 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0918 (0.0985)	MaskBCELoss 0.0223 (0.0214)	MaskDICELoss 0.0695 (0.0771)
Epoch: [0][418/500]	Time 33.038 (33.038)	Loss 0.3000 (0.3557)	CeLoss 0.0003 (0.0090)	SegCLSLoss 0.0010 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0793 (0.0963)	MaskBCELoss 0.0097 (0.0195)	MaskDICELoss 0.0696 (0.0768)
Epoch: [0][419/500]	Time 33.888 (33.888)	Loss 0.3548 (0.3543)	CeLoss 0.0002 (0.0252)	SegCLSLoss 0.0000 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0984 (0.0899)	MaskBCELoss 0.0194 (0.0156)	MaskDICELoss 0.0789 (0.0743)
Epoch: [0][420/500]	Time 31.480 (31.480)	Loss 0.3653 (0.3529)	CeLoss 0.0245 (0.0274)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0960 (0.0917)	MaskBCELoss 0.0217 (0.0208)	MaskDICELoss 0.0744 (0.0709)
Epoch: [0][421/500]	Time 34.925 (34.925)	Loss 0.3558 (0.3678)	CeLoss 0.0011 (0.0374)	SegCLSLoss 0.0001 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0976 (0.0906)	MaskBCELoss 0.0180 (0.0160)	MaskDICELoss 0.0796 (0.0746)
Epoch: [0][422/500]	Time 30.578 (30.578)	Loss 0.3024 (0.3468)	CeLoss 0.0620 (0.0276)	SegCLSLoss 0.0003 (0.0004)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0620 (0.0894)	MaskBCELoss 0.0042 (0.0197)	MaskDICELoss 0.0578 (0.0697)
Epoch: [0][423/500]	Time 31.093 (31.093)	Loss 0.3198 (0.3405)	CeLoss 0.0002 (0.0287)	SegCLSLoss 0.0001 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0910 (0.0852)	MaskBCELoss 0.0223 (0.0147)	MaskDICELoss 0.0687 (0.0706)
Epoch: [0][424/500]	Time 29.396 (29.396)	Loss 0.3159 (0.3359)	CeLoss 0.0337 (0.0339)	SegCLSLoss 0.0000 (0.0006)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0763 (0.0835)	MaskBCELoss 0.0115 (0.0166)	MaskDICELoss 0.0648 (0.0669)
Epoch: [0][425/500]	Time 28.622 (28.622)	Loss 0.3476 (0.3306)	CeLoss 0.0002 (0.0227)	SegCLSLoss 0.0000 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1054 (0.0848)	MaskBCELoss 0.0371 (0.0160)	MaskDICELoss 0.0683 (0.0688)
Epoch: [0][426/500]	Time 35.040 (35.040)	Loss 0.4007 (0.3986)	CeLoss 0.0002 (0.0347)	SegCLSLoss 0.0009 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1124 (0.1037)	MaskBCELoss 0.0255 (0.0257)	MaskDICELoss 0.0870 (0.0780)
Epoch: [0][427/500]	Time 30.833 (30.833)	Loss 0.3582 (0.3401)	CeLoss 0.0002 (0.0183)	SegCLSLoss 0.0003 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1024 (0.0894)	MaskBCELoss 0.0260 (0.0181)	MaskDICELoss 0.0764 (0.0714)
Epoch: [0][428/500]	Time 31.980 (31.980)	Loss 0.3842 (0.3485)	CeLoss 0.0762 (0.0299)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0875 (0.0877)	MaskBCELoss 0.0211 (0.0162)	MaskDICELoss 0.0665 (0.0715)
Epoch: [0][429/500]	Time 31.297 (31.297)	Loss 0.2787 (0.3339)	CeLoss 0.0003 (0.0208)	SegCLSLoss 0.0001 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0732 (0.0859)	MaskBCELoss 0.0074 (0.0154)	MaskDICELoss 0.0658 (0.0705)
Epoch: [0][430/500]	Time 29.837 (29.837)	Loss 0.5348 (0.3235)	CeLoss 0.0879 (0.0236)	SegCLSLoss 0.0000 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1337 (0.0818)	MaskBCELoss 0.0440 (0.0139)	MaskDICELoss 0.0897 (0.0679)
Epoch: [0][431/500]	Time 31.159 (31.159)	Loss 0.3429 (0.3580)	CeLoss 0.0001 (0.0248)	SegCLSLoss 0.0004 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0952 (0.0937)	MaskBCELoss 0.0194 (0.0208)	MaskDICELoss 0.0758 (0.0728)
Epoch: [0][432/500]	Time 31.609 (31.609)	Loss 0.3320 (0.3639)	CeLoss 0.0559 (0.0499)	SegCLSLoss 0.0001 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0725 (0.0870)	MaskBCELoss 0.0069 (0.0172)	MaskDICELoss 0.0656 (0.0698)
Epoch: [0][433/500]	Time 31.926 (31.926)	Loss 0.4114 (0.3299)	CeLoss 0.0354 (0.0172)	SegCLSLoss 0.0001 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1019 (0.0864)	MaskBCELoss 0.0159 (0.0165)	MaskDICELoss 0.0860 (0.0699)
Epoch: [0][434/500]	Time 30.998 (30.998)	Loss 0.3749 (0.3440)	CeLoss 0.0237 (0.0136)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0954 (0.0917)	MaskBCELoss 0.0153 (0.0183)	MaskDICELoss 0.0802 (0.0734)
Epoch: [0][435/500]	Time 29.878 (29.878)	Loss 0.2651 (0.3084)	CeLoss 0.0004 (0.0186)	SegCLSLoss 0.0001 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0683 (0.0776)	MaskBCELoss 0.0044 (0.0104)	MaskDICELoss 0.0639 (0.0672)
Epoch: [0][436/500]	Time 36.130 (36.130)	Loss 0.3689 (0.3404)	CeLoss 0.0598 (0.0156)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0879 (0.0910)	MaskBCELoss 0.0212 (0.0198)	MaskDICELoss 0.0667 (0.0712)
Epoch: [0][437/500]	Time 34.773 (34.773)	Loss 0.3512 (0.3681)	CeLoss 0.0001 (0.0198)	SegCLSLoss 0.0001 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0949 (0.0973)	MaskBCELoss 0.0143 (0.0206)	MaskDICELoss 0.0806 (0.0767)
Epoch: [0][438/500]	Time 32.552 (32.552)	Loss 0.2661 (0.3109)	CeLoss 0.0007 (0.0145)	SegCLSLoss 0.0002 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0689 (0.0808)	MaskBCELoss 0.0054 (0.0137)	MaskDICELoss 0.0636 (0.0671)
Epoch: [0][439/500]	Time 30.596 (30.596)	Loss 0.4114 (0.3523)	CeLoss 0.0286 (0.0360)	SegCLSLoss 0.0001 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1130 (0.0884)	MaskBCELoss 0.0347 (0.0188)	MaskDICELoss 0.0783 (0.0696)
Epoch: [0][440/500]	Time 32.194 (32.194)	Loss 0.3691 (0.3670)	CeLoss 0.0315 (0.0265)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0967 (0.0961)	MaskBCELoss 0.0245 (0.0219)	MaskDICELoss 0.0722 (0.0741)
Epoch: [0][441/500]	Time 31.516 (31.516)	Loss 0.2892 (0.2971)	CeLoss 0.0004 (0.0053)	SegCLSLoss 0.0001 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0757 (0.0810)	MaskBCELoss 0.0072 (0.0162)	MaskDICELoss 0.0686 (0.0648)
Epoch: [0][442/500]	Time 32.524 (32.524)	Loss 0.3009 (0.3420)	CeLoss 0.0006 (0.0335)	SegCLSLoss 0.0001 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0809 (0.0847)	MaskBCELoss 0.0116 (0.0154)	MaskDICELoss 0.0692 (0.0694)
Epoch: [0][443/500]	Time 26.618 (26.618)	Loss 0.3132 (0.3282)	CeLoss 0.0002 (0.0246)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0863 (0.0856)	MaskBCELoss 0.0161 (0.0195)	MaskDICELoss 0.0702 (0.0661)
Epoch: [0][444/500]	Time 33.583 (33.583)	Loss 0.4929 (0.3409)	CeLoss 0.0713 (0.0179)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1167 (0.0891)	MaskBCELoss 0.0225 (0.0168)	MaskDICELoss 0.0942 (0.0724)
Epoch: [0][445/500]	Time 31.612 (31.612)	Loss 0.2832 (0.3353)	CeLoss 0.0004 (0.0098)	SegCLSLoss 0.0010 (0.0005)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0760 (0.0916)	MaskBCELoss 0.0116 (0.0208)	MaskDICELoss 0.0644 (0.0707)
Epoch: [0][446/500]	Time 31.082 (31.082)	Loss 0.3188 (0.3515)	CeLoss 0.0004 (0.0281)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0898 (0.0924)	MaskBCELoss 0.0204 (0.0232)	MaskDICELoss 0.0694 (0.0692)
Epoch: [0][447/500]	Time 28.716 (28.716)	Loss 0.2898 (0.2892)	CeLoss 0.0002 (0.0054)	SegCLSLoss 0.0014 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0774 (0.0760)	MaskBCELoss 0.0115 (0.0102)	MaskDICELoss 0.0660 (0.0657)
Epoch: [0][448/500]	Time 28.703 (28.703)	Loss 0.3263 (0.3533)	CeLoss 0.0148 (0.0236)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0890 (0.0898)	MaskBCELoss 0.0223 (0.0148)	MaskDICELoss 0.0667 (0.0750)
Epoch: [0][449/500]	Time 30.991 (30.991)	Loss 0.2499 (0.3441)	CeLoss 0.0004 (0.0291)	SegCLSLoss 0.0002 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0670 (0.0871)	MaskBCELoss 0.0094 (0.0169)	MaskDICELoss 0.0576 (0.0702)
Epoch: [0][450/500]	Time 33.403 (33.403)	Loss 0.3528 (0.3332)	CeLoss 0.0001 (0.0159)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0940 (0.0861)	MaskBCELoss 0.0117 (0.0137)	MaskDICELoss 0.0823 (0.0724)
Epoch: [0][451/500]	Time 31.932 (31.932)	Loss 0.3348 (0.3229)	CeLoss 0.0011 (0.0266)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0925 (0.0799)	MaskBCELoss 0.0182 (0.0118)	MaskDICELoss 0.0743 (0.0681)
Epoch: [0][452/500]	Time 33.467 (33.467)	Loss 0.3378 (0.3474)	CeLoss 0.0742 (0.0333)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0685 (0.0866)	MaskBCELoss 0.0053 (0.0163)	MaskDICELoss 0.0632 (0.0703)
Epoch: [0][453/500]	Time 32.455 (32.455)	Loss 0.4548 (0.3567)	CeLoss 0.0413 (0.0239)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1236 (0.0930)	MaskBCELoss 0.0404 (0.0198)	MaskDICELoss 0.0832 (0.0732)
Epoch: [0][454/500]	Time 29.665 (29.665)	Loss 0.2956 (0.3407)	CeLoss 0.0010 (0.0353)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0838 (0.0830)	MaskBCELoss 0.0203 (0.0135)	MaskDICELoss 0.0635 (0.0695)
Epoch: [0][455/500]	Time 32.929 (32.929)	Loss 0.4972 (0.3836)	CeLoss 0.0679 (0.0468)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1240 (0.0954)	MaskBCELoss 0.0335 (0.0223)	MaskDICELoss 0.0905 (0.0730)
Epoch: [0][456/500]	Time 26.952 (26.952)	Loss 0.2185 (0.3336)	CeLoss 0.0008 (0.0264)	SegCLSLoss 0.0001 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0554 (0.0842)	MaskBCELoss 0.0019 (0.0148)	MaskDICELoss 0.0534 (0.0693)
Epoch: [0][457/500]	Time 29.099 (29.099)	Loss 0.2833 (0.3242)	CeLoss 0.0305 (0.0323)	SegCLSLoss 0.0006 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0693 (0.0782)	MaskBCELoss 0.0128 (0.0105)	MaskDICELoss 0.0565 (0.0676)
Epoch: [0][458/500]	Time 31.090 (31.090)	Loss 0.3397 (0.3458)	CeLoss 0.0003 (0.0215)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0973 (0.0910)	MaskBCELoss 0.0249 (0.0199)	MaskDICELoss 0.0724 (0.0711)
Epoch: [0][459/500]	Time 30.594 (30.594)	Loss 0.3070 (0.3271)	CeLoss 0.0002 (0.0210)	SegCLSLoss 0.0001 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0811 (0.0846)	MaskBCELoss 0.0088 (0.0164)	MaskDICELoss 0.0722 (0.0683)
Epoch: [0][460/500]	Time 28.595 (28.595)	Loss 0.4298 (0.3468)	CeLoss 0.0232 (0.0341)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1149 (0.0859)	MaskBCELoss 0.0265 (0.0156)	MaskDICELoss 0.0883 (0.0703)
Epoch: [0][461/500]	Time 27.800 (27.800)	Loss 0.3561 (0.3418)	CeLoss 0.0001 (0.0464)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1010 (0.0805)	MaskBCELoss 0.0240 (0.0133)	MaskDICELoss 0.0770 (0.0672)
Epoch: [0][462/500]	Time 34.793 (34.793)	Loss 0.3548 (0.3351)	CeLoss 0.0002 (0.0046)	SegCLSLoss 0.0001 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0966 (0.0907)	MaskBCELoss 0.0160 (0.0164)	MaskDICELoss 0.0806 (0.0743)
Epoch: [0][463/500]	Time 33.084 (33.084)	Loss 0.2914 (0.3333)	CeLoss 0.0003 (0.0377)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0774 (0.0808)	MaskBCELoss 0.0092 (0.0139)	MaskDICELoss 0.0682 (0.0669)
Epoch: [0][464/500]	Time 32.911 (32.911)	Loss 0.3882 (0.3316)	CeLoss 0.0889 (0.0243)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0836 (0.0839)	MaskBCELoss 0.0175 (0.0142)	MaskDICELoss 0.0662 (0.0697)
Epoch: [0][465/500]	Time 33.409 (33.409)	Loss 0.3014 (0.3266)	CeLoss 0.0002 (0.0187)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0815 (0.0828)	MaskBCELoss 0.0124 (0.0117)	MaskDICELoss 0.0691 (0.0711)
Epoch: [0][466/500]	Time 30.726 (30.726)	Loss 0.2470 (0.3467)	CeLoss 0.0005 (0.0303)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0632 (0.0875)	MaskBCELoss 0.0031 (0.0168)	MaskDICELoss 0.0601 (0.0707)
Epoch: [0][467/500]	Time 32.393 (32.393)	Loss 0.2670 (0.3246)	CeLoss 0.0016 (0.0118)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0709 (0.0856)	MaskBCELoss 0.0092 (0.0151)	MaskDICELoss 0.0617 (0.0706)
Epoch: [0][468/500]	Time 31.639 (31.639)	Loss 0.3608 (0.3599)	CeLoss 0.0195 (0.0322)	SegCLSLoss 0.0003 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0943 (0.0900)	MaskBCELoss 0.0182 (0.0162)	MaskDICELoss 0.0761 (0.0738)
Epoch: [0][469/500]	Time 27.272 (27.272)	Loss 0.3345 (0.3280)	CeLoss 0.0535 (0.0397)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0748 (0.0786)	MaskBCELoss 0.0091 (0.0130)	MaskDICELoss 0.0657 (0.0655)
Epoch: [0][470/500]	Time 30.611 (30.611)	Loss 0.3510 (0.3274)	CeLoss 0.0815 (0.0260)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0726 (0.0826)	MaskBCELoss 0.0104 (0.0147)	MaskDICELoss 0.0621 (0.0679)
Epoch: [0][471/500]	Time 32.914 (32.914)	Loss 0.2541 (0.3549)	CeLoss 0.0020 (0.0225)	SegCLSLoss 0.0001 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0676 (0.0931)	MaskBCELoss 0.0092 (0.0201)	MaskDICELoss 0.0584 (0.0730)
Epoch: [0][472/500]	Time 30.326 (30.326)	Loss 0.3471 (0.3471)	CeLoss 0.0004 (0.0238)	SegCLSLoss 0.0001 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0977 (0.0892)	MaskBCELoss 0.0221 (0.0167)	MaskDICELoss 0.0756 (0.0724)
Epoch: [0][473/500]	Time 34.890 (34.890)	Loss 0.3419 (0.3413)	CeLoss 0.0011 (0.0265)	SegCLSLoss 0.0001 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1005 (0.0886)	MaskBCELoss 0.0305 (0.0198)	MaskDICELoss 0.0699 (0.0688)
Epoch: [0][474/500]	Time 28.794 (28.794)	Loss 0.2937 (0.3850)	CeLoss 0.0002 (0.0484)	SegCLSLoss 0.0001 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0751 (0.0926)	MaskBCELoss 0.0035 (0.0169)	MaskDICELoss 0.0715 (0.0757)
Epoch: [0][475/500]	Time 30.770 (30.770)	Loss 0.3336 (0.3443)	CeLoss 0.0003 (0.0343)	SegCLSLoss 0.0001 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0921 (0.0862)	MaskBCELoss 0.0177 (0.0174)	MaskDICELoss 0.0744 (0.0688)
Epoch: [0][476/500]	Time 27.686 (27.686)	Loss 0.2161 (0.3083)	CeLoss 0.0013 (0.0312)	SegCLSLoss 0.0001 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0545 (0.0753)	MaskBCELoss 0.0016 (0.0123)	MaskDICELoss 0.0529 (0.0630)
Epoch: [0][477/500]	Time 29.974 (29.974)	Loss 0.3405 (0.3443)	CeLoss 0.0547 (0.0290)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0769 (0.0868)	MaskBCELoss 0.0108 (0.0159)	MaskDICELoss 0.0660 (0.0708)
Epoch: [0][478/500]	Time 32.134 (32.134)	Loss 0.3613 (0.3624)	CeLoss 0.0309 (0.0410)	SegCLSLoss 0.0001 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0866 (0.0877)	MaskBCELoss 0.0081 (0.0146)	MaskDICELoss 0.0785 (0.0730)
Epoch: [0][479/500]	Time 30.944 (30.944)	Loss 0.2840 (0.3203)	CeLoss 0.0004 (0.0086)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0749 (0.0855)	MaskBCELoss 0.0081 (0.0152)	MaskDICELoss 0.0668 (0.0703)
Epoch: [0][480/500]	Time 30.838 (30.838)	Loss 0.4172 (0.3347)	CeLoss 0.0009 (0.0133)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1229 (0.0887)	MaskBCELoss 0.0376 (0.0170)	MaskDICELoss 0.0852 (0.0718)
Epoch: [0][481/500]	Time 34.007 (34.007)	Loss 0.2118 (0.3283)	CeLoss 0.0006 (0.0215)	SegCLSLoss 0.0001 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0535 (0.0854)	MaskBCELoss 0.0016 (0.0175)	MaskDICELoss 0.0519 (0.0679)
Epoch: [0][482/500]	Time 29.157 (29.157)	Loss 0.3443 (0.3500)	CeLoss 0.0002 (0.0208)	SegCLSLoss 0.0002 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1024 (0.0925)	MaskBCELoss 0.0328 (0.0206)	MaskDICELoss 0.0695 (0.0719)
Epoch: [0][483/500]	Time 29.042 (29.042)	Loss 0.2862 (0.3698)	CeLoss 0.0003 (0.0439)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0815 (0.0925)	MaskBCELoss 0.0202 (0.0223)	MaskDICELoss 0.0614 (0.0702)
Epoch: [0][484/500]	Time 33.218 (33.218)	Loss 0.3020 (0.3640)	CeLoss 0.0344 (0.0434)	SegCLSLoss 0.0001 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0717 (0.0891)	MaskBCELoss 0.0096 (0.0178)	MaskDICELoss 0.0621 (0.0713)
Epoch: [0][485/500]	Time 33.866 (33.866)	Loss 0.2848 (0.3307)	CeLoss 0.0002 (0.0139)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0768 (0.0888)	MaskBCELoss 0.0114 (0.0193)	MaskDICELoss 0.0654 (0.0695)
Epoch: [0][486/500]	Time 29.103 (29.103)	Loss 0.3875 (0.3498)	CeLoss 0.0508 (0.0387)	SegCLSLoss 0.0004 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0906 (0.0842)	MaskBCELoss 0.0132 (0.0130)	MaskDICELoss 0.0774 (0.0713)
Epoch: [0][487/500]	Time 32.818 (32.818)	Loss 0.2504 (0.3458)	CeLoss 0.0003 (0.0275)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0677 (0.0861)	MaskBCELoss 0.0104 (0.0132)	MaskDICELoss 0.0573 (0.0729)
Epoch: [0][488/500]	Time 28.857 (28.857)	Loss 0.4186 (0.3440)	CeLoss 0.0002 (0.0189)	SegCLSLoss 0.0010 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1189 (0.0894)	MaskBCELoss 0.0295 (0.0167)	MaskDICELoss 0.0894 (0.0728)
Epoch: [0][489/500]	Time 33.820 (33.820)	Loss 0.3262 (0.3561)	CeLoss 0.0002 (0.0269)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0888 (0.0931)	MaskBCELoss 0.0146 (0.0217)	MaskDICELoss 0.0742 (0.0714)
Epoch: [0][490/500]	Time 30.070 (30.070)	Loss 0.4256 (0.3293)	CeLoss 0.0493 (0.0198)	SegCLSLoss 0.0006 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1041 (0.0850)	MaskBCELoss 0.0206 (0.0155)	MaskDICELoss 0.0835 (0.0695)
Epoch: [0][491/500]	Time 32.648 (32.648)	Loss 0.2617 (0.3200)	CeLoss 0.0021 (0.0339)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0718 (0.0777)	MaskBCELoss 0.0139 (0.0124)	MaskDICELoss 0.0579 (0.0653)
Epoch: [0][492/500]	Time 32.043 (32.043)	Loss 0.3522 (0.3507)	CeLoss 0.0520 (0.0166)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0854 (0.0936)	MaskBCELoss 0.0207 (0.0203)	MaskDICELoss 0.0647 (0.0733)
Epoch: [0][493/500]	Time 31.550 (31.550)	Loss 0.2461 (0.3417)	CeLoss 0.0002 (0.0320)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0653 (0.0865)	MaskBCELoss 0.0077 (0.0182)	MaskDICELoss 0.0576 (0.0683)
Epoch: [0][494/500]	Time 31.991 (31.991)	Loss 0.3407 (0.3438)	CeLoss 0.0226 (0.0333)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0840 (0.0846)	MaskBCELoss 0.0090 (0.0141)	MaskDICELoss 0.0750 (0.0705)
Epoch: [0][495/500]	Time 29.109 (29.109)	Loss 0.3237 (0.3211)	CeLoss 0.0234 (0.0132)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0791 (0.0858)	MaskBCELoss 0.0080 (0.0178)	MaskDICELoss 0.0710 (0.0680)
Epoch: [0][496/500]	Time 31.726 (31.726)	Loss 0.4836 (0.3657)	CeLoss 0.0659 (0.0389)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1275 (0.0922)	MaskBCELoss 0.0463 (0.0212)	MaskDICELoss 0.0812 (0.0710)
Epoch: [0][497/500]	Time 27.585 (27.585)	Loss 0.3266 (0.3188)	CeLoss 0.0825 (0.0224)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0628 (0.0826)	MaskBCELoss 0.0036 (0.0170)	MaskDICELoss 0.0593 (0.0656)
Epoch: [0][498/500]	Time 30.920 (30.920)	Loss 0.2958 (0.3142)	CeLoss 0.0001 (0.0129)	SegCLSLoss 0.0000 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0790 (0.0816)	MaskBCELoss 0.0102 (0.0127)	MaskDICELoss 0.0688 (0.0688)
Epoch: [0][499/500]	Time 35.332 (35.332)	Loss 0.3356 (0.3357)	CeLoss 0.0884 (0.0322)	SegCLSLoss 0.0001 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0647 (0.0828)	MaskBCELoss 0.0057 (0.0139)	MaskDICELoss 0.0590 (0.0689)
[2025-04-05 00:08:50,964] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[0.00029461445783132527], mom=[(0.9, 0.95)]
[2025-04-05 00:08:50,972] [INFO] [timer.py:215:stop] epoch=0/micro_step=500/global_step=50, RunningAvgSamplesPerSec=1.5624296983739177, CurrSamplesPerSec=1.2482436499760652, MemAllocated=33.48GB, MaxMemAllocated=46.18GB
Epoch: [0][500/500]	Time 31.960 (31.960)	Loss 0.2269 (0.3009)	CeLoss 0.0002 (0.0161)	SegCLSLoss 0.0006 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0586 (0.0778)	MaskBCELoss 0.0044 (0.0135)	MaskDICELoss 0.0542 (0.0644)
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something showing that the man is playing sports in this image? Please output segmentation mask. ASSISTANT: something showing that the man is playing sports</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
  0%|▋                                                                                                                                                | 1/200 [00:01<04:52,  1.47s/it]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3168, 4416])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3168, 4416])):  [tensor([[[-12.3750, -12.3750, -12.3750,  ..., -12.9375, -12.9375, -12.9375],
         [-12.3750, -12.3750, -12.3750,  ..., -12.9375, -12.9375, -12.9375],
         [-12.3750, -12.3750, -12.3750,  ..., -12.9375, -12.9375, -12.9375],
         ...,
         [ -8.0025,  -8.0025,  -8.0025,  ...,  -2.7544,  -2.7544,  -2.7544],
         [ -8.0312,  -8.0312,  -8.0312,  ...,  -2.7930,  -2.7930,  -2.7930],
         [ -8.0312,  -8.0312,  -8.0312,  ...,  -2.7930,  -2.7930,  -2.7930]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Horses can be trained to follow commands and be controlled while being ridden. What object in the picture is used for controlling and guiding a horse? Please output segmentation mask. ASSISTANT: horses can be trained to follow commands and be controlled while being ridden. what object in the picture is used for controlling and guiding a horse</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1024, 768])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1024, 768])):  [tensor([[[-7.5000, -7.5000, -7.5625,  ..., -9.5078, -9.5469, -9.3906],
         [-7.5000, -7.5000, -7.5625,  ..., -9.5078, -9.5469, -9.3906],
         [-7.5391, -7.5391, -7.5938,  ..., -9.5605, -9.5928, -9.4346],
         ...,
         [-7.9805, -7.9805, -8.6519,  ..., -6.4102, -6.6616, -7.0864],
         [-7.9062, -7.9062, -8.5664,  ..., -6.4531, -6.6875, -7.0625],
         [-7.9062, -7.9062, -8.5664,  ..., -6.4531, -6.6875, -7.0625]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that allows people to drink without touching the rim of the cup in this image? Please output segmentation mask. ASSISTANT: something that allows people to drink without touching the rim of the cup</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[71]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1600, 1200])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1600, 1200])):  [tensor([[[-6.9375, -6.9375, -6.9398,  ..., -7.3902, -7.4737, -7.5312],
         [-6.9375, -6.9375, -6.9398,  ..., -7.3902, -7.4737, -7.5312],
         [-6.9418, -6.9418, -6.9442,  ..., -7.3904, -7.4749, -7.5333],
         ...,
         [-3.4859, -3.4859, -3.4854,  ..., -3.8909, -4.0858, -4.2288],
         [-3.4844, -3.4844, -3.4838,  ..., -3.8930, -4.0866, -4.2285],
         [-3.4844, -3.4844, -3.4838,  ..., -3.8930, -4.0866, -4.2285]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In the picture, there seems to be a symbiotic relationship between two species, where one provides protection for the other. What animal in the picture is known to seek refuge in the tentacles of another creature for safety? Please output segmentation mask. ASSISTANT: in the picture, there seems to be a symbiotic relationship between two species, where one provides protection for the other. what animal in the picture is known to seek refuge in the tentacles of another creature for safety</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[96]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2736, 3648])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2736, 3648])):  [tensor([[[-5.1250, -5.1250, -5.1250,  ..., -8.5625, -8.5625, -8.5625],
         [-5.1250, -5.1250, -5.1250,  ..., -8.5625, -8.5625, -8.5625],
         [-5.1250, -5.1250, -5.1250,  ..., -8.5625, -8.5625, -8.5625],
         ...,
         [-5.1996, -5.1996, -5.1996,  ..., -2.5006, -2.5006, -2.5006],
         [-5.2500, -5.2500, -5.2500,  ..., -2.6172, -2.6172, -2.6172],
         [-5.2500, -5.2500, -5.2500,  ..., -2.6172, -2.6172, -2.6172]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the slogan for welcoming in this image? Please output segmentation mask. ASSISTANT: the slogan for welcoming</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[-10.2500, -10.2500, -10.2570,  ..., -10.5562, -10.5625, -10.5625],
         [-10.2500, -10.2500, -10.2570,  ..., -10.5562, -10.5625, -10.5625],
         [-10.2477, -10.2477, -10.2549,  ..., -10.5657, -10.5719, -10.5719],
         ...,
         [ -4.4016,  -4.4016,  -4.4022,  ...,  -1.9026,  -1.9040,  -1.9040],
         [ -4.6156,  -4.6156,  -4.6187,  ...,  -2.4734,  -2.4739,  -2.4739],
         [ -4.7773,  -4.7773,  -4.7825,  ...,  -2.9154,  -2.9150,  -2.9150]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n For the safety of newborns, they are often placed in a secure and comfortable space when they sleep. What furniture in the picture is commonly used to provide a safe sleeping environment for babies? Please output segmentation mask. ASSISTANT: for the safety of newborns, they are often placed in a secure and comfortable space when they sleep. what furniture in the picture is commonly used to provide a safe sleeping environment for babies</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[94]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1002, 1455])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1002, 1455])):  [tensor([[[-7.8750, -7.8750, -7.8689,  ..., -7.9203, -7.9375, -7.9375],
         [-7.8750, -7.8750, -7.8689,  ..., -7.9203, -7.9375, -7.9375],
         [-7.8730, -7.8730, -7.8677,  ..., -7.9188, -7.9355, -7.9355],
         ...,
         [-6.4582, -6.4582, -6.4988,  ..., -4.3115, -4.3175, -4.3175],
         [-6.0624, -6.0624, -6.0921,  ..., -4.0001, -4.0014, -4.0014],
         [-5.7500, -5.7500, -5.7712,  ..., -3.7542, -3.7520, -3.7520]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When using a film camera to take photos, what part of the camera helps focus the light on the film to capture the image? Please output segmentation mask. ASSISTANT: when using a film camera to take photos, what part of the camera helps focus the light on the film to capture the image</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[77]]

  4%|█████                                                                                                                                            | 7/200 [00:03<01:01,  3.13it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1067, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1067, 1600])):  [tensor([[[-7.0938, -7.0938, -7.0988,  ..., -7.9918, -8.0000, -8.0000],
         [-7.0937, -7.0937, -7.0988,  ..., -7.9918, -8.0000, -8.0000],
         [-7.0941, -7.0941, -7.0993,  ..., -7.9974, -8.0055, -8.0055],
         ...,
         [-5.7281, -5.7281, -5.7305,  ..., -2.3074, -2.3024, -2.3024],
         [-5.9480, -5.9480, -5.9490,  ..., -2.2897, -2.2834, -2.2834],
         [-6.1367, -6.1367, -6.1369,  ..., -2.3041, -2.2969, -2.2969]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Birds have various ways of searching for food. What part of their body helps them to grab and pick up food from the ground in the picture? Please output segmentation mask. ASSISTANT: birds have various ways of searching for food. what part of their body helps them to grab and pick up food from the ground in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2570, 2229])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2570, 2229])):  [tensor([[[-11.1875, -11.1875, -11.1875,  ...,  -8.6870,  -8.6372,  -8.6250],
         [-11.1875, -11.1875, -11.1875,  ...,  -8.6870,  -8.6372,  -8.6250],
         [-11.1875, -11.1875, -11.1875,  ...,  -8.6870,  -8.6372,  -8.6250],
         ...,
         [-12.6250, -12.6250, -12.6250,  ..., -11.5640, -11.7134, -11.7500],
         [-12.6250, -12.6250, -12.6250,  ..., -11.5640, -11.7134, -11.7500],
         [-12.6250, -12.6250, -12.6250,  ..., -11.5640, -11.7134, -11.7500]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the sour food in this image? Please output segmentation mask. ASSISTANT: the sour food</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[-6.4062, -6.4414, -6.5625,  ..., -6.7031, -6.9574, -7.0312],
         [-6.4133, -6.4413, -6.5379,  ..., -6.6416, -6.8373, -6.8941],
         [-6.4375, -6.4410, -6.4531,  ..., -6.4297, -6.4236, -6.4219],
         ...,
         [-2.3281, -2.3308, -2.3398,  ..., -7.7734, -8.0459, -8.1250],
         [-2.4303, -2.4465, -2.5022,  ..., -7.7312, -8.0328, -8.1203],
         [-3.6797, -3.7133, -3.8289,  ..., -7.7234, -7.9475, -8.0125]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that protects the woman's eyes from getting wet in this image? Please output segmentation mask. ASSISTANT: something that protects the woman's eyes from getting wet</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[69]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2112, 2370])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2112, 2370])):  [tensor([[[-7.8750, -7.8750, -7.8750,  ..., -6.3750, -6.3750, -6.3750],
         [-7.8750, -7.8750, -7.8750,  ..., -6.3750, -6.3750, -6.3750],
         [-7.8750, -7.8750, -7.8750,  ..., -6.3750, -6.3750, -6.3750],
         ...,
         [-6.5133, -6.5133, -6.5133,  ..., -5.5768, -5.5768, -5.5768],
         [-6.0236, -6.0236, -6.0236,  ..., -5.2492, -5.2492, -5.2492],
         [-5.8555, -5.8555, -5.8555,  ..., -5.1367, -5.1367, -5.1367]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When participating in water activities such as kayaking, it is important to ensure personal safety. What item in the picture is designed to provide buoyancy and keep the person afloat in case of an accident? Please output segmentation mask. ASSISTANT: when participating in water activities such as kayaking, it is important to ensure personal safety. what item in the picture is designed to provide buoyancy and keep the person afloat in case of an accident</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[95]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1037, 1322])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1037, 1322])):  [tensor([[[-10.1250, -10.1250, -10.1386,  ..., -10.4488, -10.5000, -10.5000],
         [-10.1250, -10.1250, -10.1386,  ..., -10.4488, -10.5000, -10.5000],
         [-10.1012, -10.1012, -10.1157,  ..., -10.4327, -10.4830, -10.4830],
         ...,
         [ -6.9913,  -6.9913,  -7.0553,  ...,  -4.7409,  -4.7268,  -4.7268],
         [ -5.6460,  -5.6460,  -5.6713,  ...,  -3.7128,  -3.6833,  -3.6833],
         [ -5.0000,  -5.0000,  -5.0002,  ...,  -3.1899,  -3.1445,  -3.1445]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the sewage outlet in this image? Please output segmentation mask. ASSISTANT: the sewage outlet</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]

  6%|████████▋                                                                                                                                       | 12/200 [00:05<01:00,  3.12it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 4032, 3024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 4032, 3024])):  [tensor([[[-6.8125, -6.8125, -6.8125,  ..., -9.1463, -9.1484, -9.1484],
         [-6.8125, -6.8125, -6.8125,  ..., -9.1463, -9.1484, -9.1484],
         [-6.8125, -6.8125, -6.8125,  ..., -9.1463, -9.1484, -9.1484],
         ...,
         [-8.1250, -8.1250, -8.1250,  ..., -5.8289, -5.8984, -5.8984],
         [-8.1250, -8.1250, -8.1250,  ..., -5.8289, -5.8984, -5.8984],
         [-8.1250, -8.1250, -8.1250,  ..., -5.8289, -5.8984, -5.8984]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In some circus performances, gymnasts perform aerial acrobatics using long pieces of fabric that hang from the ceiling. What in the picture could be used for this type of performance? Please output segmentation mask. ASSISTANT: in some circus performances, gymnasts perform aerial acrobatics using long pieces of fabric that hang from the ceiling. what in the picture could be used for this type of performance</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[92]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1064, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1064, 1600])):  [tensor([[[-11.9375, -11.9375, -11.9523,  ..., -14.5320, -14.5625, -14.5625],
         [-11.9375, -11.9375, -11.9523,  ..., -14.5320, -14.5625, -14.5625],
         [-11.9445, -11.9445, -11.9593,  ..., -14.5213, -14.5516, -14.5516],
         ...,
         [ -8.3375,  -8.3375,  -8.3312,  ...,  -5.6942,  -5.6828,  -5.6828],
         [ -8.3775,  -8.3775,  -8.3710,  ...,  -5.6492,  -5.6378,  -5.6378],
         [ -8.4062,  -8.4062,  -8.3997,  ...,  -5.6168,  -5.6055,  -5.6055]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the stacked cookies in this image? Please output segmentation mask. ASSISTANT: the stacked cookies</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1402, 2102])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1402, 2102])):  [tensor([[[-7.3750, -7.3750, -7.3750,  ..., -6.2500, -6.2500, -6.2500],
         [-7.3750, -7.3750, -7.3750,  ..., -6.2500, -6.2500, -6.2500],
         [-7.3750, -7.3750, -7.3750,  ..., -6.2500, -6.2500, -6.2500],
         ...,
         [-5.3009, -5.3009, -5.3009,  ..., -4.8281, -4.8281, -4.8281],
         [-5.4360, -5.4360, -5.4360,  ..., -4.8281, -4.8281, -4.8281],
         [-5.5000, -5.5000, -5.5000,  ..., -4.8281, -4.8281, -4.8281]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If a person wanted to charge their electric car while parked in the street, what object in the picture could they use to connect the car to an electrical power source? Please output segmentation mask. ASSISTANT: if a person wanted to charge their electric car while parked in the street, what object in the picture could they use to connect the car to an electrical power source</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[86]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2016, 3032])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2016, 3032])):  [tensor([[[-8.1250, -8.1250, -8.1250,  ..., -9.1250, -9.1250, -9.1250],
         [-8.1250, -8.1250, -8.1250,  ..., -9.1250, -9.1250, -9.1250],
         [-8.1250, -8.1250, -8.1250,  ..., -9.1250, -9.1250, -9.1250],
         ...,
         [-4.3896, -4.3896, -4.3896,  ..., -3.2719, -3.2719, -3.2719],
         [-4.4212, -4.4212, -4.4212,  ..., -3.3194, -3.3194, -3.3194],
         [-4.4219, -4.4219, -4.4219,  ..., -3.3203, -3.3203, -3.3203]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Where do people put their dirty hygiene products to keep the bathroom clean? Please output segmentation mask. ASSISTANT: where do people put their dirty hygiene products to keep the bathroom clean</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-8.2279, -7.7013, -7.3002,  ..., -5.7469, -5.7291, -5.8950],
         [-7.5509, -7.6003, -7.4828,  ..., -5.5750, -5.6087, -5.4498],
         [-6.7250, -7.0688, -7.2266,  ..., -4.8281, -4.9000, -4.8141],
         ...,
         [-6.0730, -5.7937, -5.2656,  ..., -4.9531, -4.5453, -4.2553],
         [-5.9090, -5.8875, -5.2172,  ..., -5.0172, -4.6850, -4.5432],
         [-5.9999, -6.2110, -5.5697,  ..., -5.0094, -4.6212, -4.4039]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When the wind blows, small white objects are blown away and scattered in the air. What in the picture is responsible for this phenomenon? Please output segmentation mask. ASSISTANT: when the wind blows, small white objects are blown away and scattered in the air. what in the picture is responsible for this phenomenon</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[-8.0000, -8.0427, -8.2437,  ..., -8.4094, -8.5873, -8.6250],
         [-7.9738, -8.0164, -8.2175,  ..., -8.3768, -8.5437, -8.5791],
         [-7.8501, -7.8928, -8.0939,  ..., -8.2230, -8.3383, -8.3627],
         ...,
         [-7.6809, -7.7954, -8.3352,  ..., -4.2808, -4.4176, -4.4467],
         [-7.3211, -7.4062, -7.8073,  ..., -4.1254, -4.1966, -4.2117],
         [-6.9612, -7.0169, -7.2795,  ..., -3.9699, -3.9756, -3.9768]]],

  8%|████████████▏                                                                                                                                   | 17/200 [00:07<01:04,  2.84it/s]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that the dog's food should be put into in this image? Please output segmentation mask. ASSISTANT: something that the dog's food should be put into</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[-5.7812, -5.8340, -6.0156,  ..., -5.8125, -6.1031, -6.1875],
         [-5.7637, -5.8164, -5.9980,  ..., -5.8318, -6.0802, -6.1523],
         [-5.7031, -5.7559, -5.9375,  ..., -5.8984, -6.0014, -6.0312],
         ...,
         [-6.7031, -6.8156, -7.2031,  ..., -4.1250, -4.1734, -4.1875],
         [-6.7004, -6.8277, -7.2660,  ..., -4.0553, -4.0839, -4.0922],
         [-6.3625, -6.4581, -6.7875,  ..., -4.1016, -4.1004, -4.1000]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the food that makes people feel spicy or hot in this image? Please output segmentation mask. ASSISTANT: the food that makes people feel spicy or hot</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[67]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 679, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 679, 1024])):  [tensor([[[-10.8750, -10.8750, -10.8906,  ...,  -3.5312,  -3.5156,  -3.5156],
         [-10.8750, -10.8750, -10.8906,  ...,  -3.5312,  -3.5156,  -3.5156],
         [-10.8125, -10.8125, -10.8330,  ...,  -3.3713,  -3.3418,  -3.3418],
         ...,
         [ -3.9043,  -3.9043,  -3.9023,  ...,  -2.4565,  -2.4512,  -2.4512],
         [ -4.3223,  -4.3223,  -4.2930,  ...,  -2.5884,  -2.5566,  -2.5566],
         [ -5.2695,  -5.2695,  -5.2358,  ...,  -3.1125,  -3.0566,  -3.0566]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the object used to protect the ears in this image? Please output segmentation mask. ASSISTANT: the object used to protect the ears</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2112, 2816])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2112, 2816])):  [tensor([[[-12.9375, -12.9375, -12.9375,  ..., -15.0000, -15.0000, -15.0000],
         [-12.9375, -12.9375, -12.9375,  ..., -15.0000, -15.0000, -15.0000],
         [-12.9375, -12.9375, -12.9375,  ..., -15.0000, -15.0000, -15.0000],
         ...,
         [-12.9545, -12.9545, -12.9545,  ...,  -4.4006,  -4.4006,  -4.4006],
         [-12.7727, -12.7727, -12.7727,  ...,  -4.4403,  -4.4403,  -4.4403],
         [-12.7500, -12.7500, -12.7500,  ...,  -4.4453,  -4.4453,  -4.4453]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the place where piano players should sit in this image? Please output segmentation mask. ASSISTANT: the place where piano players should sit</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 960, 1280])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 960, 1280])):  [tensor([[[-6.3438, -6.3438, -6.3379,  ..., -4.2930, -4.3125, -4.3125],
         [-6.3438, -6.3438, -6.3379,  ..., -4.2930, -4.3125, -4.3125],
         [-6.3301, -6.3301, -6.3247,  ..., -4.2397, -4.2568, -4.2568],
         ...,
         [-3.6055, -3.6055, -3.6493,  ..., -2.3377, -2.3457, -2.3457],
         [-3.7875, -3.7875, -3.8234,  ..., -2.6394, -2.6437, -2.6437],
         [-4.0254, -4.0254, -4.0526,  ..., -2.9785, -2.9746, -2.9746]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the person who is shooting a free throw in this image? Please output segmentation mask. ASSISTANT: the person who is shooting a free throw</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2287, 1831])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2287, 1831])):  [tensor([[[-8.4375, -8.4375, -8.4375,  ..., -7.7500, -7.7500, -7.7500],
         [-8.4375, -8.4375, -8.4375,  ..., -7.7500, -7.7500, -7.7500],
         [-8.4375, -8.4375, -8.4375,  ..., -7.7500, -7.7500, -7.7500],
         ...,
         [-6.3125, -6.3125, -6.3125,  ..., -5.0352, -5.1961, -5.2578],
         [-6.3125, -6.3125, -6.3125,  ..., -5.0352, -5.1961, -5.2578],
         [-6.3125, -6.3125, -6.3125,  ..., -5.0352, -5.1961, -5.2578]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When soldiers gather for a group photo, what object in the picture is usually held by one of them to represent their unit or country? Please output segmentation mask. ASSISTANT: when soldiers gather for a group photo, what object in the picture is usually held by one of them to represent their unit or country</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]

 11%|███████████████▊                                                                                                                                | 22/200 [00:09<01:02,  2.86it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 431, 600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 431, 600])):  [tensor([[[-10.9375, -11.0075, -11.2208,  ..., -12.3563, -13.0763, -13.3125],
         [-10.9901, -11.0589, -11.2685,  ..., -12.3905, -13.0319, -13.2423],
         [-11.1502, -11.2153, -11.4135,  ..., -12.4945, -12.8968, -13.0289],
         ...,
         [ -5.5541,  -5.5622,  -5.5871,  ...,  -7.4214,  -7.6435,  -7.7164],
         [ -5.6075,  -5.6194,  -5.6556,  ...,  -7.4837,  -7.8196,  -7.9298],
         [ -6.6458,  -6.6752,  -6.7649,  ...,  -7.0678,  -7.3112,  -7.3911]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Sometimes, water leaks from faulty plumbing or faucets. What part of the plumbing system in the picture can be a potential source of the water leak? Please output segmentation mask. ASSISTANT: sometimes, water leaks from faulty plumbing or faucets. what part of the plumbing system in the picture can be a potential source of the water leak</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3872, 2592])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3872, 2592])):  [tensor([[[-7.0938, -7.0938, -7.0938,  ..., -0.9628, -0.9868, -0.9868],
         [-7.0938, -7.0938, -7.0938,  ..., -0.9628, -0.9868, -0.9868],
         [-7.0938, -7.0938, -7.0938,  ..., -0.9628, -0.9868, -0.9868],
         ...,
         [-3.6562, -3.6562, -3.6562,  ..., -3.2367, -3.2305, -3.2305],
         [-3.6562, -3.6562, -3.6562,  ..., -3.2367, -3.2305, -3.2305],
         [-3.6562, -3.6562, -3.6562,  ..., -3.2367, -3.2305, -3.2305]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the part of the persons that is above the water in this image? Please output segmentation mask. ASSISTANT: the part of the persons that is above the water</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[67]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[ -9.5000,  -9.5000,  -9.5000,  ...,  -9.2500,  -9.2500,  -9.2500],
         [ -9.5000,  -9.5000,  -9.5000,  ...,  -9.2500,  -9.2500,  -9.2500],
         [ -9.5000,  -9.5000,  -9.5000,  ...,  -9.2500,  -9.2500,  -9.2500],
         ...,
         [-13.1016, -13.1016, -13.1016,  ...,  -4.8945,  -4.8945,  -4.8945],
         [-13.1484, -13.1484, -13.1484,  ...,  -5.1055,  -5.1055,  -5.1055],
         [-13.1484, -13.1484, -13.1484,  ...,  -5.1055,  -5.1055,  -5.1055]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When we need to access or store things above our reach, what would be helpful to stand on? Please output segmentation mask. ASSISTANT: when we need to access or store things above our reach, what would be helpful to stand on</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[71]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-7.2852, -7.5242, -7.4109,  ..., -3.6745, -3.5515, -3.6716],
         [-7.2664, -7.6531, -7.6687,  ..., -3.3898, -3.2883, -2.9284],
         [-7.3676, -7.8187, -7.7031,  ..., -3.2500, -3.1977, -2.9882],
         ...,
         [-5.9684, -6.8063, -6.2578,  ..., -2.9844, -2.7203, -2.6881],
         [-6.3276, -7.1569, -6.3625,  ..., -3.0336, -2.8427, -2.8760],
         [-6.1406, -6.9584, -6.3561,  ..., -3.4833, -3.1534, -3.0156]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What structure can be used to drain the water from the bathroom floor and prevent the accumulation of wastewater? Please output segmentation mask. ASSISTANT: what structure can be used to drain the water from the bathroom floor and prevent the accumulation of wastewater</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[75]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[ -7.9305,  -7.5650,  -7.4846,  ...,  -5.0418,  -4.9546,  -5.1342],
         [ -7.9087,  -7.5284,  -7.4953,  ...,  -4.7625,  -4.6903,  -4.1167],
         [ -7.8258,  -7.6969,  -7.8750,  ...,  -4.7969,  -4.8359,  -4.3525],
         ...,
         [-10.3699, -11.8094, -10.9375,  ...,  -4.7656,  -4.1531,  -3.8953],
         [-10.3305, -12.0062, -10.7250,  ...,  -4.9937,  -4.3250,  -4.2391],
         [ -9.4043, -10.6275,  -9.5631,  ...,  -4.4004,  -3.9568,  -4.0913]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the dog that puts its tongue out in this image? Please output segmentation mask. ASSISTANT: the dog that puts its tongue out</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]

 14%|████████████████████▏                                                                                                                           | 28/200 [00:11<00:53,  3.20it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1600, 1200])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1600, 1200])):  [tensor([[[-7.5938, -7.5938, -7.5969,  ..., -7.6879, -7.6363, -7.5859],
         [-7.5938, -7.5938, -7.5969,  ..., -7.6879, -7.6363, -7.5859],
         [-7.5988, -7.5988, -7.6020,  ..., -7.6879, -7.6371, -7.5875],
         ...,
         [-6.0207, -6.0207, -6.0221,  ..., -1.2395, -1.7977, -2.2286],
         [-6.0312, -6.0312, -6.0324,  ..., -1.2427, -1.7990, -2.2285],
         [-6.0312, -6.0312, -6.0324,  ..., -1.2427, -1.7990, -2.2285]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a rural landscape, what objects in the picture could provide shade and shelter for animals or humans? Please output segmentation mask. ASSISTANT: in a rural landscape, what objects in the picture could provide shade and shelter for animals or humans</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[72]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[-7.3125, -7.3230, -7.3594,  ..., -7.4688, -7.5172, -7.5312],
         [-7.3336, -7.3457, -7.3875,  ..., -7.4863, -7.5157, -7.5242],
         [-7.4062, -7.4238, -7.4844,  ..., -7.5469, -7.5105, -7.5000],
         ...,
         [-4.6719, -4.8230, -5.3438,  ..., -2.9141, -2.9322, -2.9375],
         [-4.5297, -4.6682, -5.1453,  ..., -2.8309, -2.8527, -2.8590],
         [-4.8344, -4.9293, -5.2563,  ..., -3.2547, -3.2535, -3.2531]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something used for playing videos or music in this image? Please output segmentation mask. ASSISTANT: something used for playing videos or music</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-4.4062, -4.4062, -4.3867,  ..., -5.7344, -5.7500, -5.7500],
         [-4.4062, -4.4062, -4.3867,  ..., -5.7344, -5.7500, -5.7500],
         [-4.3477, -4.3477, -4.3286,  ..., -5.6938, -5.7070, -5.7070],
         ...,
         [-2.6172, -2.6172, -2.6465,  ..., -2.0374, -2.0342, -2.0342],
         [-2.8965, -2.8965, -2.9207,  ..., -2.3162, -2.3086, -2.3086],
         [-3.2832, -3.2832, -3.2971,  ..., -2.8196, -2.8008, -2.8008]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When celebrating birthdays, it is common to have a cake with decorations. What part of the cake in the picture is typically used to write birthday greetings or the name of the person celebrating? Please output segmentation mask. ASSISTANT: when celebrating birthdays, it is common to have a cake with decorations. what part of the cake in the picture is typically used to write birthday greetings or the name of the person celebrating</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[97]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[-5.0312, -5.0277, -5.0156,  ..., -5.9375, -6.2039, -6.2812],
         [-5.0207, -5.0200, -5.0174,  ..., -5.9375, -6.1876, -6.2602],
         [-4.9844, -4.9932, -5.0234,  ..., -5.9375, -6.1313, -6.1875],
         ...,
         [-5.7812, -5.8586, -6.1250,  ..., -3.1992, -3.2204, -3.2266],
         [-5.7219, -5.8198, -6.1572,  ..., -3.2019, -3.2517, -3.2662],
         [-5.4562, -5.5332, -5.7984,  ..., -3.1937, -3.2107, -3.2156]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that allow pedestrians to cross the canyon in this image? Please output segmentation mask. ASSISTANT: something that allow pedestrians to cross the canyon</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1989, 5815])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1989, 5815])):  [tensor([[[-6.3750, -6.3750, -6.3750,  ..., -4.0938, -4.0938, -4.0938],
         [-6.3750, -6.3750, -6.3750,  ..., -4.0938, -4.0938, -4.0938],
         [-6.3750, -6.3750, -6.3750,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [-3.8379, -3.8379, -3.8379,  ..., -1.7109, -1.7109, -1.7109],
         [-3.8379, -3.8379, -3.8379,  ..., -1.7109, -1.7109, -1.7109],
         [-3.8379, -3.8379, -3.8379,  ..., -1.7109, -1.7109, -1.7109]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the place where the dishes and meals should be put for eating in this image? Please output segmentation mask. ASSISTANT: the place where the dishes and meals should be put for eating</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[72]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 601, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 601, 800])):  [tensor([[[ -8.9375,  -8.9736,  -9.1438,  ...,  -9.8062, -10.0692, -10.1250],
         [ -8.9277,  -8.9646,  -9.1388,  ...,  -9.7863, -10.0306, -10.0824],
         [ -8.8813,  -8.9223,  -9.1157,  ...,  -9.6920,  -9.8483,  -9.8815],
         ...,
         [ -5.6050,  -5.6716,  -5.9855,  ...,  -4.1763,  -4.2638,  -4.2824],
         [ -5.4224,  -5.4665,  -5.6743,  ...,  -4.2480,  -4.2824,  -4.2897],
         [ -5.1825,  -5.1972,  -5.2664,  ...,  -4.3584,  -4.3100,  -4.2997]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that the persons use to cross the water in this image? Please output segmentation mask. ASSISTANT: something that the persons use to cross the water</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]

 17%|████████████████████████▍                                                                                                                       | 34/200 [00:13<00:47,  3.47it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[-8.9375, -8.9375, -8.9445,  ..., -8.7988, -8.8125, -8.8125],
         [-8.9375, -8.9375, -8.9445,  ..., -8.7988, -8.8125, -8.8125],
         [-8.9367, -8.9367, -8.9436,  ..., -8.7888, -8.8023, -8.8023],
         ...,
         [-8.9859, -8.9859, -9.0029,  ..., -4.7927, -4.7949, -4.7949],
         [-9.1350, -9.1350, -9.1512,  ..., -5.2240, -5.2244, -5.2244],
         [-9.2500, -9.2500, -9.2657,  ..., -5.5596, -5.5586, -5.5586]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Driving at night can be very dangerous due to poor visibility, which can lead to accidents. What part of the car needs to be turned on when driving at night? Please output segmentation mask. ASSISTANT: driving at night can be very dangerous due to poor visibility, which can lead to accidents. what part of the car needs to be turned on when driving at night</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 864, 1152])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 864, 1152])):  [tensor([[[-5.9375, -5.9375, -5.9826,  ..., -7.2817, -7.3438, -7.3438],
         [-5.9375, -5.9375, -5.9826,  ..., -7.2817, -7.3438, -7.3438],
         [-5.9544, -5.9544, -5.9978,  ..., -7.2850, -7.3409, -7.3409],
         ...,
         [-6.8181, -6.8181, -6.9397,  ..., -4.2003, -4.2214, -4.2214],
         [-6.6667, -6.6667, -6.7823,  ..., -4.2165, -4.2344, -4.2344],
         [-6.4453, -6.4453, -6.5469,  ..., -4.2064, -4.2148, -4.2148]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n This rocky terrain can be challenging to navigate. What object in the picture could provide information to guide travelers through this area? Please output segmentation mask. ASSISTANT: this rocky terrain can be challenging to navigate. what object in the picture could provide information to guide travelers through this area</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2592, 3888])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2592, 3888])):  [tensor([[[ -9.2500,  -9.2500,  -9.2500,  ..., -10.5000, -10.5000, -10.5000],
         [ -9.2500,  -9.2500,  -9.2500,  ..., -10.5000, -10.5000, -10.5000],
         [ -9.2500,  -9.2500,  -9.2500,  ..., -10.5000, -10.5000, -10.5000],
         ...,
         [ -6.7509,  -6.7509,  -6.7509,  ...,  -5.4098,  -5.4098,  -5.4098],
         [ -6.7812,  -6.7812,  -6.7812,  ...,  -5.4023,  -5.4023,  -5.4023],
         [ -6.7812,  -6.7812,  -6.7812,  ...,  -5.4023,  -5.4023,  -5.4023]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is The fastest sinking ship in this image? Please output segmentation mask. ASSISTANT: the fastest sinking ship</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 333, 500])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 333, 500])):  [tensor([[[-8.8750, -8.9420, -9.0700,  ..., -7.9606, -8.2326, -8.3750],
         [-8.7745, -8.8550, -9.0087,  ..., -8.3082, -8.4730, -8.5593],
         [-8.5825, -8.6887, -8.8916,  ..., -8.9722, -8.9322, -8.9113],
         ...,
         [-4.9962, -5.1225, -5.3638,  ..., -3.8971, -3.9687, -4.0063],
         [-5.0385, -5.0574, -5.0935,  ..., -3.7675, -3.7719, -3.7741],
         [-5.0545, -4.8933, -4.5853,  ..., -3.6654, -3.4322, -3.3101]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In some regions, people raise certain animals for their milk, meat, and skin. What animal in the picture could be domesticated for such purposes? Please output segmentation mask. ASSISTANT: in some regions, people raise certain animals for their milk, meat, and skin. what animal in the picture could be domesticated for such purposes</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1365, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1365, 2048])):  [tensor([[[-8.1875, -8.1875, -8.1875,  ..., -8.6250, -8.6250, -8.6250],
         [-8.1875, -8.1875, -8.1875,  ..., -8.6250, -8.6250, -8.6250],
         [-8.1875, -8.1875, -8.1875,  ..., -8.6250, -8.6250, -8.6250],
         ...,
         [-6.5084, -6.5084, -6.5084,  ..., -3.6473, -3.6473, -3.6473],
         [-6.7527, -6.7527, -6.7527,  ..., -3.7197, -3.7197, -3.7197],
         [-6.8750, -6.8750, -6.8750,  ..., -3.7559, -3.7559, -3.7559]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the place where the child is about to slip/fall off in this image? Please output segmentation mask. ASSISTANT: the place where the child is about to slip/fall off</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2152, 1704])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2152, 1704])):  [tensor([[[-5.9062, -5.9062, -5.9062,  ..., -5.2005, -5.3344, -5.3945],
         [-5.9062, -5.9062, -5.9062,  ..., -5.2005, -5.3344, -5.3945],
         [-5.9062, -5.9062, -5.9062,  ..., -5.2005, -5.3344, -5.3945],
         ...,
         [-4.0625, -4.0625, -4.0625,  ..., -1.8725, -1.9645, -2.0059],
         [-4.0625, -4.0625, -4.0625,  ..., -1.8725, -1.9645, -2.0059],
         [-4.0625, -4.0625, -4.0625,  ..., -1.8725, -1.9645, -2.0059]]],

 20%|████████████████████████████                                                                                                                    | 39/200 [00:15<00:56,  2.86it/s]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In order to facilitate transportation and connect different regions, what structure in the picture was built across the water? Please output segmentation mask. ASSISTANT: in order to facilitate transportation and connect different regions, what structure in the picture was built across the water</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[74]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3264, 4928])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3264, 4928])):  [tensor([[[-10.1250, -10.1250, -10.1250,  ..., -10.9375, -10.9375, -10.9375],
         [-10.1250, -10.1250, -10.1250,  ..., -10.9375, -10.9375, -10.9375],
         [-10.1250, -10.1250, -10.1250,  ..., -10.9375, -10.9375, -10.9375],
         ...,
         [ -4.0504,  -4.0504,  -4.0504,  ...,  -2.6895,  -2.6895,  -2.6895],
         [ -4.0566,  -4.0566,  -4.0566,  ...,  -2.6895,  -2.6895,  -2.6895],
         [ -4.0566,  -4.0566,  -4.0566,  ...,  -2.6895,  -2.6895,  -2.6895]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the cesspit in this image? Please output segmentation mask. ASSISTANT: the cesspit</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1712, 2288])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1712, 2288])):  [tensor([[[-9.3125, -9.3125, -9.3125,  ..., -7.0000, -7.0000, -7.0000],
         [-9.3125, -9.3125, -9.3125,  ..., -7.0000, -7.0000, -7.0000],
         [-9.3125, -9.3125, -9.3125,  ..., -7.0000, -7.0000, -7.0000],
         ...,
         [-3.0109, -3.0109, -3.0109,  ..., -2.5682, -2.5682, -2.5682],
         [-3.2128, -3.2128, -3.2128,  ..., -2.5909, -2.5909, -2.5909],
         [-3.2900, -3.2900, -3.2900,  ..., -2.5996, -2.5996, -2.5996]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the goat nearest to the bottom stone in this image? Please output segmentation mask. ASSISTANT: the goat nearest to the bottom stone</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2344, 3190])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2344, 3190])):  [tensor([[[-12.1250, -12.1250, -12.1250,  ..., -14.2500, -14.2500, -14.2500],
         [-12.1250, -12.1250, -12.1250,  ..., -14.2500, -14.2500, -14.2500],
         [-12.1250, -12.1250, -12.1250,  ..., -14.2500, -14.2500, -14.2500],
         ...,
         [ -8.8539,  -8.8539,  -8.8539,  ...,  -6.9971,  -6.9971,  -6.9971],
         [ -8.7383,  -8.7383,  -8.7383,  ...,  -6.9570,  -6.9570,  -6.9570],
         [ -8.7383,  -8.7383,  -8.7383,  ...,  -6.9570,  -6.9570,  -6.9570]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the objects that can protect the snail and prevent it from getting injured in this image? Please output segmentation mask. ASSISTANT: the objects that can protect the snail and prevent it from getting injured</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[71]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1350, 1800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1350, 1800])):  [tensor([[[-5.3438, -5.3438, -5.3438,  ..., -8.4375, -8.4375, -8.4375],
         [-5.3438, -5.3438, -5.3438,  ..., -8.4375, -8.4375, -8.4375],
         [-5.3438, -5.3438, -5.3438,  ..., -8.4375, -8.4375, -8.4375],
         ...,
         [-4.8580, -4.8580, -4.8580,  ..., -1.5610, -1.5610, -1.5610],
         [-4.9335, -4.9335, -4.9335,  ..., -1.8209, -1.8209, -1.8209],
         [-4.9805, -4.9805, -4.9805,  ..., -1.9824, -1.9824, -1.9824]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If a person wants to watch TV or a movie, which furniture is the most suitable for them to sit and watch? Please output segmentation mask. ASSISTANT: if a person wants to watch tv or a movie, which furniture is the most suitable for them to sit and watch</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 1., 1., 0.],
         [0., 0., 0.,  ..., 1., 1., 0.],
         [0., 0., 0.,  ..., 1., 1., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[ -8.8143,  -9.0398,  -8.5611,  ...,  -7.4447,  -7.7105,  -8.1767],
         [ -8.6961,  -9.5125,  -9.3453,  ...,  -7.8422,  -7.9425,  -8.0542],
         [ -8.1402,  -8.9781,  -9.1250,  ...,  -7.4141,  -7.4469,  -7.5758],
         ...,
         [-10.8938, -10.2063,  -9.4219,  ...,  -5.2891,  -5.3234,  -5.3127],
         [-11.6238, -10.5281,  -9.4844,  ...,  -5.4281,  -5.5025,  -5.9021],
         [-11.5224, -11.3784, -10.3492,  ...,  -5.5068,  -5.5618,  -6.1583]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In historical buildings, there are often signs or symbols displayed on the walls or floors to represent a specific meaning or identity. What in the picture could be used to display such signs or symbols? Please output segmentation mask. ASSISTANT: in historical buildings, there are often signs or symbols displayed on the walls or floors to represent a specific meaning or identity. what in the picture could be used to display such signs or symbols</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[90]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-6.8438, -6.8438, -6.8438,  ..., -6.6094, -6.6562, -6.6562],
         [-6.8438, -6.8438, -6.8438,  ..., -6.6094, -6.6562, -6.6562],
         [-6.8164, -6.8164, -6.8223,  ..., -6.5776, -6.6094, -6.6094],
         ...,
         [-5.8516, -5.8516, -6.0127,  ..., -5.5825, -5.6094, -5.6094],
         [-5.7266, -5.7266, -5.8599,  ..., -5.2000, -5.2227, -5.2227],
         [-5.5547, -5.5547, -5.6499,  ..., -4.4827, -4.4805, -4.4805]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Insects can help pollinate flowers effectively, but some insects also collect pollen and nectar to produce honey. What in the picture does not make honey? Please output segmentation mask. ASSISTANT: insects can help pollinate flowers effectively, but some insects also collect pollen and nectar to produce honey. what in the picture does not make honey</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[89]]

 23%|█████████████████████████████████                                                                                                               | 46/200 [00:17<00:38,  3.98it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 651, 500])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 651, 500])):  [tensor([[[ -8.1875,  -8.1540,  -8.0367,  ...,  -8.9045,  -7.8277,  -6.5259],
         [ -8.2076,  -8.1900,  -8.1281,  ...,  -8.9726,  -7.8726,  -6.5431],
         [ -8.2781,  -8.3158,  -8.4478,  ...,  -9.2107,  -8.0297,  -6.6033],
         ...,
         [ -7.8644,  -8.3795, -10.1814,  ...,  -5.9705,  -5.4938,  -4.9010],
         [ -7.6296,  -8.1195,  -9.8333,  ...,  -6.0248,  -5.4989,  -4.8323],
         [ -7.5625,  -8.0451,  -9.7338,  ...,  -6.0403,  -5.5004,  -4.8126]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Flowers are often used to decorate tables during special occasions or events. What item in the picture can be used to hold the flowers and keep them fresh by providing water to them? Please output segmentation mask. ASSISTANT: flowers are often used to decorate tables during special occasions or events. what item in the picture can be used to hold the flowers and keep them fresh by providing water to them</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1370, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1370, 2048])):  [tensor([[[-4.3750, -4.3750, -4.3750,  ..., -8.8750, -8.8750, -8.8750],
         [-4.3750, -4.3750, -4.3750,  ..., -8.8750, -8.8750, -8.8750],
         [-4.3750, -4.3750, -4.3750,  ..., -8.8750, -8.8750, -8.8750],
         ...,
         [-4.4922, -4.4922, -4.4922,  ..., -4.5996, -4.5996, -4.5996],
         [-4.6328, -4.6328, -4.6328,  ..., -4.6191, -4.6191, -4.6191],
         [-4.7031, -4.7031, -4.7031,  ..., -4.6289, -4.6289, -4.6289]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Bamboo is very hard, and its sharp edges can easily scratch people. What tool in the picture can I use to split the bamboo and cross this bamboo forest? Please output segmentation mask. ASSISTANT: bamboo is very hard, and its sharp edges can easily scratch people. what tool in the picture can i use to split the bamboo and cross this bamboo forest</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[92]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[-5.1562, -5.1562, -5.1613,  ..., -2.1891, -2.1875, -2.1875],
         [-5.1562, -5.1562, -5.1613,  ..., -2.1891, -2.1875, -2.1875],
         [-5.1574, -5.1574, -5.1624,  ..., -2.1782, -2.1766, -2.1766],
         ...,
         [-3.6912, -3.6912, -3.6921,  ..., -4.4469, -4.4445, -4.4445],
         [-4.0387, -4.0387, -4.0404,  ..., -4.4899, -4.4875, -4.4875],
         [-4.3047, -4.3047, -4.3070,  ..., -4.5259, -4.5234, -4.5234]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the damaged part on the pants in this image? Please output segmentation mask. ASSISTANT: the damaged part on the pants</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[-5.4062, -5.4062, -5.4062,  ..., -3.2188, -3.2188, -3.2188],
         [-5.4062, -5.4062, -5.4062,  ..., -3.2188, -3.2188, -3.2188],
         [-5.4062, -5.4062, -5.4062,  ..., -3.2188, -3.2188, -3.2188],
         ...,
         [-5.3737, -5.3737, -5.3737,  ..., -2.2552, -2.2552, -2.2552],
         [-5.2930, -5.2930, -5.2930,  ..., -2.3281, -2.3281, -2.3281],
         [-5.2930, -5.2930, -5.2930,  ..., -2.3281, -2.3281, -2.3281]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Seafood dishes often include a tangy condiment that enhances the flavor. What item in the picture can be squeezed onto the seafood as a tangy flavor enhancer? Please output segmentation mask. ASSISTANT: seafood dishes often include a tangy condiment that enhances the flavor. what item in the picture can be squeezed onto the seafood as a tangy flavor enhancer</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[96]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1080, 1080])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1080, 1080])):  [tensor([[[-3.6562, -3.6562, -3.6427,  ..., -4.9280, -4.9688, -4.9688],
         [-3.6562, -3.6562, -3.6427,  ..., -4.9280, -4.9688, -4.9688],
         [-3.6410, -3.6410, -3.6270,  ..., -4.9135, -4.9518, -4.9518],
         ...,
         [-4.5170, -4.5170, -4.6707,  ..., -4.7729, -4.7948, -4.7948],
         [-4.5000, -4.5000, -4.6530,  ..., -4.7575, -4.7812, -4.7812],
         [-4.5000, -4.5000, -4.6530,  ..., -4.7575, -4.7812, -4.7812]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Dogs often like to find a comfortable place to rest. What object in the picture can offer a soft and comfortable surface for the dog to lie on? Please output segmentation mask. ASSISTANT: dogs often like to find a comfortable place to rest. what object in the picture can offer a soft and comfortable surface for the dog to lie on</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[82]]

 26%|████████████████████████████████████▋                                                                                                           | 51/200 [00:19<00:47,  3.12it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 299, 400])):  [tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 0., 0., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 299, 400])):  [tensor([[[-5.2500, -5.3075, -5.3277,  ..., -3.6807, -3.8516, -4.0312],
         [-5.1926, -5.2831, -5.3525,  ..., -3.6648, -3.6998, -3.7441],
         [-5.1056, -5.2566, -5.4005,  ..., -3.6096, -3.5135, -3.4249],
         ...,
         [-5.2565, -5.7737, -6.2460,  ..., -3.2676, -3.1895, -3.1719],
         [-4.3570, -4.6904, -4.9461,  ..., -2.7372, -2.6736, -2.6422],
         [-4.3986, -4.6470, -4.8042,  ..., -2.7916, -2.6378, -2.5091]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the area that people can walk on in this image? Please output segmentation mask. ASSISTANT: the area that people can walk on</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 300, 400])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 300, 400])):  [tensor([[[-2.3594, -2.3522, -2.3404,  ..., -2.1043, -2.1975, -2.3125],
         [-2.2659, -2.1860, -2.1159,  ..., -2.0092, -2.0141, -2.0250],
         [-2.1333, -1.9878, -1.8646,  ..., -1.8935, -1.8209, -1.7357],
         ...,
         [-1.5087, -1.4588, -1.3838,  ..., -2.8558, -2.8440, -2.8117],
         [-1.6856, -1.6004, -1.5091,  ..., -2.8254, -2.7724, -2.6953],
         [-2.1953, -2.0919, -1.9889,  ..., -2.8661, -2.7459, -2.6197]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When admiring a beautiful sunset, what part of the picture would we most likely focus on? Please output segmentation mask. ASSISTANT: when admiring a beautiful sunset, what part of the picture would we most likely focus on</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[71]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1542, 3494])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1542, 3494])):  [tensor([[[-5.5938, -5.5938, -5.5938,  ..., -5.6562, -5.6562, -5.6562],
         [-5.5938, -5.5938, -5.5938,  ..., -5.6562, -5.6562, -5.6562],
         [-5.5938, -5.5938, -5.5938,  ..., -5.6562, -5.6562, -5.6562],
         ...,
         [-2.3102, -2.3102, -2.3102,  ..., -1.2805, -1.2805, -1.2805],
         [-2.3916, -2.3916, -2.3916,  ..., -1.3657, -1.3657, -1.3657],
         [-2.3916, -2.3916, -2.3916,  ..., -1.3657, -1.3657, -1.3657]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n As a speaker, relying solely on verbal explanations may not effectively convey the intended message to the audience. What area of the picture should be used to project the key content and make it more understandable for the audience during the presentation? Please output segmentation mask. ASSISTANT: as a speaker, relying solely on verbal explanations may not effectively convey the intended message to the audience. what area of the picture should be used to project the key content and make it more understandable for the audience during the presentation</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[101]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2848, 4288])):  [tensor([[[0., 0., 0.,  ..., 1., 1., 1.],
         [0., 0., 0.,  ..., 1., 1., 1.],
         [0., 0., 0.,  ..., 1., 1., 1.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2848, 4288])):  [tensor([[[-7.3125, -7.3125, -7.3125,  ..., -1.7344, -1.7344, -1.7344],
         [-7.3125, -7.3125, -7.3125,  ..., -1.7344, -1.7344, -1.7344],
         [-7.3125, -7.3125, -7.3125,  ..., -1.7344, -1.7344, -1.7344],
         ...,
         [-4.0574, -4.0574, -4.0574,  ..., -0.9717, -0.9717, -0.9717],
         [-4.0938, -4.0938, -4.0938,  ..., -0.9902, -0.9902, -0.9902],
         [-4.0938, -4.0938, -4.0938,  ..., -0.9902, -0.9902, -0.9902]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a modern office, what object in the picture is commonly used for inputting data and controlling the computer? Please output segmentation mask. ASSISTANT: in a modern office, what object in the picture is commonly used for inputting data and controlling the computer</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[74]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1024, 1280])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1024, 1280])):  [tensor([[[-11.1250, -11.1250, -11.1758,  ...,  -7.4609,  -7.5000,  -7.5000],
         [-11.1250, -11.1250, -11.1758,  ...,  -7.4609,  -7.5000,  -7.5000],
         [-11.1250, -11.1250, -11.1775,  ...,  -7.4135,  -7.4493,  -7.4493],
         ...,
         [ -7.6319,  -7.6319,  -7.7120,  ...,  -7.0797,  -7.0616,  -7.0616],
         [ -6.5123,  -6.5123,  -6.5664,  ...,  -5.9891,  -5.9658,  -5.9658],
         [ -6.0586,  -6.0586,  -6.0955,  ...,  -5.4854,  -5.4492,  -5.4492]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a zoo, there are separate areas designated for different animals. What structure in the picture is used to confine and display animals? Please output segmentation mask. ASSISTANT: in a zoo, there are separate areas designated for different animals. what structure in the picture is used to confine and display animals</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[0., 0., 1.,  ..., 0., 0., 0.],
         [0., 0., 1.,  ..., 0., 0., 0.],
         [0., 0., 1.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[-1.3906, -1.3906, -1.3906,  ..., -4.7812, -4.7812, -4.7812],
         [-1.3906, -1.3906, -1.3906,  ..., -4.7812, -4.7812, -4.7812],
         [-1.3906, -1.3906, -1.3906,  ..., -4.7812, -4.7812, -4.7812],
         ...,
         [-3.9503, -3.9503, -3.9503,  ..., -3.3498, -3.3498, -3.3498],
         [-3.9858, -3.9858, -3.9858,  ..., -3.4193, -3.4193, -3.4193],
         [-3.9941, -3.9941, -3.9941,  ..., -3.4355, -3.4355, -3.4355]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that tastes spicy in this image? Please output segmentation mask. ASSISTANT: something that tastes spicy</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]

 28%|█████████████████████████████████████████                                                                                                       | 57/200 [00:21<00:44,  3.18it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-6.0625, -6.0625, -6.0039,  ..., -5.1992, -5.1875, -5.1875],
         [-6.0625, -6.0625, -6.0039,  ..., -5.1992, -5.1875, -5.1875],
         [-6.0312, -6.0312, -5.9727,  ..., -5.0938, -5.0703, -5.0703],
         ...,
         [-7.0781, -7.0781, -7.2627,  ..., -5.5142, -5.5195, -5.5195],
         [-6.9492, -6.9492, -7.1240,  ..., -5.4492, -5.4492, -5.4492],
         [-6.8477, -6.8477, -6.9814,  ..., -5.3164, -5.2852, -5.2852]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When going fishing on a calm sea, what type of boat shown in the picture would be an ideal choice for a peaceful fishing experience? Please output segmentation mask. ASSISTANT: when going fishing on a calm sea, what type of boat shown in the picture would be an ideal choice for a peaceful fishing experience</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[ -8.8750,  -9.3504,  -9.0297,  ...,  -9.6621,  -9.9899, -10.6704],
         [ -8.4051,  -9.0281,  -8.9438,  ...,  -9.9844, -10.1231, -10.4196],
         [ -8.5125,  -9.2000,  -9.0781,  ...,  -9.9062,  -9.7781, -10.0574],
         ...,
         [ -5.1561,  -5.1453,  -5.0781,  ...,  -3.2227,  -2.9742,  -2.8614],
         [ -5.2462,  -5.5513,  -5.3359,  ...,  -3.3172,  -3.0263,  -3.0757],
         [ -5.6945,  -5.9061,  -5.8365,  ...,  -3.6690,  -3.3518,  -3.3773]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Insects have various ways to protect themselves from predators. What characteristics can a moth use to deter potential threats? Please output segmentation mask. ASSISTANT: insects have various ways to protect themselves from predators. what characteristics can a moth use to deter potential threats</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[77]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3872, 2592])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3872, 2592])):  [tensor([[[-10.0000, -10.0000, -10.0000,  ...,  -8.7109,  -8.7812,  -8.7812],
         [-10.0000, -10.0000, -10.0000,  ...,  -8.7109,  -8.7812,  -8.7812],
         [-10.0000, -10.0000, -10.0000,  ...,  -8.7109,  -8.7812,  -8.7812],
         ...,
         [ -9.1250,  -9.1250,  -9.1250,  ...,  -7.4309,  -7.4648,  -7.4648],
         [ -9.1250,  -9.1250,  -9.1250,  ...,  -7.4309,  -7.4648,  -7.4648],
         [ -9.1250,  -9.1250,  -9.1250,  ...,  -7.4309,  -7.4648,  -7.4648]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Fishing is a popular activity for relaxation and leisure. What tool is the man in the picture using to catch fish? Please output segmentation mask. ASSISTANT: fishing is a popular activity for relaxation and leisure. what tool is the man in the picture using to catch fish</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[-13.2500, -13.2500, -13.2602,  ..., -13.6820, -13.6875, -13.6875],
         [-13.2500, -13.2500, -13.2602,  ..., -13.6820, -13.6875, -13.6875],
         [-13.2555, -13.2555, -13.2658,  ..., -13.7071, -13.7125, -13.7125],
         ...,
         [ -5.3195,  -5.3195,  -5.3178,  ...,  -7.3640,  -7.3734,  -7.3734],
         [ -6.1243,  -6.1243,  -6.1237,  ...,  -7.5034,  -7.5100,  -7.5100],
         [ -6.7461,  -6.7461,  -6.7464,  ...,  -7.6206,  -7.6250,  -7.6250]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In some cultures, people use sculptures and figurines to represent various aspects of life, including celebrations, rituals, and art. What object in the picture can be considered a representation of a human figure? Please output segmentation mask. ASSISTANT: in some cultures, people use sculptures and figurines to represent various aspects of life, including celebrations, rituals, and art. what object in the picture can be considered a representation of a human figure</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[95]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2868, 4309])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2868, 4309])):  [tensor([[[-6.0938, -6.0938, -6.0938,  ..., -9.1875, -9.1875, -9.1875],
         [-6.0938, -6.0938, -6.0938,  ..., -9.1875, -9.1875, -9.1875],
         [-6.0938, -6.0938, -6.0938,  ..., -9.1875, -9.1875, -9.1875],
         ...,
         [-5.4122, -5.4122, -5.4122,  ..., -4.3910, -4.3910, -4.3910],
         [-5.4062, -5.4062, -5.4062,  ..., -4.3438, -4.3438, -4.3438],
         [-5.4062, -5.4062, -5.4062,  ..., -4.3438, -4.3438, -4.3438]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the shadow of the red car in this image? Please output segmentation mask. ASSISTANT: the shadow of the red car</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2736, 3648])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2736, 3648])):  [tensor([[[-9.0625, -9.0625, -9.0625,  ..., -6.0000, -6.0000, -6.0000],
         [-9.0625, -9.0625, -9.0625,  ..., -6.0000, -6.0000, -6.0000],
         [-9.0625, -9.0625, -9.0625,  ..., -6.0000, -6.0000, -6.0000],
         ...,
         [-6.7788, -6.7788, -6.7788,  ..., -4.3963, -4.3963, -4.3963],
         [-6.7930, -6.7930, -6.7930,  ..., -4.5137, -4.5137, -4.5137],
         [-6.7930, -6.7930, -6.7930,  ..., -4.5137, -4.5137, -4.5137]]],

 31%|████████████████████████████████████████████▋                                                                                                   | 62/200 [00:23<00:42,  3.28it/s]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the part of this animal's body that comes into contact with the air in this image? Please output segmentation mask. ASSISTANT: the part of this animal's body that comes into contact with the air</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[72]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1000, 1504])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1000, 1504])):  [tensor([[[-12.4375, -12.4375, -12.4486,  ..., -13.4359, -13.4375, -13.4375],
         [-12.4375, -12.4375, -12.4486,  ..., -13.4359, -13.4375, -13.4375],
         [-12.4359, -12.4359, -12.4475,  ..., -13.4943, -13.4945, -13.4945],
         ...,
         [ -7.3168,  -7.3168,  -7.3187,  ...,  -4.9042,  -4.9046,  -4.9046],
         [ -7.4338,  -7.4338,  -7.4281,  ...,  -4.9426,  -4.9365,  -4.9365],
         [ -7.5234,  -7.5234,  -7.5119,  ...,  -4.9720,  -4.9609,  -4.9609]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Many people use bags to carry their belongings when they go out. What part of the bag in the picture can be used to carry the bag comfortably over the shoulder? Please output segmentation mask. ASSISTANT: many people use bags to carry their belongings when they go out. what part of the bag in the picture can be used to carry the bag comfortably over the shoulder</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1920, 2560])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1920, 2560])):  [tensor([[[-6.4688, -6.4688, -6.4688,  ..., -6.9688, -6.9688, -6.9688],
         [-6.4688, -6.4688, -6.4688,  ..., -6.9688, -6.9688, -6.9688],
         [-6.4688, -6.4688, -6.4688,  ..., -6.9688, -6.9688, -6.9688],
         ...,
         [-6.3828, -6.3828, -6.3828,  ..., -2.8906, -2.8906, -2.8906],
         [-6.3234, -6.3234, -6.3234,  ..., -2.9906, -2.9906, -2.9906],
         [-6.3086, -6.3086, -6.3086,  ..., -3.0156, -3.0156, -3.0156]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n During a meal, people typically use utensils to bring food to their mouths. What tool in the picture can be used to fulfill this need? Please output segmentation mask. ASSISTANT: during a meal, people typically use utensils to bring food to their mouths. what tool in the picture can be used to fulfill this need</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-8.5625, -8.5887, -8.7125,  ..., -9.4437, -9.6448, -9.6875],
         [-8.5559, -8.5829, -8.7099,  ..., -9.4306, -9.6236, -9.6645],
         [-8.5250, -8.5552, -8.6975,  ..., -9.3687, -9.5234, -9.5562],
         ...,
         [-8.2250, -8.2932, -8.6150,  ..., -6.5216, -6.6043, -6.6219],
         [-8.1517, -8.2199, -8.5419,  ..., -6.4884, -6.5292, -6.5378],
         [-7.4506, -7.5078, -7.7771,  ..., -6.0244, -6.0368, -6.0394]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n During the move, what object can be used to store and transport various sundries and small household items, which is sturdy and relatively easy to carry? Please output segmentation mask. ASSISTANT: during the move, what object can be used to store and transport various sundries and small household items, which is sturdy and relatively easy to carry</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-7.0637e+00, -7.2796e+00, -7.1104e+00,  ..., -7.1311e+00,
          -7.1839e+00, -7.5876e+00],
         [-6.9144e+00, -7.1894e+00, -7.1641e+00,  ..., -7.4641e+00,
          -7.5041e+00, -7.6566e+00],
         [-6.8809e+00, -7.2031e+00, -7.1172e+00,  ..., -7.3828e+00,
          -7.3250e+00, -7.4969e+00],
         ...,
         [ 4.7911e-02, -7.5409e-03,  1.2549e-01,  ..., -2.6758e+00,
          -2.5281e+00, -2.4314e+00],
         [-1.0379e-01, -2.4787e-01, -1.8208e-01,  ..., -2.6859e+00,
          -2.5017e+00, -2.4663e+00],
         [-1.0283e+00, -1.0177e+00, -1.0678e+00,  ..., -2.7559e+00,
          -2.4793e+00, -2.4789e+00]]], device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If we were to take a flight in the sky, what part of the plane in the picture would be spinning to generate lift and propel the aircraft forward? Please output segmentation mask. ASSISTANT: if we were to take a flight in the sky, what part of the plane in the picture would be spinning to generate lift and propel the aircraft forward</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2499, 3680])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2499, 3680])):  [tensor([[[-8.4375, -8.4375, -8.4375,  ..., -8.3750, -8.3750, -8.3750],
         [-8.4375, -8.4375, -8.4375,  ..., -8.3750, -8.3750, -8.3750],
         [-8.4375, -8.4375, -8.4375,  ..., -8.3750, -8.3750, -8.3750],
         ...,
         [-4.9117, -4.9117, -4.9117,  ..., -3.4443, -3.4443, -3.4443],
         [-5.0078, -5.0078, -5.0078,  ..., -3.4824, -3.4824, -3.4824],
         [-5.0078, -5.0078, -5.0078,  ..., -3.4824, -3.4824, -3.4824]]],

 34%|████████████████████████████████████████████████▏                                                                                               | 67/200 [00:25<00:47,  2.81it/s]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something used by a Jedi warrior to attack enemies in this image? Please output segmentation mask. ASSISTANT: something used by a jedi warrior to attack enemies</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 720, 502])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 720, 502])):  [tensor([[[-5.6875, -5.7073, -5.7847,  ..., -5.5302, -5.2604, -4.9827],
         [-5.7197, -5.7422, -5.8304,  ..., -5.5712, -5.3070, -5.0344],
         [-5.8455, -5.8788, -6.0089,  ..., -5.7318, -5.4889, -5.2370],
         ...,
         [-3.5660, -3.5932, -3.6997,  ..., -2.8303, -3.0158, -3.2038],
         [-3.5757, -3.5960, -3.6755,  ..., -2.7545, -2.9451, -3.1383],
         [-3.5781, -3.5967, -3.6693,  ..., -2.7351, -2.9271, -3.1215]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What object in the picture should be used with water to create foam and effectively remove oil and dirt when your hands are very dirty? Please output segmentation mask. ASSISTANT: what object in the picture should be used with water to create foam and effectively remove oil and dirt when your hands are very dirty</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[79]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[-9.8125, -9.8125, -9.8125,  ..., -6.7188, -6.7188, -6.7188],
         [-9.8125, -9.8125, -9.8125,  ..., -6.7188, -6.7188, -6.7188],
         [-9.8125, -9.8125, -9.8125,  ..., -6.7188, -6.7188, -6.7188],
         ...,
         [-4.9866, -4.9866, -4.9866,  ..., -2.7710, -2.7710, -2.7710],
         [-5.1732, -5.1732, -5.1732,  ..., -3.0157, -3.0157, -3.0157],
         [-5.2461, -5.2461, -5.2461,  ..., -3.1113, -3.1113, -3.1113]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a concert or public speaking event, what object in the picture would be used by the speaker or performer to amplify their voice? Please output segmentation mask. ASSISTANT: in a concert or public speaking event, what object in the picture would be used by the speaker or performer to amplify their voice</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[79]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 482, 720])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 482, 720])):  [tensor([[[ -9.1250,  -9.1695,  -9.3438,  ..., -10.4896, -10.8961, -11.0000],
         [ -9.0903,  -9.1360,  -9.3148,  ..., -10.4427, -10.8155, -10.9107],
         [ -8.9546,  -9.0049,  -9.2017,  ..., -10.2597, -10.5003, -10.5618],
         ...,
         [ -8.0408,  -8.2242,  -8.9417,  ...,  -3.5240,  -3.4800,  -3.4687],
         [ -7.9605,  -8.1154,  -8.7214,  ...,  -3.6136,  -3.5546,  -3.5395],
         [ -7.8827,  -8.0085,  -8.5009,  ...,  -3.7109,  -3.6364,  -3.6173]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In the living room, people often sit on the sofa to watch TV or chat. What object can they use to adjust the TV screen or change channels? Please output segmentation mask. ASSISTANT: in the living room, people often sit on the sofa to watch tv or chat. what object can they use to adjust the tv screen or change channels</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[83]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-7.3971, -7.1497, -6.8854,  ..., -5.5861, -5.6801, -6.1474],
         [-7.1497, -7.4784, -7.4547,  ..., -6.0266, -5.9594, -6.1313],
         [-6.8197, -7.2172, -7.5781,  ..., -6.0078, -5.7547, -5.7654],
         ...,
         [-6.1055, -5.8906, -5.0859,  ..., -2.6367, -2.9766, -2.9766],
         [-6.2629, -5.7688, -4.9047,  ..., -2.7633, -3.0077, -3.2365],
         [-6.6097, -6.6266, -5.7748,  ..., -3.1616, -3.3228, -3.4525]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that produces pollen in this image? Please output segmentation mask. ASSISTANT: something that produces pollen</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1023, 1392])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1023, 1392])):  [tensor([[[-2.6094, -2.6094, -2.6047,  ..., -5.3265, -5.3438, -5.3438],
         [-2.6094, -2.6094, -2.6047,  ..., -5.3265, -5.3438, -5.3438],
         [-2.5994, -2.5994, -2.5944,  ..., -5.3088, -5.3251, -5.3251],
         ...,
         [-5.4526, -5.4526, -5.4838,  ..., -4.2168, -4.2157, -4.2157],
         [-4.9465, -4.9465, -4.9672,  ..., -3.7032, -3.6982, -3.6982],
         [-4.5312, -4.5312, -4.5433,  ..., -3.2818, -3.2734, -3.2734]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the part that can transmit traction and braking torque, coming into contact with the ground in this image? Please output segmentation mask. ASSISTANT: the part that can transmit traction and braking torque, coming into contact with the ground</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]

 36%|███████████████████████████████████████████████████▊                                                                                            | 72/200 [00:27<00:46,  2.77it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 752, 1169])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 752, 1169])):  [tensor([[[-6.7812, -6.7812, -6.7839,  ..., -7.7656, -7.8438, -7.8438],
         [-6.7812, -6.7812, -6.7839,  ..., -7.7656, -7.8438, -7.8438],
         [-6.7866, -6.7866, -6.7905,  ..., -7.7363, -7.8087, -7.8087],
         ...,
         [-4.1169, -4.1169, -4.1613,  ..., -3.0598, -3.0558, -3.0558],
         [-3.5962, -3.5962, -3.6141,  ..., -2.6567, -2.6442, -2.6442],
         [-3.4785, -3.4785, -3.4863,  ..., -2.5535, -2.5312, -2.5312]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the area where people can stand without getting wet in this image? Please output segmentation mask. ASSISTANT: the area where people can stand without getting wet</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-9.7500, -9.7631, -9.8250,  ..., -9.4562, -9.5955, -9.6250],
         [-9.7598, -9.7752, -9.8476,  ..., -9.4986, -9.6272, -9.6545],
         [-9.8063, -9.8322, -9.9544,  ..., -9.6981, -9.7770, -9.7937],
         ...,
         [-5.1562, -5.2046, -5.4328,  ..., -2.5472, -2.5263, -2.5219],
         [-5.1472, -5.1814, -5.3426,  ..., -2.7375, -2.7542, -2.7578],
         [-5.1138, -5.1309, -5.2116,  ..., -3.2436, -3.2058, -3.1978]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a motorcycle race, there are often sharp turns that require skilled maneuvering. What part of the race track in the picture indicates a sharp turn? Please output segmentation mask. ASSISTANT: in a motorcycle race, there are often sharp turns that require skilled maneuvering. what part of the race track in the picture indicates a sharp turn</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 682, 1023])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [0., 0., 0.,  ..., 1., 1., 1.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 682, 1023])):  [tensor([[[ -7.8438,  -7.8437,  -7.7927,  ..., -11.5148, -11.6872, -11.6875],
         [ -7.8437,  -7.8436,  -7.7927,  ..., -11.5147, -11.6871, -11.6873],
         [ -7.8201,  -7.8201,  -7.7652,  ..., -11.4608, -11.6164, -11.6167],
         ...,
         [ -9.7594,  -9.7594,  -9.7969,  ...,  -5.4497,  -5.3936,  -5.3935],
         [ -9.3369,  -9.3368,  -9.3076,  ...,  -4.7135,  -4.6112,  -4.6111],
         [ -9.8122,  -9.8121,  -9.7572,  ...,  -4.8990,  -4.7540,  -4.7538]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the region showing current time in this image? Please output segmentation mask. ASSISTANT: the region showing current time</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-2.3125, -2.3125, -2.2910,  ..., -2.0859, -2.0938, -2.0938],
         [-2.3125, -2.3125, -2.2910,  ..., -2.0859, -2.0938, -2.0938],
         [-2.2930, -2.2930, -2.2739,  ..., -2.0697, -2.0752, -2.0752],
         ...,
         [-1.7920, -1.7920, -1.8129,  ..., -1.4146, -1.4023, -1.4023],
         [-1.8213, -1.8213, -1.8386,  ..., -1.4587, -1.4443, -1.4443],
         [-1.8857, -1.8857, -1.8889,  ..., -1.5364, -1.5205, -1.5205]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the reflection of the object in this image? Please output segmentation mask. ASSISTANT: the reflection of the object</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2820, 3760])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2820, 3760])):  [tensor([[[-7.3750, -7.3750, -7.3750,  ..., -8.0625, -8.0625, -8.0625],
         [-7.3750, -7.3750, -7.3750,  ..., -8.0625, -8.0625, -8.0625],
         [-7.3750, -7.3750, -7.3750,  ..., -8.0625, -8.0625, -8.0625],
         ...,
         [-5.9448, -5.9448, -5.9448,  ..., -3.3211, -3.3211, -3.3211],
         [-5.9844, -5.9844, -5.9844,  ..., -3.3438, -3.3438, -3.3438],
         [-5.9844, -5.9844, -5.9844,  ..., -3.3438, -3.3438, -3.3438]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Dogs use their mouths to perform various tasks, including eating and vocalizing. What part of the dog's body is primarily responsible for these actions? Please output segmentation mask. ASSISTANT: dogs use their mouths to perform various tasks, including eating and vocalizing. what part of the dog's body is primarily responsible for these actions</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3264, 4928])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3264, 4928])):  [tensor([[[-4.0938, -4.0938, -4.0938,  ..., -8.0000, -8.0000, -8.0000],
         [-4.0938, -4.0938, -4.0938,  ..., -8.0000, -8.0000, -8.0000],
         [-4.0938, -4.0938, -4.0938,  ..., -8.0000, -8.0000, -8.0000],
         ...,
         [-3.1930, -3.1930, -3.1930,  ..., -2.5171, -2.5171, -2.5171],
         [-3.1992, -3.1992, -3.1992,  ..., -2.5176, -2.5176, -2.5176],
         [-3.1992, -3.1992, -3.1992,  ..., -2.5176, -2.5176, -2.5176]]],

 39%|████████████████████████████████████████████████████████▏                                                                                       | 78/200 [00:29<00:43,  2.79it/s]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that protects the persons' heads in this image? Please output segmentation mask. ASSISTANT: something that protects the persons' heads</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1360, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1360, 2048])):  [tensor([[[-9.1250, -9.1250, -9.1250,  ..., -4.3750, -4.3750, -4.3750],
         [-9.1250, -9.1250, -9.1250,  ..., -4.3750, -4.3750, -4.3750],
         [-9.1250, -9.1250, -9.1250,  ..., -4.3750, -4.3750, -4.3750],
         ...,
         [-8.5176, -8.5176, -8.5176,  ..., -4.3057, -4.3057, -4.3057],
         [-9.0918, -9.0918, -9.0918,  ..., -4.5615, -4.5615, -4.5615],
         [-9.3789, -9.3789, -9.3789,  ..., -4.6895, -4.6895, -4.6895]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the transportation vehicle that does not require electricity or gasoline in this image? Please output segmentation mask. ASSISTANT: the transportation vehicle that does not require electricity or gasoline</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3456, 5184])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 5184])):  [tensor([[[-6.9688, -6.9688, -6.9688,  ..., -8.5625, -8.5625, -8.5625],
         [-6.9688, -6.9688, -6.9688,  ..., -8.5625, -8.5625, -8.5625],
         [-6.9688, -6.9688, -6.9688,  ..., -8.5625, -8.5625, -8.5625],
         ...,
         [-5.5820, -5.5820, -5.5820,  ..., -1.5791, -1.5791, -1.5791],
         [-5.5820, -5.5820, -5.5820,  ..., -1.5791, -1.5791, -1.5791],
         [-5.5820, -5.5820, -5.5820,  ..., -1.5791, -1.5791, -1.5791]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When constructing a house, what part of the building process involves assembling a sturdy base and framework? Please output segmentation mask. ASSISTANT: when constructing a house, what part of the building process involves assembling a sturdy base and framework</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[74]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[-10.8750, -10.8750, -10.8953,  ..., -11.9805, -12.0000, -12.0000],
         [-10.8750, -10.8750, -10.8953,  ..., -11.9805, -12.0000, -12.0000],
         [-10.8805, -10.8805, -10.9007,  ..., -11.9729, -11.9922, -11.9922],
         ...,
         [ -3.6840,  -3.6840,  -3.6853,  ...,  -2.4523,  -2.4635,  -2.4635],
         [ -4.3575,  -4.3575,  -4.3603,  ...,  -3.0170,  -3.0259,  -3.0259],
         [ -4.8750,  -4.8750,  -4.8791,  ...,  -3.4519,  -3.4590,  -3.4590]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n The office is a place where people focus on their work. What structure in the room can help block noise and reduces Interruptions from the outside environment? Please output segmentation mask. ASSISTANT: the office is a place where people focus on their work. what structure in the room can help block noise and reduces interruptions from the outside environment</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[82]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-8.4910, -8.2944, -7.9875,  ..., -6.8543, -6.7714, -7.1544],
         [-8.2084, -9.0162, -9.0188,  ..., -7.4344, -7.2613, -7.4374],
         [-8.1078, -8.8813, -8.9844,  ..., -7.5469, -7.2500, -7.4648],
         ...,
         [-7.4133, -6.9406, -5.9531,  ..., -5.4688, -5.3453, -5.3131],
         [-7.4963, -6.7788, -5.9047,  ..., -5.4813, -5.4122, -5.6120],
         [-7.5967, -7.5457, -6.6746,  ..., -5.8043, -5.6949, -5.8587]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the camera lens that is more suitable for photographing nearby objects in this image? Please output segmentation mask. ASSISTANT: the camera lens that is more suitable for photographing nearby objects</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1620, 2160])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1620, 2160])):  [tensor([[[-6.1875, -6.1875, -6.1875,  ..., -6.7812, -6.7812, -6.7812],
         [-6.1875, -6.1875, -6.1875,  ..., -6.7812, -6.7812, -6.7812],
         [-6.1875, -6.1875, -6.1875,  ..., -6.7812, -6.7812, -6.7812],
         ...,
         [-6.3391, -6.3391, -6.3391,  ..., -3.8137, -3.8137, -3.8137],
         [-6.1910, -6.1910, -6.1910,  ..., -3.7507, -3.7507, -3.7507],
         [-6.1250, -6.1250, -6.1250,  ..., -3.7227, -3.7227, -3.7227]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the part of the house that can be opened in this image? Please output segmentation mask. ASSISTANT: the part of the house that can be opened</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[-6.5938, -6.5938, -6.5938,  ..., -6.9062, -6.9062, -6.9062],
         [-6.5938, -6.5938, -6.5938,  ..., -6.9062, -6.9062, -6.9062],
         [-6.5938, -6.5938, -6.5938,  ..., -6.9062, -6.9062, -6.9062],
         ...,
         [-3.0120, -3.0120, -3.0120,  ..., -2.6460, -2.6460, -2.6460],
         [-3.3166, -3.3166, -3.3166,  ..., -2.8907, -2.8907, -2.8907],
         [-3.4355, -3.4355, -3.4355,  ..., -2.9863, -2.9863, -2.9863]]],

 42%|████████████████████████████████████████████████████████████▍                                                                                   | 84/200 [00:31<00:33,  3.50it/s]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the source of power for the ship in this image? Please output segmentation mask. ASSISTANT: the source of power for the ship</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[-12.5000, -12.5000, -12.5000,  ..., -13.3125, -13.3125, -13.3125],
         [-12.5000, -12.5000, -12.5000,  ..., -13.3125, -13.3125, -13.3125],
         [-12.5000, -12.5000, -12.5000,  ..., -13.3125, -13.3125, -13.3125],
         ...,
         [ -9.4688,  -9.4688,  -9.4688,  ...,  -3.7510,  -3.7510,  -3.7510],
         [ -9.9062,  -9.9062,  -9.9062,  ...,  -4.2412,  -4.2412,  -4.2412],
         [-10.1250, -10.1250, -10.1250,  ...,  -4.4863,  -4.4863,  -4.4863]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When hanging laundry outside to dry, what material in the picture would be suitable for holding the clothes securely in place? Please output segmentation mask. ASSISTANT: when hanging laundry outside to dry, what material in the picture would be suitable for holding the clothes securely in place</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2592, 3872])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2592, 3872])):  [tensor([[[-8.2500, -8.2500, -8.2500,  ..., -8.9375, -8.9375, -8.9375],
         [-8.2500, -8.2500, -8.2500,  ..., -8.9375, -8.9375, -8.9375],
         [-8.2500, -8.2500, -8.2500,  ..., -8.9375, -8.9375, -8.9375],
         ...,
         [-5.9061, -5.9061, -5.9061,  ..., -3.5681, -3.5681, -3.5681],
         [-5.9727, -5.9727, -5.9727,  ..., -3.5781, -3.5781, -3.5781],
         [-5.9727, -5.9727, -5.9727,  ..., -3.5781, -3.5781, -3.5781]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the friend of humans in this image? Please output segmentation mask. ASSISTANT: the friend of humans</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-10.0000, -10.0000, -10.0000,  ...,  -8.9141,  -9.0625,  -9.0625],
         [-10.0000, -10.0000, -10.0000,  ...,  -8.9141,  -9.0625,  -9.0625],
         [ -9.9922,  -9.9922,  -9.9873,  ...,  -8.8213,  -8.9453,  -8.9453],
         ...,
         [ -8.2031,  -8.2031,  -8.2627,  ...,  -6.3760,  -6.3672,  -6.3672],
         [ -8.5547,  -8.5547,  -8.6113,  ...,  -6.5327,  -6.5117,  -6.5117],
         [ -8.9141,  -8.9141,  -8.9902,  ...,  -7.0278,  -6.9727,  -6.9727]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Dogs have keen sense of smell, which is why they can be used as drug-sniffing dogs. Which part in the picture gives dogs this characteristic? Please output segmentation mask. ASSISTANT: dogs have keen sense of smell, which is why they can be used as drug-sniffing dogs. which part in the picture gives dogs this characteristic</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-6.7812, -6.7812, -6.8281,  ..., -7.8516, -7.9688, -7.9688],
         [-6.7812, -6.7812, -6.8281,  ..., -7.8516, -7.9688, -7.9688],
         [-6.7852, -6.7852, -6.8281,  ..., -7.7959, -7.9023, -7.9023],
         ...,
         [-8.0586, -8.0586, -8.2192,  ..., -3.8079, -3.7910, -3.7910],
         [-7.9258, -7.9258, -8.0781,  ..., -3.8435, -3.8262, -3.8262],
         [-7.5273, -7.5273, -7.6719,  ..., -3.9641, -3.9473, -3.9473]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the place where the patient lies down to receive examination in this image? Please output segmentation mask. ASSISTANT: the place where the patient lies down to receive examination</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2428, 3636])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2428, 3636])):  [tensor([[[-10.2500, -10.2500, -10.2500,  ..., -11.2500, -11.2500, -11.2500],
         [-10.2500, -10.2500, -10.2500,  ..., -11.2500, -11.2500, -11.2500],
         [-10.2500, -10.2500, -10.2500,  ..., -11.2500, -11.2500, -11.2500],
         ...,
         [ -6.8147,  -6.8147,  -6.8147,  ...,  -2.6186,  -2.6186,  -2.6186],
         [ -6.8594,  -6.8594,  -6.8594,  ...,  -2.6816,  -2.6816,  -2.6816],
         [ -6.8594,  -6.8594,  -6.8594,  ...,  -2.6816,  -2.6816,  -2.6816]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In an intense dragon boat race. What object in the picture should be struck to boost the morale of the competing team and cheer them on? Please output segmentation mask. ASSISTANT: in an intense dragon boat race. what object in the picture should be struck to boost the morale of the competing team and cheer them on</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[83]]

 44%|████████████████████████████████████████████████████████████████                                                                                | 89/200 [00:33<00:38,  2.91it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 986, 1544])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 986, 1544])):  [tensor([[[-13.6875, -13.6875, -13.7048,  ..., -15.4128, -15.4375, -15.4375],
         [-13.6875, -13.6875, -13.7048,  ..., -15.4128, -15.4375, -15.4375],
         [-13.6986, -13.6986, -13.7161,  ..., -15.4566, -15.4808, -15.4808],
         ...,
         [-10.8339, -10.8339, -10.8742,  ...,  -6.4255,  -6.4124,  -6.4124],
         [-10.8753, -10.8753, -10.9148,  ...,  -6.4977,  -6.4849,  -6.4849],
         [-10.9062, -10.9062, -10.9451,  ...,  -6.5516,  -6.5391,  -6.5391]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the equipment for sweeping away rain on rainy days in this image? Please output segmentation mask. ASSISTANT: the equipment for sweeping away rain on rainy days</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[-3.5469, -3.5469, -3.5469,  ..., -3.4062, -3.4062, -3.4062],
         [-3.5469, -3.5469, -3.5469,  ..., -3.4062, -3.4062, -3.4062],
         [-3.5469, -3.5469, -3.5469,  ..., -3.4062, -3.4062, -3.4062],
         ...,
         [-3.6250, -3.6250, -3.6250,  ..., -2.8528, -2.8528, -2.8528],
         [-3.6250, -3.6250, -3.6250,  ..., -2.8929, -2.8929, -2.8929],
         [-3.6250, -3.6250, -3.6250,  ..., -2.9023, -2.9023, -2.9023]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Soldiers are often equipped with various tools and weapons to carry out their duties. What item in the picture can be classified as a weapon? Please output segmentation mask. ASSISTANT: soldiers are often equipped with various tools and weapons to carry out their duties. what item in the picture can be classified as a weapon</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 300, 400])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 300, 400])):  [tensor([[[-7.9688, -7.9400, -7.8543,  ..., -3.3594, -3.5150, -3.6875],
         [-8.0119, -7.9765, -7.8978,  ..., -3.3109, -3.2338, -3.1484],
         [-8.0367, -8.0132, -7.9663,  ..., -3.2476, -2.9546, -2.6252],
         ...,
         [-6.2367, -6.6274, -6.8302,  ..., -4.7046, -4.5300, -4.5266],
         [-6.3319, -6.7111, -6.9164,  ..., -4.5310, -4.3625, -4.3769],
         [-6.5775, -6.9446, -7.1773,  ..., -4.7385, -4.5504, -4.5337]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What object is used to cover the entrance of the bathroom and ensure privacy? Please output segmentation mask. ASSISTANT: what object is used to cover the entrance of the bathroom and ensure privacy</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-10.5741, -11.4711, -10.9312,  ...,  -7.5516,  -7.8650,  -9.7578],
         [-10.3109, -10.9125, -10.5875,  ...,  -7.8094,  -7.8306,  -8.6599],
         [-10.3570, -11.0875, -10.7500,  ...,  -8.2578,  -7.9922,  -8.3682],
         ...,
         [ -7.2738,  -7.7250,  -7.3438,  ...,  -6.8750,  -6.2703,  -6.1736],
         [ -7.3098,  -7.8513,  -7.1516,  ...,  -6.9844,  -6.2972,  -6.5486],
         [ -7.7087,  -8.2754,  -7.7445,  ...,  -6.5281,  -5.9011,  -6.0846]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the person in the air in this image? Please output segmentation mask. ASSISTANT: the person in the air</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3034, 2148])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3034, 2148])):  [tensor([[[-11.4375, -11.4375, -11.4375,  ...,  -8.4233,  -8.4603,  -8.4609],
         [-11.4375, -11.4375, -11.4375,  ...,  -8.4233,  -8.4603,  -8.4609],
         [-11.4375, -11.4375, -11.4375,  ...,  -8.4233,  -8.4603,  -8.4609],
         ...,
         [ -8.6250,  -8.6250,  -8.6250,  ...,  -6.9455,  -7.0642,  -7.0664],
         [ -8.6250,  -8.6250,  -8.6250,  ...,  -6.9455,  -7.0642,  -7.0664],
         [ -8.6250,  -8.6250,  -8.6250,  ...,  -6.9455,  -7.0642,  -7.0664]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is a place where bees can suck nectar from flowers in this image? Please output segmentation mask. ASSISTANT: a place where bees can suck nectar from flowers</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]

 48%|████████████████████████████████████████████████████████████████████▍                                                                           | 95/200 [00:35<00:35,  2.99it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 566, 939])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 566, 939])):  [tensor([[[-6.5625, -6.5678, -6.6192,  ..., -6.4575, -6.5244, -6.5312],
         [-6.5567, -6.5620, -6.6133,  ..., -6.4514, -6.5161, -6.5228],
         [-6.5002, -6.5054, -6.5559,  ..., -6.3925, -6.4361, -6.4406],
         ...,
         [-3.6632, -3.6772, -3.8133,  ..., -3.2455, -3.1728, -3.1654],
         [-3.7046, -3.7151, -3.8168,  ..., -3.1877, -3.1202, -3.1132],
         [-3.7557, -3.7621, -3.8238,  ..., -3.1323, -3.0726, -3.0664]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n I want to take a trip around the world, but I need some transportation to help me cross the oceans. What type of transportation in the picture can fulfill this requirement? Please output segmentation mask. ASSISTANT: i want to take a trip around the world, but i need some transportation to help me cross the oceans. what type of transportation in the picture can fulfill this requirement</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[90]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 960, 1280])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 960, 1280])):  [tensor([[[-3.1875, -3.1875, -3.1611,  ..., -5.9062, -5.9375, -5.9375],
         [-3.1875, -3.1875, -3.1611,  ..., -5.9062, -5.9375, -5.9375],
         [-3.1807, -3.1807, -3.1544,  ..., -5.8895, -5.9180, -5.9180],
         ...,
         [-6.9766, -6.9766, -7.0544,  ..., -3.2914, -3.3027, -3.3027],
         [-6.7875, -6.7875, -6.8570,  ..., -3.5680, -3.5750, -3.5750],
         [-6.5469, -6.5469, -6.6055,  ..., -3.9000, -3.9004, -3.9004]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is where we can see the speed of the car in this image? Please output segmentation mask. ASSISTANT: where we can see the speed of the car</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[-5.4375, -5.4375, -5.4375,  ..., -5.0000, -5.0000, -5.0000],
         [-5.4375, -5.4375, -5.4375,  ..., -5.0000, -5.0000, -5.0000],
         [-5.4375, -5.4375, -5.4375,  ..., -5.0000, -5.0000, -5.0000],
         ...,
         [-2.0254, -2.0254, -2.0254,  ..., -2.9316, -2.9316, -2.9316],
         [-2.4590, -2.4590, -2.4590,  ..., -3.0527, -3.0527, -3.0527],
         [-2.6758, -2.6758, -2.6758,  ..., -3.1133, -3.1133, -3.1133]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When a photographer wants to adjust the focus of a camera lens, what part in the picture could they manipulate to achieve this? Please output segmentation mask. ASSISTANT: when a photographer wants to adjust the focus of a camera lens, what part in the picture could they manipulate to achieve this</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1584, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1584, 1600])):  [tensor([[[-6.1250, -6.1250, -6.1219,  ..., -5.2801, -5.2812, -5.2812],
         [-6.1250, -6.1250, -6.1219,  ..., -5.2801, -5.2812, -5.2812],
         [-6.1195, -6.1195, -6.1165,  ..., -5.2796, -5.2809, -5.2809],
         ...,
         [-4.3938, -4.3938, -4.4014,  ..., -5.1827, -5.1875, -5.1875],
         [-4.3238, -4.3238, -4.3307,  ..., -5.1584, -5.1625, -5.1625],
         [-4.2734, -4.2734, -4.2798,  ..., -5.1409, -5.1445, -5.1445]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is unopened flower bud in this image? Please output segmentation mask. ASSISTANT: unopened flower bud</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 566, 850])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 566, 850])):  [tensor([[[-12.1250, -12.1682, -12.4096,  ..., -14.0739, -14.3823, -14.4375],
         [-12.1370, -12.1806, -12.4246,  ..., -14.0951, -14.3896, -14.4423],
         [-12.2041, -12.2503, -12.5087,  ..., -14.2136, -14.4303, -14.4691],
         ...,
         [-10.4684, -10.5335, -10.8979,  ...,  -7.1188,  -7.0374,  -7.0228],
         [-10.5060, -10.5307, -10.6689,  ...,  -6.5008,  -6.3527,  -6.3262],
         [-10.5437, -10.5279, -10.4399,  ...,  -5.8827,  -5.6680,  -5.6296]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n This image depicts a forest. Which of the animals in the picture pose a threat to human safety? Please output segmentation mask. ASSISTANT: this image depicts a forest. which of the animals in the picture pose a threat to human safety</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[73]]

 50%|███████████████████████████████████████████████████████████████████████▌                                                                       | 100/200 [00:37<00:34,  2.94it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 300, 400])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 300, 400])):  [tensor([[[-7.6562, -7.7712, -7.8258,  ..., -7.2504, -7.4262, -7.6562],
         [-7.5269, -7.6088, -7.6416,  ..., -7.1848, -7.0859, -7.0381],
         [-7.3398, -7.3984, -7.4249,  ..., -7.0817, -6.7246, -6.4102],
         ...,
         [-6.1812, -6.6269, -6.8225,  ..., -4.5579, -4.4815, -4.5719],
         [-6.2694, -6.7371, -6.9800,  ..., -4.4412, -4.4128, -4.5600],
         [-6.5037, -6.9160, -7.1768,  ..., -4.7766, -4.7166, -4.8181]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a graduation ceremony, it is a tradition for the graduates to wear a specific type of clothing to signify their achievement. What item of clothing can be seen in the picture that is commonly worn by graduates? Please output segmentation mask. ASSISTANT: in a graduation ceremony, it is a tradition for the graduates to wear a specific type of clothing to signify their achievement. what item of clothing can be seen in the picture that is commonly worn by graduates</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[99]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3456, 2304])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 2304])):  [tensor([[[-6.7500, -6.7500, -6.7500,  ..., -5.2520, -5.2539, -5.2539],
         [-6.7500, -6.7500, -6.7500,  ..., -5.2520, -5.2539, -5.2539],
         [-6.7500, -6.7500, -6.7500,  ..., -5.2520, -5.2539, -5.2539],
         ...,
         [-5.2188, -5.2188, -5.2188,  ..., -2.9408, -2.9766, -2.9766],
         [-5.2188, -5.2188, -5.2188,  ..., -2.9408, -2.9766, -2.9766],
         [-5.2188, -5.2188, -5.2188,  ..., -2.9408, -2.9766, -2.9766]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When visiting a library or bookstore, people often browse through the shelves to find interesting books to read. Which area in the picture could provide a variety of reading materials for visitors? Please output segmentation mask. ASSISTANT: when visiting a library or bookstore, people often browse through the shelves to find interesting books to read. which area in the picture could provide a variety of reading materials for visitors</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[91]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 4000, 2683])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 4000, 2683])):  [tensor([[[-8.4375, -8.4375, -8.4375,  ..., -5.9726, -6.0234, -6.0234],
         [-8.4375, -8.4375, -8.4375,  ..., -5.9726, -6.0234, -6.0234],
         [-8.4375, -8.4375, -8.4375,  ..., -5.9726, -6.0234, -6.0234],
         ...,
         [-5.4375, -5.4375, -5.4375,  ..., -4.6113, -4.6523, -4.6523],
         [-5.4375, -5.4375, -5.4375,  ..., -4.6113, -4.6523, -4.6523],
         [-5.4375, -5.4375, -5.4375,  ..., -4.6113, -4.6523, -4.6523]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the person who appears to have already won in the battle in this image? Please output segmentation mask. ASSISTANT: the person who appears to have already won in the battle</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1504, 2256])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1504, 2256])):  [tensor([[[-6.1875, -6.1875, -6.1875,  ..., -6.3125, -6.3125, -6.3125],
         [-6.1875, -6.1875, -6.1875,  ..., -6.3125, -6.3125, -6.3125],
         [-6.1875, -6.1875, -6.1875,  ..., -6.3125, -6.3125, -6.3125],
         ...,
         [-5.7393, -5.7393, -5.7393,  ..., -3.7707, -3.7707, -3.7707],
         [-5.8670, -5.8670, -5.8670,  ..., -3.7405, -3.7405, -3.7405],
         [-5.9180, -5.9180, -5.9180,  ..., -3.7285, -3.7285, -3.7285]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a healthy meal, vegetables are often included to provide essential nutrients. What in the picture can be used to eat the vegetables? Please output segmentation mask. ASSISTANT: in a healthy meal, vegetables are often included to provide essential nutrients. what in the picture can be used to eat the vegetables</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[83]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 854, 1280])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 854, 1280])):  [tensor([[[-7.8125, -7.8125, -7.8281,  ..., -7.7793, -7.8125, -7.8125],
         [-7.8125, -7.8125, -7.8281,  ..., -7.7793, -7.8125, -7.8125],
         [-7.8066, -7.8066, -7.8241,  ..., -7.7659, -7.7988, -7.7988],
         ...,
         [-5.4375, -5.4375, -5.4531,  ..., -3.9773, -3.9607, -3.9607],
         [-5.5337, -5.5337, -5.5328,  ..., -3.7750, -3.7500, -3.7500],
         [-5.7578, -5.7578, -5.7480,  ..., -3.7524, -3.7227, -3.7227]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that prevents people from getting into the building in this image? Please output segmentation mask. ASSISTANT: something that prevents people from getting into the building</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         ...,
         [255., 255., 255.,  ..., 255., 255., 255.],
         [255., 255., 255.,  ..., 255., 255., 255.],
         [255., 255., 255.,  ..., 255., 255., 255.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-5.2812, -5.2878, -5.3187,  ..., -5.0594, -5.1135, -5.1250],
         [-5.2714, -5.2788, -5.3138,  ..., -5.0579, -5.1092, -5.1201],
         [-5.2250, -5.2365, -5.2906,  ..., -5.0509, -5.0888, -5.0969],
         ...,
         [-1.0492, -1.0341, -0.9629,  ..., -0.7702, -0.7226, -0.7125],
         [-1.1834, -1.1666, -1.0877,  ..., -0.8292, -0.7642, -0.7504],
         [-1.5506, -1.5351, -1.4618,  ..., -1.2089, -1.1347, -1.1189]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that the monkey uses to pierce its food in this image? Please output segmentation mask. ASSISTANT: something that the monkey uses to pierce its food</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1600, 1066])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1600, 1066])):  [tensor([[[-9.3750, -9.3750, -9.3812,  ..., -8.4872, -8.1673, -7.9375],
         [-9.3750, -9.3750, -9.3812,  ..., -8.4872, -8.1673, -7.9375],
         [-9.3797, -9.3797, -9.3859,  ..., -8.5013, -8.1808, -7.9505],
         ...,
         [-4.9645, -4.9645, -4.9592,  ..., -6.0341, -5.7753, -5.5894],
         [-4.9688, -4.9688, -4.9633,  ..., -6.0248, -5.7649, -5.5781],
         [-4.9688, -4.9688, -4.9633,  ..., -6.0248, -5.7649, -5.5781]]],

 53%|███████████████████████████████████████████████████████████████████████████▊                                                                   | 106/200 [00:39<00:30,  3.05it/s]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that avoids falling down in this image? Please output segmentation mask. ASSISTANT: something that avoids falling down</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1224, 1632])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1224, 1632])):  [tensor([[[-7.1250, -7.1250, -7.1352,  ..., -9.3696, -9.3750, -9.3750],
         [-7.1250, -7.1250, -7.1352,  ..., -9.3696, -9.3750, -9.3750],
         [-7.1247, -7.1247, -7.1349,  ..., -9.3649, -9.3702, -9.3702],
         ...,
         [-4.8158, -4.8158, -4.8206,  ..., -2.3133, -2.3108, -2.3108],
         [-4.9504, -4.9504, -4.9544,  ..., -2.5852, -2.5818, -2.5818],
         [-5.0469, -5.0469, -5.0503,  ..., -2.7893, -2.7852, -2.7852]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the container that can be used to hold soup currently in this image? Please output segmentation mask. ASSISTANT: the container that can be used to hold soup currently</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[67]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-4.7500, -4.7336, -4.6562,  ..., -4.8687, -5.0544, -5.0938],
         [-4.7434, -4.7271, -4.6502,  ..., -4.8533, -5.0260, -5.0626],
         [-4.7125, -4.6966, -4.6216,  ..., -4.7806, -4.8920, -4.9156],
         ...,
         [-4.7406, -4.7669, -4.8906,  ..., -4.6622, -4.6831, -4.6875],
         [-4.8367, -4.8587, -4.9622,  ..., -4.5874, -4.6345, -4.6445],
         [-5.1644, -5.1846, -5.2799,  ..., -4.4809, -4.4848, -4.4856]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When you look closely into a person's eye, what is the part that surrounds the dark-colored iris and helps protect the eye from dust and debris? Please output segmentation mask. ASSISTANT: when you look closely into a person's eye, what is the part that surrounds the dark-colored iris and helps protect the eye from dust and debris</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[-8.6250, -8.6250, -8.6250,  ..., -7.7812, -7.7812, -7.7812],
         [-8.6250, -8.6250, -8.6250,  ..., -7.7812, -7.7812, -7.7812],
         [-8.6250, -8.6250, -8.6250,  ..., -7.7812, -7.7812, -7.7812],
         ...,
         [-8.3005, -8.3005, -8.3005,  ..., -5.5245, -5.5245, -5.5245],
         [-8.4178, -8.4178, -8.4178,  ..., -5.5584, -5.5584, -5.5584],
         [-8.4453, -8.4453, -8.4453,  ..., -5.5664, -5.5664, -5.5664]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the object that reaches the sky in this image? Please output segmentation mask. ASSISTANT: the object that reaches the sky</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2112, 2816])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2112, 2816])):  [tensor([[[-11.5625, -11.5625, -11.5625,  ..., -12.4375, -12.4375, -12.4375],
         [-11.5625, -11.5625, -11.5625,  ..., -12.4375, -12.4375, -12.4375],
         [-11.5625, -11.5625, -11.5625,  ..., -12.4375, -12.4375, -12.4375],
         ...,
         [ -7.3466,  -7.3466,  -7.3466,  ...,  -4.9318,  -4.9318,  -4.9318],
         [ -7.4517,  -7.4517,  -7.4517,  ...,  -5.0341,  -5.0341,  -5.0341],
         [ -7.4648,  -7.4648,  -7.4648,  ...,  -5.0469,  -5.0469,  -5.0469]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In an orchestra, musicians play different instruments. What object in the picture is commonly played with a bow to produce sound? Please output segmentation mask. ASSISTANT: in an orchestra, musicians play different instruments. what object in the picture is commonly played with a bow to produce sound</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[77]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 280, 330])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 280, 330])):  [tensor([[[ -9.3173,  -9.9340,  -9.8928,  ...,  -9.3242,  -9.3380, -10.1158],
         [ -9.1946,  -9.8653,  -9.8697,  ...,  -9.5527,  -9.4463,  -9.9541],
         [ -9.1324,  -9.8848,  -9.8438,  ...,  -9.5421,  -9.4098,  -9.7072],
         ...,
         [ -1.4899,  -1.9016,  -1.9595,  ...,  -5.3057,  -5.2030,  -5.0426],
         [ -1.4086,  -1.6177,  -1.4215,  ...,  -3.7979,  -3.7657,  -3.8121],
         [ -2.1908,  -2.1070,  -1.9310,  ...,  -3.0698,  -3.1549,  -3.1418]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What object in a typical modern kitchen can be used to quickly heat up leftovers, boil water, or cook frozen food? Please output segmentation mask. ASSISTANT: what object in a typical modern kitchen can be used to quickly heat up leftovers, boil water, or cook frozen food</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[80]]

 56%|███████████████████████████████████████████████████████████████████████████████▎                                                               | 111/200 [00:41<00:37,  2.36it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-8.0653, -8.4131, -8.2051,  ..., -4.6982, -4.4278, -4.7673],
         [-7.8760, -8.4453, -8.3125,  ..., -4.4297, -4.2388, -4.3419],
         [-7.9010, -8.4703, -8.3359,  ..., -4.2227, -4.0914, -4.2472],
         ...,
         [-5.1516, -5.7531, -5.3516,  ..., -3.2930, -3.2617, -3.1274],
         [-4.9641, -5.5678, -5.0047,  ..., -3.2008, -3.2662, -3.1674],
         [-5.0344, -5.5285, -4.9662,  ..., -3.2411, -3.2530, -2.9721]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When plants grow excessively in unwanted areas, it is necessary to remove them to maintain the aesthetics of the surroundings. What part of the plants in the picture may need to be removed in this situation? Please output segmentation mask. ASSISTANT: when plants grow excessively in unwanted areas, it is necessary to remove them to maintain the aesthetics of the surroundings. what part of the plants in the picture may need to be removed in this situation</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[97]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 853, 1280])):  [tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 853, 1280])):  [tensor([[[-2.7188, -2.7188, -2.7109,  ..., -3.7197, -3.7344, -3.7344],
         [-2.7188, -2.7188, -2.7109,  ..., -3.7197, -3.7344, -3.7344],
         [-2.7100, -2.7100, -2.7024,  ..., -3.7013, -3.7139, -3.7139],
         ...,
         [-2.1641, -2.1641, -2.1592,  ..., -1.7283, -1.7149, -1.7149],
         [-2.1734, -2.1734, -2.1631,  ..., -1.7782, -1.7633, -1.7633],
         [-2.1816, -2.1816, -2.1665,  ..., -1.8218, -1.8057, -1.8057]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a television studio, various equipment is used to capture and record video footage. What in the picture could be used to stabilize and hold the camera steady during filming? Please output segmentation mask. ASSISTANT: in a television studio, various equipment is used to capture and record video footage. what in the picture could be used to stabilize and hold the camera steady during filming</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2573, 3593])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2573, 3593])):  [tensor([[[ -9.0625,  -9.0625,  -9.0625,  ..., -10.1250, -10.1250, -10.1250],
         [ -9.0625,  -9.0625,  -9.0625,  ..., -10.1250, -10.1250, -10.1250],
         [ -9.0625,  -9.0625,  -9.0625,  ..., -10.1250, -10.1250, -10.1250],
         ...,
         [ -8.8305,  -8.8305,  -8.8305,  ...,  -6.1030,  -6.1030,  -6.1030],
         [ -8.8438,  -8.8438,  -8.8438,  ...,  -6.1328,  -6.1328,  -6.1328],
         [ -8.8438,  -8.8438,  -8.8438,  ...,  -6.1328,  -6.1328,  -6.1328]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n There are two washing machines as shown in the picture. If I need to do laundry, where in the picture would I put the clothes? Please output segmentation mask. ASSISTANT: there are two washing machines as shown in the picture. if i need to do laundry, where in the picture would i put the clothes</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[82]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3254, 2168])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3254, 2168])):  [tensor([[[-6.0000, -6.0000, -6.0000,  ..., -5.0895, -5.0000, -5.0000],
         [-6.0000, -6.0000, -6.0000,  ..., -5.0895, -5.0000, -5.0000],
         [-6.0000, -6.0000, -6.0000,  ..., -5.0895, -5.0000, -5.0000],
         ...,
         [-1.4531, -1.4531, -1.4531,  ..., -2.9075, -2.8672, -2.8672],
         [-1.4531, -1.4531, -1.4531,  ..., -2.9075, -2.8672, -2.8672],
         [-1.4531, -1.4531, -1.4531,  ..., -2.9075, -2.8672, -2.8672]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Insects are often found on or near trees, where they can find shelter and food. What part of the tree in this picture could insects commonly be found on or around? Please output segmentation mask. ASSISTANT: insects are often found on or near trees, where they can find shelter and food. what part of the tree in this picture could insects commonly be found on or around</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1728, 2304])):  [tensor([[[1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.],
         ...,
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1728, 2304])):  [tensor([[[ -6.7188,  -6.7188,  -6.7188,  ..., -10.3125, -10.3125, -10.3125],
         [ -6.7188,  -6.7188,  -6.7188,  ..., -10.3125, -10.3125, -10.3125],
         [ -6.7188,  -6.7188,  -6.7188,  ..., -10.3125, -10.3125, -10.3125],
         ...,
         [ -8.1042,  -8.1042,  -8.1042,  ...,  -6.1146,  -6.1146,  -6.1146],
         [ -8.1250,  -8.1250,  -8.1250,  ...,  -5.9688,  -5.9688,  -5.9688],
         [ -8.1328,  -8.1328,  -8.1328,  ...,  -5.9141,  -5.9141,  -5.9141]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If the person in the picture is the owner of the blue car and wants to drive it, which car door is least likely to be the one he enters based on the picture shown? Please output segmentation mask. ASSISTANT: if the person in the picture is the owner of the blue car and wants to drive it, which car door is least likely to be the one he enters based on the picture shown</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[ -8.0625,  -8.0625,  -8.0625,  ..., -10.5000, -10.5000, -10.5000],
         [ -8.0625,  -8.0625,  -8.0625,  ..., -10.5000, -10.5000, -10.5000],
         [ -8.0625,  -8.0625,  -8.0625,  ..., -10.5000, -10.5000, -10.5000],
         ...,
         [ -7.8242,  -7.8242,  -7.8242,  ...,  -4.6328,  -4.6328,  -4.6328],
         [ -7.8320,  -7.8320,  -7.8320,  ...,  -4.6797,  -4.6797,  -4.6797],
         [ -7.8359,  -7.8359,  -7.8359,  ...,  -4.7031,  -4.7031,  -4.7031]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the keys on a piano used to play notes of half-steps or semitones in this image? Please output segmentation mask. ASSISTANT: the keys on a piano used to play notes of half-steps or semitones</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[74]]

 58%|███████████████████████████████████████████████████████████████████████████████████▋                                                           | 117/200 [00:43<00:27,  3.04it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1941, 2919])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 1., 1.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1941, 2919])):  [tensor([[[-5.9688, -5.9688, -5.9688,  ..., -3.5625, -3.5625, -3.5625],
         [-5.9688, -5.9688, -5.9688,  ..., -3.5625, -3.5625, -3.5625],
         [-5.9688, -5.9688, -5.9688,  ..., -3.5625, -3.5625, -3.5625],
         ...,
         [-8.0052, -8.0052, -8.0052,  ..., -4.1423, -4.1423, -4.1423],
         [-8.1094, -8.1094, -8.1094,  ..., -4.1916, -4.1916, -4.1916],
         [-8.1172, -8.1172, -8.1172,  ..., -4.1953, -4.1953, -4.1953]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n The general steps for brewing tea are to put tea leaves into a teacup and then pour hot water over them. What container in the picture is most likely to be used next for pouring hot water to make tea? Please output segmentation mask. ASSISTANT: the general steps for brewing tea are to put tea leaves into a teacup and then pour hot water over them. what container in the picture is most likely to be used next for pouring hot water to make tea</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[97]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3056, 4592])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3056, 4592])):  [tensor([[[-8.2500, -8.2500, -8.2500,  ..., -8.4375, -8.4375, -8.4375],
         [-8.2500, -8.2500, -8.2500,  ..., -8.4375, -8.4375, -8.4375],
         [-8.2500, -8.2500, -8.2500,  ..., -8.4375, -8.4375, -8.4375],
         ...,
         [-6.5084, -6.5084, -6.5084,  ..., -3.3824, -3.3824, -3.3824],
         [-6.5195, -6.5195, -6.5195,  ..., -3.3672, -3.3672, -3.3672],
         [-6.5195, -6.5195, -6.5195,  ..., -3.3672, -3.3672, -3.3672]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that is noticeably different from the other plants in the picture in this image? Please output segmentation mask. ASSISTANT: something that is noticeably different from the other plants in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3456, 5184])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 5184])):  [tensor([[[-6.9375, -6.9375, -6.9375,  ..., -6.4688, -6.4688, -6.4688],
         [-6.9375, -6.9375, -6.9375,  ..., -6.4688, -6.4688, -6.4688],
         [-6.9375, -6.9375, -6.9375,  ..., -6.4688, -6.4688, -6.4688],
         ...,
         [-2.5137, -2.5137, -2.5137,  ..., -0.8955, -0.8955, -0.8955],
         [-2.5137, -2.5137, -2.5137,  ..., -0.8955, -0.8955, -0.8955],
         [-2.5137, -2.5137, -2.5137,  ..., -0.8955, -0.8955, -0.8955]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When the weather is very cold outside, what equipment do we need to use to increase the indoor temperature in the room? Please output segmentation mask. ASSISTANT: when the weather is very cold outside, what equipment do we need to use to increase the indoor temperature in the room</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]

 62%|███████████████████████████████████████████████████████████████████████████████████████▉                                                       | 123/200 [00:45<00:24,  3.18it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-5.1146, -4.9921, -4.5176,  ..., -4.1006, -4.0219, -4.1805],
         [-4.6698, -4.9019, -4.6250,  ..., -4.0469, -3.9392, -3.9575],
         [-4.3300, -4.4320, -4.2539,  ..., -3.9844, -3.8617, -3.8671],
         ...,
         [-4.5399, -4.1586, -3.1016,  ..., -3.2188, -3.3945, -3.3677],
         [-4.5298, -4.1077, -3.2008,  ..., -3.2836, -3.3959, -3.5549],
         [-4.6493, -4.5224, -3.6927,  ..., -3.6770, -3.7416, -3.8323]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If we wanted to identify the specific vehicle shown in the picture, what part of the car should we look at? Please output segmentation mask. ASSISTANT: if we wanted to identify the specific vehicle shown in the picture, what part of the car should we look at</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[74]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 800, 1200])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 800, 1200])):  [tensor([[[-8.9375, -8.9375, -8.9622,  ..., -9.1479, -9.1875, -9.1875],
         [-8.9375, -8.9375, -8.9622,  ..., -9.1479, -9.1875, -9.1875],
         [-8.9226, -8.9226, -8.9584,  ..., -9.1879, -9.2172, -9.2172],
         ...,
         [-4.8321, -4.8321, -4.8342,  ..., -2.6247, -2.5931, -2.5931],
         [-4.7095, -4.7095, -4.6775,  ..., -2.4317, -2.3840, -2.3840],
         [-4.9961, -4.9961, -4.9506,  ..., -2.4754, -2.4160, -2.4160]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something hot and light in this image? Please output segmentation mask. ASSISTANT: something hot and light</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 616, 816])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 616, 816])):  [tensor([[[-8.3750, -8.4019, -8.5349,  ..., -7.2292, -7.4805, -7.5312],
         [-8.3690, -8.3959, -8.5290,  ..., -7.2035, -7.4439, -7.4924],
         [-8.3395, -8.3663, -8.4994,  ..., -7.0765, -7.2626, -7.3003],
         ...,
         [-4.5622, -4.6082, -4.8359,  ..., -3.7981, -3.9002, -3.9209],
         [-4.3957, -4.4245, -4.5671,  ..., -3.7266, -3.7646, -3.7723],
         [-4.6996, -4.7172, -4.8041,  ..., -3.9328, -3.8931, -3.8850]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is objects that can help women appear taller in this image? Please output segmentation mask. ASSISTANT: objects that can help women appear taller</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 465, 500])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 465, 500])):  [tensor([[[-1.5000, -1.4016, -1.2136,  ..., -4.9400, -5.1640, -5.2812],
         [-1.3975, -1.3008, -1.1160,  ..., -4.9559, -5.1585, -5.2645],
         [-1.2016, -1.1081, -0.9294,  ..., -4.9863, -5.1479, -5.2325],
         ...,
         [-1.9457, -1.9783, -2.0405,  ..., -2.8499, -2.9098, -2.9412],
         [-2.0737, -2.1041, -2.1623,  ..., -2.9419, -2.8994, -2.8772],
         [-2.4839, -2.4794, -2.4710,  ..., -2.9850, -2.8772, -2.8209]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What container in the picture is used for arranging the flowers to make them look more beautiful? Please output segmentation mask. ASSISTANT: what container in the picture is used for arranging the flowers to make them look more beautiful</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-5.8337, -5.6471, -5.1443,  ..., -4.9471, -4.7982, -5.0528],
         [-5.4752, -5.6578, -5.3484,  ..., -4.6141, -4.6994, -4.7767],
         [-5.2002, -5.0391, -4.5078,  ..., -3.9727, -4.3211, -4.4661],
         ...,
         [-4.6145, -4.8938, -4.3711,  ..., -3.4922, -3.6039, -3.2333],
         [-4.4170, -4.6641, -4.1602,  ..., -3.4320, -3.5352, -3.4224],
         [-4.7343, -5.1184, -4.5815,  ..., -3.7612, -3.7801, -3.6728]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the places where the driver can observe the speed in this image? Please output segmentation mask. ASSISTANT: the places where the driver can observe the speed</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 944, 1417])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 944, 1417])):  [tensor([[[-8.7500, -8.7500, -8.7308,  ..., -7.1755, -7.1875, -7.1875],
         [-8.7500, -8.7500, -8.7308,  ..., -7.1755, -7.1875, -7.1875],
         [-8.7524, -8.7524, -8.7349,  ..., -7.1276, -7.1385, -7.1385],
         ...,
         [-4.0868, -4.0868, -4.0682,  ..., -5.0126, -4.9798, -4.9798],
         [-4.4396, -4.4396, -4.4150,  ..., -4.8272, -4.7879, -4.7879],
         [-4.7246, -4.7246, -4.6952,  ..., -4.6774, -4.6328, -4.6328]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something indicating that someone is celerating the birthday in this image? Please output segmentation mask. ASSISTANT: something indicating that someone is celerating the birthday</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]

 64%|███████████████████████████████████████████████████████████████████████████████████████████▌                                                   | 128/200 [00:47<00:23,  3.04it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 651, 383])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 651, 383])):  [tensor([[[-5.5938, -5.5770, -5.5184,  ..., -5.0090, -4.7931, -4.5352],
         [-5.5971, -5.5847, -5.5412,  ..., -5.0339, -4.8372, -4.5991],
         [-5.6088, -5.6115, -5.6209,  ..., -5.1211, -4.9917, -4.8228],
         ...,
         [-3.8325, -3.8497, -3.9097,  ..., -2.8554, -2.8527, -2.8313],
         [-3.7562, -3.7639, -3.7910,  ..., -2.6983, -2.7548, -2.8026],
         [-3.7344, -3.7394, -3.7570,  ..., -2.6534, -2.7268, -2.7944]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that we use to control computer games in this image? Please output segmentation mask. ASSISTANT: something that we use to control computer games</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-6.8750, -6.8783, -6.8938,  ..., -6.7125, -6.8208, -6.8438],
         [-6.8635, -6.8674, -6.8857,  ..., -6.6938, -6.7931, -6.8142],
         [-6.8094, -6.8161, -6.8478,  ..., -6.6056, -6.6629, -6.6750],
         ...,
         [-4.8438, -4.8520, -4.8906,  ..., -5.5378, -5.5711, -5.5781],
         [-4.8483, -4.8594, -4.9116,  ..., -5.4588, -5.4936, -5.5009],
         [-4.8650, -4.8725, -4.9081,  ..., -5.3743, -5.3548, -5.3506]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When serving wine, it is common to pour it into a glass from a container. What object in the picture could be used to pour wine? Please output segmentation mask. ASSISTANT: when serving wine, it is common to pour it into a glass from a container. what object in the picture could be used to pour wine</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[80]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 370, 658])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 370, 658])):  [tensor([[[-8.4375, -8.4831, -8.6443,  ..., -7.2461, -7.6145, -7.7188],
         [-8.4245, -8.4721, -8.6405,  ..., -7.2359, -7.5766, -7.6731],
         [-8.3784, -8.4333, -8.6271,  ..., -7.1999, -7.4430, -7.5118],
         ...,
         [-4.9709, -5.0830, -5.4790,  ..., -2.6388, -2.7655, -2.8013],
         [-5.2172, -5.3006, -5.5950,  ..., -2.8311, -2.9536, -2.9883],
         [-5.3481, -5.3914, -5.5447,  ..., -3.2280, -3.2931, -3.3115]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the container that contains the vegetable in this image? Please output segmentation mask. ASSISTANT: the container that contains the vegetable</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1568, 2352])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1568, 2352])):  [tensor([[[-4.1875, -4.1875, -4.1875,  ..., -5.4688, -5.4688, -5.4688],
         [-4.1875, -4.1875, -4.1875,  ..., -5.4688, -5.4688, -5.4688],
         [-4.1875, -4.1875, -4.1875,  ..., -5.4688, -5.4688, -5.4688],
         ...,
         [-3.1655, -3.1655, -3.1655,  ..., -2.8055, -2.8055, -2.8055],
         [-3.2208, -3.2208, -3.2208,  ..., -2.7775, -2.7775, -2.7775],
         [-3.2402, -3.2402, -3.2402,  ..., -2.7676, -2.7676, -2.7676]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In the animal kingdom, the males of certain species have a distinctive feature that sets them apart from the females. What part of the lion in this picture is a defining characteristic of male lions? Please output segmentation mask. ASSISTANT: in the animal kingdom, the males of certain species have a distinctive feature that sets them apart from the females. what part of the lion in this picture is a defining characteristic of male lions</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[92]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[-8.0625, -8.0625, -8.0625,  ..., -9.3125, -9.6219, -9.6875],
         [-8.0576, -8.0578, -8.0591,  ..., -9.3079, -9.6076, -9.6711],
         [-8.0344, -8.0359, -8.0428,  ..., -9.2863, -9.5400, -9.5938],
         ...,
         [-2.3694, -2.3498, -2.2577,  ..., -4.1740, -4.2157, -4.2245],
         [-3.0466, -3.0070, -2.8202,  ..., -3.7782, -3.7011, -3.6847],
         [-3.7239, -3.6642, -3.3828,  ..., -3.3823, -3.1865, -3.1449]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n During times of war, armored vehicles are commonly used to protect soldiers and engage in combat. What object in the picture can provide such protection? Please output segmentation mask. ASSISTANT: during times of war, armored vehicles are commonly used to protect soldiers and engage in combat. what object in the picture can provide such protection</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]


 70%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                                           | 139/200 [00:51<00:20,  3.01it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[-7.7500, -7.7500, -7.7500,  ..., -8.4375, -8.4375, -8.4375],
         [-7.7500, -7.7500, -7.7500,  ..., -8.4375, -8.4375, -8.4375],
         [-7.7500, -7.7500, -7.7500,  ..., -8.4375, -8.4375, -8.4375],
         ...,
         [-4.4961, -4.4961, -4.4961,  ..., -5.7266, -5.7266, -5.7266],
         [-4.8477, -4.8477, -4.8477,  ..., -5.8359, -5.8359, -5.8359],
         [-5.0234, -5.0234, -5.0234,  ..., -5.8906, -5.8906, -5.8906]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n It is common for some bird species to live near bodies of water and rely on them as their primary habitat. What in the picture can be considered as the habitat for the birds mentioned? Please output segmentation mask. ASSISTANT: it is common for some bird species to live near bodies of water and rely on them as their primary habitat. what in the picture can be considered as the habitat for the birds mentioned</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1193, 1271])):  [tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 0., 0.],
         [1., 1., 1.,  ..., 1., 0., 0.],
         [1., 1., 1.,  ..., 1., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1193, 1271])):  [tensor([[[-13.2500, -13.2500, -13.2661,  ..., -12.6634, -12.6875, -12.6875],
         [-13.2500, -13.2500, -13.2661,  ..., -12.6634, -12.6875, -12.6875],
         [-13.2580, -13.2580, -13.2826,  ..., -12.7630, -12.7798, -12.7798],
         ...,
         [ -9.0413,  -9.0413,  -9.1580,  ...,  -8.1011,  -8.0859,  -8.0859],
         [ -8.0029,  -8.0029,  -8.0640,  ...,  -6.7444,  -6.7265,  -6.7265],
         [ -7.0898,  -7.0898,  -7.1020,  ...,  -5.5515,  -5.5312,  -5.5312]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In historical architecture, a building often has a grand entrance that consists of a large opening with a curved or pointed top. What feature in the picture resembles such an entrance? Please output segmentation mask. ASSISTANT: in historical architecture, a building often has a grand entrance that consists of a large opening with a curved or pointed top. what feature in the picture resembles such an entrance</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 576])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 576])):  [tensor([[[-3.7500, -3.7422, -3.7083,  ..., -3.2969, -3.4619, -3.8646],
         [-3.7676, -3.7580, -3.7165,  ..., -3.2744, -3.4282, -3.8438],
         [-3.8438, -3.8265, -3.7517,  ..., -3.1771, -3.2819, -3.7535],
         ...,
         [-4.0260, -4.0342, -4.0694,  ..., -2.1806, -2.3555, -2.9861],
         [-4.1572, -4.1453, -4.0934,  ..., -2.1608, -2.3380, -2.9551],
         [-4.1875, -4.1709, -4.0990,  ..., -2.1563, -2.3340, -2.9479]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the people who are about to get on the vehicle in this image? Please output segmentation mask. ASSISTANT: the people who are about to get on the vehicle</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[67]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-11.5000, -11.5230, -11.6313,  ...,  -4.5406,  -4.5329,  -4.5312],
         [-11.4967, -11.5221, -11.6418,  ...,  -4.4967,  -4.4751,  -4.4705],
         [-11.4812, -11.5180, -11.6912,  ...,  -4.2894,  -4.2028,  -4.1844],
         ...,
         [ -8.5562,  -8.6242,  -8.9444,  ...,  -4.5613,  -4.5288,  -4.5219],
         [ -8.6600,  -8.7371,  -9.1005,  ...,  -4.5883,  -4.5303,  -4.5180],
         [ -8.6663,  -8.7461,  -9.1226,  ...,  -4.8816,  -4.7694,  -4.7456]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something indicating the identity of the bus in this image? Please output segmentation mask. ASSISTANT: something indicating the identity of the bus</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2048, 1536])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2048, 1536])):  [tensor([[[-2.4688, -2.4688, -2.4688,  ..., -4.8418, -4.8613, -4.8711],
         [-2.4688, -2.4688, -2.4688,  ..., -4.8418, -4.8613, -4.8711],
         [-2.4688, -2.4688, -2.4688,  ..., -4.8418, -4.8613, -4.8711],
         ...,
         [-3.1875, -3.1875, -3.1875,  ..., -1.9941, -2.1152, -2.1758],
         [-3.1875, -3.1875, -3.1875,  ..., -1.9941, -2.1152, -2.1758],
         [-3.1875, -3.1875, -3.1875,  ..., -1.9941, -2.1152, -2.1758]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the object that can be used by the owner to lead the dog in this image? Please output segmentation mask. ASSISTANT: the object that can be used by the owner to lead the dog</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 762, 1000])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 762, 1000])):  [tensor([[[-8.1875, -8.1892, -8.2400,  ..., -7.3406, -7.4041, -7.4062],
         [-8.1903, -8.1920, -8.2430,  ..., -7.3376, -7.3995, -7.4015],
         [-8.2749, -8.2767, -8.3335,  ..., -7.2435, -7.2573, -7.2578],
         ...,
         [-8.6062, -8.6085, -8.6797,  ..., -3.1099, -3.1014, -3.1011],
         [-8.7899, -8.7918, -8.8484,  ..., -2.8102, -2.7830, -2.7822],
         [-9.2601, -9.2617, -9.3116,  ..., -2.7234, -2.6616, -2.6595]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the rabbit on the woman's back in this image? Please output segmentation mask. ASSISTANT: the rabbit on the woman's back</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]

 72%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉                                        | 144/200 [00:53<00:19,  2.93it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 802, 1089])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 802, 1089])):  [tensor([[[-3.4219, -3.4219, -3.4302,  ..., -7.3378, -7.4375, -7.4375],
         [-3.4219, -3.4219, -3.4302,  ..., -7.3378, -7.4375, -7.4375],
         [-3.3920, -3.3920, -3.3968,  ..., -7.2924, -7.3844, -7.3844],
         ...,
         [-3.9016, -3.9016, -3.9098,  ..., -2.1395, -2.1342, -2.1342],
         [-3.8722, -3.8722, -3.8539,  ..., -2.1417, -2.1305, -2.1305],
         [-3.8438, -3.8438, -3.7997,  ..., -2.1438, -2.1270, -2.1270]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What musical instrument in the picture is usually played with both hands on a keyboard? Please output segmentation mask. ASSISTANT: what musical instrument in the picture is usually played with both hands on a keyboard</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[67]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[ -5.9015,  -5.8771,  -5.7928,  ..., -10.0281,  -9.9445, -11.0940],
         [ -5.6622,  -5.4753,  -5.5672,  ...,  -9.8562,  -9.9875, -10.5461],
         [ -5.1551,  -5.0906,  -5.2266,  ...,  -9.7812,  -9.8031,  -9.9965],
         ...,
         [ -7.5295,  -7.4328,  -7.2266,  ...,  -6.2109,  -5.6234,  -5.6557],
         [ -8.0672,  -8.3250,  -7.8906,  ...,  -5.9516,  -5.3659,  -5.5958],
         [ -8.0216,  -8.2867,  -8.0773,  ...,  -5.7248,  -5.1709,  -5.4588]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When eating scrambled eggs for breakfast, people often add a side dish made of potatoes. What item in the picture can be used to serve the potatoes? Please output segmentation mask. ASSISTANT: when eating scrambled eggs for breakfast, people often add a side dish made of potatoes. what item in the picture can be used to serve the potatoes</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1000, 667])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1000, 667])):  [tensor([[[-9.2500, -9.2534, -9.3550,  ..., -4.8212, -4.4053, -4.5141],
         [-9.2522, -9.2556, -9.3574,  ..., -4.8238, -4.4072, -4.5160],
         [-9.3200, -9.3235, -9.4299,  ..., -4.9010, -4.4671, -4.5711],
         ...,
         [-6.7013, -6.7080, -6.9114,  ..., -7.5690, -6.6548, -6.4999],
         [-6.7182, -6.7243, -6.9070,  ..., -7.4556, -6.5436, -6.3731],
         [-6.7188, -6.7248, -6.9069,  ..., -7.4519, -6.5399, -6.3689]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that can control the fan speed in this image? Please output segmentation mask. ASSISTANT: something that can control the fan speed</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2304, 3456])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2304, 3456])):  [tensor([[[-9.7500, -9.7500, -9.7500,  ..., -9.6875, -9.6875, -9.6875],
         [-9.7500, -9.7500, -9.7500,  ..., -9.6875, -9.6875, -9.6875],
         [-9.7500, -9.7500, -9.7500,  ..., -9.6875, -9.6875, -9.6875],
         ...,
         [-7.4217, -7.4217, -7.4217,  ..., -4.6185, -4.6185, -4.6185],
         [-7.5234, -7.5234, -7.5234,  ..., -4.6016, -4.6016, -4.6016],
         [-7.5234, -7.5234, -7.5234,  ..., -4.6016, -4.6016, -4.6016]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What structure separates two areas in the room and is commonly used to hold onto for support when going up and down? Please output segmentation mask. ASSISTANT: what structure separates two areas in the room and is commonly used to hold onto for support when going up and down</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[75]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[255.,   1.,   1.,  ...,   0.,   0.,   0.],
         [255.,   1.,   1.,  ...,   0.,   0.,   0.],
         [255., 255.,   1.,  ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-4.7445, -4.5506, -4.5600,  ..., -5.4980, -5.4387, -5.7985],
         [-4.6366, -4.4131, -4.4203,  ..., -5.3906, -5.3313, -5.3957],
         [-4.5596, -4.3984, -4.3047,  ..., -5.2969, -5.2734, -5.3271],
         ...,
         [-1.4065, -1.4227, -1.2344,  ..., -3.9453, -3.8859, -3.7678],
         [-1.5823, -1.6420, -1.4238,  ..., -3.8938, -3.8170, -3.8203],
         [-2.0674, -2.0547, -1.9731,  ..., -3.9631, -3.7470, -3.6728]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In urban areas, there are designated areas for bicycles to ride safely. What area in the picture would a cyclist use to navigate through the city? Please output segmentation mask. ASSISTANT: in urban areas, there are designated areas for bicycles to ride safely. what area in the picture would a cyclist use to navigate through the city</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[83]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[ -7.9062,  -7.9062,  -7.9062,  ..., -11.7500, -11.7500, -11.7500],
         [ -7.9062,  -7.9062,  -7.9062,  ..., -11.7500, -11.7500, -11.7500],
         [ -7.9062,  -7.9062,  -7.9062,  ..., -11.7500, -11.7500, -11.7500],
         ...,
         [ -6.4727,  -6.4727,  -6.4727,  ...,  -4.2136,  -4.2136,  -4.2136],
         [ -6.6523,  -6.6523,  -6.6523,  ...,  -4.3906,  -4.3906,  -4.3906],
         [ -6.6523,  -6.6523,  -6.6523,  ...,  -4.3906,  -4.3906,  -4.3906]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a mechanical workshop, there are various machines and tools used for different purposes. What in the picture could be used to rotate or spin other parts or objects? Please output segmentation mask. ASSISTANT: in a mechanical workshop, there are various machines and tools used for different purposes. what in the picture could be used to rotate or spin other parts or objects</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1606, 1610])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1606, 1610])):  [tensor([[[-4.3750, -4.3750, -4.3768,  ..., -4.6819, -4.6875, -4.6875],
         [-4.3750, -4.3750, -4.3768,  ..., -4.6819, -4.6875, -4.6875],
         [-4.3750, -4.3750, -4.3768,  ..., -4.6816, -4.6872, -4.6872],
         ...,
         [-3.8915, -3.8915, -3.9041,  ..., -4.2259, -4.2262, -4.2262],
         [-3.9411, -3.9411, -3.9530,  ..., -4.1588, -4.1592, -4.1592],
         [-3.9766, -3.9766, -3.9879,  ..., -4.1110, -4.1113, -4.1113]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the boxes inside the refrigerator in this image? Please output segmentation mask. ASSISTANT: the boxes inside the refrigerator</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2048, 1536])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2048, 1536])):  [tensor([[[-3.4531, -3.4531, -3.4531,  ..., -5.2070, -5.2617, -5.2891],
         [-3.4531, -3.4531, -3.4531,  ..., -5.2070, -5.2617, -5.2891],
         [-3.4531, -3.4531, -3.4531,  ..., -5.2070, -5.2617, -5.2891],
         ...,
         [-3.2188, -3.2188, -3.2188,  ..., -2.4023, -2.5977, -2.6953],
         [-3.2188, -3.2188, -3.2188,  ..., -2.4023, -2.5977, -2.6953],
         [-3.2188, -3.2188, -3.2188,  ..., -2.4023, -2.5977, -2.6953]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the stronger Mario in this image? Please output segmentation mask. ASSISTANT: the stronger mario</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[60]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2448, 3264])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2448, 3264])):  [tensor([[[ -9.3750,  -9.3750,  -9.3750,  ..., -12.5000, -12.5000, -12.5000],
         [ -9.3750,  -9.3750,  -9.3750,  ..., -12.5000, -12.5000, -12.5000],
         [ -9.3750,  -9.3750,  -9.3750,  ..., -12.5000, -12.5000, -12.5000],
         ...,
         [ -5.7806,  -5.7806,  -5.7806,  ...,  -6.9550,  -6.9550,  -6.9550],
         [ -5.8828,  -5.8828,  -5.8828,  ...,  -6.9883,  -6.9883,  -6.9883],
         [ -5.8828,  -5.8828,  -5.8828,  ...,  -6.9883,  -6.9883,  -6.9883]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is where to wash hands in this image? Please output segmentation mask. ASSISTANT: where to wash hands</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 5953, 4793])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 5953, 4793])):  [tensor([[[-7.4688, -7.4688, -7.4688,  ..., -5.6836, -5.6836, -5.6836],
         [-7.4688, -7.4688, -7.4688,  ..., -5.6836, -5.6836, -5.6836],
         [-7.4688, -7.4688, -7.4688,  ..., -5.6836, -5.6836, -5.6836],
         ...,
         [-5.1562, -5.1562, -5.1562,  ..., -4.6992, -4.6992, -4.6992],
         [-5.1562, -5.1562, -5.1562,  ..., -4.6992, -4.6992, -4.6992],
         [-5.1562, -5.1562, -5.1562,  ..., -4.6992, -4.6992, -4.6992]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the food that the bird likes to eat in this image? Please output segmentation mask. ASSISTANT: the food that the bird likes to eat</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]


 78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                | 155/200 [00:57<00:11,  3.77it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[-9.2500, -9.2500, -9.2602,  ..., -8.7344, -8.7500, -8.7500],
         [-9.2500, -9.2500, -9.2602,  ..., -8.7344, -8.7500, -8.7500],
         [-9.2437, -9.2437, -9.2539,  ..., -8.7221, -8.7375, -8.7375],
         ...,
         [-7.1453, -7.1453, -7.1521,  ..., -5.6002, -5.6043, -5.6043],
         [-7.2994, -7.2994, -7.3053,  ..., -5.4975, -5.5006, -5.5006],
         [-7.4180, -7.4180, -7.4232,  ..., -5.4155, -5.4180, -5.4180]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a large building, it is common to have designated areas for swimming. What area in the picture could be used for swimming? Please output segmentation mask. ASSISTANT: in a large building, it is common to have designated areas for swimming. what area in the picture could be used for swimming</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[79]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[-9.6875, -9.6875, -9.6875,  ..., -4.5625, -4.5625, -4.5625],
         [-9.6875, -9.6875, -9.6875,  ..., -4.5625, -4.5625, -4.5625],
         [-9.6875, -9.6875, -9.6875,  ..., -4.5625, -4.5625, -4.5625],
         ...,
         [-2.8714, -2.8714, -2.8714,  ..., -2.1384, -2.1384, -2.1384],
         [-3.4259, -3.4259, -3.4259,  ..., -2.5283, -2.5283, -2.5283],
         [-3.6426, -3.6426, -3.6426,  ..., -2.6807, -2.6807, -2.6807]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In order to shape and carve hard materials, such as wood or stone, what part of the tool in the picture would be used to provide precise control and force? Please output segmentation mask. ASSISTANT: in order to shape and carve hard materials, such as wood or stone, what part of the tool in the picture would be used to provide precise control and force</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 790, 916])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 790, 916])):  [tensor([[[ -9.6875,  -9.6999,  -9.7993,  ...,  -9.9636, -10.1070, -10.1250],
         [ -9.6861,  -9.6988,  -9.8001,  ...,  -9.9787, -10.1198, -10.1374],
         [ -9.6751,  -9.6897,  -9.8066,  ..., -10.0999, -10.2215, -10.2367],
         ...,
         [ -9.1381,  -9.1889,  -9.5944,  ...,  -6.0352,  -6.0459,  -6.0472],
         [ -6.8329,  -6.8555,  -7.0359,  ...,  -5.2073,  -5.2010,  -5.2002],
         [ -5.5754,  -5.5807,  -5.6230,  ...,  -4.6800,  -4.6444,  -4.6399]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In the picture, there is a legal requirement for vehicles to display identifying information. What part of the car is used to display this information? Please output segmentation mask. ASSISTANT: in the picture, there is a legal requirement for vehicles to display identifying information. what part of the car is used to display this information</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[80]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2762, 3948])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2762, 3948])):  [tensor([[[ -9.6875,  -9.6875,  -9.6875,  ..., -11.1250, -11.1250, -11.1250],
         [ -9.6875,  -9.6875,  -9.6875,  ..., -11.1250, -11.1250, -11.1250],
         [ -9.6875,  -9.6875,  -9.6875,  ..., -11.1250, -11.1250, -11.1250],
         ...,
         [ -9.8247,  -9.8247,  -9.8247,  ...,  -4.8740,  -4.8740,  -4.8740],
         [ -9.9219,  -9.9219,  -9.9219,  ...,  -4.9492,  -4.9492,  -4.9492],
         [ -9.9219,  -9.9219,  -9.9219,  ...,  -4.9492,  -4.9492,  -4.9492]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the ship that is most likely to carry a fleet commander in this image? Please output segmentation mask. ASSISTANT: the ship that is most likely to carry a fleet commander</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2448, 3264])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2448, 3264])):  [tensor([[[-10.6250, -10.6250, -10.6250,  ..., -10.8750, -10.8750, -10.8750],
         [-10.6250, -10.6250, -10.6250,  ..., -10.8750, -10.8750, -10.8750],
         [-10.6250, -10.6250, -10.6250,  ..., -10.8750, -10.8750, -10.8750],
         ...,
         [ -8.7653,  -8.7653,  -8.7653,  ...,  -3.9263,  -3.9263,  -3.9263],
         [ -8.9297,  -8.9297,  -8.9297,  ...,  -4.2051,  -4.2051,  -4.2051],
         [ -8.9297,  -8.9297,  -8.9297,  ...,  -4.2051,  -4.2051,  -4.2051]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In outdoor recreational activities, people can experience the thrill of flying in the sky with the help of equipment. What object in the picture can provide this experience? Please output segmentation mask. ASSISTANT: in outdoor recreational activities, people can experience the thrill of flying in the sky with the help of equipment. what object in the picture can provide this experience</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[-6.5625, -6.5658, -6.5813,  ..., -6.0094, -6.0016, -6.0000],
         [-6.5527, -6.5567, -6.5758,  ..., -6.0449, -6.0376, -6.0360],
         [-6.5063, -6.5140, -6.5503,  ..., -6.2126, -6.2072, -6.2061],
         ...,
         [-1.9626, -1.9210, -1.7249,  ..., -0.2104, -0.1814, -0.1753],
         [-2.0525, -2.0081, -1.7984,  ..., -0.5997, -0.5243, -0.5083],
         [-2.1425, -2.0952, -1.8719,  ..., -0.9890, -0.8671, -0.8413]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n We are currently watching a game and it's halftime. Who are the cheerleaders who come out to liven up the atmosphere? Please output segmentation mask. ASSISTANT: we are currently watching a game and it's halftime. who are the cheerleaders who come out to liven up the atmosphere</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]

 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                            | 160/200 [00:59<00:12,  3.15it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 272, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 272, 1024])):  [tensor([[[-9.3125, -9.3125, -9.4297,  ..., -9.8047, -9.9375, -9.9375],
         [-9.3125, -9.3125, -9.4297,  ..., -9.8047, -9.9375, -9.9375],
         [-9.4062, -9.4062, -9.5186,  ..., -9.8564, -9.9766, -9.9766],
         ...,
         [-6.8555, -6.8555, -6.9712,  ..., -6.5591, -6.4023, -6.4023],
         [-6.9102, -6.9102, -7.0166,  ..., -6.4595, -6.3164, -6.3164],
         [-7.1055, -7.1055, -7.1904,  ..., -6.2065, -6.0742, -6.0742]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In an educational setting, children often use different materials to learn about letters, numbers, and words. What object in the picture could be used as a visual aid for learning about letters and words? Please output segmentation mask. ASSISTANT: in an educational setting, children often use different materials to learn about letters, numbers, and words. what object in the picture could be used as a visual aid for learning about letters and words</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[90]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1274, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1274, 2048])):  [tensor([[[-12.1250, -12.1250, -12.1250,  ..., -12.6250, -12.6250, -12.6250],
         [-12.1250, -12.1250, -12.1250,  ..., -12.6250, -12.6250, -12.6250],
         [-12.1250, -12.1250, -12.1250,  ..., -12.6250, -12.6250, -12.6250],
         ...,
         [ -7.9082,  -7.9082,  -7.9082,  ...,  -5.2871,  -5.2871,  -5.2871],
         [ -8.2480,  -8.2480,  -8.2480,  ...,  -5.3066,  -5.3066,  -5.3066],
         [ -8.4180,  -8.4180,  -8.4180,  ...,  -5.3164,  -5.3164,  -5.3164]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is a car with a color that is closer to lipstick color in this image? Please output segmentation mask. ASSISTANT: a car with a color that is closer to lipstick color</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[-4.6875, -4.6875, -4.6840,  ..., -8.2391, -8.2500, -8.2500],
         [-4.6875, -4.6875, -4.6840,  ..., -8.2391, -8.2500, -8.2500],
         [-4.6852, -4.6852, -4.6816,  ..., -8.2334, -8.2441, -8.2441],
         ...,
         [-8.0309, -8.0309, -8.0579,  ..., -5.5894, -5.5926, -5.5926],
         [-7.9569, -7.9569, -7.9816,  ..., -5.5833, -5.5856, -5.5856],
         [-7.9102, -7.9102, -7.9331,  ..., -5.5805, -5.5820, -5.5820]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If you want to play table tennis indoors, what furniture in the picture should be used as the playing surface? Please output segmentation mask. ASSISTANT: if you want to play table tennis indoors, what furniture in the picture should be used as the playing surface</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0.,  ..., 255., 255., 255.],
         [  0.,   0.,   0.,  ..., 255., 255., 255.],
         [  0.,   0.,   0.,  ..., 255., 255., 255.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-4.0631, -4.2766, -4.1845,  ..., -3.4722, -3.4006, -3.5372],
         [-3.8791, -4.0188, -4.0180,  ..., -3.2305, -3.1761, -3.1428],
         [-3.8385, -3.9781, -3.9375,  ..., -3.1641, -3.1914, -3.1646],
         ...,
         [-4.1322, -4.5297, -4.1992,  ..., -1.8359, -1.7922, -1.8083],
         [-4.1700, -4.5803, -4.2367,  ..., -1.9277, -1.8468, -1.9021],
         [-3.9786, -4.4343, -4.1366,  ..., -2.0611, -1.9111, -1.9526]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a military airfield, what area is specifically designed for aircraft to take off and land? Please output segmentation mask. ASSISTANT: in a military airfield, what area is specifically designed for aircraft to take off and land</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2848, 4288])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2848, 4288])):  [tensor([[[-11.1250, -11.1250, -11.1250,  ...,  -8.6875,  -8.6875,  -8.6875],
         [-11.1250, -11.1250, -11.1250,  ...,  -8.6875,  -8.6875,  -8.6875],
         [-11.1250, -11.1250, -11.1250,  ...,  -8.6875,  -8.6875,  -8.6875],
         ...,
         [ -8.3684,  -8.3684,  -8.3684,  ...,  -6.1232,  -6.1232,  -6.1232],
         [ -8.5039,  -8.5039,  -8.5039,  ...,  -6.1641,  -6.1641,  -6.1641],
         [ -8.5039,  -8.5039,  -8.5039,  ...,  -6.1641,  -6.1641,  -6.1641]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Generally speaking, dogs do not have horns on their heads, only a pair of ears. What part of the dog's head in this picture looks strange? Please output segmentation mask. ASSISTANT: generally speaking, dogs do not have horns on their heads, only a pair of ears. what part of the dog's head in this picture looks strange</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]

 82%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                         | 165/200 [01:01<00:10,  3.29it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1533, 1039])):  [tensor([[[  0.,   0.,   0.,  ..., 255., 255.,   0.],
         [  0.,   0.,   0.,  ..., 255., 255.,   0.],
         [  0.,   0.,   0.,  ..., 255.,   0.,   0.],
         ...,
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1533, 1039])):  [tensor([[[-5.7188, -5.7188, -5.7267,  ..., -3.0658, -3.1833, -3.2715],
         [-5.7188, -5.7188, -5.7267,  ..., -3.0658, -3.1833, -3.2715],
         [-5.7334, -5.7334, -5.7415,  ..., -3.0903, -3.2110, -3.3018],
         ...,
         [-4.9688, -4.9688, -4.9954,  ..., -2.4452, -2.5172, -2.5714],
         [-4.9688, -4.9688, -4.9953,  ..., -2.4424, -2.5154, -2.5703],
         [-4.9688, -4.9688, -4.9953,  ..., -2.4424, -2.5154, -2.5703]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In an organized workspace, one might have a designated area to store important documents and files. What piece of furniture in the picture could be used for this purpose? Please output segmentation mask. ASSISTANT: in an organized workspace, one might have a designated area to store important documents and files. what piece of furniture in the picture could be used for this purpose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 750, 1000])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 750, 1000])):  [tensor([[[-4.1875, -4.1875, -4.1875,  ..., -3.0447, -3.0468, -3.0469],
         [-4.1885, -4.1885, -4.1885,  ..., -3.0437, -3.0456, -3.0457],
         [-4.2181, -4.2181, -4.2175,  ..., -3.0139, -3.0098, -3.0097],
         ...,
         [-3.8162, -3.8194, -3.9150,  ..., -2.7581, -2.7633, -2.7634],
         [-3.7409, -3.7439, -3.8326,  ..., -2.7242, -2.7260, -2.7260],
         [-3.6534, -3.6557, -3.7263,  ..., -2.6473, -2.6437, -2.6436]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that ensures the person to land safely in this image? Please output segmentation mask. ASSISTANT: something that ensures the person to land safely</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1920, 2560])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1920, 2560])):  [tensor([[[-10.2500, -10.2500, -10.2500,  ..., -10.6875, -10.6875, -10.6875],
         [-10.2500, -10.2500, -10.2500,  ..., -10.6875, -10.6875, -10.6875],
         [-10.2500, -10.2500, -10.2500,  ..., -10.6875, -10.6875, -10.6875],
         ...,
         [ -8.4531,  -8.4531,  -8.4531,  ...,  -4.4141,  -4.4141,  -4.4141],
         [ -8.3594,  -8.3594,  -8.3594,  ...,  -4.4922,  -4.4922,  -4.4922],
         [ -8.3359,  -8.3359,  -8.3359,  ...,  -4.5117,  -4.5117,  -4.5117]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n A bride and groom often walk together down the aisle during a wedding ceremony. What object in the picture is the bride most likely holding during this moment? Please output segmentation mask. ASSISTANT: a bride and groom often walk together down the aisle during a wedding ceremony. what object in the picture is the bride most likely holding during this moment</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 900])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 900])):  [tensor([[[-8.3750, -8.3750, -8.3948,  ..., -8.9148, -8.8987, -8.7891],
         [-8.3750, -8.3750, -8.3948,  ..., -8.9148, -8.8987, -8.7891],
         [-8.3997, -8.3997, -8.4203,  ..., -8.8715, -8.8626, -8.7674],
         ...,
         [-7.2986, -7.2986, -7.5708,  ..., -5.3507, -5.6730, -6.0279],
         [-7.2812, -7.2812, -7.5361,  ..., -5.3697, -5.6844, -6.0195],
         [-7.2812, -7.2812, -7.5361,  ..., -5.3697, -5.6844, -6.0195]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Some dishes are baked in the oven to enhance their flavors. What object in the picture is commonly used to place the dishes in the oven for baking? Please output segmentation mask. ASSISTANT: some dishes are baked in the oven to enhance their flavors. what object in the picture is commonly used to place the dishes in the oven for baking</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[89]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 900, 1200])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 900, 1200])):  [tensor([[[-6.1250, -6.1250, -6.0632,  ..., -6.3979, -6.4375, -6.4375],
         [-6.1250, -6.1250, -6.0632,  ..., -6.3979, -6.4375, -6.4375],
         [-6.1027, -6.1027, -6.0472,  ..., -6.3742, -6.4103, -6.4103],
         ...,
         [-4.9932, -4.9932, -5.0092,  ..., -4.1260, -4.1348, -4.1348],
         [-4.9694, -4.9694, -4.9845,  ..., -3.9537, -3.9572, -3.9572],
         [-4.9023, -4.9023, -4.9150,  ..., -3.6826, -3.6738, -3.6738]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the object that might reflect this person's marital status in this image? Please output segmentation mask. ASSISTANT: the object that might reflect this person's marital status</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[69]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1066, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1066, 1600])):  [tensor([[[-9.5000, -9.5000, -9.5094,  ..., -3.7781, -3.7812, -3.7812],
         [-9.5000, -9.5000, -9.5094,  ..., -3.7781, -3.7812, -3.7812],
         [-9.4977, -9.4977, -9.5070,  ..., -3.7703, -3.7733, -3.7733],
         ...,
         [-8.8000, -8.8000, -8.8079,  ..., -2.4372, -2.4343, -2.4343],
         [-8.7800, -8.7800, -8.7845,  ..., -2.3964, -2.3919, -2.3919],
         [-8.7656, -8.7656, -8.7676,  ..., -2.3671, -2.3613, -2.3613]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is knobs without screws in the center in this image? Please output segmentation mask. ASSISTANT: knobs without screws in the center</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]

 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                     | 170/200 [01:03<00:10,  2.98it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 615, 461])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 615, 461])):  [tensor([[[-7.0938, -7.1484, -7.3305,  ..., -6.8934, -7.0234, -6.9804],
         [-7.1639, -7.2346, -7.4703,  ..., -6.9637, -7.0726, -7.0153],
         [-7.3979, -7.5221, -7.9364,  ..., -7.1981, -7.2367, -7.1318],
         ...,
         [-6.1757, -6.7642, -8.7264,  ..., -3.9852, -3.8706, -4.0050],
         [-6.1367, -6.6327, -8.2865,  ..., -3.9225, -3.9161, -3.9881],
         [-6.1250, -6.5932, -8.1546,  ..., -3.9037, -3.9297, -3.9831]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is where the garbage should be put in this image? Please output segmentation mask. ASSISTANT: where the garbage should be put</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[-3.4688, -3.4688, -3.4688,  ..., -3.6562, -3.6562, -3.6562],
         [-3.4688, -3.4688, -3.4688,  ..., -3.6562, -3.6562, -3.6562],
         [-3.4688, -3.4688, -3.4688,  ..., -3.6562, -3.6562, -3.6562],
         ...,
         [-3.3584, -3.3584, -3.3584,  ..., -1.4374, -1.4374, -1.4374],
         [-3.3275, -3.3275, -3.3275,  ..., -1.5593, -1.5593, -1.5593],
         [-3.3203, -3.3203, -3.3203,  ..., -1.5879, -1.5879, -1.5879]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Dogs are faithful companions to humans, and humans often play fetch games with them. What object will the dog likely retrieve and bring back to the human for the next round of fetch in the picture? Please output segmentation mask. ASSISTANT: dogs are faithful companions to humans, and humans often play fetch games with them. what object will the dog likely retrieve and bring back to the human for the next round of fetch in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[92]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1816, 3003])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1816, 3003])):  [tensor([[[-8.2500, -8.2500, -8.2500,  ..., -8.3750, -8.3750, -8.3750],
         [-8.2500, -8.2500, -8.2500,  ..., -8.3750, -8.3750, -8.3750],
         [-8.2500, -8.2500, -8.2500,  ..., -8.3750, -8.3750, -8.3750],
         ...,
         [-5.1028, -5.1028, -5.1028,  ..., -2.8784, -2.8784, -2.8784],
         [-5.1281, -5.1281, -5.1281,  ..., -2.8751, -2.8751, -2.8751],
         [-5.1289, -5.1289, -5.1289,  ..., -2.8750, -2.8750, -2.8750]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n People often add gas to water to improve its taste. What part of the picture will be consumed as a beverage? Please output segmentation mask. ASSISTANT: people often add gas to water to improve its taste. what part of the picture will be consumed as a beverage</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2331, 1749])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2331, 1749])):  [tensor([[[-6.9062, -6.9062, -6.9062,  ..., -7.1661, -7.2622, -7.2969],
         [-6.9062, -6.9062, -6.9062,  ..., -7.1661, -7.2622, -7.2969],
         [-6.9062, -6.9062, -6.9062,  ..., -7.1661, -7.2622, -7.2969],
         ...,
         [-4.2812, -4.2812, -4.2812,  ..., -3.4593, -3.6069, -3.6602],
         [-4.2812, -4.2812, -4.2812,  ..., -3.4593, -3.6069, -3.6602],
         [-4.2812, -4.2812, -4.2812,  ..., -3.4593, -3.6069, -3.6602]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When people want to take their pets to the park and enjoy some outdoor activities, they often want to make sure their pets are safe and cannot run away. What in the picture could help with this? Please output segmentation mask. ASSISTANT: when people want to take their pets to the park and enjoy some outdoor activities, they often want to make sure their pets are safe and cannot run away. what in the picture could help with this</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[94]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2705, 3269])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2705, 3269])):  [tensor([[[-9.5625, -9.5625, -9.5625,  ..., -2.2031, -2.2031, -2.2031],
         [-9.5625, -9.5625, -9.5625,  ..., -2.2031, -2.2031, -2.2031],
         [-9.5625, -9.5625, -9.5625,  ..., -2.2031, -2.2031, -2.2031],
         ...,
         [-7.4292, -7.4292, -7.4292,  ..., -5.6981, -5.6981, -5.6981],
         [-7.4922, -7.4922, -7.4922,  ..., -5.7500, -5.7500, -5.7500],
         [-7.4922, -7.4922, -7.4922,  ..., -5.7500, -5.7500, -5.7500]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n I feel my commute is too slow now and I'm hoping to find a convenient mode of transportation that can also help me exercise. Can you help me find the corresponding part in the picture? Please output segmentation mask. ASSISTANT: i feel my commute is too slow now and i'm hoping to find a convenient mode of transportation that can also help me exercise. can you help me find the corresponding part in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[92]]

 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                 | 175/200 [01:05<00:08,  3.02it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1189, 1656])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1189, 1656])):  [tensor([[[-8.5000, -8.5000, -8.5022,  ..., -2.9196, -2.9219, -2.9219],
         [-8.5000, -8.5000, -8.5022,  ..., -2.9196, -2.9219, -2.9219],
         [-8.4989, -8.4989, -8.5011,  ..., -2.9130, -2.9152, -2.9152],
         ...,
         [-4.9879, -4.9879, -4.9875,  ..., -6.2202, -6.2202, -6.2202],
         [-5.2156, -5.2156, -5.2154,  ..., -6.2338, -6.2338, -6.2338],
         [-5.3750, -5.3750, -5.3751,  ..., -6.2421, -6.2422, -6.2422]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If we were to have a friendly competition to see who can paddle the fastest in the water, what would be the most suitable vessels for this activity in the picture? Please output segmentation mask. ASSISTANT: if we were to have a friendly competition to see who can paddle the fastest in the water, what would be the most suitable vessels for this activity in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[86]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2112, 2816])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2112, 2816])):  [tensor([[[-13.9375, -13.9375, -13.9375,  ..., -16.3750, -16.3750, -16.3750],
         [-13.9375, -13.9375, -13.9375,  ..., -16.3750, -16.3750, -16.3750],
         [-13.9375, -13.9375, -13.9375,  ..., -16.3750, -16.3750, -16.3750],
         ...,
         [ -9.8977,  -9.8977,  -9.8977,  ...,  -6.5568,  -6.5568,  -6.5568],
         [ -9.8636,  -9.8636,  -9.8636,  ...,  -6.7529,  -6.7529,  -6.7529],
         [ -9.8594,  -9.8594,  -9.8594,  ...,  -6.7773,  -6.7773,  -6.7773]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the man who seems to lose in this image? Please output segmentation mask. ASSISTANT: the man who seems to lose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1000, 1500])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1000, 1500])):  [tensor([[[-9.4375, -9.4375, -9.4423,  ..., -2.9743, -2.9844, -2.9844],
         [-9.4375, -9.4375, -9.4423,  ..., -2.9743, -2.9844, -2.9844],
         [-9.4294, -9.4294, -9.4345,  ..., -2.9520, -2.9617, -2.9617],
         ...,
         [-4.9505, -4.9505, -4.9403,  ..., -3.1826, -3.1767, -3.1767],
         [-5.1734, -5.1734, -5.1581,  ..., -3.2086, -3.2006, -3.2006],
         [-5.4336, -5.4336, -5.4160,  ..., -3.2632, -3.2539, -3.2539]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a dark cave, there is no natural light source. What object in the picture can be used to provide light to navigate and explore the cave? Please output segmentation mask. ASSISTANT: in a dark cave, there is no natural light source. what object in the picture can be used to provide light to navigate and explore the cave</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[-9.3125, -9.3125, -9.3125,  ..., -9.3125, -9.5547, -9.6250],
         [-9.2633, -9.2720, -9.3020,  ..., -9.1789, -9.3939, -9.4562],
         [-9.0938, -9.1324, -9.2656,  ..., -8.7188, -8.8398, -8.8750],
         ...,
         [-5.9531, -6.0779, -6.5078,  ..., -4.0312, -4.1281, -4.1562],
         [-6.0125, -6.1456, -6.6041,  ..., -4.0434, -4.2365, -4.2926],
         [-6.5531, -6.6607, -7.0313,  ..., -4.5969, -4.6598, -4.6781]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the car that may need repair in this image? Please output segmentation mask. ASSISTANT: the car that may need repair</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1632, 2176])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1632, 2176])):  [tensor([[[-8.9375, -8.9375, -8.9375,  ..., -9.4375, -9.4375, -9.4375],
         [-8.9375, -8.9375, -8.9375,  ..., -9.4375, -9.4375, -9.4375],
         [-8.9375, -8.9375, -8.9375,  ..., -9.4375, -9.4375, -9.4375],
         ...,
         [-7.7408, -7.7408, -7.7408,  ..., -4.3745, -4.3745, -4.3745],
         [-7.8070, -7.8070, -7.8070,  ..., -4.6154, -4.6154, -4.6154],
         [-7.8359, -7.8359, -7.8359,  ..., -4.7207, -4.7207, -4.7207]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In ancient times, people used different methods to measure time during the day. What object in the picture could have been used as a timekeeping device based on the position of the sun? Please output segmentation mask. ASSISTANT: in ancient times, people used different methods to measure time during the day. what object in the picture could have been used as a timekeeping device based on the position of the sun</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[89]]

 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋              | 180/200 [01:07<00:06,  2.87it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-9.1875, -9.2006, -9.2625,  ..., -8.1844, -8.3932, -8.4375],
         [-9.2006, -9.2160, -9.2884,  ..., -8.1791, -8.3896, -8.4342],
         [-9.2625, -9.2884, -9.4106,  ..., -8.1544, -8.3725, -8.4187],
         ...,
         [-5.2938, -5.3197, -5.4419,  ..., -3.4030, -3.4211, -3.4250],
         [-5.3905, -5.4175, -5.5451,  ..., -3.4944, -3.5141, -3.5183],
         [-5.9625, -5.9882, -6.1091,  ..., -3.9297, -3.9186, -3.9162]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a busy and bustling city, it is important for businesses to have eye-catching signs to attract customers. What feature in the picture can serve this purpose? Please output segmentation mask. ASSISTANT: in a busy and bustling city, it is important for businesses to have eye-catching signs to attract customers. what feature in the picture can serve this purpose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2592, 1944])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2592, 1944])):  [tensor([[[-6.6562, -6.6562, -6.6562,  ..., -6.8821, -6.8605, -6.8555],
         [-6.6562, -6.6562, -6.6562,  ..., -6.8821, -6.8605, -6.8555],
         [-6.6562, -6.6562, -6.6562,  ..., -6.8821, -6.8605, -6.8555],
         ...,
         [-3.7344, -3.7344, -3.7344,  ..., -0.8711, -1.1443, -1.2084],
         [-3.7344, -3.7344, -3.7344,  ..., -0.8711, -1.1443, -1.2084],
         [-3.7344, -3.7344, -3.7344,  ..., -0.8711, -1.1443, -1.2084]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When taking pictures with a camera, what part of the camera is responsible for focusing the image and adjusting the depth of field? Please output segmentation mask. ASSISTANT: when taking pictures with a camera, what part of the camera is responsible for focusing the image and adjusting the depth of field</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1664, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1664, 2048])):  [tensor([[[-8.0000, -8.0000, -8.0000,  ..., -8.8125, -8.8125, -8.8125],
         [-8.0000, -8.0000, -8.0000,  ..., -8.8125, -8.8125, -8.8125],
         [-8.0000, -8.0000, -8.0000,  ..., -8.8125, -8.8125, -8.8125],
         ...,
         [-8.5176, -8.5176, -8.5176,  ..., -6.3057, -6.3057, -6.3057],
         [-8.1543, -8.1543, -8.1543,  ..., -5.9053, -5.9053, -5.9053],
         [-7.9727, -7.9727, -7.9727,  ..., -5.7051, -5.7051, -5.7051]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Birds often need a place to rest or observe their surroundings. What part of a tree in the picture offers a suitable spot for birds to do so? Please output segmentation mask. ASSISTANT: birds often need a place to rest or observe their surroundings. what part of a tree in the picture offers a suitable spot for birds to do so</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 612, 612])):  [tensor([[[  0.,   0.,   0.,  ..., 255., 255., 255.],
         [  0.,   0.,   0.,  ..., 255., 255., 255.],
         [  0.,   0.,   0.,  ..., 255., 255., 255.],
         ...,
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 612, 612])):  [tensor([[[-6.6562, -6.6801, -6.7586,  ..., -6.2583, -6.6112, -6.7188],
         [-6.6483, -6.6773, -6.7723,  ..., -6.2489, -6.5602, -6.6550],
         [-6.6221, -6.6678, -6.8175,  ..., -6.2181, -6.3927, -6.4459],
         ...,
         [-7.9006, -8.2240, -9.2852,  ..., -8.0385, -8.0765, -8.0881],
         [-7.8091, -8.1075, -9.0867,  ..., -7.9328, -7.9817, -7.9966],
         [-7.7812, -8.0720, -9.0262,  ..., -7.9005, -7.9528, -7.9688]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that we can sit on in this image? Please output segmentation mask. ASSISTANT: something that we can sit on</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 540, 720])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 540, 720])):  [tensor([[[-5.0938, -5.1135, -5.1910,  ..., -5.4010, -5.6043, -5.6562],
         [-5.1036, -5.1234, -5.2009,  ..., -5.3792, -5.5664, -5.6142],
         [-5.1424, -5.1622, -5.2396,  ..., -5.2937, -5.4179, -5.4497],
         ...,
         [-6.3038, -6.4189, -6.8689,  ..., -3.3298, -3.3720, -3.3828],
         [-6.1802, -6.2974, -6.7558,  ..., -3.4268, -3.5067, -3.5271],
         [-5.8260, -5.9163, -6.2692,  ..., -3.5719, -3.6282, -3.6425]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n To keep bread fresh and protected, it is often placed in a protective covering. What item in the picture is commonly used for this purpose? Please output segmentation mask. ASSISTANT: to keep bread fresh and protected, it is often placed in a protective covering. what item in the picture is commonly used for this purpose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[80]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 612, 612])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 612, 612])):  [tensor([[[-5.0312, -5.0432, -5.0824,  ..., -4.6505, -4.7028, -4.7188],
         [-5.0113, -5.0314, -5.0973,  ..., -4.6618, -4.7024, -4.7148],
         [-4.9460, -4.9927, -5.1461,  ..., -4.6986, -4.7010, -4.7017],
         ...,
         [-4.8495, -5.0377, -5.6557,  ..., -4.6374, -4.6671, -4.6762],
         [-4.7972, -4.9938, -5.6390,  ..., -4.5625, -4.6196, -4.6369],
         [-4.7812, -4.9804, -5.6340,  ..., -4.5397, -4.6051, -4.6250]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Sometimes, when a car is parked on the side of the road, the driver may accidentally leave the car door open. What part in the picture can indicate that the car door is open? Please output segmentation mask. ASSISTANT: sometimes, when a car is parked on the side of the road, the driver may accidentally leave the car door open. what part in the picture can indicate that the car door is open</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[91]]

 93%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉          | 186/200 [01:09<00:04,  3.17it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2592, 1936])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2592, 1936])):  [tensor([[[-9.3750, -9.3750, -9.3750,  ..., -7.9099, -8.0272, -8.0547],
         [-9.3750, -9.3750, -9.3750,  ..., -7.9099, -8.0272, -8.0547],
         [-9.3750, -9.3750, -9.3750,  ..., -7.9099, -8.0272, -8.0547],
         ...,
         [-8.1875, -8.1875, -8.1875,  ..., -4.7835, -4.8576, -4.8750],
         [-8.1875, -8.1875, -8.1875,  ..., -4.7835, -4.8576, -4.8750],
         [-8.1875, -8.1875, -8.1875,  ..., -4.7835, -4.8576, -4.8750]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that produces pollen in this image? Please output segmentation mask. ASSISTANT: something that produces pollen</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[-10.1250, -10.1545, -10.2937,  ..., -11.3625, -11.7337, -11.8125],
         [-10.1086, -10.1387, -10.2803,  ..., -11.3556, -11.7001, -11.7732],
         [-10.0313, -10.0638, -10.2169,  ..., -11.3232, -11.5414, -11.5877],
         ...,
         [ -8.5433,  -8.6135,  -8.9443,  ...,  -7.2295,  -7.2511,  -7.2556],
         [ -8.0135,  -8.0490,  -8.2166,  ...,  -6.6477,  -6.5802,  -6.5659],
         [ -7.4837,  -7.4846,  -7.4889,  ...,  -6.0659,  -5.9093,  -5.8761]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When purchasing meat from a grocery store, it is often stored and sold in a certain type of container. What object in the picture could be used for this purpose? Please output segmentation mask. ASSISTANT: when purchasing meat from a grocery store, it is often stored and sold in a certain type of container. what object in the picture could be used for this purpose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1920, 2560])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1920, 2560])):  [tensor([[[-6.0312, -6.0312, -6.0312,  ..., -5.5312, -5.5312, -5.5312],
         [-6.0312, -6.0312, -6.0312,  ..., -5.5312, -5.5312, -5.5312],
         [-6.0312, -6.0312, -6.0312,  ..., -5.5312, -5.5312, -5.5312],
         ...,
         [-6.5000, -6.5000, -6.5000,  ..., -4.9688, -4.9688, -4.9688],
         [-6.3750, -6.3750, -6.3750,  ..., -4.8437, -4.8437, -4.8437],
         [-6.3438, -6.3438, -6.3438,  ..., -4.8125, -4.8125, -4.8125]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What object in the picture could be used for defense and firepower in an ancient fort? Please output segmentation mask. ASSISTANT: what object in the picture could be used for defense and firepower in an ancient fort</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[69]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-5.0938, -5.0921, -5.0844,  ..., -4.7906, -4.8602, -4.8750],
         [-5.0839, -5.0834, -5.0809,  ..., -4.8026, -4.8677, -4.8816],
         [-5.0375, -5.0423, -5.0647,  ..., -4.8591, -4.9031, -4.9125],
         ...,
         [-1.6273, -1.6282, -1.6320,  ..., -0.5437, -0.5236, -0.5193],
         [-1.7723, -1.7715, -1.7677,  ..., -0.6523, -0.6176, -0.6102],
         [-2.1123, -2.1090, -2.0933,  ..., -1.1261, -1.0646, -1.0515]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something showing that they are skating in this image? Please output segmentation mask. ASSISTANT: something showing that they are skating</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 580, 435])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 580, 435])):  [tensor([[[-6.6875, -6.6571, -6.5744,  ..., -5.9413, -6.2585, -6.5234],
         [-6.6824, -6.6759, -6.6579,  ..., -6.0061, -6.3167, -6.5678],
         [-6.6686, -6.7269, -6.8855,  ..., -6.1828, -6.4751, -6.6887],
         ...,
         [-4.5372, -4.5281, -4.5034,  ..., -1.8105, -1.8578, -2.2576],
         [-4.5786, -4.5650, -4.5281,  ..., -1.7371, -1.8254, -2.2240],
         [-4.5938, -4.5786, -4.5372,  ..., -1.7101, -1.8136, -2.2116]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Dogs have keen sense of smell, which is why they can be used as drug-sniffing dogs. Which part in the picture gives dogs this characteristic? Please output segmentation mask. ASSISTANT: dogs have keen sense of smell, which is why they can be used as drug-sniffing dogs. which part in the picture gives dogs this characteristic</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[-7.3438, -7.3438, -7.3438,  ..., -6.8750, -6.8750, -6.8750],
         [-7.3438, -7.3438, -7.3438,  ..., -6.8750, -6.8750, -6.8750],
         [-7.3438, -7.3438, -7.3438,  ..., -6.8750, -6.8750, -6.8750],
         ...,
         [-7.6312, -7.6312, -7.6312,  ..., -4.4784, -4.4784, -4.4784],
         [-7.5537, -7.5537, -7.5537,  ..., -4.4996, -4.4996, -4.4996],
         [-7.5234, -7.5234, -7.5234,  ..., -4.5078, -4.5078, -4.5078]]],

 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌      | 191/200 [01:11<00:03,  2.82it/s]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Some birds have long bills that they use to catch food from the water. What part of the bird's body in the picture may have this characteristic? Please output segmentation mask. ASSISTANT: some birds have long bills that they use to catch food from the water. what part of the bird's body in the picture may have this characteristic</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[83]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 800, 1200])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 800, 1200])):  [tensor([[[-10.1250, -10.1250, -10.1794,  ..., -12.1362, -12.2500, -12.2500],
         [-10.1250, -10.1250, -10.1794,  ..., -12.1362, -12.2500, -12.2500],
         [-10.1300, -10.1300, -10.1848,  ..., -12.0499, -12.1558, -12.1558],
         ...,
         [ -7.1428,  -7.1428,  -7.1501,  ...,  -5.1105,  -5.0803,  -5.0803],
         [ -6.8933,  -6.8933,  -6.8616,  ...,  -4.7733,  -4.7263,  -4.7263],
         [ -7.2227,  -7.2227,  -7.1750,  ...,  -4.7470,  -4.6836,  -4.6836]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n After cooking, consuming food, and preparing for food, where can we throw away the rest of the food and scraps? Please output segmentation mask. ASSISTANT: after cooking, consuming food, and preparing for food, where can we throw away the rest of the food and scraps</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[79]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-6.0549, -5.7428, -5.4973,  ..., -3.7943, -3.8800, -3.9622],
         [-5.9577, -6.2069, -6.2063,  ..., -3.6547, -3.8112, -3.6866],
         [-5.5881, -6.1359, -6.3984,  ..., -3.0859, -3.3586, -3.3210],
         ...,
         [-4.5482, -4.6234, -4.0391,  ..., -2.1641, -1.8129, -1.7780],
         [-4.6753, -4.6903, -3.9773,  ..., -1.9090, -1.4163, -1.5376],
         [-5.1920, -5.3110, -4.6681,  ..., -2.2712, -1.7952, -1.9095]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the person who is most likely to be the girl's trainer in this image? Please output segmentation mask. ASSISTANT: the person who is most likely to be the girl's trainer</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[71]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3456, 2304])):  [tensor([[[0., 0., 0.,  ..., 1., 1., 0.],
         [0., 0., 0.,  ..., 1., 1., 0.],
         [0., 0., 0.,  ..., 1., 1., 0.],
         ...,
         [0., 0., 0.,  ..., 1., 1., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 2304])):  [tensor([[[-6.7188, -6.7188, -6.7188,  ..., -1.4406, -1.4932, -1.4932],
         [-6.7188, -6.7188, -6.7188,  ..., -1.4406, -1.4932, -1.4932],
         [-6.7188, -6.7188, -6.7188,  ..., -1.4406, -1.4932, -1.4932],
         ...,
         [-5.5938, -5.5938, -5.5938,  ..., -0.9294, -0.9951, -0.9951],
         [-5.5938, -5.5938, -5.5938,  ..., -0.9294, -0.9951, -0.9951],
         [-5.5938, -5.5938, -5.5938,  ..., -0.9294, -0.9951, -0.9951]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n At a car show, visitors can get close to the displayed vehicles to admire their design and features. What part of the car in this picture is open, allowing viewers to see the engine compartment? Please output segmentation mask. ASSISTANT: at a car show, visitors can get close to the displayed vehicles to admire their design and features. what part of the car in this picture is open, allowing viewers to see the engine compartment</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[93]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2356, 3141])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2356, 3141])):  [tensor([[[-8.3750, -8.3750, -8.3750,  ..., -8.9375, -8.9375, -8.9375],
         [-8.3750, -8.3750, -8.3750,  ..., -8.9375, -8.9375, -8.9375],
         [-8.3750, -8.3750, -8.3750,  ..., -8.9375, -8.9375, -8.9375],
         ...,
         [-8.8473, -8.8473, -8.8473,  ..., -4.9328, -4.9328, -4.9328],
         [-8.7734, -8.7734, -8.7734,  ..., -5.0312, -5.0312, -5.0312],
         [-8.7734, -8.7734, -8.7734,  ..., -5.0312, -5.0312, -5.0312]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When snorkeling or scuba diving, it is important to wear certain equipment to protect the eyes and aid in vision. What object in the picture can be used for this purpose? Please output segmentation mask. ASSISTANT: when snorkeling or scuba diving, it is important to wear certain equipment to protect the eyes and aid in vision. what object in the picture can be used for this purpose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[89]]

 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 196/200 [01:13<00:01,  2.64it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-7.9062, -7.9062, -7.9414,  ..., -8.3750, -8.4375, -8.4375],
         [-7.9062, -7.9062, -7.9414,  ..., -8.3750, -8.4375, -8.4375],
         [-7.8867, -7.8867, -7.9282,  ..., -8.4160, -8.4766, -8.4766],
         ...,
         [-6.8789, -6.8789, -7.0210,  ..., -3.7668, -3.8145, -3.8145],
         [-6.8672, -6.8672, -6.9893,  ..., -3.8992, -3.9414, -3.9414],
         [-6.8516, -6.8516, -6.9365,  ..., -4.0530, -4.0742, -4.0742]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the places for further exploration in this image? Please output segmentation mask. ASSISTANT: the places for further exploration</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-1.6172, -1.6037, -1.5398,  ..., -1.4562, -1.4795, -1.4844],
         [-1.6069, -1.5933, -1.5290,  ..., -1.4390, -1.4599, -1.4643],
         [-1.5586, -1.5444, -1.4777,  ..., -1.3576, -1.3674, -1.3695],
         ...,
         [-1.0836, -1.0846, -1.0896,  ..., -1.1240, -1.1029, -1.0984],
         [-1.1314, -1.1333, -1.1424,  ..., -1.1403, -1.1156, -1.1103],
         [-1.3011, -1.2986, -1.2871,  ..., -1.2671, -1.2406, -1.2350]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When enjoying an ice cream sundae, what can we use to scoop up the whipped cream and place it on top of the ice cream? Please output segmentation mask. ASSISTANT: when enjoying an ice cream sundae, what can we use to scoop up the whipped cream and place it on top of the ice cream</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[86]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2048, 1536])):  [tensor([[[0., 0., 0.,  ..., 1., 1., 1.],
         [0., 0., 0.,  ..., 1., 1., 0.],
         [0., 0., 0.,  ..., 1., 1., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2048, 1536])):  [tensor([[[-5.0312, -5.0312, -5.0312,  ..., -1.9614, -2.4097, -2.6338],
         [-5.0312, -5.0312, -5.0312,  ..., -1.9614, -2.4097, -2.6338],
         [-5.0312, -5.0312, -5.0312,  ..., -1.9614, -2.4097, -2.6338],
         ...,
         [-6.6562, -6.6562, -6.6562,  ..., -5.0000, -5.0000, -5.0000],
         [-6.6562, -6.6562, -6.6562,  ..., -5.0000, -5.0000, -5.0000],
         [-6.6562, -6.6562, -6.6562,  ..., -5.0000, -5.0000, -5.0000]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the person who is speaking currently in this image? Please output segmentation mask. ASSISTANT: the person who is speaking currently</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[ -7.4688,  -7.4688,  -7.4688,  ..., -12.9375, -12.9375, -12.9375],
         [ -7.4688,  -7.4688,  -7.4688,  ..., -12.9375, -12.9375, -12.9375],
         [ -7.4688,  -7.4688,  -7.4688,  ..., -12.9375, -12.9375, -12.9375],
         ...,
         [ -4.4065,  -4.4065,  -4.4065,  ...,  -5.5894,  -5.5894,  -5.5894],
         [ -4.5186,  -4.5186,  -4.5186,  ...,  -5.5820,  -5.5820,  -5.5820],
         [ -4.5186,  -4.5186,  -4.5186,  ...,  -5.5820,  -5.5820,  -5.5820]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the ball that can only be hit into the hole at last in this image? Please output segmentation mask. ASSISTANT: the ball that can only be hit into the hole at last</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[69]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 960, 1280])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 960, 1280])):  [tensor([[[-15.1875, -15.1875, -15.2461,  ..., -15.6055, -15.6250, -15.6250],
         [-15.1875, -15.1875, -15.2461,  ..., -15.6055, -15.6250, -15.6250],
         [-15.1758, -15.1758, -15.2371,  ..., -15.7732, -15.7891, -15.7891],
         ...,
         [-15.0898, -15.0898, -15.4370,  ...,  -6.9159,  -6.9492,  -6.9492],
         [-14.6125, -14.6125, -14.9203,  ...,  -7.1742,  -7.2000,  -7.2000],
         [-14.2734, -14.2734, -14.5361,  ...,  -7.5796,  -7.5938,  -7.5938]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the reflection of the camera in the mirror in this image? Please output segmentation mask. ASSISTANT: the reflection of the camera in the mirror</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [01:15<00:00,  2.64it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2880, 1920])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2880, 1920])):  [tensor([[[-6.2812, -6.2812, -6.2813,  ..., -7.2009, -7.2065, -7.2070],
         [-6.2812, -6.2812, -6.2813,  ..., -7.2009, -7.2065, -7.2070],
         [-6.2812, -6.2812, -6.2813,  ..., -7.2009, -7.2065, -7.2070],
         ...,
         [-8.9375, -8.9375, -8.9375,  ..., -6.4384, -6.4162, -6.4141],
         [-8.9375, -8.9375, -8.9375,  ..., -6.4384, -6.4162, -6.4141],
         [-8.9375, -8.9375, -8.9375,  ..., -6.4384, -6.4162, -6.4141]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In some rural areas, horse-drawn carts are still used for transportation and carrying goods. What is the main source of power that drives the cart in the picture? Please output segmentation mask. ASSISTANT: in some rural areas, horse-drawn carts are still used for transportation and carrying goods. what is the main source of power that drives the cart in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2304, 3456])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2304, 3456])):  [tensor([[[ -8.7500,  -8.7500,  -8.7500,  ..., -10.1250, -10.1250, -10.1250],
         [ -8.7500,  -8.7500,  -8.7500,  ..., -10.1250, -10.1250, -10.1250],
         [ -8.7500,  -8.7500,  -8.7500,  ..., -10.1250, -10.1250, -10.1250],
         ...,
         [ -6.8134,  -6.8134,  -6.8134,  ...,  -4.1190,  -4.1190,  -4.1190],
         [ -6.9180,  -6.9180,  -6.9180,  ...,  -4.1680,  -4.1680,  -4.1680],
         [ -6.9180,  -6.9180,  -6.9180,  ...,  -4.1680,  -4.1680,  -4.1680]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In case of a fire, it is important to have access to fire safety equipment. What object in the picture is specifically designed to store and release fire extinguishing substances? Please output segmentation mask. ASSISTANT: in case of a fire, it is important to have access to fire safety equipment. what object in the picture is specifically designed to store and release fire extinguishing substances</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-5.5312, -5.5613, -5.6901,  ..., -5.9959, -6.2019, -6.2500],
         [-5.5312, -5.5612, -5.6894,  ..., -6.0069, -6.1959, -6.2400],
         [-5.5312, -5.5606, -5.6865,  ..., -6.0541, -6.1700, -6.1971],
         ...,
         [-3.3217, -3.3139, -3.2804,  ..., -2.1643, -2.1428, -2.1377],
         [-3.3165, -3.2908, -3.1810,  ..., -1.9592, -1.8851, -1.8678],
         [-3.4683, -3.4321, -3.2771,  ..., -1.9672, -1.8450, -1.8165]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the solitary tree in this image? Please output segmentation mask. ASSISTANT: the solitary tree</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[-8.3750, -8.3750, -8.3750,  ..., -9.4375, -9.4375, -9.4375],
         [-8.3750, -8.3750, -8.3750,  ..., -9.4375, -9.4375, -9.4375],
         [-8.3750, -8.3750, -8.3750,  ..., -9.4375, -9.4375, -9.4375],
         ...,
         [-5.4531, -5.4531, -5.4531,  ..., -1.7334, -1.7334, -1.7334],
         [-5.6719, -5.6719, -5.6719,  ..., -2.0869, -2.0869, -2.0869],
         [-5.7812, -5.7812, -5.7812,  ..., -2.2637, -2.2637, -2.2637]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something used for contacting other people in this image? Please output segmentation mask. ASSISTANT: something used for contacting other people</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[-5.3750, -5.3750, -5.3750,  ..., -9.1250, -9.1250, -9.1250],
         [-5.3750, -5.3750, -5.3750,  ..., -9.1250, -9.1250, -9.1250],
         [-5.3750, -5.3750, -5.3750,  ..., -9.1250, -9.1250, -9.1250],
         ...,
         [-6.0324, -6.0324, -6.0324,  ..., -5.5438, -5.5438, -5.5438],
         [-6.0007, -6.0007, -6.0007,  ..., -5.5825, -5.5825, -5.5825],
         [-5.9883, -5.9883, -5.9883,  ..., -5.5977, -5.5977, -5.5977]]],
       device='cuda:0')]
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
giou: 0.1971, ciou: 0.1719 | BIO per cls acc: O=1.0000, B=0.5550, I=0.8722
[2025-04-05 00:10:07,017] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step50 is about to be saved!
[34m[1mwandb[39m[22m: [33mWARNING[39m Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 499. Dropping entry: {'val/giou': 0.19710150361061096, 'val/ciou': 0.17185737192630768, 'val/b_acc': 0.5549999722500014, 'val/i_acc': 0.8721639402972337, 'val/o_acc': 0.9999849282592336, '_timestamp': 1743829807.0151749}).
[2025-04-05 00:10:19,689] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./runs/plum-13b_kld_0_dice_4_v1_partonomy/plum-13b_kld_0_dice_4_v1_partonomy_accum_10_maxlen512_epochs50_segloss_2_bce_loss_2_kld_loss_0_dice_loss_4_bidir_bio_exp_seg_ckpt_model/global_step50/mp_rank_00_model_states.pt
[2025-04-05 00:10:19,689] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./runs/plum-13b_kld_0_dice_4_v1_partonomy/plum-13b_kld_0_dice_4_v1_partonomy_accum_10_maxlen512_epochs50_segloss_2_bce_loss_2_kld_loss_0_dice_loss_4_bidir_bio_exp_seg_ckpt_model/global_step50/mp_rank_00_model_states.pt...
[2025-04-05 00:12:01,070] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./runs/plum-13b_kld_0_dice_4_v1_partonomy/plum-13b_kld_0_dice_4_v1_partonomy_accum_10_maxlen512_epochs50_segloss_2_bce_loss_2_kld_loss_0_dice_loss_4_bidir_bio_exp_seg_ckpt_model/global_step50/mp_rank_00_model_states.pt.
[2025-04-05 00:12:01,847] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./runs/plum-13b_kld_0_dice_4_v1_partonomy/plum-13b_kld_0_dice_4_v1_partonomy_accum_10_maxlen512_epochs50_segloss_2_bce_loss_2_kld_loss_0_dice_loss_4_bidir_bio_exp_seg_ckpt_model/global_step50/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-04-05 00:12:15,260] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./runs/plum-13b_kld_0_dice_4_v1_partonomy/plum-13b_kld_0_dice_4_v1_partonomy_accum_10_maxlen512_epochs50_segloss_2_bce_loss_2_kld_loss_0_dice_loss_4_bidir_bio_exp_seg_ckpt_model/global_step50/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-04-05 00:12:15,263] [INFO] [engine.py:3244:_save_zero_checkpoint] zero checkpoint saved ./runs/plum-13b_kld_0_dice_4_v1_partonomy/plum-13b_kld_0_dice_4_v1_partonomy_accum_10_maxlen512_epochs50_segloss_2_bce_loss_2_kld_loss_0_dice_loss_4_bidir_bio_exp_seg_ckpt_model/global_step50/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-04-05 00:12:15,263] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step50 is ready now!
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch: [1][501/500]	Time 33.040 (33.040)	Loss 0.4917 (0.3335)	CeLoss 0.0654 (0.0364)	SegCLSLoss 0.0005 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1154 (0.0827)	MaskBCELoss 0.0179 (0.0169)	MaskDICELoss 0.0975 (0.0658)
Epoch: [1][502/500]	Time 32.928 (32.928)	Loss 0.2777 (0.3070)	CeLoss 0.0001 (0.0171)	SegCLSLoss 0.0001 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0745 (0.0790)	MaskBCELoss 0.0103 (0.0131)	MaskDICELoss 0.0642 (0.0659)
Epoch: [1][503/500]	Time 31.928 (31.928)	Loss 0.5246 (0.3422)	CeLoss 0.0811 (0.0331)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1406 (0.0865)	MaskBCELoss 0.0595 (0.0186)	MaskDICELoss 0.0811 (0.0679)
Epoch: [1][504/500]	Time 34.320 (34.320)	Loss 0.3012 (0.3556)	CeLoss 0.0001 (0.0275)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0829 (0.0929)	MaskBCELoss 0.0152 (0.0220)	MaskDICELoss 0.0677 (0.0709)
Epoch: [1][505/500]	Time 34.429 (34.429)	Loss 0.3546 (0.3625)	CeLoss 0.0718 (0.0331)	SegCLSLoss 0.0001 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0760 (0.0966)	MaskBCELoss 0.0109 (0.0286)	MaskDICELoss 0.0651 (0.0680)
Epoch: [1][506/500]	Time 29.971 (29.971)	Loss 0.2057 (0.3473)	CeLoss 0.0003 (0.0478)	SegCLSLoss 0.0001 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0534 (0.0836)	MaskBCELoss 0.0041 (0.0175)	MaskDICELoss 0.0493 (0.0661)
Epoch: [1][507/500]	Time 32.093 (32.093)	Loss 0.2966 (0.3322)	CeLoss 0.1025 (0.0298)	SegCLSLoss 0.0001 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0490 (0.0830)	MaskBCELoss 0.0008 (0.0150)	MaskDICELoss 0.0481 (0.0680)
Epoch: [1][508/500]	Time 34.369 (34.369)	Loss 0.3014 (0.3153)	CeLoss 0.0007 (0.0231)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0822 (0.0812)	MaskBCELoss 0.0140 (0.0165)	MaskDICELoss 0.0682 (0.0647)
Epoch: [1][509/500]	Time 33.418 (33.418)	Loss 0.2257 (0.2941)	CeLoss 0.0001 (0.0103)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0600 (0.0769)	MaskBCELoss 0.0073 (0.0121)	MaskDICELoss 0.0528 (0.0648)
Epoch: [1][510/500]	Time 31.242 (31.242)	Loss 0.3713 (0.3282)	CeLoss 0.0327 (0.0222)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0906 (0.0847)	MaskBCELoss 0.0118 (0.0165)	MaskDICELoss 0.0787 (0.0682)
Epoch: [1][511/500]	Time 30.058 (30.058)	Loss 0.4024 (0.3362)	CeLoss 0.0001 (0.0212)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1132 (0.0852)	MaskBCELoss 0.0254 (0.0130)	MaskDICELoss 0.0879 (0.0722)
Epoch: [1][512/500]	Time 33.533 (33.533)	Loss 0.2779 (0.3002)	CeLoss 0.0957 (0.0164)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0460 (0.0781)	MaskBCELoss 0.0010 (0.0143)	MaskDICELoss 0.0451 (0.0638)
Epoch: [1][513/500]	Time 28.884 (28.884)	Loss 0.1825 (0.2772)	CeLoss 0.0003 (0.0170)	SegCLSLoss 0.0001 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0460 (0.0714)	MaskBCELoss 0.0009 (0.0126)	MaskDICELoss 0.0450 (0.0587)
Epoch: [1][514/500]	Time 29.772 (29.772)	Loss 0.3988 (0.3101)	CeLoss 0.0879 (0.0473)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0801 (0.0702)	MaskBCELoss 0.0048 (0.0090)	MaskDICELoss 0.0753 (0.0612)
Epoch: [1][515/500]	Time 29.599 (29.599)	Loss 0.2393 (0.2765)	CeLoss 0.0574 (0.0316)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0459 (0.0667)	MaskBCELoss 0.0009 (0.0110)	MaskDICELoss 0.0450 (0.0558)
Epoch: [1][516/500]	Time 33.266 (33.266)	Loss 0.3740 (0.3285)	CeLoss 0.0586 (0.0189)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0870 (0.0869)	MaskBCELoss 0.0162 (0.0191)	MaskDICELoss 0.0707 (0.0678)
Epoch: [1][517/500]	Time 32.933 (32.933)	Loss 0.2788 (0.3292)	CeLoss 0.0240 (0.0270)	SegCLSLoss 0.0000 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0714 (0.0836)	MaskBCELoss 0.0155 (0.0164)	MaskDICELoss 0.0560 (0.0672)
Epoch: [1][518/500]	Time 32.685 (32.685)	Loss 0.3083 (0.3467)	CeLoss 0.0001 (0.0295)	SegCLSLoss 0.0002 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0897 (0.0890)	MaskBCELoss 0.0254 (0.0194)	MaskDICELoss 0.0642 (0.0695)
Epoch: [1][519/500]	Time 19.744 (19.744)	Loss 0.4512 (0.3134)	CeLoss 0.0420 (0.0263)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1147 (0.0783)	MaskBCELoss 0.0248 (0.0131)	MaskDICELoss 0.0899 (0.0652)
Epoch: [1][520/500]	Time 19.750 (19.750)	Loss 0.2236 (0.3169)	CeLoss 0.0001 (0.0275)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0580 (0.0828)	MaskBCELoss 0.0043 (0.0209)	MaskDICELoss 0.0537 (0.0619)
Epoch: [1][521/500]	Time 21.873 (21.873)	Loss 0.2657 (0.3322)	CeLoss 0.0001 (0.0356)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0686 (0.0802)	MaskBCELoss 0.0043 (0.0121)	MaskDICELoss 0.0643 (0.0681)
Epoch: [1][522/500]	Time 21.970 (21.970)	Loss 0.2837 (0.3097)	CeLoss 0.0005 (0.0209)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0795 (0.0793)	MaskBCELoss 0.0175 (0.0142)	MaskDICELoss 0.0620 (0.0651)
Epoch: [1][523/500]	Time 23.946 (23.946)	Loss 0.2699 (0.3328)	CeLoss 0.0003 (0.0304)	SegCLSLoss 0.0003 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0813 (0.0840)	MaskBCELoss 0.0282 (0.0169)	MaskDICELoss 0.0532 (0.0671)
Epoch: [1][524/500]	Time 21.886 (21.886)	Loss 0.2974 (0.3018)	CeLoss 0.0007 (0.0027)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0852 (0.0814)	MaskBCELoss 0.0221 (0.0133)	MaskDICELoss 0.0631 (0.0681)
Epoch: [1][525/500]	Time 24.898 (24.898)	Loss 0.2421 (0.3629)	CeLoss 0.0209 (0.0247)	SegCLSLoss 0.0009 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0598 (0.0951)	MaskBCELoss 0.0099 (0.0213)	MaskDICELoss 0.0499 (0.0738)
Epoch: [1][526/500]	Time 22.621 (22.621)	Loss 0.3221 (0.3161)	CeLoss 0.0001 (0.0426)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0867 (0.0752)	MaskBCELoss 0.0125 (0.0136)	MaskDICELoss 0.0743 (0.0615)
Epoch: [1][527/500]	Time 24.203 (24.203)	Loss 0.4042 (0.3114)	CeLoss 0.0327 (0.0139)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1093 (0.0827)	MaskBCELoss 0.0329 (0.0167)	MaskDICELoss 0.0764 (0.0660)
Epoch: [1][528/500]	Time 22.559 (22.559)	Loss 0.2040 (0.2887)	CeLoss 0.0002 (0.0293)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0557 (0.0731)	MaskBCELoss 0.0095 (0.0165)	MaskDICELoss 0.0462 (0.0565)
Epoch: [1][529/500]	Time 24.984 (24.984)	Loss 0.2950 (0.3151)	CeLoss 0.0342 (0.0386)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0697 (0.0751)	MaskBCELoss 0.0091 (0.0120)	MaskDICELoss 0.0606 (0.0631)
Epoch: [1][530/500]	Time 26.700 (26.700)	Loss 0.3783 (0.3281)	CeLoss 0.0598 (0.0291)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0872 (0.0817)	MaskBCELoss 0.0152 (0.0140)	MaskDICELoss 0.0720 (0.0677)
Epoch: [1][531/500]	Time 23.624 (23.624)	Loss 0.3309 (0.3221)	CeLoss 0.0002 (0.0212)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0923 (0.0825)	MaskBCELoss 0.0194 (0.0145)	MaskDICELoss 0.0730 (0.0680)
Epoch: [1][532/500]	Time 29.042 (29.042)	Loss 0.2209 (0.3489)	CeLoss 0.0002 (0.0241)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0573 (0.0910)	MaskBCELoss 0.0042 (0.0197)	MaskDICELoss 0.0531 (0.0713)
Epoch: [1][533/500]	Time 21.953 (21.953)	Loss 0.2332 (0.2790)	CeLoss 0.0591 (0.0213)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0439 (0.0694)	MaskBCELoss 0.0006 (0.0100)	MaskDICELoss 0.0432 (0.0594)
Epoch: [1][534/500]	Time 20.597 (20.597)	Loss 0.3619 (0.3008)	CeLoss 0.0001 (0.0182)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1017 (0.0789)	MaskBCELoss 0.0226 (0.0166)	MaskDICELoss 0.0791 (0.0623)
Epoch: [1][535/500]	Time 25.328 (25.328)	Loss 0.3492 (0.3420)	CeLoss 0.0698 (0.0408)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0734 (0.0826)	MaskBCELoss 0.0071 (0.0146)	MaskDICELoss 0.0662 (0.0680)
Epoch: [1][536/500]	Time 19.006 (19.006)	Loss 0.1258 (0.2579)	CeLoss 0.1260 (0.0311)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0000 (0.0611)	MaskBCELoss 0.0000 (0.0088)	MaskDICELoss 0.0000 (0.0523)
Epoch: [1][537/500]	Time 26.323 (26.323)	Loss 0.3129 (0.3183)	CeLoss 0.0679 (0.0339)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0639 (0.0806)	MaskBCELoss 0.0053 (0.0189)	MaskDICELoss 0.0586 (0.0616)
Epoch: [1][538/500]	Time 23.145 (23.145)	Loss 0.3333 (0.3142)	CeLoss 0.0000 (0.0197)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0920 (0.0819)	MaskBCELoss 0.0174 (0.0165)	MaskDICELoss 0.0746 (0.0654)
Epoch: [1][539/500]	Time 25.465 (25.465)	Loss 0.2881 (0.3043)	CeLoss 0.0001 (0.0205)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0807 (0.0791)	MaskBCELoss 0.0175 (0.0162)	MaskDICELoss 0.0633 (0.0628)
Epoch: [1][540/500]	Time 25.568 (25.568)	Loss 0.2928 (0.3304)	CeLoss 0.0442 (0.0345)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0667 (0.0804)	MaskBCELoss 0.0091 (0.0129)	MaskDICELoss 0.0576 (0.0675)
Epoch: [1][541/500]	Time 25.829 (25.829)	Loss 0.3351 (0.3058)	CeLoss 0.0001 (0.0132)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0913 (0.0797)	MaskBCELoss 0.0151 (0.0131)	MaskDICELoss 0.0762 (0.0666)
Epoch: [1][542/500]	Time 21.040 (21.040)	Loss 0.3092 (0.2851)	CeLoss 0.0854 (0.0256)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0588 (0.0709)	MaskBCELoss 0.0058 (0.0121)	MaskDICELoss 0.0530 (0.0588)
Epoch: [1][543/500]	Time 22.749 (22.749)	Loss 0.4425 (0.2967)	CeLoss 0.0493 (0.0153)	SegCLSLoss 0.0002 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1130 (0.0772)	MaskBCELoss 0.0297 (0.0138)	MaskDICELoss 0.0833 (0.0634)
Epoch: [1][544/500]	Time 40.672 (40.672)	Loss 0.3358 (0.3170)	CeLoss 0.0001 (0.0164)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0892 (0.0828)	MaskBCELoss 0.0106 (0.0152)	MaskDICELoss 0.0786 (0.0675)
Epoch: [1][545/500]	Time 24.706 (24.706)	Loss 0.3803 (0.3417)	CeLoss 0.0366 (0.0322)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1004 (0.0872)	MaskBCELoss 0.0290 (0.0196)	MaskDICELoss 0.0714 (0.0675)
Epoch: [1][546/500]	Time 29.384 (29.384)	Loss 0.3144 (0.3008)	CeLoss 0.0923 (0.0417)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0580 (0.0693)	MaskBCELoss 0.0048 (0.0092)	MaskDICELoss 0.0532 (0.0602)
Epoch: [1][547/500]	Time 64.756 (64.756)	Loss 0.3926 (0.3744)	CeLoss 0.0300 (0.0393)	SegCLSLoss 0.0004 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1038 (0.0950)	MaskBCELoss 0.0268 (0.0224)	MaskDICELoss 0.0770 (0.0726)
Epoch: [1][548/500]	Time 22.029 (22.029)	Loss 0.2330 (0.3276)	CeLoss 0.0001 (0.0241)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0611 (0.0829)	MaskBCELoss 0.0057 (0.0141)	MaskDICELoss 0.0553 (0.0688)
Epoch: [1][549/500]	Time 32.306 (32.306)	Loss 0.2767 (0.2994)	CeLoss 0.0001 (0.0184)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0747 (0.0770)	MaskBCELoss 0.0111 (0.0136)	MaskDICELoss 0.0636 (0.0635)
Epoch: [1][550/500]	Time 30.389 (30.389)	Loss 0.3569 (0.3210)	CeLoss 0.0586 (0.0330)	SegCLSLoss 0.0001 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0848 (0.0794)	MaskBCELoss 0.0207 (0.0148)	MaskDICELoss 0.0641 (0.0646)
Epoch: [1][551/500]	Time 25.751 (25.751)	Loss 0.2664 (0.3330)	CeLoss 0.0002 (0.0354)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0766 (0.0816)	MaskBCELoss 0.0200 (0.0144)	MaskDICELoss 0.0565 (0.0672)
Epoch: [1][552/500]	Time 35.066 (35.066)	Loss 0.3104 (0.3403)	CeLoss 0.0002 (0.0193)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0862 (0.0890)	MaskBCELoss 0.0173 (0.0175)	MaskDICELoss 0.0689 (0.0715)
Epoch: [1][553/500]	Time 24.140 (24.140)	Loss 0.3826 (0.3342)	CeLoss 0.0001 (0.0327)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1010 (0.0810)	MaskBCELoss 0.0108 (0.0113)	MaskDICELoss 0.0902 (0.0697)
Epoch: [1][554/500]	Time 23.806 (23.806)	Loss 0.3563 (0.3508)	CeLoss 0.0000 (0.0293)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0966 (0.0885)	MaskBCELoss 0.0151 (0.0164)	MaskDICELoss 0.0815 (0.0721)
Epoch: [1][555/500]	Time 21.769 (21.769)	Loss 0.3276 (0.2965)	CeLoss 0.0001 (0.0187)	SegCLSLoss 0.0002 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0961 (0.0754)	MaskBCELoss 0.0286 (0.0120)	MaskDICELoss 0.0675 (0.0634)
Epoch: [1][556/500]	Time 26.319 (26.319)	Loss 0.3625 (0.3179)	CeLoss 0.0432 (0.0189)	SegCLSLoss 0.0007 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0937 (0.0829)	MaskBCELoss 0.0285 (0.0164)	MaskDICELoss 0.0653 (0.0665)
Epoch: [1][557/500]	Time 23.582 (23.582)	Loss 0.3044 (0.2987)	CeLoss 0.0908 (0.0217)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0547 (0.0759)	MaskBCELoss 0.0028 (0.0133)	MaskDICELoss 0.0520 (0.0626)
Epoch: [1][558/500]	Time 22.009 (22.009)	Loss 0.3710 (0.3199)	CeLoss 0.0474 (0.0271)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0858 (0.0792)	MaskBCELoss 0.0097 (0.0120)	MaskDICELoss 0.0761 (0.0671)
Epoch: [1][559/500]	Time 26.198 (26.198)	Loss 0.2132 (0.3022)	CeLoss 0.0002 (0.0310)	SegCLSLoss 0.0005 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0568 (0.0738)	MaskBCELoss 0.0076 (0.0122)	MaskDICELoss 0.0492 (0.0617)
Epoch: [1][560/500]	Time 23.204 (23.204)	Loss 0.2921 (0.3192)	CeLoss 0.0469 (0.0231)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0670 (0.0831)	MaskBCELoss 0.0114 (0.0183)	MaskDICELoss 0.0556 (0.0648)
Epoch: [1][561/500]	Time 22.457 (22.457)	Loss 0.3638 (0.3407)	CeLoss 0.1064 (0.0330)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0678 (0.0872)	MaskBCELoss 0.0067 (0.0204)	MaskDICELoss 0.0610 (0.0667)
Epoch: [1][562/500]	Time 21.319 (21.319)	Loss 0.3332 (0.3213)	CeLoss 0.0908 (0.0330)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0659 (0.0778)	MaskBCELoss 0.0107 (0.0115)	MaskDICELoss 0.0552 (0.0663)
Epoch: [1][563/500]	Time 20.220 (20.220)	Loss 0.4935 (0.3203)	CeLoss 0.0869 (0.0348)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1204 (0.0781)	MaskBCELoss 0.0374 (0.0135)	MaskDICELoss 0.0830 (0.0646)
Epoch: [1][564/500]	Time 24.078 (24.078)	Loss 0.2898 (0.3283)	CeLoss 0.0003 (0.0151)	SegCLSLoss 0.0008 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0832 (0.0880)	MaskBCELoss 0.0226 (0.0196)	MaskDICELoss 0.0606 (0.0684)
Epoch: [1][565/500]	Time 22.225 (22.225)	Loss 0.2360 (0.2956)	CeLoss 0.0002 (0.0364)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0665 (0.0702)	MaskBCELoss 0.0152 (0.0109)	MaskDICELoss 0.0514 (0.0593)
Epoch: [1][566/500]	Time 20.183 (20.183)	Loss 0.3286 (0.2993)	CeLoss 0.0000 (0.0183)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0921 (0.0759)	MaskBCELoss 0.0199 (0.0113)	MaskDICELoss 0.0722 (0.0645)
Epoch: [1][567/500]	Time 19.913 (19.913)	Loss 0.3092 (0.3201)	CeLoss 0.0002 (0.0266)	SegCLSLoss 0.0000 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0867 (0.0801)	MaskBCELoss 0.0190 (0.0138)	MaskDICELoss 0.0678 (0.0663)
Epoch: [1][568/500]	Time 21.069 (21.069)	Loss 0.3307 (0.3103)	CeLoss 0.0001 (0.0172)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0926 (0.0802)	MaskBCELoss 0.0200 (0.0140)	MaskDICELoss 0.0727 (0.0662)
Epoch: [1][569/500]	Time 23.264 (23.264)	Loss 0.2728 (0.3038)	CeLoss 0.0003 (0.0063)	SegCLSLoss 0.0003 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0746 (0.0821)	MaskBCELoss 0.0133 (0.0156)	MaskDICELoss 0.0613 (0.0665)
Epoch: [1][570/500]	Time 20.223 (20.223)	Loss 0.2198 (0.3049)	CeLoss 0.0002 (0.0224)	SegCLSLoss 0.0000 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0593 (0.0780)	MaskBCELoss 0.0087 (0.0151)	MaskDICELoss 0.0505 (0.0629)
Epoch: [1][571/500]	Time 19.536 (19.536)	Loss 0.2526 (0.3060)	CeLoss 0.0732 (0.0360)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0457 (0.0729)	MaskBCELoss 0.0015 (0.0108)	MaskDICELoss 0.0441 (0.0621)
Epoch: [1][572/500]	Time 22.360 (22.360)	Loss 0.2405 (0.3088)	CeLoss 0.0001 (0.0243)	SegCLSLoss 0.0001 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0666 (0.0765)	MaskBCELoss 0.0132 (0.0109)	MaskDICELoss 0.0534 (0.0657)
Epoch: [1][573/500]	Time 21.312 (21.312)	Loss 0.2143 (0.3425)	CeLoss 0.0003 (0.0427)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0567 (0.0824)	MaskBCELoss 0.0065 (0.0150)	MaskDICELoss 0.0502 (0.0674)
Epoch: [1][574/500]	Time 22.477 (22.477)	Loss 0.4600 (0.3417)	CeLoss 0.0610 (0.0231)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1082 (0.0891)	MaskBCELoss 0.0168 (0.0189)	MaskDICELoss 0.0914 (0.0701)
Epoch: [1][575/500]	Time 25.481 (25.481)	Loss 0.2640 (0.3461)	CeLoss 0.0001 (0.0187)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0707 (0.0911)	MaskBCELoss 0.0094 (0.0186)	MaskDICELoss 0.0612 (0.0726)
Epoch: [1][576/500]	Time 19.147 (19.147)	Loss 0.2660 (0.3115)	CeLoss 0.0001 (0.0221)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0739 (0.0814)	MaskBCELoss 0.0149 (0.0181)	MaskDICELoss 0.0590 (0.0633)
Epoch: [1][577/500]	Time 20.727 (20.727)	Loss 0.3111 (0.3195)	CeLoss 0.0347 (0.0190)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0733 (0.0806)	MaskBCELoss 0.0083 (0.0111)	MaskDICELoss 0.0650 (0.0696)
Epoch: [1][578/500]	Time 19.677 (19.677)	Loss 0.3302 (0.3181)	CeLoss 0.0001 (0.0460)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0885 (0.0743)	MaskBCELoss 0.0120 (0.0127)	MaskDICELoss 0.0765 (0.0616)
Epoch: [1][579/500]	Time 23.420 (23.420)	Loss 0.3763 (0.3257)	CeLoss 0.0251 (0.0257)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0994 (0.0825)	MaskBCELoss 0.0233 (0.0152)	MaskDICELoss 0.0761 (0.0673)
Epoch: [1][580/500]	Time 19.603 (19.603)	Loss 0.3062 (0.3154)	CeLoss 0.0012 (0.0197)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0886 (0.0817)	MaskBCELoss 0.0248 (0.0156)	MaskDICELoss 0.0638 (0.0661)
Epoch: [1][581/500]	Time 21.548 (21.548)	Loss 0.3363 (0.3222)	CeLoss 0.0001 (0.0315)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0942 (0.0816)	MaskBCELoss 0.0203 (0.0178)	MaskDICELoss 0.0739 (0.0638)
Epoch: [1][582/500]	Time 21.579 (21.579)	Loss 0.3134 (0.3239)	CeLoss 0.0002 (0.0187)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0867 (0.0848)	MaskBCELoss 0.0168 (0.0171)	MaskDICELoss 0.0699 (0.0677)
Epoch: [1][583/500]	Time 22.611 (22.611)	Loss 0.2439 (0.2881)	CeLoss 0.0003 (0.0112)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0656 (0.0774)	MaskBCELoss 0.0094 (0.0163)	MaskDICELoss 0.0562 (0.0611)
Epoch: [1][584/500]	Time 20.961 (20.961)	Loss 0.2747 (0.3585)	CeLoss 0.0001 (0.0349)	SegCLSLoss 0.0000 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0744 (0.0932)	MaskBCELoss 0.0115 (0.0248)	MaskDICELoss 0.0629 (0.0684)
Epoch: [1][585/500]	Time 24.603 (24.603)	Loss 0.3477 (0.3169)	CeLoss 0.0635 (0.0361)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0845 (0.0793)	MaskBCELoss 0.0269 (0.0183)	MaskDICELoss 0.0577 (0.0610)
Epoch: [1][586/500]	Time 22.870 (22.870)	Loss 0.3457 (0.3259)	CeLoss 0.0559 (0.0242)	SegCLSLoss 0.0000 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0765 (0.0841)	MaskBCELoss 0.0080 (0.0176)	MaskDICELoss 0.0685 (0.0665)
Epoch: [1][587/500]	Time 21.427 (21.427)	Loss 0.2967 (0.2845)	CeLoss 0.0001 (0.0250)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0857 (0.0720)	MaskBCELoss 0.0231 (0.0143)	MaskDICELoss 0.0626 (0.0577)
Epoch: [1][588/500]	Time 19.848 (19.848)	Loss 0.2972 (0.3192)	CeLoss 0.0491 (0.0302)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0675 (0.0784)	MaskBCELoss 0.0109 (0.0123)	MaskDICELoss 0.0566 (0.0660)
Epoch: [1][589/500]	Time 22.620 (22.620)	Loss 0.3116 (0.3225)	CeLoss 0.0001 (0.0261)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0838 (0.0840)	MaskBCELoss 0.0117 (0.0197)	MaskDICELoss 0.0720 (0.0642)
Epoch: [1][590/500]	Time 21.992 (21.992)	Loss 0.3477 (0.3363)	CeLoss 0.0001 (0.0255)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0961 (0.0852)	MaskBCELoss 0.0185 (0.0151)	MaskDICELoss 0.0777 (0.0701)
Epoch: [1][591/500]	Time 22.468 (22.468)	Loss 0.3257 (0.3098)	CeLoss 0.0002 (0.0181)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0955 (0.0813)	MaskBCELoss 0.0282 (0.0167)	MaskDICELoss 0.0672 (0.0646)
Epoch: [1][592/500]	Time 23.831 (23.831)	Loss 0.2149 (0.3039)	CeLoss 0.0001 (0.0226)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0561 (0.0752)	MaskBCELoss 0.0047 (0.0097)	MaskDICELoss 0.0514 (0.0654)
Epoch: [1][593/500]	Time 19.306 (19.306)	Loss 0.2876 (0.3348)	CeLoss 0.1108 (0.0485)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0446 (0.0772)	MaskBCELoss 0.0009 (0.0112)	MaskDICELoss 0.0437 (0.0660)
Epoch: [1][594/500]	Time 21.906 (21.906)	Loss 0.2249 (0.3465)	CeLoss 0.0001 (0.0339)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0566 (0.0892)	MaskBCELoss 0.0009 (0.0221)	MaskDICELoss 0.0557 (0.0671)
Epoch: [1][595/500]	Time 24.314 (24.314)	Loss 0.3838 (0.3140)	CeLoss 0.0547 (0.0178)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0930 (0.0824)	MaskBCELoss 0.0215 (0.0168)	MaskDICELoss 0.0715 (0.0657)
Epoch: [1][596/500]	Time 20.647 (20.647)	Loss 0.2685 (0.3216)	CeLoss 0.0001 (0.0225)	SegCLSLoss 0.0001 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0737 (0.0832)	MaskBCELoss 0.0133 (0.0169)	MaskDICELoss 0.0604 (0.0663)
Epoch: [1][597/500]	Time 20.908 (20.908)	Loss 0.4665 (0.3242)	CeLoss 0.0752 (0.0364)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1171 (0.0795)	MaskBCELoss 0.0386 (0.0151)	MaskDICELoss 0.0785 (0.0644)
Epoch: [1][598/500]	Time 19.976 (19.976)	Loss 0.4739 (0.2768)	CeLoss 0.0417 (0.0259)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1576 (0.0717)	MaskBCELoss 0.0992 (0.0180)	MaskDICELoss 0.0584 (0.0537)
Epoch: [1][599/500]	Time 19.892 (19.892)	Loss 0.2874 (0.2992)	CeLoss 0.0001 (0.0257)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0749 (0.0756)	MaskBCELoss 0.0061 (0.0145)	MaskDICELoss 0.0688 (0.0611)
[2025-04-05 00:54:18,599] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[0.0002933012048192771], mom=[(0.9, 0.95)]
[2025-04-05 00:54:18,626] [INFO] [timer.py:215:stop] epoch=0/micro_step=600/global_step=60, RunningAvgSamplesPerSec=1.5468458891260126, CurrSamplesPerSec=2.05442657837583, MemAllocated=33.73GB, MaxMemAllocated=46.18GB
Epoch: [1][600/500]	Time 21.968 (21.968)	Loss 0.2200 (0.3157)	CeLoss 0.0001 (0.0183)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0593 (0.0821)	MaskBCELoss 0.0087 (0.0155)	MaskDICELoss 0.0506 (0.0666)
Epoch: [1][601/500]	Time 20.993 (20.993)	Loss 0.1633 (0.3055)	CeLoss 0.0001 (0.0391)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0412 (0.0735)	MaskBCELoss 0.0009 (0.0138)	MaskDICELoss 0.0403 (0.0597)
Epoch: [1][602/500]	Time 20.550 (20.550)	Loss 0.4213 (0.3270)	CeLoss 0.0698 (0.0400)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1040 (0.0799)	MaskBCELoss 0.0323 (0.0164)	MaskDICELoss 0.0717 (0.0636)
Epoch: [1][603/500]	Time 19.119 (19.119)	Loss 0.2631 (0.2782)	CeLoss 0.0286 (0.0292)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0633 (0.0675)	MaskBCELoss 0.0093 (0.0105)	MaskDICELoss 0.0540 (0.0570)
Epoch: [1][604/500]	Time 23.144 (23.144)	Loss 0.3276 (0.3077)	CeLoss 0.0002 (0.0231)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0952 (0.0798)	MaskBCELoss 0.0268 (0.0173)	MaskDICELoss 0.0684 (0.0625)
Epoch: [1][605/500]	Time 24.287 (24.287)	Loss 0.2912 (0.2824)	CeLoss 0.0003 (0.0231)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0832 (0.0724)	MaskBCELoss 0.0210 (0.0151)	MaskDICELoss 0.0622 (0.0573)
Epoch: [1][606/500]	Time 20.079 (20.079)	Loss 0.3087 (0.2863)	CeLoss 0.0001 (0.0210)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0801 (0.0742)	MaskBCELoss 0.0058 (0.0158)	MaskDICELoss 0.0742 (0.0584)
Epoch: [1][607/500]	Time 21.622 (21.622)	Loss 0.2991 (0.2896)	CeLoss 0.0488 (0.0184)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0677 (0.0731)	MaskBCELoss 0.0104 (0.0106)	MaskDICELoss 0.0574 (0.0625)
Epoch: [1][608/500]	Time 22.835 (22.835)	Loss 0.2030 (0.3171)	CeLoss 0.0248 (0.0223)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0462 (0.0845)	MaskBCELoss 0.0034 (0.0216)	MaskDICELoss 0.0429 (0.0629)
Epoch: [1][609/500]	Time 22.216 (22.216)	Loss 0.2842 (0.2929)	CeLoss 0.0001 (0.0144)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0847 (0.0791)	MaskBCELoss 0.0274 (0.0191)	MaskDICELoss 0.0573 (0.0601)
Epoch: [1][610/500]	Time 20.559 (20.559)	Loss 0.2810 (0.3039)	CeLoss 0.0593 (0.0229)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0604 (0.0783)	MaskBCELoss 0.0099 (0.0161)	MaskDICELoss 0.0504 (0.0622)
Epoch: [1][611/500]	Time 22.924 (22.924)	Loss 0.2435 (0.3226)	CeLoss 0.0003 (0.0146)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0690 (0.0890)	MaskBCELoss 0.0164 (0.0240)	MaskDICELoss 0.0526 (0.0650)
Epoch: [1][612/500]	Time 21.461 (21.461)	Loss 0.2501 (0.2942)	CeLoss 0.0002 (0.0292)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0695 (0.0723)	MaskBCELoss 0.0140 (0.0121)	MaskDICELoss 0.0555 (0.0602)
Epoch: [1][613/500]	Time 21.865 (21.865)	Loss 0.3672 (0.2677)	CeLoss 0.0001 (0.0169)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1025 (0.0671)	MaskBCELoss 0.0214 (0.0089)	MaskDICELoss 0.0811 (0.0583)
Epoch: [1][614/500]	Time 24.700 (24.700)	Loss 0.3301 (0.3063)	CeLoss 0.0001 (0.0197)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0932 (0.0804)	MaskBCELoss 0.0213 (0.0174)	MaskDICELoss 0.0719 (0.0630)
Epoch: [1][615/500]	Time 23.110 (23.110)	Loss 0.4865 (0.3233)	CeLoss 0.0752 (0.0320)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1208 (0.0805)	MaskBCELoss 0.0360 (0.0154)	MaskDICELoss 0.0848 (0.0651)
Epoch: [1][616/500]	Time 24.068 (24.068)	Loss 0.4628 (0.3275)	CeLoss 0.0791 (0.0336)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1099 (0.0802)	MaskBCELoss 0.0278 (0.0135)	MaskDICELoss 0.0821 (0.0667)
Epoch: [1][617/500]	Time 20.285 (20.285)	Loss 0.2872 (0.3138)	CeLoss 0.0737 (0.0419)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0588 (0.0741)	MaskBCELoss 0.0109 (0.0123)	MaskDICELoss 0.0479 (0.0618)
Epoch: [1][618/500]	Time 21.130 (21.130)	Loss 0.2804 (0.2764)	CeLoss 0.0001 (0.0039)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0728 (0.0735)	MaskBCELoss 0.0054 (0.0108)	MaskDICELoss 0.0674 (0.0627)
Epoch: [1][619/500]	Time 22.627 (22.627)	Loss 0.2493 (0.3180)	CeLoss 0.0002 (0.0332)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0689 (0.0779)	MaskBCELoss 0.0133 (0.0135)	MaskDICELoss 0.0556 (0.0644)
Epoch: [1][620/500]	Time 20.696 (20.696)	Loss 0.3407 (0.2846)	CeLoss 0.0547 (0.0213)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0837 (0.0725)	MaskBCELoss 0.0245 (0.0133)	MaskDICELoss 0.0593 (0.0592)
Epoch: [1][621/500]	Time 23.965 (23.965)	Loss 0.3191 (0.3261)	CeLoss 0.0002 (0.0229)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0852 (0.0832)	MaskBCELoss 0.0109 (0.0149)	MaskDICELoss 0.0743 (0.0683)
Epoch: [1][622/500]	Time 24.561 (24.561)	Loss 0.2793 (0.3583)	CeLoss 0.0383 (0.0264)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0615 (0.0928)	MaskBCELoss 0.0026 (0.0196)	MaskDICELoss 0.0590 (0.0731)
Epoch: [1][623/500]	Time 22.926 (22.926)	Loss 0.2389 (0.2906)	CeLoss 0.0002 (0.0077)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0647 (0.0787)	MaskBCELoss 0.0102 (0.0159)	MaskDICELoss 0.0546 (0.0628)
Epoch: [1][624/500]	Time 21.616 (21.616)	Loss 0.2469 (0.2880)	CeLoss 0.0000 (0.0240)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0669 (0.0719)	MaskBCELoss 0.0103 (0.0118)	MaskDICELoss 0.0566 (0.0601)
Epoch: [1][625/500]	Time 20.566 (20.566)	Loss 0.2635 (0.3203)	CeLoss 0.0001 (0.0401)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0732 (0.0775)	MaskBCELoss 0.0148 (0.0149)	MaskDICELoss 0.0585 (0.0626)
Epoch: [1][626/500]	Time 20.792 (20.792)	Loss 0.1919 (0.2681)	CeLoss 0.0001 (0.0128)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0511 (0.0700)	MaskBCELoss 0.0063 (0.0123)	MaskDICELoss 0.0448 (0.0576)
Epoch: [1][627/500]	Time 20.607 (20.607)	Loss 0.2959 (0.3050)	CeLoss 0.0786 (0.0200)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0555 (0.0787)	MaskBCELoss 0.0022 (0.0149)	MaskDICELoss 0.0532 (0.0638)
Epoch: [1][628/500]	Time 21.043 (21.043)	Loss 0.1705 (0.2671)	CeLoss 0.0001 (0.0308)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0432 (0.0643)	MaskBCELoss 0.0012 (0.0104)	MaskDICELoss 0.0420 (0.0538)
Epoch: [1][629/500]	Time 21.094 (21.094)	Loss 0.4249 (0.3464)	CeLoss 0.0001 (0.0414)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1177 (0.0827)	MaskBCELoss 0.0231 (0.0129)	MaskDICELoss 0.0946 (0.0698)
Epoch: [1][630/500]	Time 23.471 (23.471)	Loss 0.2752 (0.3178)	CeLoss 0.0464 (0.0127)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0585 (0.0856)	MaskBCELoss 0.0027 (0.0186)	MaskDICELoss 0.0558 (0.0669)
Epoch: [1][631/500]	Time 22.344 (22.344)	Loss 0.2635 (0.2809)	CeLoss 0.0305 (0.0311)	SegCLSLoss 0.0001 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0654 (0.0671)	MaskBCELoss 0.0144 (0.0093)	MaskDICELoss 0.0510 (0.0578)
Epoch: [1][632/500]	Time 21.296 (21.296)	Loss 0.2306 (0.3125)	CeLoss 0.0002 (0.0323)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0629 (0.0760)	MaskBCELoss 0.0107 (0.0120)	MaskDICELoss 0.0523 (0.0640)
Epoch: [1][633/500]	Time 20.518 (20.518)	Loss 0.2693 (0.3263)	CeLoss 0.0001 (0.0300)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0712 (0.0812)	MaskBCELoss 0.0078 (0.0143)	MaskDICELoss 0.0634 (0.0669)
Epoch: [1][634/500]	Time 22.390 (22.390)	Loss 0.4386 (0.3099)	CeLoss 0.0327 (0.0188)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1186 (0.0799)	MaskBCELoss 0.0344 (0.0143)	MaskDICELoss 0.0843 (0.0656)
Epoch: [1][635/500]	Time 21.846 (21.846)	Loss 0.2861 (0.3009)	CeLoss 0.0400 (0.0359)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0638 (0.0713)	MaskBCELoss 0.0046 (0.0102)	MaskDICELoss 0.0592 (0.0612)
Epoch: [1][636/500]	Time 19.887 (19.887)	Loss 0.2450 (0.2825)	CeLoss 0.0001 (0.0179)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0664 (0.0726)	MaskBCELoss 0.0104 (0.0130)	MaskDICELoss 0.0560 (0.0596)
Epoch: [1][637/500]	Time 23.251 (23.251)	Loss 0.3241 (0.3202)	CeLoss 0.0771 (0.0349)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0648 (0.0785)	MaskBCELoss 0.0061 (0.0145)	MaskDICELoss 0.0587 (0.0641)
Epoch: [1][638/500]	Time 21.569 (21.569)	Loss 0.3333 (0.2787)	CeLoss 0.0001 (0.0101)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0935 (0.0729)	MaskBCELoss 0.0203 (0.0115)	MaskDICELoss 0.0732 (0.0614)
Epoch: [1][639/500]	Time 21.797 (21.797)	Loss 0.2107 (0.2950)	CeLoss 0.0001 (0.0166)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0569 (0.0775)	MaskBCELoss 0.0086 (0.0159)	MaskDICELoss 0.0484 (0.0617)
Epoch: [1][640/500]	Time 24.901 (24.901)	Loss 0.3332 (0.3368)	CeLoss 0.0008 (0.0350)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1046 (0.0856)	MaskBCELoss 0.0429 (0.0204)	MaskDICELoss 0.0616 (0.0652)
Epoch: [1][641/500]	Time 21.244 (21.244)	Loss 0.3176 (0.3067)	CeLoss 0.0001 (0.0340)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0858 (0.0748)	MaskBCELoss 0.0129 (0.0133)	MaskDICELoss 0.0729 (0.0615)
Epoch: [1][642/500]	Time 20.057 (20.057)	Loss 0.2523 (0.2816)	CeLoss 0.0001 (0.0194)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0665 (0.0705)	MaskBCELoss 0.0069 (0.0099)	MaskDICELoss 0.0596 (0.0606)
Epoch: [1][643/500]	Time 25.034 (25.034)	Loss 0.2611 (0.2993)	CeLoss 0.0002 (0.0333)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0733 (0.0731)	MaskBCELoss 0.0162 (0.0131)	MaskDICELoss 0.0571 (0.0599)
Epoch: [1][644/500]	Time 25.397 (25.397)	Loss 0.3929 (0.3101)	CeLoss 0.0184 (0.0305)	SegCLSLoss 0.0001 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1056 (0.0775)	MaskBCELoss 0.0241 (0.0155)	MaskDICELoss 0.0815 (0.0620)
Epoch: [1][645/500]	Time 20.734 (20.734)	Loss 0.3056 (0.3105)	CeLoss 0.0197 (0.0364)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0795 (0.0753)	MaskBCELoss 0.0160 (0.0135)	MaskDICELoss 0.0635 (0.0618)
Epoch: [1][646/500]	Time 25.172 (25.172)	Loss 0.4048 (0.3425)	CeLoss 0.0001 (0.0347)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1167 (0.0862)	MaskBCELoss 0.0310 (0.0186)	MaskDICELoss 0.0857 (0.0676)
Epoch: [1][647/500]	Time 22.124 (22.124)	Loss 0.2646 (0.3113)	CeLoss 0.0001 (0.0168)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0747 (0.0806)	MaskBCELoss 0.0172 (0.0141)	MaskDICELoss 0.0575 (0.0666)
Epoch: [1][648/500]	Time 20.985 (20.985)	Loss 0.2720 (0.2828)	CeLoss 0.0625 (0.0064)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0575 (0.0770)	MaskBCELoss 0.0103 (0.0159)	MaskDICELoss 0.0472 (0.0611)
Epoch: [1][649/500]	Time 20.928 (20.928)	Loss 0.3403 (0.3114)	CeLoss 0.0000 (0.0200)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0884 (0.0794)	MaskBCELoss 0.0068 (0.0130)	MaskDICELoss 0.0817 (0.0663)
Epoch: [1][650/500]	Time 19.843 (19.843)	Loss 0.3301 (0.3043)	CeLoss 0.0496 (0.0243)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0807 (0.0777)	MaskBCELoss 0.0211 (0.0154)	MaskDICELoss 0.0596 (0.0623)
Epoch: [1][651/500]	Time 22.046 (22.046)	Loss 0.3164 (0.3200)	CeLoss 0.0000 (0.0240)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0890 (0.0837)	MaskBCELoss 0.0199 (0.0195)	MaskDICELoss 0.0691 (0.0642)
Epoch: [1][652/500]	Time 21.634 (21.634)	Loss 0.2173 (0.3026)	CeLoss 0.0001 (0.0262)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0550 (0.0760)	MaskBCELoss 0.0015 (0.0138)	MaskDICELoss 0.0536 (0.0622)
Epoch: [1][653/500]	Time 20.760 (20.760)	Loss 0.3474 (0.3460)	CeLoss 0.0400 (0.0372)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0781 (0.0855)	MaskBCELoss 0.0024 (0.0166)	MaskDICELoss 0.0756 (0.0689)
Epoch: [1][654/500]	Time 24.848 (24.848)	Loss 0.2536 (0.3120)	CeLoss 0.0003 (0.0164)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0710 (0.0817)	MaskBCELoss 0.0154 (0.0155)	MaskDICELoss 0.0557 (0.0661)
Epoch: [1][655/500]	Time 22.432 (22.432)	Loss 0.3289 (0.3233)	CeLoss 0.0001 (0.0292)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0875 (0.0796)	MaskBCELoss 0.0106 (0.0121)	MaskDICELoss 0.0769 (0.0675)
Epoch: [1][656/500]	Time 22.298 (22.298)	Loss 0.3290 (0.3249)	CeLoss 0.0001 (0.0322)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0884 (0.0797)	MaskBCELoss 0.0123 (0.0129)	MaskDICELoss 0.0761 (0.0667)
Epoch: [1][657/500]	Time 21.418 (21.418)	Loss 0.4108 (0.3387)	CeLoss 0.0002 (0.0382)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1114 (0.0822)	MaskBCELoss 0.0175 (0.0141)	MaskDICELoss 0.0939 (0.0681)
Epoch: [1][658/500]	Time 21.484 (21.484)	Loss 0.2958 (0.3211)	CeLoss 0.0002 (0.0312)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0841 (0.0806)	MaskBCELoss 0.0203 (0.0164)	MaskDICELoss 0.0638 (0.0642)
Epoch: [1][659/500]	Time 20.364 (20.364)	Loss 0.3903 (0.3006)	CeLoss 0.0000 (0.0063)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1061 (0.0807)	MaskBCELoss 0.0170 (0.0143)	MaskDICELoss 0.0891 (0.0664)
Epoch: [1][660/500]	Time 24.005 (24.005)	Loss 0.2881 (0.3193)	CeLoss 0.0254 (0.0378)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0689 (0.0759)	MaskBCELoss 0.0064 (0.0110)	MaskDICELoss 0.0625 (0.0649)
Epoch: [1][661/500]	Time 23.241 (23.241)	Loss 0.2429 (0.2858)	CeLoss 0.0000 (0.0242)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0673 (0.0710)	MaskBCELoss 0.0131 (0.0112)	MaskDICELoss 0.0541 (0.0598)
Epoch: [1][662/500]	Time 21.596 (21.596)	Loss 0.2774 (0.3213)	CeLoss 0.0005 (0.0310)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0753 (0.0799)	MaskBCELoss 0.0121 (0.0147)	MaskDICELoss 0.0632 (0.0653)
Epoch: [1][663/500]	Time 19.790 (19.790)	Loss 0.4067 (0.3117)	CeLoss 0.0869 (0.0444)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0961 (0.0749)	MaskBCELoss 0.0325 (0.0162)	MaskDICELoss 0.0637 (0.0587)
Epoch: [1][664/500]	Time 22.848 (22.848)	Loss 0.2210 (0.3032)	CeLoss 0.0003 (0.0450)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0596 (0.0703)	MaskBCELoss 0.0090 (0.0115)	MaskDICELoss 0.0507 (0.0587)
Epoch: [1][665/500]	Time 22.453 (22.453)	Loss 0.3715 (0.3073)	CeLoss 0.0835 (0.0293)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0785 (0.0768)	MaskBCELoss 0.0130 (0.0146)	MaskDICELoss 0.0655 (0.0622)
Epoch: [1][666/500]	Time 19.970 (19.970)	Loss 0.3368 (0.3207)	CeLoss 0.0361 (0.0352)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0921 (0.0798)	MaskBCELoss 0.0339 (0.0170)	MaskDICELoss 0.0582 (0.0629)
Epoch: [1][667/500]	Time 21.408 (21.408)	Loss 0.2908 (0.2970)	CeLoss 0.0522 (0.0232)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0616 (0.0752)	MaskBCELoss 0.0039 (0.0134)	MaskDICELoss 0.0577 (0.0617)
Epoch: [1][668/500]	Time 20.657 (20.657)	Loss 0.2132 (0.2947)	CeLoss 0.0001 (0.0302)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0578 (0.0722)	MaskBCELoss 0.0091 (0.0121)	MaskDICELoss 0.0487 (0.0600)
Epoch: [1][669/500]	Time 21.621 (21.621)	Loss 0.3447 (0.3065)	CeLoss 0.0001 (0.0221)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0909 (0.0770)	MaskBCELoss 0.0094 (0.0119)	MaskDICELoss 0.0815 (0.0652)
Epoch: [1][670/500]	Time 22.910 (22.910)	Loss 0.2531 (0.2980)	CeLoss 0.0001 (0.0171)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0694 (0.0751)	MaskBCELoss 0.0123 (0.0099)	MaskDICELoss 0.0571 (0.0653)
Epoch: [1][671/500]	Time 21.482 (21.482)	Loss 0.3020 (0.2710)	CeLoss 0.0981 (0.0182)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0576 (0.0704)	MaskBCELoss 0.0132 (0.0144)	MaskDICELoss 0.0444 (0.0560)
Epoch: [1][672/500]	Time 23.684 (23.684)	Loss 0.4668 (0.3342)	CeLoss 0.0593 (0.0360)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1155 (0.0830)	MaskBCELoss 0.0273 (0.0170)	MaskDICELoss 0.0882 (0.0660)
Epoch: [1][673/500]	Time 21.831 (21.831)	Loss 0.2553 (0.2716)	CeLoss 0.0005 (0.0268)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0693 (0.0662)	MaskBCELoss 0.0112 (0.0102)	MaskDICELoss 0.0581 (0.0560)
Epoch: [1][674/500]	Time 25.171 (25.171)	Loss 0.3117 (0.3240)	CeLoss 0.0000 (0.0204)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0844 (0.0849)	MaskBCELoss 0.0130 (0.0179)	MaskDICELoss 0.0714 (0.0669)
Epoch: [1][675/500]	Time 23.346 (23.346)	Loss 0.3300 (0.3119)	CeLoss 0.0571 (0.0205)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0803 (0.0783)	MaskBCELoss 0.0241 (0.0110)	MaskDICELoss 0.0562 (0.0673)
Epoch: [1][676/500]	Time 23.571 (23.571)	Loss 0.1598 (0.3170)	CeLoss 0.0001 (0.0335)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0404 (0.0796)	MaskBCELoss 0.0010 (0.0174)	MaskDICELoss 0.0394 (0.0622)
Epoch: [1][677/500]	Time 22.780 (22.780)	Loss 0.3320 (0.2908)	CeLoss 0.0513 (0.0190)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0741 (0.0743)	MaskBCELoss 0.0078 (0.0127)	MaskDICELoss 0.0663 (0.0616)
Epoch: [1][678/500]	Time 21.750 (21.750)	Loss 0.2772 (0.3017)	CeLoss 0.0001 (0.0104)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0814 (0.0806)	MaskBCELoss 0.0242 (0.0156)	MaskDICELoss 0.0571 (0.0650)
Epoch: [1][679/500]	Time 21.846 (21.846)	Loss 0.2541 (0.2913)	CeLoss 0.0947 (0.0229)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0404 (0.0733)	MaskBCELoss 0.0010 (0.0124)	MaskDICELoss 0.0394 (0.0609)
Epoch: [1][680/500]	Time 22.583 (22.583)	Loss 0.1908 (0.2841)	CeLoss 0.0415 (0.0200)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0420 (0.0723)	MaskBCELoss 0.0092 (0.0125)	MaskDICELoss 0.0327 (0.0598)
Epoch: [1][681/500]	Time 19.782 (19.782)	Loss 0.3036 (0.3103)	CeLoss 0.0001 (0.0215)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0849 (0.0810)	MaskBCELoss 0.0180 (0.0177)	MaskDICELoss 0.0669 (0.0634)
Epoch: [1][682/500]	Time 22.335 (22.335)	Loss 0.3212 (0.3153)	CeLoss 0.0002 (0.0301)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0884 (0.0820)	MaskBCELoss 0.0164 (0.0214)	MaskDICELoss 0.0720 (0.0606)
Epoch: [1][683/500]	Time 20.050 (20.050)	Loss 0.4813 (0.3020)	CeLoss 0.0898 (0.0207)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1150 (0.0782)	MaskBCELoss 0.0343 (0.0158)	MaskDICELoss 0.0807 (0.0624)
Epoch: [1][684/500]	Time 21.566 (21.566)	Loss 0.3136 (0.3070)	CeLoss 0.0001 (0.0379)	SegCLSLoss 0.0000 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0812 (0.0758)	MaskBCELoss 0.0056 (0.0172)	MaskDICELoss 0.0756 (0.0586)
Epoch: [1][685/500]	Time 22.283 (22.283)	Loss 0.3291 (0.3000)	CeLoss 0.0300 (0.0114)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0834 (0.0815)	MaskBCELoss 0.0173 (0.0188)	MaskDICELoss 0.0661 (0.0627)
Epoch: [1][686/500]	Time 23.415 (23.415)	Loss 0.3407 (0.3303)	CeLoss 0.0376 (0.0367)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0872 (0.0812)	MaskBCELoss 0.0228 (0.0157)	MaskDICELoss 0.0644 (0.0655)
Epoch: [1][687/500]	Time 22.457 (22.457)	Loss 0.2678 (0.3069)	CeLoss 0.1094 (0.0373)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0401 (0.0745)	MaskBCELoss 0.0011 (0.0142)	MaskDICELoss 0.0391 (0.0603)
Epoch: [1][688/500]	Time 21.832 (21.832)	Loss 0.2530 (0.2647)	CeLoss 0.0001 (0.0248)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0687 (0.0654)	MaskBCELoss 0.0109 (0.0110)	MaskDICELoss 0.0578 (0.0545)
Epoch: [1][689/500]	Time 22.236 (22.236)	Loss 0.4025 (0.2703)	CeLoss 0.0000 (0.0075)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1086 (0.0735)	MaskBCELoss 0.0160 (0.0156)	MaskDICELoss 0.0926 (0.0579)
Epoch: [1][690/500]	Time 20.710 (20.710)	Loss 0.2569 (0.2632)	CeLoss 0.0801 (0.0409)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0467 (0.0613)	MaskBCELoss 0.0050 (0.0116)	MaskDICELoss 0.0417 (0.0497)
Epoch: [1][691/500]	Time 22.119 (22.119)	Loss 0.2038 (0.2621)	CeLoss 0.0001 (0.0125)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0522 (0.0670)	MaskBCELoss 0.0026 (0.0093)	MaskDICELoss 0.0496 (0.0578)
Epoch: [1][692/500]	Time 21.729 (21.729)	Loss 0.2293 (0.2895)	CeLoss 0.0001 (0.0071)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0616 (0.0804)	MaskBCELoss 0.0087 (0.0196)	MaskDICELoss 0.0530 (0.0608)
Epoch: [1][693/500]	Time 21.834 (21.834)	Loss 0.2626 (0.2929)	CeLoss 0.0598 (0.0307)	SegCLSLoss 0.0003 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0559 (0.0724)	MaskBCELoss 0.0107 (0.0138)	MaskDICELoss 0.0451 (0.0586)
Epoch: [1][694/500]	Time 22.110 (22.110)	Loss 0.2789 (0.2765)	CeLoss 0.0001 (0.0360)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0753 (0.0649)	MaskBCELoss 0.0111 (0.0096)	MaskDICELoss 0.0641 (0.0553)
Epoch: [1][695/500]	Time 19.721 (19.721)	Loss 0.3019 (0.2914)	CeLoss 0.0117 (0.0265)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0743 (0.0719)	MaskBCELoss 0.0034 (0.0113)	MaskDICELoss 0.0708 (0.0606)
Epoch: [1][696/500]	Time 22.398 (22.398)	Loss 0.2277 (0.3009)	CeLoss 0.0001 (0.0243)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0599 (0.0769)	MaskBCELoss 0.0060 (0.0156)	MaskDICELoss 0.0539 (0.0614)
Epoch: [1][697/500]	Time 23.571 (23.571)	Loss 0.2401 (0.3037)	CeLoss 0.0820 (0.0304)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0400 (0.0749)	MaskBCELoss 0.0010 (0.0132)	MaskDICELoss 0.0390 (0.0617)
Epoch: [1][698/500]	Time 20.821 (20.821)	Loss 0.3447 (0.2636)	CeLoss 0.0001 (0.0104)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0990 (0.0714)	MaskBCELoss 0.0258 (0.0162)	MaskDICELoss 0.0732 (0.0552)
Epoch: [1][699/500]	Time 22.299 (22.299)	Loss 0.3256 (0.3012)	CeLoss 0.0000 (0.0128)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0916 (0.0797)	MaskBCELoss 0.0204 (0.0153)	MaskDICELoss 0.0712 (0.0644)
[2025-04-05 01:30:59,758] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[0.0002919759036144578], mom=[(0.9, 0.95)]
[2025-04-05 01:30:59,766] [INFO] [timer.py:215:stop] epoch=0/micro_step=700/global_step=70, RunningAvgSamplesPerSec=1.5813861757839422, CurrSamplesPerSec=2.0049334087335695, MemAllocated=35.12GB, MaxMemAllocated=46.18GB
Epoch: [1][700/500]	Time 21.818 (21.818)	Loss 0.3310 (0.2905)	CeLoss 0.0000 (0.0339)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0853 (0.0703)	MaskBCELoss 0.0052 (0.0123)	MaskDICELoss 0.0801 (0.0580)
Epoch: [1][701/500]	Time 21.299 (21.299)	Loss 0.2224 (0.3052)	CeLoss 0.0012 (0.0285)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0617 (0.0774)	MaskBCELoss 0.0128 (0.0166)	MaskDICELoss 0.0489 (0.0609)
Epoch: [1][702/500]	Time 20.337 (20.337)	Loss 0.2730 (0.2996)	CeLoss 0.0003 (0.0204)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0777 (0.0799)	MaskBCELoss 0.0192 (0.0201)	MaskDICELoss 0.0586 (0.0597)
Epoch: [1][703/500]	Time 21.507 (21.507)	Loss 0.1919 (0.2861)	CeLoss 0.0001 (0.0131)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0512 (0.0754)	MaskBCELoss 0.0066 (0.0144)	MaskDICELoss 0.0447 (0.0611)
Epoch: [1][704/500]	Time 25.544 (25.544)	Loss 0.3448 (0.3342)	CeLoss 0.0002 (0.0330)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1111 (0.0864)	MaskBCELoss 0.0500 (0.0221)	MaskDICELoss 0.0611 (0.0642)
Epoch: [1][705/500]	Time 19.437 (19.437)	Loss 0.3409 (0.2680)	CeLoss 0.0664 (0.0356)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0714 (0.0634)	MaskBCELoss 0.0055 (0.0107)	MaskDICELoss 0.0659 (0.0527)
Epoch: [1][706/500]	Time 21.216 (21.216)	Loss 0.2472 (0.3076)	CeLoss 0.0000 (0.0476)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0662 (0.0731)	MaskBCELoss 0.0088 (0.0163)	MaskDICELoss 0.0574 (0.0568)
Epoch: [1][707/500]	Time 22.142 (22.142)	Loss 0.3680 (0.3051)	CeLoss 0.0520 (0.0367)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0851 (0.0734)	MaskBCELoss 0.0122 (0.0125)	MaskDICELoss 0.0729 (0.0609)
Epoch: [1][708/500]	Time 21.438 (21.438)	Loss 0.3077 (0.2884)	CeLoss 0.0145 (0.0218)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0816 (0.0738)	MaskBCELoss 0.0166 (0.0142)	MaskDICELoss 0.0650 (0.0596)
Epoch: [1][709/500]	Time 24.230 (24.230)	Loss 0.2554 (0.3111)	CeLoss 0.0005 (0.0304)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0722 (0.0757)	MaskBCELoss 0.0170 (0.0111)	MaskDICELoss 0.0552 (0.0646)
Epoch: [1][710/500]	Time 21.901 (21.901)	Loss 0.3267 (0.2639)	CeLoss 0.0002 (0.0276)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1040 (0.0667)	MaskBCELoss 0.0447 (0.0152)	MaskDICELoss 0.0593 (0.0515)
Epoch: [1][711/500]	Time 20.989 (20.989)	Loss 0.2572 (0.2475)	CeLoss 0.0000 (0.0078)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0712 (0.0646)	MaskBCELoss 0.0138 (0.0094)	MaskDICELoss 0.0574 (0.0552)
Epoch: [1][712/500]	Time 23.222 (23.222)	Loss 0.2141 (0.2842)	CeLoss 0.0287 (0.0213)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0527 (0.0734)	MaskBCELoss 0.0128 (0.0155)	MaskDICELoss 0.0400 (0.0579)
Epoch: [1][713/500]	Time 23.490 (23.490)	Loss 0.2964 (0.3079)	CeLoss 0.0004 (0.0226)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0822 (0.0779)	MaskBCELoss 0.0165 (0.0130)	MaskDICELoss 0.0657 (0.0648)
Epoch: [1][714/500]	Time 27.034 (27.034)	Loss 0.3062 (0.3314)	CeLoss 0.0004 (0.0178)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0805 (0.0872)	MaskBCELoss 0.0081 (0.0175)	MaskDICELoss 0.0724 (0.0696)
Epoch: [1][715/500]	Time 24.558 (24.558)	Loss 0.3162 (0.3008)	CeLoss 0.0000 (0.0177)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0857 (0.0810)	MaskBCELoss 0.0134 (0.0204)	MaskDICELoss 0.0724 (0.0606)
Epoch: [1][716/500]	Time 19.383 (19.383)	Loss 0.1673 (0.2543)	CeLoss 0.0004 (0.0360)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0427 (0.0595)	MaskBCELoss 0.0019 (0.0098)	MaskDICELoss 0.0408 (0.0497)
Epoch: [1][717/500]	Time 21.957 (21.957)	Loss 0.3502 (0.3507)	CeLoss 0.0422 (0.0464)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0858 (0.0909)	MaskBCELoss 0.0177 (0.0297)	MaskDICELoss 0.0682 (0.0612)
Epoch: [1][718/500]	Time 23.614 (23.614)	Loss 0.2295 (0.2930)	CeLoss 0.0593 (0.0256)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0440 (0.0740)	MaskBCELoss 0.0030 (0.0143)	MaskDICELoss 0.0410 (0.0597)
Epoch: [1][719/500]	Time 19.731 (19.731)	Loss 0.2791 (0.3084)	CeLoss 0.0693 (0.0457)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0561 (0.0740)	MaskBCELoss 0.0074 (0.0167)	MaskDICELoss 0.0487 (0.0573)
Epoch: [1][720/500]	Time 23.082 (23.082)	Loss 0.3219 (0.3008)	CeLoss 0.0405 (0.0318)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0772 (0.0741)	MaskBCELoss 0.0138 (0.0137)	MaskDICELoss 0.0634 (0.0604)
Epoch: [1][721/500]	Time 22.707 (22.707)	Loss 0.2572 (0.2901)	CeLoss 0.0513 (0.0356)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0648 (0.0691)	MaskBCELoss 0.0265 (0.0109)	MaskDICELoss 0.0382 (0.0582)
Epoch: [1][722/500]	Time 19.372 (19.372)	Loss 0.2926 (0.2561)	CeLoss 0.0001 (0.0218)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0878 (0.0644)	MaskBCELoss 0.0293 (0.0116)	MaskDICELoss 0.0585 (0.0528)
Epoch: [1][723/500]	Time 24.123 (24.123)	Loss 0.2975 (0.3225)	CeLoss 0.0825 (0.0231)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0615 (0.0830)	MaskBCELoss 0.0155 (0.0164)	MaskDICELoss 0.0460 (0.0666)
Epoch: [1][724/500]	Time 21.680 (21.680)	Loss 0.2054 (0.2987)	CeLoss 0.0001 (0.0320)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0562 (0.0740)	MaskBCELoss 0.0096 (0.0147)	MaskDICELoss 0.0465 (0.0593)
Epoch: [1][725/500]	Time 20.926 (20.926)	Loss 0.3045 (0.2597)	CeLoss 0.0415 (0.0230)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0697 (0.0637)	MaskBCELoss 0.0078 (0.0092)	MaskDICELoss 0.0619 (0.0546)
Epoch: [1][726/500]	Time 25.129 (25.129)	Loss 0.3380 (0.3192)	CeLoss 0.0194 (0.0516)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1028 (0.0750)	MaskBCELoss 0.0464 (0.0161)	MaskDICELoss 0.0564 (0.0588)
Epoch: [1][727/500]	Time 20.284 (20.284)	Loss 0.2222 (0.2660)	CeLoss 0.0723 (0.0268)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0378 (0.0649)	MaskBCELoss 0.0007 (0.0102)	MaskDICELoss 0.0372 (0.0547)
Epoch: [1][728/500]	Time 21.079 (21.079)	Loss 0.4037 (0.2854)	CeLoss 0.0354 (0.0287)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1107 (0.0710)	MaskBCELoss 0.0372 (0.0136)	MaskDICELoss 0.0735 (0.0574)
Epoch: [1][729/500]	Time 26.280 (26.280)	Loss 0.1501 (0.3059)	CeLoss 0.0001 (0.0447)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0378 (0.0722)	MaskBCELoss 0.0007 (0.0139)	MaskDICELoss 0.0371 (0.0583)
Epoch: [1][730/500]	Time 22.671 (22.671)	Loss 0.2400 (0.2694)	CeLoss 0.0001 (0.0074)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0655 (0.0727)	MaskBCELoss 0.0110 (0.0144)	MaskDICELoss 0.0545 (0.0583)
Epoch: [1][731/500]	Time 20.497 (20.497)	Loss 0.1475 (0.2558)	CeLoss 0.0001 (0.0097)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0372 (0.0687)	MaskBCELoss 0.0007 (0.0144)	MaskDICELoss 0.0365 (0.0543)
Epoch: [1][732/500]	Time 24.689 (24.689)	Loss 0.3349 (0.3305)	CeLoss 0.0491 (0.0281)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0781 (0.0829)	MaskBCELoss 0.0132 (0.0147)	MaskDICELoss 0.0649 (0.0682)
Epoch: [1][733/500]	Time 22.262 (22.262)	Loss 0.3303 (0.2465)	CeLoss 0.0002 (0.0129)	SegCLSLoss 0.0001 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0950 (0.0634)	MaskBCELoss 0.0251 (0.0099)	MaskDICELoss 0.0699 (0.0534)
Epoch: [1][734/500]	Time 21.217 (21.217)	Loss 0.2481 (0.3052)	CeLoss 0.1006 (0.0338)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0371 (0.0743)	MaskBCELoss 0.0007 (0.0128)	MaskDICELoss 0.0365 (0.0614)
Epoch: [1][735/500]	Time 24.460 (24.460)	Loss 0.2690 (0.2839)	CeLoss 0.0869 (0.0241)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0500 (0.0721)	MaskBCELoss 0.0088 (0.0143)	MaskDICELoss 0.0412 (0.0578)
Epoch: [1][736/500]	Time 23.536 (23.536)	Loss 0.2397 (0.2963)	CeLoss 0.0003 (0.0191)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0625 (0.0748)	MaskBCELoss 0.0053 (0.0109)	MaskDICELoss 0.0572 (0.0638)
Epoch: [1][737/500]	Time 23.346 (23.346)	Loss 0.2451 (0.3280)	CeLoss 0.0001 (0.0297)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0689 (0.0841)	MaskBCELoss 0.0153 (0.0193)	MaskDICELoss 0.0536 (0.0649)
Epoch: [1][738/500]	Time 25.140 (25.140)	Loss 0.2350 (0.3151)	CeLoss 0.0036 (0.0447)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0600 (0.0765)	MaskBCELoss 0.0043 (0.0178)	MaskDICELoss 0.0557 (0.0587)
Epoch: [1][739/500]	Time 20.248 (20.248)	Loss 0.3280 (0.2728)	CeLoss 0.0000 (0.0222)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0933 (0.0669)	MaskBCELoss 0.0226 (0.0085)	MaskDICELoss 0.0707 (0.0584)
Epoch: [1][740/500]	Time 22.146 (22.146)	Loss 0.2741 (0.3150)	CeLoss 0.0547 (0.0305)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0595 (0.0767)	MaskBCELoss 0.0093 (0.0112)	MaskDICELoss 0.0502 (0.0655)
Epoch: [1][741/500]	Time 22.341 (22.341)	Loss 0.4160 (0.3019)	CeLoss 0.0000 (0.0095)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1225 (0.0813)	MaskBCELoss 0.0370 (0.0164)	MaskDICELoss 0.0855 (0.0649)
Epoch: [1][742/500]	Time 23.666 (23.666)	Loss 0.3270 (0.2677)	CeLoss 0.0566 (0.0220)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0763 (0.0669)	MaskBCELoss 0.0175 (0.0110)	MaskDICELoss 0.0589 (0.0559)
Epoch: [1][743/500]	Time 24.357 (24.357)	Loss 0.3113 (0.3244)	CeLoss 0.0001 (0.0147)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0893 (0.0877)	MaskBCELoss 0.0230 (0.0205)	MaskDICELoss 0.0663 (0.0672)
Epoch: [1][744/500]	Time 19.987 (19.987)	Loss 0.3400 (0.2582)	CeLoss 0.0001 (0.0181)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0980 (0.0658)	MaskBCELoss 0.0260 (0.0116)	MaskDICELoss 0.0720 (0.0542)
Epoch: [1][745/500]	Time 19.309 (19.309)	Loss 0.3164 (0.3045)	CeLoss 0.0688 (0.0400)	SegCLSLoss 0.0002 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0706 (0.0717)	MaskBCELoss 0.0176 (0.0112)	MaskDICELoss 0.0530 (0.0606)
Epoch: [1][746/500]	Time 21.678 (21.678)	Loss 0.2142 (0.2789)	CeLoss 0.0442 (0.0262)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0441 (0.0699)	MaskBCELoss 0.0033 (0.0135)	MaskDICELoss 0.0409 (0.0564)
Epoch: [1][747/500]	Time 20.986 (20.986)	Loss 0.2413 (0.3030)	CeLoss 0.0186 (0.0312)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0585 (0.0734)	MaskBCELoss 0.0057 (0.0110)	MaskDICELoss 0.0528 (0.0625)
Epoch: [1][748/500]	Time 22.404 (22.404)	Loss 0.3445 (0.3028)	CeLoss 0.0535 (0.0479)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0880 (0.0698)	MaskBCELoss 0.0305 (0.0122)	MaskDICELoss 0.0575 (0.0576)
Epoch: [1][749/500]	Time 22.243 (22.243)	Loss 0.1732 (0.2934)	CeLoss 0.0001 (0.0159)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0467 (0.0775)	MaskBCELoss 0.0068 (0.0164)	MaskDICELoss 0.0399 (0.0612)
Epoch: [1][750/500]	Time 25.363 (25.363)	Loss 0.2377 (0.2808)	CeLoss 0.0918 (0.0245)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0368 (0.0698)	MaskBCELoss 0.0007 (0.0114)	MaskDICELoss 0.0361 (0.0584)
Epoch: [1][751/500]	Time 22.334 (22.334)	Loss 0.2314 (0.2759)	CeLoss 0.0859 (0.0347)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0368 (0.0657)	MaskBCELoss 0.0008 (0.0109)	MaskDICELoss 0.0360 (0.0548)
Epoch: [1][752/500]	Time 22.450 (22.450)	Loss 0.3045 (0.2913)	CeLoss 0.0503 (0.0387)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0775 (0.0698)	MaskBCELoss 0.0279 (0.0132)	MaskDICELoss 0.0496 (0.0565)
Epoch: [1][753/500]	Time 24.046 (24.046)	Loss 0.4111 (0.2895)	CeLoss 0.0001 (0.0303)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1122 (0.0705)	MaskBCELoss 0.0189 (0.0115)	MaskDICELoss 0.0933 (0.0591)
Epoch: [1][754/500]	Time 22.877 (22.877)	Loss 0.2968 (0.2725)	CeLoss 0.0000 (0.0046)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0761 (0.0724)	MaskBCELoss 0.0038 (0.0108)	MaskDICELoss 0.0723 (0.0616)
Epoch: [1][755/500]	Time 23.309 (23.309)	Loss 0.3101 (0.2799)	CeLoss 0.0271 (0.0195)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0760 (0.0711)	MaskBCELoss 0.0106 (0.0120)	MaskDICELoss 0.0654 (0.0591)
Epoch: [1][756/500]	Time 19.829 (19.829)	Loss 0.3479 (0.2723)	CeLoss 0.0347 (0.0322)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0823 (0.0651)	MaskBCELoss 0.0079 (0.0103)	MaskDICELoss 0.0743 (0.0549)
Epoch: [1][757/500]	Time 20.515 (20.515)	Loss 0.2431 (0.2941)	CeLoss 0.0732 (0.0329)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0459 (0.0728)	MaskBCELoss 0.0070 (0.0150)	MaskDICELoss 0.0389 (0.0578)
Epoch: [1][758/500]	Time 24.459 (24.459)	Loss 0.3651 (0.2961)	CeLoss 0.0991 (0.0321)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0759 (0.0720)	MaskBCELoss 0.0189 (0.0120)	MaskDICELoss 0.0570 (0.0600)
Epoch: [1][759/500]	Time 21.196 (21.196)	Loss 0.2281 (0.2743)	CeLoss 0.0493 (0.0497)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0463 (0.0607)	MaskBCELoss 0.0033 (0.0091)	MaskDICELoss 0.0430 (0.0516)
Epoch: [1][760/500]	Time 23.592 (23.592)	Loss 0.1889 (0.3177)	CeLoss 0.0001 (0.0149)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0506 (0.0832)	MaskBCELoss 0.0069 (0.0149)	MaskDICELoss 0.0438 (0.0682)
Epoch: [1][761/500]	Time 20.216 (20.216)	Loss 0.2655 (0.2472)	CeLoss 0.0006 (0.0215)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0773 (0.0626)	MaskBCELoss 0.0221 (0.0123)	MaskDICELoss 0.0552 (0.0503)
Epoch: [1][762/500]	Time 22.462 (22.462)	Loss 0.2408 (0.2717)	CeLoss 0.0002 (0.0157)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0647 (0.0706)	MaskBCELoss 0.0092 (0.0131)	MaskDICELoss 0.0555 (0.0575)
Epoch: [1][763/500]	Time 21.759 (21.759)	Loss 0.2484 (0.2770)	CeLoss 0.0613 (0.0174)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0516 (0.0712)	MaskBCELoss 0.0097 (0.0126)	MaskDICELoss 0.0419 (0.0586)
Epoch: [1][764/500]	Time 23.312 (23.312)	Loss 0.2770 (0.3209)	CeLoss 0.0000 (0.0171)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0748 (0.0872)	MaskBCELoss 0.0111 (0.0224)	MaskDICELoss 0.0637 (0.0647)
Epoch: [1][765/500]	Time 19.327 (19.327)	Loss 0.1815 (0.3152)	CeLoss 0.0001 (0.0365)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0477 (0.0753)	MaskBCELoss 0.0048 (0.0113)	MaskDICELoss 0.0430 (0.0640)
Epoch: [1][766/500]	Time 18.657 (18.657)	Loss 0.2163 (0.3010)	CeLoss 0.0002 (0.0149)	SegCLSLoss 0.0008 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0616 (0.0798)	MaskBCELoss 0.0160 (0.0167)	MaskDICELoss 0.0456 (0.0631)
Epoch: [1][767/500]	Time 16.997 (16.997)	Loss 0.2213 (0.2733)	CeLoss 0.0001 (0.0248)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0610 (0.0675)	MaskBCELoss 0.0114 (0.0108)	MaskDICELoss 0.0496 (0.0566)
Epoch: [1][768/500]	Time 19.108 (19.108)	Loss 0.3384 (0.2876)	CeLoss 0.0277 (0.0281)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0790 (0.0718)	MaskBCELoss 0.0026 (0.0139)	MaskDICELoss 0.0764 (0.0579)
Epoch: [1][769/500]	Time 15.469 (15.469)	Loss 0.2804 (0.2568)	CeLoss 0.0001 (0.0208)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0742 (0.0628)	MaskBCELoss 0.0082 (0.0075)	MaskDICELoss 0.0660 (0.0552)
Epoch: [1][770/500]	Time 18.904 (18.904)	Loss 0.2575 (0.2797)	CeLoss 0.0001 (0.0178)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0657 (0.0725)	MaskBCELoss 0.0026 (0.0141)	MaskDICELoss 0.0631 (0.0584)
Epoch: [1][771/500]	Time 17.737 (17.737)	Loss 0.2524 (0.2664)	CeLoss 0.0571 (0.0232)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0510 (0.0659)	MaskBCELoss 0.0044 (0.0103)	MaskDICELoss 0.0466 (0.0556)
Epoch: [1][772/500]	Time 17.350 (17.350)	Loss 0.1625 (0.2353)	CeLoss 0.0001 (0.0289)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0434 (0.0568)	MaskBCELoss 0.0055 (0.0105)	MaskDICELoss 0.0378 (0.0463)
Epoch: [1][773/500]	Time 17.447 (17.447)	Loss 0.3862 (0.2869)	CeLoss 0.0009 (0.0226)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1130 (0.0724)	MaskBCELoss 0.0333 (0.0126)	MaskDICELoss 0.0797 (0.0598)
Epoch: [1][774/500]	Time 19.028 (19.028)	Loss 0.2776 (0.2733)	CeLoss 0.0264 (0.0075)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0680 (0.0735)	MaskBCELoss 0.0104 (0.0141)	MaskDICELoss 0.0576 (0.0594)
Epoch: [1][775/500]	Time 18.288 (18.288)	Loss 0.3932 (0.3072)	CeLoss 0.0811 (0.0346)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0939 (0.0741)	MaskBCELoss 0.0317 (0.0119)	MaskDICELoss 0.0623 (0.0622)
Epoch: [1][776/500]	Time 17.827 (17.827)	Loss 0.2078 (0.2599)	CeLoss 0.0001 (0.0122)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0613 (0.0675)	MaskBCELoss 0.0188 (0.0112)	MaskDICELoss 0.0425 (0.0563)
Epoch: [1][777/500]	Time 17.350 (17.350)	Loss 0.2280 (0.2611)	CeLoss 0.0003 (0.0149)	SegCLSLoss 0.0001 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0631 (0.0684)	MaskBCELoss 0.0125 (0.0138)	MaskDICELoss 0.0506 (0.0546)
Epoch: [1][778/500]	Time 19.653 (19.653)	Loss 0.2609 (0.3046)	CeLoss 0.0000 (0.0266)	SegCLSLoss 0.0001 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0714 (0.0750)	MaskBCELoss 0.0124 (0.0110)	MaskDICELoss 0.0589 (0.0640)
Epoch: [1][779/500]	Time 19.309 (19.309)	Loss 0.2058 (0.2765)	CeLoss 0.0625 (0.0264)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0362 (0.0677)	MaskBCELoss 0.0007 (0.0103)	MaskDICELoss 0.0354 (0.0573)
Epoch: [1][780/500]	Time 17.393 (17.393)	Loss 0.3125 (0.2548)	CeLoss 0.0771 (0.0240)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0636 (0.0633)	MaskBCELoss 0.0095 (0.0113)	MaskDICELoss 0.0540 (0.0521)
Epoch: [1][781/500]	Time 20.085 (20.085)	Loss 0.2227 (0.3014)	CeLoss 0.0786 (0.0404)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0363 (0.0718)	MaskBCELoss 0.0006 (0.0131)	MaskDICELoss 0.0358 (0.0587)
Epoch: [1][782/500]	Time 18.291 (18.291)	Loss 0.3082 (0.2918)	CeLoss 0.0000 (0.0255)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0840 (0.0747)	MaskBCELoss 0.0138 (0.0162)	MaskDICELoss 0.0701 (0.0585)
Epoch: [1][783/500]	Time 17.939 (17.939)	Loss 0.3221 (0.2878)	CeLoss 0.0001 (0.0266)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0952 (0.0707)	MaskBCELoss 0.0294 (0.0109)	MaskDICELoss 0.0658 (0.0598)
Epoch: [1][784/500]	Time 19.672 (19.672)	Loss 0.2605 (0.2924)	CeLoss 0.0001 (0.0249)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0699 (0.0740)	MaskBCELoss 0.0096 (0.0143)	MaskDICELoss 0.0603 (0.0597)
Epoch: [1][785/500]	Time 19.430 (19.430)	Loss 0.3400 (0.2845)	CeLoss 0.0001 (0.0042)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0932 (0.0780)	MaskBCELoss 0.0165 (0.0158)	MaskDICELoss 0.0767 (0.0622)
Epoch: [1][786/500]	Time 17.752 (17.752)	Loss 0.2627 (0.2745)	CeLoss 0.0001 (0.0241)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0741 (0.0690)	MaskBCELoss 0.0169 (0.0129)	MaskDICELoss 0.0572 (0.0562)
Epoch: [1][787/500]	Time 20.033 (20.033)	Loss 0.3938 (0.3386)	CeLoss 0.0378 (0.0360)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1059 (0.0859)	MaskBCELoss 0.0338 (0.0204)	MaskDICELoss 0.0720 (0.0654)
Epoch: [1][788/500]	Time 18.345 (18.345)	Loss 0.4087 (0.2830)	CeLoss 0.0001 (0.0267)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1072 (0.0687)	MaskBCELoss 0.0100 (0.0092)	MaskDICELoss 0.0971 (0.0595)
Epoch: [1][789/500]	Time 16.310 (16.310)	Loss 0.1578 (0.2593)	CeLoss 0.0002 (0.0208)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0405 (0.0649)	MaskBCELoss 0.0021 (0.0105)	MaskDICELoss 0.0383 (0.0544)
Epoch: [1][790/500]	Time 19.126 (19.126)	Loss 0.2851 (0.2939)	CeLoss 0.0001 (0.0142)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0794 (0.0770)	MaskBCELoss 0.0163 (0.0143)	MaskDICELoss 0.0631 (0.0628)
Epoch: [1][791/500]	Time 18.557 (18.557)	Loss 0.2323 (0.2704)	CeLoss 0.0002 (0.0186)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0665 (0.0701)	MaskBCELoss 0.0170 (0.0143)	MaskDICELoss 0.0495 (0.0558)
Epoch: [1][792/500]	Time 17.835 (17.835)	Loss 0.2523 (0.2949)	CeLoss 0.0007 (0.0432)	SegCLSLoss 0.0013 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0698 (0.0684)	MaskBCELoss 0.0151 (0.0111)	MaskDICELoss 0.0547 (0.0573)
Epoch: [1][793/500]	Time 21.037 (21.037)	Loss 0.3067 (0.2845)	CeLoss 0.0398 (0.0297)	SegCLSLoss 0.0001 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0754 (0.0685)	MaskBCELoss 0.0175 (0.0096)	MaskDICELoss 0.0579 (0.0589)
Epoch: [1][794/500]	Time 19.942 (19.942)	Loss 0.2950 (0.2897)	CeLoss 0.0001 (0.0074)	SegCLSLoss 0.0001 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0806 (0.0773)	MaskBCELoss 0.0139 (0.0134)	MaskDICELoss 0.0668 (0.0639)
Epoch: [1][795/500]	Time 18.768 (18.768)	Loss 0.3495 (0.2881)	CeLoss 0.0439 (0.0155)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0880 (0.0760)	MaskBCELoss 0.0232 (0.0157)	MaskDICELoss 0.0648 (0.0603)
Epoch: [1][796/500]	Time 18.800 (18.800)	Loss 0.3580 (0.2972)	CeLoss 0.0271 (0.0227)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0970 (0.0772)	MaskBCELoss 0.0285 (0.0172)	MaskDICELoss 0.0685 (0.0600)
Epoch: [1][797/500]	Time 18.022 (18.022)	Loss 0.2738 (0.2950)	CeLoss 0.0001 (0.0319)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0726 (0.0722)	MaskBCELoss 0.0083 (0.0131)	MaskDICELoss 0.0643 (0.0592)
Epoch: [1][798/500]	Time 17.237 (17.237)	Loss 0.2345 (0.2798)	CeLoss 0.0001 (0.0284)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0657 (0.0700)	MaskBCELoss 0.0142 (0.0144)	MaskDICELoss 0.0515 (0.0557)
Epoch: [1][799/500]	Time 18.466 (18.466)	Loss 0.3248 (0.2726)	CeLoss 0.0001 (0.0145)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0948 (0.0728)	MaskBCELoss 0.0272 (0.0166)	MaskDICELoss 0.0676 (0.0562)
[2025-04-05 02:05:58,318] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[0.00029065060240963855], mom=[(0.9, 0.95)]
[2025-04-05 02:05:58,327] [INFO] [timer.py:215:stop] epoch=0/micro_step=800/global_step=80, RunningAvgSamplesPerSec=1.6190815662129656, CurrSamplesPerSec=1.9635985789988597, MemAllocated=33.72GB, MaxMemAllocated=46.18GB
Epoch: [1][800/500]	Time 20.475 (20.475)	Loss 0.2049 (0.2961)	CeLoss 0.0002 (0.0252)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0524 (0.0762)	MaskBCELoss 0.0024 (0.0169)	MaskDICELoss 0.0500 (0.0593)
Epoch: [1][801/500]	Time 17.282 (17.282)	Loss 0.1444 (0.2758)	CeLoss 0.0001 (0.0353)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0364 (0.0681)	MaskBCELoss 0.0007 (0.0161)	MaskDICELoss 0.0358 (0.0520)
Epoch: [1][802/500]	Time 18.811 (18.811)	Loss 0.3833 (0.3040)	CeLoss 0.0001 (0.0284)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1110 (0.0781)	MaskBCELoss 0.0304 (0.0184)	MaskDICELoss 0.0806 (0.0597)
Epoch: [1][803/500]	Time 18.463 (18.463)	Loss 0.2379 (0.2815)	CeLoss 0.0001 (0.0320)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0658 (0.0678)	MaskBCELoss 0.0127 (0.0109)	MaskDICELoss 0.0531 (0.0569)
Epoch: [1][804/500]	Time 18.319 (18.319)	Loss 0.3018 (0.2773)	CeLoss 0.0002 (0.0225)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0940 (0.0723)	MaskBCELoss 0.0372 (0.0171)	MaskDICELoss 0.0568 (0.0551)
Epoch: [1][805/500]	Time 17.306 (17.306)	Loss 0.2833 (0.2640)	CeLoss 0.0001 (0.0174)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0773 (0.0678)	MaskBCELoss 0.0131 (0.0123)	MaskDICELoss 0.0643 (0.0555)
Epoch: [1][806/500]	Time 18.494 (18.494)	Loss 0.3366 (0.2838)	CeLoss 0.0354 (0.0269)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0928 (0.0748)	MaskBCELoss 0.0351 (0.0211)	MaskDICELoss 0.0578 (0.0537)
Epoch: [1][807/500]	Time 16.451 (16.451)	Loss 0.1979 (0.2738)	CeLoss 0.0398 (0.0422)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0421 (0.0664)	MaskBCELoss 0.0052 (0.0169)	MaskDICELoss 0.0369 (0.0494)
Epoch: [1][808/500]	Time 16.403 (16.403)	Loss 0.2206 (0.2401)	CeLoss 0.0762 (0.0215)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0364 (0.0583)	MaskBCELoss 0.0007 (0.0073)	MaskDICELoss 0.0358 (0.0510)
Epoch: [1][809/500]	Time 19.353 (19.353)	Loss 0.3534 (0.3141)	CeLoss 0.0762 (0.0284)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0702 (0.0813)	MaskBCELoss 0.0018 (0.0198)	MaskDICELoss 0.0684 (0.0615)
Epoch: [1][810/500]	Time 18.411 (18.411)	Loss 0.2599 (0.3041)	CeLoss 0.0559 (0.0315)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0580 (0.0773)	MaskBCELoss 0.0140 (0.0182)	MaskDICELoss 0.0440 (0.0590)
Epoch: [1][811/500]	Time 20.135 (20.135)	Loss 0.2388 (0.2960)	CeLoss 0.0012 (0.0246)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0702 (0.0761)	MaskBCELoss 0.0216 (0.0164)	MaskDICELoss 0.0486 (0.0596)
Epoch: [1][812/500]	Time 22.521 (22.521)	Loss 0.3088 (0.3436)	CeLoss 0.0347 (0.0207)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0766 (0.0901)	MaskBCELoss 0.0160 (0.0188)	MaskDICELoss 0.0605 (0.0713)
Epoch: [1][813/500]	Time 19.584 (19.584)	Loss 0.3144 (0.3104)	CeLoss 0.0002 (0.0237)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0852 (0.0808)	MaskBCELoss 0.0134 (0.0183)	MaskDICELoss 0.0718 (0.0625)
Epoch: [1][814/500]	Time 19.538 (19.538)	Loss 0.2528 (0.2633)	CeLoss 0.0002 (0.0097)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0745 (0.0709)	MaskBCELoss 0.0228 (0.0151)	MaskDICELoss 0.0518 (0.0558)
Epoch: [1][815/500]	Time 16.662 (16.662)	Loss 0.2741 (0.2387)	CeLoss 0.0547 (0.0244)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0557 (0.0569)	MaskBCELoss 0.0018 (0.0067)	MaskDICELoss 0.0540 (0.0502)
Epoch: [1][816/500]	Time 22.104 (22.104)	Loss 0.3757 (0.3021)	CeLoss 0.0430 (0.0366)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0935 (0.0757)	MaskBCELoss 0.0206 (0.0188)	MaskDICELoss 0.0729 (0.0570)
Epoch: [1][817/500]	Time 17.647 (17.647)	Loss 0.1932 (0.2679)	CeLoss 0.0001 (0.0038)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0531 (0.0746)	MaskBCELoss 0.0096 (0.0172)	MaskDICELoss 0.0435 (0.0574)
Epoch: [1][818/500]	Time 16.466 (16.466)	Loss 0.3194 (0.2596)	CeLoss 0.0742 (0.0305)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0621 (0.0625)	MaskBCELoss 0.0015 (0.0105)	MaskDICELoss 0.0605 (0.0520)
Epoch: [1][819/500]	Time 17.985 (17.985)	Loss 0.2897 (0.2564)	CeLoss 0.0005 (0.0169)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0811 (0.0669)	MaskBCELoss 0.0176 (0.0140)	MaskDICELoss 0.0635 (0.0529)
Epoch: [1][820/500]	Time 19.317 (19.317)	Loss 0.3805 (0.2989)	CeLoss 0.0002 (0.0078)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1029 (0.0818)	MaskBCELoss 0.0156 (0.0181)	MaskDICELoss 0.0873 (0.0637)
Epoch: [1][821/500]	Time 20.389 (20.389)	Loss 0.3280 (0.3011)	CeLoss 0.0194 (0.0110)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0874 (0.0795)	MaskBCELoss 0.0204 (0.0140)	MaskDICELoss 0.0669 (0.0656)
Epoch: [1][822/500]	Time 18.772 (18.772)	Loss 0.3448 (0.2893)	CeLoss 0.0003 (0.0197)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0919 (0.0720)	MaskBCELoss 0.0115 (0.0094)	MaskDICELoss 0.0804 (0.0626)
Epoch: [1][823/500]	Time 18.491 (18.491)	Loss 0.2075 (0.2684)	CeLoss 0.0000 (0.0245)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0543 (0.0670)	MaskBCELoss 0.0049 (0.0120)	MaskDICELoss 0.0494 (0.0550)
Epoch: [1][824/500]	Time 20.666 (20.666)	Loss 0.4593 (0.2784)	CeLoss 0.0211 (0.0093)	SegCLSLoss 0.0002 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1358 (0.0758)	MaskBCELoss 0.0528 (0.0170)	MaskDICELoss 0.0830 (0.0587)
Epoch: [1][825/500]	Time 20.040 (20.040)	Loss 0.3442 (0.3084)	CeLoss 0.0004 (0.0314)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0945 (0.0750)	MaskBCELoss 0.0170 (0.0116)	MaskDICELoss 0.0774 (0.0635)
Epoch: [1][826/500]	Time 19.489 (19.489)	Loss 0.1962 (0.2859)	CeLoss 0.0620 (0.0175)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0338 (0.0739)	MaskBCELoss 0.0006 (0.0135)	MaskDICELoss 0.0332 (0.0604)
Epoch: [1][827/500]	Time 19.421 (19.421)	Loss 0.2418 (0.2810)	CeLoss 0.0001 (0.0045)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0651 (0.0769)	MaskBCELoss 0.0094 (0.0155)	MaskDICELoss 0.0557 (0.0614)
Epoch: [1][828/500]	Time 21.300 (21.300)	Loss 0.3248 (0.3149)	CeLoss 0.0928 (0.0262)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0661 (0.0788)	MaskBCELoss 0.0161 (0.0132)	MaskDICELoss 0.0500 (0.0656)
Epoch: [1][829/500]	Time 17.965 (17.965)	Loss 0.2701 (0.2557)	CeLoss 0.0005 (0.0228)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0704 (0.0626)	MaskBCELoss 0.0060 (0.0087)	MaskDICELoss 0.0644 (0.0539)
Epoch: [1][830/500]	Time 20.469 (20.469)	Loss 0.2756 (0.3109)	CeLoss 0.0613 (0.0197)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0544 (0.0793)	MaskBCELoss 0.0018 (0.0132)	MaskDICELoss 0.0527 (0.0662)
Epoch: [1][831/500]	Time 19.342 (19.342)	Loss 0.3532 (0.2771)	CeLoss 0.0625 (0.0164)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0741 (0.0699)	MaskBCELoss 0.0028 (0.0094)	MaskDICELoss 0.0713 (0.0605)
Epoch: [1][832/500]	Time 16.774 (16.774)	Loss 0.2872 (0.2891)	CeLoss 0.0000 (0.0313)	SegCLSLoss 0.0000 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0791 (0.0711)	MaskBCELoss 0.0147 (0.0137)	MaskDICELoss 0.0645 (0.0575)
Epoch: [1][833/500]	Time 19.360 (19.360)	Loss 0.2969 (0.3123)	CeLoss 0.0153 (0.0164)	SegCLSLoss 0.0001 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0770 (0.0875)	MaskBCELoss 0.0133 (0.0270)	MaskDICELoss 0.0637 (0.0605)
Epoch: [1][834/500]	Time 19.211 (19.211)	Loss 0.2773 (0.2711)	CeLoss 0.0635 (0.0217)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0537 (0.0671)	MaskBCELoss 0.0005 (0.0096)	MaskDICELoss 0.0533 (0.0576)
Epoch: [1][835/500]	Time 17.636 (17.636)	Loss 0.2573 (0.2477)	CeLoss 0.0002 (0.0117)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0650 (0.0654)	MaskBCELoss 0.0013 (0.0128)	MaskDICELoss 0.0636 (0.0526)
Epoch: [1][836/500]	Time 17.334 (17.334)	Loss 0.2358 (0.2553)	CeLoss 0.0001 (0.0177)	SegCLSLoss 0.0002 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0632 (0.0628)	MaskBCELoss 0.0087 (0.0069)	MaskDICELoss 0.0545 (0.0560)
Epoch: [1][837/500]	Time 18.098 (18.098)	Loss 0.1868 (0.2826)	CeLoss 0.0001 (0.0258)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0498 (0.0697)	MaskBCELoss 0.0062 (0.0110)	MaskDICELoss 0.0436 (0.0587)
Epoch: [1][838/500]	Time 19.080 (19.080)	Loss 0.2326 (0.2574)	CeLoss 0.0002 (0.0288)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0615 (0.0618)	MaskBCELoss 0.0069 (0.0093)	MaskDICELoss 0.0547 (0.0525)
Epoch: [1][839/500]	Time 18.379 (18.379)	Loss 0.3738 (0.2464)	CeLoss 0.0005 (0.0288)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1055 (0.0603)	MaskBCELoss 0.0243 (0.0119)	MaskDICELoss 0.0812 (0.0484)
Epoch: [1][840/500]	Time 17.874 (17.874)	Loss 0.2384 (0.2661)	CeLoss 0.0005 (0.0101)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0674 (0.0707)	MaskBCELoss 0.0159 (0.0135)	MaskDICELoss 0.0515 (0.0572)
Epoch: [1][841/500]	Time 18.082 (18.082)	Loss 0.2619 (0.2396)	CeLoss 0.0437 (0.0308)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0621 (0.0557)	MaskBCELoss 0.0151 (0.0071)	MaskDICELoss 0.0470 (0.0486)
Epoch: [1][842/500]	Time 17.250 (17.250)	Loss 0.1681 (0.2589)	CeLoss 0.0001 (0.0200)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0432 (0.0657)	MaskBCELoss 0.0024 (0.0119)	MaskDICELoss 0.0408 (0.0538)
Epoch: [1][843/500]	Time 18.909 (18.909)	Loss 0.2138 (0.2734)	CeLoss 0.0001 (0.0236)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0633 (0.0694)	MaskBCELoss 0.0196 (0.0139)	MaskDICELoss 0.0436 (0.0555)
Epoch: [1][844/500]	Time 17.329 (17.329)	Loss 0.2127 (0.2840)	CeLoss 0.0830 (0.0357)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0328 (0.0679)	MaskBCELoss 0.0006 (0.0117)	MaskDICELoss 0.0322 (0.0562)
Epoch: [1][845/500]	Time 18.819 (18.819)	Loss 0.2603 (0.2498)	CeLoss 0.0156 (0.0152)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0666 (0.0637)	MaskBCELoss 0.0108 (0.0101)	MaskDICELoss 0.0558 (0.0536)
Epoch: [1][846/500]	Time 20.605 (20.605)	Loss 0.2964 (0.3117)	CeLoss 0.0430 (0.0280)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0728 (0.0780)	MaskBCELoss 0.0188 (0.0143)	MaskDICELoss 0.0540 (0.0638)
Epoch: [1][847/500]	Time 18.452 (18.452)	Loss 0.1577 (0.2459)	CeLoss 0.0000 (0.0126)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0452 (0.0660)	MaskBCELoss 0.0117 (0.0154)	MaskDICELoss 0.0336 (0.0506)
Epoch: [1][848/500]	Time 19.829 (19.829)	Loss 0.1301 (0.2935)	CeLoss 0.0001 (0.0190)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0328 (0.0757)	MaskBCELoss 0.0006 (0.0141)	MaskDICELoss 0.0322 (0.0615)
Epoch: [1][849/500]	Time 18.444 (18.444)	Loss 0.1595 (0.2490)	CeLoss 0.0425 (0.0205)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0314 (0.0622)	MaskBCELoss 0.0043 (0.0102)	MaskDICELoss 0.0271 (0.0520)
Epoch: [1][850/500]	Time 17.001 (17.001)	Loss 0.2187 (0.2433)	CeLoss 0.0771 (0.0256)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0394 (0.0620)	MaskBCELoss 0.0079 (0.0151)	MaskDICELoss 0.0315 (0.0469)
Epoch: [1][851/500]	Time 18.236 (18.236)	Loss 0.5001 (0.3350)	CeLoss 0.1006 (0.0324)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1090 (0.0841)	MaskBCELoss 0.0184 (0.0170)	MaskDICELoss 0.0906 (0.0672)
Epoch: [1][852/500]	Time 19.297 (19.297)	Loss 0.3614 (0.3059)	CeLoss 0.0004 (0.0304)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1058 (0.0766)	MaskBCELoss 0.0311 (0.0156)	MaskDICELoss 0.0747 (0.0610)
Epoch: [1][853/500]	Time 20.576 (20.576)	Loss 0.3583 (0.3290)	CeLoss 0.0001 (0.0200)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0978 (0.0865)	MaskBCELoss 0.0164 (0.0185)	MaskDICELoss 0.0813 (0.0680)
Epoch: [1][854/500]	Time 20.655 (20.655)	Loss 0.2463 (0.2828)	CeLoss 0.0009 (0.0230)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0691 (0.0737)	MaskBCELoss 0.0155 (0.0176)	MaskDICELoss 0.0536 (0.0561)
Epoch: [1][855/500]	Time 18.257 (18.257)	Loss 0.2030 (0.2691)	CeLoss 0.0006 (0.0179)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0550 (0.0702)	MaskBCELoss 0.0087 (0.0148)	MaskDICELoss 0.0462 (0.0553)
Epoch: [1][856/500]	Time 21.793 (21.793)	Loss 0.2900 (0.3264)	CeLoss 0.0189 (0.0191)	SegCLSLoss 0.0004 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0794 (0.0870)	MaskBCELoss 0.0237 (0.0204)	MaskDICELoss 0.0557 (0.0666)
Epoch: [1][857/500]	Time 18.683 (18.683)	Loss 0.3146 (0.2834)	CeLoss 0.0001 (0.0240)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0863 (0.0719)	MaskBCELoss 0.0154 (0.0142)	MaskDICELoss 0.0709 (0.0577)
Epoch: [1][858/500]	Time 17.680 (17.680)	Loss 0.2905 (0.2819)	CeLoss 0.0000 (0.0159)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0816 (0.0740)	MaskBCELoss 0.0181 (0.0150)	MaskDICELoss 0.0636 (0.0590)
Epoch: [1][859/500]	Time 20.250 (20.250)	Loss 0.2313 (0.3221)	CeLoss 0.0015 (0.0380)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0627 (0.0786)	MaskBCELoss 0.0106 (0.0151)	MaskDICELoss 0.0522 (0.0635)
Epoch: [1][860/500]	Time 16.889 (16.889)	Loss 0.1517 (0.2469)	CeLoss 0.0001 (0.0087)	SegCLSLoss 0.0003 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0395 (0.0633)	MaskBCELoss 0.0035 (0.0076)	MaskDICELoss 0.0360 (0.0557)
Epoch: [1][861/500]	Time 16.891 (16.891)	Loss 0.2441 (0.2573)	CeLoss 0.0001 (0.0239)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0634 (0.0642)	MaskBCELoss 0.0048 (0.0117)	MaskDICELoss 0.0586 (0.0525)
Epoch: [1][862/500]	Time 18.200 (18.200)	Loss 0.2891 (0.2786)	CeLoss 0.0134 (0.0390)	SegCLSLoss 0.0001 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0757 (0.0652)	MaskBCELoss 0.0137 (0.0107)	MaskDICELoss 0.0621 (0.0546)
Epoch: [1][863/500]	Time 21.397 (21.397)	Loss 0.3366 (0.3215)	CeLoss 0.0134 (0.0309)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0919 (0.0813)	MaskBCELoss 0.0221 (0.0173)	MaskDICELoss 0.0698 (0.0640)
Epoch: [1][864/500]	Time 18.978 (18.978)	Loss 0.2685 (0.2853)	CeLoss 0.0598 (0.0319)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0529 (0.0708)	MaskBCELoss 0.0013 (0.0152)	MaskDICELoss 0.0515 (0.0557)
Epoch: [1][865/500]	Time 17.103 (17.103)	Loss 0.2429 (0.2588)	CeLoss 0.0527 (0.0293)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0560 (0.0636)	MaskBCELoss 0.0169 (0.0124)	MaskDICELoss 0.0391 (0.0512)
Epoch: [1][866/500]	Time 17.660 (17.660)	Loss 0.3295 (0.2844)	CeLoss 0.0168 (0.0333)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0932 (0.0691)	MaskBCELoss 0.0301 (0.0126)	MaskDICELoss 0.0631 (0.0564)
Epoch: [1][867/500]	Time 17.878 (17.878)	Loss 0.2607 (0.2495)	CeLoss 0.0552 (0.0194)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0548 (0.0627)	MaskBCELoss 0.0068 (0.0103)	MaskDICELoss 0.0480 (0.0524)
Epoch: [1][868/500]	Time 18.563 (18.563)	Loss 0.1681 (0.2877)	CeLoss 0.0001 (0.0088)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0479 (0.0785)	MaskBCELoss 0.0119 (0.0176)	MaskDICELoss 0.0361 (0.0609)
Epoch: [1][869/500]	Time 17.894 (17.894)	Loss 0.2876 (0.2611)	CeLoss 0.0001 (0.0117)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0916 (0.0725)	MaskBCELoss 0.0395 (0.0203)	MaskDICELoss 0.0521 (0.0522)
Epoch: [1][870/500]	Time 18.903 (18.903)	Loss 0.2620 (0.2563)	CeLoss 0.0001 (0.0161)	SegCLSLoss 0.0000 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0718 (0.0645)	MaskBCELoss 0.0125 (0.0093)	MaskDICELoss 0.0592 (0.0552)
Epoch: [1][871/500]	Time 20.401 (20.401)	Loss 0.3405 (0.2728)	CeLoss 0.0356 (0.0149)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0855 (0.0722)	MaskBCELoss 0.0185 (0.0156)	MaskDICELoss 0.0670 (0.0566)
Epoch: [1][872/500]	Time 18.751 (18.751)	Loss 0.2359 (0.2611)	CeLoss 0.0001 (0.0138)	SegCLSLoss 0.0001 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0678 (0.0686)	MaskBCELoss 0.0177 (0.0135)	MaskDICELoss 0.0501 (0.0551)
Epoch: [1][873/500]	Time 14.978 (14.978)	Loss 0.2929 (0.2406)	CeLoss 0.0000 (0.0163)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0776 (0.0619)	MaskBCELoss 0.0089 (0.0117)	MaskDICELoss 0.0688 (0.0502)
Epoch: [1][874/500]	Time 19.047 (19.047)	Loss 0.2346 (0.2557)	CeLoss 0.0000 (0.0202)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0641 (0.0641)	MaskBCELoss 0.0109 (0.0105)	MaskDICELoss 0.0532 (0.0536)
Epoch: [1][875/500]	Time 19.552 (19.552)	Loss 0.3967 (0.2814)	CeLoss 0.0001 (0.0317)	SegCLSLoss 0.0001 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1100 (0.0702)	MaskBCELoss 0.0217 (0.0156)	MaskDICELoss 0.0883 (0.0546)
Epoch: [1][876/500]	Time 19.493 (19.493)	Loss 0.1628 (0.3015)	CeLoss 0.0000 (0.0319)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0415 (0.0728)	MaskBCELoss 0.0017 (0.0109)	MaskDICELoss 0.0399 (0.0619)
Epoch: [1][877/500]	Time 18.628 (18.628)	Loss 0.3253 (0.2728)	CeLoss 0.0913 (0.0348)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0678 (0.0659)	MaskBCELoss 0.0187 (0.0128)	MaskDICELoss 0.0491 (0.0531)
Epoch: [1][878/500]	Time 17.598 (17.598)	Loss 0.2389 (0.2689)	CeLoss 0.0000 (0.0181)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0655 (0.0683)	MaskBCELoss 0.0117 (0.0113)	MaskDICELoss 0.0539 (0.0570)
Epoch: [1][879/500]	Time 21.842 (21.842)	Loss 0.3298 (0.3145)	CeLoss 0.0508 (0.0349)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0755 (0.0790)	MaskBCELoss 0.0114 (0.0183)	MaskDICELoss 0.0641 (0.0607)
Epoch: [1][880/500]	Time 20.735 (20.735)	Loss 0.2494 (0.3107)	CeLoss 0.0001 (0.0284)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0638 (0.0765)	MaskBCELoss 0.0030 (0.0118)	MaskDICELoss 0.0608 (0.0646)
Epoch: [1][881/500]	Time 18.649 (18.649)	Loss 0.3376 (0.2845)	CeLoss 0.0003 (0.0363)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0934 (0.0692)	MaskBCELoss 0.0182 (0.0144)	MaskDICELoss 0.0752 (0.0549)
Epoch: [1][882/500]	Time 17.915 (17.915)	Loss 0.2196 (0.2508)	CeLoss 0.0034 (0.0335)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0595 (0.0610)	MaskBCELoss 0.0110 (0.0133)	MaskDICELoss 0.0485 (0.0477)
Epoch: [1][883/500]	Time 21.761 (21.761)	Loss 0.2153 (0.2605)	CeLoss 0.0001 (0.0164)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0591 (0.0675)	MaskBCELoss 0.0105 (0.0130)	MaskDICELoss 0.0485 (0.0545)
Epoch: [1][884/500]	Time 18.687 (18.687)	Loss 0.2043 (0.2385)	CeLoss 0.0001 (0.0158)	SegCLSLoss 0.0001 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0548 (0.0607)	MaskBCELoss 0.0076 (0.0102)	MaskDICELoss 0.0472 (0.0506)
Epoch: [1][885/500]	Time 18.023 (18.023)	Loss 0.3085 (0.2770)	CeLoss 0.0732 (0.0168)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0640 (0.0709)	MaskBCELoss 0.0105 (0.0117)	MaskDICELoss 0.0535 (0.0592)
Epoch: [1][886/500]	Time 18.452 (18.452)	Loss 0.3017 (0.2405)	CeLoss 0.0125 (0.0223)	SegCLSLoss 0.0013 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0823 (0.0587)	MaskBCELoss 0.0213 (0.0084)	MaskDICELoss 0.0610 (0.0503)
Epoch: [1][887/500]	Time 19.214 (19.214)	Loss 0.2421 (0.2887)	CeLoss 0.0002 (0.0340)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0638 (0.0703)	MaskBCELoss 0.0067 (0.0133)	MaskDICELoss 0.0571 (0.0570)
Epoch: [1][888/500]	Time 22.128 (22.128)	Loss 0.2771 (0.2851)	CeLoss 0.0430 (0.0224)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0636 (0.0720)	MaskBCELoss 0.0102 (0.0126)	MaskDICELoss 0.0534 (0.0594)
Epoch: [1][889/500]	Time 17.863 (17.863)	Loss 0.2582 (0.2511)	CeLoss 0.0001 (0.0160)	SegCLSLoss 0.0000 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0677 (0.0633)	MaskBCELoss 0.0064 (0.0094)	MaskDICELoss 0.0613 (0.0539)
Epoch: [1][890/500]	Time 17.948 (17.948)	Loss 0.2332 (0.2476)	CeLoss 0.0001 (0.0181)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0614 (0.0628)	MaskBCELoss 0.0062 (0.0110)	MaskDICELoss 0.0552 (0.0518)
Epoch: [1][891/500]	Time 19.731 (19.731)	Loss 0.2129 (0.2694)	CeLoss 0.0004 (0.0397)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0607 (0.0624)	MaskBCELoss 0.0152 (0.0100)	MaskDICELoss 0.0455 (0.0524)
Epoch: [1][892/500]	Time 17.859 (17.859)	Loss 0.3326 (0.2477)	CeLoss 0.0001 (0.0172)	SegCLSLoss 0.0002 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0926 (0.0631)	MaskBCELoss 0.0192 (0.0112)	MaskDICELoss 0.0735 (0.0520)
Epoch: [1][893/500]	Time 21.274 (21.274)	Loss 0.4048 (0.2897)	CeLoss 0.0203 (0.0155)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1056 (0.0761)	MaskBCELoss 0.0190 (0.0152)	MaskDICELoss 0.0867 (0.0610)
Epoch: [1][894/500]	Time 19.374 (19.374)	Loss 0.3402 (0.2660)	CeLoss 0.0388 (0.0257)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0820 (0.0649)	MaskBCELoss 0.0133 (0.0097)	MaskDICELoss 0.0687 (0.0552)
Epoch: [1][895/500]	Time 16.787 (16.787)	Loss 0.3592 (0.2393)	CeLoss 0.0723 (0.0159)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0826 (0.0622)	MaskBCELoss 0.0218 (0.0127)	MaskDICELoss 0.0608 (0.0495)
Epoch: [1][896/500]	Time 20.858 (20.858)	Loss 0.3590 (0.2980)	CeLoss 0.0006 (0.0103)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1025 (0.0786)	MaskBCELoss 0.0258 (0.0133)	MaskDICELoss 0.0767 (0.0653)
Epoch: [1][897/500]	Time 20.731 (20.731)	Loss 0.3697 (0.2794)	CeLoss 0.0869 (0.0166)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0813 (0.0720)	MaskBCELoss 0.0211 (0.0126)	MaskDICELoss 0.0602 (0.0594)
Epoch: [1][898/500]	Time 22.059 (22.059)	Loss 0.2759 (0.2881)	CeLoss 0.0688 (0.0369)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0633 (0.0724)	MaskBCELoss 0.0230 (0.0192)	MaskDICELoss 0.0403 (0.0532)
Epoch: [1][899/500]	Time 19.031 (19.031)	Loss 0.3127 (0.2754)	CeLoss 0.0493 (0.0395)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0749 (0.0655)	MaskBCELoss 0.0182 (0.0130)	MaskDICELoss 0.0567 (0.0525)
[2025-04-05 02:37:26,443] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[0.0002893253012048192], mom=[(0.9, 0.95)]
[2025-04-05 02:37:26,450] [INFO] [timer.py:215:stop] epoch=0/micro_step=900/global_step=90, RunningAvgSamplesPerSec=1.6607016264973697, CurrSamplesPerSec=1.8172394697381478, MemAllocated=35.76GB, MaxMemAllocated=46.18GB
Epoch: [1][900/500]	Time 16.905 (16.905)	Loss 0.4199 (0.2388)	CeLoss 0.0000 (0.0001)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1115 (0.0646)	MaskBCELoss 0.0130 (0.0099)	MaskDICELoss 0.0985 (0.0547)
Epoch: [1][901/500]	Time 20.089 (20.089)	Loss 0.2018 (0.2741)	CeLoss 0.0811 (0.0214)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0305 (0.0714)	MaskBCELoss 0.0005 (0.0167)	MaskDICELoss 0.0300 (0.0548)
Epoch: [1][902/500]	Time 15.062 (15.062)	Loss 0.4456 (0.2449)	CeLoss 0.0620 (0.0400)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1202 (0.0567)	MaskBCELoss 0.0486 (0.0109)	MaskDICELoss 0.0716 (0.0457)
Epoch: [1][903/500]	Time 17.679 (17.679)	Loss 0.1831 (0.2529)	CeLoss 0.0001 (0.0359)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0463 (0.0593)	MaskBCELoss 0.0011 (0.0102)	MaskDICELoss 0.0452 (0.0491)
Epoch: [1][904/500]	Time 17.185 (17.185)	Loss 0.2547 (0.2095)	CeLoss 0.0009 (0.0186)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0711 (0.0516)	MaskBCELoss 0.0152 (0.0077)	MaskDICELoss 0.0558 (0.0439)
Epoch: [1][905/500]	Time 18.015 (18.015)	Loss 0.2695 (0.2636)	CeLoss 0.0767 (0.0228)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0497 (0.0663)	MaskBCELoss 0.0029 (0.0122)	MaskDICELoss 0.0468 (0.0541)
Epoch: [1][906/500]	Time 20.332 (20.332)	Loss 0.2012 (0.2593)	CeLoss 0.0000 (0.0157)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0593 (0.0678)	MaskBCELoss 0.0180 (0.0139)	MaskDICELoss 0.0413 (0.0539)
Epoch: [1][907/500]	Time 20.902 (20.902)	Loss 0.2680 (0.3067)	CeLoss 0.0000 (0.0334)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0731 (0.0769)	MaskBCELoss 0.0122 (0.0172)	MaskDICELoss 0.0609 (0.0597)
Epoch: [1][908/500]	Time 21.951 (21.951)	Loss 0.1957 (0.2863)	CeLoss 0.0001 (0.0188)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0571 (0.0755)	MaskBCELoss 0.0165 (0.0173)	MaskDICELoss 0.0406 (0.0582)
Epoch: [1][909/500]	Time 15.242 (15.242)	Loss 0.2797 (0.2284)	CeLoss 0.0674 (0.0238)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0564 (0.0564)	MaskBCELoss 0.0068 (0.0104)	MaskDICELoss 0.0496 (0.0459)
Epoch: [1][910/500]	Time 20.129 (20.129)	Loss 0.3536 (0.2896)	CeLoss 0.0001 (0.0064)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1020 (0.0789)	MaskBCELoss 0.0272 (0.0162)	MaskDICELoss 0.0747 (0.0627)
Epoch: [1][911/500]	Time 18.164 (18.164)	Loss 0.2578 (0.2430)	CeLoss 0.0008 (0.0197)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0784 (0.0636)	MaskBCELoss 0.0284 (0.0155)	MaskDICELoss 0.0500 (0.0481)
Epoch: [1][912/500]	Time 15.954 (15.954)	Loss 0.1875 (0.2410)	CeLoss 0.0022 (0.0296)	SegCLSLoss 0.0005 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0537 (0.0579)	MaskBCELoss 0.0153 (0.0103)	MaskDICELoss 0.0384 (0.0475)
Epoch: [1][913/500]	Time 19.702 (19.702)	Loss 0.2652 (0.2750)	CeLoss 0.0635 (0.0253)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0511 (0.0686)	MaskBCELoss 0.0015 (0.0124)	MaskDICELoss 0.0496 (0.0562)
Epoch: [1][914/500]	Time 19.370 (19.370)	Loss 0.3490 (0.3114)	CeLoss 0.0620 (0.0472)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0866 (0.0717)	MaskBCELoss 0.0298 (0.0114)	MaskDICELoss 0.0568 (0.0603)
Epoch: [1][915/500]	Time 20.776 (20.776)	Loss 0.2638 (0.2778)	CeLoss 0.0001 (0.0157)	SegCLSLoss 0.0001 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0779 (0.0728)	MaskBCELoss 0.0240 (0.0149)	MaskDICELoss 0.0539 (0.0579)
Epoch: [1][916/500]	Time 17.008 (17.008)	Loss 0.2434 (0.2622)	CeLoss 0.0001 (0.0241)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0683 (0.0672)	MaskBCELoss 0.0150 (0.0154)	MaskDICELoss 0.0533 (0.0518)
Epoch: [1][917/500]	Time 22.904 (22.904)	Loss 0.3551 (0.3084)	CeLoss 0.0664 (0.0364)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0859 (0.0753)	MaskBCELoss 0.0274 (0.0147)	MaskDICELoss 0.0585 (0.0607)
Epoch: [1][918/500]	Time 20.564 (20.564)	Loss 0.3962 (0.2807)	CeLoss 0.0002 (0.0326)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1060 (0.0701)	MaskBCELoss 0.0139 (0.0161)	MaskDICELoss 0.0920 (0.0539)
Epoch: [1][919/500]	Time 15.543 (15.543)	Loss 0.2490 (0.2136)	CeLoss 0.0003 (0.0256)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0732 (0.0513)	MaskBCELoss 0.0220 (0.0087)	MaskDICELoss 0.0512 (0.0427)
Epoch: [1][920/500]	Time 19.018 (19.018)	Loss 0.2227 (0.2423)	CeLoss 0.0422 (0.0128)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0502 (0.0653)	MaskBCELoss 0.0102 (0.0160)	MaskDICELoss 0.0400 (0.0493)
Epoch: [1][921/500]	Time 18.015 (18.015)	Loss 0.1695 (0.2415)	CeLoss 0.0566 (0.0200)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0285 (0.0613)	MaskBCELoss 0.0005 (0.0119)	MaskDICELoss 0.0280 (0.0494)
Epoch: [1][922/500]	Time 17.844 (17.844)	Loss 0.1438 (0.2151)	CeLoss 0.0256 (0.0172)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0326 (0.0554)	MaskBCELoss 0.0062 (0.0118)	MaskDICELoss 0.0265 (0.0436)
Epoch: [1][923/500]	Time 18.096 (18.096)	Loss 0.2827 (0.2560)	CeLoss 0.0001 (0.0230)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0866 (0.0668)	MaskBCELoss 0.0318 (0.0173)	MaskDICELoss 0.0547 (0.0495)
Epoch: [1][924/500]	Time 19.895 (19.895)	Loss 0.2464 (0.2668)	CeLoss 0.0295 (0.0150)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0577 (0.0688)	MaskBCELoss 0.0069 (0.0116)	MaskDICELoss 0.0508 (0.0571)
Epoch: [1][925/500]	Time 18.377 (18.377)	Loss 0.2133 (0.2677)	CeLoss 0.0000 (0.0176)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0541 (0.0677)	MaskBCELoss 0.0015 (0.0104)	MaskDICELoss 0.0526 (0.0573)
Epoch: [1][926/500]	Time 17.559 (17.559)	Loss 0.2970 (0.2334)	CeLoss 0.0001 (0.0285)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0859 (0.0567)	MaskBCELoss 0.0234 (0.0110)	MaskDICELoss 0.0626 (0.0457)
Epoch: [1][927/500]	Time 18.083 (18.083)	Loss 0.2646 (0.2695)	CeLoss 0.0693 (0.0312)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0502 (0.0643)	MaskBCELoss 0.0026 (0.0097)	MaskDICELoss 0.0476 (0.0547)
Epoch: [1][928/500]	Time 17.280 (17.280)	Loss 0.3800 (0.2487)	CeLoss 0.0415 (0.0255)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0902 (0.0612)	MaskBCELoss 0.0112 (0.0108)	MaskDICELoss 0.0790 (0.0504)
Epoch: [1][929/500]	Time 19.338 (19.338)	Loss 0.2847 (0.2891)	CeLoss 0.0293 (0.0322)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0686 (0.0703)	MaskBCELoss 0.0094 (0.0121)	MaskDICELoss 0.0591 (0.0582)
Epoch: [1][930/500]	Time 19.822 (19.822)	Loss 0.2179 (0.2718)	CeLoss 0.0011 (0.0100)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0600 (0.0729)	MaskBCELoss 0.0116 (0.0151)	MaskDICELoss 0.0484 (0.0578)
Epoch: [1][931/500]	Time 19.804 (19.804)	Loss 0.4188 (0.2650)	CeLoss 0.0244 (0.0222)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1104 (0.0686)	MaskBCELoss 0.0235 (0.0158)	MaskDICELoss 0.0868 (0.0528)
Epoch: [1][932/500]	Time 18.836 (18.836)	Loss 0.4165 (0.2667)	CeLoss 0.0540 (0.0085)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1066 (0.0726)	MaskBCELoss 0.0319 (0.0162)	MaskDICELoss 0.0747 (0.0564)
Epoch: [1][933/500]	Time 20.046 (20.046)	Loss 0.2766 (0.2917)	CeLoss 0.0002 (0.0210)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0762 (0.0747)	MaskBCELoss 0.0142 (0.0140)	MaskDICELoss 0.0620 (0.0607)
Epoch: [1][934/500]	Time 18.431 (18.431)	Loss 0.2999 (0.2614)	CeLoss 0.0869 (0.0294)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0587 (0.0638)	MaskBCELoss 0.0108 (0.0116)	MaskDICELoss 0.0479 (0.0522)
Epoch: [1][935/500]	Time 19.128 (19.128)	Loss 0.1959 (0.2287)	CeLoss 0.0403 (0.0157)	SegCLSLoss 0.0001 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0418 (0.0575)	MaskBCELoss 0.0058 (0.0086)	MaskDICELoss 0.0359 (0.0489)
Epoch: [1][936/500]	Time 18.424 (18.424)	Loss 0.1611 (0.2356)	CeLoss 0.0449 (0.0279)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0292 (0.0570)	MaskBCELoss 0.0004 (0.0102)	MaskDICELoss 0.0289 (0.0468)
Epoch: [1][937/500]	Time 17.926 (17.926)	Loss 0.2255 (0.2650)	CeLoss 0.0349 (0.0237)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0493 (0.0667)	MaskBCELoss 0.0034 (0.0127)	MaskDICELoss 0.0460 (0.0540)
Epoch: [1][938/500]	Time 17.379 (17.379)	Loss 0.1167 (0.2360)	CeLoss 0.0001 (0.0111)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0293 (0.0600)	MaskBCELoss 0.0004 (0.0077)	MaskDICELoss 0.0290 (0.0523)
Epoch: [1][939/500]	Time 19.279 (19.279)	Loss 0.1755 (0.2512)	CeLoss 0.0001 (0.0096)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0459 (0.0657)	MaskBCELoss 0.0041 (0.0106)	MaskDICELoss 0.0418 (0.0551)
Epoch: [1][940/500]	Time 18.462 (18.462)	Loss 0.2485 (0.2820)	CeLoss 0.0000 (0.0188)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0723 (0.0750)	MaskBCELoss 0.0203 (0.0186)	MaskDICELoss 0.0520 (0.0565)
Epoch: [1][941/500]	Time 18.725 (18.725)	Loss 0.2115 (0.2410)	CeLoss 0.0432 (0.0161)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0452 (0.0646)	MaskBCELoss 0.0062 (0.0169)	MaskDICELoss 0.0390 (0.0477)
Epoch: [1][942/500]	Time 17.277 (17.277)	Loss 0.2602 (0.2719)	CeLoss 0.0825 (0.0353)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0549 (0.0651)	MaskBCELoss 0.0209 (0.0118)	MaskDICELoss 0.0340 (0.0532)
Epoch: [1][943/500]	Time 17.361 (17.361)	Loss 0.2098 (0.2480)	CeLoss 0.0002 (0.0266)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0580 (0.0614)	MaskBCELoss 0.0112 (0.0122)	MaskDICELoss 0.0468 (0.0493)
Epoch: [1][944/500]	Time 17.491 (17.491)	Loss 0.2948 (0.2304)	CeLoss 0.0271 (0.0196)	SegCLSLoss 0.0001 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0741 (0.0574)	MaskBCELoss 0.0145 (0.0095)	MaskDICELoss 0.0596 (0.0480)
Epoch: [1][945/500]	Time 17.926 (17.926)	Loss 0.2175 (0.2604)	CeLoss 0.0723 (0.0296)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0374 (0.0646)	MaskBCELoss 0.0021 (0.0139)	MaskDICELoss 0.0352 (0.0508)
Epoch: [1][946/500]	Time 20.212 (20.212)	Loss 0.3868 (0.2998)	CeLoss 0.0430 (0.0268)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1011 (0.0761)	MaskBCELoss 0.0304 (0.0157)	MaskDICELoss 0.0708 (0.0604)
Epoch: [1][947/500]	Time 16.965 (16.965)	Loss 0.1207 (0.2307)	CeLoss 0.0001 (0.0159)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0305 (0.0588)	MaskBCELoss 0.0007 (0.0103)	MaskDICELoss 0.0298 (0.0485)
Epoch: [1][948/500]	Time 19.450 (19.450)	Loss 0.2135 (0.2512)	CeLoss 0.0493 (0.0269)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0443 (0.0632)	MaskBCELoss 0.0066 (0.0142)	MaskDICELoss 0.0378 (0.0489)
Epoch: [1][949/500]	Time 19.078 (19.078)	Loss 0.2460 (0.2468)	CeLoss 0.0002 (0.0165)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0761 (0.0643)	MaskBCELoss 0.0292 (0.0136)	MaskDICELoss 0.0468 (0.0507)
Epoch: [1][950/500]	Time 20.900 (20.900)	Loss 0.2419 (0.2763)	CeLoss 0.0001 (0.0165)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0664 (0.0717)	MaskBCELoss 0.0119 (0.0135)	MaskDICELoss 0.0545 (0.0582)
Epoch: [1][951/500]	Time 20.681 (20.681)	Loss 0.2594 (0.3332)	CeLoss 0.0000 (0.0415)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0714 (0.0831)	MaskBCELoss 0.0131 (0.0203)	MaskDICELoss 0.0583 (0.0628)
Epoch: [1][952/500]	Time 19.177 (19.177)	Loss 0.1976 (0.2915)	CeLoss 0.0001 (0.0161)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0531 (0.0766)	MaskBCELoss 0.0074 (0.0156)	MaskDICELoss 0.0457 (0.0610)
Epoch: [1][953/500]	Time 18.561 (18.561)	Loss 0.2860 (0.2969)	CeLoss 0.0001 (0.0379)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0803 (0.0718)	MaskBCELoss 0.0177 (0.0141)	MaskDICELoss 0.0626 (0.0577)
Epoch: [1][954/500]	Time 20.118 (20.118)	Loss 0.3209 (0.3164)	CeLoss 0.0003 (0.0299)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0941 (0.0793)	MaskBCELoss 0.0279 (0.0155)	MaskDICELoss 0.0662 (0.0639)
Epoch: [1][955/500]	Time 19.235 (19.235)	Loss 0.2015 (0.2620)	CeLoss 0.0195 (0.0288)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0517 (0.0647)	MaskBCELoss 0.0124 (0.0128)	MaskDICELoss 0.0393 (0.0518)
Epoch: [1][956/500]	Time 16.594 (16.594)	Loss 0.4019 (0.2485)	CeLoss 0.0500 (0.0231)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0967 (0.0618)	MaskBCELoss 0.0174 (0.0109)	MaskDICELoss 0.0793 (0.0509)
Epoch: [1][957/500]	Time 16.913 (16.913)	Loss 0.2354 (0.2422)	CeLoss 0.0020 (0.0180)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0650 (0.0640)	MaskBCELoss 0.0133 (0.0159)	MaskDICELoss 0.0517 (0.0481)
Epoch: [1][958/500]	Time 19.430 (19.430)	Loss 0.2918 (0.2625)	CeLoss 0.0001 (0.0211)	SegCLSLoss 0.0001 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0811 (0.0669)	MaskBCELoss 0.0164 (0.0132)	MaskDICELoss 0.0647 (0.0537)
Epoch: [1][959/500]	Time 18.316 (18.316)	Loss 0.2637 (0.2537)	CeLoss 0.0356 (0.0260)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0616 (0.0623)	MaskBCELoss 0.0091 (0.0107)	MaskDICELoss 0.0525 (0.0516)
Epoch: [1][960/500]	Time 17.260 (17.260)	Loss 0.1325 (0.2312)	CeLoss 0.0000 (0.0254)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0375 (0.0565)	MaskBCELoss 0.0088 (0.0101)	MaskDICELoss 0.0287 (0.0464)
Epoch: [1][961/500]	Time 17.460 (17.460)	Loss 0.4444 (0.2638)	CeLoss 0.0884 (0.0144)	SegCLSLoss 0.0002 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0939 (0.0680)	MaskBCELoss 0.0100 (0.0116)	MaskDICELoss 0.0839 (0.0565)
Epoch: [1][962/500]	Time 17.596 (17.596)	Loss 0.3197 (0.2768)	CeLoss 0.0264 (0.0537)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0842 (0.0609)	MaskBCELoss 0.0217 (0.0103)	MaskDICELoss 0.0625 (0.0506)
Epoch: [1][963/500]	Time 19.105 (19.105)	Loss 0.2725 (0.2783)	CeLoss 0.0000 (0.0276)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0717 (0.0690)	MaskBCELoss 0.0072 (0.0126)	MaskDICELoss 0.0645 (0.0564)
Epoch: [1][964/500]	Time 21.337 (21.337)	Loss 0.3294 (0.2649)	CeLoss 0.0474 (0.0285)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0743 (0.0658)	MaskBCELoss 0.0077 (0.0133)	MaskDICELoss 0.0667 (0.0525)
Epoch: [1][965/500]	Time 18.652 (18.652)	Loss 0.3003 (0.2734)	CeLoss 0.0366 (0.0282)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0730 (0.0668)	MaskBCELoss 0.0143 (0.0111)	MaskDICELoss 0.0587 (0.0557)
Epoch: [1][966/500]	Time 17.746 (17.746)	Loss 0.1833 (0.2469)	CeLoss 0.0001 (0.0215)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0522 (0.0612)	MaskBCELoss 0.0127 (0.0098)	MaskDICELoss 0.0395 (0.0514)
Epoch: [1][967/500]	Time 21.764 (21.764)	Loss 0.2876 (0.2968)	CeLoss 0.0001 (0.0168)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0843 (0.0770)	MaskBCELoss 0.0249 (0.0141)	MaskDICELoss 0.0594 (0.0630)
Epoch: [1][968/500]	Time 18.142 (18.142)	Loss 0.3347 (0.2523)	CeLoss 0.0486 (0.0201)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0820 (0.0645)	MaskBCELoss 0.0209 (0.0130)	MaskDICELoss 0.0610 (0.0515)
Epoch: [1][969/500]	Time 20.252 (20.252)	Loss 0.1719 (0.2613)	CeLoss 0.0000 (0.0059)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0457 (0.0722)	MaskBCELoss 0.0054 (0.0166)	MaskDICELoss 0.0402 (0.0555)
Epoch: [1][970/500]	Time 16.122 (16.122)	Loss 0.2098 (0.2122)	CeLoss 0.0000 (0.0151)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0562 (0.0547)	MaskBCELoss 0.0074 (0.0109)	MaskDICELoss 0.0487 (0.0438)
Epoch: [1][971/500]	Time 18.688 (18.688)	Loss 0.2030 (0.2667)	CeLoss 0.0806 (0.0274)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0309 (0.0660)	MaskBCELoss 0.0005 (0.0124)	MaskDICELoss 0.0304 (0.0536)
Epoch: [1][972/500]	Time 18.982 (18.982)	Loss 0.3100 (0.2573)	CeLoss 0.0000 (0.0104)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0798 (0.0669)	MaskBCELoss 0.0047 (0.0104)	MaskDICELoss 0.0751 (0.0565)
Epoch: [1][973/500]	Time 18.344 (18.344)	Loss 0.2923 (0.2846)	CeLoss 0.0003 (0.0124)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0813 (0.0725)	MaskBCELoss 0.0166 (0.0090)	MaskDICELoss 0.0647 (0.0636)
Epoch: [1][974/500]	Time 16.382 (16.382)	Loss 0.3184 (0.2566)	CeLoss 0.0850 (0.0312)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0634 (0.0628)	MaskBCELoss 0.0102 (0.0129)	MaskDICELoss 0.0532 (0.0499)
Epoch: [1][975/500]	Time 17.398 (17.398)	Loss 0.2489 (0.2300)	CeLoss 0.0732 (0.0116)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0507 (0.0584)	MaskBCELoss 0.0136 (0.0078)	MaskDICELoss 0.0371 (0.0506)
Epoch: [1][976/500]	Time 18.397 (18.397)	Loss 0.2439 (0.2228)	CeLoss 0.0273 (0.0168)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0575 (0.0560)	MaskBCELoss 0.0068 (0.0093)	MaskDICELoss 0.0507 (0.0467)
Epoch: [1][977/500]	Time 17.730 (17.730)	Loss 0.2867 (0.2416)	CeLoss 0.0117 (0.0154)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0726 (0.0597)	MaskBCELoss 0.0077 (0.0063)	MaskDICELoss 0.0649 (0.0534)
Epoch: [1][978/500]	Time 17.829 (17.829)	Loss 0.1697 (0.2399)	CeLoss 0.0003 (0.0105)	SegCLSLoss 0.0000 (0.0002)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0461 (0.0622)	MaskBCELoss 0.0076 (0.0099)	MaskDICELoss 0.0386 (0.0523)
Epoch: [1][979/500]	Time 19.319 (19.319)	Loss 0.2332 (0.2788)	CeLoss 0.0493 (0.0171)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0469 (0.0709)	MaskBCELoss 0.0018 (0.0110)	MaskDICELoss 0.0451 (0.0599)
Epoch: [1][980/500]	Time 20.231 (20.231)	Loss 0.2823 (0.2706)	CeLoss 0.0001 (0.0340)	SegCLSLoss 0.0001 (0.0003)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0757 (0.0654)	MaskBCELoss 0.0103 (0.0127)	MaskDICELoss 0.0654 (0.0526)
Epoch: [1][981/500]	Time 19.667 (19.667)	Loss 0.2632 (0.2646)	CeLoss 0.0001 (0.0218)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0758 (0.0681)	MaskBCELoss 0.0200 (0.0147)	MaskDICELoss 0.0558 (0.0533)
Epoch: [1][982/500]	Time 18.567 (18.567)	Loss 0.2372 (0.2374)	CeLoss 0.0002 (0.0276)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0605 (0.0567)	MaskBCELoss 0.0025 (0.0085)	MaskDICELoss 0.0580 (0.0482)
Epoch: [1][983/500]	Time 18.363 (18.363)	Loss 0.3705 (0.2522)	CeLoss 0.0435 (0.0302)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0938 (0.0598)	MaskBCELoss 0.0242 (0.0086)	MaskDICELoss 0.0696 (0.0512)
Epoch: [1][984/500]	Time 19.375 (19.375)	Loss 0.3130 (0.2712)	CeLoss 0.0271 (0.0302)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0743 (0.0672)	MaskBCELoss 0.0057 (0.0139)	MaskDICELoss 0.0686 (0.0533)
Epoch: [1][985/500]	Time 20.553 (20.553)	Loss 0.2388 (0.2857)	CeLoss 0.0001 (0.0224)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0634 (0.0733)	MaskBCELoss 0.0074 (0.0150)	MaskDICELoss 0.0560 (0.0583)
Epoch: [1][986/500]	Time 17.385 (17.385)	Loss 0.2285 (0.2572)	CeLoss 0.0001 (0.0219)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0625 (0.0670)	MaskBCELoss 0.0108 (0.0164)	MaskDICELoss 0.0517 (0.0506)
Epoch: [1][987/500]	Time 19.533 (19.533)	Loss 0.3596 (0.2745)	CeLoss 0.0001 (0.0296)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1009 (0.0661)	MaskBCELoss 0.0222 (0.0098)	MaskDICELoss 0.0788 (0.0563)
Epoch: [1][988/500]	Time 16.459 (16.459)	Loss 0.1655 (0.2548)	CeLoss 0.0001 (0.0249)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0443 (0.0642)	MaskBCELoss 0.0060 (0.0135)	MaskDICELoss 0.0384 (0.0507)
Epoch: [1][989/500]	Time 19.034 (19.034)	Loss 0.2600 (0.2530)	CeLoss 0.0001 (0.0181)	SegCLSLoss 0.0002 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0679 (0.0640)	MaskBCELoss 0.0061 (0.0106)	MaskDICELoss 0.0619 (0.0534)
Epoch: [1][990/500]	Time 20.545 (20.545)	Loss 0.2785 (0.3146)	CeLoss 0.0002 (0.0302)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0752 (0.0780)	MaskBCELoss 0.0114 (0.0138)	MaskDICELoss 0.0639 (0.0642)
Epoch: [1][991/500]	Time 18.064 (18.064)	Loss 0.3686 (0.2833)	CeLoss 0.0601 (0.0224)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0880 (0.0721)	MaskBCELoss 0.0219 (0.0137)	MaskDICELoss 0.0662 (0.0584)
Epoch: [1][992/500]	Time 17.617 (17.617)	Loss 0.3195 (0.2687)	CeLoss 0.0001 (0.0229)	SegCLSLoss 0.0001 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0816 (0.0673)	MaskBCELoss 0.0036 (0.0117)	MaskDICELoss 0.0781 (0.0555)
Epoch: [1][993/500]	Time 19.862 (19.862)	Loss 0.2997 (0.2796)	CeLoss 0.0001 (0.0128)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0867 (0.0759)	MaskBCELoss 0.0235 (0.0185)	MaskDICELoss 0.0631 (0.0575)
Epoch: [1][994/500]	Time 18.409 (18.409)	Loss 0.2036 (0.2567)	CeLoss 0.0001 (0.0270)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0543 (0.0632)	MaskBCELoss 0.0069 (0.0116)	MaskDICELoss 0.0474 (0.0516)
Epoch: [1][995/500]	Time 17.622 (17.622)	Loss 0.4046 (0.2598)	CeLoss 0.0535 (0.0167)	SegCLSLoss 0.0000 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1001 (0.0658)	MaskBCELoss 0.0247 (0.0102)	MaskDICELoss 0.0754 (0.0557)
Epoch: [1][996/500]	Time 18.883 (18.883)	Loss 0.3143 (0.2840)	CeLoss 0.0425 (0.0293)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0688 (0.0696)	MaskBCELoss 0.0017 (0.0119)	MaskDICELoss 0.0671 (0.0577)
Epoch: [1][997/500]	Time 19.661 (19.661)	Loss 0.2261 (0.2402)	CeLoss 0.0203 (0.0125)	SegCLSLoss 0.0006 (0.0001)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0558 (0.0624)	MaskBCELoss 0.0092 (0.0110)	MaskDICELoss 0.0466 (0.0514)
Epoch: [1][998/500]	Time 19.049 (19.049)	Loss 0.3773 (0.3067)	CeLoss 0.0435 (0.0301)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0930 (0.0760)	MaskBCELoss 0.0192 (0.0136)	MaskDICELoss 0.0739 (0.0623)
Epoch: [1][999/500]	Time 17.331 (17.331)	Loss 0.2845 (0.2669)	CeLoss 0.0918 (0.0461)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0495 (0.0616)	MaskBCELoss 0.0026 (0.0127)	MaskDICELoss 0.0469 (0.0489)
  0%|                                                                                                                                                         | 0/200 [00:00<?, ?it/s]
[2025-04-05 03:08:31,671] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[0.00028799999999999995], mom=[(0.9, 0.95)]
[2025-04-05 03:08:31,679] [INFO] [timer.py:215:stop] epoch=0/micro_step=1000/global_step=100, RunningAvgSamplesPerSec=1.697729266472313, CurrSamplesPerSec=2.075809589651617, MemAllocated=33.92GB, MaxMemAllocated=46.18GB
Epoch: [1][1000/500]	Time 18.490 (18.490)	Loss 0.1544 (0.2445)	CeLoss 0.0001 (0.0236)	SegCLSLoss 0.0000 (0.0000)	KLLoss 0.0000 (0.0000)	MaskLoss 0.0394 (0.0632)	MaskBCELoss 0.0017 (0.0160)	MaskDICELoss 0.0377 (0.0472)
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something showing that the man is playing sports in this image? Please output segmentation mask. ASSISTANT: something showing that the man is playing sports</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3168, 4416])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3168, 4416])):  [tensor([[[-15.1875, -15.1875, -15.1875,  ..., -11.5625, -11.5625, -11.5625],
         [-15.1875, -15.1875, -15.1875,  ..., -11.5625, -11.5625, -11.5625],
         [-15.1875, -15.1875, -15.1875,  ..., -11.5625, -11.5625, -11.5625],
         ...,
         [ -8.0297,  -8.0297,  -8.0297,  ...,  -6.1398,  -6.1398,  -6.1398],
         [ -8.0625,  -8.0625,  -8.0625,  ...,  -6.1758,  -6.1758,  -6.1758],
         [ -8.0625,  -8.0625,  -8.0625,  ...,  -6.1758,  -6.1758,  -6.1758]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Horses can be trained to follow commands and be controlled while being ridden. What object in the picture is used for controlling and guiding a horse? Please output segmentation mask. ASSISTANT: horses can be trained to follow commands and be controlled while being ridden. what object in the picture is used for controlling and guiding a horse</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]

  5%|███████▏                                                                                                                                        | 10/200 [00:02<00:32,  5.92it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1024, 768])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1024, 768])):  [tensor([[[-7.8125, -7.8125, -7.9219,  ..., -8.8828, -8.6562, -8.5938],
         [-7.8125, -7.8125, -7.9219,  ..., -8.8828, -8.6562, -8.5938],
         [-7.9688, -7.9688, -8.0820,  ..., -8.9570, -8.7402, -8.6582],
         ...,
         [-5.6016, -5.6016, -6.0127,  ..., -6.3428, -6.0576, -6.1260],
         [-5.4688, -5.4688, -5.8945,  ..., -6.3320, -6.0469, -6.0781],
         [-5.4688, -5.4688, -5.8945,  ..., -6.3320, -6.0469, -6.0781]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that allows people to drink without touching the rim of the cup in this image? Please output segmentation mask. ASSISTANT: something that allows people to drink without touching the rim of the cup</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[71]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1600, 1200])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1600, 1200])):  [tensor([[[-5.5625, -5.5625, -5.5633,  ..., -5.0387, -5.0650, -5.0938],
         [-5.5625, -5.5625, -5.5633,  ..., -5.0387, -5.0650, -5.0938],
         [-5.5648, -5.5648, -5.5658,  ..., -5.0436, -5.0695, -5.0978],
         ...,
         [-2.3449, -2.3449, -2.3431,  ..., -4.2826, -4.2656, -4.2591],
         [-2.3438, -2.3438, -2.3420,  ..., -4.2828, -4.2650, -4.2578],
         [-2.3438, -2.3438, -2.3420,  ..., -4.2828, -4.2650, -4.2578]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In the picture, there seems to be a symbiotic relationship between two species, where one provides protection for the other. What animal in the picture is known to seek refuge in the tentacles of another creature for safety? Please output segmentation mask. ASSISTANT: in the picture, there seems to be a symbiotic relationship between two species, where one provides protection for the other. what animal in the picture is known to seek refuge in the tentacles of another creature for safety</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[96]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2736, 3648])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2736, 3648])):  [tensor([[[ -5.2188,  -5.2188,  -5.2188,  ..., -10.0625, -10.0625, -10.0625],
         [ -5.2188,  -5.2188,  -5.2188,  ..., -10.0625, -10.0625, -10.0625],
         [ -5.2188,  -5.2188,  -5.2188,  ..., -10.0625, -10.0625, -10.0625],
         ...,
         [ -5.8676,  -5.8676,  -5.8676,  ...,  -4.6859,  -4.6859,  -4.6859],
         [ -5.9102,  -5.9102,  -5.9102,  ...,  -4.6953,  -4.6953,  -4.6953],
         [ -5.9102,  -5.9102,  -5.9102,  ...,  -4.6953,  -4.6953,  -4.6953]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the slogan for welcoming in this image? Please output segmentation mask. ASSISTANT: the slogan for welcoming</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[-14.5000, -14.5000, -14.4945,  ..., -12.2695, -12.2500, -12.2500],
         [-14.5000, -14.5000, -14.4945,  ..., -12.2695, -12.2500, -12.2500],
         [-14.4922, -14.4922, -14.4874,  ..., -12.2950, -12.2758, -12.2758],
         ...,
         [ -5.7199,  -5.7199,  -5.7341,  ...,  -3.8286,  -3.8295,  -3.8295],
         [ -6.1200,  -6.1200,  -6.1346,  ...,  -4.5828,  -4.5825,  -4.5825],
         [ -6.4219,  -6.4219,  -6.4369,  ...,  -5.1731,  -5.1719,  -5.1719]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n For the safety of newborns, they are often placed in a secure and comfortable space when they sleep. What furniture in the picture is commonly used to provide a safe sleeping environment for babies? Please output segmentation mask. ASSISTANT: for the safety of newborns, they are often placed in a secure and comfortable space when they sleep. what furniture in the picture is commonly used to provide a safe sleeping environment for babies</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[94]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1002, 1455])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1002, 1455])):  [tensor([[[-7.1250, -7.1250, -7.0986,  ..., -6.7782, -6.7812, -6.7812],
         [-7.1250, -7.1250, -7.0986,  ..., -6.7782, -6.7812, -6.7812],
         [-7.1371, -7.1371, -7.1130,  ..., -6.7981, -6.8015, -6.8015],
         ...,
         [-5.3804, -5.3804, -5.4228,  ..., -4.6177, -4.6090, -4.6090],
         [-5.2595, -5.2595, -5.2903,  ..., -4.3456, -4.3341, -4.3341],
         [-5.1641, -5.1641, -5.1857,  ..., -4.1307, -4.1172, -4.1172]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When using a film camera to take photos, what part of the camera helps focus the light on the film to capture the image? Please output segmentation mask. ASSISTANT: when using a film camera to take photos, what part of the camera helps focus the light on the film to capture the image</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[77]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1067, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1067, 1600])):  [tensor([[[-8.1250, -8.1250, -8.1227,  ..., -7.3867, -7.3750, -7.3750],
         [-8.1250, -8.1250, -8.1227,  ..., -7.3867, -7.3750, -7.3750],
         [-8.1266, -8.1266, -8.1245,  ..., -7.3999, -7.3883, -7.3883],
         ...,
         [-6.7079, -6.7079, -6.7200,  ..., -5.4576, -5.4517, -5.4517],
         [-6.7869, -6.7869, -6.7986,  ..., -5.4293, -5.4213, -5.4213],
         [-6.8750, -6.8750, -6.8865,  ..., -5.4470, -5.4375, -5.4375]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Birds have various ways of searching for food. What part of their body helps them to grab and pick up food from the ground in the picture? Please output segmentation mask. ASSISTANT: birds have various ways of searching for food. what part of their body helps them to grab and pick up food from the ground in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2570, 2229])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2570, 2229])):  [tensor([[[-13.0625, -13.0625, -13.0625,  ...,  -9.8598,  -9.9034,  -9.9141],
         [-13.0625, -13.0625, -13.0625,  ...,  -9.8598,  -9.9034,  -9.9141],
         [-13.0625, -13.0625, -13.0625,  ...,  -9.8598,  -9.9034,  -9.9141],
         ...,
         [ -9.3125,  -9.3125,  -9.3125,  ..., -13.1417, -13.2475, -13.2734],
         [ -9.3125,  -9.3125,  -9.3125,  ..., -13.1417, -13.2475, -13.2734],
         [ -9.3125,  -9.3125,  -9.3125,  ..., -13.1417, -13.2475, -13.2734]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the sour food in this image? Please output segmentation mask. ASSISTANT: the sour food</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[-3.8750, -3.9066, -4.0156,  ..., -3.7500, -3.7984, -3.8125],
         [-3.8820, -3.9168, -4.0367,  ..., -3.7368, -3.7750, -3.7861],
         [-3.9062, -3.9520, -4.1094,  ..., -3.6914, -3.6944, -3.6953],
         ...,
         [-1.3477, -1.3490, -1.3535,  ..., -4.7500, -4.7984, -4.8125],
         [-1.5286, -1.5333, -1.5492,  ..., -4.7709, -4.9018, -4.9398],
         [-2.4594, -2.4705, -2.5086,  ..., -4.9766, -5.0868, -5.1188]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that protects the woman's eyes from getting wet in this image? Please output segmentation mask. ASSISTANT: something that protects the woman's eyes from getting wet</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[69]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2112, 2370])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2112, 2370])):  [tensor([[[-7.4375, -7.4375, -7.4375,  ..., -5.9688, -5.9688, -5.9688],
         [-7.4375, -7.4375, -7.4375,  ..., -5.9688, -5.9688, -5.9688],
         [-7.4375, -7.4375, -7.4375,  ..., -5.9688, -5.9688, -5.9688],
         ...,
         [-5.2916, -5.2916, -5.2916,  ..., -6.7105, -6.7105, -6.7105],
         [-5.0687, -5.0687, -5.0687,  ..., -6.4200, -6.4200, -6.4200],
         [-4.9922, -4.9922, -4.9922,  ..., -6.3203, -6.3203, -6.3203]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When participating in water activities such as kayaking, it is important to ensure personal safety. What item in the picture is designed to provide buoyancy and keep the person afloat in case of an accident? Please output segmentation mask. ASSISTANT: when participating in water activities such as kayaking, it is important to ensure personal safety. what item in the picture is designed to provide buoyancy and keep the person afloat in case of an accident</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[95]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1037, 1322])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1037, 1322])):  [tensor([[[-7.8438, -7.8438, -7.8114,  ..., -7.3239, -7.2812, -7.2812],
         [-7.8438, -7.8438, -7.8114,  ..., -7.3239, -7.2812, -7.2812],
         [-7.8386, -7.8386, -7.8112,  ..., -7.3763, -7.3374, -7.3374],
         ...,
         [-4.3853, -4.3853, -4.4315,  ..., -4.4692, -4.4491, -4.4491],
         [-3.6529, -3.6529, -3.6744,  ..., -3.6750, -3.6526, -3.6526],
         [-3.2266, -3.2266, -3.2332,  ..., -3.2124, -3.1875, -3.1875]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the sewage outlet in this image? Please output segmentation mask. ASSISTANT: the sewage outlet</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 4032, 3024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 4032, 3024])):  [tensor([[[-5.7188, -5.7188, -5.7188,  ..., -7.3750, -7.3750, -7.3750],
         [-5.7188, -5.7188, -5.7188,  ..., -7.3750, -7.3750, -7.3750],
         [-5.7188, -5.7188, -5.7188,  ..., -7.3750, -7.3750, -7.3750],
         ...,
         [-5.7188, -5.7188, -5.7188,  ..., -5.6982, -5.6992, -5.6992],
         [-5.7188, -5.7188, -5.7188,  ..., -5.6982, -5.6992, -5.6992],
         [-5.7188, -5.7188, -5.7188,  ..., -5.6982, -5.6992, -5.6992]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In some circus performances, gymnasts perform aerial acrobatics using long pieces of fabric that hang from the ceiling. What in the picture could be used for this type of performance? Please output segmentation mask. ASSISTANT: in some circus performances, gymnasts perform aerial acrobatics using long pieces of fabric that hang from the ceiling. what in the picture could be used for this type of performance</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[92]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1064, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1064, 1600])):  [tensor([[[-8.6250, -8.6250, -8.6250,  ..., -8.0648, -8.0625, -8.0625],
         [-8.6250, -8.6250, -8.6250,  ..., -8.0648, -8.0625, -8.0625],
         [-8.6305, -8.6305, -8.6307,  ..., -8.0741, -8.0719, -8.0719],
         ...,
         [-6.1250, -6.1250, -6.1309,  ..., -6.5736, -6.5719, -6.5719],
         [-6.2250, -6.2250, -6.2298,  ..., -6.2674, -6.2619, -6.2619],
         [-6.2969, -6.2969, -6.3009,  ..., -6.0473, -6.0391, -6.0391]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the stacked cookies in this image? Please output segmentation mask. ASSISTANT: the stacked cookies</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]

 12%|████████████████▌                                                                                                                               | 23/200 [00:04<00:27,  6.51it/s]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1402, 2102])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1402, 2102])):  [tensor([[[-5.0000, -5.0000, -5.0000,  ..., -4.4375, -4.4375, -4.4375],
         [-5.0000, -5.0000, -5.0000,  ..., -4.4375, -4.4375, -4.4375],
         [-5.0000, -5.0000, -5.0000,  ..., -4.4375, -4.4375, -4.4375],
         ...,
         [-1.8556, -1.8556, -1.8556,  ..., -1.8073, -1.8073, -1.8073],
         [-1.9841, -1.9841, -1.9841,  ..., -1.8069, -1.8069, -1.8069],
         [-2.0449, -2.0449, -2.0449,  ..., -1.8066, -1.8066, -1.8066]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If a person wanted to charge their electric car while parked in the street, what object in the picture could they use to connect the car to an electrical power source? Please output segmentation mask. ASSISTANT: if a person wanted to charge their electric car while parked in the street, what object in the picture could they use to connect the car to an electrical power source</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[86]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2016, 3032])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2016, 3032])):  [tensor([[[-5.6562, -5.6562, -5.6562,  ..., -6.8125, -6.8125, -6.8125],
         [-5.6562, -5.6562, -5.6562,  ..., -6.8125, -6.8125, -6.8125],
         [-5.6562, -5.6562, -5.6562,  ..., -6.8125, -6.8125, -6.8125],
         ...,
         [-4.1684, -4.1684, -4.1684,  ..., -3.6686, -3.6686, -3.6686],
         [-4.1948, -4.1948, -4.1948,  ..., -3.7293, -3.7293, -3.7293],
         [-4.1953, -4.1953, -4.1953,  ..., -3.7305, -3.7305, -3.7305]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Where do people put their dirty hygiene products to keep the bathroom clean? Please output segmentation mask. ASSISTANT: where do people put their dirty hygiene products to keep the bathroom clean</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-7.1027, -6.4780, -6.3361,  ..., -5.3633, -5.7105, -5.8483],
         [-6.5210, -6.9894, -6.7766,  ..., -5.5781, -5.6009, -6.2261],
         [-5.5711, -6.0437, -6.3984,  ..., -4.8359, -4.9375, -5.2598],
         ...,
         [-5.7566, -6.1219, -6.2266,  ..., -5.6562, -5.6219, -5.6863],
         [-5.3289, -5.7156, -6.2016,  ..., -5.7719, -5.7481, -5.8298],
         [-5.3155, -6.1238, -6.4117,  ..., -5.9883, -5.8242, -5.7766]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When the wind blows, small white objects are blown away and scattered in the air. What in the picture is responsible for this phenomenon? Please output segmentation mask. ASSISTANT: when the wind blows, small white objects are blown away and scattered in the air. what in the picture is responsible for this phenomenon</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[-11.9375, -11.9408, -11.9563,  ..., -13.4000, -13.4309, -13.4375],
         [-11.9113, -11.9209, -11.9664,  ..., -13.3941, -13.4218, -13.4277],
         [-11.7876, -11.8273, -12.0143,  ..., -13.3663, -13.3787, -13.3813],
         ...,
         [ -9.9117, -10.0619, -10.7703,  ...,  -8.6170,  -9.1026,  -9.2056],
         [ -8.9720,  -9.0950,  -9.6747,  ...,  -8.1202,  -8.3806,  -8.4358],
         [ -8.0323,  -8.1280,  -8.5791,  ...,  -7.6234,  -7.6586,  -7.6661]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that the dog's food should be put into in this image? Please output segmentation mask. ASSISTANT: something that the dog's food should be put into</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[-6.8125, -6.8160, -6.8281,  ..., -6.8125, -6.8367, -6.8438],
         [-6.7914, -6.8151, -6.8967,  ..., -6.8336, -6.8714, -6.8824],
         [-6.7188, -6.8119, -7.1328,  ..., -6.9062, -6.9910, -7.0156],
         ...,
         [-5.8750, -6.0068, -6.4609,  ..., -4.7891, -4.7830, -4.7812],
         [-5.9473, -6.0786, -6.5309,  ..., -4.7719, -4.7504, -4.7441],
         [-5.9594, -6.0476, -6.3516,  ..., -5.0641, -5.0072, -4.9906]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the food that makes people feel spicy or hot in this image? Please output segmentation mask. ASSISTANT: the food that makes people feel spicy or hot</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[67]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 679, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 679, 1024])):  [tensor([[[-8.3125, -8.3125, -8.2969,  ..., -3.4824, -3.4219, -3.4219],
         [-8.3125, -8.3125, -8.2969,  ..., -3.4824, -3.4219, -3.4219],
         [-8.3125, -8.3125, -8.3174,  ..., -3.4478, -3.3926, -3.3926],
         ...,
         [-4.7148, -4.7148, -4.8242,  ..., -4.3389, -4.3203, -4.3203],
         [-4.9258, -4.9258, -5.0273,  ..., -4.4150, -4.3984, -4.3984],
         [-5.5820, -5.5820, -5.6909,  ..., -5.0078, -4.9609, -4.9609]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the object used to protect the ears in this image? Please output segmentation mask. ASSISTANT: the object used to protect the ears</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2112, 2816])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2112, 2816])):  [tensor([[[-10.1875, -10.1875, -10.1875,  ..., -10.0000, -10.0000, -10.0000],
         [-10.1875, -10.1875, -10.1875,  ..., -10.0000, -10.0000, -10.0000],
         [-10.1875, -10.1875, -10.1875,  ..., -10.0000, -10.0000, -10.0000],
         ...,
         [ -6.1761,  -6.1761,  -6.1761,  ...,  -3.8722,  -3.8722,  -3.8722],
         [ -6.2244,  -6.2244,  -6.2244,  ...,  -3.9233,  -3.9233,  -3.9233],
         [ -6.2305,  -6.2305,  -6.2305,  ...,  -3.9297,  -3.9297,  -3.9297]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the place where piano players should sit in this image? Please output segmentation mask. ASSISTANT: the place where piano players should sit</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 960, 1280])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 960, 1280])):  [tensor([[[-7.4375, -7.4375, -7.4043,  ..., -4.8613, -4.9062, -4.9062],
         [-7.4375, -7.4375, -7.4043,  ..., -4.8613, -4.9062, -4.9062],
         [-7.4766, -7.4766, -7.4501,  ..., -4.8647, -4.9082, -4.9082],
         ...,
         [-2.6680, -2.6680, -2.6989,  ..., -1.6735, -1.6743, -1.6743],
         [-3.0125, -3.0125, -3.0359,  ..., -2.1648, -2.1562, -2.1562],
         [-3.4062, -3.4062, -3.4220,  ..., -2.7180, -2.6963, -2.6963]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the person who is shooting a free throw in this image? Please output segmentation mask. ASSISTANT: the person who is shooting a free throw</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2287, 1831])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2287, 1831])):  [tensor([[[-8.8750, -8.8750, -8.8750,  ..., -7.9387, -7.8757, -7.8516],
         [-8.8750, -8.8750, -8.8750,  ..., -7.9387, -7.8757, -7.8516],
         [-8.8750, -8.8750, -8.8750,  ..., -7.9387, -7.8757, -7.8516],
         ...,
         [-4.9688, -4.9688, -4.9688,  ..., -3.8836, -4.0708, -4.1426],
         [-4.9688, -4.9688, -4.9688,  ..., -3.8836, -4.0708, -4.1426],
         [-4.9688, -4.9688, -4.9688,  ..., -3.8836, -4.0708, -4.1426]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When soldiers gather for a group photo, what object in the picture is usually held by one of them to represent their unit or country? Please output segmentation mask. ASSISTANT: when soldiers gather for a group photo, what object in the picture is usually held by one of them to represent their unit or country</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 431, 600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 431, 600])):  [tensor([[[-8.6875, -8.7050, -8.7583,  ..., -9.3667, -9.4200, -9.4375],
         [-8.7314, -8.7636, -8.8619,  ..., -9.3959, -9.4867, -9.5165],
         [-8.8648, -8.9418, -9.1767,  ..., -9.4849, -9.6895, -9.7566],
         ...,
         [-3.9524, -3.9512, -3.9478,  ..., -5.4484, -5.5065, -5.5256],
         [-4.1058, -4.0841, -4.0180,  ..., -5.6534, -5.8083, -5.8591],
         [-4.6487, -4.6310, -4.5768,  ..., -5.7113, -5.8445, -5.8882]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Sometimes, water leaks from faulty plumbing or faucets. What part of the plumbing system in the picture can be a potential source of the water leak? Please output segmentation mask. ASSISTANT: sometimes, water leaks from faulty plumbing or faucets. what part of the plumbing system in the picture can be a potential source of the water leak</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3872, 2592])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3872, 2592])):  [tensor([[[-10.4375, -10.4375, -10.4375,  ...,  -1.7104,  -1.7656,  -1.7656],
         [-10.4375, -10.4375, -10.4375,  ...,  -1.7104,  -1.7656,  -1.7656],
         [-10.4375, -10.4375, -10.4375,  ...,  -1.7104,  -1.7656,  -1.7656],
         ...,
         [ -5.9688,  -5.9688,  -5.9688,  ...,  -6.6752,  -6.6250,  -6.6250],
         [ -5.9688,  -5.9688,  -5.9688,  ...,  -6.6752,  -6.6250,  -6.6250],
         [ -5.9688,  -5.9688,  -5.9688,  ...,  -6.6752,  -6.6250,  -6.6250]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the part of the persons that is above the water in this image? Please output segmentation mask. ASSISTANT: the part of the persons that is above the water</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[67]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[-10.5000, -10.5000, -10.5000,  ..., -10.6250, -10.6250, -10.6250],
         [-10.5000, -10.5000, -10.5000,  ..., -10.6250, -10.6250, -10.6250],
         [-10.5000, -10.5000, -10.5000,  ..., -10.6250, -10.6250, -10.6250],
         ...,
         [ -6.7943,  -6.7943,  -6.7943,  ...,  -6.4036,  -6.4036,  -6.4036],
         [ -6.7891,  -6.7891,  -6.7891,  ...,  -6.4297,  -6.4297,  -6.4297],
         [ -6.7891,  -6.7891,  -6.7891,  ...,  -6.4297,  -6.4297,  -6.4297]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When we need to access or store things above our reach, what would be helpful to stand on? Please output segmentation mask. ASSISTANT: when we need to access or store things above our reach, what would be helpful to stand on</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[71]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-7.0515, -6.7478, -6.5713,  ..., -3.7530, -4.0817, -4.1231],
         [-6.7693, -7.4847, -7.0547,  ..., -3.9195, -4.1075, -4.0602],
         [-6.3693, -6.9172, -6.8359,  ..., -3.6367, -3.7133, -3.6649],
         ...,
         [-5.2668, -6.1906, -5.9688,  ..., -3.3750, -3.3617, -3.5282],
         [-5.3362, -6.1247, -5.8281,  ..., -3.1680, -3.2448, -3.3405],
         [-5.2151, -5.9187, -5.4723,  ..., -3.8118, -3.7229, -3.8128]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What structure can be used to drain the water from the bathroom floor and prevent the accumulation of wastewater? Please output segmentation mask. ASSISTANT: what structure can be used to drain the water from the bathroom floor and prevent the accumulation of wastewater</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[75]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[ -8.3232,  -8.0528,  -8.0285,  ...,  -6.1219,  -6.6927,  -6.9615],
         [ -9.1914,  -9.4084,  -8.7375,  ...,  -6.8094,  -7.1503,  -6.8431],
         [ -8.3039,  -8.6906,  -8.3281,  ...,  -6.2656,  -6.3687,  -6.0680],
         ...,
         [-12.1801, -15.2094, -15.2500,  ..., -11.0625, -10.9562,  -9.7102],
         [-10.3819, -12.8269, -13.7062,  ..., -10.2703, -10.3047,  -9.1767],
         [ -9.0797, -10.4588, -10.6086,  ...,  -7.5168,  -7.2707,  -7.2726]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the dog that puts its tongue out in this image? Please output segmentation mask. ASSISTANT: the dog that puts its tongue out</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]

(PLUM.py) >> gt_masks (shape: torch.Size([1, 1600, 1200])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1600, 1200])):  [tensor([[[-7.3438, -7.3438, -7.3395,  ..., -7.1836, -7.0875, -7.0156],
         [-7.3438, -7.3438, -7.3395,  ..., -7.1836, -7.0875, -7.0156],
         [-7.3434, -7.3434, -7.3393,  ..., -7.1905, -7.0931, -7.0200],
         ...,
         [-3.6592, -3.6592, -3.6619,  ..., -0.7450, -1.1116, -1.3940],
         [-3.6719, -3.6719, -3.6744,  ..., -0.7524, -1.1167, -1.3972],
         [-3.6719, -3.6719, -3.6744,  ..., -0.7524, -1.1167, -1.3972]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a rural landscape, what objects in the picture could provide shade and shelter for animals or humans? Please output segmentation mask. ASSISTANT: in a rural landscape, what objects in the picture could provide shade and shelter for animals or humans</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[72]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[-3.8750, -3.9031, -4.0000,  ..., -4.4375, -4.6070, -4.6562],
         [-3.8434, -3.8786, -4.0000,  ..., -4.3610, -4.5381, -4.5895],
         [-3.7344, -3.7941, -4.0000,  ..., -4.0977, -4.3005, -4.3594],
         ...,
         [-1.0215, -1.0199, -1.0146,  ..., -0.7527, -0.8331, -0.8564],
         [-1.3671, -1.3684, -1.3729,  ..., -1.3286, -1.3402, -1.3436],
         [-2.1305, -2.1396, -2.1711,  ..., -2.7277, -2.7323, -2.7336]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something used for playing videos or music in this image? Please output segmentation mask. ASSISTANT: something used for playing videos or music</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-2.1094, -2.1094, -2.1230,  ..., -3.9531, -3.9688, -3.9688],
         [-2.1094, -2.1094, -2.1230,  ..., -3.9531, -3.9688, -3.9688],
         [-2.0977, -2.0977, -2.1152,  ..., -3.9558, -3.9727, -3.9727],
         ...,
         [-0.8547, -0.8547, -0.8376,  ..., -0.7715, -0.7906, -0.7906],
         [-1.2515, -1.2515, -1.2386,  ..., -1.3923, -1.4102, -1.4102],
         [-1.9028, -1.9028, -1.9015,  ..., -2.3997, -2.4180, -2.4180]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When celebrating birthdays, it is common to have a cake with decorations. What part of the cake in the picture is typically used to write birthday greetings or the name of the person celebrating? Please output segmentation mask. ASSISTANT: when celebrating birthdays, it is common to have a cake with decorations. what part of the cake in the picture is typically used to write birthday greetings or the name of the person celebrating</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[97]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[-7.8438, -7.8297, -7.7812,  ..., -8.5000, -8.4031, -8.3750],
         [-7.8438, -7.8546, -7.8920,  ..., -8.5387, -8.4827, -8.4664],
         [-7.8438, -7.9404, -8.2734,  ..., -8.6719, -8.7566, -8.7812],
         ...,
         [-5.9062, -6.0170, -6.3984,  ..., -6.0625, -5.9535, -5.9219],
         [-5.9703, -6.1183, -6.6281,  ..., -5.7566, -5.6755, -5.6520],
         [-6.3438, -6.4517, -6.8234,  ..., -5.8719, -5.7920, -5.7688]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that allow pedestrians to cross the canyon in this image? Please output segmentation mask. ASSISTANT: something that allow pedestrians to cross the canyon</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1989, 5815])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1989, 5815])):  [tensor([[[-3.8438, -3.8438, -3.8438,  ..., -2.6094, -2.6094, -2.6094],
         [-3.8438, -3.8438, -3.8438,  ..., -2.6094, -2.6094, -2.6094],
         [-3.8438, -3.8438, -3.8438,  ..., -2.6094, -2.6094, -2.6094],
         ...,
         [-2.0850, -2.0850, -2.0850,  ..., -1.8281, -1.8281, -1.8281],
         [-2.0850, -2.0850, -2.0850,  ..., -1.8281, -1.8281, -1.8281],
         [-2.0850, -2.0850, -2.0850,  ..., -1.8281, -1.8281, -1.8281]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the place where the dishes and meals should be put for eating in this image? Please output segmentation mask. ASSISTANT: the place where the dishes and meals should be put for eating</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[72]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 601, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 601, 800])):  [tensor([[[-10.4375, -10.4309, -10.4000,  ..., -11.0250, -11.0559, -11.0625],
         [-10.4342, -10.4344, -10.4351,  ..., -11.0352, -11.0685, -11.0756],
         [-10.4188, -10.4506, -10.6004,  ..., -11.0831, -11.1279, -11.1374],
         ...,
         [ -6.5552,  -6.6353,  -7.0131,  ...,  -5.2994,  -5.2721,  -5.2663],
         [ -6.2163,  -6.2713,  -6.5306,  ...,  -5.2654,  -5.2039,  -5.1909],
         [ -5.8065,  -5.8295,  -5.9378,  ...,  -5.2614,  -5.1455,  -5.1209]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that the persons use to cross the water in this image? Please output segmentation mask. ASSISTANT: something that the persons use to cross the water</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[-8.9375, -8.9375, -8.9391,  ..., -9.3109, -9.3125, -9.3125],
         [-8.9375, -8.9375, -8.9391,  ..., -9.3109, -9.3125, -9.3125],
         [-8.9445, -8.9445, -8.9463,  ..., -9.3256, -9.3273, -9.3273],
         ...,
         [-8.1719, -8.1719, -8.1914,  ..., -6.6691, -6.6668, -6.6668],
         [-8.4319, -8.4319, -8.4484,  ..., -6.9442, -6.9412, -6.9412],
         [-8.6367, -8.6367, -8.6510,  ..., -7.1677, -7.1641, -7.1641]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Driving at night can be very dangerous due to poor visibility, which can lead to accidents. What part of the car needs to be turned on when driving at night? Please output segmentation mask. ASSISTANT: driving at night can be very dangerous due to poor visibility, which can lead to accidents. what part of the car needs to be turned on when driving at night</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 864, 1152])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 864, 1152])):  [tensor([[[-6.3438, -6.3438, -6.3720,  ..., -6.1558, -6.0938, -6.0938],
         [-6.3438, -6.3438, -6.3720,  ..., -6.1558, -6.0938, -6.0938],
         [-6.3409, -6.3409, -6.3773,  ..., -6.2041, -6.1502, -6.1502],
         ...,
         [-5.0330, -5.0330, -5.1432,  ..., -5.1992, -5.2023, -5.2023],
         [-5.0469, -5.0469, -5.1480,  ..., -5.2273, -5.2344, -5.2344],
         [-5.1445, -5.1445, -5.2204,  ..., -5.2521, -5.2539, -5.2539]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n This rocky terrain can be challenging to navigate. What object in the picture could provide information to guide travelers through this area? Please output segmentation mask. ASSISTANT: this rocky terrain can be challenging to navigate. what object in the picture could provide information to guide travelers through this area</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2592, 3888])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2592, 3888])):  [tensor([[[-7.8125, -7.8125, -7.8125,  ..., -8.8125, -8.8125, -8.8125],
         [-7.8125, -7.8125, -7.8125,  ..., -8.8125, -8.8125, -8.8125],
         [-7.8125, -7.8125, -7.8125,  ..., -8.8125, -8.8125, -8.8125],
         ...,
         [-5.5776, -5.5776, -5.5776,  ..., -4.7919, -4.7919, -4.7919],
         [-5.6055, -5.6055, -5.6055,  ..., -4.7969, -4.7969, -4.7969],
         [-5.6055, -5.6055, -5.6055,  ..., -4.7969, -4.7969, -4.7969]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is The fastest sinking ship in this image? Please output segmentation mask. ASSISTANT: the fastest sinking ship</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 333, 500])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 333, 500])):  [tensor([[[-13.9375, -13.8370, -13.6450,  ..., -13.1062, -12.6262, -12.3750],
         [-14.0213, -14.1183, -14.3037,  ..., -13.5505, -13.2077, -13.0283],
         [-14.1813, -14.6556, -15.5619,  ..., -14.3991, -14.3185, -14.2763],
         ...,
         [ -8.1775,  -8.4763,  -9.0472,  ...,  -8.8476,  -8.6326,  -8.5200],
         [ -8.1975,  -8.5101,  -9.1074,  ...,  -8.6064,  -8.4283,  -8.3350],
         [ -8.3575,  -8.4729,  -8.6933,  ...,  -8.5655,  -7.9941,  -7.6950]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In some regions, people raise certain animals for their milk, meat, and skin. What animal in the picture could be domesticated for such purposes? Please output segmentation mask. ASSISTANT: in some regions, people raise certain animals for their milk, meat, and skin. what animal in the picture could be domesticated for such purposes</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1365, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1365, 2048])):  [tensor([[[-7.7812, -7.7812, -7.7812,  ..., -8.3750, -8.3750, -8.3750],
         [-7.7812, -7.7812, -7.7812,  ..., -8.3750, -8.3750, -8.3750],
         [-7.7812, -7.7812, -7.7812,  ..., -8.3750, -8.3750, -8.3750],
         ...,
         [-5.6522, -5.6522, -5.6522,  ..., -4.2627, -4.2627, -4.2627],
         [-5.7226, -5.7226, -5.7226,  ..., -4.2803, -4.2803, -4.2803],
         [-5.7578, -5.7578, -5.7578,  ..., -4.2891, -4.2891, -4.2891]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the place where the child is about to slip/fall off in this image? Please output segmentation mask. ASSISTANT: the place where the child is about to slip/fall off</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2152, 1704])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2152, 1704])):  [tensor([[[-5.4688, -5.4688, -5.4688,  ..., -6.2130, -6.3841, -6.4609],
         [-5.4688, -5.4688, -5.4688,  ..., -6.2130, -6.3841, -6.4609],
         [-5.4688, -5.4688, -5.4688,  ..., -6.2130, -6.3841, -6.4609],
         ...,
         [-1.7969, -1.7969, -1.7969,  ..., -1.9429, -2.0652, -2.1201],
         [-1.7969, -1.7969, -1.7969,  ..., -1.9429, -2.0652, -2.1201],
         [-1.7969, -1.7969, -1.7969,  ..., -1.9429, -2.0652, -2.1201]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In order to facilitate transportation and connect different regions, what structure in the picture was built across the water? Please output segmentation mask. ASSISTANT: in order to facilitate transportation and connect different regions, what structure in the picture was built across the water</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[74]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3264, 4928])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3264, 4928])):  [tensor([[[-10.1250, -10.1250, -10.1250,  ...,  -9.6875,  -9.6875,  -9.6875],
         [-10.1250, -10.1250, -10.1250,  ...,  -9.6875,  -9.6875,  -9.6875],
         [-10.1250, -10.1250, -10.1250,  ...,  -9.6875,  -9.6875,  -9.6875],
         ...,
         [ -3.7162,  -3.7162,  -3.7162,  ...,  -4.1677,  -4.1677,  -4.1677],
         [ -3.7227,  -3.7227,  -3.7227,  ...,  -4.1719,  -4.1719,  -4.1719],
         [ -3.7227,  -3.7227,  -3.7227,  ...,  -4.1719,  -4.1719,  -4.1719]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the cesspit in this image? Please output segmentation mask. ASSISTANT: the cesspit</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]

(PLUM.py) >> gt_masks (shape: torch.Size([1, 2344, 3190])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2344, 3190])):  [tensor([[[-17.1250, -17.1250, -17.1250,  ..., -16.2500, -16.2500, -16.2500],
         [-17.1250, -17.1250, -17.1250,  ..., -16.2500, -16.2500, -16.2500],
         [-17.1250, -17.1250, -17.1250,  ..., -16.2500, -16.2500, -16.2500],
         ...,
         [ -8.1631,  -8.1631,  -8.1631,  ...,  -7.6076,  -7.6076,  -7.6076],
         [ -8.0781,  -8.0781,  -8.0781,  ...,  -7.4141,  -7.4141,  -7.4141],
         [ -8.0781,  -8.0781,  -8.0781,  ...,  -7.4141,  -7.4141,  -7.4141]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the objects that can protect the snail and prevent it from getting injured in this image? Please output segmentation mask. ASSISTANT: the objects that can protect the snail and prevent it from getting injured</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[71]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1350, 1800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1350, 1800])):  [tensor([[[-3.4375, -3.4375, -3.4375,  ..., -8.9375, -8.9375, -8.9375],
         [-3.4375, -3.4375, -3.4375,  ..., -8.9375, -8.9375, -8.9375],
         [-3.4375, -3.4375, -3.4375,  ..., -8.9375, -8.9375, -8.9375],
         ...,
         [-4.5420, -4.5420, -4.5420,  ..., -1.9591, -1.9591, -1.9591],
         [-4.7065, -4.7065, -4.7065,  ..., -2.2879, -2.2879, -2.2879],
         [-4.8086, -4.8086, -4.8086,  ..., -2.4922, -2.4922, -2.4922]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If a person wants to watch TV or a movie, which furniture is the most suitable for them to sit and watch? Please output segmentation mask. ASSISTANT: if a person wants to watch tv or a movie, which furniture is the most suitable for them to sit and watch</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 1., 1., 0.],
         [0., 0., 0.,  ..., 1., 1., 0.],
         [0., 0., 0.,  ..., 1., 1., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-4.6483, -4.4684, -4.0341,  ..., -4.0521, -4.3260, -4.5011],
         [-4.4039, -4.9625, -4.6195,  ..., -4.3422, -4.3819, -4.7342],
         [-3.7202, -4.2305, -4.2695,  ..., -3.8867, -3.9922, -4.2070],
         ...,
         [-5.6998, -5.5172, -5.8594,  ..., -4.5156, -4.2836, -3.9667],
         [-5.4232, -5.1547, -5.8672,  ..., -4.5250, -4.3317, -4.1180],
         [-5.3366, -5.3490, -5.7407,  ..., -4.5502, -4.3284, -4.5733]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In historical buildings, there are often signs or symbols displayed on the walls or floors to represent a specific meaning or identity. What in the picture could be used to display such signs or symbols? Please output segmentation mask. ASSISTANT: in historical buildings, there are often signs or symbols displayed on the walls or floors to represent a specific meaning or identity. what in the picture could be used to display such signs or symbols</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[90]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-9.6875, -9.6875, -9.6406,  ..., -9.0234, -9.0000, -9.0000],
         [-9.6875, -9.6875, -9.6406,  ..., -9.0234, -9.0000, -9.0000],
         [-9.7422, -9.7422, -9.7334,  ..., -9.1133, -9.0859, -9.0859],
         ...,
         [-5.9414, -5.9414, -6.1108,  ..., -7.0566, -7.0742, -7.0742],
         [-5.9258, -5.9258, -6.0518,  ..., -6.7168, -6.7344, -6.7344],
         [-5.7773, -5.7773, -5.8506,  ..., -6.0957, -6.0781, -6.0781]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Insects can help pollinate flowers effectively, but some insects also collect pollen and nectar to produce honey. What in the picture does not make honey? Please output segmentation mask. ASSISTANT: insects can help pollinate flowers effectively, but some insects also collect pollen and nectar to produce honey. what in the picture does not make honey</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[89]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 651, 500])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 651, 500])):  [tensor([[[-9.0000, -8.8760, -8.4421,  ..., -8.9080, -7.9208, -6.8278],
         [-8.9933, -8.9121, -8.6282,  ..., -8.9983, -7.9701, -6.8151],
         [-8.9698, -9.0385, -9.2787,  ..., -9.3139, -8.1428, -6.7709],
         ...,
         [-6.3992, -6.7470, -7.9638,  ..., -9.6279, -7.8736, -5.8568],
         [-5.8944, -6.3366, -7.8838,  ..., -9.8086, -7.9247, -5.7094],
         [-5.7500, -6.2192, -7.8609,  ..., -9.8603, -7.9393, -5.6673]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Flowers are often used to decorate tables during special occasions or events. What item in the picture can be used to hold the flowers and keep them fresh by providing water to them? Please output segmentation mask. ASSISTANT: flowers are often used to decorate tables during special occasions or events. what item in the picture can be used to hold the flowers and keep them fresh by providing water to them</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1370, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1370, 2048])):  [tensor([[[-3.6250, -3.6250, -3.6250,  ..., -5.3125, -5.3125, -5.3125],
         [-3.6250, -3.6250, -3.6250,  ..., -5.3125, -5.3125, -5.3125],
         [-3.6250, -3.6250, -3.6250,  ..., -5.3125, -5.3125, -5.3125],
         ...,
         [-3.9385, -3.9385, -3.9385,  ..., -3.4229, -3.4229, -3.4229],
         [-4.0146, -4.0146, -4.0146,  ..., -3.4365, -3.4365, -3.4365],
         [-4.0527, -4.0527, -4.0527,  ..., -3.4434, -3.4434, -3.4434]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Bamboo is very hard, and its sharp edges can easily scratch people. What tool in the picture can I use to split the bamboo and cross this bamboo forest? Please output segmentation mask. ASSISTANT: bamboo is very hard, and its sharp edges can easily scratch people. what tool in the picture can i use to split the bamboo and cross this bamboo forest</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[92]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[-4.5938, -4.5938, -4.5977,  ..., -1.8052, -1.8047, -1.8047],
         [-4.5938, -4.5938, -4.5977,  ..., -1.8052, -1.8047, -1.8047],
         [-4.5961, -4.5961, -4.6000,  ..., -1.8000, -1.7995, -1.7995],
         ...,
         [-3.1359, -3.1359, -3.1365,  ..., -5.8166, -5.8191, -5.8191],
         [-3.4169, -3.4169, -3.4174,  ..., -5.5504, -5.5519, -5.5519],
         [-3.6289, -3.6289, -3.6294,  ..., -5.3393, -5.3398, -5.3398]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the damaged part on the pants in this image? Please output segmentation mask. ASSISTANT: the damaged part on the pants</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[-3.4531, -3.4531, -3.4531,  ..., -1.8594, -1.8594, -1.8594],
         [-3.4531, -3.4531, -3.4531,  ..., -1.8594, -1.8594, -1.8594],
         [-3.4531, -3.4531, -3.4531,  ..., -1.8594, -1.8594, -1.8594],
         ...,
         [-2.8483, -2.8483, -2.8483,  ..., -2.2214, -2.2214, -2.2214],
         [-2.8809, -2.8809, -2.8809,  ..., -2.2578, -2.2578, -2.2578],
         [-2.8809, -2.8809, -2.8809,  ..., -2.2578, -2.2578, -2.2578]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Seafood dishes often include a tangy condiment that enhances the flavor. What item in the picture can be squeezed onto the seafood as a tangy flavor enhancer? Please output segmentation mask. ASSISTANT: seafood dishes often include a tangy condiment that enhances the flavor. what item in the picture can be squeezed onto the seafood as a tangy flavor enhancer</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[96]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1080, 1080])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1080, 1080])):  [tensor([[[-4.7812, -4.7812, -4.7846,  ..., -7.6882, -7.7188, -7.7188],
         [-4.7812, -4.7812, -4.7846,  ..., -7.6882, -7.7188, -7.7188],
         [-4.7982, -4.7982, -4.8094,  ..., -7.7358, -7.7697, -7.7697],
         ...,
         [-5.3288, -5.3288, -5.5587,  ..., -7.6434, -7.6569, -7.6569],
         [-5.2812, -5.2812, -5.5192,  ..., -7.6603, -7.6875, -7.6875],
         [-5.2812, -5.2812, -5.5192,  ..., -7.6603, -7.6875, -7.6875]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Dogs often like to find a comfortable place to rest. What object in the picture can offer a soft and comfortable surface for the dog to lie on? Please output segmentation mask. ASSISTANT: dogs often like to find a comfortable place to rest. what object in the picture can offer a soft and comfortable surface for the dog to lie on</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[82]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 299, 400])):  [tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 0., 0., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 299, 400])):  [tensor([[[-5.9688, -5.9113, -5.8172,  ..., -3.2330, -3.2716, -3.2500],
         [-5.8682, -6.0749, -6.2241,  ..., -3.1190, -3.1695, -3.1710],
         [-5.6583, -6.1426, -6.5556,  ..., -2.8976, -2.9584, -2.9841],
         ...,
         [-5.1304, -5.9259, -6.6804,  ..., -3.8570, -3.7902, -3.7883],
         [-3.4313, -4.1257, -4.7811,  ..., -2.9879, -3.1158, -3.2886],
         [-3.5337, -4.0775, -4.5800,  ..., -3.2811, -3.3366, -3.4275]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the area that people can walk on in this image? Please output segmentation mask. ASSISTANT: the area that people can walk on</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 300, 400])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 300, 400])):  [tensor([[[-1.9219, -1.9147, -1.8959,  ..., -1.0444, -1.0115, -0.9414],
         [-2.0728, -2.2177, -2.3289,  ..., -1.0177, -1.0049, -0.9630],
         [-2.1606, -2.4428, -2.6740,  ..., -0.9815, -0.9765, -0.9539],
         ...,
         [-2.1457, -2.2678, -2.4237,  ..., -0.6932, -0.7708, -0.8546],
         [-2.1841, -2.2470, -2.3662,  ..., -0.8483, -0.8413, -0.8388],
         [-2.3412, -2.3701, -2.4588,  ..., -1.4407, -1.3656, -1.2903]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When admiring a beautiful sunset, what part of the picture would we most likely focus on? Please output segmentation mask. ASSISTANT: when admiring a beautiful sunset, what part of the picture would we most likely focus on</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[71]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1542, 3494])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1542, 3494])):  [tensor([[[-3.0156, -3.0156, -3.0156,  ..., -3.0469, -3.0469, -3.0469],
         [-3.0156, -3.0156, -3.0156,  ..., -3.0469, -3.0469, -3.0469],
         [-3.0156, -3.0156, -3.0156,  ..., -3.0469, -3.0469, -3.0469],
         ...,
         [-1.7283, -1.7283, -1.7283,  ..., -1.8777, -1.8777, -1.8777],
         [-1.8047, -1.8047, -1.8047,  ..., -1.9414, -1.9414, -1.9414],
         [-1.8047, -1.8047, -1.8047,  ..., -1.9414, -1.9414, -1.9414]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n As a speaker, relying solely on verbal explanations may not effectively convey the intended message to the audience. What area of the picture should be used to project the key content and make it more understandable for the audience during the presentation? Please output segmentation mask. ASSISTANT: as a speaker, relying solely on verbal explanations may not effectively convey the intended message to the audience. what area of the picture should be used to project the key content and make it more understandable for the audience during the presentation</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[101]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2848, 4288])):  [tensor([[[0., 0., 0.,  ..., 1., 1., 1.],
         [0., 0., 0.,  ..., 1., 1., 1.],
         [0., 0., 0.,  ..., 1., 1., 1.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2848, 4288])):  [tensor([[[-7.6250, -7.6250, -7.6250,  ..., -1.6797, -1.6797, -1.6797],
         [-7.6250, -7.6250, -7.6250,  ..., -1.6797, -1.6797, -1.6797],
         [-7.6250, -7.6250, -7.6250,  ..., -1.6797, -1.6797, -1.6797],
         ...,
         [-5.4767, -5.4767, -5.4767,  ..., -2.3193, -2.3193, -2.3193],
         [-5.5312, -5.5312, -5.5312,  ..., -2.3359, -2.3359, -2.3359],
         [-5.5312, -5.5312, -5.5312,  ..., -2.3359, -2.3359, -2.3359]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a modern office, what object in the picture is commonly used for inputting data and controlling the computer? Please output segmentation mask. ASSISTANT: in a modern office, what object in the picture is commonly used for inputting data and controlling the computer</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[74]]
>> (validate) sampled_classes_list:  [None]

         [-4.4531, -4.4531, -4.4968,  ..., -4.8633, -4.8203, -4.8203]]],0., 0., 0.,  ..., 0., 0., 0.],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a zoo, there are separate areas designated for different animals. What structure in the picture is used to confine and display animals? Please output segmentation mask. ASSISTANT: in a zoo, there are separate areas designated for different animals. what structure in the picture is used to confine and display animals</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[0., 0., 1.,  ..., 0., 0., 0.],
         [0., 0., 1.,  ..., 0., 0., 0.],
         [0., 0., 1.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[-1.4766, -1.4766, -1.4766,  ..., -2.0781, -2.0781, -2.0781],
         [-1.4766, -1.4766, -1.4766,  ..., -2.0781, -2.0781, -2.0781],
         [-1.4766, -1.4766, -1.4766,  ..., -2.0781, -2.0781, -2.0781],
         ...,
         [-1.2748, -1.2748, -1.2748,  ..., -1.5021, -1.5021, -1.5021],
         [-1.4430, -1.4430, -1.4430,  ..., -2.0059, -2.0059, -2.0059],
         [-1.4824, -1.4824, -1.4824,  ..., -2.1240, -2.1240, -2.1240]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that tastes spicy in this image? Please output segmentation mask. ASSISTANT: something that tastes spicy</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-7.1250, -7.1250, -7.0859,  ..., -7.2656, -7.3125, -7.3125],
         [-7.1250, -7.1250, -7.0859,  ..., -7.2656, -7.3125, -7.3125],
         [-7.1523, -7.1523, -7.1353,  ..., -7.2896, -7.3398, -7.3398],
         ...,
         [-7.9258, -7.9258, -8.1460,  ..., -8.3286, -8.3828, -8.3828],
         [-7.8789, -7.8789, -8.0767,  ..., -8.0923, -8.1445, -8.1445],
         [-7.9492, -7.9492, -8.0972,  ..., -7.5503, -7.5586, -7.5586]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When going fishing on a calm sea, what type of boat shown in the picture would be an ideal choice for a peaceful fishing experience? Please output segmentation mask. ASSISTANT: when going fishing on a calm sea, what type of boat shown in the picture would be an ideal choice for a peaceful fishing experience</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-8.1125, -8.0246, -7.8484,  ..., -8.0533, -8.6350, -8.3858],
         [-7.5305, -8.5209, -8.1062,  ..., -8.4078, -8.4975, -8.7209],
         [-7.1877, -8.0578, -7.8281,  ..., -8.1484, -8.2156, -8.2801],
         ...,
         [-3.5812, -4.1773, -4.2148,  ..., -4.3750, -4.2531, -4.2961],
         [-3.7790, -4.5138, -4.6141,  ..., -4.4984, -4.2788, -4.3948],
         [-4.5222, -5.1831, -5.2445,  ..., -4.9457, -4.6263, -4.6596]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Insects have various ways to protect themselves from predators. What characteristics can a moth use to deter potential threats? Please output segmentation mask. ASSISTANT: insects have various ways to protect themselves from predators. what characteristics can a moth use to deter potential threats</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[77]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3872, 2592])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3872, 2592])):  [tensor([[[-9.5000, -9.5000, -9.5000,  ..., -8.5307, -8.5156, -8.5156],
         [-9.5000, -9.5000, -9.5000,  ..., -8.5307, -8.5156, -8.5156],
         [-9.5000, -9.5000, -9.5000,  ..., -8.5307, -8.5156, -8.5156],
         ...,
         [-6.3125, -6.3125, -6.3125,  ..., -7.0283, -7.0195, -7.0195],
         [-6.3125, -6.3125, -6.3125,  ..., -7.0283, -7.0195, -7.0195],
         [-6.3125, -6.3125, -6.3125,  ..., -7.0283, -7.0195, -7.0195]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Fishing is a popular activity for relaxation and leisure. What tool is the man in the picture using to catch fish? Please output segmentation mask. ASSISTANT: fishing is a popular activity for relaxation and leisure. what tool is the man in the picture using to catch fish</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[-11.5000, -11.5000, -11.4937,  ..., -10.0195, -10.0000, -10.0000],
         [-11.5000, -11.5000, -11.4937,  ..., -10.0195, -10.0000, -10.0000],
         [-11.4992, -11.4992, -11.4934,  ..., -10.0357, -10.0164, -10.0164],
         ...,
         [ -4.2693,  -4.2693,  -4.2754,  ...,  -7.9006,  -7.8969,  -7.8969],
         [ -4.7884,  -4.7884,  -4.7930,  ...,  -7.9791,  -7.9750,  -7.9750],
         [ -5.1855,  -5.1855,  -5.1889,  ...,  -8.0515,  -8.0469,  -8.0469]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In some cultures, people use sculptures and figurines to represent various aspects of life, including celebrations, rituals, and art. What object in the picture can be considered a representation of a human figure? Please output segmentation mask. ASSISTANT: in some cultures, people use sculptures and figurines to represent various aspects of life, including celebrations, rituals, and art. what object in the picture can be considered a representation of a human figure</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[95]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2868, 4309])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2868, 4309])):  [tensor([[[-4.3750, -4.3750, -4.3750,  ..., -5.9688, -5.9688, -5.9688],
         [-4.3750, -4.3750, -4.3750,  ..., -5.9688, -5.9688, -5.9688],
         [-4.3750, -4.3750, -4.3750,  ..., -5.9688, -5.9688, -5.9688],
         ...,
         [-4.2631, -4.2631, -4.2631,  ..., -3.5986, -3.5986, -3.5986],
         [-4.2461, -4.2461, -4.2461,  ..., -3.5605, -3.5605, -3.5605],
         [-4.2461, -4.2461, -4.2461,  ..., -3.5605, -3.5605, -3.5605]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the shadow of the red car in this image? Please output segmentation mask. ASSISTANT: the shadow of the red car</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2736, 3648])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2736, 3648])):  [tensor([[[-11.5000, -11.5000, -11.5000,  ...,  -7.0625,  -7.0625,  -7.0625],
         [-11.5000, -11.5000, -11.5000,  ...,  -7.0625,  -7.0625,  -7.0625],
         [-11.5000, -11.5000, -11.5000,  ...,  -7.0625,  -7.0625,  -7.0625],
         ...,
         [ -6.5847,  -6.5847,  -6.5847,  ...,  -6.3128,  -6.3128,  -6.3128],
         [ -6.6367,  -6.6367,  -6.6367,  ...,  -6.5508,  -6.5508,  -6.5508],
         [ -6.6367,  -6.6367,  -6.6367,  ...,  -6.5508,  -6.5508,  -6.5508]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the part of this animal's body that comes into contact with the air in this image? Please output segmentation mask. ASSISTANT: the part of this animal's body that comes into contact with the air</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[72]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1000, 1504])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1000, 1504])):  [tensor([[[-14.1875, -14.1875, -14.1512,  ..., -10.9682, -10.8750, -10.8750],
         [-14.1875, -14.1875, -14.1512,  ..., -10.9682, -10.8750, -10.8750],
         [-14.1922, -14.1922, -14.1592,  ..., -11.1327, -11.0395, -11.0395],
         ...,
         [-12.7667, -12.7667, -12.8159,  ..., -10.0311, -10.0168, -10.0168],
         [-12.0537, -12.0537, -12.0889,  ...,  -9.7294,  -9.6976,  -9.6976],
         [-11.5078, -11.5078, -11.5323,  ...,  -9.4983,  -9.4531,  -9.4531]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Many people use bags to carry their belongings when they go out. What part of the bag in the picture can be used to carry the bag comfortably over the shoulder? Please output segmentation mask. ASSISTANT: many people use bags to carry their belongings when they go out. what part of the bag in the picture can be used to carry the bag comfortably over the shoulder</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1920, 2560])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1920, 2560])):  [tensor([[[-5.3438, -5.3438, -5.3438,  ..., -4.8750, -4.8750, -4.8750],
         [-5.3438, -5.3438, -5.3438,  ..., -4.8750, -4.8750, -4.8750],
         [-5.3438, -5.3438, -5.3438,  ..., -4.8750, -4.8750, -4.8750],
         ...,
         [-3.9922, -3.9922, -3.9922,  ..., -3.4336, -3.4336, -3.4336],
         [-4.0516, -4.0516, -4.0516,  ..., -3.5258, -3.5258, -3.5258],
         [-4.0664, -4.0664, -4.0664,  ..., -3.5488, -3.5488, -3.5488]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n During a meal, people typically use utensils to bring food to their mouths. What tool in the picture can be used to fulfill this need? Please output segmentation mask. ASSISTANT: during a meal, people typically use utensils to bring food to their mouths. what tool in the picture can be used to fulfill this need</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-5.8125, -5.8223, -5.8688,  ..., -5.6156, -5.6234, -5.6250],
         [-5.8109, -5.8231, -5.8809,  ..., -5.6140, -5.6258, -5.6283],
         [-5.8031, -5.8267, -5.9381,  ..., -5.6062, -5.6372, -5.6437],
         ...,
         [-3.5250, -3.5729, -3.7987,  ..., -4.2080, -4.2478, -4.2562],
         [-3.5616, -3.6077, -3.8248,  ..., -4.3032, -4.3348, -4.3415],
         [-3.5891, -3.6198, -3.7649,  ..., -4.2202, -4.2247, -4.2256]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n During the move, what object can be used to store and transport various sundries and small household items, which is sturdy and relatively easy to carry? Please output segmentation mask. ASSISTANT: during the move, what object can be used to store and transport various sundries and small household items, which is sturdy and relatively easy to carry</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-7.7116, -7.4677, -7.1953,  ..., -7.5422, -7.7867, -7.5772],
         [-7.5752, -8.0994, -7.6250,  ..., -8.1438, -8.0875, -8.1734],
         [-7.2111, -7.6516, -7.4141,  ..., -7.9297, -7.9859, -7.9537],
         ...,
         [ 0.6067,  0.5644,  0.5674,  ..., -5.0625, -5.0984, -5.1092],
         [ 0.4403,  0.3655,  0.2352,  ..., -4.9516, -5.0106, -5.0708],
         [-0.4639, -0.4360, -0.6105,  ..., -4.9434, -4.8518, -4.8262]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If we were to take a flight in the sky, what part of the plane in the picture would be spinning to generate lift and propel the aircraft forward? Please output segmentation mask. ASSISTANT: if we were to take a flight in the sky, what part of the plane in the picture would be spinning to generate lift and propel the aircraft forward</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2499, 3680])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2499, 3680])):  [tensor([[[-8.8125, -8.8125, -8.8125,  ..., -7.7500, -7.7500, -7.7500],
         [-8.8125, -8.8125, -8.8125,  ..., -7.7500, -7.7500, -7.7500],
         [-8.8125, -8.8125, -8.8125,  ..., -7.7500, -7.7500, -7.7500],
         ...,
         [-6.0372, -6.0372, -6.0372,  ..., -5.6628, -5.6628, -5.6628],
         [-6.0898, -6.0898, -6.0898,  ..., -5.7070, -5.7070, -5.7070],
         [-6.0898, -6.0898, -6.0898,  ..., -5.7070, -5.7070, -5.7070]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something used by a Jedi warrior to attack enemies in this image? Please output segmentation mask. ASSISTANT: something used by a jedi warrior to attack enemies</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 720, 502])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 720, 502])):  [tensor([[[-5.2188, -5.2188, -5.2188,  ..., -5.0699, -4.8063, -4.5396],
         [-5.2311, -5.2395, -5.2725,  ..., -5.1564, -4.8591, -4.5572],
         [-5.2795, -5.3209, -5.4828,  ..., -5.4946, -5.0656, -4.6260],
         ...,
         [-2.7422, -2.8075, -3.0629,  ..., -3.1877, -3.2885, -3.3872],
         [-2.7857, -2.8572, -3.1366,  ..., -3.1619, -3.2288, -3.2931],
         [-2.7969, -2.8699, -3.1554,  ..., -3.1553, -3.2135, -3.2691]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What object in the picture should be used with water to create foam and effectively remove oil and dirt when your hands are very dirty? Please output segmentation mask. ASSISTANT: what object in the picture should be used with water to create foam and effectively remove oil and dirt when your hands are very dirty</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[79]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')] -4.8203]]],0., 0., 0.,  ..., 0., 0., 0.],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[-14.4375, -14.4375, -14.4375,  ...,  -9.7500,  -9.7500,  -9.7500],
         [-14.4375, -14.4375, -14.4375,  ...,  -9.7500,  -9.7500,  -9.7500],
         [-14.4375, -14.4375, -14.4375,  ...,  -9.7500,  -9.7500,  -9.7500],
         ...,
         [ -8.2555,  -8.2555,  -8.2555,  ...,  -7.2377,  -7.2377,  -7.2377],
         [ -8.5970,  -8.5970,  -8.5970,  ...,  -7.3926,  -7.3926,  -7.3926],
         [ -8.7305,  -8.7305,  -8.7305,  ...,  -7.4531,  -7.4531,  -7.4531]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a concert or public speaking event, what object in the picture would be used by the speaker or performer to amplify their voice? Please output segmentation mask. ASSISTANT: in a concert or public speaking event, what object in the picture would be used by the speaker or performer to amplify their voice</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[79]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 482, 720])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 482, 720])):  [tensor([[[-11.5625, -11.5378, -11.4410,  ..., -13.7569, -13.9505, -14.0000],
         [-11.5724, -11.5650, -11.5358,  ..., -13.7710, -13.9692, -14.0198],
         [-11.6112, -11.6713, -11.9062,  ..., -13.8259, -14.0421, -14.0974],
         ...,
         [ -9.6123,  -9.8561, -10.8100,  ...,  -8.7817,  -8.6176,  -8.5757],
         [ -9.6499,  -9.8835, -10.7972,  ...,  -8.4876,  -8.4113,  -8.3919],
         [ -9.7166,  -9.9396, -10.8120,  ...,  -8.1972,  -8.2106,  -8.2140]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In the living room, people often sit on the sofa to watch TV or chat. What object can they use to adjust the TV screen or change channels? Please output segmentation mask. ASSISTANT: in the living room, people often sit on the sofa to watch tv or chat. what object can they use to adjust the tv screen or change channels</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[83]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-4.5841, -4.4744, -4.3715,  ..., -4.1627, -4.2944, -4.2592],
         [-4.1844, -5.1308, -5.0375,  ..., -4.5172, -4.3309, -4.7241],
         [-3.6464, -4.3930, -4.9297,  ..., -4.3672, -4.2703, -4.3885],
         ...,
         [-4.6718, -4.6664, -5.1680,  ..., -3.8672, -3.8289, -3.2220],
         [-4.1373, -3.9536, -5.0055,  ..., -3.7641, -3.8234, -3.3186],
         [-4.2659, -4.4981, -4.9700,  ..., -4.0797, -4.0686, -4.0227]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that produces pollen in this image? Please output segmentation mask. ASSISTANT: something that produces pollen</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1023, 1392])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1023, 1392])):  [tensor([[[-2.2812, -2.2812, -2.2670,  ..., -4.2307, -4.2188, -4.2188],
         [-2.2812, -2.2812, -2.2670,  ..., -4.2307, -4.2188, -4.2188],
         [-2.2640, -2.2640, -2.2504,  ..., -4.2378, -4.2267, -4.2267],
         ...,
         [-3.8577, -3.8577, -3.9007,  ..., -4.5032, -4.5048, -4.5048],
         [-3.6880, -3.6880, -3.7172,  ..., -4.1334, -4.1310, -4.1310],
         [-3.5488, -3.5488, -3.5665,  ..., -3.8299, -3.8242, -3.8242]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the part that can transmit traction and braking torque, coming into contact with the ground in this image? Please output segmentation mask. ASSISTANT: the part that can transmit traction and braking torque, coming into contact with the ground</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 752, 1169])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 752, 1169])):  [tensor([[[-6.1875, -6.1875, -6.1848,  ..., -6.1956, -6.1875, -6.1875],
         [-6.1875, -6.1875, -6.1848,  ..., -6.1956, -6.1875, -6.1875],
         [-6.1983, -6.1983, -6.2040,  ..., -6.2173, -6.2118, -6.2118],
         ...,
         [-4.0017, -4.0017, -4.0627,  ..., -4.0839, -4.1150, -4.1150],
         [-3.6331, -3.6331, -3.6794,  ..., -3.6586, -3.6868, -3.6868],
         [-3.5312, -3.5312, -3.5723,  ..., -3.5499, -3.5723, -3.5723]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the area where people can stand without getting wet in this image? Please output segmentation mask. ASSISTANT: the area where people can stand without getting wet</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-3.9531, -3.9539, -3.9578,  ..., -5.7969, -5.9902, -6.0312],
         [-3.9548, -3.9585, -3.9762,  ..., -5.8335, -6.0223, -6.0624],
         [-3.9625, -3.9801, -4.0628,  ..., -6.0059, -6.1738, -6.2094],
         ...,
         [ 0.1181,  0.1243,  0.1535,  ...,  0.8649,  0.7279,  0.6988],
         [-0.4234, -0.4191, -0.3986,  ..., -0.1217, -0.2270, -0.2494],
         [-1.8320, -1.8369, -1.8598,  ..., -2.3377, -2.4667, -2.4941]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a motorcycle race, there are often sharp turns that require skilled maneuvering. What part of the race track in the picture indicates a sharp turn? Please output segmentation mask. ASSISTANT: in a motorcycle race, there are often sharp turns that require skilled maneuvering. what part of the race track in the picture indicates a sharp turn</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 682, 1023])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [0., 0., 0.,  ..., 1., 1., 1.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 682, 1023])):  [tensor([[[-5.5312, -5.5312, -5.4998,  ..., -9.3282, -9.3125, -9.3125],
         [-5.5313, -5.5312, -5.4999,  ..., -9.3284, -9.3128, -9.3128],
         [-5.5470, -5.5470, -5.5265,  ..., -9.4363, -9.4384, -9.4384],
         ...,
         [-5.7006, -5.7007, -5.7769,  ..., -4.5267, -4.5293, -4.5293],
         [-5.3172, -5.3173, -5.3375,  ..., -4.0409, -3.9895, -3.9895],
         [-5.4608, -5.4608, -5.4555,  ..., -4.1171, -4.0235, -4.0234]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the region showing current time in this image? Please output segmentation mask. ASSISTANT: the region showing current time</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-2.1094, -2.1094, -2.0957,  ..., -2.3301, -2.3438, -2.3438],
         [-2.1094, -2.1094, -2.0957,  ..., -2.3301, -2.3438, -2.3438],
         [-2.1055, -2.1055, -2.0959,  ..., -2.3208, -2.3301, -2.3301],
         ...,
         [-1.5898, -1.5898, -1.6113,  ..., -1.3895, -1.3721, -1.3721],
         [-1.6650, -1.6650, -1.6852,  ..., -1.5256, -1.5078, -1.5078],
         [-1.7295, -1.7295, -1.7489,  ..., -1.7839, -1.7734, -1.7734]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the reflection of the object in this image? Please output segmentation mask. ASSISTANT: the reflection of the object</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2820, 3760])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2820, 3760])):  [tensor([[[-9.5000, -9.5000, -9.5000,  ..., -9.7500, -9.7500, -9.7500],
         [-9.5000, -9.5000, -9.5000,  ..., -9.7500, -9.7500, -9.7500],
         [-9.5000, -9.5000, -9.5000,  ..., -9.7500, -9.7500, -9.7500],
         ...,
         [-5.2390, -5.2390, -5.2390,  ..., -5.7716, -5.7716, -5.7716],
         [-5.3789, -5.3789, -5.3789,  ..., -5.8945, -5.8945, -5.8945],
         [-5.3789, -5.3789, -5.3789,  ..., -5.8945, -5.8945, -5.8945]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Dogs use their mouths to perform various tasks, including eating and vocalizing. What part of the dog's body is primarily responsible for these actions? Please output segmentation mask. ASSISTANT: dogs use their mouths to perform various tasks, including eating and vocalizing. what part of the dog's body is primarily responsible for these actions</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3264, 4928])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3264, 4928])):  [tensor([[[-3.4375, -3.4375, -3.4375,  ..., -5.1250, -5.1250, -5.1250],
         [-3.4375, -3.4375, -3.4375,  ..., -5.1250, -5.1250, -5.1250],
         [-3.4375, -3.4375, -3.4375,  ..., -5.1250, -5.1250, -5.1250],
         ...,
         [-2.8825, -2.8825, -2.8825,  ..., -2.9811, -2.9811, -2.9811],
         [-2.8828, -2.8828, -2.8828,  ..., -2.9824, -2.9824, -2.9824],
         [-2.8828, -2.8828, -2.8828,  ..., -2.9824, -2.9824, -2.9824]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that protects the persons' heads in this image? Please output segmentation mask. ASSISTANT: something that protects the persons' heads</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1360, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1360, 2048])):  [tensor([[[-8.0625, -8.0625, -8.0625,  ..., -4.4688, -4.4688, -4.4688],
         [-8.0625, -8.0625, -8.0625,  ..., -4.4688, -4.4688, -4.4688],
         [-8.0625, -8.0625, -8.0625,  ..., -4.4688, -4.4688, -4.4688],
         ...,
         [-7.2051, -7.2051, -7.2051,  ..., -5.8164, -5.8164, -5.8164],
         [-7.5293, -7.5293, -7.5293,  ..., -5.9648, -5.9648, -5.9648],
         [-7.6914, -7.6914, -7.6914,  ..., -6.0391, -6.0391, -6.0391]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the transportation vehicle that does not require electricity or gasoline in this image? Please output segmentation mask. ASSISTANT: the transportation vehicle that does not require electricity or gasoline</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3456, 5184])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 5184])):  [tensor([[[-5.3125, -5.3125, -5.3125,  ..., -7.1875, -7.1875, -7.1875],
         [-5.3125, -5.3125, -5.3125,  ..., -7.1875, -7.1875, -7.1875],
         [-5.3125, -5.3125, -5.3125,  ..., -7.1875, -7.1875, -7.1875],
         ...,
         [-5.5430, -5.5430, -5.5430,  ..., -3.0469, -3.0469, -3.0469],
         [-5.5430, -5.5430, -5.5430,  ..., -3.0469, -3.0469, -3.0469],
         [-5.5430, -5.5430, -5.5430,  ..., -3.0469, -3.0469, -3.0469]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When constructing a house, what part of the building process involves assembling a sturdy base and framework? Please output segmentation mask. ASSISTANT: when constructing a house, what part of the building process involves assembling a sturdy base and framework</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[74]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.],], device='cuda:0')] -4.8203]]],0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1620, 2160])):  [tensor([[[-7.2812, -7.2812, -7.2812,  ..., -7.0312, -7.0312, -7.0312],
         [-7.2812, -7.2812, -7.2812,  ..., -7.0312, -7.0312, -7.0312],
         [-7.2812, -7.2812, -7.2812,  ..., -7.0312, -7.0312, -7.0312],
         ...,
         [-5.6343, -5.6343, -5.6343,  ..., -4.6887, -4.6887, -4.6887],
         [-5.7306, -5.7306, -5.7306,  ..., -4.7257, -4.7257, -4.7257],
         [-5.7734, -5.7734, -5.7734,  ..., -4.7422, -4.7422, -4.7422]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the part of the house that can be opened in this image? Please output segmentation mask. ASSISTANT: the part of the house that can be opened</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[-7.0000, -7.0000, -7.0000,  ..., -7.0938, -7.0938, -7.0938],
         [-7.0000, -7.0000, -7.0000,  ..., -7.0938, -7.0938, -7.0938],
         [-7.0000, -7.0000, -7.0000,  ..., -7.0938, -7.0938, -7.0938],
         ...,
         [-3.4744, -3.4744, -3.4744,  ..., -5.2154, -5.2154, -5.2154],
         [-3.6627, -3.6627, -3.6627,  ..., -5.3105, -5.3105, -5.3105],
         [-3.7363, -3.7363, -3.7363,  ..., -5.3477, -5.3477, -5.3477]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the source of power for the ship in this image? Please output segmentation mask. ASSISTANT: the source of power for the ship</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[-12.3750, -12.3750, -12.3750,  ..., -10.1875, -10.1875, -10.1875],
         [-12.3750, -12.3750, -12.3750,  ..., -10.1875, -10.1875, -10.1875],
         [-12.3750, -12.3750, -12.3750,  ..., -10.1875, -10.1875, -10.1875],
         ...,
         [ -9.8867,  -9.8867,  -9.8867,  ...,  -7.2598,  -7.2598,  -7.2598],
         [-10.1445, -10.1445, -10.1445,  ...,  -7.5371,  -7.5371,  -7.5371],
         [-10.2734, -10.2734, -10.2734,  ...,  -7.6758,  -7.6758,  -7.6758]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When hanging laundry outside to dry, what material in the picture would be suitable for holding the clothes securely in place? Please output segmentation mask. ASSISTANT: when hanging laundry outside to dry, what material in the picture would be suitable for holding the clothes securely in place</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2592, 3872])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2592, 3872])):  [tensor([[[-9.1250, -9.1250, -9.1250,  ..., -8.5625, -8.5625, -8.5625],
         [-9.1250, -9.1250, -9.1250,  ..., -8.5625, -8.5625, -8.5625],
         [-9.1250, -9.1250, -9.1250,  ..., -8.5625, -8.5625, -8.5625],
         ...,
         [-6.7599, -6.7599, -6.7599,  ..., -6.4316, -6.4316, -6.4316],
         [-6.8164, -6.8164, -6.8164,  ..., -6.4141, -6.4141, -6.4141],
         [-6.8164, -6.8164, -6.8164,  ..., -6.4141, -6.4141, -6.4141]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the friend of humans in this image? Please output segmentation mask. ASSISTANT: the friend of humans</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-11.2500, -11.2500, -11.2500,  ..., -12.6094, -12.6250, -12.6250],
         [-11.2500, -11.2500, -11.2500,  ..., -12.6094, -12.6250, -12.6250],
         [-11.3828, -11.3828, -11.4180,  ..., -12.7607, -12.7891, -12.7891],
         ...,
         [ -7.5859,  -7.5859,  -7.9570,  ...,  -9.5459,  -9.4688,  -9.4688],
         [ -7.6562,  -7.6562,  -7.9814,  ...,  -9.3018,  -9.2344,  -9.2344],
         [ -7.1562,  -7.1562,  -7.3896,  ...,  -9.1709,  -9.0781,  -9.0781]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Dogs have keen sense of smell, which is why they can be used as drug-sniffing dogs. Which part in the picture gives dogs this characteristic? Please output segmentation mask. ASSISTANT: dogs have keen sense of smell, which is why they can be used as drug-sniffing dogs. which part in the picture gives dogs this characteristic</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-6.3750, -6.3750, -6.3789,  ..., -6.2852, -6.2812, -6.2812],
         [-6.3750, -6.3750, -6.3789,  ..., -6.2852, -6.2812, -6.2812],
         [-6.3633, -6.3633, -6.3809,  ..., -6.3516, -6.3555, -6.3555],
         ...,
         [-4.5391, -4.5391, -4.6641,  ..., -4.8374, -4.8594, -4.8594],
         [-4.6211, -4.6211, -4.7339,  ..., -4.8594, -4.8828, -4.8828],
         [-4.6758, -4.6758, -4.7563,  ..., -4.8359, -4.8359, -4.8359]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the place where the patient lies down to receive examination in this image? Please output segmentation mask. ASSISTANT: the place where the patient lies down to receive examination</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2428, 3636])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2428, 3636])):  [tensor([[[-11.2500, -11.2500, -11.2500,  ..., -10.7500, -10.7500, -10.7500],
         [-11.2500, -11.2500, -11.2500,  ..., -10.7500, -10.7500, -10.7500],
         [-11.2500, -11.2500, -11.2500,  ..., -10.7500, -10.7500, -10.7500],
         ...,
         [ -5.1466,  -5.1466,  -5.1466,  ...,  -2.9916,  -2.9916,  -2.9916],
         [ -5.1992,  -5.1992,  -5.1992,  ...,  -3.0938,  -3.0938,  -3.0938],
         [ -5.1992,  -5.1992,  -5.1992,  ...,  -3.0938,  -3.0938,  -3.0938]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In an intense dragon boat race. What object in the picture should be struck to boost the morale of the competing team and cheer them on? Please output segmentation mask. ASSISTANT: in an intense dragon boat race. what object in the picture should be struck to boost the morale of the competing team and cheer them on</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[83]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 986, 1544])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 986, 1544])):  [tensor([[[-8.1250, -8.1250, -8.1250,  ..., -7.1049, -7.0938, -7.0938],
         [-8.1250, -8.1250, -8.1250,  ..., -7.1049, -7.0937, -7.0937],
         [-8.1225, -8.1225, -8.1230,  ..., -7.1187, -7.1080, -7.1080],
         ...,
         [-5.2447, -5.2447, -5.2678,  ..., -5.1835, -5.1791, -5.1791],
         [-5.2343, -5.2343, -5.2573,  ..., -5.1354, -5.1325, -5.1325],
         [-5.2266, -5.2266, -5.2495,  ..., -5.0994, -5.0977, -5.0977]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the equipment for sweeping away rain on rainy days in this image? Please output segmentation mask. ASSISTANT: the equipment for sweeping away rain on rainy days</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[-1.9688, -1.9688, -1.9688,  ..., -2.1094, -2.1094, -2.1094],
         [-1.9688, -1.9688, -1.9688,  ..., -2.1094, -2.1094, -2.1094],
         [-1.9688, -1.9688, -1.9688,  ..., -2.1094, -2.1094, -2.1094],
         ...,
         [-2.2351, -2.2351, -2.2351,  ..., -1.7680, -1.7680, -1.7680],
         [-2.2598, -2.2598, -2.2598,  ..., -1.9061, -1.9061, -1.9061],
         [-2.2656, -2.2656, -2.2656,  ..., -1.9385, -1.9385, -1.9385]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Soldiers are often equipped with various tools and weapons to carry out their duties. What item in the picture can be classified as a weapon? Please output segmentation mask. ASSISTANT: soldiers are often equipped with various tools and weapons to carry out their duties. what item in the picture can be classified as a weapon</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 300, 400])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 300, 400])):  [tensor([[[-8.5625, -8.2463, -7.9398,  ..., -5.7098, -5.8825, -5.9688],
         [-8.5913, -8.6850, -8.7323,  ..., -5.7313, -5.9058, -6.0119],
         [-8.4168, -8.9144, -9.3248,  ..., -5.6340, -5.7883, -5.8961],
         ...,
         [-6.8668, -7.3554, -7.7546,  ..., -7.3719, -6.9462, -6.5699],
         [-6.8725, -7.4268, -7.8546,  ..., -6.7677, -6.4232, -6.1794],
         [-6.8606, -7.3885, -7.7875,  ..., -6.4636, -6.2389, -6.1256]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What object is used to cover the entrance of the bathroom and ensure privacy? Please output segmentation mask. ASSISTANT: what object is used to cover the entrance of the bathroom and ensure privacy</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[ -9.4448,  -9.8438,  -9.2555,  ...,  -5.6328,  -5.9828,  -6.5851],
         [ -9.4571, -10.5356,  -9.5562,  ...,  -6.0625,  -6.0666,  -6.7132],
         [ -8.8590,  -9.6969,  -9.2969,  ...,  -6.2266,  -6.3453,  -6.5279],
         ...,
         [ -5.5410,  -6.0781,  -6.3281,  ...,  -8.5156,  -8.2812,  -8.2812],
         [ -4.8634,  -5.2544,  -5.8969,  ...,  -8.1328,  -8.0025,  -8.1013],
         [ -5.3291,  -5.8351,  -6.2502,  ...,  -7.9012,  -7.4577,  -7.5522]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the person in the air in this image? Please output segmentation mask. ASSISTANT: the person in the air</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3034, 2148])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3034, 2148])):  [tensor([[[-14.6875, -14.6875, -14.6875,  ...,  -9.0904,  -8.9480,  -8.9453],
         [-14.6875, -14.6875, -14.6875,  ...,  -9.0904,  -8.9480,  -8.9453],
         [-14.6875, -14.6875, -14.6875,  ...,  -9.0904,  -8.9480,  -8.9453],
         ...,
         [ -7.3125,  -7.3125,  -7.3125,  ...,  -6.1977,  -6.2874,  -6.2891],
         [ -7.3125,  -7.3125,  -7.3125,  ...,  -6.1977,  -6.2874,  -6.2891],
         [ -7.3125,  -7.3125,  -7.3125,  ...,  -6.1977,  -6.2874,  -6.2891]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is a place where bees can suck nectar from flowers in this image? Please output segmentation mask. ASSISTANT: a place where bees can suck nectar from flowers</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 566, 939])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 566, 939])):  [tensor([[[-4.8438, -4.8427, -4.8324,  ..., -4.5794, -4.5074, -4.5000],
         [-4.8353, -4.8345, -4.8264,  ..., -4.5746, -4.5031, -4.4958],
         [-4.7531, -4.7544, -4.7675,  ..., -4.5279, -4.4615, -4.4547],
         ...,
         [-3.7072, -3.7253, -3.9004,  ..., -2.3137, -2.3066, -2.3058],
         [-3.6921, -3.7073, -3.8546,  ..., -2.2585, -2.2588, -2.2588],
         [-3.6708, -3.6825, -3.7954,  ..., -2.1939, -2.2026, -2.2035]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n I want to take a trip around the world, but I need some transportation to help me cross the oceans. What type of transportation in the picture can fulfill this requirement? Please output segmentation mask. ASSISTANT: i want to take a trip around the world, but i need some transportation to help me cross the oceans. what type of transportation in the picture can fulfill this requirement</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[90]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')] -4.8203]]],0., 0., 0.,  ..., 0., 0., 0.],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[-6.9062, -6.9062, -6.9062,  ..., -7.6875, -7.6875, -7.6875],
         [-6.9062, -6.9062, -6.9062,  ..., -7.6875, -7.6875, -7.6875],
         [-6.9062, -6.9062, -6.9062,  ..., -7.6875, -7.6875, -7.6875],
         ...,
         [-1.8472, -1.8472, -1.8472,  ..., -4.5703, -4.5703, -4.5703],
         [-2.3130, -2.3130, -2.3130,  ..., -4.6172, -4.6172, -4.6172],
         [-2.5459, -2.5459, -2.5459,  ..., -4.6406, -4.6406, -4.6406]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When a photographer wants to adjust the focus of a camera lens, what part in the picture could they manipulate to achieve this? Please output segmentation mask. ASSISTANT: when a photographer wants to adjust the focus of a camera lens, what part in the picture could they manipulate to achieve this</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1584, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1584, 1600])):  [tensor([[[-4.6875, -4.6875, -4.6836,  ..., -3.4250, -3.4219, -3.4219],
         [-4.6875, -4.6875, -4.6836,  ..., -3.4250, -3.4219, -3.4219],
         [-4.6816, -4.6816, -4.6779,  ..., -3.4272, -3.4242, -3.4242],
         ...,
         [-3.0625, -3.0625, -3.0674,  ..., -3.6562, -3.6594, -3.6594],
         [-3.0000, -3.0000, -3.0060,  ..., -3.6734, -3.6769, -3.6769],
         [-2.9551, -2.9551, -2.9618,  ..., -3.6858, -3.6895, -3.6895]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is unopened flower bud in this image? Please output segmentation mask. ASSISTANT: unopened flower bud</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 566, 850])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 566, 850])):  [tensor([[[-10.8125, -10.8197, -10.8599,  ..., -11.2199, -11.1394, -11.1250],
         [-10.8389, -10.8489, -10.9046,  ..., -11.2643, -11.1828, -11.1682],
         [-10.9865, -11.0119, -11.1540,  ..., -11.5126, -11.4253, -11.4097],
         ...,
         [ -8.3411,  -8.4100,  -8.7952,  ...,  -8.0487,  -8.0179,  -8.0124],
         [ -7.8421,  -7.8868,  -8.1368,  ...,  -7.7277,  -7.5939,  -7.5699],
         [ -7.3432,  -7.3637,  -7.4783,  ...,  -7.4066,  -7.1698,  -7.1275]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n This image depicts a forest. Which of the animals in the picture pose a threat to human safety? Please output segmentation mask. ASSISTANT: this image depicts a forest. which of the animals in the picture pose a threat to human safety</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[73]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 300, 400])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 300, 400])):  [tensor([[[-7.4062, -7.4925, -7.4438,  ..., -7.7805, -7.9956, -8.1250],
         [-7.4062, -7.6710, -7.7720,  ..., -7.9373, -7.9853, -7.9956],
         [-7.2516, -7.6984, -7.9737,  ..., -8.0480, -7.9191, -7.7945],
         ...,
         [-5.5164, -6.0790, -6.5491,  ..., -6.0089, -5.6727, -5.4195],
         [-5.6012, -6.1331, -6.5708,  ..., -5.5462, -5.2973, -5.1650],
         [-5.9012, -6.3348, -6.6964,  ..., -5.3338, -5.1596, -5.0975]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a graduation ceremony, it is a tradition for the graduates to wear a specific type of clothing to signify their achievement. What item of clothing can be seen in the picture that is commonly worn by graduates? Please output segmentation mask. ASSISTANT: in a graduation ceremony, it is a tradition for the graduates to wear a specific type of clothing to signify their achievement. what item of clothing can be seen in the picture that is commonly worn by graduates</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[99]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3456, 2304])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 2304])):  [tensor([[[-8.5000, -8.5000, -8.5000,  ..., -6.8785, -6.8672, -6.8672],
         [-8.5000, -8.5000, -8.5000,  ..., -6.8785, -6.8672, -6.8672],
         [-8.5000, -8.5000, -8.5000,  ..., -6.8785, -6.8672, -6.8672],
         ...,
         [-6.0000, -6.0000, -6.0000,  ..., -3.5724, -3.7227, -3.7227],
         [-6.0000, -6.0000, -6.0000,  ..., -3.5724, -3.7227, -3.7227],
         [-6.0000, -6.0000, -6.0000,  ..., -3.5724, -3.7227, -3.7227]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When visiting a library or bookstore, people often browse through the shelves to find interesting books to read. Which area in the picture could provide a variety of reading materials for visitors? Please output segmentation mask. ASSISTANT: when visiting a library or bookstore, people often browse through the shelves to find interesting books to read. which area in the picture could provide a variety of reading materials for visitors</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[91]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 4000, 2683])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 4000, 2683])):  [tensor([[[-6.9375, -6.9375, -6.9375,  ..., -5.2329, -5.2695, -5.2695],
         [-6.9375, -6.9375, -6.9375,  ..., -5.2329, -5.2695, -5.2695],
         [-6.9375, -6.9375, -6.9375,  ..., -5.2329, -5.2695, -5.2695],
         ...,
         [-3.2344, -3.2344, -3.2344,  ..., -3.0629, -3.0820, -3.0820],
         [-3.2344, -3.2344, -3.2344,  ..., -3.0629, -3.0820, -3.0820],
         [-3.2344, -3.2344, -3.2344,  ..., -3.0629, -3.0820, -3.0820]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the person who appears to have already won in the battle in this image? Please output segmentation mask. ASSISTANT: the person who appears to have already won in the battle</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1504, 2256])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1504, 2256])):  [tensor([[[-6.8125, -6.8125, -6.8125,  ..., -7.3125, -7.3125, -7.3125],
         [-6.8125, -6.8125, -6.8125,  ..., -7.3125, -7.3125, -7.3125],
         [-6.8125, -6.8125, -6.8125,  ..., -7.3125, -7.3125, -7.3125],
         ...,
         [-5.0197, -5.0197, -5.0197,  ..., -4.5096, -4.5096, -4.5096],
         [-5.0782, -5.0782, -5.0782,  ..., -4.4971, -4.4971, -4.4971],
         [-5.1016, -5.1016, -5.1016,  ..., -4.4922, -4.4922, -4.4922]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a healthy meal, vegetables are often included to provide essential nutrients. What in the picture can be used to eat the vegetables? Please output segmentation mask. ASSISTANT: in a healthy meal, vegetables are often included to provide essential nutrients. what in the picture can be used to eat the vegetables</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[83]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 854, 1280])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 854, 1280])):  [tensor([[[-8.1250, -8.1250, -8.1133,  ..., -6.6309, -6.6250, -6.6250],
         [-8.1250, -8.1250, -8.1133,  ..., -6.6309, -6.6250, -6.6250],
         [-8.1328, -8.1328, -8.1289,  ..., -6.6607, -6.6582, -6.6582],
         ...,
         [-4.5701, -4.5701, -4.6248,  ..., -4.4946, -4.5036, -4.5036],
         [-4.4809, -4.4809, -4.5208,  ..., -4.2475, -4.2453, -4.2453],
         [-4.6094, -4.6094, -4.6399,  ..., -4.2579, -4.2480, -4.2480]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that prevents people from getting into the building in this image? Please output segmentation mask. ASSISTANT: something that prevents people from getting into the building</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         ...,
         [255., 255., 255.,  ..., 255., 255., 255.],
         [255., 255., 255.,  ..., 255., 255., 255.],
         [255., 255., 255.,  ..., 255., 255., 255.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-5.4062, -5.3980, -5.3594,  ..., -5.4219, -5.3832, -5.3750],
         [-5.4112, -5.4061, -5.3820,  ..., -5.4619, -5.4240, -5.4160],
         [-5.4344, -5.4439, -5.4887,  ..., -5.6506, -5.6166, -5.6094],
         ...,
         [ 0.3001,  0.2955,  0.2740,  ...,  0.1814,  0.1712,  0.1691],
         [ 0.0538,  0.0525,  0.0466,  ...,  0.0621,  0.0797,  0.0834],
         [-0.3401, -0.3411, -0.3456,  ..., -0.4029, -0.3522, -0.3415]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that the monkey uses to pierce its food in this image? Please output segmentation mask. ASSISTANT: something that the monkey uses to pierce its food</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1600, 1066])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1600, 1066])):  [tensor([[[-5.5000, -5.5000, -5.5004,  ..., -4.8561, -4.7262, -4.6328],
         [-5.5000, -5.5000, -5.5004,  ..., -4.8561, -4.7262, -4.6328],
         [-5.5047, -5.5047, -5.5052,  ..., -4.8629, -4.7304, -4.6352],
         ...,
         [-2.9543, -2.9543, -2.9506,  ..., -4.0628, -3.9049, -3.7914],
         [-2.9531, -2.9531, -2.9494,  ..., -4.0561, -3.8962, -3.7812],
         [-2.9531, -2.9531, -2.9494,  ..., -4.0561, -3.8962, -3.7812]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that avoids falling down in this image? Please output segmentation mask. ASSISTANT: something that avoids falling down</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1224, 1632])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1224, 1632])):  [tensor([[[-4.3125, -4.3125, -4.3128,  ..., -5.6878, -5.6875, -5.6875],
         [-4.3125, -4.3125, -4.3128,  ..., -5.6878, -5.6875, -5.6875],
         [-4.3089, -4.3089, -4.3092,  ..., -5.6907, -5.6904, -5.6904],
         ...,
         [-2.4301, -2.4301, -2.4301,  ..., -1.2872, -1.2855, -1.2855],
         [-2.6020, -2.6020, -2.6017,  ..., -1.5392, -1.5363, -1.5363],
         [-2.7227, -2.7227, -2.7221,  ..., -1.7245, -1.7207, -1.7207]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the container that can be used to hold soup currently in this image? Please output segmentation mask. ASSISTANT: the container that can be used to hold soup currently</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[67]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-5.4375, -5.4211, -5.3438,  ..., -5.4594, -5.6991, -5.7500],
         [-5.4490, -5.4358, -5.3734,  ..., -5.4717, -5.7053, -5.7549],
         [-5.5031, -5.5049, -5.5134,  ..., -5.5297, -5.7346, -5.7781],
         ...,
         [-4.1891, -4.2292, -4.4183,  ..., -6.3853, -6.2917, -6.2719],
         [-4.6254, -4.6606, -4.8266,  ..., -6.2568, -6.2364, -6.2320],
         [-5.2537, -5.2871, -5.4442,  ..., -6.1350, -6.0716, -6.0581]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When you look closely into a person's eye, what is the part that surrounds the dark-colored iris and helps protect the eye from dust and debris? Please output segmentation mask. ASSISTANT: when you look closely into a person's eye, what is the part that surrounds the dark-colored iris and helps protect the eye from dust and debris</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]

         ..., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')] -4.8203]]],0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 280, 330])):  [tensor([[[-8.5005, -8.6703, -8.5596,  ..., -7.7287, -7.8858, -7.6635],
         [-8.3828, -9.1197, -8.7486,  ..., -8.0667, -8.0313, -8.1735],
         [-7.8238, -8.4352, -8.3886,  ..., -7.6046, -7.6097, -7.6674],
         ...,
         [-1.7236, -2.1055, -3.2745,  ..., -5.9349, -6.0345, -6.0975],
         [-1.9237, -2.5009, -3.0066,  ..., -4.2095, -4.2563, -4.2400],
         [-2.8435, -3.4281, -3.4226,  ..., -3.4764, -3.5980, -3.4468]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What object in a typical modern kitchen can be used to quickly heat up leftovers, boil water, or cook frozen food? Please output segmentation mask. ASSISTANT: what object in a typical modern kitchen can be used to quickly heat up leftovers, boil water, or cook frozen food</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[80]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-14.3732, -14.2320, -13.6152,  ...,  -7.4123,  -7.7213,  -7.8792],
         [-14.4468, -15.9894, -14.7969,  ...,  -7.7453,  -7.6869,  -8.2584],
         [-13.7738, -15.0844, -14.3750,  ...,  -7.1797,  -7.1172,  -7.4932],
         ...,
         [ -9.9359, -11.5688, -10.9531,  ...,  -8.4766,  -8.7188,  -9.1484],
         [ -8.6742, -10.1759,  -9.9562,  ...,  -7.7781,  -7.6934,  -8.0007],
         [ -8.0702,  -9.3115,  -8.8934,  ...,  -8.0277,  -7.8941,  -8.1339]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When plants grow excessively in unwanted areas, it is necessary to remove them to maintain the aesthetics of the surroundings. What part of the plants in the picture may need to be removed in this situation? Please output segmentation mask. ASSISTANT: when plants grow excessively in unwanted areas, it is necessary to remove them to maintain the aesthetics of the surroundings. what part of the plants in the picture may need to be removed in this situation</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[97]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 853, 1280])):  [tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 853, 1280])):  [tensor([[[-0.8359, -0.8359, -0.8198,  ..., -2.4092, -2.4219, -2.4219],
         [-0.8359, -0.8359, -0.8198,  ..., -2.4092, -2.4219, -2.4219],
         [-0.8221, -0.8221, -0.8069,  ..., -2.4010, -2.4121, -2.4121],
         ...,
         [-1.4963, -1.4963, -1.5055,  ..., -1.0171, -1.0099, -1.0099],
         [-1.6165, -1.6165, -1.6254,  ..., -1.1161, -1.1091, -1.1091],
         [-1.7217, -1.7217, -1.7302,  ..., -1.2027, -1.1958, -1.1958]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a television studio, various equipment is used to capture and record video footage. What in the picture could be used to stabilize and hold the camera steady during filming? Please output segmentation mask. ASSISTANT: in a television studio, various equipment is used to capture and record video footage. what in the picture could be used to stabilize and hold the camera steady during filming</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2573, 3593])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2573, 3593])):  [tensor([[[-8.3750, -8.3750, -8.3750,  ..., -7.4062, -7.4062, -7.4062],
         [-8.3750, -8.3750, -8.3750,  ..., -7.4062, -7.4062, -7.4062],
         [-8.3750, -8.3750, -8.3750,  ..., -7.4062, -7.4062, -7.4062],
         ...,
         [-6.3258, -6.3258, -6.3258,  ..., -6.1432, -6.1432, -6.1432],
         [-6.3125, -6.3125, -6.3125,  ..., -6.1797, -6.1797, -6.1797],
         [-6.3125, -6.3125, -6.3125,  ..., -6.1797, -6.1797, -6.1797]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n There are two washing machines as shown in the picture. If I need to do laundry, where in the picture would I put the clothes? Please output segmentation mask. ASSISTANT: there are two washing machines as shown in the picture. if i need to do laundry, where in the picture would i put the clothes</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[82]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3254, 2168])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3254, 2168])):  [tensor([[[-6.8125, -6.8125, -6.8125,  ..., -6.6413, -6.5898, -6.5898],
         [-6.8125, -6.8125, -6.8125,  ..., -6.6413, -6.5898, -6.5898],
         [-6.8125, -6.8125, -6.8125,  ..., -6.6413, -6.5898, -6.5898],
         ...,
         [-1.3828, -1.3828, -1.3828,  ..., -4.1827, -4.0820, -4.0820],
         [-1.3828, -1.3828, -1.3828,  ..., -4.1827, -4.0820, -4.0820],
         [-1.3828, -1.3828, -1.3828,  ..., -4.1827, -4.0820, -4.0820]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Insects are often found on or near trees, where they can find shelter and food. What part of the tree in this picture could insects commonly be found on or around? Please output segmentation mask. ASSISTANT: insects are often found on or near trees, where they can find shelter and food. what part of the tree in this picture could insects commonly be found on or around</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1728, 2304])):  [tensor([[[1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.],
         ...,
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1728, 2304])):  [tensor([[[-5.8125, -5.8125, -5.8125,  ..., -6.5625, -6.5625, -6.5625],
         [-5.8125, -5.8125, -5.8125,  ..., -6.5625, -6.5625, -6.5625],
         [-5.8125, -5.8125, -5.8125,  ..., -6.5625, -6.5625, -6.5625],
         ...,
         [-4.7986, -4.7986, -4.7986,  ..., -5.5347, -5.5347, -5.5347],
         [-4.9167, -4.9167, -4.9167,  ..., -5.4583, -5.4583, -5.4583],
         [-4.9609, -4.9609, -4.9609,  ..., -5.4297, -5.4297, -5.4297]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If the person in the picture is the owner of the blue car and wants to drive it, which car door is least likely to be the one he enters based on the picture shown? Please output segmentation mask. ASSISTANT: if the person in the picture is the owner of the blue car and wants to drive it, which car door is least likely to be the one he enters based on the picture shown</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[-6.9375, -6.9375, -6.9375,  ..., -8.0000, -8.0000, -8.0000],
         [-6.9375, -6.9375, -6.9375,  ..., -8.0000, -8.0000, -8.0000],
         [-6.9375, -6.9375, -6.9375,  ..., -8.0000, -8.0000, -8.0000],
         ...,
         [-5.3926, -5.3926, -5.3926,  ..., -4.1250, -4.1250, -4.1250],
         [-5.3418, -5.3418, -5.3418,  ..., -4.0625, -4.0625, -4.0625],
         [-5.3164, -5.3164, -5.3164,  ..., -4.0312, -4.0312, -4.0312]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the keys on a piano used to play notes of half-steps or semitones in this image? Please output segmentation mask. ASSISTANT: the keys on a piano used to play notes of half-steps or semitones</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[74]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1941, 2919])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 1., 1.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1941, 2919])):  [tensor([[[-3.8750, -3.8750, -3.8750,  ..., -2.4688, -2.4688, -2.4688],
         [-3.8750, -3.8750, -3.8750,  ..., -2.4688, -2.4688, -2.4688],
         [-3.8750, -3.8750, -3.8750,  ..., -2.4688, -2.4688, -2.4688],
         ...,
         [-5.9274, -5.9274, -5.9274,  ..., -4.6250, -4.6250, -4.6250],
         [-5.9877, -5.9877, -5.9877,  ..., -4.6250, -4.6250, -4.6250],
         [-5.9922, -5.9922, -5.9922,  ..., -4.6250, -4.6250, -4.6250]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n The general steps for brewing tea are to put tea leaves into a teacup and then pour hot water over them. What container in the picture is most likely to be used next for pouring hot water to make tea? Please output segmentation mask. ASSISTANT: the general steps for brewing tea are to put tea leaves into a teacup and then pour hot water over them. what container in the picture is most likely to be used next for pouring hot water to make tea</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[97]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3056, 4592])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3056, 4592])):  [tensor([[[-10.5000, -10.5000, -10.5000,  ...,  -9.9375,  -9.9375,  -9.9375],
         [-10.5000, -10.5000, -10.5000,  ...,  -9.9375,  -9.9375,  -9.9375],
         [-10.5000, -10.5000, -10.5000,  ...,  -9.9375,  -9.9375,  -9.9375],
         ...,
         [ -7.7171,  -7.7171,  -7.7171,  ...,  -6.9661,  -6.9661,  -6.9661],
         [ -7.7148,  -7.7148,  -7.7148,  ...,  -6.9375,  -6.9375,  -6.9375],
         [ -7.7148,  -7.7148,  -7.7148,  ...,  -6.9375,  -6.9375,  -6.9375]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that is noticeably different from the other plants in the picture in this image? Please output segmentation mask. ASSISTANT: something that is noticeably different from the other plants in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3456, 5184])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 5184])):  [tensor([[[-5.5312, -5.5312, -5.5312,  ..., -5.8750, -5.8750, -5.8750],
         [-5.5312, -5.5312, -5.5312,  ..., -5.8750, -5.8750, -5.8750],
         [-5.5312, -5.5312, -5.5312,  ..., -5.8750, -5.8750, -5.8750],
         ...,
         [-2.0938, -2.0938, -2.0938,  ..., -1.4736, -1.4736, -1.4736],
         [-2.0938, -2.0938, -2.0938,  ..., -1.4736, -1.4736, -1.4736],
         [-2.0938, -2.0938, -2.0938,  ..., -1.4736, -1.4736, -1.4736]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When the weather is very cold outside, what equipment do we need to use to increase the indoor temperature in the room? Please output segmentation mask. ASSISTANT: when the weather is very cold outside, what equipment do we need to use to increase the indoor temperature in the room</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-4.3302, -4.0983, -3.7181,  ..., -3.4444, -3.5388, -3.6390],
         [-3.8297, -4.4323, -4.1102,  ..., -3.3961, -3.3541, -3.6463],
         [-3.3993, -3.8773, -3.8281,  ..., -3.2305, -3.2156, -3.2801],
         ...,
         [-3.3182, -3.2000, -2.2471,  ..., -3.0625, -3.0938, -3.0938],
         [-2.9802, -2.7492, -2.3742,  ..., -3.0914, -3.0678, -3.1946],
         [-3.3920, -3.2634, -2.9111,  ..., -3.6448, -3.5838, -3.8301]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If we wanted to identify the specific vehicle shown in the picture, what part of the car should we look at? Please output segmentation mask. ASSISTANT: if we wanted to identify the specific vehicle shown in the picture, what part of the car should we look at</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[74]]
>> (validate) sampled_classes_list:  [None]

         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0.,  ..., 0., 0., 0.],
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-6.8462, -6.3786, -5.8494,  ..., -6.9604, -7.2215, -7.1240],
         [-5.7556, -6.7503, -6.1609,  ..., -6.4984, -6.5147, -6.9637],
         [-5.3800, -5.6539, -5.0000,  ..., -5.9531, -6.0328, -6.1080],
         ...,
         [-6.6230, -7.3750, -7.6328,  ..., -6.9688, -6.5859, -5.5654],
         [-6.2486, -6.8566, -7.3688,  ..., -6.6766, -6.5928, -5.7313],
         [-6.6375, -7.4955, -7.4801,  ..., -6.8459, -6.6535, -6.4636]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the places where the driver can observe the speed in this image? Please output segmentation mask. ASSISTANT: the places where the driver can observe the speed</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 944, 1417])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 944, 1417])):  [tensor([[[-6.4062, -6.4062, -6.3967,  ..., -6.6178, -6.6250, -6.6250],
         [-6.4062, -6.4062, -6.3967,  ..., -6.6178, -6.6250, -6.6250],
         [-6.4350, -6.4350, -6.4275,  ..., -6.6151, -6.6226, -6.6226],
         ...,
         [-4.4113, -4.4113, -4.4299,  ..., -6.9467, -6.9211, -6.9211],
         [-4.5468, -4.5468, -4.5608,  ..., -6.3203, -6.2777, -6.2777],
         [-4.6562, -4.6562, -4.6666,  ..., -5.8143, -5.7578, -5.7578]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something indicating that someone is celerating the birthday in this image? Please output segmentation mask. ASSISTANT: something indicating that someone is celerating the birthday</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 651, 383])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 651, 383])):  [tensor([[[-5.5938, -5.5535, -5.4129,  ..., -4.6139, -4.5553, -4.5184],
         [-5.5938, -5.5651, -5.4647,  ..., -4.6872, -4.6144, -4.5538],
         [-5.5938, -5.6053, -5.6459,  ..., -4.9436, -4.8211, -4.6775],
         ...,
         [-3.2107, -3.2325, -3.3090,  ..., -2.2833, -2.5253, -2.8013],
         [-3.2048, -3.2097, -3.2267,  ..., -2.0895, -2.3505, -2.6530],
         [-3.2031, -3.2031, -3.2031,  ..., -2.0341, -2.3005, -2.6106]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that we use to control computer games in this image? Please output segmentation mask. ASSISTANT: something that we use to control computer games</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-10.9375, -10.9145, -10.8062,  ..., -10.5562, -10.6645, -10.6875],
         [-10.9277, -10.9116, -10.8358,  ..., -10.5855, -10.6994, -10.7236],
         [-10.8812, -10.8977, -10.9750,  ..., -10.7231, -10.8639, -10.8938],
         ...,
         [ -6.9312,  -6.9933,  -7.2856,  ...,  -9.1500,  -9.1809,  -9.1875],
         [ -6.9897,  -7.0531,  -7.3521,  ...,  -8.9937,  -9.0433,  -9.0538],
         [ -6.8288,  -6.8776,  -7.1078,  ...,  -8.5784,  -8.5637,  -8.5606]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When serving wine, it is common to pour it into a glass from a container. What object in the picture could be used to pour wine? Please output segmentation mask. ASSISTANT: when serving wine, it is common to pour it into a glass from a container. what object in the picture could be used to pour wine</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[80]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 370, 658])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 370, 658])):  [tensor([[[-10.4375, -10.4114, -10.3193,  ..., -10.6250, -10.6250, -10.6250],
         [-10.4962, -10.4946, -10.4891,  ..., -10.6862, -10.7199, -10.7294],
         [-10.7035, -10.7885, -11.0884,  ..., -10.9024, -11.0548, -11.0980],
         ...,
         [ -8.2272,  -8.4272,  -9.1338,  ...,  -5.5928,  -5.4810,  -5.4493],
         [ -8.4003,  -8.6122,  -9.3606,  ...,  -5.7037,  -5.7139,  -5.7168],
         [ -9.2340,  -9.3819,  -9.9045,  ...,  -7.1900,  -7.2219,  -7.2310]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the container that contains the vegetable in this image? Please output segmentation mask. ASSISTANT: the container that contains the vegetable</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1568, 2352])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1568, 2352])):  [tensor([[[-2.6562, -2.6562, -2.6562,  ..., -4.4375, -4.4375, -4.4375],
         [-2.6562, -2.6562, -2.6562,  ..., -4.4375, -4.4375, -4.4375],
         [-2.6562, -2.6562, -2.6562,  ..., -4.4375, -4.4375, -4.4375],
         ...,
         [-0.6740, -0.6740, -0.6740,  ..., -1.9183, -1.9183, -1.9183],
         [-0.7461, -0.7461, -0.7461,  ..., -1.9072, -1.9072, -1.9072],
         [-0.7715, -0.7715, -0.7715,  ..., -1.9033, -1.9033, -1.9033]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In the animal kingdom, the males of certain species have a distinctive feature that sets them apart from the females. What part of the lion in this picture is a defining characteristic of male lions? Please output segmentation mask. ASSISTANT: in the animal kingdom, the males of certain species have a distinctive feature that sets them apart from the females. what part of the lion in this picture is a defining characteristic of male lions</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[92]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[-6.2188, -6.2040, -6.1344,  ..., -6.1906, -6.2138, -6.2188],
         [-6.1975, -6.1865, -6.1347,  ..., -6.1713, -6.2010, -6.2073],
         [-6.0970, -6.1039, -6.1362,  ..., -6.0801, -6.1404, -6.1532],
         ...,
         [-2.5692, -2.5641, -2.5401,  ..., -4.3596, -4.5087, -4.5404],
         [-3.0790, -3.0724, -3.0410,  ..., -4.2201, -4.2493, -4.2555],
         [-3.5888, -3.5806, -3.5418,  ..., -4.0807, -3.9898, -3.9706]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n During times of war, armored vehicles are commonly used to protect soldiers and engage in combat. What object in the picture can provide such protection? Please output segmentation mask. ASSISTANT: during times of war, armored vehicles are commonly used to protect soldiers and engage in combat. what object in the picture can provide such protection</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[-7.2500, -7.2500, -7.2500,  ..., -6.9062, -6.9062, -6.9062],
         [-7.2500, -7.2500, -7.2500,  ..., -6.9062, -6.9062, -6.9062],
         [-7.2500, -7.2500, -7.2500,  ..., -6.9062, -6.9062, -6.9062],
         ...,
         [-1.9141, -1.9141, -1.9141,  ..., -4.1152, -4.1152, -4.1152],
         [-2.3359, -2.3359, -2.3359,  ..., -4.4004, -4.4004, -4.4004],
         [-2.5469, -2.5469, -2.5469,  ..., -4.5430, -4.5430, -4.5430]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n It is common for some bird species to live near bodies of water and rely on them as their primary habitat. What in the picture can be considered as the habitat for the birds mentioned? Please output segmentation mask. ASSISTANT: it is common for some bird species to live near bodies of water and rely on them as their primary habitat. what in the picture can be considered as the habitat for the birds mentioned</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1193, 1271])):  [tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 0., 0.],
         [1., 1., 1.,  ..., 1., 0., 0.],
         [1., 1., 1.,  ..., 1., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1193, 1271])):  [tensor([[[-12.8750, -12.8750, -12.8067,  ..., -10.4754, -10.3750, -10.3750],
         [-12.8750, -12.8750, -12.8067,  ..., -10.4754, -10.3750, -10.3750],
         [-12.9232, -12.9232, -12.8753,  ..., -10.6464, -10.5516, -10.5516],
         ...,
         [ -9.5892,  -9.5892,  -9.7264,  ..., -11.6084, -11.6090, -11.6090],
         [ -8.9472,  -8.9472,  -9.0193,  ..., -10.2740, -10.2685, -10.2685],
         [ -8.3828,  -8.3828,  -8.3976,  ...,  -9.1006,  -9.0898,  -9.0898]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In historical architecture, a building often has a grand entrance that consists of a large opening with a curved or pointed top. What feature in the picture resembles such an entrance? Please output segmentation mask. ASSISTANT: in historical architecture, a building often has a grand entrance that consists of a large opening with a curved or pointed top. what feature in the picture resembles such an entrance</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 576])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 576])):  [tensor([[[-4.1875, -4.2402, -4.4688,  ..., -3.1927, -3.1436, -3.6250],
         [-4.2773, -4.3322, -4.5697,  ..., -3.2051, -3.1558, -3.6354],
         [-4.6667, -4.7305, -5.0069,  ..., -3.2587, -3.2087, -3.6806],
         ...,
         [-3.4844, -3.6462, -4.3472,  ..., -3.4271, -3.2760, -3.4201],
         [-3.4590, -3.6536, -4.4967,  ..., -3.7233, -3.5500, -3.5697],
         [-3.4531, -3.6553, -4.5312,  ..., -3.7917, -3.6133, -3.6042]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the people who are about to get on the vehicle in this image? Please output segmentation mask. ASSISTANT: the people who are about to get on the vehicle</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[67]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-13.5625, -13.5166, -13.3000,  ...,  -4.8281,  -4.7895,  -4.7812],
         [-13.5428, -13.5077, -13.3423,  ...,  -4.8280,  -4.7881,  -4.7796],
         [-13.4500, -13.4661, -13.5419,  ...,  -4.8272,  -4.7816,  -4.7719],
         ...,
         [ -5.4969,  -5.5359,  -5.7200,  ...,  -3.0292,  -3.0025,  -2.9969],
         [ -5.6439,  -5.6909,  -5.9125,  ...,  -3.1971,  -3.1756,  -3.1711],
         [ -5.9981,  -6.0406,  -6.2409,  ...,  -3.7212,  -3.6424,  -3.6256]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something indicating the identity of the bus in this image? Please output segmentation mask. ASSISTANT: something indicating the identity of the bus</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]

         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,0.,  ..., 0., 0., 0.],
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[67]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-5.4368, -5.5923, -5.4045,  ..., -8.8188, -9.0413, -9.1289],
         [-5.2486, -5.6847, -5.3078,  ..., -9.1625, -9.2519, -9.4280],
         [-4.7539, -4.9688, -4.9062,  ..., -8.6641, -8.8187, -8.6898],
         ...,
         [-3.7981, -4.2977, -4.4219,  ..., -7.7031, -7.4250, -7.0383],
         [-3.9524, -4.5120, -4.7359,  ..., -7.3312, -7.2506, -6.5932],
         [-4.5653, -5.0278, -5.1457,  ..., -6.5166, -6.0024, -6.0008]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When eating scrambled eggs for breakfast, people often add a side dish made of potatoes. What item in the picture can be used to serve the potatoes? Please output segmentation mask. ASSISTANT: when eating scrambled eggs for breakfast, people often add a side dish made of potatoes. what item in the picture can be used to serve the potatoes</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1000, 667])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1000, 667])):  [tensor([[[-9.2500, -9.2514, -9.2937,  ..., -5.6219, -5.5419, -5.7668],
         [-9.2528, -9.2543, -9.2977,  ..., -5.6236, -5.5409, -5.7649],
         [-9.3375, -9.3400, -9.4168,  ..., -5.6755, -5.5115, -5.7091],
         ...,
         [-6.1519, -6.1609, -6.4332,  ..., -7.4426, -6.5687, -6.4330],
         [-6.1561, -6.1652, -6.4404,  ..., -7.2701, -6.3593, -6.2119],
         [-6.1562, -6.1654, -6.4406,  ..., -7.2644, -6.3524, -6.2045]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that can control the fan speed in this image? Please output segmentation mask. ASSISTANT: something that can control the fan speed</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2304, 3456])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2304, 3456])):  [tensor([[[-9.5000, -9.5000, -9.5000,  ..., -8.6875, -8.6875, -8.6875],
         [-9.5000, -9.5000, -9.5000,  ..., -8.6875, -8.6875, -8.6875],
         [-9.5000, -9.5000, -9.5000,  ..., -8.6875, -8.6875, -8.6875],
         ...,
         [-6.0801, -6.0801, -6.0801,  ..., -5.9746, -5.9746, -5.9746],
         [-6.0820, -6.0820, -6.0820,  ..., -5.9219, -5.9219, -5.9219],
         [-6.0820, -6.0820, -6.0820,  ..., -5.9219, -5.9219, -5.9219]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What structure separates two areas in the room and is commonly used to hold onto for support when going up and down? Please output segmentation mask. ASSISTANT: what structure separates two areas in the room and is commonly used to hold onto for support when going up and down</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[75]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[255.,   1.,   1.,  ...,   0.,   0.,   0.],
         [255.,   1.,   1.,  ...,   0.,   0.,   0.],
         [255., 255.,   1.,  ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-4.5353, -4.4784, -4.5227,  ..., -6.0229, -6.3685, -6.3499],
         [-4.8221, -5.1788, -4.9094,  ..., -6.4203, -6.5253, -6.6693],
         [-4.4137, -4.8219, -4.4805,  ..., -6.0625, -6.2078, -6.2186],
         ...,
         [-0.2075, -0.3941, -0.4045,  ..., -4.2852, -4.2820, -4.2659],
         [-0.4356, -0.6292, -0.6204,  ..., -4.1227, -4.0527, -4.1912],
         [-1.3445, -1.4860, -1.5742,  ..., -4.3894, -4.2243, -4.2580]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In urban areas, there are designated areas for bicycles to ride safely. What area in the picture would a cyclist use to navigate through the city? Please output segmentation mask. ASSISTANT: in urban areas, there are designated areas for bicycles to ride safely. what area in the picture would a cyclist use to navigate through the city</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[83]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[-6.9062, -6.9062, -6.9062,  ..., -7.8438, -7.8438, -7.8438],
         [-6.9062, -6.9062, -6.9062,  ..., -7.8438, -7.8438, -7.8438],
         [-6.9062, -6.9062, -6.9062,  ..., -7.8438, -7.8438, -7.8438],
         ...,
         [-4.4024, -4.4024, -4.4024,  ..., -2.8190, -2.8190, -2.8190],
         [-4.5977, -4.5977, -4.5977,  ..., -3.0352, -3.0352, -3.0352],
         [-4.5977, -4.5977, -4.5977,  ..., -3.0352, -3.0352, -3.0352]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a mechanical workshop, there are various machines and tools used for different purposes. What in the picture could be used to rotate or spin other parts or objects? Please output segmentation mask. ASSISTANT: in a mechanical workshop, there are various machines and tools used for different purposes. what in the picture could be used to rotate or spin other parts or objects</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1606, 1610])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1606, 1610])):  [tensor([[[-6.0625, -6.0625, -6.0597,  ..., -5.8146, -5.8125, -5.8125],
         [-6.0625, -6.0625, -6.0597,  ..., -5.8146, -5.8125, -5.8125],
         [-6.0601, -6.0601, -6.0574,  ..., -5.8229, -5.8209, -5.8209],
         ...,
         [-3.2646, -3.2646, -3.2789,  ..., -5.4909, -5.4891, -5.4891],
         [-3.2994, -3.2994, -3.3133,  ..., -5.3994, -5.3997, -5.3997],
         [-3.3242, -3.3242, -3.3378,  ..., -5.3341, -5.3359, -5.3359]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the boxes inside the refrigerator in this image? Please output segmentation mask. ASSISTANT: the boxes inside the refrigerator</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2048, 1536])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2048, 1536])):  [tensor([[[-3.2812, -3.2812, -3.2812,  ..., -5.9922, -5.9453, -5.9219],
         [-3.2812, -3.2812, -3.2812,  ..., -5.9922, -5.9453, -5.9219],
         [-3.2812, -3.2812, -3.2812,  ..., -5.9922, -5.9453, -5.9219],
         ...,
         [-2.3125, -2.3125, -2.3125,  ..., -2.0645, -2.2012, -2.2695],
         [-2.3125, -2.3125, -2.3125,  ..., -2.0645, -2.2012, -2.2695],
         [-2.3125, -2.3125, -2.3125,  ..., -2.0645, -2.2012, -2.2695]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the stronger Mario in this image? Please output segmentation mask. ASSISTANT: the stronger mario</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[60]]
>> (validate) sampled_classes_list:  [None]

         [-8.5055, -8.5025, -8.4784,  ..., -7.3761, -7.1824, -7.1581],, 2, 2, 2,0.,  ..., 0., 0., 0.],
         [-8.5496, -8.5557, -8.6036,  ..., -7.5971, -7.4422, -7.4229],
         ...,
         [-6.4645, -6.5083, -6.8578,  ..., -7.1877, -7.2136, -7.2168],
         [-5.3381, -5.3606, -5.5406,  ..., -5.9145, -5.9312, -5.9333],
         [-4.7447, -4.7554, -4.8403,  ..., -5.2421, -5.2546, -5.2562]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In the picture, there is a legal requirement for vehicles to display identifying information. What part of the car is used to display this information? Please output segmentation mask. ASSISTANT: in the picture, there is a legal requirement for vehicles to display identifying information. what part of the car is used to display this information</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[80]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2762, 3948])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2762, 3948])):  [tensor([[[-9.1875, -9.1875, -9.1875,  ..., -9.5625, -9.5625, -9.5625],
         [-9.1875, -9.1875, -9.1875,  ..., -9.5625, -9.5625, -9.5625],
         [-9.1875, -9.1875, -9.1875,  ..., -9.5625, -9.5625, -9.5625],
         ...,
         [-8.4871, -8.4871, -8.4871,  ..., -7.0535, -7.0535, -7.0535],
         [-8.5508, -8.5508, -8.5508,  ..., -7.1484, -7.1484, -7.1484],
         [-8.5508, -8.5508, -8.5508,  ..., -7.1484, -7.1484, -7.1484]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the ship that is most likely to carry a fleet commander in this image? Please output segmentation mask. ASSISTANT: the ship that is most likely to carry a fleet commander</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2448, 3264])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2448, 3264])):  [tensor([[[-11.6250, -11.6250, -11.6250,  ..., -10.5000, -10.5000, -10.5000],
         [-11.6250, -11.6250, -11.6250,  ..., -10.5000, -10.5000, -10.5000],
         [-11.6250, -11.6250, -11.6250,  ..., -10.5000, -10.5000, -10.5000],
         ...,
         [ -6.5968,  -6.5968,  -6.5968,  ...,  -5.2503,  -5.2503,  -5.2503],
         [ -6.7656,  -6.7656,  -6.7656,  ...,  -5.4258,  -5.4258,  -5.4258],
         [ -6.7656,  -6.7656,  -6.7656,  ...,  -5.4258,  -5.4258,  -5.4258]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In outdoor recreational activities, people can experience the thrill of flying in the sky with the help of equipment. What object in the picture can provide this experience? Please output segmentation mask. ASSISTANT: in outdoor recreational activities, people can experience the thrill of flying in the sky with the help of equipment. what object in the picture can provide this experience</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[-6.6875, -6.6809, -6.6500,  ..., -6.1156, -5.8140, -5.7500],
         [-6.7137, -6.7108, -6.6968,  ..., -6.1977, -5.9013, -5.8385],
         [-6.8374, -6.8514, -6.9179,  ..., -6.5849, -6.3134, -6.2558],
         ...,
         [-0.9101, -0.9054, -0.8829,  ..., -1.0193, -1.0579, -1.0661],
         [-1.7398, -1.7232, -1.6447,  ..., -1.6416, -1.6091, -1.6022],
         [-2.5695, -2.5410, -2.4066,  ..., -2.2639, -2.1602, -2.1382]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n We are currently watching a game and it's halftime. Who are the cheerleaders who come out to liven up the atmosphere? Please output segmentation mask. ASSISTANT: we are currently watching a game and it's halftime. who are the cheerleaders who come out to liven up the atmosphere</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 272, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 272, 1024])):  [tensor([[[-17.1250, -17.1250, -17.2500,  ..., -16.0234, -16.1250, -16.1250],
         [-17.1250, -17.1250, -17.2500,  ..., -16.0234, -16.1250, -16.1250],
         [-17.4844, -17.4844, -17.6289,  ..., -16.3076, -16.3906, -16.3906],
         ...,
         [-17.6094, -17.6094, -18.0742,  ..., -14.3105, -13.8750, -13.8750],
         [-17.7812, -17.7812, -18.2109,  ..., -14.1094, -13.7031, -13.7031],
         [-18.0938, -18.0938, -18.3828,  ..., -13.3750, -12.9844, -12.9844]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In an educational setting, children often use different materials to learn about letters, numbers, and words. What object in the picture could be used as a visual aid for learning about letters and words? Please output segmentation mask. ASSISTANT: in an educational setting, children often use different materials to learn about letters, numbers, and words. what object in the picture could be used as a visual aid for learning about letters and words</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[90]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1274, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1274, 2048])):  [tensor([[[-6.2188, -6.2188, -6.2188,  ..., -5.8750, -5.8750, -5.8750],
         [-6.2188, -6.2188, -6.2188,  ..., -5.8750, -5.8750, -5.8750],
         [-6.2188, -6.2188, -6.2188,  ..., -5.8750, -5.8750, -5.8750],
         ...,
         [-4.7109, -4.7109, -4.7109,  ..., -4.3926, -4.3926, -4.3926],
         [-4.7891, -4.7891, -4.7891,  ..., -4.3262, -4.3262, -4.3262],
         [-4.8281, -4.8281, -4.8281,  ..., -4.2930, -4.2930, -4.2930]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is a car with a color that is closer to lipstick color in this image? Please output segmentation mask. ASSISTANT: a car with a color that is closer to lipstick color</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[-4.2812, -4.2812, -4.2809,  ..., -9.0625, -9.0625, -9.0625],
         [-4.2812, -4.2812, -4.2809,  ..., -9.0625, -9.0625, -9.0625],
         [-4.2859, -4.2859, -4.2857,  ..., -9.0641, -9.0641, -9.0641],
         ...,
         [-6.9809, -6.9809, -7.0028,  ..., -6.9090, -6.9109, -6.9109],
         [-7.1444, -7.1444, -7.1640,  ..., -6.9551, -6.9562, -6.9562],
         [-7.2773, -7.2773, -7.2952,  ..., -6.9918, -6.9922, -6.9922]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If you want to play table tennis indoors, what furniture in the picture should be used as the playing surface? Please output segmentation mask. ASSISTANT: if you want to play table tennis indoors, what furniture in the picture should be used as the playing surface</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0.,  ..., 255., 255., 255.],
         [  0.,   0.,   0.,  ..., 255., 255., 255.],
         [  0.,   0.,   0.,  ..., 255., 255., 255.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-2.7477, -2.7726, -2.6245,  ..., -1.6565, -1.8739, -1.7435],
         [-2.6007, -2.8102, -2.5977,  ..., -1.4551, -1.5967, -1.6805],
         [-2.4795, -2.6406, -2.4727,  ..., -1.3867, -1.5020, -1.5422],
         ...,
         [-2.2887, -2.6969, -2.4629,  ...,  0.0125, -0.0089, -0.0955],
         [-2.2344, -2.4911, -2.3449,  ..., -0.0925, -0.0511, -0.1893],
         [-2.4424, -2.6245, -2.5497,  ..., -0.5759, -0.5312, -0.5526]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a military airfield, what area is specifically designed for aircraft to take off and land? Please output segmentation mask. ASSISTANT: in a military airfield, what area is specifically designed for aircraft to take off and land</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.],., -7.3761, -7.1824, -7.1581],, 2, 2, 2,0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 900])):  [tensor([[[-6.8438, -6.8438, -6.8363,  ..., -6.5919, -6.4562, -6.2734],
         [-6.8438, -6.8438, -6.8363,  ..., -6.5919, -6.4562, -6.2734],
         [-6.8660, -6.8660, -6.8623,  ..., -6.6014, -6.4671, -6.2799],
         ...,
         [-4.9993, -4.9993, -5.1124,  ..., -4.2299, -4.2929, -4.4517],
         [-4.9375, -4.9375, -5.0488,  ..., -4.2302, -4.2913, -4.4375],
         [-4.9375, -4.9375, -5.0488,  ..., -4.2302, -4.2913, -4.4375]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Some dishes are baked in the oven to enhance their flavors. What object in the picture is commonly used to place the dishes in the oven for baking? Please output segmentation mask. ASSISTANT: some dishes are baked in the oven to enhance their flavors. what object in the picture is commonly used to place the dishes in the oven for baking</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[89]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 900, 1200])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 900, 1200])):  [tensor([[[-6.3438, -6.3438, -6.2596,  ..., -6.4762, -6.4688, -6.4688],
         [-6.3438, -6.3438, -6.2596,  ..., -6.4762, -6.4688, -6.4688],
         [-6.3289, -6.3289, -6.2563,  ..., -6.5048, -6.5009, -6.5009],
         ...,
         [-4.0464, -4.0464, -4.0947,  ..., -4.3557, -4.3658, -4.3658],
         [-4.1831, -4.1831, -4.2257,  ..., -4.3375, -4.3444, -4.3444],
         [-4.3477, -4.3477, -4.3801,  ..., -4.2792, -4.2773, -4.2773]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the object that might reflect this person's marital status in this image? Please output segmentation mask. ASSISTANT: the object that might reflect this person's marital status</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[69]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1066, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1066, 1600])):  [tensor([[[-9.2500, -9.2500, -9.2508,  ..., -4.3215, -4.3125, -4.3125],
         [-9.2500, -9.2500, -9.2508,  ..., -4.3215, -4.3125, -4.3125],
         [-9.2508, -9.2508, -9.2518,  ..., -4.3274, -4.3183, -4.3183],
         ...,
         [-7.3123, -7.3123, -7.3247,  ..., -4.5548, -4.5499, -4.5499],
         [-7.0624, -7.0624, -7.0731,  ..., -4.4390, -4.4299, -4.4299],
         [-6.8828, -6.8828, -6.8923,  ..., -4.3558, -4.3438, -4.3438]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is knobs without screws in the center in this image? Please output segmentation mask. ASSISTANT: knobs without screws in the center</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 615, 461])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 615, 461])):  [tensor([[[-6.8438, -6.8320, -6.7930,  ..., -6.6023, -6.0428, -5.9844],
         [-6.8944, -6.9143, -6.9808,  ..., -6.5879, -6.0755, -6.0096],
         [-7.0634, -7.1888, -7.6070,  ..., -6.5398, -6.1844, -6.0935],
         ...,
         [-4.4097, -4.7518, -5.8925,  ..., -5.3393, -4.5233, -4.1154],
         [-4.2147, -4.5698, -5.7538,  ..., -5.0698, -4.4811, -4.0636],
         [-4.1562, -4.5152, -5.7122,  ..., -4.9890, -4.4685, -4.0481]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is where the garbage should be put in this image? Please output segmentation mask. ASSISTANT: where the garbage should be put</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[-2.7344, -2.7344, -2.7344,  ..., -3.5000, -3.5000, -3.5000],
         [-2.7344, -2.7344, -2.7344,  ..., -3.5000, -3.5000, -3.5000],
         [-2.7344, -2.7344, -2.7344,  ..., -3.5000, -3.5000, -3.5000],
         ...,
         [-2.4767, -2.4767, -2.4767,  ..., -1.7325, -1.7325, -1.7325],
         [-2.4797, -2.4797, -2.4797,  ..., -1.7989, -1.7989, -1.7989],
         [-2.4805, -2.4805, -2.4805,  ..., -1.8145, -1.8145, -1.8145]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Dogs are faithful companions to humans, and humans often play fetch games with them. What object will the dog likely retrieve and bring back to the human for the next round of fetch in the picture? Please output segmentation mask. ASSISTANT: dogs are faithful companions to humans, and humans often play fetch games with them. what object will the dog likely retrieve and bring back to the human for the next round of fetch in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[92]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1816, 3003])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1816, 3003])):  [tensor([[[-6.4062, -6.4062, -6.4062,  ..., -5.7812, -5.7812, -5.7812],
         [-6.4062, -6.4062, -6.4062,  ..., -5.7812, -5.7812, -5.7812],
         [-6.4062, -6.4062, -6.4062,  ..., -5.7812, -5.7812, -5.7812],
         ...,
         [-5.4339, -5.4339, -5.4339,  ..., -5.3052, -5.3052, -5.3052],
         [-5.3807, -5.3807, -5.3807,  ..., -5.2480, -5.2480, -5.2480],
         [-5.3789, -5.3789, -5.3789,  ..., -5.2461, -5.2461, -5.2461]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n People often add gas to water to improve its taste. What part of the picture will be consumed as a beverage? Please output segmentation mask. ASSISTANT: people often add gas to water to improve its taste. what part of the picture will be consumed as a beverage</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2331, 1749])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2331, 1749])):  [tensor([[[-6.2188, -6.2188, -6.2188,  ..., -6.4648, -6.4476, -6.4414],
         [-6.2188, -6.2188, -6.2188,  ..., -6.4648, -6.4476, -6.4414],
         [-6.2188, -6.2188, -6.2188,  ..., -6.4648, -6.4476, -6.4414],
         ...,
         [-3.9062, -3.9062, -3.9062,  ..., -3.9991, -3.9682, -3.9570],
         [-3.9062, -3.9062, -3.9062,  ..., -3.9991, -3.9682, -3.9570],
         [-3.9062, -3.9062, -3.9062,  ..., -3.9991, -3.9682, -3.9570]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When people want to take their pets to the park and enjoy some outdoor activities, they often want to make sure their pets are safe and cannot run away. What in the picture could help with this? Please output segmentation mask. ASSISTANT: when people want to take their pets to the park and enjoy some outdoor activities, they often want to make sure their pets are safe and cannot run away. what in the picture could help with this</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[94]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2705, 3269])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2705, 3269])):  [tensor([[[-7.4062, -7.4062, -7.4062,  ..., -1.8047, -1.8047, -1.8047],
         [-7.4062, -7.4062, -7.4062,  ..., -1.8047, -1.8047, -1.8047],
         [-7.4062, -7.4062, -7.4062,  ..., -1.8047, -1.8047, -1.8047],
         ...,
         [-6.3889, -6.3889, -6.3889,  ..., -5.5652, -5.5652, -5.5652],
         [-6.4961, -6.4961, -6.4961,  ..., -5.6094, -5.6094, -5.6094],
         [-6.4961, -6.4961, -6.4961,  ..., -5.6094, -5.6094, -5.6094]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n I feel my commute is too slow now and I'm hoping to find a convenient mode of transportation that can also help me exercise. Can you help me find the corresponding part in the picture? Please output segmentation mask. ASSISTANT: i feel my commute is too slow now and i'm hoping to find a convenient mode of transportation that can also help me exercise. can you help me find the corresponding part in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[92]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1189, 1656])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1189, 1656])):  [tensor([[[-7.8750, -7.8750, -7.8734,  ..., -2.5916, -2.5938, -2.5938],
         [-7.8750, -7.8750, -7.8734,  ..., -2.5916, -2.5938, -2.5938],
         [-7.8741, -7.8741, -7.8726,  ..., -2.5916, -2.5938, -2.5938],
         ...,
         [-4.7486, -4.7486, -4.7520,  ..., -6.6306, -6.6321, -6.6321],
         [-4.9206, -4.9206, -4.9235,  ..., -6.5589, -6.5606, -6.5606],
         [-5.0391, -5.0391, -5.0417,  ..., -6.5021, -6.5039, -6.5039]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If we were to have a friendly competition to see who can paddle the fastest in the water, what would be the most suitable vessels for this activity in the picture? Please output segmentation mask. ASSISTANT: if we were to have a friendly competition to see who can paddle the fastest in the water, what would be the most suitable vessels for this activity in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[86]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2112, 2816])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2112, 2816])):  [tensor([[[-12.0625, -12.0625, -12.0625,  ..., -11.4375, -11.4375, -11.4375],
         [-12.0625, -12.0625, -12.0625,  ..., -11.4375, -11.4375, -11.4375],
         [-12.0625, -12.0625, -12.0625,  ..., -11.4375, -11.4375, -11.4375],
         ...,
         [ -6.7216,  -6.7216,  -6.7216,  ...,  -4.6193,  -4.6193,  -4.6193],
         [ -6.7330,  -6.7330,  -6.7330,  ...,  -4.6591,  -4.6591,  -4.6591],
         [ -6.7344,  -6.7344,  -6.7344,  ...,  -4.6641,  -4.6641,  -4.6641]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:

>> (validate) mask_positions_in_input_ids:  [[87]], -7.1824, -7.1581],, 2, 2, 2,0.,  ..., 0., 0., 0.],
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2592, 1944])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2592, 1944])):  [tensor([[[-7.5938, -7.5938, -7.5938,  ..., -7.4992, -7.4745, -7.4688],
         [-7.5938, -7.5938, -7.5938,  ..., -7.4992, -7.4745, -7.4688],
         [-7.5938, -7.5938, -7.5938,  ..., -7.4992, -7.4745, -7.4688],
         ...,
         [-3.8125, -3.8125, -3.8125,  ..., -0.4143, -0.7025, -0.7700],
         [-3.8125, -3.8125, -3.8125,  ..., -0.4143, -0.7025, -0.7700],
         [-3.8125, -3.8125, -3.8125,  ..., -0.4143, -0.7025, -0.7700]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When taking pictures with a camera, what part of the camera is responsible for focusing the image and adjusting the depth of field? Please output segmentation mask. ASSISTANT: when taking pictures with a camera, what part of the camera is responsible for focusing the image and adjusting the depth of field</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1664, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1664, 2048])):  [tensor([[[-8.9375, -8.9375, -8.9375,  ..., -8.4375, -8.4375, -8.4375],
         [-8.9375, -8.9375, -8.9375,  ..., -8.4375, -8.4375, -8.4375],
         [-8.9375, -8.9375, -8.9375,  ..., -8.4375, -8.4375, -8.4375],
         ...,
         [-8.3203, -8.3203, -8.3203,  ..., -8.6309, -8.6309, -8.6309],
         [-8.4922, -8.4922, -8.4922,  ..., -8.3848, -8.3848, -8.3848],
         [-8.5781, -8.5781, -8.5781,  ..., -8.2617, -8.2617, -8.2617]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Birds often need a place to rest or observe their surroundings. What part of a tree in the picture offers a suitable spot for birds to do so? Please output segmentation mask. ASSISTANT: birds often need a place to rest or observe their surroundings. what part of a tree in the picture offers a suitable spot for birds to do so</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 612, 612])):  [tensor([[[  0.,   0.,   0.,  ..., 255., 255., 255.],
         [  0.,   0.,   0.,  ..., 255., 255., 255.],
         [  0.,   0.,   0.,  ..., 255., 255., 255.],
         ...,
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 612, 612])):  [tensor([[[-7.6250, -7.5613, -7.3521,  ..., -6.4460, -6.5113, -6.5312],
         [-7.6330, -7.6027, -7.5036,  ..., -6.4597, -6.5268, -6.5472],
         [-7.6591, -7.7388, -8.0005,  ..., -6.5049, -6.5774, -6.5995],
         ...,
         [-5.1763, -5.3501, -5.9204,  ..., -7.8792, -7.6212, -7.5426],
         [-4.9933, -5.1804, -5.7945,  ..., -7.7258, -7.6154, -7.5818],
         [-4.9375, -5.1287, -5.7561,  ..., -7.6790, -7.6137, -7.5938]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that we can sit on in this image? Please output segmentation mask. ASSISTANT: something that we can sit on</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 540, 720])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 540, 720])):  [tensor([[[-5.4688, -5.4514, -5.3837,  ..., -5.1128, -5.2967, -5.3438],
         [-5.4688, -5.4597, -5.4241,  ..., -5.1089, -5.2920, -5.3388],
         [-5.4688, -5.4918, -5.5822,  ..., -5.0933, -5.2734, -5.3194],
         ...,
         [-3.5590, -3.6147, -3.8325,  ..., -2.4561, -2.4952, -2.5052],
         [-3.7587, -3.8126, -4.0235,  ..., -2.7923, -2.8165, -2.8227],
         [-4.0123, -4.0556, -4.2251,  ..., -3.3639, -3.3895, -3.3960]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n To keep bread fresh and protected, it is often placed in a protective covering. What item in the picture is commonly used for this purpose? Please output segmentation mask. ASSISTANT: to keep bread fresh and protected, it is often placed in a protective covering. what item in the picture is commonly used for this purpose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[80]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 612, 612])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 612, 612])):  [tensor([[[-7.1875, -7.1078, -6.8464,  ..., -6.9148, -6.8364, -6.8125],
         [-7.1397, -7.0986, -6.9638,  ..., -6.9362, -6.8811, -6.8643],
         [-6.9828, -7.0684, -7.3491,  ..., -7.0062, -7.0277, -7.0342],
         ...,
         [-4.6052, -4.7949, -5.4175,  ..., -7.1122, -7.0217, -6.9942],
         [-4.5006, -4.7220, -5.4485,  ..., -7.0087, -7.1479, -7.1903],
         [-4.4688, -4.6998, -5.4579,  ..., -6.9771, -7.1863, -7.2500]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Sometimes, when a car is parked on the side of the road, the driver may accidentally leave the car door open. What part in the picture can indicate that the car door is open? Please output segmentation mask. ASSISTANT: sometimes, when a car is parked on the side of the road, the driver may accidentally leave the car door open. what part in the picture can indicate that the car door is open</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[91]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2592, 1936])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2592, 1936])):  [tensor([[[-9.1875, -9.1875, -9.1875,  ..., -7.7297, -7.5785, -7.5430],
         [-9.1875, -9.1875, -9.1875,  ..., -7.7297, -7.5785, -7.5430],
         [-9.1875, -9.1875, -9.1875,  ..., -7.7297, -7.5785, -7.5430],
         ...,
         [-4.8438, -4.8438, -4.8438,  ..., -5.9847, -5.9939, -5.9961],
         [-4.8438, -4.8438, -4.8438,  ..., -5.9847, -5.9939, -5.9961],
         [-4.8438, -4.8438, -4.8438,  ..., -5.9847, -5.9939, -5.9961]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that produces pollen in this image? Please output segmentation mask. ASSISTANT: something that produces pollen</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[ -9.8125,  -9.8387,  -9.9625,  ...,  -9.7688,  -9.7533,  -9.7500],
         [ -9.8158,  -9.8462,  -9.9894,  ...,  -9.8140,  -9.8017,  -9.7991],
         [ -9.8312,  -9.8811, -10.1161,  ..., -10.0273, -10.0303, -10.0310],
         ...,
         [ -7.9120,  -8.0311,  -8.5925,  ...,  -9.3275,  -9.5362,  -9.5805],
         [ -7.3722,  -7.4666,  -7.9118,  ...,  -8.6528,  -8.7006,  -8.7108],
         [ -6.8324,  -6.9022,  -7.2310,  ...,  -7.9780,  -7.8651,  -7.8411]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When purchasing meat from a grocery store, it is often stored and sold in a certain type of container. What object in the picture could be used for this purpose? Please output segmentation mask. ASSISTANT: when purchasing meat from a grocery store, it is often stored and sold in a certain type of container. what object in the picture could be used for this purpose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1920, 2560])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1920, 2560])):  [tensor([[[-6.6562, -6.6562, -6.6562,  ..., -5.9062, -5.9062, -5.9062],
         [-6.6562, -6.6562, -6.6562,  ..., -5.9062, -5.9062, -5.9062],
         [-6.6562, -6.6562, -6.6562,  ..., -5.9062, -5.9062, -5.9062],
         ...,
         [-5.5078, -5.5078, -5.5078,  ..., -4.8203, -4.8203, -4.8203],
         [-5.4984, -5.4984, -5.4984,  ..., -4.8484, -4.8484, -4.8484],
         [-5.4961, -5.4961, -5.4961,  ..., -4.8555, -4.8555, -4.8555]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What object in the picture could be used for defense and firepower in an ancient fort? Please output segmentation mask. ASSISTANT: what object in the picture could be used for defense and firepower in an ancient fort</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[69]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-8.5000, -8.4902, -8.4438,  ..., -8.2813, -8.2039, -8.1875],
         [-8.5033, -8.4986, -8.4766,  ..., -8.3206, -8.2514, -8.2367],
         [-8.5188, -8.5384, -8.6313,  ..., -8.5063, -8.4753, -8.4688],
         ...,
         [-1.2449, -1.2687, -1.3810,  ..., -0.6486, -0.7261, -0.7426],
         [-1.6584, -1.6789, -1.7754,  ..., -0.9854, -1.0403, -1.0519],
         [-2.5419, -2.5548, -2.6158,  ..., -1.9352, -1.9179, -1.9143]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something showing that they are skating in this image? Please output segmentation mask. ASSISTANT: something showing that they are skating</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 580, 435])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 580, 435])):  [tensor([[[-10.6250, -10.5136, -10.2101,  ...,  -8.8621,  -8.9172,  -9.4612],
         [-10.6149, -10.5839, -10.4995,  ...,  -8.9221,  -9.0667,  -9.5671],
         [-10.5873, -10.7754, -11.2876,  ...,  -9.0858,  -9.4739,  -9.8557],
         ...,
         [ -5.4919,  -5.6432,  -6.0554,  ...,  -4.4136,  -4.8989,  -5.0355],
         [ -5.7264,  -5.8263,  -6.0984,  ...,  -4.1058,  -4.6245,  -4.7272],
         [ -5.8125,  -5.8935,  -6.1142,  ...,  -3.9927,  -4.5238,  -4.6140]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Dogs have keen sense of smell, which is why they can be used as drug-sniffing dogs. Which part in the picture gives dogs this characteristic? Please output segmentation mask. ASSISTANT: dogs have keen sense of smell, which is why they can be used as drug-sniffing dogs. which part in the picture gives dogs this characteristic</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]

>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[93]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2356, 3141])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2356, 3141])):  [tensor([[[-7.7812, -7.7812, -7.7812,  ..., -7.1875, -7.1875, -7.1875],
         [-7.7812, -7.7812, -7.7812,  ..., -7.1875, -7.1875, -7.1875],
         [-7.7812, -7.7812, -7.7812,  ..., -7.1875, -7.1875, -7.1875],
         ...,
         [-5.8426, -5.8426, -5.8426,  ..., -5.5243, -5.5243, -5.5243],
         [-5.8672, -5.8672, -5.8672,  ..., -5.6055, -5.6055, -5.6055],
         [-5.8672, -5.8672, -5.8672,  ..., -5.6055, -5.6055, -5.6055]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When snorkeling or scuba diving, it is important to wear certain equipment to protect the eyes and aid in vision. What object in the picture can be used for this purpose? Please output segmentation mask. ASSISTANT: when snorkeling or scuba diving, it is important to wear certain equipment to protect the eyes and aid in vision. what object in the picture can be used for this purpose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[89]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-7.3438, -7.3438, -7.3281,  ..., -6.2812, -6.1250, -6.1250],
         [-7.3438, -7.3438, -7.3281,  ..., -6.2812, -6.1250, -6.1250],
         [-7.3203, -7.3203, -7.3286,  ..., -6.3765, -6.2422, -6.2422],
         ...,
         [-4.9219, -4.9219, -5.0137,  ..., -5.5054, -5.5234, -5.5234],
         [-4.8750, -4.8750, -4.9565,  ..., -5.5562, -5.5898, -5.5898],
         [-4.8750, -4.8750, -4.9243,  ..., -5.6919, -5.7070, -5.7070]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the places for further exploration in this image? Please output segmentation mask. ASSISTANT: the places for further exploration</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-1.3594, -1.3618, -1.3734,  ..., -1.8281, -1.7701, -1.7578],
         [-1.3540, -1.3586, -1.3803,  ..., -1.8302, -1.7806, -1.7701],
         [-1.3289, -1.3435, -1.4126,  ..., -1.8401, -1.8302, -1.8281],
         ...,
         [ 1.1953,  1.1908,  1.1695,  ...,  0.7487,  0.6711,  0.6547],
         [ 0.5279,  0.5223,  0.4956,  ...,  0.0774,  0.0428,  0.0355],
         [-0.7915, -0.8085, -0.8885,  ..., -1.7599, -1.7976, -1.8056]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When enjoying an ice cream sundae, what can we use to scoop up the whipped cream and place it on top of the ice cream? Please output segmentation mask. ASSISTANT: when enjoying an ice cream sundae, what can we use to scoop up the whipped cream and place it on top of the ice cream</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[86]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2048, 1536])):  [tensor([[[0., 0., 0.,  ..., 1., 1., 1.],
         [0., 0., 0.,  ..., 1., 1., 0.],
         [0., 0., 0.,  ..., 1., 1., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2048, 1536])):  [tensor([[[-5.2500, -5.2500, -5.2500,  ..., -2.1641, -2.5859, -2.7969],
         [-5.2500, -5.2500, -5.2500,  ..., -2.1641, -2.5859, -2.7969],
         [-5.2500, -5.2500, -5.2500,  ..., -2.1641, -2.5859, -2.7969],
         ...,
         [-5.2500, -5.2500, -5.2500,  ..., -6.2734, -6.2891, -6.2969],
         [-5.2500, -5.2500, -5.2500,  ..., -6.2734, -6.2891, -6.2969],
         [-5.2500, -5.2500, -5.2500,  ..., -6.2734, -6.2891, -6.2969]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the person who is speaking currently in this image? Please output segmentation mask. ASSISTANT: the person who is speaking currently</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-5.5625, -5.5625, -5.5625,  ..., -7.6562, -7.6562, -7.6562],
         [-5.5625, -5.5625, -5.5625,  ..., -7.6562, -7.6562, -7.6562],
         [-5.5625, -5.5625, -5.5625,  ..., -7.6562, -7.6562, -7.6562],
         ...,
         [-2.5000, -2.5000, -2.5000,  ..., -4.4766, -4.4766, -4.4766],
         [-2.5469, -2.5469, -2.5469,  ..., -4.4805, -4.4805, -4.4805],
         [-2.5469, -2.5469, -2.5469,  ..., -4.4805, -4.4805, -4.4805]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the ball that can only be hit into the hole at last in this image? Please output segmentation mask. ASSISTANT: the ball that can only be hit into the hole at last</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[69]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 960, 1280])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 960, 1280])):  [tensor([[[-10.5625, -10.5625, -10.5391,  ...,  -9.0195,  -8.9375,  -8.9375],
         [-10.5625, -10.5625, -10.5391,  ...,  -9.0195,  -8.9375,  -8.9375],
         [-10.6094, -10.6094, -10.5938,  ...,  -9.1523,  -9.0703,  -9.0703],
         ...,
         [ -7.8008,  -7.8008,  -7.9503,  ...,  -7.5496,  -7.5234,  -7.5234],
         [ -7.6500,  -7.6500,  -7.7762,  ...,  -7.5687,  -7.5500,  -7.5500],
         [ -7.6992,  -7.6992,  -7.7905,  ...,  -7.8066,  -7.7852,  -7.7852]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the reflection of the camera in the mirror in this image? Please output segmentation mask. ASSISTANT: the reflection of the camera in the mirror</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2880, 1920])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2880, 1920])):  [tensor([[[-5.8438, -5.8438, -5.8438,  ..., -7.9371, -7.9232, -7.9219],
         [-5.8438, -5.8438, -5.8438,  ..., -7.9371, -7.9232, -7.9219],
         [-5.8438, -5.8438, -5.8438,  ..., -7.9371, -7.9232, -7.9219],
         ...,
         [-5.7812, -5.7812, -5.7812,  ..., -5.3720, -5.3498, -5.3477],
         [-5.7812, -5.7812, -5.7812,  ..., -5.3720, -5.3498, -5.3477],
         [-5.7812, -5.7812, -5.7812,  ..., -5.3720, -5.3498, -5.3477]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In some rural areas, horse-drawn carts are still used for transportation and carrying goods. What is the main source of power that drives the cart in the picture? Please output segmentation mask. ASSISTANT: in some rural areas, horse-drawn carts are still used for transportation and carrying goods. what is the main source of power that drives the cart in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2304, 3456])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2304, 3456])):  [tensor([[[-5.2188, -5.2188, -5.2188,  ..., -5.3125, -5.3125, -5.3125],
         [-5.2188, -5.2188, -5.2188,  ..., -5.3125, -5.3125, -5.3125],
         [-5.2188, -5.2188, -5.2188,  ..., -5.3125, -5.3125, -5.3125],
         ...,
         [-3.3063, -3.3063, -3.3063,  ..., -2.7752, -2.7752, -2.7752],
         [-3.3223, -3.3223, -3.3223,  ..., -2.7969, -2.7969, -2.7969],
         [-3.3223, -3.3223, -3.3223,  ..., -2.7969, -2.7969, -2.7969]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In case of a fire, it is important to have access to fire safety equipment. What object in the picture is specifically designed to store and release fire extinguishing substances? Please output segmentation mask. ASSISTANT: in case of a fire, it is important to have access to fire safety equipment. what object in the picture is specifically designed to store and release fire extinguishing substances</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-6.9375, -6.9475, -6.9904,  ..., -6.9798, -6.9455, -6.9375],
         [-6.9515, -6.9672, -7.0343,  ..., -7.0005, -6.9689, -6.9615],
         [-7.0116, -7.0515, -7.2223,  ..., -7.0889, -7.0691, -7.0645],
         ...,
         [-4.8898, -4.9391, -5.1503,  ..., -4.1736, -4.2361, -4.2507],
         [-4.7745, -4.8117, -4.9710,  ..., -3.7402, -3.7206, -3.7160],
         [-4.9215, -4.9502, -5.0727,  ..., -3.7062, -3.6063, -3.5830]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the solitary tree in this image? Please output segmentation mask. ASSISTANT: the solitary tree</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[-8.7500, -8.7500, -8.7500,  ..., -8.3750, -8.3750, -8.3750],
         [-8.7500, -8.7500, -8.7500,  ..., -8.3750, -8.3750, -8.3750],
         [-8.7500, -8.7500, -8.7500,  ..., -8.3750, -8.3750, -8.3750],
         ...,
         [-4.3584, -4.3584, -4.3584,  ..., -3.6211, -3.6211, -3.6211],
         [-4.7119, -4.7119, -4.7119,  ..., -3.9727, -3.9727, -3.9727],
         [-4.8887, -4.8887, -4.8887,  ..., -4.1484, -4.1484, -4.1484]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something used for contacting other people in this image? Please output segmentation mask. ASSISTANT: something used for contacting other people</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:32<00:00,  6.18it/s]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 1 is less than current step: 999. Dropping entry: {'val/giou': 0.2171332836151123, 'val/ciou': 0.20119187235832214, 'val/b_acc': 0.6149999692500016, 'val/i_acc': 0.9468162943478501, 'val/o_acc': 0.9999849282592336, '_timestamp': 1743840544.043041}).
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
  0%|                                                                                                                                                         | 0/200 [00:00<?, ?it/s]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,

(PLUM.py) >> pred_masks (shape: torch.Size([1, 2570, 2229])):  [tensor([[[ -7.9062,  -7.9062,  -7.9062,  ...,  -6.2117,  -6.2832,  -6.3008],
         [ -7.9062,  -7.9062,  -7.9062,  ...,  -6.2117,  -6.2832,  -6.3008],
         [ -7.9062,  -7.9062,  -7.9062,  ...,  -6.2117,  -6.2832,  -6.3008],
         ...,
         [ -7.9062,  -7.9062,  -7.9062,  ..., -11.2644, -11.1462, -11.1172],
         [ -7.9062,  -7.9062,  -7.9062,  ..., -11.2644, -11.1462, -11.1172],
         [ -7.9062,  -7.9062,  -7.9062,  ..., -11.2644, -11.1462, -11.1172]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the sour food in this image? Please output segmentation mask. ASSISTANT: the sour food</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[-2.5625, -2.6170, -2.8047,  ..., -3.4609, -3.4549, -3.4531],
         [-2.5273, -2.5870, -2.7924,  ..., -3.4117, -3.3948, -3.3898],
         [-2.4062, -2.4836, -2.7500,  ..., -3.2422, -3.1877, -3.1719],
         ...,
         [-2.5781, -2.5298, -2.3633,  ..., -6.4375, -6.5949, -6.6406],
         [-2.5725, -2.5523, -2.4828,  ..., -6.3723, -6.5025, -6.5403],
         [-4.0922, -4.0857, -4.0633,  ..., -7.6219, -7.7648, -7.8063]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that protects the woman's eyes from getting wet in this image? Please output segmentation mask. ASSISTANT: something that protects the woman's eyes from getting wet</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[69]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2112, 2370])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2112, 2370])):  [tensor([[[-11.3125, -11.3125, -11.3125,  ...,  -8.2500,  -8.2500,  -8.2500],
         [-11.3125, -11.3125, -11.3125,  ...,  -8.2500,  -8.2500,  -8.2500],
         [-11.3125, -11.3125, -11.3125,  ...,  -8.2500,  -8.2500,  -8.2500],
         ...,
         [-12.2881, -12.2881, -12.2881,  ..., -13.5631, -13.5631, -13.5631],
         [-11.9166, -11.9166, -11.9166,  ..., -13.2253, -13.2253, -13.2253],
         [-11.7891, -11.7891, -11.7891,  ..., -13.1094, -13.1094, -13.1094]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When participating in water activities such as kayaking, it is important to ensure personal safety. What item in the picture is designed to provide buoyancy and keep the person afloat in case of an accident? Please output segmentation mask. ASSISTANT: when participating in water activities such as kayaking, it is important to ensure personal safety. what item in the picture is designed to provide buoyancy and keep the person afloat in case of an accident</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[95]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1037, 1322])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1037, 1322])):  [tensor([[[-12.8125, -12.8125, -12.7375,  ..., -13.6284, -13.6250, -13.6250],
         [-12.8125, -12.8125, -12.7375,  ..., -13.6284, -13.6250, -13.6250],
         [-12.7989, -12.7989, -12.7315,  ..., -13.7860, -13.7816, -13.7816],
         ...,
         [ -9.8157,  -9.8157,  -9.9229,  ..., -10.3752, -10.3955, -10.3955],
         [ -8.3868,  -8.3868,  -8.4224,  ...,  -8.9996,  -9.0049,  -9.0049],
         [ -7.7305,  -7.7305,  -7.7251,  ...,  -8.3133,  -8.2969,  -8.2969]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the sewage outlet in this image? Please output segmentation mask. ASSISTANT: the sewage outlet</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 4032, 3024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 4032, 3024])):  [tensor([[[-2.7656, -2.7656, -2.7656,  ..., -5.0809, -5.0703, -5.0703],
         [-2.7656, -2.7656, -2.7656,  ..., -5.0809, -5.0703, -5.0703],
         [-2.7656, -2.7656, -2.7656,  ..., -5.0809, -5.0703, -5.0703],
         ...,
         [-3.9844, -3.9844, -3.9844,  ..., -3.8533, -3.8496, -3.8496],
         [-3.9844, -3.9844, -3.9844,  ..., -3.8533, -3.8496, -3.8496],
         [-3.9844, -3.9844, -3.9844,  ..., -3.8533, -3.8496, -3.8496]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In some circus performances, gymnasts perform aerial acrobatics using long pieces of fabric that hang from the ceiling. What in the picture could be used for this type of performance? Please output segmentation mask. ASSISTANT: in some circus performances, gymnasts perform aerial acrobatics using long pieces of fabric that hang from the ceiling. what in the picture could be used for this type of performance</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[92]]
>> (validate) sampled_classes_list:  [None]

         [-6.8964, -6.8737, -6.7665,  ..., -9.2909, -9.3357, -9.3453],([[[ -7.9062,  -7.9062,  -7.9062,  ...,  -6.2117,  -6.2832,  -6.3008],
         [-6.8501, -6.8517, -6.8593,  ..., -9.5428, -9.5074, -9.4998],
         ...,
         [-7.1309, -7.2159, -7.6164,  ..., -6.6865, -7.0376, -7.1120],
         [-6.7410, -6.8097, -7.1336,  ..., -6.3107, -6.5429, -6.5922],
         [-6.3512, -6.4036, -6.6508,  ..., -5.9348, -6.0483, -6.0724]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that the dog's food should be put into in this image? Please output segmentation mask. ASSISTANT: something that the dog's food should be put into</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[-3.9062, -3.8992, -3.8750,  ..., -3.9297, -3.9236, -3.9219],
         [-3.9449, -3.9573, -3.9998,  ..., -4.2804, -4.2110, -4.1908],
         [-4.0781, -4.1572, -4.4297,  ..., -5.4883, -5.2007, -5.1172],
         ...,
         [-2.1562, -2.2020, -2.3594,  ..., -2.6113, -2.4796, -2.4414],
         [-2.2248, -2.2674, -2.4139,  ..., -2.0680, -1.7852, -1.7031],
         [-2.9766, -3.0082, -3.1172,  ..., -3.4238, -3.1565, -3.0789]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the food that makes people feel spicy or hot in this image? Please output segmentation mask. ASSISTANT: the food that makes people feel spicy or hot</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[67]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 679, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 679, 1024])):  [tensor([[[-7.3750, -7.3750, -7.3555,  ..., -3.4258, -3.3906, -3.3906],
         [-7.3750, -7.3750, -7.3555,  ..., -3.4258, -3.3906, -3.3906],
         [-7.3125, -7.3125, -7.3179,  ..., -3.4192, -3.3848, -3.3848],
         ...,
         [-4.2090, -4.2090, -4.3059,  ..., -5.3096, -5.3125, -5.3125],
         [-4.4238, -4.4238, -4.5212,  ..., -5.2100, -5.1875, -5.1875],
         [-5.1289, -5.1289, -5.2339,  ..., -5.9399, -5.8906, -5.8906]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the object used to protect the ears in this image? Please output segmentation mask. ASSISTANT: the object used to protect the ears</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2112, 2816])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2112, 2816])):  [tensor([[[-11.3125, -11.3125, -11.3125,  ..., -12.0000, -12.0000, -12.0000],
         [-11.3125, -11.3125, -11.3125,  ..., -12.0000, -12.0000, -12.0000],
         [-11.3125, -11.3125, -11.3125,  ..., -12.0000, -12.0000, -12.0000],
         ...,
         [ -9.5171,  -9.5171,  -9.5171,  ...,  -6.7216,  -6.7216,  -6.7216],
         [ -9.6477,  -9.6477,  -9.6477,  ...,  -6.7330,  -6.7330,  -6.7330],
         [ -9.6641,  -9.6641,  -9.6641,  ...,  -6.7344,  -6.7344,  -6.7344]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the place where piano players should sit in this image? Please output segmentation mask. ASSISTANT: the place where piano players should sit</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 960, 1280])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 960, 1280])):  [tensor([[[-5.0625, -5.0625, -5.0547,  ..., -6.2520, -6.3125, -6.3125],
         [-5.0625, -5.0625, -5.0547,  ..., -6.2520, -6.3125, -6.3125],
         [-5.0781, -5.0781, -5.0751,  ..., -6.2405, -6.2969, -6.2969],
         ...,
         [-2.5176, -2.5176, -2.5528,  ..., -1.9433, -1.9453, -1.9453],
         [-2.7937, -2.7937, -2.8215,  ..., -2.3699, -2.3687, -2.3687],
         [-3.1465, -3.1465, -3.1660,  ..., -2.8594, -2.8555, -2.8555]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the person who is shooting a free throw in this image? Please output segmentation mask. ASSISTANT: the person who is shooting a free throw</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2287, 1831])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2287, 1831])):  [tensor([[[-10.5625, -10.5625, -10.5625,  ...,  -8.8475,  -8.8335,  -8.8281],
         [-10.5625, -10.5625, -10.5625,  ...,  -8.8475,  -8.8335,  -8.8281],
         [-10.5625, -10.5625, -10.5625,  ...,  -8.8475,  -8.8335,  -8.8281],
         ...,
         [ -6.5000,  -6.5000,  -6.5000,  ...,  -6.9791,  -7.3500,  -7.4922],
         [ -6.5000,  -6.5000,  -6.5000,  ...,  -6.9791,  -7.3500,  -7.4922],
         [ -6.5000,  -6.5000,  -6.5000,  ...,  -6.9791,  -7.3500,  -7.4922]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When soldiers gather for a group photo, what object in the picture is usually held by one of them to represent their unit or country? Please output segmentation mask. ASSISTANT: when soldiers gather for a group photo, what object in the picture is usually held by one of them to represent their unit or country</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]

(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[ -8.3821,  -8.0421,  -7.9227,  ...,  -8.1131,  -9.1620, -10.2904],],
         [ -9.1163,  -8.9831,  -8.8250,  ..., -10.0359, -10.8506, -10.2362],
         [ -9.4453,  -9.8750,  -9.2344,  ...,  -9.9688,  -9.9125,  -9.5687],
         ...,
         [-18.0148, -22.2687, -22.1562,  ..., -16.4531, -16.5031, -16.6535],
         [-17.1062, -20.5394, -21.3062,  ..., -15.8906, -15.2119, -16.2130],
         [-12.7265, -14.3144, -14.5072,  ..., -10.8773, -10.3895, -10.9653]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the dog that puts its tongue out in this image? Please output segmentation mask. ASSISTANT: the dog that puts its tongue out</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1600, 1200])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1600, 1200])):  [tensor([[[-8.0000, -8.0000, -7.9961,  ..., -9.0156, -8.7975, -8.6250],
         [-8.0000, -8.0000, -7.9961,  ..., -9.0156, -8.7975, -8.6250],
         [-7.9859, -7.9859, -7.9823,  ..., -9.0152, -8.7945, -8.6200],
         ...,
         [-3.6896, -3.6896, -3.6861,  ..., -1.1790, -1.6027, -1.9264],
         [-3.7031, -3.7031, -3.6994,  ..., -1.1809, -1.6022, -1.9238],
         [-3.7031, -3.7031, -3.6994,  ..., -1.1809, -1.6022, -1.9238]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a rural landscape, what objects in the picture could provide shade and shelter for animals or humans? Please output segmentation mask. ASSISTANT: in a rural landscape, what objects in the picture could provide shade and shelter for animals or humans</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[72]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[-4.6250, -4.5582, -4.3281,  ..., -4.6172, -4.1025, -3.9531],
         [-4.3930, -4.3539, -4.2191,  ..., -4.4766, -4.0259, -3.8951],
         [-3.5938, -3.6500, -3.8438,  ..., -3.9922, -3.7621, -3.6953],
         ...,
         [-5.7188, -5.7627, -5.9141,  ..., -5.7109, -6.0197, -6.1094],
         [-5.6739, -5.7557, -6.0375,  ..., -6.0704, -6.1797, -6.2114],
         [-7.9406, -7.9965, -8.1891,  ..., -8.9735, -9.2895, -9.3813]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something used for playing videos or music in this image? Please output segmentation mask. ASSISTANT: something used for playing videos or music</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-0.1162, -0.1162, -0.1327,  ..., -3.2969, -3.2812, -3.2812],
         [-0.1162, -0.1162, -0.1327,  ..., -3.2969, -3.2812, -3.2812],
         [-0.0939, -0.0939, -0.1111,  ..., -3.3135, -3.3125, -3.3125],
         ...,
         [-2.4434, -2.4434, -2.4521,  ..., -2.7223, -2.7402, -2.7402],
         [-3.0469, -3.0469, -3.0588,  ..., -3.5510, -3.5762, -3.5762],
         [-4.1406, -4.1406, -4.1570,  ..., -5.1414, -5.1973, -5.1973]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When celebrating birthdays, it is common to have a cake with decorations. What part of the cake in the picture is typically used to write birthday greetings or the name of the person celebrating? Please output segmentation mask. ASSISTANT: when celebrating birthdays, it is common to have a cake with decorations. what part of the cake in the picture is typically used to write birthday greetings or the name of the person celebrating</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[97]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[-6.9062, -6.9836, -7.2500,  ..., -9.0938, -9.2148, -9.2500],
         [-6.8957, -6.9889, -7.3098,  ..., -9.2238, -9.3585, -9.3977],
         [-6.8594, -7.0070, -7.5156,  ..., -9.6719, -9.8535, -9.9062],
         ...,
         [-8.0938, -8.2590, -8.8281,  ..., -8.5312, -8.8945, -9.0000],
         [-8.0492, -8.2727, -9.0426,  ..., -8.1301, -8.2545, -8.2906],
         [-8.6750, -8.8156, -9.3000,  ..., -9.3531, -9.5251, -9.5750]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that allow pedestrians to cross the canyon in this image? Please output segmentation mask. ASSISTANT: something that allow pedestrians to cross the canyon</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1989, 5815])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1989, 5815])):  [tensor([[[-5.0312, -5.0312, -5.0312,  ..., -3.3125, -3.3125, -3.3125],
         [-5.0312, -5.0312, -5.0312,  ..., -3.3125, -3.3125, -3.3125],
         [-5.0312, -5.0312, -5.0312,  ..., -3.3125, -3.3125, -3.3125],
         ...,
         [-1.2861, -1.2861, -1.2861,  ..., -2.0518, -2.0518, -2.0518],
         [-1.2861, -1.2861, -1.2861,  ..., -2.0518, -2.0518, -2.0518],
         [-1.2861, -1.2861, -1.2861,  ..., -2.0518, -2.0518, -2.0518]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the place where the dishes and meals should be put for eating in this image? Please output segmentation mask. ASSISTANT: the place where the dishes and meals should be put for eating</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[72]]
>> (validate) sampled_classes_list:  [None]

         [ -9.3963,  -9.5934,  -9.9702,  ..., -10.7953, -11.0794, -11.2280],3821,  -8.0421,  -7.9227,  ...,  -8.1131,  -9.1620, -10.2904],],
         [ -9.5563, -10.0022, -10.8540,  ..., -11.7726, -12.1714, -12.3801],
         ...,
         [ -6.7988,  -6.9632,  -7.2775,  ...,  -8.6338,  -8.4623,  -8.3725],
         [ -7.2670,  -7.4871,  -7.9077,  ...,  -8.9773,  -8.6522,  -8.4820],
         [ -8.3390,  -8.4905,  -8.7801,  ...,  -9.6141,  -9.2070,  -8.9940]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In some regions, people raise certain animals for their milk, meat, and skin. What animal in the picture could be domesticated for such purposes? Please output segmentation mask. ASSISTANT: in some regions, people raise certain animals for their milk, meat, and skin. what animal in the picture could be domesticated for such purposes</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1365, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1365, 2048])):  [tensor([[[-3.3125, -3.3125, -3.3125,  ..., -3.9062, -3.9062, -3.9062],
         [-3.3125, -3.3125, -3.3125,  ..., -3.9062, -3.9062, -3.9062],
         [-3.3125, -3.3125, -3.3125,  ..., -3.9062, -3.9062, -3.9062],
         ...,
         [-3.1395, -3.1395, -3.1395,  ..., -1.8975, -1.8975, -1.8975],
         [-3.2157, -3.2157, -3.2157,  ..., -1.9014, -1.9014, -1.9014],
         [-3.2539, -3.2539, -3.2539,  ..., -1.9033, -1.9033, -1.9033]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the place where the child is about to slip/fall off in this image? Please output segmentation mask. ASSISTANT: the place where the child is about to slip/fall off</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2152, 1704])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2152, 1704])):  [tensor([[[-2.9531, -2.9531, -2.9531,  ..., -4.9158, -5.0682, -5.1367],
         [-2.9531, -2.9531, -2.9531,  ..., -4.9158, -5.0682, -5.1367],
         [-2.9531, -2.9531, -2.9531,  ..., -4.9158, -5.0682, -5.1367],
         ...,
         [ 0.0089,  0.0089,  0.0089,  ...,  0.3492,  0.2744,  0.2408],
         [ 0.0089,  0.0089,  0.0089,  ...,  0.3492,  0.2744,  0.2408],
         [ 0.0089,  0.0089,  0.0089,  ...,  0.3492,  0.2744,  0.2408]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In order to facilitate transportation and connect different regions, what structure in the picture was built across the water? Please output segmentation mask. ASSISTANT: in order to facilitate transportation and connect different regions, what structure in the picture was built across the water</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[74]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3264, 4928])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3264, 4928])):  [tensor([[[-9.3125, -9.3125, -9.3125,  ..., -8.4375, -8.4375, -8.4375],
         [-9.3125, -9.3125, -9.3125,  ..., -8.4375, -8.4375, -8.4375],
         [-9.3125, -9.3125, -9.3125,  ..., -8.4375, -8.4375, -8.4375],
         ...,
         [-4.9311, -4.9311, -4.9311,  ..., -6.3721, -6.3721, -6.3721],
         [-4.9395, -4.9395, -4.9395,  ..., -6.3711, -6.3711, -6.3711],
         [-4.9395, -4.9395, -4.9395,  ..., -6.3711, -6.3711, -6.3711]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the cesspit in this image? Please output segmentation mask. ASSISTANT: the cesspit</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1712, 2288])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1712, 2288])):  [tensor([[[-3.7656, -3.7656, -3.7656,  ..., -4.3438, -4.3438, -4.3438],
         [-3.7656, -3.7656, -3.7656,  ..., -4.3438, -4.3438, -4.3438],
         [-3.7656, -3.7656, -3.7656,  ..., -4.3438, -4.3438, -4.3438],
         ...,
         [-1.1464, -1.1464, -1.1464,  ..., -0.8736, -0.8736, -0.8736],
         [-1.2316, -1.2316, -1.2316,  ..., -0.8679, -0.8679, -0.8679],
         [-1.2642, -1.2642, -1.2642,  ..., -0.8657, -0.8657, -0.8657]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the goat nearest to the bottom stone in this image? Please output segmentation mask. ASSISTANT: the goat nearest to the bottom stone</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]0794, -11.2280],3821,  -8.0421,  -7.9227,  ...,  -8.1131,  -9.1620, -10.2904],],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 651, 500])):  [tensor([[[ -8.0000,  -7.9296,  -7.6834,  ...,  -9.4059,  -8.0455,  -6.4735],
         [ -7.9228,  -7.9028,  -7.8329,  ...,  -9.5543,  -8.1153,  -6.4431],
         [ -7.6528,  -7.8091,  -8.3559,  ..., -10.0734,  -8.3595,  -6.3366],
         ...,
         [ -8.6685,  -9.2463, -11.2677,  ..., -11.7998, -10.0175,  -7.9092],
         [ -8.2459,  -8.9269, -11.3096,  ..., -11.7356,  -9.8482,  -7.5553],
         [ -8.1250,  -8.8355, -11.3216,  ..., -11.7172,  -9.7998,  -7.4541]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Flowers are often used to decorate tables during special occasions or events. What item in the picture can be used to hold the flowers and keep them fresh by providing water to them? Please output segmentation mask. ASSISTANT: flowers are often used to decorate tables during special occasions or events. what item in the picture can be used to hold the flowers and keep them fresh by providing water to them</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1370, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1370, 2048])):  [tensor([[[-4.8750, -4.8750, -4.8750,  ..., -8.3125, -8.3125, -8.3125],
         [-4.8750, -4.8750, -4.8750,  ..., -8.3125, -8.3125, -8.3125],
         [-4.8750, -4.8750, -4.8750,  ..., -8.3125, -8.3125, -8.3125],
         ...,
         [-4.7168, -4.7168, -4.7168,  ..., -6.5430, -6.5430, -6.5430],
         [-4.8145, -4.8145, -4.8145,  ..., -6.3945, -6.3945, -6.3945],
         [-4.8633, -4.8633, -4.8633,  ..., -6.3203, -6.3203, -6.3203]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Bamboo is very hard, and its sharp edges can easily scratch people. What tool in the picture can I use to split the bamboo and cross this bamboo forest? Please output segmentation mask. ASSISTANT: bamboo is very hard, and its sharp edges can easily scratch people. what tool in the picture can i use to split the bamboo and cross this bamboo forest</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[92]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[-3.7500, -3.7500, -3.7508,  ..., -0.0844, -0.0767, -0.0767],
         [-3.7500, -3.7500, -3.7508,  ..., -0.0844, -0.0767, -0.0767],
         [-3.7494, -3.7494, -3.7503,  ..., -0.0723, -0.0646, -0.0646],
         ...,
         [-4.0799, -4.0799, -4.0826,  ..., -4.8525, -4.8488, -4.8488],
         [-4.5381, -4.5381, -4.5396,  ..., -5.2576, -5.2544, -5.2544],
         [-4.8867, -4.8867, -4.8873,  ..., -5.5771, -5.5742, -5.5742]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the damaged part on the pants in this image? Please output segmentation mask. ASSISTANT: the damaged part on the pants</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[-1.7422, -1.7422, -1.7422,  ..., -0.3340, -0.3340, -0.3340],
         [-1.7422, -1.7422, -1.7422,  ..., -0.3340, -0.3340, -0.3340],
         [-1.7422, -1.7422, -1.7422,  ..., -0.3340, -0.3340, -0.3340],
         ...,
         [-3.6569, -3.6569, -3.6569,  ..., -2.5150, -2.5150, -2.5150],
         [-3.8223, -3.8223, -3.8223,  ..., -2.9434, -2.9434, -2.9434],
         [-3.8223, -3.8223, -3.8223,  ..., -2.9434, -2.9434, -2.9434]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Seafood dishes often include a tangy condiment that enhances the flavor. What item in the picture can be squeezed onto the seafood as a tangy flavor enhancer? Please output segmentation mask. ASSISTANT: seafood dishes often include a tangy condiment that enhances the flavor. what item in the picture can be squeezed onto the seafood as a tangy flavor enhancer</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[96]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1080, 1080])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1080, 1080])):  [tensor([[[-2.4219, -2.4219, -2.4933,  ..., -6.5965, -6.5625, -6.5625],
         [-2.4219, -2.4219, -2.4933,  ..., -6.5965, -6.5625, -6.5625],
         [-2.4270, -2.4270, -2.5076,  ..., -6.7075, -6.6883, -6.6883],
         ...,
         [-8.4485, -8.4485, -8.8389,  ..., -8.6740, -8.7908, -8.7908],
         [-8.3125, -8.3125, -8.7273,  ..., -8.6140, -8.7500, -8.7500],
         [-8.3125, -8.3125, -8.7273,  ..., -8.6140, -8.7500, -8.7500]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Dogs often like to find a comfortable place to rest. What object in the picture can offer a soft and comfortable surface for the dog to lie on? Please output segmentation mask. ASSISTANT: dogs often like to find a comfortable place to rest. what object in the picture can offer a soft and comfortable surface for the dog to lie on</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[82]]
>> (validate) sampled_classes_list:  [None]

 26%|█████████████████████████████████████▍                                                                                                          | 52/200 [00:14<00:50,  2.96it/s]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1024, 1280])):  [tensor([[[-10.4375, -10.4375, -10.4141,  ...,  -9.0078,  -9.0625,  -9.0625],
         [-10.4375, -10.4375, -10.4141,  ...,  -9.0078,  -9.0625,  -9.0625],
         [-10.4570, -10.4570, -10.4419,  ...,  -9.0012,  -9.0547,  -9.0547],
         ...,
         [ -7.5620,  -7.5620,  -7.6581,  ...,  -9.0419,  -9.0698,  -9.0698],
         [ -6.9374,  -6.9374,  -6.9957,  ...,  -8.3384,  -8.3541,  -8.3541],
         [ -6.7188,  -6.7188,  -6.7576,  ...,  -7.9692,  -7.9688,  -7.9688]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a zoo, there are separate areas designated for different animals. What structure in the picture is used to confine and display animals? Please output segmentation mask. ASSISTANT: in a zoo, there are separate areas designated for different animals. what structure in the picture is used to confine and display animals</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[0., 0., 1.,  ..., 0., 0., 0.],
         [0., 0., 1.,  ..., 0., 0., 0.],
         [0., 0., 1.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[-1.3516, -1.3516, -1.3516,  ...,  0.3867,  0.3867,  0.3867],
         [-1.3516, -1.3516, -1.3516,  ...,  0.3867,  0.3867,  0.3867],
         [-1.3516, -1.3516, -1.3516,  ...,  0.3867,  0.3867,  0.3867],
         ...,
         [-0.9385, -0.9385, -0.9385,  ..., -2.1427, -2.1427, -2.1427],
         [-1.3020, -1.3020, -1.3020,  ..., -2.8180, -2.8180, -2.8180],
         [-1.3872, -1.3872, -1.3872,  ..., -2.9763, -2.9763, -2.9763]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that tastes spicy in this image? Please output segmentation mask. ASSISTANT: something that tastes spicy</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[ -6.0625,  -6.0625,  -6.0859,  ...,  -7.4453,  -7.5312,  -7.5312],
         [ -6.0625,  -6.0625,  -6.0859,  ...,  -7.4453,  -7.5312,  -7.5312],
         [ -6.0508,  -6.0508,  -6.0928,  ...,  -7.4199,  -7.5000,  -7.5000],
         ...,
         [-10.7891, -10.7891, -11.2393,  ...,  -9.4658,  -9.5547,  -9.5547],
         [-10.7344, -10.7344, -11.1396,  ...,  -9.6562,  -9.7422,  -9.7422],
         [-11.0781, -11.0781, -11.3408,  ..., -10.1562, -10.2266, -10.2266]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When going fishing on a calm sea, what type of boat shown in the picture would be an ideal choice for a peaceful fishing experience? Please output segmentation mask. ASSISTANT: when going fishing on a calm sea, what type of boat shown in the picture would be an ideal choice for a peaceful fishing experience</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-4.9893, -4.8309, -4.9432,  ..., -6.8572, -7.5267, -8.3138],
         [-4.5730, -5.1531, -5.2547,  ..., -8.4578, -8.8222, -9.1810],
         [-4.9029, -5.5797, -5.4062,  ..., -8.1875, -8.2438, -8.5875],
         ...,
         [-1.4094, -1.5812, -1.5977,  ..., -1.5059, -1.5480, -1.6850],
         [-1.6040, -1.8645, -1.8773,  ..., -1.6488, -1.6538, -1.8595],
         [-2.4840, -2.8430, -2.9028,  ..., -2.8549, -2.7475, -2.6948]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Insects have various ways to protect themselves from predators. What characteristics can a moth use to deter potential threats? Please output segmentation mask. ASSISTANT: insects have various ways to protect themselves from predators. what characteristics can a moth use to deter potential threats</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[77]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3872, 2592])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3872, 2592])):  [tensor([[[-7.8438, -7.8438, -7.8438,  ..., -9.7087, -9.7188, -9.7188],
         [-7.8438, -7.8438, -7.8438,  ..., -9.7087, -9.7188, -9.7188],
         [-7.8438, -7.8438, -7.8438,  ..., -9.7087, -9.7188, -9.7188],
         ...,
         [-8.5625, -8.5625, -8.5625,  ..., -7.9339, -7.9766, -7.9766],
         [-8.5625, -8.5625, -8.5625,  ..., -7.9339, -7.9766, -7.9766],
         [-8.5625, -8.5625, -8.5625,  ..., -7.9339, -7.9766, -7.9766]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Fishing is a popular activity for relaxation and leisure. What tool is the man in the picture using to catch fish? Please output segmentation mask. ASSISTANT: fishing is a popular activity for relaxation and leisure. what tool is the man in the picture using to catch fish</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]

 30%|███████████████████████████████████████████▉                                                                                                    | 61/200 [00:16<00:42,  3.26it/s]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1920, 2560])):  [tensor([[[-6.3750, -6.3750, -6.3750,  ..., -7.1875, -7.1875, -7.1875],
         [-6.3750, -6.3750, -6.3750,  ..., -7.1875, -7.1875, -7.1875],
         [-6.3750, -6.3750, -6.3750,  ..., -7.1875, -7.1875, -7.1875],
         ...,
         [-6.1094, -6.1094, -6.1094,  ..., -6.4766, -6.4766, -6.4766],
         [-6.2906, -6.2906, -6.2906,  ..., -6.7547, -6.7547, -6.7547],
         [-6.3359, -6.3359, -6.3359,  ..., -6.8242, -6.8242, -6.8242]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n During a meal, people typically use utensils to bring food to their mouths. What tool in the picture can be used to fulfill this need? Please output segmentation mask. ASSISTANT: during a meal, people typically use utensils to bring food to their mouths. what tool in the picture can be used to fulfill this need</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-5.5938, -5.5970, -5.6125,  ..., -5.9375, -5.9375, -5.9375],
         [-5.5790, -5.5861, -5.6199,  ..., -5.9593, -5.9589, -5.9588],
         [-5.5094, -5.5348, -5.6547,  ..., -6.0622, -6.0599, -6.0594],
         ...,
         [-5.0500, -5.1099, -5.3922,  ..., -6.2650, -6.3841, -6.4094],
         [-5.3025, -5.3629, -5.6475,  ..., -6.4185, -6.5668, -6.5983],
         [-5.7500, -5.7910, -5.9846,  ..., -6.8127, -6.9224, -6.9456]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n During the move, what object can be used to store and transport various sundries and small household items, which is sturdy and relatively easy to carry? Please output segmentation mask. ASSISTANT: during the move, what object can be used to store and transport various sundries and small household items, which is sturdy and relatively easy to carry</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-4.5681e+00, -4.4908e+00, -4.4705e+00,  ..., -5.8373e+00,
          -6.3948e+00, -6.8833e+00],
         [-4.9634e+00, -5.3587e+00, -5.4266e+00,  ..., -7.2016e+00,
          -7.6559e+00, -7.8343e+00],
         [-5.1635e+00, -5.5609e+00, -5.5000e+00,  ..., -7.0938e+00,
          -7.2891e+00, -7.4502e+00],
         ...,
         [ 1.2313e+00,  1.2797e+00,  1.2852e+00,  ..., -3.7070e+00,
          -3.9789e+00, -4.1454e+00],
         [ 9.3321e-01,  8.4795e-01,  8.9013e-01,  ..., -3.6961e+00,
          -3.9917e+00, -4.3237e+00],
         [-2.6561e-03, -7.8154e-02, -1.4633e-01,  ..., -3.9888e+00,
          -4.1393e+00, -4.0539e+00]]], device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If we were to take a flight in the sky, what part of the plane in the picture would be spinning to generate lift and propel the aircraft forward? Please output segmentation mask. ASSISTANT: if we were to take a flight in the sky, what part of the plane in the picture would be spinning to generate lift and propel the aircraft forward</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2499, 3680])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2499, 3680])):  [tensor([[[-10.6875, -10.6875, -10.6875,  ...,  -9.9375,  -9.9375,  -9.9375],
         [-10.6875, -10.6875, -10.6875,  ...,  -9.9375,  -9.9375,  -9.9375],
         [-10.6875, -10.6875, -10.6875,  ...,  -9.9375,  -9.9375,  -9.9375],
         ...,
         [-10.2726, -10.2726, -10.2726,  ..., -11.6301, -11.6301, -11.6301],
         [-10.3672, -10.3672, -10.3672,  ..., -11.7109, -11.7109, -11.7109],
         [-10.3672, -10.3672, -10.3672,  ..., -11.7109, -11.7109, -11.7109]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something used by a Jedi warrior to attack enemies in this image? Please output segmentation mask. ASSISTANT: something used by a jedi warrior to attack enemies</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]

(PLUM.py) >> pred_masks (shape: torch.Size([1, 1023, 1392])):  [tensor([[[-1.7891, -1.7891, -1.7622,  ..., -5.8059, -5.8125, -5.8125],04],],
         [-1.7891, -1.7891, -1.7622,  ..., -5.8059, -5.8125, -5.8125],
         [-1.7837, -1.7837, -1.7575,  ..., -5.8034, -5.8098, -5.8098],
         ...,
         [-5.9047, -5.9047, -5.9454,  ..., -6.3429, -6.3535, -6.3535],
         [-5.6460, -5.6460, -5.6713,  ..., -5.9979, -6.0027, -6.0027],
         [-5.4336, -5.4336, -5.4463,  ..., -5.7148, -5.7148, -5.7148]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the part that can transmit traction and braking torque, coming into contact with the ground in this image? Please output segmentation mask. ASSISTANT: the part that can transmit traction and braking torque, coming into contact with the ground</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 752, 1169])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 752, 1169])):  [tensor([[[-5.1562, -5.1562, -5.1751,  ..., -5.9386, -5.9062, -5.9062],
         [-5.1562, -5.1562, -5.1751,  ..., -5.9386, -5.9062, -5.9062],
         [-5.1401, -5.1401, -5.1680,  ..., -5.9839, -5.9548, -5.9548],
         ...,
         [-5.2796, -5.2796, -5.3235,  ..., -5.1352, -5.1799, -5.1799],
         [-4.9078, -4.9078, -4.9271,  ..., -4.7909, -4.8261, -4.8261],
         [-4.8633, -4.8633, -4.8724,  ..., -4.7474, -4.7656, -4.7656]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the area where people can stand without getting wet in this image? Please output segmentation mask. ASSISTANT: the area where people can stand without getting wet</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-1.8594, -1.8581, -1.8523,  ..., -1.7625, -1.6620, -1.6406],
         [-1.8188, -1.8190, -1.8204,  ..., -1.7221, -1.6261, -1.6058],
         [-1.6273, -1.6347, -1.6695,  ..., -1.5316, -1.4572, -1.4414],
         ...,
         [-1.2438, -1.2412, -1.2295,  ..., -1.3523, -1.4213, -1.4359],
         [-1.4647, -1.4671, -1.4785,  ..., -1.8268, -1.9284, -1.9499],
         [-2.4412, -2.4487, -2.4840,  ..., -3.2303, -3.3836, -3.4161]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a motorcycle race, there are often sharp turns that require skilled maneuvering. What part of the race track in the picture indicates a sharp turn? Please output segmentation mask. ASSISTANT: in a motorcycle race, there are often sharp turns that require skilled maneuvering. what part of the race track in the picture indicates a sharp turn</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 682, 1023])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [0., 0., 0.,  ..., 1., 1., 1.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 682, 1023])):  [tensor([[[-1.3672, -1.3672, -1.4035,  ..., -1.9737, -1.9610, -1.9609],
         [-1.3671, -1.3672, -1.4035,  ..., -1.9737, -1.9610, -1.9610],
         [-1.3495, -1.3495, -1.3849,  ..., -1.9909, -1.9836, -1.9836],
         ...,
         [-1.8538, -1.8537, -1.8358,  ..., -1.3296, -1.3260, -1.3260],
         [-2.0552, -2.0552, -2.0276,  ..., -1.5056, -1.4991, -1.4991],
         [-2.3299, -2.3298, -2.2927,  ..., -1.7404, -1.7215, -1.7215]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the region showing current time in this image? Please output segmentation mask. ASSISTANT: the region showing current time</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-3.0000, -3.0000, -2.9922,  ..., -2.8691, -2.8750, -2.8750],
         [-3.0000, -3.0000, -2.9922,  ..., -2.8691, -2.8750, -2.8750],
         [-3.0742, -3.0742, -3.0752,  ..., -3.0481, -3.0352, -3.0352],
         ...,
         [-2.9199, -2.9199, -2.9658,  ..., -2.7839, -2.7402, -2.7402],
         [-2.9004, -2.9004, -2.9429,  ..., -2.7717, -2.7324, -2.7324],
         [-2.9512, -2.9512, -2.9849,  ..., -3.1238, -3.1035, -3.1035]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the reflection of the object in this image? Please output segmentation mask. ASSISTANT: the reflection of the object</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2820, 3760])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2820, 3760])):  [tensor([[[-5.6250, -5.6250, -5.6250,  ..., -6.9375, -6.9375, -6.9375],
         [-5.6250, -5.6250, -5.6250,  ..., -6.9375, -6.9375, -6.9375],
         [-5.6250, -5.6250, -5.6250,  ..., -6.9375, -6.9375, -6.9375],
         ...,
         [-4.0452, -4.0452, -4.0452,  ..., -4.9578, -4.9578, -4.9578],
         [-4.2656, -4.2656, -4.2656,  ..., -5.2305, -5.2305, -5.2305],
         [-4.2656, -4.2656, -4.2656,  ..., -5.2305, -5.2305, -5.2305]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Dogs use their mouths to perform various tasks, including eating and vocalizing. What part of the dog's body is primarily responsible for these actions? Please output segmentation mask. ASSISTANT: dogs use their mouths to perform various tasks, including eating and vocalizing. what part of the dog's body is primarily responsible for these actions</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]

         [-3.8502, -4.6344, -4.6953,  ..., -4.3164, -4.2023, -4.2185],([[[-1.7891, -1.7891, -1.7622,  ..., -5.8059, -5.8125, -5.8125],04],],
         ...,
         [-5.1730, -5.2375, -5.2812,  ..., -4.4219, -4.3094, -4.0516],
         [-5.5869, -5.5547, -5.6094,  ..., -4.3188, -4.1141, -3.9744],
         [-5.7487, -5.9770, -5.8084,  ..., -4.7055, -4.4839, -4.4084]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the camera lens that is more suitable for photographing nearby objects in this image? Please output segmentation mask. ASSISTANT: the camera lens that is more suitable for photographing nearby objects</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1620, 2160])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1620, 2160])):  [tensor([[[-6.8438, -6.8438, -6.8438,  ..., -7.7188, -7.7188, -7.7188],
         [-6.8438, -6.8438, -6.8438,  ..., -7.7188, -7.7188, -7.7188],
         [-6.8438, -6.8438, -6.8438,  ..., -7.7188, -7.7188, -7.7188],
         ...,
         [-7.4716, -7.4716, -7.4716,  ..., -6.9051, -6.9051, -6.9051],
         [-7.7642, -7.7642, -7.7642,  ..., -7.4680, -7.4680, -7.4680],
         [-7.8945, -7.8945, -7.8945,  ..., -7.7188, -7.7188, -7.7188]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the part of the house that can be opened in this image? Please output segmentation mask. ASSISTANT: the part of the house that can be opened</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[-4.7188, -4.7188, -4.7188,  ..., -7.0625, -7.0625, -7.0625],
         [-4.7188, -4.7188, -4.7188,  ..., -7.0625, -7.0625, -7.0625],
         [-4.7188, -4.7188, -4.7188,  ..., -7.0625, -7.0625, -7.0625],
         ...,
         [-2.9944, -2.9944, -2.9944,  ..., -4.6460, -4.6460, -4.6460],
         [-3.3060, -3.3060, -3.3060,  ..., -5.3907, -5.3907, -5.3907],
         [-3.4277, -3.4277, -3.4277,  ..., -5.6816, -5.6816, -5.6816]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the source of power for the ship in this image? Please output segmentation mask. ASSISTANT: the source of power for the ship</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[ -8.4375,  -8.4375,  -8.4375,  ...,  -8.3750,  -8.3750,  -8.3750],
         [ -8.4375,  -8.4375,  -8.4375,  ...,  -8.3750,  -8.3750,  -8.3750],
         [ -8.4375,  -8.4375,  -8.4375,  ...,  -8.3750,  -8.3750,  -8.3750],
         ...,
         [ -9.3789,  -9.3789,  -9.3789,  ...,  -8.2617,  -8.2617,  -8.2617],
         [ -9.9648,  -9.9648,  -9.9648,  ...,  -9.2695,  -9.2695,  -9.2695],
         [-10.2578, -10.2578, -10.2578,  ...,  -9.7734,  -9.7734,  -9.7734]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When hanging laundry outside to dry, what material in the picture would be suitable for holding the clothes securely in place? Please output segmentation mask. ASSISTANT: when hanging laundry outside to dry, what material in the picture would be suitable for holding the clothes securely in place</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2592, 3872])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2592, 3872])):  [tensor([[[-7.8125, -7.8125, -7.8125,  ..., -7.7188, -7.7188, -7.7188],
         [-7.8125, -7.8125, -7.8125,  ..., -7.7188, -7.7188, -7.7188],
         [-7.8125, -7.8125, -7.8125,  ..., -7.7188, -7.7188, -7.7188],
         ...,
         [-7.6504, -7.6504, -7.6504,  ..., -6.7072, -6.7072, -6.7072],
         [-7.6328, -7.6328, -7.6328,  ..., -6.6055, -6.6055, -6.6055],
         [-7.6328, -7.6328, -7.6328,  ..., -6.6055, -6.6055, -6.6055]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the friend of humans in this image? Please output segmentation mask. ASSISTANT: the friend of humans</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]

(PLUM.py) >> pred_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[-1.5000, -1.5000, -1.5000,  ..., -1.8984, -1.8984, -1.8984],04],],
         [-1.5000, -1.5000, -1.5000,  ..., -1.8984, -1.8984, -1.8984],
         [-1.5000, -1.5000, -1.5000,  ..., -1.8984, -1.8984, -1.8984],
         ...,
         [-2.3294, -2.3294, -2.3294,  ..., -2.6485, -2.6485, -2.6485],
         [-2.4945, -2.4945, -2.4945,  ..., -2.9016, -2.9016, -2.9016],
         [-2.5332, -2.5332, -2.5332,  ..., -2.9609, -2.9609, -2.9609]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Soldiers are often equipped with various tools and weapons to carry out their duties. What item in the picture can be classified as a weapon? Please output segmentation mask. ASSISTANT: soldiers are often equipped with various tools and weapons to carry out their duties. what item in the picture can be classified as a weapon</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 300, 400])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 300, 400])):  [tensor([[[-11.3750, -10.9725, -10.6234,  ...,  -7.9680,  -8.2312,  -8.3750],
         [-10.7712, -11.0697, -11.4080,  ...,  -8.0251,  -8.2732,  -8.4038],
         [-10.1773, -11.1663, -12.1820,  ...,  -7.9653,  -8.1634,  -8.2609],
         ...,
         [-11.3250, -11.9571, -12.4809,  ..., -13.5237, -13.5343, -13.5602],
         [-11.1338, -11.9859, -12.6502,  ..., -12.2401, -12.2548, -12.4388],
         [-11.1225, -11.9223, -12.5293,  ..., -11.5405, -11.6358, -11.9262]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What object is used to cover the entrance of the bathroom and ensure privacy? Please output segmentation mask. ASSISTANT: what object is used to cover the entrance of the bathroom and ensure privacy</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-5.9909, -5.7052, -5.5691,  ..., -4.0985, -4.6454, -5.3965],
         [-5.7911, -6.3325, -5.9344,  ..., -4.7914, -4.9053, -5.6122],
         [-5.9445, -6.3313, -6.1094,  ..., -5.5859, -5.7625, -5.8270],
         ...,
         [-7.3393, -7.9516, -8.1562,  ..., -8.3281, -8.5281, -9.2371],
         [-7.5857, -8.3441, -8.6156,  ..., -8.3531, -8.3181, -9.4482],
         [-8.1253, -9.3445, -9.2629,  ..., -8.6449, -8.5614, -9.1641]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the person in the air in this image? Please output segmentation mask. ASSISTANT: the person in the air</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3034, 2148])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3034, 2148])):  [tensor([[[-13.3750, -13.3750, -13.3750,  ...,  -8.8169,  -8.7589,  -8.7578],
         [-13.3750, -13.3750, -13.3750,  ...,  -8.8169,  -8.7589,  -8.7578],
         [-13.3750, -13.3750, -13.3750,  ...,  -8.8169,  -8.7589,  -8.7578],
         ...,
         [-10.5625, -10.5625, -10.5625,  ..., -10.0014, -10.2071, -10.2109],
         [-10.5625, -10.5625, -10.5625,  ..., -10.0014, -10.2071, -10.2109],
         [-10.5625, -10.5625, -10.5625,  ..., -10.0014, -10.2071, -10.2109]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is a place where bees can suck nectar from flowers in this image? Please output segmentation mask. ASSISTANT: a place where bees can suck nectar from flowers</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 566, 939])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 566, 939])):  [tensor([[[-1.9609, -1.9568, -1.9170,  ..., -1.7890, -1.7466, -1.7422],
         [-1.9595, -1.9555, -1.9172,  ..., -1.7955, -1.7508, -1.7461],
         [-1.9454, -1.9429, -1.9191,  ..., -1.8593, -1.7917, -1.7847],
         ...,
         [-2.4016, -2.4070, -2.4594,  ..., -2.5761, -2.5244, -2.5191],
         [-2.3743, -2.3803, -2.4387,  ..., -2.3139, -2.2745, -2.2705],
         [-2.3487, -2.3551, -2.4170,  ..., -2.0134, -1.9897, -1.9873]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n I want to take a trip around the world, but I need some transportation to help me cross the oceans. What type of transportation in the picture can fulfill this requirement? Please output segmentation mask. ASSISTANT: i want to take a trip around the world, but i need some transportation to help me cross the oceans. what type of transportation in the picture can fulfill this requirement</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[90]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 960, 1280])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 960, 1280])):  [tensor([[[-1.3828, -1.3828, -1.3726,  ..., -4.1094, -4.1250, -4.1250],
         [-1.3828, -1.3828, -1.3726,  ..., -4.1094, -4.1250, -4.1250],
         [-1.3921, -1.3921, -1.3833,  ..., -4.1708, -4.1875, -4.1875],
         ...,
         [-7.3613, -7.3613, -7.4752,  ..., -1.9631, -1.9639, -1.9639],
         [-7.2625, -7.2625, -7.3586,  ..., -2.9297, -2.9312, -2.9312],
         [-7.1367, -7.1367, -7.2126,  ..., -4.1174, -4.1221, -4.1221]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is where we can see the speed of the car in this image? Please output segmentation mask. ASSISTANT: where we can see the speed of the car</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]:  [tensor([[[-1.5000, -1.5000, -1.5000,  ..., -1.8984, -1.8984, -1.8984],04],],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 2304])):  [tensor([[[-8.2500, -8.2500, -8.2500,  ..., -7.1008, -7.1328, -7.1328],
         [-8.2500, -8.2500, -8.2500,  ..., -7.1008, -7.1328, -7.1328],
         [-8.2500, -8.2500, -8.2500,  ..., -7.1008, -7.1328, -7.1328],
         ...,
         [-8.8750, -8.8750, -8.8750,  ..., -4.0598, -4.1934, -4.1934],
         [-8.8750, -8.8750, -8.8750,  ..., -4.0598, -4.1934, -4.1934],
         [-8.8750, -8.8750, -8.8750,  ..., -4.0598, -4.1934, -4.1934]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When visiting a library or bookstore, people often browse through the shelves to find interesting books to read. Which area in the picture could provide a variety of reading materials for visitors? Please output segmentation mask. ASSISTANT: when visiting a library or bookstore, people often browse through the shelves to find interesting books to read. which area in the picture could provide a variety of reading materials for visitors</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[91]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 4000, 2683])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 4000, 2683])):  [tensor([[[-4.1250, -4.1250, -4.1250,  ..., -3.3287, -3.3457, -3.3457],
         [-4.1250, -4.1250, -4.1250,  ..., -3.3287, -3.3457, -3.3457],
         [-4.1250, -4.1250, -4.1250,  ..., -3.3287, -3.3457, -3.3457],
         ...,
         [-2.7969, -2.7969, -2.7969,  ..., -2.2577, -2.2676, -2.2676],
         [-2.7969, -2.7969, -2.7969,  ..., -2.2577, -2.2676, -2.2676],
         [-2.7969, -2.7969, -2.7969,  ..., -2.2577, -2.2676, -2.2676]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the person who appears to have already won in the battle in this image? Please output segmentation mask. ASSISTANT: the person who appears to have already won in the battle</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1504, 2256])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1504, 2256])):  [tensor([[[-5.8750, -5.8750, -5.8750,  ..., -6.5000, -6.5000, -6.5000],
         [-5.8750, -5.8750, -5.8750,  ..., -6.5000, -6.5000, -6.5000],
         [-5.8750, -5.8750, -5.8750,  ..., -6.5000, -6.5000, -6.5000],
         ...,
         [-6.0002, -6.0002, -6.0002,  ..., -5.2887, -5.2887, -5.2887],
         [-6.0978, -6.0978, -6.0978,  ..., -5.2638, -5.2638, -5.2638],
         [-6.1367, -6.1367, -6.1367,  ..., -5.2539, -5.2539, -5.2539]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a healthy meal, vegetables are often included to provide essential nutrients. What in the picture can be used to eat the vegetables? Please output segmentation mask. ASSISTANT: in a healthy meal, vegetables are often included to provide essential nutrients. what in the picture can be used to eat the vegetables</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[83]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 854, 1280])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 854, 1280])):  [tensor([[[-3.8438, -3.8438, -3.8438,  ..., -3.4424, -3.4375, -3.4375],
         [-3.8438, -3.8438, -3.8438,  ..., -3.4424, -3.4375, -3.4375],
         [-3.8730, -3.8730, -3.8772,  ..., -3.4776, -3.4726, -3.4726],
         ...,
         [-4.5389, -4.5389, -4.5843,  ..., -4.1337, -4.1171, -4.1171],
         [-4.4770, -4.4770, -4.5188,  ..., -4.0265, -4.0082, -4.0082],
         [-4.6328, -4.6328, -4.6711,  ..., -4.0234, -4.0000, -4.0000]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that prevents people from getting into the building in this image? Please output segmentation mask. ASSISTANT: something that prevents people from getting into the building</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         ...,
         [255., 255., 255.,  ..., 255., 255., 255.],
         [255., 255., 255.,  ..., 255., 255., 255.],
         [255., 255., 255.,  ..., 255., 255., 255.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-10.8750, -10.8520, -10.7438,  ..., -11.2187, -11.4508, -11.5000],
         [-10.9472, -10.9277, -10.8356,  ..., -11.5255, -11.7210, -11.7625],
         [-11.2875, -11.2842, -11.2687,  ..., -12.9719, -12.9951, -13.0000],
         ...,
         [  1.3508,   1.3359,   1.2655,  ...,   2.7637,   2.9161,   2.9484],
         [  0.8259,   0.8137,   0.7563,  ...,   2.4777,   2.6503,   2.6869],
         [ -0.3511,  -0.3595,  -0.3990,  ...,   0.8251,   0.9991,   1.0360]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that the monkey uses to pierce its food in this image? Please output segmentation mask. ASSISTANT: something that the monkey uses to pierce its food</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]

         [-3.0156, -3.0156, -3.0156,  ..., -3.4375, -3.4375, -3.4375],([[[-1.5000, -1.5000, -1.5000,  ..., -1.8984, -1.8984, -1.8984],04],],
         ...,
         [-3.6676, -3.6676, -3.6676,  ..., -4.3097, -4.3097, -4.3097],
         [-3.7912, -3.7912, -3.7912,  ..., -4.4858, -4.4858, -4.4858],
         [-3.8066, -3.8066, -3.8066,  ..., -4.5078, -4.5078, -4.5078]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In an orchestra, musicians play different instruments. What object in the picture is commonly played with a bow to produce sound? Please output segmentation mask. ASSISTANT: in an orchestra, musicians play different instruments. what object in the picture is commonly played with a bow to produce sound</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[77]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 280, 330])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 280, 330])):  [tensor([[[-8.8670, -8.7131, -8.8760,  ..., -8.7519, -8.9804, -8.5742],
         [-8.2200, -9.1191, -9.2613,  ..., -9.1410, -9.2975, -9.3503],
         [-7.9389, -8.8527, -9.1752,  ..., -9.0739, -8.9909, -8.9534],
         ...,
         [-4.4237, -4.5501, -4.3979,  ..., -7.2873, -6.8700, -6.8243],
         [-4.0376, -4.1863, -3.8219,  ..., -6.0918, -6.1903, -6.1748],
         [-4.4956, -4.7973, -4.7089,  ..., -5.9585, -6.4946, -6.3345]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What object in a typical modern kitchen can be used to quickly heat up leftovers, boil water, or cook frozen food? Please output segmentation mask. ASSISTANT: what object in a typical modern kitchen can be used to quickly heat up leftovers, boil water, or cook frozen food</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[80]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-11.8887, -12.0423, -11.7551,  ...,  -6.4652,  -7.2998,  -7.9019],
         [-12.5150, -14.0275, -13.7531,  ...,  -7.1312,  -7.6350,  -8.2666],
         [-12.9035, -14.4719, -14.0156,  ...,  -7.0859,  -6.9953,  -7.4143],
         ...,
         [-10.4102, -12.3438, -11.6562,  ...,  -7.5547,  -8.1281,  -8.6008],
         [ -9.9845, -12.2275, -11.2906,  ...,  -7.1094,  -7.2553,  -7.7258],
         [ -9.7130, -11.5032, -10.8152,  ...,  -7.9385,  -8.0024,  -8.3043]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When plants grow excessively in unwanted areas, it is necessary to remove them to maintain the aesthetics of the surroundings. What part of the plants in the picture may need to be removed in this situation? Please output segmentation mask. ASSISTANT: when plants grow excessively in unwanted areas, it is necessary to remove them to maintain the aesthetics of the surroundings. what part of the plants in the picture may need to be removed in this situation</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[97]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 853, 1280])):  [tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 853, 1280])):  [tensor([[[ 0.0103,  0.0103,  0.0284,  ..., -2.2300, -2.2500, -2.2500],
         [ 0.0103,  0.0103,  0.0284,  ..., -2.2300, -2.2500, -2.2500],
         [ 0.0351,  0.0351,  0.0519,  ..., -2.2147, -2.2325, -2.2325],
         ...,
         [-0.6456, -0.6456, -0.6546,  ..., -0.3323, -0.3167, -0.3167],
         [-0.8905, -0.8905, -0.8978,  ..., -0.5466, -0.5385, -0.5385],
         [-1.1047, -1.1047, -1.1106,  ..., -0.7339, -0.7324, -0.7324]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a television studio, various equipment is used to capture and record video footage. What in the picture could be used to stabilize and hold the camera steady during filming? Please output segmentation mask. ASSISTANT: in a television studio, various equipment is used to capture and record video footage. what in the picture could be used to stabilize and hold the camera steady during filming</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2573, 3593])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2573, 3593])):  [tensor([[[-9.0000, -9.0000, -9.0000,  ..., -9.2500, -9.2500, -9.2500],
         [-9.0000, -9.0000, -9.0000,  ..., -9.2500, -9.2500, -9.2500],
         [-9.0000, -9.0000, -9.0000,  ..., -9.2500, -9.2500, -9.2500],
         ...,
         [-7.6602, -7.6602, -7.6602,  ..., -7.4866, -7.4866, -7.4866],
         [-7.6055, -7.6055, -7.6055,  ..., -7.4883, -7.4883, -7.4883],
         [-7.6055, -7.6055, -7.6055,  ..., -7.4883, -7.4883, -7.4883]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n There are two washing machines as shown in the picture. If I need to do laundry, where in the picture would I put the clothes? Please output segmentation mask. ASSISTANT: there are two washing machines as shown in the picture. if i need to do laundry, where in the picture would i put the clothes</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[82]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3254, 2168])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3254, 2168])):  [tensor([[[-6.9375, -6.9375, -6.9375,  ..., -7.4298, -7.3984, -7.3984],
         [-6.9375, -6.9375, -6.9375,  ..., -7.4298, -7.3984, -7.3984],
         [-6.9375, -6.9375, -6.9375,  ..., -7.4298, -7.3984, -7.3984],
         ...,
         [-1.4297, -1.4297, -1.4297,  ..., -5.2364, -5.0820, -5.0820],
         [-1.4297, -1.4297, -1.4297,  ..., -5.2364, -5.0820, -5.0820],
         [-1.4297, -1.4297, -1.4297,  ..., -5.2364, -5.0820, -5.0820]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Insects are often found on or near trees, where they can find shelter and food. What part of the tree in this picture could insects commonly be found on or around? Please output segmentation mask. ASSISTANT: insects are often found on or near trees, where they can find shelter and food. what part of the tree in this picture could insects commonly be found on or around</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.],., -3.4375, -3.4375, -3.4375],([[[-1.5000, -1.5000, -1.5000,  ..., -1.8984, -1.8984, -1.8984],04],],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 5184])):  [tensor([[[-3.7344, -3.7344, -3.7344,  ..., -4.7188, -4.7188, -4.7188],
         [-3.7344, -3.7344, -3.7344,  ..., -4.7188, -4.7188, -4.7188],
         [-3.7344, -3.7344, -3.7344,  ..., -4.7188, -4.7188, -4.7188],
         ...,
         [-0.3002, -0.3002, -0.3002,  ..., -0.9697, -0.9697, -0.9697],
         [-0.3002, -0.3002, -0.3002,  ..., -0.9697, -0.9697, -0.9697],
         [-0.3002, -0.3002, -0.3002,  ..., -0.9697, -0.9697, -0.9697]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When the weather is very cold outside, what equipment do we need to use to increase the indoor temperature in the room? Please output segmentation mask. ASSISTANT: when the weather is very cold outside, what equipment do we need to use to increase the indoor temperature in the room</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-4.8215, -4.3915, -4.2748,  ..., -4.0146, -4.4212, -4.7391],
         [-4.0478, -4.7847, -4.7797,  ..., -4.3852, -4.3825, -4.4641],
         [-4.2895, -4.7406, -4.6406,  ..., -4.5781, -4.3789, -4.1372],
         ...,
         [-3.3925, -3.4945, -2.7520,  ..., -3.2656, -3.4062, -3.2988],
         [-3.3621, -3.4233, -2.9688,  ..., -3.2219, -3.2089, -3.3367],
         [-4.2485, -4.4005, -3.9068,  ..., -4.0636, -4.0292, -4.0266]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If we wanted to identify the specific vehicle shown in the picture, what part of the car should we look at? Please output segmentation mask. ASSISTANT: if we wanted to identify the specific vehicle shown in the picture, what part of the car should we look at</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[74]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 800, 1200])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 800, 1200])):  [tensor([[[-7.0000, -7.0000, -6.9802,  ..., -7.7624, -7.7500, -7.7500],
         [-7.0000, -7.0000, -6.9802,  ..., -7.7624, -7.7500, -7.7500],
         [-6.9579, -6.9579, -6.9536,  ..., -7.7958, -7.7797, -7.7797],
         ...,
         [-5.6581, -5.6581, -5.7329,  ..., -5.3993, -5.4254, -5.4254],
         [-5.5257, -5.5257, -5.5889,  ..., -5.1624, -5.1806, -5.1806],
         [-5.7422, -5.7422, -5.7948,  ..., -5.1535, -5.1562, -5.1562]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something hot and light in this image? Please output segmentation mask. ASSISTANT: something hot and light</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 616, 816])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 616, 816])):  [tensor([[[-5.6562, -5.6682, -5.7273,  ..., -5.9825, -6.0491, -6.0625],
         [-5.6219, -5.6368, -5.7108,  ..., -5.9775, -6.0557, -6.0715],
         [-5.4519, -5.4817, -5.6291,  ..., -5.9525, -6.0884, -6.1158],
         ...,
         [-4.8454, -4.8616, -4.9415,  ..., -5.6889, -5.8818, -5.9207],
         [-4.7882, -4.7993, -4.8545,  ..., -5.5866, -5.6923, -5.7137],
         [-5.1117, -5.1303, -5.2227,  ..., -5.7526, -5.7447, -5.7431]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is objects that can help women appear taller in this image? Please output segmentation mask. ASSISTANT: objects that can help women appear taller</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 465, 500])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 465, 500])):  [tensor([[[-5.6562, -6.0750, -6.8750,  ..., -5.0819, -5.4179, -5.5938],
         [-5.8654, -6.4501, -7.5671,  ..., -5.5396, -5.8113, -5.9535],
         [-6.2653, -7.1672, -8.8902,  ..., -6.4145, -6.5634, -6.6413],
         ...,
         [-0.5173, -0.4286, -0.2593,  ...,  1.0999,  1.1317,  1.1483],
         [-0.5393, -0.6346, -0.8167,  ...,  0.7977,  0.8848,  0.9304],
         [-1.4461, -1.6399, -2.0103,  ...,  0.4675,  0.5110,  0.5337]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What container in the picture is used for arranging the flowers to make them look more beautiful? Please output segmentation mask. ASSISTANT: what container in the picture is used for arranging the flowers to make them look more beautiful</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]

         ..., 0., 0.,  ..., 0., 0., 0.],., -3.4375, -3.4375, -3.4375],([[[-1.5000, -1.5000, -1.5000,  ..., -1.8984, -1.8984, -1.8984],04],],
         [ -9.1444,  -9.3483, -10.0682,  ...,  -6.6128,  -6.9551,  -7.0519],
         [ -9.0836,  -9.3036, -10.0803,  ...,  -6.4874,  -6.7931,  -6.8797],
         [-10.4263, -10.5436, -10.9575,  ...,  -8.8508,  -9.2265,  -9.3328]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the container that contains the vegetable in this image? Please output segmentation mask. ASSISTANT: the container that contains the vegetable</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1568, 2352])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1568, 2352])):  [tensor([[[-0.9062, -0.9062, -0.9062,  ..., -4.0312, -4.0312, -4.0312],
         [-0.9062, -0.9062, -0.9062,  ..., -4.0312, -4.0312, -4.0312],
         [-0.9062, -0.9062, -0.9062,  ..., -4.0312, -4.0312, -4.0312],
         ...,
         [-0.7768, -0.7768, -0.7768,  ..., -2.7876, -2.7876, -2.7876],
         [-0.8577, -0.8577, -0.8577,  ..., -2.7757, -2.7757, -2.7757],
         [-0.8862, -0.8862, -0.8862,  ..., -2.7715, -2.7715, -2.7715]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In the animal kingdom, the males of certain species have a distinctive feature that sets them apart from the females. What part of the lion in this picture is a defining characteristic of male lions? Please output segmentation mask. ASSISTANT: in the animal kingdom, the males of certain species have a distinctive feature that sets them apart from the females. what part of the lion in this picture is a defining characteristic of male lions</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[92]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[-3.5625, -3.5674, -3.5906,  ..., -5.0187, -5.0806, -5.0938],
         [-3.5568, -3.5628, -3.5910,  ..., -5.0340, -5.0955, -5.1085],
         [-3.5297, -3.5408, -3.5930,  ..., -5.1059, -5.1654, -5.1780],
         ...,
         [-3.5628, -3.5874, -3.7034,  ..., -6.0165, -6.1572, -6.1871],
         [-3.8627, -3.8873, -4.0033,  ..., -5.6037, -5.6726, -5.6872],
         [-4.1626, -4.1872, -4.3032,  ..., -5.1908, -5.1880, -5.1874]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n During times of war, armored vehicles are commonly used to protect soldiers and engage in combat. What object in the picture can provide such protection? Please output segmentation mask. ASSISTANT: during times of war, armored vehicles are commonly used to protect soldiers and engage in combat. what object in the picture can provide such protection</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[ -7.9688,  -7.9688,  -7.9688,  ..., -10.0000, -10.0000, -10.0000],
         [ -7.9688,  -7.9688,  -7.9688,  ..., -10.0000, -10.0000, -10.0000],
         [ -7.9688,  -7.9688,  -7.9688,  ..., -10.0000, -10.0000, -10.0000],
         ...,
         [ -3.3379,  -3.3379,  -3.3379,  ...,  -7.1367,  -7.1367,  -7.1367],
         [ -4.0840,  -4.0840,  -4.0840,  ...,  -7.7695,  -7.7695,  -7.7695],
         [ -4.4570,  -4.4570,  -4.4570,  ...,  -8.0859,  -8.0859,  -8.0859]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n It is common for some bird species to live near bodies of water and rely on them as their primary habitat. What in the picture can be considered as the habitat for the birds mentioned? Please output segmentation mask. ASSISTANT: it is common for some bird species to live near bodies of water and rely on them as their primary habitat. what in the picture can be considered as the habitat for the birds mentioned</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1193, 1271])):  [tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 0., 0.],
         [1., 1., 1.,  ..., 1., 0., 0.],
         [1., 1., 1.,  ..., 1., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1193, 1271])):  [tensor([[[-0.4062, -0.4062, -0.3998,  ..., -1.0534, -1.0469, -1.0469],
         [-0.4062, -0.4062, -0.3998,  ..., -1.0534, -1.0469, -1.0469],
         [-0.3322, -0.3322, -0.3251,  ..., -0.9576, -0.9517, -0.9517],
         ...,
         [-1.8959, -1.8959, -1.9040,  ..., -0.7402, -0.7456, -0.7456],
         [-2.6087, -2.6087, -2.6237,  ..., -1.3790, -1.3899, -1.3899],
         [-3.2354, -3.2354, -3.2565,  ..., -1.9406, -1.9565, -1.9565]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In historical architecture, a building often has a grand entrance that consists of a large opening with a curved or pointed top. What feature in the picture resembles such an entrance? Please output segmentation mask. ASSISTANT: in historical architecture, a building often has a grand entrance that consists of a large opening with a curved or pointed top. what feature in the picture resembles such an entrance</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 576])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 576])):  [tensor([[[ 0.0461,  0.0677,  0.1610,  ...,  0.0117,  0.0945, -0.5690],
         [ 0.0391,  0.0639,  0.1714,  ...,  0.0207,  0.0961, -0.5594],
         [ 0.0088,  0.0478,  0.2168,  ...,  0.0598,  0.1033, -0.5178],
         ...,
         [-1.4453, -1.4166, -1.2921,  ..., -0.9193, -0.8713, -1.1971],
         [-1.4136, -1.3995, -1.3383,  ..., -0.8494, -0.7926, -1.0729],
         [-1.4062, -1.3955, -1.3490,  ..., -0.8333, -0.7744, -1.0443]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the people who are about to get on the vehicle in this image? Please output segmentation mask. ASSISTANT: the people who are about to get on the vehicle</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[67]]
>> (validate) sampled_classes_list:  [None]

         [ -5.4447,  -5.8422,  -5.7578,  ..., -12.6875, -12.6500, -13.4234],.5000, -1.5000, -1.5000,  ..., -1.8984, -1.8984, -1.8984],04],],
         [ -5.7790,  -6.1700,  -5.9906,  ..., -12.2687, -11.8987, -13.0417],
         [ -6.8347,  -7.2147,  -7.0254,  ..., -11.8781, -10.9326, -10.8998]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When eating scrambled eggs for breakfast, people often add a side dish made of potatoes. What item in the picture can be used to serve the potatoes? Please output segmentation mask. ASSISTANT: when eating scrambled eggs for breakfast, people often add a side dish made of potatoes. what item in the picture can be used to serve the potatoes</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1000, 667])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1000, 667])):  [tensor([[[-5.0312, -5.0349, -5.1450,  ..., -4.0513, -4.2432, -4.3619],
         [-5.0329, -5.0366, -5.1474,  ..., -4.0527, -4.2434, -4.3615],
         [-5.0838, -5.0881, -5.2195,  ..., -4.0952, -4.2469, -4.3501],
         ...,
         [-5.3500, -5.3595, -5.6452,  ..., -5.0819, -4.5227, -4.4758],
         [-5.4347, -5.4445, -5.7406,  ..., -4.9225, -4.4012, -4.3708],
         [-5.4375, -5.4473, -5.7437,  ..., -4.9172, -4.3972, -4.3673]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that can control the fan speed in this image? Please output segmentation mask. ASSISTANT: something that can control the fan speed</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2304, 3456])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2304, 3456])):  [tensor([[[-7.2188, -7.2188, -7.2187,  ..., -7.8125, -7.8125, -7.8125],
         [-7.2188, -7.2188, -7.2187,  ..., -7.8125, -7.8125, -7.8125],
         [-7.2188, -7.2188, -7.2187,  ..., -7.8125, -7.8125, -7.8125],
         ...,
         [-6.9546, -6.9546, -6.9546,  ..., -6.4652, -6.4652, -6.4652],
         [-6.9961, -6.9961, -6.9961,  ..., -6.4219, -6.4219, -6.4219],
         [-6.9961, -6.9961, -6.9961,  ..., -6.4219, -6.4219, -6.4219]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What structure separates two areas in the room and is commonly used to hold onto for support when going up and down? Please output segmentation mask. ASSISTANT: what structure separates two areas in the room and is commonly used to hold onto for support when going up and down</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[75]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[255.,   1.,   1.,  ...,   0.,   0.,   0.],
         [255.,   1.,   1.,  ...,   0.,   0.,   0.],
         [255., 255.,   1.,  ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-1.6687, -1.8489, -2.0580,  ..., -2.3921, -2.6086, -2.2386],
         [-1.7469, -2.0895, -2.2406,  ..., -2.0430, -2.1381, -2.0930],
         [-1.7520, -2.0770, -2.1152,  ..., -1.9531, -1.9629, -1.8420],
         ...,
         [-1.2646, -1.2109, -1.2480,  ..., -3.8125, -4.0102, -3.8544],
         [-1.1383, -1.1589, -1.2707,  ..., -3.9445, -4.2045, -4.0788],
         [-1.5028, -1.6745, -1.9970,  ..., -4.1672, -4.2691, -4.2028]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In urban areas, there are designated areas for bicycles to ride safely. What area in the picture would a cyclist use to navigate through the city? Please output segmentation mask. ASSISTANT: in urban areas, there are designated areas for bicycles to ride safely. what area in the picture would a cyclist use to navigate through the city</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[83]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[-4.2500, -4.2500, -4.2500,  ..., -5.7188, -5.7188, -5.7188],
         [-4.2500, -4.2500, -4.2500,  ..., -5.7188, -5.7188, -5.7188],
         [-4.2500, -4.2500, -4.2500,  ..., -5.7188, -5.7188, -5.7188],
         ...,
         [-2.6839, -2.6839, -2.6839,  ..., -1.1245, -1.1245, -1.1245],
         [-2.9307, -2.9307, -2.9307,  ..., -1.6099, -1.6099, -1.6099],
         [-2.9307, -2.9307, -2.9307,  ..., -1.6099, -1.6099, -1.6099]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a mechanical workshop, there are various machines and tools used for different purposes. What in the picture could be used to rotate or spin other parts or objects? Please output segmentation mask. ASSISTANT: in a mechanical workshop, there are various machines and tools used for different purposes. what in the picture could be used to rotate or spin other parts or objects</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1606, 1610])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1606, 1610])):  [tensor([[[-3.7344, -3.7344, -3.7321,  ..., -4.6513, -4.6562, -4.6562],
         [-3.7344, -3.7344, -3.7321,  ..., -4.6513, -4.6562, -4.6562],
         [-3.7319, -3.7319, -3.7298,  ..., -4.6566, -4.6615, -4.6615],
         ...,
         [-3.4899, -3.4899, -3.5064,  ..., -3.0947, -3.0986, -3.0986],
         [-3.4924, -3.4924, -3.5088,  ..., -2.9635, -2.9669, -2.9669],
         [-3.4941, -3.4941, -3.5105,  ..., -2.8698, -2.8730, -2.8730]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the boxes inside the refrigerator in this image? Please output segmentation mask. ASSISTANT: the boxes inside the refrigerator</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]

         [-2.3194, -2.3194, -2.3194,  ..., -0.4491, -0.4491, -0.4491],4234],.5000, -1.5000, -1.5000,  ..., -1.8984, -1.8984, -1.8984],04],],
         [-2.6698, -2.6698, -2.6698,  ..., -1.2132, -1.2132, -1.2132],
         [-2.8066, -2.8066, -2.8066,  ..., -1.5117, -1.5117, -1.5117]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In order to shape and carve hard materials, such as wood or stone, what part of the tool in the picture would be used to provide precise control and force? Please output segmentation mask. ASSISTANT: in order to shape and carve hard materials, such as wood or stone, what part of the tool in the picture would be used to provide precise control and force</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 790, 916])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 790, 916])):  [tensor([[[ -8.6875,  -8.6695,  -8.5261,  ...,  -8.6863,  -8.5207,  -8.5000],
         [ -8.6689,  -8.6529,  -8.5248,  ...,  -8.7219,  -8.5627,  -8.5428],
         [ -8.5200,  -8.5194,  -8.5151,  ...,  -9.0069,  -8.8983,  -8.8847],
         ...,
         [ -9.4604,  -9.5270, -10.0590,  ..., -10.3695, -10.5804, -10.6068],
         [ -8.1419,  -8.1631,  -8.3328,  ...,  -9.2952,  -9.4675,  -9.4891],
         [ -7.4777,  -7.4760,  -7.4625,  ...,  -8.7104,  -8.8338,  -8.8493]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In the picture, there is a legal requirement for vehicles to display identifying information. What part of the car is used to display this information? Please output segmentation mask. ASSISTANT: in the picture, there is a legal requirement for vehicles to display identifying information. what part of the car is used to display this information</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[80]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2762, 3948])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2762, 3948])):  [tensor([[[ -9.4375,  -9.4375,  -9.4375,  ..., -12.3125, -12.3125, -12.3125],
         [ -9.4375,  -9.4375,  -9.4375,  ..., -12.3125, -12.3125, -12.3125],
         [ -9.4375,  -9.4375,  -9.4375,  ..., -12.3125, -12.3125, -12.3125],
         ...,
         [-10.4479, -10.4479, -10.4479,  ...,  -9.0307,  -9.0307,  -9.0307],
         [-10.5312, -10.5312, -10.5312,  ...,  -9.1094,  -9.1094,  -9.1094],
         [-10.5312, -10.5312, -10.5312,  ...,  -9.1094,  -9.1094,  -9.1094]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the ship that is most likely to carry a fleet commander in this image? Please output segmentation mask. ASSISTANT: the ship that is most likely to carry a fleet commander</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2448, 3264])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2448, 3264])):  [tensor([[[-10.7500, -10.7500, -10.7500,  ..., -11.9375, -11.9375, -11.9375],
         [-10.7500, -10.7500, -10.7500,  ..., -11.9375, -11.9375, -11.9375],
         [-10.7500, -10.7500, -10.7500,  ..., -11.9375, -11.9375, -11.9375],
         ...,
         [ -5.5866,  -5.5866,  -5.5866,  ...,  -6.8766,  -6.8766,  -6.8766],
         [ -6.1074,  -6.1074,  -6.1074,  ...,  -7.5273,  -7.5273,  -7.5273],
         [ -6.1074,  -6.1074,  -6.1074,  ...,  -7.5273,  -7.5273,  -7.5273]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In outdoor recreational activities, people can experience the thrill of flying in the sky with the help of equipment. What object in the picture can provide this experience? Please output segmentation mask. ASSISTANT: in outdoor recreational activities, people can experience the thrill of flying in the sky with the help of equipment. what object in the picture can provide this experience</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[-5.2500, -5.2516, -5.2594,  ..., -5.1156, -5.0460, -5.0312],
         [-5.2156, -5.2205, -5.2437,  ..., -5.1671, -5.1023, -5.0886],
         [-5.0533, -5.0736, -5.1695,  ..., -5.4097, -5.3679, -5.3591],
         ...,
         [-2.7816, -2.7903, -2.8312,  ..., -3.1348, -3.1116, -3.1067],
         [-3.1314, -3.1317, -3.1331,  ..., -3.6221, -3.6051, -3.6015],
         [-3.4813, -3.4732, -3.4350,  ..., -4.1095, -4.0986, -4.0963]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n We are currently watching a game and it's halftime. Who are the cheerleaders who come out to liven up the atmosphere? Please output segmentation mask. ASSISTANT: we are currently watching a game and it's halftime. who are the cheerleaders who come out to liven up the atmosphere</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]

         [-8.5625, -8.5625, -8.5625,  ..., -7.6562, -7.6562, -7.6562],4234],.5000, -1.5000, -1.5000,  ..., -1.8984, -1.8984, -1.8984],04],],
         [-8.5625, -8.5625, -8.5625,  ..., -7.6562, -7.6562, -7.6562],
         ...,
         [-1.4246, -1.4246, -1.4246,  ..., -1.8147, -1.8147, -1.8147],
         [-1.5044, -1.5044, -1.5044,  ..., -1.8975, -1.8975, -1.8975],
         [-1.5044, -1.5044, -1.5044,  ..., -1.8975, -1.8975, -1.8975]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Generally speaking, dogs do not have horns on their heads, only a pair of ears. What part of the dog's head in this picture looks strange? Please output segmentation mask. ASSISTANT: generally speaking, dogs do not have horns on their heads, only a pair of ears. what part of the dog's head in this picture looks strange</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1533, 1039])):  [tensor([[[  0.,   0.,   0.,  ..., 255., 255.,   0.],
         [  0.,   0.,   0.,  ..., 255., 255.,   0.],
         [  0.,   0.,   0.,  ..., 255.,   0.,   0.],
         ...,
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1533, 1039])):  [tensor([[[-5.5938, -5.5938, -5.5984,  ..., -1.7082, -1.8687, -1.9893],
         [-5.5938, -5.5938, -5.5984,  ..., -1.7082, -1.8687, -1.9893],
         [-5.6037, -5.6037, -5.6093,  ..., -1.7420, -1.9114, -2.0387],
         ...,
         [-5.9767, -5.9767, -6.0147,  ..., -4.9166, -4.9973, -5.0579],
         [-5.9688, -5.9688, -6.0072,  ..., -4.9163, -4.9998, -5.0625],
         [-5.9688, -5.9688, -6.0072,  ..., -4.9163, -4.9998, -5.0625]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In an organized workspace, one might have a designated area to store important documents and files. What piece of furniture in the picture could be used for this purpose? Please output segmentation mask. ASSISTANT: in an organized workspace, one might have a designated area to store important documents and files. what piece of furniture in the picture could be used for this purpose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 750, 1000])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 750, 1000])):  [tensor([[[-1.2734, -1.2731, -1.2614,  ..., -1.9314, -1.9600, -1.9609],
         [-1.2729, -1.2726, -1.2611,  ..., -1.9302, -1.9585, -1.9594],
         [-1.2581, -1.2579, -1.2505,  ..., -1.8942, -1.9122, -1.9128],
         ...,
         [-2.2211, -2.2219, -2.2459,  ..., -2.5571, -2.5493, -2.5491],
         [-2.4846, -2.4853, -2.5092,  ..., -2.9656, -2.9559, -2.9555],
         [-2.9148, -2.9157, -2.9405,  ..., -3.7994, -3.8079, -3.8082]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that ensures the person to land safely in this image? Please output segmentation mask. ASSISTANT: something that ensures the person to land safely</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1920, 2560])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1920, 2560])):  [tensor([[[-6.4375, -6.4375, -6.4375,  ..., -6.0312, -6.0312, -6.0312],
         [-6.4375, -6.4375, -6.4375,  ..., -6.0312, -6.0312, -6.0312],
         [-6.4375, -6.4375, -6.4375,  ..., -6.0312, -6.0312, -6.0312],
         ...,
         [-7.3438, -7.3438, -7.3438,  ..., -7.3047, -7.3047, -7.3047],
         [-7.5813, -7.5813, -7.5813,  ..., -7.6891, -7.6891, -7.6891],
         [-7.6406, -7.6406, -7.6406,  ..., -7.7852, -7.7852, -7.7852]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n A bride and groom often walk together down the aisle during a wedding ceremony. What object in the picture is the bride most likely holding during this moment? Please output segmentation mask. ASSISTANT: a bride and groom often walk together down the aisle during a wedding ceremony. what object in the picture is the bride most likely holding during this moment</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 900])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 900])):  [tensor([[[-11.8750, -11.8750, -11.9443,  ..., -12.7633, -12.6225, -12.2812],
         [-11.8750, -11.8750, -11.9443,  ..., -12.7633, -12.6225, -12.2812],
         [-11.9146, -11.9146, -12.0027,  ..., -12.8055, -12.6353, -12.2652],
         ...,
         [-13.2487, -13.2487, -14.0067,  ..., -14.3685, -13.6992, -12.8901],
         [-13.1250, -13.1250, -13.8771,  ..., -14.2039, -13.5362, -12.6953],
         [-13.1250, -13.1250, -13.8771,  ..., -14.2039, -13.5362, -12.6953]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Some dishes are baked in the oven to enhance their flavors. What object in the picture is commonly used to place the dishes in the oven for baking? Please output segmentation mask. ASSISTANT: some dishes are baked in the oven to enhance their flavors. what object in the picture is commonly used to place the dishes in the oven for baking</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[89]]
>> (validate) sampled_classes_list:  [None]

         [-14.8125, -14.8125, -14.8125,  ..., -12.9375, -12.9375, -12.9375],.5000, -1.5000, -1.5000,  ..., -1.8984, -1.8984, -1.8984],04],],
         ...,
         [-15.3516, -15.3516, -15.3516,  ..., -14.1781, -14.1781, -14.1781],
         [-15.3516, -15.3516, -15.3516,  ..., -13.8772, -13.8772, -13.8772],
         [-15.3516, -15.3516, -15.3516,  ..., -13.8672, -13.8672, -13.8672]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n People often add gas to water to improve its taste. What part of the picture will be consumed as a beverage? Please output segmentation mask. ASSISTANT: people often add gas to water to improve its taste. what part of the picture will be consumed as a beverage</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2331, 1749])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2331, 1749])):  [tensor([[[-4.5938, -4.5938, -4.5938,  ..., -5.3116, -5.2807, -5.2695],
         [-4.5938, -4.5938, -4.5938,  ..., -5.3116, -5.2807, -5.2695],
         [-4.5938, -4.5938, -4.5938,  ..., -5.3116, -5.2807, -5.2695],
         ...,
         [-4.6562, -4.6562, -4.6562,  ..., -4.9932, -4.8834, -4.8438],
         [-4.6562, -4.6562, -4.6562,  ..., -4.9932, -4.8834, -4.8438],
         [-4.6562, -4.6562, -4.6562,  ..., -4.9932, -4.8834, -4.8438]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When people want to take their pets to the park and enjoy some outdoor activities, they often want to make sure their pets are safe and cannot run away. What in the picture could help with this? Please output segmentation mask. ASSISTANT: when people want to take their pets to the park and enjoy some outdoor activities, they often want to make sure their pets are safe and cannot run away. what in the picture could help with this</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[94]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2705, 3269])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2705, 3269])):  [tensor([[[-5.6250, -5.6250, -5.6250,  ..., -4.7500, -4.7500, -4.7500],
         [-5.6250, -5.6250, -5.6250,  ..., -4.7500, -4.7500, -4.7500],
         [-5.6250, -5.6250, -5.6250,  ..., -4.7500, -4.7500, -4.7500],
         ...,
         [-7.7232, -7.7232, -7.7232,  ..., -5.5275, -5.5275, -5.5275],
         [-7.6680, -7.6680, -7.6680,  ..., -5.5430, -5.5430, -5.5430],
         [-7.6680, -7.6680, -7.6680,  ..., -5.5430, -5.5430, -5.5430]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n I feel my commute is too slow now and I'm hoping to find a convenient mode of transportation that can also help me exercise. Can you help me find the corresponding part in the picture? Please output segmentation mask. ASSISTANT: i feel my commute is too slow now and i'm hoping to find a convenient mode of transportation that can also help me exercise. can you help me find the corresponding part in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[92]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1189, 1656])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1189, 1656])):  [tensor([[[-5.1875, -5.1875, -5.1879,  ..., -2.9946, -3.0000, -3.0000],
         [-5.1875, -5.1875, -5.1879,  ..., -2.9946, -3.0000, -3.0000],
         [-5.1864, -5.1864, -5.1868,  ..., -2.9919, -2.9973, -2.9973],
         ...,
         [-4.6371, -4.6371, -4.6380,  ..., -6.6002, -6.6008, -6.6008],
         [-4.9127, -4.9127, -4.9133,  ..., -6.7019, -6.7028, -6.7028],
         [-5.1055, -5.1055, -5.1058,  ..., -6.7801, -6.7812, -6.7812]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If we were to have a friendly competition to see who can paddle the fastest in the water, what would be the most suitable vessels for this activity in the picture? Please output segmentation mask. ASSISTANT: if we were to have a friendly competition to see who can paddle the fastest in the water, what would be the most suitable vessels for this activity in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[86]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2112, 2816])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2112, 2816])):  [tensor([[[-17.1250, -17.1250, -17.1250,  ..., -19.5000, -19.5000, -19.5000],
         [-17.1250, -17.1250, -17.1250,  ..., -19.5000, -19.5000, -19.5000],
         [-17.1250, -17.1250, -17.1250,  ..., -19.5000, -19.5000, -19.5000],
         ...,
         [ -8.5966,  -8.5966,  -8.5966,  ...,  -7.2387,  -7.2387,  -7.2387],
         [ -8.8580,  -8.8580,  -8.8580,  ...,  -7.5682,  -7.5682,  -7.5682],
         [ -8.8906,  -8.8906,  -8.8906,  ...,  -7.6094,  -7.6094,  -7.6094]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the man who seems to lose in this image? Please output segmentation mask. ASSISTANT: the man who seems to lose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1000, 1500])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1000, 1500])):  [tensor([[[-8.7500, -8.7500, -8.7532,  ..., -7.1288, -7.1562, -7.1562],
         [-8.7500, -8.7500, -8.7532,  ..., -7.1288, -7.1562, -7.1562],
         [-8.7451, -8.7451, -8.7495,  ..., -7.1135, -7.1400, -7.1400],
         ...,
         [-5.5423, -5.5423, -5.5568,  ..., -5.0752, -5.0850, -5.0850],
         [-5.7341, -5.7341, -5.7457,  ..., -5.0296, -5.0364, -5.0364],
         [-5.9492, -5.9492, -5.9594,  ..., -5.0465, -5.0508, -5.0508]]],
       device='cuda:0')]

>> (validate) mask_positions_in_input_ids:  [[78]]9375, -12.9375, -12.9375],.5000, -1.5000, -1.5000,  ..., -1.8984, -1.8984, -1.8984],04],],
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1664, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1664, 2048])):  [tensor([[[-6.3750, -6.3750, -6.3750,  ..., -8.1250, -8.1250, -8.1250],
         [-6.3750, -6.3750, -6.3750,  ..., -8.1250, -8.1250, -8.1250],
         [-6.3750, -6.3750, -6.3750,  ..., -8.1250, -8.1250, -8.1250],
         ...,
         [-8.6328, -8.6328, -8.6328,  ..., -8.8359, -8.8359, -8.8359],
         [-8.9297, -8.9297, -8.9297,  ..., -9.1016, -9.1016, -9.1016],
         [-9.0781, -9.0781, -9.0781,  ..., -9.2344, -9.2344, -9.2344]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Birds often need a place to rest or observe their surroundings. What part of a tree in the picture offers a suitable spot for birds to do so? Please output segmentation mask. ASSISTANT: birds often need a place to rest or observe their surroundings. what part of a tree in the picture offers a suitable spot for birds to do so</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 612, 612])):  [tensor([[[  0.,   0.,   0.,  ..., 255., 255., 255.],
         [  0.,   0.,   0.,  ..., 255., 255., 255.],
         [  0.,   0.,   0.,  ..., 255., 255., 255.],
         ...,
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 612, 612])):  [tensor([[[-8.5000, -8.5239, -8.6023,  ..., -8.4315, -8.9151, -9.0625],
         [-8.4243, -8.4772, -8.6506,  ..., -8.4612, -8.8732, -8.9988],
         [-8.1760, -8.3238, -8.8088,  ..., -8.5588, -8.7357, -8.7896],
         ...,
         [-7.0428, -7.2647, -7.9930,  ..., -6.5794, -6.6994, -6.7359],
         [-6.7944, -7.0813, -8.0229,  ..., -6.2941, -6.5125, -6.5790],
         [-6.7188, -7.0254, -8.0320,  ..., -6.2072, -6.4556, -6.5312]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that we can sit on in this image? Please output segmentation mask. ASSISTANT: something that we can sit on</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 540, 720])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 540, 720])):  [tensor([[[-3.4375, -3.4462, -3.4800,  ..., -3.4965, -3.7482, -3.8125],
         [-3.4301, -3.4410, -3.4837,  ..., -3.5042, -3.7340, -3.7927],
         [-3.4010, -3.4208, -3.4979,  ..., -3.5340, -3.6784, -3.7153],
         ...,
         [-2.9375, -3.0192, -3.3389,  ..., -3.4930, -3.3624, -3.3290],
         [-2.7428, -2.8250, -3.1469,  ..., -3.1142, -2.9698, -2.9329],
         [-3.0776, -3.1521, -3.4438,  ..., -3.5122, -3.4412, -3.4231]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n To keep bread fresh and protected, it is often placed in a protective covering. What item in the picture is commonly used for this purpose? Please output segmentation mask. ASSISTANT: to keep bread fresh and protected, it is often placed in a protective covering. what item in the picture is commonly used for this purpose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[80]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 612, 612])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 612, 612])):  [tensor([[[-3.4531, -3.3934, -3.1973,  ..., -3.5824, -3.6150, -3.6250],
         [-3.4233, -3.3889, -3.2761,  ..., -3.6403, -3.6805, -3.6927],
         [-3.3252, -3.3742, -3.5348,  ..., -3.8304, -3.8952, -3.9149],
         ...,
         [-3.6619, -3.8140, -4.3130,  ..., -3.3785, -3.4444, -3.4645],
         [-3.6815, -3.8544, -4.4218,  ..., -3.2353, -3.3859, -3.4318],
         [-3.6875, -3.8667, -4.4550,  ..., -3.1916, -3.3681, -3.4219]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Sometimes, when a car is parked on the side of the road, the driver may accidentally leave the car door open. What part in the picture can indicate that the car door is open? Please output segmentation mask. ASSISTANT: sometimes, when a car is parked on the side of the road, the driver may accidentally leave the car door open. what part in the picture can indicate that the car door is open</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[91]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2592, 1936])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2592, 1936])):  [tensor([[[-7.5938, -7.5938, -7.5938,  ..., -6.8415, -6.7674, -6.7500],
         [-7.5938, -7.5938, -7.5938,  ..., -6.8415, -6.7674, -6.7500],
         [-7.5938, -7.5938, -7.5938,  ..., -6.8415, -6.7674, -6.7500],
         ...,
         [-6.5938, -6.5938, -6.5938,  ..., -6.7212, -6.8015, -6.8203],
         [-6.5938, -6.5938, -6.5938,  ..., -6.7212, -6.8015, -6.8203],
         [-6.5938, -6.5938, -6.5938,  ..., -6.7212, -6.8015, -6.8203]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that produces pollen in this image? Please output segmentation mask. ASSISTANT: something that produces pollen</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]

(PLUM.py) >> pred_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         ...,
         [-11.4525, -11.4525, -11.4525,  ..., -12.2967, -12.2967, -12.2967],
         [-11.6215, -11.6215, -11.6215,  ..., -12.3530, -12.3530, -12.3530],
         [-11.6875, -11.6875, -11.6875,  ..., -12.3750, -12.3750, -12.3750]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Some birds have long bills that they use to catch food from the water. What part of the bird's body in the picture may have this characteristic? Please output segmentation mask. ASSISTANT: some birds have long bills that they use to catch food from the water. what part of the bird's body in the picture may have this characteristic</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[83]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 800, 1200])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 800, 1200])):  [tensor([[[ -9.0625,  -9.0625,  -9.0971,  ...,  -8.9177,  -8.9375,  -8.9375],
         [ -9.0625,  -9.0625,  -9.0971,  ...,  -8.9177,  -8.9375,  -8.9375],
         [ -9.0080,  -9.0080,  -9.0509,  ...,  -8.9418,  -8.9573,  -8.9573],
         ...,
         [-10.8022, -10.8022, -10.9320,  ..., -10.2061, -10.1726, -10.1726],
         [-10.7614, -10.7614, -10.8659,  ...,  -9.7720,  -9.7163,  -9.7163],
         [-10.9688, -10.9688, -11.0646,  ...,  -9.7533,  -9.6797,  -9.6797]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n After cooking, consuming food, and preparing for food, where can we throw away the rest of the food and scraps? Please output segmentation mask. ASSISTANT: after cooking, consuming food, and preparing for food, where can we throw away the rest of the food and scraps</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[79]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-4.1142, -4.0014, -3.9441,  ..., -3.5683, -4.0985, -4.2500],
         [-3.6576, -5.0219, -5.1688,  ..., -3.8852, -4.1888, -4.3993],
         [-3.6209, -5.1141, -5.7656,  ..., -3.7461, -3.9719, -4.0148],
         ...,
         [-4.4797, -5.9406, -6.3203,  ..., -6.1406, -6.1516, -6.1408],
         [-4.4498, -5.6444, -6.0766,  ..., -5.9391, -5.6716, -5.7854],
         [-5.3189, -6.5070, -6.5611,  ..., -6.3008, -5.8798, -5.8728]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the person who is most likely to be the girl's trainer in this image? Please output segmentation mask. ASSISTANT: the person who is most likely to be the girl's trainer</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[71]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3456, 2304])):  [tensor([[[0., 0., 0.,  ..., 1., 1., 0.],
         [0., 0., 0.,  ..., 1., 1., 0.],
         [0., 0., 0.,  ..., 1., 1., 0.],
         ...,
         [0., 0., 0.,  ..., 1., 1., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 2304])):  [tensor([[[-3.8438, -3.8438, -3.8438,  ..., -0.4219, -0.4731, -0.4731],
         [-3.8438, -3.8438, -3.8438,  ..., -0.4219, -0.4731, -0.4731],
         [-3.8438, -3.8438, -3.8438,  ..., -0.4219, -0.4731, -0.4731],
         ...,
         [-4.5625, -4.5625, -4.5625,  ..., -1.3108, -1.3428, -1.3428],
         [-4.5625, -4.5625, -4.5625,  ..., -1.3108, -1.3428, -1.3428],
         [-4.5625, -4.5625, -4.5625,  ..., -1.3108, -1.3428, -1.3428]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n At a car show, visitors can get close to the displayed vehicles to admire their design and features. What part of the car in this picture is open, allowing viewers to see the engine compartment? Please output segmentation mask. ASSISTANT: at a car show, visitors can get close to the displayed vehicles to admire their design and features. what part of the car in this picture is open, allowing viewers to see the engine compartment</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[93]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2356, 3141])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2356, 3141])):  [tensor([[[-6.3750, -6.3750, -6.3750,  ..., -7.1250, -7.1250, -7.1250],
         [-6.3750, -6.3750, -6.3750,  ..., -7.1250, -7.1250, -7.1250],
         [-6.3750, -6.3750, -6.3750,  ..., -7.1250, -7.1250, -7.1250],
         ...,
         [-6.8472, -6.8472, -6.8472,  ..., -6.8517, -6.8517, -6.8517],
         [-6.9062, -6.9062, -6.9062,  ..., -7.2109, -7.2109, -7.2109],
         [-6.9062, -6.9062, -6.9062,  ..., -7.2109, -7.2109, -7.2109]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When snorkeling or scuba diving, it is important to wear certain equipment to protect the eyes and aid in vision. What object in the picture can be used for this purpose? Please output segmentation mask. ASSISTANT: when snorkeling or scuba diving, it is important to wear certain equipment to protect the eyes and aid in vision. what object in the picture can be used for this purpose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[89]]
>> (validate) sampled_classes_list:  [None]

         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         ...,
         [-7.8438, -7.8438, -8.0002,  ..., -8.1772, -8.1953, -8.1953],
         [-7.7625, -7.7625, -7.8945,  ..., -8.5500, -8.5625, -8.5625],
         [-7.7461, -7.7461, -7.8450,  ..., -9.1851, -9.1914, -9.1914]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the reflection of the camera in the mirror in this image? Please output segmentation mask. ASSISTANT: the reflection of the camera in the mirror</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2880, 1920])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2880, 1920])):  [tensor([[[-2.3125, -2.3125, -2.3125,  ..., -6.6091, -6.6522, -6.6562],
         [-2.3125, -2.3125, -2.3125,  ..., -6.6091, -6.6522, -6.6562],
         [-2.3125, -2.3125, -2.3125,  ..., -6.6091, -6.6522, -6.6562],
         ...,
         [-7.5625, -7.5625, -7.5625,  ..., -7.6242, -7.5464, -7.5391],
         [-7.5625, -7.5625, -7.5625,  ..., -7.6242, -7.5464, -7.5391],
         [-7.5625, -7.5625, -7.5625,  ..., -7.6242, -7.5464, -7.5391]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In some rural areas, horse-drawn carts are still used for transportation and carrying goods. What is the main source of power that drives the cart in the picture? Please output segmentation mask. ASSISTANT: in some rural areas, horse-drawn carts are still used for transportation and carrying goods. what is the main source of power that drives the cart in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2304, 3456])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2304, 3456])):  [tensor([[[-4.5312, -4.5312, -4.5312,  ..., -7.2500, -7.2500, -7.2500],
         [-4.5312, -4.5312, -4.5312,  ..., -7.2500, -7.2500, -7.2500],
         [-4.5312, -4.5312, -4.5312,  ..., -7.2500, -7.2500, -7.2500],
         ...,
         [-3.8288, -3.8288, -3.8288,  ..., -4.5109, -4.5109, -4.5109],
         [-3.9277, -3.9277, -3.9277,  ..., -4.5938, -4.5938, -4.5938],
         [-3.9277, -3.9277, -3.9277,  ..., -4.5938, -4.5938, -4.5938]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In case of a fire, it is important to have access to fire safety equipment. What object in the picture is specifically designed to store and release fire extinguishing substances? Please output segmentation mask. ASSISTANT: in case of a fire, it is important to have access to fire safety equipment. what object in the picture is specifically designed to store and release fire extinguishing substances</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-5.3750, -5.3970, -5.4915,  ..., -6.7591, -6.8277, -6.8438],
         [-5.3870, -5.4134, -5.5266,  ..., -6.7904, -6.8596, -6.8758],
         [-5.4385, -5.4836, -5.6769,  ..., -6.9248, -6.9964, -7.0131],
         ...,
         [-5.4559, -5.4850, -5.6097,  ..., -6.0961, -6.0990, -6.0997],
         [-5.4454, -5.4705, -5.5780,  ..., -5.6725, -5.7129, -5.7224],
         [-5.6875, -5.7057, -5.7834,  ..., -5.6508, -5.6575, -5.6591]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the solitary tree in this image? Please output segmentation mask. ASSISTANT: the solitary tree</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[-4.4688, -4.4688, -4.4688,  ..., -5.1875, -5.1875, -5.1875],
         [-4.4688, -4.4688, -4.4688,  ..., -5.1875, -5.1875, -5.1875],
         [-4.4688, -4.4688, -4.4688,  ..., -5.1875, -5.1875, -5.1875],
         ...,
         [-4.6689, -4.6689, -4.6689,  ..., -5.5254, -5.5254, -5.5254],
         [-5.3545, -5.3545, -5.3545,  ..., -6.3965, -6.3965, -6.3965],
         [-5.6973, -5.6973, -5.6973,  ..., -6.8320, -6.8320, -6.8320]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something used for contacting other people in this image? Please output segmentation mask. ASSISTANT: something used for contacting other people</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:46<00:00,  4.32it/s]
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 2 is less than current step: 1499. Dropping entry: {'val/giou': 0.316310852766037, 'val/ciou': 0.3269733488559723, 'val/b_acc': 0.49999997500000126, 'val/i_acc': 0.9612100488870212, 'val/o_acc': 0.9999547850791326, '_timestamp': 1743851562.0812292}).
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-9.0508, -9.0508, -9.0466,  ..., -9.8718, -9.8711, -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
  0%|                                                                                                                                                         | 0/200 [00:00<?, ?it/s]

         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')] -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1002, 1455])):  [tensor([[[-5.3125, -5.3125, -5.3014,  ..., -6.9274, -6.9375, -6.9375],
         [-5.3125, -5.3125, -5.3014,  ..., -6.9274, -6.9375, -6.9375],
         [-5.3115, -5.3115, -5.3016,  ..., -6.9502, -6.9598, -6.9598],
         ...,
         [-5.3006, -5.3006, -5.3348,  ..., -6.1589, -6.1561, -6.1561],
         [-5.3116, -5.3116, -5.3376,  ..., -6.4798, -6.4749, -6.4749],
         [-5.3203, -5.3203, -5.3398,  ..., -6.7332, -6.7266, -6.7266]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When using a film camera to take photos, what part of the camera helps focus the light on the film to capture the image? Please output segmentation mask. ASSISTANT: when using a film camera to take photos, what part of the camera helps focus the light on the film to capture the image</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[77]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1067, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1067, 1600])):  [tensor([[[-13.2500, -13.2500, -13.2289,  ..., -11.3953, -11.3750, -11.3750],
         [-13.2500, -13.2500, -13.2289,  ..., -11.3953, -11.3750, -11.3750],
         [-13.2257, -13.2257, -13.2053,  ..., -11.4155, -11.3954, -11.3954],
         ...,
         [-11.9031, -11.9031, -11.9137,  ..., -12.1012, -12.1033, -12.1033],
         [-12.0882, -12.0882, -12.0979,  ..., -12.0413, -12.0420, -12.0420],
         [-12.2500, -12.2500, -12.2592,  ..., -12.0314, -12.0312, -12.0312]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Birds have various ways of searching for food. What part of their body helps them to grab and pick up food from the ground in the picture? Please output segmentation mask. ASSISTANT: birds have various ways of searching for food. what part of their body helps them to grab and pick up food from the ground in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2570, 2229])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2570, 2229])):  [tensor([[[-16.2500, -16.2500, -16.2500,  ..., -12.1133, -12.5055, -12.6016],
         [-16.2500, -16.2500, -16.2500,  ..., -12.1133, -12.5055, -12.6016],
         [-16.2500, -16.2500, -16.2500,  ..., -12.1133, -12.5055, -12.6016],
         ...,
         [-14.1250, -14.1250, -14.1250,  ..., -17.2784, -16.9920, -16.9219],
         [-14.1250, -14.1250, -14.1250,  ..., -17.2784, -16.9920, -16.9219],
         [-14.1250, -14.1250, -14.1250,  ..., -17.2784, -16.9920, -16.9219]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the sour food in this image? Please output segmentation mask. ASSISTANT: the sour food</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]

  8%|██████████▊                                                                                                                                     | 15/200 [00:04<00:29,  6.35it/s]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1064, 1600])):  [tensor([[[-10.4375, -10.4375, -10.4328,  ..., -10.9523, -10.9375, -10.9375],
         [-10.4375, -10.4375, -10.4328,  ..., -10.9523, -10.9375, -10.9375],
         [-10.4195, -10.4195, -10.4152,  ..., -10.9653, -10.9508, -10.9508],
         ...,
         [ -9.6343,  -9.6343,  -9.6416,  ...,  -7.7256,  -7.7172,  -7.7172],
         [ -9.9243,  -9.9243,  -9.9309,  ...,  -7.7127,  -7.7022,  -7.7022],
         [-10.1328, -10.1328, -10.1390,  ...,  -7.7034,  -7.6914,  -7.6914]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the stacked cookies in this image? Please output segmentation mask. ASSISTANT: the stacked cookies</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1402, 2102])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1402, 2102])):  [tensor([[[-6.9688, -6.9688, -6.9688,  ..., -6.3125, -6.3125, -6.3125],
         [-6.9688, -6.9688, -6.9688,  ..., -6.3125, -6.3125, -6.3125],
         [-6.9688, -6.9688, -6.9688,  ..., -6.3125, -6.3125, -6.3125],
         ...,
         [-2.9826, -2.9826, -2.9826,  ..., -4.7097, -4.7097, -4.7097],
         [-3.1786, -3.1786, -3.1786,  ..., -4.6469, -4.6469, -4.6469],
         [-3.2715, -3.2715, -3.2715,  ..., -4.6172, -4.6172, -4.6172]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If a person wanted to charge their electric car while parked in the street, what object in the picture could they use to connect the car to an electrical power source? Please output segmentation mask. ASSISTANT: if a person wanted to charge their electric car while parked in the street, what object in the picture could they use to connect the car to an electrical power source</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[86]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2016, 3032])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2016, 3032])):  [tensor([[[-10.9375, -10.9375, -10.9375,  ..., -11.9375, -11.9375, -11.9375],
         [-10.9375, -10.9375, -10.9375,  ..., -11.9375, -11.9375, -11.9375],
         [-10.9375, -10.9375, -10.9375,  ..., -11.9375, -11.9375, -11.9375],
         ...,
         [ -9.5826,  -9.5826,  -9.5826,  ...,  -9.5531,  -9.5531,  -9.5531],
         [ -9.7621,  -9.7621,  -9.7621,  ...,  -9.6006,  -9.6006,  -9.6006],
         [ -9.7656,  -9.7656,  -9.7656,  ...,  -9.6016,  -9.6016,  -9.6016]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Where do people put their dirty hygiene products to keep the bathroom clean? Please output segmentation mask. ASSISTANT: where do people put their dirty hygiene products to keep the bathroom clean</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-15.3355, -12.6891, -12.5480,  ..., -11.7039, -13.0509, -13.4521],
         [-12.4313, -13.1231, -13.4719,  ..., -12.6062, -13.4419, -14.6407],
         [-11.3664, -12.6125, -13.3125,  ..., -12.0625, -12.7438, -13.5172],
         ...,
         [-13.2262, -13.6344, -13.2500,  ...,  -9.1250,  -9.4969,  -9.7762],
         [-13.4409, -14.3131, -13.2094,  ...,  -8.7344,  -8.7888,  -9.5364],
         [-13.2759, -15.3459, -13.7160,  ...,  -9.0063,  -9.2812, -10.0670]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When the wind blows, small white objects are blown away and scattered in the air. What in the picture is responsible for this phenomenon? Please output segmentation mask. ASSISTANT: when the wind blows, small white objects are blown away and scattered in the air. what in the picture is responsible for this phenomenon</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[-13.2500, -13.1548, -12.7063,  ..., -15.1500, -14.8716, -14.8125],
         [-13.1910, -13.1064, -12.7072,  ..., -15.1913, -14.9193, -14.8616],
         [-12.9128, -12.8777, -12.7119,  ..., -15.3860, -15.1447, -15.0935],
         ...,
         [-13.9496, -14.0073, -14.2795,  ..., -11.7356, -12.2211, -12.3241],
         [-13.4697, -13.5201, -13.7577,  ..., -10.9838, -11.2814, -11.3445],
         [-12.9899, -13.0330, -13.2359,  ..., -10.2321, -10.3416, -10.3648]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that the dog's food should be put into in this image? Please output segmentation mask. ASSISTANT: something that the dog's food should be put into</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[-8.1250, -8.0547, -7.8125,  ..., -7.5156, -6.8980, -6.7188],
         [-8.0688, -8.0245, -7.8723,  ..., -7.6070, -7.0167, -6.8453],
         [-7.8750, -7.9207, -8.0781,  ..., -7.9219, -7.4254, -7.2812],
         ...,
         [-6.5625, -6.6873, -7.1172,  ..., -4.1875, -3.9453, -3.8750],
         [-6.2762, -6.4302, -6.9606,  ..., -4.4588, -4.2292, -4.1625],
         [-6.8406, -6.9542, -7.3453,  ..., -6.5735, -6.4197, -6.3750]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the food that makes people feel spicy or hot in this image? Please output segmentation mask. ASSISTANT: the food that makes people feel spicy or hot</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[67]]
>> (validate) sampled_classes_list:  [None]

         ..., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')] -9.8711],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-5.7136, -5.7763, -5.9673,  ..., -8.1126, -8.0342, -8.0084],
         [-5.6469, -5.7283, -5.9762,  ..., -7.8476, -7.9285, -7.9550],
         [-6.5025, -6.5499, -6.6943,  ..., -9.1209, -9.2008, -9.2270]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Sometimes, water leaks from faulty plumbing or faucets. What part of the plumbing system in the picture can be a potential source of the water leak? Please output segmentation mask. ASSISTANT: sometimes, water leaks from faulty plumbing or faucets. what part of the plumbing system in the picture can be a potential source of the water leak</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3872, 2592])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3872, 2592])):  [tensor([[[-12.3750, -12.3750, -12.3750,  ...,  -1.1177,  -1.2136,  -1.2136],
         [-12.3750, -12.3750, -12.3750,  ...,  -1.1177,  -1.2136,  -1.2136],
         [-12.3750, -12.3750, -12.3750,  ...,  -1.1177,  -1.2136,  -1.2136],
         ...,
         [-12.3125, -12.3125, -12.3125,  ..., -11.2829, -11.3281, -11.3281],
         [-12.3125, -12.3125, -12.3125,  ..., -11.2829, -11.3281, -11.3281],
         [-12.3125, -12.3125, -12.3125,  ..., -11.2829, -11.3281, -11.3281]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the part of the persons that is above the water in this image? Please output segmentation mask. ASSISTANT: the part of the persons that is above the water</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[67]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[-15.2500, -15.2500, -15.2500,  ..., -15.4375, -15.4375, -15.4375],
         [-15.2500, -15.2500, -15.2500,  ..., -15.4375, -15.4375, -15.4375],
         [-15.2500, -15.2500, -15.2500,  ..., -15.4375, -15.4375, -15.4375],
         ...,
         [-16.2396, -16.2396, -16.2396,  ..., -14.2084, -14.2084, -14.2084],
         [-16.5938, -16.5938, -16.5938,  ..., -15.1250, -15.1250, -15.1250],
         [-16.5938, -16.5938, -16.5938,  ..., -15.1250, -15.1250, -15.1250]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When we need to access or store things above our reach, what would be helpful to stand on? Please output segmentation mask. ASSISTANT: when we need to access or store things above our reach, what would be helpful to stand on</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[71]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[ -7.7911,  -7.1922,  -7.6521,  ...,  -5.8137,  -6.3972,  -6.3719],
         [ -7.2352,  -7.9656,  -8.2859,  ...,  -5.8781,  -6.4144,  -6.1823],
         [ -7.0746,  -7.7406,  -8.0703,  ...,  -5.4062,  -5.6703,  -5.4877],
         ...,
         [ -8.8309,  -9.8406, -10.1406,  ...,  -3.0977,  -3.3852,  -3.3368],
         [ -9.0760, -10.1631, -10.0312,  ...,  -2.9680,  -3.3955,  -3.3171],
         [ -8.6060, -10.0959,  -9.8441,  ...,  -4.4247,  -4.7299,  -4.6490]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What structure can be used to drain the water from the bathroom floor and prevent the accumulation of wastewater? Please output segmentation mask. ASSISTANT: what structure can be used to drain the water from the bathroom floor and prevent the accumulation of wastewater</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[75]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-13.5395, -12.2757, -12.5168,  ..., -11.2832, -12.3666, -12.8199],
         [-13.5648, -13.6894, -13.4406,  ..., -13.1094, -14.3087, -13.6986],
         [-13.4492, -14.0938, -13.3438,  ..., -12.7812, -13.0594, -12.9090],
         ...,
         [-22.3617, -25.0688, -23.5938,  ..., -17.7344, -17.8688, -18.4273],
         [-22.8657, -25.4481, -22.7875,  ..., -16.5187, -15.5062, -17.0101],
         [-15.0098, -17.6115, -16.3550,  ..., -11.7746, -11.7680, -13.1806]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the dog that puts its tongue out in this image? Please output segmentation mask. ASSISTANT: the dog that puts its tongue out</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1600, 1200])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1600, 1200])):  [tensor([[[-12.1250, -12.1250, -12.1055,  ..., -13.0797, -12.8438, -12.6641],
         [-12.1250, -12.1250, -12.1055,  ..., -13.0797, -12.8438, -12.6641],
         [-12.1031, -12.1031, -12.0841,  ..., -13.0733, -12.8346, -12.6525],
         ...,
         [ -5.6762,  -5.6762,  -5.6770,  ...,  -2.7415,  -3.1381,  -3.4414],
         [ -5.6875,  -5.6875,  -5.6883,  ...,  -2.7390,  -3.1315,  -3.4316],
         [ -5.6875,  -5.6875,  -5.6883,  ...,  -2.7390,  -3.1315,  -3.4316]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a rural landscape, what objects in the picture could provide shade and shelter for animals or humans? Please output segmentation mask. ASSISTANT: in a rural landscape, what objects in the picture could provide shade and shelter for animals or humans</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[72]]
>> (validate) sampled_classes_list:  [None]

         [-6.8226, -6.7798, -6.5782,  ..., -6.4852, -6.2788, -6.2350],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [-6.5753, -6.5630, -6.5048,  ..., -6.6514, -6.4928, -6.4591],
         ...,
         [-3.7727, -3.8467, -4.1956,  ..., -4.7815, -4.8631, -4.8804],
         [-4.0522, -4.1147, -4.4095,  ..., -5.4557, -5.5303, -5.5462],
         [-4.6320, -4.6714, -4.8573,  ..., -6.4503, -6.5126, -6.5258]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that the persons use to cross the water in this image? Please output segmentation mask. ASSISTANT: something that the persons use to cross the water</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[-11.4375, -11.4375, -11.4312,  ..., -11.1984, -11.1875, -11.1875],
         [-11.4375, -11.4375, -11.4312,  ..., -11.1984, -11.1875, -11.1875],
         [-11.4484, -11.4484, -11.4426,  ..., -11.2310, -11.2203, -11.2203],
         ...,
         [ -8.4914,  -8.4914,  -8.5107,  ...,  -8.3780,  -8.3758,  -8.3758],
         [ -9.0981,  -9.0981,  -9.1111,  ...,  -9.2812,  -9.2756,  -9.2756],
         [ -9.5977,  -9.5977,  -9.6058,  ..., -10.0274, -10.0195, -10.0195]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Driving at night can be very dangerous due to poor visibility, which can lead to accidents. What part of the car needs to be turned on when driving at night? Please output segmentation mask. ASSISTANT: driving at night can be very dangerous due to poor visibility, which can lead to accidents. what part of the car needs to be turned on when driving at night</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 864, 1152])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 864, 1152])):  [tensor([[[-16.5000, -16.5000, -16.3646,  ..., -15.0551, -14.8125, -14.8125],
         [-16.5000, -16.5000, -16.3646,  ..., -15.0551, -14.8125, -14.8125],
         [-16.3759, -16.3759, -16.2731,  ..., -15.1184, -14.9028, -14.9028],
         ...,
         [-18.0764, -18.0764, -18.5728,  ..., -15.0813, -15.2552, -15.2552],
         [-18.1667, -18.1667, -18.5861,  ..., -15.8867, -16.0625, -16.0625],
         [-18.8438, -18.8438, -19.0821,  ..., -17.2559, -17.3906, -17.3906]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n This rocky terrain can be challenging to navigate. What object in the picture could provide information to guide travelers through this area? Please output segmentation mask. ASSISTANT: this rocky terrain can be challenging to navigate. what object in the picture could provide information to guide travelers through this area</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2592, 3888])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2592, 3888])):  [tensor([[[ -8.6250,  -8.6250,  -8.6250,  ..., -12.2500, -12.2500, -12.2500],
         [ -8.6250,  -8.6250,  -8.6250,  ..., -12.2500, -12.2500, -12.2500],
         [ -8.6250,  -8.6250,  -8.6250,  ..., -12.2500, -12.2500, -12.2500],
         ...,
         [ -7.3529,  -7.3529,  -7.3529,  ...,  -6.8935,  -6.8935,  -6.8935],
         [ -7.4062,  -7.4062,  -7.4062,  ...,  -6.8984,  -6.8984,  -6.8984],
         [ -7.4062,  -7.4062,  -7.4062,  ...,  -6.8984,  -6.8984,  -6.8984]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is The fastest sinking ship in this image? Please output segmentation mask. ASSISTANT: the fastest sinking ship</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 333, 500])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 333, 500])):  [tensor([[[-12.4375, -12.2365, -11.8525,  ..., -12.9312, -12.7712, -12.6875],
         [-12.4543, -12.4194, -12.3527,  ..., -13.6103, -13.5104, -13.4581],
         [-12.4863, -12.7687, -13.3082,  ..., -14.9077, -14.9224, -14.9301],
         ...,
         [ -8.8025,  -8.8883,  -9.0521,  ..., -10.6827, -10.7006, -10.7100],
         [ -9.1850,  -9.3770,  -9.7438,  ..., -10.4066, -10.3918, -10.3840],
         [-10.1450, -10.3542, -10.7538,  ..., -10.2005, -10.1529, -10.1280]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In some regions, people raise certain animals for their milk, meat, and skin. What animal in the picture could be domesticated for such purposes? Please output segmentation mask. ASSISTANT: in some regions, people raise certain animals for their milk, meat, and skin. what animal in the picture could be domesticated for such purposes</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]

         ...,8226, -6.7798, -6.5782,  ..., -6.4852, -6.2788, -6.2350],([[[ -8.3750,  -8.3750,  -8.3750,  ...,  -8.1250,  -8.1250,  -8.1250],
         [ -7.5530,  -7.5530,  -7.5530,  ...,  -9.5311,  -9.5311,  -9.5311],
         [ -7.5742,  -7.5742,  -7.5742,  ...,  -9.5547,  -9.5547,  -9.5547],
         [ -7.5742,  -7.5742,  -7.5742,  ...,  -9.5547,  -9.5547,  -9.5547]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the objects that can protect the snail and prevent it from getting injured in this image? Please output segmentation mask. ASSISTANT: the objects that can protect the snail and prevent it from getting injured</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[71]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1350, 1800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1350, 1800])):  [tensor([[[-2.4375, -2.4375, -2.4375,  ..., -5.4062, -5.4062, -5.4062],
         [-2.4375, -2.4375, -2.4375,  ..., -5.4062, -5.4062, -5.4062],
         [-2.4375, -2.4375, -2.4375,  ..., -5.4062, -5.4062, -5.4062],
         ...,
         [-1.5086, -1.5086, -1.5086,  ..., -0.7677, -0.7677, -0.7677],
         [-1.8786, -1.8786, -1.8786,  ..., -1.6115, -1.6115, -1.6115],
         [-2.1084, -2.1084, -2.1084,  ..., -2.1356, -2.1356, -2.1356]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If a person wants to watch TV or a movie, which furniture is the most suitable for them to sit and watch? Please output segmentation mask. ASSISTANT: if a person wants to watch tv or a movie, which furniture is the most suitable for them to sit and watch</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 1., 1., 0.],
         [0., 0., 0.,  ..., 1., 1., 0.],
         [0., 0., 0.,  ..., 1., 1., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-11.1899,  -9.4760,  -9.4641,  ...,  -8.9346, -10.2598, -10.5671],
         [ -9.6479, -10.5631, -10.9250,  ...,  -9.6328, -10.4575, -11.1192],
         [ -9.7004, -10.4094, -10.6562,  ...,  -8.4844,  -9.1625,  -9.5707],
         ...,
         [-14.6633, -13.3312, -12.5781,  ...,  -5.3984,  -5.7203,  -5.7955],
         [-15.8653, -13.9575, -12.6750,  ...,  -5.1828,  -5.4303,  -5.6430],
         [-15.1224, -14.2474, -13.0176,  ...,  -5.8385,  -6.1507,  -6.9070]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In historical buildings, there are often signs or symbols displayed on the walls or floors to represent a specific meaning or identity. What in the picture could be used to display such signs or symbols? Please output segmentation mask. ASSISTANT: in historical buildings, there are often signs or symbols displayed on the walls or floors to represent a specific meaning or identity. what in the picture could be used to display such signs or symbols</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[90]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-5.7188, -5.7188, -5.6562,  ..., -5.9336, -5.9062, -5.9062],
         [-5.7188, -5.7188, -5.6562,  ..., -5.9336, -5.9062, -5.9062],
         [-5.7539, -5.7539, -5.7104,  ..., -6.0977, -6.0664, -6.0664],
         ...,
         [-4.6875, -4.6875, -4.8252,  ..., -4.9170, -4.9297, -4.9297],
         [-4.7734, -4.7734, -4.8809,  ..., -5.2974, -5.2969, -5.2969],
         [-5.2578, -5.2578, -5.2910,  ..., -6.1655, -6.1406, -6.1406]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Insects can help pollinate flowers effectively, but some insects also collect pollen and nectar to produce honey. What in the picture does not make honey? Please output segmentation mask. ASSISTANT: insects can help pollinate flowers effectively, but some insects also collect pollen and nectar to produce honey. what in the picture does not make honey</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[89]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 651, 500])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 651, 500])):  [tensor([[[-10.8750, -10.6203,  -9.7291,  ..., -10.7361,  -9.8231,  -8.8160],
         [-10.6333, -10.4463,  -9.7919,  ..., -10.8255,  -9.8225,  -8.6927],
         [ -9.7880,  -9.8377, -10.0115,  ..., -11.1383,  -9.8204,  -8.2616],
         ...,
         [-10.5308, -10.9644, -12.4812,  ..., -12.4890, -11.2096,  -9.6522],
         [ -9.8264, -10.3607, -12.2300,  ..., -12.2201, -10.8139,  -9.0351],
         [ -9.6250, -10.1881, -12.1581,  ..., -12.1432, -10.7007,  -8.8586]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Flowers are often used to decorate tables during special occasions or events. What item in the picture can be used to hold the flowers and keep them fresh by providing water to them? Please output segmentation mask. ASSISTANT: flowers are often used to decorate tables during special occasions or events. what item in the picture can be used to hold the flowers and keep them fresh by providing water to them</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1370, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1370, 2048])):  [tensor([[[-5.2812, -5.2812, -5.2812,  ..., -8.8125, -8.8125, -8.8125],
         [-5.2812, -5.2812, -5.2812,  ..., -8.8125, -8.8125, -8.8125],
         [-5.2812, -5.2812, -5.2812,  ..., -8.8125, -8.8125, -8.8125],
         ...,
         [-5.1328, -5.1328, -5.1328,  ..., -7.4375, -7.4375, -7.4375],
         [-5.3047, -5.3047, -5.3047,  ..., -7.3125, -7.3125, -7.3125],
         [-5.3906, -5.3906, -5.3906,  ..., -7.2500, -7.2500, -7.2500]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Bamboo is very hard, and its sharp edges can easily scratch people. What tool in the picture can I use to split the bamboo and cross this bamboo forest? Please output segmentation mask. ASSISTANT: bamboo is very hard, and its sharp edges can easily scratch people. what tool in the picture can i use to split the bamboo and cross this bamboo forest</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[92]]
>> (validate) sampled_classes_list:  [None]

(PLUM.py) >> pred_masks (shape: torch.Size([1, 300, 400])):  [tensor([[[-6.5000, -7.0175, -7.5195,  ..., -6.7523, -7.0437, -7.1875],8.1250],
         [-7.3625, -8.0652, -8.7504,  ..., -7.2653, -7.3672, -7.3456],
         [-8.1582, -9.0489, -9.9235,  ..., -7.7127, -7.5923, -7.3828],
         ...,
         [-8.3699, -8.8499, -9.2346,  ..., -2.2244, -2.0121, -1.7996],
         [-7.0775, -7.5421, -7.9618,  ..., -2.2574, -1.9617, -1.6834],
         [-5.7844, -6.1622, -6.5729,  ..., -3.0484, -2.6940, -2.3653]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When admiring a beautiful sunset, what part of the picture would we most likely focus on? Please output segmentation mask. ASSISTANT: when admiring a beautiful sunset, what part of the picture would we most likely focus on</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[71]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1542, 3494])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1542, 3494])):  [tensor([[[-2.5938, -2.5938, -2.5938,  ..., -2.6406, -2.6406, -2.6406],
         [-2.5938, -2.5938, -2.5938,  ..., -2.6406, -2.6406, -2.6406],
         [-2.5938, -2.5938, -2.5938,  ..., -2.6406, -2.6406, -2.6406],
         ...,
         [-1.4849, -1.4849, -1.4849,  ..., -2.3954, -2.3954, -2.3954],
         [-1.5381, -1.5381, -1.5381,  ..., -2.4336, -2.4336, -2.4336],
         [-1.5381, -1.5381, -1.5381,  ..., -2.4336, -2.4336, -2.4336]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n As a speaker, relying solely on verbal explanations may not effectively convey the intended message to the audience. What area of the picture should be used to project the key content and make it more understandable for the audience during the presentation? Please output segmentation mask. ASSISTANT: as a speaker, relying solely on verbal explanations may not effectively convey the intended message to the audience. what area of the picture should be used to project the key content and make it more understandable for the audience during the presentation</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[101]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2848, 4288])):  [tensor([[[0., 0., 0.,  ..., 1., 1., 1.],
         [0., 0., 0.,  ..., 1., 1., 1.],
         [0., 0., 0.,  ..., 1., 1., 1.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2848, 4288])):  [tensor([[[-8.3750, -8.3750, -8.3750,  ..., -0.3086, -0.3086, -0.3086],
         [-8.3750, -8.3750, -8.3750,  ..., -0.3086, -0.3086, -0.3086],
         [-8.3750, -8.3750, -8.3750,  ..., -0.3086, -0.3086, -0.3086],
         ...,
         [-9.9021, -9.9021, -9.9021,  ..., -3.4327, -3.4327, -3.4327],
         [-9.9688, -9.9688, -9.9688,  ..., -3.5273, -3.5273, -3.5273],
         [-9.9688, -9.9688, -9.9688,  ..., -3.5273, -3.5273, -3.5273]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a modern office, what object in the picture is commonly used for inputting data and controlling the computer? Please output segmentation mask. ASSISTANT: in a modern office, what object in the picture is commonly used for inputting data and controlling the computer</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[74]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1024, 1280])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1024, 1280])):  [tensor([[[-15.6250, -15.6250, -15.5195,  ..., -11.2422, -11.1875, -11.1875],
         [-15.6250, -15.6250, -15.5195,  ..., -11.2422, -11.1875, -11.1875],
         [-15.5040, -15.5040, -15.4161,  ..., -11.2856, -11.2382, -11.2382],
         ...,
         [-11.3118, -11.3118, -11.3840,  ..., -12.5269, -12.5620, -12.5620],
         [-10.4624, -10.4624, -10.4785,  ..., -11.9216, -11.9311, -11.9311],
         [-10.1562, -10.1562, -10.1440,  ..., -11.6387, -11.6250, -11.6250]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a zoo, there are separate areas designated for different animals. What structure in the picture is used to confine and display animals? Please output segmentation mask. ASSISTANT: in a zoo, there are separate areas designated for different animals. what structure in the picture is used to confine and display animals</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[0., 0., 1.,  ..., 0., 0., 0.],
         [0., 0., 1.,  ..., 0., 0., 0.],
         [0., 0., 1.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[-0.7422, -0.7422, -0.7422,  ..., -0.2754, -0.2754, -0.2754],
         [-0.7422, -0.7422, -0.7422,  ..., -0.2754, -0.2754, -0.2754],
         [-0.7422, -0.7422, -0.7422,  ..., -0.2754, -0.2754, -0.2754],
         ...,
         [-0.5836, -0.5836, -0.5836,  ..., -1.4641, -1.4641, -1.4641],
         [-0.8453, -0.8453, -0.8453,  ..., -2.1589, -2.1589, -2.1589],
         [-0.9066, -0.9066, -0.9066,  ..., -2.3218, -2.3218, -2.3218]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that tastes spicy in this image? Please output segmentation mask. ASSISTANT: something that tastes spicy</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[ -9.1875,  -9.1875,  -9.0859,  ...,  -8.2930,  -8.3750,  -8.3750],
         [ -9.1875,  -9.1875,  -9.0859,  ...,  -8.2930,  -8.3750,  -8.3750],
         [ -9.1641,  -9.1641,  -9.0908,  ...,  -8.2568,  -8.3242,  -8.3242],
         ...,
         [-11.0312, -11.0312, -11.3428,  ...,  -9.2686,  -9.2969,  -9.2969],
         [-10.9531, -10.9531, -11.2266,  ...,  -9.7354,  -9.7578,  -9.7578],
         [-11.3594, -11.3594, -11.5078,  ..., -10.7686, -10.7734, -10.7734]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When going fishing on a calm sea, what type of boat shown in the picture would be an ideal choice for a peaceful fishing experience? Please output segmentation mask. ASSISTANT: when going fishing on a calm sea, what type of boat shown in the picture would be an ideal choice for a peaceful fishing experience</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')] [tensor([[[-6.5000, -7.0175, -7.5195,  ..., -6.7523, -7.0437, -7.1875],8.1250],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2736, 3648])):  [tensor([[[-9.4375, -9.4375, -9.4375,  ..., -8.4375, -8.4375, -8.4375],
         [-9.4375, -9.4375, -9.4375,  ..., -8.4375, -8.4375, -8.4375],
         [-9.4375, -9.4375, -9.4375,  ..., -8.4375, -8.4375, -8.4375],
         ...,
         [-4.6693, -4.6693, -4.6693,  ..., -6.3311, -6.3311, -6.3311],
         [-4.8340, -4.8340, -4.8340,  ..., -6.7637, -6.7637, -6.7637],
         [-4.8340, -4.8340, -4.8340,  ..., -6.7637, -6.7637, -6.7637]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the part of this animal's body that comes into contact with the air in this image? Please output segmentation mask. ASSISTANT: the part of this animal's body that comes into contact with the air</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[72]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1000, 1504])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1000, 1504])):  [tensor([[[-17.5000, -17.5000, -17.4100,  ..., -12.2870, -12.1875, -12.1875],
         [-17.5000, -17.5000, -17.4100,  ..., -12.2870, -12.1875, -12.1875],
         [-17.3940, -17.3940, -17.3087,  ..., -12.4485, -12.3504, -12.3504],
         ...,
         [-16.4798, -16.4798, -16.5213,  ..., -16.1780, -16.1993, -16.1993],
         [-16.8629, -16.8629, -16.8963,  ..., -15.9690, -15.9758, -15.9758],
         [-17.1562, -17.1562, -17.1835,  ..., -15.8090, -15.8047, -15.8047]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Many people use bags to carry their belongings when they go out. What part of the bag in the picture can be used to carry the bag comfortably over the shoulder? Please output segmentation mask. ASSISTANT: many people use bags to carry their belongings when they go out. what part of the bag in the picture can be used to carry the bag comfortably over the shoulder</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1920, 2560])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1920, 2560])):  [tensor([[[-13.4375, -13.4375, -13.4375,  ..., -10.8750, -10.8750, -10.8750],
         [-13.4375, -13.4375, -13.4375,  ..., -10.8750, -10.8750, -10.8750],
         [-13.4375, -13.4375, -13.4375,  ..., -10.8750, -10.8750, -10.8750],
         ...,
         [-13.2344, -13.2344, -13.2344,  ..., -12.8125, -12.8125, -12.8125],
         [-13.7032, -13.7032, -13.7032,  ..., -13.6376, -13.6376, -13.6376],
         [-13.8203, -13.8203, -13.8203,  ..., -13.8438, -13.8438, -13.8438]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n During a meal, people typically use utensils to bring food to their mouths. What tool in the picture can be used to fulfill this need? Please output segmentation mask. ASSISTANT: during a meal, people typically use utensils to bring food to their mouths. what tool in the picture can be used to fulfill this need</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-10.5625, -10.4772, -10.0750,  ...,  -8.6063,  -8.1577,  -8.0625],
         [-10.4542, -10.3813, -10.0376,  ...,  -8.6535,  -8.2390,  -8.1511],
         [ -9.9437,  -9.9293,  -9.8612,  ...,  -8.8763,  -8.6226,  -8.5688],
         ...,
         [ -8.1000,  -8.2368,  -8.8819,  ..., -10.2837, -10.4570, -10.4937],
         [ -8.2451,  -8.3972,  -9.1140,  ..., -10.6420, -10.9048, -10.9606],
         [ -9.4262,  -9.5186,  -9.9542,  ..., -11.6304, -11.7445, -11.7687]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n During the move, what object can be used to store and transport various sundries and small household items, which is sturdy and relatively easy to carry? Please output segmentation mask. ASSISTANT: during the move, what object can be used to store and transport various sundries and small household items, which is sturdy and relatively easy to carry</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-5.2944, -4.9635, -5.1857,  ..., -6.0061, -6.5876, -6.3569],
         [-4.6413, -5.0194, -5.0891,  ..., -6.3391, -6.7659, -6.8454],
         [-4.7236, -4.9922, -5.0469,  ..., -6.2500, -6.5500, -6.4855],
         ...,
         [ 0.9241,  1.3686,  1.3281,  ..., -4.6172, -5.0781, -5.1855],
         [ 0.5913,  0.9227,  0.9429,  ..., -4.5250, -4.9987, -5.2179],
         [-0.3712, -0.2746, -0.4686,  ..., -4.8065, -5.0695, -5.1516]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If we were to take a flight in the sky, what part of the plane in the picture would be spinning to generate lift and propel the aircraft forward? Please output segmentation mask. ASSISTANT: if we were to take a flight in the sky, what part of the plane in the picture would be spinning to generate lift and propel the aircraft forward</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2499, 3680])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2499, 3680])):  [tensor([[[-18.3750, -18.3750, -18.3750,  ..., -14.8750, -14.8750, -14.8750],
         [-18.3750, -18.3750, -18.3750,  ..., -14.8750, -14.8750, -14.8750],
         [-18.3750, -18.3750, -18.3750,  ..., -14.8750, -14.8750, -14.8750],
         ...,
         [-15.2804, -15.2804, -15.2804,  ..., -17.8799, -17.8799, -17.8799],
         [-15.3750, -15.3750, -15.3750,  ..., -17.9531, -17.9531, -17.9531],
         [-15.3750, -15.3750, -15.3750,  ..., -17.9531, -17.9531, -17.9531]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something used by a Jedi warrior to attack enemies in this image? Please output segmentation mask. ASSISTANT: something used by a jedi warrior to attack enemies</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]

(PLUM.py) >> pred_masks (shape: torch.Size([1, 1023, 1392])):  [tensor([[[-4.1562, -4.1562, -4.1132,  ..., -7.3901, -7.3438, -7.3438],1250],
         [-4.1562, -4.1562, -4.1132,  ..., -7.3901, -7.3438, -7.3438],
         [-4.1523, -4.1523, -4.1098,  ..., -7.4014, -7.3570, -7.3570],
         ...,
         [-8.4654, -8.4654, -8.5258,  ..., -8.1559, -8.1694, -8.1694],
         [-8.1261, -8.1261, -8.1626,  ..., -8.1527, -8.1579, -8.1579],
         [-7.8477, -7.8477, -7.8645,  ..., -8.1501, -8.1484, -8.1484]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the part that can transmit traction and braking torque, coming into contact with the ground in this image? Please output segmentation mask. ASSISTANT: the part that can transmit traction and braking torque, coming into contact with the ground</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 752, 1169])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 752, 1169])):  [tensor([[[-10.2500, -10.2500, -10.1745,  ..., -10.6951, -10.6250, -10.6250],
         [-10.2500, -10.2500, -10.1745,  ..., -10.6951, -10.6250, -10.6250],
         [-10.1906, -10.1906, -10.1352,  ..., -10.7596, -10.6952, -10.6952],
         ...,
         [ -7.5401,  -7.5401,  -7.6346,  ...,  -7.4975,  -7.5826,  -7.5826],
         [ -7.0056,  -7.0056,  -7.0678,  ...,  -6.8993,  -6.9495,  -6.9495],
         [ -6.9102,  -6.9102,  -6.9614,  ...,  -6.8301,  -6.8477,  -6.8477]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the area where people can stand without getting wet in this image? Please output segmentation mask. ASSISTANT: the area where people can stand without getting wet</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-5.0625, -5.0461, -4.9688,  ..., -4.0281, -3.6182, -3.5312],
         [-4.9198, -4.9091, -4.8590,  ..., -3.9198, -3.5485, -3.4697],
         [-4.2469, -4.2634, -4.3416,  ..., -3.4094, -3.2199, -3.1797],
         ...,
         [-5.1563, -5.1879, -5.3372,  ..., -5.0731, -5.1598, -5.1781],
         [-5.2770, -5.3202, -5.5241,  ..., -5.5936, -5.6956, -5.7173],
         [-7.0675, -7.0842, -7.1631,  ..., -8.3426, -8.4616, -8.4868]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a motorcycle race, there are often sharp turns that require skilled maneuvering. What part of the race track in the picture indicates a sharp turn? Please output segmentation mask. ASSISTANT: in a motorcycle race, there are often sharp turns that require skilled maneuvering. what part of the race track in the picture indicates a sharp turn</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 682, 1023])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [0., 0., 0.,  ..., 1., 1., 1.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 682, 1023])):  [tensor([[[-5.4375, -5.4375, -5.4139,  ..., -8.5393, -8.5001, -8.5000],
         [-5.4374, -5.4374, -5.4139,  ..., -8.5394, -8.5003, -8.5002],
         [-5.4100, -5.4099, -5.3913,  ..., -8.6268, -8.5945, -8.5944],
         ...,
         [-7.5244, -7.5245, -7.5549,  ..., -6.8612, -6.8533, -6.8533],
         [-7.2584, -7.2584, -7.2790,  ..., -6.3761, -6.3683, -6.3683],
         [-7.4920, -7.4920, -7.4969,  ..., -6.5353, -6.4844, -6.4843]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the region showing current time in this image? Please output segmentation mask. ASSISTANT: the region showing current time</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-4.6562, -4.6562, -4.6016,  ..., -4.6211, -4.5938, -4.5938],
         [-4.6562, -4.6562, -4.6016,  ..., -4.6211, -4.5938, -4.5938],
         [-4.5898, -4.5898, -4.5513,  ..., -4.6143, -4.5859, -4.5859],
         ...,
         [-3.8027, -3.8027, -3.9055,  ..., -3.8350, -3.8145, -3.8145],
         [-3.8203, -3.8203, -3.9214,  ..., -4.1985, -4.1836, -4.1836],
         [-3.8984, -3.8984, -3.9751,  ..., -4.9275, -4.9258, -4.9258]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the reflection of the object in this image? Please output segmentation mask. ASSISTANT: the reflection of the object</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2820, 3760])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2820, 3760])):  [tensor([[[-8.3750, -8.3750, -8.3750,  ..., -8.5000, -8.5000, -8.5000],
         [-8.3750, -8.3750, -8.3750,  ..., -8.5000, -8.5000, -8.5000],
         [-8.3750, -8.3750, -8.3750,  ..., -8.5000, -8.5000, -8.5000],
         ...,
         [-5.5625, -5.5625, -5.5625,  ..., -7.3853, -7.3853, -7.3853],
         [-5.8281, -5.8281, -5.8281,  ..., -7.7852, -7.7852, -7.7852],
         [-5.8281, -5.8281, -5.8281,  ..., -7.7852, -7.7852, -7.7852]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Dogs use their mouths to perform various tasks, including eating and vocalizing. What part of the dog's body is primarily responsible for these actions? Please output segmentation mask. ASSISTANT: dogs use their mouths to perform various tasks, including eating and vocalizing. what part of the dog's body is primarily responsible for these actions</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]

         [-7.2186, -8.0672, -8.2656,  ..., -7.2031, -7.2016, -7.2338],([[[-4.1562, -4.1562, -4.1132,  ..., -7.3901, -7.3438, -7.3438],1250],
         ...,
         [-6.8436, -6.8328, -6.6797,  ..., -6.9219, -6.9172, -6.6916],
         [-7.1914, -7.2344, -6.9406,  ..., -6.6656, -6.5328, -6.5436],
         [-7.4349, -7.7639, -7.3338,  ..., -7.0492, -7.0237, -7.1666]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the camera lens that is more suitable for photographing nearby objects in this image? Please output segmentation mask. ASSISTANT: the camera lens that is more suitable for photographing nearby objects</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1620, 2160])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1620, 2160])):  [tensor([[[-7.8750, -7.8750, -7.8750,  ..., -7.7812, -7.7812, -7.7812],
         [-7.8750, -7.8750, -7.8750,  ..., -7.7812, -7.7812, -7.7812],
         [-7.8750, -7.8750, -7.8750,  ..., -7.7812, -7.7812, -7.7812],
         ...,
         [-7.3079, -7.3079, -7.3079,  ..., -7.3710, -7.3710, -7.3710],
         [-7.5597, -7.5597, -7.5597,  ..., -8.0413, -8.0413, -8.0413],
         [-7.6719, -7.6719, -7.6719,  ..., -8.3398, -8.3398, -8.3398]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the part of the house that can be opened in this image? Please output segmentation mask. ASSISTANT: the part of the house that can be opened</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[-7.5625, -7.5625, -7.5625,  ..., -8.6875, -8.6875, -8.6875],
         [-7.5625, -7.5625, -7.5625,  ..., -8.6875, -8.6875, -8.6875],
         [-7.5625, -7.5625, -7.5625,  ..., -8.6875, -8.6875, -8.6875],
         ...,
         [-5.0227, -5.0227, -5.0227,  ..., -7.8470, -7.8470, -7.8470],
         [-5.4698, -5.4698, -5.4698,  ..., -8.7519, -8.7519, -8.7519],
         [-5.6445, -5.6445, -5.6445,  ..., -9.1055, -9.1055, -9.1055]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the source of power for the ship in this image? Please output segmentation mask. ASSISTANT: the source of power for the ship</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[-11.6250, -11.6250, -11.6250,  ..., -10.1250, -10.1250, -10.1250],
         [-11.6250, -11.6250, -11.6250,  ..., -10.1250, -10.1250, -10.1250],
         [-11.6250, -11.6250, -11.6250,  ..., -10.1250, -10.1250, -10.1250],
         ...,
         [-13.1484, -13.1484, -13.1484,  ..., -12.8047, -12.8047, -12.8047],
         [-13.9141, -13.9141, -13.9141,  ..., -14.2578, -14.2578, -14.2578],
         [-14.2969, -14.2969, -14.2969,  ..., -14.9844, -14.9844, -14.9844]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When hanging laundry outside to dry, what material in the picture would be suitable for holding the clothes securely in place? Please output segmentation mask. ASSISTANT: when hanging laundry outside to dry, what material in the picture would be suitable for holding the clothes securely in place</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2592, 3872])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2592, 3872])):  [tensor([[[-7.3125, -7.3125, -7.3125,  ..., -7.0938, -7.0938, -7.0938],
         [-7.3125, -7.3125, -7.3125,  ..., -7.0938, -7.0938, -7.0938],
         [-7.3125, -7.3125, -7.3125,  ..., -7.0938, -7.0938, -7.0938],
         ...,
         [-6.9399, -6.9399, -6.9399,  ..., -6.7059, -6.7059, -6.7059],
         [-6.9336, -6.9336, -6.9336,  ..., -6.6406, -6.6406, -6.6406],
         [-6.9336, -6.9336, -6.9336,  ..., -6.6406, -6.6406, -6.6406]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the friend of humans in this image? Please output segmentation mask. ASSISTANT: the friend of humans</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]

(PLUM.py) >> pred_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[-2.5312, -2.5312, -2.5312,  ..., -2.7188, -2.7188, -2.7188],1250],
         [-2.5312, -2.5312, -2.5312,  ..., -2.7188, -2.7188, -2.7188],
         [-2.5312, -2.5312, -2.5312,  ..., -2.7188, -2.7188, -2.7188],
         ...,
         [-3.8812, -3.8812, -3.8812,  ..., -4.4604, -4.4604, -4.4604],
         [-4.0787, -4.0787, -4.0787,  ..., -4.9450, -4.9450, -4.9450],
         [-4.1250, -4.1250, -4.1250,  ..., -5.0586, -5.0586, -5.0586]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Soldiers are often equipped with various tools and weapons to carry out their duties. What item in the picture can be classified as a weapon? Please output segmentation mask. ASSISTANT: soldiers are often equipped with various tools and weapons to carry out their duties. what item in the picture can be classified as a weapon</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 300, 400])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 300, 400])):  [tensor([[[-17.8750, -16.2938, -14.9234,  ..., -12.5758, -12.6100, -12.4375],
         [-16.3225, -16.1828, -16.2035,  ..., -12.3479, -12.5243, -12.5238],
         [-14.8094, -16.1286, -17.5410,  ..., -11.8835, -12.1772, -12.3414],
         ...,
         [-17.3219, -18.0787, -18.7358,  ..., -18.5995, -18.3982, -18.1125],
         [-16.6800, -18.0221, -19.0589,  ..., -16.6232, -16.7170, -16.9775],
         [-16.3925, -17.7208, -18.7195,  ..., -16.3768, -16.7755, -17.3499]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What object is used to cover the entrance of the bathroom and ensure privacy? Please output segmentation mask. ASSISTANT: what object is used to cover the entrance of the bathroom and ensure privacy</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-8.4214, -7.9885, -8.1139,  ..., -6.3010, -6.9904, -7.1947],
         [-7.9885, -8.5600, -8.3609,  ..., -6.5266, -6.9753, -7.7853],
         [-7.9611, -8.4016, -8.1719,  ..., -6.9688, -7.3188, -7.5121],
         ...,
         [-7.8133, -7.8562, -7.6641,  ..., -9.0625, -9.6625, -9.9633],
         [-8.1530, -8.3163, -7.6609,  ..., -8.5766, -8.9047, -9.5385],
         [-8.4725, -8.8909, -8.4088,  ..., -8.6762, -9.0810, -9.3989]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the person in the air in this image? Please output segmentation mask. ASSISTANT: the person in the air</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3034, 2148])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3034, 2148])):  [tensor([[[-21.2500, -21.2500, -21.2500,  ..., -13.8936, -13.7986, -13.7969],
         [-21.2500, -21.2500, -21.2500,  ..., -13.8936, -13.7986, -13.7969],
         [-21.2500, -21.2500, -21.2500,  ..., -13.8936, -13.7986, -13.7969],
         ...,
         [-14.6875, -14.6875, -14.6875,  ..., -15.3495, -15.8347, -15.8438],
         [-14.6875, -14.6875, -14.6875,  ..., -15.3495, -15.8347, -15.8438],
         [-14.6875, -14.6875, -14.6875,  ..., -15.3495, -15.8347, -15.8438]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is a place where bees can suck nectar from flowers in this image? Please output segmentation mask. ASSISTANT: a place where bees can suck nectar from flowers</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 566, 939])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 566, 939])):  [tensor([[[-5.4062, -5.3941, -5.2757,  ..., -5.3322, -5.1727, -5.1562],
         [-5.4015, -5.3896, -5.2743,  ..., -5.3416, -5.1827, -5.1663],
         [-5.3553, -5.3464, -5.2608,  ..., -5.4336, -5.2798, -5.2639],
         ...,
         [-4.6134, -4.6267, -4.7553,  ..., -6.0547, -6.0803, -6.0829],
         [-4.5176, -4.5296, -4.6457,  ..., -5.4431, -5.4573, -5.4587],
         [-4.4069, -4.4170, -4.5149,  ..., -4.7509, -4.7518, -4.7519]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n I want to take a trip around the world, but I need some transportation to help me cross the oceans. What type of transportation in the picture can fulfill this requirement? Please output segmentation mask. ASSISTANT: i want to take a trip around the world, but i need some transportation to help me cross the oceans. what type of transportation in the picture can fulfill this requirement</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[90]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 960, 1280])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 960, 1280])):  [tensor([[[ -3.2188,  -3.2188,  -3.1826,  ...,  -7.1445,  -7.0938,  -7.0938],
         [ -3.2188,  -3.2188,  -3.1826,  ...,  -7.1445,  -7.0938,  -7.0938],
         [ -3.2168,  -3.2168,  -3.1830,  ...,  -7.2083,  -7.1621,  -7.1621],
         ...,
         [-12.0664, -12.0664, -12.2432,  ...,  -6.4026,  -6.4238,  -6.4238],
         [-12.1125, -12.1125, -12.2555,  ...,  -7.8609,  -7.8750,  -7.8750],
         [-12.2656, -12.2656, -12.3594,  ...,  -9.7542,  -9.7617,  -9.7617]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is where we can see the speed of the car in this image? Please output segmentation mask. ASSISTANT: where we can see the speed of the car</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]:  [tensor([[[-2.5312, -2.5312, -2.5312,  ..., -2.7188, -2.7188, -2.7188],1250],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 2304])):  [tensor([[[-14.4375, -14.4375, -14.4375,  ..., -14.9087, -14.9766, -14.9766],
         [-14.4375, -14.4375, -14.4375,  ..., -14.9087, -14.9766, -14.9766],
         [-14.4375, -14.4375, -14.4375,  ..., -14.9087, -14.9766, -14.9766],
         ...,
         [-13.8125, -13.8125, -13.8125,  ...,  -6.5067,  -6.6367,  -6.6367],
         [-13.8125, -13.8125, -13.8125,  ...,  -6.5067,  -6.6367,  -6.6367],
         [-13.8125, -13.8125, -13.8125,  ...,  -6.5067,  -6.6367,  -6.6367]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When visiting a library or bookstore, people often browse through the shelves to find interesting books to read. Which area in the picture could provide a variety of reading materials for visitors? Please output segmentation mask. ASSISTANT: when visiting a library or bookstore, people often browse through the shelves to find interesting books to read. which area in the picture could provide a variety of reading materials for visitors</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[91]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 4000, 2683])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 4000, 2683])):  [tensor([[[-7.8750, -7.8750, -7.8750,  ..., -4.1006, -4.1504, -4.1504],
         [-7.8750, -7.8750, -7.8750,  ..., -4.1006, -4.1504, -4.1504],
         [-7.8750, -7.8750, -7.8750,  ..., -4.1006, -4.1504, -4.1504],
         ...,
         [-5.3438, -5.3438, -5.3438,  ..., -5.5602, -5.5586, -5.5586],
         [-5.3438, -5.3438, -5.3438,  ..., -5.5602, -5.5586, -5.5586],
         [-5.3438, -5.3438, -5.3438,  ..., -5.5602, -5.5586, -5.5586]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the person who appears to have already won in the battle in this image? Please output segmentation mask. ASSISTANT: the person who appears to have already won in the battle</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1504, 2256])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1504, 2256])):  [tensor([[[-8.1875, -8.1875, -8.1875,  ..., -7.8750, -7.8750, -7.8750],
         [-8.1875, -8.1875, -8.1875,  ..., -7.8750, -7.8750, -7.8750],
         [-8.1875, -8.1875, -8.1875,  ..., -7.8750, -7.8750, -7.8750],
         ...,
         [-8.1916, -8.1916, -8.1916,  ..., -7.1296, -7.1296, -7.1296],
         [-8.2892, -8.2892, -8.2892,  ..., -7.1012, -7.1012, -7.1012],
         [-8.3281, -8.3281, -8.3281,  ..., -7.0898, -7.0898, -7.0898]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a healthy meal, vegetables are often included to provide essential nutrients. What in the picture can be used to eat the vegetables? Please output segmentation mask. ASSISTANT: in a healthy meal, vegetables are often included to provide essential nutrients. what in the picture can be used to eat the vegetables</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[83]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 854, 1280])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 854, 1280])):  [tensor([[[-6.0938, -6.0938, -6.0664,  ..., -5.3984, -5.3125, -5.3125],
         [-6.0938, -6.0938, -6.0664,  ..., -5.3984, -5.3125, -5.3125],
         [-6.0411, -6.0411, -6.0224,  ..., -5.4322, -5.3535, -5.3535],
         ...,
         [-7.5935, -7.5935, -7.6316,  ..., -7.2482, -7.2106, -7.2106],
         [-7.4923, -7.4923, -7.5235,  ..., -6.9340, -6.8883, -6.8883],
         [-7.6562, -7.6562, -7.6841,  ..., -6.9070, -6.8555, -6.8555]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that prevents people from getting into the building in this image? Please output segmentation mask. ASSISTANT: something that prevents people from getting into the building</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         ...,
         [255., 255., 255.,  ..., 255., 255., 255.],
         [255., 255., 255.,  ..., 255., 255., 255.],
         [255., 255., 255.,  ..., 255., 255., 255.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-15.3125, -15.2534, -14.9750,  ..., -16.1625, -16.4409, -16.5000],
         [-15.3322, -15.2802, -15.0350,  ..., -16.4499, -16.6861, -16.7362],
         [-15.4250, -15.4063, -15.3181,  ..., -17.8050, -17.8421, -17.8500],
         ...,
         [  0.9988,   0.9429,   0.6791,  ...,   2.7244,   2.9827,   3.0375],
         [  0.5338,   0.4766,   0.2066,  ...,   2.2469,   2.5065,   2.5615],
         [ -1.0844,  -1.1238,  -1.3094,  ...,  -0.5076,  -0.2234,  -0.1631]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that the monkey uses to pierce its food in this image? Please output segmentation mask. ASSISTANT: something that the monkey uses to pierce its food</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1600, 1066])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1600, 1066])):  [tensor([[[-13.2500, -13.2500, -13.2360,  ..., -12.3250, -12.2950, -12.2734],
         [-13.2500, -13.2500, -13.2360,  ..., -12.3250, -12.2950, -12.2734],
         [-13.2367, -13.2367, -13.2233,  ..., -12.3428, -12.3044, -12.2769],
         ...,
         [ -9.1938,  -9.1938,  -9.2230,  ..., -13.3381, -12.2010, -11.3840],
         [ -9.1875,  -9.1875,  -9.2170,  ..., -13.3115, -12.1619, -11.3359],
         [ -9.1875,  -9.1875,  -9.2170,  ..., -13.3115, -12.1619, -11.3359]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that avoids falling down in this image? Please output segmentation mask. ASSISTANT: something that avoids falling down</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]

         [-13.7033, -14.6666, -15.2010,  ..., -13.5002, -13.8933, -13.6016],.5312, -2.5312, -2.5312,  ..., -2.7188, -2.7188, -2.7188],1250],
         [-13.3890, -14.5889, -15.1039,  ..., -13.3110, -13.3530, -13.0166],
         ...,
         [ -7.9845,  -8.4284,  -8.1505,  ..., -12.0594, -11.3523, -11.7014],
         [ -6.3613,  -6.4974,  -5.8721,  ..., -10.3859, -10.9678, -11.1576],
         [ -6.6939,  -6.8043,  -6.6735,  ..., -10.3601, -11.8012, -11.3770]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What object in a typical modern kitchen can be used to quickly heat up leftovers, boil water, or cook frozen food? Please output segmentation mask. ASSISTANT: what object in a typical modern kitchen can be used to quickly heat up leftovers, boil water, or cook frozen food</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[80]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-11.9177, -11.1733, -11.4258,  ...,  -5.0148,  -5.6090,  -5.4452],
         [-11.6030, -12.4538, -12.5000,  ...,  -4.9719,  -5.4156,  -5.6949],
         [-11.7676, -12.7344, -12.6406,  ...,  -4.7031,  -4.7859,  -5.0115],
         ...,
         [ -9.2680, -10.2563,  -9.9062,  ...,  -5.6484,  -6.1578,  -6.3404],
         [ -8.9525, -10.1212,  -9.2594,  ...,  -5.6375,  -5.9066,  -6.0505],
         [ -8.1694,  -9.1904,  -8.7660,  ...,  -6.5377,  -6.8254,  -6.7610]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When plants grow excessively in unwanted areas, it is necessary to remove them to maintain the aesthetics of the surroundings. What part of the plants in the picture may need to be removed in this situation? Please output segmentation mask. ASSISTANT: when plants grow excessively in unwanted areas, it is necessary to remove them to maintain the aesthetics of the surroundings. what part of the plants in the picture may need to be removed in this situation</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[97]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 853, 1280])):  [tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 853, 1280])):  [tensor([[[-1.5078, -1.5078, -1.4878,  ..., -3.8066, -3.8125, -3.8125],
         [-1.5078, -1.5078, -1.4878,  ..., -3.8066, -3.8125, -3.8125],
         [-1.4937, -1.4937, -1.4756,  ..., -3.8049, -3.8086, -3.8086],
         ...,
         [-2.7111, -2.7111, -2.7130,  ..., -1.7324, -1.7227, -1.7227],
         [-2.7954, -2.7954, -2.7981,  ..., -1.7354, -1.7273, -1.7273],
         [-2.8691, -2.8691, -2.8726,  ..., -1.7379, -1.7314, -1.7314]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a television studio, various equipment is used to capture and record video footage. What in the picture could be used to stabilize and hold the camera steady during filming? Please output segmentation mask. ASSISTANT: in a television studio, various equipment is used to capture and record video footage. what in the picture could be used to stabilize and hold the camera steady during filming</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2573, 3593])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2573, 3593])):  [tensor([[[-12.3125, -12.3125, -12.3125,  ..., -11.3125, -11.3125, -11.3125],
         [-12.3125, -12.3125, -12.3125,  ..., -11.3125, -11.3125, -11.3125],
         [-12.3125, -12.3125, -12.3125,  ..., -11.3125, -11.3125, -11.3125],
         ...,
         [ -9.7401,  -9.7401,  -9.7401,  ...,  -9.4915,  -9.4915,  -9.4915],
         [ -9.6406,  -9.6406,  -9.6406,  ...,  -9.5312,  -9.5312,  -9.5312],
         [ -9.6406,  -9.6406,  -9.6406,  ...,  -9.5312,  -9.5312,  -9.5312]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n There are two washing machines as shown in the picture. If I need to do laundry, where in the picture would I put the clothes? Please output segmentation mask. ASSISTANT: there are two washing machines as shown in the picture. if i need to do laundry, where in the picture would i put the clothes</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[82]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3254, 2168])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3254, 2168])):  [tensor([[[-12.5000, -12.5000, -12.5000,  ..., -12.7809, -12.9062, -12.9062],
         [-12.5000, -12.5000, -12.5000,  ..., -12.7809, -12.9062, -12.9062],
         [-12.5000, -12.5000, -12.5000,  ..., -12.7809, -12.9062, -12.9062],
         ...,
         [ -6.0625,  -6.0625,  -6.0625,  ...,  -8.7570,  -8.6406,  -8.6406],
         [ -6.0625,  -6.0625,  -6.0625,  ...,  -8.7570,  -8.6406,  -8.6406],
         [ -6.0625,  -6.0625,  -6.0625,  ...,  -8.7570,  -8.6406,  -8.6406]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Insects are often found on or near trees, where they can find shelter and food. What part of the tree in this picture could insects commonly be found on or around? Please output segmentation mask. ASSISTANT: insects are often found on or near trees, where they can find shelter and food. what part of the tree in this picture could insects commonly be found on or around</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1728, 2304])):  [tensor([[[1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.],
         ...,
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1728, 2304])):  [tensor([[[-10.3125, -10.3125, -10.3125,  ..., -10.2500, -10.2500, -10.2500],
         [-10.3125, -10.3125, -10.3125,  ..., -10.2500, -10.2500, -10.2500],
         [-10.3125, -10.3125, -10.3125,  ..., -10.2500, -10.2500, -10.2500],
         ...,
         [-10.0417, -10.0417, -10.0417,  ...,  -7.6980,  -7.6980,  -7.6980],
         [-10.4375, -10.4375, -10.4375,  ...,  -8.5625,  -8.5625,  -8.5625],
         [-10.5859, -10.5859, -10.5859,  ...,  -8.8867,  -8.8867,  -8.8867]]],
       device='cuda:0')]

         2, 2, 2, 2, 0]], device='cuda:0').., -13.5002, -13.8933, -13.6016],.5312, -2.5312, -2.5312,  ..., -2.7188, -2.7188, -2.7188],1250],
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-6.5884, -5.8864, -6.2527,  ..., -6.1254, -6.8021, -6.6531],
         [-5.3063, -6.1700, -6.4031,  ..., -6.1469, -6.4047, -6.3295],
         [-5.5428, -6.1766, -6.2891,  ..., -6.2891, -6.1312, -5.8949],
         ...,
         [-4.6617, -4.9625, -4.2734,  ..., -5.4531, -5.6609, -5.3494],
         [-5.0923, -5.5306, -4.8211,  ..., -5.3578, -5.2803, -5.4071],
         [-6.1485, -6.6010, -5.8513,  ..., -6.1867, -6.2739, -6.2079]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If we wanted to identify the specific vehicle shown in the picture, what part of the car should we look at? Please output segmentation mask. ASSISTANT: if we wanted to identify the specific vehicle shown in the picture, what part of the car should we look at</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[74]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 800, 1200])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 800, 1200])):  [tensor([[[-20.0000, -20.0000, -19.7526,  ..., -18.0333, -17.8750, -17.8750],
         [-20.0000, -20.0000, -19.7526,  ..., -18.0333, -17.8750, -17.8750],
         [-19.8018, -19.8018, -19.5999,  ..., -18.2269, -18.0435, -18.0435],
         ...,
         [-16.4683, -16.4683, -16.5959,  ..., -15.3730, -15.3919, -15.3919],
         [-16.2010, -16.2010, -16.3018,  ..., -14.5892, -14.5902, -14.5902],
         [-16.5547, -16.5547, -16.6468,  ..., -14.3983, -14.3828, -14.3828]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something hot and light in this image? Please output segmentation mask. ASSISTANT: something hot and light</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 616, 816])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 616, 816])):  [tensor([[[-9.5625, -9.5506, -9.4914,  ..., -9.2592, -9.3035, -9.3125],
         [-9.5207, -9.5130, -9.4751,  ..., -9.2750, -9.3236, -9.3334],
         [-9.3137, -9.3273, -9.3942,  ..., -9.3533, -9.4228, -9.4369],
         ...,
         [-6.6048, -6.6248, -6.7242,  ..., -9.4174, -9.5337, -9.5572],
         [-6.4582, -6.4734, -6.5484,  ..., -9.1250, -9.1441, -9.1480],
         [-6.7621, -6.7876, -6.9136,  ..., -9.0039, -8.9117, -8.8931]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is objects that can help women appear taller in this image? Please output segmentation mask. ASSISTANT: objects that can help women appear taller</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 465, 500])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 465, 500])):  [tensor([[[ -7.7812,  -8.2586,  -9.1706,  ...,  -7.1138,  -7.3058,  -7.4062],
         [ -8.0239,  -8.6291,  -9.7853,  ...,  -7.7751,  -7.8900,  -7.9501],
         [ -8.4877,  -9.3373, -10.9602,  ...,  -9.0393,  -9.0068,  -8.9898],
         ...,
         [  1.6510,   1.8264,   2.1616,  ...,  -2.8630,  -2.7462,  -2.6851],
         [  1.6749,   1.7036,   1.7583,  ...,  -2.9142,  -2.8793,  -2.8611],
         [  0.5396,   0.4469,   0.2700,  ...,  -3.3180,  -3.3513,  -3.3688]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What container in the picture is used for arranging the flowers to make them look more beautiful? Please output segmentation mask. ASSISTANT: what container in the picture is used for arranging the flowers to make them look more beautiful</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-4.3181, -3.9264, -3.9925,  ..., -3.9821, -4.3480, -3.9588],
         [-3.3141, -3.8308, -3.5789,  ..., -3.4289, -3.8066, -3.8753],
         [-2.9743, -3.2805, -2.9062,  ..., -2.9414, -3.2031, -3.3105],
         ...,
         [-4.3369, -4.8203, -5.2344,  ..., -3.9883, -3.9922, -3.5088],
         [-4.7125, -5.2281, -5.4188,  ..., -3.9234, -3.7797, -3.5434],
         [-5.1727, -5.7321, -5.6467,  ..., -4.3624, -4.3220, -4.1826]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the places where the driver can observe the speed in this image? Please output segmentation mask. ASSISTANT: the places where the driver can observe the speed</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 944, 1417])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 944, 1417])):  [tensor([[[-10.8125, -10.8125, -10.7526,  ..., -10.8341, -10.8125, -10.8125],
         [-10.8125, -10.8125, -10.7526,  ..., -10.8341, -10.8125, -10.8125],
         [-10.7503, -10.7503, -10.6977,  ..., -10.8363, -10.8173, -10.8173],
         ...,
         [-10.7058, -10.7058, -10.7275,  ..., -12.3544, -12.3345, -12.3345],
         [-11.1235, -11.1235, -11.1413,  ..., -11.9866, -11.9507, -11.9507],
         [-11.4609, -11.4609, -11.4756,  ..., -11.6894, -11.6406, -11.6406]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something indicating that someone is celerating the birthday in this image? Please output segmentation mask. ASSISTANT: something indicating that someone is celerating the birthday</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 651, 383])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 651, 383])):  [tensor([[[ -9.8125,  -9.8125,  -9.8125,  ...,  -9.3472,  -9.4809,  -9.7020],
         [ -9.7588,  -9.7998,  -9.9434,  ...,  -9.5651,  -9.6412,  -9.7778],
         [ -9.5709,  -9.7555, -10.4011,  ..., -10.3273, -10.2014, -10.0428],
         ...,
         [ -6.0797,  -6.1617,  -6.4488,  ...,  -0.8999,  -1.7515,  -2.7388],
         [ -6.1149,  -6.1957,  -6.4783,  ...,  -1.0087,  -1.7549,  -2.5951],
         [ -6.1250,  -6.2054,  -6.4868,  ...,  -1.0398,  -1.7558,  -2.5540]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that we use to control computer games in this image? Please output segmentation mask. ASSISTANT: something that we use to control computer games</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-12.3125, -12.2698, -12.0687,  ..., -15.7312, -16.0561, -16.1250],
         [-12.3912, -12.3555, -12.1869,  ..., -15.8490, -16.1633, -16.2300],
         [-12.7625, -12.7592, -12.7438,  ..., -16.4044, -16.6689, -16.7250],
         ...,
         [-11.2625, -11.3498, -11.7612,  ..., -11.2312, -11.4169, -11.4563],
         [-11.5242, -11.6223, -12.0849,  ..., -11.3719, -11.5104, -11.5398],
         [-11.0913, -11.1503, -11.4288,  ..., -12.4827, -12.4475, -12.4400]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When serving wine, it is common to pour it into a glass from a container. What object in the picture could be used to pour wine? Please output segmentation mask. ASSISTANT: when serving wine, it is common to pour it into a glass from a container. what object in the picture could be used to pour wine</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[80]]
>> (validate) sampled_classes_list:  [None]

         [1., 1., 1.,  ..., 1., 0., 0.]]], device='cuda:0')]8933, -13.6016],.5312, -2.5312, -2.5312,  ..., -2.7188, -2.7188, -2.7188],1250],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1193, 1271])):  [tensor([[[-1.2578, -1.2578, -1.2468,  ..., -1.4377, -1.4141, -1.4141],
         [-1.2578, -1.2578, -1.2468,  ..., -1.4377, -1.4141, -1.4141],
         [-1.2259, -1.2259, -1.2152,  ..., -1.4149, -1.3925, -1.3925],
         ...,
         [-1.1508, -1.1508, -1.1664,  ..., -1.0669, -1.0624, -1.0624],
         [-1.4655, -1.4655, -1.4850,  ..., -1.2416, -1.2378, -1.2378],
         [-1.7422, -1.7422, -1.7651,  ..., -1.3951, -1.3921, -1.3921]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In historical architecture, a building often has a grand entrance that consists of a large opening with a curved or pointed top. What feature in the picture resembles such an entrance? Please output segmentation mask. ASSISTANT: in historical architecture, a building often has a grand entrance that consists of a large opening with a curved or pointed top. what feature in the picture resembles such an entrance</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 576])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 576])):  [tensor([[[-1.0391, -0.9856, -0.7539,  ..., -1.2266, -1.0327, -1.6120],
         [-1.0237, -0.9671, -0.7218,  ..., -1.2028, -1.0204, -1.5956],
         [-0.9570, -0.8869, -0.5829,  ..., -1.0998, -0.9673, -1.5243],
         ...,
         [-1.5885, -1.4548, -0.8750,  ..., -1.1632, -1.0960, -1.4358],
         [-1.4658, -1.3473, -0.8337,  ..., -1.0687, -0.9736, -1.2721],
         [-1.4375, -1.3225, -0.8242,  ..., -1.0469, -0.9453, -1.2344]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the people who are about to get on the vehicle in this image? Please output segmentation mask. ASSISTANT: the people who are about to get on the vehicle</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[67]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-15.6875, -15.5431, -14.8625,  ...,  -5.3438,  -5.3438,  -5.3438],
         [-15.5661, -15.4365, -14.8257,  ...,  -5.3408,  -5.3432,  -5.3438],
         [-14.9938, -14.9340, -14.6525,  ...,  -5.3269,  -5.3408,  -5.3438],
         ...,
         [ -8.7313,  -8.8369,  -9.3350,  ...,  -5.4275,  -5.4461,  -5.4500],
         [ -8.7189,  -8.8347,  -9.3808,  ...,  -5.6935,  -5.6450,  -5.6347],
         [ -9.1575,  -9.2374,  -9.6142,  ...,  -6.9334,  -6.8238,  -6.8006]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something indicating the identity of the bus in this image? Please output segmentation mask. ASSISTANT: something indicating the identity of the bus</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2048, 1536])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2048, 1536])):  [tensor([[[ -6.3750,  -6.3750,  -6.3750,  ..., -10.9648, -10.9414, -10.9297],
         [ -6.3750,  -6.3750,  -6.3750,  ..., -10.9648, -10.9414, -10.9297],
         [ -6.3750,  -6.3750,  -6.3750,  ..., -10.9648, -10.9414, -10.9297],
         ...,
         [ -9.2500,  -9.2500,  -9.2500,  ...,  -7.6562,  -7.4688,  -7.3750],
         [ -9.2500,  -9.2500,  -9.2500,  ...,  -7.6562,  -7.4688,  -7.3750],
         [ -9.2500,  -9.2500,  -9.2500,  ...,  -7.6562,  -7.4688,  -7.3750]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the object that can be used by the owner to lead the dog in this image? Please output segmentation mask. ASSISTANT: the object that can be used by the owner to lead the dog</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 762, 1000])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 762, 1000])):  [tensor([[[-24.8750, -24.8446, -23.9300,  ..., -20.3099, -19.7680, -19.7500],
         [-24.8711, -24.8409, -23.9322,  ..., -20.3075, -19.7604, -19.7422],
         [-24.7527, -24.7285, -23.9985,  ..., -20.2316, -19.5287, -19.5054],
         ...,
         [-23.1344, -23.1443, -23.4433,  ..., -23.2525, -23.1277, -23.1236],
         [-23.1088, -23.1192, -23.4319,  ..., -22.0293, -21.8613, -21.8557],
         [-23.8196, -23.8339, -24.2644,  ..., -21.0883, -20.9524, -20.9479]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the rabbit on the woman's back in this image? Please output segmentation mask. ASSISTANT: the rabbit on the woman's back</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 802, 1089])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 802, 1089])):  [tensor([[[-3.1719, -3.1719, -3.1536,  ..., -6.6995, -6.6562, -6.6562],
         [-3.1719, -3.1719, -3.1536,  ..., -6.6995, -6.6562, -6.6562],
         [-3.1569, -3.1569, -3.1429,  ..., -6.7549, -6.7227, -6.7227],
         ...,
         [-7.1694, -7.1694, -7.2805,  ..., -4.8027, -4.8277, -4.8277],
         [-6.5891, -6.5891, -6.6651,  ..., -4.9963, -4.9966, -4.9966],
         [-6.0273, -6.0273, -6.0693,  ..., -5.1836, -5.1602, -5.1602]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What musical instrument in the picture is usually played with both hands on a keyboard? Please output segmentation mask. ASSISTANT: what musical instrument in the picture is usually played with both hands on a keyboard</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[67]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]8933, -13.6016],.5312, -2.5312, -2.5312,  ..., -2.7188, -2.7188, -2.7188],1250],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[ -8.1875,  -8.1875,  -8.1875,  ..., -10.8125, -10.8125, -10.8125],
         [ -8.1875,  -8.1875,  -8.1875,  ..., -10.8125, -10.8125, -10.8125],
         [ -8.1875,  -8.1875,  -8.1875,  ..., -10.8125, -10.8125, -10.8125],
         ...,
         [ -8.3685,  -8.3685,  -8.3685,  ...,  -7.6810,  -7.6810,  -7.6810],
         [ -8.8398,  -8.8398,  -8.8398,  ...,  -8.4023,  -8.4023,  -8.4023],
         [ -8.8398,  -8.8398,  -8.8398,  ...,  -8.4023,  -8.4023,  -8.4023]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a mechanical workshop, there are various machines and tools used for different purposes. What in the picture could be used to rotate or spin other parts or objects? Please output segmentation mask. ASSISTANT: in a mechanical workshop, there are various machines and tools used for different purposes. what in the picture could be used to rotate or spin other parts or objects</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1606, 1610])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1606, 1610])):  [tensor([[[-5.3750, -5.3750, -5.3637,  ..., -5.5994, -5.5938, -5.5938],
         [-5.3750, -5.3750, -5.3637,  ..., -5.5994, -5.5938, -5.5938],
         [-5.3638, -5.3638, -5.3528,  ..., -5.6073, -5.6018, -5.6018],
         ...,
         [-4.7722, -4.7722, -4.7901,  ..., -4.9898, -4.9911, -4.9911],
         [-4.6977, -4.6977, -4.7166,  ..., -4.8301, -4.8321, -4.8321],
         [-4.6445, -4.6445, -4.6641,  ..., -4.7162, -4.7188, -4.7188]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the boxes inside the refrigerator in this image? Please output segmentation mask. ASSISTANT: the boxes inside the refrigerator</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2048, 1536])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2048, 1536])):  [tensor([[[-3.8281, -3.8281, -3.8281,  ..., -6.3867, -6.2695, -6.2109],
         [-3.8281, -3.8281, -3.8281,  ..., -6.3867, -6.2695, -6.2109],
         [-3.8281, -3.8281, -3.8281,  ..., -6.3867, -6.2695, -6.2109],
         ...,
         [-4.4062, -4.4062, -4.4062,  ..., -3.1396, -3.1494, -3.1543],
         [-4.4062, -4.4062, -4.4062,  ..., -3.1396, -3.1494, -3.1543],
         [-4.4062, -4.4062, -4.4062,  ..., -3.1396, -3.1494, -3.1543]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the stronger Mario in this image? Please output segmentation mask. ASSISTANT: the stronger mario</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[60]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2448, 3264])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2448, 3264])):  [tensor([[[-6.9688, -6.9688, -6.9688,  ..., -7.8125, -7.8125, -7.8125],
         [-6.9688, -6.9688, -6.9688,  ..., -7.8125, -7.8125, -7.8125],
         [-6.9688, -6.9688, -6.9688,  ..., -7.8125, -7.8125, -7.8125],
         ...,
         [-8.4314, -8.4314, -8.4314,  ..., -9.0058, -9.0058, -9.0058],
         [-8.5469, -8.5469, -8.5469,  ..., -9.3945, -9.3945, -9.3945],
         [-8.5469, -8.5469, -8.5469,  ..., -9.3945, -9.3945, -9.3945]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is where to wash hands in this image? Please output segmentation mask. ASSISTANT: where to wash hands</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 5953, 4793])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 5953, 4793])):  [tensor([[[-17.5000, -17.5000, -17.5000,  ..., -15.2422, -15.2422, -15.2422],
         [-17.5000, -17.5000, -17.5000,  ..., -15.2422, -15.2422, -15.2422],
         [-17.5000, -17.5000, -17.5000,  ..., -15.2422, -15.2422, -15.2422],
         ...,
         [-12.7500, -12.7500, -12.7500,  ..., -14.8438, -14.8438, -14.8438],
         [-12.7500, -12.7500, -12.7500,  ..., -14.8438, -14.8438, -14.8438],
         [-12.7500, -12.7500, -12.7500,  ..., -14.8438, -14.8438, -14.8438]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the food that the bird likes to eat in this image? Please output segmentation mask. ASSISTANT: the food that the bird likes to eat</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]

(PLUM.py) >> pred_masks (shape: torch.Size([1, 2448, 3264])):  [tensor([[[-15.7500, -15.7500, -15.7500,  ..., -14.6875, -14.6875, -14.6875],
         [-15.7500, -15.7500, -15.7500,  ..., -14.6875, -14.6875, -14.6875],
         [-15.7500, -15.7500, -15.7500,  ..., -14.6875, -14.6875, -14.6875],
         ...,
         [-14.6814, -14.6814, -14.6814,  ..., -15.8965, -15.8965, -15.8965],
         [-15.2500, -15.2500, -15.2500,  ..., -16.8516, -16.8516, -16.8516],
         [-15.2500, -15.2500, -15.2500,  ..., -16.8516, -16.8516, -16.8516]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In outdoor recreational activities, people can experience the thrill of flying in the sky with the help of equipment. What object in the picture can provide this experience? Please output segmentation mask. ASSISTANT: in outdoor recreational activities, people can experience the thrill of flying in the sky with the help of equipment. what object in the picture can provide this experience</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[-8.0625, -8.0149, -7.7906,  ..., -6.3750, -5.7563, -5.6250],
         [-7.9282, -7.8885, -7.7015,  ..., -6.3799, -5.7936, -5.6692],
         [-7.2945, -7.2921, -7.2811,  ..., -6.4031, -5.9698, -5.8779],
         ...,
         [-6.4693, -6.4936, -6.6080,  ..., -5.7499, -5.7452, -5.7442],
         [-7.0691, -7.0771, -7.1148,  ..., -6.2837, -6.2840, -6.2840],
         [-7.6689, -7.6606, -7.6216,  ..., -6.8175, -6.8227, -6.8238]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n We are currently watching a game and it's halftime. Who are the cheerleaders who come out to liven up the atmosphere? Please output segmentation mask. ASSISTANT: we are currently watching a game and it's halftime. who are the cheerleaders who come out to liven up the atmosphere</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 272, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 272, 1024])):  [tensor([[[-11.5000, -11.5000, -11.2422,  ...,  -8.9297,  -8.6875,  -8.6875],
         [-11.5000, -11.5000, -11.2422,  ...,  -8.9297,  -8.6875,  -8.6875],
         [-11.2109, -11.2109, -11.0156,  ...,  -9.1016,  -8.9141,  -8.9141],
         ...,
         [ -7.9961,  -7.9961,  -8.0923,  ...,  -6.1650,  -6.0742,  -6.0742],
         [ -8.2148,  -8.2148,  -8.2817,  ...,  -6.4312,  -6.3438,  -6.3438],
         [ -8.9570,  -8.9570,  -8.9312,  ...,  -6.8716,  -6.7188,  -6.7188]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In an educational setting, children often use different materials to learn about letters, numbers, and words. What object in the picture could be used as a visual aid for learning about letters and words? Please output segmentation mask. ASSISTANT: in an educational setting, children often use different materials to learn about letters, numbers, and words. what object in the picture could be used as a visual aid for learning about letters and words</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[90]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1274, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1274, 2048])):  [tensor([[[-7.6562, -7.6562, -7.6562,  ..., -7.7188, -7.7188, -7.7188],
         [-7.6562, -7.6562, -7.6562,  ..., -7.7188, -7.7188, -7.7188],
         [-7.6562, -7.6562, -7.6562,  ..., -7.7188, -7.7188, -7.7188],
         ...,
         [-5.9297, -5.9297, -5.9297,  ..., -5.0820, -5.0820, -5.0820],
         [-6.1328, -6.1328, -6.1328,  ..., -5.1055, -5.1055, -5.1055],
         [-6.2344, -6.2344, -6.2344,  ..., -5.1172, -5.1172, -5.1172]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is a car with a color that is closer to lipstick color in this image? Please output segmentation mask. ASSISTANT: a car with a color that is closer to lipstick color</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[ -7.7500,  -7.7500,  -7.7344,  ..., -13.1172, -13.1250, -13.1250],
         [ -7.7500,  -7.7500,  -7.7344,  ..., -13.1172, -13.1250, -13.1250],
         [ -7.7426,  -7.7426,  -7.7273,  ..., -13.1313, -13.1391, -13.1391],
         ...,
         [ -8.9719,  -8.9719,  -8.9953,  ..., -10.2286, -10.2343, -10.2343],
         [ -9.4887,  -9.4887,  -9.5086,  ..., -11.4047, -11.4087, -11.4087],
         [ -9.8984,  -9.8984,  -9.9153,  ..., -12.3333, -12.3359, -12.3359]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If you want to play table tennis indoors, what furniture in the picture should be used as the playing surface? Please output segmentation mask. ASSISTANT: if you want to play table tennis indoors, what furniture in the picture should be used as the playing surface</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0.,  ..., 255., 255., 255.],
         [  0.,   0.,   0.,  ..., 255., 255., 255.],
         [  0.,   0.,   0.,  ..., 255., 255., 255.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[ -9.8985, -10.1590,  -9.7840,  ...,  -5.9143,  -6.6793,  -6.7391],
         [-10.9324, -11.3406, -11.6531,  ...,  -7.2141,  -8.1166,  -7.9254],
         [-10.6965, -11.1906, -11.3594,  ...,  -6.7500,  -7.4422,  -7.1307],
         ...,
         [-10.3617, -11.3500, -11.0000,  ...,   1.0596,   0.9135,   1.1136],
         [-10.8596, -11.7319, -10.7703,  ...,   1.3078,   1.3801,   1.6084],
         [ -6.2948,  -7.3397,  -6.9123,  ...,   0.8405,   0.9806,   1.0359]]],
       device='cuda:0')]

>> (validate) mask_positions_in_input_ids:  [[87]]8, 3264])):  [tensor([[[-15.7500, -15.7500, -15.7500,  ..., -14.6875, -14.6875, -14.6875],
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 900])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 900])):  [tensor([[[-17.3750, -17.3750, -17.2612,  ..., -15.1057, -15.0687, -15.0078],
         [-17.3750, -17.3750, -17.2612,  ..., -15.1057, -15.0687, -15.0078],
         [-17.2859, -17.2859, -17.2046,  ..., -15.1933, -15.1152, -14.9973],
         ...,
         [-17.6026, -17.6026, -18.4778,  ..., -17.9235, -17.2041, -15.9426],
         [-17.3750, -17.3750, -18.2557,  ..., -17.7752, -17.0462, -15.7422],
         [-17.3750, -17.3750, -18.2557,  ..., -17.7752, -17.0462, -15.7422]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Some dishes are baked in the oven to enhance their flavors. What object in the picture is commonly used to place the dishes in the oven for baking? Please output segmentation mask. ASSISTANT: some dishes are baked in the oven to enhance their flavors. what object in the picture is commonly used to place the dishes in the oven for baking</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[89]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 900, 1200])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 900, 1200])):  [tensor([[[-2.3438, -2.3438, -2.3264,  ..., -2.7823, -2.7031, -2.7031],
         [-2.3438, -2.3438, -2.3264,  ..., -2.7823, -2.7031, -2.7031],
         [-2.3023, -2.3023, -2.2886,  ..., -2.8372, -2.7674, -2.7674],
         ...,
         [-3.4603, -3.4603, -3.4955,  ..., -3.6252, -3.6273, -3.6273],
         [-3.8500, -3.8500, -3.8778,  ..., -4.1296, -4.1294, -4.1294],
         [-4.3984, -4.3984, -4.4108,  ..., -4.8873, -4.8789, -4.8789]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the object that might reflect this person's marital status in this image? Please output segmentation mask. ASSISTANT: the object that might reflect this person's marital status</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[69]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1066, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1066, 1600])):  [tensor([[[-10.9375, -10.9375, -10.9242,  ...,  -8.0613,  -8.0625,  -8.0625],
         [-10.9375, -10.9375, -10.9242,  ...,  -8.0613,  -8.0625,  -8.0625],
         [-10.9437, -10.9437, -10.9308,  ...,  -8.0662,  -8.0672,  -8.0672],
         ...,
         [ -9.4375,  -9.4375,  -9.4547,  ...,  -7.2744,  -7.2751,  -7.2751],
         [ -9.4375,  -9.4375,  -9.4547,  ...,  -7.3423,  -7.3400,  -7.3400],
         [ -9.4375,  -9.4375,  -9.4547,  ...,  -7.3911,  -7.3867,  -7.3867]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is knobs without screws in the center in this image? Please output segmentation mask. ASSISTANT: knobs without screws in the center</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 615, 461])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 615, 461])):  [tensor([[[-10.3750, -10.3438, -10.2397,  ..., -10.9684, -10.6560, -10.3437],
         [-10.3984, -10.4090, -10.4444,  ..., -11.0375, -10.6959, -10.3180],
         [-10.4764, -10.6266, -11.1275,  ..., -11.2682, -10.8291, -10.2324],
         ...,
         [ -9.5018,  -9.9557, -11.4692,  ...,  -9.7219,  -9.6743,  -8.8612],
         [ -9.1638,  -9.6047, -11.0749,  ...,  -9.0765,  -9.2454,  -8.3266],
         [ -9.0625,  -9.4995, -10.9568,  ...,  -8.8831,  -9.1168,  -8.1663]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is where the garbage should be put in this image? Please output segmentation mask. ASSISTANT: where the garbage should be put</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[-8.3750, -8.3750, -8.3750,  ..., -9.8750, -9.8750, -9.8750],
         [-8.3750, -8.3750, -8.3750,  ..., -9.8750, -9.8750, -9.8750],
         [-8.3750, -8.3750, -8.3750,  ..., -9.8750, -9.8750, -9.8750],
         ...,
         [-5.4738, -5.4738, -5.4738,  ..., -1.9872, -1.9872, -1.9872],
         [-5.6343, -5.6343, -5.6343,  ..., -2.6087, -2.6087, -2.6087],
         [-5.6719, -5.6719, -5.6719,  ..., -2.7544, -2.7544, -2.7544]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Dogs are faithful companions to humans, and humans often play fetch games with them. What object will the dog likely retrieve and bring back to the human for the next round of fetch in the picture? Please output segmentation mask. ASSISTANT: dogs are faithful companions to humans, and humans often play fetch games with them. what object will the dog likely retrieve and bring back to the human for the next round of fetch in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[92]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1816, 3003])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1816, 3003])):  [tensor([[[-22.1250, -22.1250, -22.1250,  ..., -17.8750, -17.8750, -17.8750],
         [-22.1250, -22.1250, -22.1250,  ..., -17.8750, -17.8750, -17.8750],
         [-22.1250, -22.1250, -22.1250,  ..., -17.8750, -17.8750, -17.8750],
         ...,
         [-23.1793, -23.1793, -23.1793,  ..., -18.9552, -18.9552, -18.9552],
         [-23.2326, -23.2326, -23.2326,  ..., -18.6356, -18.6356, -18.6356],
         [-23.2344, -23.2344, -23.2344,  ..., -18.6250, -18.6250, -18.6250]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n People often add gas to water to improve its taste. What part of the picture will be consumed as a beverage? Please output segmentation mask. ASSISTANT: people often add gas to water to improve its taste. what part of the picture will be consumed as a beverage</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.],s:  [[87]]8, 3264])):  [tensor([[[-15.7500, -15.7500, -15.7500,  ..., -14.6875, -14.6875, -14.6875],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1000, 1500])):  [tensor([[[-13.5625, -13.5625, -13.5447,  ..., -11.9742, -12.0000, -12.0000],
         [-13.5625, -13.5625, -13.5447,  ..., -11.9742, -12.0000, -12.0000],
         [-13.5528, -13.5528, -13.5365,  ..., -11.9555, -11.9805, -11.9805],
         ...,
         [ -7.3055,  -7.3055,  -7.3201,  ...,  -7.3949,  -7.4126,  -7.4126],
         [ -7.4623,  -7.4623,  -7.4744,  ...,  -7.2326,  -7.2488,  -7.2488],
         [ -7.6836,  -7.6836,  -7.6942,  ...,  -7.1771,  -7.1914,  -7.1914]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a dark cave, there is no natural light source. What object in the picture can be used to provide light to navigate and explore the cave? Please output segmentation mask. ASSISTANT: in a dark cave, there is no natural light source. what object in the picture can be used to provide light to navigate and explore the cave</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[-22.0000, -21.4938, -19.7500,  ..., -21.1875, -21.1391, -21.1250],
         [-21.8031, -21.3934, -19.9820,  ..., -21.0750, -21.0048, -20.9844],
         [-21.1250, -21.0477, -20.7812,  ..., -20.6875, -20.5422, -20.5000],
         ...,
         [-18.6875, -19.2500, -21.1875,  ..., -19.8750, -20.8438, -21.1250],
         [-15.9328, -16.7708, -19.6570,  ..., -19.1367, -20.5675, -20.9828],
         [-17.4750, -17.9658, -19.6563,  ..., -20.8563, -21.8299, -22.1125]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the car that may need repair in this image? Please output segmentation mask. ASSISTANT: the car that may need repair</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1632, 2176])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1632, 2176])):  [tensor([[[-12.1250, -12.1250, -12.1250,  ..., -12.0625, -12.0625, -12.0625],
         [-12.1250, -12.1250, -12.1250,  ..., -12.0625, -12.0625, -12.0625],
         [-12.1250, -12.1250, -12.1250,  ..., -12.0625, -12.0625, -12.0625],
         ...,
         [ -7.6581,  -7.6581,  -7.6581,  ...,  -8.4955,  -8.4955,  -8.4955],
         [ -8.3199,  -8.3199,  -8.3199,  ...,  -9.9035,  -9.9035,  -9.9035],
         [ -8.6094,  -8.6094,  -8.6094,  ..., -10.5195, -10.5195, -10.5195]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In ancient times, people used different methods to measure time during the day. What object in the picture could have been used as a timekeeping device based on the position of the sun? Please output segmentation mask. ASSISTANT: in ancient times, people used different methods to measure time during the day. what object in the picture could have been used as a timekeeping device based on the position of the sun</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[89]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]:  [tensor([[[-15.7500, -15.7500, -15.7500,  ..., -14.6875, -14.6875, -14.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 540, 720])):  [tensor([[[-11.5000, -11.4901, -11.4514,  ..., -13.4549, -14.2873, -14.5000],
         [-11.5049, -11.5017, -11.4890,  ..., -13.5263, -14.2821, -14.4753],
         [-11.5243, -11.5471, -11.6364,  ..., -13.8059, -14.2619, -14.3785],
         ...,
         [ -8.2691,  -8.6381, -10.0819,  ..., -10.3565, -10.4597, -10.4861],
         [ -6.7529,  -7.0937,  -8.4270,  ...,  -8.9020,  -8.9666,  -8.9831],
         [ -7.6142,  -7.8778,  -8.9088,  ..., -10.1540, -10.1934, -10.2035]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n To keep bread fresh and protected, it is often placed in a protective covering. What item in the picture is commonly used for this purpose? Please output segmentation mask. ASSISTANT: to keep bread fresh and protected, it is often placed in a protective covering. what item in the picture is commonly used for this purpose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[80]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 612, 612])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 612, 612])):  [tensor([[[-6.6250, -6.4936, -6.0622,  ..., -6.6281, -6.3143, -6.2188],
         [-6.5055, -6.4208, -6.1427,  ..., -6.6729, -6.4408, -6.3701],
         [-6.1134, -6.1819, -6.4069,  ..., -6.8201, -6.8559, -6.8668],
         ...,
         [-6.8894, -7.1280, -7.9110,  ..., -7.5404, -7.5770, -7.5881],
         [-6.6149, -6.9118, -7.8861,  ..., -7.0280, -7.3872, -7.4966],
         [-6.5312, -6.8459, -7.8786,  ..., -6.8718, -7.3294, -7.4688]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Sometimes, when a car is parked on the side of the road, the driver may accidentally leave the car door open. What part in the picture can indicate that the car door is open? Please output segmentation mask. ASSISTANT: sometimes, when a car is parked on the side of the road, the driver may accidentally leave the car door open. what part in the picture can indicate that the car door is open</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[91]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2592, 1936])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2592, 1936])):  [tensor([[[-15.4375, -15.4375, -15.4375,  ..., -12.0257, -11.8467, -11.8047],
         [-15.4375, -15.4375, -15.4375,  ..., -12.0257, -11.8467, -11.8047],
         [-15.4375, -15.4375, -15.4375,  ..., -12.0257, -11.8467, -11.8047],
         ...,
         [-12.0625, -12.0625, -12.0625,  ..., -12.9417, -13.0775, -13.1094],
         [-12.0625, -12.0625, -12.0625,  ..., -12.9417, -13.0775, -13.1094],
         [-12.0625, -12.0625, -12.0625,  ..., -12.9417, -13.0775, -13.1094]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that produces pollen in this image? Please output segmentation mask. ASSISTANT: something that produces pollen</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[-10.3125, -10.2698, -10.0688,  ..., -11.3250, -10.9538, -10.8750],
         [-10.2437, -10.2093, -10.0471,  ..., -11.3479, -10.9929, -10.9176],
         [ -9.9191,  -9.9237,  -9.9451,  ..., -11.4561, -11.1776, -11.1185],
         ...,
         [-10.9376, -11.0294, -11.4625,  ..., -13.8453, -14.1575, -14.2238],
         [-11.0376, -11.1084, -11.4425,  ..., -12.7397, -12.8590, -12.8843],
         [-11.1375, -11.1874, -11.4225,  ..., -11.6341, -11.5604, -11.5448]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When purchasing meat from a grocery store, it is often stored and sold in a certain type of container. What object in the picture could be used for this purpose? Please output segmentation mask. ASSISTANT: when purchasing meat from a grocery store, it is often stored and sold in a certain type of container. what object in the picture could be used for this purpose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1920, 2560])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1920, 2560])):  [tensor([[[-3.2031, -3.2031, -3.2031,  ..., -3.0312, -3.0312, -3.0312],
         [-3.2031, -3.2031, -3.2031,  ..., -3.0312, -3.0312, -3.0312],
         [-3.2031, -3.2031, -3.2031,  ..., -3.0312, -3.0312, -3.0312],
         ...,
         [-4.1953, -4.1953, -4.1953,  ..., -4.3867, -4.3867, -4.3867],
         [-4.2734, -4.2734, -4.2734,  ..., -4.7477, -4.7477, -4.7477],
         [-4.2930, -4.2930, -4.2930,  ..., -4.8379, -4.8379, -4.8379]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What object in the picture could be used for defense and firepower in an ancient fort? Please output segmentation mask. ASSISTANT: what object in the picture could be used for defense and firepower in an ancient fort</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[69]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-8.9375, -8.8817, -8.6187,  ..., -8.5063, -8.3980, -8.3750],
         [-8.8948, -8.8468, -8.6204,  ..., -8.5689, -8.4712, -8.4505],
         [-8.6938, -8.6823, -8.6281,  ..., -8.8644, -8.8164, -8.8062],
         ...,
         [-2.1664, -2.1795, -2.2414,  ..., -2.4058, -2.4178, -2.4203],
         [-2.6806, -2.6947, -2.7615,  ..., -2.9911, -2.9424, -2.9320],
         [-3.9794, -3.9762, -3.9611,  ..., -4.9613, -4.8782, -4.8606]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something showing that they are skating in this image? Please output segmentation mask. ASSISTANT: something showing that they are skating</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]

 93%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉          | 186/200 [00:40<00:03,  3.72it/s]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 2304])):  [tensor([[[-9.0625, -9.0625, -9.0625,  ..., -1.0027, -1.1035, -1.1035],
         [-9.0625, -9.0625, -9.0625,  ..., -1.0027, -1.1035, -1.1035],
         [-9.0625, -9.0625, -9.0625,  ..., -1.0027, -1.1035, -1.1035],
         ...,
         [-9.5000, -9.5000, -9.5000,  ..., -2.8193, -2.8457, -2.8457],
         [-9.5000, -9.5000, -9.5000,  ..., -2.8193, -2.8457, -2.8457],
         [-9.5000, -9.5000, -9.5000,  ..., -2.8193, -2.8457, -2.8457]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n At a car show, visitors can get close to the displayed vehicles to admire their design and features. What part of the car in this picture is open, allowing viewers to see the engine compartment? Please output segmentation mask. ASSISTANT: at a car show, visitors can get close to the displayed vehicles to admire their design and features. what part of the car in this picture is open, allowing viewers to see the engine compartment</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[93]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2356, 3141])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2356, 3141])):  [tensor([[[-10.4375, -10.4375, -10.4375,  ...,  -9.3750,  -9.3750,  -9.3750],
         [-10.4375, -10.4375, -10.4375,  ...,  -9.3750,  -9.3750,  -9.3750],
         [-10.4375, -10.4375, -10.4375,  ...,  -9.3750,  -9.3750,  -9.3750],
         ...,
         [ -8.6805,  -8.6805,  -8.6805,  ...,  -9.3934,  -9.3934,  -9.3934],
         [ -8.8281,  -8.8281,  -8.8281,  ...,  -9.7969,  -9.7969,  -9.7969],
         [ -8.8281,  -8.8281,  -8.8281,  ...,  -9.7969,  -9.7969,  -9.7969]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When snorkeling or scuba diving, it is important to wear certain equipment to protect the eyes and aid in vision. What object in the picture can be used for this purpose? Please output segmentation mask. ASSISTANT: when snorkeling or scuba diving, it is important to wear certain equipment to protect the eyes and aid in vision. what object in the picture can be used for this purpose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[89]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-19.7500, -19.7500, -19.3281,  ..., -15.2969, -14.8750, -14.8750],
         [-19.7500, -19.7500, -19.3281,  ..., -15.2969, -14.8750, -14.8750],
         [-19.2109, -19.2109, -18.9111,  ..., -15.4258, -15.0781, -15.0781],
         ...,
         [-15.7188, -15.7188, -15.9844,  ..., -15.6689, -15.8516, -15.8516],
         [-15.9375, -15.9375, -16.1660,  ..., -16.4268, -16.6016, -16.6016],
         [-16.8125, -16.8125, -16.9043,  ..., -18.3271, -18.4297, -18.4297]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the places for further exploration in this image? Please output segmentation mask. ASSISTANT: the places for further exploration</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-3.0312, -3.0337, -3.0453,  ..., -4.2312, -4.3241, -4.3438],
         [-3.0747, -3.0786, -3.0967,  ..., -4.2736, -4.3599, -4.3782],
         [-3.2797, -3.2900, -3.3387,  ..., -4.4731, -4.5288, -4.5406],
         ...,
         [ 0.9316,  0.9262,  0.9007,  ..., -0.4973, -0.3927, -0.3705],
         [ 0.5244,  0.5140,  0.4651,  ..., -0.9968, -0.8593, -0.8302],
         [-0.4907, -0.5067, -0.5820,  ..., -2.5195, -2.4128, -2.3902]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When enjoying an ice cream sundae, what can we use to scoop up the whipped cream and place it on top of the ice cream? Please output segmentation mask. ASSISTANT: when enjoying an ice cream sundae, what can we use to scoop up the whipped cream and place it on top of the ice cream</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[86]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2048, 1536])):  [tensor([[[0., 0., 0.,  ..., 1., 1., 1.],
         [0., 0., 0.,  ..., 1., 1., 0.],
         [0., 0., 0.,  ..., 1., 1., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2048, 1536])):  [tensor([[[ -6.3125,  -6.3125,  -6.3125,  ...,  -2.9482,  -3.5283,  -3.8184],
         [ -6.3125,  -6.3125,  -6.3125,  ...,  -2.9482,  -3.5283,  -3.8184],
         [ -6.3125,  -6.3125,  -6.3125,  ...,  -2.9482,  -3.5283,  -3.8184],
         ...,
         [-10.2500, -10.2500, -10.2500,  ..., -11.3750, -11.0000, -10.8125],
         [-10.2500, -10.2500, -10.2500,  ..., -11.3750, -11.0000, -10.8125],
         [-10.2500, -10.2500, -10.2500,  ..., -11.3750, -11.0000, -10.8125]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the person who is speaking currently in this image? Please output segmentation mask. ASSISTANT: the person who is speaking currently</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]

(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
         [-7.8702, -7.8553, -7.7917,  ..., -8.7304, -8.5891, -8.5561],
         [-7.7158, -7.7317, -7.8001,  ..., -8.9008, -8.8161, -8.7963],
         ...,
         [-7.3546, -7.3832, -7.5054,  ..., -8.3475, -8.4374, -8.4583],
         [-7.4909, -7.5100, -7.5919,  ..., -7.8335, -7.9406, -7.9656],
         [-7.7956, -7.8099, -7.8710,  ..., -7.7415, -7.8126, -7.8292]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the solitary tree in this image? Please output segmentation mask. ASSISTANT: the solitary tree</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[ -9.7500,  -9.7500,  -9.7500,  ...,  -9.0625,  -9.0625,  -9.0625],
         [ -9.7500,  -9.7500,  -9.7500,  ...,  -9.0625,  -9.0625,  -9.0625],
         [ -9.7500,  -9.7500,  -9.7500,  ...,  -9.0625,  -9.0625,  -9.0625],
         ...,
         [ -7.7812,  -7.7812,  -7.7812,  ...,  -9.9492,  -9.9492,  -9.9492],
         [ -8.5938,  -8.5938,  -8.5938,  ..., -11.2070, -11.2070, -11.2070],
         [ -9.0000,  -9.0000,  -9.0000,  ..., -11.8359, -11.8359, -11.8359]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something used for contacting other people in this image? Please output segmentation mask. ASSISTANT: something used for contacting other people</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[ -9.1250,  -9.1250,  -9.1250,  ..., -10.5000, -10.5000, -10.5000],
         [ -9.1250,  -9.1250,  -9.1250,  ..., -10.5000, -10.5000, -10.5000],
         [ -9.1250,  -9.1250,  -9.1250,  ..., -10.5000, -10.5000, -10.5000],
         ...,
         [-10.2003, -10.2003, -10.2003,  ..., -11.8662, -11.8662, -11.8662],
         [-10.4326, -10.4326, -10.4326,  ..., -12.1197, -12.1197, -12.1197],
         [-10.5234, -10.5234, -10.5234,  ..., -12.2188, -12.2188, -12.2188]]],
       device='cuda:0')]
giou: 0.3315, ciou: 0.2730 | BIO per cls acc: O=1.0000, B=0.6800, I=0.9275

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:43<00:00,  4.58it/s]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 3 is less than current step: 1999. Dropping entry: {'val/giou': 0.331498384475708, 'val/ciou': 0.2729766368865967, 'val/b_acc': 0.6799999660000017, 'val/i_acc': 0.9275433009818412, 'val/o_acc': 0.9999849282592336, '_timestamp': 1743863144.8237505}).
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
  0%|                                                                                                                                                         | 0/200 [00:00<?, ?it/s]

         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')] [tensor([[[-7.9062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[-19.8750, -19.8750, -19.8484,  ..., -18.7625, -18.7500, -18.7500],
         [-19.8750, -19.8750, -19.8484,  ..., -18.7625, -18.7500, -18.7500],
         [-19.8531, -19.8531, -19.8277,  ..., -18.8074, -18.7953, -18.7953],
         ...,
         [-19.7515, -19.7515, -19.8094,  ..., -18.8263, -18.8500, -18.8500],
         [-20.2525, -20.2525, -20.2981,  ..., -20.5021, -20.5224, -20.5224],
         [-20.6406, -20.6406, -20.6768,  ..., -21.8415, -21.8594, -21.8594]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n For the safety of newborns, they are often placed in a secure and comfortable space when they sleep. What furniture in the picture is commonly used to provide a safe sleeping environment for babies? Please output segmentation mask. ASSISTANT: for the safety of newborns, they are often placed in a secure and comfortable space when they sleep. what furniture in the picture is commonly used to provide a safe sleeping environment for babies</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[94]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1002, 1455])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1002, 1455])):  [tensor([[[-7.3125, -7.3125, -7.3095,  ..., -8.4030, -8.4375, -8.4375],
         [-7.3125, -7.3125, -7.3095,  ..., -8.4030, -8.4375, -8.4375],
         [-7.3246, -7.3246, -7.3229,  ..., -8.4428, -8.4759, -8.4759],
         ...,
         [-5.7220, -5.7220, -5.7566,  ..., -6.1756, -6.1777, -6.1777],
         [-5.6176, -5.6176, -5.6436,  ..., -6.3329, -6.3316, -6.3316],
         [-5.5352, -5.5352, -5.5544,  ..., -6.4571, -6.4531, -6.4531]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When using a film camera to take photos, what part of the camera helps focus the light on the film to capture the image? Please output segmentation mask. ASSISTANT: when using a film camera to take photos, what part of the camera helps focus the light on the film to capture the image</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[77]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1067, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1067, 1600])):  [tensor([[[-14.6250, -14.6250, -14.6047,  ..., -14.0656, -14.0625, -14.0625],
         [-14.6250, -14.6250, -14.6047,  ..., -14.0656, -14.0625, -14.0625],
         [-14.6172, -14.6172, -14.5974,  ..., -14.0789, -14.0758, -14.0758],
         ...,
         [-13.2844, -13.2844, -13.3047,  ..., -13.0409, -13.0376, -13.0376],
         [-13.4223, -13.4223, -13.4430,  ..., -13.0167, -13.0127, -13.0127],
         [-13.5625, -13.5625, -13.5834,  ..., -13.0280, -13.0234, -13.0234]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Birds have various ways of searching for food. What part of their body helps them to grab and pick up food from the ground in the picture? Please output segmentation mask. ASSISTANT: birds have various ways of searching for food. what part of their body helps them to grab and pick up food from the ground in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2570, 2229])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2570, 2229])):  [tensor([[[-16.2500, -16.2500, -16.2500,  ..., -13.5018, -13.6761, -13.7188],
         [-16.2500, -16.2500, -16.2500,  ..., -13.5018, -13.6761, -13.7188],
         [-16.2500, -16.2500, -16.2500,  ..., -13.5018, -13.6761, -13.7188],
         ...,
         [-13.7500, -13.7500, -13.7500,  ..., -17.9960, -17.5976, -17.5000],
         [-13.7500, -13.7500, -13.7500,  ..., -17.9960, -17.5976, -17.5000],
         [-13.7500, -13.7500, -13.7500,  ..., -17.9960, -17.5976, -17.5000]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the sour food in this image? Please output segmentation mask. ASSISTANT: the sour food</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[-4.4688, -4.4969, -4.5938,  ..., -4.0000, -3.9516, -3.9375],
         [-4.4617, -4.4887, -4.5814,  ..., -3.9473, -3.8975, -3.8830],
         [-4.4375, -4.4604, -4.5391,  ..., -3.7656, -3.7111, -3.6953],
         ...,
         [-3.3438, -3.3332, -3.2969,  ..., -6.7188, -6.6945, -6.6875],
         [-3.2842, -3.2789, -3.2608,  ..., -6.7184, -6.8243, -6.8551],
         [-4.3891, -4.3813, -4.3547,  ..., -7.8094, -7.9256, -7.9594]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that protects the woman's eyes from getting wet in this image? Please output segmentation mask. ASSISTANT: something that protects the woman's eyes from getting wet</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[69]]
>> (validate) sampled_classes_list:  [None]

  5%|███████▏                                                                                                                                        | 10/200 [00:04<01:00,  3.14it/s]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1402, 2102])):  [tensor([[[-7.4062, -7.4062, -7.4062,  ..., -5.8750, -5.8750, -5.8750],
         [-7.4062, -7.4062, -7.4062,  ..., -5.8750, -5.8750, -5.8750],
         [-7.4062, -7.4062, -7.4062,  ..., -5.8750, -5.8750, -5.8750],
         ...,
         [-3.6180, -3.6180, -3.6180,  ..., -3.7610, -3.7610, -3.7610],
         [-3.7778, -3.7778, -3.7778,  ..., -3.7429, -3.7429, -3.7429],
         [-3.8535, -3.8535, -3.8535,  ..., -3.7344, -3.7344, -3.7344]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If a person wanted to charge their electric car while parked in the street, what object in the picture could they use to connect the car to an electrical power source? Please output segmentation mask. ASSISTANT: if a person wanted to charge their electric car while parked in the street, what object in the picture could they use to connect the car to an electrical power source</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[86]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2016, 3032])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2016, 3032])):  [tensor([[[-11.3750, -11.3750, -11.3750,  ..., -12.2500, -12.2500, -12.2500],
         [-11.3750, -11.3750, -11.3750,  ..., -12.2500, -12.2500, -12.2500],
         [-11.3750, -11.3750, -11.3750,  ..., -12.2500, -12.2500, -12.2500],
         ...,
         [ -9.1451,  -9.1451,  -9.1451,  ...,  -7.8614,  -7.8614,  -7.8614],
         [ -9.3246,  -9.3246,  -9.3246,  ...,  -8.0356,  -8.0356,  -8.0356],
         [ -9.3281,  -9.3281,  -9.3281,  ...,  -8.0391,  -8.0391,  -8.0391]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Where do people put their dirty hygiene products to keep the bathroom clean? Please output segmentation mask. ASSISTANT: where do people put their dirty hygiene products to keep the bathroom clean</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-18.2375, -14.6689, -14.5496,  ..., -14.6438, -16.2742, -17.4935],
         [-14.3252, -15.3650, -15.5594,  ..., -15.6750, -16.2313, -18.3367],
         [-13.2523, -15.1000, -15.5781,  ..., -15.4062, -15.7344, -17.7754],
         ...,
         [-14.7352, -15.6375, -14.5000,  ..., -11.9688, -12.0219, -13.0746],
         [-14.7658, -16.3213, -14.6344,  ..., -11.8969, -11.7769, -13.4570],
         [-14.3530, -17.0498, -14.8000,  ..., -11.6598, -11.4860, -13.3444]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When the wind blows, small white objects are blown away and scattered in the air. What in the picture is responsible for this phenomenon? Please output segmentation mask. ASSISTANT: when the wind blows, small white objects are blown away and scattered in the air. what in the picture is responsible for this phenomenon</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[-15.4375, -15.3522, -14.9500,  ..., -16.6375, -16.7303, -16.7500],
         [-15.4801, -15.4034, -15.0417,  ..., -16.7384, -16.8020, -16.8155],
         [-15.6810, -15.6449, -15.4745,  ..., -17.2145, -17.1404, -17.1246],
         ...,
         [-15.7491, -15.7773, -15.9105,  ..., -12.5491, -13.2605, -13.4114],
         [-14.7495, -14.8018, -15.0488,  ..., -11.5194, -12.0577, -12.1718],
         [-13.7498, -13.8263, -14.1871,  ..., -10.4898, -10.8548, -10.9323]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that the dog's food should be put into in this image? Please output segmentation mask. ASSISTANT: something that the dog's food should be put into</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[-14.6250, -14.5477, -14.2812,  ..., -15.7812, -15.9508, -16.0000],
         [-14.8359, -14.7831, -14.6012,  ..., -16.1645, -16.2877, -16.3234],
         [-15.5625, -15.5941, -15.7031,  ..., -17.4844, -17.4480, -17.4375],
         ...,
         [-16.4375, -16.7469, -17.8125,  ..., -10.0547, -10.6420, -10.8125],
         [-14.6766, -15.0224, -16.2137,  ...,  -8.4055,  -8.9347,  -9.0883],
         [-14.8750, -15.0698, -15.7406,  ..., -10.7610, -11.1255, -11.2313]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the food that makes people feel spicy or hot in this image? Please output segmentation mask. ASSISTANT: the food that makes people feel spicy or hot</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[67]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 679, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 679, 1024])):  [tensor([[[-12.0625, -12.0625, -11.9219,  ...,  -5.0000,  -5.0625,  -5.0625],
         [-12.0625, -12.0625, -11.9219,  ...,  -5.0000,  -5.0625,  -5.0625],
         [-11.8594, -11.8594, -11.7646,  ...,  -5.0503,  -5.0820,  -5.0820],
         ...,
         [ -6.5156,  -6.5156,  -6.5762,  ...,  -7.5239,  -7.5117,  -7.5117],
         [ -6.5469,  -6.5469,  -6.6270,  ...,  -7.0991,  -7.0664,  -7.0664],
         [ -7.3438,  -7.3438,  -7.4409,  ...,  -7.6431,  -7.6055,  -7.6055]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the object used to protect the ears in this image? Please output segmentation mask. ASSISTANT: the object used to protect the ears</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]

         [-12.6250, -12.6250, -12.6250,  ...,  -3.2646,  -3.3418,  -3.3418],062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
         [-12.6250, -12.6250, -12.6250,  ...,  -3.2646,  -3.3418,  -3.3418],
         ...,
         [-12.2500, -12.2500, -12.2500,  ..., -11.5907, -11.6484, -11.6484],
         [-12.2500, -12.2500, -12.2500,  ..., -11.5907, -11.6484, -11.6484],
         [-12.2500, -12.2500, -12.2500,  ..., -11.5907, -11.6484, -11.6484]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the part of the persons that is above the water in this image? Please output segmentation mask. ASSISTANT: the part of the persons that is above the water</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[67]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[-15.1875, -15.1875, -15.1875,  ..., -15.8750, -15.8750, -15.8750],
         [-15.1875, -15.1875, -15.1875,  ..., -15.8750, -15.8750, -15.8750],
         [-15.1875, -15.1875, -15.1875,  ..., -15.8750, -15.8750, -15.8750],
         ...,
         [-13.9427, -13.9427, -13.9427,  ..., -12.9714, -12.9714, -12.9714],
         [-14.1406, -14.1406, -14.1406,  ..., -13.6953, -13.6953, -13.6953],
         [-14.1406, -14.1406, -14.1406,  ..., -13.6953, -13.6953, -13.6953]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When we need to access or store things above our reach, what would be helpful to stand on? Please output segmentation mask. ASSISTANT: when we need to access or store things above our reach, what would be helpful to stand on</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[71]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[ -8.3730,  -8.1753,  -8.5480,  ...,  -7.2873,  -7.8036,  -8.9226],
         [ -8.4761,  -9.2238,  -9.4719,  ...,  -8.4797,  -9.2022,  -9.2430],
         [ -8.4211,  -9.2375,  -9.4062,  ...,  -8.0703,  -8.3813,  -8.6820],
         ...,
         [ -9.3398, -10.8438, -10.7812,  ...,  -3.1367,  -3.2680,  -3.3808],
         [ -9.7182, -11.4069, -10.9219,  ...,  -2.7930,  -3.0003,  -3.1872],
         [ -8.9393, -10.4109,  -9.7754,  ...,  -3.9164,  -4.0862,  -4.2003]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What structure can be used to drain the water from the bathroom floor and prevent the accumulation of wastewater? Please output segmentation mask. ASSISTANT: what structure can be used to drain the water from the bathroom floor and prevent the accumulation of wastewater</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[75]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-19.8132, -17.5294, -17.4484,  ..., -13.8852, -14.9713, -16.4773],
         [-18.7325, -18.4575, -18.0500,  ..., -16.1625, -17.5150, -16.7759],
         [-18.5719, -19.0875, -17.9062,  ..., -15.5781, -16.0719, -15.8785],
         ...,
         [-22.2961, -24.8312, -22.9688,  ..., -18.5000, -18.3375, -19.1109],
         [-22.1975, -25.1581, -22.2531,  ..., -17.2781, -16.1619, -17.7646],
         [-15.7351, -18.5243, -17.0574,  ..., -12.2910, -12.2805, -14.3591]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the dog that puts its tongue out in this image? Please output segmentation mask. ASSISTANT: the dog that puts its tongue out</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1600, 1200])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1600, 1200])):  [tensor([[[-12.6250, -12.6250, -12.6062,  ..., -12.4320, -12.1700, -11.9688],
         [-12.6250, -12.6250, -12.6062,  ..., -12.4320, -12.1700, -11.9688],
         [-12.5953, -12.5953, -12.5772,  ..., -12.4298, -12.1645, -11.9604],
         ...,
         [-12.2453, -12.2453, -12.2716,  ..., -10.0691, -10.0104,  -9.9573],
         [-12.2500, -12.2500, -12.2758,  ..., -10.0594,  -9.9950,  -9.9375],
         [-12.2500, -12.2500, -12.2758,  ..., -10.0594,  -9.9950,  -9.9375]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a rural landscape, what objects in the picture could provide shade and shelter for animals or humans? Please output segmentation mask. ASSISTANT: in a rural landscape, what objects in the picture could provide shade and shelter for animals or humans</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[72]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[ -6.8438,  -6.8297,  -6.7812,  ...,  -7.4844,  -7.3754,  -7.3438],
         [ -6.8121,  -6.8099,  -6.8023,  ...,  -7.5090,  -7.4354,  -7.4141],
         [ -6.7031,  -6.7418,  -6.8750,  ...,  -7.5938,  -7.6422,  -7.6562],
         ...,
         [ -7.8281,  -7.8967,  -8.1328,  ...,  -7.8047,  -8.1861,  -8.2969],
         [ -7.6813,  -7.7620,  -8.0403,  ...,  -7.8293,  -8.0204,  -8.0758],
         [ -9.5281,  -9.5496,  -9.6235,  ..., -10.7328, -10.9133, -10.9657]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something used for playing videos or music in this image? Please output segmentation mask. ASSISTANT: something used for playing videos or music</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-2.9219, -2.9219, -2.9219,  ..., -6.2266, -6.1875, -6.1875],
         [-2.9219, -2.9219, -2.9219,  ..., -6.2266, -6.1875, -6.1875],
         [-2.8359, -2.8359, -2.8364,  ..., -6.2915, -6.2773, -6.2773],
         ...,
         [-7.0820, -7.0820, -7.1533,  ..., -5.8789, -5.9375, -5.9375],
         [-7.4883, -7.4883, -7.5474,  ..., -6.8760, -6.9219, -6.9219],
         [-8.2773, -8.2773, -8.3062,  ..., -8.9717, -9.0156, -9.0156]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When celebrating birthdays, it is common to have a cake with decorations. What part of the cake in the picture is typically used to write birthday greetings or the name of the person celebrating? Please output segmentation mask. ASSISTANT: when celebrating birthdays, it is common to have a cake with decorations. what part of the cake in the picture is typically used to write birthday greetings or the name of the person celebrating</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[97]]
>> (validate) sampled_classes_list:  [None]


         [-14.9149, -14.9149, -14.8481,  ..., -15.5505, -15.4878, -15.4878],062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
         ...,
         [-13.3885, -13.3885, -13.7245,  ..., -13.2934, -13.3906, -13.3906],
         [-13.4896, -13.4896, -13.7783,  ..., -13.7640, -13.8646, -13.8646],
         [-14.0234, -14.0234, -14.2005,  ..., -14.6468, -14.7109, -14.7109]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n This rocky terrain can be challenging to navigate. What object in the picture could provide information to guide travelers through this area? Please output segmentation mask. ASSISTANT: this rocky terrain can be challenging to navigate. what object in the picture could provide information to guide travelers through this area</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2592, 3888])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2592, 3888])):  [tensor([[[-11.6250, -11.6250, -11.6250,  ..., -13.8750, -13.8750, -13.8750],
         [-11.6250, -11.6250, -11.6250,  ..., -13.8750, -13.8750, -13.8750],
         [-11.6250, -11.6250, -11.6250,  ..., -13.8750, -13.8750, -13.8750],
         ...,
         [ -9.3447,  -9.3447,  -9.3447,  ...,  -8.8486,  -8.8486,  -8.8486],
         [ -9.3906,  -9.3906,  -9.3906,  ...,  -8.8672,  -8.8672,  -8.8672],
         [ -9.3906,  -9.3906,  -9.3906,  ...,  -8.8672,  -8.8672,  -8.8672]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is The fastest sinking ship in this image? Please output segmentation mask. ASSISTANT: the fastest sinking ship</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 333, 500])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 333, 500])):  [tensor([[[-11.8125, -11.6450, -11.3250,  ..., -12.1938, -12.3538, -12.4375],
         [-11.8795, -11.8287, -11.7317,  ..., -12.6293, -12.7893, -12.8730],
         [-12.0075, -12.1797, -12.5087,  ..., -13.4613, -13.6213, -13.7050],
         ...,
         [ -8.9050,  -8.8628,  -8.7821,  ...,  -9.0390,  -9.1529,  -9.2125],
         [ -9.4540,  -9.5239,  -9.6575,  ...,  -9.0947,  -9.1907,  -9.2410],
         [-10.3180, -10.5423, -10.9708,  ...,  -9.3507,  -9.4467,  -9.4970]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In some regions, people raise certain animals for their milk, meat, and skin. What animal in the picture could be domesticated for such purposes? Please output segmentation mask. ASSISTANT: in some regions, people raise certain animals for their milk, meat, and skin. what animal in the picture could be domesticated for such purposes</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1365, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1365, 2048])):  [tensor([[[-9.0625, -9.0625, -9.0625,  ..., -7.8125, -7.8125, -7.8125],
         [-9.0625, -9.0625, -9.0625,  ..., -7.8125, -7.8125, -7.8125],
         [-9.0625, -9.0625, -9.0625,  ..., -7.8125, -7.8125, -7.8125],
         ...,
         [-7.7918, -7.7918, -7.7918,  ..., -3.2539, -3.2539, -3.2539],
         [-7.8837, -7.8837, -7.8837,  ..., -3.2851, -3.2851, -3.2851],
         [-7.9297, -7.9297, -7.9297,  ..., -3.3008, -3.3008, -3.3008]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the place where the child is about to slip/fall off in this image? Please output segmentation mask. ASSISTANT: the place where the child is about to slip/fall off</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2152, 1704])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2152, 1704])):  [tensor([[[-4.9688, -4.9688, -4.9688,  ..., -6.7215, -6.8517, -6.9102],
         [-4.9688, -4.9688, -4.9688,  ..., -6.7215, -6.8517, -6.9102],
         [-4.9688, -4.9688, -4.9688,  ..., -6.7215, -6.8517, -6.9102],
         ...,
         [-2.5469, -2.5469, -2.5469,  ..., -1.6338, -1.7072, -1.7402],
         [-2.5469, -2.5469, -2.5469,  ..., -1.6338, -1.7072, -1.7402],
         [-2.5469, -2.5469, -2.5469,  ..., -1.6338, -1.7072, -1.7402]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In order to facilitate transportation and connect different regions, what structure in the picture was built across the water? Please output segmentation mask. ASSISTANT: in order to facilitate transportation and connect different regions, what structure in the picture was built across the water</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[74]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3264, 4928])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3264, 4928])):  [tensor([[[-10.3750, -10.3750, -10.3750,  ..., -11.8750, -11.8750, -11.8750],
         [-10.3750, -10.3750, -10.3750,  ..., -11.8750, -11.8750, -11.8750],
         [-10.3750, -10.3750, -10.3750,  ..., -11.8750, -11.8750, -11.8750],
         ...,
         [ -6.4416,  -6.4416,  -6.4416,  ...,  -7.9849,  -7.9849,  -7.9849],
         [ -6.4414,  -6.4414,  -6.4414,  ...,  -7.9727,  -7.9727,  -7.9727],
         [ -6.4414,  -6.4414,  -6.4414,  ...,  -7.9727,  -7.9727,  -7.9727]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the cesspit in this image? Please output segmentation mask. ASSISTANT: the cesspit</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]

         [-5.6562, -5.6562, -5.6289,  ..., -6.2734, -6.3750, -6.3750],4878],062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
         [-5.7891, -5.7891, -5.7710,  ..., -6.4600, -6.5312, -6.5312],
         ...,
         [-3.2754, -3.2754, -3.3579,  ..., -3.7881, -3.8125, -3.8125],
         [-3.3789, -3.3789, -3.4441,  ..., -3.9272, -3.9336, -3.9336],
         [-3.6367, -3.6367, -3.6643,  ..., -4.3208, -4.3008, -4.3008]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Insects can help pollinate flowers effectively, but some insects also collect pollen and nectar to produce honey. What in the picture does not make honey? Please output segmentation mask. ASSISTANT: insects can help pollinate flowers effectively, but some insects also collect pollen and nectar to produce honey. what in the picture does not make honey</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[89]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 651, 500])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 651, 500])):  [tensor([[[-12.8750, -12.6605, -11.9100,  ..., -13.8659, -12.0948, -10.0316],
         [-12.6601, -12.5148, -12.0062,  ..., -13.8858, -12.0523,  -9.9033],
         [-11.9088, -12.0052, -12.3424,  ..., -13.9554, -11.9034,  -9.4545],
         ...,
         [-12.0610, -12.6050, -14.5085,  ..., -14.3571, -13.2104, -11.7775],
         [-11.3331, -11.9754, -14.2224,  ..., -13.9997, -12.7593, -11.1419],
         [-11.1250, -11.7953, -14.1406,  ..., -13.8975, -12.6303, -10.9601]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Flowers are often used to decorate tables during special occasions or events. What item in the picture can be used to hold the flowers and keep them fresh by providing water to them? Please output segmentation mask. ASSISTANT: flowers are often used to decorate tables during special occasions or events. what item in the picture can be used to hold the flowers and keep them fresh by providing water to them</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1370, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1370, 2048])):  [tensor([[[ -8.8750,  -8.8750,  -8.8750,  ..., -12.1875, -12.1875, -12.1875],
         [ -8.8750,  -8.8750,  -8.8750,  ..., -12.1875, -12.1875, -12.1875],
         [ -8.8750,  -8.8750,  -8.8750,  ..., -12.1875, -12.1875, -12.1875],
         ...,
         [ -7.4570,  -7.4570,  -7.4570,  ...,  -9.2266,  -9.2266,  -9.2266],
         [ -7.6055,  -7.6055,  -7.6055,  ...,  -9.1484,  -9.1484,  -9.1484],
         [ -7.6797,  -7.6797,  -7.6797,  ...,  -9.1094,  -9.1094,  -9.1094]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Bamboo is very hard, and its sharp edges can easily scratch people. What tool in the picture can I use to split the bamboo and cross this bamboo forest? Please output segmentation mask. ASSISTANT: bamboo is very hard, and its sharp edges can easily scratch people. what tool in the picture can i use to split the bamboo and cross this bamboo forest</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[92]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[-7.2188, -7.2188, -7.2230,  ..., -0.9339, -0.9336, -0.9336],
         [-7.2188, -7.2188, -7.2230,  ..., -0.9339, -0.9336, -0.9336],
         [-7.2258, -7.2258, -7.2301,  ..., -0.9235, -0.9232, -0.9232],
         ...,
         [-8.2523, -8.2523, -8.2645,  ..., -7.2625, -7.2648, -7.2648],
         [-8.5687, -8.5687, -8.5775,  ..., -7.8934, -7.8944, -7.8944],
         [-8.8203, -8.8203, -8.8264,  ..., -8.3869, -8.3867, -8.3867]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the damaged part on the pants in this image? Please output segmentation mask. ASSISTANT: the damaged part on the pants</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[-5.3750, -5.3750, -5.3750,  ..., -4.5625, -4.5625, -4.5625],
         [-5.3750, -5.3750, -5.3750,  ..., -4.5625, -4.5625, -4.5625],
         [-5.3750, -5.3750, -5.3750,  ..., -4.5625, -4.5625, -4.5625],
         ...,
         [-3.7539, -3.7539, -3.7539,  ..., -2.6953, -2.6953, -2.6953],
         [-3.8086, -3.8086, -3.8086,  ..., -2.9922, -2.9922, -2.9922],
         [-3.8086, -3.8086, -3.8086,  ..., -2.9922, -2.9922, -2.9922]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Seafood dishes often include a tangy condiment that enhances the flavor. What item in the picture can be squeezed onto the seafood as a tangy flavor enhancer? Please output segmentation mask. ASSISTANT: seafood dishes often include a tangy condiment that enhances the flavor. what item in the picture can be squeezed onto the seafood as a tangy flavor enhancer</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[96]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1080, 1080])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1080, 1080])):  [tensor([[[ -8.2500,  -8.2500,  -8.2194,  ..., -10.1250, -10.1250, -10.1250],
         [ -8.2500,  -8.2500,  -8.2194,  ..., -10.1250, -10.1250, -10.1250],
         [ -8.1752,  -8.1752,  -8.1612,  ..., -10.2082, -10.2134, -10.2134],
         ...,
         [ -7.7814,  -7.7814,  -8.0718,  ...,  -8.3538,  -8.4796,  -8.4796],
         [ -7.6250,  -7.6250,  -7.9310,  ...,  -8.3538,  -8.5000,  -8.5000],
         [ -7.6250,  -7.6250,  -7.9310,  ...,  -8.3538,  -8.5000,  -8.5000]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Dogs often like to find a comfortable place to rest. What object in the picture can offer a soft and comfortable surface for the dog to lie on? Please output segmentation mask. ASSISTANT: dogs often like to find a comfortable place to rest. what object in the picture can offer a soft and comfortable surface for the dog to lie on</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[82]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')] -6.3750],4878],062, -7.8842, -7.7898,  ..., -8.6906, -8.5361, -8.5000],4.6875],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1024, 1280])):  [tensor([[[-18.2500, -18.2500, -18.1406,  ..., -15.1523, -15.1875, -15.1875],
         [-18.2500, -18.2500, -18.1406,  ..., -15.1523, -15.1875, -15.1875],
         [-18.2110, -18.2110, -18.1177,  ..., -15.1558, -15.1953, -15.1953],
         ...,
         [-14.0462, -14.0462, -14.1350,  ..., -14.2474, -14.3274, -14.3274],
         [-13.2968, -13.2968, -13.3200,  ..., -13.3732, -13.4287, -13.4287],
         [-13.1328, -13.1328, -13.1201,  ..., -12.9253, -12.9531, -12.9531]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a zoo, there are separate areas designated for different animals. What structure in the picture is used to confine and display animals? Please output segmentation mask. ASSISTANT: in a zoo, there are separate areas designated for different animals. what structure in the picture is used to confine and display animals</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[0., 0., 1.,  ..., 0., 0., 0.],
         [0., 0., 1.,  ..., 0., 0., 0.],
         [0., 0., 1.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[-0.9102, -0.9102, -0.9102,  ..., -1.0469, -1.0469, -1.0469],
         [-0.9102, -0.9102, -0.9102,  ..., -1.0469, -1.0469, -1.0469],
         [-0.9102, -0.9102, -0.9102,  ..., -1.0469, -1.0469, -1.0469],
         ...,
         [-0.7205, -0.7205, -0.7205,  ..., -1.0514, -1.0514, -1.0514],
         [-0.9187, -0.9187, -0.9187,  ..., -1.5322, -1.5322, -1.5322],
         [-0.9651, -0.9651, -0.9651,  ..., -1.6449, -1.6449, -1.6449]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that tastes spicy in this image? Please output segmentation mask. ASSISTANT: something that tastes spicy</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-11.7500, -11.7500, -11.5469,  ...,  -9.4844,  -9.6250,  -9.6250],
         [-11.7500, -11.7500, -11.5469,  ...,  -9.4844,  -9.6250,  -9.6250],
         [-11.6797, -11.6797, -11.5127,  ...,  -9.4619,  -9.5781,  -9.5781],
         ...,
         [-12.6406, -12.6406, -13.1426,  ..., -11.5713, -11.7422, -11.7422],
         [-12.4688, -12.4688, -12.9043,  ..., -11.8291, -11.9922, -11.9922],
         [-12.9062, -12.9062, -13.1504,  ..., -12.4873, -12.6016, -12.6016]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When going fishing on a calm sea, what type of boat shown in the picture would be an ideal choice for a peaceful fishing experience? Please output segmentation mask. ASSISTANT: when going fishing on a calm sea, what type of boat shown in the picture would be an ideal choice for a peaceful fishing experience</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[ -7.7061,  -7.4193,  -7.9271,  ...,  -8.7383,  -9.7686, -10.1908],
         [ -7.3119,  -7.7931,  -8.2172,  ...,  -9.8125, -10.7913, -10.7998],
         [ -7.8551,  -8.4781,  -8.4844,  ...,  -9.5781, -10.1688, -10.3406],
         ...,
         [ -4.9377,  -4.9484,  -4.9375,  ...,  -3.7852,  -4.1344,  -4.2633],
         [ -4.9286,  -5.1263,  -5.0203,  ...,  -3.6430,  -3.9628,  -4.3431],
         [ -5.6782,  -5.8310,  -5.7262,  ...,  -4.5093,  -4.8434,  -4.8917]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Insects have various ways to protect themselves from predators. What characteristics can a moth use to deter potential threats? Please output segmentation mask. ASSISTANT: insects have various ways to protect themselves from predators. what characteristics can a moth use to deter potential threats</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[77]]
>> (validate) sampled_classes_list:  [None]

(PLUM.py) >> pred_masks (shape: torch.Size([1, 1000, 1504])):  [tensor([[[-18.2500, -18.2500, -18.1695,  ..., -15.9319, -15.8750, -15.8750],
         [-18.2500, -18.2500, -18.1695,  ..., -15.9319, -15.8750, -15.8750],
         [-18.1646, -18.1646, -18.0882,  ..., -16.0124, -15.9573, -15.9573],
         ...,
         [-16.5981, -16.5981, -16.6309,  ..., -16.2374, -16.2466, -16.2466],
         [-17.1089, -17.1089, -17.1390,  ..., -16.3163, -16.3105, -16.3105],
         [-17.5000, -17.5000, -17.5280,  ..., -16.3767, -16.3594, -16.3594]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Many people use bags to carry their belongings when they go out. What part of the bag in the picture can be used to carry the bag comfortably over the shoulder? Please output segmentation mask. ASSISTANT: many people use bags to carry their belongings when they go out. what part of the bag in the picture can be used to carry the bag comfortably over the shoulder</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1920, 2560])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1920, 2560])):  [tensor([[[-11.5625, -11.5625, -11.5625,  ..., -11.2500, -11.2500, -11.2500],
         [-11.5625, -11.5625, -11.5625,  ..., -11.2500, -11.2500, -11.2500],
         [-11.5625, -11.5625, -11.5625,  ..., -11.2500, -11.2500, -11.2500],
         ...,
         [ -8.1484,  -8.1484,  -8.1484,  ...,  -8.9062,  -8.9062,  -8.9062],
         [ -8.5203,  -8.5203,  -8.5203,  ...,  -9.3938,  -9.3938,  -9.3938],
         [ -8.6133,  -8.6133,  -8.6133,  ...,  -9.5156,  -9.5156,  -9.5156]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n During a meal, people typically use utensils to bring food to their mouths. What tool in the picture can be used to fulfill this need? Please output segmentation mask. ASSISTANT: during a meal, people typically use utensils to bring food to their mouths. what tool in the picture can be used to fulfill this need</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-12.5000, -12.4705, -12.3313,  ..., -14.2875, -14.2566, -14.2500],
         [-12.4573, -12.4352, -12.3309,  ..., -14.3033, -14.2674, -14.2598],
         [-12.2563, -12.2690, -12.3294,  ..., -14.3775, -14.3187, -14.3062],
         ...,
         [-10.2250, -10.3464, -10.9187,  ..., -10.7644, -11.0103, -11.0625],
         [-10.2620, -10.3880, -10.9820,  ..., -10.9566, -11.2047, -11.2573],
         [-10.5062, -10.5927, -11.0001,  ..., -11.7790, -11.9417, -11.9762]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n During the move, what object can be used to store and transport various sundries and small household items, which is sturdy and relatively easy to carry? Please output segmentation mask. ASSISTANT: during the move, what object can be used to store and transport various sundries and small household items, which is sturdy and relatively easy to carry</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-7.0584, -6.7879, -7.0687,  ..., -7.8369, -8.4752, -8.6906],
         [-7.1102, -7.3272, -7.4125,  ..., -8.3203, -8.8319, -9.0768],
         [-7.3490, -7.6391, -7.5469,  ..., -8.2344, -8.5938, -8.8086],
         ...,
         [-1.1534, -0.9990, -0.8662,  ..., -4.6133, -5.0719, -5.3082],
         [-1.2309, -1.0843, -0.8699,  ..., -4.3602, -4.9838, -5.3834],
         [-1.8601, -1.6843, -1.7897,  ..., -4.9421, -5.3632, -5.4534]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If we were to take a flight in the sky, what part of the plane in the picture would be spinning to generate lift and propel the aircraft forward? Please output segmentation mask. ASSISTANT: if we were to take a flight in the sky, what part of the plane in the picture would be spinning to generate lift and propel the aircraft forward</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2499, 3680])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2499, 3680])):  [tensor([[[-22.7500, -22.7500, -22.7500,  ..., -21.5000, -21.5000, -21.5000],
         [-22.7500, -22.7500, -22.7500,  ..., -21.5000, -21.5000, -21.5000],
         [-22.7500, -22.7500, -22.7500,  ..., -21.5000, -21.5000, -21.5000],
         ...,
         [-18.8814, -18.8814, -18.8814,  ..., -20.4607, -20.4607, -20.4607],
         [-19.0156, -19.0156, -19.0156,  ..., -20.5156, -20.5156, -20.5156],
         [-19.0156, -19.0156, -19.0156,  ..., -20.5156, -20.5156, -20.5156]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something used by a Jedi warrior to attack enemies in this image? Please output segmentation mask. ASSISTANT: something used by a jedi warrior to attack enemies</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 720, 502])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 720, 502])):  [tensor([[[-18.1250, -17.9963, -17.4930,  ..., -15.5991, -15.5333, -15.5111],
         [-17.8776, -17.8085, -17.5381,  ..., -15.8656, -15.6541, -15.4788],
         [-16.9097, -17.0736, -17.7146,  ..., -16.9082, -16.1265, -15.3524],
         ...,
         [-13.3194, -13.7283, -15.3276,  ..., -18.4496, -16.2072, -13.8885],
         [-13.5130, -14.0338, -16.0708,  ..., -19.4722, -16.1918, -12.7994],
         [-13.5625, -14.1118, -16.2608,  ..., -19.7336, -16.1879, -12.5210]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What object in the picture should be used with water to create foam and effectively remove oil and dirt when your hands are very dirty? Please output segmentation mask. ASSISTANT: what object in the picture should be used with water to create foam and effectively remove oil and dirt when your hands are very dirty</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[79]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[-16.5000, -16.5000, -16.5000,  ..., -11.9375, -11.9375, -11.9375],
         [-16.5000, -16.5000, -16.5000,  ..., -11.9375, -11.9375, -11.9375],
         [-16.5000, -16.5000, -16.5000,  ..., -11.9375, -11.9375, -11.9375],
         ...,
         [-13.1835, -13.1835, -13.1835,  ..., -12.3283, -12.3283, -12.3283],
         [-13.6976, -13.6976, -13.6976,  ..., -13.2719, -13.2719, -13.2719],
         [-13.8984, -13.8984, -13.8984,  ..., -13.6406, -13.6406, -13.6406]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a concert or public speaking event, what object in the picture would be used by the speaker or performer to amplify their voice? Please output segmentation mask. ASSISTANT: in a concert or public speaking event, what object in the picture would be used by the speaker or performer to amplify their voice</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[79]]
>> (validate) sampled_classes_list:  [None]

         [ -6.2815,  -6.3029,  -6.4038,  ...,  -5.8982,  -5.7530,  -5.7222],8.2500, -18.2500, -18.1695,  ..., -15.9319, -15.8750, -15.8750],
         [ -5.8406,  -5.8779,  -6.0534,  ...,  -5.4770,  -5.4496,  -5.4437],
         ...,
         [ -7.4188,  -7.4617,  -7.6644,  ...,  -6.8803,  -7.0203,  -7.0500],
         [ -7.5825,  -7.6270,  -7.8368,  ...,  -7.3269,  -7.4208,  -7.4407],
         [ -9.2087,  -9.2221,  -9.2852,  ..., -10.1556, -10.2592, -10.2812]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a motorcycle race, there are often sharp turns that require skilled maneuvering. What part of the race track in the picture indicates a sharp turn? Please output segmentation mask. ASSISTANT: in a motorcycle race, there are often sharp turns that require skilled maneuvering. what part of the race track in the picture indicates a sharp turn</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 682, 1023])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [0., 0., 0.,  ..., 1., 1., 1.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 682, 1023])):  [tensor([[[ -9.6875,  -9.6873,  -9.5540,  ..., -11.9837, -12.1248, -12.1250],
         [ -9.6873,  -9.6872,  -9.5539,  ..., -11.9838, -12.1249, -12.1251],
         [ -9.6167,  -9.6165,  -9.4891,  ..., -12.0507, -12.1642, -12.1643],
         ...,
         [ -7.4429,  -7.4429,  -7.4992,  ...,  -6.7112,  -6.7279,  -6.7279],
         [ -7.0438,  -7.0440,  -7.1228,  ...,  -6.3415,  -6.3679,  -6.3680],
         [ -7.2459,  -7.2460,  -7.3063,  ...,  -6.6171,  -6.6014,  -6.6014]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the region showing current time in this image? Please output segmentation mask. ASSISTANT: the region showing current time</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-10.3750, -10.3750, -10.3281,  ..., -11.0703, -11.1250, -11.1250],
         [-10.3750, -10.3750, -10.3281,  ..., -11.0703, -11.1250, -11.1250],
         [-10.4688, -10.4688, -10.4404,  ..., -11.2119, -11.2344, -11.2344],
         ...,
         [ -8.4609,  -8.4609,  -8.7256,  ...,  -8.4932,  -8.5078,  -8.5078],
         [ -8.3438,  -8.3438,  -8.5898,  ...,  -8.8057,  -8.8359,  -8.8359],
         [ -8.4062,  -8.4062,  -8.5820,  ...,  -9.5889,  -9.6328,  -9.6328]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the reflection of the object in this image? Please output segmentation mask. ASSISTANT: the reflection of the object</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2820, 3760])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2820, 3760])):  [tensor([[[-7.1250, -7.1250, -7.1250,  ..., -8.2500, -8.2500, -8.2500],
         [-7.1250, -7.1250, -7.1250,  ..., -8.2500, -8.2500, -8.2500],
         [-7.1250, -7.1250, -7.1250,  ..., -8.2500, -8.2500, -8.2500],
         ...,
         [-4.1415, -4.1415, -4.1415,  ..., -4.8321, -4.8321, -4.8321],
         [-4.3301, -4.3301, -4.3301,  ..., -5.1133, -5.1133, -5.1133],
         [-4.3301, -4.3301, -4.3301,  ..., -5.1133, -5.1133, -5.1133]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Dogs use their mouths to perform various tasks, including eating and vocalizing. What part of the dog's body is primarily responsible for these actions? Please output segmentation mask. ASSISTANT: dogs use their mouths to perform various tasks, including eating and vocalizing. what part of the dog's body is primarily responsible for these actions</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3264, 4928])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3264, 4928])):  [tensor([[[-11.1250, -11.1250, -11.1250,  ..., -14.5625, -14.5625, -14.5625],
         [-11.1250, -11.1250, -11.1250,  ..., -14.5625, -14.5625, -14.5625],
         [-11.1250, -11.1250, -11.1250,  ..., -14.5625, -14.5625, -14.5625],
         ...,
         [ -8.7743,  -8.7743,  -8.7743,  ...,  -8.3368,  -8.3368,  -8.3368],
         [ -8.7734,  -8.7734,  -8.7734,  ...,  -8.3359,  -8.3359,  -8.3359],
         [ -8.7734,  -8.7734,  -8.7734,  ...,  -8.3359,  -8.3359,  -8.3359]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that protects the persons' heads in this image? Please output segmentation mask. ASSISTANT: something that protects the persons' heads</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]

         [-6.0938, -6.0938, -6.0938,  ..., -6.2188, -6.2188, -6.2188],7222],8.2500, -18.2500, -18.1695,  ..., -15.9319, -15.8750, -15.8750],
         ...,
         [-5.9983, -5.9983, -5.9983,  ..., -5.6887, -5.6887, -5.6887],
         [-6.1427, -6.1427, -6.1427,  ..., -6.1257, -6.1257, -6.1257],
         [-6.2070, -6.2070, -6.2070,  ..., -6.3203, -6.3203, -6.3203]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the part of the house that can be opened in this image? Please output segmentation mask. ASSISTANT: the part of the house that can be opened</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[-7.5938, -7.5938, -7.5938,  ..., -9.3750, -9.3750, -9.3750],
         [-7.5938, -7.5938, -7.5938,  ..., -9.3750, -9.3750, -9.3750],
         [-7.5938, -7.5938, -7.5938,  ..., -9.3750, -9.3750, -9.3750],
         ...,
         [-5.0748, -5.0748, -5.0748,  ..., -6.9672, -6.9672, -6.9672],
         [-5.4199, -5.4199, -5.4199,  ..., -7.6115, -7.6115, -7.6115],
         [-5.5547, -5.5547, -5.5547,  ..., -7.8633, -7.8633, -7.8633]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the source of power for the ship in this image? Please output segmentation mask. ASSISTANT: the source of power for the ship</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[-13.1250, -13.1250, -13.1250,  ..., -13.6250, -13.6250, -13.6250],
         [-13.1250, -13.1250, -13.1250,  ..., -13.6250, -13.6250, -13.6250],
         [-13.1250, -13.1250, -13.1250,  ..., -13.6250, -13.6250, -13.6250],
         ...,
         [-11.8320, -11.8320, -11.8320,  ..., -12.3008, -12.3008, -12.3008],
         [-12.3867, -12.3867, -12.3867,  ..., -13.5430, -13.5430, -13.5430],
         [-12.6641, -12.6641, -12.6641,  ..., -14.1641, -14.1641, -14.1641]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When hanging laundry outside to dry, what material in the picture would be suitable for holding the clothes securely in place? Please output segmentation mask. ASSISTANT: when hanging laundry outside to dry, what material in the picture would be suitable for holding the clothes securely in place</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2592, 3872])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2592, 3872])):  [tensor([[[-8.1875, -8.1875, -8.1875,  ..., -8.8750, -8.8750, -8.8750],
         [-8.1875, -8.1875, -8.1875,  ..., -8.8750, -8.8750, -8.8750],
         [-8.1875, -8.1875, -8.1875,  ..., -8.8750, -8.8750, -8.8750],
         ...,
         [-6.3951, -6.3951, -6.3951,  ..., -7.2572, -7.2572, -7.2572],
         [-6.3750, -6.3750, -6.3750,  ..., -7.1719, -7.1719, -7.1719],
         [-6.3750, -6.3750, -6.3750,  ..., -7.1719, -7.1719, -7.1719]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the friend of humans in this image? Please output segmentation mask. ASSISTANT: the friend of humans</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-15.5625, -15.5625, -15.2422,  ..., -18.0625, -18.1250, -18.1250],
         [-15.5625, -15.5625, -15.2422,  ..., -18.0625, -18.1250, -18.1250],
         [-15.2891, -15.2891, -15.0498,  ..., -18.1445, -18.2188, -18.2188],
         ...,
         [-11.4609, -11.4609, -11.6221,  ..., -11.1309, -11.3438, -11.3438],
         [-11.5938, -11.5938, -11.7168,  ..., -11.2480, -11.3750, -11.3750],
         [-11.4062, -11.4062, -11.4629,  ..., -12.6191, -12.6250, -12.6250]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Dogs have keen sense of smell, which is why they can be used as drug-sniffing dogs. Which part in the picture gives dogs this characteristic? Please output segmentation mask. ASSISTANT: dogs have keen sense of smell, which is why they can be used as drug-sniffing dogs. which part in the picture gives dogs this characteristic</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-15.5625, -15.5625, -15.5000,  ..., -18.4688, -18.7500, -18.7500],
         [-15.5625, -15.5625, -15.5000,  ..., -18.4688, -18.7500, -18.7500],
         [-15.6016, -15.6016, -15.5625,  ..., -18.4297, -18.6719, -18.6719],
         ...,
         [-15.7266, -15.7266, -16.0869,  ..., -14.4326, -14.6797, -14.6797],
         [-15.6328, -15.6328, -15.9521,  ..., -14.6387, -14.8672, -14.8672],
         [-15.5234, -15.5234, -15.7158,  ..., -15.0723, -15.2266, -15.2266]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the place where the patient lies down to receive examination in this image? Please output segmentation mask. ASSISTANT: the place where the patient lies down to receive examination</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]

         [-7.1841, -7.3387, -7.1219,  ..., -5.1484, -5.2988, -6.0765],7222],8.2500, -18.2500, -18.1695,  ..., -15.9319, -15.8750, -15.8750],
         [-7.0344, -7.2063, -6.9844,  ..., -5.2812, -5.4922, -5.9756],
         ...,
         [-7.0213, -7.3328, -7.1641,  ..., -6.0234, -6.3828, -6.6514],
         [-7.4182, -7.9059, -7.4797,  ..., -5.8531, -6.3441, -6.8103],
         [-7.7420, -8.0940, -7.6975,  ..., -6.4020, -6.8711, -7.2525]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the person in the air in this image? Please output segmentation mask. ASSISTANT: the person in the air</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3034, 2148])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3034, 2148])):  [tensor([[[-15.8125, -15.8125, -15.8125,  ..., -11.7637, -11.6582, -11.6562],
         [-15.8125, -15.8125, -15.8125,  ..., -11.7637, -11.6582, -11.6562],
         [-15.8125, -15.8125, -15.8125,  ..., -11.7637, -11.6582, -11.6562],
         ...,
         [-13.8125, -13.8125, -13.8125,  ..., -16.5698, -16.9231, -16.9297],
         [-13.8125, -13.8125, -13.8125,  ..., -16.5698, -16.9231, -16.9297],
         [-13.8125, -13.8125, -13.8125,  ..., -16.5698, -16.9231, -16.9297]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is a place where bees can suck nectar from flowers in this image? Please output segmentation mask. ASSISTANT: a place where bees can suck nectar from flowers</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 566, 939])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 566, 939])):  [tensor([[[-8.3125, -8.2966, -8.1423,  ..., -8.5056, -8.4439, -8.4375],
         [-8.3252, -8.3097, -8.1594,  ..., -8.5225, -8.4588, -8.4523],
         [-8.4485, -8.4370, -8.3256,  ..., -8.6869, -8.6046, -8.5962],
         ...,
         [-6.4422, -6.4612, -6.6458,  ..., -7.6100, -7.6393, -7.6424],
         [-6.4154, -6.4340, -6.6138,  ..., -6.9823, -7.0079, -7.0106],
         [-6.3729, -6.3905, -6.5619,  ..., -6.2716, -6.2930, -6.2952]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n I want to take a trip around the world, but I need some transportation to help me cross the oceans. What type of transportation in the picture can fulfill this requirement? Please output segmentation mask. ASSISTANT: i want to take a trip around the world, but i need some transportation to help me cross the oceans. what type of transportation in the picture can fulfill this requirement</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[90]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 960, 1280])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 960, 1280])):  [tensor([[[-4.6875, -4.6875, -4.6484,  ..., -7.7891, -7.8438, -7.8438],
         [-4.6875, -4.6875, -4.6484,  ..., -7.7891, -7.8438, -7.8438],
         [-4.6953, -4.6953, -4.6592,  ..., -7.8760, -7.9277, -7.9277],
         ...,
         [-7.9375, -7.9375, -8.0348,  ..., -4.5176, -4.5293, -4.5293],
         [-7.8625, -7.8625, -7.9418,  ..., -5.3961, -5.4000, -5.4000],
         [-7.7969, -7.7969, -7.8533,  ..., -6.5186, -6.5156, -6.5156]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is where we can see the speed of the car in this image? Please output segmentation mask. ASSISTANT: where we can see the speed of the car</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[-12.0000, -12.0000, -12.0000,  ..., -13.9375, -13.9375, -13.9375],
         [-12.0000, -12.0000, -12.0000,  ..., -13.9375, -13.9375, -13.9375],
         [-12.0000, -12.0000, -12.0000,  ..., -13.9375, -13.9375, -13.9375],
         ...,
         [ -7.6523,  -7.6523,  -7.6523,  ...,  -7.0664,  -7.0664,  -7.0664],
         [ -8.2539,  -8.2539,  -8.2539,  ...,  -8.0273,  -8.0273,  -8.0273],
         [ -8.5547,  -8.5547,  -8.5547,  ...,  -8.5078,  -8.5078,  -8.5078]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When a photographer wants to adjust the focus of a camera lens, what part in the picture could they manipulate to achieve this? Please output segmentation mask. ASSISTANT: when a photographer wants to adjust the focus of a camera lens, what part in the picture could they manipulate to achieve this</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1584, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1584, 1600])):  [tensor([[[-11.7500, -11.7500, -11.7258,  ..., -10.8813, -10.8750, -10.8750],
         [-11.7500, -11.7500, -11.7258,  ..., -10.8813, -10.8750, -10.8750],
         [-11.7327, -11.7327, -11.7092,  ..., -10.8865, -10.8805, -10.8805],
         ...,
         [ -7.7501,  -7.7501,  -7.7567,  ..., -10.7072, -10.7125, -10.7125],
         [ -7.6001,  -7.6001,  -7.6143,  ..., -10.6974, -10.7025, -10.7025],
         [ -7.4922,  -7.4922,  -7.5120,  ..., -10.6904, -10.6953, -10.6953]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is unopened flower bud in this image? Please output segmentation mask. ASSISTANT: unopened flower bud</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 566, 850])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 566, 850])):  [tensor([[[-12.5000, -12.4664, -12.2787,  ..., -14.2960, -14.3630, -14.3750],
         [-12.4880, -12.4591, -12.2976,  ..., -14.3356, -14.3996, -14.4110],
         [-12.4209, -12.4183, -12.4036,  ..., -14.5572, -14.6039, -14.6123],
         ...,
         [ -9.9514, -10.0101, -10.3386,  ..., -10.6730, -10.7167, -10.7246],
         [-10.1208, -10.1810, -10.5176,  ..., -10.1225, -10.1541, -10.1597],
         [-10.2903, -10.3519, -10.6965,  ...,  -9.5720,  -9.5914,  -9.5949]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n This image depicts a forest. Which of the animals in the picture pose a threat to human safety? Please output segmentation mask. ASSISTANT: this image depicts a forest. which of the animals in the picture pose a threat to human safety</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[73]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.],., -5.1484, -5.2988, -6.0765],7222],8.2500, -18.2500, -18.1695,  ..., -15.9319, -15.8750, -15.8750],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 854, 1280])):  [tensor([[[-9.8750, -9.8750, -9.8477,  ..., -9.5859, -9.5625, -9.5625],
         [-9.8750, -9.8750, -9.8477,  ..., -9.5859, -9.5625, -9.5625],
         [-9.9140, -9.9140, -9.8940,  ..., -9.6596, -9.6366, -9.6366],
         ...,
         [-9.2028, -9.2028, -9.2732,  ..., -8.8453, -8.8590, -8.8590],
         [-9.0352, -9.0352, -9.1051,  ..., -8.4902, -8.5000, -8.5000],
         [-9.1719, -9.1719, -9.2363,  ..., -8.4424, -8.4453, -8.4453]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that prevents people from getting into the building in this image? Please output segmentation mask. ASSISTANT: something that prevents people from getting into the building</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         ...,
         [255., 255., 255.,  ..., 255., 255., 255.],
         [255., 255., 255.,  ..., 255., 255., 255.],
         [255., 255., 255.,  ..., 255., 255., 255.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-10.8750, -10.8750, -10.8750,  ..., -12.1812, -12.4442, -12.5000],
         [-10.8914, -10.8954, -10.9140,  ..., -12.2958, -12.5401, -12.5919],
         [-10.9688, -10.9914, -11.0981,  ..., -12.8356, -12.9919, -13.0250],
         ...,
         [ -0.8148,  -0.8415,  -0.9672,  ...,   1.7566,   1.7686,   1.7711],
         [ -1.1943,  -1.2187,  -1.3336,  ...,   1.2373,   1.2551,   1.2588],
         [ -2.3995,  -2.4154,  -2.4905,  ...,  -0.7608,  -0.7150,  -0.7053]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that the monkey uses to pierce its food in this image? Please output segmentation mask. ASSISTANT: something that the monkey uses to pierce its food</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1600, 1066])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1600, 1066])):  [tensor([[[-10.7500, -10.7500, -10.7469,  ..., -10.4248, -10.2549, -10.1328],
         [-10.7500, -10.7500, -10.7469,  ..., -10.4248, -10.2549, -10.1328],
         [-10.7430, -10.7430, -10.7404,  ..., -10.4395, -10.2643, -10.1385],
         ...,
         [ -7.1906,  -7.1906,  -7.2131,  ..., -11.9199, -10.7707,  -9.9450],
         [ -7.1875,  -7.1875,  -7.2100,  ..., -11.8990, -10.7394,  -9.9062],
         [ -7.1875,  -7.1875,  -7.2100,  ..., -11.8990, -10.7394,  -9.9062]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that avoids falling down in this image? Please output segmentation mask. ASSISTANT: something that avoids falling down</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1224, 1632])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1224, 1632])):  [tensor([[[-2.7031, -2.7031, -2.7021,  ..., -6.5908, -6.5938, -6.5938],
         [-2.7031, -2.7031, -2.7021,  ..., -6.5908, -6.5938, -6.5938],
         [-2.7057, -2.7057, -2.7046,  ..., -6.5930, -6.5959, -6.5959],
         ...,
         [-3.9432, -3.9432, -3.9441,  ..., -1.2855, -1.2837, -1.2837],
         [-4.2183, -4.2183, -4.2192,  ..., -2.0871, -2.0848, -2.0848],
         [-4.4199, -4.4199, -4.4208,  ..., -2.6794, -2.6768, -2.6768]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the container that can be used to hold soup currently in this image? Please output segmentation mask. ASSISTANT: the container that can be used to hold soup currently</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[67]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[ -8.2500,  -8.1991,  -7.9594,  ..., -10.8500, -11.4378, -11.5625],
         [ -8.2500,  -8.2025,  -7.9786,  ..., -10.8657, -11.4406, -11.5625],
         [ -8.2500,  -8.2183,  -8.0691,  ..., -10.9400, -11.4536, -11.5625],
         ...,
         [ -8.1188,  -8.1818,  -8.4788,  ...,  -9.1138,  -9.2777,  -9.3125],
         [ -8.3714,  -8.4338,  -8.7280,  ...,  -9.2113,  -9.3418,  -9.3695],
         [ -9.4650,  -9.5082,  -9.7121,  ..., -11.1528, -11.1866, -11.1937]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When you look closely into a person's eye, what is the part that surrounds the dark-colored iris and helps protect the eye from dust and debris? Please output segmentation mask. ASSISTANT: when you look closely into a person's eye, what is the part that surrounds the dark-colored iris and helps protect the eye from dust and debris</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[-13.5000, -13.5000, -13.5000,  ..., -15.8125, -15.8125, -15.8125],
         [-13.5000, -13.5000, -13.5000,  ..., -15.8125, -15.8125, -15.8125],
         [-13.5000, -13.5000, -13.5000,  ..., -15.8125, -15.8125, -15.8125],
         ...,
         [-11.8850, -11.8850, -11.8850,  ..., -12.1539, -12.1539, -12.1539],
         [-12.2060, -12.2060, -12.2060,  ..., -12.5799, -12.5799, -12.5799],
         [-12.2812, -12.2812, -12.2812,  ..., -12.6797, -12.6797, -12.6797]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the object that reaches the sky in this image? Please output segmentation mask. ASSISTANT: the object that reaches the sky</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2112, 2816])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2112, 2816])):  [tensor([[[-4.5312, -4.5312, -4.5312,  ..., -4.3125, -4.3125, -4.3125],
         [-4.5312, -4.5312, -4.5312,  ..., -4.3125, -4.3125, -4.3125],
         [-4.5312, -4.5312, -4.5312,  ..., -4.3125, -4.3125, -4.3125],
         ...,
         [-5.3267, -5.3267, -5.3267,  ..., -5.3665, -5.3665, -5.3665],
         [-5.5085, -5.5085, -5.5085,  ..., -5.5512, -5.5512, -5.5512],
         [-5.5312, -5.5312, -5.5312,  ..., -5.5742, -5.5742, -5.5742]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In an orchestra, musicians play different instruments. What object in the picture is commonly played with a bow to produce sound? Please output segmentation mask. ASSISTANT: in an orchestra, musicians play different instruments. what object in the picture is commonly played with a bow to produce sound</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[77]]
>> (validate) sampled_classes_list:  [None]

 60%|█████████████████████████████████████████████████████████████████████████████████████                                                          | 119/200 [00:25<00:12,  6.49it/s]
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3254, 2168])):  [tensor([[[-12.5625, -12.5625, -12.5625,  ..., -12.8381, -12.8516, -12.8516],
         [-12.5625, -12.5625, -12.5625,  ..., -12.8381, -12.8516, -12.8516],
         [-12.5625, -12.5625, -12.5625,  ..., -12.8381, -12.8516, -12.8516],
         ...,
         [ -5.5312,  -5.5312,  -5.5312,  ...,  -8.7805,  -8.6328,  -8.6328],
         [ -5.5312,  -5.5312,  -5.5312,  ...,  -8.7805,  -8.6328,  -8.6328],
         [ -5.5312,  -5.5312,  -5.5312,  ...,  -8.7805,  -8.6328,  -8.6328]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Insects are often found on or near trees, where they can find shelter and food. What part of the tree in this picture could insects commonly be found on or around? Please output segmentation mask. ASSISTANT: insects are often found on or near trees, where they can find shelter and food. what part of the tree in this picture could insects commonly be found on or around</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1728, 2304])):  [tensor([[[1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.],
         ...,
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1728, 2304])):  [tensor([[[-8.5000, -8.5000, -8.5000,  ..., -9.4375, -9.4375, -9.4375],
         [-8.5000, -8.5000, -8.5000,  ..., -9.4375, -9.4375, -9.4375],
         [-8.5000, -8.5000, -8.5000,  ..., -9.4375, -9.4375, -9.4375],
         ...,
         [-7.2847, -7.2847, -7.2847,  ..., -5.4514, -5.4514, -5.4514],
         [-7.4583, -7.4583, -7.4583,  ..., -5.8958, -5.8958, -5.8958],
         [-7.5234, -7.5234, -7.5234,  ..., -6.0625, -6.0625, -6.0625]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If the person in the picture is the owner of the blue car and wants to drive it, which car door is least likely to be the one he enters based on the picture shown? Please output segmentation mask. ASSISTANT: if the person in the picture is the owner of the blue car and wants to drive it, which car door is least likely to be the one he enters based on the picture shown</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[-10.9375, -10.9375, -10.9375,  ..., -13.4375, -13.4375, -13.4375],
         [-10.9375, -10.9375, -10.9375,  ..., -13.4375, -13.4375, -13.4375],
         [-10.9375, -10.9375, -10.9375,  ..., -13.4375, -13.4375, -13.4375],
         ...,
         [ -9.1953,  -9.1953,  -9.1953,  ...,  -7.6074,  -7.6074,  -7.6074],
         [ -9.2422,  -9.2422,  -9.2422,  ...,  -7.7832,  -7.7832,  -7.7832],
         [ -9.2656,  -9.2656,  -9.2656,  ...,  -7.8711,  -7.8711,  -7.8711]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the keys on a piano used to play notes of half-steps or semitones in this image? Please output segmentation mask. ASSISTANT: the keys on a piano used to play notes of half-steps or semitones</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[74]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1941, 2919])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 1., 1.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1941, 2919])):  [tensor([[[-3.4531, -3.4531, -3.4531,  ..., -2.9219, -2.9219, -2.9219],
         [-3.4531, -3.4531, -3.4531,  ..., -2.9219, -2.9219, -2.9219],
         [-3.4531, -3.4531, -3.4531,  ..., -2.9219, -2.9219, -2.9219],
         ...,
         [-4.5506, -4.5506, -4.5506,  ..., -3.9532, -3.9532, -3.9532],
         [-4.7397, -4.7397, -4.7397,  ..., -4.0204, -4.0204, -4.0204],
         [-4.7539, -4.7539, -4.7539,  ..., -4.0254, -4.0254, -4.0254]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n The general steps for brewing tea are to put tea leaves into a teacup and then pour hot water over them. What container in the picture is most likely to be used next for pouring hot water to make tea? Please output segmentation mask. ASSISTANT: the general steps for brewing tea are to put tea leaves into a teacup and then pour hot water over them. what container in the picture is most likely to be used next for pouring hot water to make tea</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[97]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3056, 4592])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3056, 4592])):  [tensor([[[-8.3750, -8.3750, -8.3750,  ..., -8.7500, -8.7500, -8.7500],
         [-8.3750, -8.3750, -8.3750,  ..., -8.7500, -8.7500, -8.7500],
         [-8.3750, -8.3750, -8.3750,  ..., -8.7500, -8.7500, -8.7500],
         ...,
         [-7.3207, -7.3207, -7.3207,  ..., -6.7347, -6.7347, -6.7347],
         [-7.3398, -7.3398, -7.3398,  ..., -6.7227, -6.7227, -6.7227],
         [-7.3398, -7.3398, -7.3398,  ..., -6.7227, -6.7227, -6.7227]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that is noticeably different from the other plants in the picture in this image? Please output segmentation mask. ASSISTANT: something that is noticeably different from the other plants in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3456, 5184])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 5184])):  [tensor([[[-4.0312, -4.0312, -4.0312,  ..., -4.5000, -4.5000, -4.5000],
         [-4.0312, -4.0312, -4.0312,  ..., -4.5000, -4.5000, -4.5000],
         [-4.0312, -4.0312, -4.0312,  ..., -4.5000, -4.5000, -4.5000],
         ...,
         [-1.5312, -1.5312, -1.5312,  ..., -1.0557, -1.0557, -1.0557],
         [-1.5312, -1.5312, -1.5312,  ..., -1.0557, -1.0557, -1.0557],
         [-1.5312, -1.5312, -1.5312,  ..., -1.0557, -1.0557, -1.0557]]],
       device='cuda:0')]

 64%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                  | 129/200 [00:27<00:09,  7.20it/s]
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[ -8.3854,  -7.1729,  -7.0580,  ...,  -6.8658,  -7.3518,  -6.9817],
         [ -6.7646,  -7.3984,  -6.8109,  ...,  -6.3609,  -6.7631,  -7.3088],
         [ -7.0148,  -6.9719,  -6.1094,  ...,  -5.7656,  -5.9328,  -6.3518],
         ...,
         [ -9.3566, -10.0656,  -9.9062,  ...,  -8.3203,  -7.8750,  -7.2305],
         [ -9.9435, -10.9619, -10.4312,  ...,  -8.2703,  -7.4769,  -7.2448],
         [-10.0151, -11.2757, -10.3168,  ...,  -8.4070,  -7.9300,  -8.2029]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the places where the driver can observe the speed in this image? Please output segmentation mask. ASSISTANT: the places where the driver can observe the speed</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 944, 1417])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 944, 1417])):  [tensor([[[-13.3125, -13.3125, -13.2526,  ..., -13.9327, -13.9375, -13.9375],
         [-13.3125, -13.3125, -13.2526,  ..., -13.9327, -13.9375, -13.9375],
         [-13.2407, -13.2407, -13.1890,  ..., -13.9265, -13.9327, -13.9327],
         ...,
         [-11.6675, -11.6675, -11.7083,  ..., -13.2115, -13.1995, -13.1995],
         [-12.1755, -12.1755, -12.2163,  ..., -12.9249, -12.8947, -12.8947],
         [-12.5859, -12.5859, -12.6267,  ..., -12.6934, -12.6484, -12.6484]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something indicating that someone is celerating the birthday in this image? Please output segmentation mask. ASSISTANT: something indicating that someone is celerating the birthday</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 651, 383])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 651, 383])):  [tensor([[[-13.2500, -13.2902, -13.4309,  ..., -13.2100, -13.1507, -13.1752],
         [-13.2500, -13.3420, -13.6641,  ..., -13.4780, -13.3772, -13.3358],
         [-13.2500, -13.5233, -14.4796,  ..., -14.4150, -14.1693, -13.8972],
         ...,
         [ -7.6531,  -7.8100,  -8.3590,  ...,  -1.5985,  -2.3341,  -3.1787],
         [ -7.5826,  -7.7270,  -8.2320,  ...,  -1.8464,  -2.4439,  -3.0900],
         [ -7.5625,  -7.7032,  -8.1956,  ...,  -1.9173,  -2.4752,  -3.0647]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that we use to control computer games in this image? Please output segmentation mask. ASSISTANT: something that we use to control computer games</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-11.9375, -11.8981, -11.7125,  ..., -15.0062, -15.5167, -15.6250],
         [-12.0491, -12.0135, -11.8457,  ..., -15.0912, -15.5749, -15.6775],
         [-12.5750, -12.5573, -12.4738,  ..., -15.4919, -15.8492, -15.9250],
         ...,
         [-10.3000, -10.3453, -10.5587,  ..., -11.0412, -11.3011, -11.3563],
         [-10.3988, -10.4492, -10.6871,  ..., -11.0699, -11.2698, -11.3122],
         [-10.0100, -10.0401, -10.1818,  ..., -11.5629, -11.6213, -11.6337]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When serving wine, it is common to pour it into a glass from a container. What object in the picture could be used to pour wine? Please output segmentation mask. ASSISTANT: when serving wine, it is common to pour it into a glass from a container. what object in the picture could be used to pour wine</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[80]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 370, 658])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 370, 658])):  [tensor([[[-10.4375, -10.2811,  -9.7285,  ..., -10.7761, -10.7071, -10.6875],
         [-10.6137, -10.4763,  -9.9910,  ..., -10.8339, -10.7961, -10.7854],
         [-11.2356, -11.1655, -10.9179,  ..., -11.0379, -11.1104, -11.1309],
         ...,
         [-12.2289, -12.4066, -13.0345,  ...,  -8.4924,  -8.9953,  -9.1377],
         [-11.5998, -11.7924, -12.4728,  ...,  -8.3813,  -8.9028,  -9.0504],
         [-12.5723, -12.6455, -12.9041,  ..., -10.7926, -11.1684, -11.2748]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the container that contains the vegetable in this image? Please output segmentation mask. ASSISTANT: the container that contains the vegetable</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1568, 2352])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1568, 2352])):  [tensor([[[-2.6719, -2.6719, -2.6719,  ..., -5.2500, -5.2500, -5.2500],
         [-2.6719, -2.6719, -2.6719,  ..., -5.2500, -5.2500, -5.2500],
         [-2.6719, -2.6719, -2.6719,  ..., -5.2500, -5.2500, -5.2500],
         ...,
         [-1.2122, -1.2122, -1.2122,  ..., -3.4592, -3.4592, -3.4592],
         [-1.2950, -1.2950, -1.2950,  ..., -3.4388, -3.4388, -3.4388],
         [-1.3242, -1.3242, -1.3242,  ..., -3.4316, -3.4316, -3.4316]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In the animal kingdom, the males of certain species have a distinctive feature that sets them apart from the females. What part of the lion in this picture is a defining characteristic of male lions? Please output segmentation mask. ASSISTANT: in the animal kingdom, the males of certain species have a distinctive feature that sets them apart from the females. what part of the lion in this picture is a defining characteristic of male lions</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[92]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')] -6.0765],7222],8.2500, -18.2500, -18.1695,  ..., -15.9319, -15.8750, -15.8750],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-21.6250, -21.3822, -20.2375,  ...,  -9.1500,  -9.3356,  -9.3750],
         [-21.4478, -21.2281, -20.1922,  ...,  -9.1513,  -9.3223,  -9.3586],
         [-20.6125, -20.5016, -19.9788,  ...,  -9.1575,  -9.2596,  -9.2812],
         ...,
         [-11.3750, -11.5814, -12.5544,  ...,  -8.6894,  -8.8580,  -8.8938],
         [-11.5012, -11.7122, -12.7067,  ...,  -8.9201,  -9.0154,  -9.0356],
         [-12.5050, -12.6565, -13.3709,  ..., -10.6913, -10.6892, -10.6887]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something indicating the identity of the bus in this image? Please output segmentation mask. ASSISTANT: something indicating the identity of the bus</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2048, 1536])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2048, 1536])):  [tensor([[[ -6.7188,  -6.7188,  -6.7188,  ..., -11.1758, -11.0430, -10.9766],
         [ -6.7188,  -6.7188,  -6.7188,  ..., -11.1758, -11.0430, -10.9766],
         [ -6.7188,  -6.7188,  -6.7188,  ..., -11.1758, -11.0430, -10.9766],
         ...,
         [ -8.3750,  -8.3750,  -8.3750,  ...,  -8.0371,  -7.8535,  -7.7617],
         [ -8.3750,  -8.3750,  -8.3750,  ...,  -8.0371,  -7.8535,  -7.7617],
         [ -8.3750,  -8.3750,  -8.3750,  ...,  -8.0371,  -7.8535,  -7.7617]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the object that can be used by the owner to lead the dog in this image? Please output segmentation mask. ASSISTANT: the object that can be used by the owner to lead the dog</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 762, 1000])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 762, 1000])):  [tensor([[[-24.2500, -24.2129, -23.0950,  ..., -21.7024, -21.1435, -21.1250],
         [-24.2683, -24.2314, -23.1198,  ..., -21.6963, -21.1305, -21.1117],
         [-24.8265, -24.7960, -23.8770,  ..., -21.5081, -20.7315, -20.7057],
         ...,
         [-21.8642, -21.8835, -22.4641,  ..., -22.8815, -22.9753, -22.9784],
         [-21.5111, -21.5327, -22.1822,  ..., -21.5132, -21.6206, -21.6241],
         [-21.7470, -21.7737, -22.5764,  ..., -20.5507, -20.7381, -20.7444]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the rabbit on the woman's back in this image? Please output segmentation mask. ASSISTANT: the rabbit on the woman's back</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 802, 1089])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 802, 1089])):  [tensor([[[-3.1875, -3.1875, -3.2025,  ..., -7.4123, -7.4688, -7.4688],
         [-3.1875, -3.1875, -3.2025,  ..., -7.4123, -7.4688, -7.4688],
         [-3.2772, -3.2772, -3.2863,  ..., -7.4354, -7.4887, -7.4887],
         ...,
         [-5.9178, -5.9178, -5.9860,  ..., -4.1165, -4.1585, -4.1585],
         [-5.4257, -5.4257, -5.4892,  ..., -4.2949, -4.3201, -4.3201],
         [-4.9492, -4.9492, -5.0082,  ..., -4.4676, -4.4766, -4.4766]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What musical instrument in the picture is usually played with both hands on a keyboard? Please output segmentation mask. ASSISTANT: what musical instrument in the picture is usually played with both hands on a keyboard</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[67]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-11.3063, -10.4953, -10.7277,  ..., -12.3152, -12.8901, -14.0400],
         [-11.7844, -11.2688, -11.2219,  ..., -12.4656, -13.3756, -13.9643],
         [-11.6520, -11.4156, -10.9531,  ..., -11.7812, -12.2250, -12.7406],
         ...,
         [-11.7523, -11.8813, -11.2656,  ..., -12.0156, -12.3656, -13.2035],
         [-12.2539, -12.8125, -11.8469,  ..., -11.4062, -11.5013, -12.8075],
         [-12.3935, -12.7684, -11.7719,  ..., -10.8949, -10.7609, -11.5628]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When eating scrambled eggs for breakfast, people often add a side dish made of potatoes. What item in the picture can be used to serve the potatoes? Please output segmentation mask. ASSISTANT: when eating scrambled eggs for breakfast, people often add a side dish made of potatoes. what item in the picture can be used to serve the potatoes</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1000, 667])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1000, 667])):  [tensor([[[-5.6875, -5.6886, -5.7225,  ..., -5.0794, -5.1914, -5.2725],
         [-5.6872, -5.6884, -5.7227,  ..., -5.0785, -5.1902, -5.2713],
         [-5.6788, -5.6803, -5.7284,  ..., -5.0527, -5.1535, -5.2348],
         ...,
         [-5.0363, -5.0388, -5.1154,  ..., -5.6635, -5.4405, -5.4347],
         [-5.0617, -5.0639, -5.1319,  ..., -5.5751, -5.3434, -5.3316],
         [-5.0625, -5.0647, -5.1325,  ..., -5.5722, -5.3402, -5.3282]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that can control the fan speed in this image? Please output segmentation mask. ASSISTANT: something that can control the fan speed</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]

 74%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                     | 148/200 [00:31<00:11,  4.63it/s]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2048, 1536])):  [tensor([[[ -8.0625,  -8.0625,  -8.0625,  ..., -13.0469, -12.7031, -12.5312],
         [ -8.0625,  -8.0625,  -8.0625,  ..., -13.0469, -12.7031, -12.5312],
         [ -8.0625,  -8.0625,  -8.0625,  ..., -13.0469, -12.7031, -12.5312],
         ...,
         [ -8.0000,  -8.0000,  -8.0000,  ...,  -8.3613,  -8.1855,  -8.0977],
         [ -8.0000,  -8.0000,  -8.0000,  ...,  -8.3613,  -8.1855,  -8.0977],
         [ -8.0000,  -8.0000,  -8.0000,  ...,  -8.3613,  -8.1855,  -8.0977]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the stronger Mario in this image? Please output segmentation mask. ASSISTANT: the stronger mario</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[60]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2448, 3264])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2448, 3264])):  [tensor([[[-6.8750, -6.8750, -6.8750,  ..., -6.8438, -6.8438, -6.8438],
         [-6.8750, -6.8750, -6.8750,  ..., -6.8438, -6.8438, -6.8438],
         [-6.8750, -6.8750, -6.8750,  ..., -6.8438, -6.8438, -6.8438],
         ...,
         [-6.9004, -6.9004, -6.9004,  ..., -7.9335, -7.9335, -7.9335],
         [-6.9648, -6.9648, -6.9648,  ..., -8.1445, -8.1445, -8.1445],
         [-6.9648, -6.9648, -6.9648,  ..., -8.1445, -8.1445, -8.1445]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is where to wash hands in this image? Please output segmentation mask. ASSISTANT: where to wash hands</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 5953, 4793])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 5953, 4793])):  [tensor([[[-18.2500, -18.2500, -18.2500,  ..., -17.8984, -17.8984, -17.8984],
         [-18.2500, -18.2500, -18.2500,  ..., -17.8984, -17.8984, -17.8984],
         [-18.2500, -18.2500, -18.2500,  ..., -17.8984, -17.8984, -17.8984],
         ...,
         [-13.8750, -13.8750, -13.8750,  ..., -17.9219, -17.9219, -17.9219],
         [-13.8750, -13.8750, -13.8750,  ..., -17.9219, -17.9219, -17.9219],
         [-13.8750, -13.8750, -13.8750,  ..., -17.9219, -17.9219, -17.9219]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the food that the bird likes to eat in this image? Please output segmentation mask. ASSISTANT: the food that the bird likes to eat</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[ -9.8750,  -9.8750,  -9.8664,  ...,  -8.9930,  -9.0000,  -9.0000],
         [ -9.8750,  -9.8750,  -9.8664,  ...,  -8.9930,  -9.0000,  -9.0000],
         [ -9.8742,  -9.8742,  -9.8658,  ...,  -8.9978,  -9.0047,  -9.0047],
         ...,
         [ -9.8578,  -9.8578,  -9.8590,  ...,  -8.0109,  -8.0211,  -8.0211],
         [-10.0100, -10.0100, -10.0076,  ...,  -8.4175,  -8.4256,  -8.4256],
         [-10.1250, -10.1250, -10.1198,  ...,  -8.7318,  -8.7383,  -8.7383]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a large building, it is common to have designated areas for swimming. What area in the picture could be used for swimming? Please output segmentation mask. ASSISTANT: in a large building, it is common to have designated areas for swimming. what area in the picture could be used for swimming</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[79]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[ -9.1250,  -9.1250,  -9.1250,  ..., -10.9375, -10.9375, -10.9375],
         [ -9.1250,  -9.1250,  -9.1250,  ..., -10.9375, -10.9375, -10.9375],
         [ -9.1250,  -9.1250,  -9.1250,  ..., -10.9375, -10.9375, -10.9375],
         ...,
         [ -3.0521,  -3.0521,  -3.0521,  ...,  -0.7278,  -0.7278,  -0.7278],
         [ -3.4500,  -3.4500,  -3.4500,  ...,  -1.6679,  -1.6679,  -1.6679],
         [ -3.6055,  -3.6055,  -3.6055,  ...,  -2.0352,  -2.0352,  -2.0352]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In order to shape and carve hard materials, such as wood or stone, what part of the tool in the picture would be used to provide precise control and force? Please output segmentation mask. ASSISTANT: in order to shape and carve hard materials, such as wood or stone, what part of the tool in the picture would be used to provide precise control and force</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 790, 916])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 790, 916])):  [tensor([[[-16.0000, -15.9530, -15.5778,  ..., -15.4988, -15.3332, -15.3125],
         [-15.9752, -15.9309, -15.5776,  ..., -15.5212, -15.3664, -15.3470],
         [-15.7766, -15.7543, -15.5763,  ..., -15.7005, -15.6314, -15.6228],
         ...,
         [-14.4110, -14.4919, -15.1378,  ..., -16.0383, -16.3920, -16.4362],
         [-12.9091, -12.9302, -13.0981,  ..., -15.3144, -15.5509, -15.5805],
         [-12.6278, -12.6133, -12.4980,  ..., -15.0010, -15.1410, -15.1586]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In the picture, there is a legal requirement for vehicles to display identifying information. What part of the car is used to display this information? Please output segmentation mask. ASSISTANT: in the picture, there is a legal requirement for vehicles to display identifying information. what part of the car is used to display this information</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[80]]
>> (validate) sampled_classes_list:  [None]

 78%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                              | 157/200 [00:33<00:10,  4.25it/s]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1274, 2048])):  [tensor([[[-8.1875, -8.1875, -8.1875,  ..., -9.1875, -9.1875, -9.1875],
         [-8.1875, -8.1875, -8.1875,  ..., -9.1875, -9.1875, -9.1875],
         [-8.1875, -8.1875, -8.1875,  ..., -9.1875, -9.1875, -9.1875],
         ...,
         [-5.6191, -5.6191, -5.6191,  ..., -5.1797, -5.1797, -5.1797],
         [-5.7871, -5.7871, -5.7871,  ..., -5.1328, -5.1328, -5.1328],
         [-5.8711, -5.8711, -5.8711,  ..., -5.1094, -5.1094, -5.1094]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is a car with a color that is closer to lipstick color in this image? Please output segmentation mask. ASSISTANT: a car with a color that is closer to lipstick color</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[-10.1250, -10.1250, -10.0977,  ..., -14.2226, -14.2500, -14.2500],
         [-10.1250, -10.1250, -10.0977,  ..., -14.2226, -14.2500, -14.2500],
         [-10.1133, -10.1133, -10.0864,  ..., -14.2512, -14.2781, -14.2781],
         ...,
         [ -8.2016,  -8.2016,  -8.2204,  ...,  -8.8087,  -8.8140,  -8.8140],
         [ -8.7156,  -8.7156,  -8.7301,  ...,  -9.7350,  -9.7362,  -9.7362],
         [ -9.1289,  -9.1289,  -9.1399,  ..., -10.4785, -10.4766, -10.4766]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If you want to play table tennis indoors, what furniture in the picture should be used as the playing surface? Please output segmentation mask. ASSISTANT: if you want to play table tennis indoors, what furniture in the picture should be used as the playing surface</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0.,  ..., 255., 255., 255.],
         [  0.,   0.,   0.,  ..., 255., 255., 255.],
         [  0.,   0.,   0.,  ..., 255., 255., 255.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-11.1022, -11.8692, -11.8371,  ...,  -8.8426,  -9.5273, -10.5010],
         [-12.5138, -12.9263, -13.1906,  ..., -10.4969, -11.3706, -11.5468],
         [-12.2922, -12.8938, -12.8125,  ...,  -9.7188, -10.4813, -10.6531],
         ...,
         [-10.1449, -11.2406, -10.6719,  ...,   1.7656,   1.7730,   1.7542],
         [ -9.6064, -10.8203,  -9.7266,  ...,   1.8762,   1.9252,   1.9649],
         [ -6.5385,  -7.7733,  -7.5912,  ...,   0.8329,   0.9259,   0.9776]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a military airfield, what area is specifically designed for aircraft to take off and land? Please output segmentation mask. ASSISTANT: in a military airfield, what area is specifically designed for aircraft to take off and land</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2848, 4288])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2848, 4288])):  [tensor([[[-13.6875, -13.6875, -13.6875,  ..., -15.4375, -15.4375, -15.4375],
         [-13.6875, -13.6875, -13.6875,  ..., -15.4375, -15.4375, -15.4375],
         [-13.6875, -13.6875, -13.6875,  ..., -15.4375, -15.4375, -15.4375],
         ...,
         [ -6.6937,  -6.6937,  -6.6937,  ...,  -7.2280,  -7.2280,  -7.2280],
         [ -6.7891,  -6.7891,  -6.7891,  ...,  -7.3711,  -7.3711,  -7.3711],
         [ -6.7891,  -6.7891,  -6.7891,  ...,  -7.3711,  -7.3711,  -7.3711]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Generally speaking, dogs do not have horns on their heads, only a pair of ears. What part of the dog's head in this picture looks strange? Please output segmentation mask. ASSISTANT: generally speaking, dogs do not have horns on their heads, only a pair of ears. what part of the dog's head in this picture looks strange</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1533, 1039])):  [tensor([[[  0.,   0.,   0.,  ..., 255., 255.,   0.],
         [  0.,   0.,   0.,  ..., 255., 255.,   0.],
         [  0.,   0.,   0.,  ..., 255.,   0.,   0.],
         ...,
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1533, 1039])):  [tensor([[[-10.6875, -10.6875, -10.6848,  ...,  -4.1654,  -4.4367,  -4.6406],
         [-10.6875, -10.6875, -10.6848,  ...,  -4.1654,  -4.4367,  -4.6406],
         [-10.6902, -10.6902, -10.6890,  ...,  -4.2019,  -4.4794,  -4.6878],
         ...,
         [-10.6343, -10.6343, -10.7001,  ...,  -9.0872,  -9.1992,  -9.2833],
         [-10.6250, -10.6250, -10.6914,  ...,  -9.0880,  -9.2028,  -9.2891],
         [-10.6250, -10.6250, -10.6914,  ...,  -9.0880,  -9.2028,  -9.2891]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In an organized workspace, one might have a designated area to store important documents and files. What piece of furniture in the picture could be used for this purpose? Please output segmentation mask. ASSISTANT: in an organized workspace, one might have a designated area to store important documents and files. what piece of furniture in the picture could be used for this purpose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 750, 1000])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 750, 1000])):  [tensor([[[-6.7812, -6.7779, -6.6762,  ..., -6.8506, -6.9650, -6.9688],
         [-6.7810, -6.7776, -6.6764,  ..., -6.8495, -6.9633, -6.9671],
         [-6.7725, -6.7696, -6.6822,  ..., -6.8165, -6.9130, -6.9162],
         ...,
         [-4.4325, -4.4333, -4.4567,  ..., -4.5058, -4.5080, -4.5081],
         [-4.4824, -4.4829, -4.4980,  ..., -4.8190, -4.8129, -4.8127],
         [-4.6388, -4.6391, -4.6497,  ..., -5.5289, -5.5267, -5.5267]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that ensures the person to land safely in this image? Please output segmentation mask. ASSISTANT: something that ensures the person to land safely</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]

(PLUM.py) >> pred_masks (shape: torch.Size([1, 615, 461])):  [tensor([[[-12.1875, -12.1641, -12.0860,  ..., -12.6353, -12.5310, -12.0988],],
         [-12.1953, -12.2176, -12.2921,  ..., -12.7009, -12.5641, -12.0790],
         [-12.2213, -12.3962, -12.9793,  ..., -12.9196, -12.6745, -12.0129],
         ...,
         [-11.2281, -11.7848, -13.6408,  ..., -11.5932, -11.6511, -10.8115],
         [-10.8122, -11.3428, -13.1123,  ..., -11.0270, -11.4204, -10.4994],
         [-10.6875, -11.2104, -12.9538,  ..., -10.8573, -11.3512, -10.4059]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is where the garbage should be put in this image? Please output segmentation mask. ASSISTANT: where the garbage should be put</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[-13.9375, -13.9375, -13.9375,  ..., -15.3750, -15.3750, -15.3750],
         [-13.9375, -13.9375, -13.9375,  ..., -15.3750, -15.3750, -15.3750],
         [-13.9375, -13.9375, -13.9375,  ..., -15.3750, -15.3750, -15.3750],
         ...,
         [ -8.7245,  -8.7245,  -8.7245,  ...,  -2.0772,  -2.0772,  -2.0772],
         [ -8.9097,  -8.9097,  -8.9097,  ...,  -2.6745,  -2.6745,  -2.6745],
         [ -8.9531,  -8.9531,  -8.9531,  ...,  -2.8145,  -2.8145,  -2.8145]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Dogs are faithful companions to humans, and humans often play fetch games with them. What object will the dog likely retrieve and bring back to the human for the next round of fetch in the picture? Please output segmentation mask. ASSISTANT: dogs are faithful companions to humans, and humans often play fetch games with them. what object will the dog likely retrieve and bring back to the human for the next round of fetch in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[92]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1816, 3003])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1816, 3003])):  [tensor([[[-26.5000, -26.5000, -26.5000,  ..., -22.6250, -22.6250, -22.6250],
         [-26.5000, -26.5000, -26.5000,  ..., -22.6250, -22.6250, -22.6250],
         [-26.5000, -26.5000, -26.5000,  ..., -22.6250, -22.6250, -22.6250],
         ...,
         [-22.8290, -22.8290, -22.8290,  ..., -19.7173, -19.7173, -19.7173],
         [-22.8130, -22.8130, -22.8130,  ..., -19.4616, -19.4616, -19.4616],
         [-22.8125, -22.8125, -22.8125,  ..., -19.4531, -19.4531, -19.4531]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n People often add gas to water to improve its taste. What part of the picture will be consumed as a beverage? Please output segmentation mask. ASSISTANT: people often add gas to water to improve its taste. what part of the picture will be consumed as a beverage</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2331, 1749])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2331, 1749])):  [tensor([[[-12.7500, -12.7500, -12.7500,  ..., -14.6720, -14.3907, -14.2891],
         [-12.7500, -12.7500, -12.7500,  ..., -14.6720, -14.3907, -14.2891],
         [-12.7500, -12.7500, -12.7500,  ..., -14.6720, -14.3907, -14.2891],
         ...,
         [ -9.5625,  -9.5625,  -9.5625,  ..., -10.8656, -10.5568, -10.4453],
         [ -9.5625,  -9.5625,  -9.5625,  ..., -10.8656, -10.5568, -10.4453],
         [ -9.5625,  -9.5625,  -9.5625,  ..., -10.8656, -10.5568, -10.4453]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When people want to take their pets to the park and enjoy some outdoor activities, they often want to make sure their pets are safe and cannot run away. What in the picture could help with this? Please output segmentation mask. ASSISTANT: when people want to take their pets to the park and enjoy some outdoor activities, they often want to make sure their pets are safe and cannot run away. what in the picture could help with this</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[94]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2705, 3269])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2705, 3269])):  [tensor([[[-8.9375, -8.9375, -8.9375,  ..., -7.1562, -7.1562, -7.1562],
         [-8.9375, -8.9375, -8.9375,  ..., -7.1562, -7.1562, -7.1562],
         [-8.9375, -8.9375, -8.9375,  ..., -7.1562, -7.1562, -7.1562],
         ...,
         [-7.3468, -7.3468, -7.3468,  ..., -7.1789, -7.1789, -7.1789],
         [-7.4219, -7.4219, -7.4219,  ..., -7.1523, -7.1523, -7.1523],
         [-7.4219, -7.4219, -7.4219,  ..., -7.1523, -7.1523, -7.1523]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n I feel my commute is too slow now and I'm hoping to find a convenient mode of transportation that can also help me exercise. Can you help me find the corresponding part in the picture? Please output segmentation mask. ASSISTANT: i feel my commute is too slow now and i'm hoping to find a convenient mode of transportation that can also help me exercise. can you help me find the corresponding part in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[92]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1189, 1656])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1189, 1656])):  [tensor([[[-9.8125, -9.8125, -9.8107,  ..., -7.5837, -7.5938, -7.5938],
         [-9.8125, -9.8125, -9.8107,  ..., -7.5837, -7.5938, -7.5938],
         [-9.8146, -9.8146, -9.8129,  ..., -7.5795, -7.5895, -7.5895],
         ...,
         [-5.3054, -5.3054, -5.3056,  ..., -8.4889, -8.4915, -8.4915],
         [-5.5401, -5.5401, -5.5397,  ..., -8.6373, -8.6399, -8.6399],
         [-5.7070, -5.7070, -5.7063,  ..., -8.7475, -8.7500, -8.7500]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If we were to have a friendly competition to see who can paddle the fastest in the water, what would be the most suitable vessels for this activity in the picture? Please output segmentation mask. ASSISTANT: if we were to have a friendly competition to see who can paddle the fastest in the water, what would be the most suitable vessels for this activity in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[86]]
>> (validate) sampled_classes_list:  [None]

         [-15.9344, -15.7996, -15.1644,  ..., -13.7421, -13.7730, -13.7795],1875, -12.1641, -12.0860,  ..., -12.6353, -12.5310, -12.0988],],
         [-15.6250, -15.5666, -15.2913,  ..., -13.7050, -13.8813, -13.9187],
         ...,
         [-13.6000, -13.7719, -14.5825,  ..., -14.7425, -15.1818, -15.2750],
         [-13.5633, -13.7403, -14.5748,  ..., -14.6125, -15.0484, -15.1409],
         [-14.6112, -14.6884, -15.0522,  ..., -15.8330, -16.0966, -16.1525]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a busy and bustling city, it is important for businesses to have eye-catching signs to attract customers. What feature in the picture can serve this purpose? Please output segmentation mask. ASSISTANT: in a busy and bustling city, it is important for businesses to have eye-catching signs to attract customers. what feature in the picture can serve this purpose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2592, 1944])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2592, 1944])):  [tensor([[[-18.5000, -18.5000, -18.5000,  ..., -19.2122, -19.0023, -18.9531],
         [-18.5000, -18.5000, -18.5000,  ..., -19.2122, -19.0023, -18.9531],
         [-18.5000, -18.5000, -18.5000,  ..., -19.2122, -19.0023, -18.9531],
         ...,
         [-17.3750, -17.3750, -17.3750,  ..., -12.7485, -12.6991, -12.6875],
         [-17.3750, -17.3750, -17.3750,  ..., -12.7485, -12.6991, -12.6875],
         [-17.3750, -17.3750, -17.3750,  ..., -12.7485, -12.6991, -12.6875]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When taking pictures with a camera, what part of the camera is responsible for focusing the image and adjusting the depth of field? Please output segmentation mask. ASSISTANT: when taking pictures with a camera, what part of the camera is responsible for focusing the image and adjusting the depth of field</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1664, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1664, 2048])):  [tensor([[[-13.2500, -13.2500, -13.2500,  ..., -15.3750, -15.3750, -15.3750],
         [-13.2500, -13.2500, -13.2500,  ..., -15.3750, -15.3750, -15.3750],
         [-13.2500, -13.2500, -13.2500,  ..., -15.3750, -15.3750, -15.3750],
         ...,
         [-13.6523, -13.6523, -13.6523,  ..., -14.4375, -14.4375, -14.4375],
         [-14.0039, -14.0039, -14.0039,  ..., -15.0625, -15.0625, -15.0625],
         [-14.1797, -14.1797, -14.1797,  ..., -15.3750, -15.3750, -15.3750]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Birds often need a place to rest or observe their surroundings. What part of a tree in the picture offers a suitable spot for birds to do so? Please output segmentation mask. ASSISTANT: birds often need a place to rest or observe their surroundings. what part of a tree in the picture offers a suitable spot for birds to do so</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 612, 612])):  [tensor([[[  0.,   0.,   0.,  ..., 255., 255., 255.],
         [  0.,   0.,   0.,  ..., 255., 255., 255.],
         [  0.,   0.,   0.,  ..., 255., 255., 255.],
         ...,
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 612, 612])):  [tensor([[[-10.7500, -10.8297, -11.0911,  ..., -11.2100, -11.5760, -11.6875],
         [-10.7500, -10.8388, -11.1302,  ..., -11.2550, -11.5010, -11.5760],
         [-10.7500, -10.8688, -11.2586,  ..., -11.4026, -11.2549, -11.2100],
         ...,
         [ -7.5654,  -7.7837,  -8.5002,  ...,  -7.4665,  -7.7362,  -7.8184],
         [ -7.5392,  -7.7642,  -8.5026,  ...,  -7.0696,  -7.3830,  -7.4785],
         [ -7.5312,  -7.7583,  -8.5034,  ...,  -6.9486,  -7.2754,  -7.3750]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that we can sit on in this image? Please output segmentation mask. ASSISTANT: something that we can sit on</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 540, 720])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 540, 720])):  [tensor([[[ -8.1875,  -8.2518,  -8.5035,  ...,  -9.6354, -10.2742, -10.4375],
         [ -8.2766,  -8.3346,  -8.5617,  ...,  -9.6618, -10.2363, -10.3831],
         [ -8.6250,  -8.6585,  -8.7897,  ...,  -9.7650, -10.0877, -10.1701],
         ...,
         [ -4.3142,  -4.5110,  -5.2810,  ...,  -5.4277,  -5.2917,  -5.2569],
         [ -3.7803,  -3.9530,  -4.6287,  ...,  -4.6401,  -4.5899,  -4.5771],
         [ -4.3055,  -4.4302,  -4.9178,  ...,  -5.4828,  -5.4135,  -5.3958]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n To keep bread fresh and protected, it is often placed in a protective covering. What item in the picture is commonly used for this purpose? Please output segmentation mask. ASSISTANT: to keep bread fresh and protected, it is often placed in a protective covering. what item in the picture is commonly used for this purpose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[80]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 612, 612])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 612, 612])):  [tensor([[[ -8.8750,  -8.7714,  -8.4316,  ...,  -9.5680,  -9.8033,  -9.8750],
         [ -8.7714,  -8.7075,  -8.4976,  ...,  -9.6651,  -9.8870,  -9.9547],
         [ -8.4316,  -8.4976,  -8.7142,  ...,  -9.9836, -10.1618, -10.2161],
         ...,
         [ -8.3127,  -8.6074,  -9.5746,  ...,  -8.2542,  -8.5167,  -8.5966],
         [ -8.0251,  -8.3565,  -9.4440,  ...,  -7.8500,  -8.4022,  -8.5705],
         [ -7.9375,  -8.2800,  -9.4042,  ...,  -7.7268,  -8.3674,  -8.5625]]],
       device='cuda:0')]

(PLUM.py) >> gt_masks (shape: torch.Size([1, 580, 435])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],,  ..., -12.6353, -12.5310, -12.0988],],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 580, 435])):  [tensor([[[-11.8750, -11.9864, -12.2899,  ..., -15.0517, -14.8310, -14.4009],
         [-11.9864, -12.1766, -12.6947,  ..., -15.1218, -14.7535, -14.2267],
         [-12.2899, -12.6947, -13.7972,  ..., -15.3125, -14.5424, -13.7525],
         ...,
         [ -2.8269,  -2.8191,  -2.7977,  ...,  -7.1709,  -6.9486,  -7.0991],
         [ -3.9821,  -3.9726,  -3.9467,  ...,  -7.1141,  -7.1048,  -6.8829],
         [ -4.4062,  -4.3961,  -4.3685,  ...,  -7.0932,  -7.1622,  -6.8034]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Dogs have keen sense of smell, which is why they can be used as drug-sniffing dogs. Which part in the picture gives dogs this characteristic? Please output segmentation mask. ASSISTANT: dogs have keen sense of smell, which is why they can be used as drug-sniffing dogs. which part in the picture gives dogs this characteristic</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[-12.8125, -12.8125, -12.8125,  ..., -10.8750, -10.8750, -10.8750],
         [-12.8125, -12.8125, -12.8125,  ..., -10.8750, -10.8750, -10.8750],
         [-12.8125, -12.8125, -12.8125,  ..., -10.8750, -10.8750, -10.8750],
         ...,
         [-12.1329, -12.1329, -12.1329,  ..., -11.5629, -11.5629, -11.5629],
         [-12.1048, -12.1048, -12.1048,  ..., -11.9502, -11.9502, -11.9502],
         [-12.0938, -12.0938, -12.0938,  ..., -12.1016, -12.1016, -12.1016]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Some birds have long bills that they use to catch food from the water. What part of the bird's body in the picture may have this characteristic? Please output segmentation mask. ASSISTANT: some birds have long bills that they use to catch food from the water. what part of the bird's body in the picture may have this characteristic</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[83]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 800, 1200])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 800, 1200])):  [tensor([[[-16.8750, -16.8750, -16.8354,  ..., -19.5323, -19.7500, -19.7500],
         [-16.8750, -16.8750, -16.8354,  ..., -19.5323, -19.7500, -19.7500],
         [-16.7660, -16.7660, -16.7413,  ..., -19.5225, -19.7104, -19.7104],
         ...,
         [-14.4515, -14.4515, -14.6728,  ..., -15.5594, -15.5635, -15.5635],
         [-14.1983, -14.1983, -14.4242,  ..., -14.9356, -14.9310, -14.9310],
         [-14.3203, -14.3203, -14.5597,  ..., -14.8043, -14.7969, -14.7969]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n After cooking, consuming food, and preparing for food, where can we throw away the rest of the food and scraps? Please output segmentation mask. ASSISTANT: after cooking, consuming food, and preparing for food, where can we throw away the rest of the food and scraps</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[79]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-7.9683, -7.3197, -7.8277,  ..., -6.1133, -6.8713, -7.1519],
         [-6.9115, -8.4047, -8.6656,  ..., -6.3281, -6.8391, -7.1936],
         [-6.9711, -8.8188, -9.3125,  ..., -6.0938, -6.3953, -6.7283],
         ...,
         [-5.9689, -7.6984, -7.7969,  ..., -6.8750, -6.8109, -7.1654],
         [-6.5368, -8.3866, -8.1453,  ..., -6.5422, -6.0938, -6.8027],
         [-7.3455, -8.8368, -8.2858,  ..., -7.0867, -6.8064, -7.1450]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the person who is most likely to be the girl's trainer in this image? Please output segmentation mask. ASSISTANT: the person who is most likely to be the girl's trainer</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[71]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3456, 2304])):  [tensor([[[0., 0., 0.,  ..., 1., 1., 0.],
         [0., 0., 0.,  ..., 1., 1., 0.],
         [0., 0., 0.,  ..., 1., 1., 0.],
         ...,
         [0., 0., 0.,  ..., 1., 1., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 2304])):  [tensor([[[-11.0625, -11.0625, -11.0625,  ...,  -0.7326,  -0.8579,  -0.8579],
         [-11.0625, -11.0625, -11.0625,  ...,  -0.7326,  -0.8579,  -0.8579],
         [-11.0625, -11.0625, -11.0625,  ...,  -0.7326,  -0.8579,  -0.8579],
         ...,
         [-12.4375, -12.4375, -12.4375,  ...,  -5.7819,  -5.8008,  -5.8008],
         [-12.4375, -12.4375, -12.4375,  ...,  -5.7819,  -5.8008,  -5.8008],
         [-12.4375, -12.4375, -12.4375,  ...,  -5.7819,  -5.8008,  -5.8008]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n At a car show, visitors can get close to the displayed vehicles to admire their design and features. What part of the car in this picture is open, allowing viewers to see the engine compartment? Please output segmentation mask. ASSISTANT: at a car show, visitors can get close to the displayed vehicles to admire their design and features. what part of the car in this picture is open, allowing viewers to see the engine compartment</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[93]]
>> (validate) sampled_classes_list:  [None]

(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
         [-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
         [-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
         ...,
         [ -8.9358,  -8.9358,  -8.9358,  ...,  -9.4167,  -9.4167,  -9.4167],
         [ -8.9766,  -8.9766,  -8.9766,  ...,  -9.4375,  -9.4375,  -9.4375],
         [ -8.9766,  -8.9766,  -8.9766,  ...,  -9.4375,  -9.4375,  -9.4375]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the ball that can only be hit into the hole at last in this image? Please output segmentation mask. ASSISTANT: the ball that can only be hit into the hole at last</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[69]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 960, 1280])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 960, 1280])):  [tensor([[[-12.8125, -12.8125, -12.7969,  ..., -14.0977, -14.1250, -14.1250],
         [-12.8125, -12.8125, -12.7969,  ..., -14.0977, -14.1250, -14.1250],
         [-12.8008, -12.8008, -12.7930,  ..., -14.1206, -14.1445, -14.1445],
         ...,
         [-10.8828, -10.8828, -11.0803,  ..., -11.0271, -11.0781, -11.0781],
         [-10.9875, -10.9875, -11.1555,  ..., -11.6523, -11.7000, -11.7000],
         [-11.2500, -11.2500, -11.3735,  ..., -12.4966, -12.5312, -12.5312]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the reflection of the camera in the mirror in this image? Please output segmentation mask. ASSISTANT: the reflection of the camera in the mirror</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2880, 1920])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2880, 1920])):  [tensor([[[ -4.3125,  -4.3125,  -4.3125,  ..., -10.2109, -10.2609, -10.2656],
         [ -4.3125,  -4.3125,  -4.3125,  ..., -10.2109, -10.2609, -10.2656],
         [ -4.3125,  -4.3125,  -4.3125,  ..., -10.2109, -10.2609, -10.2656],
         ...,
         [ -9.1250,  -9.1250,  -9.1250,  ...,  -9.4680,  -9.3402,  -9.3281],
         [ -9.1250,  -9.1250,  -9.1250,  ...,  -9.4680,  -9.3402,  -9.3281],
         [ -9.1250,  -9.1250,  -9.1250,  ...,  -9.4680,  -9.3402,  -9.3281]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In some rural areas, horse-drawn carts are still used for transportation and carrying goods. What is the main source of power that drives the cart in the picture? Please output segmentation mask. ASSISTANT: in some rural areas, horse-drawn carts are still used for transportation and carrying goods. what is the main source of power that drives the cart in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2304, 3456])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2304, 3456])):  [tensor([[[ -9.5000,  -9.5000,  -9.5000,  ..., -11.1250, -11.1250, -11.1250],
         [ -9.5000,  -9.5000,  -9.5000,  ..., -11.1250, -11.1250, -11.1250],
         [ -9.5000,  -9.5000,  -9.5000,  ..., -11.1250, -11.1250, -11.1250],
         ...,
         [ -5.8890,  -5.8890,  -5.8890,  ...,  -5.2087,  -5.2087,  -5.2087],
         [ -6.0312,  -6.0312,  -6.0312,  ...,  -5.2812,  -5.2812,  -5.2812],
         [ -6.0312,  -6.0312,  -6.0312,  ...,  -5.2812,  -5.2812,  -5.2812]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In case of a fire, it is important to have access to fire safety equipment. What object in the picture is specifically designed to store and release fire extinguishing substances? Please output segmentation mask. ASSISTANT: in case of a fire, it is important to have access to fire safety equipment. what object in the picture is specifically designed to store and release fire extinguishing substances</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[ -8.4375,  -8.4295,  -8.3952,  ...,  -9.7247,  -9.9479, -10.0000],
         [ -8.4856,  -8.4830,  -8.4717,  ...,  -9.8332, -10.0431, -10.0921],
         [ -8.6915,  -8.7120,  -8.7997,  ..., -10.2976, -10.4510, -10.4868],
         ...,
         [ -7.5184,  -7.5684,  -7.7825,  ...,  -8.2253,  -8.3025,  -8.3205],
         [ -7.5079,  -7.5545,  -7.7542,  ...,  -7.6617,  -7.7879,  -7.8173],
         [ -7.7144,  -7.7591,  -7.9507,  ...,  -7.5008,  -7.6225,  -7.6509]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the solitary tree in this image? Please output segmentation mask. ASSISTANT: the solitary tree</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[-10.4375, -10.4375, -10.4375,  ..., -10.9375, -10.9375, -10.9375],
         [-10.4375, -10.4375, -10.4375,  ..., -10.9375, -10.9375, -10.9375],
         [-10.4375, -10.4375, -10.4375,  ..., -10.9375, -10.9375, -10.9375],
         ...,
         [ -8.6426,  -8.6426,  -8.6426,  ..., -10.6445, -10.6445, -10.6445],
         [ -9.2168,  -9.2168,  -9.2168,  ..., -11.6992, -11.6992, -11.6992],
         [ -9.5039,  -9.5039,  -9.5039,  ..., -12.2266, -12.2266, -12.2266]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:42<00:00,  4.67it/s]
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 4 is less than current step: 2499. Dropping entry: {'val/giou': 0.3745441436767578, 'val/ciou': 0.3144300878047943, 'val/b_acc': 0.4749999762500012, 'val/i_acc': 0.9648694780071494, 'val/o_acc': 0.999999999849284, '_timestamp': 1743874708.4403527}).
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
  0%|                                                                                                                                                         | 0/200 [00:00<?, ?it/s]

>> (validate) sampled_classes_list:  [None][1, 3456, 4608])):  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[-15.6250, -15.6250, -15.6070,  ..., -14.0141, -14.0000, -14.0000],
         [-15.6250, -15.6250, -15.6070,  ..., -14.0141, -14.0000, -14.0000],
         [-15.6078, -15.6078, -15.5906,  ..., -14.0434, -14.0297, -14.0297],
         ...,
         [-16.1266, -16.1266, -16.1780,  ..., -14.9195, -14.9414, -14.9414],
         [-16.4600, -16.4600, -16.5036,  ..., -16.0446, -16.0612, -16.0612],
         [-16.7188, -16.7188, -16.7561,  ..., -16.9327, -16.9453, -16.9453]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n For the safety of newborns, they are often placed in a secure and comfortable space when they sleep. What furniture in the picture is commonly used to provide a safe sleeping environment for babies? Please output segmentation mask. ASSISTANT: for the safety of newborns, they are often placed in a secure and comfortable space when they sleep. what furniture in the picture is commonly used to provide a safe sleeping environment for babies</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[94]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1002, 1455])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1002, 1455])):  [tensor([[[-5.2500, -5.2500, -5.2449,  ..., -5.3819, -5.4062, -5.4062],
         [-5.2500, -5.2500, -5.2449,  ..., -5.3819, -5.4062, -5.4062],
         [-5.2733, -5.2733, -5.2683,  ..., -5.3881, -5.4113, -5.4113],
         ...,
         [-5.1055, -5.1055, -5.1241,  ..., -5.2146, -5.2131, -5.2131],
         [-5.2539, -5.2539, -5.2703,  ..., -5.6558, -5.6529, -5.6529],
         [-5.3711, -5.3711, -5.3858,  ..., -6.0041, -6.0000, -6.0000]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When using a film camera to take photos, what part of the camera helps focus the light on the film to capture the image? Please output segmentation mask. ASSISTANT: when using a film camera to take photos, what part of the camera helps focus the light on the film to capture the image</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[77]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1067, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1067, 1600])):  [tensor([[[-10.9375, -10.9375, -10.9227,  ..., -10.2547, -10.2500, -10.2500],
         [-10.9375, -10.9375, -10.9227,  ..., -10.2547, -10.2500, -10.2500],
         [-10.9375, -10.9375, -10.9230,  ..., -10.2641, -10.2594, -10.2594],
         ...,
         [ -9.5626,  -9.5626,  -9.5715,  ...,  -9.4509,  -9.4470,  -9.4470],
         [ -9.6114,  -9.6114,  -9.6198,  ...,  -9.4054,  -9.4014,  -9.4014],
         [ -9.6797,  -9.6797,  -9.6876,  ...,  -9.3947,  -9.3906,  -9.3906]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Birds have various ways of searching for food. What part of their body helps them to grab and pick up food from the ground in the picture? Please output segmentation mask. ASSISTANT: birds have various ways of searching for food. what part of their body helps them to grab and pick up food from the ground in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2570, 2229])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2570, 2229])):  [tensor([[[-17.1250, -17.1250, -17.1250,  ..., -14.9398, -15.1639, -15.2188],
         [-17.1250, -17.1250, -17.1250,  ..., -14.9398, -15.1639, -15.2188],
         [-17.1250, -17.1250, -17.1250,  ..., -14.9398, -15.1639, -15.2188],
         ...,
         [-12.1250, -12.1250, -12.1250,  ..., -16.3867, -15.9945, -15.8984],
         [-12.1250, -12.1250, -12.1250,  ..., -16.3867, -15.9945, -15.8984],
         [-12.1250, -12.1250, -12.1250,  ..., -16.3867, -15.9945, -15.8984]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the sour food in this image? Please output segmentation mask. ASSISTANT: the sour food</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]:  [tensor([[[-11.7500, -11.7500, -11.7500,  ..., -14.2500, -14.2500, -14.2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1064, 1600])):  [tensor([[[-5.8125, -5.8125, -5.8098,  ..., -7.3520, -7.3438, -7.3438],
         [-5.8125, -5.8125, -5.8098,  ..., -7.3520, -7.3437, -7.3437],
         [-5.8191, -5.8191, -5.8165,  ..., -7.3656, -7.3574, -7.3574],
         ...,
         [-7.0672, -7.0672, -7.0695,  ..., -5.7672, -5.7641, -5.7641],
         [-7.0122, -7.0122, -7.0139,  ..., -5.6022, -5.5991, -5.5991],
         [-6.9727, -6.9727, -6.9740,  ..., -5.4836, -5.4805, -5.4805]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the stacked cookies in this image? Please output segmentation mask. ASSISTANT: the stacked cookies</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1402, 2102])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1402, 2102])):  [tensor([[[-7.4688, -7.4688, -7.4688,  ..., -5.4375, -5.4375, -5.4375],
         [-7.4688, -7.4688, -7.4688,  ..., -5.4375, -5.4375, -5.4375],
         [-7.4688, -7.4688, -7.4688,  ..., -5.4375, -5.4375, -5.4375],
         ...,
         [-3.9392, -3.9392, -3.9392,  ..., -3.7097, -3.7097, -3.7097],
         [-4.1276, -4.1276, -4.1276,  ..., -3.6840, -3.6840, -3.6840],
         [-4.2168, -4.2168, -4.2168,  ..., -3.6719, -3.6719, -3.6719]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If a person wanted to charge their electric car while parked in the street, what object in the picture could they use to connect the car to an electrical power source? Please output segmentation mask. ASSISTANT: if a person wanted to charge their electric car while parked in the street, what object in the picture could they use to connect the car to an electrical power source</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[86]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2016, 3032])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2016, 3032])):  [tensor([[[-10.1250, -10.1250, -10.1250,  ...,  -8.9375,  -8.9375,  -8.9375],
         [-10.1250, -10.1250, -10.1250,  ...,  -8.9375,  -8.9375,  -8.9375],
         [-10.1250, -10.1250, -10.1250,  ...,  -8.9375,  -8.9375,  -8.9375],
         ...,
         [ -8.1067,  -8.1067,  -8.1067,  ...,  -6.9408,  -6.9408,  -6.9408],
         [ -8.3047,  -8.3047,  -8.3047,  ...,  -7.1176,  -7.1176,  -7.1176],
         [ -8.3086,  -8.3086,  -8.3086,  ...,  -7.1211,  -7.1211,  -7.1211]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Where do people put their dirty hygiene products to keep the bathroom clean? Please output segmentation mask. ASSISTANT: where do people put their dirty hygiene products to keep the bathroom clean</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-13.6032, -11.2845, -11.5035,  ..., -10.7844, -11.4960, -11.9381],
         [-11.3274, -11.4606, -11.6969,  ..., -10.9563, -11.2769, -12.3984],
         [-10.2328, -11.0063, -11.5781,  ..., -10.3438, -10.7531, -11.9777],
         ...,
         [ -9.9770, -10.4281, -10.0625,  ...,  -8.7812,  -8.8781,  -9.4582],
         [-10.0660, -10.6031, -10.1000,  ...,  -8.6000,  -8.8569,  -9.5745],
         [ -9.7004, -10.9669, -10.1961,  ...,  -8.7945,  -8.8185, -10.1462]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When the wind blows, small white objects are blown away and scattered in the air. What in the picture is responsible for this phenomenon? Please output segmentation mask. ASSISTANT: when the wind blows, small white objects are blown away and scattered in the air. what in the picture is responsible for this phenomenon</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[-10.9375, -10.8784, -10.6000,  ..., -11.2250, -11.1941, -11.1875],
         [-11.0784, -11.0212, -10.7517,  ..., -11.3243, -11.2682, -11.2563],
         [-11.7430, -11.6947, -11.4673,  ..., -11.7926, -11.6179, -11.5809],
         ...,
         [-10.6741, -10.6583, -10.5841,  ...,  -7.8137,  -8.2762,  -8.3743],
         [ -9.6545,  -9.6503,  -9.6305,  ...,  -7.1220,  -7.4954,  -7.5746],
         [ -8.6348,  -8.6422,  -8.6768,  ...,  -6.4302,  -6.7145,  -6.7749]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that the dog's food should be put into in this image? Please output segmentation mask. ASSISTANT: something that the dog's food should be put into</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[-6.9375, -6.9199, -6.8594,  ..., -6.4688, -6.2992, -6.2500],
         [-6.9410, -6.9258, -6.8734,  ..., -6.4529, -6.2929, -6.2465],
         [-6.9531, -6.9461, -6.9219,  ..., -6.3984, -6.2713, -6.2344],
         ...,
         [-4.9531, -4.9786, -5.0664,  ..., -0.4174, -0.3692, -0.3552],
         [-4.2645, -4.2629, -4.2576,  ..., -0.7805, -0.7786, -0.7780],
         [-4.6500, -4.6301, -4.5617,  ..., -2.9340, -2.9876, -3.0031]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the food that makes people feel spicy or hot in this image? Please output segmentation mask. ASSISTANT: the food that makes people feel spicy or hot</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[67]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 679, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 679, 1024])):  [tensor([[[-12.8750, -12.8750, -12.6719,  ...,  -5.3906,  -5.5000,  -5.5000],
         [-12.8750, -12.8750, -12.6719,  ...,  -5.3906,  -5.5000,  -5.5000],
         [-12.6094, -12.6094, -12.4512,  ...,  -5.5762,  -5.6406,  -5.6406],
         ...,
         [ -3.8633,  -3.8633,  -3.9312,  ...,  -6.6597,  -6.6016,  -6.6016],
         [ -3.9023,  -3.9023,  -3.9888,  ...,  -6.2485,  -6.1797,  -6.1797],
         [ -4.5723,  -4.5723,  -4.6707,  ...,  -6.6558,  -6.5898,  -6.5898]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the object used to protect the ears in this image? Please output segmentation mask. ASSISTANT: the object used to protect the ears</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2112, 2816])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2112, 2816])):  [tensor([[[-20.1250, -20.1250, -20.1250,  ..., -21.3750, -21.3750, -21.3750],
         [-20.1250, -20.1250, -20.1250,  ..., -21.3750, -21.3750, -21.3750],
         [-20.1250, -20.1250, -20.1250,  ..., -21.3750, -21.3750, -21.3750],
         ...,
         [-15.0568, -15.0568, -15.0568,  ..., -14.8693, -14.8693, -14.8693],
         [-15.3466, -15.3466, -15.3466,  ..., -14.9091, -14.9091, -14.9091],
         [-15.3828, -15.3828, -15.3828,  ..., -14.9141, -14.9141, -14.9141]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the place where piano players should sit in this image? Please output segmentation mask. ASSISTANT: the place where piano players should sit</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 960, 1280])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 960, 1280])):  [tensor([[[-7.7500, -7.7500, -7.7148,  ..., -7.0879, -7.1250, -7.1250],
         [-7.7500, -7.7500, -7.7148,  ..., -7.0879, -7.1250, -7.1250],
         [-7.7227, -7.7227, -7.6926,  ..., -7.0774, -7.1113, -7.1113],
         ...,
         [-3.9902, -3.9902, -4.0200,  ..., -4.8215, -4.8301, -4.8301],
         [-4.3812, -4.3812, -4.4066,  ..., -5.3687, -5.3750, -5.3750],
         [-4.9473, -4.9473, -4.9684,  ..., -6.0896, -6.0859, -6.0859]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the person who is shooting a free throw in this image? Please output segmentation mask. ASSISTANT: the person who is shooting a free throw</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2287, 1831])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2287, 1831])):  [tensor([[[-10.6250, -10.6250, -10.6250,  ..., -10.1899, -10.0639, -10.0156],
         [-10.6250, -10.6250, -10.6250,  ..., -10.1899, -10.0639, -10.0156],
         [-10.6250, -10.6250, -10.6250,  ..., -10.1899, -10.0639, -10.0156],
         ...,
         [ -6.8750,  -6.8750,  -6.8750,  ...,  -8.7354,  -9.1412,  -9.2969],
         [ -6.8750,  -6.8750,  -6.8750,  ...,  -8.7354,  -9.1412,  -9.2969],
         [ -6.8750,  -6.8750,  -6.8750,  ...,  -8.7354,  -9.1412,  -9.2969]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When soldiers gather for a group photo, what object in the picture is usually held by one of them to represent their unit or country? Please output segmentation mask. ASSISTANT: when soldiers gather for a group photo, what object in the picture is usually held by one of them to represent their unit or country</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]

(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-4.6443, -4.5229, -4.0546,  ..., -2.0688, -2.4747, -2.1898],4.2500],
         [-4.1147, -4.4069, -4.0492,  ..., -1.5545, -2.0295, -2.0289],
         [-3.3353, -3.7273, -3.6914,  ..., -1.1963, -1.5590, -1.3629],
         ...,
         [-0.5726, -0.2678, -0.2583,  ..., -0.0995, -0.1965, -0.1282],
         [-0.5113,  0.1324,  0.0647,  ...,  0.1875, -0.0321,  0.5191],
         [-1.6308, -1.4092, -1.1617,  ..., -1.2826, -1.3418, -1.5188]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the dog that puts its tongue out in this image? Please output segmentation mask. ASSISTANT: the dog that puts its tongue out</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1600, 1200])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1600, 1200])):  [tensor([[[-10.3750, -10.3750, -10.3570,  ...,  -9.3406,  -9.2775,  -9.2344],
         [-10.3750, -10.3750, -10.3570,  ...,  -9.3406,  -9.2775,  -9.2344],
         [-10.3484, -10.3484, -10.3309,  ...,  -9.3403,  -9.2743,  -9.2287],
         ...,
         [ -9.5648,  -9.5648,  -9.5886,  ...,  -8.6465,  -8.5410,  -8.4475],
         [ -9.5625,  -9.5625,  -9.5859,  ...,  -8.6406,  -8.5306,  -8.4336],
         [ -9.5625,  -9.5625,  -9.5859,  ...,  -8.6406,  -8.5306,  -8.4336]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a rural landscape, what objects in the picture could provide shade and shelter for animals or humans? Please output segmentation mask. ASSISTANT: in a rural landscape, what objects in the picture could provide shade and shelter for animals or humans</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[72]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[ -5.1250,  -5.1461,  -5.2188,  ...,  -6.0781,  -5.9449,  -5.9062],
         [ -5.1250,  -5.1382,  -5.1836,  ...,  -5.9920,  -5.8765,  -5.8430],
         [ -5.1250,  -5.1109,  -5.0625,  ...,  -5.6953,  -5.6408,  -5.6250],
         ...,
         [ -7.8125,  -7.8266,  -7.8750,  ...,  -7.7656,  -7.9715,  -8.0312],
         [ -7.4828,  -7.5285,  -7.6860,  ...,  -7.7538,  -7.9374,  -7.9907],
         [ -9.8000,  -9.8310,  -9.9375,  ..., -10.7797, -10.8778, -10.9063]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something used for playing videos or music in this image? Please output segmentation mask. ASSISTANT: something used for playing videos or music</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-0.3320, -0.3320, -0.3323,  ..., -5.0547, -5.0625, -5.0625],
         [-0.3320, -0.3320, -0.3323,  ..., -5.0547, -5.0625, -5.0625],
         [-0.2725, -0.2725, -0.2675,  ..., -5.0928, -5.1172, -5.1172],
         ...,
         [-5.0039, -5.0039, -5.0283,  ..., -4.6226, -4.6445, -4.6445],
         [-5.5547, -5.5547, -5.5859,  ..., -5.4629, -5.4727, -5.4727],
         [-6.6641, -6.6641, -6.7109,  ..., -7.3574, -7.3555, -7.3555]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When celebrating birthdays, it is common to have a cake with decorations. What part of the cake in the picture is typically used to write birthday greetings or the name of the person celebrating? Please output segmentation mask. ASSISTANT: when celebrating birthdays, it is common to have a cake with decorations. what part of the cake in the picture is typically used to write birthday greetings or the name of the person celebrating</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[97]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[-8.1875, -8.1102, -7.8438,  ..., -7.4219, -7.1676, -7.0938],
         [-8.0820, -8.0300, -7.8508,  ..., -7.3920, -7.1881, -7.1289],
         [-7.7188, -7.7539, -7.8750,  ..., -7.2891, -7.2588, -7.2500],
         ...,
         [-6.5156, -6.5297, -6.5781,  ..., -5.7891, -5.8920, -5.9219],
         [-6.3910, -6.4464, -6.6373,  ..., -5.8651, -5.9996, -6.0387],
         [-7.1000, -7.1165, -7.1734,  ..., -7.2000, -7.3163, -7.3500]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that allow pedestrians to cross the canyon in this image? Please output segmentation mask. ASSISTANT: something that allow pedestrians to cross the canyon</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1989, 5815])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1989, 5815])):  [tensor([[[-8.3750, -8.3750, -8.3750,  ..., -4.4062, -4.4062, -4.4062],
         [-8.3750, -8.3750, -8.3750,  ..., -4.4062, -4.4062, -4.4062],
         [-8.3750, -8.3750, -8.3750,  ..., -4.4062, -4.4062, -4.4062],
         ...,
         [-0.9542, -0.9542, -0.9542,  ..., -3.1387, -3.1387, -3.1387],
         [-0.9542, -0.9542, -0.9542,  ..., -3.1387, -3.1387, -3.1387],
         [-0.9542, -0.9542, -0.9542,  ..., -3.1387, -3.1387, -3.1387]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the place where the dishes and meals should be put for eating in this image? Please output segmentation mask. ASSISTANT: the place where the dishes and meals should be put for eating</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[72]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 601, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 601, 800])):  [tensor([[[-6.3750, -6.3405, -6.1781,  ..., -5.5688, -5.3831, -5.3438],
         [-6.2947, -6.2654, -6.1274,  ..., -5.5661, -5.3975, -5.3618],
         [-5.9161, -5.9111, -5.8878,  ..., -5.5538, -5.4655, -5.4468],
         ...,
         [-2.8549, -2.8603, -2.8860,  ..., -3.2411, -3.2965, -3.3083],
         [-3.6019, -3.6133, -3.6672,  ..., -4.2910, -4.3439, -4.3551],
         [-4.7215, -4.7387, -4.8198,  ..., -5.7345, -5.7676, -5.7746]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that the persons use to cross the water in this image? Please output segmentation mask. ASSISTANT: something that the persons use to cross the water</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[-5.1875, -5.1875, -5.1797,  ..., -4.1541, -4.1562, -4.1562],
         [-5.1875, -5.1875, -5.1797,  ..., -4.1541, -4.1562, -4.1562],
         [-5.1922, -5.1922, -5.1845,  ..., -4.1662, -4.1684, -4.1684],
         ...,
         [-3.8930, -3.8930, -3.8865,  ..., -3.5095, -3.5094, -3.5094],
         [-4.2087, -4.2087, -4.2029,  ..., -4.1288, -4.1281, -4.1281],
         [-4.4531, -4.4531, -4.4479,  ..., -4.6143, -4.6133, -4.6133]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Driving at night can be very dangerous due to poor visibility, which can lead to accidents. What part of the car needs to be turned on when driving at night? Please output segmentation mask. ASSISTANT: driving at night can be very dangerous due to poor visibility, which can lead to accidents. what part of the car needs to be turned on when driving at night</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 864, 1152])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 864, 1152])):  [tensor([[[-14.4375, -14.4375, -14.2852,  ..., -12.7548, -12.6250, -12.6250],
         [-14.4375, -14.4375, -14.2852,  ..., -12.7548, -12.6250, -12.6250],
         [-14.3754, -14.3754, -14.2424,  ..., -12.8420, -12.7209, -12.7209],
         ...,
         [-12.6215, -12.6215, -12.8149,  ..., -11.6162, -11.6636, -11.6636],
         [-12.7396, -12.7396, -12.9079,  ..., -12.2315, -12.2917, -12.2917],
         [-13.2734, -13.2734, -13.3701,  ..., -13.1735, -13.2031, -13.2031]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n This rocky terrain can be challenging to navigate. What object in the picture could provide information to guide travelers through this area? Please output segmentation mask. ASSISTANT: this rocky terrain can be challenging to navigate. what object in the picture could provide information to guide travelers through this area</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2592, 3888])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2592, 3888])):  [tensor([[[-10.8750, -10.8750, -10.8750,  ..., -11.1875, -11.1875, -11.1875],
         [-10.8750, -10.8750, -10.8750,  ..., -11.1875, -11.1875, -11.1875],
         [-10.8750, -10.8750, -10.8750,  ..., -11.1875, -11.1875, -11.1875],
         ...,
         [ -6.4534,  -6.4534,  -6.4534,  ...,  -6.5332,  -6.5332,  -6.5332],
         [ -6.4844,  -6.4844,  -6.4844,  ...,  -6.5469,  -6.5469,  -6.5469],
         [ -6.4844,  -6.4844,  -6.4844,  ...,  -6.5469,  -6.5469,  -6.5469]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is The fastest sinking ship in this image? Please output segmentation mask. ASSISTANT: the fastest sinking ship</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 333, 500])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 333, 500])):  [tensor([[[ -9.5625,  -9.4788,  -9.3187,  ...,  -9.7913,  -9.8873,  -9.9375],
         [ -9.6798,  -9.6185,  -9.5013,  ..., -10.0556, -10.1430, -10.1888],
         [ -9.9038,  -9.8853,  -9.8501,  ..., -10.5606, -10.6316, -10.6688],
         ...,
         [ -7.1075,  -6.9976,  -6.7877,  ...,  -7.5121,  -7.4968,  -7.4887],
         [ -7.6700,  -7.6200,  -7.5245,  ...,  -7.6443,  -7.6492,  -7.6517],
         [ -8.3900,  -8.4515,  -8.5690,  ...,  -7.8098,  -7.7901,  -7.7797]]],
       device='cuda:0')]

(PLUM.py) >> pred_masks (shape: torch.Size([1, 1712, 2288])):  [tensor([[[-7.6875, -7.6875, -7.6875,  ..., -6.0312, -6.0312, -6.0312],2500],
         [-7.6875, -7.6875, -7.6875,  ..., -6.0312, -6.0312, -6.0312],
         [-7.6875, -7.6875, -7.6875,  ..., -6.0312, -6.0312, -6.0312],
         ...,
         [-3.9707, -3.9707, -3.9707,  ..., -3.3537, -3.3537, -3.3537],
         [-4.0074, -4.0074, -4.0074,  ..., -3.3310, -3.3310, -3.3310],
         [-4.0215, -4.0215, -4.0215,  ..., -3.3223, -3.3223, -3.3223]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the goat nearest to the bottom stone in this image? Please output segmentation mask. ASSISTANT: the goat nearest to the bottom stone</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2344, 3190])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2344, 3190])):  [tensor([[[-17.6250, -17.6250, -17.6250,  ..., -19.2500, -19.2500, -19.2500],
         [-17.6250, -17.6250, -17.6250,  ..., -19.2500, -19.2500, -19.2500],
         [-17.6250, -17.6250, -17.6250,  ..., -19.2500, -19.2500, -19.2500],
         ...,
         [ -7.3931,  -7.3931,  -7.3931,  ..., -10.4181, -10.4181, -10.4181],
         [ -7.4922,  -7.4922,  -7.4922,  ..., -10.5078, -10.5078, -10.5078],
         [ -7.4922,  -7.4922,  -7.4922,  ..., -10.5078, -10.5078, -10.5078]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the objects that can protect the snail and prevent it from getting injured in this image? Please output segmentation mask. ASSISTANT: the objects that can protect the snail and prevent it from getting injured</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[71]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1350, 1800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1350, 1800])):  [tensor([[[ 0.4590,  0.4590,  0.4590,  ..., -3.3125, -3.3125, -3.3125],
         [ 0.4590,  0.4590,  0.4590,  ..., -3.3125, -3.3125, -3.3125],
         [ 0.4590,  0.4590,  0.4590,  ..., -3.3125, -3.3125, -3.3125],
         ...,
         [-2.8396, -2.8396, -2.8396,  ..., -3.1315, -3.1315, -3.1315],
         [-3.2663, -3.2663, -3.2663,  ..., -3.7070, -3.7070, -3.7070],
         [-3.5312, -3.5312, -3.5312,  ..., -4.0645, -4.0645, -4.0645]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If a person wants to watch TV or a movie, which furniture is the most suitable for them to sit and watch? Please output segmentation mask. ASSISTANT: if a person wants to watch tv or a movie, which furniture is the most suitable for them to sit and watch</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 1., 1., 0.],
         [0., 0., 0.,  ..., 1., 1., 0.],
         [0., 0., 0.,  ..., 1., 1., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-7.3160, -6.5845, -6.5998,  ..., -4.0380, -4.3566, -4.4951],
         [-6.5415, -6.9325, -6.9328,  ..., -3.5492, -3.6863, -4.3566],
         [-6.5967, -6.7578, -6.8594,  ..., -2.5977, -2.8961, -3.4815],
         ...,
         [-7.6248, -6.7547, -6.1562,  ..., -0.6458, -0.7543, -1.3831],
         [-7.8291, -7.0256, -6.1813,  ..., -0.5153, -0.9808, -1.5053],
         [-7.6828, -7.1680, -6.5229,  ..., -1.4515, -1.8336, -2.7046]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In historical buildings, there are often signs or symbols displayed on the walls or floors to represent a specific meaning or identity. What in the picture could be used to display such signs or symbols? Please output segmentation mask. ASSISTANT: in historical buildings, there are often signs or symbols displayed on the walls or floors to represent a specific meaning or identity. what in the picture could be used to display such signs or symbols</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[90]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-5.5625, -5.5625, -5.5078,  ..., -5.3125, -5.3438, -5.3438],
         [-5.5625, -5.5625, -5.5078,  ..., -5.3125, -5.3438, -5.3438],
         [-5.6133, -5.6133, -5.5591,  ..., -5.3696, -5.3867, -5.3867],
         ...,
         [-3.4512, -3.4512, -3.4854,  ..., -3.3416, -3.3672, -3.3672],
         [-3.8027, -3.8027, -3.8372,  ..., -3.8628, -3.8770, -3.8770],
         [-4.5020, -4.5020, -4.5232,  ..., -4.9243, -4.9121, -4.9121]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Insects can help pollinate flowers effectively, but some insects also collect pollen and nectar to produce honey. What in the picture does not make honey? Please output segmentation mask. ASSISTANT: insects can help pollinate flowers effectively, but some insects also collect pollen and nectar to produce honey. what in the picture does not make honey</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[89]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 651, 500])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 651, 500])):  [tensor([[[-11.0625, -10.9150, -10.3991,  ..., -11.3923, -10.4849,  -9.4533],
         [-10.8342, -10.7472, -10.4429,  ..., -11.4172, -10.4394,  -9.3128],
         [-10.0359, -10.1605, -10.5962,  ..., -11.5044, -10.2803,  -8.8215],
         ...,
         [ -9.2851,  -9.6597, -10.9704,  ..., -12.6229, -11.8808, -10.9392],
         [ -8.6746,  -9.1071, -10.6205,  ..., -12.3104, -11.4532, -10.3271],
         [ -8.5000,  -8.9491, -10.5205,  ..., -12.2211, -11.3310, -10.1520]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Flowers are often used to decorate tables during special occasions or events. What item in the picture can be used to hold the flowers and keep them fresh by providing water to them? Please output segmentation mask. ASSISTANT: flowers are often used to decorate tables during special occasions or events. what item in the picture can be used to hold the flowers and keep them fresh by providing water to them</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1370, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1370, 2048])):  [tensor([[[-6.1562, -6.1562, -6.1562,  ..., -8.0000, -8.0000, -8.0000],
         [-6.1562, -6.1562, -6.1562,  ..., -8.0000, -8.0000, -8.0000],
         [-6.1562, -6.1562, -6.1562,  ..., -8.0000, -8.0000, -8.0000],
         ...,
         [-1.8521, -1.8521, -1.8521,  ..., -8.1914, -8.1914, -8.1914],
         [-2.1870, -2.1870, -2.1870,  ..., -7.8711, -7.8711, -7.8711],
         [-2.3545, -2.3545, -2.3545,  ..., -7.7109, -7.7109, -7.7109]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Bamboo is very hard, and its sharp edges can easily scratch people. What tool in the picture can I use to split the bamboo and cross this bamboo forest? Please output segmentation mask. ASSISTANT: bamboo is very hard, and its sharp edges can easily scratch people. what tool in the picture can i use to split the bamboo and cross this bamboo forest</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[92]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[-6.8125, -6.8125, -6.8148,  ..., -1.6863, -1.6875, -1.6875],
         [-6.8125, -6.8125, -6.8148,  ..., -1.6863, -1.6875, -1.6875],
         [-6.8133, -6.8133, -6.8156,  ..., -1.6734, -1.6744, -1.6744],
         ...,
         [-6.0883, -6.0883, -6.0894,  ..., -5.7529, -5.7527, -5.7527],
         [-6.6556, -6.6556, -6.6562,  ..., -6.4892, -6.4875, -6.4875],
         [-7.0977, -7.0977, -7.0978,  ..., -7.0656, -7.0625, -7.0625]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the damaged part on the pants in this image? Please output segmentation mask. ASSISTANT: the damaged part on the pants</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[-4.9688, -4.9688, -4.9688,  ..., -3.0156, -3.0156, -3.0156],
         [-4.9688, -4.9688, -4.9688,  ..., -3.0156, -3.0156, -3.0156],
         [-4.9688, -4.9688, -4.9688,  ..., -3.0156, -3.0156, -3.0156],
         ...,
         [-4.1003, -4.1003, -4.1003,  ..., -2.5951, -2.5951, -2.5951],
         [-4.2539, -4.2539, -4.2539,  ..., -2.9883, -2.9883, -2.9883],
         [-4.2539, -4.2539, -4.2539,  ..., -2.9883, -2.9883, -2.9883]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Seafood dishes often include a tangy condiment that enhances the flavor. What item in the picture can be squeezed onto the seafood as a tangy flavor enhancer? Please output segmentation mask. ASSISTANT: seafood dishes often include a tangy condiment that enhances the flavor. what item in the picture can be squeezed onto the seafood as a tangy flavor enhancer</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[96]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]:  [tensor([[[-7.6875, -7.6875, -7.6875,  ..., -6.0312, -6.0312, -6.0312],2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2848, 4288])):  [tensor([[[-8.0625, -8.0625, -8.0625,  ..., -1.5078, -1.5078, -1.5078],
         [-8.0625, -8.0625, -8.0625,  ..., -1.5078, -1.5078, -1.5078],
         [-8.0625, -8.0625, -8.0625,  ..., -1.5078, -1.5078, -1.5078],
         ...,
         [-9.2048, -9.2048, -9.2048,  ..., -6.6789, -6.6789, -6.6789],
         [-9.2578, -9.2578, -9.2578,  ..., -6.7539, -6.7539, -6.7539],
         [-9.2578, -9.2578, -9.2578,  ..., -6.7539, -6.7539, -6.7539]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a modern office, what object in the picture is commonly used for inputting data and controlling the computer? Please output segmentation mask. ASSISTANT: in a modern office, what object in the picture is commonly used for inputting data and controlling the computer</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[74]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1024, 1280])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1024, 1280])):  [tensor([[[-14.7500, -14.7500, -14.6484,  ..., -11.0859, -11.1250, -11.1250],
         [-14.7500, -14.7500, -14.6484,  ..., -11.0859, -11.1250, -11.1250],
         [-14.7617, -14.7617, -14.6679,  ..., -11.0889, -11.1289, -11.1289],
         ...,
         [-12.1247, -12.1247, -12.1666,  ..., -10.7301, -10.7652, -10.7652],
         [-11.8242, -11.8242, -11.8149,  ..., -10.1971, -10.2131, -10.2131],
         [-11.8516, -11.8516, -11.8125,  ...,  -9.8965,  -9.8906,  -9.8906]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a zoo, there are separate areas designated for different animals. What structure in the picture is used to confine and display animals? Please output segmentation mask. ASSISTANT: in a zoo, there are separate areas designated for different animals. what structure in the picture is used to confine and display animals</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[0., 0., 1.,  ..., 0., 0., 0.],
         [0., 0., 1.,  ..., 0., 0., 0.],
         [0., 0., 1.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[ 0.5430,  0.5430,  0.5430,  ...,  0.8203,  0.8203,  0.8203],
         [ 0.5430,  0.5430,  0.5430,  ...,  0.8203,  0.8203,  0.8203],
         [ 0.5430,  0.5430,  0.5430,  ...,  0.8203,  0.8203,  0.8203],
         ...,
         [-1.0857, -1.0857, -1.0857,  ..., -1.7469, -1.7469, -1.7469],
         [-1.5868, -1.5868, -1.5868,  ..., -2.6482, -2.6482, -2.6482],
         [-1.7042, -1.7042, -1.7042,  ..., -2.8594, -2.8594, -2.8594]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that tastes spicy in this image? Please output segmentation mask. ASSISTANT: something that tastes spicy</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-11.5000, -11.5000, -11.2344,  ...,  -9.3672,  -9.5000,  -9.5000],
         [-11.5000, -11.5000, -11.2344,  ...,  -9.3672,  -9.5000,  -9.5000],
         [-11.3984, -11.3984, -11.1621,  ...,  -9.3770,  -9.4922,  -9.4922],
         ...,
         [-10.1094, -10.1094, -10.3535,  ...,  -9.5166,  -9.6406,  -9.6406],
         [-10.1094, -10.1094, -10.3350,  ...,  -9.8887, -10.0156, -10.0156],
         [-10.5781, -10.5781, -10.7236,  ..., -10.6035, -10.6719, -10.6719]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When going fishing on a calm sea, what type of boat shown in the picture would be an ideal choice for a peaceful fishing experience? Please output segmentation mask. ASSISTANT: when going fishing on a calm sea, what type of boat shown in the picture would be an ideal choice for a peaceful fishing experience</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-6.7419, -6.6178, -6.8393,  ..., -7.7934, -8.4252, -8.9307],
         [-6.4245, -6.5662, -6.5922,  ..., -8.4594, -9.1256, -9.0698],
         [-6.9252, -7.1078, -6.8906,  ..., -8.2266, -8.6344, -8.8707],
         ...,
         [-2.2284, -2.3305, -2.4336,  ..., -0.7203, -0.9314, -0.9758],
         [-2.2699, -2.2463, -2.3914,  ..., -0.6206, -1.0589, -1.0839],
         [-2.9909, -2.9414, -3.0437,  ..., -2.0121, -2.3112, -2.2685]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Insects have various ways to protect themselves from predators. What characteristics can a moth use to deter potential threats? Please output segmentation mask. ASSISTANT: insects have various ways to protect themselves from predators. what characteristics can a moth use to deter potential threats</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[77]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3872, 2592])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3872, 2592])):  [tensor([[[-12.8125, -12.8125, -12.8125,  ..., -13.3680, -13.3203, -13.3203],
         [-12.8125, -12.8125, -12.8125,  ..., -13.3680, -13.3203, -13.3203],
         [-12.8125, -12.8125, -12.8125,  ..., -13.3680, -13.3203, -13.3203],
         ...,
         [ -8.3750,  -8.3750,  -8.3750,  ...,  -8.5459,  -8.5898,  -8.5898],
         [ -8.3750,  -8.3750,  -8.3750,  ...,  -8.5459,  -8.5898,  -8.5898],
         [ -8.3750,  -8.3750,  -8.3750,  ...,  -8.5459,  -8.5898,  -8.5898]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Fishing is a popular activity for relaxation and leisure. What tool is the man in the picture using to catch fish? Please output segmentation mask. ASSISTANT: fishing is a popular activity for relaxation and leisure. what tool is the man in the picture using to catch fish</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[-16.1250, -16.1250, -16.0750,  ..., -11.8375, -11.8125, -11.8125],
         [-16.1250, -16.1250, -16.0750,  ..., -11.8375, -11.8125, -11.8125],
         [-16.1297, -16.1297, -16.0805,  ..., -11.9573, -11.9320, -11.9320],
         ...,
         [-16.0187, -16.0187, -16.0575,  ..., -14.1637, -14.2140, -14.2140],
         [-17.4424, -17.4424, -17.4580,  ..., -15.4242, -15.4650, -15.4650],
         [-18.5781, -18.5781, -18.5750,  ..., -16.4676, -16.5000, -16.5000]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In some cultures, people use sculptures and figurines to represent various aspects of life, including celebrations, rituals, and art. What object in the picture can be considered a representation of a human figure? Please output segmentation mask. ASSISTANT: in some cultures, people use sculptures and figurines to represent various aspects of life, including celebrations, rituals, and art. what object in the picture can be considered a representation of a human figure</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[95]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2868, 4309])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2868, 4309])):  [tensor([[[-8.1875, -8.1875, -8.1875,  ..., -9.6875, -9.6875, -9.6875],
         [-8.1875, -8.1875, -8.1875,  ..., -9.6875, -9.6875, -9.6875],
         [-8.1875, -8.1875, -8.1875,  ..., -9.6875, -9.6875, -9.6875],
         ...,
         [-7.1178, -7.1178, -7.1178,  ..., -4.9636, -4.9636, -4.9636],
         [-7.1289, -7.1289, -7.1289,  ..., -4.9297, -4.9297, -4.9297],
         [-7.1289, -7.1289, -7.1289,  ..., -4.9297, -4.9297, -4.9297]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the shadow of the red car in this image? Please output segmentation mask. ASSISTANT: the shadow of the red car</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2736, 3648])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2736, 3648])):  [tensor([[[-8.1875, -8.1875, -8.1875,  ..., -6.0000, -6.0000, -6.0000],
         [-8.1875, -8.1875, -8.1875,  ..., -6.0000, -6.0000, -6.0000],
         [-8.1875, -8.1875, -8.1875,  ..., -6.0000, -6.0000, -6.0000],
         ...,
         [-5.3136, -5.3136, -5.3136,  ..., -6.2703, -6.2703, -6.2703],
         [-5.5469, -5.5469, -5.5469,  ..., -6.6328, -6.6328, -6.6328],
         [-5.5469, -5.5469, -5.5469,  ..., -6.6328, -6.6328, -6.6328]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the part of this animal's body that comes into contact with the air in this image? Please output segmentation mask. ASSISTANT: the part of this animal's body that comes into contact with the air</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[72]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1000, 1504])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1000, 1504])):  [tensor([[[-16.8750, -16.8750, -16.7992,  ..., -13.1787, -13.1250, -13.1250],
         [-16.8750, -16.8750, -16.7992,  ..., -13.1787, -13.1250, -13.1250],
         [-16.7975, -16.7975, -16.7251,  ..., -13.2599, -13.2073, -13.2073],
         ...,
         [-15.4899, -15.4899, -15.5208,  ..., -15.4074, -15.4087, -15.4087],
         [-15.6815, -15.6815, -15.7054,  ..., -15.1769, -15.1640, -15.1640],
         [-15.8281, -15.8281, -15.8467,  ..., -15.0004, -14.9766, -14.9766]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Many people use bags to carry their belongings when they go out. What part of the bag in the picture can be used to carry the bag comfortably over the shoulder? Please output segmentation mask. ASSISTANT: many people use bags to carry their belongings when they go out. what part of the bag in the picture can be used to carry the bag comfortably over the shoulder</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]

>> (validate) sampled_classes_list:  [None]device='cuda:0')]:  [tensor([[[-7.6875, -7.6875, -7.6875,  ..., -6.0312, -6.0312, -6.0312],2500],
(PLUM.py) >> gt_masks (shape: torch.Size([1, 720, 502])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 720, 502])):  [tensor([[[-18.5000, -18.2674, -17.3575,  ..., -15.9733, -15.9083, -15.8861],
         [-18.1339, -17.9643, -17.3012,  ..., -16.1837, -15.9813, -15.8148],
         [-16.7014, -16.7786, -17.0809,  ..., -17.0067, -16.2665, -15.5356],
         ...,
         [-10.1250, -10.4953, -11.9441,  ..., -16.4800, -14.3188, -12.0878],
         [ -9.7766, -10.2895, -12.2959,  ..., -17.3930, -14.1614, -10.8222],
         [ -9.6875, -10.2368, -12.3858,  ..., -17.6264, -14.1212, -10.4988]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What object in the picture should be used with water to create foam and effectively remove oil and dirt when your hands are very dirty? Please output segmentation mask. ASSISTANT: what object in the picture should be used with water to create foam and effectively remove oil and dirt when your hands are very dirty</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[79]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[-15.8750, -15.8750, -15.8750,  ..., -10.5625, -10.5625, -10.5625],
         [-15.8750, -15.8750, -15.8750,  ..., -10.5625, -10.5625, -10.5625],
         [-15.8750, -15.8750, -15.8750,  ..., -10.5625, -10.5625, -10.5625],
         ...,
         [-13.7509, -13.7509, -13.7509,  ..., -11.9832, -11.9832, -11.9832],
         [-14.5255, -14.5255, -14.5255,  ..., -13.2649, -13.2649, -13.2649],
         [-14.8281, -14.8281, -14.8281,  ..., -13.7656, -13.7656, -13.7656]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a concert or public speaking event, what object in the picture would be used by the speaker or performer to amplify their voice? Please output segmentation mask. ASSISTANT: in a concert or public speaking event, what object in the picture would be used by the speaker or performer to amplify their voice</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[79]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 482, 720])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 482, 720])):  [tensor([[[ -9.2500,  -9.1337,  -8.6788,  ..., -11.5590, -11.4622, -11.4375],
         [ -9.2450,  -9.1435,  -8.7462,  ..., -11.6279, -11.5434, -11.5218],
         [ -9.2257,  -9.1817,  -9.0095,  ..., -11.8972, -11.8607, -11.8514],
         ...,
         [ -9.3018,  -9.3874,  -9.7223,  ..., -10.0811, -10.3158, -10.3757],
         [ -8.7132,  -8.8312,  -9.2931,  ...,  -9.1435,  -9.3984,  -9.4635],
         [ -8.1127,  -8.2642,  -8.8569,  ...,  -8.1879,  -8.4600,  -8.5295]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In the living room, people often sit on the sofa to watch TV or chat. What object can they use to adjust the TV screen or change channels? Please output segmentation mask. ASSISTANT: in the living room, people often sit on the sofa to watch tv or chat. what object can they use to adjust the tv screen or change channels</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[83]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-8.7348, -8.2428, -8.6066,  ..., -7.0744, -7.4318, -7.5314],
         [-8.4147, -9.2225, -9.3156,  ..., -6.8703, -6.7722, -7.6251],
         [-8.4211, -9.2375, -9.5312,  ..., -6.3203, -6.3547, -6.9885],
         ...,
         [-8.7715, -8.2344, -7.6328,  ..., -2.6484, -3.0383, -3.4841],
         [-8.6975, -8.2163, -7.6422,  ..., -2.5203, -3.2117, -3.6468],
         [-8.6410, -8.5309, -8.0125,  ..., -4.0564, -4.4819, -5.2495]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that produces pollen in this image? Please output segmentation mask. ASSISTANT: something that produces pollen</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1023, 1392])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1023, 1392])):  [tensor([[[-1.5312, -1.5312, -1.5094,  ..., -5.3882, -5.3750, -5.3750],
         [-1.5312, -1.5312, -1.5094,  ..., -5.3882, -5.3750, -5.3750],
         [-1.5429, -1.5429, -1.5183,  ..., -5.3808, -5.3684, -5.3684],
         ...,
         [-7.7988, -7.7988, -7.8278,  ..., -7.6146, -7.6328, -7.6328],
         [-7.5918, -7.5918, -7.6139,  ..., -7.7644, -7.7765, -7.7765],
         [-7.4219, -7.4219, -7.4384,  ..., -7.8874, -7.8945, -7.8945]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the part that can transmit traction and braking torque, coming into contact with the ground in this image? Please output segmentation mask. ASSISTANT: the part that can transmit traction and braking torque, coming into contact with the ground</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 752, 1169])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 752, 1169])):  [tensor([[[-15.3750, -15.3750, -15.2510,  ..., -14.1444, -14.1875, -14.1875],
         [-15.3750, -15.3750, -15.2510,  ..., -14.1444, -14.1875, -14.1875],
         [-15.3318, -15.3318, -15.2316,  ..., -14.1171, -14.1551, -14.1551],
         ...,
         [ -8.8306,  -8.8306,  -8.8968,  ...,  -9.0685,  -9.1556,  -9.1556],
         [ -8.3241,  -8.3241,  -8.3788,  ...,  -8.3165,  -8.3554,  -8.3554],
         [ -8.3750,  -8.3750,  -8.4242,  ...,  -8.1826,  -8.1836,  -8.1836]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the area where people can stand without getting wet in this image? Please output segmentation mask. ASSISTANT: the area where people can stand without getting wet</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-5.3750, -5.3865, -5.4406,  ..., -5.0656, -4.9341, -4.9062],
         [-5.3192, -5.3312, -5.3878,  ..., -5.0066, -4.8859, -4.8603],
         [-5.0563, -5.0707, -5.1388,  ..., -4.7286, -4.6586, -4.6438],
         ...,
         [-5.5656, -5.5916, -5.7138,  ..., -4.8344, -4.8653, -4.8719],
         [-5.6093, -5.6409, -5.7899,  ..., -5.2458, -5.2778, -5.2846],
         [-7.5725, -7.6053, -7.7600,  ..., -7.9205, -7.9567, -7.9643]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a motorcycle race, there are often sharp turns that require skilled maneuvering. What part of the race track in the picture indicates a sharp turn? Please output segmentation mask. ASSISTANT: in a motorcycle race, there are often sharp turns that require skilled maneuvering. what part of the race track in the picture indicates a sharp turn</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 682, 1023])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [0., 0., 0.,  ..., 1., 1., 1.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 682, 1023])):  [tensor([[[-6.0938, -6.0936, -5.9995,  ..., -6.5705, -6.5313, -6.5312],
         [-6.0934, -6.0933, -5.9993,  ..., -6.5706, -6.5314, -6.5313],
         [-5.9560, -5.9559, -5.8722,  ..., -6.6010, -6.5667, -6.5667],
         ...,
         [-3.6425, -3.6425, -3.6523,  ..., -3.8379, -3.8322, -3.8322],
         [-3.6621, -3.6621, -3.6680,  ..., -3.7885, -3.7775, -3.7775],
         [-3.9158, -3.9158, -3.8972,  ..., -4.0466, -4.0116, -4.0115]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the region showing current time in this image? Please output segmentation mask. ASSISTANT: the region showing current time</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-7.6562, -7.6562, -7.5547,  ..., -6.4062, -6.3438, -6.3438],
         [-7.6562, -7.6562, -7.5547,  ..., -6.4062, -6.3438, -6.3438],
         [-7.5430, -7.5430, -7.4678,  ..., -6.3799, -6.3242, -6.3242],
         ...,
         [-5.0391, -5.0391, -5.1416,  ..., -4.8828, -4.8672, -4.8672],
         [-5.3555, -5.3555, -5.4551,  ..., -5.6074, -5.6133, -5.6133],
         [-6.0664, -6.0664, -6.1387,  ..., -6.9004, -6.9023, -6.9023]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the reflection of the object in this image? Please output segmentation mask. ASSISTANT: the reflection of the object</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2820, 3760])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2820, 3760])):  [tensor([[[-5.5938, -5.5938, -5.5938,  ..., -5.9688, -5.9688, -5.9688],
         [-5.5938, -5.5938, -5.5938,  ..., -5.9688, -5.9688, -5.9688],
         [-5.5938, -5.5938, -5.5938,  ..., -5.9688, -5.9688, -5.9688],
         ...,
         [-3.3335, -3.3335, -3.3335,  ..., -3.4206, -3.4206, -3.4206],
         [-3.5303, -3.5303, -3.5303,  ..., -3.7021, -3.7021, -3.7021],
         [-3.5303, -3.5303, -3.5303,  ..., -3.7021, -3.7021, -3.7021]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Dogs use their mouths to perform various tasks, including eating and vocalizing. What part of the dog's body is primarily responsible for these actions? Please output segmentation mask. ASSISTANT: dogs use their mouths to perform various tasks, including eating and vocalizing. what part of the dog's body is primarily responsible for these actions</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]

         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')875, -7.6875, -7.6875,  ..., -6.0312, -6.0312, -6.0312],2500],
>> (validate) mask_positions_in_input_ids:  [[82]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-4.3027, -4.3889, -4.2719,  ..., -3.3499, -3.5067, -3.3971],
         [-3.7014, -4.9841, -4.6156,  ..., -3.2586, -3.2725, -3.6356],
         [-3.7670, -4.7016, -4.4531,  ..., -2.9570, -3.0313, -3.1924],
         ...,
         [-6.1082, -6.0437, -6.8281,  ..., -4.7578, -5.1125, -4.9406],
         [-6.4036, -6.2575, -6.9734,  ..., -4.8297, -5.1294, -5.1680],
         [-7.0072, -6.9736, -7.3762,  ..., -5.5096, -5.8727, -5.9130]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the camera lens that is more suitable for photographing nearby objects in this image? Please output segmentation mask. ASSISTANT: the camera lens that is more suitable for photographing nearby objects</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1620, 2160])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1620, 2160])):  [tensor([[[-7.0000, -7.0000, -7.0000,  ..., -7.0938, -7.0938, -7.0938],
         [-7.0000, -7.0000, -7.0000,  ..., -7.0938, -7.0938, -7.0938],
         [-7.0000, -7.0000, -7.0000,  ..., -7.0938, -7.0938, -7.0938],
         ...,
         [-6.2841, -6.2841, -6.2841,  ..., -5.7581, -5.7581, -5.7581],
         [-6.5767, -6.5767, -6.5767,  ..., -6.4173, -6.4173, -6.4173],
         [-6.7070, -6.7070, -6.7070,  ..., -6.7109, -6.7109, -6.7109]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the part of the house that can be opened in this image? Please output segmentation mask. ASSISTANT: the part of the house that can be opened</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[-7.0938, -7.0938, -7.0938,  ..., -7.4062, -7.4062, -7.4062],
         [-7.0938, -7.0938, -7.0938,  ..., -7.4062, -7.4062, -7.4062],
         [-7.0938, -7.0938, -7.0938,  ..., -7.4062, -7.4062, -7.4062],
         ...,
         [-6.1963, -6.1963, -6.1963,  ..., -6.7933, -6.7933, -6.7933],
         [-6.4428, -6.4428, -6.4428,  ..., -7.4447, -7.4447, -7.4447],
         [-6.5391, -6.5391, -6.5391,  ..., -7.6992, -7.6992, -7.6992]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the source of power for the ship in this image? Please output segmentation mask. ASSISTANT: the source of power for the ship</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[-5.8750, -5.8750, -5.8750,  ..., -5.5625, -5.5625, -5.5625],
         [-5.8750, -5.8750, -5.8750,  ..., -5.5625, -5.5625, -5.5625],
         [-5.8750, -5.8750, -5.8750,  ..., -5.5625, -5.5625, -5.5625],
         ...,
         [-5.6367, -5.6367, -5.6367,  ..., -5.7480, -5.7480, -5.7480],
         [-6.2695, -6.2695, -6.2695,  ..., -6.7676, -6.7676, -6.7676],
         [-6.5859, -6.5859, -6.5859,  ..., -7.2773, -7.2773, -7.2773]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When hanging laundry outside to dry, what material in the picture would be suitable for holding the clothes securely in place? Please output segmentation mask. ASSISTANT: when hanging laundry outside to dry, what material in the picture would be suitable for holding the clothes securely in place</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2592, 3872])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2592, 3872])):  [tensor([[[-8.2500, -8.2500, -8.2500,  ..., -7.4375, -7.4375, -7.4375],
         [-8.2500, -8.2500, -8.2500,  ..., -7.4375, -7.4375, -7.4375],
         [-8.2500, -8.2500, -8.2500,  ..., -7.4375, -7.4375, -7.4375],
         ...,
         [-5.0619, -5.0619, -5.0619,  ..., -6.4996, -6.4996, -6.4996],
         [-5.0469, -5.0469, -5.0469,  ..., -6.4180, -6.4180, -6.4180],
         [-5.0469, -5.0469, -5.0469,  ..., -6.4180, -6.4180, -6.4180]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the friend of humans in this image? Please output segmentation mask. ASSISTANT: the friend of humans</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-11.3750, -11.3750, -11.1250,  ..., -12.8984, -12.9375, -12.9375],
         [-11.3750, -11.3750, -11.1250,  ..., -12.8984, -12.9375, -12.9375],
         [-11.1172, -11.1172, -10.9170,  ..., -13.0078, -13.0469, -13.0469],
         ...,
         [ -6.0820,  -6.0820,  -6.1304,  ...,  -8.5000,  -8.6328,  -8.6328],
         [ -6.4062,  -6.4062,  -6.4443,  ...,  -9.0010,  -9.0703,  -9.0703],
         [ -6.7812,  -6.7812,  -6.7939,  ..., -10.5029, -10.4609, -10.4609]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Dogs have keen sense of smell, which is why they can be used as drug-sniffing dogs. Which part in the picture gives dogs this characteristic? Please output segmentation mask. ASSISTANT: dogs have keen sense of smell, which is why they can be used as drug-sniffing dogs. which part in the picture gives dogs this characteristic</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-12.1250, -12.1250, -11.9453,  ..., -12.3125, -12.3125, -12.3125],
         [-12.1250, -12.1250, -11.9453,  ..., -12.3125, -12.3125, -12.3125],
         [-12.0078, -12.0078, -11.8604,  ..., -12.3271, -12.3281, -12.3281],
         ...,
         [-10.8359, -10.8359, -11.0381,  ...,  -9.5703,  -9.6875,  -9.6875],
         [-10.7656, -10.7656, -10.9570,  ...,  -9.9619, -10.0703, -10.0703],
         [-10.7969, -10.7969, -10.9180,  ..., -10.6670, -10.7109, -10.7109]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the place where the patient lies down to receive examination in this image? Please output segmentation mask. ASSISTANT: the place where the patient lies down to receive examination</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2428, 3636])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2428, 3636])):  [tensor([[[-10.3125, -10.3125, -10.3125,  ..., -10.8125, -10.8125, -10.8125],
         [-10.3125, -10.3125, -10.3125,  ..., -10.8125, -10.8125, -10.8125],
         [-10.3125, -10.3125, -10.3125,  ..., -10.8125, -10.8125, -10.8125],
         ...,
         [ -6.1888,  -6.1888,  -6.1888,  ...,  -5.4318,  -5.4318,  -5.4318],
         [ -6.2383,  -6.2383,  -6.2383,  ...,  -5.5195,  -5.5195,  -5.5195],
         [ -6.2383,  -6.2383,  -6.2383,  ...,  -5.5195,  -5.5195,  -5.5195]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In an intense dragon boat race. What object in the picture should be struck to boost the morale of the competing team and cheer them on? Please output segmentation mask. ASSISTANT: in an intense dragon boat race. what object in the picture should be struck to boost the morale of the competing team and cheer them on</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[83]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 986, 1544])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 986, 1544])):  [tensor([[[-14.0625, -14.0625, -14.0390,  ..., -12.5773, -12.5625, -12.5625],
         [-14.0625, -14.0625, -14.0390,  ..., -12.5773, -12.5625, -12.5625],
         [-14.0415, -14.0415, -14.0192,  ..., -12.6105, -12.5959, -12.5959],
         ...,
         [ -8.4877,  -8.4877,  -8.5096,  ...,  -9.4696,  -9.4618,  -9.4618],
         [ -8.3426,  -8.3426,  -8.3627,  ...,  -9.3332,  -9.3271,  -9.3271],
         [ -8.2344,  -8.2344,  -8.2530,  ...,  -9.2315,  -9.2266,  -9.2266]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the equipment for sweeping away rain on rainy days in this image? Please output segmentation mask. ASSISTANT: the equipment for sweeping away rain on rainy days</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]

         ...,, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')875, -7.6875, -7.6875,  ..., -6.0312, -6.0312, -6.0312],2500],
         [-3.7139, -3.7211, -3.7911,  ..., -4.1567, -4.1528, -4.1524],
         [-4.2137, -4.2209, -4.2912,  ..., -4.0218, -4.0264, -4.0268],
         [-4.7587, -4.7658, -4.8347,  ..., -3.8600, -3.8723, -3.8735]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n I want to take a trip around the world, but I need some transportation to help me cross the oceans. What type of transportation in the picture can fulfill this requirement? Please output segmentation mask. ASSISTANT: i want to take a trip around the world, but i need some transportation to help me cross the oceans. what type of transportation in the picture can fulfill this requirement</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[90]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 960, 1280])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 960, 1280])):  [tensor([[[-3.6875, -3.6875, -3.6338,  ..., -4.9355, -4.9688, -4.9688],
         [-3.6875, -3.6875, -3.6338,  ..., -4.9355, -4.9688, -4.9688],
         [-3.6895, -3.6895, -3.6370,  ..., -4.9851, -5.0156, -5.0156],
         ...,
         [-6.6211, -6.6211, -6.6550,  ..., -3.5013, -3.5059, -3.5059],
         [-6.8187, -6.8187, -6.8496,  ..., -4.5406, -4.5375, -4.5375],
         [-7.0703, -7.0703, -7.0974,  ..., -5.8254, -5.8145, -5.8145]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is where we can see the speed of the car in this image? Please output segmentation mask. ASSISTANT: where we can see the speed of the car</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[-10.1875, -10.1875, -10.1875,  ..., -10.3750, -10.3750, -10.3750],
         [-10.1875, -10.1875, -10.1875,  ..., -10.3750, -10.3750, -10.3750],
         [-10.1875, -10.1875, -10.1875,  ..., -10.3750, -10.3750, -10.3750],
         ...,
         [ -4.7266,  -4.7266,  -4.7266,  ...,  -6.4766,  -6.4766,  -6.4766],
         [ -5.3359,  -5.3359,  -5.3359,  ...,  -7.0859,  -7.0859,  -7.0859],
         [ -5.6406,  -5.6406,  -5.6406,  ...,  -7.3906,  -7.3906,  -7.3906]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When a photographer wants to adjust the focus of a camera lens, what part in the picture could they manipulate to achieve this? Please output segmentation mask. ASSISTANT: when a photographer wants to adjust the focus of a camera lens, what part in the picture could they manipulate to achieve this</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1584, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1584, 1600])):  [tensor([[[-11.6250, -11.6250, -11.5984,  ...,  -8.5117,  -8.5000,  -8.5000],
         [-11.6250, -11.6250, -11.5984,  ...,  -8.5117,  -8.5000,  -8.5000],
         [-11.6085, -11.6085, -11.5825,  ...,  -8.5185,  -8.5071,  -8.5071],
         ...,
         [ -7.3375,  -7.3375,  -7.3383,  ...,  -7.9118,  -7.9124,  -7.9124],
         [ -7.2775,  -7.2775,  -7.2833,  ...,  -8.0727,  -8.0724,  -8.0724],
         [ -7.2344,  -7.2344,  -7.2438,  ...,  -8.1884,  -8.1875,  -8.1875]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is unopened flower bud in this image? Please output segmentation mask. ASSISTANT: unopened flower bud</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 566, 850])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 566, 850])):  [tensor([[[-11.5000, -11.4760, -11.3419,  ..., -12.6257, -12.5721, -12.5625],
         [-11.4688, -11.4483, -11.3338,  ..., -12.6503, -12.5982, -12.5889],
         [-11.2944, -11.2934, -11.2883,  ..., -12.7877, -12.7443, -12.7365],
         ...,
         [ -8.3256,  -8.3797,  -8.6818,  ...,  -9.9427,  -9.9429,  -9.9429],
         [ -8.5704,  -8.6172,  -8.8789,  ...,  -9.4346,  -9.4186,  -9.4158],
         [ -8.8151,  -8.8547,  -9.0761,  ...,  -8.9265,  -8.8943,  -8.8886]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n This image depicts a forest. Which of the animals in the picture pose a threat to human safety? Please output segmentation mask. ASSISTANT: this image depicts a forest. which of the animals in the picture pose a threat to human safety</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[73]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 300, 400])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 300, 400])):  [tensor([[[-4.3438, -4.2288, -4.1004,  ..., -3.7795, -3.7497, -3.6562],
         [-4.1425, -4.0209, -3.8461,  ..., -3.6182, -3.5954, -3.5053],
         [-3.7957, -3.6811, -3.4741,  ..., -3.3578, -3.3350, -3.2479],
         ...,
         [-4.0436, -3.9948, -3.9419,  ..., -3.9864, -4.0142, -3.9746],
         [-4.0794, -4.0618, -4.0235,  ..., -3.8822, -3.9835, -4.0219],
         [-4.9431, -4.9563, -4.9743,  ..., -5.0281, -5.1852, -5.2581]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a graduation ceremony, it is a tradition for the graduates to wear a specific type of clothing to signify their achievement. What item of clothing can be seen in the picture that is commonly worn by graduates? Please output segmentation mask. ASSISTANT: in a graduation ceremony, it is a tradition for the graduates to wear a specific type of clothing to signify their achievement. what item of clothing can be seen in the picture that is commonly worn by graduates</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[99]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3456, 2304])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 2304])):  [tensor([[[-10.7500, -10.7500, -10.7500,  ..., -10.3495, -10.3984, -10.3984],
         [-10.7500, -10.7500, -10.7500,  ..., -10.3495, -10.3984, -10.3984],
         [-10.7500, -10.7500, -10.7500,  ..., -10.3495, -10.3984, -10.3984],
         ...,
         [ -8.3750,  -8.3750,  -8.3750,  ...,  -6.2000,  -6.2227,  -6.2227],
         [ -8.3750,  -8.3750,  -8.3750,  ...,  -6.2000,  -6.2227,  -6.2227],
         [ -8.3750,  -8.3750,  -8.3750,  ...,  -6.2000,  -6.2227,  -6.2227]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:

(PLUM.py) >> gt_masks (shape: torch.Size([1, 1600, 1066])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],..., -6.0312, -6.0312, -6.0312],2500],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1600, 1066])):  [tensor([[[-10.2500, -10.2500, -10.2376,  ...,  -9.6625,  -9.6225,  -9.5938],
         [-10.2500, -10.2500, -10.2376,  ...,  -9.6625,  -9.6225,  -9.5938],
         [-10.2281, -10.2281, -10.2163,  ...,  -9.6700,  -9.6238,  -9.5907],
         ...,
         [ -1.1828,  -1.1828,  -1.1569,  ...,  -7.0643,  -6.3681,  -5.8679],
         [ -1.1719,  -1.1719,  -1.1460,  ...,  -7.0431,  -6.3384,  -5.8320],
         [ -1.1719,  -1.1719,  -1.1460,  ...,  -7.0431,  -6.3384,  -5.8320]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that avoids falling down in this image? Please output segmentation mask. ASSISTANT: something that avoids falling down</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1224, 1632])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1224, 1632])):  [tensor([[[-1.3516, -1.3516, -1.3485,  ..., -5.2799, -5.2812, -5.2812],
         [-1.3516, -1.3516, -1.3485,  ..., -5.2799, -5.2812, -5.2812],
         [-1.3498, -1.3498, -1.3467,  ..., -5.2751, -5.2764, -5.2764],
         ...,
         [-2.6131, -2.6131, -2.6106,  ..., -0.0936, -0.0925, -0.0925],
         [-3.3713, -3.3713, -3.3699,  ..., -1.2630, -1.2613, -1.2613],
         [-3.9297, -3.9297, -3.9290,  ..., -2.1243, -2.1221, -2.1221]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the container that can be used to hold soup currently in this image? Please output segmentation mask. ASSISTANT: the container that can be used to hold soup currently</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[67]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-8.0000, -7.8884, -7.3625,  ..., -8.8062, -9.2239, -9.3125],
         [-7.9639, -7.8573, -7.3545,  ..., -8.8286, -9.2413, -9.3289],
         [-7.7938, -7.7102, -7.3166,  ..., -8.9337, -9.3236, -9.4062],
         ...,
         [-6.7469, -6.7533, -6.7834,  ..., -7.0231, -7.1639, -7.1938],
         [-7.0496, -7.0625, -7.1235,  ..., -7.1783, -7.2913, -7.3153],
         [-8.5162, -8.5293, -8.5910,  ..., -9.2518, -9.2225, -9.2162]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When you look closely into a person's eye, what is the part that surrounds the dark-colored iris and helps protect the eye from dust and debris? Please output segmentation mask. ASSISTANT: when you look closely into a person's eye, what is the part that surrounds the dark-colored iris and helps protect the eye from dust and debris</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[-11.5625, -11.5625, -11.5625,  ..., -11.8750, -11.8750, -11.8750],
         [-11.5625, -11.5625, -11.5625,  ..., -11.8750, -11.8750, -11.8750],
         [-11.5625, -11.5625, -11.5625,  ..., -11.8750, -11.8750, -11.8750],
         ...,
         [ -9.9016,  -9.9016,  -9.9016,  ..., -10.2496, -10.2496, -10.2496],
         [-10.2535, -10.2535, -10.2535,  ..., -10.7373, -10.7373, -10.7373],
         [-10.3359, -10.3359, -10.3359,  ..., -10.8516, -10.8516, -10.8516]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the object that reaches the sky in this image? Please output segmentation mask. ASSISTANT: the object that reaches the sky</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2112, 2816])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2112, 2816])):  [tensor([[[-5.2812, -5.2812, -5.2812,  ..., -6.4688, -6.4688, -6.4688],
         [-5.2812, -5.2812, -5.2812,  ..., -6.4688, -6.4688, -6.4688],
         [-5.2812, -5.2812, -5.2812,  ..., -6.4688, -6.4688, -6.4688],
         ...,
         [-5.1620, -5.1620, -5.1620,  ..., -5.4091, -5.4091, -5.4091],
         [-5.4659, -5.4659, -5.4659,  ..., -5.9049, -5.9049, -5.9049],
         [-5.5039, -5.5039, -5.5039,  ..., -5.9668, -5.9668, -5.9668]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In an orchestra, musicians play different instruments. What object in the picture is commonly played with a bow to produce sound? Please output segmentation mask. ASSISTANT: in an orchestra, musicians play different instruments. what object in the picture is commonly played with a bow to produce sound</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[77]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 280, 330])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 280, 330])):  [tensor([[[-8.0527, -7.6074, -8.0324,  ..., -8.1288, -8.3525, -8.0689],
         [-7.5045, -7.7207, -7.8615,  ..., -8.0809, -8.3132, -8.4347],
         [-7.2210, -7.4504, -7.5451,  ..., -7.9002, -7.9991, -8.1290],
         ...,
         [-3.0958, -2.8874, -2.8163,  ..., -5.0851, -5.2475, -5.4122],
         [-3.0263, -2.9088, -2.8072,  ..., -4.1580, -4.5227, -4.6849],
         [-3.9236, -3.9123, -4.0621,  ..., -4.2269, -4.7304, -4.6100]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What object in a typical modern kitchen can be used to quickly heat up leftovers, boil water, or cook frozen food? Please output segmentation mask. ASSISTANT: what object in a typical modern kitchen can be used to quickly heat up leftovers, boil water, or cook frozen food</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[80]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-8.3685, -8.0116, -7.9895,  ..., -3.6611, -3.8944, -4.0869],
         [-8.3768, -8.4778, -8.2688,  ..., -3.5000, -3.7569, -3.9051],
         [-8.7523, -8.8813, -8.6016,  ..., -3.3008, -3.2898, -3.5423],
         ...,
         [-5.7230, -5.9594, -5.8281,  ..., -3.6406, -3.9820, -4.1378],
         [-5.6231, -5.8616, -5.4937,  ..., -3.7875, -3.9561, -4.2171],
         [-5.4523, -5.5375, -5.3748,  ..., -4.6879, -4.9124, -4.9206]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When plants grow excessively in unwanted areas, it is necessary to remove them to maintain the aesthetics of the surroundings. What part of the plants in the picture may need to be removed in this situation? Please output segmentation mask. ASSISTANT: when plants grow excessively in unwanted areas, it is necessary to remove them to maintain the aesthetics of the surroundings. what part of the plants in the picture may need to be removed in this situation</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[97]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 853, 1280])):  [tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 853, 1280])):  [tensor([[[-2.1094, -2.1094, -2.0737,  ..., -4.3408, -4.3750, -4.3750],
         [-2.1094, -2.1094, -2.0737,  ..., -4.3408, -4.3750, -4.3750],
         [-2.1581, -2.1581, -2.1223,  ..., -4.3289, -4.3594, -4.3594],
         ...,
         [-2.3125, -2.3125, -2.2876,  ..., -1.2135, -1.2228, -1.2228],
         [-2.3000, -2.3000, -2.2823,  ..., -1.2869, -1.2962, -1.2962],
         [-2.2891, -2.2891, -2.2777,  ..., -1.3511, -1.3604, -1.3604]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a television studio, various equipment is used to capture and record video footage. What in the picture could be used to stabilize and hold the camera steady during filming? Please output segmentation mask. ASSISTANT: in a television studio, various equipment is used to capture and record video footage. what in the picture could be used to stabilize and hold the camera steady during filming</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2573, 3593])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2573, 3593])):  [tensor([[[-6.8750, -6.8750, -6.8750,  ..., -5.0625, -5.0625, -5.0625],
         [-6.8750, -6.8750, -6.8750,  ..., -5.0625, -5.0625, -5.0625],
         [-6.8750, -6.8750, -6.8750,  ..., -5.0625, -5.0625, -5.0625],
         ...,
         [-3.8791, -3.8791, -3.8791,  ..., -4.1569, -4.1569, -4.1569],
         [-3.8477, -3.8477, -3.8477,  ..., -4.2266, -4.2266, -4.2266],
         [-3.8477, -3.8477, -3.8477,  ..., -4.2266, -4.2266, -4.2266]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n There are two washing machines as shown in the picture. If I need to do laundry, where in the picture would I put the clothes? Please output segmentation mask. ASSISTANT: there are two washing machines as shown in the picture. if i need to do laundry, where in the picture would i put the clothes</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[82]]
>> (validate) sampled_classes_list:  [None]

         ...,gt_masks (shape: torch.Size([1, 1600, 1066])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],..., -6.0312, -6.0312, -6.0312],2500],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3056, 4592])):  [tensor([[[-6.8438, -6.8438, -6.8438,  ..., -6.7188, -6.7188, -6.7188],
         [-6.8438, -6.8438, -6.8438,  ..., -6.7188, -6.7188, -6.7188],
         [-6.8438, -6.8438, -6.8438,  ..., -6.7188, -6.7188, -6.7188],
         ...,
         [-5.8102, -5.8102, -5.8102,  ..., -6.1305, -6.1305, -6.1305],
         [-5.8320, -5.8320, -5.8320,  ..., -6.1211, -6.1211, -6.1211],
         [-5.8320, -5.8320, -5.8320,  ..., -6.1211, -6.1211, -6.1211]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that is noticeably different from the other plants in the picture in this image? Please output segmentation mask. ASSISTANT: something that is noticeably different from the other plants in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3456, 5184])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 5184])):  [tensor([[[-5.8125, -5.8125, -5.8125,  ..., -6.0625, -6.0625, -6.0625],
         [-5.8125, -5.8125, -5.8125,  ..., -6.0625, -6.0625, -6.0625],
         [-5.8125, -5.8125, -5.8125,  ..., -6.0625, -6.0625, -6.0625],
         ...,
         [-3.5098, -3.5098, -3.5098,  ..., -1.7822, -1.7822, -1.7822],
         [-3.5098, -3.5098, -3.5098,  ..., -1.7822, -1.7822, -1.7822],
         [-3.5098, -3.5098, -3.5098,  ..., -1.7822, -1.7822, -1.7822]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When the weather is very cold outside, what equipment do we need to use to increase the indoor temperature in the room? Please output segmentation mask. ASSISTANT: when the weather is very cold outside, what equipment do we need to use to increase the indoor temperature in the room</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-5.9633, -5.5127, -5.8320,  ..., -5.8484, -6.1220, -6.1464],
         [-4.4170, -5.1775, -5.1875,  ..., -5.2469, -5.4409, -5.6279],
         [-4.4789, -5.0375, -5.1484,  ..., -5.3203, -5.0719, -5.3082],
         ...,
         [-2.2326, -2.5656, -2.3926,  ..., -4.7656, -4.7203, -4.5807],
         [-2.9031, -3.1577, -2.8758,  ..., -4.8203, -4.6303, -4.9504],
         [-4.3162, -4.3151, -3.9750,  ..., -5.8731, -5.7478, -5.7032]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If we wanted to identify the specific vehicle shown in the picture, what part of the car should we look at? Please output segmentation mask. ASSISTANT: if we wanted to identify the specific vehicle shown in the picture, what part of the car should we look at</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[74]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 800, 1200])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 800, 1200])):  [tensor([[[-11.8125, -11.8125, -11.7086,  ..., -11.1646, -11.1250, -11.1250],
         [-11.8125, -11.8125, -11.7086,  ..., -11.1646, -11.1250, -11.1250],
         [-11.7481, -11.7481, -11.6606,  ..., -11.2097, -11.1646, -11.1646],
         ...,
         [ -9.7472,  -9.7472,  -9.7974,  ...,  -8.3653,  -8.3713,  -8.3713],
         [ -9.5802,  -9.5802,  -9.6253,  ...,  -8.0938,  -8.0955,  -8.0955],
         [ -9.7266,  -9.7266,  -9.7649,  ...,  -8.1081,  -8.1016,  -8.1016]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something hot and light in this image? Please output segmentation mask. ASSISTANT: something hot and light</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 616, 816])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 616, 816])):  [tensor([[[-8.0000, -7.9866, -7.9200,  ..., -6.5723, -6.7201, -6.7500],
         [-7.9851, -7.9733, -7.9149,  ..., -6.5891, -6.7317, -6.7605],
         [-7.9112, -7.9075, -7.8893,  ..., -6.6724, -6.7887, -6.8122],
         ...,
         [-5.2968, -5.3125, -5.3899,  ..., -6.6762, -6.6622, -6.6593],
         [-5.1958, -5.2072, -5.2636,  ..., -6.5450, -6.4567, -6.4389],
         [-5.4507, -5.4682, -5.5547,  ..., -6.5655, -6.4031, -6.3702]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is objects that can help women appear taller in this image? Please output segmentation mask. ASSISTANT: objects that can help women appear taller</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 465, 500])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 465, 500])):  [tensor([[[-10.2500, -10.9200, -12.2000,  ..., -10.6225, -11.1985, -11.5000],
         [-10.4508, -11.2015, -12.6358,  ..., -11.7186, -12.0804, -12.2698],
         [-10.8347, -11.7397, -13.4688,  ..., -13.8139, -13.7662, -13.7413],
         ...,
         [  1.0977,   1.3966,   1.9676,  ...,  -5.6971,  -5.0086,  -4.6482],
         [  1.1977,   1.3787,   1.7245,  ...,  -4.8007,  -4.2596,  -3.9764],
         [ -0.1306,  -0.1283,  -0.1238,  ...,  -3.9578,  -3.6617,  -3.5068]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What container in the picture is used for arranging the flowers to make them look more beautiful? Please output segmentation mask. ASSISTANT: what container in the picture is used for arranging the flowers to make them look more beautiful</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-5.3505, -4.6019, -4.3623,  ..., -5.9915, -6.1476, -5.4751],
         [-3.0550, -3.6727, -2.8033,  ..., -4.6648, -4.9831, -5.1378],
         [-2.0576, -2.6484, -1.8125,  ..., -4.3633, -4.3781, -4.4211],
         ...,
         [-6.0900, -6.7453, -7.5156,  ..., -7.1484, -6.8266, -6.2143],
         [-6.9486, -7.3847, -7.8078,  ..., -7.3500, -6.5972, -6.4618],
         [-7.5413, -8.1353, -8.0199,  ..., -7.7168, -7.1702, -7.0034]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the places where the driver can observe the speed in this image? Please output segmentation mask. ASSISTANT: the places where the driver can observe the speed</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 944, 1417])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 944, 1417])):  [tensor([[[-9.5000, -9.5000, -9.4545,  ..., -8.3798, -8.3750, -8.3750],
         [-9.5000, -9.5000, -9.4545,  ..., -8.3798, -8.3750, -8.3750],
         [-9.4593, -9.4593, -9.4180,  ..., -8.3907, -8.3870, -8.3870],
         ...,
         [-8.6856, -8.6856, -8.6961,  ..., -8.7855, -8.7762, -8.7762],
         [-8.9114, -8.9114, -8.9253,  ..., -8.6626, -8.6407, -8.6407],
         [-9.0938, -9.0938, -9.1105,  ..., -8.5633, -8.5312, -8.5312]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something indicating that someone is celerating the birthday in this image? Please output segmentation mask. ASSISTANT: something indicating that someone is celerating the birthday</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 651, 383])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 651, 383])):  [tensor([[[-11.2500, -11.2969, -11.4610,  ..., -11.6519, -11.4052, -11.1351],
         [-11.1426, -11.2276, -11.5253,  ..., -11.7546, -11.4973, -11.2061],
         [-10.7669, -10.9854, -11.7499,  ..., -12.1139, -11.8194, -11.4543],
         ...,
         [ -5.1791,  -5.1971,  -5.2602,  ...,  -2.5205,  -2.9446,  -3.4024],
         [ -4.9912,  -5.0004,  -5.0327,  ...,  -2.7521,  -2.9610,  -3.1535],
         [ -4.9375,  -4.9442,  -4.9676,  ...,  -2.8184,  -2.9656,  -3.0823]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that we use to control computer games in this image? Please output segmentation mask. ASSISTANT: something that we use to control computer games</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-10.3125, -10.2600, -10.0125,  ..., -12.1250, -12.4344, -12.5000],
         [-10.3978, -10.3479, -10.1126,  ..., -12.2077, -12.4976, -12.5591],
         [-10.8000, -10.7623, -10.5844,  ..., -12.5975, -12.7955, -12.8375],
         ...,
         [ -8.4719,  -8.4999,  -8.6322,  ...,  -8.1422,  -8.3085,  -8.3438],
         [ -8.5614,  -8.5957,  -8.7574,  ...,  -8.2985,  -8.4215,  -8.4476],
         [ -8.2731,  -8.2980,  -8.4153,  ...,  -9.1037,  -9.1006,  -9.1000]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When serving wine, it is common to pour it into a glass from a container. What object in the picture could be used to pour wine? Please output segmentation mask. ASSISTANT: when serving wine, it is common to pour it into a glass from a container. what object in the picture could be used to pour wine</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[80]]
>> (validate) sampled_classes_list:  [None]

         [1., 1., 1.,  ..., 1., 0., 0.],([1, 1600, 1066])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],..., -6.0312, -6.0312, -6.0312],2500],
         [1., 1., 1.,  ..., 1., 0., 0.],
         [1., 1., 1.,  ..., 1., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1193, 1271])):  [tensor([[[ 0.9219,  0.9219,  0.8915,  ..., -0.3222, -0.3555, -0.3555],
         [ 0.9219,  0.9219,  0.8915,  ..., -0.3222, -0.3555, -0.3555],
         [ 0.9951,  0.9951,  0.9719,  ..., -0.1585, -0.1911, -0.1911],
         ...,
         [-1.8929, -1.8929, -1.8923,  ...,  0.3856,  0.3723,  0.3723],
         [-3.5001, -3.5001, -3.5239,  ..., -0.7014, -0.7211, -0.7211],
         [-4.9134, -4.9134, -4.9585,  ..., -1.6573, -1.6826, -1.6826]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In historical architecture, a building often has a grand entrance that consists of a large opening with a curved or pointed top. What feature in the picture resembles such an entrance? Please output segmentation mask. ASSISTANT: in historical architecture, a building often has a grand entrance that consists of a large opening with a curved or pointed top. what feature in the picture resembles such an entrance</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 576])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 576])):  [tensor([[[-3.2344, -3.1611, -2.8438,  ..., -3.2135, -3.1670, -3.5208],
         [-3.2090, -3.1346, -2.8122,  ..., -3.2230, -3.1710, -3.5195],
         [-3.0990, -3.0195, -2.6753,  ..., -3.2639, -3.1882, -3.5139],
         ...,
         [-2.1094, -2.0058, -1.5569,  ..., -2.6406, -2.5986, -2.9549],
         [-1.9634, -1.8602, -1.4133,  ..., -2.6068, -2.5352, -2.8900],
         [-1.9297, -1.8267, -1.3802,  ..., -2.5990, -2.5205, -2.8750]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the people who are about to get on the vehicle in this image? Please output segmentation mask. ASSISTANT: the people who are about to get on the vehicle</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[67]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-13.6250, -13.4937, -12.8750,  ...,  -4.4531,  -4.5691,  -4.5938],
         [-13.5331, -13.4138, -12.8510,  ...,  -4.4507,  -4.5646,  -4.5888],
         [-13.1000, -13.0367, -12.7381,  ...,  -4.4391,  -4.5435,  -4.5656],
         ...,
         [ -5.3687,  -5.3683,  -5.3659,  ...,  -3.7086,  -3.7299,  -3.7344],
         [ -5.6330,  -5.6334,  -5.6353,  ...,  -4.1346,  -4.1148,  -4.1106],
         [ -6.5006,  -6.5028,  -6.5134,  ...,  -5.4308,  -5.3760,  -5.3644]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something indicating the identity of the bus in this image? Please output segmentation mask. ASSISTANT: something indicating the identity of the bus</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2048, 1536])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2048, 1536])):  [tensor([[[-1.2656, -1.2656, -1.2656,  ..., -6.8145, -6.7949, -6.7852],
         [-1.2656, -1.2656, -1.2656,  ..., -6.8145, -6.7949, -6.7852],
         [-1.2656, -1.2656, -1.2656,  ..., -6.8145, -6.7949, -6.7852],
         ...,
         [-4.6875, -4.6875, -4.6875,  ..., -5.2246, -5.1035, -5.0430],
         [-4.6875, -4.6875, -4.6875,  ..., -5.2246, -5.1035, -5.0430],
         [-4.6875, -4.6875, -4.6875,  ..., -5.2246, -5.1035, -5.0430]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the object that can be used by the owner to lead the dog in this image? Please output segmentation mask. ASSISTANT: the object that can be used by the owner to lead the dog</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 762, 1000])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 762, 1000])):  [tensor([[[-19.6250, -19.6008, -18.8725,  ..., -19.0774, -18.5185, -18.5000],
         [-19.6289, -19.6048, -18.8809,  ..., -19.0712, -18.5076, -18.4889],
         [-19.7473, -19.7277, -19.1391,  ..., -18.8821, -18.1741, -18.1506],
         ...,
         [-13.2567, -13.2652, -13.5231,  ..., -16.6655, -16.6067, -16.6047],
         [-12.7058, -12.7144, -12.9735,  ..., -15.6046, -15.5452, -15.5433],
         [-12.5640, -12.5757, -12.9292,  ..., -15.1448, -15.1524, -15.1527]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the rabbit on the woman's back in this image? Please output segmentation mask. ASSISTANT: the rabbit on the woman's back</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]

         [-0.9370, -1.1342, -0.7904,  ..., -3.0938, -3.3172, -3.1668],[[0., 0., 0.,  ..., 0., 0., 0.],..., -6.0312, -6.0312, -6.0312],2500],
         [-0.4330, -0.5594, -0.5736,  ..., -3.0352, -3.1453, -2.7264],
         ...,
         [-5.6980, -5.4187, -5.4844,  ..., -5.3125, -5.3766, -5.3443],
         [-5.0775, -5.0431, -5.1891,  ..., -5.6594, -5.6694, -5.9358],
         [-5.0140, -4.9428, -5.2365,  ..., -5.8652, -5.6518, -5.7605]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In urban areas, there are designated areas for bicycles to ride safely. What area in the picture would a cyclist use to navigate through the city? Please output segmentation mask. ASSISTANT: in urban areas, there are designated areas for bicycles to ride safely. what area in the picture would a cyclist use to navigate through the city</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[83]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[-6.5312, -6.5312, -6.5312,  ..., -8.8750, -8.8750, -8.8750],
         [-6.5312, -6.5312, -6.5312,  ..., -8.8750, -8.8750, -8.8750],
         [-6.5312, -6.5312, -6.5312,  ..., -8.8750, -8.8750, -8.8750],
         ...,
         [-3.2230, -3.2230, -3.2230,  ..., -4.6784, -4.6784, -4.6784],
         [-3.7041, -3.7041, -3.7041,  ..., -5.2383, -5.2383, -5.2383],
         [-3.7041, -3.7041, -3.7041,  ..., -5.2383, -5.2383, -5.2383]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a mechanical workshop, there are various machines and tools used for different purposes. What in the picture could be used to rotate or spin other parts or objects? Please output segmentation mask. ASSISTANT: in a mechanical workshop, there are various machines and tools used for different purposes. what in the picture could be used to rotate or spin other parts or objects</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1606, 1610])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1606, 1610])):  [tensor([[[-7.6250, -7.6250, -7.6151,  ..., -7.6562, -7.6562, -7.6562],
         [-7.6250, -7.6250, -7.6151,  ..., -7.6562, -7.6562, -7.6562],
         [-7.6173, -7.6173, -7.6077,  ..., -7.6656, -7.6657, -7.6657],
         ...,
         [-5.5572, -5.5572, -5.5728,  ..., -5.1251, -5.1271, -5.1271],
         [-5.4281, -5.4281, -5.4437,  ..., -4.9693, -4.9731, -4.9731],
         [-5.3359, -5.3359, -5.3516,  ..., -4.8581, -4.8633, -4.8633]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the boxes inside the refrigerator in this image? Please output segmentation mask. ASSISTANT: the boxes inside the refrigerator</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2048, 1536])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2048, 1536])):  [tensor([[[-3.7031, -3.7031, -3.7031,  ..., -4.8906, -4.8594, -4.8438],
         [-3.7031, -3.7031, -3.7031,  ..., -4.8906, -4.8594, -4.8438],
         [-3.7031, -3.7031, -3.7031,  ..., -4.8906, -4.8594, -4.8438],
         ...,
         [-4.4062, -4.4062, -4.4062,  ..., -3.6895, -3.7949, -3.8477],
         [-4.4062, -4.4062, -4.4062,  ..., -3.6895, -3.7949, -3.8477],
         [-4.4062, -4.4062, -4.4062,  ..., -3.6895, -3.7949, -3.8477]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the stronger Mario in this image? Please output segmentation mask. ASSISTANT: the stronger mario</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[60]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2448, 3264])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2448, 3264])):  [tensor([[[-8.8125, -8.8125, -8.8125,  ..., -6.8125, -6.8125, -6.8125],
         [-8.8125, -8.8125, -8.8125,  ..., -6.8125, -6.8125, -6.8125],
         [-8.8125, -8.8125, -8.8125,  ..., -6.8125, -6.8125, -6.8125],
         ...,
         [-8.0046, -8.0046, -8.0046,  ..., -7.7399, -7.7399, -7.7399],
         [-8.1445, -8.1445, -8.1445,  ..., -8.0664, -8.0664, -8.0664],
         [-8.1445, -8.1445, -8.1445,  ..., -8.0664, -8.0664, -8.0664]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is where to wash hands in this image? Please output segmentation mask. ASSISTANT: where to wash hands</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 5953, 4793])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 5953, 4793])):  [tensor([[[-16.0000, -16.0000, -16.0000,  ..., -15.3359, -15.3359, -15.3359],
         [-16.0000, -16.0000, -16.0000,  ..., -15.3359, -15.3359, -15.3359],
         [-16.0000, -16.0000, -16.0000,  ..., -15.3359, -15.3359, -15.3359],
         ...,
         [-10.1250, -10.1250, -10.1250,  ..., -14.4062, -14.4062, -14.4062],
         [-10.1250, -10.1250, -10.1250,  ..., -14.4062, -14.4062, -14.4062],
         [-10.1250, -10.1250, -10.1250,  ..., -14.4062, -14.4062, -14.4062]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the food that the bird likes to eat in this image? Please output segmentation mask. ASSISTANT: the food that the bird likes to eat</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[-10.6875, -10.6875, -10.6719,  ...,  -8.6812,  -8.6875,  -8.6875],
         [-10.6875, -10.6875, -10.6719,  ...,  -8.6812,  -8.6875,  -8.6875],
         [-10.6938, -10.6938, -10.6784,  ...,  -8.6931,  -8.6992,  -8.6992],
         ...,
         [ -9.7523,  -9.7523,  -9.7509,  ...,  -6.9335,  -6.9445,  -6.9445],
         [-10.1850, -10.1850, -10.1809,  ...,  -7.9502,  -7.9587,  -7.9587],
         [-10.5156, -10.5156, -10.5094,  ...,  -8.7356,  -8.7422,  -8.7422]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a large building, it is common to have designated areas for swimming. What area in the picture could be used for swimming? Please output segmentation mask. ASSISTANT: in a large building, it is common to have designated areas for swimming. what area in the picture could be used for swimming</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[79]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')] -3.1668],[[0., 0., 0.,  ..., 0., 0., 0.],..., -6.0312, -6.0312, -6.0312],2500],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 533, 800])):  [tensor([[[-3.8281, -3.7592, -3.4344,  ..., -3.4000, -3.0442, -2.9688],
         [-3.7135, -3.6475, -3.3363,  ..., -3.2694, -2.9433, -2.8742],
         [-3.1725, -3.1202, -2.8736,  ..., -2.6531, -2.4673, -2.4279],
         ...,
         [-8.8124, -8.8363, -8.9491,  ..., -9.1973, -9.3952, -9.4371],
         [-8.7124, -8.7012, -8.6482,  ..., -8.9534, -9.0226, -9.0373],
         [-8.6125, -8.5661, -8.3473,  ..., -8.7095, -8.6500, -8.6374]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n We are currently watching a game and it's halftime. Who are the cheerleaders who come out to liven up the atmosphere? Please output segmentation mask. ASSISTANT: we are currently watching a game and it's halftime. who are the cheerleaders who come out to liven up the atmosphere</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 272, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 272, 1024])):  [tensor([[[-9.0625, -9.0625, -8.8750,  ..., -7.3008, -7.2188, -7.2188],
         [-9.0625, -9.0625, -8.8750,  ..., -7.3008, -7.2188, -7.2188],
         [-8.8945, -8.8945, -8.7437,  ..., -7.4141, -7.3633, -7.3633],
         ...,
         [-6.0430, -6.0430, -6.0024,  ..., -4.5344, -4.5527, -4.5527],
         [-6.2930, -6.2930, -6.2456,  ..., -4.9326, -4.9531, -4.9531],
         [-6.9414, -6.9414, -6.8696,  ..., -5.5713, -5.5469, -5.5469]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In an educational setting, children often use different materials to learn about letters, numbers, and words. What object in the picture could be used as a visual aid for learning about letters and words? Please output segmentation mask. ASSISTANT: in an educational setting, children often use different materials to learn about letters, numbers, and words. what object in the picture could be used as a visual aid for learning about letters and words</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[90]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1274, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1274, 2048])):  [tensor([[[-8.4375, -8.4375, -8.4375,  ..., -8.2500, -8.2500, -8.2500],
         [-8.4375, -8.4375, -8.4375,  ..., -8.2500, -8.2500, -8.2500],
         [-8.4375, -8.4375, -8.4375,  ..., -8.2500, -8.2500, -8.2500],
         ...,
         [-4.9902, -4.9902, -4.9902,  ..., -4.3965, -4.3965, -4.3965],
         [-5.1660, -5.1660, -5.1660,  ..., -4.4473, -4.4473, -4.4473],
         [-5.2539, -5.2539, -5.2539,  ..., -4.4727, -4.4727, -4.4727]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is a car with a color that is closer to lipstick color in this image? Please output segmentation mask. ASSISTANT: a car with a color that is closer to lipstick color</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[ -9.1250,  -9.1250,  -9.0945,  ..., -12.6016, -12.6250, -12.6250],
         [ -9.1250,  -9.1250,  -9.0945,  ..., -12.6016, -12.6250, -12.6250],
         [ -9.1074,  -9.1074,  -9.0774,  ..., -12.6395, -12.6625, -12.6625],
         ...,
         [ -7.5765,  -7.5765,  -7.5907,  ...,  -8.4574,  -8.4609,  -8.4609],
         [ -8.3000,  -8.3000,  -8.3113,  ...,  -9.4533,  -9.4518,  -9.4518],
         [ -8.8750,  -8.8750,  -8.8841,  ..., -10.2514, -10.2461, -10.2461]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If you want to play table tennis indoors, what furniture in the picture should be used as the playing surface? Please output segmentation mask. ASSISTANT: if you want to play table tennis indoors, what furniture in the picture should be used as the playing surface</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         ...,
         [  0.,   0.,   0.,  ..., 255., 255., 255.],
         [  0.,   0.,   0.,  ..., 255., 255., 255.],
         [  0.,   0.,   0.,  ..., 255., 255., 255.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-8.8828, -8.9677, -8.6324,  ..., -5.7629, -6.0990, -6.5013],
         [-9.2255, -9.2513, -9.0406,  ..., -6.4719, -6.9025, -6.9798],
         [-9.0328, -9.1187, -8.8906,  ..., -6.0312, -6.4938, -6.6227],
         ...,
         [-6.5676, -6.8469, -6.5859,  ...,  2.2070,  2.1937,  2.1186],
         [-6.1987, -6.3845, -5.9219,  ...,  2.3516,  2.2730,  2.3686],
         [-4.0239, -4.2849, -4.2713,  ...,  1.3290,  1.3513,  1.3347]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a military airfield, what area is specifically designed for aircraft to take off and land? Please output segmentation mask. ASSISTANT: in a military airfield, what area is specifically designed for aircraft to take off and land</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2848, 4288])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2848, 4288])):  [tensor([[[-11.9375, -11.9375, -11.9375,  ..., -11.3750, -11.3750, -11.3750],
         [-11.9375, -11.9375, -11.9375,  ..., -11.3750, -11.3750, -11.3750],
         [-11.9375, -11.9375, -11.9375,  ..., -11.3750, -11.3750, -11.3750],
         ...,
         [ -5.2377,  -5.2377,  -5.2377,  ...,  -5.4437,  -5.4437,  -5.4437],
         [ -5.2930,  -5.2930,  -5.2930,  ...,  -5.5391,  -5.5391,  -5.5391],
         [ -5.2930,  -5.2930,  -5.2930,  ...,  -5.5391,  -5.5391,  -5.5391]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Generally speaking, dogs do not have horns on their heads, only a pair of ears. What part of the dog's head in this picture looks strange? Please output segmentation mask. ASSISTANT: generally speaking, dogs do not have horns on their heads, only a pair of ears. what part of the dog's head in this picture looks strange</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1533, 1039])):  [tensor([[[  0.,   0.,   0.,  ..., 255., 255.,   0.],
         [  0.,   0.,   0.,  ..., 255., 255.,   0.],
         [  0.,   0.,   0.,  ..., 255.,   0.,   0.],
         ...,
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1533, 1039])):  [tensor([[[-9.8125, -9.8125, -9.8152,  ..., -4.0254, -4.4402, -4.7520],
         [-9.8125, -9.8125, -9.8152,  ..., -4.0254, -4.4402, -4.7520],
         [-9.8006, -9.8006, -9.8045,  ..., -4.0637, -4.4791, -4.7913],
         ...,
         [-7.7034, -7.7034, -7.7534,  ..., -6.8424, -6.8786, -6.9058],
         [-7.6875, -7.6875, -7.7379,  ..., -6.8462, -6.8827, -6.9102],
         [-7.6875, -7.6875, -7.7379,  ..., -6.8462, -6.8827, -6.9102]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In an organized workspace, one might have a designated area to store important documents and files. What piece of furniture in the picture could be used for this purpose? Please output segmentation mask. ASSISTANT: in an organized workspace, one might have a designated area to store important documents and files. what piece of furniture in the picture could be used for this purpose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 750, 1000])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 750, 1000])):  [tensor([[[-7.8438, -7.8371, -7.6381,  ..., -5.6531, -5.7166, -5.7188],
         [-7.8415, -7.8349, -7.6364,  ..., -5.6529, -5.7161, -5.7182],
         [-7.7737, -7.7677, -7.5859,  ..., -5.6467, -5.6995, -5.7013],
         ...,
         [-6.6769, -6.6768, -6.6749,  ..., -5.4787, -5.4951, -5.4956],
         [-6.9090, -6.9088, -6.9036,  ..., -6.0869, -6.0946, -6.0949],
         [-7.5586, -7.5587, -7.5597,  ..., -7.3486, -7.3372, -7.3368]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that ensures the person to land safely in this image? Please output segmentation mask. ASSISTANT: something that ensures the person to land safely</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]

(PLUM.py) >> pred_masks (shape: torch.Size([1, 615, 461])):  [tensor([[[-11.8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-11.8203, -11.8270, -11.8496,  ..., -12.0019, -11.9823, -11.7009],
         [-11.8463, -11.9537, -12.3117,  ..., -12.0084, -11.9238, -11.5546],
         ...,
         [ -9.1606,  -9.3631, -10.0383,  ..., -10.1316, -10.3304, -10.0330],
         [ -8.7966,  -8.9634,  -9.5195,  ...,  -9.6801, -10.1062,  -9.8011],
         [ -8.6875,  -8.8436,  -9.3640,  ...,  -9.5448, -10.0390,  -9.7316]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is where the garbage should be put in this image? Please output segmentation mask. ASSISTANT: where the garbage should be put</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1944, 2592])):  [tensor([[[-3.7656, -3.7656, -3.7656,  ..., -3.2188, -3.2188, -3.2188],
         [-3.7656, -3.7656, -3.7656,  ..., -3.2188, -3.2188, -3.2188],
         [-3.7656, -3.7656, -3.7656,  ..., -3.2188, -3.2188, -3.2188],
         ...,
         [-2.6211, -2.6211, -2.6211,  ..., -0.9104, -0.9104, -0.9104],
         [-2.7477, -2.7477, -2.7477,  ..., -1.2776, -1.2776, -1.2776],
         [-2.7773, -2.7773, -2.7773,  ..., -1.3637, -1.3637, -1.3637]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Dogs are faithful companions to humans, and humans often play fetch games with them. What object will the dog likely retrieve and bring back to the human for the next round of fetch in the picture? Please output segmentation mask. ASSISTANT: dogs are faithful companions to humans, and humans often play fetch games with them. what object will the dog likely retrieve and bring back to the human for the next round of fetch in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[92]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1816, 3003])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1816, 3003])):  [tensor([[[-13.5000, -13.5000, -13.5000,  ..., -11.1250, -11.1250, -11.1250],
         [-13.5000, -13.5000, -13.5000,  ..., -11.1250, -11.1250, -11.1250],
         [-13.5000, -13.5000, -13.5000,  ..., -11.1250, -11.1250, -11.1250],
         ...,
         [ -9.8138,  -9.8138,  -9.8138,  ...,  -9.2336,  -9.2336,  -9.2336],
         [ -9.7899,  -9.7899,  -9.7899,  ...,  -9.1058,  -9.1058,  -9.1058],
         [ -9.7891,  -9.7891,  -9.7891,  ...,  -9.1016,  -9.1016,  -9.1016]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n People often add gas to water to improve its taste. What part of the picture will be consumed as a beverage? Please output segmentation mask. ASSISTANT: people often add gas to water to improve its taste. what part of the picture will be consumed as a beverage</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2331, 1749])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2331, 1749])):  [tensor([[[-10.3125, -10.3125, -10.3125,  ..., -12.0268, -11.7661, -11.6719],
         [-10.3125, -10.3125, -10.3125,  ..., -12.0268, -11.7661, -11.6719],
         [-10.3125, -10.3125, -10.3125,  ..., -12.0268, -11.7661, -11.6719],
         ...,
         [ -5.8750,  -5.8750,  -5.8750,  ...,  -7.3741,  -7.3432,  -7.3320],
         [ -5.8750,  -5.8750,  -5.8750,  ...,  -7.3741,  -7.3432,  -7.3320],
         [ -5.8750,  -5.8750,  -5.8750,  ...,  -7.3741,  -7.3432,  -7.3320]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When people want to take their pets to the park and enjoy some outdoor activities, they often want to make sure their pets are safe and cannot run away. What in the picture could help with this? Please output segmentation mask. ASSISTANT: when people want to take their pets to the park and enjoy some outdoor activities, they often want to make sure their pets are safe and cannot run away. what in the picture could help with this</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[94]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2705, 3269])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2705, 3269])):  [tensor([[[-7.7500, -7.7500, -7.7500,  ..., -6.0312, -6.0312, -6.0312],
         [-7.7500, -7.7500, -7.7500,  ..., -6.0312, -6.0312, -6.0312],
         [-7.7500, -7.7500, -7.7500,  ..., -6.0312, -6.0312, -6.0312],
         ...,
         [-5.8566, -5.8566, -5.8566,  ..., -4.7421, -4.7421, -4.7421],
         [-6.0234, -6.0234, -6.0234,  ..., -4.7852, -4.7852, -4.7852],
         [-6.0234, -6.0234, -6.0234,  ..., -4.7852, -4.7852, -4.7852]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n I feel my commute is too slow now and I'm hoping to find a convenient mode of transportation that can also help me exercise. Can you help me find the corresponding part in the picture? Please output segmentation mask. ASSISTANT: i feel my commute is too slow now and i'm hoping to find a convenient mode of transportation that can also help me exercise. can you help me find the corresponding part in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[92]]
>> (validate) sampled_classes_list:  [None]

         [-10.0625, -10.0625, -10.0625,  ..., -10.3750, -10.3750, -10.3750],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-10.0625, -10.0625, -10.0625,  ..., -10.3750, -10.3750, -10.3750],
         ...,
         [ -7.0901,  -7.0901,  -7.0901,  ...,  -7.7087,  -7.7087,  -7.7087],
         [ -7.6416,  -7.6416,  -7.6416,  ...,  -8.6315,  -8.6315,  -8.6315],
         [ -7.8828,  -7.8828,  -7.8828,  ...,  -9.0352,  -9.0352,  -9.0352]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In ancient times, people used different methods to measure time during the day. What object in the picture could have been used as a timekeeping device based on the position of the sun? Please output segmentation mask. ASSISTANT: in ancient times, people used different methods to measure time during the day. what object in the picture could have been used as a timekeeping device based on the position of the sun</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[89]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.],
         [1., 1., 1.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-13.9375, -13.7767, -13.0188,  ..., -12.0000, -11.8453, -11.8125],
         [-13.8522, -13.7071, -13.0230,  ..., -11.9754, -11.8573, -11.8322],
         [-13.4500, -13.3788, -13.0431,  ..., -11.8594, -11.9135, -11.9250],
         ...,
         [-10.5438, -10.6612, -11.2150,  ..., -12.4544, -12.8581, -12.9438],
         [-10.3897, -10.5215, -11.1428,  ..., -12.3201, -12.7672, -12.8620],
         [-11.3812, -11.4416, -11.7262,  ..., -13.2717, -13.4394, -13.4750]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a busy and bustling city, it is important for businesses to have eye-catching signs to attract customers. What feature in the picture can serve this purpose? Please output segmentation mask. ASSISTANT: in a busy and bustling city, it is important for businesses to have eye-catching signs to attract customers. what feature in the picture can serve this purpose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2592, 1944])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2592, 1944])):  [tensor([[[-15.7500, -15.7500, -15.7500,  ..., -14.9383, -14.9630, -14.9688],
         [-15.7500, -15.7500, -15.7500,  ..., -14.9383, -14.9630, -14.9688],
         [-15.7500, -15.7500, -15.7500,  ..., -14.9383, -14.9630, -14.9688],
         ...,
         [-14.0000, -14.0000, -14.0000,  ..., -15.0621, -14.5498, -14.4297],
         [-14.0000, -14.0000, -14.0000,  ..., -15.0621, -14.5498, -14.4297],
         [-14.0000, -14.0000, -14.0000,  ..., -15.0621, -14.5498, -14.4297]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When taking pictures with a camera, what part of the camera is responsible for focusing the image and adjusting the depth of field? Please output segmentation mask. ASSISTANT: when taking pictures with a camera, what part of the camera is responsible for focusing the image and adjusting the depth of field</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1664, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1664, 2048])):  [tensor([[[-11.4375, -11.4375, -11.4375,  ..., -12.8750, -12.8750, -12.8750],
         [-11.4375, -11.4375, -11.4375,  ..., -12.8750, -12.8750, -12.8750],
         [-11.4375, -11.4375, -11.4375,  ..., -12.8750, -12.8750, -12.8750],
         ...,
         [-12.6133, -12.6133, -12.6133,  ..., -11.6055, -11.6055, -11.6055],
         [-12.9805, -12.9805, -12.9805,  ..., -12.1758, -12.1758, -12.1758],
         [-13.1641, -13.1641, -13.1641,  ..., -12.4609, -12.4609, -12.4609]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Birds often need a place to rest or observe their surroundings. What part of a tree in the picture offers a suitable spot for birds to do so? Please output segmentation mask. ASSISTANT: birds often need a place to rest or observe their surroundings. what part of a tree in the picture offers a suitable spot for birds to do so</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 612, 612])):  [tensor([[[  0.,   0.,   0.,  ..., 255., 255., 255.],
         [  0.,   0.,   0.,  ..., 255., 255., 255.],
         [  0.,   0.,   0.,  ..., 255., 255., 255.],
         ...,
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 612, 612])):  [tensor([[[-11.0625, -11.1581, -11.4718,  ...,  -9.6761,  -9.7806,  -9.8125],
         [-11.0067, -11.1064, -11.4334,  ...,  -9.6834,  -9.7213,  -9.7328],
         [-10.8237, -10.9367, -11.3075,  ...,  -9.7073,  -9.5265,  -9.4714],
         ...,
         [ -5.6422,  -5.7794,  -6.2298,  ...,  -5.6509,  -5.8054,  -5.8524],
         [ -5.4853,  -5.6242,  -6.0800,  ...,  -5.3217,  -5.5582,  -5.6302],
         [ -5.4375,  -5.5769,  -6.0344,  ...,  -5.2214,  -5.4828,  -5.5625]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that we can sit on in this image? Please output segmentation mask. ASSISTANT: something that we can sit on</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 540, 720])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 540, 720])):  [tensor([[[-3.5312, -3.5251, -3.5009,  ..., -2.9844, -2.9844, -2.9844],
         [-3.4830, -3.4805, -3.4709,  ..., -2.9451, -2.9547, -2.9572],
         [-3.2943, -3.3064, -3.3537,  ..., -2.7916, -2.8387, -2.8507],
         ...,
         [-2.5000, -2.5210, -2.6030,  ..., -2.7302, -2.5310, -2.4800],
         [-2.5091, -2.5364, -2.6432,  ..., -2.9213, -2.8090, -2.7804],
         [-3.5701, -3.6069, -3.7508,  ..., -4.3856, -4.3046, -4.2838]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n To keep bread fresh and protected, it is often placed in a protective covering. What item in the picture is commonly used for this purpose? Please output segmentation mask. ASSISTANT: to keep bread fresh and protected, it is often placed in a protective covering. what item in the picture is commonly used for this purpose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[80]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 612, 612])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 612, 612])):  [tensor([[[-7.5625, -7.5506, -7.5113,  ..., -7.5396, -7.7488, -7.8125],
         [-7.4988, -7.5076, -7.5367,  ..., -7.6722, -7.8713, -7.9320],
         [-7.2896, -7.3668, -7.6201,  ..., -8.1071, -8.2735, -8.3241],
         ...,
         [-6.6110, -6.8134, -7.4776,  ..., -5.8993, -6.0941, -6.1534],
         [-6.3103, -6.5377, -7.2840,  ..., -5.6758, -6.0619, -6.1795],
         [-6.2188, -6.4537, -7.2250,  ..., -5.6076, -6.0521, -6.1875]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Sometimes, when a car is parked on the side of the road, the driver may accidentally leave the car door open. What part in the picture can indicate that the car door is open? Please output segmentation mask. ASSISTANT: sometimes, when a car is parked on the side of the road, the driver may accidentally leave the car door open. what part in the picture can indicate that the car door is open</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[91]]
>> (validate) sampled_classes_list:  [None]

         [-12.8265, -12.9650, -13.3422,  ..., -13.0679, -12.4862, -12.2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         ...,
         [ -4.2112,  -4.2516,  -4.3617,  ...,  -3.4632,  -3.2885,  -3.9113],
         [ -4.3767,  -4.4283,  -4.5689,  ...,  -3.3851,  -3.3231,  -3.8774],
         [ -4.4375,  -4.4932,  -4.6449,  ...,  -3.3564,  -3.3357,  -3.8649]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Dogs have keen sense of smell, which is why they can be used as drug-sniffing dogs. Which part in the picture gives dogs this characteristic? Please output segmentation mask. ASSISTANT: dogs have keen sense of smell, which is why they can be used as drug-sniffing dogs. which part in the picture gives dogs this characteristic</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[-12.8750, -12.8750, -12.8750,  ...,  -9.4375,  -9.4375,  -9.4375],
         [-12.8750, -12.8750, -12.8750,  ...,  -9.4375,  -9.4375,  -9.4375],
         [-12.8750, -12.8750, -12.8750,  ...,  -9.4375,  -9.4375,  -9.4375],
         ...,
         [-11.2557, -11.2557, -11.2557,  ..., -10.2302, -10.2302, -10.2302],
         [-11.2909, -11.2909, -11.2909,  ..., -10.8006, -10.8006, -10.8006],
         [-11.3047, -11.3047, -11.3047,  ..., -11.0234, -11.0234, -11.0234]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Some birds have long bills that they use to catch food from the water. What part of the bird's body in the picture may have this characteristic? Please output segmentation mask. ASSISTANT: some birds have long bills that they use to catch food from the water. what part of the bird's body in the picture may have this characteristic</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[83]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 800, 1200])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 800, 1200])):  [tensor([[[-17.5000, -17.5000, -17.4010,  ..., -17.3651, -17.3750, -17.3750],
         [-17.5000, -17.5000, -17.4010,  ..., -17.3651, -17.3750, -17.3750],
         [-17.4108, -17.4108, -17.3267,  ..., -17.4606, -17.4642, -17.4642],
         ...,
         [-14.0112, -14.0112, -14.1188,  ..., -13.4918, -13.4478, -13.4478],
         [-13.5156, -13.5156, -13.6294,  ..., -12.9136, -12.8537, -12.8537],
         [-13.5156, -13.5156, -13.6294,  ..., -12.8162, -12.7500, -12.7500]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n After cooking, consuming food, and preparing for food, where can we throw away the rest of the food and scraps? Please output segmentation mask. ASSISTANT: after cooking, consuming food, and preparing for food, where can we throw away the rest of the food and scraps</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[79]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-2.9791, -3.0967, -3.0721,  ..., -1.7331, -1.9183, -1.9739],
         [-2.5811, -3.2600, -3.1688,  ..., -1.4457, -1.7217, -1.8807],
         [-2.3627, -2.9750, -3.2656,  ..., -0.9980, -1.2629, -1.3327],
         ...,
         [-2.2639, -2.5969, -2.7930,  ..., -3.4141, -3.4828, -3.4613],
         [-2.4969, -2.6720, -2.8063,  ..., -3.0828, -3.1152, -3.2430],
         [-3.4900, -3.3879, -3.4766,  ..., -3.8532, -3.9506, -4.0644]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the person who is most likely to be the girl's trainer in this image? Please output segmentation mask. ASSISTANT: the person who is most likely to be the girl's trainer</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[71]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 3456, 2304])):  [tensor([[[0., 0., 0.,  ..., 1., 1., 0.],
         [0., 0., 0.,  ..., 1., 1., 0.],
         [0., 0., 0.,  ..., 1., 1., 0.],
         ...,
         [0., 0., 0.,  ..., 1., 1., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 3456, 2304])):  [tensor([[[-6.5625, -6.5625, -6.5625,  ..., -1.1274, -1.1914, -1.1914],
         [-6.5625, -6.5625, -6.5625,  ..., -1.1274, -1.1914, -1.1914],
         [-6.5625, -6.5625, -6.5625,  ..., -1.1274, -1.1914, -1.1914],
         ...,
         [-8.1250, -8.1250, -8.1250,  ..., -3.9704, -3.9883, -3.9883],
         [-8.1250, -8.1250, -8.1250,  ..., -3.9704, -3.9883, -3.9883],
         [-8.1250, -8.1250, -8.1250,  ..., -3.9704, -3.9883, -3.9883]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n At a car show, visitors can get close to the displayed vehicles to admire their design and features. What part of the car in this picture is open, allowing viewers to see the engine compartment? Please output segmentation mask. ASSISTANT: at a car show, visitors can get close to the displayed vehicles to admire their design and features. what part of the car in this picture is open, allowing viewers to see the engine compartment</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[93]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2356, 3141])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2356, 3141])):  [tensor([[[-11.6250, -11.6250, -11.6250,  ..., -10.4375, -10.4375, -10.4375],
         [-11.6250, -11.6250, -11.6250,  ..., -10.4375, -10.4375, -10.4375],
         [-11.6250, -11.6250, -11.6250,  ..., -10.4375, -10.4375, -10.4375],
         ...,
         [-10.9721, -10.9721, -10.9721,  ..., -11.4952, -11.4952, -11.4952],
         [-11.1641, -11.1641, -11.1641,  ..., -11.8594, -11.8594, -11.8594],
         [-11.1641, -11.1641, -11.1641,  ..., -11.8594, -11.8594, -11.8594]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When snorkeling or scuba diving, it is important to wear certain equipment to protect the eyes and aid in vision. What object in the picture can be used for this purpose? Please output segmentation mask. ASSISTANT: when snorkeling or scuba diving, it is important to wear certain equipment to protect the eyes and aid in vision. what object in the picture can be used for this purpose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[89]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 1024])):  [tensor([[[-16.0000, -16.0000, -15.7734,  ..., -14.0547, -13.8750, -13.8750],
         [-16.0000, -16.0000, -15.7734,  ..., -14.0547, -13.8750, -13.8750],
         [-15.7109, -15.7109, -15.5508,  ..., -14.0576, -13.9141, -13.9141],
         ...,
         [-13.9375, -13.9375, -14.0859,  ..., -12.8408, -12.9141, -12.9141],
         [-14.0859, -14.0859, -14.2051,  ..., -13.5547, -13.6250, -13.6250],
         [-14.6328, -14.6328, -14.6621,  ..., -15.1016, -15.1250, -15.1250]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the places for further exploration in this image? Please output segmentation mask. ASSISTANT: the places for further exploration</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]

         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         ...,
         [-5.3750, -5.3750, -5.3750,  ..., -6.0052, -5.9969, -5.9961],
         [-5.3750, -5.3750, -5.3750,  ..., -6.0052, -5.9969, -5.9961],
         [-5.3750, -5.3750, -5.3750,  ..., -6.0052, -5.9969, -5.9961]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In some rural areas, horse-drawn carts are still used for transportation and carrying goods. What is the main source of power that drives the cart in the picture? Please output segmentation mask. ASSISTANT: in some rural areas, horse-drawn carts are still used for transportation and carrying goods. what is the main source of power that drives the cart in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2304, 3456])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2304, 3456])):  [tensor([[[-7.5000, -7.5000, -7.5000,  ..., -8.1250, -8.1250, -8.1250],
         [-7.5000, -7.5000, -7.5000,  ..., -8.1250, -8.1250, -8.1250],
         [-7.5000, -7.5000, -7.5000,  ..., -8.1250, -8.1250, -8.1250],
         ...,
         [-4.3963, -4.3963, -4.3963,  ..., -4.6998, -4.6998, -4.6998],
         [-4.4844, -4.4844, -4.4844,  ..., -4.7422, -4.7422, -4.7422],
         [-4.4844, -4.4844, -4.4844,  ..., -4.7422, -4.7422, -4.7422]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In case of a fire, it is important to have access to fire safety equipment. What object in the picture is specifically designed to store and release fire extinguishing substances? Please output segmentation mask. ASSISTANT: in case of a fire, it is important to have access to fire safety equipment. what object in the picture is specifically designed to store and release fire extinguishing substances</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[88]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 509, 763])):  [tensor([[[-6.3125, -6.3105, -6.3019,  ..., -7.6718, -7.9379, -8.0000],
         [-6.3966, -6.3963, -6.3949,  ..., -7.8559, -8.0961, -8.1522],
         [-6.7570, -6.7638, -6.7930,  ..., -8.6446, -8.7741, -8.8043],
         ...,
         [-6.4156, -6.4625, -6.6635,  ..., -7.6174, -7.6077, -7.6055],
         [-6.3212, -6.3574, -6.5123,  ..., -7.0159, -7.0265, -7.0289],
         [-6.3532, -6.3838, -6.5149,  ..., -6.8058, -6.8081, -6.8086]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the solitary tree in this image? Please output segmentation mask. ASSISTANT: the solitary tree</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[ -9.3125,  -9.3125,  -9.3125,  ..., -10.3125, -10.3125, -10.3125],
         [ -9.3125,  -9.3125,  -9.3125,  ..., -10.3125, -10.3125, -10.3125],
         [ -9.3125,  -9.3125,  -9.3125,  ..., -10.3125, -10.3125, -10.3125],
         ...,
         [ -7.8848,  -7.8848,  -7.8848,  ...,  -9.9180,  -9.9180,  -9.9180],
         [ -8.5371,  -8.5371,  -8.5371,  ..., -10.7383, -10.7383, -10.7383],
         [ -8.8633,  -8.8633,  -8.8633,  ..., -11.1484, -11.1484, -11.1484]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something used for contacting other people in this image? Please output segmentation mask. ASSISTANT: something used for contacting other people</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[-6.7188, -6.7188, -6.7188,  ..., -8.8125, -8.8125, -8.8125],
         [-6.7188, -6.7188, -6.7188,  ..., -8.8125, -8.8125, -8.8125],
         [-6.7188, -6.7188, -6.7188,  ..., -8.8125, -8.8125, -8.8125],
         ...,
         [-7.5163, -7.5163, -7.5163,  ..., -8.3631, -8.3631, -8.3631],
         [-7.8472, -7.8472, -7.8472,  ..., -8.9053, -8.9053, -8.9053],
         [-7.9766, -7.9766, -7.9766,  ..., -9.1172, -9.1172, -9.1172]]],
       device='cuda:0')]
giou: 0.3847, ciou: 0.3312 | BIO per cls acc: O=1.0000, B=0.2550, I=0.7819

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:34<00:00,  5.79it/s]
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 5 is less than current step: 2999. Dropping entry: {'val/giou': 0.38468438386917114, 'val/ciou': 0.3312070369720459, 'val/b_acc': 0.2549999872500006, 'val/i_acc': 0.7818980220007368, 'val/o_acc': 0.999999999849284, '_timestamp': 1743885675.1875403}).
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-2.5469, -2.5469, -2.5469,  ..., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
  0%|                                                                                                                                                         | 0/200 [00:00<?, ?it/s]

         [0., 0., 0.,  ..., 0., 0., 0.],., -4.9116, -4.9353, -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1067, 1600])):  [tensor([[[-15.8750, -15.8750, -15.8617,  ..., -15.3773, -15.3750, -15.3750],
         [-15.8750, -15.8750, -15.8617,  ..., -15.3773, -15.3750, -15.3750],
         [-15.8695, -15.8695, -15.8567,  ..., -15.3844, -15.3821, -15.3821],
         ...,
         [-15.0157, -15.0157, -15.0305,  ..., -14.5857, -14.5657, -14.5657],
         [-15.0723, -15.0723, -15.0868,  ..., -14.5225, -14.5023, -14.5023],
         [-15.1406, -15.1406, -15.1547,  ..., -14.5043, -14.4844, -14.4844]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Birds have various ways of searching for food. What part of their body helps them to grab and pick up food from the ground in the picture? Please output segmentation mask. ASSISTANT: birds have various ways of searching for food. what part of their body helps them to grab and pick up food from the ground in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2570, 2229])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2570, 2229])):  [tensor([[[-17.7500, -17.7500, -17.7500,  ..., -16.3148, -16.5389, -16.5938],
         [-17.7500, -17.7500, -17.7500,  ..., -16.3148, -16.5389, -16.5938],
         [-17.7500, -17.7500, -17.7500,  ..., -16.3148, -16.5389, -16.5938],
         ...,
         [-12.8125, -12.8125, -12.8125,  ..., -18.5881, -18.0279, -17.8906],
         [-12.8125, -12.8125, -12.8125,  ..., -18.5881, -18.0279, -17.8906],
         [-12.8125, -12.8125, -12.8125,  ..., -18.5881, -18.0279, -17.8906]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the sour food in this image? Please output segmentation mask. ASSISTANT: the sour food</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')] -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1064, 1600])):  [tensor([[[-11.2500, -11.2500, -11.2422,  ..., -11.5758, -11.5625, -11.5625],
         [-11.2500, -11.2500, -11.2422,  ..., -11.5758, -11.5625, -11.5625],
         [-11.2359, -11.2359, -11.2285,  ..., -11.5920, -11.5789, -11.5789],
         ...,
         [-12.0750, -12.0750, -12.0831,  ..., -10.3171, -10.3157, -10.3157],
         [-11.9950, -11.9950, -12.0036,  ..., -10.1489, -10.1456, -10.1456],
         [-11.9375, -11.9375, -11.9465,  ..., -10.0279, -10.0234, -10.0234]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the stacked cookies in this image? Please output segmentation mask. ASSISTANT: the stacked cookies</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[61]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1402, 2102])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1402, 2102])):  [tensor([[[-10.1875, -10.1875, -10.1875,  ...,  -8.3125,  -8.3125,  -8.3125],
         [-10.1875, -10.1875, -10.1875,  ...,  -8.3125,  -8.3125,  -8.3125],
         [-10.1875, -10.1875, -10.1875,  ...,  -8.3125,  -8.3125,  -8.3125],
         ...,
         [ -5.5409,  -5.5409,  -5.5409,  ...,  -5.3422,  -5.3422,  -5.3422],
         [ -5.6722,  -5.6722,  -5.6722,  ...,  -5.3194,  -5.3194,  -5.3194],
         [ -5.7344,  -5.7344,  -5.7344,  ...,  -5.3086,  -5.3086,  -5.3086]]],
       device='cuda:0')]

         [0., 0., 0.,  ..., 0., 0., 0.],], device='cuda:0')] -4.9375],2272],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 679, 1024])):  [tensor([[[-18.2500, -18.2500, -17.8906,  ..., -10.6094, -10.6875, -10.6875],
         [-18.2500, -18.2500, -17.8906,  ..., -10.6094, -10.6875, -10.6875],
         [-17.8203, -17.8203, -17.5420,  ..., -10.6836, -10.7266, -10.7266],
         ...,
         [ -9.2266,  -9.2266,  -9.2598,  ..., -10.5850, -10.5078, -10.5078],
         [ -9.1172,  -9.1172,  -9.2012,  ..., -10.1846, -10.0859, -10.0859],
         [-10.1172, -10.1172, -10.2383,  ..., -10.8516, -10.7188, -10.7188]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the object used to protect the ears in this image? Please output segmentation mask. ASSISTANT: the object used to protect the ears</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2112, 2816])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2112, 2816])):  [tensor([[[-22.8750, -22.8750, -22.8750,  ..., -22.6250, -22.6250, -22.6250],
         [-22.8750, -22.8750, -22.8750,  ..., -22.6250, -22.6250, -22.6250],
         [-22.8750, -22.8750, -22.8750,  ..., -22.6250, -22.6250, -22.6250],
         ...,
         [-18.0455, -18.0455, -18.0455,  ..., -17.7273, -17.7273, -17.7273],
         [-18.3523, -18.3523, -18.3523,  ..., -17.7614, -17.7614, -17.7614],
         [-18.3906, -18.3906, -18.3906,  ..., -17.7656, -17.7656, -17.7656]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the place where piano players should sit in this image? Please output segmentation mask. ASSISTANT: the place where piano players should sit</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]

         [-17.2500, -17.2500, -17.2500,  ..., -17.0000, -17.0000, -17.0000],8125, -11.7891, -11.7110,  ..., -12.0000, -11.9999, -11.7447],],
         [-17.2500, -17.2500, -17.2500,  ..., -17.0000, -17.0000, -17.0000],
         ...,
         [-15.3776, -15.3776, -15.3776,  ..., -16.4636, -16.4636, -16.4636],
         [-15.7891, -15.7891, -15.7891,  ..., -17.2031, -17.2031, -17.2031],
         [-15.7891, -15.7891, -15.7891,  ..., -17.2031, -17.2031, -17.2031]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When we need to access or store things above our reach, what would be helpful to stand on? Please output segmentation mask. ASSISTANT: when we need to access or store things above our reach, what would be helpful to stand on</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[71]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-12.6160, -12.0332, -12.3844,  ...,  -9.3426,  -9.9373, -10.3610],
         [-12.7207, -13.1719, -12.9000,  ...,  -9.2781,  -9.9244, -10.0232],
         [-12.9785, -13.5156, -13.2500,  ...,  -8.7969,  -9.0344,  -9.3996],
         ...,
         [-14.6453, -15.7625, -15.5000,  ...,  -8.2109,  -8.5125,  -8.8992],
         [-15.0633, -16.4812, -15.7469,  ...,  -7.8953,  -8.4663,  -8.8186],
         [-13.7549, -15.2266, -14.4441,  ...,  -9.3201,  -9.6520, -10.0306]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What structure can be used to drain the water from the bathroom floor and prevent the accumulation of wastewater? Please output segmentation mask. ASSISTANT: what structure can be used to drain the water from the bathroom floor and prevent the accumulation of wastewater</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[75]]
>> (validate) sampled_classes_list:  [None]

(PLUM.py) >> pred_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[-10.7500, -10.6234, -10.1875,  ..., -10.6250, -10.1406, -10.0000],],
         [-10.5813, -10.4903, -10.1770,  ..., -10.6039, -10.2013, -10.0844],
         [-10.0000, -10.0316, -10.1406,  ..., -10.5312, -10.4102, -10.3750],
         ...,
         [ -9.7188,  -9.7715,  -9.9531,  ...,  -8.4844,  -8.4480,  -8.4375],
         [ -9.4805,  -9.5764,  -9.9066,  ...,  -8.6188,  -8.7399,  -8.7750],
         [-10.0500, -10.0866, -10.2125,  ...,  -9.7844,  -9.9031,  -9.9375]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that allow pedestrians to cross the canyon in this image? Please output segmentation mask. ASSISTANT: something that allow pedestrians to cross the canyon</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]

         [-15.3125, -15.3125, -15.3125,  ..., -15.1875, -15.1875, -15.1875],7500, -10.6234, -10.1875,  ..., -10.6250, -10.1406, -10.0000],],
         [-15.3125, -15.3125, -15.3125,  ..., -15.1875, -15.1875, -15.1875],
         ...,
         [-12.0927, -12.0927, -12.0927,  ..., -11.1685, -11.1685, -11.1685],
         [-12.1250, -12.1250, -12.1250,  ..., -11.1797, -11.1797, -11.1797],
         [-12.1250, -12.1250, -12.1250,  ..., -11.1797, -11.1797, -11.1797]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is The fastest sinking ship in this image? Please output segmentation mask. ASSISTANT: the fastest sinking ship</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 333, 500])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 333, 500])):  [tensor([[[-12.3125, -12.1785, -11.9225,  ..., -12.5413, -12.6373, -12.6875],
         [-12.4298, -12.3541, -12.2096,  ..., -12.8578, -12.9110, -12.9388],
         [-12.6538, -12.6896, -12.7581,  ..., -13.4627, -13.4339, -13.4188],
         ...,
         [ -9.1575,  -9.1313,  -9.0814,  ..., -10.3510, -10.1961, -10.1150],
         [ -9.5825,  -9.6435,  -9.7599,  ..., -10.1931, -10.0503,  -9.9755],
         [ -9.9025, -10.1350, -10.5791,  ..., -10.0510,  -9.8754,  -9.7835]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In some regions, people raise certain animals for their milk, meat, and skin. What animal in the picture could be domesticated for such purposes? Please output segmentation mask. ASSISTANT: in some regions, people raise certain animals for their milk, meat, and skin. what animal in the picture could be domesticated for such purposes</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]

         ...,.3125, -15.3125, -15.3125,  ..., -15.1875, -15.1875, -15.1875],7500, -10.6234, -10.1875,  ..., -10.6250, -10.1406, -10.0000],],
         [ -8.5082,  -8.5082,  -8.5082,  ..., -12.3049, -12.3049, -12.3049],
         [ -8.6758,  -8.6758,  -8.6758,  ..., -12.5078, -12.5078, -12.5078],
         [ -8.6758,  -8.6758,  -8.6758,  ..., -12.5078, -12.5078, -12.5078]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the objects that can protect the snail and prevent it from getting injured in this image? Please output segmentation mask. ASSISTANT: the objects that can protect the snail and prevent it from getting injured</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[71]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1350, 1800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1350, 1800])):  [tensor([[[-3.9844, -3.9844, -3.9844,  ..., -6.5625, -6.5625, -6.5625],
         [-3.9844, -3.9844, -3.9844,  ..., -6.5625, -6.5625, -6.5625],
         [-3.9844, -3.9844, -3.9844,  ..., -6.5625, -6.5625, -6.5625],
         ...,
         [-4.7854, -4.7854, -4.7854,  ..., -3.5386, -3.5386, -3.5386],
         [-5.1588, -5.1588, -5.1588,  ..., -4.1919, -4.1919, -4.1919],
         [-5.3906, -5.3906, -5.3906,  ..., -4.5977, -4.5977, -4.5977]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If a person wants to watch TV or a movie, which furniture is the most suitable for them to sit and watch? Please output segmentation mask. ASSISTANT: if a person wants to watch tv or a movie, which furniture is the most suitable for them to sit and watch</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]

 22%|██████████████████████████████▉                                                                                                                 | 43/200 [00:15<00:53,  2.94it/s]
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[-10.1250, -10.1250, -10.1227,  ...,   1.6713,   1.6797,   1.6797],
         [-10.1250, -10.1250, -10.1227,  ...,   1.6713,   1.6797,   1.6797],
         [-10.1281, -10.1281, -10.1259,  ...,   1.6972,   1.7054,   1.7054],
         ...,
         [-13.4625, -13.4625, -13.4820,  ...,  -9.5805,  -9.5914,  -9.5914],
         [-13.8912, -13.8912, -13.9038,  ..., -10.5235, -10.5325, -10.5325],
         [-14.2578, -14.2578, -14.2649,  ..., -11.2584, -11.2656, -11.2656]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the damaged part on the pants in this image? Please output segmentation mask. ASSISTANT: the damaged part on the pants</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2304, 3072])):  [tensor([[[-8.0000, -8.0000, -8.0000,  ..., -6.0938, -6.0938, -6.0938],
         [-8.0000, -8.0000, -8.0000,  ..., -6.0938, -6.0938, -6.0938],
         [-8.0000, -8.0000, -8.0000,  ..., -6.0938, -6.0938, -6.0938],
         ...,
         [-6.2774, -6.2774, -6.2774,  ..., -5.0267, -5.0267, -5.0267],
         [-6.4727, -6.4727, -6.4727,  ..., -5.4941, -5.4941, -5.4941],
         [-6.4727, -6.4727, -6.4727,  ..., -5.4941, -5.4941, -5.4941]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Seafood dishes often include a tangy condiment that enhances the flavor. What item in the picture can be squeezed onto the seafood as a tangy flavor enhancer? Please output segmentation mask. ASSISTANT: seafood dishes often include a tangy condiment that enhances the flavor. what item in the picture can be squeezed onto the seafood as a tangy flavor enhancer</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[96]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]1875, -15.1875],7500, -10.6234, -10.1875,  ..., -10.6250, -10.1406, -10.0000],],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2848, 4288])):  [tensor([[[-11.8750, -11.8750, -11.8750,  ...,  -4.8438,  -4.8438,  -4.8438],
         [-11.8750, -11.8750, -11.8750,  ...,  -4.8438,  -4.8438,  -4.8438],
         [-11.8750, -11.8750, -11.8750,  ...,  -4.8438,  -4.8438,  -4.8438],
         ...,
         [-13.6652, -13.6652, -13.6652,  ..., -10.3560, -10.3560, -10.3560],
         [-13.7500, -13.7500, -13.7500,  ..., -10.4453, -10.4453, -10.4453],
         [-13.7500, -13.7500, -13.7500,  ..., -10.4453, -10.4453, -10.4453]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a modern office, what object in the picture is commonly used for inputting data and controlling the computer? Please output segmentation mask. ASSISTANT: in a modern office, what object in the picture is commonly used for inputting data and controlling the computer</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[74]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1024, 1280])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1024, 1280])):  [tensor([[[-15.8750, -15.8750, -15.7852,  ..., -15.3555, -15.3750, -15.3750],
         [-15.8750, -15.8750, -15.7852,  ..., -15.3555, -15.3750, -15.3750],
         [-15.8360, -15.8360, -15.7571,  ..., -15.3123, -15.3321, -15.3321],
         ...,
         [-10.9685, -10.9685, -10.9978,  ..., -13.2653, -13.2809, -13.2809],
         [-10.7578, -10.7578, -10.7398,  ..., -12.8809, -12.8734, -12.8734],
         [-10.8125, -10.8125, -10.7729,  ..., -12.7217, -12.6875, -12.6875]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a zoo, there are separate areas designated for different animals. What structure in the picture is used to confine and display animals? Please output segmentation mask. ASSISTANT: in a zoo, there are separate areas designated for different animals. what structure in the picture is used to confine and display animals</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[78]]
>> (validate) sampled_classes_list:  [None]

         [-21.7500, -21.7500, -21.6688,  ..., -15.8273, -15.8125, -15.8125],7500, -10.6234, -10.1875,  ..., -10.6250, -10.1406, -10.0000],],
         [-21.6984, -21.6984, -21.6187,  ..., -15.8999, -15.8852, -15.8852],
         ...,
         [-19.1218, -19.1218, -19.1640,  ..., -15.9091, -15.9593, -15.9593],
         [-20.9399, -20.9399, -20.9536,  ..., -17.3759, -17.4149, -17.4149],
         [-22.4062, -22.4062, -22.3969,  ..., -18.5644, -18.5938, -18.5938]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In some cultures, people use sculptures and figurines to represent various aspects of life, including celebrations, rituals, and art. What object in the picture can be considered a representation of a human figure? Please output segmentation mask. ASSISTANT: in some cultures, people use sculptures and figurines to represent various aspects of life, including celebrations, rituals, and art. what object in the picture can be considered a representation of a human figure</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[95]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2868, 4309])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2868, 4309])):  [tensor([[[-11.1875, -11.1875, -11.1875,  ..., -14.6250, -14.6250, -14.6250],
         [-11.1875, -11.1875, -11.1875,  ..., -14.6250, -14.6250, -14.6250],
         [-11.1875, -11.1875, -11.1875,  ..., -14.6250, -14.6250, -14.6250],
         ...,
         [-10.1250, -10.1250, -10.1250,  ...,  -8.4915,  -8.4915,  -8.4915],
         [-10.1250, -10.1250, -10.1250,  ...,  -8.4531,  -8.4531,  -8.4531],
         [-10.1250, -10.1250, -10.1250,  ...,  -8.4531,  -8.4531,  -8.4531]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the shadow of the red car in this image? Please output segmentation mask. ASSISTANT: the shadow of the red car</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.], ..., -15.8273, -15.8125, -15.8125],7500, -10.6234, -10.1875,  ..., -10.6250, -10.1406, -10.0000],],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[ -7.8681,  -7.6415,  -7.6582,  ...,  -8.8574,  -9.4352,  -9.6914],
         [ -7.7274,  -8.0325,  -7.7656,  ...,  -9.6094,  -9.9981, -10.1657],
         [ -8.3314,  -8.5141,  -8.0625,  ...,  -9.6406,  -9.8469,  -9.9973],
         ...,
         [  1.6104,   2.5234,   2.5020,  ...,  -7.7656,  -8.3219,  -8.5582],
         [  0.9097,   1.7531,   1.8646,  ...,  -7.6094,  -8.2450,  -8.6704],
         [ -0.6416,   0.1254,   0.1482,  ...,  -7.7934,  -8.1717,  -8.2839]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If we were to take a flight in the sky, what part of the plane in the picture would be spinning to generate lift and propel the aircraft forward? Please output segmentation mask. ASSISTANT: if we were to take a flight in the sky, what part of the plane in the picture would be spinning to generate lift and propel the aircraft forward</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[84]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2499, 3680])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2499, 3680])):  [tensor([[[-22.8750, -22.8750, -22.8750,  ..., -21.0000, -21.0000, -21.0000],
         [-22.8750, -22.8750, -22.8750,  ..., -21.0000, -21.0000, -21.0000],
         [-22.8750, -22.8750, -22.8750,  ..., -21.0000, -21.0000, -21.0000],
         ...,
         [-20.0193, -20.0193, -20.0193,  ..., -19.5918, -19.5918, -19.5918],
         [-20.1719, -20.1719, -20.1719,  ..., -19.6406, -19.6406, -19.6406],
         [-20.1719, -20.1719, -20.1719,  ..., -19.6406, -19.6406, -19.6406]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something used by a Jedi warrior to attack enemies in this image? Please output segmentation mask. ASSISTANT: something used by a jedi warrior to attack enemies</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]

(PLUM.py) >> pred_masks (shape: torch.Size([1, 1023, 1392])):  [tensor([[[-3.6562, -3.6562, -3.6304,  ..., -8.0837, -8.0625, -8.0625],00],],
         [-3.6562, -3.6562, -3.6304,  ..., -8.0837, -8.0625, -8.0625],
         [-3.6343, -3.6343, -3.6072,  ..., -8.0748, -8.0545, -8.0545],
         ...,
         [-7.7175, -7.7175, -7.7476,  ..., -9.0692, -9.0925, -9.0925],
         [-7.8555, -7.8555, -7.8778,  ..., -9.2160, -9.2305, -9.2305],
         [-7.9688, -7.9688, -7.9846,  ..., -9.3365, -9.3438, -9.3438]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the part that can transmit traction and braking torque, coming into contact with the ground in this image? Please output segmentation mask. ASSISTANT: the part that can transmit traction and braking torque, coming into contact with the ground</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 752, 1169])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 752, 1169])):  [tensor([[[-19.5000, -19.5000, -19.3383,  ..., -16.6250, -16.6250, -16.6250],
         [-19.5000, -19.5000, -19.3383,  ..., -16.6250, -16.6250, -16.6250],
         [-19.4352, -19.4352, -19.3024,  ..., -16.6447, -16.6466, -16.6466],
         ...,
         [-10.4156, -10.4156, -10.5220,  ..., -13.3735, -13.4952, -13.4952],
         [ -9.7220,  -9.7220,  -9.8018,  ..., -12.5802, -12.6467, -12.6467],
         [ -9.8047,  -9.8047,  -9.8741,  ..., -12.5461, -12.5703, -12.5703]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the area where people can stand without getting wet in this image? Please output segmentation mask. ASSISTANT: the area where people can stand without getting wet</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,
         2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[66]]
>> (validate) sampled_classes_list:  [None]

(PLUM.py) >> pred_masks (shape: torch.Size([1, 3264, 4928])):  [tensor([[[-20.7500, -20.7500, -20.7500,  ..., -19.8750, -19.8750, -19.8750],
         [-20.7500, -20.7500, -20.7500,  ..., -19.8750, -19.8750, -19.8750],
         [-20.7500, -20.7500, -20.7500,  ..., -19.8750, -19.8750, -19.8750],
         ...,
         [-15.3726, -15.3726, -15.3726,  ..., -14.2347, -14.2347, -14.2347],
         [-15.3750, -15.3750, -15.3750,  ..., -14.2422, -14.2422, -14.2422],
         [-15.3750, -15.3750, -15.3750,  ..., -14.2422, -14.2422, -14.2422]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that protects the persons' heads in this image? Please output segmentation mask. ASSISTANT: something that protects the persons' heads</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1360, 2048])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1360, 2048])):  [tensor([[[-19.0000, -19.0000, -19.0000,  ..., -16.3750, -16.3750, -16.3750],
         [-19.0000, -19.0000, -19.0000,  ..., -16.3750, -16.3750, -16.3750],
         [-19.0000, -19.0000, -19.0000,  ..., -16.3750, -16.3750, -16.3750],
         ...,
         [-17.6094, -17.6094, -17.6094,  ..., -17.1094, -17.1094, -17.1094],
         [-18.0156, -18.0156, -18.0156,  ..., -17.2656, -17.2656, -17.2656],
         [-18.2188, -18.2188, -18.2188,  ..., -17.3438, -17.3438, -17.3438]]],
       device='cuda:0')]

         [0., 0., 0.,  ..., 0., 0., 0.],ze([1, 3264, 4928])):  [tensor([[[-20.7500, -20.7500, -20.7500,  ..., -19.8750, -19.8750, -19.8750],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1704, 2272])):  [tensor([[[-11.3125, -11.3125, -11.3125,  ..., -11.7500, -11.7500, -11.7500],
         [-11.3125, -11.3125, -11.3125,  ..., -11.7500, -11.7500, -11.7500],
         [-11.3125, -11.3125, -11.3125,  ..., -11.7500, -11.7500, -11.7500],
         ...,
         [-11.2570, -11.2570, -11.2570,  ..., -10.9348, -10.9348, -10.9348],
         [-11.4542, -11.4542, -11.4542,  ..., -11.6109, -11.6109, -11.6109],
         [-11.5312, -11.5312, -11.5312,  ..., -11.8750, -11.8750, -11.8750]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the source of power for the ship in this image? Please output segmentation mask. ASSISTANT: the source of power for the ship</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]

         [-17.8750, -17.8750, -17.8750,  ..., -17.1250, -17.1250, -17.1250],0.7500, -20.7500, -20.7500,  ..., -19.8750, -19.8750, -19.8750],
         [-17.8750, -17.8750, -17.8750,  ..., -17.1250, -17.1250, -17.1250],
         ...,
         [-13.9745, -13.9745, -13.9745,  ..., -12.2051, -12.2051, -12.2051],
         [-14.0000, -14.0000, -14.0000,  ..., -12.3359, -12.3359, -12.3359],
         [-14.0000, -14.0000, -14.0000,  ..., -12.3359, -12.3359, -12.3359]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In an intense dragon boat race. What object in the picture should be struck to boost the morale of the competing team and cheer them on? Please output segmentation mask. ASSISTANT: in an intense dragon boat race. what object in the picture should be struck to boost the morale of the competing team and cheer them on</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[83]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 986, 1544])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 986, 1544])):  [tensor([[[-21.1250, -21.1250, -21.0657,  ..., -16.7796, -16.7500, -16.7500],
         [-21.1250, -21.1250, -21.0657,  ..., -16.7796, -16.7500, -16.7500],
         [-21.0409, -21.0409, -20.9845,  ..., -16.7961, -16.7673, -16.7673],
         ...,
         [-13.8459, -13.8459, -13.8852,  ..., -13.2974, -13.2607, -13.2607],
         [-13.6075, -13.6075, -13.6464,  ..., -13.3079, -13.2814, -13.2814],
         [-13.4297, -13.4297, -13.4683,  ..., -13.3157, -13.2969, -13.2969]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the equipment for sweeping away rain on rainy days in this image? Please output segmentation mask. ASSISTANT: the equipment for sweeping away rain on rainy days</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]

         ...,.8750, -17.8750, -17.8750,  ..., -17.1250, -17.1250, -17.1250],0.7500, -20.7500, -20.7500,  ..., -19.8750, -19.8750, -19.8750],
         [ -9.1322,  -9.1525,  -9.3501,  ..., -11.2543, -11.2756, -11.2778],
         [ -9.4905,  -9.5099,  -9.6981,  ..., -10.7040, -10.7103, -10.7109],
         [ -9.8822,  -9.9002, -10.0744,  ..., -10.0954, -10.0821, -10.0807]]],
       device='cuda:0')]

(PLUM.py) >> gt_masks (shape: torch.Size([1, 300, 400])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],00,  ..., -19.8750, -19.8750, -19.8750],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 300, 400])):  [tensor([[[ -8.7500,  -8.3906,  -8.0734,  ...,  -8.7484,  -8.6675,  -8.4375],
         [ -8.3188,  -8.1181,  -7.8955,  ...,  -8.4627,  -8.5088,  -8.4375],
         [ -7.7996,  -7.7680,  -7.6471,  ...,  -8.0618,  -8.2093,  -8.2828],
         ...,
         [ -9.1641,  -9.1702,  -9.1548,  ...,  -8.8840,  -8.7858,  -8.6445],
         [ -8.8788,  -9.0541,  -9.1538,  ...,  -8.4265,  -8.4971,  -8.5575],
         [ -9.7100,  -9.9440, -10.1176,  ...,  -9.5399,  -9.7102,  -9.8499]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a graduation ceremony, it is a tradition for the graduates to wear a specific type of clothing to signify their achievement. What item of clothing can be seen in the picture that is commonly worn by graduates? Please output segmentation mask. ASSISTANT: in a graduation ceremony, it is a tradition for the graduates to wear a specific type of clothing to signify their achievement. what item of clothing can be seen in the picture that is commonly worn by graduates</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[99]]
>> (validate) sampled_classes_list:  [None]

(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-14.0625, -13.9805, -13.5938,  ..., -12.5250, -12.4013, -12.3750],],
         [-13.9608, -13.8879, -13.5442,  ..., -12.5152, -12.3995, -12.3750],
         [-13.4812, -13.4514, -13.3106,  ..., -12.4688, -12.3914, -12.3750],
         ...,
         [ -2.5797,  -2.5776,  -2.5675,  ...,   2.3403,   2.4849,   2.5156],
         [ -3.2295,  -3.2211,  -3.1818,  ...,   1.4403,   1.5826,   1.6128],
         [ -5.1834,  -5.1719,  -5.1175,  ...,  -1.8325,  -1.7384,  -1.7184]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that the monkey uses to pierce its food in this image? Please output segmentation mask. ASSISTANT: something that the monkey uses to pierce its food</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[68]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1600, 1066])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1600, 1066])):  [tensor([[[-13.5625, -13.5625, -13.5431,  ..., -11.1250, -11.0750, -11.0391],
         [-13.5625, -13.5625, -13.5431,  ..., -11.1250, -11.0750, -11.0391],
         [-13.5211, -13.5211, -13.5026,  ..., -11.1340, -11.0754, -11.0333],
         ...,
         [ -5.5133,  -5.5133,  -5.5093,  ..., -10.3567,  -9.1761,  -8.3278],
         [ -5.5000,  -5.5000,  -5.4961,  ..., -10.3302,  -9.1356,  -8.2773],
         [ -5.5000,  -5.5000,  -5.4961,  ..., -10.3302,  -9.1356,  -8.2773]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that avoids falling down in this image? Please output segmentation mask. ASSISTANT: something that avoids falling down</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]

         [-16.3803, -16.7019, -16.5243,  ..., -18.1765, -18.4022, -18.3732],0625, -13.9805, -13.5938,  ..., -12.5250, -12.4013, -12.3750],],
         [-16.6267, -17.3433, -16.9718,  ..., -18.2450, -17.9629, -17.8021],
         ...,
         [-15.0563, -15.9606, -15.2052,  ..., -13.2101, -12.4049, -13.5662],
         [-14.0393, -13.2052, -12.6655,  ..., -11.8220, -12.1856, -13.1757],
         [-13.7904, -12.3124, -12.2430,  ..., -11.9225, -13.0502, -13.1904]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What object in a typical modern kitchen can be used to quickly heat up leftovers, boil water, or cook frozen food? Please output segmentation mask. ASSISTANT: what object in a typical modern kitchen can be used to quickly heat up leftovers, boil water, or cook frozen food</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[80]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 240, 320])):  [tensor([[[-16.7464, -15.7770, -16.2395,  ..., -11.4434, -12.2648, -12.7390],
         [-17.5387, -17.6031, -17.3781,  ..., -11.7656, -12.0456, -12.6085],
         [-18.2578, -18.6875, -18.0625,  ..., -11.7812, -11.6281, -12.2082],
         ...,
         [-14.1445, -15.2188, -14.5625,  ...,  -7.8906,  -8.1000,  -8.5082],
         [-14.0172, -15.3062, -14.3844,  ...,  -7.7813,  -7.8556,  -8.1908],
         [-12.8827, -13.6927, -13.4188,  ...,  -9.0656,  -9.1442,  -9.5792]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When plants grow excessively in unwanted areas, it is necessary to remove them to maintain the aesthetics of the surroundings. What part of the plants in the picture may need to be removed in this situation? Please output segmentation mask. ASSISTANT: when plants grow excessively in unwanted areas, it is necessary to remove them to maintain the aesthetics of the surroundings. what part of the plants in the picture may need to be removed in this situation</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[97]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.], ..., -18.1765, -18.4022, -18.3732],0625, -13.9805, -13.5938,  ..., -12.5250, -12.4013, -12.3750],],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1536, 2048])):  [tensor([[[-14.1875, -14.1875, -14.1875,  ..., -13.9375, -13.9375, -13.9375],
         [-14.1875, -14.1875, -14.1875,  ..., -13.9375, -13.9375, -13.9375],
         [-14.1875, -14.1875, -14.1875,  ..., -13.9375, -13.9375, -13.9375],
         ...,
         [-12.9766, -12.9766, -12.9766,  ..., -11.3555, -11.3555, -11.3555],
         [-13.0859, -13.0859, -13.0859,  ..., -11.6758, -11.6758, -11.6758],
         [-13.1406, -13.1406, -13.1406,  ..., -11.8359, -11.8359, -11.8359]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the keys on a piano used to play notes of half-steps or semitones in this image? Please output segmentation mask. ASSISTANT: the keys on a piano used to play notes of half-steps or semitones</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[74]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1941, 2919])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 1., 1.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1941, 2919])):  [tensor([[[-6.6875, -6.6875, -6.6875,  ..., -5.2500, -5.2500, -5.2500],
         [-6.6875, -6.6875, -6.6875,  ..., -5.2500, -5.2500, -5.2500],
         [-6.6875, -6.6875, -6.6875,  ..., -5.2500, -5.2500, -5.2500],
         ...,
         [-8.2350, -8.2350, -8.2350,  ..., -6.7673, -6.7673, -6.7673],
         [-8.4597, -8.4597, -8.4597,  ..., -6.8166, -6.8166, -6.8166],
         [-8.4766, -8.4766, -8.4766,  ..., -6.8203, -6.8203, -6.8203]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n The general steps for brewing tea are to put tea leaves into a teacup and then pour hot water over them. What container in the picture is most likely to be used next for pouring hot water to make tea? Please output segmentation mask. ASSISTANT: the general steps for brewing tea are to put tea leaves into a teacup and then pour hot water over them. what container in the picture is most likely to be used next for pouring hot water to make tea</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[97]]
>> (validate) sampled_classes_list:  [None]

         [-11.2512, -11.2392, -11.1795,  ..., -10.3598, -10.4020, -10.4105],0625, -13.9805, -13.5938,  ..., -12.5250, -12.4013, -12.3750],],
         ...,
         [ -7.4572,  -7.4601,  -7.4745,  ...,  -9.0617,  -9.0253,  -9.0179],
         [ -7.5841,  -7.5849,  -7.5890,  ...,  -9.0701,  -8.9514,  -8.9274],
         [ -8.0645,  -8.0780,  -8.1446,  ...,  -9.2292,  -9.0271,  -8.9862]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is objects that can help women appear taller in this image? Please output segmentation mask. ASSISTANT: objects that can help women appear taller</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]

         [-11.6000, -11.6627, -11.9581,  ..., -12.6437, -12.8294, -12.8688],0625, -13.9805, -13.5938,  ..., -12.5250, -12.4013, -12.3750],],
         [-11.5645, -11.6294, -11.9354,  ..., -12.8190, -13.0273, -13.0715],
         [-11.5413, -11.5777, -11.7494,  ..., -13.9497, -13.9757, -13.9812]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When serving wine, it is common to pour it into a glass from a container. What object in the picture could be used to pour wine? Please output segmentation mask. ASSISTANT: when serving wine, it is common to pour it into a glass from a container. what object in the picture could be used to pour wine</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[80]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 370, 658])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 370, 658])):  [tensor([[[ -9.4375,  -9.3332,  -8.9649,  ..., -10.1807, -10.0886, -10.0625],
         [ -9.5484,  -9.4537,  -9.1189,  ..., -10.1636, -10.1051, -10.0886],
         [ -9.9400,  -9.8789,  -9.6630,  ..., -10.1033, -10.1637, -10.1807],
         ...,
         [ -9.2922,  -9.3297,  -9.4619,  ...,  -9.1868,  -9.2512,  -9.2694],
         [ -9.1072,  -9.1628,  -9.3591,  ...,  -9.0343,  -9.1792,  -9.2202],
         [-10.2927, -10.3062, -10.3536,  ..., -10.7304, -10.7247, -10.7231]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the container that contains the vegetable in this image? Please output segmentation mask. ASSISTANT: the container that contains the vegetable</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.], ..., -12.6437, -12.8294, -12.8688],0625, -13.9805, -13.5938,  ..., -12.5250, -12.4013, -12.3750],],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 768, 576])):  [tensor([[[-7.3750, -7.2773, -6.8542,  ..., -3.4062, -3.3574, -4.1875],
         [-7.3164, -7.2191, -6.7975,  ..., -3.4427, -3.4048, -4.2214],
         [-7.0625, -6.9668, -6.5521,  ..., -3.6007, -3.6100, -4.3681],
         ...,
         [-5.3854, -5.2083, -4.4410,  ..., -4.1528, -4.0918, -4.8229],
         [-5.0215, -4.8415, -4.0615,  ..., -4.1217, -4.0505, -4.7468],
         [-4.9375, -4.7568, -3.9740,  ..., -4.1146, -4.0410, -4.7292]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the people who are about to get on the vehicle in this image? Please output segmentation mask. ASSISTANT: the people who are about to get on the vehicle</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[67]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-14.3125, -14.2141, -13.7500,  ...,  -8.7250,  -8.8487,  -8.8750],
         [-14.2370, -14.1484, -13.7306,  ...,  -8.7142,  -8.8306,  -8.8553],
         [-13.8812, -13.8389, -13.6394,  ...,  -8.6631,  -8.7451,  -8.7625],
         ...,
         [ -8.5750,  -8.6013,  -8.7250,  ...,  -6.8269,  -6.8176,  -6.8156],
         [ -8.7034,  -8.7266,  -8.8358,  ...,  -7.0872,  -7.0219,  -7.0081],
         [ -9.3925,  -9.4042,  -9.4596,  ...,  -8.2950,  -8.1965,  -8.1756]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something indicating the identity of the bus in this image? Please output segmentation mask. ASSISTANT: something indicating the identity of the bus</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]

         [-7.2238, -7.2238, -7.2241,  ..., -7.2866, -7.4837, -7.5674],8688],0625, -13.9805, -13.5938,  ..., -12.5250, -12.4013, -12.3750],],
         ...,
         [-3.8294, -3.8282, -3.7923,  ..., -4.6628, -4.4074, -4.3916],
         [-3.7828, -3.7813, -3.7350,  ..., -4.5551, -4.2759, -4.2509],
         [-3.7812, -3.7797, -3.7331,  ..., -4.5516, -4.2716, -4.2463]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that can control the fan speed in this image? Please output segmentation mask. ASSISTANT: something that can control the fan speed</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')] -7.5674],8688],0625, -13.9805, -13.5938,  ..., -12.5250, -12.4013, -12.3750],],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2048, 1536])):  [tensor([[[-6.5000, -6.5000, -6.5000,  ..., -7.2285, -7.1934, -7.1758],
         [-6.5000, -6.5000, -6.5000,  ..., -7.2285, -7.1934, -7.1758],
         [-6.5000, -6.5000, -6.5000,  ..., -7.2285, -7.1934, -7.1758],
         ...,
         [-6.0312, -6.0312, -6.0312,  ..., -5.7988, -5.9355, -6.0039],
         [-6.0312, -6.0312, -6.0312,  ..., -5.7988, -5.9355, -6.0039],
         [-6.0312, -6.0312, -6.0312,  ..., -5.7988, -5.9355, -6.0039]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the stronger Mario in this image? Please output segmentation mask. ASSISTANT: the stronger mario</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[60]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2448, 3264])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2448, 3264])):  [tensor([[[-8.4375, -8.4375, -8.4375,  ..., -7.8750, -7.8750, -7.8750],
         [-8.4375, -8.4375, -8.4375,  ..., -7.8750, -7.8750, -7.8750],
         [-8.4375, -8.4375, -8.4375,  ..., -7.8750, -7.8750, -7.8750],
         ...,
         [-7.4501, -7.4501, -7.4501,  ..., -8.2019, -8.2019, -8.2019],
         [-7.6211, -7.6211, -7.6211,  ..., -8.5195, -8.5195, -8.5195],
         [-7.6211, -7.6211, -7.6211,  ..., -8.5195, -8.5195, -8.5195]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is where to wash hands in this image? Please output segmentation mask. ASSISTANT: where to wash hands</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[62]]
>> (validate) sampled_classes_list:  [None]

 73%|████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                      | 146/200 [00:51<00:19,  2.75it/s]

 75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                   | 150/200 [00:53<00:20,  2.49it/s]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1274, 2048])):  [tensor([[[-13.6250, -13.6250, -13.6250,  ..., -12.5000, -12.5000, -12.5000],
         [-13.6250, -13.6250, -13.6250,  ..., -12.5000, -12.5000, -12.5000],
         [-13.6250, -13.6250, -13.6250,  ..., -12.5000, -12.5000, -12.5000],
         ...,
         [ -9.6172,  -9.6172,  -9.6172,  ...,  -8.1367,  -8.1367,  -8.1367],
         [ -9.7578,  -9.7578,  -9.7578,  ...,  -8.1758,  -8.1758,  -8.1758],
         [ -9.8281,  -9.8281,  -9.8281,  ...,  -8.1953,  -8.1953,  -8.1953]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is a car with a color that is closer to lipstick color in this image? Please output segmentation mask. ASSISTANT: a car with a color that is closer to lipstick color</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[70]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 1600])):  [tensor([[[-10.0000, -10.0000,  -9.9586,  ..., -12.2328, -12.2500, -12.2500],
         [-10.0000, -10.0000,  -9.9586,  ..., -12.2328, -12.2500, -12.2500],
         [ -9.9664,  -9.9664,  -9.9256,  ..., -12.2455, -12.2625, -12.2625],
         ...,
         [ -8.8875,  -8.8875,  -8.9098,  ...,  -9.5228,  -9.5258,  -9.5258],
         [ -9.6637,  -9.6637,  -9.6797,  ..., -10.4754, -10.4725, -10.4725],
         [-10.2891, -10.2891, -10.3000,  ..., -11.2417, -11.2344, -11.2344]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n If you want to play table tennis indoors, what furniture in the picture should be used as the playing surface? Please output segmentation mask. ASSISTANT: if you want to play table tennis indoors, what furniture in the picture should be used as the playing surface</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]

(PLUM.py) >> pred_masks (shape: torch.Size([1, 1920, 2560])):  [tensor([[[-12.0625, -12.0625, -12.0625,  ..., -11.3750, -11.3750, -11.3750],
         [-12.0625, -12.0625, -12.0625,  ..., -11.3750, -11.3750, -11.3750],
         [-12.0625, -12.0625, -12.0625,  ..., -11.3750, -11.3750, -11.3750],
         ...,
         [-13.3281, -13.3281, -13.3281,  ..., -13.3906, -13.3906, -13.3906],
         [-13.9844, -13.9844, -13.9844,  ..., -14.3219, -14.3219, -14.3219],
         [-14.1484, -14.1484, -14.1484,  ..., -14.5547, -14.5547, -14.5547]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n A bride and groom often walk together down the aisle during a wedding ceremony. What object in the picture is the bride most likely holding during this moment? Please output segmentation mask. ASSISTANT: a bride and groom often walk together down the aisle during a wedding ceremony. what object in the picture is the bride most likely holding during this moment</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 1200, 900])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1200, 900])):  [tensor([[[-22.0000, -22.0000, -21.7724,  ..., -19.9276, -20.3375, -20.7031],
         [-22.0000, -22.0000, -21.7724,  ..., -19.9276, -20.3375, -20.7031],
         [-21.7724, -21.7724, -21.5941,  ..., -19.9035, -20.2520, -20.5250],
         ...,
         [-17.3859, -17.3859, -18.3823,  ..., -19.3606, -19.3222, -18.7985],
         [-17.0000, -17.0000, -17.9995,  ..., -19.3500, -19.2900, -18.6562],
         [-17.0000, -17.0000, -17.9995,  ..., -19.3500, -19.2900, -18.6562]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Some dishes are baked in the oven to enhance their flavors. What object in the picture is commonly used to place the dishes in the oven for baking? Please output segmentation mask. ASSISTANT: some dishes are baked in the oven to enhance their flavors. what object in the picture is commonly used to place the dishes in the oven for baking</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[89]]
>> (validate) sampled_classes_list:  [None]

         [-24.7500, -24.7500, -24.7500,  ..., -20.0000, -20.0000, -20.0000],2.0625, -12.0625, -12.0625,  ..., -11.3750, -11.3750, -11.3750],
         ...,
         [-20.3013, -20.3013, -20.3013,  ..., -18.1411, -18.1411, -18.1411],
         [-20.2214, -20.2214, -20.2214,  ..., -17.9440, -17.9440, -17.9440],
         [-20.2188, -20.2188, -20.2188,  ..., -17.9375, -17.9375, -17.9375]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n People often add gas to water to improve its taste. What part of the picture will be consumed as a beverage? Please output segmentation mask. ASSISTANT: people often add gas to water to improve its taste. what part of the picture will be consumed as a beverage</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[76]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.], ..., -20.0000, -20.0000, -20.0000],2.0625, -12.0625, -12.0625,  ..., -11.3750, -11.3750, -11.3750],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 1000, 1500])):  [tensor([[[-14.8125, -14.8125, -14.7608,  ..., -12.2980, -12.3125, -12.3125],
         [-14.8125, -14.8125, -14.7608,  ..., -12.2980, -12.3125, -12.3125],
         [-14.7720, -14.7720, -14.7225,  ..., -12.2742, -12.2882, -12.2882],
         ...,
         [ -7.5633,  -7.5633,  -7.5625,  ...,  -7.9319,  -7.9458,  -7.9458],
         [ -7.6946,  -7.6946,  -7.6941,  ...,  -7.8107,  -7.8242,  -7.8242],
         [ -7.9609,  -7.9609,  -7.9612,  ...,  -7.8119,  -7.8242,  -7.8242]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In a dark cave, there is no natural light source. What object in the picture can be used to provide light to navigate and explore the cave? Please output segmentation mask. ASSISTANT: in a dark cave, there is no natural light source. what object in the picture can be used to provide light to navigate and explore the cave</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[81]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 480, 640])):  [tensor([[[-24.8750, -24.2281, -22.0000,  ..., -24.0000, -24.5813, -24.7500],
         [-24.7766, -24.2278, -22.3375,  ..., -23.8172, -24.3984, -24.5672],
         [-24.4375, -24.2266, -23.5000,  ..., -23.1875, -23.7688, -23.9375],
         ...,
         [-17.1875, -17.6937, -19.4375,  ..., -18.6875, -19.1719, -19.3125],
         [-13.8656, -14.7018, -17.5820,  ..., -17.7055, -19.2440, -19.6906],
         [-15.5625, -16.0674, -17.8063,  ..., -19.3188, -20.3796, -20.6875]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the car that may need repair in this image? Please output segmentation mask. ASSISTANT: the car that may need repair</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]

         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],00, -20.0000, -20.0000],2.0625, -12.0625, -12.0625,  ..., -11.3750, -11.3750, -11.3750],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],
         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 612, 612])):  [tensor([[[-10.4375, -10.4375, -10.4375,  ...,  -7.9263,  -7.7433,  -7.6875],
         [-10.3180, -10.3393, -10.4093,  ...,  -7.9733,  -7.7970,  -7.7433],
         [ -9.9259, -10.0172, -10.3168,  ...,  -8.1278,  -7.9733,  -7.9263],
         ...,
         [ -7.4321,  -7.5291,  -7.8476,  ...,  -8.1885,  -8.0921,  -8.0627],
         [ -7.0530,  -7.1550,  -7.4899,  ...,  -7.6084,  -7.7362,  -7.7751],
         [ -6.9375,  -7.0411,  -7.3809,  ...,  -7.4317,  -7.6278,  -7.6875]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something that we can sit on in this image? Please output segmentation mask. ASSISTANT: something that we can sit on</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[63]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 540, 720])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 540, 720])):  [tensor([[[-8.0625, -8.0303, -7.9045,  ..., -7.7292, -7.6711, -7.6562],
         [-7.9660, -7.9436, -7.8561,  ..., -7.6577, -7.6073, -7.5944],
         [-7.5885, -7.6045, -7.6669,  ..., -7.3781, -7.3577, -7.3524],
         ...,
         [-3.9418, -3.9949, -4.2024,  ..., -5.3661, -4.9252, -4.8125],
         [-3.4801, -3.5322, -3.7363,  ..., -5.0400, -4.8786, -4.8374],
         [-4.5241, -4.5531, -4.6666,  ..., -6.1223, -6.0196, -5.9934]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n To keep bread fresh and protected, it is often placed in a protective covering. What item in the picture is commonly used for this purpose? Please output segmentation mask. ASSISTANT: to keep bread fresh and protected, it is often placed in a protective covering. what item in the picture is commonly used for this purpose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[80]]
>> (validate) sampled_classes_list:  [None]

(PLUM.py) >> pred_masks (shape: torch.Size([1, 600, 800])):  [tensor([[[-13.8750, -13.8159, -13.5375,  ..., -13.8750, -13.8750, -13.8750],],
         [-13.8717, -13.8202, -13.5775,  ..., -13.9439, -13.9439, -13.9439],
         [-13.8562, -13.8405, -13.7663,  ..., -14.2688, -14.2688, -14.2688],
         ...,
         [ -3.8578,  -3.8358,  -3.7322,  ...,  -6.3369,  -6.4869,  -6.5188],
         [ -4.5469,  -4.5280,  -4.4391,  ...,  -6.9542,  -7.0046,  -7.0153],
         [ -6.5918,  -6.5732,  -6.4855,  ...,  -8.9792,  -8.9149,  -8.9012]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something showing that they are skating in this image? Please output segmentation mask. ASSISTANT: something showing that they are skating</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[64]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 580, 435])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 580, 435])):  [tensor([[[-15.3750, -15.2028, -14.7338,  ..., -13.7489, -13.8869, -14.0597],
         [-15.3244, -15.2145, -14.9155,  ..., -13.8790, -13.9588, -14.0453],
         [-15.1864, -15.2465, -15.4101,  ..., -14.2334, -14.1549, -14.0060],
         ...,
         [ -6.0059,  -6.0179,  -6.0505,  ...,  -6.7448,  -6.5417,  -7.1057],
         [ -6.0473,  -6.0839,  -6.1834,  ...,  -6.6520,  -6.6681,  -7.1392],
         [ -6.0625,  -6.1081,  -6.2322,  ...,  -6.6180,  -6.7145,  -7.1515]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n Dogs have keen sense of smell, which is why they can be used as drug-sniffing dogs. Which part in the picture gives dogs this characteristic? Please output segmentation mask. ASSISTANT: dogs have keen sense of smell, which is why they can be used as drug-sniffing dogs. which part in the picture gives dogs this characteristic</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]],
       device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[85]]
>> (validate) sampled_classes_list:  [None]

         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')] [tensor([[[-13.8750, -13.8159, -13.5375,  ..., -13.8750, -13.8750, -13.8750],],
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2356, 3141])):  [tensor([[[-16.1250, -16.1250, -16.1250,  ..., -14.7500, -14.7500, -14.7500],
         [-16.1250, -16.1250, -16.1250,  ..., -14.7500, -14.7500, -14.7500],
         [-16.1250, -16.1250, -16.1250,  ..., -14.7500, -14.7500, -14.7500],
         ...,
         [-15.0277, -15.0277, -15.0277,  ..., -15.8285, -15.8285, -15.8285],
         [-15.2344, -15.2344, -15.2344,  ..., -16.2812, -16.2812, -16.2812],
         [-15.2344, -15.2344, -15.2344,  ..., -16.2812, -16.2812, -16.2812]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n When snorkeling or scuba diving, it is important to wear certain equipment to protect the eyes and aid in vision. What object in the picture can be used for this purpose? Please output segmentation mask. ASSISTANT: when snorkeling or scuba diving, it is important to wear certain equipment to protect the eyes and aid in vision. what object in the picture can be used for this purpose</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[89]]
>> (validate) sampled_classes_list:  [None]

         [-16.2930, -16.2930, -16.2461,  ..., -15.9592, -15.9414, -15.9414],8750, -13.8159, -13.5375,  ..., -13.8750, -13.8750, -13.8750],],
         ...,
         [-12.6055, -12.6055, -12.7771,  ..., -13.2019, -13.2188, -13.2188],
         [-12.9750, -12.9750, -13.1187,  ..., -14.2390, -14.2500, -14.2500],
         [-13.5547, -13.5547, -13.6567,  ..., -15.5693, -15.5625, -15.5625]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is the reflection of the camera in the mirror in this image? Please output segmentation mask. ASSISTANT: the reflection of the camera in the mirror</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2,
         2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[65]]
>> (validate) sampled_classes_list:  [None]
(PLUM.py) >> gt_masks (shape: torch.Size([1, 2880, 1920])):  [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')]
(PLUM.py) >> pred_masks (shape: torch.Size([1, 2880, 1920])):  [tensor([[[ -7.2500,  -7.2500,  -7.2500,  ..., -10.4553, -10.4747, -10.4766],
         [ -7.2500,  -7.2500,  -7.2500,  ..., -10.4553, -10.4747, -10.4766],
         [ -7.2500,  -7.2500,  -7.2500,  ..., -10.4553, -10.4747, -10.4766],
         ...,
         [ -9.0625,  -9.0625,  -9.0625,  ...,  -9.9676,  -9.8759,  -9.8672],
         [ -9.0625,  -9.0625,  -9.0625,  ...,  -9.9676,  -9.8759,  -9.8672],
         [ -9.0625,  -9.0625,  -9.0625,  ...,  -9.9676,  -9.8759,  -9.8672]]],
       device='cuda:0')]
>> (validate) input_dict[conversation_list]:  ["A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n In some rural areas, horse-drawn carts are still used for transportation and carrying goods. What is the main source of power that drives the cart in the picture? Please output segmentation mask. ASSISTANT: in some rural areas, horse-drawn carts are still used for transportation and carrying goods. what is the main source of power that drives the cart in the picture</s>"]
>> (validate) per_token_labels:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
         2, 2, 0]], device='cuda:0')
>> (validate) mask_positions_in_input_ids:  [[87]]
>> (validate) sampled_classes_list:  [None]

 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 196/200 [01:10<00:01,  2.63it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [01:11<00:00,  2.78it/s]
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 6 is less than current step: 3499. Dropping entry: {'val/giou': 0.3680786192417145, 'val/ciou': 0.3348158299922943, 'val/b_acc': 0.5499999725000013, 'val/i_acc': 0.8614296148781908, 'val/o_acc': 0.999999999849284, '_timestamp': 1743905881.577687}).
         [-16.2930, -16.2930, -16.2461,  ..., -15.9592, -15.9414, -15.9414],8750, -13.8159, -13.5375,  ..., -13.8750, -13.8750, -13.8750],],
         [-16.2930, -16.2930, -16.2461,  ..., -15.9592, -15.9414, -15.9414],8750, -13.8159, -13.5375,  ..., -13.8750, -13.8750, -13.8750],],
         [-16.2930, -16.2930, -16.2461,  ..., -15.9592, -15.9414, -15.9414],8750, -13.8159, -13.5375,  ..., -13.8750, -13.8750, -13.8750],],
         [-16.2930, -16.2930, -16.2461,  ..., -15.9592, -15.9414, -15.9414],8750, -13.8159, -13.5375,  ..., -13.8750, -13.8750, -13.8750],],
         [-16.2930, -16.2930, -16.2461,  ..., -15.9592, -15.9414, -15.9414],8750, -13.8159, -13.5375,  ..., -13.8750, -13.8750, -13.8750],],
         [-16.2930, -16.2930, -16.2461,  ..., -15.9592, -15.9414, -15.9414],8750, -13.8159, -13.5375,  ..., -13.8750, -13.8750, -13.8750],],
         [-16.2930, -16.2930, -16.2461,  ..., -15.9592, -15.9414, -15.9414],8750, -13.8159, -13.5375,  ..., -13.8750, -13.8750, -13.8750],],
         [-16.2930, -16.2930, -16.2461,  ..., -15.9592, -15.9414, -15.9414],8750, -13.8159, -13.5375,  ..., -13.8750, -13.8750, -13.8750],],
         [-16.2930, -16.2930, -16.2461,  ..., -15.9592, -15.9414, -15.9414],8750, -13.8159, -13.5375,  ..., -13.8750, -13.8750, -13.8750],],
         [-16.2930, -16.2930, -16.2461,  ..., -15.9592, -15.9414, -15.9414],8750, -13.8159, -13.5375,  ..., -13.8750, -13.8750, -13.8750],],
         [-16.2930, -16.2930, -16.2461,  ..., -15.9592, -15.9414, -15.9414],8750, -13.8159, -13.5375,  ..., -13.8750, -13.8750, -13.8750],],
         [-16.2930, -16.2930, -16.2461,  ..., -15.9592, -15.9414, -15.9414],8750, -13.8159, -13.5375,  ..., -13.8750, -13.8750, -13.8750],],
         [-16.2930, -16.2930, -16.2461,  ..., -15.9592, -15.9414, -15.9414],8750, -13.8159, -13.5375,  ..., -13.8750, -13.8750, -13.8750],],
         [-16.2930, -16.2930, -16.2461,  ..., -15.9592, -15.9414, -15.9414],8750, -13.8159, -13.5375,  ..., -13.8750, -13.8750, -13.8750],],
         [-16.2930, -16.2930, -16.2461,  ..., -15.9592, -15.9414, -15.9414],8750, -13.8159, -13.5375,  ..., -13.8750, -13.8750, -13.8750],],
         [-16.2930, -16.2930, -16.2461,  ..., -15.9592, -15.9414, -15.9414],8750, -13.8159, -13.5375,  ..., -13.8750, -13.8750, -13.8750],],
         [-16.2930, -16.2930, -16.2461,  ..., -15.9592, -15.9414, -15.9414],8750, -13.8159, -13.5375,  ..., -13.8750, -13.8750, -13.8750],],
         [-16.2930, -16.2930, -16.2461,  ..., -15.9592, -15.9414, -15.9414],8750, -13.8159, -13.5375,  ..., -13.8750, -13.8750, -13.8750],],
         [-16.2930, -16.2930, -16.2461,  ..., -15.9592, -15.9414, -15.9414],8750, -13.8159, -13.5375,  ..., -13.8750, -13.8750, -13.8750],],
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 813, in <module>
[rank0]:     main(sys.argv[1:])
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 498, in main
[rank0]:     train_iter = train(
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 628, in train
[rank0]:     model.backward(loss)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1861, in backward
[rank0]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1900, in backward
[rank0]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank0]:     scaled_loss.backward(retain_graph=retain_graph)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_tensor.py", line 525, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/__init__.py", line 267, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.66 GiB. GPU