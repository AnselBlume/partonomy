You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565



Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.89s/it]
trainable params: 6,553,600 || all params: 14,025,701,683 || trainable%: 0.0467256480147682
n:  base_model.model.model.embed_tokens.weight p.shape:  torch.Size([32002, 5120])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_token.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.mask_decoder.mask_tokens.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.weight p.shape:  torch.Size([256, 64, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.weight p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.weight p.shape:  torch.Size([64, 32, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.bias p.shape:  torch.Size([4])
n:  base_model.model.model.text_hidden_fcs.0.0.weight p.shape:  torch.Size([5120, 5120])
n:  base_model.model.model.text_hidden_fcs.0.0.bias p.shape:  torch.Size([5120])
n:  base_model.model.model.text_hidden_fcs.0.2.weight p.shape:  torch.Size([256, 5120])
n:  base_model.model.model.text_hidden_fcs.0.2.bias p.shape:  torch.Size([256])
n:  base_model.model.lm_head.weight p.shape:  torch.Size([32002, 5120])
>> (PLUM.py) Initializing teacher LLM...
>> (PLUM.py) Teacher LLM initialized.
ade20k:  20210
cocostuff:  118287
loading annotations into memory...
Done (t=0.77s)
creating index...
index created!
pascal_part:  4366
loading annotations into memory...
Done (t=8.88s)
creating index...
index created!
paco_lvis:  45790
mapillary:  18000
loading dataset refclef into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refclef/refs(unc).p
creating index...
index created.
DONE (t=3.01s)
dataset refclef (refs unc) (train split) has 17978 images and 99523 annotations.
loading dataset refcoco into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco/refs(unc).p
creating index...
index created.
DONE (t=5.58s)
dataset refcoco (refs unc) (train split) has 16994 images and 196771 annotations.
loading dataset refcoco+ into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco+/refs(unc).p
creating index...
index created.
DONE (t=6.75s)
dataset refcoco+ (refs unc) (train split) has 16992 images and 196737 annotations.
loading dataset refcocog into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcocog/refs(umd).p
creating index...
index created.
DONE (t=7.07s)
dataset refcocog (refs umd) (train split) has 21899 images and 208960 annotations.
vqa_data:  157712
number of reason_seg samples:  239
len(self.img_to_explanation):  239
Training with 5000 examples and validating with 200 examples.
[2025-03-04 20:49:45,073] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5, git-hash=unknown, git-branch=unknown
[2025-03-04 20:49:45,073] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2025-03-04 20:49:45,074] [INFO] [comm.py:594:init_distributed] cdb=None
[2025-03-04 20:49:45,074] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-03-04 20:50:14,634] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Time to load fused_adam op: 0.6208078861236572 seconds
[2025-03-04 20:50:15,459] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-03-04 20:50:16,178] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-03-04 20:50:16,178] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-03-04 20:50:16,178] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-03-04 20:50:16,179] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500000000
[2025-03-04 20:50:16,179] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500000000
[2025-03-04 20:50:16,179] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2025-03-04 20:50:16,179] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(365842916, False)]
Loading extension module fused_adam...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/tensor/python_tensor.cpp:78.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
[2025-03-04 20:50:23,855] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2025-03-04 20:50:23,856] [INFO] [utils.py:786:see_memory_usage] MA 53.71 GB         Max_MA 54.39 GB         CA 54.6 GB         Max_CA 55 GB
[2025-03-04 20:50:23,856] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 234.98 GB, percent = 23.3%
[2025-03-04 20:50:28,417] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2025-03-04 20:50:28,419] [INFO] [utils.py:786:see_memory_usage] MA 56.44 GB         Max_MA 57.8 GB         CA 58.69 GB         Max_CA 59 GB
[2025-03-04 20:50:28,420] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 236.41 GB, percent = 23.5%
[2025-03-04 20:50:28,420] [INFO] [stage_1_and_2.py:488:__init__] optimizer state initialized
[2025-03-04 20:50:33,567] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2025-03-04 20:50:33,569] [INFO] [utils.py:786:see_memory_usage] MA 56.44 GB         Max_MA 56.44 GB         CA 58.69 GB         Max_CA 59 GB
[2025-03-04 20:50:33,569] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 234.98 GB, percent = 23.3%
[2025-03-04 20:50:33,577] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-03-04 20:50:33,578] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2025-03-04 20:50:33,578] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7f395a995570>
[2025-03-04 20:50:33,578] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[2e-05], mom=[(0.9, 0.95)]
[2025-03-04 20:50:33,585] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2025-03-04 20:50:33,585] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-03-04 20:50:33,585] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-03-04 20:50:33,585] [INFO] [config.py:964:print]   amp_enabled .................. False
[2025-03-04 20:50:33,585] [INFO] [config.py:964:print]   amp_params ................... False
[2025-03-04 20:50:33,586] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-04 20:50:33,586] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2025-03-04 20:50:33,586] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2025-03-04 20:50:33,586] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2025-03-04 20:50:33,586] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2025-03-04 20:50:33,586] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f23f4370460>
[2025-03-04 20:50:33,586] [INFO] [config.py:964:print]   communication_data_type ...... None
[2025-03-04 20:50:33,586] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-04 20:50:33,586] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2025-03-04 20:50:33,586] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2025-03-04 20:50:33,587] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-04 20:50:33,587] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2025-03-04 20:50:33,587] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2025-03-04 20:50:33,587] [INFO] [config.py:964:print]   disable_allgather ............ False
[2025-03-04 20:50:33,587] [INFO] [config.py:964:print]   dump_state ................... False
[2025-03-04 20:50:33,587] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2025-03-04 20:50:33,587] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2025-03-04 20:50:33,587] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-04 20:50:33,587] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-04 20:50:33,587] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2025-03-04 20:50:33,587] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2025-03-04 20:50:33,587] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2025-03-04 20:50:33,587] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2025-03-04 20:50:33,587] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2025-03-04 20:50:33,587] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2025-03-04 20:50:33,588] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-03-04 20:50:33,588] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2025-03-04 20:50:33,588] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2025-03-04 20:50:33,588] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2025-03-04 20:50:33,588] [INFO] [config.py:964:print]   global_rank .................. 0
[2025-03-04 20:50:33,588] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2025-03-04 20:50:33,588] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 10
[2025-03-04 20:50:33,588] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0
[2025-03-04 20:50:33,588] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2025-03-04 20:50:33,588] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-04 20:50:33,588] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2025-03-04 20:50:33,588] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2025-03-04 20:50:33,588] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2025-03-04 20:50:33,588] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2025-03-04 20:50:33,589] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2025-03-04 20:50:33,589] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2025-03-04 20:50:33,589] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-03-04 20:50:33,589] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-03-04 20:50:33,589] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2025-03-04 20:50:33,589] [INFO] [config.py:964:print]   optimizer_name ............... adamw
[2025-03-04 20:50:33,589] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 2e-05, 'weight_decay': 0.0, 'betas': (0.9, 0.95)}
[2025-03-04 20:50:33,589] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2025-03-04 20:50:33,589] [INFO] [config.py:964:print]   pld_enabled .................. False
[2025-03-04 20:50:33,589] [INFO] [config.py:964:print]   pld_params ................... False
[2025-03-04 20:50:33,589] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2025-03-04 20:50:33,589] [INFO] [config.py:964:print]   scheduler_name ............... WarmupDecayLR
[2025-03-04 20:50:33,589] [INFO] [config.py:964:print]   scheduler_params ............. {'total_num_steps': 25000, 'warmup_min_lr': 0, 'warmup_max_lr': 2e-05, 'warmup_num_steps': 100, 'warmup_type': 'linear'}
[2025-03-04 20:50:33,589] [INFO] [config.py:964:print]   sparse_attention ............. None
[2025-03-04 20:50:33,589] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2025-03-04 20:50:33,589] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2025-03-04 20:50:33,590] [INFO] [config.py:964:print]   train_batch_size ............. 10
[2025-03-04 20:50:33,590] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2025-03-04 20:50:33,590] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2025-03-04 20:50:33,590] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2025-03-04 20:50:33,590] [INFO] [config.py:964:print]   world_size ................... 1
[2025-03-04 20:50:33,590] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2025-03-04 20:50:33,590] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2025-03-04 20:50:33,590] [INFO] [config.py:964:print]   zero_enabled ................. True
[2025-03-04 20:50:33,590] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-04 20:50:33,590] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2025-03-04 20:50:33,590] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1,
    "gradient_accumulation_steps": 10,
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 2e-05,
            "weight_decay": 0.0,
            "betas": [0.9, 0.95]
        }
    },
    "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "total_num_steps": 2.500000e+04,
            "warmup_min_lr": 0,
            "warmup_max_lr": 2e-05,
            "warmup_num_steps": 100,
            "warmup_type": "linear"
        }
    },
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "gradient_clipping": 1.0,
    "zero_optimization": {
        "stage": 2,
        "contiguous_gradients": true,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 5.000000e+08,
        "allgather_bucket_size": 5.000000e+08
    }
}
(train) >> AFTER DEEPSPEED
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)
  return F.conv2d(input, weight, bias, self.stride,
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([7, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([5, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([4, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([8, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([1, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([2, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([3, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][  1/500]	Time 12.639 (12.639)	Loss 8.3256 (8.5577)	CeLoss 3.4375 (3.1844)	SegCLSLoss 0.9336 (0.8668)	KLLoss 0.0354 (0.0730)	MaskLoss 1.2113 (1.6057)	MaskBCELoss 0.2286 (0.7786)	MaskDICELoss 0.9827 (0.8271)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([6, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][  2/500]	Time  6.747 ( 6.747)	Loss 9.3156 (5.3598)	CeLoss 3.9688 (2.4805)	SegCLSLoss 0.9805 (0.4898)	KLLoss 0.0312 (0.0364)	MaskLoss 1.4992 (0.8586)	MaskBCELoss 0.5905 (0.4174)	MaskDICELoss 0.9086 (0.4412)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([15, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([16, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([9, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][  3/500]	Time 10.269 (10.269)	Loss 8.4776 (13.1557)	CeLoss 3.0938 (3.3172)	SegCLSLoss 0.9492 (0.9793)	KLLoss 0.1582 (0.1086)	MaskLoss 1.4196 (3.7444)	MaskBCELoss 0.4597 (2.8704)	MaskDICELoss 0.9598 (0.8740)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([20, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][  4/500]	Time  8.062 ( 8.062)	Loss 8.1054 (7.6672)	CeLoss 3.1094 (2.5906)	SegCLSLoss 0.9492 (0.5742)	KLLoss 0.1748 (0.0721)	MaskLoss 1.1979 (1.7893)	MaskBCELoss 0.2181 (1.2184)	MaskDICELoss 0.9798 (0.5709)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([14, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][  5/500]	Time  7.524 ( 7.524)	Loss 29.6274 (10.3323)	CeLoss 2.4062 (2.1566)	SegCLSLoss 1.0156 (0.5973)	KLLoss 0.1729 (0.0902)	MaskLoss 12.3387 (3.3391)	MaskBCELoss 11.4028 (2.7827)	MaskDICELoss 0.9359 (0.5565)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([10, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][  6/500]	Time  8.577 ( 8.577)	Loss 19.1440 (7.3827)	CeLoss 2.6562 (2.4641)	SegCLSLoss 1.0078 (0.5926)	KLLoss 0.2188 (0.0964)	MaskLoss 6.9912 (1.7100)	MaskBCELoss 6.0979 (1.1576)	MaskDICELoss 0.8933 (0.5524)
Epoch: [0][  7/500]	Time 10.036 (10.036)	Loss 17.8913 (9.5383)	CeLoss 2.8750 (3.2547)	SegCLSLoss 1.0078 (0.8816)	KLLoss 0.1235 (0.0997)	MaskLoss 6.4528 (2.0838)	MaskBCELoss 5.7100 (1.2953)	MaskDICELoss 0.7428 (0.7885)
Epoch: [0][  8/500]	Time 10.122 (10.122)	Loss 1.1484 (4.9314)	CeLoss 1.1484 (2.3117)	SegCLSLoss 0.0000 (0.4730)	KLLoss 0.0000 (0.0464)	MaskLoss 0.0000 (0.7144)	MaskBCELoss 0.0000 (0.2611)	MaskDICELoss 0.0000 (0.4533)
Epoch: [0][  9/500]	Time 10.359 (10.359)	Loss 8.9386 (7.9766)	CeLoss 3.8438 (2.8633)	SegCLSLoss 0.9727 (0.6855)	KLLoss 0.1396 (0.0710)	MaskLoss 1.2526 (1.7061)	MaskBCELoss 0.2703 (1.0618)	MaskDICELoss 0.9823 (0.6443)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([11, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
[2025-03-04 20:52:09,533] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[3.4000000000000005e-06], mom=[(0.9, 0.95)]
[2025-03-04 20:52:09,538] [INFO] [timer.py:215:stop] epoch=0/micro_step=100/global_step=10, RunningAvgSamplesPerSec=1.0489462026165153, CurrSamplesPerSec=0.8823786193572096, MemAllocated=57.26GB, MaxMemAllocated=61.9GB
Epoch: [0][ 10/500]	Time 11.335 (11.335)	Loss 8.0483 (11.0367)	CeLoss 3.3594 (3.0648)	SegCLSLoss 0.9570 (0.8770)	KLLoss 0.0388 (0.1052)	MaskLoss 1.0945 (2.8958)	MaskBCELoss 0.1023 (2.0767)	MaskDICELoss 0.9922 (0.8191)
Epoch: [0][ 11/500]	Time  9.822 ( 9.822)	Loss 8.7214 (7.1712)	CeLoss 3.5000 (2.6336)	SegCLSLoss 0.9297 (0.7789)	KLLoss 0.0001 (0.1196)	MaskLoss 1.3986 (1.2454)	MaskBCELoss 0.4209 (0.4775)	MaskDICELoss 0.9777 (0.7679)
Epoch: [0][ 12/500]	Time  8.942 ( 8.942)	Loss 9.3509 (7.7647)	CeLoss 4.0625 (2.8988)	SegCLSLoss 0.9883 (0.7762)	KLLoss 0.0610 (0.0770)	MaskLoss 1.4120 (1.4797)	MaskBCELoss 0.4611 (0.7609)	MaskDICELoss 0.9509 (0.7188)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([12, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][ 13/500]	Time 10.399 (10.399)	Loss 8.7025 (6.5512)	CeLoss 4.1250 (2.6922)	SegCLSLoss 0.9453 (0.6828)	KLLoss 0.0309 (0.0799)	MaskLoss 1.1596 (1.0754)	MaskBCELoss 0.2805 (0.4327)	MaskDICELoss 0.8791 (0.6427)
Epoch: [0][ 14/500]	Time  7.857 ( 7.857)	Loss 1.2344 (4.6138)	CeLoss 1.2344 (2.0961)	SegCLSLoss 0.0000 (0.4816)	KLLoss 0.0000 (0.0655)	MaskLoss 0.0000 (0.6204)	MaskBCELoss 0.0000 (0.1343)	MaskDICELoss 0.0000 (0.4861)
Epoch: [0][ 15/500]	Time 11.965 (11.965)	Loss 8.9415 (8.8607)	CeLoss 2.9688 (3.4125)	SegCLSLoss 1.0078 (0.9762)	KLLoss 0.1533 (0.1153)	MaskLoss 1.6665 (1.4816)	MaskBCELoss 0.6747 (0.5414)	MaskDICELoss 0.9918 (0.9402)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([17, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][ 16/500]	Time  6.666 ( 6.666)	Loss 6.9370 (5.7590)	CeLoss 1.8906 (2.0422)	SegCLSLoss 0.9414 (0.5863)	KLLoss 0.1836 (0.0959)	MaskLoss 1.1993 (1.0884)	MaskBCELoss 0.2036 (0.5133)	MaskDICELoss 0.9957 (0.5751)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([13, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][ 17/500]	Time  9.482 ( 9.482)	Loss 7.2553 (7.8909)	CeLoss 2.1719 (2.9906)	SegCLSLoss 1.0156 (0.8859)	KLLoss 0.1846 (0.1123)	MaskLoss 1.2144 (1.3298)	MaskBCELoss 0.2386 (0.4876)	MaskDICELoss 0.9758 (0.8422)
Epoch: [0][ 18/500]	Time  9.747 ( 9.747)	Loss 10.1812 (7.2313)	CeLoss 3.7344 (2.6246)	SegCLSLoss 0.9492 (0.7848)	KLLoss 0.0530 (0.1064)	MaskLoss 2.0009 (1.2863)	MaskBCELoss 1.0518 (0.5181)	MaskDICELoss 0.9491 (0.7682)
Epoch: [0][ 19/500]	Time  9.068 ( 9.068)	Loss 0.9453 (6.2313)	CeLoss 0.9453 (2.4977)	SegCLSLoss 0.0000 (0.6832)	KLLoss 0.0000 (0.0793)	MaskLoss 0.0000 (1.0206)	MaskBCELoss 0.0000 (0.3830)	MaskDICELoss 0.0000 (0.6376)
[2025-03-04 20:53:41,186] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[7.4e-06], mom=[(0.9, 0.95)]
[2025-03-04 20:53:41,192] [INFO] [timer.py:215:stop] epoch=0/micro_step=200/global_step=20, RunningAvgSamplesPerSec=1.0720240134831227, CurrSamplesPerSec=1.298103770024871, MemAllocated=56.71GB, MaxMemAllocated=61.98GB
Epoch: [0][ 20/500]	Time  7.706 ( 7.706)	Loss 1.0703 (6.3004)	CeLoss 1.0703 (2.7020)	SegCLSLoss 0.0000 (0.6691)	KLLoss 0.0000 (0.0587)	MaskLoss 0.0000 (0.9405)	MaskBCELoss 0.0000 (0.2795)	MaskDICELoss 0.0000 (0.6610)
Epoch: [0][ 21/500]	Time  8.965 ( 8.965)	Loss 9.1110 (5.1746)	CeLoss 3.9688 (2.4082)	SegCLSLoss 0.9727 (0.4813)	KLLoss 0.0393 (0.0485)	MaskLoss 1.3680 (0.7666)	MaskBCELoss 0.4305 (0.2960)	MaskDICELoss 0.9375 (0.4706)
Epoch: [0][ 22/500]	Time  8.723 ( 8.723)	Loss 8.1737 (5.9901)	CeLoss 3.6875 (2.4676)	SegCLSLoss 1.0234 (0.6816)	KLLoss 0.0723 (0.0760)	MaskLoss 1.0050 (0.8721)	MaskBCELoss 0.0482 (0.1915)	MaskDICELoss 0.9568 (0.6806)
Epoch: [0][ 23/500]	Time 10.395 (10.395)	Loss 10.1117 (7.6625)	CeLoss 3.7031 (3.0605)	SegCLSLoss 0.9414 (0.8805)	KLLoss 0.0952 (0.0898)	MaskLoss 2.0347 (1.1935)	MaskBCELoss 1.1542 (0.3516)	MaskDICELoss 0.8805 (0.8419)
Epoch: [0][ 24/500]	Time 10.005 (10.005)	Loss 1.3516 (6.7860)	CeLoss 1.3516 (2.4219)	SegCLSLoss 0.0000 (0.6883)	KLLoss 0.0000 (0.0830)	MaskLoss 0.0000 (1.2843)	MaskBCELoss 0.0000 (0.6014)	MaskDICELoss 0.0000 (0.6829)
Epoch: [0][ 25/500]	Time  9.166 ( 9.166)	Loss 7.3247 (7.2358)	CeLoss 2.5469 (2.7266)	SegCLSLoss 0.9492 (0.7816)	KLLoss 0.1226 (0.1014)	MaskLoss 1.1099 (1.2999)	MaskBCELoss 0.1278 (0.5912)	MaskDICELoss 0.9821 (0.7087)
Epoch: [0][ 26/500]	Time  9.795 ( 9.795)	Loss 7.7828 (6.4596)	CeLoss 2.9844 (2.4832)	SegCLSLoss 1.0078 (0.7836)	KLLoss 0.1465 (0.1082)	MaskLoss 1.1238 (0.9650)	MaskBCELoss 0.1687 (0.1918)	MaskDICELoss 0.9551 (0.7732)
Epoch: [0][ 27/500]	Time 10.253 (10.253)	Loss 8.6707 (5.9237)	CeLoss 3.2969 (2.3719)	SegCLSLoss 1.0000 (0.5875)	KLLoss 0.0337 (0.0483)	MaskLoss 1.4289 (1.0268)	MaskBCELoss 0.4366 (0.4487)	MaskDICELoss 0.9923 (0.5781)
Epoch: [0][ 28/500]	Time  8.749 ( 8.749)	Loss 7.8442 (5.3958)	CeLoss 2.9062 (1.8789)	SegCLSLoss 0.9844 (0.5973)	KLLoss 0.2188 (0.1109)	MaskLoss 1.1327 (0.9734)	MaskBCELoss 0.1558 (0.3923)	MaskDICELoss 0.9769 (0.5811)
Epoch: [0][ 29/500]	Time  9.945 ( 9.945)	Loss 7.4790 (6.0404)	CeLoss 2.5938 (2.4172)	SegCLSLoss 0.9727 (0.6785)	KLLoss 0.1348 (0.0774)	MaskLoss 1.1381 (0.9288)	MaskBCELoss 0.1460 (0.2547)	MaskDICELoss 0.9920 (0.6742)
[2025-03-04 20:55:16,517] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[1.14e-05], mom=[(0.9, 0.95)]
[2025-03-04 20:55:16,522] [INFO] [timer.py:215:stop] epoch=0/micro_step=300/global_step=30, RunningAvgSamplesPerSec=1.0637411400844146, CurrSamplesPerSec=1.0718676573985797, MemAllocated=56.74GB, MaxMemAllocated=61.98GB
Epoch: [0][ 30/500]	Time  9.332 ( 9.332)	Loss 1.4766 (6.9060)	CeLoss 1.4766 (2.7930)	SegCLSLoss 0.0000 (0.7762)	KLLoss 0.0000 (0.0757)	MaskLoss 0.0000 (1.0694)	MaskBCELoss 0.0000 (0.3143)	MaskDICELoss 0.0000 (0.7551)
Epoch: [0][ 31/500]	Time  9.721 ( 9.721)	Loss 1.0156 (5.9782)	CeLoss 1.0156 (2.3977)	SegCLSLoss 0.0000 (0.6816)	KLLoss 0.0000 (0.0984)	MaskLoss 0.0000 (0.9010)	MaskBCELoss 0.0000 (0.2312)	MaskDICELoss 0.0000 (0.6698)
Epoch: [0][ 32/500]	Time 10.101 (10.101)	Loss 9.3053 (6.7069)	CeLoss 3.5469 (2.5680)	SegCLSLoss 0.9453 (0.7699)	KLLoss 0.0913 (0.0947)	MaskLoss 1.6154 (1.0721)	MaskBCELoss 0.6407 (0.3138)	MaskDICELoss 0.9747 (0.7583)
Epoch: [0][ 33/500]	Time  9.006 ( 9.006)	Loss 1.0625 (6.6561)	CeLoss 1.0625 (2.2992)	SegCLSLoss 0.0000 (0.7840)	KLLoss 0.0000 (0.1215)	MaskLoss 0.0000 (1.1532)	MaskBCELoss 0.0000 (0.3850)	MaskDICELoss 0.0000 (0.7682)
Epoch: [0][ 34/500]	Time  8.898 ( 8.898)	Loss 0.9336 (5.9137)	CeLoss 0.9336 (2.2277)	SegCLSLoss 0.0000 (0.6793)	KLLoss 0.0000 (0.1049)	MaskLoss 0.0000 (0.9654)	MaskBCELoss 0.0000 (0.3102)	MaskDICELoss 0.0000 (0.6553)
Epoch: [0][ 35/500]	Time  9.229 ( 9.229)	Loss 8.1228 (6.8606)	CeLoss 3.0625 (2.5320)	SegCLSLoss 0.9766 (0.7852)	KLLoss 0.0894 (0.0835)	MaskLoss 1.3762 (1.2208)	MaskBCELoss 0.5113 (0.5155)	MaskDICELoss 0.8649 (0.7052)
Epoch: [0][ 36/500]	Time  9.701 ( 9.701)	Loss 0.8984 (6.0384)	CeLoss 0.8984 (2.2117)	SegCLSLoss 0.0000 (0.6930)	KLLoss 0.0000 (0.0992)	MaskLoss 0.0000 (1.0366)	MaskBCELoss 0.0000 (0.3817)	MaskDICELoss 0.0000 (0.6549)
Epoch: [0][ 37/500]	Time 10.664 (10.664)	Loss 7.4995 (6.4447)	CeLoss 2.5312 (2.4227)	SegCLSLoss 0.9766 (0.7828)	KLLoss 0.1196 (0.1121)	MaskLoss 1.2098 (1.0313)	MaskBCELoss 0.2402 (0.3032)	MaskDICELoss 0.9696 (0.7281)
Epoch: [0][ 38/500]	Time  9.335 ( 9.335)	Loss 8.0637 (5.0844)	CeLoss 3.1094 (2.1098)	SegCLSLoss 1.0156 (0.5762)	KLLoss 0.1040 (0.0721)	MaskLoss 1.1751 (0.7436)	MaskBCELoss 0.1856 (0.1787)	MaskDICELoss 0.9895 (0.5649)
Epoch: [0][ 39/500]	Time  9.707 ( 9.707)	Loss 0.8477 (6.4007)	CeLoss 0.8477 (2.3379)	SegCLSLoss 0.0000 (0.7781)	KLLoss 0.0000 (0.1075)	MaskLoss 0.0000 (1.0282)	MaskBCELoss 0.0000 (0.2741)	MaskDICELoss 0.0000 (0.7540)
[2025-03-04 20:56:53,174] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[1.54e-05], mom=[(0.9, 0.95)]
[2025-03-04 20:56:53,180] [INFO] [timer.py:215:stop] epoch=0/micro_step=400/global_step=40, RunningAvgSamplesPerSec=1.055949933193409, CurrSamplesPerSec=0.971353179399449, MemAllocated=57.7GB, MaxMemAllocated=61.98GB
Epoch: [0][ 40/500]	Time 10.297 (10.297)	Loss 1.0469 (6.3577)	CeLoss 1.0469 (2.3008)	SegCLSLoss 0.0000 (0.7832)	KLLoss 0.0000 (0.0962)	MaskLoss 0.0000 (1.0800)	MaskBCELoss 0.0000 (0.3756)	MaskDICELoss 0.0000 (0.7044)
Epoch: [0][ 41/500]	Time 10.940 (10.940)	Loss 1.3125 (6.2846)	CeLoss 1.3125 (2.2879)	SegCLSLoss 0.0000 (0.7715)	KLLoss 0.0000 (0.0924)	MaskLoss 0.0000 (1.0278)	MaskBCELoss 0.0000 (0.2956)	MaskDICELoss 0.0000 (0.7322)
Epoch: [0][ 42/500]	Time 10.674 (10.674)	Loss 7.2615 (6.4773)	CeLoss 2.2656 (2.5156)	SegCLSLoss 0.9531 (0.7688)	KLLoss 0.0552 (0.0710)	MaskLoss 1.3525 (1.0279)	MaskBCELoss 0.4805 (0.3043)	MaskDICELoss 0.8720 (0.7236)
Epoch: [0][ 43/500]	Time  9.081 ( 9.081)	Loss 7.5742 (5.1109)	CeLoss 2.7188 (2.1582)	SegCLSLoss 0.9570 (0.5762)	KLLoss 0.0003 (0.0504)	MaskLoss 1.3632 (0.7765)	MaskBCELoss 0.5409 (0.2462)	MaskDICELoss 0.8223 (0.5303)
Epoch: [0][ 44/500]	Time 11.703 (11.703)	Loss 7.4899 (6.9262)	CeLoss 2.5625 (2.4813)	SegCLSLoss 0.9414 (0.8727)	KLLoss 0.0004 (0.0935)	MaskLoss 1.3413 (1.0980)	MaskBCELoss 0.4533 (0.2384)	MaskDICELoss 0.8880 (0.8596)
Epoch: [0][ 45/500]	Time  8.474 ( 8.474)	Loss 1.1719 (4.8444)	CeLoss 1.1719 (1.8520)	SegCLSLoss 0.0000 (0.5926)	KLLoss 0.0000 (0.0721)	MaskLoss 0.0000 (0.7737)	MaskBCELoss 0.0000 (0.2368)	MaskDICELoss 0.0000 (0.5370)
Epoch: [0][ 46/500]	Time 10.125 (10.125)	Loss 7.2765 (6.7450)	CeLoss 2.3750 (2.3312)	SegCLSLoss 0.9492 (0.8805)	KLLoss 0.0938 (0.1159)	MaskLoss 1.2430 (1.1450)	MaskBCELoss 0.3166 (0.3593)	MaskDICELoss 0.9264 (0.7857)
Epoch: [0][ 47/500]	Time  9.215 ( 9.215)	Loss 1.2578 (5.6577)	CeLoss 1.2578 (2.2188)	SegCLSLoss 0.0000 (0.6617)	KLLoss 0.0000 (0.0597)	MaskLoss 0.0000 (0.8966)	MaskBCELoss 0.0000 (0.2675)	MaskDICELoss 0.0000 (0.6291)
Epoch: [0][ 48/500]	Time  9.109 ( 9.109)	Loss 7.0323 (6.0053)	CeLoss 2.2500 (2.1316)	SegCLSLoss 0.9609 (0.7738)	KLLoss 0.1108 (0.0647)	MaskLoss 1.0991 (1.0047)	MaskBCELoss 0.1039 (0.2963)	MaskDICELoss 0.9952 (0.7084)
Epoch: [0][ 49/500]	Time  9.028 ( 9.028)	Loss 6.6513 (5.3692)	CeLoss 1.6641 (1.8621)	SegCLSLoss 0.9453 (0.6754)	KLLoss 0.2168 (0.0843)	MaskLoss 1.1595 (0.8921)	MaskBCELoss 0.1730 (0.2411)	MaskDICELoss 0.9865 (0.6509)
[2025-03-04 20:58:31,160] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[1.94e-05], mom=[(0.9, 0.95)]
[2025-03-04 20:58:31,166] [INFO] [timer.py:215:stop] epoch=0/micro_step=500/global_step=50, RunningAvgSamplesPerSec=1.0484104570809212, CurrSamplesPerSec=1.038030518211673, MemAllocated=57.88GB, MaxMemAllocated=61.98GB
Epoch: [0][ 50/500]	Time  9.635 ( 9.635)	Loss 0.9414 (4.8371)	CeLoss 0.9414 (1.8074)	SegCLSLoss 0.0000 (0.5789)	KLLoss 0.0000 (0.0607)	MaskLoss 0.0000 (0.7974)	MaskBCELoss 0.0000 (0.2561)	MaskDICELoss 0.0000 (0.5413)
Epoch: [0][ 51/500]	Time  9.832 ( 9.832)	Loss 0.6836 (5.2052)	CeLoss 0.6836 (1.7941)	SegCLSLoss 0.0000 (0.6852)	KLLoss 0.0000 (0.0790)	MaskLoss 0.0000 (0.8450)	MaskBCELoss 0.0000 (0.1957)	MaskDICELoss 0.0000 (0.6493)
Epoch: [0][ 52/500]	Time 10.443 (10.443)	Loss 1.1641 (6.0969)	CeLoss 1.1641 (2.0523)	SegCLSLoss 0.0000 (0.7809)	KLLoss 0.0000 (0.0747)	MaskLoss 0.0000 (1.0804)	MaskBCELoss 0.0000 (0.3736)	MaskDICELoss 0.0000 (0.7067)
Epoch: [0][ 53/500]	Time 10.665 (10.665)	Loss 6.6570 (5.6750)	CeLoss 1.9375 (1.8422)	SegCLSLoss 0.9531 (0.7758)	KLLoss 0.0864 (0.1036)	MaskLoss 1.0895 (0.9184)	MaskBCELoss 0.1004 (0.1652)	MaskDICELoss 0.9890 (0.7531)
Epoch: [0][ 54/500]	Time  9.172 ( 9.172)	Loss 0.8867 (5.0945)	CeLoss 0.8867 (1.6168)	SegCLSLoss 0.0000 (0.6785)	KLLoss 0.0000 (0.0931)	MaskLoss 0.0000 (0.8712)	MaskBCELoss 0.0000 (0.2200)	MaskDICELoss 0.0000 (0.6512)
Epoch: [0][ 55/500]	Time  9.052 ( 9.052)	Loss 1.1719 (5.4165)	CeLoss 1.1719 (1.9258)	SegCLSLoss 0.0000 (0.6750)	KLLoss 0.0000 (0.0559)	MaskLoss 0.0000 (0.9153)	MaskBCELoss 0.0000 (0.2821)	MaskDICELoss 0.0000 (0.6332)
Epoch: [0][ 56/500]	Time 10.112 (10.112)	Loss 6.1975 (5.1112)	CeLoss 1.3203 (1.6816)	SegCLSLoss 1.0000 (0.6762)	KLLoss 0.1553 (0.0658)	MaskLoss 1.2635 (0.8833)	MaskBCELoss 0.4165 (0.2527)	MaskDICELoss 0.8470 (0.6306)
Epoch: [0][ 57/500]	Time 10.480 (10.480)	Loss 6.6157 (4.9998)	CeLoss 1.7969 (1.6316)	SegCLSLoss 0.9844 (0.6848)	KLLoss 0.0693 (0.0859)	MaskLoss 1.1966 (0.8254)	MaskBCELoss 0.2573 (0.1792)	MaskDICELoss 0.9394 (0.6462)
Epoch: [0][ 58/500]	Time  8.929 ( 8.929)	Loss 6.4755 (6.1980)	CeLoss 1.6406 (1.7535)	SegCLSLoss 0.9961 (0.8719)	KLLoss 0.1943 (0.1121)	MaskLoss 1.2413 (1.1403)	MaskBCELoss 0.4089 (0.3314)	MaskDICELoss 0.8324 (0.8089)
Epoch: [0][ 59/500]	Time  9.171 ( 9.171)	Loss 6.2982 (5.1192)	CeLoss 1.4141 (1.6664)	SegCLSLoss 1.0000 (0.6898)	KLLoss 0.1797 (0.0894)	MaskLoss 1.1923 (0.8732)	MaskBCELoss 0.2823 (0.2380)	MaskDICELoss 0.9100 (0.6352)
[2025-03-04 21:00:08,875] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[1.9986345381526108e-05], mom=[(0.9, 0.95)]
[2025-03-04 21:00:08,881] [INFO] [timer.py:215:stop] epoch=0/micro_step=600/global_step=60, RunningAvgSamplesPerSec=1.044034881181325, CurrSamplesPerSec=1.0144745152552823, MemAllocated=56.8GB, MaxMemAllocated=61.98GB
Epoch: [0][ 60/500]	Time  9.859 ( 9.859)	Loss 6.7225 (6.0850)	CeLoss 1.5547 (1.6273)	SegCLSLoss 0.9609 (0.8641)	KLLoss 0.0369 (0.1143)	MaskLoss 1.4879 (1.1268)	MaskBCELoss 0.6457 (0.2970)	MaskDICELoss 0.8421 (0.8298)
Epoch: [0][ 61/500]	Time  7.099 ( 7.099)	Loss 6.0423 (4.7731)	CeLoss 1.1328 (1.4375)	SegCLSLoss 1.0000 (0.6891)	KLLoss 0.1924 (0.0854)	MaskLoss 1.1697 (0.7932)	MaskBCELoss 0.2323 (0.1312)	MaskDICELoss 0.9374 (0.6621)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([24, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][ 62/500]	Time  8.319 ( 8.319)	Loss 6.2560 (4.1829)	CeLoss 1.2266 (1.2070)	SegCLSLoss 1.0000 (0.5887)	KLLoss 0.1758 (0.1036)	MaskLoss 1.2891 (0.7504)	MaskBCELoss 0.4033 (0.2126)	MaskDICELoss 0.8858 (0.5379)
Epoch: [0][ 63/500]	Time 10.413 (10.413)	Loss 6.2690 (5.2402)	CeLoss 1.6328 (1.3875)	SegCLSLoss 0.9570 (0.7789)	KLLoss 0.0260 (0.1087)	MaskLoss 1.1272 (0.9318)	MaskBCELoss 0.1901 (0.1857)	MaskDICELoss 0.9370 (0.7461)
Epoch: [0][ 64/500]	Time 10.600 (10.600)	Loss 6.3439 (5.9077)	CeLoss 1.4453 (1.5305)	SegCLSLoss 0.9805 (0.8770)	KLLoss 0.1309 (0.1059)	MaskLoss 1.1415 (1.1024)	MaskBCELoss 0.1424 (0.2881)	MaskDICELoss 0.9992 (0.8143)
Epoch: [0][ 65/500]	Time  9.095 ( 9.095)	Loss 6.0906 (4.7284)	CeLoss 1.2422 (1.3070)	SegCLSLoss 0.9922 (0.6754)	KLLoss 0.1055 (0.0816)	MaskLoss 1.2726 (0.8556)	MaskBCELoss 0.4217 (0.2106)	MaskDICELoss 0.8509 (0.6450)
Epoch: [0][ 66/500]	Time  8.415 ( 8.415)	Loss 6.1550 (4.2087)	CeLoss 1.2969 (1.2941)	SegCLSLoss 0.9414 (0.5766)	KLLoss 0.1738 (0.0915)	MaskLoss 1.1100 (0.7124)	MaskBCELoss 0.1113 (0.1569)	MaskDICELoss 0.9987 (0.5555)
Epoch: [0][ 67/500]	Time 10.559 (10.559)	Loss 6.1807 (5.6041)	CeLoss 1.4688 (1.2574)	SegCLSLoss 0.9492 (0.8770)	KLLoss 0.0640 (0.1194)	MaskLoss 1.1598 (1.0440)	MaskBCELoss 0.2370 (0.1943)	MaskDICELoss 0.9228 (0.8497)
Epoch: [0][ 68/500]	Time  6.767 ( 6.767)	Loss 6.3313 (3.8425)	CeLoss 1.5234 (1.3918)	SegCLSLoss 0.9883 (0.4754)	KLLoss 0.1455 (0.0437)	MaskLoss 1.1804 (0.6427)	MaskBCELoss 0.2732 (0.2003)	MaskDICELoss 0.9072 (0.4424)
Epoch: [0][ 69/500]	Time  9.287 ( 9.287)	Loss 0.9805 (4.7662)	CeLoss 0.9805 (1.3371)	SegCLSLoss 0.0000 (0.6758)	KLLoss 0.0000 (0.0804)	MaskLoss 0.0000 (0.8663)	MaskBCELoss 0.0000 (0.2274)	MaskDICELoss 0.0000 (0.6389)
[2025-03-04 21:01:40,454] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[1.9970281124497996e-05], mom=[(0.9, 0.95)]
[2025-03-04 21:01:40,459] [INFO] [timer.py:215:stop] epoch=0/micro_step=700/global_step=70, RunningAvgSamplesPerSec=1.0508424856621732, CurrSamplesPerSec=0.9073015892656133, MemAllocated=56.73GB, MaxMemAllocated=62.75GB
Epoch: [0][ 70/500]	Time 11.024 (11.024)	Loss 0.7344 (5.0970)	CeLoss 0.7344 (1.2188)	SegCLSLoss 0.0000 (0.7691)	KLLoss 0.0000 (0.0978)	MaskLoss 0.0000 (0.9559)	MaskBCELoss 0.0000 (0.2137)	MaskDICELoss 0.0000 (0.7422)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([19, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][ 71/500]	Time 10.316 (10.316)	Loss 0.8945 (4.6580)	CeLoss 0.8945 (1.2160)	SegCLSLoss 0.0000 (0.6809)	KLLoss 0.0000 (0.1077)	MaskLoss 0.0000 (0.8448)	MaskBCELoss 0.0000 (0.1921)	MaskDICELoss 0.0000 (0.6527)
Epoch: [0][ 72/500]	Time  9.750 ( 9.750)	Loss 6.2714 (5.6235)	CeLoss 1.4375 (1.2859)	SegCLSLoss 0.9609 (0.8703)	KLLoss 0.0113 (0.1023)	MaskLoss 1.2913 (1.0476)	MaskBCELoss 0.4117 (0.1956)	MaskDICELoss 0.8796 (0.8520)
Epoch: [0][ 73/500]	Time 10.563 (10.563)	Loss 6.2685 (5.6042)	CeLoss 1.3047 (1.2387)	SegCLSLoss 0.9492 (0.8609)	KLLoss 0.0776 (0.1068)	MaskLoss 1.3364 (1.0970)	MaskBCELoss 0.4683 (0.2797)	MaskDICELoss 0.8681 (0.8173)
Epoch: [0][ 74/500]	Time  7.822 ( 7.822)	Loss 1.0156 (2.9633)	CeLoss 1.0156 (1.0855)	SegCLSLoss 0.0000 (0.3812)	KLLoss 0.0000 (0.0421)	MaskLoss 0.0000 (0.4473)	MaskBCELoss 0.0000 (0.0713)	MaskDICELoss 0.0000 (0.3760)
Epoch: [0][ 75/500]	Time  8.691 ( 8.691)	Loss 5.7918 (5.1524)	CeLoss 1.0469 (1.2797)	SegCLSLoss 0.9336 (0.7637)	KLLoss 0.1865 (0.1058)	MaskLoss 1.0471 (0.9322)	MaskBCELoss 0.0498 (0.1719)	MaskDICELoss 0.9973 (0.7602)
Epoch: [0][ 76/500]	Time  7.203 ( 7.203)	Loss 0.8711 (3.9352)	CeLoss 0.8711 (1.0734)	SegCLSLoss 0.0000 (0.5715)	KLLoss 0.0000 (0.0460)	MaskLoss 0.0000 (0.7303)	MaskBCELoss 0.0000 (0.1962)	MaskDICELoss 0.0000 (0.5341)
Epoch: [0][ 77/500]	Time  8.504 ( 8.504)	Loss 5.8872 (4.5574)	CeLoss 1.0234 (1.1676)	SegCLSLoss 0.8867 (0.6609)	KLLoss 0.0796 (0.0655)	MaskLoss 1.2442 (0.8786)	MaskBCELoss 0.3182 (0.2598)	MaskDICELoss 0.9260 (0.6188)
Epoch: [0][ 78/500]	Time  8.687 ( 8.687)	Loss 1.2188 (3.9990)	CeLoss 1.2188 (1.0742)	SegCLSLoss 0.0000 (0.5785)	KLLoss 0.0000 (0.0869)	MaskLoss 0.0000 (0.7218)	MaskBCELoss 0.0000 (0.1695)	MaskDICELoss 0.0000 (0.5523)
Epoch: [0][ 79/500]	Time 11.023 (11.023)	Loss 6.1046 (5.5928)	CeLoss 0.9727 (1.1707)	SegCLSLoss 0.9844 (0.8684)	KLLoss 0.1738 (0.1188)	MaskLoss 1.3007 (1.1169)	MaskBCELoss 0.3694 (0.2992)	MaskDICELoss 0.9313 (0.8178)
[2025-03-04 21:03:14,241] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[1.9954216867469884e-05], mom=[(0.9, 0.95)]
[2025-03-04 21:03:14,246] [INFO] [timer.py:215:stop] epoch=0/micro_step=800/global_step=80, RunningAvgSamplesPerSec=1.0528137226952576, CurrSamplesPerSec=0.8907477091773834, MemAllocated=57.26GB, MaxMemAllocated=62.75GB
Epoch: [0][ 80/500]	Time 11.228 (11.228)	Loss 6.4701 (4.8988)	CeLoss 1.4297 (1.2531)	SegCLSLoss 0.9766 (0.7688)	KLLoss 0.0532 (0.0625)	MaskLoss 1.2999 (0.8985)	MaskBCELoss 0.3529 (0.1975)	MaskDICELoss 0.9469 (0.7009)
Epoch: [0][ 81/500]	Time  8.075 ( 8.075)	Loss 0.9336 (3.9190)	CeLoss 0.9336 (0.9547)	SegCLSLoss 0.0000 (0.5777)	KLLoss 0.0000 (0.0965)	MaskLoss 0.0000 (0.7559)	MaskBCELoss 0.0000 (0.2232)	MaskDICELoss 0.0000 (0.5327)
Epoch: [0][ 82/500]	Time  9.926 ( 9.926)	Loss 4.1244 (4.7883)	CeLoss 1.1484 (1.1434)	SegCLSLoss 0.9336 (0.7609)	KLLoss 0.0574 (0.0964)	MaskLoss 0.7940 (0.9226)	MaskBCELoss 0.3618 (0.2614)	MaskDICELoss 0.4322 (0.6612)
Epoch: [0][ 83/500]	Time 10.442 (10.442)	Loss 5.4618 (4.8237)	CeLoss 0.7656 (1.0863)	SegCLSLoss 0.9805 (0.7719)	KLLoss 0.2314 (0.0998)	MaskLoss 1.0196 (0.8931)	MaskBCELoss 0.0545 (0.1608)	MaskDICELoss 0.9652 (0.7322)
Epoch: [0][ 84/500]	Time 10.091 (10.091)	Loss 5.5437 (4.8294)	CeLoss 1.0234 (1.0652)	SegCLSLoss 1.0000 (0.7691)	KLLoss 0.0262 (0.0763)	MaskLoss 1.3368 (1.0011)	MaskBCELoss 0.6752 (0.3503)	MaskDICELoss 0.6616 (0.6509)
Epoch: [0][ 85/500]	Time  6.612 ( 6.612)	Loss 6.1635 (2.4232)	CeLoss 1.2031 (0.9781)	SegCLSLoss 0.9844 (0.2898)	KLLoss 0.2266 (0.0485)	MaskLoss 1.1977 (0.3415)	MaskBCELoss 0.2746 (0.0570)	MaskDICELoss 0.9231 (0.2845)
Epoch: [0][ 86/500]	Time 10.152 (10.152)	Loss 5.7847 (4.2107)	CeLoss 1.1953 (0.9605)	SegCLSLoss 0.9648 (0.6711)	KLLoss 0.0874 (0.1062)	MaskLoss 1.0998 (0.7472)	MaskBCELoss 0.1901 (0.0904)	MaskDICELoss 0.9097 (0.6568)
Epoch: [0][ 87/500]	Time  7.338 ( 7.338)	Loss 1.0938 (2.9854)	CeLoss 1.0938 (1.0875)	SegCLSLoss 0.0000 (0.3867)	KLLoss 0.0000 (0.0558)	MaskLoss 0.0000 (0.4749)	MaskBCELoss 0.0000 (0.1253)	MaskDICELoss 0.0000 (0.3496)
Epoch: [0][ 88/500]	Time  9.723 ( 9.723)	Loss 5.9531 (4.8610)	CeLoss 1.1406 (1.1074)	SegCLSLoss 0.9492 (0.7602)	KLLoss 0.1650 (0.1083)	MaskLoss 1.0867 (0.9062)	MaskBCELoss 0.0875 (0.1795)	MaskDICELoss 0.9992 (0.7266)
Epoch: [0][ 89/500]	Time 10.471 (10.471)	Loss 5.6233 (5.1071)	CeLoss 0.8828 (1.0461)	SegCLSLoss 0.9727 (0.8484)	KLLoss 0.1484 (0.0917)	MaskLoss 1.0640 (0.9905)	MaskBCELoss 0.0742 (0.2082)	MaskDICELoss 0.9898 (0.7822)
[2025-03-04 21:04:45,087] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[1.9938152610441768e-05], mom=[(0.9, 0.95)]
[2025-03-04 21:04:45,092] [INFO] [timer.py:215:stop] epoch=0/micro_step=900/global_step=90, RunningAvgSamplesPerSec=1.0580704888226364, CurrSamplesPerSec=1.2478562392916095, MemAllocated=57.1GB, MaxMemAllocated=62.78GB
Epoch: [0][ 90/500]	Time  8.016 ( 8.016)	Loss 5.2816 (3.9254)	CeLoss 1.2266 (1.0527)	SegCLSLoss 0.9219 (0.5668)	KLLoss 0.0698 (0.0906)	MaskLoss 1.0212 (0.7179)	MaskBCELoss 0.2806 (0.1871)	MaskDICELoss 0.7407 (0.5308)
Epoch: [0][ 91/500]	Time  9.521 ( 9.521)	Loss 5.8099 (4.1737)	CeLoss 0.8711 (1.0613)	SegCLSLoss 0.9805 (0.6687)	KLLoss 0.2266 (0.0864)	MaskLoss 1.2272 (0.7692)	MaskBCELoss 0.3424 (0.1929)	MaskDICELoss 0.8848 (0.5763)
Epoch: [0][ 92/500]	Time  9.486 ( 9.486)	Loss 0.8633 (3.3610)	CeLoss 0.8633 (1.0602)	SegCLSLoss 0.0000 (0.4688)	KLLoss 0.0000 (0.0440)	MaskLoss 0.0000 (0.5821)	MaskBCELoss 0.0000 (0.1521)	MaskDICELoss 0.0000 (0.4300)
Epoch: [0][ 93/500]	Time  9.536 ( 9.536)	Loss 1.1016 (3.8388)	CeLoss 1.1016 (0.9625)	SegCLSLoss 0.0000 (0.5477)	KLLoss 0.0000 (0.0729)	MaskLoss 0.0000 (0.6919)	MaskBCELoss 0.0000 (0.1185)	MaskDICELoss 0.0000 (0.5734)
Epoch: [0][ 94/500]	Time 10.348 (10.348)	Loss 5.8116 (5.2978)	CeLoss 0.9727 (0.9586)	SegCLSLoss 0.9570 (0.8414)	KLLoss 0.1523 (0.1214)	MaskLoss 1.1649 (1.0627)	MaskBCELoss 0.2286 (0.2262)	MaskDICELoss 0.9363 (0.8366)
Epoch: [0][ 95/500]	Time 10.860 (10.860)	Loss 6.9370 (4.9703)	CeLoss 0.9766 (0.9934)	SegCLSLoss 0.9258 (0.7594)	KLLoss 0.1357 (0.1200)	MaskLoss 1.7210 (0.9969)	MaskBCELoss 0.7587 (0.2547)	MaskDICELoss 0.9623 (0.7422)
Epoch: [0][ 96/500]	Time  9.703 ( 9.703)	Loss 0.7188 (3.7336)	CeLoss 0.7188 (0.8379)	SegCLSLoss 0.0000 (0.5684)	KLLoss 0.0000 (0.0895)	MaskLoss 0.0000 (0.7065)	MaskBCELoss 0.0000 (0.1517)	MaskDICELoss 0.0000 (0.5548)
Epoch: [0][ 97/500]	Time  8.274 ( 8.274)	Loss 5.5859 (3.7479)	CeLoss 0.8984 (1.0145)	SegCLSLoss 0.9180 (0.5629)	KLLoss 0.1602 (0.0874)	MaskLoss 1.1373 (0.6391)	MaskBCELoss 0.2395 (0.0959)	MaskDICELoss 0.8978 (0.5432)
Epoch: [0][ 98/500]	Time  8.108 ( 8.108)	Loss 5.6281 (4.4370)	CeLoss 1.0312 (1.0293)	SegCLSLoss 0.8984 (0.6504)	KLLoss 0.1680 (0.0865)	MaskLoss 1.0417 (0.8592)	MaskBCELoss 0.0974 (0.2213)	MaskDICELoss 0.9442 (0.6379)
Epoch: [0][ 99/500]	Time 10.844 (10.844)	Loss 5.5418 (4.7856)	CeLoss 0.7422 (0.8562)	SegCLSLoss 0.9570 (0.7609)	KLLoss 0.1592 (0.1292)	MaskLoss 1.2805 (0.9857)	MaskBCELoss 0.4776 (0.2604)	MaskDICELoss 0.8029 (0.7253)
[2025-03-04 21:06:20,663] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[1.9922088353413656e-05], mom=[(0.9, 0.95)]
[2025-03-04 21:06:20,669] [INFO] [timer.py:215:stop] epoch=0/micro_step=1000/global_step=100, RunningAvgSamplesPerSec=1.056869481291728, CurrSamplesPerSec=1.1241346244739174, MemAllocated=57.25GB, MaxMemAllocated=62.78GB
Epoch: [0][100/500]	Time  8.897 ( 8.897)	Loss 5.6838 (4.5171)	CeLoss 0.9414 (1.0500)	SegCLSLoss 0.9180 (0.6527)	KLLoss 0.1777 (0.0720)	MaskLoss 1.1545 (0.8998)	MaskBCELoss 0.2562 (0.2651)	MaskDICELoss 0.8983 (0.6347)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([18, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][101/500]	Time  9.414 ( 9.414)	Loss 0.9648 (3.7696)	CeLoss 0.9648 (0.8613)	SegCLSLoss 0.0000 (0.5586)	KLLoss 0.0000 (0.0850)	MaskLoss 0.0000 (0.7111)	MaskBCELoss 0.0000 (0.1496)	MaskDICELoss 0.0000 (0.5614)
Epoch: [0][102/500]	Time  9.750 ( 9.750)	Loss 5.7313 (3.6668)	CeLoss 1.0234 (0.8613)	SegCLSLoss 0.9531 (0.5477)	KLLoss 0.1621 (0.0864)	MaskLoss 1.3353 (0.6741)	MaskBCELoss 0.6369 (0.1264)	MaskDICELoss 0.6984 (0.5478)
Epoch: [0][103/500]	Time  9.639 ( 9.639)	Loss 6.0688 (4.6893)	CeLoss 1.0078 (0.8980)	SegCLSLoss 0.9492 (0.7402)	KLLoss 0.1504 (0.1042)	MaskLoss 1.2880 (0.9033)	MaskBCELoss 0.3580 (0.1482)	MaskDICELoss 0.9300 (0.7551)
Epoch: [0][104/500]	Time  9.439 ( 9.439)	Loss 5.5115 (4.7484)	CeLoss 0.8320 (0.8953)	SegCLSLoss 0.9570 (0.7539)	KLLoss 0.1143 (0.1086)	MaskLoss 1.2055 (0.9692)	MaskBCELoss 0.3701 (0.2543)	MaskDICELoss 0.8354 (0.7149)
Epoch: [0][105/500]	Time  9.376 ( 9.376)	Loss 5.9161 (4.7322)	CeLoss 1.0234 (0.8660)	SegCLSLoss 0.9531 (0.7277)	KLLoss 0.1689 (0.0898)	MaskLoss 1.2563 (1.0107)	MaskBCELoss 0.3906 (0.3159)	MaskDICELoss 0.8658 (0.6948)
Epoch: [0][106/500]	Time 10.634 (10.634)	Loss 5.4727 (5.0848)	CeLoss 0.7930 (0.9176)	SegCLSLoss 0.8984 (0.8129)	KLLoss 0.1309 (0.0980)	MaskLoss 1.0598 (1.0211)	MaskBCELoss 0.0707 (0.2107)	MaskDICELoss 0.9891 (0.8104)
Epoch: [0][107/500]	Time  8.624 ( 8.624)	Loss 5.8692 (4.0169)	CeLoss 0.8672 (0.8977)	SegCLSLoss 0.8555 (0.6484)	KLLoss 0.0272 (0.0794)	MaskLoss 1.5596 (0.7752)	MaskBCELoss 0.8448 (0.1919)	MaskDICELoss 0.7148 (0.5833)
Epoch: [0][108/500]	Time 11.197 (11.197)	Loss 5.6561 (5.5616)	CeLoss 0.9375 (0.8398)	SegCLSLoss 0.9062 (0.9262)	KLLoss 0.0547 (0.1344)	MaskLoss 1.2024 (1.1579)	MaskBCELoss 0.2994 (0.2538)	MaskDICELoss 0.9030 (0.9041)
Epoch: [0][109/500]	Time  8.704 ( 8.704)	Loss 5.5367 (4.2122)	CeLoss 0.7031 (0.8215)	SegCLSLoss 0.9492 (0.6535)	KLLoss 0.1807 (0.0996)	MaskLoss 1.2065 (0.8475)	MaskBCELoss 0.3244 (0.2123)	MaskDICELoss 0.8822 (0.6352)
[2025-03-04 21:07:55,076] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[1.9906024096385544e-05], mom=[(0.9, 0.95)]
[2025-03-04 21:07:55,081] [INFO] [timer.py:215:stop] epoch=0/micro_step=1100/global_step=110, RunningAvgSamplesPerSec=1.0570970750191377, CurrSamplesPerSec=1.3101270856108183, MemAllocated=56.81GB, MaxMemAllocated=62.78GB
Epoch: [0][110/500]	Time  7.634 ( 7.634)	Loss 5.3358 (3.4748)	CeLoss 0.7266 (0.8328)	SegCLSLoss 0.9219 (0.5539)	KLLoss 0.0918 (0.0797)	MaskLoss 1.0321 (0.6522)	MaskBCELoss 0.0370 (0.1616)	MaskDICELoss 0.9951 (0.4906)
Epoch: [0][111/500]	Time  6.877 ( 6.877)	Loss 5.8531 (3.1908)	CeLoss 0.9453 (0.9184)	SegCLSLoss 0.9414 (0.4633)	KLLoss 0.1641 (0.0724)	MaskLoss 1.2482 (0.5577)	MaskBCELoss 0.3590 (0.1312)	MaskDICELoss 0.8892 (0.4266)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([21, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][112/500]	Time  8.946 ( 8.946)	Loss 5.4641 (4.5945)	CeLoss 0.6797 (0.8117)	SegCLSLoss 0.9062 (0.7395)	KLLoss 0.1377 (0.1193)	MaskLoss 1.1685 (0.9489)	MaskBCELoss 0.2417 (0.2509)	MaskDICELoss 0.9268 (0.6980)
Epoch: [0][113/500]	Time 11.574 (11.574)	Loss 4.7261 (4.9540)	CeLoss 0.6523 (0.8219)	SegCLSLoss 0.8945 (0.8160)	KLLoss 0.0947 (0.0980)	MaskLoss 0.9631 (1.0232)	MaskBCELoss 0.1609 (0.2337)	MaskDICELoss 0.8022 (0.7895)
Epoch: [0][114/500]	Time  8.937 ( 8.937)	Loss 5.1955 (4.7005)	CeLoss 0.6016 (0.9105)	SegCLSLoss 0.8906 (0.7367)	KLLoss 0.1406 (0.0847)	MaskLoss 1.0688 (0.9367)	MaskBCELoss 0.1337 (0.2051)	MaskDICELoss 0.9351 (0.7315)
Epoch: [0][115/500]	Time 10.310 (10.310)	Loss 4.7237 (4.9646)	CeLoss 0.6523 (0.7316)	SegCLSLoss 0.9375 (0.8348)	KLLoss 0.1689 (0.1423)	MaskLoss 0.9088 (1.1013)	MaskBCELoss 0.1042 (0.3658)	MaskDICELoss 0.8046 (0.7355)
Epoch: [0][116/500]	Time  7.363 ( 7.363)	Loss 4.7157 (3.7500)	CeLoss 0.6094 (1.0320)	SegCLSLoss 0.8789 (0.5453)	KLLoss 0.1001 (0.0824)	MaskLoss 1.0639 (0.7114)	MaskBCELoss 0.3442 (0.2416)	MaskDICELoss 0.7197 (0.4698)
Epoch: [0][117/500]	Time  9.135 ( 9.135)	Loss 5.6457 (3.6002)	CeLoss 0.8047 (0.7613)	SegCLSLoss 0.9258 (0.5352)	KLLoss 0.1426 (0.0524)	MaskLoss 1.2848 (0.7303)	MaskBCELoss 0.4498 (0.2003)	MaskDICELoss 0.8350 (0.5300)
Epoch: [0][118/500]	Time  8.031 ( 8.031)	Loss 4.5486 (2.7900)	CeLoss 0.7500 (0.7664)	SegCLSLoss 0.9062 (0.4566)	KLLoss 0.0771 (0.0487)	MaskLoss 0.9879 (0.5163)	MaskBCELoss 0.3421 (0.1595)	MaskDICELoss 0.6458 (0.3568)
Epoch: [0][119/500]	Time  8.947 ( 8.947)	Loss 4.0862 (4.5363)	CeLoss 0.6797 (0.8398)	SegCLSLoss 0.8672 (0.7195)	KLLoss 0.0327 (0.1034)	MaskLoss 0.9751 (0.9592)	MaskBCELoss 0.4775 (0.3018)	MaskDICELoss 0.4976 (0.6574)
[2025-03-04 21:09:25,162] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[1.988995983935743e-05], mom=[(0.9, 0.95)]
[2025-03-04 21:09:25,167] [INFO] [timer.py:215:stop] epoch=0/micro_step=1200/global_step=120, RunningAvgSamplesPerSec=1.0613998264062001, CurrSamplesPerSec=1.00366010271719, MemAllocated=57.13GB, MaxMemAllocated=62.78GB
Epoch: [0][120/500]	Time  9.965 ( 9.965)	Loss 5.8420 (4.0095)	CeLoss 0.6016 (0.8207)	SegCLSLoss 0.9336 (0.6168)	KLLoss 0.2354 (0.0953)	MaskLoss 1.3770 (0.8349)	MaskBCELoss 0.4853 (0.2770)	MaskDICELoss 0.8917 (0.5579)
Epoch: [0][121/500]	Time  8.244 ( 8.244)	Loss 0.9805 (2.7792)	CeLoss 0.9805 (0.7746)	SegCLSLoss 0.0000 (0.4609)	KLLoss 0.0000 (0.0615)	MaskLoss 0.0000 (0.5116)	MaskBCELoss 0.0000 (0.1668)	MaskDICELoss 0.0000 (0.3448)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([25, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][122/500]	Time  9.083 ( 9.083)	Loss 0.6484 (3.9490)	CeLoss 0.6484 (0.7057)	SegCLSLoss 0.0000 (0.6340)	KLLoss 0.0000 (0.0896)	MaskLoss 0.0000 (0.8315)	MaskBCELoss 0.0000 (0.2447)	MaskDICELoss 0.0000 (0.5868)
Epoch: [0][123/500]	Time  9.599 ( 9.599)	Loss 4.8830 (4.0542)	CeLoss 0.7266 (0.8285)	SegCLSLoss 0.8906 (0.6219)	KLLoss 0.1426 (0.0997)	MaskLoss 0.9868 (0.7992)	MaskBCELoss 0.1884 (0.1907)	MaskDICELoss 0.7984 (0.6085)
Epoch: [0][124/500]	Time  9.325 ( 9.325)	Loss 5.3998 (4.6185)	CeLoss 0.5742 (0.7383)	SegCLSLoss 0.8672 (0.7184)	KLLoss 0.0962 (0.1235)	MaskLoss 1.2903 (0.9966)	MaskBCELoss 0.4314 (0.2940)	MaskDICELoss 0.8588 (0.7026)
Epoch: [0][125/500]	Time 10.299 (10.299)	Loss 5.6902 (4.9125)	CeLoss 0.7344 (0.6984)	SegCLSLoss 0.9258 (0.8043)	KLLoss 0.2480 (0.1503)	MaskLoss 1.2509 (1.0277)	MaskBCELoss 0.3793 (0.2249)	MaskDICELoss 0.8716 (0.8028)
Epoch: [0][126/500]	Time  8.822 ( 8.822)	Loss 5.1236 (4.8131)	CeLoss 0.5430 (0.7074)	SegCLSLoss 0.8984 (0.8023)	KLLoss 0.1934 (0.1335)	MaskLoss 1.0613 (0.9960)	MaskBCELoss 0.1546 (0.2071)	MaskDICELoss 0.9067 (0.7889)
Epoch: [0][127/500]	Time 10.367 (10.367)	Loss 5.4609 (4.4471)	CeLoss 0.5391 (0.6965)	SegCLSLoss 0.9023 (0.7008)	KLLoss 0.1904 (0.1055)	MaskLoss 1.3059 (0.9833)	MaskBCELoss 0.4713 (0.3190)	MaskDICELoss 0.8347 (0.6643)
Epoch: [0][128/500]	Time  8.040 ( 8.040)	Loss 4.0764 (3.6639)	CeLoss 0.5938 (0.6646)	SegCLSLoss 0.8867 (0.6145)	KLLoss 0.0391 (0.0966)	MaskLoss 1.0210 (0.7231)	MaskBCELoss 0.5429 (0.1486)	MaskDICELoss 0.4781 (0.5745)
Epoch: [0][129/500]	Time 10.658 (10.658)	Loss 5.2482 (5.4138)	CeLoss 0.6836 (0.6547)	SegCLSLoss 0.8633 (0.8836)	KLLoss 0.1416 (0.1528)	MaskLoss 1.0065 (1.1815)	MaskBCELoss 0.0178 (0.2814)	MaskDICELoss 0.9887 (0.9000)
[2025-03-04 21:10:59,104] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[1.987389558232932e-05], mom=[(0.9, 0.95)]
[2025-03-04 21:10:59,110] [INFO] [timer.py:215:stop] epoch=0/micro_step=1300/global_step=130, RunningAvgSamplesPerSec=1.061650799192593, CurrSamplesPerSec=1.052200119175479, MemAllocated=57.26GB, MaxMemAllocated=62.78GB
Epoch: [0][130/500]	Time  9.505 ( 9.505)	Loss 5.5863 (4.5737)	CeLoss 0.6172 (0.8443)	SegCLSLoss 0.8594 (0.7121)	KLLoss 0.0942 (0.1201)	MaskLoss 1.2902 (0.8996)	MaskBCELoss 0.3575 (0.1721)	MaskDICELoss 0.9327 (0.7275)
Epoch: [0][131/500]	Time 11.245 (11.245)	Loss 5.6705 (4.5005)	CeLoss 0.8906 (0.7777)	SegCLSLoss 0.9336 (0.7023)	KLLoss 0.0352 (0.0827)	MaskLoss 1.1833 (0.9360)	MaskBCELoss 0.2266 (0.2271)	MaskDICELoss 0.9567 (0.7088)
Epoch: [0][132/500]	Time 10.268 (10.268)	Loss 4.7085 (4.3759)	CeLoss 0.5312 (0.7270)	SegCLSLoss 0.8711 (0.7031)	KLLoss 0.0684 (0.0654)	MaskLoss 1.0492 (0.9592)	MaskBCELoss 0.2637 (0.3028)	MaskDICELoss 0.7855 (0.6564)
Epoch: [0][133/500]	Time 11.313 (11.313)	Loss 4.2930 (5.2573)	CeLoss 0.6914 (0.6418)	SegCLSLoss 0.8594 (0.8691)	KLLoss 0.0786 (0.0927)	MaskLoss 1.0016 (1.2195)	MaskBCELoss 0.4582 (0.3951)	MaskDICELoss 0.5434 (0.8244)
Epoch: [0][134/500]	Time 10.775 (10.775)	Loss 5.3224 (4.9065)	CeLoss 0.6211 (0.6820)	SegCLSLoss 0.8672 (0.7727)	KLLoss 0.1475 (0.1207)	MaskLoss 1.0801 (1.0515)	MaskBCELoss 0.1006 (0.2446)	MaskDICELoss 0.9795 (0.8070)
Epoch: [0][135/500]	Time  9.044 ( 9.044)	Loss 4.9391 (4.3127)	CeLoss 0.6094 (0.6557)	SegCLSLoss 0.8789 (0.6875)	KLLoss 0.1279 (0.1075)	MaskLoss 1.1421 (0.9125)	MaskBCELoss 0.4005 (0.2218)	MaskDICELoss 0.7415 (0.6907)
Epoch: [0][136/500]	Time  8.405 ( 8.405)	Loss 5.1153 (3.1652)	CeLoss 0.6680 (0.8988)	SegCLSLoss 0.8398 (0.4266)	KLLoss 0.1709 (0.0715)	MaskLoss 1.1229 (0.5804)	MaskBCELoss 0.3170 (0.1699)	MaskDICELoss 0.8059 (0.4106)
Epoch: [0][137/500]	Time 10.272 (10.272)	Loss 5.2007 (4.7004)	CeLoss 0.4531 (0.6027)	SegCLSLoss 0.8359 (0.7637)	KLLoss 0.1650 (0.1277)	MaskLoss 1.1317 (1.0521)	MaskBCELoss 0.1825 (0.3101)	MaskDICELoss 0.9491 (0.7420)
Epoch: [0][138/500]	Time  6.689 ( 6.689)	Loss 4.9756 (3.4430)	CeLoss 0.5039 (0.7297)	SegCLSLoss 0.8398 (0.5137)	KLLoss 0.0801 (0.0747)	MaskLoss 1.1915 (0.7096)	MaskBCELoss 0.3952 (0.2283)	MaskDICELoss 0.7963 (0.4813)
Epoch: [0][139/500]	Time  9.292 ( 9.292)	Loss 5.3662 (3.6380)	CeLoss 0.4629 (0.7168)	SegCLSLoss 0.8906 (0.5230)	KLLoss 0.2314 (0.0976)	MaskLoss 1.2158 (0.7609)	MaskBCELoss 0.3187 (0.2405)	MaskDICELoss 0.8970 (0.5204)
[2025-03-04 21:12:36,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[1.9857831325301207e-05], mom=[(0.9, 0.95)]
[2025-03-04 21:12:36,528] [INFO] [timer.py:215:stop] epoch=0/micro_step=1400/global_step=140, RunningAvgSamplesPerSec=1.05903375515887, CurrSamplesPerSec=0.9888715687800744, MemAllocated=57.25GB, MaxMemAllocated=62.78GB
Epoch: [0][140/500]	Time 10.114 (10.114)	Loss 5.0663 (4.6711)	CeLoss 0.5977 (0.5742)	SegCLSLoss 0.8477 (0.7727)	KLLoss 0.1865 (0.1438)	MaskLoss 1.0353 (0.9941)	MaskBCELoss 0.1430 (0.2050)	MaskDICELoss 0.8923 (0.7891)
Epoch: [0][141/500]	Time 10.747 (10.747)	Loss 5.2331 (4.2413)	CeLoss 0.5430 (0.6641)	SegCLSLoss 0.8711 (0.6746)	KLLoss 0.1934 (0.1104)	MaskLoss 1.0571 (0.8661)	MaskBCELoss 0.0835 (0.1671)	MaskDICELoss 0.9736 (0.6989)
Epoch: [0][142/500]	Time  9.474 ( 9.474)	Loss 4.4766 (3.8863)	CeLoss 0.5547 (0.6771)	SegCLSLoss 0.7812 (0.5777)	KLLoss 0.0688 (0.0852)	MaskLoss 0.9669 (0.7891)	MaskBCELoss 0.2033 (0.1611)	MaskDICELoss 0.7636 (0.6281)
Epoch: [0][143/500]	Time  9.398 ( 9.398)	Loss 5.3508 (4.0219)	CeLoss 0.3809 (0.7951)	SegCLSLoss 0.8242 (0.5938)	KLLoss 0.0747 (0.1009)	MaskLoss 1.2810 (0.7616)	MaskBCELoss 0.3202 (0.1089)	MaskDICELoss 0.9608 (0.6527)
Epoch: [0][144/500]	Time  8.460 ( 8.460)	Loss 0.8047 (3.8171)	CeLoss 0.8047 (0.6531)	SegCLSLoss 0.0000 (0.5871)	KLLoss 0.0000 (0.1062)	MaskLoss 0.0000 (0.7919)	MaskBCELoss 0.0000 (0.2016)	MaskDICELoss 0.0000 (0.5903)
Epoch: [0][145/500]	Time  8.731 ( 8.731)	Loss 4.9139 (3.6569)	CeLoss 0.4844 (0.7371)	SegCLSLoss 0.8320 (0.5020)	KLLoss 0.1514 (0.0741)	MaskLoss 0.9855 (0.7363)	MaskBCELoss 0.0374 (0.1747)	MaskDICELoss 0.9480 (0.5617)
Epoch: [0][146/500]	Time  9.972 ( 9.972)	Loss 5.3928 (4.0082)	CeLoss 0.4785 (0.7592)	SegCLSLoss 0.8047 (0.5844)	KLLoss 0.1040 (0.0940)	MaskLoss 1.2993 (0.8185)	MaskBCELoss 0.3963 (0.2060)	MaskDICELoss 0.9030 (0.6125)
Epoch: [0][147/500]	Time  8.420 ( 8.420)	Loss 3.8731 (3.6385)	CeLoss 0.5977 (0.6824)	SegCLSLoss 0.8711 (0.5914)	KLLoss 0.1553 (0.1023)	MaskLoss 0.8073 (0.7082)	MaskBCELoss 0.2718 (0.1372)	MaskDICELoss 0.5355 (0.5709)
Epoch: [0][148/500]	Time  8.963 ( 8.963)	Loss 0.8711 (3.1032)	CeLoss 0.8711 (0.8967)	SegCLSLoss 0.0000 (0.4094)	KLLoss 0.0000 (0.0624)	MaskLoss 0.0000 (0.5336)	MaskBCELoss 0.0000 (0.0974)	MaskDICELoss 0.0000 (0.4362)
Epoch: [0][149/500]	Time  8.626 ( 8.626)	Loss 4.9164 (3.6866)	CeLoss 0.5898 (0.6908)	SegCLSLoss 0.8477 (0.5730)	KLLoss 0.1406 (0.0813)	MaskLoss 1.0215 (0.7270)	MaskBCELoss 0.1630 (0.1400)	MaskDICELoss 0.8586 (0.5870)
[2025-03-04 21:14:08,450] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[1.9841767068273095e-05], mom=[(0.9, 0.95)]
[2025-03-04 21:14:08,456] [INFO] [timer.py:215:stop] epoch=0/micro_step=1500/global_step=150, RunningAvgSamplesPerSec=1.0609402400307708, CurrSamplesPerSec=1.0947299516007307, MemAllocated=57.64GB, MaxMemAllocated=62.78GB
Epoch: [0][150/500]	Time  9.136 ( 9.136)	Loss 0.9492 (3.3490)	CeLoss 0.9492 (0.7912)	SegCLSLoss 0.0000 (0.4887)	KLLoss 0.0000 (0.0804)	MaskLoss 0.0000 (0.6088)	MaskBCELoss 0.0000 (0.1010)	MaskDICELoss 0.0000 (0.5078)
Epoch: [0][151/500]	Time  8.792 ( 8.792)	Loss 5.1803 (4.4856)	CeLoss 0.4570 (0.5391)	SegCLSLoss 0.8320 (0.7488)	KLLoss 0.2090 (0.1514)	MaskLoss 1.1042 (0.9612)	MaskBCELoss 0.1612 (0.2121)	MaskDICELoss 0.9430 (0.7492)
Epoch: [0][152/500]	Time  9.556 ( 9.556)	Loss 5.1747 (4.6637)	CeLoss 0.4336 (0.6137)	SegCLSLoss 0.8125 (0.7238)	KLLoss 0.1143 (0.1232)	MaskLoss 1.1844 (1.0215)	MaskBCELoss 0.2581 (0.2608)	MaskDICELoss 0.9263 (0.7607)
Epoch: [0][153/500]	Time  8.259 ( 8.259)	Loss 0.8438 (3.2445)	CeLoss 0.8438 (0.7023)	SegCLSLoss 0.0000 (0.4785)	KLLoss 0.0000 (0.0729)	MaskLoss 0.0000 (0.6039)	MaskBCELoss 0.0000 (0.0930)	MaskDICELoss 0.0000 (0.5109)
Epoch: [0][154/500]	Time  9.482 ( 9.482)	Loss 4.8106 (4.2059)	CeLoss 0.3652 (0.5721)	SegCLSLoss 0.8359 (0.6500)	KLLoss 0.1230 (0.1153)	MaskLoss 1.0153 (0.8704)	MaskBCELoss 0.0785 (0.1442)	MaskDICELoss 0.9369 (0.7263)
Epoch: [0][155/500]	Time  9.830 ( 9.830)	Loss 4.9699 (4.2915)	CeLoss 0.4824 (0.5801)	SegCLSLoss 0.8203 (0.6559)	KLLoss 0.1719 (0.1293)	MaskLoss 1.0461 (0.9268)	MaskBCELoss 0.1385 (0.2262)	MaskDICELoss 0.9076 (0.7006)
Epoch: [0][156/500]	Time  8.518 ( 8.518)	Loss 5.0051 (4.1017)	CeLoss 0.4219 (0.7682)	SegCLSLoss 0.8086 (0.5602)	KLLoss 0.1631 (0.0874)	MaskLoss 1.0196 (0.8451)	MaskBCELoss 0.0326 (0.2070)	MaskDICELoss 0.9869 (0.6381)
Epoch: [0][157/500]	Time 11.185 (11.185)	Loss 5.0847 (5.4485)	CeLoss 0.4961 (0.5719)	SegCLSLoss 0.7812 (0.8020)	KLLoss 0.0884 (0.1230)	MaskLoss 1.1444 (1.2567)	MaskBCELoss 0.2347 (0.3370)	MaskDICELoss 0.9097 (0.9197)
Epoch: [0][158/500]	Time  9.705 ( 9.705)	Loss 5.0066 (4.6548)	CeLoss 0.4316 (0.4771)	SegCLSLoss 0.7891 (0.7145)	KLLoss 0.1543 (0.1354)	MaskLoss 1.0425 (1.0361)	MaskBCELoss 0.0739 (0.2299)	MaskDICELoss 0.9686 (0.8061)
Epoch: [0][159/500]	Time  8.049 ( 8.049)	Loss 4.9509 (3.1447)	CeLoss 0.5586 (0.7391)	SegCLSLoss 0.8164 (0.3973)	KLLoss 0.1885 (0.0789)	MaskLoss 0.9965 (0.6330)	MaskBCELoss 0.0957 (0.2022)	MaskDICELoss 0.9008 (0.4307)
[2025-03-04 21:15:41,569] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[1.9825702811244983e-05], mom=[(0.9, 0.95)]
[2025-03-04 21:15:41,574] [INFO] [timer.py:215:stop] epoch=0/micro_step=1600/global_step=160, RunningAvgSamplesPerSec=1.0617613208426833, CurrSamplesPerSec=1.0268083559495107, MemAllocated=57.46GB, MaxMemAllocated=62.78GB
Epoch: [0][160/500]	Time  9.741 ( 9.741)	Loss 5.8311 (4.8706)	CeLoss 0.4355 (0.5162)	SegCLSLoss 0.7930 (0.6996)	KLLoss 0.1367 (0.1067)	MaskLoss 1.4720 (1.1408)	MaskBCELoss 0.5128 (0.3331)	MaskDICELoss 0.9592 (0.8078)
Epoch: [0][161/500]	Time  8.018 ( 8.018)	Loss 4.7581 (3.4292)	CeLoss 0.4727 (0.6154)	SegCLSLoss 0.8008 (0.5492)	KLLoss 0.2070 (0.1040)	MaskLoss 1.0481 (0.7192)	MaskBCELoss 0.2562 (0.2204)	MaskDICELoss 0.7919 (0.4989)
Epoch: [0][162/500]	Time 11.105 (11.105)	Loss 5.4295 (4.6187)	CeLoss 0.4062 (0.5799)	SegCLSLoss 0.8047 (0.7047)	KLLoss 0.1855 (0.1174)	MaskLoss 1.3000 (1.0527)	MaskBCELoss 0.3812 (0.3204)	MaskDICELoss 0.9187 (0.7323)
Epoch: [0][163/500]	Time 24.962 (24.962)	Loss 5.2773 (4.7964)	CeLoss 0.4629 (0.5523)	SegCLSLoss 0.7852 (0.6930)	KLLoss 0.1309 (0.1359)	MaskLoss 1.3305 (1.1665)	MaskBCELoss 0.5146 (0.4519)	MaskDICELoss 0.8159 (0.7145)
Epoch: [0][164/500]	Time  9.450 ( 9.450)	Loss 5.3501 (3.2925)	CeLoss 0.4727 (0.5676)	SegCLSLoss 0.7930 (0.4746)	KLLoss 0.2100 (0.0998)	MaskLoss 1.1385 (0.7175)	MaskBCELoss 0.1409 (0.2408)	MaskDICELoss 0.9975 (0.4767)
Epoch: [0][165/500]	Time  7.229 ( 7.229)	Loss 5.4528 (2.9387)	CeLoss 0.6914 (0.6471)	SegCLSLoss 0.7773 (0.3887)	KLLoss 0.1533 (0.0819)	MaskLoss 1.2738 (0.5957)	MaskBCELoss 0.4384 (0.1836)	MaskDICELoss 0.8354 (0.4121)
Epoch: [0][166/500]	Time  9.455 ( 9.455)	Loss 5.1678 (3.9090)	CeLoss 0.4824 (0.5871)	SegCLSLoss 0.7734 (0.5367)	KLLoss 0.1602 (0.1030)	MaskLoss 1.0856 (0.8424)	MaskBCELoss 0.1030 (0.2097)	MaskDICELoss 0.9826 (0.6328)
Epoch: [0][167/500]	Time  9.433 ( 9.433)	Loss 5.1755 (3.6419)	CeLoss 0.4863 (0.7994)	SegCLSLoss 0.7734 (0.4566)	KLLoss 0.0806 (0.0711)	MaskLoss 1.3730 (0.7403)	MaskBCELoss 0.6368 (0.2094)	MaskDICELoss 0.7362 (0.5309)
Epoch: [0][168/500]	Time 10.275 (10.275)	Loss 0.6211 (3.6949)	CeLoss 0.6211 (0.5598)	SegCLSLoss 0.0000 (0.5383)	KLLoss 0.0000 (0.0826)	MaskLoss 0.0000 (0.8042)	MaskBCELoss 0.0000 (0.2171)	MaskDICELoss 0.0000 (0.5870)
Epoch: [0][169/500]	Time  7.698 ( 7.698)	Loss 5.3348 (3.7076)	CeLoss 0.5391 (0.6646)	SegCLSLoss 0.7617 (0.5223)	KLLoss 0.1157 (0.0922)	MaskLoss 1.2322 (0.7663)	MaskBCELoss 0.3165 (0.1885)	MaskDICELoss 0.9157 (0.5778)
[2025-03-04 21:17:27,750] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[1.980963855421687e-05], mom=[(0.9, 0.95)]
[2025-03-04 21:17:27,755] [INFO] [timer.py:215:stop] epoch=0/micro_step=1700/global_step=170, RunningAvgSamplesPerSec=1.0537791806970154, CurrSamplesPerSec=1.1688189834324811, MemAllocated=56.74GB, MaxMemAllocated=62.78GB
Epoch: [0][170/500]	Time  8.557 ( 8.557)	Loss 0.8789 (3.3438)	CeLoss 0.8789 (0.5855)	SegCLSLoss 0.0000 (0.4535)	KLLoss 0.0000 (0.0831)	MaskLoss 0.0000 (0.7192)	MaskBCELoss 0.0000 (0.2142)	MaskDICELoss 0.0000 (0.5051)
Epoch: [0][171/500]	Time  9.725 ( 9.725)	Loss 5.6014 (4.6439)	CeLoss 0.5508 (0.5320)	SegCLSLoss 0.7344 (0.6691)	KLLoss 0.0703 (0.1094)	MaskLoss 1.3566 (1.0648)	MaskBCELoss 0.4066 (0.2953)	MaskDICELoss 0.9500 (0.7695)
Epoch: [0][172/500]	Time  7.950 ( 7.950)	Loss 1.3516 (3.7576)	CeLoss 1.3516 (0.5762)	SegCLSLoss 0.0000 (0.5910)	KLLoss 0.0000 (0.1123)	MaskLoss 0.0000 (0.7680)	MaskBCELoss 0.0000 (0.1488)	MaskDICELoss 0.0000 (0.6192)
Epoch: [0][173/500]	Time  8.673 ( 8.673)	Loss 5.2078 (4.8014)	CeLoss 0.5430 (0.4947)	SegCLSLoss 0.7617 (0.6027)	KLLoss 0.1543 (0.1015)	MaskLoss 1.0691 (1.2308)	MaskBCELoss 0.0733 (0.5093)	MaskDICELoss 0.9958 (0.7215)
Epoch: [0][174/500]	Time 10.847 (10.847)	Loss 5.1923 (4.2758)	CeLoss 0.3926 (0.5148)	SegCLSLoss 0.7422 (0.6691)	KLLoss 0.1924 (0.1359)	MaskLoss 1.1954 (0.9208)	MaskBCELoss 0.2732 (0.1964)	MaskDICELoss 0.9222 (0.7244)
Epoch: [0][175/500]	Time 10.262 (10.262)	Loss 5.1729 (4.2669)	CeLoss 0.4414 (0.5631)	SegCLSLoss 0.7383 (0.5910)	KLLoss 0.1758 (0.1364)	MaskLoss 1.1514 (0.9126)	MaskBCELoss 0.2105 (0.1895)	MaskDICELoss 0.9409 (0.7232)
Epoch: [0][176/500]	Time 10.181 (10.181)	Loss 4.3824 (4.2482)	CeLoss 0.5234 (0.4947)	SegCLSLoss 0.6914 (0.6535)	KLLoss 0.0598 (0.1131)	MaskLoss 0.9751 (0.9556)	MaskBCELoss 0.2219 (0.2541)	MaskDICELoss 0.7532 (0.7015)
Epoch: [0][177/500]	Time  9.135 ( 9.135)	Loss 0.9141 (3.9647)	CeLoss 0.9141 (0.5373)	SegCLSLoss 0.0000 (0.5758)	KLLoss 0.0000 (0.1155)	MaskLoss 0.0000 (0.8638)	MaskBCELoss 0.0000 (0.2155)	MaskDICELoss 0.0000 (0.6484)
Epoch: [0][178/500]	Time  8.310 ( 8.310)	Loss 4.1624 (3.1226)	CeLoss 0.5000 (0.6941)	SegCLSLoss 0.6836 (0.4266)	KLLoss 0.0693 (0.0697)	MaskLoss 0.9690 (0.6095)	MaskBCELoss 0.3137 (0.1461)	MaskDICELoss 0.6552 (0.4634)
Epoch: [0][179/500]	Time  8.933 ( 8.933)	Loss 0.6211 (3.2348)	CeLoss 0.6211 (0.6100)	SegCLSLoss 0.0000 (0.4324)	KLLoss 0.0000 (0.0974)	MaskLoss 0.0000 (0.6449)	MaskBCELoss 0.0000 (0.1347)	MaskDICELoss 0.0000 (0.5102)
[2025-03-04 21:19:00,672] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[1.9793574297188755e-05], mom=[(0.9, 0.95)]
[2025-03-04 21:19:00,677] [INFO] [timer.py:215:stop] epoch=0/micro_step=1800/global_step=180, RunningAvgSamplesPerSec=1.0550209278471592, CurrSamplesPerSec=1.123230203779468, MemAllocated=57.28GB, MaxMemAllocated=62.78GB
Epoch: [0][180/500]	Time  8.905 ( 8.905)	Loss 5.3444 (3.4469)	CeLoss 0.4844 (0.5898)	SegCLSLoss 0.7266 (0.4191)	KLLoss 0.1855 (0.0795)	MaskLoss 1.2401 (0.7351)	MaskBCELoss 0.3236 (0.1858)	MaskDICELoss 0.9165 (0.5493)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([23, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
Epoch: [0][181/500]	Time  9.822 ( 9.822)	Loss 1.6328 (3.7115)	CeLoss 1.6328 (0.6683)	SegCLSLoss 0.0000 (0.4992)	KLLoss 0.0000 (0.1054)	MaskLoss 0.0000 (0.7751)	MaskBCELoss 0.0000 (0.2063)	MaskDICELoss 0.0000 (0.5688)
Epoch: [0][182/500]	Time 10.497 (10.497)	Loss 5.2421 (4.4522)	CeLoss 0.5312 (0.5199)	SegCLSLoss 0.7031 (0.6355)	KLLoss 0.1592 (0.1334)	MaskLoss 1.2373 (0.9969)	MaskBCELoss 0.3732 (0.2533)	MaskDICELoss 0.8642 (0.7436)
Epoch: [0][183/500]	Time  9.332 ( 9.332)	Loss 0.9062 (3.9734)	CeLoss 0.9062 (0.5848)	SegCLSLoss 0.0000 (0.4824)	KLLoss 0.0000 (0.1079)	MaskLoss 0.0000 (0.8908)	MaskBCELoss 0.0000 (0.2624)	MaskDICELoss 0.0000 (0.6285)
Epoch: [0][184/500]	Time  8.813 ( 8.813)	Loss 4.6108 (3.7371)	CeLoss 0.4160 (0.6143)	SegCLSLoss 0.6523 (0.4715)	KLLoss 0.0928 (0.0853)	MaskLoss 1.0258 (0.8217)	MaskBCELoss 0.1641 (0.2426)	MaskDICELoss 0.8617 (0.5791)
Epoch: [0][185/500]	Time  7.701 ( 7.701)	Loss 3.7722 (3.1446)	CeLoss 0.4160 (0.6723)	SegCLSLoss 0.6953 (0.4195)	KLLoss 0.1758 (0.0898)	MaskLoss 0.8193 (0.6393)	MaskBCELoss 0.2231 (0.1922)	MaskDICELoss 0.5961 (0.4471)
Epoch: [0][186/500]	Time  9.851 ( 9.851)	Loss 4.2036 (4.0517)	CeLoss 0.4492 (0.5062)	SegCLSLoss 0.6445 (0.5449)	KLLoss 0.0825 (0.1344)	MaskLoss 0.9583 (0.9081)	MaskBCELoss 0.2425 (0.2470)	MaskDICELoss 0.7158 (0.6611)
Epoch: [0][187/500]	Time 10.286 (10.286)	Loss 4.8646 (4.4363)	CeLoss 0.3379 (0.5021)	SegCLSLoss 0.6641 (0.6148)	KLLoss 0.1562 (0.1608)	MaskLoss 1.0539 (0.9723)	MaskBCELoss 0.0896 (0.2118)	MaskDICELoss 0.9643 (0.7605)
Epoch: [0][188/500]	Time  6.838 ( 6.838)	Loss 4.8495 (3.1789)	CeLoss 0.4355 (0.5929)	SegCLSLoss 0.6406 (0.4059)	KLLoss 0.0806 (0.0924)	MaskLoss 1.1086 (0.6309)	MaskBCELoss 0.2124 (0.1165)	MaskDICELoss 0.8962 (0.5144)
Epoch: [0][189/500]	Time 10.175 (10.175)	Loss 5.0626 (3.7688)	CeLoss 0.3418 (0.5346)	SegCLSLoss 0.6797 (0.5355)	KLLoss 0.1797 (0.1051)	MaskLoss 1.2469 (0.8289)	MaskBCELoss 0.3921 (0.2269)	MaskDICELoss 0.8547 (0.6020)
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([22, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
[2025-03-04 21:20:33,047] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[1.9777510040160643e-05], mom=[(0.9, 0.95)]
[2025-03-04 21:20:33,053] [INFO] [timer.py:215:stop] epoch=0/micro_step=1900/global_step=190, RunningAvgSamplesPerSec=1.0564569775698438, CurrSamplesPerSec=1.1038302951031937, MemAllocated=57.39GB, MaxMemAllocated=62.78GB
Epoch: [0][190/500]	Time  9.061 ( 9.061)	Loss 4.9378 (4.0719)	CeLoss 0.2812 (0.4752)	SegCLSLoss 0.6953 (0.5418)	KLLoss 0.1689 (0.1435)	MaskLoss 1.0822 (0.9200)	MaskBCELoss 0.0939 (0.2493)	MaskDICELoss 0.9883 (0.6707)
Epoch: [0][191/500]	Time  8.285 ( 8.285)	Loss 4.6052 (3.0238)	CeLoss 0.4590 (0.7568)	SegCLSLoss 0.6094 (0.3301)	KLLoss 0.0732 (0.0764)	MaskLoss 1.0378 (0.5614)	MaskBCELoss 0.1930 (0.1102)	MaskDICELoss 0.8449 (0.4512)
Epoch: [0][192/500]	Time  9.796 ( 9.796)	Loss 4.6210 (4.0633)	CeLoss 0.4199 (0.6693)	SegCLSLoss 0.6602 (0.5316)	KLLoss 0.1016 (0.1097)	MaskLoss 1.0168 (0.8624)	MaskBCELoss 0.1488 (0.2156)	MaskDICELoss 0.8679 (0.6468)
Epoch: [0][193/500]	Time 11.214 (11.214)	Loss 4.2221 (4.5844)	CeLoss 0.4043 (0.3803)	SegCLSLoss 0.6602 (0.6586)	KLLoss 0.1621 (0.1494)	MaskLoss 0.8630 (1.0201)	MaskBCELoss 0.0642 (0.1775)	MaskDICELoss 0.7988 (0.8426)
Epoch: [0][194/500]	Time  7.460 ( 7.460)	Loss 5.1941 (3.5244)	CeLoss 0.3691 (0.7141)	SegCLSLoss 0.6523 (0.3852)	KLLoss 0.2109 (0.0889)	MaskLoss 1.2459 (0.7298)	MaskBCELoss 0.3478 (0.1950)	MaskDICELoss 0.8980 (0.5348)
Epoch: [0][195/500]	Time 10.087 (10.087)	Loss 5.1381 (4.0641)	CeLoss 0.3301 (0.5033)	SegCLSLoss 0.6523 (0.5172)	KLLoss 0.1992 (0.1345)	MaskLoss 1.2552 (0.8541)	MaskBCELoss 0.3691 (0.1239)	MaskDICELoss 0.8861 (0.7301)
Epoch: [0][196/500]	Time  9.801 ( 9.801)	Loss 0.5938 (3.5684)	CeLoss 0.5938 (0.5096)	SegCLSLoss 0.0000 (0.4984)	KLLoss 0.0000 (0.1042)	MaskLoss 0.0000 (0.7748)	MaskBCELoss 0.0000 (0.1972)	MaskDICELoss 0.0000 (0.5776)
Epoch: [0][197/500]	Time  9.819 ( 9.819)	Loss 5.0089 (3.9467)	CeLoss 0.3906 (0.5039)	SegCLSLoss 0.6211 (0.5031)	KLLoss 0.0884 (0.1042)	MaskLoss 1.2342 (0.8812)	MaskBCELoss 0.3604 (0.2189)	MaskDICELoss 0.8738 (0.6623)
Epoch: [0][198/500]	Time 10.335 (10.335)	Loss 0.5977 (3.7984)	CeLoss 0.5977 (0.4518)	SegCLSLoss 0.0000 (0.5066)	KLLoss 0.0000 (0.1088)	MaskLoss 0.0000 (0.8123)	MaskBCELoss 0.0000 (0.1325)	MaskDICELoss 0.0000 (0.6798)
Epoch: [0][199/500]	Time 10.293 (10.293)	Loss 4.9808 (4.0596)	CeLoss 0.3730 (0.4580)	SegCLSLoss 0.5859 (0.4988)	KLLoss 0.1543 (0.1411)	MaskLoss 1.1652 (0.9031)	MaskBCELoss 0.2501 (0.2007)	MaskDICELoss 0.9151 (0.7025)
[2025-03-04 21:22:09,373] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[1.976144578313253e-05], mom=[(0.9, 0.95)]
[2025-03-04 21:22:09,376] [INFO] [timer.py:215:stop] epoch=0/micro_step=2000/global_step=200, RunningAvgSamplesPerSec=1.0555251814395616, CurrSamplesPerSec=1.0831477752985057, MemAllocated=56.74GB, MaxMemAllocated=62.78GB
Epoch: [0][200/500]	Time  9.234 ( 9.234)	Loss 0.9492 (3.4952)	CeLoss 0.9492 (0.5514)	SegCLSLoss 0.0000 (0.4293)	KLLoss 0.0000 (0.1290)	MaskLoss 0.0000 (0.7195)	MaskBCELoss 0.0000 (0.1387)	MaskDICELoss 0.0000 (0.5808)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 659, in <module>
[rank0]:     return giou, ciou
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 401, in main
[rank0]:     train_iter = train(
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 497, in train
[rank0]:     output_dict = model(**input_dict)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1735, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/peft/peft_model.py", line 922, in forward
[rank0]:     return self.base_model(
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py", line 213, in forward
[rank0]:     return self.model_forward(**kwargs)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py", line 398, in model_forward
[rank0]:     low_res_masks, iou_predictions = self.model.visual_model.mask_decoder(
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/segment_anything/modeling/mask_decoder.py", line 98, in forward
[rank0]:     masks, iou_pred = self.predict_masks(
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/segment_anything/modeling/mask_decoder.py", line 143, in predict_masks
[rank0]:     hs, src = self.transformer(src, pos_src, tokens)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/segment_anything/modeling/transformer.py", line 102, in forward
[rank0]:     attn_out = self.final_attn_token_to_image(q=q, k=k, v=keys)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/segment_anything/modeling/transformer.py", line 224, in forward
[rank0]:     v = self.v_proj(v)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 116, in forward
[rank0]:     return F.linear(input, self.weight, self.bias)
[rank0]: KeyboardInterrupt