You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565



Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [02:00<00:00, 40.02s/it]
trainable params: 6,553,600 || all params: 14,025,701,683 || trainable%: 0.0467256480147682
n:  base_model.model.model.embed_tokens.weight p.shape:  torch.Size([32002, 5120])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_token.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.mask_decoder.mask_tokens.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.weight p.shape:  torch.Size([256, 64, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.weight p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.weight p.shape:  torch.Size([64, 32, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.bias p.shape:  torch.Size([4])
n:  base_model.model.model.text_hidden_fcs.0.0.weight p.shape:  torch.Size([5120, 5120])
n:  base_model.model.model.text_hidden_fcs.0.0.bias p.shape:  torch.Size([5120])
n:  base_model.model.model.text_hidden_fcs.0.2.weight p.shape:  torch.Size([256, 5120])
n:  base_model.model.model.text_hidden_fcs.0.2.bias p.shape:  torch.Size([256])
n:  base_model.model.lm_head.weight p.shape:  torch.Size([32002, 5120])
ade20k:  20210
cocostuff:  118287
loading annotations into memory...
Done (t=0.88s)
creating index...
index created!
pascal_part:  4366
loading annotations into memory...
Done (t=10.69s)
creating index...
index created!
paco_lvis:  45790
mapillary:  18000
loading dataset refclef into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refclef/refs(unc).p
creating index...
index created.
DONE (t=4.96s)
dataset refclef (refs unc) (train split) has 17978 images and 99523 annotations.
loading dataset refcoco into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco/refs(unc).p
creating index...
index created.
DONE (t=6.21s)
dataset refcoco (refs unc) (train split) has 16994 images and 196771 annotations.
loading dataset refcoco+ into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco+/refs(unc).p
creating index...
index created.
DONE (t=7.06s)
dataset refcoco+ (refs unc) (train split) has 16992 images and 196737 annotations.
loading dataset refcocog into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcocog/refs(umd).p
creating index...
index created.
DONE (t=4.21s)
dataset refcocog (refs umd) (train split) has 21899 images and 208960 annotations.
vqa_data:  157712
number of reason_seg samples:  239
len(self.img_to_explanation):  239
Training with 5000 examples and validating with 200 examples.
[2025-03-02 16:24:16,335] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5, git-hash=unknown, git-branch=unknown
[2025-03-02 16:24:16,336] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2025-03-02 16:24:16,336] [INFO] [comm.py:594:init_distributed] cdb=None
[2025-03-02 16:24:16,336] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Using /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module fused_adam...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/tensor/python_tensor.cpp:78.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
[2025-03-02 16:24:35,246] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Time to load fused_adam op: 0.6423561573028564 seconds
[2025-03-02 16:24:36,075] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-03-02 16:24:36,261] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-03-02 16:24:36,261] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-03-02 16:24:36,261] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-03-02 16:24:36,262] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500000000
[2025-03-02 16:24:36,262] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500000000
[2025-03-02 16:24:36,262] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2025-03-02 16:24:36,262] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1] and sizes[(365842916, False)]
[2025-03-02 16:24:40,493] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2025-03-02 16:24:40,494] [INFO] [utils.py:786:see_memory_usage] MA 27.69 GB         Max_MA 28.37 GB         CA 28.51 GB         Max_CA 29 GB
[2025-03-02 16:24:40,494] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 81.61 GB, percent = 8.1%
[2025-03-02 16:24:43,414] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2025-03-02 16:24:43,414] [INFO] [utils.py:786:see_memory_usage] MA 30.41 GB         Max_MA 31.78 GB         CA 32.6 GB         Max_CA 33 GB
[2025-03-02 16:24:43,415] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 80.77 GB, percent = 8.0%
[2025-03-02 16:24:43,415] [INFO] [stage_1_and_2.py:488:__init__] optimizer state initialized
[2025-03-02 16:24:46,295] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2025-03-02 16:24:46,296] [INFO] [utils.py:786:see_memory_usage] MA 30.41 GB         Max_MA 30.41 GB         CA 32.6 GB         Max_CA 33 GB
[2025-03-02 16:24:46,296] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 80.95 GB, percent = 8.0%
[2025-03-02 16:24:46,301] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-03-02 16:24:46,301] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2025-03-02 16:24:46,302] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7f709f2583a0>
[2025-03-02 16:24:46,302] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
[2025-03-02 16:24:46,305] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2025-03-02 16:24:46,305] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-03-02 16:24:46,305] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-03-02 16:24:46,305] [INFO] [config.py:964:print]   amp_enabled .................. False
[2025-03-02 16:24:46,305] [INFO] [config.py:964:print]   amp_params ................... False
[2025-03-02 16:24:46,305] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-02 16:24:46,305] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2025-03-02 16:24:46,305] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2025-03-02 16:24:46,306] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2025-03-02 16:24:46,306] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2025-03-02 16:24:46,306] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f6a497fefb0>
[2025-03-02 16:24:46,306] [INFO] [config.py:964:print]   communication_data_type ...... None
[2025-03-02 16:24:46,306] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-02 16:24:46,306] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2025-03-02 16:24:46,306] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2025-03-02 16:24:46,306] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-02 16:24:46,306] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2025-03-02 16:24:46,306] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2025-03-02 16:24:46,306] [INFO] [config.py:964:print]   disable_allgather ............ False
[2025-03-02 16:24:46,306] [INFO] [config.py:964:print]   dump_state ................... False
[2025-03-02 16:24:46,306] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2025-03-02 16:24:46,306] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2025-03-02 16:24:46,306] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-02 16:24:46,306] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-02 16:24:46,306] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2025-03-02 16:24:46,306] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2025-03-02 16:24:46,306] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2025-03-02 16:24:46,306] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2025-03-02 16:24:46,307] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2025-03-02 16:24:46,307] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2025-03-02 16:24:46,307] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-03-02 16:24:46,307] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2025-03-02 16:24:46,307] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2025-03-02 16:24:46,307] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2025-03-02 16:24:46,307] [INFO] [config.py:964:print]   global_rank .................. 0
[2025-03-02 16:24:46,307] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2025-03-02 16:24:46,307] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 10
[2025-03-02 16:24:46,307] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0
[2025-03-02 16:24:46,307] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2025-03-02 16:24:46,307] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-02 16:24:46,307] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2025-03-02 16:24:46,307] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2025-03-02 16:24:46,307] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2025-03-02 16:24:46,307] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2025-03-02 16:24:46,307] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2025-03-02 16:24:46,307] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2025-03-02 16:24:46,307] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-03-02 16:24:46,308] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-03-02 16:24:46,308] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2025-03-02 16:24:46,308] [INFO] [config.py:964:print]   optimizer_name ............... adamw
[2025-03-02 16:24:46,308] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.0003, 'weight_decay': 0.0, 'betas': (0.9, 0.95)}
[2025-03-02 16:24:46,308] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2025-03-02 16:24:46,308] [INFO] [config.py:964:print]   pld_enabled .................. False
[2025-03-02 16:24:46,308] [INFO] [config.py:964:print]   pld_params ................... False
[2025-03-02 16:24:46,308] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2025-03-02 16:24:46,308] [INFO] [config.py:964:print]   scheduler_name ............... WarmupDecayLR
[2025-03-02 16:24:46,308] [INFO] [config.py:964:print]   scheduler_params ............. {'total_num_steps': 5000, 'warmup_min_lr': 0, 'warmup_max_lr': 0.0003, 'warmup_num_steps': 100, 'warmup_type': 'linear'}
[2025-03-02 16:24:46,308] [INFO] [config.py:964:print]   sparse_attention ............. None
[2025-03-02 16:24:46,308] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2025-03-02 16:24:46,308] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2025-03-02 16:24:46,308] [INFO] [config.py:964:print]   train_batch_size ............. 10
[2025-03-02 16:24:46,308] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2025-03-02 16:24:46,308] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2025-03-02 16:24:46,308] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2025-03-02 16:24:46,308] [INFO] [config.py:964:print]   world_size ................... 1
[2025-03-02 16:24:46,309] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2025-03-02 16:24:46,309] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2025-03-02 16:24:46,309] [INFO] [config.py:964:print]   zero_enabled ................. True
[2025-03-02 16:24:46,309] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-02 16:24:46,309] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2025-03-02 16:24:46,309] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1,
    "gradient_accumulation_steps": 10,
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 0.0003,
            "weight_decay": 0.0,
            "betas": [0.9, 0.95]
        }
    },
    "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "total_num_steps": 5.000000e+03,
            "warmup_min_lr": 0,
            "warmup_max_lr": 0.0003,
            "warmup_num_steps": 100,
            "warmup_type": "linear"
        }
    },
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "gradient_clipping": 1.0,
    "zero_optimization": {
        "stage": 2,
        "contiguous_gradients": true,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 5.000000e+08,
        "allgather_bucket_size": 5.000000e+08
    }
}
(train) >> AFTER DEEPSPEED
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)
  return F.conv2d(input, weight, bias, self.stride,
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([332, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch: [0][  1/500]	Time  7.526 ( 7.526)	Loss 6.9302 (9.7439)	CeLoss 3.2656 (3.1367)	SegCLSLoss 1.1484 (0.9273)	KLLoss 0.4746 (0.4168)	MaskLoss 1.4182 (2.9661)	MaskBCELoss 0.5402 (2.3050)	MaskDICELoss 0.8780 (0.6611)
Epoch: [0][  2/500]	Time  6.725 ( 6.725)	Loss 5.6801 (4.9433)	CeLoss 2.5625 (2.3996)	SegCLSLoss 1.1484 (0.9117)	KLLoss 0.4922 (0.4111)	MaskLoss 1.1447 (0.9421)	MaskBCELoss 0.1523 (0.1474)	MaskDICELoss 0.9924 (0.7947)
Epoch: [0][  3/500]	Time  5.932 ( 5.932)	Loss 1.5547 (5.0586)	CeLoss 1.5547 (2.3750)	SegCLSLoss 0.0000 (0.8148)	KLLoss 0.0000 (0.3980)	MaskLoss 0.0000 (1.0394)	MaskBCELoss 0.0000 (0.3435)	MaskDICELoss 0.0000 (0.6959)
Epoch: [0][  4/500]	Time  5.826 ( 5.826)	Loss 5.4599 (5.8327)	CeLoss 2.1562 (2.3383)	SegCLSLoss 1.2422 (0.9523)	KLLoss 0.3984 (0.4174)	MaskLoss 1.2377 (1.4043)	MaskBCELoss 0.2460 (0.6098)	MaskDICELoss 0.9917 (0.7945)
Epoch: [0][  5/500]	Time  5.710 ( 5.710)	Loss 1.0000 (4.3479)	CeLoss 1.0000 (1.9984)	SegCLSLoss 0.0000 (0.8281)	KLLoss 0.0000 (0.3191)	MaskLoss 0.0000 (0.8872)	MaskBCELoss 0.0000 (0.1917)	MaskDICELoss 0.0000 (0.6956)
Epoch: [0][  6/500]	Time  7.051 ( 7.051)	Loss 0.7266 (5.2297)	CeLoss 0.7266 (2.4676)	SegCLSLoss 0.0000 (0.9398)	KLLoss 0.0000 (0.4385)	MaskLoss 0.0000 (1.0381)	MaskBCELoss 0.0000 (0.2517)	MaskDICELoss 0.0000 (0.7864)
Epoch: [0][  7/500]	Time  6.325 ( 6.325)	Loss 5.4879 (4.9763)	CeLoss 2.5781 (2.4141)	SegCLSLoss 1.1328 (0.9320)	KLLoss 0.4961 (0.4090)	MaskLoss 1.0487 (0.9468)	MaskBCELoss 0.0503 (0.1618)	MaskDICELoss 0.9983 (0.7849)
Epoch: [0][  8/500]	Time  5.919 ( 5.919)	Loss 1.0234 (5.2544)	CeLoss 1.0234 (2.4656)	SegCLSLoss 0.0000 (0.9305)	KLLoss 0.0000 (0.4561)	MaskLoss 0.0000 (1.0479)	MaskBCELoss 0.0000 (0.2865)	MaskDICELoss 0.0000 (0.7614)
Epoch: [0][  9/500]	Time  6.367 ( 6.367)	Loss 5.7917 (4.3439)	CeLoss 2.6719 (2.1168)	SegCLSLoss 1.1172 (0.8266)	KLLoss 0.5391 (0.3482)	MaskLoss 1.1459 (0.8198)	MaskBCELoss 0.1704 (0.1551)	MaskDICELoss 0.9755 (0.6647)
[2025-03-02 16:25:51,648] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[5.1e-05], mom=[(0.9, 0.95)]
[2025-03-02 16:25:51,654] [INFO] [timer.py:215:stop] epoch=0/micro_step=100/global_step=10, RunningAvgSamplesPerSec=1.5705441492371617, CurrSamplesPerSec=1.2781495150408004, MemAllocated=31.23GB, MaxMemAllocated=35.94GB
Epoch: [0][ 10/500]	Time  7.825 ( 7.825)	Loss 6.9660 (6.0327)	CeLoss 3.2188 (2.7219)	SegCLSLoss 1.1484 (1.1469)	KLLoss 0.6602 (0.5416)	MaskLoss 1.4205 (1.2328)	MaskBCELoss 0.5338 (0.2817)	MaskDICELoss 0.8867 (0.9511)
Epoch: [0][ 11/500]	Time  5.437 ( 5.437)	Loss 7.0799 (4.4106)	CeLoss 3.0156 (1.9438)	SegCLSLoss 1.1406 (0.8086)	KLLoss 0.6172 (0.3623)	MaskLoss 1.5868 (0.9404)	MaskBCELoss 0.8572 (0.3263)	MaskDICELoss 0.7296 (0.6141)
Epoch: [0][ 12/500]	Time  6.671 ( 6.671)	Loss 1.5000 (5.4847)	CeLoss 1.5000 (2.3531)	SegCLSLoss 0.0000 (1.0375)	KLLoss 0.0000 (0.5189)	MaskLoss 0.0000 (1.1752)	MaskBCELoss 0.0000 (0.3974)	MaskDICELoss 0.0000 (0.7778)
Epoch: [0][ 13/500]	Time  5.180 ( 5.180)	Loss 5.3906 (3.7347)	CeLoss 2.2031 (1.6918)	SegCLSLoss 1.1484 (0.6945)	KLLoss 0.4473 (0.3117)	MaskLoss 1.1953 (0.7695)	MaskBCELoss 0.2295 (0.2393)	MaskDICELoss 0.9658 (0.5302)
Epoch: [0][ 14/500]	Time  5.442 ( 5.442)	Loss 4.9996 (3.1075)	CeLoss 1.5234 (1.3961)	SegCLSLoss 1.1406 (0.5820)	KLLoss 0.4473 (0.2607)	MaskLoss 1.3436 (0.6456)	MaskBCELoss 0.3700 (0.1893)	MaskDICELoss 0.9735 (0.4563)
Epoch: [0][ 15/500]	Time  6.741 ( 6.741)	Loss 0.5586 (4.6079)	CeLoss 0.5586 (1.6715)	SegCLSLoss 0.0000 (1.0305)	KLLoss 0.0000 (0.4586)	MaskLoss 0.0000 (1.0956)	MaskBCELoss 0.0000 (0.2565)	MaskDICELoss 0.0000 (0.8391)
Epoch: [0][ 16/500]	Time  5.773 ( 5.773)	Loss 4.5897 (4.6480)	CeLoss 1.5156 (1.5770)	SegCLSLoss 1.1719 (1.0547)	KLLoss 0.5586 (0.4428)	MaskLoss 1.1073 (1.1613)	MaskBCELoss 0.1133 (0.3289)	MaskDICELoss 0.9941 (0.8324)
Epoch: [0][ 17/500]	Time  6.457 ( 6.457)	Loss 5.0423 (4.6676)	CeLoss 1.7969 (1.5082)	SegCLSLoss 1.2266 (1.0609)	KLLoss 0.4336 (0.4404)	MaskLoss 1.2087 (1.2043)	MaskBCELoss 0.2844 (0.3610)	MaskDICELoss 0.9243 (0.8433)
Epoch: [0][ 18/500]	Time  6.089 ( 6.089)	Loss 4.4502 (3.0202)	CeLoss 1.2500 (1.3457)	SegCLSLoss 1.2188 (0.5758)	KLLoss 0.4023 (0.2512)	MaskLoss 1.1939 (0.6306)	MaskBCELoss 0.2466 (0.1563)	MaskDICELoss 0.9472 (0.4743)
Epoch: [0][ 19/500]	Time  5.434 ( 5.434)	Loss 0.9727 (3.1143)	CeLoss 0.9727 (1.1488)	SegCLSLoss 0.0000 (0.7055)	KLLoss 0.0000 (0.2842)	MaskLoss 0.0000 (0.7359)	MaskBCELoss 0.0000 (0.1605)	MaskDICELoss 0.0000 (0.5754)
[2025-03-02 16:26:50,411] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[0.00011099999999999999], mom=[(0.9, 0.95)]
[2025-03-02 16:26:50,416] [INFO] [timer.py:215:stop] epoch=0/micro_step=200/global_step=20, RunningAvgSamplesPerSec=1.6410504496123421, CurrSamplesPerSec=1.8062495494392894, MemAllocated=31.25GB, MaxMemAllocated=35.94GB
Epoch: [0][ 20/500]	Time  5.538 ( 5.538)	Loss 4.2103 (2.9671)	CeLoss 1.2969 (1.0992)	SegCLSLoss 1.0938 (0.6664)	KLLoss 0.4219 (0.3117)	MaskLoss 1.0739 (0.6894)	MaskBCELoss 0.0982 (0.1094)	MaskDICELoss 0.9757 (0.5800)
Epoch: [0][ 21/500]	Time  6.506 ( 6.506)	Loss 4.7086 (3.2646)	CeLoss 1.3047 (1.0465)	SegCLSLoss 1.1094 (0.7969)	KLLoss 0.5859 (0.3613)	MaskLoss 1.2762 (0.8192)	MaskBCELoss 0.3182 (0.1650)	MaskDICELoss 0.9580 (0.6542)
Epoch: [0][ 22/500]	Time  5.709 ( 5.709)	Loss 0.8789 (3.2294)	CeLoss 0.8789 (1.2293)	SegCLSLoss 0.0000 (0.6758)	KLLoss 0.0000 (0.3266)	MaskLoss 0.0000 (0.7493)	MaskBCELoss 0.0000 (0.1865)	MaskDICELoss 0.0000 (0.5628)
Epoch: [0][ 23/500]	Time  5.828 ( 5.828)	Loss 4.3683 (3.3326)	CeLoss 0.9062 (1.1551)	SegCLSLoss 1.0859 (0.7828)	KLLoss 0.5977 (0.3504)	MaskLoss 1.3091 (0.8054)	MaskBCELoss 0.3804 (0.1343)	MaskDICELoss 0.9287 (0.6711)
Epoch: [0][ 24/500]	Time  5.037 ( 5.037)	Loss 4.1778 (3.0627)	CeLoss 0.9922 (1.0281)	SegCLSLoss 1.1250 (0.6836)	KLLoss 0.4648 (0.2771)	MaskLoss 1.1944 (0.7774)	MaskBCELoss 0.2687 (0.2229)	MaskDICELoss 0.9257 (0.5546)
Epoch: [0][ 25/500]	Time  6.337 ( 6.337)	Loss 4.0784 (3.3501)	CeLoss 1.0625 (0.9437)	SegCLSLoss 1.1328 (0.7781)	KLLoss 0.4316 (0.3525)	MaskLoss 1.1173 (0.9204)	MaskBCELoss 0.1185 (0.2812)	MaskDICELoss 0.9988 (0.6392)
Epoch: [0][ 26/500]	Time  5.634 ( 5.634)	Loss 3.8769 (3.3746)	CeLoss 0.7461 (0.8516)	SegCLSLoss 1.0469 (0.8797)	KLLoss 0.4316 (0.3621)	MaskLoss 1.1963 (0.9510)	MaskBCELoss 0.2027 (0.1864)	MaskDICELoss 0.9936 (0.7646)
Epoch: [0][ 27/500]	Time  5.803 ( 5.803)	Loss 4.3730 (3.0407)	CeLoss 0.8672 (1.0898)	SegCLSLoss 1.0938 (0.6609)	KLLoss 0.6133 (0.2930)	MaskLoss 1.3271 (0.7364)	MaskBCELoss 0.4840 (0.1879)	MaskDICELoss 0.8431 (0.5484)
Epoch: [0][ 28/500]	Time  5.684 ( 5.684)	Loss 1.0312 (2.4477)	CeLoss 1.0312 (0.8195)	SegCLSLoss 0.0000 (0.5406)	KLLoss 0.0000 (0.2373)	MaskLoss 0.0000 (0.6198)	MaskBCELoss 0.0000 (0.1534)	MaskDICELoss 0.0000 (0.4664)
Epoch: [0][ 29/500]	Time  5.889 ( 5.889)	Loss 3.8567 (3.2304)	CeLoss 0.7852 (0.9674)	SegCLSLoss 1.0469 (0.7500)	KLLoss 0.4492 (0.3262)	MaskLoss 1.1588 (0.8617)	MaskBCELoss 0.2317 (0.2085)	MaskDICELoss 0.9271 (0.6532)
[2025-03-02 16:27:48,057] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[0.00017099999999999998], mom=[(0.9, 0.95)]
[2025-03-02 16:27:48,063] [INFO] [timer.py:215:stop] epoch=0/micro_step=300/global_step=30, RunningAvgSamplesPerSec=1.6734389398463991, CurrSamplesPerSec=1.9165589613314047, MemAllocated=31.24GB, MaxMemAllocated=35.94GB
Epoch: [0][ 30/500]	Time  5.219 ( 5.219)	Loss 4.0556 (2.6296)	CeLoss 0.7070 (0.9070)	SegCLSLoss 1.0078 (0.5211)	KLLoss 0.5391 (0.2525)	MaskLoss 1.2856 (0.6679)	MaskBCELoss 0.3535 (0.2111)	MaskDICELoss 0.9321 (0.4568)
Epoch: [0][ 31/500]	Time  6.983 ( 6.983)	Loss 3.7889 (3.1511)	CeLoss 0.7422 (0.7051)	SegCLSLoss 0.9844 (0.7914)	KLLoss 0.5195 (0.3783)	MaskLoss 1.1484 (0.9312)	MaskBCELoss 0.1633 (0.1538)	MaskDICELoss 0.9851 (0.7774)
Epoch: [0][ 32/500]	Time  6.695 ( 6.695)	Loss 4.5484 (3.3061)	CeLoss 0.8047 (0.7357)	SegCLSLoss 0.9727 (0.7926)	KLLoss 0.5742 (0.3768)	MaskLoss 1.4851 (0.9940)	MaskBCELoss 0.5456 (0.2453)	MaskDICELoss 0.9395 (0.7488)
Epoch: [0][ 33/500]	Time  6.595 ( 6.595)	Loss 3.6873 (3.2532)	CeLoss 0.6289 (0.6875)	SegCLSLoss 0.9375 (0.7637)	KLLoss 0.5039 (0.3967)	MaskLoss 1.1718 (0.9928)	MaskBCELoss 0.2031 (0.2392)	MaskDICELoss 0.9687 (0.7536)
Epoch: [0][ 34/500]	Time  5.940 ( 5.940)	Loss 3.7501 (3.0395)	CeLoss 0.7656 (0.6074)	SegCLSLoss 0.9375 (0.7555)	KLLoss 0.5117 (0.3455)	MaskLoss 1.1290 (0.9416)	MaskBCELoss 0.1485 (0.1807)	MaskDICELoss 0.9805 (0.7609)
Epoch: [0][ 35/500]	Time  5.436 ( 5.436)	Loss 0.9961 (2.4252)	CeLoss 0.9961 (0.7555)	SegCLSLoss 0.0000 (0.4578)	KLLoss 0.0000 (0.2441)	MaskLoss 0.0000 (0.6597)	MaskBCELoss 0.0000 (0.1996)	MaskDICELoss 0.0000 (0.4601)
Epoch: [0][ 36/500]	Time  6.302 ( 6.302)	Loss 3.5237 (3.4577)	CeLoss 0.2617 (0.6143)	SegCLSLoss 0.9258 (0.7668)	KLLoss 0.3477 (0.4076)	MaskLoss 1.3126 (1.1281)	MaskBCELoss 0.4322 (0.2853)	MaskDICELoss 0.8804 (0.8428)
Epoch: [0][ 37/500]	Time  5.462 ( 5.462)	Loss 3.2838 (3.0144)	CeLoss 0.4395 (0.8309)	SegCLSLoss 0.7734 (0.5391)	KLLoss 0.4414 (0.3652)	MaskLoss 1.1185 (0.8654)	MaskBCELoss 0.1540 (0.2153)	MaskDICELoss 0.9645 (0.6501)
Epoch: [0][ 38/500]	Time  6.101 ( 6.101)	Loss 3.4105 (3.3490)	CeLoss 0.4199 (0.4688)	SegCLSLoss 0.7852 (0.6625)	KLLoss 0.3867 (0.4385)	MaskLoss 1.2013 (1.1647)	MaskBCELoss 0.2531 (0.3371)	MaskDICELoss 0.9483 (0.8277)
Epoch: [0][ 39/500]	Time  5.833 ( 5.833)	Loss 3.5813 (2.4011)	CeLoss 0.3984 (0.5430)	SegCLSLoss 0.6641 (0.4090)	KLLoss 0.5234 (0.2875)	MaskLoss 1.2945 (0.7548)	MaskBCELoss 0.4352 (0.2148)	MaskDICELoss 0.8593 (0.5401)
[2025-03-02 16:28:50,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[0.00023099999999999998], mom=[(0.9, 0.95)]
[2025-03-02 16:28:50,289] [INFO] [timer.py:215:stop] epoch=0/micro_step=400/global_step=40, RunningAvgSamplesPerSec=1.6555329037601296, CurrSamplesPerSec=1.4540470539064994, MemAllocated=30.96GB, MaxMemAllocated=35.94GB
Epoch: [0][ 40/500]	Time  6.879 ( 6.879)	Loss 3.3144 (3.0533)	CeLoss 0.4375 (0.3996)	SegCLSLoss 0.6289 (0.5578)	KLLoss 0.3926 (0.4010)	MaskLoss 1.1846 (1.0875)	MaskBCELoss 0.2107 (0.2469)	MaskDICELoss 0.9739 (0.8406)
Epoch: [0][ 41/500]	Time  6.431 ( 6.431)	Loss 2.9540 (2.9140)	CeLoss 0.2930 (0.5350)	SegCLSLoss 0.5234 (0.4238)	KLLoss 0.4082 (0.3721)	MaskLoss 1.0981 (0.9904)	MaskBCELoss 0.1104 (0.2433)	MaskDICELoss 0.9877 (0.7471)
Epoch: [0][ 42/500]	Time  5.709 ( 5.709)	Loss 0.9336 (2.4572)	CeLoss 0.9336 (0.7430)	SegCLSLoss 0.0000 (0.2873)	KLLoss 0.0000 (0.2961)	MaskLoss 0.0000 (0.7112)	MaskBCELoss 0.0000 (0.1357)	MaskDICELoss 0.0000 (0.5755)
Epoch: [0][ 43/500]	Time  6.647 ( 6.647)	Loss 1.4375 (2.5567)	CeLoss 1.4375 (0.5939)	SegCLSLoss 0.0000 (0.2801)	KLLoss 0.0000 (0.3182)	MaskLoss 0.0000 (0.8319)	MaskBCELoss 0.0000 (0.1663)	MaskDICELoss 0.0000 (0.6656)
Epoch: [0][ 44/500]	Time  6.480 ( 6.480)	Loss 0.8086 (2.6327)	CeLoss 0.8086 (0.5250)	SegCLSLoss 0.0000 (0.2424)	KLLoss 0.0000 (0.3246)	MaskLoss 0.0000 (0.9117)	MaskBCELoss 0.0000 (0.2418)	MaskDICELoss 0.0000 (0.6698)
Epoch: [0][ 45/500]	Time  6.787 ( 6.787)	Loss 3.3532 (2.5020)	CeLoss 0.4043 (0.5572)	SegCLSLoss 0.2559 (0.1838)	KLLoss 0.5156 (0.3551)	MaskLoss 1.2821 (0.8379)	MaskBCELoss 0.3380 (0.1661)	MaskDICELoss 0.9440 (0.6718)
Epoch: [0][ 46/500]	Time  6.260 ( 6.260)	Loss 3.1845 (3.1059)	CeLoss 0.3984 (0.5250)	SegCLSLoss 0.2012 (0.2020)	KLLoss 0.5508 (0.4680)	MaskLoss 1.2055 (1.1227)	MaskBCELoss 0.2771 (0.2964)	MaskDICELoss 0.9284 (0.8263)
Epoch: [0][ 47/500]	Time  6.474 ( 6.474)	Loss 3.2227 (2.6700)	CeLoss 0.3027 (0.4131)	SegCLSLoss 0.1953 (0.1685)	KLLoss 0.4688 (0.3701)	MaskLoss 1.2949 (0.9940)	MaskBCELoss 0.4277 (0.2523)	MaskDICELoss 0.8672 (0.7416)
Epoch: [0][ 48/500]	Time  6.006 ( 6.006)	Loss 1.7734 (2.5678)	CeLoss 1.7734 (0.6408)	SegCLSLoss 0.0000 (0.1238)	KLLoss 0.0000 (0.3291)	MaskLoss 0.0000 (0.8505)	MaskBCELoss 0.0000 (0.1919)	MaskDICELoss 0.0000 (0.6586)
Epoch: [0][ 49/500]	Time  5.350 ( 5.350)	Loss 0.3359 (1.7062)	CeLoss 0.3359 (0.6604)	SegCLSLoss 0.0000 (0.0618)	KLLoss 0.0000 (0.1881)	MaskLoss 0.0000 (0.4603)	MaskBCELoss 0.0000 (0.0846)	MaskDICELoss 0.0000 (0.3757)
[2025-03-02 16:29:51,942] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[0.00029099999999999997], mom=[(0.9, 0.95)]
[2025-03-02 16:29:51,948] [INFO] [timer.py:215:stop] epoch=0/micro_step=500/global_step=50, RunningAvgSamplesPerSec=1.6484697461304776, CurrSamplesPerSec=1.8137133166420956, MemAllocated=30.69GB, MaxMemAllocated=35.94GB
Epoch: [0][ 50/500]	Time  5.515 ( 5.515)	Loss 1.0156 (2.8901)	CeLoss 1.0156 (0.3701)	SegCLSLoss 0.0000 (0.1477)	KLLoss 0.0000 (0.4139)	MaskLoss 0.0000 (1.1195)	MaskBCELoss 0.0000 (0.2698)	MaskDICELoss 0.0000 (0.8497)
Epoch: [0][ 51/500]	Time  6.389 ( 6.389)	Loss 3.3009 (2.7652)	CeLoss 0.2676 (0.3531)	SegCLSLoss 0.1113 (0.1144)	KLLoss 0.5234 (0.3965)	MaskLoss 1.3575 (1.0783)	MaskBCELoss 0.4142 (0.3361)	MaskDICELoss 0.9432 (0.7423)
Epoch: [0][ 52/500]	Time  5.848 ( 5.848)	Loss 3.0213 (2.6258)	CeLoss 0.2617 (0.5968)	SegCLSLoss 0.1514 (0.0762)	KLLoss 0.4590 (0.3613)	MaskLoss 1.2275 (0.9051)	MaskBCELoss 0.3485 (0.2781)	MaskDICELoss 0.8789 (0.6270)
Epoch: [0][ 53/500]	Time  5.617 ( 5.617)	Loss 3.4619 (2.6464)	CeLoss 0.4922 (0.4187)	SegCLSLoss 0.0811 (0.1170)	KLLoss 0.5469 (0.3797)	MaskLoss 1.3286 (0.9899)	MaskBCELoss 0.4923 (0.2534)	MaskDICELoss 0.8363 (0.7365)
Epoch: [0][ 54/500]	Time  5.933 ( 5.933)	Loss 3.0720 (1.7361)	CeLoss 0.1992 (0.6216)	SegCLSLoss 0.1138 (0.0467)	KLLoss 0.4316 (0.1930)	MaskLoss 1.2997 (0.4972)	MaskBCELoss 0.5503 (0.1419)	MaskDICELoss 0.7494 (0.3553)
Epoch: [0][ 55/500]	Time  5.470 ( 5.470)	Loss 2.9159 (2.6741)	CeLoss 0.2080 (0.4467)	SegCLSLoss 0.2002 (0.1099)	KLLoss 0.4102 (0.3508)	MaskLoss 1.2001 (0.9984)	MaskBCELoss 0.2732 (0.2661)	MaskDICELoss 0.9269 (0.7322)
Epoch: [0][ 56/500]	Time  7.292 ( 7.292)	Loss 2.7606 (2.5146)	CeLoss 0.2422 (0.3518)	SegCLSLoss 0.1035 (0.0719)	KLLoss 0.4336 (0.3871)	MaskLoss 1.1264 (0.9668)	MaskBCELoss 0.1491 (0.2196)	MaskDICELoss 0.9773 (0.7472)
Epoch: [0][ 57/500]	Time  6.554 ( 6.554)	Loss 2.8320 (2.8526)	CeLoss 0.3242 (0.2705)	SegCLSLoss 0.1406 (0.0949)	KLLoss 0.3945 (0.4055)	MaskLoss 1.1191 (1.1661)	MaskBCELoss 0.1489 (0.3228)	MaskDICELoss 0.9702 (0.8433)
Epoch: [0][ 58/500]	Time  5.259 ( 5.259)	Loss 2.7847 (1.7729)	CeLoss 0.2207 (0.5242)	SegCLSLoss 0.1216 (0.0684)	KLLoss 0.3887 (0.2035)	MaskLoss 1.1541 (0.5562)	MaskBCELoss 0.2136 (0.0729)	MaskDICELoss 0.9405 (0.4833)
Epoch: [0][ 59/500]	Time  6.761 ( 6.761)	Loss 2.9080 (3.0263)	CeLoss 0.2676 (0.2851)	SegCLSLoss 0.1172 (0.1071)	KLLoss 0.3926 (0.4191)	MaskLoss 1.1923 (1.2389)	MaskBCELoss 0.2936 (0.2945)	MaskDICELoss 0.8986 (0.9444)
[2025-03-02 16:30:52,550] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[0.00029895918367346935], mom=[(0.9, 0.95)]
[2025-03-02 16:30:52,555] [INFO] [timer.py:215:stop] epoch=0/micro_step=600/global_step=60, RunningAvgSamplesPerSec=1.6487933255227027, CurrSamplesPerSec=1.823779491211968, MemAllocated=30.86GB, MaxMemAllocated=36.24GB
Epoch: [0][ 60/500]	Time  5.485 ( 5.485)	Loss 0.7656 (1.8849)	CeLoss 0.7656 (0.6207)	SegCLSLoss 0.0000 (0.0601)	KLLoss 0.0000 (0.1988)	MaskLoss 0.0000 (0.5673)	MaskBCELoss 0.0000 (0.0884)	MaskDICELoss 0.0000 (0.4789)
Epoch: [0][ 61/500]	Time  7.777 ( 7.777)	Loss 3.3204 (2.8783)	CeLoss 0.2324 (0.2815)	SegCLSLoss 0.1299 (0.1114)	KLLoss 0.3750 (0.4020)	MaskLoss 1.4180 (1.1701)	MaskBCELoss 0.5380 (0.2167)	MaskDICELoss 0.8800 (0.9534)
Epoch: [0][ 62/500]	Time  6.710 ( 6.710)	Loss 2.6235 (2.7954)	CeLoss 0.2334 (0.3300)	SegCLSLoss 0.0874 (0.0869)	KLLoss 0.4160 (0.3629)	MaskLoss 1.0696 (1.1206)	MaskBCELoss 0.0905 (0.2697)	MaskDICELoss 0.9791 (0.8509)
Epoch: [0][ 63/500]	Time  6.435 ( 6.435)	Loss 2.7023 (2.5397)	CeLoss 0.2656 (0.3361)	SegCLSLoss 0.0674 (0.0756)	KLLoss 0.4336 (0.3254)	MaskLoss 1.0933 (1.0018)	MaskBCELoss 0.1136 (0.2415)	MaskDICELoss 0.9797 (0.7603)
Epoch: [0][ 64/500]	Time  6.341 ( 6.341)	Loss 2.8638 (2.3043)	CeLoss 0.2520 (0.4014)	SegCLSLoss 0.1084 (0.0784)	KLLoss 0.3633 (0.2709)	MaskLoss 1.1877 (0.8639)	MaskBCELoss 0.2670 (0.2357)	MaskDICELoss 0.9207 (0.6282)
Epoch: [0][ 65/500]	Time  6.616 ( 6.616)	Loss 2.7632 (2.1361)	CeLoss 0.2266 (0.5119)	SegCLSLoss 0.1104 (0.0581)	KLLoss 0.3594 (0.2301)	MaskLoss 1.1511 (0.7399)	MaskBCELoss 0.1667 (0.2065)	MaskDICELoss 0.9844 (0.5335)
Epoch: [0][ 66/500]	Time  6.624 ( 6.624)	Loss 3.0034 (2.9812)	CeLoss 0.3496 (0.2768)	SegCLSLoss 0.0659 (0.1036)	KLLoss 0.4023 (0.3674)	MaskLoss 1.2087 (1.2342)	MaskBCELoss 0.2518 (0.3116)	MaskDICELoss 0.9569 (0.9227)
Epoch: [0][ 67/500]	Time  5.965 ( 5.965)	Loss 2.6851 (2.0755)	CeLoss 0.2129 (0.5419)	SegCLSLoss 0.0996 (0.0629)	KLLoss 0.3770 (0.2145)	MaskLoss 1.1179 (0.6977)	MaskBCELoss 0.1284 (0.1412)	MaskDICELoss 0.9895 (0.5565)
Epoch: [0][ 68/500]	Time  5.655 ( 5.655)	Loss 2.9346 (2.5481)	CeLoss 0.3203 (0.4724)	SegCLSLoss 0.0598 (0.0752)	KLLoss 0.4121 (0.2936)	MaskLoss 1.1899 (0.9458)	MaskBCELoss 0.2408 (0.1961)	MaskDICELoss 0.9492 (0.7496)
Epoch: [0][ 69/500]	Time  7.601 ( 7.601)	Loss 3.2808 (2.9947)	CeLoss 0.3145 (0.2815)	SegCLSLoss 0.0625 (0.0908)	KLLoss 0.3945 (0.3572)	MaskLoss 1.3689 (1.2445)	MaskBCELoss 0.4400 (0.2898)	MaskDICELoss 0.9289 (0.9547)
[2025-03-02 16:31:58,586] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[0.000297734693877551], mom=[(0.9, 0.95)]
[2025-03-02 16:31:58,593] [INFO] [timer.py:215:stop] epoch=0/micro_step=700/global_step=70, RunningAvgSamplesPerSec=1.6275879318726254, CurrSamplesPerSec=1.5847339744151645, MemAllocated=30.8GB, MaxMemAllocated=36.24GB
Epoch: [0][ 70/500]	Time  6.312 ( 6.312)	Loss 3.8236 (3.2456)	CeLoss 0.1973 (0.3320)	SegCLSLoss 0.1338 (0.0811)	KLLoss 0.3164 (0.3670)	MaskLoss 1.7008 (1.3449)	MaskBCELoss 0.7904 (0.4006)	MaskDICELoss 0.9104 (0.9442)
Epoch: [0][ 71/500]	Time  5.956 ( 5.956)	Loss 3.0313 (2.6091)	CeLoss 0.2402 (0.4642)	SegCLSLoss 0.1113 (0.0704)	KLLoss 0.3398 (0.2941)	MaskLoss 1.2822 (0.9811)	MaskBCELoss 0.4173 (0.2585)	MaskDICELoss 0.8649 (0.7226)
Epoch: [0][ 72/500]	Time  5.520 ( 5.520)	Loss 3.0980 (2.4287)	CeLoss 0.2314 (0.4750)	SegCLSLoss 0.1133 (0.0652)	KLLoss 0.3320 (0.2533)	MaskLoss 1.3215 (0.8971)	MaskBCELoss 0.4212 (0.2856)	MaskDICELoss 0.9003 (0.6115)
Epoch: [0][ 73/500]	Time  4.870 ( 4.870)	Loss 1.3359 (2.2494)	CeLoss 1.3359 (0.6231)	SegCLSLoss 0.0000 (0.0555)	KLLoss 0.0000 (0.2109)	MaskLoss 0.0000 (0.7466)	MaskBCELoss 0.0000 (0.1899)	MaskDICELoss 0.0000 (0.5567)
Epoch: [0][ 74/500]	Time  6.005 ( 6.005)	Loss 2.7182 (2.5910)	CeLoss 0.2070 (0.4366)	SegCLSLoss 0.1050 (0.0615)	KLLoss 0.3223 (0.2852)	MaskLoss 1.1482 (0.9902)	MaskBCELoss 0.1626 (0.2491)	MaskDICELoss 0.9856 (0.7411)
Epoch: [0][ 75/500]	Time  6.023 ( 6.023)	Loss 3.0050 (3.0202)	CeLoss 0.3418 (0.2909)	SegCLSLoss 0.0593 (0.0788)	KLLoss 0.3711 (0.3588)	MaskLoss 1.2252 (1.2554)	MaskBCELoss 0.3388 (0.3676)	MaskDICELoss 0.8864 (0.8878)
Epoch: [0][ 76/500]	Time  7.455 ( 7.455)	Loss 0.7930 (2.3622)	CeLoss 0.7930 (0.4146)	SegCLSLoss 0.0000 (0.0507)	KLLoss 0.0000 (0.2477)	MaskLoss 0.0000 (0.8992)	MaskBCELoss 0.0000 (0.2551)	MaskDICELoss 0.0000 (0.6441)
Epoch: [0][ 77/500]	Time  6.628 ( 6.628)	Loss 3.4861 (2.3259)	CeLoss 0.3965 (0.3873)	SegCLSLoss 0.0535 (0.0599)	KLLoss 0.3906 (0.2443)	MaskLoss 1.4345 (0.8933)	MaskBCELoss 0.5260 (0.2504)	MaskDICELoss 0.9085 (0.6430)
Epoch: [0][ 78/500]	Time  7.547 ( 7.547)	Loss 3.7078 (2.6858)	CeLoss 0.2910 (0.3434)	SegCLSLoss 0.0542 (0.0673)	KLLoss 0.3574 (0.3006)	MaskLoss 1.6059 (1.0792)	MaskBCELoss 0.7369 (0.2373)	MaskDICELoss 0.8690 (0.8419)
Epoch: [0][ 79/500]	Time  6.800 ( 6.800)	Loss 2.9796 (2.6515)	CeLoss 0.3828 (0.2383)	SegCLSLoss 0.0664 (0.0816)	KLLoss 0.3477 (0.2967)	MaskLoss 1.1949 (1.1120)	MaskBCELoss 0.3086 (0.3046)	MaskDICELoss 0.8863 (0.8074)
[2025-03-02 16:33:01,128] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[0.0002965102040816326], mom=[(0.9, 0.95)]
[2025-03-02 16:33:01,134] [INFO] [timer.py:215:stop] epoch=0/micro_step=800/global_step=80, RunningAvgSamplesPerSec=1.6239060779223675, CurrSamplesPerSec=1.7435854714402361, MemAllocated=30.73GB, MaxMemAllocated=36.25GB
Epoch: [0][ 80/500]	Time  5.737 ( 5.737)	Loss 1.0703 (2.4413)	CeLoss 1.0703 (0.3350)	SegCLSLoss 0.0000 (0.0793)	KLLoss 0.0000 (0.2553)	MaskLoss 0.0000 (0.9698)	MaskBCELoss 0.0000 (0.2350)	MaskDICELoss 0.0000 (0.7348)
Epoch: [0][ 81/500]	Time  5.863 ( 5.863)	Loss 2.9243 (2.5083)	CeLoss 0.2012 (0.3953)	SegCLSLoss 0.0483 (0.0515)	KLLoss 0.3438 (0.2678)	MaskLoss 1.2630 (0.9766)	MaskBCELoss 0.3852 (0.2579)	MaskDICELoss 0.8778 (0.7187)
Epoch: [0][ 82/500]	Time  6.072 ( 6.072)	Loss 2.6395 (2.4969)	CeLoss 0.2656 (0.4082)	SegCLSLoss 0.0801 (0.0478)	KLLoss 0.3184 (0.2625)	MaskLoss 1.0873 (0.9668)	MaskBCELoss 0.1009 (0.2181)	MaskDICELoss 0.9865 (0.7487)
Epoch: [0][ 83/500]	Time  5.747 ( 5.747)	Loss 2.5472 (2.4633)	CeLoss 0.1836 (0.3942)	SegCLSLoss 0.0732 (0.0641)	KLLoss 0.3105 (0.2553)	MaskLoss 1.0861 (0.9549)	MaskBCELoss 0.1046 (0.1958)	MaskDICELoss 0.9815 (0.7591)
Epoch: [0][ 84/500]	Time  6.818 ( 6.818)	Loss 2.6830 (2.6522)	CeLoss 0.3047 (0.3424)	SegCLSLoss 0.0669 (0.0762)	KLLoss 0.3184 (0.2771)	MaskLoss 1.0935 (1.0667)	MaskBCELoss 0.1085 (0.2235)	MaskDICELoss 0.9849 (0.8432)
Epoch: [0][ 85/500]	Time  6.155 ( 6.155)	Loss 0.9727 (2.6091)	CeLoss 0.9727 (0.4603)	SegCLSLoss 0.0000 (0.0496)	KLLoss 0.0000 (0.2562)	MaskLoss 0.0000 (0.9980)	MaskBCELoss 0.0000 (0.2385)	MaskDICELoss 0.0000 (0.7595)
Epoch: [0][ 86/500]	Time  7.074 ( 7.074)	Loss 2.6982 (2.5443)	CeLoss 0.2266 (0.3201)	SegCLSLoss 0.0608 (0.0636)	KLLoss 0.3281 (0.2869)	MaskLoss 1.1381 (1.0245)	MaskBCELoss 0.1939 (0.1750)	MaskDICELoss 0.9442 (0.8495)
Epoch: [0][ 87/500]	Time  6.795 ( 6.795)	Loss 2.5043 (2.8968)	CeLoss 0.2080 (0.2737)	SegCLSLoss 0.0752 (0.0787)	KLLoss 0.3008 (0.3150)	MaskLoss 1.0539 (1.2132)	MaskBCELoss 0.1040 (0.2863)	MaskDICELoss 0.9499 (0.9269)
Epoch: [0][ 88/500]	Time  5.676 ( 5.676)	Loss 2.8417 (2.3147)	CeLoss 0.3008 (0.3370)	SegCLSLoss 0.0444 (0.0513)	KLLoss 0.3359 (0.2523)	MaskLoss 1.1757 (0.9131)	MaskBCELoss 0.2295 (0.1666)	MaskDICELoss 0.9462 (0.7465)
Epoch: [0][ 89/500]	Time  6.067 ( 6.067)	Loss 0.9883 (2.1894)	CeLoss 0.9883 (0.4549)	SegCLSLoss 0.0000 (0.0480)	KLLoss 0.0000 (0.2168)	MaskLoss 0.0000 (0.8009)	MaskBCELoss 0.0000 (0.1400)	MaskDICELoss 0.0000 (0.6609)
[2025-03-02 16:34:04,134] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[0.0002952857142857143], mom=[(0.9, 0.95)]
[2025-03-02 16:34:04,140] [INFO] [timer.py:215:stop] epoch=0/micro_step=900/global_step=90, RunningAvgSamplesPerSec=1.619681528654259, CurrSamplesPerSec=1.4839834105934726, MemAllocated=31.62GB, MaxMemAllocated=36.25GB
Epoch: [0][ 90/500]	Time  6.740 ( 6.740)	Loss 0.8125 (2.5793)	CeLoss 0.8125 (0.3125)	SegCLSLoss 0.0000 (0.0586)	KLLoss 0.0000 (0.2799)	MaskLoss 0.0000 (1.0486)	MaskBCELoss 0.0000 (0.2421)	MaskDICELoss 0.0000 (0.8066)
Epoch: [0][ 91/500]	Time  5.550 ( 5.550)	Loss 0.1748 (1.9276)	CeLoss 0.1748 (0.4809)	SegCLSLoss 0.0000 (0.0309)	KLLoss 0.0000 (0.1559)	MaskLoss 0.0000 (0.6770)	MaskBCELoss 0.0000 (0.2118)	MaskDICELoss 0.0000 (0.4652)
Epoch: [0][ 92/500]	Time  5.622 ( 5.622)	Loss 1.1875 (1.6726)	CeLoss 1.1875 (0.4727)	SegCLSLoss 0.0000 (0.0421)	KLLoss 0.0000 (0.1500)	MaskLoss 0.0000 (0.5520)	MaskBCELoss 0.0000 (0.0731)	MaskDICELoss 0.0000 (0.4789)
Epoch: [0][ 93/500]	Time  6.674 ( 6.674)	Loss 2.5952 (2.6248)	CeLoss 0.2734 (0.2297)	SegCLSLoss 0.0549 (0.0584)	KLLoss 0.3047 (0.2703)	MaskLoss 1.0710 (1.1154)	MaskBCELoss 0.0726 (0.3043)	MaskDICELoss 0.9985 (0.8111)
Epoch: [0][ 94/500]	Time  6.785 ( 6.785)	Loss 2.6711 (2.8249)	CeLoss 0.2285 (0.2673)	SegCLSLoss 0.0540 (0.0565)	KLLoss 0.2969 (0.3043)	MaskLoss 1.1334 (1.1884)	MaskBCELoss 0.1514 (0.2673)	MaskDICELoss 0.9821 (0.9211)
Epoch: [0][ 95/500]	Time  5.788 ( 5.788)	Loss 0.3789 (2.1452)	CeLoss 0.3789 (0.4131)	SegCLSLoss 0.0000 (0.0522)	KLLoss 0.0000 (0.2078)	MaskLoss 0.0000 (0.8012)	MaskBCELoss 0.0000 (0.1473)	MaskDICELoss 0.0000 (0.6539)
Epoch: [0][ 96/500]	Time  6.954 ( 6.954)	Loss 2.4028 (2.9600)	CeLoss 0.2031 (0.3432)	SegCLSLoss 0.0762 (0.0584)	KLLoss 0.2871 (0.2711)	MaskLoss 1.0090 (1.2261)	MaskBCELoss 0.0101 (0.3940)	MaskDICELoss 0.9989 (0.8321)
Epoch: [0][ 97/500]	Time  6.214 ( 6.214)	Loss 2.5432 (2.2279)	CeLoss 0.2109 (0.4650)	SegCLSLoss 0.0598 (0.0411)	KLLoss 0.2949 (0.2145)	MaskLoss 1.0773 (0.8176)	MaskBCELoss 0.0922 (0.1434)	MaskDICELoss 0.9851 (0.6742)
Epoch: [0][ 98/500]	Time  7.108 ( 7.108)	Loss 2.5649 (2.5155)	CeLoss 0.2256 (0.2224)	SegCLSLoss 0.0972 (0.0596)	KLLoss 0.2988 (0.2727)	MaskLoss 1.0715 (1.0635)	MaskBCELoss 0.0788 (0.2257)	MaskDICELoss 0.9927 (0.8378)
Epoch: [0][ 99/500]	Time  5.390 ( 5.390)	Loss 2.6944 (2.4538)	CeLoss 0.2383 (0.4660)	SegCLSLoss 0.0481 (0.0396)	KLLoss 0.3145 (0.2451)	MaskLoss 1.1382 (0.9229)	MaskBCELoss 0.1767 (0.1771)	MaskDICELoss 0.9615 (0.7458)
[2025-03-02 16:35:06,118] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[0.00029406122448979587], mom=[(0.9, 0.95)]
[2025-03-02 16:35:06,124] [INFO] [timer.py:215:stop] epoch=0/micro_step=1000/global_step=100, RunningAvgSamplesPerSec=1.6190662317832658, CurrSamplesPerSec=1.6960712592683231, MemAllocated=31.24GB, MaxMemAllocated=36.32GB
Epoch: [0][100/500]	Time  5.898 ( 5.898)	Loss 3.2955 (2.3489)	CeLoss 0.4434 (0.6698)	SegCLSLoss 0.0439 (0.0357)	KLLoss 0.3086 (0.1805)	MaskLoss 1.3392 (0.7858)	MaskBCELoss 0.4119 (0.2536)	MaskDICELoss 0.9273 (0.5322)
Epoch: [0][101/500]	Time  5.558 ( 5.558)	Loss 1.4219 (2.3852)	CeLoss 1.4219 (0.5498)	SegCLSLoss 0.0000 (0.0366)	KLLoss 0.0000 (0.2125)	MaskLoss 0.0000 (0.8555)	MaskBCELoss 0.0000 (0.2152)	MaskDICELoss 0.0000 (0.6403)
Epoch: [0][102/500]	Time  6.264 ( 6.264)	Loss 2.8153 (2.4529)	CeLoss 0.2236 (0.4181)	SegCLSLoss 0.0933 (0.0490)	KLLoss 0.2832 (0.2416)	MaskLoss 1.2026 (0.9448)	MaskBCELoss 0.3134 (0.1949)	MaskDICELoss 0.8891 (0.7499)
Epoch: [0][103/500]	Time  7.209 ( 7.209)	Loss 2.9501 (2.6867)	CeLoss 0.2441 (0.3656)	SegCLSLoss 0.0520 (0.0435)	KLLoss 0.3086 (0.2744)	MaskLoss 1.2631 (1.0811)	MaskBCELoss 0.4101 (0.2346)	MaskDICELoss 0.8530 (0.8465)
Epoch: [0][104/500]	Time  7.162 ( 7.162)	Loss 3.1466 (2.6481)	CeLoss 0.2422 (0.3165)	SegCLSLoss 0.0425 (0.0621)	KLLoss 0.3086 (0.2641)	MaskLoss 1.3643 (1.0843)	MaskBCELoss 0.4312 (0.2449)	MaskDICELoss 0.9331 (0.8394)
Epoch: [0][105/500]	Time  6.545 ( 6.545)	Loss 2.6990 (2.5706)	CeLoss 0.2773 (0.4777)	SegCLSLoss 0.0508 (0.0381)	KLLoss 0.2910 (0.2396)	MaskLoss 1.1249 (0.9772)	MaskBCELoss 0.1644 (0.2299)	MaskDICELoss 0.9604 (0.7473)
Epoch: [0][106/500]	Time  6.140 ( 6.140)	Loss 2.6791 (2.6580)	CeLoss 0.2734 (0.2651)	SegCLSLoss 0.0432 (0.0484)	KLLoss 0.3105 (0.2703)	MaskLoss 1.1150 (1.1169)	MaskBCELoss 0.1551 (0.3170)	MaskDICELoss 0.9598 (0.7999)
Epoch: [0][107/500]	Time  6.442 ( 6.442)	Loss 1.3828 (2.4581)	CeLoss 1.3828 (0.4312)	SegCLSLoss 0.0000 (0.0464)	KLLoss 0.0000 (0.2408)	MaskLoss 0.0000 (0.9419)	MaskBCELoss 0.0000 (0.2170)	MaskDICELoss 0.0000 (0.7249)
Epoch: [0][108/500]	Time  5.536 ( 5.536)	Loss 2.8084 (2.2623)	CeLoss 0.3203 (0.4829)	SegCLSLoss 0.0410 (0.0299)	KLLoss 0.3066 (0.2119)	MaskLoss 1.1581 (0.8293)	MaskBCELoss 0.2234 (0.1777)	MaskDICELoss 0.9347 (0.6516)
Epoch: [0][109/500]	Time  5.642 ( 5.642)	Loss 2.8711 (2.3067)	CeLoss 0.2041 (0.4711)	SegCLSLoss 0.0559 (0.0347)	KLLoss 0.2910 (0.2090)	MaskLoss 1.2461 (0.8567)	MaskBCELoss 0.3660 (0.2167)	MaskDICELoss 0.8801 (0.6399)
[2025-03-02 16:36:08,832] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[0.00029283673469387755], mom=[(0.9, 0.95)]
[2025-03-02 16:36:08,837] [INFO] [timer.py:215:stop] epoch=0/micro_step=1100/global_step=110, RunningAvgSamplesPerSec=1.6167997662287665, CurrSamplesPerSec=1.6094193862885258, MemAllocated=31.24GB, MaxMemAllocated=36.58GB
Epoch: [0][110/500]	Time  6.215 ( 6.215)	Loss 2.9285 (2.5550)	CeLoss 0.3633 (0.4787)	SegCLSLoss 0.0312 (0.0314)	KLLoss 0.3105 (0.2463)	MaskLoss 1.1967 (0.9685)	MaskBCELoss 0.3160 (0.2603)	MaskDICELoss 0.8806 (0.7082)
Epoch: [0][111/500]	Time  5.711 ( 5.711)	Loss 2.7493 (2.5077)	CeLoss 0.2246 (0.4641)	SegCLSLoss 0.0386 (0.0290)	KLLoss 0.3008 (0.2418)	MaskLoss 1.1774 (0.9541)	MaskBCELoss 0.2607 (0.2432)	MaskDICELoss 0.9167 (0.7109)
Epoch: [0][112/500]	Time  5.672 ( 5.672)	Loss 2.4016 (2.3915)	CeLoss 0.1758 (0.5620)	SegCLSLoss 0.0549 (0.0281)	KLLoss 0.2812 (0.2090)	MaskLoss 1.0289 (0.8556)	MaskBCELoss 0.0332 (0.2241)	MaskDICELoss 0.9958 (0.6315)
Epoch: [0][113/500]	Time  6.571 ( 6.571)	Loss 4.0168 (2.5844)	CeLoss 0.3379 (0.2369)	SegCLSLoss 0.0579 (0.0465)	KLLoss 0.2930 (0.2641)	MaskLoss 1.7506 (1.0963)	MaskBCELoss 0.8892 (0.2570)	MaskDICELoss 0.8613 (0.8392)
Epoch: [0][114/500]	Time  5.862 ( 5.862)	Loss 1.5938 (2.4397)	CeLoss 1.5938 (0.5536)	SegCLSLoss 0.0000 (0.0404)	KLLoss 0.0000 (0.2062)	MaskLoss 0.0000 (0.8813)	MaskBCELoss 0.0000 (0.2555)	MaskDICELoss 0.0000 (0.6258)
Epoch: [0][115/500]	Time  7.381 ( 7.381)	Loss 2.7837 (2.1213)	CeLoss 0.2207 (0.3316)	SegCLSLoss 0.0588 (0.0398)	KLLoss 0.2773 (0.2002)	MaskLoss 1.1975 (0.8348)	MaskBCELoss 0.2425 (0.1807)	MaskDICELoss 0.9550 (0.6540)
Epoch: [0][116/500]	Time  5.542 ( 5.542)	Loss 0.8867 (1.9172)	CeLoss 0.8867 (0.4609)	SegCLSLoss 0.0000 (0.0273)	KLLoss 0.0000 (0.1736)	MaskLoss 0.0000 (0.6780)	MaskBCELoss 0.0000 (0.1430)	MaskDICELoss 0.0000 (0.5350)
Epoch: [0][117/500]	Time  4.992 ( 4.992)	Loss 2.3768 (2.3885)	CeLoss 0.1719 (0.4337)	SegCLSLoss 0.0339 (0.0297)	KLLoss 0.2969 (0.2318)	MaskLoss 1.0204 (0.9122)	MaskBCELoss 0.0236 (0.1306)	MaskDICELoss 0.9968 (0.7816)
Epoch: [0][118/500]	Time  6.145 ( 6.145)	Loss 2.4800 (1.9888)	CeLoss 0.2217 (0.4995)	SegCLSLoss 0.0618 (0.0358)	KLLoss 0.2754 (0.1711)	MaskLoss 1.0447 (0.6931)	MaskBCELoss 0.0654 (0.1331)	MaskDICELoss 0.9793 (0.5600)
Epoch: [0][119/500]	Time  6.157 ( 6.157)	Loss 3.2576 (2.1788)	CeLoss 0.3496 (0.4352)	SegCLSLoss 0.0344 (0.0249)	KLLoss 0.2910 (0.2027)	MaskLoss 1.3729 (0.8149)	MaskBCELoss 0.5818 (0.1725)	MaskDICELoss 0.7912 (0.6424)
[2025-03-02 16:37:08,856] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[0.0002916122448979592], mom=[(0.9, 0.95)]
[2025-03-02 16:37:08,862] [INFO] [timer.py:215:stop] epoch=0/micro_step=1200/global_step=120, RunningAvgSamplesPerSec=1.6208886467646655, CurrSamplesPerSec=1.6693837626344963, MemAllocated=30.95GB, MaxMemAllocated=36.58GB
Epoch: [0][120/500]	Time  5.992 ( 5.992)	Loss 2.5571 (2.5798)	CeLoss 0.2930 (0.3626)	SegCLSLoss 0.0493 (0.0468)	KLLoss 0.2832 (0.2553)	MaskLoss 1.0481 (1.0331)	MaskBCELoss 0.0584 (0.2020)	MaskDICELoss 0.9897 (0.8310)
Epoch: [0][121/500]	Time  4.798 ( 4.798)	Loss 1.6484 (2.0499)	CeLoss 1.6484 (0.7207)	SegCLSLoss 0.0000 (0.0432)	KLLoss 0.0000 (0.1410)	MaskLoss 0.0000 (0.6186)	MaskBCELoss 0.0000 (0.1512)	MaskDICELoss 0.0000 (0.4674)
Epoch: [0][122/500]	Time  6.098 ( 6.098)	Loss 0.7070 (2.4466)	CeLoss 0.7070 (0.3845)	SegCLSLoss 0.0000 (0.0281)	KLLoss 0.0000 (0.2008)	MaskLoss 0.0000 (0.9738)	MaskBCELoss 0.0000 (0.3820)	MaskDICELoss 0.0000 (0.5918)
Epoch: [0][123/500]	Time  5.111 ( 5.111)	Loss 2.8062 (2.0323)	CeLoss 0.2852 (0.5106)	SegCLSLoss 0.0210 (0.0281)	KLLoss 0.2871 (0.1693)	MaskLoss 1.1843 (0.7115)	MaskBCELoss 0.2202 (0.1648)	MaskDICELoss 0.9641 (0.5468)
Epoch: [0][124/500]	Time  6.690 ( 6.690)	Loss 2.5519 (2.4468)	CeLoss 0.2314 (0.3652)	SegCLSLoss 0.0322 (0.0387)	KLLoss 0.2871 (0.2270)	MaskLoss 1.0807 (0.9744)	MaskBCELoss 0.0836 (0.2550)	MaskDICELoss 0.9971 (0.7194)
Epoch: [0][125/500]	Time  4.977 ( 4.977)	Loss 2.8687 (1.9599)	CeLoss 0.2139 (0.6397)	SegCLSLoss 0.0532 (0.0238)	KLLoss 0.2754 (0.1410)	MaskLoss 1.2449 (0.6189)	MaskBCELoss 0.3661 (0.1785)	MaskDICELoss 0.8788 (0.4404)
Epoch: [0][126/500]	Time  7.391 ( 7.391)	Loss 2.6666 (2.3743)	CeLoss 0.2266 (0.3905)	SegCLSLoss 0.0635 (0.0331)	KLLoss 0.2773 (0.2301)	MaskLoss 1.1350 (0.9261)	MaskBCELoss 0.1606 (0.1664)	MaskDICELoss 0.9744 (0.7597)
Epoch: [0][127/500]	Time  4.968 ( 4.968)	Loss 2.6121 (1.9480)	CeLoss 0.3398 (0.5524)	SegCLSLoss 0.0284 (0.0255)	KLLoss 0.2910 (0.1717)	MaskLoss 1.0560 (0.6486)	MaskBCELoss 0.0734 (0.0750)	MaskDICELoss 0.9826 (0.5735)
Epoch: [0][128/500]	Time  5.074 ( 5.074)	Loss 0.7031 (1.9821)	CeLoss 0.7031 (0.6396)	SegCLSLoss 0.0000 (0.0268)	KLLoss 0.0000 (0.1455)	MaskLoss 0.0000 (0.6281)	MaskBCELoss 0.0000 (0.1686)	MaskDICELoss 0.0000 (0.4595)
Epoch: [0][129/500]	Time  6.925 ( 6.925)	Loss 2.3722 (2.7442)	CeLoss 0.1875 (0.4461)	SegCLSLoss 0.0623 (0.0370)	KLLoss 0.2852 (0.2322)	MaskLoss 1.0054 (1.0819)	MaskBCELoss 0.0152 (0.3374)	MaskDICELoss 0.9903 (0.7445)
[2025-03-02 16:38:06,459] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[0.00029038775510204076], mom=[(0.9, 0.95)]
[2025-03-02 16:38:06,465] [INFO] [timer.py:215:stop] epoch=0/micro_step=1300/global_step=130, RunningAvgSamplesPerSec=1.6293582115463356, CurrSamplesPerSec=1.795785518942901, MemAllocated=31.26GB, MaxMemAllocated=36.58GB
Epoch: [0][130/500]	Time  5.570 ( 5.570)	Loss 2.7105 (1.8489)	CeLoss 0.2002 (0.5363)	SegCLSLoss 0.0723 (0.0264)	KLLoss 0.2773 (0.1426)	MaskLoss 1.1677 (0.6142)	MaskBCELoss 0.2575 (0.1487)	MaskDICELoss 0.9103 (0.4654)
Epoch: [0][131/500]	Time  5.243 ( 5.243)	Loss 0.3867 (2.1770)	CeLoss 0.3867 (0.4627)	SegCLSLoss 0.0000 (0.0452)	KLLoss 0.0000 (0.2008)	MaskLoss 0.0000 (0.7958)	MaskBCELoss 0.0000 (0.1552)	MaskDICELoss 0.0000 (0.6405)
Epoch: [0][132/500]	Time  6.717 ( 6.717)	Loss 0.0996 (2.1437)	CeLoss 0.0996 (0.3214)	SegCLSLoss 0.0000 (0.0361)	KLLoss 0.0000 (0.2055)	MaskLoss 0.0000 (0.8508)	MaskBCELoss 0.0000 (0.2423)	MaskDICELoss 0.0000 (0.6086)
Epoch: [0][133/500]	Time  5.813 ( 5.813)	Loss 2.5259 (2.2938)	CeLoss 0.1836 (0.5673)	SegCLSLoss 0.0608 (0.0380)	KLLoss 0.2949 (0.2027)	MaskLoss 1.0823 (0.8031)	MaskBCELoss 0.0959 (0.1318)	MaskDICELoss 0.9864 (0.6712)
Epoch: [0][134/500]	Time  5.635 ( 5.635)	Loss 2.7014 (2.5370)	CeLoss 0.2393 (0.4241)	SegCLSLoss 0.0398 (0.0406)	KLLoss 0.2910 (0.2334)	MaskLoss 1.1476 (0.9879)	MaskBCELoss 0.1628 (0.2531)	MaskDICELoss 0.9848 (0.7348)
Epoch: [0][135/500]	Time  6.262 ( 6.262)	Loss 2.7941 (2.6045)	CeLoss 0.2871 (0.3131)	SegCLSLoss 0.0376 (0.0398)	KLLoss 0.2969 (0.2605)	MaskLoss 1.1695 (1.0706)	MaskBCELoss 0.2740 (0.2620)	MaskDICELoss 0.8956 (0.8087)
Epoch: [0][136/500]	Time  6.099 ( 6.099)	Loss 2.6777 (2.2383)	CeLoss 0.2832 (0.3646)	SegCLSLoss 0.0464 (0.0385)	KLLoss 0.2793 (0.2309)	MaskLoss 1.1162 (0.8695)	MaskBCELoss 0.2103 (0.0959)	MaskDICELoss 0.9059 (0.7737)
Epoch: [0][137/500]	Time  5.749 ( 5.749)	Loss 1.5391 (2.3554)	CeLoss 1.5391 (0.5580)	SegCLSLoss 0.0000 (0.0286)	KLLoss 0.0000 (0.1996)	MaskLoss 0.0000 (0.8416)	MaskBCELoss 0.0000 (0.2361)	MaskDICELoss 0.0000 (0.6055)
Epoch: [0][138/500]	Time  6.184 ( 6.184)	Loss 2.4053 (2.8342)	CeLoss 0.2197 (0.3358)	SegCLSLoss 0.0396 (0.0435)	KLLoss 0.2754 (0.2568)	MaskLoss 1.0142 (1.1740)	MaskBCELoss 0.0178 (0.3332)	MaskDICELoss 0.9964 (0.8408)
Epoch: [0][139/500]	Time  6.692 ( 6.692)	Loss 1.0703 (2.5058)	CeLoss 1.0703 (0.4538)	SegCLSLoss 0.0000 (0.0408)	KLLoss 0.0000 (0.2246)	MaskLoss 0.0000 (0.9595)	MaskBCELoss 0.0000 (0.2299)	MaskDICELoss 0.0000 (0.7295)
[2025-03-02 16:39:07,544] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[0.00028916326530612244], mom=[(0.9, 0.95)]
[2025-03-02 16:39:07,550] [INFO] [timer.py:215:stop] epoch=0/micro_step=1400/global_step=140, RunningAvgSamplesPerSec=1.6299402121337172, CurrSamplesPerSec=1.4948171488932045, MemAllocated=31.0GB, MaxMemAllocated=36.58GB
Epoch: [0][140/500]	Time  6.692 ( 6.692)	Loss 0.7930 (2.2116)	CeLoss 0.7930 (0.4918)	SegCLSLoss 0.0000 (0.0368)	KLLoss 0.0000 (0.2020)	MaskLoss 0.0000 (0.8004)	MaskBCELoss 0.0000 (0.1273)	MaskDICELoss 0.0000 (0.6731)
Epoch: [0][141/500]	Time  6.855 ( 6.855)	Loss 2.5668 (2.0822)	CeLoss 0.2100 (0.3423)	SegCLSLoss 0.0457 (0.0241)	KLLoss 0.2734 (0.2021)	MaskLoss 1.0988 (0.8132)	MaskBCELoss 0.1062 (0.1491)	MaskDICELoss 0.9926 (0.6641)
Epoch: [0][142/500]	Time  6.297 ( 6.297)	Loss 3.1280 (2.5601)	CeLoss 0.2754 (0.3818)	SegCLSLoss 0.0221 (0.0275)	KLLoss 0.2949 (0.2348)	MaskLoss 1.3472 (1.0234)	MaskBCELoss 0.5318 (0.3396)	MaskDICELoss 0.8154 (0.6839)
Epoch: [0][143/500]	Time  6.547 ( 6.547)	Loss 2.7004 (2.6172)	CeLoss 0.2676 (0.3187)	SegCLSLoss 0.0334 (0.0334)	KLLoss 0.2793 (0.2684)	MaskLoss 1.1373 (1.0736)	MaskBCELoss 0.1600 (0.2792)	MaskDICELoss 0.9772 (0.7943)
Epoch: [0][144/500]	Time  5.364 ( 5.364)	Loss 1.0859 (2.4908)	CeLoss 1.0859 (0.4429)	SegCLSLoss 0.0000 (0.0356)	KLLoss 0.0000 (0.2320)	MaskLoss 0.0000 (0.9571)	MaskBCELoss 0.0000 (0.2692)	MaskDICELoss 0.0000 (0.6879)
Epoch: [0][145/500]	Time  5.949 ( 5.949)	Loss 2.6816 (2.0538)	CeLoss 0.2578 (0.4046)	SegCLSLoss 0.0193 (0.0244)	KLLoss 0.2988 (0.2025)	MaskLoss 1.1318 (0.7679)	MaskBCELoss 0.3021 (0.1281)	MaskDICELoss 0.8297 (0.6398)
Epoch: [0][146/500]	Time  5.914 ( 5.914)	Loss 2.6314 (2.7796)	CeLoss 0.3184 (0.3792)	SegCLSLoss 0.0192 (0.0326)	KLLoss 0.3027 (0.2547)	MaskLoss 1.0754 (1.1285)	MaskBCELoss 0.2220 (0.3390)	MaskDICELoss 0.8535 (0.7894)
Epoch: [0][147/500]	Time  6.779 ( 6.779)	Loss 1.0625 (2.2511)	CeLoss 1.0625 (0.3386)	SegCLSLoss 0.0000 (0.0399)	KLLoss 0.0000 (0.2295)	MaskLoss 0.0000 (0.8891)	MaskBCELoss 0.0000 (0.1775)	MaskDICELoss 0.0000 (0.7116)
Epoch: [0][148/500]	Time  6.701 ( 6.701)	Loss 2.5476 (2.1235)	CeLoss 0.2295 (0.5040)	SegCLSLoss 0.0352 (0.0237)	KLLoss 0.2832 (0.1965)	MaskLoss 1.0795 (0.7546)	MaskBCELoss 0.1272 (0.1240)	MaskDICELoss 0.9523 (0.6306)
Epoch: [0][149/500]	Time  7.277 ( 7.277)	Loss 2.4594 (2.6684)	CeLoss 0.1982 (0.2469)	SegCLSLoss 0.0300 (0.0389)	KLLoss 0.2910 (0.2861)	MaskLoss 1.0500 (1.1296)	MaskBCELoss 0.0652 (0.2634)	MaskDICELoss 0.9849 (0.8662)
[2025-03-02 16:40:12,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[0.000287938775510204], mom=[(0.9, 0.95)]
[2025-03-02 16:40:12,557] [INFO] [timer.py:215:stop] epoch=0/micro_step=1500/global_step=150, RunningAvgSamplesPerSec=1.6234298047997489, CurrSamplesPerSec=1.365487083978629, MemAllocated=31.24GB, MaxMemAllocated=36.58GB
Epoch: [0][150/500]	Time  7.325 ( 7.325)	Loss 2.4764 (2.4315)	CeLoss 0.1973 (0.4454)	SegCLSLoss 0.0391 (0.0321)	KLLoss 0.2773 (0.2275)	MaskLoss 1.0605 (0.9282)	MaskBCELoss 0.0667 (0.2347)	MaskDICELoss 0.9937 (0.6934)
Epoch: [0][151/500]	Time  7.101 ( 7.101)	Loss 2.6316 (2.6944)	CeLoss 0.2344 (0.2572)	SegCLSLoss 0.0305 (0.0475)	KLLoss 0.2949 (0.2863)	MaskLoss 1.1166 (1.1351)	MaskBCELoss 0.4194 (0.2479)	MaskDICELoss 0.6972 (0.8872)
Epoch: [0][152/500]	Time  5.078 ( 5.078)	Loss 2.7624 (2.1977)	CeLoss 0.2695 (0.5737)	SegCLSLoss 0.0265 (0.0256)	KLLoss 0.2969 (0.1707)	MaskLoss 1.1654 (0.7628)	MaskBCELoss 0.1795 (0.2178)	MaskDICELoss 0.9858 (0.5450)
Epoch: [0][153/500]	Time  5.568 ( 5.568)	Loss 1.2969 (2.0665)	CeLoss 1.2969 (0.5661)	SegCLSLoss 0.0000 (0.0296)	KLLoss 0.0000 (0.1697)	MaskLoss 0.0000 (0.7004)	MaskBCELoss 0.0000 (0.1702)	MaskDICELoss 0.0000 (0.5302)
Epoch: [0][154/500]	Time  7.245 ( 7.245)	Loss 2.5023 (2.4697)	CeLoss 0.1865 (0.4106)	SegCLSLoss 0.0403 (0.0334)	KLLoss 0.2988 (0.2355)	MaskLoss 1.0734 (0.9622)	MaskBCELoss 0.4459 (0.2917)	MaskDICELoss 0.6275 (0.6705)
Epoch: [0][155/500]	Time  6.495 ( 6.495)	Loss 2.4611 (2.0006)	CeLoss 0.1992 (0.3763)	SegCLSLoss 0.0549 (0.0305)	KLLoss 0.2793 (0.1975)	MaskLoss 1.0469 (0.7551)	MaskBCELoss 0.0549 (0.1100)	MaskDICELoss 0.9920 (0.6451)
Epoch: [0][156/500]	Time  6.658 ( 6.658)	Loss 3.2190 (2.1404)	CeLoss 0.2070 (0.4276)	SegCLSLoss 0.0476 (0.0288)	KLLoss 0.2734 (0.1977)	MaskLoss 1.4259 (0.7997)	MaskBCELoss 0.5587 (0.1591)	MaskDICELoss 0.8672 (0.6406)
Epoch: [0][157/500]	Time  6.720 ( 6.720)	Loss 2.7047 (2.6365)	CeLoss 0.1543 (0.2425)	SegCLSLoss 0.0850 (0.0478)	KLLoss 0.2695 (0.2500)	MaskLoss 1.1863 (1.1225)	MaskBCELoss 0.2710 (0.3249)	MaskDICELoss 0.9153 (0.7977)
Epoch: [0][158/500]	Time  5.617 ( 5.617)	Loss 2.4402 (2.3374)	CeLoss 0.2090 (0.4986)	SegCLSLoss 0.0295 (0.0250)	KLLoss 0.2754 (0.2230)	MaskLoss 1.0394 (0.8574)	MaskBCELoss 0.0477 (0.1705)	MaskDICELoss 0.9917 (0.6869)
Epoch: [0][159/500]	Time  5.276 ( 5.276)	Loss 2.9456 (2.3513)	CeLoss 0.1982 (0.5098)	SegCLSLoss 0.0732 (0.0343)	KLLoss 0.2656 (0.1914)	MaskLoss 1.2892 (0.8644)	MaskBCELoss 0.4047 (0.2550)	MaskDICELoss 0.8845 (0.6094)
[2025-03-02 16:41:16,054] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[0.0002867142857142857], mom=[(0.9, 0.95)]
[2025-03-02 16:41:16,059] [INFO] [timer.py:215:stop] epoch=0/micro_step=1600/global_step=160, RunningAvgSamplesPerSec=1.6202855080579475, CurrSamplesPerSec=1.292082119206114, MemAllocated=30.75GB, MaxMemAllocated=36.58GB
Epoch: [0][160/500]	Time  7.743 ( 7.743)	Loss 0.8125 (2.5190)	CeLoss 0.8125 (0.3136)	SegCLSLoss 0.0000 (0.0309)	KLLoss 0.0000 (0.2525)	MaskLoss 0.0000 (1.0320)	MaskBCELoss 0.0000 (0.2491)	MaskDICELoss 0.0000 (0.7828)
Epoch: [0][161/500]	Time  6.362 ( 6.362)	Loss 2.4634 (2.4143)	CeLoss 0.1895 (0.4636)	SegCLSLoss 0.0498 (0.0308)	KLLoss 0.2832 (0.2254)	MaskLoss 1.0540 (0.9114)	MaskBCELoss 0.0758 (0.2420)	MaskDICELoss 0.9782 (0.6693)
Epoch: [0][162/500]	Time  6.066 ( 6.066)	Loss 0.7695 (2.4708)	CeLoss 0.7695 (0.4025)	SegCLSLoss 0.0000 (0.0282)	KLLoss 0.0000 (0.2232)	MaskLoss 0.0000 (0.9712)	MaskBCELoss 0.0000 (0.2926)	MaskDICELoss 0.0000 (0.6787)
Epoch: [0][163/500]	Time  5.191 ( 5.191)	Loss 2.8966 (1.8948)	CeLoss 0.2871 (0.4326)	SegCLSLoss 0.0232 (0.0215)	KLLoss 0.2852 (0.1713)	MaskLoss 1.2276 (0.6830)	MaskBCELoss 0.3669 (0.1899)	MaskDICELoss 0.8607 (0.4931)
Epoch: [0][164/500]	Time  6.210 ( 6.210)	Loss 2.6153 (2.3468)	CeLoss 0.1885 (0.4523)	SegCLSLoss 0.0559 (0.0308)	KLLoss 0.2773 (0.2271)	MaskLoss 1.1299 (0.8828)	MaskBCELoss 0.1415 (0.1734)	MaskDICELoss 0.9885 (0.7094)
Epoch: [0][165/500]	Time  6.192 ( 6.192)	Loss 2.1525 (2.0885)	CeLoss 0.2383 (0.3105)	SegCLSLoss 0.0177 (0.0281)	KLLoss 0.3086 (0.2268)	MaskLoss 0.8751 (0.8252)	MaskBCELoss 0.1243 (0.0759)	MaskDICELoss 0.7508 (0.7493)
Epoch: [0][166/500]	Time  6.475 ( 6.475)	Loss 0.9180 (2.2386)	CeLoss 0.9180 (0.3547)	SegCLSLoss 0.0000 (0.0275)	KLLoss 0.0000 (0.2264)	MaskLoss 0.0000 (0.8787)	MaskBCELoss 0.0000 (0.1860)	MaskDICELoss 0.0000 (0.6926)
Epoch: [0][167/500]	Time  5.331 ( 5.331)	Loss 2.0962 (1.9806)	CeLoss 0.2656 (0.3945)	SegCLSLoss 0.0366 (0.0211)	KLLoss 0.3223 (0.1756)	MaskLoss 0.8255 (0.7438)	MaskBCELoss 0.1854 (0.2676)	MaskDICELoss 0.6401 (0.4762)
Epoch: [0][168/500]	Time  5.946 ( 5.946)	Loss 2.9638 (2.6488)	CeLoss 0.2148 (0.2150)	SegCLSLoss 0.0393 (0.0416)	KLLoss 0.2695 (0.2818)	MaskLoss 1.2973 (1.1360)	MaskBCELoss 0.4279 (0.3396)	MaskDICELoss 0.8694 (0.7965)
Epoch: [0][169/500]	Time  6.050 ( 6.050)	Loss 2.9286 (3.3118)	CeLoss 0.2578 (0.4676)	SegCLSLoss 0.0280 (0.0274)	KLLoss 0.2773 (0.2289)	MaskLoss 1.2592 (1.3579)	MaskBCELoss 0.3907 (0.6752)	MaskDICELoss 0.8685 (0.6826)
[2025-03-02 16:42:16,201] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[0.00028548979591836734], mom=[(0.9, 0.95)]
[2025-03-02 16:42:16,207] [INFO] [timer.py:215:stop] epoch=0/micro_step=1700/global_step=170, RunningAvgSamplesPerSec=1.6227663621399866, CurrSamplesPerSec=1.5817007843407738, MemAllocated=30.82GB, MaxMemAllocated=36.58GB
Epoch: [0][170/500]	Time  6.324 ( 6.324)	Loss 2.7840 (2.2031)	CeLoss 0.1201 (0.4840)	SegCLSLoss 0.1167 (0.0292)	KLLoss 0.2617 (0.1979)	MaskLoss 1.2377 (0.8027)	MaskBCELoss 0.3648 (0.2187)	MaskDICELoss 0.8730 (0.5840)
Epoch: [0][171/500]	Time  6.588 ( 6.588)	Loss 1.8281 (2.3765)	CeLoss 1.8281 (0.4832)	SegCLSLoss 0.0000 (0.0275)	KLLoss 0.0000 (0.2275)	MaskLoss 0.0000 (0.8830)	MaskBCELoss 0.0000 (0.2202)	MaskDICELoss 0.0000 (0.6628)
Epoch: [0][172/500]	Time  6.133 ( 6.133)	Loss 2.4507 (2.3361)	CeLoss 0.2617 (0.5141)	SegCLSLoss 0.0415 (0.0311)	KLLoss 0.2832 (0.2242)	MaskLoss 1.0125 (0.8474)	MaskBCELoss 0.0150 (0.1218)	MaskDICELoss 0.9974 (0.7256)
Epoch: [0][173/500]	Time  6.339 ( 6.339)	Loss 2.3814 (2.0801)	CeLoss 0.1953 (0.3312)	SegCLSLoss 0.0413 (0.0268)	KLLoss 0.2695 (0.1953)	MaskLoss 1.0149 (0.8188)	MaskBCELoss 0.0220 (0.1823)	MaskDICELoss 0.9929 (0.6366)
Epoch: [0][174/500]	Time  6.657 ( 6.657)	Loss 2.4765 (2.3895)	CeLoss 0.2432 (0.2938)	SegCLSLoss 0.0267 (0.0494)	KLLoss 0.2891 (0.2525)	MaskLoss 1.0381 (0.9725)	MaskBCELoss 0.0683 (0.2006)	MaskDICELoss 0.9697 (0.7719)
Epoch: [0][175/500]	Time  7.772 ( 7.772)	Loss 2.7244 (2.4784)	CeLoss 0.2383 (0.2923)	SegCLSLoss 0.0262 (0.0343)	KLLoss 0.2930 (0.2576)	MaskLoss 1.1630 (1.0200)	MaskBCELoss 0.3254 (0.1921)	MaskDICELoss 0.8375 (0.8278)
Epoch: [0][176/500]	Time  6.142 ( 6.142)	Loss 1.0469 (1.7782)	CeLoss 1.0469 (0.6183)	SegCLSLoss 0.0000 (0.0185)	KLLoss 0.0000 (0.1396)	MaskLoss 0.0000 (0.5406)	MaskBCELoss 0.0000 (0.0702)	MaskDICELoss 0.0000 (0.4704)
Epoch: [0][177/500]	Time  6.882 ( 6.882)	Loss 2.5218 (2.2127)	CeLoss 0.3398 (0.3728)	SegCLSLoss 0.0269 (0.0367)	KLLoss 0.2852 (0.2238)	MaskLoss 1.0129 (0.8548)	MaskBCELoss 0.0187 (0.0892)	MaskDICELoss 0.9941 (0.7656)
Epoch: [0][178/500]	Time  7.107 ( 7.107)	Loss 0.1611 (2.3534)	CeLoss 0.1611 (0.3753)	SegCLSLoss 0.0000 (0.0289)	KLLoss 0.0000 (0.2334)	MaskLoss 0.0000 (0.9234)	MaskBCELoss 0.0000 (0.2694)	MaskDICELoss 0.0000 (0.6540)
Epoch: [0][179/500]	Time  7.393 ( 7.393)	Loss 2.0913 (2.2415)	CeLoss 0.2158 (0.3569)	SegCLSLoss 0.0312 (0.0321)	KLLoss 0.3008 (0.2301)	MaskLoss 0.8543 (0.8765)	MaskBCELoss 0.1265 (0.1726)	MaskDICELoss 0.7278 (0.7039)
[2025-03-02 16:43:22,623] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[0.00028426530612244897], mom=[(0.9, 0.95)]
[2025-03-02 16:43:22,629] [INFO] [timer.py:215:stop] epoch=0/micro_step=1800/global_step=180, RunningAvgSamplesPerSec=1.6157181666433873, CurrSamplesPerSec=1.8494036656633401, MemAllocated=31.44GB, MaxMemAllocated=36.79GB
Epoch: [0][180/500]	Time  5.409 ( 5.409)	Loss 2.8846 (2.0211)	CeLoss 0.2559 (0.5274)	SegCLSLoss 0.0288 (0.0311)	KLLoss 0.2871 (0.1680)	MaskLoss 1.2353 (0.6971)	MaskBCELoss 0.2854 (0.1432)	MaskDICELoss 0.9499 (0.5539)
Epoch: [0][181/500]	Time  6.727 ( 6.727)	Loss 1.2344 (1.8946)	CeLoss 1.2344 (0.6927)	SegCLSLoss 0.0000 (0.0199)	KLLoss 0.0000 (0.1414)	MaskLoss 0.0000 (0.5608)	MaskBCELoss 0.0000 (0.1181)	MaskDICELoss 0.0000 (0.4428)
Epoch: [0][182/500]	Time  6.649 ( 6.649)	Loss 3.6589 (2.2415)	CeLoss 0.2305 (0.5559)	SegCLSLoss 0.0371 (0.0241)	KLLoss 0.2812 (0.2062)	MaskLoss 1.6342 (0.7850)	MaskBCELoss 0.9329 (0.2198)	MaskDICELoss 0.7013 (0.5652)
Epoch: [0][183/500]	Time  6.052 ( 6.052)	Loss 1.1016 (2.3267)	CeLoss 1.1016 (0.3726)	SegCLSLoss 0.0000 (0.0237)	KLLoss 0.0000 (0.2338)	MaskLoss 0.0000 (0.9127)	MaskBCELoss 0.0000 (0.2138)	MaskDICELoss 0.0000 (0.6988)
Epoch: [0][184/500]	Time  5.314 ( 5.314)	Loss 1.1875 (2.3481)	CeLoss 1.1875 (0.4302)	SegCLSLoss 0.0000 (0.0286)	KLLoss 0.0000 (0.2318)	MaskLoss 0.0000 (0.8938)	MaskBCELoss 0.0000 (0.2525)	MaskDICELoss 0.0000 (0.6413)
Epoch: [0][185/500]	Time  4.051 ( 4.051)	Loss 2.7947 (1.8809)	CeLoss 0.2178 (0.6094)	SegCLSLoss 0.0452 (0.0225)	KLLoss 0.2754 (0.1723)	MaskLoss 1.2079 (0.5870)	MaskBCELoss 0.4223 (0.1913)	MaskDICELoss 0.7856 (0.3957)
Epoch: [0][186/500]	Time  6.950 ( 6.950)	Loss 1.0000 (2.3535)	CeLoss 1.0000 (0.3693)	SegCLSLoss 0.0000 (0.0261)	KLLoss 0.0000 (0.2289)	MaskLoss 0.0000 (0.9282)	MaskBCELoss 0.0000 (0.2958)	MaskDICELoss 0.0000 (0.6325)
Epoch: [0][187/500]	Time  6.597 ( 6.597)	Loss 2.8057 (2.3674)	CeLoss 0.1182 (0.3342)	SegCLSLoss 0.0854 (0.0314)	KLLoss 0.2773 (0.2254)	MaskLoss 1.2525 (0.9523)	MaskBCELoss 0.4265 (0.2596)	MaskDICELoss 0.8260 (0.6926)
Epoch: [0][188/500]	Time  8.522 ( 8.522)	Loss 1.1406 (2.1680)	CeLoss 1.1406 (0.5123)	SegCLSLoss 0.0000 (0.0300)	KLLoss 0.0000 (0.1957)	MaskLoss 0.0000 (0.7714)	MaskBCELoss 0.0000 (0.1854)	MaskDICELoss 0.0000 (0.5860)
Epoch: [0][189/500]	Time  7.122 ( 7.122)	Loss 2.0835 (2.4224)	CeLoss 0.2734 (0.2507)	SegCLSLoss 0.0210 (0.0292)	KLLoss 0.3086 (0.2885)	MaskLoss 0.8230 (1.0064)	MaskBCELoss 0.1635 (0.1435)	MaskDICELoss 0.6595 (0.8629)
[2025-03-02 16:44:26,419] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[0.0002830408163265306], mom=[(0.9, 0.95)]
[2025-03-02 16:44:26,424] [INFO] [timer.py:215:stop] epoch=0/micro_step=1900/global_step=190, RunningAvgSamplesPerSec=1.6131029464167614, CurrSamplesPerSec=1.7211646211980571, MemAllocated=30.95GB, MaxMemAllocated=36.79GB
Epoch: [0][190/500]	Time  5.812 ( 5.812)	Loss 2.8305 (2.2076)	CeLoss 0.2256 (0.4867)	SegCLSLoss 0.0491 (0.0302)	KLLoss 0.2754 (0.1977)	MaskLoss 1.2219 (0.8038)	MaskBCELoss 0.3577 (0.1769)	MaskDICELoss 0.8641 (0.6268)
Epoch: [0][191/500]	Time  7.268 ( 7.268)	Loss 2.3968 (2.0712)	CeLoss 0.2178 (0.2511)	SegCLSLoss 0.0491 (0.0294)	KLLoss 0.2852 (0.2332)	MaskLoss 1.0060 (0.8443)	MaskBCELoss 0.0066 (0.1865)	MaskDICELoss 0.9995 (0.6578)
Epoch: [0][192/500]	Time  6.463 ( 6.463)	Loss 2.8885 (1.9028)	CeLoss 0.2373 (0.2783)	SegCLSLoss 0.0154 (0.0323)	KLLoss 0.3086 (0.2037)	MaskLoss 1.2450 (0.7534)	MaskBCELoss 0.5375 (0.1947)	MaskDICELoss 0.7075 (0.5587)
Epoch: [0][193/500]	Time  7.076 ( 7.076)	Loss 2.7669 (2.6177)	CeLoss 0.2061 (0.3031)	SegCLSLoss 0.0327 (0.0376)	KLLoss 0.2812 (0.2592)	MaskLoss 1.2018 (1.0830)	MaskBCELoss 0.3737 (0.3273)	MaskDICELoss 0.8281 (0.7557)
Epoch: [0][194/500]	Time  6.013 ( 6.013)	Loss 2.7819 (2.1417)	CeLoss 0.2793 (0.4054)	SegCLSLoss 0.0162 (0.0209)	KLLoss 0.2988 (0.2396)	MaskLoss 1.1722 (0.8030)	MaskBCELoss 0.3550 (0.2178)	MaskDICELoss 0.8172 (0.5852)
Epoch: [0][195/500]	Time  5.479 ( 5.479)	Loss 1.2500 (1.8679)	CeLoss 1.2500 (0.5366)	SegCLSLoss 0.0000 (0.0175)	KLLoss 0.0000 (0.2078)	MaskLoss 0.0000 (0.6092)	MaskBCELoss 0.0000 (0.1263)	MaskDICELoss 0.0000 (0.4830)
Epoch: [0][196/500]	Time  6.557 ( 6.557)	Loss 2.2951 (2.3302)	CeLoss 0.3223 (0.3160)	SegCLSLoss 0.0170 (0.0224)	KLLoss 0.3027 (0.2637)	MaskLoss 0.9073 (0.9358)	MaskBCELoss 0.2196 (0.2502)	MaskDICELoss 0.6877 (0.6856)
Epoch: [0][197/500]	Time  6.358 ( 6.358)	Loss 0.0967 (2.1382)	CeLoss 0.0967 (0.3679)	SegCLSLoss 0.0000 (0.0244)	KLLoss 0.0000 (0.2334)	MaskLoss 0.0000 (0.8207)	MaskBCELoss 0.0000 (0.1551)	MaskDICELoss 0.0000 (0.6657)
Epoch: [0][198/500]	Time  6.129 ( 6.129)	Loss 2.6874 (2.0327)	CeLoss 0.1543 (0.5157)	SegCLSLoss 0.0669 (0.0204)	KLLoss 0.2734 (0.1721)	MaskLoss 1.1816 (0.7105)	MaskBCELoss 0.2492 (0.1806)	MaskDICELoss 0.9324 (0.5299)
Epoch: [0][199/500]	Time  6.494 ( 6.494)	Loss 2.8881 (2.2604)	CeLoss 0.1797 (0.4664)	SegCLSLoss 0.0679 (0.0329)	KLLoss 0.2754 (0.2275)	MaskLoss 1.2683 (0.8319)	MaskBCELoss 0.4524 (0.2020)	MaskDICELoss 0.8159 (0.6299)
[2025-03-02 16:45:30,543] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[0.00028181632653061223], mom=[(0.9, 0.95)]
[2025-03-02 16:45:30,548] [INFO] [timer.py:215:stop] epoch=0/micro_step=2000/global_step=200, RunningAvgSamplesPerSec=1.6103257021776762, CurrSamplesPerSec=1.5907338466978225, MemAllocated=31.25GB, MaxMemAllocated=36.79GB
Epoch: [0][200/500]	Time  6.288 ( 6.288)	Loss 2.9123 (2.5729)	CeLoss 0.2383 (0.3187)	SegCLSLoss 0.0308 (0.0316)	KLLoss 0.2754 (0.2498)	MaskLoss 1.2609 (1.0567)	MaskBCELoss 0.4423 (0.2916)	MaskDICELoss 0.8186 (0.7651)
Epoch: [0][201/500]	Time  6.873 ( 6.873)	Loss 0.8281 (2.3817)	CeLoss 0.8281 (0.3907)	SegCLSLoss 0.0000 (0.0249)	KLLoss 0.0000 (0.2293)	MaskLoss 0.0000 (0.9318)	MaskBCELoss 0.0000 (0.2413)	MaskDICELoss 0.0000 (0.6904)
Epoch: [0][202/500]	Time  7.403 ( 7.403)	Loss 3.8183 (2.2952)	CeLoss 0.2051 (0.2525)	SegCLSLoss 0.0325 (0.0227)	KLLoss 0.2773 (0.2301)	MaskLoss 1.7295 (0.9583)	MaskBCELoss 0.9890 (0.2732)	MaskDICELoss 0.7405 (0.6852)
Epoch: [0][203/500]	Time  7.500 ( 7.500)	Loss 0.3145 (2.1956)	CeLoss 0.3145 (0.2731)	SegCLSLoss 0.0000 (0.0296)	KLLoss 0.0000 (0.2268)	MaskLoss 0.0000 (0.8972)	MaskBCELoss 0.0000 (0.1591)	MaskDICELoss 0.0000 (0.7381)
Epoch: [0][204/500]	Time  5.803 ( 5.803)	Loss 2.7316 (2.3040)	CeLoss 0.1875 (0.5348)	SegCLSLoss 0.0562 (0.0248)	KLLoss 0.2832 (0.1979)	MaskLoss 1.1871 (0.8288)	MaskBCELoss 0.3063 (0.2144)	MaskDICELoss 0.8807 (0.6144)
Epoch: [0][205/500]	Time  6.179 ( 6.179)	Loss 2.5613 (1.8686)	CeLoss 0.2500 (0.3805)	SegCLSLoss 0.0229 (0.0131)	KLLoss 0.2930 (0.1803)	MaskLoss 1.0766 (0.6958)	MaskBCELoss 0.1046 (0.1878)	MaskDICELoss 0.9719 (0.5080)
Epoch: [0][206/500]	Time  6.026 ( 6.026)	Loss 1.1641 (2.2789)	CeLoss 1.1641 (0.4796)	SegCLSLoss 0.0000 (0.0234)	KLLoss 0.0000 (0.2385)	MaskLoss 0.0000 (0.8341)	MaskBCELoss 0.0000 (0.1503)	MaskDICELoss 0.0000 (0.6838)
Epoch: [0][207/500]	Time  5.087 ( 5.087)	Loss 0.7305 (1.7568)	CeLoss 0.7305 (0.6562)	SegCLSLoss 0.0000 (0.0198)	KLLoss 0.0000 (0.1447)	MaskLoss 0.0000 (0.5091)	MaskBCELoss 0.0000 (0.0753)	MaskDICELoss 0.0000 (0.4338)
Epoch: [0][208/500]	Time  6.365 ( 6.365)	Loss 2.5946 (2.3026)	CeLoss 0.2139 (0.4113)	SegCLSLoss 0.0588 (0.0354)	KLLoss 0.2754 (0.2238)	MaskLoss 1.1069 (0.8809)	MaskBCELoss 0.1540 (0.1178)	MaskDICELoss 0.9529 (0.7631)
Epoch: [0][209/500]	Time  6.725 ( 6.725)	Loss 2.5556 (2.3825)	CeLoss 0.3008 (0.3215)	SegCLSLoss 0.0189 (0.0304)	KLLoss 0.3008 (0.2588)	MaskLoss 1.0473 (0.9581)	MaskBCELoss 0.0709 (0.1247)	MaskDICELoss 0.9765 (0.8334)
[2025-03-02 16:46:34,879] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=0, lr=[0.00028059183673469386], mom=[(0.9, 0.95)]
[2025-03-02 16:46:34,884] [INFO] [timer.py:215:stop] epoch=0/micro_step=2100/global_step=210, RunningAvgSamplesPerSec=1.6075574545435583, CurrSamplesPerSec=1.5693112558642397, MemAllocated=31.44GB, MaxMemAllocated=36.79GB
Epoch: [0][210/500]	Time  6.374 ( 6.374)	Loss 2.8371 (2.2395)	CeLoss 0.2969 (0.3848)	SegCLSLoss 0.0247 (0.0256)	KLLoss 0.2910 (0.2301)	MaskLoss 1.1920 (0.8637)	MaskBCELoss 0.2195 (0.1918)	MaskDICELoss 0.9725 (0.6719)
Epoch: [0][211/500]	Time  5.841 ( 5.841)	Loss 2.7919 (2.0597)	CeLoss 0.3535 (0.4107)	SegCLSLoss 0.0265 (0.0219)	KLLoss 0.2773 (0.2010)	MaskLoss 1.1420 (0.7687)	MaskBCELoss 0.1460 (0.2013)	MaskDICELoss 0.9961 (0.5674)
Epoch: [0][212/500]	Time  6.912 ( 6.912)	Loss 2.8836 (2.3855)	CeLoss 0.3164 (0.4420)	SegCLSLoss 0.0293 (0.0278)	KLLoss 0.2852 (0.2281)	MaskLoss 1.2045 (0.9076)	MaskBCELoss 0.2965 (0.2252)	MaskDICELoss 0.9080 (0.6824)
Epoch: [0][213/500]	Time  5.823 ( 5.823)	Loss 2.5417 (1.9658)	CeLoss 0.1855 (0.4734)	SegCLSLoss 0.0503 (0.0232)	KLLoss 0.2812 (0.2006)	MaskLoss 1.0951 (0.6902)	MaskBCELoss 0.1382 (0.1901)	MaskDICELoss 0.9569 (0.5002)
Epoch: [0][214/500]	Time  6.836 ( 6.836)	Loss 2.5575 (2.1037)	CeLoss 0.2266 (0.3335)	SegCLSLoss 0.0264 (0.0241)	KLLoss 0.2852 (0.2326)	MaskLoss 1.0873 (0.8210)	MaskBCELoss 0.0920 (0.1663)	MaskDICELoss 0.9953 (0.6547)
Epoch: [0][215/500]	Time  6.635 ( 6.635)	Loss 2.0755 (2.3573)	CeLoss 0.2363 (0.2461)	SegCLSLoss 0.0300 (0.0340)	KLLoss 0.2910 (0.2854)	MaskLoss 0.8385 (0.9758)	MaskBCELoss 0.0364 (0.1606)	MaskDICELoss 0.8022 (0.8153)
Epoch: [0][216/500]	Time  5.438 ( 5.438)	Loss 0.7969 (1.9316)	CeLoss 0.7969 (0.4825)	SegCLSLoss 0.0000 (0.0229)	KLLoss 0.0000 (0.2027)	MaskLoss 0.0000 (0.6681)	MaskBCELoss 0.0000 (0.1555)	MaskDICELoss 0.0000 (0.5125)
Epoch: [0][217/500]	Time  5.638 ( 5.638)	Loss 2.5105 (2.5774)	CeLoss 0.2373 (0.6042)	SegCLSLoss 0.0265 (0.0253)	KLLoss 0.2891 (0.2037)	MaskLoss 1.0580 (0.9294)	MaskBCELoss 0.0655 (0.3124)	MaskDICELoss 0.9925 (0.6170)
Epoch: [0][218/500]	Time  5.881 ( 5.881)	Loss 3.6382 (2.2999)	CeLoss 0.1299 (0.3291)	SegCLSLoss 0.0884 (0.0311)	KLLoss 0.2812 (0.2307)	MaskLoss 1.6619 (0.9200)	MaskBCELoss 0.8328 (0.2472)	MaskDICELoss 0.8290 (0.6728)
Epoch: [0][219/500]	Time  6.765 ( 6.765)	Loss 2.3740 (2.0592)	CeLoss 0.3008 (0.2969)	SegCLSLoss 0.0201 (0.0262)	KLLoss 0.3047 (0.2346)	MaskLoss 0.9555 (0.8160)	MaskBCELoss 0.1520 (0.2384)	MaskDICELoss 0.8036 (0.5776)
[2025-03-02 16:47:37,978] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=0, lr=[0.0002793673469387755], mom=[(0.9, 0.95)]
[2025-03-02 16:47:37,983] [INFO] [timer.py:215:stop] epoch=0/micro_step=2200/global_step=220, RunningAvgSamplesPerSec=1.6065146995016413, CurrSamplesPerSec=1.364791992499069, MemAllocated=31.56GB, MaxMemAllocated=36.79GB
Epoch: [0][220/500]	Time  7.329 ( 7.329)	Loss 2.5145 (2.5522)	CeLoss 0.2197 (0.2708)	SegCLSLoss 0.0388 (0.0286)	KLLoss 0.2949 (0.2936)	MaskLoss 1.0639 (1.0603)	MaskBCELoss 0.0866 (0.2969)	MaskDICELoss 0.9774 (0.7634)
Epoch: [0][221/500]	Time  5.640 ( 5.640)	Loss 2.1207 (2.0806)	CeLoss 0.1699 (0.4920)	SegCLSLoss 0.0369 (0.0194)	KLLoss 0.2734 (0.2018)	MaskLoss 0.8982 (0.7390)	MaskBCELoss 0.0920 (0.1282)	MaskDICELoss 0.8062 (0.6108)
Epoch: [0][222/500]	Time  7.656 ( 7.656)	Loss 2.4159 (2.4299)	CeLoss 0.2285 (0.3213)	SegCLSLoss 0.0206 (0.0281)	KLLoss 0.3164 (0.2586)	MaskLoss 1.0087 (0.9826)	MaskBCELoss 0.3528 (0.2380)	MaskDICELoss 0.6560 (0.7446)
Epoch: [0][223/500]	Time  6.066 ( 6.066)	Loss 1.9750 (1.6598)	CeLoss 0.2393 (0.4189)	SegCLSLoss 0.0212 (0.0187)	KLLoss 0.2969 (0.1719)	MaskLoss 0.7883 (0.5728)	MaskBCELoss 0.1499 (0.0885)	MaskDICELoss 0.6384 (0.4844)
Epoch: [0][224/500]	Time  6.403 ( 6.403)	Loss 1.4844 (2.1246)	CeLoss 1.4844 (0.4978)	SegCLSLoss 0.0000 (0.0223)	KLLoss 0.0000 (0.2287)	MaskLoss 0.0000 (0.7509)	MaskBCELoss 0.0000 (0.1028)	MaskDICELoss 0.0000 (0.6480)
Epoch: [0][225/500]	Time  5.592 ( 5.592)	Loss 2.8332 (2.4216)	CeLoss 0.3438 (0.3541)	SegCLSLoss 0.0215 (0.0272)	KLLoss 0.2930 (0.2582)	MaskLoss 1.1666 (0.9626)	MaskBCELoss 0.6527 (0.2654)	MaskDICELoss 0.5139 (0.6971)
Epoch: [0][226/500]	Time  6.019 ( 6.019)	Loss 0.0728 (2.0857)	CeLoss 0.0728 (0.3164)	SegCLSLoss 0.0000 (0.0281)	KLLoss 0.0000 (0.2297)	MaskLoss 0.0000 (0.8200)	MaskBCELoss 0.0000 (0.1437)	MaskDICELoss 0.0000 (0.6763)
Epoch: [0][227/500]	Time  6.313 ( 6.313)	Loss 2.4064 (2.1415)	CeLoss 0.2217 (0.4175)	SegCLSLoss 0.0298 (0.0201)	KLLoss 0.2812 (0.2338)	MaskLoss 1.0147 (0.7985)	MaskBCELoss 0.0184 (0.1472)	MaskDICELoss 0.9963 (0.6513)
Epoch: [0][228/500]	Time  5.961 ( 5.961)	Loss 1.4531 (2.3466)	CeLoss 1.4531 (0.4635)	SegCLSLoss 0.0000 (0.0202)	KLLoss 0.0000 (0.2082)	MaskLoss 0.0000 (0.8843)	MaskBCELoss 0.0000 (0.3550)	MaskDICELoss 0.0000 (0.5294)
Epoch: [0][229/500]	Time  5.332 ( 5.332)	Loss 2.0425 (2.3272)	CeLoss 0.2695 (0.4396)	SegCLSLoss 0.0352 (0.0382)	KLLoss 0.2949 (0.2324)	MaskLoss 0.8045 (0.8761)	MaskBCELoss 0.0557 (0.2084)	MaskDICELoss 0.7488 (0.6677)
[2025-03-02 16:48:38,973] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=0, lr=[0.0002781428571428571], mom=[(0.9, 0.95)]
[2025-03-02 16:48:38,978] [INFO] [timer.py:215:stop] epoch=0/micro_step=2300/global_step=230, RunningAvgSamplesPerSec=1.6079454717185235, CurrSamplesPerSec=1.662990563509939, MemAllocated=31.24GB, MaxMemAllocated=36.79GB
Epoch: [0][230/500]	Time  6.015 ( 6.015)	Loss 3.2143 (2.2200)	CeLoss 0.2676 (0.3586)	SegCLSLoss 0.0183 (0.0293)	KLLoss 0.3105 (0.2336)	MaskLoss 1.3903 (0.8649)	MaskBCELoss 0.4124 (0.1967)	MaskDICELoss 0.9779 (0.6682)
Epoch: [0][231/500]	Time  5.674 ( 5.674)	Loss 0.9727 (1.9636)	CeLoss 0.9727 (0.5392)	SegCLSLoss 0.0000 (0.0200)	KLLoss 0.0000 (0.2129)	MaskLoss 0.0000 (0.6541)	MaskBCELoss 0.0000 (0.1636)	MaskDICELoss 0.0000 (0.4905)
Epoch: [0][232/500]	Time  5.906 ( 5.906)	Loss 0.9922 (1.9754)	CeLoss 0.9922 (0.4657)	SegCLSLoss 0.0000 (0.0199)	KLLoss 0.0000 (0.2080)	MaskLoss 0.0000 (0.6978)	MaskBCELoss 0.0000 (0.1440)	MaskDICELoss 0.0000 (0.5537)
Epoch: [0][233/500]	Time  6.763 ( 6.763)	Loss 2.4133 (2.0639)	CeLoss 0.2422 (0.4021)	SegCLSLoss 0.0322 (0.0213)	KLLoss 0.2852 (0.2045)	MaskLoss 1.0065 (0.7744)	MaskBCELoss 0.0109 (0.1658)	MaskDICELoss 0.9956 (0.6086)
Epoch: [0][234/500]	Time  6.194 ( 6.194)	Loss 2.6642 (2.3441)	CeLoss 0.1660 (0.4482)	SegCLSLoss 0.0503 (0.0310)	KLLoss 0.2793 (0.2285)	MaskLoss 1.1661 (0.8831)	MaskBCELoss 0.2409 (0.1471)	MaskDICELoss 0.9252 (0.7360)
Epoch: [0][235/500]	Time  6.069 ( 6.069)	Loss 2.4063 (1.9484)	CeLoss 0.2021 (0.3259)	SegCLSLoss 0.0310 (0.0210)	KLLoss 0.2871 (0.2035)	MaskLoss 1.0225 (0.7551)	MaskBCELoss 0.1088 (0.2097)	MaskDICELoss 0.9137 (0.5454)
Epoch: [0][236/500]	Time  6.621 ( 6.621)	Loss 3.4491 (2.3297)	CeLoss 0.1855 (0.3175)	SegCLSLoss 0.0352 (0.0271)	KLLoss 0.2812 (0.2285)	MaskLoss 1.5527 (0.9422)	MaskBCELoss 0.6688 (0.2621)	MaskDICELoss 0.8839 (0.6802)
Epoch: [0][237/500]	Time  5.974 ( 5.974)	Loss 1.3047 (2.2213)	CeLoss 1.3047 (0.4141)	SegCLSLoss 0.0000 (0.0290)	KLLoss 0.0000 (0.2291)	MaskLoss 0.0000 (0.8391)	MaskBCELoss 0.0000 (0.1183)	MaskDICELoss 0.0000 (0.7208)
Epoch: [0][238/500]	Time  6.224 ( 6.224)	Loss 2.4092 (2.4525)	CeLoss 0.2031 (0.3264)	SegCLSLoss 0.0289 (0.0316)	KLLoss 0.2891 (0.2547)	MaskLoss 1.0230 (0.9914)	MaskBCELoss 0.0310 (0.2238)	MaskDICELoss 0.9919 (0.7676)
Epoch: [0][239/500]	Time  6.331 ( 6.331)	Loss 2.4621 (2.1562)	CeLoss 0.3672 (0.5553)	SegCLSLoss 0.0187 (0.0189)	KLLoss 0.2910 (0.2012)	MaskLoss 0.9693 (0.7454)	MaskBCELoss 0.2011 (0.1627)	MaskDICELoss 0.7682 (0.5827)
[2025-03-02 16:49:41,187] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=0, lr=[0.00027691836734693875], mom=[(0.9, 0.95)]
[2025-03-02 16:49:41,193] [INFO] [timer.py:215:stop] epoch=0/micro_step=2400/global_step=240, RunningAvgSamplesPerSec=1.6079326714839264, CurrSamplesPerSec=1.5487415946517713, MemAllocated=30.69GB, MaxMemAllocated=36.79GB
Epoch: [0][240/500]	Time  6.458 ( 6.458)	Loss 1.5156 (2.2537)	CeLoss 1.5156 (0.4390)	SegCLSLoss 0.0000 (0.0329)	KLLoss 0.0000 (0.2248)	MaskLoss 0.0000 (0.8428)	MaskBCELoss 0.0000 (0.1717)	MaskDICELoss 0.0000 (0.6711)
Epoch: [0][241/500]	Time  5.970 ( 5.970)	Loss 1.2031 (2.2337)	CeLoss 1.2031 (0.4093)	SegCLSLoss 0.0000 (0.0200)	KLLoss 0.0000 (0.2281)	MaskLoss 0.0000 (0.8503)	MaskBCELoss 0.0000 (0.1740)	MaskDICELoss 0.0000 (0.6763)
Epoch: [0][242/500]	Time  4.886 ( 4.886)	Loss 3.8522 (2.4706)	CeLoss 0.1904 (0.4071)	SegCLSLoss 0.0159 (0.0244)	KLLoss 0.2871 (0.2275)	MaskLoss 1.7552 (0.9689)	MaskBCELoss 0.9474 (0.3088)	MaskDICELoss 0.8078 (0.6601)
Epoch: [0][243/500]	Time  5.966 ( 5.966)	Loss 2.0882 (2.1726)	CeLoss 0.2598 (0.3729)	SegCLSLoss 0.0291 (0.0294)	KLLoss 0.2832 (0.2229)	MaskLoss 0.8371 (0.8368)	MaskBCELoss 0.2155 (0.1668)	MaskDICELoss 0.6216 (0.6700)
Epoch: [0][244/500]	Time  7.306 ( 7.306)	Loss 2.4545 (2.3800)	CeLoss 0.1816 (0.3357)	SegCLSLoss 0.0300 (0.0203)	KLLoss 0.2754 (0.2578)	MaskLoss 1.0603 (0.9528)	MaskBCELoss 0.1712 (0.3100)	MaskDICELoss 0.8890 (0.6428)
Epoch: [0][245/500]	Time  5.697 ( 5.697)	Loss 2.1947 (1.9014)	CeLoss 0.2754 (0.5932)	SegCLSLoss 0.0193 (0.0135)	KLLoss 0.2910 (0.1717)	MaskLoss 0.8825 (0.6078)	MaskBCELoss 0.0880 (0.1286)	MaskDICELoss 0.7945 (0.4792)
Epoch: [0][246/500]	Time  7.307 ( 7.307)	Loss 2.4260 (2.6118)	CeLoss 0.2656 (0.2474)	SegCLSLoss 0.0233 (0.0304)	KLLoss 0.2715 (0.2811)	MaskLoss 1.0060 (1.1042)	MaskBCELoss 0.0208 (0.2716)	MaskDICELoss 0.9852 (0.8326)
Epoch: [0][247/500]	Time  6.000 ( 6.000)	Loss 2.3730 (2.4135)	CeLoss 0.2080 (0.3042)	SegCLSLoss 0.0281 (0.0274)	KLLoss 0.2734 (0.2506)	MaskLoss 1.0068 (0.9851)	MaskBCELoss 0.0437 (0.2729)	MaskDICELoss 0.9631 (0.7121)
Epoch: [0][248/500]	Time  6.100 ( 6.100)	Loss 2.3854 (2.2012)	CeLoss 0.2539 (0.3125)	SegCLSLoss 0.0322 (0.0286)	KLLoss 0.2695 (0.2508)	MaskLoss 0.9905 (0.8743)	MaskBCELoss 0.0502 (0.1353)	MaskDICELoss 0.9403 (0.7391)
Epoch: [0][249/500]	Time 36.254 (36.254)	Loss 2.5008 (2.0253)	CeLoss 0.1216 (0.4818)	SegCLSLoss 0.0840 (0.0245)	KLLoss 0.2773 (0.1945)	MaskLoss 1.0990 (0.7170)	MaskBCELoss 0.2520 (0.1301)	MaskDICELoss 0.8471 (0.5869)
[2025-03-02 16:51:13,149] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=0, lr=[0.0002756938775510204], mom=[(0.9, 0.95)]
[2025-03-02 16:51:13,154] [INFO] [timer.py:215:stop] epoch=0/micro_step=2500/global_step=250, RunningAvgSamplesPerSec=1.5774980858879075, CurrSamplesPerSec=1.544757352377625, MemAllocated=30.95GB, MaxMemAllocated=36.79GB
Epoch: [0][250/500]	Time  6.475 ( 6.475)	Loss 2.6032 (2.0456)	CeLoss 0.1865 (0.2742)	SegCLSLoss 0.0364 (0.0282)	KLLoss 0.2734 (0.2229)	MaskLoss 1.1307 (0.8230)	MaskBCELoss 0.3006 (0.1309)	MaskDICELoss 0.8301 (0.6921)
Epoch: [0][251/500]	Time  5.867 ( 5.867)	Loss 2.3321 (2.3590)	CeLoss 0.3105 (0.4069)	SegCLSLoss 0.0275 (0.0196)	KLLoss 0.2793 (0.2252)	MaskLoss 0.9336 (0.9146)	MaskBCELoss 0.0312 (0.2537)	MaskDICELoss 0.9024 (0.6609)
Epoch: [0][252/500]	Time  5.546 ( 5.546)	Loss 0.7539 (2.0356)	CeLoss 0.7539 (0.6271)	SegCLSLoss 0.0000 (0.0210)	KLLoss 0.0000 (0.1697)	MaskLoss 0.0000 (0.6565)	MaskBCELoss 0.0000 (0.1531)	MaskDICELoss 0.0000 (0.5034)
Epoch: [0][253/500]	Time  5.865 ( 5.865)	Loss 1.4922 (2.0649)	CeLoss 1.4922 (0.5529)	SegCLSLoss 0.0000 (0.0193)	KLLoss 0.0000 (0.1994)	MaskLoss 0.0000 (0.7013)	MaskBCELoss 0.0000 (0.1565)	MaskDICELoss 0.0000 (0.5448)
Epoch: [0][254/500]	Time  6.894 ( 6.894)	Loss 0.4336 (2.4632)	CeLoss 0.4336 (0.3212)	SegCLSLoss 0.0000 (0.0260)	KLLoss 0.0000 (0.2523)	MaskLoss 0.0000 (1.0014)	MaskBCELoss 0.0000 (0.2699)	MaskDICELoss 0.0000 (0.7315)
Epoch: [0][255/500]	Time  6.692 ( 6.692)	Loss 2.5514 (2.1071)	CeLoss 0.2559 (0.3399)	SegCLSLoss 0.0194 (0.0252)	KLLoss 0.2871 (0.2525)	MaskLoss 1.0706 (0.8144)	MaskBCELoss 0.1535 (0.1065)	MaskDICELoss 0.9172 (0.7079)
Epoch: [0][256/500]	Time  5.581 ( 5.581)	Loss 1.5234 (1.6659)	CeLoss 1.5234 (0.6432)	SegCLSLoss 0.0000 (0.0114)	KLLoss 0.0000 (0.1420)	MaskLoss 0.0000 (0.4728)	MaskBCELoss 0.0000 (0.0933)	MaskDICELoss 0.0000 (0.3795)
Epoch: [0][257/500]	Time  6.685 ( 6.685)	Loss 1.8100 (2.1629)	CeLoss 0.3945 (0.2652)	SegCLSLoss 0.0193 (0.0300)	KLLoss 0.2969 (0.2846)	MaskLoss 0.6276 (0.8700)	MaskBCELoss 0.1525 (0.1833)	MaskDICELoss 0.4752 (0.6867)
Epoch: [0][258/500]	Time  6.201 ( 6.201)	Loss 2.5846 (2.2969)	CeLoss 0.2773 (0.5296)	SegCLSLoss 0.0330 (0.0273)	KLLoss 0.2695 (0.2193)	MaskLoss 1.0784 (0.8222)	MaskBCELoss 0.2742 (0.1583)	MaskDICELoss 0.8042 (0.6639)
Epoch: [0][259/500]	Time  6.434 ( 6.434)	Loss 1.4219 (2.1343)	CeLoss 1.4219 (0.5470)	SegCLSLoss 0.0000 (0.0238)	KLLoss 0.0000 (0.1949)	MaskLoss 0.0000 (0.7390)	MaskBCELoss 0.0000 (0.1452)	MaskDICELoss 0.0000 (0.5938)
[2025-03-02 16:52:15,510] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=0, lr=[0.000274469387755102], mom=[(0.9, 0.95)]
[2025-03-02 16:52:15,516] [INFO] [timer.py:215:stop] epoch=0/micro_step=2600/global_step=260, RunningAvgSamplesPerSec=1.5785040891784656, CurrSamplesPerSec=1.5163804763208175, MemAllocated=30.79GB, MaxMemAllocated=36.79GB
Epoch: [0][260/500]	Time  6.596 ( 6.596)	Loss 1.6606 (2.0574)	CeLoss 0.2812 (0.4664)	SegCLSLoss 0.0220 (0.0242)	KLLoss 0.2988 (0.2250)	MaskLoss 0.6096 (0.7331)	MaskBCELoss 0.3745 (0.1747)	MaskDICELoss 0.2351 (0.5584)
Epoch: [0][261/500]	Time  7.163 ( 7.163)	Loss 2.4547 (2.6203)	CeLoss 0.2246 (0.2283)	SegCLSLoss 0.0386 (0.0353)	KLLoss 0.2773 (0.2785)	MaskLoss 1.0359 (1.1175)	MaskBCELoss 0.0450 (0.2692)	MaskDICELoss 0.9909 (0.8483)
Epoch: [0][262/500]	Time  5.377 ( 5.377)	Loss 2.2407 (1.7396)	CeLoss 0.3027 (0.5632)	SegCLSLoss 0.0236 (0.0162)	KLLoss 0.2930 (0.1701)	MaskLoss 0.8899 (0.5415)	MaskBCELoss 0.2968 (0.1053)	MaskDICELoss 0.5931 (0.4362)
Epoch: [0][263/500]	Time  6.794 ( 6.794)	Loss 2.3401 (2.3675)	CeLoss 0.3496 (0.3365)	SegCLSLoss 0.0220 (0.0274)	KLLoss 0.2812 (0.2518)	MaskLoss 0.9200 (0.9458)	MaskBCELoss 0.1463 (0.2391)	MaskDICELoss 0.7737 (0.7066)
Epoch: [0][264/500]	Time  6.357 ( 6.357)	Loss 2.2063 (2.3621)	CeLoss 0.2451 (0.3054)	SegCLSLoss 0.0317 (0.0314)	KLLoss 0.2812 (0.2482)	MaskLoss 0.9020 (0.9585)	MaskBCELoss 0.0814 (0.1963)	MaskDICELoss 0.8206 (0.7622)
Epoch: [0][265/500]	Time  7.133 ( 7.133)	Loss 1.0547 (2.1340)	CeLoss 1.0547 (0.4109)	SegCLSLoss 0.0000 (0.0251)	KLLoss 0.0000 (0.2209)	MaskLoss 0.0000 (0.8000)	MaskBCELoss 0.0000 (0.1109)	MaskDICELoss 0.0000 (0.6891)
Epoch: [0][266/500]	Time  5.727 ( 5.727)	Loss 1.2344 (1.8543)	CeLoss 1.2344 (0.6680)	SegCLSLoss 0.0000 (0.0164)	KLLoss 0.0000 (0.1385)	MaskLoss 0.0000 (0.5543)	MaskBCELoss 0.0000 (0.1326)	MaskDICELoss 0.0000 (0.4217)
Epoch: [0][267/500]	Time  6.319 ( 6.319)	Loss 2.3670 (2.4012)	CeLoss 0.2305 (0.3704)	SegCLSLoss 0.0413 (0.0339)	KLLoss 0.2676 (0.2514)	MaskLoss 0.9902 (0.9439)	MaskBCELoss 0.0132 (0.2104)	MaskDICELoss 0.9770 (0.7335)
Epoch: [0][268/500]	Time  6.015 ( 6.015)	Loss 1.5469 (2.1955)	CeLoss 1.5469 (0.4496)	SegCLSLoss 0.0000 (0.0298)	KLLoss 0.0000 (0.2189)	MaskLoss 0.0000 (0.8109)	MaskBCELoss 0.0000 (0.0911)	MaskDICELoss 0.0000 (0.7197)
Epoch: [0][269/500]	Time  5.083 ( 5.083)	Loss 1.3047 (2.1095)	CeLoss 1.3047 (0.5297)	SegCLSLoss 0.0000 (0.0210)	KLLoss 0.0000 (0.1973)	MaskLoss 0.0000 (0.7353)	MaskBCELoss 0.0000 (0.0938)	MaskDICELoss 0.0000 (0.6416)
[2025-03-02 16:53:17,692] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=0, lr=[0.00027324489795918364], mom=[(0.9, 0.95)]
[2025-03-02 16:53:17,698] [INFO] [timer.py:215:stop] epoch=0/micro_step=2700/global_step=270, RunningAvgSamplesPerSec=1.5796035565742261, CurrSamplesPerSec=1.609650324783092, MemAllocated=30.8GB, MaxMemAllocated=36.79GB
Epoch: [0][270/500]	Time  6.214 ( 6.214)	Loss 2.2346 (2.0081)	CeLoss 0.2207 (0.4080)	SegCLSLoss 0.0398 (0.0251)	KLLoss 0.2949 (0.2582)	MaskLoss 0.9240 (0.7293)	MaskBCELoss 0.0589 (0.1499)	MaskDICELoss 0.8650 (0.5794)
Epoch: [0][271/500]	Time  6.574 ( 6.574)	Loss 2.2812 (2.4011)	CeLoss 0.2520 (0.3126)	SegCLSLoss 0.0405 (0.0291)	KLLoss 0.2734 (0.2500)	MaskLoss 0.9365 (0.9745)	MaskBCELoss 0.2822 (0.2299)	MaskDICELoss 0.6543 (0.7446)
Epoch: [0][272/500]	Time  6.814 ( 6.814)	Loss 2.6987 (2.1742)	CeLoss 0.2891 (0.3583)	SegCLSLoss 0.0162 (0.0228)	KLLoss 0.2852 (0.2285)	MaskLoss 1.1296 (0.8452)	MaskBCELoss 0.1588 (0.1711)	MaskDICELoss 0.9708 (0.6741)
Epoch: [0][273/500]	Time  6.546 ( 6.546)	Loss 2.3810 (2.5213)	CeLoss 0.2305 (0.2229)	SegCLSLoss 0.0266 (0.0455)	KLLoss 0.2715 (0.2834)	MaskLoss 1.0010 (1.0671)	MaskBCELoss 0.1466 (0.3250)	MaskDICELoss 0.8545 (0.7420)
Epoch: [0][274/500]	Time  6.805 ( 6.805)	Loss 0.0752 (2.1980)	CeLoss 0.0752 (0.2301)	SegCLSLoss 0.0000 (0.0233)	KLLoss 0.0000 (0.2617)	MaskLoss 0.0000 (0.9128)	MaskBCELoss 0.0000 (0.1960)	MaskDICELoss 0.0000 (0.7168)
Epoch: [0][275/500]	Time  6.860 ( 6.860)	Loss 2.7332 (2.3810)	CeLoss 0.3301 (0.3302)	SegCLSLoss 0.0197 (0.0239)	KLLoss 0.2969 (0.2557)	MaskLoss 1.1225 (0.9555)	MaskBCELoss 0.2522 (0.1704)	MaskDICELoss 0.8702 (0.7851)
Epoch: [0][276/500]	Time  7.028 ( 7.028)	Loss 2.0848 (2.0380)	CeLoss 0.2402 (0.4196)	SegCLSLoss 0.0276 (0.0232)	KLLoss 0.3047 (0.2012)	MaskLoss 0.8393 (0.7530)	MaskBCELoss 0.0693 (0.1286)	MaskDICELoss 0.7699 (0.6244)
Epoch: [0][277/500]	Time  6.281 ( 6.281)	Loss 2.4455 (2.1631)	CeLoss 0.2695 (0.3215)	SegCLSLoss 0.0317 (0.0265)	KLLoss 0.2754 (0.2244)	MaskLoss 1.0118 (0.8582)	MaskBCELoss 0.0131 (0.1924)	MaskDICELoss 0.9987 (0.6657)
Epoch: [0][278/500]	Time  6.210 ( 6.210)	Loss 2.4298 (1.8083)	CeLoss 0.2500 (0.5567)	SegCLSLoss 0.0234 (0.0156)	KLLoss 0.2852 (0.1727)	MaskLoss 1.0128 (0.5788)	MaskBCELoss 0.0473 (0.0690)	MaskDICELoss 0.9655 (0.5097)
Epoch: [0][279/500]	Time  5.663 ( 5.663)	Loss 2.4301 (1.8329)	CeLoss 0.1846 (0.4367)	SegCLSLoss 0.0388 (0.0153)	KLLoss 0.2812 (0.1711)	MaskLoss 1.0432 (0.6514)	MaskBCELoss 0.1438 (0.1312)	MaskDICELoss 0.8994 (0.5202)
[2025-03-02 16:54:23,700] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=0, lr=[0.00027202040816326527], mom=[(0.9, 0.95)]
[2025-03-02 16:54:23,705] [INFO] [timer.py:215:stop] epoch=0/micro_step=2800/global_step=280, RunningAvgSamplesPerSec=1.5771944848131116, CurrSamplesPerSec=1.3839607147305737, MemAllocated=31.39GB, MaxMemAllocated=36.79GB
Epoch: [0][280/500]	Time  7.227 ( 7.227)	Loss 0.0762 (2.2456)	CeLoss 0.0762 (0.3343)	SegCLSLoss 0.0000 (0.0244)	KLLoss 0.0000 (0.2297)	MaskLoss 0.0000 (0.8920)	MaskBCELoss 0.0000 (0.1959)	MaskDICELoss 0.0000 (0.6961)
Epoch: [0][281/500]	Time  6.346 ( 6.346)	Loss 2.8911 (1.9255)	CeLoss 0.1455 (0.4395)	SegCLSLoss 0.0654 (0.0214)	KLLoss 0.2852 (0.2055)	MaskLoss 1.2854 (0.6865)	MaskBCELoss 0.4354 (0.1572)	MaskDICELoss 0.8500 (0.5293)
Epoch: [0][282/500]	Time  5.337 ( 5.337)	Loss 2.4242 (1.8672)	CeLoss 0.2217 (0.5511)	SegCLSLoss 0.0317 (0.0186)	KLLoss 0.2910 (0.1688)	MaskLoss 1.0207 (0.6111)	MaskBCELoss 0.0912 (0.0989)	MaskDICELoss 0.9295 (0.5122)
Epoch: [0][283/500]	Time  5.428 ( 5.428)	Loss 2.1309 (1.6921)	CeLoss 0.2344 (0.5813)	SegCLSLoss 0.0223 (0.0139)	KLLoss 0.2949 (0.1768)	MaskLoss 0.8692 (0.5077)	MaskBCELoss 0.0788 (0.0821)	MaskDICELoss 0.7904 (0.4255)
Epoch: [0][284/500]	Time  5.628 ( 5.628)	Loss 1.7927 (2.0324)	CeLoss 0.2539 (0.4451)	SegCLSLoss 0.0176 (0.0293)	KLLoss 0.2988 (0.2338)	MaskLoss 0.6913 (0.7280)	MaskBCELoss 0.1094 (0.2114)	MaskDICELoss 0.5818 (0.5166)
Epoch: [0][285/500]	Time  7.590 ( 7.590)	Loss 2.8867 (2.4217)	CeLoss 0.2148 (0.2567)	SegCLSLoss 0.0254 (0.0236)	KLLoss 0.2793 (0.2877)	MaskLoss 1.2598 (1.0047)	MaskBCELoss 0.4874 (0.1935)	MaskDICELoss 0.7724 (0.8112)
Epoch: [0][286/500]	Time  6.004 ( 6.004)	Loss 2.1145 (2.2748)	CeLoss 0.1953 (0.3507)	SegCLSLoss 0.0361 (0.0226)	KLLoss 0.3047 (0.2338)	MaskLoss 0.8746 (0.8981)	MaskBCELoss 0.2908 (0.3234)	MaskDICELoss 0.5838 (0.5747)
Epoch: [0][287/500]	Time  5.538 ( 5.538)	Loss 2.5015 (2.0201)	CeLoss 0.2461 (0.5569)	SegCLSLoss 0.0306 (0.0176)	KLLoss 0.2715 (0.1691)	MaskLoss 1.0515 (0.6848)	MaskBCELoss 0.0866 (0.1360)	MaskDICELoss 0.9649 (0.5488)
Epoch: [0][288/500]	Time  6.902 ( 6.902)	Loss 2.7476 (2.3063)	CeLoss 0.2754 (0.4164)	SegCLSLoss 0.0159 (0.0187)	KLLoss 0.3008 (0.2289)	MaskLoss 1.1570 (0.8830)	MaskBCELoss 0.3338 (0.1815)	MaskDICELoss 0.8232 (0.7015)
Epoch: [0][289/500]	Time  6.438 ( 6.438)	Loss 3.6128 (2.1932)	CeLoss 0.3262 (0.3187)	SegCLSLoss 0.0124 (0.0234)	KLLoss 0.2988 (0.2332)	MaskLoss 1.5661 (0.8732)	MaskBCELoss 0.7774 (0.2530)	MaskDICELoss 0.7888 (0.6202)
[2025-03-02 16:55:24,851] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=0, lr=[0.00027079591836734696], mom=[(0.9, 0.95)]
[2025-03-02 16:55:24,857] [INFO] [timer.py:215:stop] epoch=0/micro_step=2900/global_step=290, RunningAvgSamplesPerSec=1.5791524063334705, CurrSamplesPerSec=1.68377815000557, MemAllocated=31.25GB, MaxMemAllocated=36.79GB
Epoch: [0][290/500]	Time  5.941 ( 5.941)	Loss 2.5012 (2.1920)	CeLoss 0.2520 (0.3492)	SegCLSLoss 0.0217 (0.0280)	KLLoss 0.3008 (0.2658)	MaskLoss 1.0436 (0.8479)	MaskBCELoss 0.1954 (0.2248)	MaskDICELoss 0.8481 (0.6231)
Epoch: [0][291/500]	Time  6.950 ( 6.950)	Loss 2.1905 (2.4591)	CeLoss 0.3613 (0.2771)	SegCLSLoss 0.0322 (0.0411)	KLLoss 0.2891 (0.2898)	MaskLoss 0.8335 (1.0082)	MaskBCELoss 0.2394 (0.1561)	MaskDICELoss 0.5941 (0.8520)
Epoch: [0][292/500]	Time  6.039 ( 6.039)	Loss 2.4681 (2.3335)	CeLoss 0.1562 (0.4645)	SegCLSLoss 0.0361 (0.0228)	KLLoss 0.3066 (0.2359)	MaskLoss 1.0700 (0.8697)	MaskBCELoss 0.0974 (0.2016)	MaskDICELoss 0.9726 (0.6681)
Epoch: [0][293/500]	Time  6.324 ( 6.324)	Loss 2.8165 (2.4525)	CeLoss 0.1924 (0.3222)	SegCLSLoss 0.0742 (0.0318)	KLLoss 0.2812 (0.2707)	MaskLoss 1.2227 (0.9894)	MaskBCELoss 0.2538 (0.2602)	MaskDICELoss 0.9689 (0.7292)
Epoch: [0][294/500]	Time  6.159 ( 6.159)	Loss 2.9995 (2.4701)	CeLoss 0.2129 (0.3675)	SegCLSLoss 0.0222 (0.0254)	KLLoss 0.3164 (0.2744)	MaskLoss 1.3083 (0.9764)	MaskBCELoss 0.4966 (0.3199)	MaskDICELoss 0.8117 (0.6565)
Epoch: [0][295/500]	Time  6.228 ( 6.228)	Loss 0.0557 (1.8684)	CeLoss 0.0557 (0.2844)	SegCLSLoss 0.0000 (0.0181)	KLLoss 0.0000 (0.2104)	MaskLoss 0.0000 (0.7348)	MaskBCELoss 0.0000 (0.2147)	MaskDICELoss 0.0000 (0.5201)
Epoch: [0][296/500]	Time  6.431 ( 6.431)	Loss 2.3433 (2.4359)	CeLoss 0.2236 (0.2630)	SegCLSLoss 0.0403 (0.0246)	KLLoss 0.2734 (0.3059)	MaskLoss 0.9812 (1.0039)	MaskBCELoss 0.2118 (0.2564)	MaskDICELoss 0.7695 (0.7475)
Epoch: [0][297/500]	Time  7.086 ( 7.086)	Loss 1.0938 (2.1254)	CeLoss 1.0938 (0.3970)	SegCLSLoss 0.0000 (0.0321)	KLLoss 0.0000 (0.1984)	MaskLoss 0.0000 (0.8066)	MaskBCELoss 0.0000 (0.1714)	MaskDICELoss 0.0000 (0.6352)
Epoch: [0][298/500]	Time  6.947 ( 6.947)	Loss 3.7630 (2.3870)	CeLoss 1.7891 (0.4014)	SegCLSLoss 0.0135 (0.0256)	KLLoss 0.3145 (0.2686)	MaskLoss 0.9049 (0.9192)	MaskBCELoss 0.5445 (0.2060)	MaskDICELoss 0.3605 (0.7132)
Epoch: [0][299/500]	Time  6.786 ( 6.786)	Loss 3.2351 (2.1631)	CeLoss 0.1855 (0.3761)	SegCLSLoss 0.0496 (0.0247)	KLLoss 0.2930 (0.2406)	MaskLoss 1.4398 (0.8271)	MaskBCELoss 0.5559 (0.1519)	MaskDICELoss 0.8839 (0.6752)
[2025-03-02 16:56:30,197] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=0, lr=[0.00026957142857142853], mom=[(0.9, 0.95)]
[2025-03-02 16:56:30,202] [INFO] [timer.py:215:stop] epoch=0/micro_step=3000/global_step=300, RunningAvgSamplesPerSec=1.577473394462608, CurrSamplesPerSec=1.5638310965594966, MemAllocated=31.26GB, MaxMemAllocated=36.79GB
Epoch: [0][300/500]	Time  6.396 ( 6.396)	Loss 2.5370 (2.3664)	CeLoss 0.2578 (0.3100)	SegCLSLoss 0.0267 (0.0294)	KLLoss 0.2891 (0.2609)	MaskLoss 1.0605 (0.9554)	MaskBCELoss 0.1000 (0.1749)	MaskDICELoss 0.9605 (0.7805)
Epoch: [0][301/500]	Time  6.435 ( 6.435)	Loss 2.5602 (2.0975)	CeLoss 0.2754 (0.5280)	SegCLSLoss 0.0195 (0.0182)	KLLoss 0.3164 (0.2115)	MaskLoss 1.0584 (0.7274)	MaskBCELoss 0.2375 (0.1749)	MaskDICELoss 0.8209 (0.5524)
Epoch: [0][302/500]	Time  6.054 ( 6.054)	Loss 2.9217 (2.1729)	CeLoss 0.2871 (0.5074)	SegCLSLoss 0.0327 (0.0253)	KLLoss 0.2715 (0.2051)	MaskLoss 1.2421 (0.7750)	MaskBCELoss 0.3705 (0.1674)	MaskDICELoss 0.8716 (0.6077)
Epoch: [0][303/500]	Time  5.431 ( 5.431)	Loss 2.3583 (1.7021)	CeLoss 0.2930 (0.5456)	SegCLSLoss 0.0289 (0.0192)	KLLoss 0.2793 (0.1713)	MaskLoss 0.9565 (0.5309)	MaskBCELoss 0.1148 (0.0578)	MaskDICELoss 0.8417 (0.4731)
Epoch: [0][304/500]	Time  6.724 ( 6.724)	Loss 2.2566 (2.3576)	CeLoss 0.1992 (0.3366)	SegCLSLoss 0.0243 (0.0251)	KLLoss 0.3242 (0.2664)	MaskLoss 0.9418 (0.9376)	MaskBCELoss 0.1716 (0.1474)	MaskDICELoss 0.7702 (0.7902)
Epoch: [0][305/500]	Time  4.693 ( 4.693)	Loss 0.0830 (1.7594)	CeLoss 0.0830 (0.6593)	SegCLSLoss 0.0000 (0.0137)	KLLoss 0.0000 (0.1166)	MaskLoss 0.0000 (0.5174)	MaskBCELoss 0.0000 (0.1769)	MaskDICELoss 0.0000 (0.3405)
Epoch: [0][306/500]	Time  6.382 ( 6.382)	Loss 1.6551 (2.3017)	CeLoss 0.2598 (0.3848)	SegCLSLoss 0.0262 (0.0277)	KLLoss 0.3223 (0.2670)	MaskLoss 0.6107 (0.8846)	MaskBCELoss 0.2561 (0.2140)	MaskDICELoss 0.3547 (0.6707)
Epoch: [0][307/500]	Time  6.845 ( 6.845)	Loss 2.3718 (2.2423)	CeLoss 0.2100 (0.2604)	SegCLSLoss 0.0378 (0.0326)	KLLoss 0.2891 (0.2662)	MaskLoss 0.9994 (0.9165)	MaskBCELoss 0.0279 (0.1443)	MaskDICELoss 0.9715 (0.7722)
Epoch: [0][308/500]	Time  6.563 ( 6.563)	Loss 2.4295 (1.9969)	CeLoss 0.1914 (0.2319)	SegCLSLoss 0.0571 (0.0290)	KLLoss 0.2832 (0.2662)	MaskLoss 1.0341 (0.8086)	MaskBCELoss 0.0491 (0.1526)	MaskDICELoss 0.9850 (0.6559)
Epoch: [0][309/500]	Time  5.548 ( 5.548)	Loss 1.2422 (2.1615)	CeLoss 1.2422 (0.5115)	SegCLSLoss 0.0000 (0.0325)	KLLoss 0.0000 (0.1994)	MaskLoss 0.0000 (0.7671)	MaskBCELoss 0.0000 (0.1537)	MaskDICELoss 0.0000 (0.6134)
[2025-03-02 16:57:31,160] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=0, lr=[0.00026834693877551016], mom=[(0.9, 0.95)]
[2025-03-02 16:57:31,166] [INFO] [timer.py:215:stop] epoch=0/micro_step=3100/global_step=310, RunningAvgSamplesPerSec=1.5794481863610754, CurrSamplesPerSec=1.5904689806704906, MemAllocated=31.24GB, MaxMemAllocated=36.79GB
Epoch: [0][310/500]	Time  6.289 ( 6.289)	Loss 2.8839 (2.5985)	CeLoss 0.2773 (0.2831)	SegCLSLoss 0.0231 (0.0306)	KLLoss 0.3008 (0.2582)	MaskLoss 1.2222 (1.0855)	MaskBCELoss 0.3833 (0.3726)	MaskDICELoss 0.8389 (0.7129)
Epoch: [0][311/500]	Time  7.328 ( 7.328)	Loss 2.2030 (2.3901)	CeLoss 0.3027 (0.3731)	SegCLSLoss 0.0214 (0.0272)	KLLoss 0.2930 (0.2602)	MaskLoss 0.8720 (0.9367)	MaskBCELoss 0.1327 (0.2612)	MaskDICELoss 0.7393 (0.6755)
Epoch: [0][312/500]	Time  6.627 ( 6.627)	Loss 2.1161 (1.8885)	CeLoss 0.2598 (0.3715)	SegCLSLoss 0.0298 (0.0200)	KLLoss 0.2930 (0.2080)	MaskLoss 0.8471 (0.7016)	MaskBCELoss 0.0447 (0.1205)	MaskDICELoss 0.8024 (0.5810)
Epoch: [0][313/500]	Time  6.426 ( 6.426)	Loss 2.8360 (2.1732)	CeLoss 0.2617 (0.5765)	SegCLSLoss 0.0172 (0.0183)	KLLoss 0.3086 (0.2070)	MaskLoss 1.2061 (0.7422)	MaskBCELoss 0.4506 (0.1592)	MaskDICELoss 0.7555 (0.5829)
Epoch: [0][314/500]	Time  5.953 ( 5.953)	Loss 2.1862 (2.4150)	CeLoss 0.2988 (0.4282)	SegCLSLoss 0.0239 (0.0176)	KLLoss 0.2949 (0.2363)	MaskLoss 0.8646 (0.9297)	MaskBCELoss 0.0892 (0.3149)	MaskDICELoss 0.7754 (0.6148)
Epoch: [0][315/500]	Time  6.153 ( 6.153)	Loss 2.2989 (1.6622)	CeLoss 0.2734 (0.6178)	SegCLSLoss 0.0203 (0.0160)	KLLoss 0.3027 (0.1426)	MaskLoss 0.9326 (0.4827)	MaskBCELoss 0.1914 (0.0729)	MaskDICELoss 0.7413 (0.4098)
Epoch: [0][316/500]	Time  6.426 ( 6.426)	Loss 1.8653 (2.0473)	CeLoss 0.2910 (0.3316)	SegCLSLoss 0.0199 (0.0250)	KLLoss 0.3027 (0.2307)	MaskLoss 0.7061 (0.7938)	MaskBCELoss 0.0852 (0.1506)	MaskDICELoss 0.6209 (0.6432)
Epoch: [0][317/500]	Time  5.458 ( 5.458)	Loss 2.6983 (2.2094)	CeLoss 0.2402 (0.4751)	SegCLSLoss 0.0305 (0.0199)	KLLoss 0.2754 (0.2336)	MaskLoss 1.1519 (0.8038)	MaskBCELoss 0.1909 (0.2021)	MaskDICELoss 0.9609 (0.6017)
Epoch: [0][318/500]	Time  5.470 ( 5.470)	Loss 2.6926 (2.0539)	CeLoss 0.2656 (0.4382)	SegCLSLoss 0.0227 (0.0212)	KLLoss 0.2852 (0.1980)	MaskLoss 1.1363 (0.7532)	MaskBCELoss 0.5550 (0.2010)	MaskDICELoss 0.5813 (0.5521)
Epoch: [0][319/500]	Time  6.457 ( 6.457)	Loss 2.7291 (2.3376)	CeLoss 0.3008 (0.4672)	SegCLSLoss 0.0175 (0.0172)	KLLoss 0.2871 (0.2328)	MaskLoss 1.1380 (0.8727)	MaskBCELoss 0.2918 (0.2170)	MaskDICELoss 0.8462 (0.6557)
[2025-03-02 16:58:33,688] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=0, lr=[0.0002671224489795918], mom=[(0.9, 0.95)]
[2025-03-02 16:58:33,694] [INFO] [timer.py:215:stop] epoch=0/micro_step=3200/global_step=320, RunningAvgSamplesPerSec=1.5800748231276196, CurrSamplesPerSec=1.605196141593297, MemAllocated=31.25GB, MaxMemAllocated=36.79GB
Epoch: [0][320/500]	Time  6.231 ( 6.231)	Loss 1.8336 (2.1428)	CeLoss 0.2637 (0.4722)	SegCLSLoss 0.0173 (0.0253)	KLLoss 0.2852 (0.2281)	MaskLoss 0.7098 (0.7720)	MaskBCELoss 0.0376 (0.1690)	MaskDICELoss 0.6722 (0.6030)
Epoch: [0][321/500]	Time  5.921 ( 5.921)	Loss 2.4546 (1.6152)	CeLoss 0.2139 (0.4089)	SegCLSLoss 0.0325 (0.0159)	KLLoss 0.2754 (0.1727)	MaskLoss 1.0437 (0.5559)	MaskBCELoss 0.0666 (0.0926)	MaskDICELoss 0.9771 (0.4633)
Epoch: [0][322/500]	Time  5.935 ( 5.935)	Loss 0.0952 (1.9794)	CeLoss 0.0952 (0.3917)	SegCLSLoss 0.0000 (0.0189)	KLLoss 0.0000 (0.1969)	MaskLoss 0.0000 (0.7401)	MaskBCELoss 0.0000 (0.2227)	MaskDICELoss 0.0000 (0.5174)
Epoch: [0][323/500]	Time  5.913 ( 5.913)	Loss 2.3632 (2.1091)	CeLoss 0.1865 (0.5015)	SegCLSLoss 0.0364 (0.0262)	KLLoss 0.2773 (0.1920)	MaskLoss 1.0097 (0.7492)	MaskBCELoss 0.0728 (0.1350)	MaskDICELoss 0.9369 (0.6142)
Epoch: [0][324/500]	Time  5.732 ( 5.732)	Loss 0.8984 (1.7849)	CeLoss 0.8984 (0.4687)	SegCLSLoss 0.0000 (0.0144)	KLLoss 0.0000 (0.1699)	MaskLoss 0.0000 (0.6118)	MaskBCELoss 0.0000 (0.1597)	MaskDICELoss 0.0000 (0.4521)
Epoch: [0][325/500]	Time  6.384 ( 6.384)	Loss 1.7261 (2.2743)	CeLoss 0.4375 (0.3536)	SegCLSLoss 0.0177 (0.0219)	KLLoss 0.2832 (0.2574)	MaskLoss 0.5681 (0.8904)	MaskBCELoss 0.1488 (0.2141)	MaskDICELoss 0.4194 (0.6762)
Epoch: [0][326/500]	Time  6.165 ( 6.165)	Loss 1.1365 (2.2833)	CeLoss 0.2559 (0.3547)	SegCLSLoss 0.0334 (0.0236)	KLLoss 0.2910 (0.2258)	MaskLoss 0.3593 (0.9021)	MaskBCELoss 0.1715 (0.2363)	MaskDICELoss 0.1877 (0.6658)
Epoch: [0][327/500]	Time  6.549 ( 6.549)	Loss 1.8129 (1.8902)	CeLoss 0.2539 (0.3008)	SegCLSLoss 0.0199 (0.0220)	KLLoss 0.2969 (0.2318)	MaskLoss 0.7004 (0.7313)	MaskBCELoss 0.1569 (0.1402)	MaskDICELoss 0.5434 (0.5911)
Epoch: [0][328/500]	Time  5.933 ( 5.933)	Loss 2.7861 (1.9656)	CeLoss 0.3516 (0.4927)	SegCLSLoss 0.0280 (0.0214)	KLLoss 0.2812 (0.1990)	MaskLoss 1.1392 (0.6811)	MaskBCELoss 0.2800 (0.1382)	MaskDICELoss 0.8592 (0.5430)
Epoch: [0][329/500]	Time  6.032 ( 6.032)	Loss 2.3606 (1.9999)	CeLoss 0.2324 (0.3176)	SegCLSLoss 0.0327 (0.0233)	KLLoss 0.2773 (0.2271)	MaskLoss 0.9869 (0.7785)	MaskBCELoss 0.0754 (0.1409)	MaskDICELoss 0.9116 (0.6376)
[2025-03-02 16:59:32,706] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=0, lr=[0.0002658979591836734], mom=[(0.9, 0.95)]
[2025-03-02 16:59:32,711] [INFO] [timer.py:215:stop] epoch=0/micro_step=3300/global_step=330, RunningAvgSamplesPerSec=1.5833416070433979, CurrSamplesPerSec=2.246647641000801, MemAllocated=31.24GB, MaxMemAllocated=36.79GB
Epoch: [0][330/500]	Time  4.453 ( 4.453)	Loss 2.4045 (1.6342)	CeLoss 0.2236 (0.6813)	SegCLSLoss 0.0312 (0.0155)	KLLoss 0.2773 (0.1482)	MaskLoss 1.0128 (0.4354)	MaskBCELoss 0.0667 (0.0621)	MaskDICELoss 0.9461 (0.3732)
Epoch: [0][331/500]	Time  6.437 ( 6.437)	Loss 2.3619 (2.1306)	CeLoss 0.2207 (0.3075)	SegCLSLoss 0.0327 (0.0257)	KLLoss 0.2871 (0.2658)	MaskLoss 0.9905 (0.8388)	MaskBCELoss 0.0636 (0.1682)	MaskDICELoss 0.9270 (0.6705)
Epoch: [0][332/500]	Time  6.116 ( 6.116)	Loss 2.4005 (2.0723)	CeLoss 0.2334 (0.4524)	SegCLSLoss 0.0493 (0.0297)	KLLoss 0.2832 (0.1982)	MaskLoss 1.0010 (0.7529)	MaskBCELoss 0.0518 (0.1580)	MaskDICELoss 0.9493 (0.5949)
Epoch: [0][333/500]	Time  6.104 ( 6.104)	Loss 1.8852 (1.8520)	CeLoss 0.2871 (0.7028)	SegCLSLoss 0.0222 (0.0153)	KLLoss 0.3359 (0.1854)	MaskLoss 0.7092 (0.5245)	MaskBCELoss 0.2585 (0.1100)	MaskDICELoss 0.4507 (0.4145)
Epoch: [0][334/500]	Time  5.653 ( 5.653)	Loss 0.9297 (1.8000)	CeLoss 0.9297 (0.5389)	SegCLSLoss 0.0000 (0.0174)	KLLoss 0.0000 (0.1770)	MaskLoss 0.0000 (0.5821)	MaskBCELoss 0.0000 (0.1112)	MaskDICELoss 0.0000 (0.4709)
Epoch: [0][335/500]	Time  6.259 ( 6.259)	Loss 2.8126 (2.1751)	CeLoss 0.2432 (0.5513)	SegCLSLoss 0.0223 (0.0228)	KLLoss 0.2949 (0.2090)	MaskLoss 1.2051 (0.7541)	MaskBCELoss 0.2435 (0.1775)	MaskDICELoss 0.9616 (0.5767)
Epoch: [0][336/500]	Time  6.529 ( 6.529)	Loss 3.1372 (2.4920)	CeLoss 0.1758 (0.2843)	SegCLSLoss 0.0393 (0.0282)	KLLoss 0.2754 (0.3010)	MaskLoss 1.4026 (1.0215)	MaskBCELoss 0.5349 (0.2986)	MaskDICELoss 0.8677 (0.7228)
Epoch: [0][337/500]	Time  6.908 ( 6.908)	Loss 2.8326 (2.3563)	CeLoss 0.1797 (0.2217)	SegCLSLoss 0.0615 (0.0292)	KLLoss 0.2812 (0.2584)	MaskLoss 1.2405 (0.9953)	MaskBCELoss 0.3872 (0.2001)	MaskDICELoss 0.8534 (0.7952)
Epoch: [0][338/500]	Time  4.863 ( 4.863)	Loss 2.5003 (1.9657)	CeLoss 0.2891 (0.5356)	SegCLSLoss 0.0183 (0.0178)	KLLoss 0.2852 (0.1797)	MaskLoss 1.0294 (0.6657)	MaskBCELoss 0.0446 (0.2349)	MaskDICELoss 0.9849 (0.4308)
Epoch: [0][339/500]	Time  5.057 ( 5.057)	Loss 2.3709 (1.8456)	CeLoss 0.2129 (0.6539)	SegCLSLoss 0.0286 (0.0169)	KLLoss 0.2832 (0.1760)	MaskLoss 1.0009 (0.5475)	MaskBCELoss 0.0139 (0.0806)	MaskDICELoss 0.9870 (0.4669)
[2025-03-02 17:00:32,752] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=0, lr=[0.0002646734693877551], mom=[(0.9, 0.95)]
[2025-03-02 17:00:32,758] [INFO] [timer.py:215:stop] epoch=0/micro_step=3400/global_step=340, RunningAvgSamplesPerSec=1.5856614369905664, CurrSamplesPerSec=1.6339377542884543, MemAllocated=31.14GB, MaxMemAllocated=36.79GB
Epoch: [0][340/500]	Time  6.122 ( 6.122)	Loss 2.7710 (2.3074)	CeLoss 0.1865 (0.4089)	SegCLSLoss 0.0859 (0.0297)	KLLoss 0.2734 (0.2236)	MaskLoss 1.2019 (0.8858)	MaskBCELoss 0.4089 (0.1752)	MaskDICELoss 0.7930 (0.7107)
Epoch: [0][341/500]	Time  5.448 ( 5.448)	Loss 2.1545 (2.0512)	CeLoss 0.2432 (0.4194)	SegCLSLoss 0.0147 (0.0246)	KLLoss 0.3184 (0.2021)	MaskLoss 0.8722 (0.7592)	MaskBCELoss 0.5117 (0.2084)	MaskDICELoss 0.3605 (0.5508)
Epoch: [0][342/500]	Time  6.129 ( 6.129)	Loss 0.3359 (2.0832)	CeLoss 0.3359 (0.4039)	SegCLSLoss 0.0000 (0.0245)	KLLoss 0.0000 (0.1959)	MaskLoss 0.0000 (0.7846)	MaskBCELoss 0.0000 (0.1781)	MaskDICELoss 0.0000 (0.6065)
Epoch: [0][343/500]	Time  6.361 ( 6.361)	Loss 2.7640 (2.2659)	CeLoss 0.2559 (0.3901)	SegCLSLoss 0.0447 (0.0233)	KLLoss 0.2754 (0.2285)	MaskLoss 1.1750 (0.8748)	MaskBCELoss 0.3022 (0.1510)	MaskDICELoss 0.8728 (0.7238)
Epoch: [0][344/500]	Time  7.012 ( 7.012)	Loss 1.9475 (2.0341)	CeLoss 0.3379 (0.3702)	SegCLSLoss 0.0146 (0.0196)	KLLoss 0.3008 (0.2643)	MaskLoss 0.7257 (0.7611)	MaskBCELoss 0.1232 (0.1182)	MaskDICELoss 0.6026 (0.6429)
Epoch: [0][345/500]	Time  6.313 ( 6.313)	Loss 2.8389 (2.0690)	CeLoss 0.2520 (0.3088)	SegCLSLoss 0.0142 (0.0214)	KLLoss 0.2930 (0.2277)	MaskLoss 1.2163 (0.8176)	MaskBCELoss 0.5333 (0.1182)	MaskDICELoss 0.6831 (0.6994)
Epoch: [0][346/500]	Time  4.741 ( 4.741)	Loss 3.0128 (2.2109)	CeLoss 0.1621 (0.4243)	SegCLSLoss 0.0747 (0.0285)	KLLoss 0.2832 (0.2299)	MaskLoss 1.3365 (0.8288)	MaskBCELoss 0.4729 (0.2148)	MaskDICELoss 0.8636 (0.6140)
Epoch: [0][347/500]	Time  6.289 ( 6.289)	Loss 2.1387 (1.8298)	CeLoss 0.2266 (0.4622)	SegCLSLoss 0.0304 (0.0141)	KLLoss 0.2910 (0.2086)	MaskLoss 0.8760 (0.6280)	MaskBCELoss 0.0246 (0.1571)	MaskDICELoss 0.8514 (0.4709)
Epoch: [0][348/500]	Time  6.825 ( 6.825)	Loss 2.4009 (2.1082)	CeLoss 0.1963 (0.4181)	SegCLSLoss 0.0288 (0.0213)	KLLoss 0.2773 (0.2254)	MaskLoss 1.0256 (0.7833)	MaskBCELoss 0.2419 (0.1254)	MaskDICELoss 0.7837 (0.6580)
Epoch: [0][349/500]	Time  5.920 ( 5.920)	Loss 2.6386 (2.1997)	CeLoss 0.1768 (0.5200)	SegCLSLoss 0.0320 (0.0191)	KLLoss 0.2754 (0.2037)	MaskLoss 1.1543 (0.7841)	MaskBCELoss 0.3229 (0.2190)	MaskDICELoss 0.8314 (0.5652)
[2025-03-02 17:01:34,571] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=0, lr=[0.0002634489795918367], mom=[(0.9, 0.95)]
[2025-03-02 17:01:34,576] [INFO] [timer.py:215:stop] epoch=0/micro_step=3500/global_step=350, RunningAvgSamplesPerSec=1.5865719800267968, CurrSamplesPerSec=1.4753126053686934, MemAllocated=31.25GB, MaxMemAllocated=36.79GB
Epoch: [0][350/500]	Time  6.780 ( 6.780)	Loss 2.5661 (2.3865)	CeLoss 0.3047 (0.3129)	SegCLSLoss 0.0259 (0.0293)	KLLoss 0.2891 (0.2551)	MaskLoss 1.0516 (0.9655)	MaskBCELoss 0.0680 (0.1714)	MaskDICELoss 0.9836 (0.7941)
Epoch: [0][351/500]	Time  6.257 ( 6.257)	Loss 1.1250 (1.9835)	CeLoss 1.1250 (0.4077)	SegCLSLoss 0.0000 (0.0195)	KLLoss 0.0000 (0.1994)	MaskLoss 0.0000 (0.7332)	MaskBCELoss 0.0000 (0.1609)	MaskDICELoss 0.0000 (0.5723)
Epoch: [0][352/500]	Time  4.936 ( 4.936)	Loss 2.1725 (1.9569)	CeLoss 0.2734 (0.6291)	SegCLSLoss 0.0151 (0.0150)	KLLoss 0.2852 (0.2031)	MaskLoss 0.8743 (0.6094)	MaskBCELoss 0.0600 (0.0758)	MaskDICELoss 0.8144 (0.5336)
Epoch: [0][353/500]	Time  5.227 ( 5.227)	Loss 1.1250 (1.5420)	CeLoss 1.1250 (0.5112)	SegCLSLoss 0.0000 (0.0137)	KLLoss 0.0000 (0.1756)	MaskLoss 0.0000 (0.4681)	MaskBCELoss 0.0000 (0.0681)	MaskDICELoss 0.0000 (0.3999)
Epoch: [0][354/500]	Time  6.641 ( 6.641)	Loss 2.3813 (1.9086)	CeLoss 0.2168 (0.3562)	SegCLSLoss 0.0278 (0.0233)	KLLoss 0.2773 (0.1980)	MaskLoss 1.0061 (0.7209)	MaskBCELoss 0.0067 (0.1104)	MaskDICELoss 0.9994 (0.6105)
Epoch: [0][355/500]	Time  7.991 ( 7.991)	Loss 2.7335 (2.3635)	CeLoss 0.2812 (0.4171)	SegCLSLoss 0.0236 (0.0291)	KLLoss 0.2773 (0.2236)	MaskLoss 1.1509 (0.9100)	MaskBCELoss 0.3311 (0.2497)	MaskDICELoss 0.8198 (0.6603)
Epoch: [0][356/500]	Time  6.482 ( 6.482)	Loss 0.9922 (2.2569)	CeLoss 0.9922 (0.3977)	SegCLSLoss 0.0000 (0.0215)	KLLoss 0.0000 (0.2283)	MaskLoss 0.0000 (0.8673)	MaskBCELoss 0.0000 (0.2260)	MaskDICELoss 0.0000 (0.6414)
Epoch: [0][357/500]	Time  6.084 ( 6.084)	Loss 2.6026 (2.1000)	CeLoss 0.1523 (0.5802)	SegCLSLoss 0.0742 (0.0218)	KLLoss 0.2812 (0.2045)	MaskLoss 1.1363 (0.7034)	MaskBCELoss 0.2290 (0.1756)	MaskDICELoss 0.9073 (0.5279)
Epoch: [0][358/500]	Time  5.709 ( 5.709)	Loss 2.3741 (1.8440)	CeLoss 0.2051 (0.6098)	SegCLSLoss 0.0334 (0.0183)	KLLoss 0.2812 (0.1762)	MaskLoss 1.0054 (0.5685)	MaskBCELoss 0.0184 (0.1159)	MaskDICELoss 0.9870 (0.4526)
Epoch: [0][359/500]	Time  6.413 ( 6.413)	Loss 2.7721 (2.1563)	CeLoss 0.2334 (0.5425)	SegCLSLoss 0.0388 (0.0204)	KLLoss 0.2734 (0.2023)	MaskLoss 1.1917 (0.7511)	MaskBCELoss 0.3483 (0.1354)	MaskDICELoss 0.8434 (0.6157)
[2025-03-02 17:02:37,715] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=0, lr=[0.00026222448979591837], mom=[(0.9, 0.95)]
[2025-03-02 17:02:37,720] [INFO] [timer.py:215:stop] epoch=0/micro_step=3600/global_step=360, RunningAvgSamplesPerSec=1.5864992998499605, CurrSamplesPerSec=1.3511315857172872, MemAllocated=31.24GB, MaxMemAllocated=36.79GB
Epoch: [0][360/500]	Time  7.403 ( 7.403)	Loss 2.3417 (2.1253)	CeLoss 0.2363 (0.2381)	SegCLSLoss 0.0176 (0.0256)	KLLoss 0.2988 (0.2926)	MaskLoss 0.9736 (0.8640)	MaskBCELoss 0.3234 (0.1103)	MaskDICELoss 0.6502 (0.7538)
Epoch: [0][361/500]	Time  6.022 ( 6.022)	Loss 2.3639 (1.9354)	CeLoss 0.2871 (0.3338)	SegCLSLoss 0.0217 (0.0221)	KLLoss 0.2832 (0.2313)	MaskLoss 0.9612 (0.7374)	MaskBCELoss 0.1522 (0.1999)	MaskDICELoss 0.8091 (0.5374)
Epoch: [0][362/500]	Time  7.409 ( 7.409)	Loss 2.5834 (2.5280)	CeLoss 0.2617 (0.2109)	SegCLSLoss 0.0178 (0.0341)	KLLoss 0.2930 (0.2848)	MaskLoss 1.0827 (1.0788)	MaskBCELoss 0.3393 (0.3343)	MaskDICELoss 0.7434 (0.7445)
Epoch: [0][363/500]	Time  6.459 ( 6.459)	Loss 3.6024 (2.1896)	CeLoss 0.2393 (0.4484)	SegCLSLoss 0.0212 (0.0261)	KLLoss 0.2695 (0.2230)	MaskLoss 1.6088 (0.8085)	MaskBCELoss 0.8172 (0.1925)	MaskDICELoss 0.7916 (0.6160)
Epoch: [0][364/500]	Time  6.925 ( 6.925)	Loss 2.7436 (2.1164)	CeLoss 0.1680 (0.4114)	SegCLSLoss 0.0339 (0.0196)	KLLoss 0.2715 (0.2271)	MaskLoss 1.2116 (0.7906)	MaskBCELoss 0.3732 (0.1123)	MaskDICELoss 0.8384 (0.6784)
Epoch: [0][365/500]	Time  7.052 ( 7.052)	Loss 2.3004 (2.1336)	CeLoss 0.2070 (0.4416)	SegCLSLoss 0.0295 (0.0253)	KLLoss 0.2852 (0.2227)	MaskLoss 0.9686 (0.7840)	MaskBCELoss 0.0331 (0.0763)	MaskDICELoss 0.9355 (0.7077)
Epoch: [0][366/500]	Time  6.221 ( 6.221)	Loss 1.8336 (2.0397)	CeLoss 0.2471 (0.4087)	SegCLSLoss 0.0192 (0.0337)	KLLoss 0.2988 (0.2002)	MaskLoss 0.7137 (0.7570)	MaskBCELoss 0.0601 (0.2197)	MaskDICELoss 0.6535 (0.5373)
Epoch: [0][367/500]	Time  5.967 ( 5.967)	Loss 1.4922 (1.7696)	CeLoss 1.4922 (0.4357)	SegCLSLoss 0.0000 (0.0171)	KLLoss 0.0000 (0.1717)	MaskLoss 0.0000 (0.6200)	MaskBCELoss 0.0000 (0.1560)	MaskDICELoss 0.0000 (0.4640)
Epoch: [0][368/500]	Time  5.576 ( 5.576)	Loss 2.5272 (2.2252)	CeLoss 0.2080 (0.5857)	SegCLSLoss 0.0317 (0.0206)	KLLoss 0.2812 (0.1965)	MaskLoss 1.0819 (0.7656)	MaskBCELoss 0.3042 (0.1708)	MaskDICELoss 0.7777 (0.5949)
Epoch: [0][369/500]	Time  5.719 ( 5.719)	Loss 2.1766 (2.1583)	CeLoss 0.2500 (0.4083)	SegCLSLoss 0.0337 (0.0233)	KLLoss 0.2891 (0.2291)	MaskLoss 0.8822 (0.8119)	MaskBCELoss 0.1627 (0.2297)	MaskDICELoss 0.7196 (0.5821)
[2025-03-02 17:03:42,325] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=0, lr=[0.000261], mom=[(0.9, 0.95)]
[2025-03-02 17:03:42,331] [INFO] [timer.py:215:stop] epoch=0/micro_step=3700/global_step=370, RunningAvgSamplesPerSec=1.5854295101214868, CurrSamplesPerSec=1.3777754710671697, MemAllocated=31.24GB, MaxMemAllocated=36.79GB
Epoch: [0][370/500]	Time  7.260 ( 7.260)	Loss 3.0241 (2.1060)	CeLoss 0.2988 (0.3725)	SegCLSLoss 0.0356 (0.0204)	KLLoss 0.2695 (0.1969)	MaskLoss 1.2865 (0.8123)	MaskBCELoss 0.3935 (0.1955)	MaskDICELoss 0.8930 (0.6168)
Epoch: [0][371/500]	Time  4.816 ( 4.816)	Loss 1.6875 (2.0314)	CeLoss 1.6875 (0.7353)	SegCLSLoss 0.0000 (0.0177)	KLLoss 0.0000 (0.1443)	MaskLoss 0.0000 (0.6077)	MaskBCELoss 0.0000 (0.1993)	MaskDICELoss 0.0000 (0.4084)
Epoch: [0][372/500]	Time  5.704 ( 5.704)	Loss 2.5278 (2.0660)	CeLoss 0.3398 (0.4832)	SegCLSLoss 0.0209 (0.0190)	KLLoss 0.3164 (0.2082)	MaskLoss 1.0100 (0.7346)	MaskBCELoss 0.4156 (0.2328)	MaskDICELoss 0.5944 (0.5018)
Epoch: [0][373/500]	Time  6.436 ( 6.436)	Loss 1.2109 (2.1763)	CeLoss 1.2109 (0.5172)	SegCLSLoss 0.0000 (0.0234)	KLLoss 0.0000 (0.2031)	MaskLoss 0.0000 (0.7731)	MaskBCELoss 0.0000 (0.2191)	MaskDICELoss 0.0000 (0.5540)
Epoch: [0][374/500]	Time  7.036 ( 7.036)	Loss 3.0863 (2.3630)	CeLoss 0.2432 (0.2737)	SegCLSLoss 0.0250 (0.0285)	KLLoss 0.2773 (0.2590)	MaskLoss 1.3459 (0.9727)	MaskBCELoss 0.4400 (0.1865)	MaskDICELoss 0.9059 (0.7862)
Epoch: [0][375/500]	Time  6.740 ( 6.740)	Loss 2.4652 (2.0021)	CeLoss 0.2197 (0.5019)	SegCLSLoss 0.0342 (0.0266)	KLLoss 0.2754 (0.1977)	MaskLoss 1.0451 (0.6940)	MaskBCELoss 0.0801 (0.1151)	MaskDICELoss 0.9650 (0.5790)
Epoch: [0][376/500]	Time  5.972 ( 5.972)	Loss 0.4863 (1.7006)	CeLoss 0.4863 (0.4146)	SegCLSLoss 0.0000 (0.0199)	KLLoss 0.0000 (0.1697)	MaskLoss 0.0000 (0.5954)	MaskBCELoss 0.0000 (0.0941)	MaskDICELoss 0.0000 (0.5013)
Epoch: [0][377/500]	Time  6.204 ( 6.204)	Loss 0.9336 (2.0799)	CeLoss 0.9336 (0.3924)	SegCLSLoss 0.0000 (0.0240)	KLLoss 0.0000 (0.2309)	MaskLoss 0.0000 (0.7801)	MaskBCELoss 0.0000 (0.1887)	MaskDICELoss 0.0000 (0.5914)
Epoch: [0][378/500]	Time  6.196 ( 6.196)	Loss 2.4561 (2.1611)	CeLoss 0.2676 (0.5778)	SegCLSLoss 0.0214 (0.0220)	KLLoss 0.2949 (0.1973)	MaskLoss 1.0151 (0.7367)	MaskBCELoss 0.2414 (0.1503)	MaskDICELoss 0.7738 (0.5865)
Epoch: [0][379/500]	Time  6.534 ( 6.534)	Loss 2.4631 (2.2974)	CeLoss 0.2598 (0.3585)	SegCLSLoss 0.0238 (0.0252)	KLLoss 0.2852 (0.2311)	MaskLoss 1.0245 (0.9056)	MaskBCELoss 0.0312 (0.1930)	MaskDICELoss 0.9933 (0.7126)
[2025-03-02 17:04:44,041] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=0, lr=[0.0002597755102040816], mom=[(0.9, 0.95)]
[2025-03-02 17:04:44,047] [INFO] [timer.py:215:stop] epoch=0/micro_step=3800/global_step=380, RunningAvgSamplesPerSec=1.5863418070719144, CurrSamplesPerSec=1.6459376227790674, MemAllocated=31.59GB, MaxMemAllocated=36.79GB
Epoch: [0][380/500]	Time  6.077 ( 6.077)	Loss 0.0571 (2.2719)	CeLoss 0.0571 (0.3688)	SegCLSLoss 0.0000 (0.0259)	KLLoss 0.0000 (0.2311)	MaskLoss 0.0000 (0.8874)	MaskBCELoss 0.0000 (0.1951)	MaskDICELoss 0.0000 (0.6922)
Epoch: [0][381/500]	Time  6.666 ( 6.666)	Loss 2.4566 (2.2800)	CeLoss 0.3164 (0.2283)	SegCLSLoss 0.0243 (0.0292)	KLLoss 0.2773 (0.2893)	MaskLoss 0.9949 (0.9463)	MaskBCELoss 0.0119 (0.1044)	MaskDICELoss 0.9830 (0.8418)
Epoch: [0][382/500]	Time  6.039 ( 6.039)	Loss 2.2840 (2.1003)	CeLoss 0.2002 (0.5041)	SegCLSLoss 0.0361 (0.0250)	KLLoss 0.2852 (0.1980)	MaskLoss 0.9613 (0.7424)	MaskBCELoss 0.0503 (0.1658)	MaskDICELoss 0.9110 (0.5766)
Epoch: [0][383/500]	Time  5.905 ( 5.905)	Loss 1.1328 (2.0094)	CeLoss 1.1328 (0.6261)	SegCLSLoss 0.0000 (0.0115)	KLLoss 0.0000 (0.1432)	MaskLoss 0.0000 (0.6531)	MaskBCELoss 0.0000 (0.2430)	MaskDICELoss 0.0000 (0.4101)
Epoch: [0][384/500]	Time  6.363 ( 6.363)	Loss 2.3660 (2.0017)	CeLoss 0.3125 (0.4869)	SegCLSLoss 0.0148 (0.0222)	KLLoss 0.2930 (0.2018)	MaskLoss 0.9496 (0.7016)	MaskBCELoss 0.0528 (0.1299)	MaskDICELoss 0.8968 (0.5717)
Epoch: [0][385/500]	Time  4.105 ( 4.105)	Loss 1.2188 (1.7490)	CeLoss 1.2188 (0.8232)	SegCLSLoss 0.0000 (0.0102)	KLLoss 0.0000 (0.1145)	MaskLoss 0.0000 (0.4316)	MaskBCELoss 0.0000 (0.1407)	MaskDICELoss 0.0000 (0.2909)
Epoch: [0][386/500]	Time  7.197 ( 7.197)	Loss 3.0142 (2.1469)	CeLoss 0.2373 (0.2885)	SegCLSLoss 0.0294 (0.0300)	KLLoss 0.2656 (0.2590)	MaskLoss 1.3147 (0.8569)	MaskBCELoss 0.4790 (0.1910)	MaskDICELoss 0.8357 (0.6659)
Epoch: [0][387/500]	Time  5.508 ( 5.508)	Loss 0.8086 (1.6668)	CeLoss 0.8086 (0.4375)	SegCLSLoss 0.0000 (0.0184)	KLLoss 0.0000 (0.1727)	MaskLoss 0.0000 (0.5668)	MaskBCELoss 0.0000 (0.1329)	MaskDICELoss 0.0000 (0.4339)
Epoch: [0][388/500]	Time  6.886 ( 6.886)	Loss 2.3562 (2.2184)	CeLoss 0.2324 (0.2277)	SegCLSLoss 0.0156 (0.0345)	KLLoss 0.2930 (0.2824)	MaskLoss 0.9847 (0.9162)	MaskBCELoss 0.1637 (0.2418)	MaskDICELoss 0.8210 (0.6744)
Epoch: [0][389/500]	Time  6.497 ( 6.497)	Loss 2.6773 (2.1249)	CeLoss 0.3730 (0.4841)	SegCLSLoss 0.0177 (0.0209)	KLLoss 0.2812 (0.2283)	MaskLoss 1.0769 (0.7580)	MaskBCELoss 0.0781 (0.2000)	MaskDICELoss 0.9988 (0.5580)
[2025-03-02 17:05:44,697] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=0, lr=[0.00025855102040816326], mom=[(0.9, 0.95)]
[2025-03-02 17:05:44,702] [INFO] [timer.py:215:stop] epoch=0/micro_step=3900/global_step=390, RunningAvgSamplesPerSec=1.5878963570346716, CurrSamplesPerSec=1.822646730135427, MemAllocated=30.78GB, MaxMemAllocated=36.87GB
Epoch: [0][390/500]	Time  5.488 ( 5.488)	Loss 0.9453 (1.8511)	CeLoss 0.9453 (0.6954)	SegCLSLoss 0.0000 (0.0144)	KLLoss 0.0000 (0.1670)	MaskLoss 0.0000 (0.5325)	MaskBCELoss 0.0000 (0.0743)	MaskDICELoss 0.0000 (0.4582)
Epoch: [0][391/500]	Time  6.570 ( 6.570)	Loss 2.7235 (1.7422)	CeLoss 0.1494 (0.2617)	SegCLSLoss 0.0540 (0.0210)	KLLoss 0.2656 (0.1955)	MaskLoss 1.2075 (0.6859)	MaskBCELoss 0.3321 (0.1967)	MaskDICELoss 0.8753 (0.4892)
Epoch: [0][392/500]	Time  4.685 ( 4.685)	Loss 0.8011 (1.7942)	CeLoss 0.3047 (0.5154)	SegCLSLoss 0.0177 (0.0230)	KLLoss 0.3164 (0.1982)	MaskLoss 0.1642 (0.5842)	MaskBCELoss 0.1306 (0.1526)	MaskDICELoss 0.0336 (0.4316)
Epoch: [0][393/500]	Time  6.055 ( 6.055)	Loss 2.5301 (2.1219)	CeLoss 0.2148 (0.3628)	SegCLSLoss 0.0171 (0.0222)	KLLoss 0.2930 (0.2256)	MaskLoss 1.0795 (0.8175)	MaskBCELoss 0.5133 (0.2343)	MaskDICELoss 0.5662 (0.5832)
Epoch: [0][394/500]	Time  5.636 ( 5.636)	Loss 0.8633 (1.9047)	CeLoss 0.8633 (0.5443)	SegCLSLoss 0.0000 (0.0153)	KLLoss 0.0000 (0.1676)	MaskLoss 0.0000 (0.6345)	MaskBCELoss 0.0000 (0.1775)	MaskDICELoss 0.0000 (0.4570)
Epoch: [0][395/500]	Time  5.976 ( 5.976)	Loss 2.5877 (1.9072)	CeLoss 0.2012 (0.3889)	SegCLSLoss 0.0206 (0.0164)	KLLoss 0.2812 (0.1951)	MaskLoss 1.1181 (0.7063)	MaskBCELoss 0.5293 (0.1267)	MaskDICELoss 0.5888 (0.5796)
Epoch: [0][396/500]	Time  5.428 ( 5.428)	Loss 0.9961 (1.6324)	CeLoss 0.9961 (0.4607)	SegCLSLoss 0.0000 (0.0181)	KLLoss 0.0000 (0.1996)	MaskLoss 0.0000 (0.5314)	MaskBCELoss 0.0000 (0.1188)	MaskDICELoss 0.0000 (0.4127)
Epoch: [0][397/500]	Time  6.796 ( 6.796)	Loss 2.3253 (1.8698)	CeLoss 0.2031 (0.5262)	SegCLSLoss 0.0200 (0.0165)	KLLoss 0.2812 (0.1684)	MaskLoss 0.9859 (0.6254)	MaskBCELoss 0.0065 (0.1300)	MaskDICELoss 0.9794 (0.4954)
Epoch: [0][398/500]	Time  5.758 ( 5.758)	Loss 2.6782 (1.8378)	CeLoss 0.2871 (0.3795)	SegCLSLoss 0.0147 (0.0161)	KLLoss 0.2891 (0.1967)	MaskLoss 1.1194 (0.6761)	MaskBCELoss 0.8744 (0.1777)	MaskDICELoss 0.2450 (0.4985)
Epoch: [0][399/500]	Time  6.356 ( 6.356)	Loss 2.6900 (2.1046)	CeLoss 0.1855 (0.3802)	SegCLSLoss 0.0466 (0.0254)	KLLoss 0.2695 (0.2230)	MaskLoss 1.1731 (0.8002)	MaskBCELoss 0.2570 (0.2022)	MaskDICELoss 0.9162 (0.5980)
[2025-03-02 17:06:44,005] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=0, lr=[0.00025732653061224484], mom=[(0.9, 0.95)]
[2025-03-02 17:06:44,011] [INFO] [timer.py:215:stop] epoch=0/micro_step=4000/global_step=400, RunningAvgSamplesPerSec=1.5902313556467524, CurrSamplesPerSec=1.6539428634521485, MemAllocated=31.23GB, MaxMemAllocated=36.87GB
Epoch: [0][400/500]	Time  6.048 ( 6.048)	Loss 2.4602 (2.1786)	CeLoss 0.3594 (0.3765)	SegCLSLoss 0.0149 (0.0180)	KLLoss 0.2832 (0.2520)	MaskLoss 0.9762 (0.8335)	MaskBCELoss 0.2221 (0.1549)	MaskDICELoss 0.7541 (0.6786)
Epoch: [0][401/500]	Time  7.337 ( 7.337)	Loss 1.9072 (2.0195)	CeLoss 0.2441 (0.2629)	SegCLSLoss 0.0183 (0.0246)	KLLoss 0.2773 (0.2205)	MaskLoss 0.7573 (0.8170)	MaskBCELoss 0.1126 (0.1290)	MaskDICELoss 0.6447 (0.6879)
Epoch: [0][402/500]	Time  5.865 ( 5.865)	Loss 2.7030 (2.0088)	CeLoss 0.2119 (0.4404)	SegCLSLoss 0.0354 (0.0224)	KLLoss 0.2695 (0.2213)	MaskLoss 1.1698 (0.7234)	MaskBCELoss 0.3525 (0.0953)	MaskDICELoss 0.8173 (0.6282)
Epoch: [0][403/500]	Time  5.348 ( 5.348)	Loss 2.1337 (1.8705)	CeLoss 0.2324 (0.4628)	SegCLSLoss 0.0232 (0.0202)	KLLoss 0.2695 (0.1881)	MaskLoss 0.8774 (0.6515)	MaskBCELoss 0.0191 (0.1488)	MaskDICELoss 0.8583 (0.5027)
Epoch: [0][404/500]	Time  5.668 ( 5.668)	Loss 2.6213 (2.0475)	CeLoss 0.2168 (0.5381)	SegCLSLoss 0.0317 (0.0165)	KLLoss 0.2715 (0.1928)	MaskLoss 1.1270 (0.7022)	MaskBCELoss 0.2954 (0.1529)	MaskDICELoss 0.8316 (0.5493)
Epoch: [0][405/500]	Time  5.767 ( 5.767)	Loss 2.3702 (2.2241)	CeLoss 0.2930 (0.4903)	SegCLSLoss 0.0150 (0.0195)	KLLoss 0.2754 (0.2213)	MaskLoss 0.9664 (0.8067)	MaskBCELoss 0.1382 (0.1240)	MaskDICELoss 0.8282 (0.6827)
Epoch: [0][406/500]	Time  6.803 ( 6.803)	Loss 1.9232 (2.1079)	CeLoss 0.2852 (0.3914)	SegCLSLoss 0.0264 (0.0230)	KLLoss 0.2832 (0.2508)	MaskLoss 0.7409 (0.7897)	MaskBCELoss 0.1733 (0.1691)	MaskDICELoss 0.5676 (0.6206)
Epoch: [0][407/500]	Time  6.123 ( 6.123)	Loss 2.5576 (2.1857)	CeLoss 0.3281 (0.2372)	SegCLSLoss 0.0170 (0.0227)	KLLoss 0.2773 (0.2502)	MaskLoss 1.0415 (0.9061)	MaskBCELoss 0.3562 (0.2191)	MaskDICELoss 0.6853 (0.6870)
Epoch: [0][408/500]	Time  6.291 ( 6.291)	Loss 2.5845 (2.1270)	CeLoss 0.3398 (0.3927)	SegCLSLoss 0.0247 (0.0203)	KLLoss 0.2734 (0.2223)	MaskLoss 1.0481 (0.8067)	MaskBCELoss 0.0765 (0.2302)	MaskDICELoss 0.9716 (0.5764)
Epoch: [0][409/500]	Time  6.082 ( 6.082)	Loss 1.9765 (2.0982)	CeLoss 0.3281 (0.4889)	SegCLSLoss 0.0156 (0.0200)	KLLoss 0.2754 (0.2213)	MaskLoss 0.7519 (0.7444)	MaskBCELoss 0.2870 (0.1790)	MaskDICELoss 0.4649 (0.5654)
[2025-03-02 17:07:44,887] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=0, lr=[0.0002561020408163265], mom=[(0.9, 0.95)]
[2025-03-02 17:07:44,892] [INFO] [timer.py:215:stop] epoch=0/micro_step=4100/global_step=410, RunningAvgSamplesPerSec=1.591481440097583, CurrSamplesPerSec=1.7873793233525193, MemAllocated=30.7GB, MaxMemAllocated=36.87GB
Epoch: [0][410/500]	Time  5.596 ( 5.596)	Loss 1.3828 (1.7071)	CeLoss 1.3828 (0.6282)	SegCLSLoss 0.0000 (0.0144)	KLLoss 0.0000 (0.1350)	MaskLoss 0.0000 (0.5022)	MaskBCELoss 0.0000 (0.0706)	MaskDICELoss 0.0000 (0.4316)
Epoch: [0][411/500]	Time  6.236 ( 6.236)	Loss 2.2861 (2.0232)	CeLoss 0.2559 (0.3380)	SegCLSLoss 0.0347 (0.0258)	KLLoss 0.2832 (0.2193)	MaskLoss 0.9360 (0.7815)	MaskBCELoss 0.0614 (0.1422)	MaskDICELoss 0.8746 (0.6393)
Epoch: [0][412/500]	Time  5.856 ( 5.856)	Loss 2.6573 (1.8167)	CeLoss 0.4316 (0.3414)	SegCLSLoss 0.0204 (0.0173)	KLLoss 0.2812 (0.2277)	MaskLoss 1.0376 (0.6766)	MaskBCELoss 0.0503 (0.1654)	MaskDICELoss 0.9873 (0.5113)
Epoch: [0][413/500]	Time  5.857 ( 5.857)	Loss 2.9560 (1.5001)	CeLoss 0.3398 (0.5074)	SegCLSLoss 0.0199 (0.0105)	KLLoss 0.2793 (0.1408)	MaskLoss 1.2338 (0.4585)	MaskBCELoss 0.4937 (0.1768)	MaskDICELoss 0.7401 (0.2817)
Epoch: [0][414/500]	Time  6.022 ( 6.022)	Loss 1.6719 (2.3387)	CeLoss 1.6719 (0.5472)	SegCLSLoss 0.0000 (0.0281)	KLLoss 0.0000 (0.1932)	MaskLoss 0.0000 (0.8403)	MaskBCELoss 0.0000 (0.2486)	MaskDICELoss 0.0000 (0.5917)
Epoch: [0][415/500]	Time  6.552 ( 6.552)	Loss 1.6978 (2.1167)	CeLoss 0.2695 (0.3281)	SegCLSLoss 0.0168 (0.0205)	KLLoss 0.2930 (0.2555)	MaskLoss 0.6370 (0.8253)	MaskBCELoss 0.1034 (0.2172)	MaskDICELoss 0.5336 (0.6081)
Epoch: [0][416/500]	Time  5.705 ( 5.705)	Loss 2.2195 (2.0476)	CeLoss 0.2021 (0.4382)	SegCLSLoss 0.0410 (0.0222)	KLLoss 0.2852 (0.2256)	MaskLoss 0.9271 (0.7429)	MaskBCELoss 0.0739 (0.1265)	MaskDICELoss 0.8532 (0.6164)
Epoch: [0][417/500]	Time  6.005 ( 6.005)	Loss 2.2850 (2.0903)	CeLoss 0.2695 (0.4090)	SegCLSLoss 0.0159 (0.0204)	KLLoss 0.2832 (0.2238)	MaskLoss 0.9335 (0.7795)	MaskBCELoss 0.2346 (0.1256)	MaskDICELoss 0.6990 (0.6540)
Epoch: [0][418/500]	Time  5.686 ( 5.686)	Loss 1.5629 (2.3234)	CeLoss 0.2432 (0.4688)	SegCLSLoss 0.0227 (0.0236)	KLLoss 0.2871 (0.2289)	MaskLoss 0.5822 (0.8640)	MaskBCELoss 0.2606 (0.3376)	MaskDICELoss 0.3217 (0.5264)
Epoch: [0][419/500]	Time  6.778 ( 6.778)	Loss 0.8711 (2.1490)	CeLoss 0.8711 (0.5024)	SegCLSLoss 0.0000 (0.0255)	KLLoss 0.0000 (0.1959)	MaskLoss 0.0000 (0.7679)	MaskBCELoss 0.0000 (0.1490)	MaskDICELoss 0.0000 (0.6190)
[2025-03-02 17:08:46,402] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=0, lr=[0.00025487755102040815], mom=[(0.9, 0.95)]
[2025-03-02 17:08:46,408] [INFO] [timer.py:215:stop] epoch=0/micro_step=4200/global_step=420, RunningAvgSamplesPerSec=1.5922891021413275, CurrSamplesPerSec=1.4667301760728464, MemAllocated=31.1GB, MaxMemAllocated=36.87GB
Epoch: [0][420/500]	Time  6.819 ( 6.819)	Loss 2.3398 (1.6289)	CeLoss 0.2031 (0.3262)	SegCLSLoss 0.0166 (0.0127)	KLLoss 0.2852 (0.2000)	MaskLoss 0.9931 (0.5982)	MaskBCELoss 0.2139 (0.1064)	MaskDICELoss 0.7793 (0.4918)
Epoch: [0][421/500]	Time  6.195 ( 6.195)	Loss 2.0224 (2.1425)	CeLoss 0.2383 (0.2047)	SegCLSLoss 0.0162 (0.0307)	KLLoss 0.2988 (0.2537)	MaskLoss 0.8139 (0.8980)	MaskBCELoss 0.3689 (0.2134)	MaskDICELoss 0.4450 (0.6845)
Epoch: [0][422/500]	Time  6.742 ( 6.742)	Loss 2.3035 (1.9538)	CeLoss 0.2012 (0.4471)	SegCLSLoss 0.0250 (0.0217)	KLLoss 0.2734 (0.1957)	MaskLoss 0.9760 (0.6989)	MaskBCELoss 0.0367 (0.1131)	MaskDICELoss 0.9393 (0.5858)
Epoch: [0][423/500]	Time  6.723 ( 6.723)	Loss 0.6328 (1.8506)	CeLoss 0.6328 (0.4096)	SegCLSLoss 0.0000 (0.0169)	KLLoss 0.0000 (0.1979)	MaskLoss 0.0000 (0.6668)	MaskBCELoss 0.0000 (0.1413)	MaskDICELoss 0.0000 (0.5255)
Epoch: [0][424/500]	Time  6.590 ( 6.590)	Loss 2.5357 (1.9822)	CeLoss 0.2793 (0.2896)	SegCLSLoss 0.0161 (0.0292)	KLLoss 0.2871 (0.2236)	MaskLoss 1.0530 (0.7832)	MaskBCELoss 0.1841 (0.0978)	MaskDICELoss 0.8689 (0.6854)
Epoch: [0][425/500]	Time  7.246 ( 7.246)	Loss 1.8287 (2.0975)	CeLoss 0.2520 (0.2219)	SegCLSLoss 0.0176 (0.0227)	KLLoss 0.2891 (0.2523)	MaskLoss 0.7112 (0.8690)	MaskBCELoss 0.2146 (0.1789)	MaskDICELoss 0.4966 (0.6901)
Epoch: [0][426/500]	Time  5.489 ( 5.489)	Loss 2.3024 (2.1521)	CeLoss 0.3086 (0.4455)	SegCLSLoss 0.0225 (0.0193)	KLLoss 0.2793 (0.1986)	MaskLoss 0.9207 (0.7988)	MaskBCELoss 0.1488 (0.2693)	MaskDICELoss 0.7719 (0.5295)
Epoch: [0][427/500]	Time  4.921 ( 4.921)	Loss 2.7064 (1.6379)	CeLoss 0.2012 (0.6262)	SegCLSLoss 0.0422 (0.0128)	KLLoss 0.2793 (0.1402)	MaskLoss 1.1716 (0.4675)	MaskBCELoss 0.2573 (0.0938)	MaskDICELoss 0.9143 (0.3737)
Epoch: [0][428/500]	Time  6.269 ( 6.269)	Loss 2.9157 (2.1279)	CeLoss 0.3672 (0.4733)	SegCLSLoss 0.0161 (0.0258)	KLLoss 0.2832 (0.2223)	MaskLoss 1.2001 (0.7654)	MaskBCELoss 0.3768 (0.1444)	MaskDICELoss 0.8233 (0.6210)
Epoch: [0][429/500]	Time  6.227 ( 6.227)	Loss 3.2729 (2.0699)	CeLoss 0.2910 (0.4538)	SegCLSLoss 0.0258 (0.0206)	KLLoss 0.2734 (0.1949)	MaskLoss 1.4157 (0.7541)	MaskBCELoss 0.5920 (0.1665)	MaskDICELoss 0.8238 (0.5876)
[2025-03-02 17:09:49,788] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=0, lr=[0.0002536530612244898], mom=[(0.9, 0.95)]
[2025-03-02 17:09:49,794] [INFO] [timer.py:215:stop] epoch=0/micro_step=4300/global_step=430, RunningAvgSamplesPerSec=1.5919518688613024, CurrSamplesPerSec=1.4323085374666156, MemAllocated=31.25GB, MaxMemAllocated=36.87GB
Epoch: [0][430/500]	Time  6.983 ( 6.983)	Loss 2.3714 (2.4675)	CeLoss 0.1748 (0.2252)	SegCLSLoss 0.0302 (0.0276)	KLLoss 0.2734 (0.2754)	MaskLoss 1.0226 (1.0453)	MaskBCELoss 0.0400 (0.2180)	MaskDICELoss 0.9826 (0.8273)
Epoch: [0][431/500]	Time  6.204 ( 6.204)	Loss 2.1794 (2.2892)	CeLoss 0.2393 (0.2857)	SegCLSLoss 0.0164 (0.0276)	KLLoss 0.2812 (0.2520)	MaskLoss 0.8964 (0.9320)	MaskBCELoss 0.2033 (0.2759)	MaskDICELoss 0.6931 (0.6561)
Epoch: [0][432/500]	Time  7.421 ( 7.421)	Loss 2.2803 (1.7375)	CeLoss 0.2383 (0.3223)	SegCLSLoss 0.0287 (0.0170)	KLLoss 0.2734 (0.1953)	MaskLoss 0.9458 (0.6546)	MaskBCELoss 0.0749 (0.0906)	MaskDICELoss 0.8709 (0.5641)
Epoch: [0][433/500]	Time  5.682 ( 5.682)	Loss 1.9604 (2.0199)	CeLoss 0.2266 (0.3986)	SegCLSLoss 0.0483 (0.0338)	KLLoss 0.2773 (0.2236)	MaskLoss 0.7858 (0.7464)	MaskBCELoss 0.1873 (0.1657)	MaskDICELoss 0.5986 (0.5807)
Epoch: [0][434/500]	Time  6.013 ( 6.013)	Loss 2.6799 (2.2852)	CeLoss 0.2734 (0.2104)	SegCLSLoss 0.0322 (0.0314)	KLLoss 0.2773 (0.2791)	MaskLoss 1.1261 (0.9600)	MaskBCELoss 0.1497 (0.1844)	MaskDICELoss 0.9764 (0.7756)
Epoch: [0][435/500]	Time  5.323 ( 5.323)	Loss 1.2109 (1.8526)	CeLoss 1.2109 (0.5556)	SegCLSLoss 0.0000 (0.0194)	KLLoss 0.0000 (0.1635)	MaskLoss 0.0000 (0.6029)	MaskBCELoss 0.0000 (0.1244)	MaskDICELoss 0.0000 (0.4785)
Epoch: [0][436/500]	Time  5.815 ( 5.815)	Loss 2.5871 (1.5298)	CeLoss 0.4492 (0.5370)	SegCLSLoss 0.0211 (0.0141)	KLLoss 0.2695 (0.1404)	MaskLoss 0.9967 (0.4578)	MaskBCELoss 0.0262 (0.0701)	MaskDICELoss 0.9705 (0.3877)
Epoch: [0][437/500]	Time  6.977 ( 6.977)	Loss 2.1797 (2.0842)	CeLoss 0.1934 (0.2091)	SegCLSLoss 0.0239 (0.0283)	KLLoss 0.2754 (0.2783)	MaskLoss 0.9180 (0.8609)	MaskBCELoss 0.0133 (0.1364)	MaskDICELoss 0.9047 (0.7245)
Epoch: [0][438/500]	Time  6.085 ( 6.085)	Loss 2.4561 (1.8691)	CeLoss 0.2715 (0.4278)	SegCLSLoss 0.0253 (0.0159)	KLLoss 0.2754 (0.1957)	MaskLoss 1.0171 (0.6678)	MaskBCELoss 0.1036 (0.0923)	MaskDICELoss 0.9135 (0.5755)
Epoch: [0][439/500]	Time  7.381 ( 7.381)	Loss 3.0457 (2.0214)	CeLoss 0.1187 (0.2917)	SegCLSLoss 0.0801 (0.0264)	KLLoss 0.2734 (0.2234)	MaskLoss 1.3754 (0.8025)	MaskBCELoss 0.4799 (0.1763)	MaskDICELoss 0.8955 (0.6263)
[2025-03-02 17:10:52,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=0, lr=[0.0002524285714285714], mom=[(0.9, 0.95)]
[2025-03-02 17:10:52,387] [INFO] [timer.py:215:stop] epoch=0/micro_step=4400/global_step=440, RunningAvgSamplesPerSec=1.592088142909204, CurrSamplesPerSec=1.757363411873628, MemAllocated=30.81GB, MaxMemAllocated=36.87GB
Epoch: [0][440/500]	Time  5.692 ( 5.692)	Loss 1.0391 (1.6422)	CeLoss 1.0391 (0.6050)	SegCLSLoss 0.0000 (0.0113)	KLLoss 0.0000 (0.1400)	MaskLoss 0.0000 (0.4809)	MaskBCELoss 0.0000 (0.1046)	MaskDICELoss 0.0000 (0.3763)
Epoch: [0][441/500]	Time  6.375 ( 6.375)	Loss 2.8545 (1.9965)	CeLoss 0.3125 (0.2840)	SegCLSLoss 0.0332 (0.0232)	KLLoss 0.2715 (0.2209)	MaskLoss 1.1948 (0.7951)	MaskBCELoss 0.3337 (0.1701)	MaskDICELoss 0.8612 (0.6249)
Epoch: [0][442/500]	Time  5.411 ( 5.411)	Loss 1.0781 (1.6590)	CeLoss 1.0781 (0.4339)	SegCLSLoss 0.0000 (0.0210)	KLLoss 0.0000 (0.1656)	MaskLoss 0.0000 (0.5659)	MaskBCELoss 0.0000 (0.0935)	MaskDICELoss 0.0000 (0.4724)
Epoch: [0][443/500]	Time  6.011 ( 6.011)	Loss 2.8391 (2.0556)	CeLoss 0.2471 (0.4445)	SegCLSLoss 0.0132 (0.0163)	KLLoss 0.2754 (0.2219)	MaskLoss 1.2242 (0.7461)	MaskBCELoss 0.4107 (0.1395)	MaskDICELoss 0.8135 (0.6066)
Epoch: [0][444/500]	Time  5.563 ( 5.563)	Loss 1.1797 (2.2807)	CeLoss 1.1797 (0.4253)	SegCLSLoss 0.0000 (0.0171)	KLLoss 0.0000 (0.2229)	MaskLoss 0.0000 (0.8677)	MaskBCELoss 0.0000 (0.2823)	MaskDICELoss 0.0000 (0.5854)
Epoch: [0][445/500]	Time  6.880 ( 6.880)	Loss 2.5412 (2.5922)	CeLoss 0.2021 (0.2521)	SegCLSLoss 0.0280 (0.0267)	KLLoss 0.2695 (0.2717)	MaskLoss 1.0948 (1.0953)	MaskBCELoss 0.1571 (0.2998)	MaskDICELoss 0.9377 (0.7955)
Epoch: [0][446/500]	Time  5.751 ( 5.751)	Loss 2.8841 (1.8659)	CeLoss 0.2754 (0.4774)	SegCLSLoss 0.0339 (0.0159)	KLLoss 0.2676 (0.1918)	MaskLoss 1.2292 (0.6424)	MaskBCELoss 0.3031 (0.1229)	MaskDICELoss 0.9261 (0.5195)
Epoch: [0][447/500]	Time  5.834 ( 5.834)	Loss 1.8342 (1.9796)	CeLoss 0.2139 (0.3577)	SegCLSLoss 0.0325 (0.0239)	KLLoss 0.2734 (0.2195)	MaskLoss 0.7335 (0.7502)	MaskBCELoss 0.0699 (0.0715)	MaskDICELoss 0.6636 (0.6787)
Epoch: [0][448/500]	Time  5.936 ( 5.936)	Loss 2.2540 (2.1144)	CeLoss 0.3066 (0.4690)	SegCLSLoss 0.0240 (0.0179)	KLLoss 0.2773 (0.1918)	MaskLoss 0.8985 (0.7703)	MaskBCELoss 0.2072 (0.1552)	MaskDICELoss 0.6913 (0.6150)
Epoch: [0][449/500]	Time  6.626 ( 6.626)	Loss 2.6498 (2.1522)	CeLoss 0.2051 (0.3370)	SegCLSLoss 0.0240 (0.0209)	KLLoss 0.2695 (0.2186)	MaskLoss 1.1491 (0.8476)	MaskBCELoss 0.1756 (0.1583)	MaskDICELoss 0.9736 (0.6893)
[2025-03-02 17:11:53,789] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=0, lr=[0.00025120408163265305], mom=[(0.9, 0.95)]
[2025-03-02 17:11:53,794] [INFO] [timer.py:215:stop] epoch=0/micro_step=4500/global_step=450, RunningAvgSamplesPerSec=1.592889760513594, CurrSamplesPerSec=1.4246991364924533, MemAllocated=31.25GB, MaxMemAllocated=36.87GB
Epoch: [0][450/500]	Time  7.021 ( 7.021)	Loss 2.4894 (1.7965)	CeLoss 0.3809 (0.4446)	SegCLSLoss 0.0267 (0.0161)	KLLoss 0.2773 (0.1961)	MaskLoss 0.9791 (0.6229)	MaskBCELoss 0.3024 (0.0770)	MaskDICELoss 0.6767 (0.5459)
Epoch: [0][451/500]	Time  6.057 ( 6.057)	Loss 3.0070 (2.3221)	CeLoss 0.1855 (0.4031)	SegCLSLoss 0.0320 (0.0190)	KLLoss 0.2695 (0.2494)	MaskLoss 1.3355 (0.8922)	MaskBCELoss 0.4835 (0.1736)	MaskDICELoss 0.8521 (0.7186)
Epoch: [0][452/500]	Time  5.729 ( 5.729)	Loss 2.2191 (1.6398)	CeLoss 0.2891 (0.4781)	SegCLSLoss 0.0175 (0.0122)	KLLoss 0.2852 (0.1664)	MaskLoss 0.8898 (0.5364)	MaskBCELoss 0.3798 (0.0945)	MaskDICELoss 0.5100 (0.4419)
Epoch: [0][453/500]	Time  6.035 ( 6.035)	Loss 2.2949 (2.0538)	CeLoss 0.1377 (0.4492)	SegCLSLoss 0.0598 (0.0225)	KLLoss 0.2773 (0.2227)	MaskLoss 0.9942 (0.7413)	MaskBCELoss 0.3158 (0.1640)	MaskDICELoss 0.6784 (0.5773)
Epoch: [0][454/500]	Time  5.654 ( 5.654)	Loss 2.5677 (1.9744)	CeLoss 0.1079 (0.3570)	SegCLSLoss 0.0845 (0.0275)	KLLoss 0.2695 (0.2215)	MaskLoss 1.1413 (0.7464)	MaskBCELoss 0.2120 (0.1928)	MaskDICELoss 0.9293 (0.5536)
Epoch: [0][455/500]	Time  6.036 ( 6.036)	Loss 3.7689 (2.0194)	CeLoss 0.3105 (0.4140)	SegCLSLoss 0.0187 (0.0213)	KLLoss 0.2871 (0.1953)	MaskLoss 1.6520 (0.7486)	MaskBCELoss 0.7880 (0.2170)	MaskDICELoss 0.8641 (0.5315)
Epoch: [0][456/500]	Time  5.878 ( 5.878)	Loss 1.2266 (2.0162)	CeLoss 1.2266 (0.5183)	SegCLSLoss 0.0000 (0.0170)	KLLoss 0.0000 (0.1979)	MaskLoss 0.0000 (0.6956)	MaskBCELoss 0.0000 (0.1944)	MaskDICELoss 0.0000 (0.5012)
Epoch: [0][457/500]	Time  5.482 ( 5.482)	Loss 0.3457 (1.8227)	CeLoss 0.3457 (0.4835)	SegCLSLoss 0.0000 (0.0244)	KLLoss 0.0000 (0.1674)	MaskLoss 0.0000 (0.6216)	MaskBCELoss 0.0000 (0.1035)	MaskDICELoss 0.0000 (0.5181)
Epoch: [0][458/500]	Time  4.987 ( 4.987)	Loss 2.5962 (1.8438)	CeLoss 0.2324 (0.7306)	SegCLSLoss 0.0160 (0.0090)	KLLoss 0.2969 (0.1453)	MaskLoss 1.1038 (0.5182)	MaskBCELoss 0.3849 (0.1400)	MaskDICELoss 0.7188 (0.3782)
Epoch: [0][459/500]	Time  6.357 ( 6.357)	Loss 2.4223 (1.9921)	CeLoss 0.2197 (0.3753)	SegCLSLoss 0.0183 (0.0141)	KLLoss 0.2969 (0.2023)	MaskLoss 1.0227 (0.7543)	MaskBCELoss 0.0911 (0.2328)	MaskDICELoss 0.9316 (0.5215)
[2025-03-02 17:12:51,592] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=0, lr=[0.0002499795918367347], mom=[(0.9, 0.95)]
[2025-03-02 17:12:51,597] [INFO] [timer.py:215:stop] epoch=0/micro_step=4600/global_step=460, RunningAvgSamplesPerSec=1.5956600040033457, CurrSamplesPerSec=1.7903366391405642, MemAllocated=30.72GB, MaxMemAllocated=36.87GB
Epoch: [0][460/500]	Time  5.588 ( 5.588)	Loss 1.5547 (2.0962)	CeLoss 1.5547 (0.5368)	SegCLSLoss 0.0000 (0.0223)	KLLoss 0.0000 (0.1980)	MaskLoss 0.0000 (0.7248)	MaskBCELoss 0.0000 (0.0778)	MaskDICELoss 0.0000 (0.6470)
Epoch: [0][461/500]	Time  7.131 ( 7.131)	Loss 2.4252 (2.2248)	CeLoss 0.2637 (0.2486)	SegCLSLoss 0.0259 (0.0215)	KLLoss 0.2773 (0.2568)	MaskLoss 1.0046 (0.9185)	MaskBCELoss 0.0598 (0.2270)	MaskDICELoss 0.9448 (0.6915)
Epoch: [0][462/500]	Time  6.031 ( 6.031)	Loss 1.1875 (1.9260)	CeLoss 1.1875 (0.4253)	SegCLSLoss 0.0000 (0.0230)	KLLoss 0.0000 (0.1959)	MaskLoss 0.0000 (0.6956)	MaskBCELoss 0.0000 (0.2039)	MaskDICELoss 0.0000 (0.4916)
Epoch: [0][463/500]	Time  5.749 ( 5.749)	Loss 1.0625 (1.8511)	CeLoss 1.0625 (0.5262)	SegCLSLoss 0.0000 (0.0168)	KLLoss 0.0000 (0.2008)	MaskLoss 0.0000 (0.6081)	MaskBCELoss 0.0000 (0.0753)	MaskDICELoss 0.0000 (0.5327)
Epoch: [0][464/500]	Time  6.530 ( 6.530)	Loss 3.0148 (1.9337)	CeLoss 0.2793 (0.4075)	SegCLSLoss 0.0310 (0.0244)	KLLoss 0.2734 (0.2004)	MaskLoss 1.2916 (0.7069)	MaskBCELoss 0.4055 (0.1786)	MaskDICELoss 0.8861 (0.5283)
Epoch: [0][465/500]	Time  4.454 ( 4.454)	Loss 1.2422 (1.7211)	CeLoss 1.2422 (0.6587)	SegCLSLoss 0.0000 (0.0186)	KLLoss 0.0000 (0.1422)	MaskLoss 0.0000 (0.4911)	MaskBCELoss 0.0000 (0.1455)	MaskDICELoss 0.0000 (0.3457)
Epoch: [0][466/500]	Time  5.786 ( 5.786)	Loss 1.7656 (1.9977)	CeLoss 1.7656 (0.5931)	SegCLSLoss 0.0000 (0.0168)	KLLoss 0.0000 (0.1691)	MaskLoss 0.0000 (0.6558)	MaskBCELoss 0.0000 (0.1202)	MaskDICELoss 0.0000 (0.5356)
Epoch: [0][467/500]	Time  5.203 ( 5.203)	Loss 0.7240 (2.0739)	CeLoss 0.1709 (0.5229)	SegCLSLoss 0.0173 (0.0206)	KLLoss 0.3066 (0.2008)	MaskLoss 0.1960 (0.7202)	MaskBCELoss 0.0856 (0.2109)	MaskDICELoss 0.1104 (0.5093)
Epoch: [0][468/500]	Time  7.552 ( 7.552)	Loss 2.2053 (2.1456)	CeLoss 0.2041 (0.2110)	SegCLSLoss 0.0275 (0.0293)	KLLoss 0.2812 (0.2545)	MaskLoss 0.9230 (0.8962)	MaskBCELoss 0.0157 (0.1757)	MaskDICELoss 0.9073 (0.7206)
Epoch: [0][469/500]	Time  5.508 ( 5.508)	Loss 2.3200 (1.9158)	CeLoss 0.2754 (0.3411)	SegCLSLoss 0.0162 (0.0262)	KLLoss 0.2949 (0.1955)	MaskLoss 0.9452 (0.7319)	MaskBCELoss 0.1422 (0.1507)	MaskDICELoss 0.8030 (0.5813)
[2025-03-02 17:13:50,222] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=0, lr=[0.0002487551020408163], mom=[(0.9, 0.95)]
[2025-03-02 17:13:50,228] [INFO] [timer.py:215:stop] epoch=0/micro_step=4700/global_step=470, RunningAvgSamplesPerSec=1.5978676881157639, CurrSamplesPerSec=2.134137238585865, MemAllocated=30.66GB, MaxMemAllocated=36.87GB
Epoch: [0][470/500]	Time  4.687 ( 4.687)	Loss 2.7515 (2.1651)	CeLoss 0.0962 (0.4348)	SegCLSLoss 0.0767 (0.0300)	KLLoss 0.2734 (0.2258)	MaskLoss 1.2400 (0.8011)	MaskBCELoss 0.2808 (0.2305)	MaskDICELoss 0.9592 (0.5706)
Epoch: [0][471/500]	Time  5.696 ( 5.696)	Loss 2.4481 (1.9145)	CeLoss 0.2061 (0.5252)	SegCLSLoss 0.0286 (0.0222)	KLLoss 0.2773 (0.1684)	MaskLoss 1.0443 (0.6469)	MaskBCELoss 0.0535 (0.2239)	MaskDICELoss 0.9908 (0.4230)
Epoch: [0][472/500]	Time  6.782 ( 6.782)	Loss 2.7503 (2.1638)	CeLoss 0.2061 (0.5473)	SegCLSLoss 0.0276 (0.0240)	KLLoss 0.2676 (0.1930)	MaskLoss 1.1984 (0.7542)	MaskBCELoss 0.2814 (0.1721)	MaskDICELoss 0.9170 (0.5821)
Epoch: [0][473/500]	Time  6.787 ( 6.787)	Loss 3.3933 (2.3107)	CeLoss 0.2480 (0.3627)	SegCLSLoss 0.0286 (0.0235)	KLLoss 0.2656 (0.2529)	MaskLoss 1.4994 (0.9051)	MaskBCELoss 0.6340 (0.1830)	MaskDICELoss 0.8654 (0.7220)
Epoch: [0][474/500]	Time  4.814 ( 4.814)	Loss 0.8672 (1.7292)	CeLoss 0.8672 (0.7177)	SegCLSLoss 0.0000 (0.0180)	KLLoss 0.0000 (0.1416)	MaskLoss 0.0000 (0.4658)	MaskBCELoss 0.0000 (0.1088)	MaskDICELoss 0.0000 (0.3570)
Epoch: [0][475/500]	Time  6.596 ( 6.596)	Loss 1.6170 (2.3306)	CeLoss 0.1963 (0.2804)	SegCLSLoss 0.0145 (0.0210)	KLLoss 0.3066 (0.2861)	MaskLoss 0.6308 (0.9484)	MaskBCELoss 0.2148 (0.1672)	MaskDICELoss 0.4160 (0.7812)
Epoch: [0][476/500]	Time  6.908 ( 6.908)	Loss 0.2422 (1.9266)	CeLoss 0.2422 (0.2931)	SegCLSLoss 0.0000 (0.0185)	KLLoss 0.0000 (0.2256)	MaskLoss 0.0000 (0.7557)	MaskBCELoss 0.0000 (0.0880)	MaskDICELoss 0.0000 (0.6677)
Epoch: [0][477/500]	Time  5.751 ( 5.751)	Loss 0.5625 (1.7356)	CeLoss 0.5625 (0.4894)	SegCLSLoss 0.0000 (0.0175)	KLLoss 0.0000 (0.1709)	MaskLoss 0.0000 (0.5758)	MaskBCELoss 0.0000 (0.1650)	MaskDICELoss 0.0000 (0.4108)
Epoch: [0][478/500]	Time  6.193 ( 6.193)	Loss 1.0703 (1.6378)	CeLoss 1.0703 (0.4534)	SegCLSLoss 0.0000 (0.0157)	KLLoss 0.0000 (0.1725)	MaskLoss 0.0000 (0.5451)	MaskBCELoss 0.0000 (0.1156)	MaskDICELoss 0.0000 (0.4296)
Epoch: [0][479/500]	Time  7.270 ( 7.270)	Loss 2.3139 (2.1639)	CeLoss 0.2871 (0.2692)	SegCLSLoss 0.0176 (0.0271)	KLLoss 0.2910 (0.2572)	MaskLoss 0.9362 (0.8761)	MaskBCELoss 0.1981 (0.1759)	MaskDICELoss 0.7382 (0.7001)
[2025-03-02 17:14:52,460] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=0, lr=[0.00024753061224489794], mom=[(0.9, 0.95)]
[2025-03-02 17:14:52,466] [INFO] [timer.py:215:stop] epoch=0/micro_step=4800/global_step=480, RunningAvgSamplesPerSec=1.5980599075242299, CurrSamplesPerSec=1.8385365538266645, MemAllocated=30.81GB, MaxMemAllocated=36.87GB
Epoch: [0][480/500]	Time  5.441 ( 5.441)	Loss 2.5882 (2.2730)	CeLoss 0.1357 (0.4190)	SegCLSLoss 0.0703 (0.0257)	KLLoss 0.2754 (0.2266)	MaskLoss 1.1398 (0.8638)	MaskBCELoss 0.1877 (0.2386)	MaskDICELoss 0.9521 (0.6252)
Epoch: [0][481/500]	Time  5.045 ( 5.045)	Loss 2.6277 (2.1464)	CeLoss 0.1895 (0.4122)	SegCLSLoss 0.0374 (0.0234)	KLLoss 0.2852 (0.2275)	MaskLoss 1.1381 (0.8044)	MaskBCELoss 0.3906 (0.1615)	MaskDICELoss 0.7475 (0.6430)
Epoch: [0][482/500]	Time  5.725 ( 5.725)	Loss 1.2734 (1.9036)	CeLoss 1.2734 (0.5505)	SegCLSLoss 0.0000 (0.0183)	KLLoss 0.0000 (0.2043)	MaskLoss 0.0000 (0.6208)	MaskBCELoss 0.0000 (0.1852)	MaskDICELoss 0.0000 (0.4356)
Epoch: [0][483/500]	Time  6.850 ( 6.850)	Loss 2.3733 (1.8394)	CeLoss 0.1787 (0.3679)	SegCLSLoss 0.0309 (0.0198)	KLLoss 0.2734 (0.1986)	MaskLoss 1.0206 (0.6811)	MaskBCELoss 0.2438 (0.1542)	MaskDICELoss 0.7768 (0.5269)
Epoch: [0][484/500]	Time  5.539 ( 5.539)	Loss 1.9475 (1.7216)	CeLoss 0.2793 (0.5395)	SegCLSLoss 0.0166 (0.0133)	KLLoss 0.2930 (0.2021)	MaskLoss 0.7570 (0.5373)	MaskBCELoss 0.2359 (0.1065)	MaskDICELoss 0.5210 (0.4307)
Epoch: [0][485/500]	Time  5.576 ( 5.576)	Loss 2.4504 (1.7156)	CeLoss 0.1582 (0.6111)	SegCLSLoss 0.0425 (0.0130)	KLLoss 0.2773 (0.1400)	MaskLoss 1.0660 (0.5139)	MaskBCELoss 0.0694 (0.0736)	MaskDICELoss 0.9966 (0.4403)
Epoch: [0][486/500]	Time  6.008 ( 6.008)	Loss 2.0797 (2.2574)	CeLoss 0.1660 (0.4386)	SegCLSLoss 0.0403 (0.0302)	KLLoss 0.2812 (0.2197)	MaskLoss 0.8758 (0.8469)	MaskBCELoss 0.0330 (0.1445)	MaskDICELoss 0.8427 (0.7023)
Epoch: [0][487/500]	Time  6.285 ( 6.285)	Loss 1.2891 (2.4203)	CeLoss 1.2891 (0.3354)	SegCLSLoss 0.0000 (0.0265)	KLLoss 0.0000 (0.2463)	MaskLoss 0.0000 (0.9743)	MaskBCELoss 0.0000 (0.1857)	MaskDICELoss 0.0000 (0.7887)
Epoch: [0][488/500]	Time  6.370 ( 6.370)	Loss 2.4201 (2.0611)	CeLoss 0.2334 (0.3330)	SegCLSLoss 0.0232 (0.0204)	KLLoss 0.2832 (0.2566)	MaskLoss 1.0167 (0.7948)	MaskBCELoss 0.0271 (0.1567)	MaskDICELoss 0.9895 (0.6381)
Epoch: [0][489/500]	Time  6.198 ( 6.198)	Loss 2.3075 (1.8349)	CeLoss 0.2139 (0.3167)	SegCLSLoss 0.0308 (0.0223)	KLLoss 0.2695 (0.2266)	MaskLoss 0.9721 (0.6970)	MaskBCELoss 0.0570 (0.1230)	MaskDICELoss 0.9151 (0.5740)
[2025-03-02 17:15:52,663] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=0, lr=[0.00024630612244897957], mom=[(0.9, 0.95)]
[2025-03-02 17:15:52,669] [INFO] [timer.py:215:stop] epoch=0/micro_step=4900/global_step=490, RunningAvgSamplesPerSec=1.5993092555430883, CurrSamplesPerSec=1.5137506191798225, MemAllocated=31.25GB, MaxMemAllocated=36.87GB
Epoch: [0][490/500]	Time  6.608 ( 6.608)	Loss 2.5119 (2.0195)	CeLoss 0.2441 (0.3212)	SegCLSLoss 0.0242 (0.0235)	KLLoss 0.2852 (0.2592)	MaskLoss 1.0567 (0.7786)	MaskBCELoss 0.2459 (0.1790)	MaskDICELoss 0.8108 (0.5996)
Epoch: [0][491/500]	Time  6.406 ( 6.406)	Loss 2.0884 (2.3175)	CeLoss 0.2100 (0.3813)	SegCLSLoss 0.0474 (0.0282)	KLLoss 0.2773 (0.2543)	MaskLoss 0.8587 (0.8973)	MaskBCELoss 0.2692 (0.2610)	MaskDICELoss 0.5894 (0.6363)
Epoch: [0][492/500]	Time  6.103 ( 6.103)	Loss 2.0295 (1.8585)	CeLoss 0.3145 (0.4790)	SegCLSLoss 0.0154 (0.0179)	KLLoss 0.2871 (0.1977)	MaskLoss 0.7823 (0.6361)	MaskBCELoss 0.1424 (0.1005)	MaskDICELoss 0.6399 (0.5356)
Epoch: [0][493/500]	Time  6.559 ( 6.559)	Loss 2.0264 (2.0317)	CeLoss 0.2637 (0.4241)	SegCLSLoss 0.0271 (0.0280)	KLLoss 0.2773 (0.2223)	MaskLoss 0.8052 (0.7412)	MaskBCELoss 0.0143 (0.1307)	MaskDICELoss 0.7909 (0.6106)
Epoch: [0][494/500]	Time  6.409 ( 6.409)	Loss 2.3903 (1.9475)	CeLoss 0.2051 (0.6596)	SegCLSLoss 0.0342 (0.0164)	KLLoss 0.2734 (0.1691)	MaskLoss 1.0155 (0.5976)	MaskBCELoss 0.0499 (0.1106)	MaskDICELoss 0.9656 (0.4870)
Epoch: [0][495/500]	Time  6.507 ( 6.507)	Loss 1.5753 (2.1134)	CeLoss 0.2676 (0.3270)	SegCLSLoss 0.0165 (0.0214)	KLLoss 0.2852 (0.2537)	MaskLoss 0.5786 (0.8242)	MaskBCELoss 0.0431 (0.1412)	MaskDICELoss 0.5355 (0.6830)
Epoch: [0][496/500]	Time  6.419 ( 6.419)	Loss 2.9213 (1.9742)	CeLoss 0.3613 (0.2970)	SegCLSLoss 0.0339 (0.0223)	KLLoss 0.2715 (0.2225)	MaskLoss 1.2028 (0.7774)	MaskBCELoss 0.3241 (0.1844)	MaskDICELoss 0.8787 (0.5930)
Epoch: [0][497/500]	Time  5.910 ( 5.910)	Loss 1.3044 (1.6287)	CeLoss 0.2266 (0.4336)	SegCLSLoss 0.0149 (0.0195)	KLLoss 0.2852 (0.1674)	MaskLoss 0.4637 (0.5508)	MaskBCELoss 0.0732 (0.1423)	MaskDICELoss 0.3905 (0.4085)
Epoch: [0][498/500]	Time  4.848 ( 4.848)	Loss 2.3888 (1.7365)	CeLoss 0.2285 (0.7015)	SegCLSLoss 0.0231 (0.0149)	KLLoss 0.2773 (0.1371)	MaskLoss 1.0049 (0.4795)	MaskBCELoss 0.0368 (0.0604)	MaskDICELoss 0.9681 (0.4190)
Epoch: [0][499/500]	Time  5.961 ( 5.961)	Loss 2.5987 (2.4280)	CeLoss 0.3457 (0.3660)	SegCLSLoss 0.0156 (0.0240)	KLLoss 0.2832 (0.2492)	MaskLoss 1.0513 (0.9625)	MaskBCELoss 0.2538 (0.1804)	MaskDICELoss 0.7975 (0.7821)
[2025-03-02 17:16:54,258] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=0, lr=[0.0002450816326530612], mom=[(0.9, 0.95)]
[2025-03-02 17:16:54,264] [INFO] [timer.py:215:stop] epoch=0/micro_step=5000/global_step=500, RunningAvgSamplesPerSec=1.5997945810600525, CurrSamplesPerSec=1.545062815659996, MemAllocated=31.52GB, MaxMemAllocated=36.87GB
Epoch: [0][500/500]	Time  6.474 ( 6.474)	Loss 0.5859 (1.6013)	CeLoss 0.5859 (0.5693)	SegCLSLoss 0.0000 (0.0123)	KLLoss 0.0000 (0.1680)	MaskLoss 0.0000 (0.4708)	MaskBCELoss 0.0000 (0.0787)	MaskDICELoss 0.0000 (0.3922)
  0%|                                                                                                                                                              | 0/200 [00:00<?, ?it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 656, in <module>
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 409, in main
[rank0]:     )
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 596, in validate
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/tqdm/std.py", line 1182, in __iter__
[rank0]:     for obj in iterable:
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: AssertionError: Caught AssertionError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 109, in collate_fn
[rank0]:     assert len(parts) == 2, (len(parts), rou)
[rank0]: AssertionError: (1, "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_start><image><im_end>\n What is something showing that the man is playing sports in this image? Please output segmentation mask. ")