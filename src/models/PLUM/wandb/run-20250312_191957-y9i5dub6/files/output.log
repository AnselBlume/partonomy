You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565



Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [02:00<00:00, 40.18s/it]
trainable params: 6,553,600 || all params: 14,025,701,683 || trainable%: 0.0467256480147682
n:  base_model.model.model.embed_tokens.weight p.shape:  torch.Size([32002, 5120])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_token.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.mask_decoder.mask_tokens.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.weight p.shape:  torch.Size([256, 64, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.weight p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.weight p.shape:  torch.Size([64, 32, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.bias p.shape:  torch.Size([4])
n:  base_model.model.model.text_hidden_fcs.0.0.weight p.shape:  torch.Size([5120, 5120])
n:  base_model.model.model.text_hidden_fcs.0.0.bias p.shape:  torch.Size([5120])
n:  base_model.model.model.text_hidden_fcs.0.2.weight p.shape:  torch.Size([256, 5120])
n:  base_model.model.model.text_hidden_fcs.0.2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.token_to_mask_fcs.0.0.weight p.shape:  torch.Size([5120, 5120])
n:  base_model.model.model.token_to_mask_fcs.0.0.bias p.shape:  torch.Size([5120])
n:  base_model.model.model.token_to_mask_fcs.0.2.weight p.shape:  torch.Size([3, 5120])
n:  base_model.model.model.token_to_mask_fcs.0.2.bias p.shape:  torch.Size([3])
n:  base_model.model.lm_head.weight p.shape:  torch.Size([32002, 5120])
>> (PLUM.py) Initializing teacher LLM...
>> (PLUM.py) Teacher LLM initialized.
ade20k:  20210
cocostuff:  118287
loading annotations into memory...
Done (t=0.62s)
creating index...
index created!
pascal_part:  4366
loading annotations into memory...
Done (t=10.68s)
creating index...
index created!
paco_lvis:  45790
mapillary:  18000
loading dataset refclef into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refclef/refs(unc).p
creating index...
index created.
DONE (t=4.10s)
dataset refclef (refs unc) (train split) has 17978 images and 99523 annotations.
loading dataset refcoco into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco/refs(unc).p
creating index...
index created.
DONE (t=6.01s)
dataset refcoco (refs unc) (train split) has 16994 images and 196771 annotations.
loading dataset refcoco+ into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco+/refs(unc).p
creating index...
index created.
DONE (t=5.84s)
dataset refcoco+ (refs unc) (train split) has 16992 images and 196737 annotations.
loading dataset refcocog into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcocog/refs(umd).p
creating index...
index created.
DONE (t=6.71s)
dataset refcocog (refs umd) (train split) has 21899 images and 208960 annotations.
vqa_data:  157712
number of reason_seg samples:  239
len(self.img_to_explanation):  239
Training with 20000 examples and validating with 200 examples.
[2025-03-12 19:24:01,007] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5, git-hash=unknown, git-branch=unknown
[2025-03-12 19:24:01,007] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2025-03-12 19:24:01,007] [INFO] [comm.py:594:init_distributed] cdb=None
[2025-03-12 19:24:01,007] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-03-12 19:25:20,539] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Time to load fused_adam op: 0.1152029037475586 seconds
[2025-03-12 19:25:20,847] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-03-12 19:25:21,575] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-03-12 19:25:21,575] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-03-12 19:25:21,575] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-03-12 19:25:21,575] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500000000
[2025-03-12 19:25:21,575] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500000000
[2025-03-12 19:25:21,576] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2025-03-12 19:25:21,576] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module fused_adam...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/tensor/python_tensor.cpp:78.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
Rank: 0 partition count [1] and sizes[(392077800, False)]
[2025-03-12 19:25:26,850] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2025-03-12 19:25:26,851] [INFO] [utils.py:786:see_memory_usage] MA 53.81 GB         Max_MA 54.54 GB         CA 54.75 GB         Max_CA 55 GB
[2025-03-12 19:25:26,852] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 406.9 GB, percent = 40.4%
[2025-03-12 19:25:29,630] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2025-03-12 19:25:29,631] [INFO] [utils.py:786:see_memory_usage] MA 56.73 GB         Max_MA 58.19 GB         CA 59.13 GB         Max_CA 59 GB
[2025-03-12 19:25:29,631] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 407.04 GB, percent = 40.4%
[2025-03-12 19:25:29,631] [INFO] [stage_1_and_2.py:488:__init__] optimizer state initialized
[2025-03-12 19:25:32,403] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2025-03-12 19:25:32,404] [INFO] [utils.py:786:see_memory_usage] MA 56.73 GB         Max_MA 56.73 GB         CA 59.13 GB         Max_CA 59 GB
[2025-03-12 19:25:32,404] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 407.36 GB, percent = 40.4%
[2025-03-12 19:25:32,410] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-03-12 19:25:32,410] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2025-03-12 19:25:32,410] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7efe2508e3b0>
[2025-03-12 19:25:32,410] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[2e-05], mom=[(0.9, 0.95)]
[2025-03-12 19:25:32,417] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2025-03-12 19:25:32,417] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-03-12 19:25:32,417] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-03-12 19:25:32,417] [INFO] [config.py:964:print]   amp_enabled .................. False
[2025-03-12 19:25:32,417] [INFO] [config.py:964:print]   amp_params ................... False
[2025-03-12 19:25:32,417] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-12 19:25:32,418] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2025-03-12 19:25:32,418] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2025-03-12 19:25:32,418] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2025-03-12 19:25:32,418] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2025-03-12 19:25:32,418] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee8bd2e5090>
[2025-03-12 19:25:32,418] [INFO] [config.py:964:print]   communication_data_type ...... None
[2025-03-12 19:25:32,418] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-12 19:25:32,418] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2025-03-12 19:25:32,418] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2025-03-12 19:25:32,418] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-12 19:25:32,418] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2025-03-12 19:25:32,418] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2025-03-12 19:25:32,418] [INFO] [config.py:964:print]   disable_allgather ............ False
[2025-03-12 19:25:32,418] [INFO] [config.py:964:print]   dump_state ................... False
[2025-03-12 19:25:32,418] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2025-03-12 19:25:32,418] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2025-03-12 19:25:32,419] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-12 19:25:32,419] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-12 19:25:32,419] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2025-03-12 19:25:32,419] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2025-03-12 19:25:32,419] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2025-03-12 19:25:32,419] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2025-03-12 19:25:32,419] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2025-03-12 19:25:32,419] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2025-03-12 19:25:32,419] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-03-12 19:25:32,419] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2025-03-12 19:25:32,419] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2025-03-12 19:25:32,419] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2025-03-12 19:25:32,419] [INFO] [config.py:964:print]   global_rank .................. 0
[2025-03-12 19:25:32,419] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2025-03-12 19:25:32,419] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 10
[2025-03-12 19:25:32,419] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0
[2025-03-12 19:25:32,419] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2025-03-12 19:25:32,420] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-12 19:25:32,420] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2025-03-12 19:25:32,420] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2025-03-12 19:25:32,420] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2025-03-12 19:25:32,420] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2025-03-12 19:25:32,420] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2025-03-12 19:25:32,420] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2025-03-12 19:25:32,420] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-03-12 19:25:32,420] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-03-12 19:25:32,420] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2025-03-12 19:25:32,420] [INFO] [config.py:964:print]   optimizer_name ............... adamw
[2025-03-12 19:25:32,420] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 2e-05, 'weight_decay': 0.0, 'betas': (0.9, 0.95)}
[2025-03-12 19:25:32,420] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2025-03-12 19:25:32,420] [INFO] [config.py:964:print]   pld_enabled .................. False
[2025-03-12 19:25:32,421] [INFO] [config.py:964:print]   pld_params ................... False
[2025-03-12 19:25:32,421] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2025-03-12 19:25:32,421] [INFO] [config.py:964:print]   scheduler_name ............... WarmupDecayLR
[2025-03-12 19:25:32,421] [INFO] [config.py:964:print]   scheduler_params ............. {'total_num_steps': 25000, 'warmup_min_lr': 0, 'warmup_max_lr': 2e-05, 'warmup_num_steps': 100, 'warmup_type': 'linear'}
[2025-03-12 19:25:32,421] [INFO] [config.py:964:print]   sparse_attention ............. None
[2025-03-12 19:25:32,421] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2025-03-12 19:25:32,421] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2025-03-12 19:25:32,421] [INFO] [config.py:964:print]   train_batch_size ............. 40
[2025-03-12 19:25:32,421] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  4
[2025-03-12 19:25:32,421] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2025-03-12 19:25:32,421] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2025-03-12 19:25:32,421] [INFO] [config.py:964:print]   world_size ................... 1
[2025-03-12 19:25:32,421] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2025-03-12 19:25:32,421] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2025-03-12 19:25:32,421] [INFO] [config.py:964:print]   zero_enabled ................. True
[2025-03-12 19:25:32,421] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-12 19:25:32,421] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2025-03-12 19:25:32,422] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 4,
    "gradient_accumulation_steps": 10,
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 2e-05,
            "weight_decay": 0.0,
            "betas": [0.9, 0.95]
        }
    },
    "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "total_num_steps": 2.500000e+04,
            "warmup_min_lr": 0,
            "warmup_max_lr": 2e-05,
            "warmup_num_steps": 100,
            "warmup_type": "linear"
        }
    },
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "gradient_clipping": 1.0,
    "zero_optimization": {
        "stage": 2,
        "contiguous_gradients": true,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 5.000000e+08,
        "allgather_bucket_size": 5.000000e+08
    }
}
(train) >> AFTER DEEPSPEED
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)
  return F.conv2d(input, weight, bias, self.stride,
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch: [0][  1/500]	Time 60.462 (60.462)	Loss 0.9994 (0.8586)	CeLoss 0.2441 (0.2468)	SegCLSLoss 0.1069 (0.0938)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1790 (0.1282)	MaskBCELoss 0.0864 (0.0442)	MaskDICELoss 0.0926 (0.0840)
Epoch: [0][  2/500]	Time 53.058 (53.058)	Loss 1.0350 (0.9009)	CeLoss 0.3789 (0.2237)	SegCLSLoss 0.1045 (0.1043)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1299 (0.1401)	MaskBCELoss 0.0361 (0.0459)	MaskDICELoss 0.0938 (0.0942)
Epoch: [0][  3/500]	Time 55.243 (55.243)	Loss 1.0118 (0.9531)	CeLoss 0.3184 (0.2479)	SegCLSLoss 0.1040 (0.1038)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1502 (0.1577)	MaskBCELoss 0.0569 (0.0663)	MaskDICELoss 0.0932 (0.0914)
Epoch: [0][  4/500]	Time 57.080 (57.080)	Loss 0.8873 (0.9760)	CeLoss 0.2285 (0.2621)	SegCLSLoss 0.1016 (0.1040)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1303 (0.1596)	MaskBCELoss 0.0326 (0.0663)	MaskDICELoss 0.0977 (0.0933)
Epoch: [0][  5/500]	Time 49.222 (49.222)	Loss 0.9539 (0.9183)	CeLoss 0.2852 (0.2386)	SegCLSLoss 0.1045 (0.1039)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1377 (0.1434)	MaskBCELoss 0.0453 (0.0509)	MaskDICELoss 0.0924 (0.0925)
Epoch: [0][  6/500]	Time 51.457 (51.457)	Loss 0.8433 (0.8686)	CeLoss 0.1455 (0.1853)	SegCLSLoss 0.1045 (0.1028)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1447 (0.1452)	MaskBCELoss 0.0450 (0.0515)	MaskDICELoss 0.0996 (0.0937)
Epoch: [0][  7/500]	Time 56.284 (56.284)	Loss 0.9188 (0.9369)	CeLoss 0.2217 (0.2491)	SegCLSLoss 0.1084 (0.1050)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1470 (0.1458)	MaskBCELoss 0.0534 (0.0529)	MaskDICELoss 0.0936 (0.0929)
Epoch: [0][  8/500]	Time 52.757 (52.757)	Loss 1.0383 (0.9381)	CeLoss 0.3672 (0.2186)	SegCLSLoss 0.1040 (0.1039)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1426 (0.1631)	MaskBCELoss 0.0536 (0.0700)	MaskDICELoss 0.0890 (0.0931)
Epoch: [0][  9/500]	Time 66.502 (66.502)	Loss 0.7765 (0.9757)	CeLoss 0.1377 (0.2450)	SegCLSLoss 0.1016 (0.1035)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1241 (0.1731)	MaskBCELoss 0.0303 (0.0842)	MaskDICELoss 0.0938 (0.0889)
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7eff0794f0a0>
Traceback (most recent call last):
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1479, in __del__
    self._shutdown_workers()
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1425, in _shutdown_workers
    self._pin_memory_thread.join()
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/threading.py", line 1093, in join
    raise RuntimeError("cannot join current thread")
RuntimeError: cannot join current thread
Exception in thread Thread-7 (_pin_memory_loop):
Traceback (most recent call last):
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 54, in _pin_memory_loop
    do_one_step()
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 31, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/multiprocessing/reductions.py", line 495, in rebuild_storage_fd
    fd = df.detach()
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
Epoch: [0][ 10/500]	Time 79.074 (79.074)	Loss 0.9431 (0.9223)	CeLoss 0.2344 (0.2530)	SegCLSLoss 0.1040 (0.1046)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1577 (0.1448)	MaskBCELoss 0.0657 (0.0596)	MaskDICELoss 0.0920 (0.0852)
Epoch: [0][ 11/500]	Time 78.942 (78.942)	Loss 0.9007 (0.9100)	CeLoss 0.1885 (0.2361)	SegCLSLoss 0.1045 (0.1029)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1578 (0.1408)	MaskBCELoss 0.0637 (0.0474)	MaskDICELoss 0.0941 (0.0934)
Epoch: [0][ 12/500]	Time 88.076 (88.076)	Loss 0.7821 (0.9466)	CeLoss 0.1270 (0.2348)	SegCLSLoss 0.1045 (0.1038)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1346 (0.1608)	MaskBCELoss 0.0468 (0.0695)	MaskDICELoss 0.0877 (0.0913)
Epoch: [0][ 13/500]	Time 78.477 (78.477)	Loss 1.2317 (0.9225)	CeLoss 0.3320 (0.2289)	SegCLSLoss 0.1030 (0.1043)	KLLoss 0.0000 (0.0000)	MaskLoss 0.2596 (0.1507)	MaskBCELoss 0.1720 (0.0588)	MaskDICELoss 0.0876 (0.0918)
Epoch: [0][ 14/500]	Time 89.724 (89.724)	Loss 0.8338 (0.9301)	CeLoss 0.1514 (0.2558)	SegCLSLoss 0.1025 (0.1033)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1532 (0.1422)	MaskBCELoss 0.0676 (0.0503)	MaskDICELoss 0.0856 (0.0919)
Epoch: [0][ 15/500]	Time 85.382 (85.382)	Loss 0.8716 (0.9388)	CeLoss 0.1846 (0.2577)	SegCLSLoss 0.1030 (0.1032)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1450 (0.1461)	MaskBCELoss 0.0496 (0.0549)	MaskDICELoss 0.0954 (0.0912)
Epoch: [0][ 16/500]	Time 78.620 (78.620)	Loss 1.1067 (0.9176)	CeLoss 0.2715 (0.2280)	SegCLSLoss 0.1064 (0.1030)	KLLoss 0.0000 (0.0000)	MaskLoss 0.2141 (0.1465)	MaskBCELoss 0.1170 (0.0514)	MaskDICELoss 0.0971 (0.0952)
Epoch: [0][ 17/500]	Time 77.258 (77.258)	Loss 0.8727 (0.9383)	CeLoss 0.2324 (0.2654)	SegCLSLoss 0.1030 (0.1038)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1285 (0.1406)	MaskBCELoss 0.0395 (0.0485)	MaskDICELoss 0.0891 (0.0921)
Epoch: [0][ 18/500]	Time 86.766 (86.766)	Loss 1.0138 (0.9728)	CeLoss 0.3320 (0.3022)	SegCLSLoss 0.1025 (0.1034)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1495 (0.1431)	MaskBCELoss 0.0607 (0.0542)	MaskDICELoss 0.0887 (0.0889)
Epoch: [0][ 19/500]	Time 82.631 (82.631)	Loss 0.8719 (0.9131)	CeLoss 0.2285 (0.2266)	SegCLSLoss 0.1001 (0.1035)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1324 (0.1484)	MaskBCELoss 0.0428 (0.0568)	MaskDICELoss 0.0895 (0.0916)
Epoch: [0][ 20/500]	Time 89.333 (89.333)	Loss 1.0258 (0.9756)	CeLoss 0.3398 (0.2588)	SegCLSLoss 0.1025 (0.1032)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1444 (0.1632)	MaskBCELoss 0.0478 (0.0709)	MaskDICELoss 0.0966 (0.0923)
Epoch: [0][ 21/500]	Time 86.578 (86.578)	Loss 0.9711 (0.9181)	CeLoss 0.2988 (0.2421)	SegCLSLoss 0.1025 (0.1023)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1434 (0.1441)	MaskBCELoss 0.0528 (0.0525)	MaskDICELoss 0.0906 (0.0916)
Epoch: [0][ 22/500]	Time 84.430 (84.430)	Loss 0.8311 (0.8926)	CeLoss 0.1816 (0.2124)	SegCLSLoss 0.1016 (0.1018)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1361 (0.1460)	MaskBCELoss 0.0488 (0.0539)	MaskDICELoss 0.0873 (0.0921)
Epoch: [0][ 23/500]	Time 84.188 (84.188)	Loss 0.9864 (0.9138)	CeLoss 0.2500 (0.2195)	SegCLSLoss 0.1025 (0.1019)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1713 (0.1555)	MaskBCELoss 0.0776 (0.0658)	MaskDICELoss 0.0937 (0.0897)
Epoch: [0][ 24/500]	Time 86.542 (86.542)	Loss 0.9978 (0.9274)	CeLoss 0.3203 (0.2389)	SegCLSLoss 0.1025 (0.1021)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1446 (0.1497)	MaskBCELoss 0.0529 (0.0569)	MaskDICELoss 0.0918 (0.0928)
Epoch: [0][ 25/500]	Time 83.215 (83.215)	Loss 0.8133 (0.9261)	CeLoss 0.1377 (0.2627)	SegCLSLoss 0.0991 (0.1024)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1463 (0.1378)	MaskBCELoss 0.0540 (0.0463)	MaskDICELoss 0.0923 (0.0915)
Epoch: [0][ 26/500]	Time 83.125 (83.125)	Loss 1.0477 (0.9752)	CeLoss 0.3164 (0.2612)	SegCLSLoss 0.1025 (0.1027)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1680 (0.1588)	MaskBCELoss 0.0731 (0.0634)	MaskDICELoss 0.0949 (0.0954)
Epoch: [0][ 27/500]	Time 77.146 (77.146)	Loss 0.9449 (0.9348)	CeLoss 0.2852 (0.2333)	SegCLSLoss 0.1045 (0.1017)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1314 (0.1572)	MaskBCELoss 0.0372 (0.0653)	MaskDICELoss 0.0942 (0.0919)
Epoch: [0][ 28/500]	Time 77.283 (77.283)	Loss 0.9374 (0.9645)	CeLoss 0.2129 (0.2263)	SegCLSLoss 0.1016 (0.1027)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1679 (0.1744)	MaskBCELoss 0.0750 (0.0822)	MaskDICELoss 0.0929 (0.0922)
Epoch: [0][ 29/500]	Time 86.549 (86.549)	Loss 1.0382 (0.9260)	CeLoss 0.3457 (0.2449)	SegCLSLoss 0.1006 (0.1024)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1494 (0.1475)	MaskBCELoss 0.0531 (0.0567)	MaskDICELoss 0.0963 (0.0907)
Epoch: [0][ 30/500]	Time 85.534 (85.534)	Loss 0.9071 (0.9106)	CeLoss 0.2295 (0.2320)	SegCLSLoss 0.1030 (0.1023)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1491 (0.1441)	MaskBCELoss 0.0633 (0.0513)	MaskDICELoss 0.0858 (0.0928)
Epoch: [0][ 31/500]	Time 78.546 (78.546)	Loss 0.9740 (0.8802)	CeLoss 0.3047 (0.2010)	SegCLSLoss 0.0967 (0.0974)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1431 (0.1493)	MaskBCELoss 0.0492 (0.0567)	MaskDICELoss 0.0939 (0.0926)
Epoch: [0][ 32/500]	Time 82.832 (82.832)	Loss 0.8825 (0.8890)	CeLoss 0.2051 (0.2319)	SegCLSLoss 0.0967 (0.0972)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1541 (0.1411)	MaskBCELoss 0.0662 (0.0510)	MaskDICELoss 0.0879 (0.0901)
Epoch: [0][ 33/500]	Time 78.468 (78.468)	Loss 1.2629 (0.9187)	CeLoss 0.2129 (0.2425)	SegCLSLoss 0.0967 (0.0968)	KLLoss 0.0000 (0.0000)	MaskLoss 0.3308 (0.1543)	MaskBCELoss 0.2333 (0.0671)	MaskDICELoss 0.0975 (0.0871)
Epoch: [0][ 34/500]	Time 77.775 (77.775)	Loss 0.9224 (0.8974)	CeLoss 0.3008 (0.2379)	SegCLSLoss 0.0981 (0.0978)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1242 (0.1437)	MaskBCELoss 0.0357 (0.0555)	MaskDICELoss 0.0885 (0.0882)
Epoch: [0][ 35/500]	Time 75.685 (75.685)	Loss 0.8872 (0.9126)	CeLoss 0.2061 (0.2363)	SegCLSLoss 0.0981 (0.0971)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1493 (0.1490)	MaskBCELoss 0.0566 (0.0570)	MaskDICELoss 0.0927 (0.0920)
Epoch: [0][ 36/500]	Time 93.893 (93.893)	Loss 1.0030 (0.9305)	CeLoss 0.3398 (0.2565)	SegCLSLoss 0.0996 (0.0973)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1363 (0.1465)	MaskBCELoss 0.0415 (0.0531)	MaskDICELoss 0.0949 (0.0934)
Epoch: [0][ 37/500]	Time 88.691 (88.691)	Loss 0.8886 (0.8997)	CeLoss 0.2373 (0.2337)	SegCLSLoss 0.0947 (0.0964)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1386 (0.1443)	MaskBCELoss 0.0454 (0.0518)	MaskDICELoss 0.0932 (0.0925)
Epoch: [0][ 38/500]	Time 93.126 (93.126)	Loss 0.9529 (0.9334)	CeLoss 0.3027 (0.2679)	SegCLSLoss 0.0986 (0.0977)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1309 (0.1409)	MaskBCELoss 0.0354 (0.0466)	MaskDICELoss 0.0955 (0.0943)
Epoch: [0][ 39/500]	Time 90.179 (90.179)	Loss 0.8926 (0.8987)	CeLoss 0.2422 (0.2207)	SegCLSLoss 0.0952 (0.0965)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1325 (0.1483)	MaskBCELoss 0.0344 (0.0537)	MaskDICELoss 0.0981 (0.0946)
Epoch: [0][ 40/500]	Time 85.016 (85.016)	Loss 1.0916 (0.9589)	CeLoss 0.1836 (0.2422)	SegCLSLoss 0.0947 (0.0975)	KLLoss 0.0000 (0.0000)	MaskLoss 0.2601 (0.1690)	MaskBCELoss 0.1611 (0.0772)	MaskDICELoss 0.0990 (0.0918)
Epoch: [0][ 41/500]	Time 77.717 (77.717)	Loss 0.9364 (0.9018)	CeLoss 0.3340 (0.2594)	SegCLSLoss 0.0830 (0.0801)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1238 (0.1483)	MaskBCELoss 0.0293 (0.0553)	MaskDICELoss 0.0944 (0.0929)
Epoch: [0][ 42/500]	Time 101.239 (101.239)	Loss 0.8673 (0.8451)	CeLoss 0.2422 (0.2274)	SegCLSLoss 0.0762 (0.0778)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1411 (0.1371)	MaskBCELoss 0.0454 (0.0430)	MaskDICELoss 0.0957 (0.0940)
Epoch: [0][ 43/500]	Time 97.063 (97.063)	Loss 0.8456 (0.8650)	CeLoss 0.2285 (0.2519)	SegCLSLoss 0.0796 (0.0803)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1296 (0.1337)	MaskBCELoss 0.0301 (0.0411)	MaskDICELoss 0.0995 (0.0926)
Epoch: [0][ 44/500]	Time 88.248 (88.248)	Loss 0.9803 (0.8671)	CeLoss 0.3320 (0.2486)	SegCLSLoss 0.0874 (0.0815)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1419 (0.1328)	MaskBCELoss 0.0467 (0.0382)	MaskDICELoss 0.0952 (0.0946)
Epoch: [0][ 45/500]	Time 76.977 (76.977)	Loss 0.8172 (0.8039)	CeLoss 0.2012 (0.2056)	SegCLSLoss 0.0723 (0.0801)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1479 (0.1304)	MaskBCELoss 0.0607 (0.0417)	MaskDICELoss 0.0873 (0.0887)
Epoch: [0][ 46/500]	Time 73.666 (73.666)	Loss 0.9766 (0.8887)	CeLoss 0.3320 (0.2517)	SegCLSLoss 0.0889 (0.0816)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1369 (0.1443)	MaskBCELoss 0.0401 (0.0520)	MaskDICELoss 0.0967 (0.0923)
Epoch: [0][ 47/500]	Time 79.523 (79.523)	Loss 0.8912 (0.8703)	CeLoss 0.2471 (0.2461)	SegCLSLoss 0.0825 (0.0813)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1484 (0.1377)	MaskBCELoss 0.0574 (0.0446)	MaskDICELoss 0.0910 (0.0931)
Epoch: [0][ 48/500]	Time 79.990 (79.990)	Loss 0.7733 (0.8733)	CeLoss 0.1367 (0.2349)	SegCLSLoss 0.0742 (0.0802)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1565 (0.1486)	MaskBCELoss 0.0686 (0.0580)	MaskDICELoss 0.0879 (0.0906)
Epoch: [0][ 49/500]	Time 92.822 (92.822)	Loss 0.9784 (0.8276)	CeLoss 0.3281 (0.2172)	SegCLSLoss 0.0869 (0.0786)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1446 (0.1359)	MaskBCELoss 0.0515 (0.0452)	MaskDICELoss 0.0931 (0.0908)
Epoch: [0][ 50/500]	Time 80.958 (80.958)	Loss 0.9428 (0.9104)	CeLoss 0.3047 (0.2825)	SegCLSLoss 0.0889 (0.0827)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1400 (0.1381)	MaskBCELoss 0.0493 (0.0446)	MaskDICELoss 0.0907 (0.0934)
Epoch: [0][ 51/500]	Time 82.959 (82.959)	Loss 0.8579 (0.8464)	CeLoss 0.2344 (0.2333)	SegCLSLoss 0.0791 (0.0750)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1351 (0.1360)	MaskBCELoss 0.0374 (0.0402)	MaskDICELoss 0.0977 (0.0957)
Epoch: [0][ 52/500]	Time 83.189 (83.189)	Loss 0.9672 (0.8675)	CeLoss 0.3477 (0.2485)	SegCLSLoss 0.0796 (0.0745)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1355 (0.1408)	MaskBCELoss 0.0404 (0.0465)	MaskDICELoss 0.0950 (0.0943)
Epoch: [0][ 53/500]	Time 79.772 (79.772)	Loss 1.0066 (0.8606)	CeLoss 0.3945 (0.2509)	SegCLSLoss 0.0815 (0.0734)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1423 (0.1400)	MaskBCELoss 0.0594 (0.0485)	MaskDICELoss 0.0829 (0.0915)
Epoch: [0][ 54/500]	Time 83.437 (83.437)	Loss 0.9173 (0.8686)	CeLoss 0.3398 (0.2627)	SegCLSLoss 0.0830 (0.0767)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1115 (0.1316)	MaskBCELoss 0.0176 (0.0370)	MaskDICELoss 0.0940 (0.0946)
Epoch: [0][ 55/500]	Time 79.622 (79.622)	Loss 0.9278 (0.8559)	CeLoss 0.2852 (0.2299)	SegCLSLoss 0.0820 (0.0752)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1442 (0.1468)	MaskBCELoss 0.0495 (0.0557)	MaskDICELoss 0.0947 (0.0911)
Epoch: [0][ 56/500]	Time 55.579 (55.579)	Loss 0.7875 (0.8309)	CeLoss 0.2012 (0.2239)	SegCLSLoss 0.0693 (0.0708)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1300 (0.1372)	MaskBCELoss 0.0366 (0.0417)	MaskDICELoss 0.0934 (0.0956)
Epoch: [0][ 57/500]	Time 49.335 (49.335)	Loss 0.9336 (0.8345)	CeLoss 0.3301 (0.2267)	SegCLSLoss 0.0845 (0.0744)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1259 (0.1366)	MaskBCELoss 0.0350 (0.0437)	MaskDICELoss 0.0909 (0.0929)
Epoch: [0][ 58/500]	Time 52.064 (52.064)	Loss 0.9080 (0.8594)	CeLoss 0.2715 (0.2462)	SegCLSLoss 0.0869 (0.0746)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1415 (0.1372)	MaskBCELoss 0.0509 (0.0424)	MaskDICELoss 0.0906 (0.0948)
Epoch: [0][ 59/500]	Time 47.624 (47.624)	Loss 0.8142 (0.8629)	CeLoss 0.2441 (0.2540)	SegCLSLoss 0.0752 (0.0756)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1204 (0.1366)	MaskBCELoss 0.0305 (0.0443)	MaskDICELoss 0.0899 (0.0923)
Epoch: [0][ 60/500]	Time 82.029 (82.029)	Loss 0.8949 (0.8442)	CeLoss 0.3027 (0.2253)	SegCLSLoss 0.0820 (0.0754)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1158 (0.1399)	MaskBCELoss 0.0185 (0.0459)	MaskDICELoss 0.0973 (0.0939)
Epoch: [0][ 61/500]	Time 86.691 (86.691)	Loss 0.6533 (0.8181)	CeLoss 0.1416 (0.2261)	SegCLSLoss 0.0356 (0.0384)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1206 (0.1590)	MaskBCELoss 0.0207 (0.0603)	MaskDICELoss 0.0998 (0.0988)
Epoch: [0][ 62/500]	Time 75.619 (75.619)	Loss 0.6548 (0.7808)	CeLoss 0.1621 (0.2146)	SegCLSLoss 0.0342 (0.0430)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1136 (0.1418)	MaskBCELoss 0.0146 (0.0435)	MaskDICELoss 0.0990 (0.0983)
Epoch: [0][ 63/500]	Time 82.050 (82.050)	Loss 0.8208 (0.8313)	CeLoss 0.2363 (0.2420)	SegCLSLoss 0.0364 (0.0413)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1603 (0.1558)	MaskBCELoss 0.0648 (0.0584)	MaskDICELoss 0.0955 (0.0974)
Epoch: [0][ 64/500]	Time 67.130 (67.130)	Loss 0.8171 (0.7803)	CeLoss 0.2852 (0.2288)	SegCLSLoss 0.0552 (0.0457)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1109 (0.1342)	MaskBCELoss 0.0117 (0.0385)	MaskDICELoss 0.0992 (0.0957)
Epoch: [0][ 65/500]	Time 51.667 (51.667)	Loss 0.9409 (0.7123)	CeLoss 0.3418 (0.2181)	SegCLSLoss 0.0532 (0.0382)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1462 (0.1212)	MaskBCELoss 0.0470 (0.0335)	MaskDICELoss 0.0992 (0.0877)
Epoch: [0][ 66/500]	Time 66.542 (66.542)	Loss 0.6582 (0.7906)	CeLoss 0.1689 (0.2307)	SegCLSLoss 0.0415 (0.0416)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1079 (0.1399)	MaskBCELoss 0.0124 (0.0415)	MaskDICELoss 0.0955 (0.0985)
Epoch: [0][ 67/500]	Time 92.948 (92.948)	Loss 0.8935 (0.7883)	CeLoss 0.3379 (0.2311)	SegCLSLoss 0.0547 (0.0425)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1281 (0.1381)	MaskBCELoss 0.0328 (0.0401)	MaskDICELoss 0.0953 (0.0980)
Epoch: [0][ 68/500]	Time 84.674 (84.674)	Loss 0.7485 (0.7359)	CeLoss 0.2480 (0.2092)	SegCLSLoss 0.0293 (0.0339)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1294 (0.1325)	MaskBCELoss 0.0378 (0.0355)	MaskDICELoss 0.0917 (0.0970)
Epoch: [0][ 69/500]	Time 78.844 (78.844)	Loss 0.8822 (0.8535)	CeLoss 0.2930 (0.2861)	SegCLSLoss 0.0620 (0.0489)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1339 (0.1364)	MaskBCELoss 0.0361 (0.0377)	MaskDICELoss 0.0978 (0.0986)
Epoch: [0][ 70/500]	Time 54.226 (54.226)	Loss 0.7699 (0.8159)	CeLoss 0.2061 (0.2490)	SegCLSLoss 0.0337 (0.0450)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1489 (0.1413)	MaskBCELoss 0.0495 (0.0439)	MaskDICELoss 0.0994 (0.0974)
Epoch: [0][ 71/500]	Time 51.964 (51.964)	Loss 0.8103 (0.8093)	CeLoss 0.2598 (0.2597)	SegCLSLoss 0.0264 (0.0334)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1503 (0.1437)	MaskBCELoss 0.0517 (0.0459)	MaskDICELoss 0.0986 (0.0978)
Epoch: [0][ 72/500]	Time 48.017 (48.017)	Loss 0.8437 (0.7964)	CeLoss 0.2793 (0.2149)	SegCLSLoss 0.0483 (0.0346)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1345 (0.1588)	MaskBCELoss 0.0354 (0.0617)	MaskDICELoss 0.0991 (0.0971)
Epoch: [0][ 73/500]	Time 57.335 (57.335)	Loss 0.7048 (0.7246)	CeLoss 0.2129 (0.1974)	SegCLSLoss 0.0256 (0.0293)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1207 (0.1351)	MaskBCELoss 0.0210 (0.0360)	MaskDICELoss 0.0997 (0.0991)
Epoch: [0][ 74/500]	Time 58.871 (58.871)	Loss 0.7151 (0.7454)	CeLoss 0.1279 (0.1858)	SegCLSLoss 0.0337 (0.0308)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1676 (0.1520)	MaskBCELoss 0.0753 (0.0550)	MaskDICELoss 0.0923 (0.0971)
Epoch: [0][ 75/500]	Time 91.827 (91.827)	Loss 0.7418 (0.7545)	CeLoss 0.2402 (0.2229)	SegCLSLoss 0.0342 (0.0325)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1174 (0.1346)	MaskBCELoss 0.0185 (0.0362)	MaskDICELoss 0.0988 (0.0984)
Epoch: [0][ 76/500]	Time 87.522 (87.522)	Loss 0.8988 (0.7974)	CeLoss 0.3262 (0.2602)	SegCLSLoss 0.0493 (0.0369)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1384 (0.1353)	MaskBCELoss 0.0398 (0.0387)	MaskDICELoss 0.0985 (0.0966)
Epoch: [0][ 77/500]	Time 90.014 (90.014)	Loss 0.7623 (0.7457)	CeLoss 0.2246 (0.2033)	SegCLSLoss 0.0327 (0.0317)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1435 (0.1432)	MaskBCELoss 0.0511 (0.0470)	MaskDICELoss 0.0924 (0.0962)
Epoch: [0][ 78/500]	Time 98.962 (98.962)	Loss 0.9322 (0.7999)	CeLoss 0.3438 (0.2597)	SegCLSLoss 0.0454 (0.0346)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1552 (0.1382)	MaskBCELoss 0.0616 (0.0410)	MaskDICELoss 0.0937 (0.0972)
Epoch: [0][ 79/500]	Time 81.775 (81.775)	Loss 0.7373 (0.8471)	CeLoss 0.1973 (0.2723)	SegCLSLoss 0.0266 (0.0404)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1494 (0.1504)	MaskBCELoss 0.0551 (0.0536)	MaskDICELoss 0.0943 (0.0968)
Epoch: [0][ 80/500]	Time 93.841 (93.841)	Loss 0.7142 (0.7528)	CeLoss 0.2285 (0.2233)	SegCLSLoss 0.0270 (0.0340)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1193 (0.1322)	MaskBCELoss 0.0222 (0.0335)	MaskDICELoss 0.0971 (0.0987)
Epoch: [0][ 81/500]	Time 93.381 (93.381)	Loss 0.8288 (0.7501)	CeLoss 0.2871 (0.2178)	SegCLSLoss 0.0415 (0.0322)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1385 (0.1386)	MaskBCELoss 0.0478 (0.0434)	MaskDICELoss 0.0907 (0.0951)
Epoch: [0][ 82/500]	Time 66.851 (66.851)	Loss 0.6968 (0.7274)	CeLoss 0.1904 (0.2081)	SegCLSLoss 0.0222 (0.0266)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1325 (0.1355)	MaskBCELoss 0.0338 (0.0380)	MaskDICELoss 0.0987 (0.0975)
Epoch: [0][ 83/500]	Time 49.902 (49.902)	Loss 0.8072 (0.7818)	CeLoss 0.2500 (0.2289)	SegCLSLoss 0.0243 (0.0312)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1552 (0.1501)	MaskBCELoss 0.0559 (0.0550)	MaskDICELoss 0.0992 (0.0951)
Epoch: [0][ 84/500]	Time 71.393 (71.393)	Loss 0.9355 (0.8296)	CeLoss 0.1377 (0.2664)	SegCLSLoss 0.0287 (0.0374)	KLLoss 0.0000 (0.0000)	MaskLoss 0.2735 (0.1475)	MaskBCELoss 0.1769 (0.0506)	MaskDICELoss 0.0966 (0.0968)
Epoch: [0][ 85/500]	Time 89.516 (89.516)	Loss 0.7879 (0.7301)	CeLoss 0.2617 (0.2251)	SegCLSLoss 0.0483 (0.0313)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1292 (0.1247)	MaskBCELoss 0.0442 (0.0282)	MaskDICELoss 0.0850 (0.0965)
Epoch: [0][ 86/500]	Time 86.747 (86.747)	Loss 0.7107 (0.7521)	CeLoss 0.1914 (0.2181)	SegCLSLoss 0.0231 (0.0312)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1385 (0.1393)	MaskBCELoss 0.0404 (0.0429)	MaskDICELoss 0.0981 (0.0964)
Epoch: [0][ 87/500]	Time 90.625 (90.625)	Loss 0.7780 (0.7931)	CeLoss 0.2188 (0.2443)	SegCLSLoss 0.0205 (0.0316)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1631 (0.1468)	MaskBCELoss 0.0668 (0.0509)	MaskDICELoss 0.0963 (0.0959)
Epoch: [0][ 88/500]	Time 89.474 (89.474)	Loss 0.6927 (0.7753)	CeLoss 0.1660 (0.2285)	SegCLSLoss 0.0260 (0.0341)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1448 (0.1427)	MaskBCELoss 0.0519 (0.0459)	MaskDICELoss 0.0929 (0.0968)
Epoch: [0][ 89/500]	Time 78.752 (78.752)	Loss 0.8651 (0.7856)	CeLoss 0.2852 (0.2214)	SegCLSLoss 0.0469 (0.0343)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1449 (0.1525)	MaskBCELoss 0.0463 (0.0571)	MaskDICELoss 0.0986 (0.0954)
Epoch: [0][ 90/500]	Time 85.144 (85.144)	Loss 0.7506 (0.7349)	CeLoss 0.2451 (0.1921)	SegCLSLoss 0.0205 (0.0266)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1328 (0.1478)	MaskBCELoss 0.0333 (0.0507)	MaskDICELoss 0.0995 (0.0971)
Epoch: [0][ 91/500]	Time 79.397 (79.397)	Loss 0.6737 (0.6822)	CeLoss 0.1963 (0.1944)	SegCLSLoss 0.0227 (0.0239)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1224 (0.1271)	MaskBCELoss 0.0289 (0.0342)	MaskDICELoss 0.0934 (0.0929)
Epoch: [0][ 92/500]	Time 87.202 (87.202)	Loss 0.8284 (0.7209)	CeLoss 0.2734 (0.2140)	SegCLSLoss 0.0464 (0.0283)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1349 (0.1308)	MaskBCELoss 0.0385 (0.0365)	MaskDICELoss 0.0964 (0.0943)
Epoch: [0][ 93/500]	Time 52.384 (52.384)	Loss 0.6636 (0.7657)	CeLoss 0.1211 (0.2589)	SegCLSLoss 0.0251 (0.0300)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1516 (0.1281)	MaskBCELoss 0.0573 (0.0330)	MaskDICELoss 0.0943 (0.0952)
Epoch: [0][ 94/500]	Time 72.889 (72.889)	Loss 0.6569 (0.7104)	CeLoss 0.1523 (0.2180)	SegCLSLoss 0.0189 (0.0252)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1409 (0.1268)	MaskBCELoss 0.0487 (0.0327)	MaskDICELoss 0.0922 (0.0941)
Epoch: [0][ 95/500]	Time 80.803 (80.803)	Loss 0.6412 (0.7076)	CeLoss 0.1807 (0.2247)	SegCLSLoss 0.0193 (0.0250)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1172 (0.1232)	MaskBCELoss 0.0232 (0.0298)	MaskDICELoss 0.0940 (0.0934)
Epoch: [0][ 96/500]	Time 54.715 (54.715)	Loss 0.7214 (0.7565)	CeLoss 0.2451 (0.2550)	SegCLSLoss 0.0222 (0.0299)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1203 (0.1239)	MaskBCELoss 0.0245 (0.0269)	MaskDICELoss 0.0958 (0.0971)
Epoch: [0][ 97/500]	Time 48.570 (48.570)	Loss 0.8301 (0.7482)	CeLoss 0.3047 (0.2422)	SegCLSLoss 0.0444 (0.0331)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1212 (0.1258)	MaskBCELoss 0.0242 (0.0319)	MaskDICELoss 0.0970 (0.0939)
Epoch: [0][ 98/500]	Time 56.167 (56.167)	Loss 0.6707 (0.7109)	CeLoss 0.2285 (0.2126)	SegCLSLoss 0.0150 (0.0244)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1082 (0.1312)	MaskBCELoss 0.0100 (0.0375)	MaskDICELoss 0.0982 (0.0936)
Epoch: [0][ 99/500]	Time 50.512 (50.512)	Loss 0.8443 (0.7440)	CeLoss 0.2852 (0.2396)	SegCLSLoss 0.0417 (0.0305)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1405 (0.1285)	MaskBCELoss 0.0432 (0.0353)	MaskDICELoss 0.0973 (0.0932)
[2025-03-12 21:31:44,158] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[1.999437751004016e-05], mom=[(0.9, 0.95)]
[2025-03-12 21:31:44,169] [INFO] [timer.py:215:stop] epoch=0/micro_step=100/global_step=10, RunningAvgSamplesPerSec=0.5346522001099138, CurrSamplesPerSec=0.6645624901430475, MemAllocated=59.55GB, MaxMemAllocated=73.97GB
Epoch: [0][100/500]	Time 47.586 (47.586)	Loss 0.8414 (0.7596)	CeLoss 0.2363 (0.2412)	SegCLSLoss 0.0183 (0.0292)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1943 (0.1361)	MaskBCELoss 0.1039 (0.0420)	MaskDICELoss 0.0904 (0.0941)
Epoch: [0][101/500]	Time 49.695 (49.695)	Loss 0.6270 (0.6738)	CeLoss 0.1777 (0.2049)	SegCLSLoss 0.0165 (0.0196)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1164 (0.1258)	MaskBCELoss 0.0247 (0.0369)	MaskDICELoss 0.0917 (0.0890)
Epoch: [0][102/500]	Time 54.540 (54.540)	Loss 0.6692 (0.6427)	CeLoss 0.2168 (0.1750)	SegCLSLoss 0.0142 (0.0176)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1185 (0.1274)	MaskBCELoss 0.0251 (0.0386)	MaskDICELoss 0.0934 (0.0888)
Epoch: [0][103/500]	Time 46.831 (46.831)	Loss 0.6364 (0.7053)	CeLoss 0.1543 (0.2191)	SegCLSLoss 0.0170 (0.0199)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1281 (0.1305)	MaskBCELoss 0.0324 (0.0378)	MaskDICELoss 0.0956 (0.0927)
Epoch: [0][104/500]	Time 46.565 (46.565)	Loss 0.6420 (0.6875)	CeLoss 0.1846 (0.2010)	SegCLSLoss 0.0165 (0.0206)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1184 (0.1292)	MaskBCELoss 0.0244 (0.0357)	MaskDICELoss 0.0940 (0.0935)
Epoch: [0][105/500]	Time 51.436 (51.436)	Loss 0.6898 (0.6694)	CeLoss 0.2246 (0.2015)	SegCLSLoss 0.0116 (0.0154)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1282 (0.1243)	MaskBCELoss 0.0357 (0.0300)	MaskDICELoss 0.0925 (0.0943)
Epoch: [0][106/500]	Time 50.466 (50.466)	Loss 0.6812 (0.6690)	CeLoss 0.1992 (0.2336)	SegCLSLoss 0.0184 (0.0214)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1283 (0.1135)	MaskBCELoss 0.0339 (0.0305)	MaskDICELoss 0.0944 (0.0829)
Epoch: [0][107/500]	Time 59.694 (59.694)	Loss 0.7998 (0.7393)	CeLoss 0.2910 (0.2485)	SegCLSLoss 0.0310 (0.0247)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1315 (0.1304)	MaskBCELoss 0.0397 (0.0400)	MaskDICELoss 0.0918 (0.0904)
Epoch: [0][108/500]	Time 84.188 (84.188)	Loss 0.6357 (0.7176)	CeLoss 0.1426 (0.2332)	SegCLSLoss 0.0135 (0.0198)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1375 (0.1311)	MaskBCELoss 0.0424 (0.0398)	MaskDICELoss 0.0952 (0.0913)
Epoch: [0][109/500]	Time 77.746 (77.746)	Loss 0.8690 (0.7204)	CeLoss 0.3672 (0.2354)	SegCLSLoss 0.0261 (0.0219)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1357 (0.1308)	MaskBCELoss 0.0462 (0.0411)	MaskDICELoss 0.0895 (0.0898)
Epoch: [0][110/500]	Time 85.022 (85.022)	Loss 0.8575 (0.7254)	CeLoss 0.3418 (0.2396)	SegCLSLoss 0.0344 (0.0230)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1289 (0.1277)	MaskBCELoss 0.0354 (0.0355)	MaskDICELoss 0.0936 (0.0921)
Epoch: [0][111/500]	Time 83.772 (83.772)	Loss 0.6685 (0.7140)	CeLoss 0.1758 (0.2268)	SegCLSLoss 0.0145 (0.0202)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1403 (0.1302)	MaskBCELoss 0.0487 (0.0370)	MaskDICELoss 0.0916 (0.0932)
Epoch: [0][112/500]	Time 91.276 (91.276)	Loss 0.6266 (0.6859)	CeLoss 0.1582 (0.2063)	SegCLSLoss 0.0151 (0.0183)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1254 (0.1312)	MaskBCELoss 0.0317 (0.0410)	MaskDICELoss 0.0937 (0.0902)
Epoch: [0][113/500]	Time 85.038 (85.038)	Loss 0.8008 (0.7391)	CeLoss 0.2949 (0.2534)	SegCLSLoss 0.0289 (0.0219)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1337 (0.1301)	MaskBCELoss 0.0435 (0.0393)	MaskDICELoss 0.0901 (0.0908)
Epoch: [0][114/500]	Time 68.089 (68.089)	Loss 0.6211 (0.6913)	CeLoss 0.1787 (0.2107)	SegCLSLoss 0.0125 (0.0184)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1214 (0.1325)	MaskBCELoss 0.0339 (0.0430)	MaskDICELoss 0.0875 (0.0895)
Epoch: [0][115/500]	Time 46.387 (46.387)	Loss 0.6676 (0.7148)	CeLoss 0.2109 (0.2360)	SegCLSLoss 0.0177 (0.0210)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1243 (0.1288)	MaskBCELoss 0.0381 (0.0393)	MaskDICELoss 0.0861 (0.0895)
Epoch: [0][116/500]	Time 55.526 (55.526)	Loss 0.6591 (0.6954)	CeLoss 0.1992 (0.2145)	SegCLSLoss 0.0094 (0.0171)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1265 (0.1323)	MaskBCELoss 0.0329 (0.0410)	MaskDICELoss 0.0937 (0.0913)
Epoch: [0][117/500]	Time 51.109 (51.109)	Loss 0.6162 (0.7348)	CeLoss 0.1465 (0.2453)	SegCLSLoss 0.0143 (0.0236)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1258 (0.1327)	MaskBCELoss 0.0314 (0.0443)	MaskDICELoss 0.0944 (0.0884)
Epoch: [0][118/500]	Time 46.880 (46.880)	Loss 0.7182 (0.7209)	CeLoss 0.2480 (0.2322)	SegCLSLoss 0.0143 (0.0170)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1327 (0.1373)	MaskBCELoss 0.0446 (0.0471)	MaskDICELoss 0.0881 (0.0901)
Epoch: [0][119/500]	Time 45.860 (45.860)	Loss 0.8093 (0.7098)	CeLoss 0.2969 (0.2240)	SegCLSLoss 0.0305 (0.0203)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1332 (0.1309)	MaskBCELoss 0.0406 (0.0394)	MaskDICELoss 0.0926 (0.0915)
Epoch: [0][120/500]	Time 54.583 (54.583)	Loss 0.6930 (0.7284)	CeLoss 0.2266 (0.2421)	SegCLSLoss 0.0121 (0.0202)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1257 (0.1321)	MaskBCELoss 0.0300 (0.0412)	MaskDICELoss 0.0958 (0.0910)
Epoch: [0][121/500]	Time 55.173 (55.173)	Loss 0.6827 (0.6901)	CeLoss 0.2002 (0.1995)	SegCLSLoss 0.0121 (0.0226)	KLLoss 0.0000 (0.0000)	MaskLoss 0.1353 (0.1360)	MaskBCELoss 0.0417 (0.0493)	MaskDICELoss 0.0936 (0.0867)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1326, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1326, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     conversation_input_ids = np.array(input_ids[conv_idx])
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     conversation_input_ids = np.array(input_ids[conv_idx])
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     conversation_input_ids = np.array(input_ids[conv_idx])
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 681, in <module>
[rank0]:     main(sys.argv[1:])
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 404, in main
[rank0]:     train_iter = train(
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 494, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1326, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1326, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 491, in train
[rank0]:     input_dict = next(train_iter)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 124, in __next__
[rank0]:     return next(self.data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/dataloader.py", line 157, in <genexpr>
[rank0]:     self.data = (x for x in self.dataloader)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1346, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py", line 705, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 1.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     if len(per_token_label_dict_list[conv_idx]) != 0:
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     conversation_input_ids = np.array(input_ids[conv_idx])
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     conversation_input_ids = np.array(input_ids[conv_idx])
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     conversation_input_ids = np.array(input_ids[conv_idx])
[rank0]: IndexError: list index out of range
[rank0]: During handling of the above exception, another exception occurred:
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
[rank0]:     return self.collate_fn(data)
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/utils/dataset.py", line 168, in collate_fn
[rank0]:     conversation_input_ids = np.array(input_ids[conv_idx])
[rank0]: IndexError: list index out of range