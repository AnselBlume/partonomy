
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
> /shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py(383)__init__()
-> if not hasattr(config, "train_mask_decoder"):
LlavaConfig {
  "_name_or_path": "liuhaotian/llava-llama-2-13b-chat-lightning-preview",
  "architectures": [
    "LlavaLlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "image_aspect_ratio": "square",
  "image_grid_pinpoints": null,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_resampler_type": null,
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14",
  "model_type": "llava",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "tune_mm_mlp_adapter": false,
  "tune_mm_vision_resampler": false,
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}
False
{'train_mask_decoder': True, 'out_dim': 256, 'ce_loss_weight': 1.0, 'dice_type': 'focal_tversky', 'dice_loss_weight': 8.0, 'dice_scale_factor': 1000.0, 'bce_loss_weight': 2.0, 'seg_cls_loss_weight': 2.0, 'seg_cls_loss_per_cls_weight': [0.1, 1.0, 1.0], 'kld_loss_weight': 0.1, 'kld_sigma': 1.0, 'seg_token_idx': -999, 'vision_pretrained': '/shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/weights/sam_vit_h_4b8939.pth', 'vision_tower': 'openai/clip-vit-large-patch14', 'use_mm_start_end': True, 'use_teacher_ref': False, 'use_bidir_bio': True, 'use_crf_bio': False, 'use_hinge_loss': False, 'use_feedback_loop': True, 'pred_binary_span': False, 'use_cross_attn_bio': False, 'train_mask_prompt_encoder': True, 'focal_tversky_alpha': 0.7, 'focal_tversky_beta': 0.3, 'bidir_nhead': 8, 'bidir_dim_feedforward': 2048}
Traceback (most recent call last):
  File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 1168, in <module>
  File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 348, in main
    )
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2700, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py", line 383, in __init__
    config.mm_vision_tower = kwargs.get(
  File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py", line 383, in __init__
    config.mm_vision_tower = kwargs.get(
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit