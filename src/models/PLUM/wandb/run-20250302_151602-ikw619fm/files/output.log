You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565



Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:15<00:00,  5.13s/it]
trainable params: 6,553,600 || all params: 14,025,701,683 || trainable%: 0.0467256480147682
n:  base_model.model.model.embed_tokens.weight p.shape:  torch.Size([32002, 5120])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.q_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.k_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.v_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.self_attn.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.weight p.shape:  torch.Size([2048, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin1.bias p.shape:  torch.Size([2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.weight p.shape:  torch.Size([256, 2048])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.mlp.lin2.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm3.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.norm4.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.q_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.k_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.weight p.shape:  torch.Size([128, 256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.v_proj.bias p.shape:  torch.Size([128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.weight p.shape:  torch.Size([256, 128])
n:  base_model.model.model.visual_model.mask_decoder.transformer.final_attn_token_to_image.out_proj.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.weight p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.transformer.norm_final_attn.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_token.weight p.shape:  torch.Size([1, 256])
n:  base_model.model.model.visual_model.mask_decoder.mask_tokens.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.weight p.shape:  torch.Size([256, 64, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.0.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.weight p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.1.bias p.shape:  torch.Size([64])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.weight p.shape:  torch.Size([64, 32, 2, 2])
n:  base_model.model.model.visual_model.mask_decoder.output_upscaling.3.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.0.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.1.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.2.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.weight p.shape:  torch.Size([32, 256])
n:  base_model.model.model.visual_model.mask_decoder.output_hypernetworks_mlps.3.layers.2.bias p.shape:  torch.Size([32])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.0.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.weight p.shape:  torch.Size([256, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.1.bias p.shape:  torch.Size([256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.weight p.shape:  torch.Size([4, 256])
n:  base_model.model.model.visual_model.mask_decoder.iou_prediction_head.layers.2.bias p.shape:  torch.Size([4])
n:  base_model.model.model.text_hidden_fcs.0.0.weight p.shape:  torch.Size([5120, 5120])
n:  base_model.model.model.text_hidden_fcs.0.0.bias p.shape:  torch.Size([5120])
n:  base_model.model.model.text_hidden_fcs.0.2.weight p.shape:  torch.Size([256, 5120])
n:  base_model.model.model.text_hidden_fcs.0.2.bias p.shape:  torch.Size([256])
n:  base_model.model.lm_head.weight p.shape:  torch.Size([32002, 5120])
ade20k:  20210
cocostuff:  118287
loading annotations into memory...
Done (t=0.54s)
creating index...
index created!
pascal_part:  4366
loading annotations into memory...
Done (t=8.18s)
creating index...
index created!
paco_lvis:  45790
mapillary:  18000
loading dataset refclef into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refclef/refs(unc).p
creating index...
index created.
DONE (t=3.09s)
dataset refclef (refs unc) (train split) has 17978 images and 99523 annotations.
loading dataset refcoco into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco/refs(unc).p
creating index...
index created.
DONE (t=5.67s)
dataset refcoco (refs unc) (train split) has 16994 images and 196771 annotations.
loading dataset refcoco+ into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcoco+/refs(unc).p
creating index...
index created.
DONE (t=6.68s)
dataset refcoco+ (refs unc) (train split) has 16992 images and 196737 annotations.
loading dataset refcocog into memory...
ref_file:  /shared/nas/data/m1/jk100/code/OpenAttrLibrary/LISA/dataset/refer_seg/refcocog/refs(umd).p
creating index...
index created.
DONE (t=3.51s)
dataset refcocog (refs umd) (train split) has 21899 images and 208960 annotations.
vqa_data:  157712
number of reason_seg samples:  239
len(self.img_to_explanation):  239
Training with 5000 examples and validating with 200 examples.
[2025-03-02 15:17:36,627] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5, git-hash=unknown, git-branch=unknown
[2025-03-02 15:17:36,627] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2025-03-02 15:17:36,627] [INFO] [comm.py:594:init_distributed] cdb=None
[2025-03-02 15:17:36,627] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Using /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /shared/nas/data/m1/jk100/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[2025-03-02 15:17:52,810] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Time to load fused_adam op: 0.645766019821167 seconds
[2025-03-02 15:17:53,643] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-03-02 15:17:53,828] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-03-02 15:17:53,828] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-03-02 15:17:53,828] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-03-02 15:17:53,828] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500000000
[2025-03-02 15:17:53,829] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500000000
[2025-03-02 15:17:53,829] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2025-03-02 15:17:53,829] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Loading extension module fused_adam...
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/tensor/python_tensor.cpp:78.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
Rank: 0 partition count [1] and sizes[(365842916, False)]
[2025-03-02 15:18:00,905] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2025-03-02 15:18:00,906] [INFO] [utils.py:786:see_memory_usage] MA 27.69 GB         Max_MA 28.37 GB         CA 28.51 GB         Max_CA 29 GB
[2025-03-02 15:18:00,907] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 105.67 GB, percent = 10.5%
[2025-03-02 15:18:03,698] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2025-03-02 15:18:03,699] [INFO] [utils.py:786:see_memory_usage] MA 30.41 GB         Max_MA 31.78 GB         CA 32.6 GB         Max_CA 33 GB
[2025-03-02 15:18:03,699] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 106.16 GB, percent = 10.5%
[2025-03-02 15:18:03,699] [INFO] [stage_1_and_2.py:488:__init__] optimizer state initialized
[2025-03-02 15:18:06,478] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2025-03-02 15:18:06,479] [INFO] [utils.py:786:see_memory_usage] MA 30.41 GB         Max_MA 30.41 GB         CA 32.6 GB         Max_CA 33 GB
[2025-03-02 15:18:06,479] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 106.36 GB, percent = 10.6%
[2025-03-02 15:18:06,485] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-03-02 15:18:06,485] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2025-03-02 15:18:06,485] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7fe5cbd9c7f0>
[2025-03-02 15:18:06,485] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003], mom=[(0.9, 0.95)]
[2025-03-02 15:18:06,488] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2025-03-02 15:18:06,488] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-03-02 15:18:06,488] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-03-02 15:18:06,488] [INFO] [config.py:964:print]   amp_enabled .................. False
[2025-03-02 15:18:06,489] [INFO] [config.py:964:print]   amp_params ................... False
[2025-03-02 15:18:06,489] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-02 15:18:06,489] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2025-03-02 15:18:06,489] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2025-03-02 15:18:06,489] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2025-03-02 15:18:06,489] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2025-03-02 15:18:06,489] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe5cbd9d2d0>
[2025-03-02 15:18:06,489] [INFO] [config.py:964:print]   communication_data_type ...... None
[2025-03-02 15:18:06,489] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-02 15:18:06,489] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2025-03-02 15:18:06,489] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2025-03-02 15:18:06,489] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-02 15:18:06,489] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2025-03-02 15:18:06,489] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2025-03-02 15:18:06,489] [INFO] [config.py:964:print]   disable_allgather ............ False
[2025-03-02 15:18:06,489] [INFO] [config.py:964:print]   dump_state ................... False
[2025-03-02 15:18:06,490] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2025-03-02 15:18:06,490] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2025-03-02 15:18:06,490] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-02 15:18:06,490] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-02 15:18:06,490] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2025-03-02 15:18:06,490] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2025-03-02 15:18:06,490] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2025-03-02 15:18:06,490] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2025-03-02 15:18:06,490] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2025-03-02 15:18:06,490] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2025-03-02 15:18:06,490] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-03-02 15:18:06,490] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2025-03-02 15:18:06,490] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2025-03-02 15:18:06,490] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2025-03-02 15:18:06,490] [INFO] [config.py:964:print]   global_rank .................. 0
[2025-03-02 15:18:06,490] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2025-03-02 15:18:06,490] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 10
[2025-03-02 15:18:06,490] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0
[2025-03-02 15:18:06,490] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2025-03-02 15:18:06,491] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-02 15:18:06,491] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2025-03-02 15:18:06,491] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2025-03-02 15:18:06,491] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2025-03-02 15:18:06,491] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2025-03-02 15:18:06,491] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2025-03-02 15:18:06,491] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2025-03-02 15:18:06,491] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-03-02 15:18:06,491] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-03-02 15:18:06,491] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2025-03-02 15:18:06,491] [INFO] [config.py:964:print]   optimizer_name ............... adamw
[2025-03-02 15:18:06,491] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.0003, 'weight_decay': 0.0, 'betas': (0.9, 0.95)}
[2025-03-02 15:18:06,491] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2025-03-02 15:18:06,491] [INFO] [config.py:964:print]   pld_enabled .................. False
[2025-03-02 15:18:06,492] [INFO] [config.py:964:print]   pld_params ................... False
[2025-03-02 15:18:06,492] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2025-03-02 15:18:06,492] [INFO] [config.py:964:print]   scheduler_name ............... WarmupDecayLR
[2025-03-02 15:18:06,492] [INFO] [config.py:964:print]   scheduler_params ............. {'total_num_steps': 5000, 'warmup_min_lr': 0, 'warmup_max_lr': 0.0003, 'warmup_num_steps': 100, 'warmup_type': 'linear'}
[2025-03-02 15:18:06,492] [INFO] [config.py:964:print]   sparse_attention ............. None
[2025-03-02 15:18:06,492] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2025-03-02 15:18:06,492] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2025-03-02 15:18:06,492] [INFO] [config.py:964:print]   train_batch_size ............. 10
[2025-03-02 15:18:06,492] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2025-03-02 15:18:06,492] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2025-03-02 15:18:06,492] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2025-03-02 15:18:06,492] [INFO] [config.py:964:print]   world_size ................... 1
[2025-03-02 15:18:06,493] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2025-03-02 15:18:06,493] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2025-03-02 15:18:06,493] [INFO] [config.py:964:print]   zero_enabled ................. True
[2025-03-02 15:18:06,493] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-02 15:18:06,493] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2025-03-02 15:18:06,493] [INFO] [config.py:950:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1,
    "gradient_accumulation_steps": 10,
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 0.0003,
            "weight_decay": 0.0,
            "betas": [0.9, 0.95]
        }
    },
    "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "total_num_steps": 5.000000e+03,
            "warmup_min_lr": 0,
            "warmup_max_lr": 0.0003,
            "warmup_num_steps": 100,
            "warmup_type": "linear"
        }
    },
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "gradient_clipping": 1.0,
    "zero_optimization": {
        "stage": 2,
        "contiguous_gradients": true,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 5.000000e+08,
        "allgather_bucket_size": 5.000000e+08
    }
}
(train) >> AFTER DEEPSPEED
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)
  return F.conv2d(input, weight, bias, self.stride,
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([333, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch: [0][  1/500]	Time 12.872 (12.872)	Loss 6.0897 (8.9672)	CeLoss 3.2812 (3.1719)	SegCLSLoss 0.9609 (0.9789)	KLLoss 0.6328 (0.5062)	MaskLoss 1.3828 (3.0003)	MaskBCELoss 0.4843 (2.1028)	MaskDICELoss 0.8985 (0.8975)
Epoch: [0][  2/500]	Time  9.744 ( 9.744)	Loss 10.1720 (6.0675)	CeLoss 3.4688 (2.3379)	SegCLSLoss 0.9648 (0.7848)	KLLoss 0.5430 (0.3959)	MaskLoss 3.5111 (2.0338)	MaskBCELoss 2.5209 (1.2354)	MaskDICELoss 0.9902 (0.7984)
Epoch: [0][  3/500]	Time 10.507 (10.507)	Loss 9.2644 (7.1010)	CeLoss 3.0312 (2.7750)	SegCLSLoss 1.0234 (0.7992)	KLLoss 0.5430 (0.4656)	MaskLoss 3.2692 (2.2492)	MaskBCELoss 2.2697 (1.4549)	MaskDICELoss 0.9995 (0.7943)
Epoch: [0][  4/500]	Time 11.786 (11.786)	Loss 0.9336 (6.4752)	CeLoss 0.9336 (2.5754)	SegCLSLoss 0.0000 (0.8938)	KLLoss 0.0000 (0.4840)	MaskLoss 0.0000 (2.0799)	MaskBCELoss 0.0000 (1.2017)	MaskDICELoss 0.0000 (0.8783)
Epoch: [0][  5/500]	Time  9.628 ( 9.628)	Loss 4.7154 (2.8119)	CeLoss 2.5000 (1.7684)	SegCLSLoss 1.0312 (0.3969)	KLLoss 0.4785 (0.1973)	MaskLoss 1.3229 (0.6066)	MaskBCELoss 0.3234 (0.2073)	MaskDICELoss 0.9995 (0.3993)
Epoch: [0][  6/500]	Time 11.133 (11.133)	Loss 5.9067 (4.3287)	CeLoss 2.9375 (1.8051)	SegCLSLoss 1.0234 (0.5969)	KLLoss 0.5391 (0.2750)	MaskLoss 1.6256 (1.4060)	MaskBCELoss 0.6383 (0.8081)	MaskDICELoss 0.9872 (0.5979)
Epoch: [0][  7/500]	Time 12.230 (12.230)	Loss 5.7015 (4.3069)	CeLoss 3.5000 (2.5102)	SegCLSLoss 0.9531 (0.7898)	KLLoss 0.6094 (0.4221)	MaskLoss 1.1257 (1.0135)	MaskBCELoss 0.2113 (0.2418)	MaskDICELoss 0.9144 (0.7717)
Epoch: [0][  8/500]	Time  9.238 ( 9.238)	Loss 5.9279 (4.3793)	CeLoss 2.0156 (2.3062)	SegCLSLoss 0.9609 (0.6895)	KLLoss 0.5195 (0.3904)	MaskLoss 2.0054 (1.0962)	MaskBCELoss 1.1559 (0.4405)	MaskDICELoss 0.8495 (0.6558)
Epoch: [0][  9/500]	Time 10.849 (10.849)	Loss 5.1061 (4.6131)	CeLoss 2.9531 (2.2203)	SegCLSLoss 0.9922 (0.7867)	KLLoss 0.5312 (0.3865)	MaskLoss 1.2262 (1.3459)	MaskBCELoss 0.2545 (0.5767)	MaskDICELoss 0.9717 (0.7692)
[2025-03-02 15:19:55,301] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[5.1e-05], mom=[(0.9, 0.95)]
[2025-03-02 15:19:55,306] [INFO] [timer.py:215:stop] epoch=0/micro_step=100/global_step=10, RunningAvgSamplesPerSec=0.9302824469046619, CurrSamplesPerSec=0.9394598519774824, MemAllocated=30.78GB, MaxMemAllocated=35.77GB
Epoch: [0][ 10/500]	Time 10.646 (10.646)	Loss 0.8242 (3.7250)	CeLoss 0.8242 (1.9902)	SegCLSLoss 0.0000 (0.6922)	KLLoss 0.0000 (0.3930)	MaskLoss 0.0000 (0.9020)	MaskBCELoss 0.0000 (0.2694)	MaskDICELoss 0.0000 (0.6326)
Epoch: [0][ 11/500]	Time  9.807 ( 9.807)	Loss 4.4285 (3.7345)	CeLoss 2.2188 (1.9434)	SegCLSLoss 0.9961 (0.6953)	KLLoss 0.5742 (0.4160)	MaskLoss 1.2256 (0.9194)	MaskBCELoss 0.2268 (0.2710)	MaskDICELoss 0.9988 (0.6484)
Epoch: [0][ 12/500]	Time 10.579 (10.579)	Loss 4.4087 (4.7165)	CeLoss 2.2500 (2.3422)	SegCLSLoss 0.9922 (0.8984)	KLLoss 0.4883 (0.4945)	MaskLoss 1.2895 (1.2327)	MaskBCELoss 0.2952 (0.4270)	MaskDICELoss 0.9943 (0.8058)
Epoch: [0][ 13/500]	Time  9.183 ( 9.183)	Loss 0.7773 (3.8407)	CeLoss 0.7773 (1.8320)	SegCLSLoss 0.0000 (0.5996)	KLLoss 0.0000 (0.3102)	MaskLoss 0.0000 (1.0969)	MaskBCELoss 0.0000 (0.5138)	MaskDICELoss 0.0000 (0.5831)
Epoch: [0][ 14/500]	Time  9.662 ( 9.662)	Loss 1.1875 (3.5418)	CeLoss 1.1875 (1.7844)	SegCLSLoss 0.0000 (0.6895)	KLLoss 0.0000 (0.3518)	MaskLoss 0.0000 (0.9426)	MaskBCELoss 0.0000 (0.3239)	MaskDICELoss 0.0000 (0.6187)
Epoch: [0][ 15/500]	Time 10.062 (10.062)	Loss 4.9171 (4.1105)	CeLoss 2.0781 (1.8500)	SegCLSLoss 1.0156 (0.8840)	KLLoss 0.5312 (0.4551)	MaskLoss 1.5710 (1.2155)	MaskBCELoss 0.5809 (0.4157)	MaskDICELoss 0.9901 (0.7999)
Epoch: [0][ 16/500]	Time 12.254 (12.254)	Loss 4.0546 (4.0470)	CeLoss 1.8203 (1.6242)	SegCLSLoss 1.0312 (0.9957)	KLLoss 0.5859 (0.5127)	MaskLoss 1.1818 (1.3452)	MaskBCELoss 0.2215 (0.4021)	MaskDICELoss 0.9604 (0.9431)
Epoch: [0][ 17/500]	Time 10.040 (10.040)	Loss 1.8828 (3.5173)	CeLoss 1.8828 (1.5914)	SegCLSLoss 0.0000 (0.6965)	KLLoss 0.0000 (0.3986)	MaskLoss 0.0000 (1.0040)	MaskBCELoss 0.0000 (0.3580)	MaskDICELoss 0.0000 (0.6460)
Epoch: [0][ 18/500]	Time 11.644 (11.644)	Loss 3.7269 (3.1212)	CeLoss 1.1875 (1.2270)	SegCLSLoss 0.9766 (0.8871)	KLLoss 0.4414 (0.4303)	MaskLoss 1.4087 (1.1000)	MaskBCELoss 0.5387 (0.2524)	MaskDICELoss 0.8700 (0.8476)
Epoch: [0][ 19/500]	Time 11.365 (11.365)	Loss 0.9766 (3.0848)	CeLoss 0.9766 (1.3070)	SegCLSLoss 0.0000 (0.6855)	KLLoss 0.0000 (0.3619)	MaskLoss 0.0000 (0.9720)	MaskBCELoss 0.0000 (0.3224)	MaskDICELoss 0.0000 (0.6496)
[2025-03-02 15:21:37,862] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[0.00011099999999999999], mom=[(0.9, 0.95)]
[2025-03-02 15:21:37,870] [INFO] [timer.py:215:stop] epoch=0/micro_step=200/global_step=20, RunningAvgSamplesPerSec=0.9546784680105346, CurrSamplesPerSec=1.2553961810826526, MemAllocated=31.24GB, MaxMemAllocated=36.33GB
Epoch: [0][ 20/500]	Time  7.968 ( 7.968)	Loss 4.5101 (1.8712)	CeLoss 1.1641 (1.1184)	SegCLSLoss 0.9688 (0.2945)	KLLoss 0.5898 (0.1629)	MaskLoss 1.7325 (0.4152)	MaskBCELoss 0.7941 (0.1258)	MaskDICELoss 0.9385 (0.2894)
Epoch: [0][ 21/500]	Time 10.614 (10.614)	Loss 3.5046 (3.2888)	CeLoss 1.1641 (1.1426)	SegCLSLoss 0.9648 (0.7910)	KLLoss 0.4746 (0.4256)	MaskLoss 1.2937 (1.1256)	MaskBCELoss 0.4113 (0.4143)	MaskDICELoss 0.8824 (0.7113)
Epoch: [0][ 22/500]	Time 10.165 (10.165)	Loss 4.2167 (2.9962)	CeLoss 0.7461 (1.0297)	SegCLSLoss 0.9453 (0.7828)	KLLoss 0.6328 (0.4127)	MaskLoss 1.5525 (1.0428)	MaskBCELoss 0.8300 (0.3399)	MaskDICELoss 0.7225 (0.7029)
Epoch: [0][ 23/500]	Time 11.445 (11.445)	Loss 1.0078 (2.8060)	CeLoss 1.0078 (1.0625)	SegCLSLoss 0.0000 (0.7824)	KLLoss 0.0000 (0.3912)	MaskLoss 0.0000 (0.9976)	MaskBCELoss 0.0000 (0.2471)	MaskDICELoss 0.0000 (0.7505)
Epoch: [0][ 24/500]	Time  9.572 ( 9.572)	Loss 0.9492 (2.9298)	CeLoss 0.9492 (1.0504)	SegCLSLoss 0.0000 (0.7730)	KLLoss 0.0000 (0.3783)	MaskLoss 0.0000 (1.0430)	MaskBCELoss 0.0000 (0.3319)	MaskDICELoss 0.0000 (0.7111)
Epoch: [0][ 25/500]	Time  9.980 ( 9.980)	Loss 2.8039 (2.5257)	CeLoss 0.8203 (0.9984)	SegCLSLoss 0.9609 (0.6695)	KLLoss 0.3848 (0.3230)	MaskLoss 1.2184 (0.8743)	MaskBCELoss 0.3261 (0.2426)	MaskDICELoss 0.8923 (0.6317)
Epoch: [0][ 26/500]	Time  9.954 ( 9.954)	Loss 4.2477 (2.4283)	CeLoss 0.9336 (1.0668)	SegCLSLoss 0.9609 (0.5719)	KLLoss 0.6094 (0.2928)	MaskLoss 1.5688 (0.7631)	MaskBCELoss 0.7715 (0.2181)	MaskDICELoss 0.7974 (0.5450)
Epoch: [0][ 27/500]	Time 11.056 (11.056)	Loss 4.1501 (2.6688)	CeLoss 0.9688 (0.9340)	SegCLSLoss 0.9336 (0.7613)	KLLoss 0.5781 (0.4014)	MaskLoss 1.6208 (0.9782)	MaskBCELoss 0.7338 (0.2384)	MaskDICELoss 0.8869 (0.7398)
Epoch: [0][ 28/500]	Time  9.579 ( 9.579)	Loss 2.2537 (2.3772)	CeLoss 0.4922 (0.8375)	SegCLSLoss 0.9375 (0.7582)	KLLoss 0.3574 (0.3389)	MaskLoss 1.1656 (0.9618)	MaskBCELoss 0.2449 (0.2034)	MaskDICELoss 0.9207 (0.7584)
Epoch: [0][ 29/500]	Time 10.757 (10.757)	Loss 3.3992 (2.5964)	CeLoss 0.6562 (0.8035)	SegCLSLoss 0.9492 (0.7438)	KLLoss 0.5078 (0.4000)	MaskLoss 1.5020 (1.0245)	MaskBCELoss 0.5751 (0.2721)	MaskDICELoss 0.9268 (0.7525)
[2025-03-02 15:23:21,997] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[0.00017099999999999998], mom=[(0.9, 0.95)]
[2025-03-02 15:23:22,005] [INFO] [timer.py:215:stop] epoch=0/micro_step=300/global_step=30, RunningAvgSamplesPerSec=0.9567214144687753, CurrSamplesPerSec=0.9081976588560544, MemAllocated=31.25GB, MaxMemAllocated=36.33GB
Epoch: [0][ 30/500]	Time 11.012 (11.012)	Loss 2.9229 (2.6581)	CeLoss 1.0312 (0.8879)	SegCLSLoss 0.8984 (0.8207)	KLLoss 0.4414 (0.4334)	MaskLoss 1.1786 (1.0704)	MaskBCELoss 0.2263 (0.2029)	MaskDICELoss 0.9523 (0.8675)
Epoch: [0][ 31/500]	Time 11.100 (11.100)	Loss 5.4489 (2.8780)	CeLoss 1.4531 (0.7871)	SegCLSLoss 0.9023 (0.8086)	KLLoss 0.5742 (0.4070)	MaskLoss 2.0201 (1.2253)	MaskBCELoss 1.1497 (0.3939)	MaskDICELoss 0.8704 (0.8314)
Epoch: [0][ 32/500]	Time  7.959 ( 7.959)	Loss 2.5532 (2.1540)	CeLoss 0.6445 (0.7527)	SegCLSLoss 0.8633 (0.6082)	KLLoss 0.4062 (0.3297)	MaskLoss 1.2209 (0.8496)	MaskBCELoss 0.2844 (0.1850)	MaskDICELoss 0.9365 (0.6646)
Epoch: [0][ 33/500]	Time  9.361 ( 9.361)	Loss 3.1788 (2.2379)	CeLoss 0.8984 (0.8672)	SegCLSLoss 0.8789 (0.5887)	KLLoss 0.5391 (0.3113)	MaskLoss 1.2482 (0.8416)	MaskBCELoss 0.3409 (0.1956)	MaskDICELoss 0.9073 (0.6459)
Epoch: [0][ 34/500]	Time 12.436 (12.436)	Loss 0.8945 (2.3507)	CeLoss 0.8945 (0.6891)	SegCLSLoss 0.0000 (0.5789)	KLLoss 0.0000 (0.3074)	MaskLoss 0.0000 (0.9671)	MaskBCELoss 0.0000 (0.3469)	MaskDICELoss 0.0000 (0.6202)
Epoch: [0][ 35/500]	Time 11.089 (11.089)	Loss 1.6875 (2.0074)	CeLoss 1.6875 (0.7725)	SegCLSLoss 0.0000 (0.4746)	KLLoss 0.0000 (0.2619)	MaskLoss 0.0000 (0.7706)	MaskBCELoss 0.0000 (0.2092)	MaskDICELoss 0.0000 (0.5614)
Epoch: [0][ 36/500]	Time 10.856 (10.856)	Loss 2.0958 (1.8358)	CeLoss 0.5234 (0.6002)	SegCLSLoss 0.7773 (0.5328)	KLLoss 0.4258 (0.2840)	MaskLoss 1.1107 (0.8204)	MaskBCELoss 0.1153 (0.1677)	MaskDICELoss 0.9954 (0.6527)
Epoch: [0][ 37/500]	Time 10.895 (10.895)	Loss 0.5195 (2.0867)	CeLoss 0.5195 (0.6746)	SegCLSLoss 0.0000 (0.5074)	KLLoss 0.0000 (0.3283)	MaskLoss 0.0000 (0.8646)	MaskBCELoss 0.0000 (0.2195)	MaskDICELoss 0.0000 (0.6451)
Epoch: [0][ 38/500]	Time 10.852 (10.852)	Loss 2.5908 (2.1068)	CeLoss 0.6328 (0.6672)	SegCLSLoss 0.7109 (0.5547)	KLLoss 0.5039 (0.3389)	MaskLoss 1.1612 (0.9421)	MaskBCELoss 0.2498 (0.2056)	MaskDICELoss 0.9113 (0.7364)
Epoch: [0][ 39/500]	Time  8.710 ( 8.710)	Loss 2.0212 (1.8325)	CeLoss 0.4395 (0.6184)	SegCLSLoss 0.6602 (0.4566)	KLLoss 0.4121 (0.3084)	MaskLoss 1.1348 (0.8116)	MaskBCELoss 0.1652 (0.1510)	MaskDICELoss 0.9696 (0.6606)
[2025-03-02 15:25:06,842] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[0.00023099999999999998], mom=[(0.9, 0.95)]
[2025-03-02 15:25:06,852] [INFO] [timer.py:215:stop] epoch=0/micro_step=400/global_step=40, RunningAvgSamplesPerSec=0.9559756535401579, CurrSamplesPerSec=0.8628750616610593, MemAllocated=31.11GB, MaxMemAllocated=36.33GB
Epoch: [0][ 40/500]	Time 11.591 (11.591)	Loss 2.3482 (1.8716)	CeLoss 0.5703 (0.7066)	SegCLSLoss 0.5859 (0.4203)	KLLoss 0.3672 (0.2842)	MaskLoss 1.2218 (0.8177)	MaskBCELoss 0.3328 (0.1610)	MaskDICELoss 0.8890 (0.6567)
Epoch: [0][ 41/500]	Time 12.051 (12.051)	Loss 1.6526 (1.7555)	CeLoss 0.4199 (0.5027)	SegCLSLoss 0.5664 (0.4410)	KLLoss 0.4023 (0.3244)	MaskLoss 1.0202 (0.9203)	MaskBCELoss 0.0225 (0.1538)	MaskDICELoss 0.9977 (0.7666)
Epoch: [0][ 42/500]	Time 10.207 (10.207)	Loss 3.5788 (1.8401)	CeLoss 0.5156 (0.5764)	SegCLSLoss 0.5195 (0.3453)	KLLoss 0.4395 (0.2977)	MaskLoss 1.8706 (0.8873)	MaskBCELoss 0.9134 (0.2145)	MaskDICELoss 0.9572 (0.6727)
Epoch: [0][ 43/500]	Time 10.588 (10.588)	Loss 1.8290 (2.0139)	CeLoss 0.4062 (0.4705)	SegCLSLoss 0.4141 (0.3861)	KLLoss 0.3652 (0.3414)	MaskLoss 1.1693 (1.1474)	MaskBCELoss 0.1939 (0.2907)	MaskDICELoss 0.9755 (0.8568)
Epoch: [0][ 44/500]	Time 10.305 (10.305)	Loss 2.5918 (2.3221)	CeLoss 0.4766 (0.5102)	SegCLSLoss 0.3516 (0.3326)	KLLoss 0.4336 (0.3654)	MaskLoss 1.4591 (1.2768)	MaskBCELoss 0.4896 (0.4140)	MaskDICELoss 0.9695 (0.8628)
Epoch: [0][ 45/500]	Time 10.393 (10.393)	Loss 0.8320 (2.0995)	CeLoss 0.8320 (0.4479)	SegCLSLoss 0.0000 (0.2939)	KLLoss 0.0000 (0.3514)	MaskLoss 0.0000 (1.2096)	MaskBCELoss 0.0000 (0.3581)	MaskDICELoss 0.0000 (0.8514)
Epoch: [0][ 46/500]	Time 11.905 (11.905)	Loss 0.9180 (1.9397)	CeLoss 0.9180 (0.5484)	SegCLSLoss 0.0000 (0.2061)	KLLoss 0.0000 (0.3193)	MaskLoss 0.0000 (1.0108)	MaskBCELoss 0.0000 (0.2886)	MaskDICELoss 0.0000 (0.7222)
Epoch: [0][ 47/500]	Time 10.189 (10.189)	Loss 0.9414 (1.4389)	CeLoss 0.9414 (0.5318)	SegCLSLoss 0.0000 (0.1441)	KLLoss 0.0000 (0.2168)	MaskLoss 0.0000 (0.7334)	MaskBCELoss 0.0000 (0.1723)	MaskDICELoss 0.0000 (0.5611)
Epoch: [0][ 48/500]	Time 12.197 (12.197)	Loss 1.7220 (1.6958)	CeLoss 0.3086 (0.4859)	SegCLSLoss 0.1719 (0.1266)	KLLoss 0.3945 (0.2701)	MaskLoss 1.2138 (0.9057)	MaskBCELoss 0.2174 (0.2719)	MaskDICELoss 0.9964 (0.6338)
Epoch: [0][ 49/500]	Time 10.966 (10.966)	Loss 2.0706 (1.7378)	CeLoss 0.4219 (0.6957)	SegCLSLoss 0.1660 (0.0971)	KLLoss 0.3691 (0.2307)	MaskLoss 1.2725 (0.7799)	MaskBCELoss 0.3690 (0.2391)	MaskDICELoss 0.9035 (0.5407)
[2025-03-02 15:26:56,809] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[0.00029099999999999997], mom=[(0.9, 0.95)]
[2025-03-02 15:26:56,817] [INFO] [timer.py:215:stop] epoch=0/micro_step=500/global_step=50, RunningAvgSamplesPerSec=0.9459031073288025, CurrSamplesPerSec=0.8960290864910675, MemAllocated=31.27GB, MaxMemAllocated=36.84GB
Epoch: [0][ 50/500]	Time 11.162 (11.162)	Loss 2.1186 (2.0427)	CeLoss 0.4707 (0.3576)	SegCLSLoss 0.1621 (0.1486)	KLLoss 0.3516 (0.3746)	MaskLoss 1.2466 (1.2737)	MaskBCELoss 0.3915 (0.3860)	MaskDICELoss 0.8551 (0.8878)
Epoch: [0][ 51/500]	Time  9.546 ( 9.546)	Loss 1.1953 (1.6054)	CeLoss 1.1953 (0.5567)	SegCLSLoss 0.0000 (0.1072)	KLLoss 0.0000 (0.2482)	MaskLoss 0.0000 (0.8628)	MaskBCELoss 0.0000 (0.2173)	MaskDICELoss 0.0000 (0.6455)
Epoch: [0][ 52/500]	Time 10.632 (10.632)	Loss 1.8637 (2.0338)	CeLoss 0.3848 (0.4982)	SegCLSLoss 0.1387 (0.1135)	KLLoss 0.3418 (0.3277)	MaskLoss 1.2045 (1.1659)	MaskBCELoss 0.3172 (0.3713)	MaskDICELoss 0.8873 (0.7945)
Epoch: [0][ 53/500]	Time 10.897 (10.897)	Loss 1.4479 (1.6926)	CeLoss 0.3223 (0.4478)	SegCLSLoss 0.1406 (0.1255)	KLLoss 0.3438 (0.3189)	MaskLoss 1.1239 (1.0888)	MaskBCELoss 0.1355 (0.2295)	MaskDICELoss 0.9884 (0.8593)
Epoch: [0][ 54/500]	Time  9.889 ( 9.889)	Loss 2.4817 (1.7137)	CeLoss 0.3066 (0.6898)	SegCLSLoss 0.1050 (0.0816)	KLLoss 0.3516 (0.2119)	MaskLoss 1.5614 (0.8034)	MaskBCELoss 0.6648 (0.2523)	MaskDICELoss 0.8967 (0.5511)
Epoch: [0][ 55/500]	Time 10.412 (10.412)	Loss 1.4181 (1.6131)	CeLoss 0.4141 (0.4795)	SegCLSLoss 0.1250 (0.0969)	KLLoss 0.3340 (0.2770)	MaskLoss 1.0806 (0.9937)	MaskBCELoss 0.0892 (0.2274)	MaskDICELoss 0.9914 (0.7662)
Epoch: [0][ 56/500]	Time 10.850 (10.850)	Loss 1.2676 (1.5307)	CeLoss 0.2930 (0.5822)	SegCLSLoss 0.0918 (0.0811)	KLLoss 0.3477 (0.2391)	MaskLoss 1.0639 (0.8484)	MaskBCELoss 0.0684 (0.1816)	MaskDICELoss 0.9956 (0.6668)
Epoch: [0][ 57/500]	Time 10.831 (10.831)	Loss 1.7867 (1.7025)	CeLoss 0.2656 (0.3400)	SegCLSLoss 0.1030 (0.0967)	KLLoss 0.3320 (0.3041)	MaskLoss 1.2989 (1.1602)	MaskBCELoss 0.3560 (0.3105)	MaskDICELoss 0.9429 (0.8497)
Epoch: [0][ 58/500]	Time 12.911 (12.911)	Loss 1.3945 (1.4843)	CeLoss 0.2275 (0.2785)	SegCLSLoss 0.1196 (0.1005)	KLLoss 0.3223 (0.2967)	MaskLoss 1.1337 (1.1001)	MaskBCELoss 0.1849 (0.2383)	MaskDICELoss 0.9488 (0.8618)
Epoch: [0][ 59/500]	Time 11.412 (11.412)	Loss 1.1815 (1.4894)	CeLoss 0.2559 (0.3806)	SegCLSLoss 0.1270 (0.0921)	KLLoss 0.3125 (0.2629)	MaskLoss 1.0629 (0.9992)	MaskBCELoss 0.0683 (0.2301)	MaskDICELoss 0.9946 (0.7691)
[2025-03-02 15:28:45,770] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[0.00029895918367346935], mom=[(0.9, 0.95)]
[2025-03-02 15:28:45,776] [INFO] [timer.py:215:stop] epoch=0/micro_step=600/global_step=60, RunningAvgSamplesPerSec=0.9409518440501752, CurrSamplesPerSec=0.8636950557865588, MemAllocated=30.95GB, MaxMemAllocated=36.84GB
Epoch: [0][ 60/500]	Time 11.580 (11.580)	Loss 1.7859 (1.6536)	CeLoss 0.2500 (0.4047)	SegCLSLoss 0.1250 (0.1023)	KLLoss 0.3223 (0.2898)	MaskLoss 1.2957 (1.1164)	MaskBCELoss 0.3680 (0.2663)	MaskDICELoss 0.9276 (0.8501)
Epoch: [0][ 61/500]	Time 10.841 (10.841)	Loss 2.8335 (1.4392)	CeLoss 0.1621 (0.5296)	SegCLSLoss 0.1157 (0.0580)	KLLoss 0.3438 (0.1971)	MaskLoss 1.8160 (0.7827)	MaskBCELoss 0.9188 (0.2153)	MaskDICELoss 0.8973 (0.5674)
Epoch: [0][ 62/500]	Time 10.910 (10.910)	Loss 1.6579 (1.3625)	CeLoss 0.2480 (0.4330)	SegCLSLoss 0.1260 (0.0725)	KLLoss 0.3086 (0.2238)	MaskLoss 1.2289 (0.8474)	MaskBCELoss 0.3186 (0.1894)	MaskDICELoss 0.9103 (0.6580)
Epoch: [0][ 63/500]	Time 10.952 (10.952)	Loss 1.3890 (1.5640)	CeLoss 0.2402 (0.5614)	SegCLSLoss 0.0933 (0.0699)	KLLoss 0.3164 (0.2570)	MaskLoss 1.1532 (0.9491)	MaskBCELoss 0.1852 (0.1891)	MaskDICELoss 0.9680 (0.7600)
Epoch: [0][ 64/500]	Time 10.280 (10.280)	Loss 1.4747 (1.5106)	CeLoss 0.2754 (0.3757)	SegCLSLoss 0.1025 (0.0785)	KLLoss 0.3086 (0.2518)	MaskLoss 1.1708 (1.0059)	MaskBCELoss 0.2170 (0.2582)	MaskDICELoss 0.9539 (0.7477)
Epoch: [0][ 65/500]	Time  9.743 ( 9.743)	Loss 1.6804 (1.2719)	CeLoss 0.3320 (0.6156)	SegCLSLoss 0.0806 (0.0504)	KLLoss 0.3164 (0.1564)	MaskLoss 1.1909 (0.5982)	MaskBCELoss 0.2913 (0.1358)	MaskDICELoss 0.8996 (0.4624)
Epoch: [0][ 66/500]	Time 11.479 (11.479)	Loss 1.2634 (1.6367)	CeLoss 0.2637 (0.2446)	SegCLSLoss 0.1348 (0.1198)	KLLoss 0.3047 (0.3094)	MaskLoss 1.1027 (1.2575)	MaskBCELoss 0.1114 (0.3094)	MaskDICELoss 0.9913 (0.9481)
Epoch: [0][ 67/500]	Time 10.423 (10.423)	Loss 1.4324 (1.3689)	CeLoss 0.2969 (0.5936)	SegCLSLoss 0.1211 (0.0624)	KLLoss 0.3027 (0.1842)	MaskLoss 1.1591 (0.7340)	MaskBCELoss 0.1872 (0.1594)	MaskDICELoss 0.9719 (0.5746)
Epoch: [0][ 68/500]	Time  8.808 ( 8.808)	Loss 1.2500 (1.5808)	CeLoss 1.2500 (0.5797)	SegCLSLoss 0.0000 (0.0717)	KLLoss 0.0000 (0.2160)	MaskLoss 0.0000 (0.8793)	MaskBCELoss 0.0000 (0.2345)	MaskDICELoss 0.0000 (0.6448)
Epoch: [0][ 69/500]	Time  8.858 ( 8.858)	Loss 1.2644 (1.4346)	CeLoss 0.3184 (0.5157)	SegCLSLoss 0.0742 (0.0701)	KLLoss 0.3066 (0.2127)	MaskLoss 1.0978 (0.8564)	MaskBCELoss 0.0979 (0.1960)	MaskDICELoss 0.9999 (0.6604)
[2025-03-02 15:30:27,181] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[0.000297734693877551], mom=[(0.9, 0.95)]
[2025-03-02 15:30:27,188] [INFO] [timer.py:215:stop] epoch=0/micro_step=700/global_step=70, RunningAvgSamplesPerSec=0.9473443336886473, CurrSamplesPerSec=1.0969153883694756, MemAllocated=30.8GB, MaxMemAllocated=36.84GB
Epoch: [0][ 70/500]	Time  9.118 ( 9.118)	Loss 2.7682 (1.8258)	CeLoss 0.3438 (0.6637)	SegCLSLoss 0.0742 (0.0526)	KLLoss 0.3047 (0.1816)	MaskLoss 1.7108 (0.8954)	MaskBCELoss 0.8447 (0.3593)	MaskDICELoss 0.8661 (0.5361)
Epoch: [0][ 71/500]	Time 11.656 (11.656)	Loss 1.1755 (1.6502)	CeLoss 0.2256 (0.3212)	SegCLSLoss 0.0913 (0.0816)	KLLoss 0.3008 (0.3010)	MaskLoss 1.0982 (1.2524)	MaskBCELoss 0.1004 (0.2956)	MaskDICELoss 0.9978 (0.9567)
Epoch: [0][ 72/500]	Time 10.898 (10.898)	Loss 1.2181 (1.4861)	CeLoss 0.1758 (0.4423)	SegCLSLoss 0.0679 (0.0661)	KLLoss 0.3105 (0.2412)	MaskLoss 1.1269 (0.9767)	MaskBCELoss 0.1439 (0.2267)	MaskDICELoss 0.9830 (0.7500)
Epoch: [0][ 73/500]	Time 10.137 (10.137)	Loss 1.2561 (1.3179)	CeLoss 0.2109 (0.3091)	SegCLSLoss 0.0791 (0.0733)	KLLoss 0.3008 (0.2398)	MaskLoss 1.1204 (0.9594)	MaskBCELoss 0.1539 (0.2087)	MaskDICELoss 0.9665 (0.7507)
Epoch: [0][ 74/500]	Time 10.238 (10.238)	Loss 1.5234 (1.4899)	CeLoss 1.5234 (0.6302)	SegCLSLoss 0.0000 (0.0367)	KLLoss 0.0000 (0.1830)	MaskLoss 0.0000 (0.7687)	MaskBCELoss 0.0000 (0.2096)	MaskDICELoss 0.0000 (0.5591)
Epoch: [0][ 75/500]	Time  9.490 ( 9.490)	Loss 1.6027 (1.8225)	CeLoss 0.2236 (0.5307)	SegCLSLoss 0.1592 (0.0689)	KLLoss 0.2949 (0.2408)	MaskLoss 1.2003 (1.0832)	MaskBCELoss 0.3116 (0.3512)	MaskDICELoss 0.8887 (0.7319)
Epoch: [0][ 76/500]	Time  9.434 ( 9.434)	Loss 2.0154 (1.2848)	CeLoss 0.3242 (0.6500)	SegCLSLoss 0.0500 (0.0366)	KLLoss 0.3047 (0.1494)	MaskLoss 1.3764 (0.6110)	MaskBCELoss 0.4825 (0.1349)	MaskDICELoss 0.8938 (0.4761)
Epoch: [0][ 77/500]	Time  9.373 ( 9.373)	Loss 1.4731 (1.2270)	CeLoss 0.3164 (0.6227)	SegCLSLoss 0.0588 (0.0333)	KLLoss 0.3066 (0.1504)	MaskLoss 1.1665 (0.5960)	MaskBCELoss 0.2081 (0.1194)	MaskDICELoss 0.9583 (0.4766)
Epoch: [0][ 78/500]	Time 10.674 (10.674)	Loss 1.6825 (1.5302)	CeLoss 0.2910 (0.3266)	SegCLSLoss 0.0540 (0.0720)	KLLoss 0.3008 (0.2684)	MaskLoss 1.2736 (1.1065)	MaskBCELoss 0.3333 (0.2738)	MaskDICELoss 0.9403 (0.8326)
Epoch: [0][ 79/500]	Time 12.331 (12.331)	Loss 1.2607 (1.3429)	CeLoss 0.3496 (0.3982)	SegCLSLoss 0.0679 (0.0554)	KLLoss 0.2930 (0.2365)	MaskLoss 1.0940 (0.9504)	MaskBCELoss 0.0961 (0.1836)	MaskDICELoss 0.9979 (0.7668)
[2025-03-02 15:32:10,887] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[0.0002965102040816326], mom=[(0.9, 0.95)]
[2025-03-02 15:32:10,895] [INFO] [timer.py:215:stop] epoch=0/micro_step=800/global_step=80, RunningAvgSamplesPerSec=0.9494963292020103, CurrSamplesPerSec=1.0556331982292924, MemAllocated=31.23GB, MaxMemAllocated=36.84GB
Epoch: [0][ 80/500]	Time  9.475 ( 9.475)	Loss 1.7740 (1.2876)	CeLoss 0.3379 (0.8304)	SegCLSLoss 0.0493 (0.0256)	KLLoss 0.2930 (0.1176)	MaskLoss 1.2967 (0.4701)	MaskBCELoss 0.3658 (0.0853)	MaskDICELoss 0.9308 (0.3848)
Epoch: [0][ 81/500]	Time 10.765 (10.765)	Loss 1.2644 (1.1342)	CeLoss 0.2002 (0.4221)	SegCLSLoss 0.0693 (0.0493)	KLLoss 0.2910 (0.2045)	MaskLoss 1.1259 (0.7849)	MaskBCELoss 0.1765 (0.1053)	MaskDICELoss 0.9494 (0.6796)
Epoch: [0][ 82/500]	Time 11.488 (11.488)	Loss 1.5729 (1.2371)	CeLoss 0.3418 (0.2588)	SegCLSLoss 0.0859 (0.0639)	KLLoss 0.2930 (0.2648)	MaskLoss 1.1767 (1.0310)	MaskBCELoss 0.2561 (0.1653)	MaskDICELoss 0.9207 (0.8657)
Epoch: [0][ 83/500]	Time 10.446 (10.446)	Loss 2.3341 (1.5064)	CeLoss 0.2969 (0.5697)	SegCLSLoss 0.0688 (0.0465)	KLLoss 0.2930 (0.2057)	MaskLoss 1.5432 (0.8848)	MaskBCELoss 0.6641 (0.2173)	MaskDICELoss 0.8790 (0.6674)
Epoch: [0][ 84/500]	Time 10.978 (10.978)	Loss 2.3145 (1.4552)	CeLoss 0.3066 (0.4478)	SegCLSLoss 0.0491 (0.0583)	KLLoss 0.2949 (0.2330)	MaskLoss 1.5952 (0.9891)	MaskBCELoss 0.6490 (0.2175)	MaskDICELoss 0.9462 (0.7716)
Epoch: [0][ 85/500]	Time 11.353 (11.353)	Loss 1.0431 (1.4234)	CeLoss 0.1631 (0.4741)	SegCLSLoss 0.0820 (0.0618)	KLLoss 0.2891 (0.2023)	MaskLoss 1.0681 (0.8996)	MaskBCELoss 0.0816 (0.2230)	MaskDICELoss 0.9865 (0.6767)
Epoch: [0][ 86/500]	Time  9.156 ( 9.156)	Loss 1.4922 (2.1728)	CeLoss 1.4922 (0.4956)	SegCLSLoss 0.0000 (0.0533)	KLLoss 0.0000 (0.2350)	MaskLoss 0.0000 (1.2973)	MaskBCELoss 0.0000 (0.5531)	MaskDICELoss 0.0000 (0.7442)
Epoch: [0][ 87/500]	Time 11.101 (11.101)	Loss 1.4890 (1.3389)	CeLoss 0.2129 (0.5252)	SegCLSLoss 0.0889 (0.0574)	KLLoss 0.2891 (0.2037)	MaskLoss 1.2197 (0.8172)	MaskBCELoss 0.2795 (0.1557)	MaskDICELoss 0.9402 (0.6615)
Epoch: [0][ 88/500]	Time  9.873 ( 9.873)	Loss 1.6879 (1.3772)	CeLoss 0.2402 (0.3571)	SegCLSLoss 0.0513 (0.0587)	KLLoss 0.3008 (0.2340)	MaskLoss 1.2925 (0.9762)	MaskBCELoss 0.3639 (0.2237)	MaskDICELoss 0.9285 (0.7524)
Epoch: [0][ 89/500]	Time 10.104 (10.104)	Loss 1.6961 (1.5103)	CeLoss 0.3184 (0.4533)	SegCLSLoss 0.0469 (0.0369)	KLLoss 0.2930 (0.2055)	MaskLoss 1.2616 (0.9118)	MaskBCELoss 0.3370 (0.2823)	MaskDICELoss 0.9247 (0.6295)
[2025-03-02 15:33:56,506] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[0.0002952857142857143], mom=[(0.9, 0.95)]
[2025-03-02 15:33:56,508] [INFO] [timer.py:215:stop] epoch=0/micro_step=900/global_step=90, RunningAvgSamplesPerSec=0.9492092940775944, CurrSamplesPerSec=0.9662514267531987, MemAllocated=31.58GB, MaxMemAllocated=36.84GB
Epoch: [0][ 90/500]	Time 10.351 (10.351)	Loss 1.4657 (1.5729)	CeLoss 0.3281 (0.5187)	SegCLSLoss 0.0664 (0.0451)	KLLoss 0.2910 (0.2332)	MaskLoss 1.1764 (0.9871)	MaskBCELoss 0.2119 (0.2455)	MaskDICELoss 0.9644 (0.7417)
Epoch: [0][ 91/500]	Time 11.245 (11.245)	Loss 2.4662 (1.4783)	CeLoss 0.3262 (0.4441)	SegCLSLoss 0.0415 (0.0489)	KLLoss 0.2930 (0.2027)	MaskLoss 1.5351 (0.8969)	MaskBCELoss 0.7258 (0.2709)	MaskDICELoss 0.8093 (0.6260)
Epoch: [0][ 92/500]	Time 10.570 (10.570)	Loss 1.4380 (1.6148)	CeLoss 0.2695 (0.4794)	SegCLSLoss 0.0630 (0.0547)	KLLoss 0.2891 (0.2299)	MaskLoss 1.1653 (1.0076)	MaskBCELoss 0.2329 (0.2880)	MaskDICELoss 0.9324 (0.7195)
Epoch: [0][ 93/500]	Time  8.395 ( 8.395)	Loss 1.5376 (1.4694)	CeLoss 0.2754 (0.6403)	SegCLSLoss 0.0439 (0.0475)	KLLoss 0.2891 (0.1721)	MaskLoss 1.2127 (0.7523)	MaskBCELoss 0.2849 (0.2031)	MaskDICELoss 0.9278 (0.5492)
Epoch: [0][ 94/500]	Time 10.033 (10.033)	Loss 1.3027 (1.4662)	CeLoss 0.2295 (0.4056)	SegCLSLoss 0.0544 (0.0531)	KLLoss 0.2852 (0.2303)	MaskLoss 1.1365 (0.9866)	MaskBCELoss 0.1899 (0.2500)	MaskDICELoss 0.9466 (0.7366)
Epoch: [0][ 95/500]	Time  8.858 ( 8.858)	Loss 1.6351 (1.4954)	CeLoss 0.2598 (0.6328)	SegCLSLoss 0.0442 (0.0387)	KLLoss 0.2930 (0.2020)	MaskLoss 1.2598 (0.8490)	MaskBCELoss 0.3379 (0.1866)	MaskDICELoss 0.9219 (0.6623)
Epoch: [0][ 96/500]	Time 11.963 (11.963)	Loss 0.6836 (1.0646)	CeLoss 0.6836 (0.3191)	SegCLSLoss 0.0000 (0.0454)	KLLoss 0.0000 (0.2016)	MaskLoss 0.0000 (0.8051)	MaskBCELoss 0.0000 (0.1262)	MaskDICELoss 0.0000 (0.6789)
Epoch: [0][ 97/500]	Time 11.076 (11.076)	Loss 1.4338 (1.3097)	CeLoss 0.2871 (0.3326)	SegCLSLoss 0.0320 (0.0381)	KLLoss 0.2852 (0.2010)	MaskLoss 1.1705 (0.8973)	MaskBCELoss 0.2325 (0.2452)	MaskDICELoss 0.9380 (0.6522)
Epoch: [0][ 98/500]	Time 11.253 (11.253)	Loss 1.0613 (1.6727)	CeLoss 0.2129 (0.2560)	SegCLSLoss 0.0479 (0.0623)	KLLoss 0.2930 (0.2865)	MaskLoss 1.0638 (1.2950)	MaskBCELoss 0.0708 (0.3595)	MaskDICELoss 0.9930 (0.9355)
Epoch: [0][ 99/500]	Time 10.238 (10.238)	Loss 1.6641 (1.5637)	CeLoss 1.6641 (0.5880)	SegCLSLoss 0.0000 (0.0347)	KLLoss 0.0000 (0.2002)	MaskLoss 0.0000 (0.9147)	MaskBCELoss 0.0000 (0.2457)	MaskDICELoss 0.0000 (0.6690)
[2025-03-02 15:35:40,531] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[0.00029406122448979587], mom=[(0.9, 0.95)]
[2025-03-02 15:35:40,547] [INFO] [timer.py:215:stop] epoch=0/micro_step=1000/global_step=100, RunningAvgSamplesPerSec=0.9504321686338488, CurrSamplesPerSec=0.9610989592309328, MemAllocated=31.29GB, MaxMemAllocated=36.84GB
Epoch: [0][100/500]	Time 10.406 (10.406)	Loss 1.2941 (1.2267)	CeLoss 0.1514 (0.3471)	SegCLSLoss 0.1357 (0.0563)	KLLoss 0.2852 (0.2002)	MaskLoss 1.1409 (0.8467)	MaskBCELoss 0.2058 (0.1929)	MaskDICELoss 0.9352 (0.6538)
Epoch: [0][101/500]	Time 10.326 (10.326)	Loss 1.7338 (1.5225)	CeLoss 0.2773 (0.4654)	SegCLSLoss 0.0835 (0.0468)	KLLoss 0.2832 (0.2277)	MaskLoss 1.2852 (1.0079)	MaskBCELoss 0.3802 (0.2513)	MaskDICELoss 0.9049 (0.7566)
Epoch: [0][102/500]	Time 13.200 (13.200)	Loss 1.2218 (1.4002)	CeLoss 0.1885 (0.2684)	SegCLSLoss 0.0352 (0.0475)	KLLoss 0.2891 (0.2855)	MaskLoss 1.1391 (1.1781)	MaskBCELoss 0.1699 (0.2201)	MaskDICELoss 0.9693 (0.9580)
Epoch: [0][103/500]	Time  8.049 ( 8.049)	Loss 1.9207 (1.2444)	CeLoss 0.2461 (0.6113)	SegCLSLoss 0.0515 (0.0438)	KLLoss 0.2852 (0.1418)	MaskLoss 1.3724 (0.6035)	MaskBCELoss 0.4946 (0.1405)	MaskDICELoss 0.8778 (0.4630)
Epoch: [0][104/500]	Time  8.971 ( 8.971)	Loss 1.2578 (1.7433)	CeLoss 1.2578 (0.6538)	SegCLSLoss 0.0000 (0.0240)	KLLoss 0.0000 (0.1717)	MaskLoss 0.0000 (0.8909)	MaskBCELoss 0.0000 (0.3394)	MaskDICELoss 0.0000 (0.5515)
Epoch: [0][105/500]	Time 12.509 (12.509)	Loss 1.3690 (1.3544)	CeLoss 0.1592 (0.3396)	SegCLSLoss 0.1006 (0.0511)	KLLoss 0.2832 (0.2566)	MaskLoss 1.1704 (1.0506)	MaskBCELoss 0.2519 (0.1956)	MaskDICELoss 0.9185 (0.8550)
Epoch: [0][106/500]	Time 12.039 (12.039)	Loss 1.5469 (1.5701)	CeLoss 0.2471 (0.3254)	SegCLSLoss 0.0591 (0.0365)	KLLoss 0.2832 (0.2549)	MaskLoss 1.2117 (1.1414)	MaskBCELoss 0.3063 (0.3168)	MaskDICELoss 0.9054 (0.8246)
Epoch: [0][107/500]	Time 11.026 (11.026)	Loss 1.2914 (1.6530)	CeLoss 0.1553 (0.3485)	SegCLSLoss 0.0618 (0.0534)	KLLoss 0.2852 (0.2557)	MaskLoss 1.1884 (1.1722)	MaskBCELoss 0.2183 (0.3416)	MaskDICELoss 0.9701 (0.8306)
Epoch: [0][108/500]	Time 10.269 (10.269)	Loss 1.5615 (1.3857)	CeLoss 0.2021 (0.4363)	SegCLSLoss 0.0344 (0.0263)	KLLoss 0.2852 (0.1961)	MaskLoss 1.2360 (0.8895)	MaskBCELoss 0.3415 (0.2396)	MaskDICELoss 0.8945 (0.6499)
Epoch: [0][109/500]	Time 10.978 (10.978)	Loss 1.1407 (1.2459)	CeLoss 0.1758 (0.3760)	SegCLSLoss 0.0491 (0.0330)	KLLoss 0.2773 (0.2246)	MaskLoss 1.1123 (0.9321)	MaskBCELoss 0.1450 (0.1636)	MaskDICELoss 0.9673 (0.7685)
[2025-03-02 15:37:29,218] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[0.00029283673469387755], mom=[(0.9, 0.95)]
[2025-03-02 15:37:29,226] [INFO] [timer.py:215:stop] epoch=0/micro_step=1100/global_step=110, RunningAvgSamplesPerSec=0.9475554367102286, CurrSamplesPerSec=0.8840887468866007, MemAllocated=30.65GB, MaxMemAllocated=36.84GB
Epoch: [0][110/500]	Time 11.313 (11.313)	Loss 1.5489 (1.4213)	CeLoss 0.2041 (0.3777)	SegCLSLoss 0.0247 (0.0331)	KLLoss 0.2812 (0.2521)	MaskLoss 1.2408 (1.0754)	MaskBCELoss 0.3387 (0.2182)	MaskDICELoss 0.9021 (0.8572)
Epoch: [0][111/500]	Time 11.579 (11.579)	Loss 1.4809 (1.4346)	CeLoss 0.4531 (0.2689)	SegCLSLoss 0.0366 (0.0401)	KLLoss 0.2812 (0.2799)	MaskLoss 1.1396 (1.1837)	MaskBCELoss 0.1767 (0.2460)	MaskDICELoss 0.9629 (0.9377)
Epoch: [0][112/500]	Time 10.523 (10.523)	Loss 1.1797 (1.3061)	CeLoss 1.1797 (0.3271)	SegCLSLoss 0.0000 (0.0416)	KLLoss 0.0000 (0.2215)	MaskLoss 0.0000 (0.9819)	MaskBCELoss 0.0000 (0.2195)	MaskDICELoss 0.0000 (0.7625)
Epoch: [0][113/500]	Time  9.943 ( 9.943)	Loss 1.4531 (1.6196)	CeLoss 1.4531 (0.4984)	SegCLSLoss 0.0000 (0.0468)	KLLoss 0.0000 (0.1947)	MaskLoss 0.0000 (0.9633)	MaskBCELoss 0.0000 (0.3221)	MaskDICELoss 0.0000 (0.6413)
Epoch: [0][114/500]	Time  8.819 ( 8.819)	Loss 1.6124 (1.5138)	CeLoss 0.2695 (0.5851)	SegCLSLoss 0.0518 (0.0236)	KLLoss 0.2793 (0.1672)	MaskLoss 1.2465 (0.8077)	MaskBCELoss 0.3348 (0.2643)	MaskDICELoss 0.9117 (0.5434)
Epoch: [0][115/500]	Time 11.303 (11.303)	Loss 0.7891 (1.4320)	CeLoss 0.7891 (0.3467)	SegCLSLoss 0.0000 (0.0499)	KLLoss 0.0000 (0.2215)	MaskLoss 0.0000 (1.0219)	MaskBCELoss 0.0000 (0.2710)	MaskDICELoss 0.0000 (0.7508)
Epoch: [0][116/500]	Time 11.221 (11.221)	Loss 1.1075 (1.2589)	CeLoss 0.1943 (0.5294)	SegCLSLoss 0.0461 (0.0340)	KLLoss 0.2754 (0.1936)	MaskLoss 1.1128 (0.8041)	MaskBCELoss 0.1193 (0.1287)	MaskDICELoss 0.9934 (0.6754)
Epoch: [0][117/500]	Time  8.759 ( 8.759)	Loss 0.7383 (1.4772)	CeLoss 0.7383 (0.4622)	SegCLSLoss 0.0000 (0.0396)	KLLoss 0.0000 (0.1932)	MaskLoss 0.0000 (0.9189)	MaskBCELoss 0.0000 (0.2725)	MaskDICELoss 0.0000 (0.6464)
Epoch: [0][118/500]	Time  8.062 ( 8.062)	Loss 1.0963 (1.4287)	CeLoss 0.1816 (0.7996)	SegCLSLoss 0.0674 (0.0253)	KLLoss 0.2773 (0.1385)	MaskLoss 1.0988 (0.6112)	MaskBCELoss 0.1142 (0.1466)	MaskDICELoss 0.9846 (0.4646)
Epoch: [0][119/500]	Time 10.583 (10.583)	Loss 0.9922 (1.1092)	CeLoss 0.9922 (0.5774)	SegCLSLoss 0.0000 (0.0160)	KLLoss 0.0000 (0.1381)	MaskLoss 0.0000 (0.5788)	MaskBCELoss 0.0000 (0.0998)	MaskDICELoss 0.0000 (0.4790)
[2025-03-02 15:39:10,581] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[0.0002916122448979592], mom=[(0.9, 0.95)]
[2025-03-02 15:39:10,588] [INFO] [timer.py:215:stop] epoch=0/micro_step=1200/global_step=120, RunningAvgSamplesPerSec=0.9507520987329063, CurrSamplesPerSec=0.9463203385261888, MemAllocated=30.99GB, MaxMemAllocated=36.84GB
Epoch: [0][120/500]	Time 10.569 (10.569)	Loss 0.9219 (1.2634)	CeLoss 0.9219 (0.5591)	SegCLSLoss 0.0000 (0.0196)	KLLoss 0.0000 (0.1654)	MaskLoss 0.0000 (0.7218)	MaskBCELoss 0.0000 (0.1535)	MaskDICELoss 0.0000 (0.5683)
Epoch: [0][121/500]	Time 10.631 (10.631)	Loss 0.1001 (1.2882)	CeLoss 0.1001 (0.3036)	SegCLSLoss 0.0000 (0.0273)	KLLoss 0.0000 (0.1654)	MaskLoss 0.0000 (0.8282)	MaskBCELoss 0.0000 (0.2931)	MaskDICELoss 0.0000 (0.5350)
Epoch: [0][122/500]	Time 12.784 (12.784)	Loss 1.6223 (1.2806)	CeLoss 0.2432 (0.3099)	SegCLSLoss 0.0327 (0.0328)	KLLoss 0.2773 (0.2209)	MaskLoss 1.2698 (0.9702)	MaskBCELoss 0.3594 (0.2187)	MaskDICELoss 0.9104 (0.7515)
Epoch: [0][123/500]	Time  9.868 ( 9.868)	Loss 1.0156 (1.1947)	CeLoss 1.0156 (0.6988)	SegCLSLoss 0.0000 (0.0177)	KLLoss 0.0000 (0.1092)	MaskLoss 0.0000 (0.4880)	MaskBCELoss 0.0000 (0.1155)	MaskDICELoss 0.0000 (0.3725)
Epoch: [0][124/500]	Time 11.860 (11.860)	Loss 0.0820 (1.0443)	CeLoss 0.0820 (0.2941)	SegCLSLoss 0.0000 (0.0338)	KLLoss 0.0000 (0.1924)	MaskLoss 0.0000 (0.8021)	MaskBCELoss 0.0000 (0.1412)	MaskDICELoss 0.0000 (0.6610)
Epoch: [0][125/500]	Time 12.474 (12.474)	Loss 1.8457 (1.4041)	CeLoss 0.2314 (0.2655)	SegCLSLoss 0.0334 (0.0380)	KLLoss 0.2754 (0.2480)	MaskLoss 1.3184 (1.1021)	MaskBCELoss 0.4826 (0.2704)	MaskDICELoss 0.8358 (0.8317)
Epoch: [0][126/500]	Time 11.117 (11.117)	Loss 1.5913 (1.6280)	CeLoss 0.2051 (0.3063)	SegCLSLoss 0.0510 (0.0339)	KLLoss 0.2793 (0.2484)	MaskLoss 1.2310 (1.1968)	MaskBCELoss 0.3575 (0.3623)	MaskDICELoss 0.8735 (0.8346)
Epoch: [0][127/500]	Time  9.719 ( 9.719)	Loss 1.4883 (1.3229)	CeLoss 0.1816 (0.5839)	SegCLSLoss 0.0513 (0.0196)	KLLoss 0.2773 (0.1652)	MaskLoss 1.2007 (0.7383)	MaskBCELoss 0.3192 (0.1705)	MaskDICELoss 0.8815 (0.5678)
Epoch: [0][128/500]	Time 10.458 (10.458)	Loss 2.0097 (1.3518)	CeLoss 0.1943 (0.3198)	SegCLSLoss 0.0645 (0.0524)	KLLoss 0.2754 (0.2207)	MaskLoss 1.4000 (0.9767)	MaskBCELoss 0.5749 (0.2458)	MaskDICELoss 0.8251 (0.7309)
Epoch: [0][129/500]	Time 12.666 (12.666)	Loss 1.7975 (1.4080)	CeLoss 0.3164 (0.3325)	SegCLSLoss 0.0239 (0.0352)	KLLoss 0.2754 (0.2465)	MaskLoss 1.3372 (1.0931)	MaskBCELoss 0.4131 (0.2401)	MaskDICELoss 0.9241 (0.8530)
[2025-03-02 15:41:01,893] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[0.00029038775510204076], mom=[(0.9, 0.95)]
[2025-03-02 15:41:01,898] [INFO] [timer.py:215:stop] epoch=0/micro_step=1300/global_step=130, RunningAvgSamplesPerSec=0.9464527240282617, CurrSamplesPerSec=1.027661420150137, MemAllocated=30.7GB, MaxMemAllocated=36.84GB
Epoch: [0][130/500]	Time  9.734 ( 9.734)	Loss 1.0781 (1.3691)	CeLoss 1.0781 (0.5589)	SegCLSLoss 0.0000 (0.0308)	KLLoss 0.0000 (0.1898)	MaskLoss 0.0000 (0.8293)	MaskBCELoss 0.0000 (0.1751)	MaskDICELoss 0.0000 (0.6543)
Epoch: [0][131/500]	Time 13.058 (13.058)	Loss 1.5178 (1.4485)	CeLoss 0.1992 (0.2353)	SegCLSLoss 0.0500 (0.0366)	KLLoss 0.2734 (0.2721)	MaskLoss 1.2105 (1.2233)	MaskBCELoss 0.3281 (0.2781)	MaskDICELoss 0.8824 (0.9452)
Epoch: [0][132/500]	Time 10.162 (10.162)	Loss 1.1155 (1.1559)	CeLoss 0.2676 (0.5289)	SegCLSLoss 0.0276 (0.0242)	KLLoss 0.2695 (0.1895)	MaskLoss 1.0866 (0.7641)	MaskBCELoss 0.0982 (0.0840)	MaskDICELoss 0.9885 (0.6802)
Epoch: [0][133/500]	Time  8.947 ( 8.947)	Loss 2.4292 (1.4603)	CeLoss 0.2451 (0.4473)	SegCLSLoss 0.0275 (0.0339)	KLLoss 0.2754 (0.1912)	MaskLoss 1.6840 (0.9276)	MaskBCELoss 0.7623 (0.2738)	MaskDICELoss 0.9217 (0.6538)
Epoch: [0][134/500]	Time 11.872 (11.872)	Loss 1.0184 (1.3884)	CeLoss 0.2227 (0.2525)	SegCLSLoss 0.0417 (0.0334)	KLLoss 0.2695 (0.2445)	MaskLoss 1.0548 (1.1273)	MaskBCELoss 0.0693 (0.2725)	MaskDICELoss 0.9855 (0.8548)
Epoch: [0][135/500]	Time 10.541 (10.541)	Loss 1.0329 (1.3398)	CeLoss 0.2393 (0.4419)	SegCLSLoss 0.0391 (0.0184)	KLLoss 0.2715 (0.1633)	MaskLoss 1.0541 (0.8102)	MaskBCELoss 0.0667 (0.2531)	MaskDICELoss 0.9874 (0.5571)
Epoch: [0][136/500]	Time  9.986 ( 9.986)	Loss 1.6502 (1.3324)	CeLoss 0.2451 (0.3861)	SegCLSLoss 0.0243 (0.0379)	KLLoss 0.2734 (0.2176)	MaskLoss 1.2556 (0.9536)	MaskBCELoss 0.3789 (0.2086)	MaskDICELoss 0.8766 (0.7450)
Epoch: [0][137/500]	Time  8.666 ( 8.666)	Loss 0.7070 (1.2615)	CeLoss 0.7070 (0.5474)	SegCLSLoss 0.0000 (0.0341)	KLLoss 0.0000 (0.1621)	MaskLoss 0.0000 (0.7148)	MaskBCELoss 0.0000 (0.1588)	MaskDICELoss 0.0000 (0.5560)
Epoch: [0][138/500]	Time 12.000 (12.000)	Loss 1.1678 (1.1211)	CeLoss 0.2314 (0.4445)	SegCLSLoss 0.0405 (0.0263)	KLLoss 0.2695 (0.1629)	MaskLoss 1.1362 (0.7051)	MaskBCELoss 0.1395 (0.1407)	MaskDICELoss 0.9967 (0.5643)
Epoch: [0][139/500]	Time 10.491 (10.491)	Loss 2.0845 (1.2564)	CeLoss 0.2422 (0.4643)	SegCLSLoss 0.0229 (0.0242)	KLLoss 0.2734 (0.1906)	MaskLoss 1.4406 (0.8298)	MaskBCELoss 0.5998 (0.1661)	MaskDICELoss 0.8408 (0.6637)
[2025-03-02 15:42:47,414] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[0.00028916326530612244], mom=[(0.9, 0.95)]
[2025-03-02 15:42:47,428] [INFO] [timer.py:215:stop] epoch=0/micro_step=1400/global_step=140, RunningAvgSamplesPerSec=0.9465461196456808, CurrSamplesPerSec=1.019932942688908, MemAllocated=31.24GB, MaxMemAllocated=36.84GB
Epoch: [0][140/500]	Time  9.806 ( 9.806)	Loss 1.0634 (1.1965)	CeLoss 0.2598 (0.4747)	SegCLSLoss 0.0312 (0.0240)	KLLoss 0.2715 (0.1900)	MaskLoss 1.0699 (0.8125)	MaskBCELoss 0.0716 (0.1305)	MaskDICELoss 0.9982 (0.6820)
Epoch: [0][141/500]	Time 11.894 (11.894)	Loss 1.5046 (1.2110)	CeLoss 0.2559 (0.3021)	SegCLSLoss 0.0518 (0.0259)	KLLoss 0.2715 (0.1904)	MaskLoss 1.2213 (0.8857)	MaskBCELoss 0.2938 (0.2244)	MaskDICELoss 0.9275 (0.6613)
Epoch: [0][142/500]	Time 11.602 (11.602)	Loss 0.9705 (1.0600)	CeLoss 0.1846 (0.4355)	SegCLSLoss 0.0674 (0.0235)	KLLoss 0.2832 (0.1648)	MaskLoss 1.0411 (0.6936)	MaskBCELoss 0.0427 (0.1125)	MaskDICELoss 0.9983 (0.5811)
Epoch: [0][143/500]	Time  9.189 ( 9.189)	Loss 1.0392 (1.8376)	CeLoss 0.3086 (0.2403)	SegCLSLoss 0.0217 (0.0400)	KLLoss 0.2695 (0.2711)	MaskLoss 1.0398 (1.4357)	MaskBCELoss 0.0399 (0.4693)	MaskDICELoss 0.9999 (0.9664)
Epoch: [0][144/500]	Time  9.356 ( 9.356)	Loss 1.0027 (1.5661)	CeLoss 0.2070 (0.2933)	SegCLSLoss 0.0317 (0.0289)	KLLoss 0.2715 (0.2445)	MaskLoss 1.0584 (1.2018)	MaskBCELoss 0.0691 (0.3416)	MaskDICELoss 0.9894 (0.8602)
Epoch: [0][145/500]	Time  8.861 ( 8.861)	Loss 1.4085 (1.3287)	CeLoss 0.2949 (0.4036)	SegCLSLoss 0.0248 (0.0362)	KLLoss 0.2715 (0.2180)	MaskLoss 1.1858 (0.9524)	MaskBCELoss 0.2307 (0.1977)	MaskDICELoss 0.9550 (0.7547)
Epoch: [0][146/500]	Time  9.478 ( 9.478)	Loss 1.7264 (1.4895)	CeLoss 0.2559 (0.3253)	SegCLSLoss 0.0339 (0.0388)	KLLoss 0.2715 (0.2437)	MaskLoss 1.2782 (1.1074)	MaskBCELoss 0.4117 (0.2878)	MaskDICELoss 0.8666 (0.8196)
Epoch: [0][147/500]	Time 10.358 (10.358)	Loss 2.0730 (1.4662)	CeLoss 0.2754 (0.2412)	SegCLSLoss 0.0200 (0.0274)	KLLoss 0.2734 (0.2715)	MaskLoss 1.4174 (1.2230)	MaskBCELoss 0.5785 (0.2877)	MaskDICELoss 0.8388 (0.9353)
Epoch: [0][148/500]	Time  8.462 ( 8.462)	Loss 0.8594 (1.3475)	CeLoss 0.8594 (0.3443)	SegCLSLoss 0.0000 (0.0469)	KLLoss 0.0000 (0.2160)	MaskLoss 0.0000 (0.9739)	MaskBCELoss 0.0000 (0.2367)	MaskDICELoss 0.0000 (0.7372)
Epoch: [0][149/500]	Time  9.231 ( 9.231)	Loss 1.2353 (1.2596)	CeLoss 0.2559 (0.4688)	SegCLSLoss 0.0344 (0.0383)	KLLoss 0.2656 (0.1879)	MaskLoss 1.1481 (0.8237)	MaskBCELoss 0.1662 (0.1651)	MaskDICELoss 0.9819 (0.6586)
[2025-03-02 15:44:27,071] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[0.000287938775510204], mom=[(0.9, 0.95)]
[2025-03-02 15:44:27,076] [INFO] [timer.py:215:stop] epoch=0/micro_step=1500/global_step=150, RunningAvgSamplesPerSec=0.9501997982778323, CurrSamplesPerSec=0.8916653087507503, MemAllocated=31.46GB, MaxMemAllocated=36.84GB
Epoch: [0][150/500]	Time 11.217 (11.217)	Loss 0.0996 (0.9608)	CeLoss 0.0996 (0.5574)	SegCLSLoss 0.0000 (0.0158)	KLLoss 0.0000 (0.1074)	MaskLoss 0.0000 (0.4565)	MaskBCELoss 0.0000 (0.0709)	MaskDICELoss 0.0000 (0.3856)
Epoch: [0][151/500]	Time  9.841 ( 9.841)	Loss 1.5892 (1.3957)	CeLoss 0.1279 (0.5610)	SegCLSLoss 0.0762 (0.0258)	KLLoss 0.2715 (0.1883)	MaskLoss 1.2677 (0.8514)	MaskBCELoss 0.3955 (0.1896)	MaskDICELoss 0.8722 (0.6618)
Epoch: [0][152/500]	Time 10.344 (10.344)	Loss 1.1675 (1.2711)	CeLoss 0.2129 (0.4160)	SegCLSLoss 0.0359 (0.0170)	KLLoss 0.2656 (0.1602)	MaskLoss 1.1218 (0.8082)	MaskBCELoss 0.1545 (0.2346)	MaskDICELoss 0.9673 (0.5735)
Epoch: [0][153/500]	Time 11.026 (11.026)	Loss 1.9474 (1.6657)	CeLoss 0.2061 (0.4068)	SegCLSLoss 0.0214 (0.0321)	KLLoss 0.2656 (0.2146)	MaskLoss 1.4652 (1.1160)	MaskBCELoss 0.5531 (0.3693)	MaskDICELoss 0.9122 (0.7466)
Epoch: [0][154/500]	Time 10.466 (10.466)	Loss 1.0498 (1.3059)	CeLoss 0.2754 (0.3963)	SegCLSLoss 0.0233 (0.0225)	KLLoss 0.2637 (0.1873)	MaskLoss 1.0659 (0.8930)	MaskBCELoss 0.0688 (0.2290)	MaskDICELoss 0.9971 (0.6640)
Epoch: [0][155/500]	Time 12.684 (12.684)	Loss 1.0291 (1.4164)	CeLoss 0.2305 (0.2378)	SegCLSLoss 0.0278 (0.0332)	KLLoss 0.2656 (0.2689)	MaskLoss 1.0755 (1.2258)	MaskBCELoss 0.0779 (0.2642)	MaskDICELoss 0.9976 (0.9616)
Epoch: [0][156/500]	Time 10.516 (10.516)	Loss 0.6914 (1.3323)	CeLoss 0.6914 (0.3869)	SegCLSLoss 0.0000 (0.0266)	KLLoss 0.0000 (0.2143)	MaskLoss 0.0000 (0.9799)	MaskBCELoss 0.0000 (0.2136)	MaskDICELoss 0.0000 (0.7663)
Epoch: [0][157/500]	Time 12.722 (12.722)	Loss 1.8531 (1.5801)	CeLoss 0.1426 (0.2586)	SegCLSLoss 0.0835 (0.0326)	KLLoss 0.2695 (0.2701)	MaskLoss 1.3841 (1.2711)	MaskBCELoss 0.5221 (0.3359)	MaskDICELoss 0.8620 (0.9353)
Epoch: [0][158/500]	Time 10.365 (10.365)	Loss 1.1069 (1.7799)	CeLoss 0.2236 (0.3139)	SegCLSLoss 0.0344 (0.0234)	KLLoss 0.2656 (0.2154)	MaskLoss 1.1068 (1.1976)	MaskBCELoss 0.1172 (0.4758)	MaskDICELoss 0.9895 (0.7219)
Epoch: [0][159/500]	Time 10.859 (10.859)	Loss 1.9649 (1.3348)	CeLoss 0.4043 (0.3970)	SegCLSLoss 0.0234 (0.0249)	KLLoss 0.2656 (0.2145)	MaskLoss 1.3565 (0.9734)	MaskBCELoss 0.4652 (0.2104)	MaskDICELoss 0.8912 (0.7630)
[2025-03-02 15:46:16,544] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[0.0002867142857142857], mom=[(0.9, 0.95)]
[2025-03-02 15:46:16,557] [INFO] [timer.py:215:stop] epoch=0/micro_step=1600/global_step=160, RunningAvgSamplesPerSec=0.9477910083387739, CurrSamplesPerSec=0.9383598211724092, MemAllocated=31.25GB, MaxMemAllocated=36.84GB
Epoch: [0][160/500]	Time 10.659 (10.659)	Loss 1.4324 (1.5601)	CeLoss 0.2422 (0.5004)	SegCLSLoss 0.0312 (0.0270)	KLLoss 0.2637 (0.2137)	MaskLoss 1.2054 (0.9965)	MaskBCELoss 0.2772 (0.2734)	MaskDICELoss 0.9282 (0.7231)
Epoch: [0][161/500]	Time 11.704 (11.704)	Loss 1.5547 (1.4938)	CeLoss 1.5547 (0.4401)	SegCLSLoss 0.0000 (0.0345)	KLLoss 0.0000 (0.2160)	MaskLoss 0.0000 (0.9989)	MaskBCELoss 0.0000 (0.2655)	MaskDICELoss 0.0000 (0.7335)
Epoch: [0][162/500]	Time 10.045 (10.045)	Loss 1.5761 (1.3572)	CeLoss 0.2139 (0.5873)	SegCLSLoss 0.0486 (0.0204)	KLLoss 0.2695 (0.1605)	MaskLoss 1.2272 (0.7480)	MaskBCELoss 0.3558 (0.1914)	MaskDICELoss 0.8714 (0.5567)
Epoch: [0][163/500]	Time  9.962 ( 9.962)	Loss 1.1875 (1.3679)	CeLoss 1.1875 (0.4580)	SegCLSLoss 0.0000 (0.0262)	KLLoss 0.0000 (0.1867)	MaskLoss 0.0000 (0.8779)	MaskBCELoss 0.0000 (0.2294)	MaskDICELoss 0.0000 (0.6485)
Epoch: [0][164/500]	Time 10.285 (10.285)	Loss 1.1939 (1.2326)	CeLoss 0.2070 (0.2942)	SegCLSLoss 0.0364 (0.0363)	KLLoss 0.2617 (0.2133)	MaskLoss 1.1316 (0.9603)	MaskBCELoss 0.1741 (0.2093)	MaskDICELoss 0.9575 (0.7509)
Epoch: [0][165/500]	Time 10.774 (10.774)	Loss 0.3223 (1.3732)	CeLoss 0.3223 (0.4969)	SegCLSLoss 0.0000 (0.0203)	KLLoss 0.0000 (0.1861)	MaskLoss 0.0000 (0.8724)	MaskBCELoss 0.0000 (0.2143)	MaskDICELoss 0.0000 (0.6580)
Epoch: [0][166/500]	Time  8.688 ( 8.688)	Loss 1.3991 (1.5127)	CeLoss 0.1768 (0.5214)	SegCLSLoss 0.0330 (0.0235)	KLLoss 0.2656 (0.1859)	MaskLoss 1.2562 (0.9258)	MaskBCELoss 0.2878 (0.2713)	MaskDICELoss 0.9684 (0.6545)
Epoch: [0][167/500]	Time 12.527 (12.527)	Loss 0.0996 (1.2117)	CeLoss 0.0996 (0.2902)	SegCLSLoss 0.0000 (0.0279)	KLLoss 0.0000 (0.2119)	MaskLoss 0.0000 (0.9655)	MaskBCELoss 0.0000 (0.2036)	MaskDICELoss 0.0000 (0.7619)
Epoch: [0][168/500]	Time 11.729 (11.729)	Loss 1.6395 (1.3152)	CeLoss 0.2539 (0.3255)	SegCLSLoss 0.0447 (0.0366)	KLLoss 0.2656 (0.2385)	MaskLoss 1.2554 (1.0615)	MaskBCELoss 0.3733 (0.2046)	MaskDICELoss 0.8821 (0.8569)
Epoch: [0][169/500]	Time 11.088 (11.088)	Loss 1.1015 (1.0453)	CeLoss 0.3027 (0.4711)	SegCLSLoss 0.0204 (0.0267)	KLLoss 0.2656 (0.1594)	MaskLoss 1.0681 (0.6685)	MaskBCELoss 0.0795 (0.0923)	MaskDICELoss 0.9887 (0.5762)
[2025-03-02 15:48:03,305] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[0.00028548979591836734], mom=[(0.9, 0.95)]
[2025-03-02 15:48:03,316] [INFO] [timer.py:215:stop] epoch=0/micro_step=1700/global_step=170, RunningAvgSamplesPerSec=0.9471302813230568, CurrSamplesPerSec=1.004450826739139, MemAllocated=31.09GB, MaxMemAllocated=36.84GB
Epoch: [0][170/500]	Time  9.957 ( 9.957)	Loss 2.4325 (1.5039)	CeLoss 0.2656 (0.4472)	SegCLSLoss 0.0250 (0.0296)	KLLoss 0.2637 (0.2113)	MaskLoss 1.7239 (1.0334)	MaskBCELoss 0.7660 (0.2715)	MaskDICELoss 0.9579 (0.7620)
Epoch: [0][171/500]	Time 12.010 (12.010)	Loss 0.9886 (1.4620)	CeLoss 0.2256 (0.2999)	SegCLSLoss 0.0400 (0.0336)	KLLoss 0.2637 (0.2377)	MaskLoss 1.0498 (1.1481)	MaskBCELoss 0.0580 (0.2919)	MaskDICELoss 0.9918 (0.8562)
Epoch: [0][172/500]	Time  9.359 ( 9.359)	Loss 1.3524 (1.4652)	CeLoss 0.1963 (0.7461)	SegCLSLoss 0.0354 (0.0156)	KLLoss 0.2617 (0.1330)	MaskLoss 1.2156 (0.6619)	MaskBCELoss 0.2593 (0.1993)	MaskDICELoss 0.9564 (0.4625)
Epoch: [0][173/500]	Time 11.363 (11.363)	Loss 1.0834 (1.1939)	CeLoss 0.2188 (0.3693)	SegCLSLoss 0.0315 (0.0200)	KLLoss 0.2637 (0.2104)	MaskLoss 1.0872 (0.9329)	MaskBCELoss 0.1121 (0.1578)	MaskDICELoss 0.9752 (0.7751)
Epoch: [0][174/500]	Time 10.482 (10.482)	Loss 0.9906 (1.3307)	CeLoss 0.2217 (0.5286)	SegCLSLoss 0.0315 (0.0178)	KLLoss 0.2617 (0.1846)	MaskLoss 1.0616 (0.8437)	MaskBCELoss 0.0646 (0.1786)	MaskDICELoss 0.9970 (0.6651)
Epoch: [0][175/500]	Time 10.297 (10.297)	Loss 2.0819 (1.2791)	CeLoss 0.2656 (0.4331)	SegCLSLoss 0.0325 (0.0238)	KLLoss 0.2617 (0.2100)	MaskLoss 1.4264 (0.9316)	MaskBCELoss 0.5971 (0.1688)	MaskDICELoss 0.8292 (0.7628)
Epoch: [0][176/500]	Time 11.825 (11.825)	Loss 1.9084 (1.3719)	CeLoss 0.3535 (0.3266)	SegCLSLoss 0.0179 (0.0191)	KLLoss 0.2617 (0.2104)	MaskLoss 1.4000 (1.0188)	MaskBCELoss 0.4640 (0.2698)	MaskDICELoss 0.9360 (0.7490)
Epoch: [0][177/500]	Time  8.564 ( 8.564)	Loss 0.9492 (1.3817)	CeLoss 0.9492 (0.7069)	SegCLSLoss 0.0000 (0.0126)	KLLoss 0.0000 (0.1316)	MaskLoss 0.0000 (0.6473)	MaskBCELoss 0.0000 (0.1792)	MaskDICELoss 0.0000 (0.4681)
Epoch: [0][178/500]	Time  8.279 ( 8.279)	Loss 1.5703 (1.2705)	CeLoss 1.5703 (0.6527)	SegCLSLoss 0.0000 (0.0223)	KLLoss 0.0000 (0.1604)	MaskLoss 0.0000 (0.6854)	MaskBCELoss 0.0000 (0.1141)	MaskDICELoss 0.0000 (0.5713)
Epoch: [0][179/500]	Time 10.462 (10.462)	Loss 1.7171 (1.3530)	CeLoss 0.2812 (0.5195)	SegCLSLoss 0.0247 (0.0260)	KLLoss 0.2656 (0.1865)	MaskLoss 1.2775 (0.8419)	MaskBCELoss 0.4027 (0.1912)	MaskDICELoss 0.8748 (0.6506)
[2025-03-02 15:49:47,927] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[0.00028426530612244897], mom=[(0.9, 0.95)]
[2025-03-02 15:49:47,943] [INFO] [timer.py:215:stop] epoch=0/micro_step=1800/global_step=180, RunningAvgSamplesPerSec=0.947620768030668, CurrSamplesPerSec=0.8345146311108819, MemAllocated=30.72GB, MaxMemAllocated=36.84GB
Epoch: [0][180/500]	Time 11.986 (11.986)	Loss 0.4238 (1.1851)	CeLoss 0.4238 (0.4231)	SegCLSLoss 0.0000 (0.0159)	KLLoss 0.0000 (0.1848)	MaskLoss 0.0000 (0.8345)	MaskBCELoss 0.0000 (0.1580)	MaskDICELoss 0.0000 (0.6764)
Epoch: [0][181/500]	Time 10.793 (10.793)	Loss 0.4648 (0.9774)	CeLoss 0.4648 (0.4735)	SegCLSLoss 0.0000 (0.0189)	KLLoss 0.0000 (0.1352)	MaskLoss 0.0000 (0.5776)	MaskBCELoss 0.0000 (0.0878)	MaskDICELoss 0.0000 (0.4898)
Epoch: [0][182/500]	Time  8.882 ( 8.882)	Loss 1.1016 (1.4654)	CeLoss 1.1016 (0.5896)	SegCLSLoss 0.0000 (0.0134)	KLLoss 0.0000 (0.1330)	MaskLoss 0.0000 (0.7513)	MaskBCELoss 0.0000 (0.2782)	MaskDICELoss 0.0000 (0.4732)
Epoch: [0][183/500]	Time 10.414 (10.414)	Loss 1.8380 (1.7239)	CeLoss 0.2812 (0.4198)	SegCLSLoss 0.0417 (0.0253)	KLLoss 0.2695 (0.2127)	MaskLoss 1.3177 (1.1256)	MaskBCELoss 0.4540 (0.3966)	MaskDICELoss 0.8637 (0.7290)
Epoch: [0][184/500]	Time 10.577 (10.577)	Loss 1.4591 (1.4211)	CeLoss 0.2676 (0.4185)	SegCLSLoss 0.0189 (0.0179)	KLLoss 0.2598 (0.2102)	MaskLoss 1.2136 (0.9837)	MaskBCELoss 0.2847 (0.2498)	MaskDICELoss 0.9289 (0.7339)
Epoch: [0][185/500]	Time 11.383 (11.383)	Loss 1.8726 (1.4795)	CeLoss 0.2393 (0.2655)	SegCLSLoss 0.0253 (0.0328)	KLLoss 0.2617 (0.2639)	MaskLoss 1.3953 (1.2173)	MaskBCELoss 0.5051 (0.2884)	MaskDICELoss 0.8902 (0.9288)
Epoch: [0][186/500]	Time 11.963 (11.963)	Loss 1.4799 (1.2289)	CeLoss 0.2578 (0.3693)	SegCLSLoss 0.0258 (0.0209)	KLLoss 0.2598 (0.2092)	MaskLoss 1.2427 (0.9420)	MaskBCELoss 0.2982 (0.1770)	MaskDICELoss 0.9445 (0.7650)
Epoch: [0][187/500]	Time  8.421 ( 8.421)	Loss 2.2090 (1.3619)	CeLoss 0.3438 (0.5027)	SegCLSLoss 0.0227 (0.0162)	KLLoss 0.2637 (0.1572)	MaskLoss 1.4862 (0.7829)	MaskBCELoss 0.6198 (0.2409)	MaskDICELoss 0.8664 (0.5420)
Epoch: [0][188/500]	Time 11.175 (11.175)	Loss 0.8711 (1.2841)	CeLoss 0.8711 (0.3151)	SegCLSLoss 0.0000 (0.0242)	KLLoss 0.0000 (0.2361)	MaskLoss 0.0000 (1.0594)	MaskBCELoss 0.0000 (0.1988)	MaskDICELoss 0.0000 (0.8606)
Epoch: [0][189/500]	Time 12.800 (12.800)	Loss 1.5089 (1.1107)	CeLoss 0.2832 (0.2884)	SegCLSLoss 0.0201 (0.0277)	KLLoss 0.2617 (0.2111)	MaskLoss 1.1979 (0.9149)	MaskBCELoss 0.3014 (0.1553)	MaskDICELoss 0.8965 (0.7596)
[2025-03-02 15:51:33,971] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[0.0002830408163265306], mom=[(0.9, 0.95)]
[2025-03-02 15:51:33,978] [INFO] [timer.py:215:stop] epoch=0/micro_step=1900/global_step=190, RunningAvgSamplesPerSec=0.9473851864219142, CurrSamplesPerSec=1.0389503329552363, MemAllocated=31.1GB, MaxMemAllocated=36.84GB
Epoch: [0][190/500]	Time  9.628 ( 9.628)	Loss 1.8927 (1.3022)	CeLoss 0.2246 (0.7164)	SegCLSLoss 0.0227 (0.0156)	KLLoss 0.2617 (0.1311)	MaskLoss 1.4344 (0.6068)	MaskBCELoss 0.5218 (0.1345)	MaskDICELoss 0.9126 (0.4723)
Epoch: [0][191/500]	Time 12.504 (12.504)	Loss 0.9907 (1.3609)	CeLoss 0.1992 (0.2613)	SegCLSLoss 0.0459 (0.0315)	KLLoss 0.2656 (0.2633)	MaskLoss 1.0562 (1.1868)	MaskBCELoss 0.0690 (0.2309)	MaskDICELoss 0.9872 (0.9559)
Epoch: [0][192/500]	Time 11.021 (11.021)	Loss 1.1172 (1.0003)	CeLoss 1.1172 (0.6102)	SegCLSLoss 0.0000 (0.0160)	KLLoss 0.0000 (0.1051)	MaskLoss 0.0000 (0.4525)	MaskBCELoss 0.0000 (0.0665)	MaskDICELoss 0.0000 (0.3860)
Epoch: [0][193/500]	Time 10.424 (10.424)	Loss 0.8750 (1.1361)	CeLoss 0.8750 (0.4505)	SegCLSLoss 0.0000 (0.0177)	KLLoss 0.0000 (0.1834)	MaskLoss 0.0000 (0.8056)	MaskBCELoss 0.0000 (0.1205)	MaskDICELoss 0.0000 (0.6851)
Epoch: [0][194/500]	Time 10.406 (10.406)	Loss 1.3594 (1.3816)	CeLoss 1.3594 (0.6027)	SegCLSLoss 0.0000 (0.0228)	KLLoss 0.0000 (0.1594)	MaskLoss 0.0000 (0.7676)	MaskBCELoss 0.0000 (0.1958)	MaskDICELoss 0.0000 (0.5719)
Epoch: [0][195/500]	Time 10.802 (10.802)	Loss 1.3058 (1.1614)	CeLoss 0.1367 (0.4257)	SegCLSLoss 0.0752 (0.0257)	KLLoss 0.2695 (0.1850)	MaskLoss 1.1829 (0.8219)	MaskBCELoss 0.2508 (0.1427)	MaskDICELoss 0.9321 (0.6793)
Epoch: [0][196/500]	Time 10.575 (10.575)	Loss 1.0564 (1.3436)	CeLoss 0.2354 (0.5686)	SegCLSLoss 0.0297 (0.0224)	KLLoss 0.2617 (0.1854)	MaskLoss 1.0802 (0.8371)	MaskBCELoss 0.0921 (0.1629)	MaskDICELoss 0.9881 (0.6742)
Epoch: [0][197/500]	Time 11.225 (11.225)	Loss 2.0229 (1.8537)	CeLoss 0.2754 (0.3556)	SegCLSLoss 0.0186 (0.0196)	KLLoss 0.2637 (0.2373)	MaskLoss 1.5005 (1.3105)	MaskBCELoss 0.5581 (0.4647)	MaskDICELoss 0.9424 (0.8459)
Epoch: [0][198/500]	Time  9.217 ( 9.217)	Loss 1.4415 (1.4813)	CeLoss 0.2812 (0.5983)	SegCLSLoss 0.0256 (0.0244)	KLLoss 0.2695 (0.1598)	MaskLoss 1.2063 (0.7989)	MaskBCELoss 0.2552 (0.2481)	MaskDICELoss 0.9511 (0.5509)
Epoch: [0][199/500]	Time 10.929 (10.929)	Loss 1.8588 (1.5132)	CeLoss 0.2734 (0.4262)	SegCLSLoss 0.0223 (0.0189)	KLLoss 0.2637 (0.2113)	MaskLoss 1.3166 (1.0260)	MaskBCELoss 0.4814 (0.2905)	MaskDICELoss 0.8352 (0.7355)
[2025-03-02 15:53:20,953] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[0.00028181632653061223], mom=[(0.9, 0.95)]
[2025-03-02 15:53:20,960] [INFO] [timer.py:215:stop] epoch=0/micro_step=2000/global_step=200, RunningAvgSamplesPerSec=0.9467455380618278, CurrSamplesPerSec=1.0124681460150367, MemAllocated=31.25GB, MaxMemAllocated=36.84GB
Epoch: [0][200/500]	Time  9.880 ( 9.880)	Loss 1.0929 (1.4897)	CeLoss 0.1953 (0.3882)	SegCLSLoss 0.0310 (0.0157)	KLLoss 0.2598 (0.1840)	MaskLoss 1.1198 (0.9142)	MaskBCELoss 0.1318 (0.3338)	MaskDICELoss 0.9879 (0.5804)
Epoch: [0][201/500]	Time 10.520 (10.520)	Loss 1.5391 (1.5285)	CeLoss 1.5391 (0.4325)	SegCLSLoss 0.0000 (0.0225)	KLLoss 0.0000 (0.2088)	MaskLoss 0.0000 (1.0114)	MaskBCELoss 0.0000 (0.2981)	MaskDICELoss 0.0000 (0.7133)
Epoch: [0][202/500]	Time 10.499 (10.499)	Loss 1.6510 (1.6355)	CeLoss 0.2031 (0.4067)	SegCLSLoss 0.0442 (0.0205)	KLLoss 0.2617 (0.2086)	MaskLoss 1.3025 (1.0559)	MaskBCELoss 0.4056 (0.3663)	MaskDICELoss 0.8969 (0.6897)
Epoch: [0][203/500]	Time 10.099 (10.099)	Loss 1.2422 (1.4638)	CeLoss 1.2422 (0.4978)	SegCLSLoss 0.0000 (0.0171)	KLLoss 0.0000 (0.1561)	MaskLoss 0.0000 (0.7994)	MaskBCELoss 0.0000 (0.2975)	MaskDICELoss 0.0000 (0.5019)
Epoch: [0][204/500]	Time 11.586 (11.586)	Loss 1.3743 (1.6465)	CeLoss 0.2793 (0.3537)	SegCLSLoss 0.0238 (0.0219)	KLLoss 0.2617 (0.2354)	MaskLoss 1.1870 (1.1677)	MaskBCELoss 0.2332 (0.3658)	MaskDICELoss 0.9538 (0.8019)
Epoch: [0][205/500]	Time 10.646 (10.646)	Loss 1.4452 (1.4430)	CeLoss 0.3398 (0.4724)	SegCLSLoss 0.0178 (0.0150)	KLLoss 0.2578 (0.1826)	MaskLoss 1.1717 (0.8955)	MaskBCELoss 0.2425 (0.2672)	MaskDICELoss 0.9292 (0.6282)
Epoch: [0][206/500]	Time 12.054 (12.054)	Loss 1.5861 (1.4874)	CeLoss 0.2334 (0.4375)	SegCLSLoss 0.0194 (0.0187)	KLLoss 0.2617 (0.2090)	MaskLoss 1.2669 (0.9979)	MaskBCELoss 0.3652 (0.2756)	MaskDICELoss 0.9017 (0.7223)
Epoch: [0][207/500]	Time  9.912 ( 9.912)	Loss 1.0547 (1.3324)	CeLoss 1.0547 (0.5000)	SegCLSLoss 0.0000 (0.0185)	KLLoss 0.0000 (0.1834)	MaskLoss 0.0000 (0.8487)	MaskBCELoss 0.0000 (0.1954)	MaskDICELoss 0.0000 (0.6533)
Epoch: [0][208/500]	Time  9.523 ( 9.523)	Loss 1.7412 (1.2245)	CeLoss 0.1729 (0.5603)	SegCLSLoss 0.0398 (0.0173)	KLLoss 0.2676 (0.1564)	MaskLoss 1.3533 (0.7150)	MaskBCELoss 0.4628 (0.1428)	MaskDICELoss 0.8905 (0.5722)
Epoch: [0][209/500]	Time 13.001 (13.001)	Loss 1.5628 (1.2144)	CeLoss 0.2334 (0.3243)	SegCLSLoss 0.0177 (0.0219)	KLLoss 0.2578 (0.2080)	MaskLoss 1.3070 (0.9632)	MaskBCELoss 0.3549 (0.1932)	MaskDICELoss 0.9521 (0.7700)
[2025-03-02 15:55:08,699] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=0, lr=[0.00028059183673469386], mom=[(0.9, 0.95)]
[2025-03-02 15:55:08,704] [INFO] [timer.py:215:stop] epoch=0/micro_step=2100/global_step=210, RunningAvgSamplesPerSec=0.9458404523289843, CurrSamplesPerSec=1.0098402449327943, MemAllocated=31.31GB, MaxMemAllocated=36.84GB
Epoch: [0][210/500]	Time  9.905 ( 9.905)	Loss 1.3275 (1.4314)	CeLoss 0.1787 (0.6657)	SegCLSLoss 0.0417 (0.0175)	KLLoss 0.2617 (0.1559)	MaskLoss 1.1880 (0.7585)	MaskBCELoss 0.2558 (0.1943)	MaskDICELoss 0.9321 (0.5642)
Epoch: [0][211/500]	Time 13.248 (13.248)	Loss 0.9785 (1.3301)	CeLoss 0.2119 (0.2406)	SegCLSLoss 0.0304 (0.0342)	KLLoss 0.2598 (0.2348)	MaskLoss 1.0595 (1.1107)	MaskBCELoss 0.0666 (0.2590)	MaskDICELoss 0.9929 (0.8517)
Epoch: [0][212/500]	Time 10.491 (10.491)	Loss 1.0234 (1.3359)	CeLoss 1.0234 (0.5631)	SegCLSLoss 0.0000 (0.0147)	KLLoss 0.0000 (0.1549)	MaskLoss 0.0000 (0.7651)	MaskBCELoss 0.0000 (0.1995)	MaskDICELoss 0.0000 (0.5656)
Epoch: [0][213/500]	Time  8.956 ( 8.956)	Loss 1.1945 (1.3663)	CeLoss 0.2852 (0.5954)	SegCLSLoss 0.0179 (0.0157)	KLLoss 0.2578 (0.1553)	MaskLoss 1.1034 (0.7582)	MaskBCELoss 0.1430 (0.1983)	MaskDICELoss 0.9604 (0.5598)
Epoch: [0][214/500]	Time 11.743 (11.743)	Loss 1.5629 (1.3922)	CeLoss 0.2832 (0.3406)	SegCLSLoss 0.0165 (0.0279)	KLLoss 0.2598 (0.2350)	MaskLoss 1.2531 (1.0817)	MaskBCELoss 0.3290 (0.2416)	MaskDICELoss 0.9241 (0.8400)
Epoch: [0][215/500]	Time 11.947 (11.947)	Loss 1.1556 (1.2741)	CeLoss 0.2471 (0.2998)	SegCLSLoss 0.0359 (0.0234)	KLLoss 0.2598 (0.2080)	MaskLoss 1.1175 (0.9818)	MaskBCELoss 0.1362 (0.2365)	MaskDICELoss 0.9814 (0.7453)
Epoch: [0][216/500]	Time  8.776 ( 8.776)	Loss 1.2031 (1.1924)	CeLoss 1.2031 (0.6403)	SegCLSLoss 0.0000 (0.0165)	KLLoss 0.0000 (0.1303)	MaskLoss 0.0000 (0.5924)	MaskBCELoss 0.0000 (0.1182)	MaskDICELoss 0.0000 (0.4742)
Epoch: [0][217/500]	Time 10.666 (10.666)	Loss 0.7734 (1.3179)	CeLoss 0.7734 (0.6209)	SegCLSLoss 0.0000 (0.0168)	KLLoss 0.0000 (0.1570)	MaskLoss 0.0000 (0.7195)	MaskBCELoss 0.0000 (0.1592)	MaskDICELoss 0.0000 (0.5602)
Epoch: [0][218/500]	Time  9.141 ( 9.141)	Loss 3.2326 (1.8148)	CeLoss 0.2812 (0.3339)	SegCLSLoss 0.0161 (0.0227)	KLLoss 0.2578 (0.2340)	MaskLoss 2.0382 (1.2962)	MaskBCELoss 1.1706 (0.4591)	MaskDICELoss 0.8676 (0.8371)
Epoch: [0][219/500]	Time  9.756 ( 9.756)	Loss 1.4212 (1.0976)	CeLoss 0.2695 (0.5601)	SegCLSLoss 0.0270 (0.0213)	KLLoss 0.2578 (0.1559)	MaskLoss 1.2229 (0.6646)	MaskBCELoss 0.2642 (0.0785)	MaskDICELoss 0.9586 (0.5862)
[2025-03-02 15:56:52,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=0, lr=[0.0002793673469387755], mom=[(0.9, 0.95)]
[2025-03-02 15:56:52,947] [INFO] [timer.py:215:stop] epoch=0/micro_step=2200/global_step=220, RunningAvgSamplesPerSec=0.9464560529283695, CurrSamplesPerSec=1.0508286087471408, MemAllocated=30.69GB, MaxMemAllocated=36.84GB
Epoch: [0][220/500]	Time  9.518 ( 9.518)	Loss 1.5781 (1.4066)	CeLoss 1.5781 (0.5567)	SegCLSLoss 0.0000 (0.0204)	KLLoss 0.0000 (0.1838)	MaskLoss 0.0000 (0.8654)	MaskBCELoss 0.0000 (0.2028)	MaskDICELoss 0.0000 (0.6626)
Epoch: [0][221/500]	Time 11.520 (11.520)	Loss 1.3256 (1.3664)	CeLoss 0.1572 (0.3942)	SegCLSLoss 0.0437 (0.0317)	KLLoss 0.2617 (0.1840)	MaskLoss 1.1864 (0.8964)	MaskBCELoss 0.2652 (0.2625)	MaskDICELoss 0.9212 (0.6340)
Epoch: [0][222/500]	Time 10.425 (10.425)	Loss 1.3346 (1.2106)	CeLoss 0.2715 (0.5386)	SegCLSLoss 0.0292 (0.0205)	KLLoss 0.2617 (0.1830)	MaskLoss 1.1491 (0.7897)	MaskBCELoss 0.2163 (0.1142)	MaskDICELoss 0.9328 (0.6755)
Epoch: [0][223/500]	Time 10.735 (10.735)	Loss 0.1040 (1.1889)	CeLoss 0.1040 (0.5761)	SegCLSLoss 0.0000 (0.0155)	KLLoss 0.0000 (0.1568)	MaskLoss 0.0000 (0.6905)	MaskBCELoss 0.0000 (0.1170)	MaskDICELoss 0.0000 (0.5735)
Epoch: [0][224/500]	Time 11.968 (11.968)	Loss 1.8426 (1.5335)	CeLoss 0.3340 (0.3187)	SegCLSLoss 0.0175 (0.0265)	KLLoss 0.2598 (0.2354)	MaskLoss 1.3600 (1.1634)	MaskBCELoss 0.4459 (0.3234)	MaskDICELoss 0.9141 (0.8400)
Epoch: [0][225/500]	Time  9.786 ( 9.786)	Loss 3.5578 (1.6661)	CeLoss 0.2598 (0.5951)	SegCLSLoss 0.0162 (0.0201)	KLLoss 0.2598 (0.1822)	MaskLoss 2.2145 (0.9657)	MaskBCELoss 1.3407 (0.3156)	MaskDICELoss 0.8738 (0.6501)
Epoch: [0][226/500]	Time 10.553 (10.553)	Loss 1.9505 (1.5649)	CeLoss 0.2969 (0.7195)	SegCLSLoss 0.0159 (0.0121)	KLLoss 0.2617 (0.1309)	MaskLoss 1.4234 (0.7212)	MaskBCELoss 0.5158 (0.2659)	MaskDICELoss 0.9076 (0.4553)
Epoch: [0][227/500]	Time  9.830 ( 9.830)	Loss 0.0938 (1.4221)	CeLoss 0.0938 (0.5813)	SegCLSLoss 0.0000 (0.0152)	KLLoss 0.0000 (0.1564)	MaskLoss 0.0000 (0.7750)	MaskBCELoss 0.0000 (0.2332)	MaskDICELoss 0.0000 (0.5418)
Epoch: [0][228/500]	Time 11.169 (11.169)	Loss 1.8777 (1.5321)	CeLoss 0.2168 (0.4077)	SegCLSLoss 0.0172 (0.0198)	KLLoss 0.2637 (0.2355)	MaskLoss 1.3938 (1.1077)	MaskBCELoss 0.5181 (0.2801)	MaskDICELoss 0.8757 (0.8275)
Epoch: [0][229/500]	Time 10.313 (10.313)	Loss 1.2275 (1.3971)	CeLoss 0.2100 (0.5031)	SegCLSLoss 0.0322 (0.0208)	KLLoss 0.2617 (0.1832)	MaskLoss 1.1500 (0.8683)	MaskBCELoss 0.1908 (0.2263)	MaskDICELoss 0.9592 (0.6420)
[2025-03-02 15:58:42,268] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=0, lr=[0.0002781428571428571], mom=[(0.9, 0.95)]
[2025-03-02 15:58:42,272] [INFO] [timer.py:215:stop] epoch=0/micro_step=2300/global_step=230, RunningAvgSamplesPerSec=0.9450238554565467, CurrSamplesPerSec=0.7678090131315847, MemAllocated=31.44GB, MaxMemAllocated=36.84GB
Epoch: [0][230/500]	Time 13.026 (13.026)	Loss 1.5366 (1.1672)	CeLoss 0.2500 (0.2077)	SegCLSLoss 0.0171 (0.0190)	KLLoss 0.2617 (0.2084)	MaskLoss 1.2486 (0.9742)	MaskBCELoss 0.3318 (0.2293)	MaskDICELoss 0.9168 (0.7449)
Epoch: [0][231/500]	Time 10.689 (10.689)	Loss 1.5848 (1.4223)	CeLoss 0.3047 (0.4532)	SegCLSLoss 0.0291 (0.0224)	KLLoss 0.2598 (0.2092)	MaskLoss 1.2772 (0.9777)	MaskBCELoss 0.3269 (0.2327)	MaskDICELoss 0.9503 (0.7451)
Epoch: [0][232/500]	Time 11.364 (11.364)	Loss 1.3125 (1.1516)	CeLoss 1.3125 (0.3145)	SegCLSLoss 0.0000 (0.0281)	KLLoss 0.0000 (0.2098)	MaskLoss 0.0000 (0.9252)	MaskBCELoss 0.0000 (0.1635)	MaskDICELoss 0.0000 (0.7616)
Epoch: [0][233/500]	Time 11.846 (11.846)	Loss 1.4447 (1.2614)	CeLoss 0.1699 (0.2370)	SegCLSLoss 0.0518 (0.0276)	KLLoss 0.2656 (0.2354)	MaskLoss 1.2113 (1.0786)	MaskBCELoss 0.3142 (0.2273)	MaskDICELoss 0.8971 (0.8513)
Epoch: [0][234/500]	Time 10.466 (10.466)	Loss 1.6595 (1.4912)	CeLoss 0.2051 (0.3844)	SegCLSLoss 0.0378 (0.0352)	KLLoss 0.2656 (0.2123)	MaskLoss 1.3294 (1.0286)	MaskBCELoss 0.4067 (0.2959)	MaskDICELoss 0.9227 (0.7327)
Epoch: [0][235/500]	Time 12.497 (12.497)	Loss 1.3085 (1.5442)	CeLoss 0.2676 (0.3412)	SegCLSLoss 0.0197 (0.0169)	KLLoss 0.2617 (0.2342)	MaskLoss 1.1588 (1.1748)	MaskBCELoss 0.2063 (0.3204)	MaskDICELoss 0.9526 (0.8544)
Epoch: [0][236/500]	Time  9.705 ( 9.705)	Loss 1.3502 (1.5293)	CeLoss 0.2490 (0.4925)	SegCLSLoss 0.0344 (0.0231)	KLLoss 0.2617 (0.2104)	MaskLoss 1.1622 (1.0284)	MaskBCELoss 0.2341 (0.2641)	MaskDICELoss 0.9280 (0.7643)
Epoch: [0][237/500]	Time 11.380 (11.380)	Loss 2.3786 (1.4340)	CeLoss 0.4004 (0.3237)	SegCLSLoss 0.0166 (0.0220)	KLLoss 0.2578 (0.2336)	MaskLoss 1.6250 (1.1286)	MaskBCELoss 0.6812 (0.2733)	MaskDICELoss 0.9438 (0.8553)
Epoch: [0][238/500]	Time 10.961 (10.961)	Loss 1.0428 (1.4211)	CeLoss 0.2578 (0.6295)	SegCLSLoss 0.0227 (0.0157)	KLLoss 0.2598 (0.1818)	MaskLoss 1.0651 (0.8517)	MaskBCELoss 0.0775 (0.1764)	MaskDICELoss 0.9876 (0.6752)
Epoch: [0][239/500]	Time 11.388 (11.388)	Loss 1.1520 (1.2840)	CeLoss 0.2344 (0.3119)	SegCLSLoss 0.0320 (0.0254)	KLLoss 0.2598 (0.2340)	MaskLoss 1.1142 (1.0594)	MaskBCELoss 0.1427 (0.2027)	MaskDICELoss 0.9715 (0.8567)
[2025-03-02 16:00:30,894] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=0, lr=[0.00027691836734693875], mom=[(0.9, 0.95)]
[2025-03-02 16:00:30,903] [INFO] [timer.py:215:stop] epoch=0/micro_step=2400/global_step=240, RunningAvgSamplesPerSec=0.9439760321399802, CurrSamplesPerSec=1.1999230777461185, MemAllocated=30.71GB, MaxMemAllocated=36.84GB
Epoch: [0][240/500]	Time  8.336 ( 8.336)	Loss 0.7734 (2.0227)	CeLoss 0.7734 (0.5295)	SegCLSLoss 0.0000 (0.0225)	KLLoss 0.0000 (0.1826)	MaskLoss 0.0000 (1.1681)	MaskBCELoss 0.0000 (0.5263)	MaskDICELoss 0.0000 (0.6418)
Epoch: [0][241/500]	Time 10.193 (10.193)	Loss 1.1060 (1.0543)	CeLoss 0.2354 (0.5262)	SegCLSLoss 0.0320 (0.0189)	KLLoss 0.2539 (0.1291)	MaskLoss 1.1085 (0.5835)	MaskBCELoss 0.1249 (0.1066)	MaskDICELoss 0.9836 (0.4769)
Epoch: [0][242/500]	Time 10.886 (10.886)	Loss 1.1170 (1.3763)	CeLoss 0.2559 (0.6311)	SegCLSLoss 0.0189 (0.0185)	KLLoss 0.2578 (0.1549)	MaskLoss 1.1057 (0.7417)	MaskBCELoss 0.1185 (0.1853)	MaskDICELoss 0.9872 (0.5564)
Epoch: [0][243/500]	Time 11.741 (11.741)	Loss 1.1417 (1.4250)	CeLoss 0.2559 (0.2902)	SegCLSLoss 0.0247 (0.0273)	KLLoss 0.2617 (0.2336)	MaskLoss 1.1233 (1.1153)	MaskBCELoss 0.1265 (0.2859)	MaskDICELoss 0.9968 (0.8294)
Epoch: [0][244/500]	Time 11.395 (11.395)	Loss 1.5891 (1.4546)	CeLoss 0.1982 (0.4636)	SegCLSLoss 0.0352 (0.0209)	KLLoss 0.2598 (0.2076)	MaskLoss 1.2475 (0.9766)	MaskBCELoss 0.3842 (0.2463)	MaskDICELoss 0.8633 (0.7303)
Epoch: [0][245/500]	Time 11.996 (11.996)	Loss 2.1561 (1.2062)	CeLoss 0.2373 (0.3916)	SegCLSLoss 0.0247 (0.0152)	KLLoss 0.2578 (0.1551)	MaskLoss 1.4872 (0.7680)	MaskBCELoss 0.6535 (0.2211)	MaskDICELoss 0.8337 (0.5469)
Epoch: [0][246/500]	Time 10.449 (10.449)	Loss 0.0879 (1.1861)	CeLoss 0.0879 (0.2669)	SegCLSLoss 0.0000 (0.0254)	KLLoss 0.0000 (0.2078)	MaskLoss 0.0000 (0.9641)	MaskBCELoss 0.0000 (0.2074)	MaskDICELoss 0.0000 (0.7566)
Epoch: [0][247/500]	Time  9.875 ( 9.875)	Loss 1.6507 (1.4430)	CeLoss 0.3926 (0.4106)	SegCLSLoss 0.0170 (0.0212)	KLLoss 0.2578 (0.2076)	MaskLoss 1.2476 (0.9966)	MaskBCELoss 0.3221 (0.2670)	MaskDICELoss 0.9256 (0.7296)
Epoch: [0][248/500]	Time 11.988 (11.988)	Loss 0.0613 (1.1486)	CeLoss 0.0613 (0.3653)	SegCLSLoss 0.0000 (0.0162)	KLLoss 0.0000 (0.1820)	MaskLoss 0.0000 (0.8335)	MaskBCELoss 0.0000 (0.1725)	MaskDICELoss 0.0000 (0.6610)
Epoch: [0][249/500]	Time 12.738 (12.738)	Loss 1.3265 (1.4607)	CeLoss 0.2871 (0.2900)	SegCLSLoss 0.0271 (0.0222)	KLLoss 0.2578 (0.2600)	MaskLoss 1.1628 (1.2196)	MaskBCELoss 0.2073 (0.2727)	MaskDICELoss 0.9555 (0.9470)
[2025-03-02 16:02:21,989] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=0, lr=[0.0002756938775510204], mom=[(0.9, 0.95)]
[2025-03-02 16:02:21,993] [INFO] [timer.py:215:stop] epoch=0/micro_step=2500/global_step=250, RunningAvgSamplesPerSec=0.9421339023263343, CurrSamplesPerSec=1.01753435912803, MemAllocated=31.24GB, MaxMemAllocated=36.84GB
Epoch: [0][250/500]	Time  9.829 ( 9.829)	Loss 1.2490 (1.4871)	CeLoss 0.2891 (0.4328)	SegCLSLoss 0.0204 (0.0234)	KLLoss 0.2617 (0.2104)	MaskLoss 1.1291 (1.0041)	MaskBCELoss 0.1662 (0.2751)	MaskDICELoss 0.9629 (0.7291)
Epoch: [0][251/500]	Time 10.792 (10.792)	Loss 1.7475 (1.5963)	CeLoss 0.3320 (0.4037)	SegCLSLoss 0.0164 (0.0194)	KLLoss 0.2559 (0.2086)	MaskLoss 1.3236 (1.0934)	MaskBCELoss 0.4019 (0.3457)	MaskDICELoss 0.9217 (0.7477)
Epoch: [0][252/500]	Time  9.297 ( 9.297)	Loss 1.6338 (1.2436)	CeLoss 0.3008 (0.7064)	SegCLSLoss 0.0172 (0.0138)	KLLoss 0.2617 (0.1305)	MaskLoss 1.2826 (0.5856)	MaskBCELoss 0.3545 (0.1112)	MaskDICELoss 0.9281 (0.4745)
Epoch: [0][253/500]	Time 13.170 (13.170)	Loss 0.9947 (1.4739)	CeLoss 0.2041 (0.3184)	SegCLSLoss 0.0317 (0.0250)	KLLoss 0.2617 (0.2334)	MaskLoss 1.0746 (1.1487)	MaskBCELoss 0.0764 (0.2955)	MaskDICELoss 0.9982 (0.8533)
Epoch: [0][254/500]	Time 11.823 (11.823)	Loss 1.7796 (1.4440)	CeLoss 0.2656 (0.2362)	SegCLSLoss 0.0229 (0.0274)	KLLoss 0.2598 (0.2586)	MaskLoss 1.3412 (1.2322)	MaskBCELoss 0.4466 (0.2914)	MaskDICELoss 0.8946 (0.9408)
Epoch: [0][255/500]	Time  8.578 ( 8.578)	Loss 1.6499 (1.4438)	CeLoss 0.2354 (0.6601)	SegCLSLoss 0.0376 (0.0229)	KLLoss 0.2637 (0.1562)	MaskLoss 1.2881 (0.7574)	MaskBCELoss 0.3894 (0.2019)	MaskDICELoss 0.8987 (0.5555)
Epoch: [0][256/500]	Time 12.826 (12.826)	Loss 1.1636 (1.4079)	CeLoss 0.2021 (0.3526)	SegCLSLoss 0.0271 (0.0230)	KLLoss 0.2578 (0.2318)	MaskLoss 1.1199 (1.0978)	MaskBCELoss 0.1690 (0.2470)	MaskDICELoss 0.9509 (0.8508)
Epoch: [0][257/500]	Time 10.439 (10.439)	Loss 1.0000 (1.2407)	CeLoss 1.0000 (0.6403)	SegCLSLoss 0.0000 (0.0158)	KLLoss 0.0000 (0.1549)	MaskLoss 0.0000 (0.6917)	MaskBCELoss 0.0000 (0.1127)	MaskDICELoss 0.0000 (0.5789)
Epoch: [0][258/500]	Time 10.784 (10.784)	Loss 1.7026 (1.3402)	CeLoss 0.3496 (0.3876)	SegCLSLoss 0.0172 (0.0226)	KLLoss 0.2578 (0.2334)	MaskLoss 1.3068 (1.0587)	MaskBCELoss 0.3668 (0.1940)	MaskDICELoss 0.9401 (0.8647)
Epoch: [0][259/500]	Time 12.777 (12.777)	Loss 2.0223 (1.4524)	CeLoss 0.3242 (0.3519)	SegCLSLoss 0.0182 (0.0264)	KLLoss 0.2598 (0.2322)	MaskLoss 1.4537 (1.1213)	MaskBCELoss 0.5396 (0.2694)	MaskDICELoss 0.9141 (0.8519)
[2025-03-02 16:04:13,622] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=0, lr=[0.000274469387755102], mom=[(0.9, 0.95)]
[2025-03-02 16:04:13,627] [INFO] [timer.py:215:stop] epoch=0/micro_step=2600/global_step=260, RunningAvgSamplesPerSec=0.9402536805050794, CurrSamplesPerSec=0.8972579354722777, MemAllocated=30.79GB, MaxMemAllocated=36.84GB
Epoch: [0][260/500]	Time 11.147 (11.147)	Loss 1.1490 (1.3080)	CeLoss 0.3711 (0.4019)	SegCLSLoss 0.0161 (0.0133)	KLLoss 0.2578 (0.1811)	MaskLoss 1.0736 (0.9014)	MaskBCELoss 0.0774 (0.2358)	MaskDICELoss 0.9961 (0.6656)
Epoch: [0][261/500]	Time 10.871 (10.871)	Loss 1.3172 (1.3772)	CeLoss 0.2373 (0.5521)	SegCLSLoss 0.0287 (0.0177)	KLLoss 0.2637 (0.1812)	MaskLoss 1.1529 (0.8545)	MaskBCELoss 0.2214 (0.1939)	MaskDICELoss 0.9315 (0.6606)
Epoch: [0][262/500]	Time  8.927 ( 8.927)	Loss 1.0458 (1.4050)	CeLoss 0.2344 (0.6078)	SegCLSLoss 0.0226 (0.0178)	KLLoss 0.2578 (0.1559)	MaskLoss 1.0844 (0.7622)	MaskBCELoss 0.0925 (0.2104)	MaskDICELoss 0.9920 (0.5517)
Epoch: [0][263/500]	Time 12.415 (12.415)	Loss 1.4642 (1.2893)	CeLoss 0.2295 (0.2472)	SegCLSLoss 0.0304 (0.0244)	KLLoss 0.2617 (0.2332)	MaskLoss 1.2074 (1.0939)	MaskBCELoss 0.3020 (0.2388)	MaskDICELoss 0.9054 (0.8552)
Epoch: [0][264/500]	Time 11.468 (11.468)	Loss 1.0208 (1.4031)	CeLoss 0.2217 (0.4052)	SegCLSLoss 0.0167 (0.0186)	KLLoss 0.2617 (0.2082)	MaskLoss 1.0827 (1.0038)	MaskBCELoss 0.0835 (0.2483)	MaskDICELoss 0.9992 (0.7554)
Epoch: [0][265/500]	Time 12.222 (12.222)	Loss 1.2446 (1.2134)	CeLoss 0.2432 (0.2694)	SegCLSLoss 0.0190 (0.0192)	KLLoss 0.2578 (0.2070)	MaskLoss 1.1340 (0.9890)	MaskBCELoss 0.1924 (0.2223)	MaskDICELoss 0.9416 (0.7667)
Exception in thread Thread-5 (_pin_memory_loop):
Traceback (most recent call last):
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 54, in _pin_memory_loop
    do_one_step()
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 31, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/multiprocessing/reductions.py", line 495, in rebuild_storage_fd
    fd = df.detach()
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/connection.py", line 502, in Client
    c = SocketClient(address)
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/multiprocessing/connection.py", line 630, in SocketClient
    s.connect(address)
FileNotFoundError: [Errno 2] No such file or directory
Epoch: [0][266/500]	Time 10.917 (10.917)	Loss 1.6328 (1.5521)	CeLoss 1.6328 (0.6079)	SegCLSLoss 0.0000 (0.0172)	KLLoss 0.0000 (0.1809)	MaskLoss 0.0000 (0.9073)	MaskBCELoss 0.0000 (0.2539)	MaskDICELoss 0.0000 (0.6534)
Epoch: [0][267/500]	Time 12.218 (12.218)	Loss 1.2277 (1.0818)	CeLoss 0.2178 (0.2725)	SegCLSLoss 0.0182 (0.0196)	KLLoss 0.2598 (0.2072)	MaskLoss 1.1528 (0.9224)	MaskBCELoss 0.1928 (0.1541)	MaskDICELoss 0.9600 (0.7683)
Epoch: [0][268/500]	Time  9.869 ( 9.869)	Loss 0.0830 (1.0888)	CeLoss 0.0830 (0.5227)	SegCLSLoss 0.0000 (0.0092)	KLLoss 0.0000 (0.1291)	MaskLoss 0.0000 (0.6047)	MaskBCELoss 0.0000 (0.1277)	MaskDICELoss 0.0000 (0.4771)
Epoch: [0][269/500]	Time 11.366 (11.366)	Loss 1.1360 (1.3541)	CeLoss 0.2217 (0.4383)	SegCLSLoss 0.0199 (0.0202)	KLLoss 0.2559 (0.2070)	MaskLoss 1.1244 (0.9656)	MaskBCELoss 0.1481 (0.2081)	MaskDICELoss 0.9764 (0.7574)
[2025-03-02 16:06:02,227] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=0, lr=[0.00027324489795918364], mom=[(0.9, 0.95)]
[2025-03-02 16:06:02,233] [INFO] [timer.py:215:stop] epoch=0/micro_step=2700/global_step=270, RunningAvgSamplesPerSec=0.9395693126189121, CurrSamplesPerSec=1.2005172440342997, MemAllocated=31.66GB, MaxMemAllocated=36.84GB
Epoch: [0][270/500]	Time  8.332 ( 8.332)	Loss 1.1256 (1.5799)	CeLoss 0.2090 (0.5938)	SegCLSLoss 0.0210 (0.0148)	KLLoss 0.2559 (0.1803)	MaskLoss 1.1187 (0.9208)	MaskBCELoss 0.1491 (0.2767)	MaskDICELoss 0.9696 (0.6441)
Epoch: [0][271/500]	Time 10.067 (10.067)	Loss 0.6602 (1.3487)	CeLoss 0.6602 (0.2672)	SegCLSLoss 0.0000 (0.0388)	KLLoss 0.0000 (0.2074)	MaskLoss 0.0000 (1.0275)	MaskBCELoss 0.0000 (0.2867)	MaskDICELoss 0.0000 (0.7408)
Epoch: [0][272/500]	Time 11.396 (11.396)	Loss 1.1016 (1.2599)	CeLoss 1.1016 (0.5131)	SegCLSLoss 0.0000 (0.0161)	KLLoss 0.0000 (0.1803)	MaskLoss 0.0000 (0.8251)	MaskBCELoss 0.0000 (0.1557)	MaskDICELoss 0.0000 (0.6694)
Epoch: [0][273/500]	Time 10.308 (10.308)	Loss 1.2865 (1.2711)	CeLoss 0.2236 (0.4711)	SegCLSLoss 0.0178 (0.0159)	KLLoss 0.2598 (0.1797)	MaskLoss 1.1859 (0.8439)	MaskBCELoss 0.2199 (0.1836)	MaskDICELoss 0.9659 (0.6603)
Epoch: [0][274/500]	Time  9.383 ( 9.383)	Loss 1.6485 (1.2107)	CeLoss 0.2637 (0.5818)	SegCLSLoss 0.0156 (0.0159)	KLLoss 0.2559 (0.1541)	MaskLoss 1.3067 (0.6979)	MaskBCELoss 0.3877 (0.1276)	MaskDICELoss 0.9190 (0.5703)
Epoch: [0][275/500]	Time  9.695 ( 9.695)	Loss 1.6016 (1.4989)	CeLoss 1.6016 (0.6057)	SegCLSLoss 0.0000 (0.0167)	KLLoss 0.0000 (0.1811)	MaskLoss 0.0000 (0.8803)	MaskBCELoss 0.0000 (0.2284)	MaskDICELoss 0.0000 (0.6519)
Epoch: [0][276/500]	Time  9.674 ( 9.674)	Loss 1.3281 (1.5348)	CeLoss 1.3281 (0.4288)	SegCLSLoss 0.0000 (0.0197)	KLLoss 0.0000 (0.2070)	MaskLoss 0.0000 (1.0520)	MaskBCELoss 0.0000 (0.3038)	MaskDICELoss 0.0000 (0.7482)
Epoch: [0][277/500]	Time  9.399 ( 9.399)	Loss 1.7972 (1.5086)	CeLoss 0.2119 (0.6132)	SegCLSLoss 0.0225 (0.0137)	KLLoss 0.2617 (0.1564)	MaskLoss 1.3316 (0.7983)	MaskBCELoss 0.4831 (0.2610)	MaskDICELoss 0.8485 (0.5373)
Epoch: [0][278/500]	Time 10.430 (10.430)	Loss 1.8939 (1.4102)	CeLoss 0.2070 (0.4231)	SegCLSLoss 0.0247 (0.0223)	KLLoss 0.2617 (0.1811)	MaskLoss 1.3983 (0.8968)	MaskBCELoss 0.5326 (0.2760)	MaskDICELoss 0.8657 (0.6208)
Epoch: [0][279/500]	Time  9.939 ( 9.939)	Loss 1.6584 (1.4932)	CeLoss 0.2832 (0.5053)	SegCLSLoss 0.0173 (0.0142)	KLLoss 0.2559 (0.1803)	MaskLoss 1.2682 (0.9107)	MaskBCELoss 0.3826 (0.2785)	MaskDICELoss 0.8856 (0.6322)
[2025-03-02 16:07:43,578] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=0, lr=[0.00027202040816326527], mom=[(0.9, 0.95)]
[2025-03-02 16:07:43,583] [INFO] [timer.py:215:stop] epoch=0/micro_step=2800/global_step=280, RunningAvgSamplesPerSec=0.9411902147526122, CurrSamplesPerSec=0.9042581608342649, MemAllocated=31.25GB, MaxMemAllocated=36.84GB
Epoch: [0][280/500]	Time 11.061 (11.061)	Loss 1.1731 (1.4906)	CeLoss 0.3242 (0.2344)	SegCLSLoss 0.0258 (0.0253)	KLLoss 0.2598 (0.2580)	MaskLoss 1.0975 (1.2371)	MaskBCELoss 0.1074 (0.3177)	MaskDICELoss 0.9901 (0.9194)
Epoch: [0][281/500]	Time 11.624 (11.624)	Loss 1.7299 (1.2338)	CeLoss 0.3086 (0.2981)	SegCLSLoss 0.0201 (0.0268)	KLLoss 0.2617 (0.2078)	MaskLoss 1.2433 (0.9522)	MaskBCELoss 0.4010 (0.2165)	MaskDICELoss 0.8423 (0.7357)
Epoch: [0][282/500]	Time 10.635 (10.635)	Loss 1.7109 (1.3645)	CeLoss 1.7109 (0.4604)	SegCLSLoss 0.0000 (0.0149)	KLLoss 0.0000 (0.1816)	MaskLoss 0.0000 (0.8904)	MaskBCELoss 0.0000 (0.2339)	MaskDICELoss 0.0000 (0.6565)
Epoch: [0][283/500]	Time 11.263 (11.263)	Loss 1.8586 (1.2277)	CeLoss 0.1709 (0.3104)	SegCLSLoss 0.0322 (0.0198)	KLLoss 0.2617 (0.2074)	MaskLoss 1.3498 (0.9750)	MaskBCELoss 0.5330 (0.2078)	MaskDICELoss 0.8168 (0.7672)
Epoch: [0][284/500]	Time  9.596 ( 9.596)	Loss 1.2984 (1.4110)	CeLoss 0.3398 (0.6114)	SegCLSLoss 0.0162 (0.0137)	KLLoss 0.2578 (0.1555)	MaskLoss 1.1340 (0.7697)	MaskBCELoss 0.1693 (0.2131)	MaskDICELoss 0.9647 (0.5566)
Epoch: [0][285/500]	Time 10.828 (10.828)	Loss 1.2093 (1.3783)	CeLoss 0.2637 (0.3833)	SegCLSLoss 0.0129 (0.0166)	KLLoss 0.2578 (0.2328)	MaskLoss 1.1575 (1.0735)	MaskBCELoss 0.1623 (0.2181)	MaskDICELoss 0.9952 (0.8554)
Epoch: [0][286/500]	Time 10.707 (10.707)	Loss 1.0325 (1.5562)	CeLoss 0.1973 (0.3484)	SegCLSLoss 0.0322 (0.0249)	KLLoss 0.2578 (0.2338)	MaskLoss 1.1011 (1.1517)	MaskBCELoss 0.1031 (0.3226)	MaskDICELoss 0.9980 (0.8291)
Epoch: [0][287/500]	Time 12.635 (12.635)	Loss 1.0145 (1.2824)	CeLoss 0.2461 (0.3792)	SegCLSLoss 0.0260 (0.0232)	KLLoss 0.2578 (0.2328)	MaskLoss 1.0658 (1.0346)	MaskBCELoss 0.0708 (0.1698)	MaskDICELoss 0.9950 (0.8647)
Epoch: [0][288/500]	Time  9.923 ( 9.923)	Loss 1.2493 (1.0543)	CeLoss 0.2949 (0.4806)	SegCLSLoss 0.0217 (0.0191)	KLLoss 0.2598 (0.1555)	MaskLoss 1.1215 (0.6745)	MaskBCELoss 0.1626 (0.0981)	MaskDICELoss 0.9589 (0.5764)
Epoch: [0][289/500]	Time 11.023 (11.023)	Loss 1.3648 (1.3067)	CeLoss 0.1953 (0.4285)	SegCLSLoss 0.0315 (0.0227)	KLLoss 0.2559 (0.2062)	MaskLoss 1.1990 (0.9470)	MaskBCELoss 0.2749 (0.1895)	MaskDICELoss 0.9242 (0.7575)
[2025-03-02 16:09:32,833] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=0, lr=[0.00027079591836734696], mom=[(0.9, 0.95)]
[2025-03-02 16:09:32,839] [INFO] [timer.py:215:stop] epoch=0/micro_step=2900/global_step=290, RunningAvgSamplesPerSec=0.9402704392446255, CurrSamplesPerSec=0.9075386414884563, MemAllocated=31.26GB, MaxMemAllocated=36.84GB
Epoch: [0][290/500]	Time 11.021 (11.021)	Loss 0.8917 (1.5586)	CeLoss 0.1934 (0.4124)	SegCLSLoss 0.0220 (0.0186)	KLLoss 0.2598 (0.2062)	MaskLoss 1.0284 (1.0927)	MaskBCELoss 0.0349 (0.3237)	MaskDICELoss 0.9936 (0.7690)
Epoch: [0][291/500]	Time 10.245 (10.245)	Loss 1.5868 (1.5949)	CeLoss 0.2578 (0.3267)	SegCLSLoss 0.0303 (0.0210)	KLLoss 0.2578 (0.2074)	MaskLoss 1.2529 (1.1154)	MaskBCELoss 0.3539 (0.3847)	MaskDICELoss 0.8990 (0.7306)
Epoch: [0][292/500]	Time 10.176 (10.176)	Loss 1.4453 (1.3452)	CeLoss 1.4453 (0.6010)	SegCLSLoss 0.0000 (0.0178)	KLLoss 0.0000 (0.1795)	MaskLoss 0.0000 (0.8208)	MaskBCELoss 0.0000 (0.1549)	MaskDICELoss 0.0000 (0.6658)
Epoch: [0][293/500]	Time  9.119 ( 9.119)	Loss 1.0706 (1.4244)	CeLoss 0.2754 (0.3591)	SegCLSLoss 0.0239 (0.0286)	KLLoss 0.2598 (0.2338)	MaskLoss 1.0795 (1.0972)	MaskBCELoss 0.0831 (0.2499)	MaskDICELoss 0.9963 (0.8473)
Epoch: [0][294/500]	Time  8.730 ( 8.730)	Loss 1.2891 (1.3007)	CeLoss 1.2891 (0.6985)	SegCLSLoss 0.0000 (0.0125)	KLLoss 0.0000 (0.1295)	MaskLoss 0.0000 (0.6178)	MaskBCELoss 0.0000 (0.1448)	MaskDICELoss 0.0000 (0.4731)
Epoch: [0][295/500]	Time 10.872 (10.872)	Loss 1.1406 (1.3126)	CeLoss 1.1406 (0.4766)	SegCLSLoss 0.0000 (0.0176)	KLLoss 0.0000 (0.1828)	MaskLoss 0.0000 (0.8532)	MaskBCELoss 0.0000 (0.1980)	MaskDICELoss 0.0000 (0.6552)
Epoch: [0][296/500]	Time 10.389 (10.389)	Loss 0.4434 (1.2957)	CeLoss 0.4434 (0.2425)	SegCLSLoss 0.0000 (0.0249)	KLLoss 0.0000 (0.2334)	MaskLoss 0.0000 (1.0983)	MaskBCELoss 0.0000 (0.2443)	MaskDICELoss 0.0000 (0.8540)
Epoch: [0][297/500]	Time 10.938 (10.938)	Loss 1.4907 (1.4750)	CeLoss 0.2012 (0.3862)	SegCLSLoss 0.0359 (0.0233)	KLLoss 0.2617 (0.2352)	MaskLoss 1.2120 (1.0957)	MaskBCELoss 0.3302 (0.2616)	MaskDICELoss 0.8819 (0.8341)
Epoch: [0][298/500]	Time 10.710 (10.710)	Loss 1.1328 (1.2411)	CeLoss 1.1328 (0.3650)	SegCLSLoss 0.0000 (0.0182)	KLLoss 0.0000 (0.2072)	MaskLoss 0.0000 (0.9511)	MaskBCELoss 0.0000 (0.1878)	MaskDICELoss 0.0000 (0.7632)
Epoch: [0][299/500]	Time 10.707 (10.707)	Loss 0.0713 (1.3019)	CeLoss 0.0713 (0.3179)	SegCLSLoss 0.0000 (0.0160)	KLLoss 0.0000 (0.2086)	MaskLoss 0.0000 (0.9979)	MaskBCELoss 0.0000 (0.2414)	MaskDICELoss 0.0000 (0.7565)
[2025-03-02 16:11:15,993] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=0, lr=[0.00026957142857142853], mom=[(0.9, 0.95)]
[2025-03-02 16:11:15,998] [INFO] [timer.py:215:stop] epoch=0/micro_step=3000/global_step=300, RunningAvgSamplesPerSec=0.9412225092314799, CurrSamplesPerSec=0.8872331224752628, MemAllocated=31.26GB, MaxMemAllocated=36.84GB
Epoch: [0][300/500]	Time 11.273 (11.273)	Loss 1.4747 (1.0808)	CeLoss 0.1982 (0.3262)	SegCLSLoss 0.0288 (0.0224)	KLLoss 0.2617 (0.2072)	MaskLoss 1.2626 (0.9003)	MaskBCELoss 0.3232 (0.1258)	MaskDICELoss 0.9393 (0.7745)
Epoch: [0][301/500]	Time 11.190 (11.190)	Loss 2.1616 (1.3950)	CeLoss 0.2461 (0.3359)	SegCLSLoss 0.0132 (0.0172)	KLLoss 0.2598 (0.2328)	MaskLoss 1.5807 (1.1175)	MaskBCELoss 0.6474 (0.2488)	MaskDICELoss 0.9333 (0.8687)
Epoch: [0][302/500]	Time 11.009 (11.009)	Loss 1.2543 (1.1119)	CeLoss 0.2520 (0.3092)	SegCLSLoss 0.0309 (0.0226)	KLLoss 0.2598 (0.2064)	MaskLoss 1.1491 (0.9205)	MaskBCELoss 0.1844 (0.1508)	MaskDICELoss 0.9647 (0.7697)
Epoch: [0][303/500]	Time 11.236 (11.236)	Loss 0.9872 (1.1730)	CeLoss 0.2461 (0.2975)	SegCLSLoss 0.0194 (0.0187)	KLLoss 0.2578 (0.2074)	MaskLoss 1.0489 (0.9447)	MaskBCELoss 0.0573 (0.1877)	MaskDICELoss 0.9916 (0.7570)
Epoch: [0][304/500]	Time 10.777 (10.777)	Loss 1.0469 (1.3064)	CeLoss 1.0469 (0.4569)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.1816)	MaskLoss 0.0000 (0.8610)	MaskBCELoss 0.0000 (0.2066)	MaskDICELoss 0.0000 (0.6544)
Epoch: [0][305/500]	Time  9.156 ( 9.156)	Loss 0.6641 (1.3669)	CeLoss 0.6641 (0.6166)	SegCLSLoss 0.0000 (0.0122)	KLLoss 0.0000 (0.1545)	MaskLoss 0.0000 (0.7450)	MaskBCELoss 0.0000 (0.1899)	MaskDICELoss 0.0000 (0.5551)
Epoch: [0][306/500]	Time  8.211 ( 8.211)	Loss 1.2812 (1.1692)	CeLoss 1.2812 (0.7105)	SegCLSLoss 0.0000 (0.0122)	KLLoss 0.0000 (0.1035)	MaskLoss 0.0000 (0.4756)	MaskBCELoss 0.0000 (0.1039)	MaskDICELoss 0.0000 (0.3716)
Epoch: [0][307/500]	Time 10.277 (10.277)	Loss 3.6664 (1.2070)	CeLoss 0.3926 (0.4007)	SegCLSLoss 0.0151 (0.0170)	KLLoss 0.2617 (0.1564)	MaskLoss 1.9981 (0.7596)	MaskBCELoss 1.3393 (0.2152)	MaskDICELoss 0.6587 (0.5444)
Epoch: [0][308/500]	Time 10.512 (10.512)	Loss 1.3365 (1.3729)	CeLoss 0.1562 (0.4729)	SegCLSLoss 0.0679 (0.0189)	KLLoss 0.2734 (0.1844)	MaskLoss 1.1665 (0.8637)	MaskBCELoss 0.2535 (0.2291)	MaskDICELoss 0.9130 (0.6345)
Epoch: [0][309/500]	Time 10.510 (10.510)	Loss 0.9436 (1.2615)	CeLoss 0.2383 (0.3621)	SegCLSLoss 0.0309 (0.0242)	KLLoss 0.2578 (0.2088)	MaskLoss 1.0345 (0.9389)	MaskBCELoss 0.0371 (0.1979)	MaskDICELoss 0.9974 (0.7409)
[2025-03-02 16:13:00,210] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=0, lr=[0.00026834693877551016], mom=[(0.9, 0.95)]
[2025-03-02 16:13:00,217] [INFO] [timer.py:215:stop] epoch=0/micro_step=3100/global_step=310, RunningAvgSamplesPerSec=0.9418094819846032, CurrSamplesPerSec=0.8819178645357958, MemAllocated=31.25GB, MaxMemAllocated=36.84GB
Epoch: [0][310/500]	Time 11.341 (11.341)	Loss 1.0584 (1.1652)	CeLoss 0.2559 (0.2950)	SegCLSLoss 0.0247 (0.0211)	KLLoss 0.2617 (0.2084)	MaskLoss 1.0717 (0.9244)	MaskBCELoss 0.0854 (0.1844)	MaskDICELoss 0.9863 (0.7400)
Epoch: [0][311/500]	Time 12.315 (12.315)	Loss 1.6023 (1.3320)	CeLoss 0.1797 (0.2931)	SegCLSLoss 0.0239 (0.0227)	KLLoss 0.2656 (0.2346)	MaskLoss 1.3180 (1.0804)	MaskBCELoss 0.3936 (0.2369)	MaskDICELoss 0.9244 (0.8435)
Epoch: [0][312/500]	Time  9.984 ( 9.984)	Loss 0.8788 (1.0254)	CeLoss 0.1777 (0.4768)	SegCLSLoss 0.0255 (0.0176)	KLLoss 0.2598 (0.1562)	MaskLoss 1.0307 (0.6652)	MaskBCELoss 0.0341 (0.0844)	MaskDICELoss 0.9965 (0.5808)
Epoch: [0][313/500]	Time 11.493 (11.493)	Loss 1.1730 (1.5233)	CeLoss 0.1816 (0.2355)	SegCLSLoss 0.0226 (0.0236)	KLLoss 0.2598 (0.2602)	MaskLoss 1.1252 (1.2705)	MaskBCELoss 0.1840 (0.3307)	MaskDICELoss 0.9412 (0.9399)
Epoch: [0][314/500]	Time 11.180 (11.180)	Loss 0.9966 (1.4605)	CeLoss 0.2520 (0.4849)	SegCLSLoss 0.0189 (0.0135)	KLLoss 0.2578 (0.1812)	MaskLoss 1.0487 (0.9315)	MaskBCELoss 0.0602 (0.2698)	MaskDICELoss 0.9885 (0.6616)
Epoch: [0][315/500]	Time  9.740 ( 9.740)	Loss 1.0736 (1.3182)	CeLoss 0.3047 (0.5561)	SegCLSLoss 0.0201 (0.0165)	KLLoss 0.2559 (0.1816)	MaskLoss 1.0631 (0.8361)	MaskBCELoss 0.0732 (0.1619)	MaskDICELoss 0.9899 (0.6742)
Epoch: [0][316/500]	Time 10.632 (10.632)	Loss 1.4269 (1.3008)	CeLoss 0.2207 (0.5112)	SegCLSLoss 0.0176 (0.0152)	KLLoss 0.2598 (0.1816)	MaskLoss 1.2392 (0.8356)	MaskBCELoss 0.2910 (0.1764)	MaskDICELoss 0.9482 (0.6591)
Epoch: [0][317/500]	Time  9.932 ( 9.932)	Loss 1.5855 (1.2156)	CeLoss 0.2891 (0.4745)	SegCLSLoss 0.0164 (0.0179)	KLLoss 0.2617 (0.1812)	MaskLoss 1.2383 (0.8221)	MaskBCELoss 0.3376 (0.1512)	MaskDICELoss 0.9007 (0.6709)
Epoch: [0][318/500]	Time 11.343 (11.343)	Loss 1.3888 (1.4366)	CeLoss 0.2441 (0.2948)	SegCLSLoss 0.0312 (0.0199)	KLLoss 0.2617 (0.2348)	MaskLoss 1.1966 (1.0997)	MaskBCELoss 0.2568 (0.2904)	MaskDICELoss 0.9399 (0.8093)
Epoch: [0][319/500]	Time  9.666 ( 9.666)	Loss 0.8555 (1.2707)	CeLoss 0.8555 (0.4650)	SegCLSLoss 0.0000 (0.0267)	KLLoss 0.0000 (0.1838)	MaskLoss 0.0000 (0.8348)	MaskBCELoss 0.0000 (0.1797)	MaskDICELoss 0.0000 (0.6551)
[2025-03-02 16:14:46,921] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=0, lr=[0.0002671224489795918], mom=[(0.9, 0.95)]
[2025-03-02 16:14:46,926] [INFO] [timer.py:215:stop] epoch=0/micro_step=3200/global_step=320, RunningAvgSamplesPerSec=0.9416653371690732, CurrSamplesPerSec=0.9594855809766237, MemAllocated=31.25GB, MaxMemAllocated=36.84GB
Epoch: [0][320/500]	Time 10.424 (10.424)	Loss 1.8724 (1.4287)	CeLoss 0.2988 (0.5959)	SegCLSLoss 0.0138 (0.0151)	KLLoss 0.2617 (0.1824)	MaskLoss 1.2786 (0.8346)	MaskBCELoss 0.4823 (0.1980)	MaskDICELoss 0.7963 (0.6366)
Epoch: [0][321/500]	Time 13.152 (13.152)	Loss 1.1797 (1.5138)	CeLoss 0.2354 (0.2188)	SegCLSLoss 0.0240 (0.0310)	KLLoss 0.2578 (0.2621)	MaskLoss 1.1208 (1.2366)	MaskBCELoss 0.1610 (0.3322)	MaskDICELoss 0.9598 (0.9044)
Epoch: [0][322/500]	Time 11.740 (11.740)	Loss 2.0010 (1.1451)	CeLoss 0.3086 (0.4639)	SegCLSLoss 0.0161 (0.0155)	KLLoss 0.2617 (0.1314)	MaskLoss 1.4747 (0.6444)	MaskBCELoss 0.5335 (0.1823)	MaskDICELoss 0.9412 (0.4621)
Epoch: [0][323/500]	Time  9.615 ( 9.615)	Loss 2.0178 (1.2865)	CeLoss 0.2969 (0.5576)	SegCLSLoss 0.0155 (0.0140)	KLLoss 0.2617 (0.1557)	MaskLoss 1.4519 (0.7332)	MaskBCELoss 0.5497 (0.1776)	MaskDICELoss 0.9022 (0.5556)
Epoch: [0][324/500]	Time 10.906 (10.906)	Loss 1.5161 (1.2811)	CeLoss 0.2871 (0.3796)	SegCLSLoss 0.0139 (0.0158)	KLLoss 0.2637 (0.2090)	MaskLoss 1.1752 (0.9288)	MaskBCELoss 0.3044 (0.2017)	MaskDICELoss 0.8708 (0.7271)
Epoch: [0][325/500]	Time 10.536 (10.536)	Loss 1.6248 (1.6441)	CeLoss 0.3008 (0.4460)	SegCLSLoss 0.0164 (0.0204)	KLLoss 0.2598 (0.2086)	MaskLoss 1.2442 (1.0452)	MaskBCELoss 0.3538 (0.3506)	MaskDICELoss 0.8903 (0.6947)
Epoch: [0][326/500]	Time  9.563 ( 9.563)	Loss 1.2656 (1.1416)	CeLoss 1.2656 (0.6393)	SegCLSLoss 0.0000 (0.0187)	KLLoss 0.0000 (0.1299)	MaskLoss 0.0000 (0.5621)	MaskBCELoss 0.0000 (0.0932)	MaskDICELoss 0.0000 (0.4688)
Epoch: [0][327/500]	Time  9.947 ( 9.947)	Loss 2.7524 (1.1884)	CeLoss 0.2812 (0.3126)	SegCLSLoss 0.0184 (0.0125)	KLLoss 0.2656 (0.1576)	MaskLoss 1.4091 (0.7408)	MaskBCELoss 0.9427 (0.2528)	MaskDICELoss 0.4664 (0.4879)
Epoch: [0][328/500]	Time 10.282 (10.282)	Loss 1.5000 (1.5824)	CeLoss 1.5000 (0.4440)	SegCLSLoss 0.0000 (0.0190)	KLLoss 0.0000 (0.2098)	MaskLoss 0.0000 (1.0277)	MaskBCELoss 0.0000 (0.3193)	MaskDICELoss 0.0000 (0.7085)
Epoch: [0][329/500]	Time  9.068 ( 9.068)	Loss 1.2109 (1.2711)	CeLoss 1.2109 (0.4825)	SegCLSLoss 0.0000 (0.0173)	KLLoss 0.0000 (0.1578)	MaskLoss 0.0000 (0.7380)	MaskBCELoss 0.0000 (0.2054)	MaskDICELoss 0.0000 (0.5326)
[2025-03-02 16:16:32,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=0, lr=[0.0002658979591836734], mom=[(0.9, 0.95)]
[2025-03-02 16:16:32,760] [INFO] [timer.py:215:stop] epoch=0/micro_step=3300/global_step=330, RunningAvgSamplesPerSec=0.9417667625038902, CurrSamplesPerSec=0.9072636135269646, MemAllocated=31.26GB, MaxMemAllocated=36.84GB
Epoch: [0][330/500]	Time 11.024 (11.024)	Loss 1.0518 (1.3554)	CeLoss 0.2451 (0.4439)	SegCLSLoss 0.0272 (0.0157)	KLLoss 0.2617 (0.1832)	MaskLoss 1.0760 (0.8996)	MaskBCELoss 0.0858 (0.2355)	MaskDICELoss 0.9903 (0.6640)
Epoch: [0][331/500]	Time 12.642 (12.642)	Loss 0.9090 (1.5246)	CeLoss 0.2178 (0.2801)	SegCLSLoss 0.0206 (0.0182)	KLLoss 0.2617 (0.2633)	MaskLoss 1.0284 (1.2291)	MaskBCELoss 0.0296 (0.3083)	MaskDICELoss 0.9988 (0.9208)
Epoch: [0][332/500]	Time 11.238 (11.238)	Loss 2.6271 (1.5101)	CeLoss 0.2080 (0.3272)	SegCLSLoss 0.0374 (0.0255)	KLLoss 0.2617 (0.2105)	MaskLoss 1.7168 (1.0632)	MaskBCELoss 0.8976 (0.3380)	MaskDICELoss 0.8193 (0.7253)
Epoch: [0][333/500]	Time 12.376 (12.376)	Loss 0.8477 (1.2156)	CeLoss 0.8477 (0.4646)	SegCLSLoss 0.0000 (0.0189)	KLLoss 0.0000 (0.1818)	MaskLoss 0.0000 (0.8197)	MaskBCELoss 0.0000 (0.1557)	MaskDICELoss 0.0000 (0.6640)
Epoch: [0][334/500]	Time 11.076 (11.076)	Loss 1.4424 (1.3351)	CeLoss 0.2715 (0.5216)	SegCLSLoss 0.0160 (0.0184)	KLLoss 0.2637 (0.1824)	MaskLoss 1.1531 (0.8427)	MaskBCELoss 0.2749 (0.1869)	MaskDICELoss 0.8781 (0.6558)
Epoch: [0][335/500]	Time 10.870 (10.870)	Loss 1.6936 (1.4235)	CeLoss 0.2285 (0.2504)	SegCLSLoss 0.0311 (0.0244)	KLLoss 0.2617 (0.2605)	MaskLoss 1.3525 (1.1863)	MaskBCELoss 0.4172 (0.2742)	MaskDICELoss 0.9353 (0.9122)
Epoch: [0][336/500]	Time  9.009 ( 9.009)	Loss 1.4609 (1.2389)	CeLoss 1.4609 (0.5502)	SegCLSLoss 0.0000 (0.0168)	KLLoss 0.0000 (0.1568)	MaskLoss 0.0000 (0.7206)	MaskBCELoss 0.0000 (0.1554)	MaskDICELoss 0.0000 (0.5653)
Epoch: [0][337/500]	Time 12.330 (12.330)	Loss 1.5827 (1.4520)	CeLoss 0.2295 (0.2362)	SegCLSLoss 0.0210 (0.0272)	KLLoss 0.2598 (0.2605)	MaskLoss 1.1331 (1.2045)	MaskBCELoss 0.3745 (0.2956)	MaskDICELoss 0.7586 (0.9089)
Epoch: [0][338/500]	Time 10.919 (10.919)	Loss 1.7188 (1.4772)	CeLoss 1.7188 (0.3725)	SegCLSLoss 0.0000 (0.0208)	KLLoss 0.0000 (0.2334)	MaskLoss 0.0000 (1.1219)	MaskBCELoss 0.0000 (0.2715)	MaskDICELoss 0.0000 (0.8504)
Epoch: [0][339/500]	Time 10.397 (10.397)	Loss 1.2847 (1.2033)	CeLoss 0.1738 (0.3895)	SegCLSLoss 0.0342 (0.0205)	KLLoss 0.2578 (0.1816)	MaskLoss 1.2162 (0.8376)	MaskBCELoss 0.2401 (0.1877)	MaskDICELoss 0.9761 (0.6499)
[2025-03-02 16:18:24,381] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=0, lr=[0.0002646734693877551], mom=[(0.9, 0.95)]
[2025-03-02 16:18:24,389] [INFO] [timer.py:215:stop] epoch=0/micro_step=3400/global_step=340, RunningAvgSamplesPerSec=0.9403439152455171, CurrSamplesPerSec=0.9284413730329465, MemAllocated=31.07GB, MaxMemAllocated=36.84GB
Epoch: [0][340/500]	Time 10.772 (10.772)	Loss 0.0786 (1.0402)	CeLoss 0.0786 (0.2824)	SegCLSLoss 0.0000 (0.0210)	KLLoss 0.0000 (0.2084)	MaskLoss 0.0000 (0.9062)	MaskBCELoss 0.0000 (0.1264)	MaskDICELoss 0.0000 (0.7798)
Epoch: [0][341/500]	Time 10.530 (10.530)	Loss 1.1316 (1.5319)	CeLoss 0.2871 (0.3899)	SegCLSLoss 0.0176 (0.0259)	KLLoss 0.2656 (0.2379)	MaskLoss 1.0584 (1.1121)	MaskBCELoss 0.1040 (0.2851)	MaskDICELoss 0.9544 (0.8269)
Epoch: [0][342/500]	Time 10.897 (10.897)	Loss 1.2523 (1.2090)	CeLoss 0.1885 (0.2650)	SegCLSLoss 0.0596 (0.0363)	KLLoss 0.2637 (0.2381)	MaskLoss 1.1421 (1.0238)	MaskBCELoss 0.2063 (0.1826)	MaskDICELoss 0.9358 (0.8412)
Epoch: [0][343/500]	Time 10.506 (10.506)	Loss 1.4704 (1.3311)	CeLoss 0.3359 (0.2934)	SegCLSLoss 0.0237 (0.0267)	KLLoss 0.2637 (0.2424)	MaskLoss 1.1568 (1.0624)	MaskBCELoss 0.2525 (0.2279)	MaskDICELoss 0.9043 (0.8345)
Epoch: [0][344/500]	Time 10.014 (10.014)	Loss 1.3903 (1.5386)	CeLoss 0.2344 (0.6373)	SegCLSLoss 0.0249 (0.0160)	KLLoss 0.2637 (0.1598)	MaskLoss 1.1794 (0.8036)	MaskBCELoss 0.2626 (0.2597)	MaskDICELoss 0.9168 (0.5439)
Epoch: [0][345/500]	Time 13.167 (13.167)	Loss 1.4238 (1.3512)	CeLoss 0.2002 (0.2609)	SegCLSLoss 0.0288 (0.0216)	KLLoss 0.2598 (0.2664)	MaskLoss 1.2031 (1.1667)	MaskBCELoss 0.2995 (0.2264)	MaskDICELoss 0.9035 (0.9403)
Epoch: [0][346/500]	Time 12.554 (12.554)	Loss 0.9140 (1.1484)	CeLoss 0.1729 (0.2248)	SegCLSLoss 0.0264 (0.0238)	KLLoss 0.2617 (0.2385)	MaskLoss 1.0450 (1.0148)	MaskBCELoss 0.0519 (0.1752)	MaskDICELoss 0.9931 (0.8396)
Epoch: [0][347/500]	Time 12.902 (12.902)	Loss 0.9917 (1.1686)	CeLoss 0.2285 (0.2418)	SegCLSLoss 0.0342 (0.0221)	KLLoss 0.2617 (0.2363)	MaskLoss 1.0568 (1.0323)	MaskBCELoss 0.0613 (0.1788)	MaskDICELoss 0.9955 (0.8535)
Epoch: [0][348/500]	Time 11.738 (11.738)	Loss 1.0303 (1.1693)	CeLoss 0.2480 (0.3079)	SegCLSLoss 0.0349 (0.0262)	KLLoss 0.2637 (0.2375)	MaskLoss 1.0400 (0.9944)	MaskBCELoss 0.0701 (0.1441)	MaskDICELoss 0.9698 (0.8503)
Epoch: [0][349/500]	Time 10.571 (10.571)	Loss 0.9727 (1.2443)	CeLoss 0.1553 (0.3481)	SegCLSLoss 0.0349 (0.0182)	KLLoss 0.2598 (0.2105)	MaskLoss 1.0453 (0.9535)	MaskBCELoss 0.0930 (0.1951)	MaskDICELoss 0.9523 (0.7585)
[2025-03-02 16:20:18,037] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=0, lr=[0.0002634489795918367], mom=[(0.9, 0.95)]
[2025-03-02 16:20:18,043] [INFO] [timer.py:215:stop] epoch=0/micro_step=3500/global_step=350, RunningAvgSamplesPerSec=0.9384936625792332, CurrSamplesPerSec=0.9281625899072976, MemAllocated=30.72GB, MaxMemAllocated=36.84GB
Epoch: [0][350/500]	Time 10.776 (10.776)	Loss 1.4609 (1.5008)	CeLoss 1.4609 (0.4218)	SegCLSLoss 0.0000 (0.0176)	KLLoss 0.0000 (0.2090)	MaskLoss 0.0000 (1.0503)	MaskBCELoss 0.0000 (0.2881)	MaskDICELoss 0.0000 (0.7623)
Epoch: [0][351/500]	Time 10.691 (10.691)	Loss 1.0138 (1.2912)	CeLoss 0.2178 (0.5904)	SegCLSLoss 0.0312 (0.0164)	KLLoss 0.2578 (0.1818)	MaskLoss 1.0719 (0.7881)	MaskBCELoss 0.0824 (0.1313)	MaskDICELoss 0.9895 (0.6568)
Epoch: [0][352/500]	Time 11.627 (11.627)	Loss 1.2580 (1.3582)	CeLoss 0.2578 (0.4473)	SegCLSLoss 0.0175 (0.0158)	KLLoss 0.2637 (0.2090)	MaskLoss 1.1086 (0.9401)	MaskBCELoss 0.1864 (0.2057)	MaskDICELoss 0.9222 (0.7344)
Epoch: [0][353/500]	Time  9.422 ( 9.422)	Loss 1.9186 (1.3412)	CeLoss 0.2578 (0.5704)	SegCLSLoss 0.0264 (0.0124)	KLLoss 0.2617 (0.1570)	MaskLoss 1.3775 (0.7372)	MaskBCELoss 0.5179 (0.1982)	MaskDICELoss 0.8597 (0.5389)
Epoch: [0][354/500]	Time  9.397 ( 9.397)	Loss 1.0547 (1.2683)	CeLoss 1.0547 (0.4704)	SegCLSLoss 0.0000 (0.0226)	KLLoss 0.0000 (0.2078)	MaskLoss 0.0000 (0.8944)	MaskBCELoss 0.0000 (0.1480)	MaskDICELoss 0.0000 (0.7464)
Epoch: [0][355/500]	Time 10.795 (10.795)	Loss 1.5081 (1.2386)	CeLoss 0.1875 (0.3985)	SegCLSLoss 0.0276 (0.0190)	KLLoss 0.2578 (0.1818)	MaskLoss 1.2046 (0.8428)	MaskBCELoss 0.3520 (0.2014)	MaskDICELoss 0.8525 (0.6414)
Epoch: [0][356/500]	Time 11.632 (11.632)	Loss 1.5577 (1.3004)	CeLoss 0.2695 (0.3288)	SegCLSLoss 0.0139 (0.0211)	KLLoss 0.2656 (0.2332)	MaskLoss 1.2051 (1.0371)	MaskBCELoss 0.3308 (0.2055)	MaskDICELoss 0.8742 (0.8316)
Epoch: [0][357/500]	Time 11.075 (11.075)	Loss 1.1172 (1.3273)	CeLoss 1.1172 (0.4513)	SegCLSLoss 0.0000 (0.0184)	KLLoss 0.0000 (0.2072)	MaskLoss 0.0000 (0.9405)	MaskBCELoss 0.0000 (0.1883)	MaskDICELoss 0.0000 (0.7522)
Epoch: [0][358/500]	Time  9.990 ( 9.990)	Loss 1.1719 (1.2389)	CeLoss 1.1719 (0.3525)	SegCLSLoss 0.0000 (0.0221)	KLLoss 0.0000 (0.2068)	MaskLoss 0.0000 (0.9605)	MaskBCELoss 0.0000 (0.1924)	MaskDICELoss 0.0000 (0.7681)
Epoch: [0][359/500]	Time  9.377 ( 9.377)	Loss 2.7078 (1.7160)	CeLoss 0.2119 (0.5064)	SegCLSLoss 0.0275 (0.0166)	KLLoss 0.2617 (0.2088)	MaskLoss 1.7627 (1.0755)	MaskBCELoss 0.9377 (0.3560)	MaskDICELoss 0.8250 (0.7195)
[2025-03-02 16:22:03,839] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=0, lr=[0.00026222448979591837], mom=[(0.9, 0.95)]
[2025-03-02 16:22:03,852] [INFO] [timer.py:215:stop] epoch=0/micro_step=3600/global_step=360, RunningAvgSamplesPerSec=0.9386802357165922, CurrSamplesPerSec=0.8472997872591307, MemAllocated=31.23GB, MaxMemAllocated=36.84GB
Epoch: [0][360/500]	Time 11.804 (11.804)	Loss 1.2843 (1.3780)	CeLoss 0.2793 (0.4044)	SegCLSLoss 0.0166 (0.0194)	KLLoss 0.2578 (0.2074)	MaskLoss 1.1678 (0.9789)	MaskBCELoss 0.1910 (0.2374)	MaskDICELoss 0.9768 (0.7416)
Epoch: [0][361/500]	Time 10.386 (10.386)	Loss 1.6244 (1.3476)	CeLoss 0.2969 (0.6524)	SegCLSLoss 0.0217 (0.0094)	KLLoss 0.2617 (0.1320)	MaskLoss 1.2074 (0.6194)	MaskBCELoss 0.3535 (0.1920)	MaskDICELoss 0.8539 (0.4274)
Epoch: [0][362/500]	Time 11.936 (11.936)	Loss 1.3260 (1.4416)	CeLoss 0.2051 (0.2774)	SegCLSLoss 0.0253 (0.0221)	KLLoss 0.2617 (0.2350)	MaskLoss 1.1944 (1.1132)	MaskBCELoss 0.2444 (0.3009)	MaskDICELoss 0.9500 (0.8124)
Epoch: [0][363/500]	Time 10.347 (10.347)	Loss 1.3877 (1.1836)	CeLoss 0.2227 (0.5853)	SegCLSLoss 0.0190 (0.0143)	KLLoss 0.2617 (0.1299)	MaskLoss 1.1594 (0.5925)	MaskBCELoss 0.2705 (0.1432)	MaskDICELoss 0.8889 (0.4493)
Epoch: [0][364/500]	Time 11.322 (11.322)	Loss 2.0999 (1.4358)	CeLoss 0.2119 (0.2366)	SegCLSLoss 0.0131 (0.0244)	KLLoss 0.2715 (0.2619)	MaskLoss 1.2960 (1.1728)	MaskBCELoss 0.6361 (0.2873)	MaskDICELoss 0.6599 (0.8855)
Epoch: [0][365/500]	Time 11.277 (11.277)	Loss 1.0238 (1.3074)	CeLoss 0.2383 (0.5079)	SegCLSLoss 0.0267 (0.0181)	KLLoss 0.2559 (0.2084)	MaskLoss 1.0622 (0.9018)	MaskBCELoss 0.0800 (0.1491)	MaskDICELoss 0.9822 (0.7527)
Epoch: [0][366/500]	Time 11.725 (11.725)	Loss 1.5457 (1.2798)	CeLoss 0.1992 (0.5186)	SegCLSLoss 0.0234 (0.0170)	KLLoss 0.2578 (0.1814)	MaskLoss 1.2515 (0.7957)	MaskBCELoss 0.3653 (0.1632)	MaskDICELoss 0.8862 (0.6325)
Epoch: [0][367/500]	Time 10.582 (10.582)	Loss 1.5879 (1.5707)	CeLoss 0.1855 (0.4802)	SegCLSLoss 0.0275 (0.0219)	KLLoss 0.2676 (0.2086)	MaskLoss 1.2147 (1.0036)	MaskBCELoss 0.3853 (0.2959)	MaskDICELoss 0.8294 (0.7077)
Epoch: [0][368/500]	Time 11.565 (11.565)	Loss 1.5895 (1.1467)	CeLoss 0.3340 (0.4062)	SegCLSLoss 0.0166 (0.0149)	KLLoss 0.2598 (0.1822)	MaskLoss 1.3003 (0.8140)	MaskBCELoss 0.3159 (0.1513)	MaskDICELoss 0.9844 (0.6627)
Epoch: [0][369/500]	Time 12.382 (12.382)	Loss 1.5175 (1.4519)	CeLoss 0.2109 (0.3796)	SegCLSLoss 0.0182 (0.0224)	KLLoss 0.2617 (0.2340)	MaskLoss 1.2988 (1.1065)	MaskBCELoss 0.3397 (0.2540)	MaskDICELoss 0.9591 (0.8524)
[2025-03-02 16:23:55,517] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=0, lr=[0.000261], mom=[(0.9, 0.95)]
[2025-03-02 16:23:55,525] [INFO] [timer.py:215:stop] epoch=0/micro_step=3700/global_step=370, RunningAvgSamplesPerSec=0.93745430595507, CurrSamplesPerSec=0.9853147762096288, MemAllocated=30.93GB, MaxMemAllocated=36.84GB
Epoch: [0][370/500]	Time 10.151 (10.151)	Loss 1.0703 (1.2764)	CeLoss 1.0703 (0.6025)	SegCLSLoss 0.0000 (0.0196)	KLLoss 0.0000 (0.1566)	MaskLoss 0.0000 (0.7118)	MaskBCELoss 0.0000 (0.1475)	MaskDICELoss 0.0000 (0.5643)
Epoch: [0][371/500]	Time 10.178 (10.178)	Loss 0.9537 (1.6129)	CeLoss 0.2168 (0.4651)	SegCLSLoss 0.0247 (0.0187)	KLLoss 0.2617 (0.2088)	MaskLoss 1.0409 (1.0594)	MaskBCELoss 0.0504 (0.3234)	MaskDICELoss 0.9905 (0.7360)
Epoch: [0][372/500]	Time 10.513 (10.513)	Loss 1.0431 (1.3507)	CeLoss 0.3262 (0.4494)	SegCLSLoss 0.0200 (0.0173)	KLLoss 0.2617 (0.2094)	MaskLoss 1.0407 (0.9268)	MaskBCELoss 0.0419 (0.2005)	MaskDICELoss 0.9988 (0.7263)
Epoch: [0][373/500]	Time 11.096 (11.096)	Loss 0.9185 (1.3568)	CeLoss 0.1777 (0.3757)	SegCLSLoss 0.0261 (0.0216)	KLLoss 0.2617 (0.1848)	MaskLoss 1.0477 (0.8953)	MaskBCELoss 0.0520 (0.2692)	MaskDICELoss 0.9956 (0.6261)
Epoch: [0][374/500]	Time 11.565 (11.565)	Loss 1.6753 (1.3931)	CeLoss 0.2305 (0.4120)	SegCLSLoss 0.0181 (0.0251)	KLLoss 0.2812 (0.2121)	MaskLoss 0.9231 (0.9207)	MaskBCELoss 0.4117 (0.2384)	MaskDICELoss 0.5114 (0.6822)
Epoch: [0][375/500]	Time 11.851 (11.851)	Loss 1.4944 (1.3049)	CeLoss 0.2471 (0.3229)	SegCLSLoss 0.0143 (0.0259)	KLLoss 0.2695 (0.2135)	MaskLoss 1.2536 (0.9719)	MaskBCELoss 0.3032 (0.2341)	MaskDICELoss 0.9504 (0.7378)
Epoch: [0][376/500]	Time 11.330 (11.330)	Loss 0.9844 (1.4192)	CeLoss 0.9844 (0.3595)	SegCLSLoss 0.0000 (0.0195)	KLLoss 0.0000 (0.2145)	MaskLoss 0.0000 (0.9569)	MaskBCELoss 0.0000 (0.2763)	MaskDICELoss 0.0000 (0.6806)
Epoch: [0][377/500]	Time 11.842 (11.842)	Loss 0.0728 (1.1906)	CeLoss 0.0728 (0.3028)	SegCLSLoss 0.0000 (0.0194)	KLLoss 0.0000 (0.2131)	MaskLoss 0.0000 (0.9312)	MaskBCELoss 0.0000 (0.1883)	MaskDICELoss 0.0000 (0.7429)
Epoch: [0][378/500]	Time  9.142 ( 9.142)	Loss 1.2188 (1.6110)	CeLoss 1.2188 (0.5558)	SegCLSLoss 0.0000 (0.0155)	KLLoss 0.0000 (0.1883)	MaskLoss 0.0000 (0.9014)	MaskBCELoss 0.0000 (0.3058)	MaskDICELoss 0.0000 (0.5955)
Epoch: [0][379/500]	Time  9.702 ( 9.702)	Loss 1.4616 (1.5002)	CeLoss 0.1973 (0.4292)	SegCLSLoss 0.0262 (0.0152)	KLLoss 0.2617 (0.1887)	MaskLoss 1.1812 (0.9151)	MaskBCELoss 0.3206 (0.3127)	MaskDICELoss 0.8606 (0.6024)
[2025-03-02 16:25:43,945] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=0, lr=[0.0002597755102040816], mom=[(0.9, 0.95)]
[2025-03-02 16:25:43,953] [INFO] [timer.py:215:stop] epoch=0/micro_step=3800/global_step=380, RunningAvgSamplesPerSec=0.9370494878466186, CurrSamplesPerSec=0.892225825942549, MemAllocated=31.11GB, MaxMemAllocated=36.84GB
Epoch: [0][380/500]	Time 11.210 (11.210)	Loss 1.5467 (1.5359)	CeLoss 0.2119 (0.4846)	SegCLSLoss 0.0262 (0.0164)	KLLoss 0.2578 (0.2135)	MaskLoss 1.2142 (0.9901)	MaskBCELoss 0.3595 (0.2725)	MaskDICELoss 0.8547 (0.7176)
Epoch: [0][381/500]	Time 11.308 (11.308)	Loss 1.1484 (1.1117)	CeLoss 1.1484 (0.4044)	SegCLSLoss 0.0000 (0.0184)	KLLoss 0.0000 (0.2098)	MaskLoss 0.0000 (0.8690)	MaskBCELoss 0.0000 (0.1004)	MaskDICELoss 0.0000 (0.7685)
Epoch: [0][382/500]	Time  9.821 ( 9.821)	Loss 1.2063 (1.3502)	CeLoss 0.4297 (0.3619)	SegCLSLoss 0.0131 (0.0203)	KLLoss 0.2656 (0.2131)	MaskLoss 1.0620 (0.9720)	MaskBCELoss 0.0691 (0.2394)	MaskDICELoss 0.9929 (0.7326)
Epoch: [0][383/500]	Time 11.609 (11.609)	Loss 1.4733 (1.3798)	CeLoss 0.2598 (0.3255)	SegCLSLoss 0.0293 (0.0210)	KLLoss 0.2637 (0.2367)	MaskLoss 1.1842 (1.0804)	MaskBCELoss 0.2916 (0.2434)	MaskDICELoss 0.8926 (0.8370)
Epoch: [0][384/500]	Time 11.780 (11.780)	Loss 1.2950 (1.1769)	CeLoss 0.2402 (0.4371)	SegCLSLoss 0.0228 (0.0150)	KLLoss 0.2617 (0.1830)	MaskLoss 1.1918 (0.8261)	MaskBCELoss 0.2118 (0.1493)	MaskDICELoss 0.9800 (0.6768)
Epoch: [0][385/500]	Time 11.509 (11.509)	Loss 1.1487 (1.2669)	CeLoss 0.1719 (0.3332)	SegCLSLoss 0.0620 (0.0262)	KLLoss 0.2598 (0.2344)	MaskLoss 1.0971 (1.0213)	MaskBCELoss 0.1665 (0.1842)	MaskDICELoss 0.9306 (0.8371)
Epoch: [0][386/500]	Time  9.610 ( 9.610)	Loss 1.5771 (1.3148)	CeLoss 0.1992 (0.3624)	SegCLSLoss 0.0403 (0.0219)	KLLoss 0.2578 (0.2080)	MaskLoss 1.2354 (0.9775)	MaskBCELoss 0.3785 (0.2250)	MaskDICELoss 0.8569 (0.7525)
Epoch: [0][387/500]	Time 12.246 (12.246)	Loss 1.3317 (1.2636)	CeLoss 0.2168 (0.3353)	SegCLSLoss 0.0179 (0.0176)	KLLoss 0.2637 (0.2334)	MaskLoss 1.1751 (1.0486)	MaskBCELoss 0.2423 (0.1826)	MaskDICELoss 0.9328 (0.8659)
Epoch: [0][388/500]	Time  9.653 ( 9.653)	Loss 1.3750 (1.5943)	CeLoss 1.3750 (0.6119)	SegCLSLoss 0.0000 (0.0117)	KLLoss 0.0000 (0.1588)	MaskLoss 0.0000 (0.8396)	MaskBCELoss 0.0000 (0.3030)	MaskDICELoss 0.0000 (0.5366)
Epoch: [0][389/500]	Time  8.863 ( 8.863)	Loss 1.4564 (1.3271)	CeLoss 0.2236 (0.5429)	SegCLSLoss 0.0175 (0.0140)	KLLoss 0.2598 (0.1566)	MaskLoss 1.2735 (0.7572)	MaskBCELoss 0.3048 (0.2042)	MaskDICELoss 0.9688 (0.5530)
[2025-03-02 16:27:31,509] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=0, lr=[0.00025855102040816326], mom=[(0.9, 0.95)]
[2025-03-02 16:27:31,519] [INFO] [timer.py:215:stop] epoch=0/micro_step=3900/global_step=390, RunningAvgSamplesPerSec=0.9368622140779427, CurrSamplesPerSec=0.8957783790491043, MemAllocated=31.23GB, MaxMemAllocated=36.84GB
Epoch: [0][390/500]	Time 11.166 (11.166)	Loss 1.6724 (1.2260)	CeLoss 0.2520 (0.3864)	SegCLSLoss 0.0157 (0.0228)	KLLoss 0.2656 (0.2096)	MaskLoss 1.2745 (0.9197)	MaskBCELoss 0.3979 (0.1671)	MaskDICELoss 0.8766 (0.7526)
Epoch: [0][391/500]	Time 10.047 (10.047)	Loss 1.1016 (1.2590)	CeLoss 1.1016 (0.5156)	SegCLSLoss 0.0000 (0.0176)	KLLoss 0.0000 (0.1822)	MaskLoss 0.0000 (0.7945)	MaskBCELoss 0.0000 (0.1531)	MaskDICELoss 0.0000 (0.6414)
Epoch: [0][392/500]	Time 10.316 (10.316)	Loss 1.7809 (1.3398)	CeLoss 0.3086 (0.2932)	SegCLSLoss 0.0177 (0.0263)	KLLoss 0.2617 (0.2375)	MaskLoss 1.3081 (1.0612)	MaskBCELoss 0.4244 (0.2381)	MaskDICELoss 0.8837 (0.8231)
Epoch: [0][393/500]	Time 11.725 (11.725)	Loss 1.0337 (1.3194)	CeLoss 0.2930 (0.3306)	SegCLSLoss 0.0281 (0.0232)	KLLoss 0.2617 (0.2367)	MaskLoss 1.0485 (1.0568)	MaskBCELoss 0.0530 (0.2097)	MaskDICELoss 0.9955 (0.8470)
Epoch: [0][394/500]	Time 10.946 (10.946)	Loss 0.9414 (1.3705)	CeLoss 0.9414 (0.6092)	SegCLSLoss 0.0000 (0.0202)	KLLoss 0.0000 (0.1576)	MaskLoss 0.0000 (0.7274)	MaskBCELoss 0.0000 (0.1912)	MaskDICELoss 0.0000 (0.5361)
Epoch: [0][395/500]	Time  9.615 ( 9.615)	Loss 0.9471 (1.4647)	CeLoss 0.2324 (0.5501)	SegCLSLoss 0.0226 (0.0159)	KLLoss 0.2617 (0.1850)	MaskLoss 1.0351 (0.8677)	MaskBCELoss 0.0410 (0.2365)	MaskDICELoss 0.9941 (0.6312)
Epoch: [0][396/500]	Time 11.066 (11.066)	Loss 1.4985 (1.3633)	CeLoss 0.2344 (0.3511)	SegCLSLoss 0.0168 (0.0231)	KLLoss 0.2676 (0.2359)	MaskLoss 1.2131 (1.0589)	MaskBCELoss 0.3157 (0.2226)	MaskDICELoss 0.8975 (0.8363)
Epoch: [0][397/500]	Time 11.586 (11.586)	Loss 0.9445 (1.1377)	CeLoss 0.2012 (0.5092)	SegCLSLoss 0.0254 (0.0143)	KLLoss 0.2637 (0.1822)	MaskLoss 1.0428 (0.7730)	MaskBCELoss 0.0516 (0.0947)	MaskDICELoss 0.9912 (0.6784)
Epoch: [0][398/500]	Time 10.452 (10.452)	Loss 0.1787 (1.1943)	CeLoss 0.1787 (0.3281)	SegCLSLoss 0.0000 (0.0193)	KLLoss 0.0000 (0.1811)	MaskLoss 0.0000 (0.8473)	MaskBCELoss 0.0000 (0.2157)	MaskDICELoss 0.0000 (0.6316)
Epoch: [0][399/500]	Time 11.458 (11.458)	Loss 1.4274 (1.3103)	CeLoss 0.2676 (0.3623)	SegCLSLoss 0.0256 (0.0241)	KLLoss 0.2598 (0.2330)	MaskLoss 1.1901 (1.0234)	MaskBCELoss 0.2672 (0.1935)	MaskDICELoss 0.9229 (0.8299)
[2025-03-02 16:29:20,410] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=0, lr=[0.00025732653061224484], mom=[(0.9, 0.95)]
[2025-03-02 16:29:20,415] [INFO] [timer.py:215:stop] epoch=0/micro_step=4000/global_step=400, RunningAvgSamplesPerSec=0.9363911047436343, CurrSamplesPerSec=0.8560076781221541, MemAllocated=31.33GB, MaxMemAllocated=36.84GB
Epoch: [0][400/500]	Time 11.684 (11.684)	Loss 0.9962 (1.1393)	CeLoss 0.2578 (0.3268)	SegCLSLoss 0.0184 (0.0151)	KLLoss 0.2578 (0.2092)	MaskLoss 1.0545 (0.9042)	MaskBCELoss 0.0576 (0.1562)	MaskDICELoss 0.9969 (0.7480)
Epoch: [0][401/500]	Time  8.833 ( 8.833)	Loss 1.0312 (0.9591)	CeLoss 1.0312 (0.5835)	SegCLSLoss 0.0000 (0.0116)	KLLoss 0.0000 (0.1041)	MaskLoss 0.0000 (0.4470)	MaskBCELoss 0.0000 (0.0616)	MaskDICELoss 0.0000 (0.3853)
Epoch: [0][402/500]	Time 11.701 (11.701)	Loss 1.8263 (1.3573)	CeLoss 0.3203 (0.4951)	SegCLSLoss 0.0134 (0.0172)	KLLoss 0.2676 (0.2084)	MaskLoss 1.2571 (0.9286)	MaskBCELoss 0.4427 (0.1807)	MaskDICELoss 0.8144 (0.7479)
Epoch: [0][403/500]	Time 10.299 (10.299)	Loss 0.9494 (1.2111)	CeLoss 0.2891 (0.4603)	SegCLSLoss 0.0244 (0.0194)	KLLoss 0.2559 (0.2062)	MaskLoss 1.0160 (0.8928)	MaskBCELoss 0.0186 (0.1260)	MaskDICELoss 0.9974 (0.7668)
Epoch: [0][404/500]	Time 11.070 (11.070)	Loss 1.0703 (1.3680)	CeLoss 1.0703 (0.3259)	SegCLSLoss 0.0000 (0.0199)	KLLoss 0.0000 (0.2336)	MaskLoss 0.0000 (1.0929)	MaskBCELoss 0.0000 (0.2404)	MaskDICELoss 0.0000 (0.8525)
Epoch: [0][405/500]	Time  9.622 ( 9.622)	Loss 1.7500 (1.4749)	CeLoss 1.7500 (0.5816)	SegCLSLoss 0.0000 (0.0176)	KLLoss 0.0000 (0.1818)	MaskLoss 0.0000 (0.8700)	MaskBCELoss 0.0000 (0.2286)	MaskDICELoss 0.0000 (0.6414)
Epoch: [0][406/500]	Time 11.100 (11.100)	Loss 1.5993 (1.3799)	CeLoss 0.2119 (0.4682)	SegCLSLoss 0.0148 (0.0275)	KLLoss 0.2598 (0.2051)	MaskLoss 1.2578 (0.9453)	MaskBCELoss 0.3870 (0.2069)	MaskDICELoss 0.8708 (0.7383)
Epoch: [0][407/500]	Time 10.463 (10.463)	Loss 1.4587 (1.2783)	CeLoss 0.1943 (0.5146)	SegCLSLoss 0.0192 (0.0139)	KLLoss 0.2559 (0.1799)	MaskLoss 1.1961 (0.8196)	MaskBCELoss 0.3286 (0.1661)	MaskDICELoss 0.8676 (0.6535)
Epoch: [0][408/500]	Time 10.831 (10.831)	Loss 1.9386 (1.3831)	CeLoss 0.2520 (0.6467)	SegCLSLoss 0.0154 (0.0110)	KLLoss 0.2617 (0.1555)	MaskLoss 1.4592 (0.7328)	MaskBCELoss 0.5323 (0.1828)	MaskDICELoss 0.9269 (0.5500)
Epoch: [0][409/500]	Time 10.254 (10.254)	Loss 1.7318 (1.3915)	CeLoss 0.2236 (0.3983)	SegCLSLoss 0.0289 (0.0195)	KLLoss 0.2559 (0.2084)	MaskLoss 1.3233 (0.9521)	MaskBCELoss 0.4471 (0.2480)	MaskDICELoss 0.8762 (0.7041)
[2025-03-02 16:31:05,688] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=0, lr=[0.0002561020408163265], mom=[(0.9, 0.95)]
[2025-03-02 16:31:05,693] [INFO] [timer.py:215:stop] epoch=0/micro_step=4100/global_step=410, RunningAvgSamplesPerSec=0.9367204839508346, CurrSamplesPerSec=0.9008073195749177, MemAllocated=30.97GB, MaxMemAllocated=36.84GB
Epoch: [0][410/500]	Time 11.105 (11.105)	Loss 1.6584 (1.3549)	CeLoss 0.1367 (0.4762)	SegCLSLoss 0.0527 (0.0224)	KLLoss 0.2598 (0.1809)	MaskLoss 1.3064 (0.8632)	MaskBCELoss 0.4443 (0.2207)	MaskDICELoss 0.8621 (0.6425)
Epoch: [0][411/500]	Time 11.606 (11.606)	Loss 1.0037 (1.1078)	CeLoss 0.2246 (0.2951)	SegCLSLoss 0.0190 (0.0202)	KLLoss 0.2617 (0.2318)	MaskLoss 1.0618 (0.9933)	MaskBCELoss 0.0735 (0.1262)	MaskDICELoss 0.9883 (0.8671)
Epoch: [0][412/500]	Time 11.615 (11.615)	Loss 1.1719 (1.2004)	CeLoss 1.1719 (0.3843)	SegCLSLoss 0.0000 (0.0179)	KLLoss 0.0000 (0.2074)	MaskLoss 0.0000 (0.9038)	MaskBCELoss 0.0000 (0.1588)	MaskDICELoss 0.0000 (0.7450)
Epoch: [0][413/500]	Time 11.555 (11.555)	Loss 1.0907 (1.2321)	CeLoss 0.2227 (0.5061)	SegCLSLoss 0.0139 (0.0123)	KLLoss 0.2617 (0.1807)	MaskLoss 1.1076 (0.8226)	MaskBCELoss 0.1190 (0.1453)	MaskDICELoss 0.9886 (0.6773)
Epoch: [0][414/500]	Time 11.403 (11.403)	Loss 1.4886 (1.2584)	CeLoss 0.2246 (0.5283)	SegCLSLoss 0.0223 (0.0118)	KLLoss 0.2617 (0.1822)	MaskLoss 1.1673 (0.7949)	MaskBCELoss 0.3232 (0.1472)	MaskDICELoss 0.8441 (0.6476)
Epoch: [0][415/500]	Time 10.918 (10.918)	Loss 1.3262 (1.1295)	CeLoss 0.2637 (0.3176)	SegCLSLoss 0.0366 (0.0161)	KLLoss 0.2617 (0.2072)	MaskLoss 1.1154 (0.9069)	MaskBCELoss 0.2158 (0.1574)	MaskDICELoss 0.8996 (0.7495)
Epoch: [0][416/500]	Time 11.966 (11.966)	Loss 1.6289 (1.4267)	CeLoss 0.2812 (0.4729)	SegCLSLoss 0.0134 (0.0173)	KLLoss 0.2656 (0.2080)	MaskLoss 1.1524 (0.9451)	MaskBCELoss 0.3649 (0.2285)	MaskDICELoss 0.7874 (0.7167)
Epoch: [0][417/500]	Time 11.016 (11.016)	Loss 0.0623 (1.0508)	CeLoss 0.0623 (0.3967)	SegCLSLoss 0.0000 (0.0156)	KLLoss 0.0000 (0.1562)	MaskLoss 0.0000 (0.6693)	MaskBCELoss 0.0000 (0.1408)	MaskDICELoss 0.0000 (0.5285)
Epoch: [0][418/500]	Time 10.873 (10.873)	Loss 1.3342 (1.0473)	CeLoss 0.1406 (0.2826)	SegCLSLoss 0.0581 (0.0222)	KLLoss 0.2617 (0.2064)	MaskLoss 1.1713 (0.8956)	MaskBCELoss 0.2746 (0.1319)	MaskDICELoss 0.8967 (0.7637)
Epoch: [0][419/500]	Time 13.256 (13.256)	Loss 1.2369 (1.3297)	CeLoss 0.2139 (0.3218)	SegCLSLoss 0.0240 (0.0215)	KLLoss 0.2559 (0.2076)	MaskLoss 1.1838 (0.9925)	MaskBCELoss 0.2001 (0.2536)	MaskDICELoss 0.9837 (0.7390)
[2025-03-02 16:32:59,825] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=0, lr=[0.00025487755102040815], mom=[(0.9, 0.95)]
[2025-03-02 16:32:59,836] [INFO] [timer.py:215:stop] epoch=0/micro_step=4200/global_step=420, RunningAvgSamplesPerSec=0.9351757829484657, CurrSamplesPerSec=1.0068319422104028, MemAllocated=30.72GB, MaxMemAllocated=36.84GB
Epoch: [0][420/500]	Time  9.935 ( 9.935)	Loss 1.6797 (1.3603)	CeLoss 1.6797 (0.6710)	SegCLSLoss 0.0000 (0.0162)	KLLoss 0.0000 (0.1557)	MaskLoss 0.0000 (0.7282)	MaskBCELoss 0.0000 (0.1565)	MaskDICELoss 0.0000 (0.5717)
Epoch: [0][421/500]	Time 10.799 (10.799)	Loss 0.9369 (1.3284)	CeLoss 0.2402 (0.3904)	SegCLSLoss 0.0320 (0.0178)	KLLoss 0.2559 (0.2062)	MaskLoss 1.0323 (0.9315)	MaskBCELoss 0.0358 (0.2231)	MaskDICELoss 0.9965 (0.7085)
Epoch: [0][422/500]	Time  8.265 ( 8.265)	Loss 1.8320 (1.2995)	CeLoss 0.2148 (0.7237)	SegCLSLoss 0.0132 (0.0113)	KLLoss 0.2578 (0.1285)	MaskLoss 1.4010 (0.5926)	MaskBCELoss 0.5019 (0.1337)	MaskDICELoss 0.8990 (0.4590)
Epoch: [0][423/500]	Time  9.431 ( 9.431)	Loss 0.0757 (1.2268)	CeLoss 0.0757 (0.4234)	SegCLSLoss 0.0000 (0.0207)	KLLoss 0.0000 (0.1807)	MaskLoss 0.0000 (0.8233)	MaskBCELoss 0.0000 (0.1837)	MaskDICELoss 0.0000 (0.6396)
Epoch: [0][424/500]	Time 11.865 (11.865)	Loss 1.9028 (1.1085)	CeLoss 0.2793 (0.4483)	SegCLSLoss 0.0134 (0.0136)	KLLoss 0.2617 (0.1551)	MaskLoss 1.3006 (0.6885)	MaskBCELoss 0.5075 (0.1443)	MaskDICELoss 0.7931 (0.5442)
Epoch: [0][425/500]	Time  9.699 ( 9.699)	Loss 0.0728 (1.1638)	CeLoss 0.0728 (0.4640)	SegCLSLoss 0.0000 (0.0169)	KLLoss 0.0000 (0.1545)	MaskLoss 0.0000 (0.6938)	MaskBCELoss 0.0000 (0.1649)	MaskDICELoss 0.0000 (0.5289)
Epoch: [0][426/500]	Time 10.958 (10.958)	Loss 0.9118 (1.1960)	CeLoss 0.2451 (0.3847)	SegCLSLoss 0.0322 (0.0168)	KLLoss 0.2559 (0.2045)	MaskLoss 1.0089 (0.8936)	MaskBCELoss 0.0197 (0.1602)	MaskDICELoss 0.9892 (0.7335)
Epoch: [0][427/500]	Time 11.186 (11.186)	Loss 1.6421 (1.4963)	CeLoss 0.2363 (0.4123)	SegCLSLoss 0.0133 (0.0197)	KLLoss 0.2559 (0.2314)	MaskLoss 1.2414 (1.0576)	MaskBCELoss 0.4022 (0.2662)	MaskDICELoss 0.8392 (0.7914)
Epoch: [0][428/500]	Time  9.982 ( 9.982)	Loss 0.9683 (1.1930)	CeLoss 0.1963 (0.6464)	SegCLSLoss 0.0356 (0.0136)	KLLoss 0.2598 (0.1539)	MaskLoss 1.0439 (0.6515)	MaskBCELoss 0.0682 (0.0878)	MaskDICELoss 0.9757 (0.5636)
Epoch: [0][429/500]	Time  9.441 ( 9.441)	Loss 0.0708 (1.2559)	CeLoss 0.0708 (0.6873)	SegCLSLoss 0.0000 (0.0086)	KLLoss 0.0000 (0.1275)	MaskLoss 0.0000 (0.6080)	MaskBCELoss 0.0000 (0.1306)	MaskDICELoss 0.0000 (0.4773)
[2025-03-02 16:34:43,095] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=0, lr=[0.0002536530612244898], mom=[(0.9, 0.95)]
[2025-03-02 16:34:43,101] [INFO] [timer.py:215:stop] epoch=0/micro_step=4300/global_step=430, RunningAvgSamplesPerSec=0.9359294064729805, CurrSamplesPerSec=0.8593306049822648, MemAllocated=31.23GB, MaxMemAllocated=36.84GB
Epoch: [0][430/500]	Time 11.639 (11.639)	Loss 1.0934 (1.1342)	CeLoss 0.4141 (0.5717)	SegCLSLoss 0.0201 (0.0105)	KLLoss 0.2539 (0.1537)	MaskLoss 0.9844 (0.6587)	MaskBCELoss 0.0343 (0.0968)	MaskDICELoss 0.9501 (0.5618)
Epoch: [0][431/500]	Time 12.408 (12.408)	Loss 1.9568 (1.1995)	CeLoss 0.3945 (0.2408)	SegCLSLoss 0.0231 (0.0174)	KLLoss 0.2578 (0.2061)	MaskLoss 1.4360 (1.0032)	MaskBCELoss 0.4691 (0.2302)	MaskDICELoss 0.9668 (0.7730)
Epoch: [0][432/500]	Time  9.933 ( 9.933)	Loss 2.3644 (1.2432)	CeLoss 0.2021 (0.5328)	SegCLSLoss 0.0140 (0.0148)	KLLoss 0.2598 (0.1553)	MaskLoss 1.7034 (0.7312)	MaskBCELoss 0.7724 (0.1680)	MaskDICELoss 0.9310 (0.5632)
Epoch: [0][433/500]	Time  9.979 ( 9.979)	Loss 0.9258 (1.2759)	CeLoss 0.9258 (0.4830)	SegCLSLoss 0.0000 (0.0215)	KLLoss 0.0000 (0.1822)	MaskLoss 0.0000 (0.8220)	MaskBCELoss 0.0000 (0.1766)	MaskDICELoss 0.0000 (0.6454)
Epoch: [0][434/500]	Time  9.735 ( 9.735)	Loss 1.0098 (1.1351)	CeLoss 0.2539 (0.6521)	SegCLSLoss 0.0176 (0.0136)	KLLoss 0.2578 (0.1295)	MaskLoss 1.0507 (0.5571)	MaskBCELoss 0.0670 (0.0849)	MaskDICELoss 0.9836 (0.4722)
Epoch: [0][435/500]	Time 10.024 (10.024)	Loss 1.0156 (1.2971)	CeLoss 1.0156 (0.6029)	SegCLSLoss 0.0000 (0.0158)	KLLoss 0.0000 (0.1557)	MaskLoss 0.0000 (0.7073)	MaskBCELoss 0.0000 (0.1601)	MaskDICELoss 0.0000 (0.5472)
Epoch: [0][436/500]	Time 11.036 (11.036)	Loss 0.7539 (1.2402)	CeLoss 0.7539 (0.4575)	SegCLSLoss 0.0000 (0.0154)	KLLoss 0.0000 (0.1816)	MaskLoss 0.0000 (0.8148)	MaskBCELoss 0.0000 (0.1738)	MaskDICELoss 0.0000 (0.6410)
Epoch: [0][437/500]	Time 10.753 (10.753)	Loss 0.5781 (0.8126)	CeLoss 0.5781 (0.4210)	SegCLSLoss 0.0000 (0.0109)	KLLoss 0.0000 (0.1033)	MaskLoss 0.0000 (0.4514)	MaskBCELoss 0.0000 (0.0707)	MaskDICELoss 0.0000 (0.3808)
Epoch: [0][438/500]	Time 11.277 (11.277)	Loss 1.4766 (1.3928)	CeLoss 1.4766 (0.5969)	SegCLSLoss 0.0000 (0.0156)	KLLoss 0.0000 (0.1566)	MaskLoss 0.0000 (0.7341)	MaskBCELoss 0.0000 (0.2117)	MaskDICELoss 0.0000 (0.5224)
Epoch: [0][439/500]	Time  9.816 ( 9.816)	Loss 2.1154 (1.3860)	CeLoss 0.2490 (0.4479)	SegCLSLoss 0.0145 (0.0145)	KLLoss 0.2754 (0.1844)	MaskLoss 1.2642 (0.8466)	MaskBCELoss 0.6233 (0.2513)	MaskDICELoss 0.6409 (0.5953)
[2025-03-02 16:36:29,961] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=0, lr=[0.0002524285714285714], mom=[(0.9, 0.95)]
[2025-03-02 16:36:29,966] [INFO] [timer.py:215:stop] epoch=0/micro_step=4400/global_step=440, RunningAvgSamplesPerSec=0.9359294994936189, CurrSamplesPerSec=0.8403182816404083, MemAllocated=30.69GB, MaxMemAllocated=36.84GB
Epoch: [0][440/500]	Time 11.903 (11.903)	Loss 0.9961 (1.2980)	CeLoss 0.9961 (0.4192)	SegCLSLoss 0.0000 (0.0209)	KLLoss 0.0000 (0.2086)	MaskLoss 0.0000 (0.9278)	MaskBCELoss 0.0000 (0.1884)	MaskDICELoss 0.0000 (0.7394)
Epoch: [0][441/500]	Time 10.599 (10.599)	Loss 0.8125 (1.2590)	CeLoss 0.8125 (0.4755)	SegCLSLoss 0.0000 (0.0151)	KLLoss 0.0000 (0.1820)	MaskLoss 0.0000 (0.8095)	MaskBCELoss 0.0000 (0.1741)	MaskDICELoss 0.0000 (0.6354)
Epoch: [0][442/500]	Time 10.973 (10.973)	Loss 1.5740 (1.2844)	CeLoss 0.1797 (0.3069)	SegCLSLoss 0.0168 (0.0212)	KLLoss 0.2617 (0.2076)	MaskLoss 1.3003 (0.9680)	MaskBCELoss 0.3858 (0.2398)	MaskDICELoss 0.9146 (0.7282)
Epoch: [0][443/500]	Time 10.676 (10.676)	Loss 2.0670 (1.0796)	CeLoss 0.1934 (0.4728)	SegCLSLoss 0.0364 (0.0118)	KLLoss 0.2617 (0.1311)	MaskLoss 1.5356 (0.5986)	MaskBCELoss 0.6206 (0.1471)	MaskDICELoss 0.9151 (0.4515)
Epoch: [0][444/500]	Time 12.433 (12.433)	Loss 0.9147 (1.0345)	CeLoss 0.2168 (0.4753)	SegCLSLoss 0.0215 (0.0107)	KLLoss 0.2598 (0.1555)	MaskLoss 1.0329 (0.6556)	MaskBCELoss 0.0344 (0.0936)	MaskDICELoss 0.9985 (0.5621)
Epoch: [0][445/500]	Time 11.288 (11.288)	Loss 1.6197 (1.1980)	CeLoss 0.1670 (0.2932)	SegCLSLoss 0.0447 (0.0168)	KLLoss 0.2598 (0.2086)	MaskLoss 1.3147 (0.9210)	MaskBCELoss 0.4091 (0.2040)	MaskDICELoss 0.9056 (0.7170)
Epoch: [0][446/500]	Time  9.571 ( 9.571)	Loss 0.6875 (1.0637)	CeLoss 0.6875 (0.5796)	SegCLSLoss 0.0000 (0.0092)	KLLoss 0.0000 (0.1043)	MaskLoss 0.0000 (0.4630)	MaskBCELoss 0.0000 (0.1181)	MaskDICELoss 0.0000 (0.3448)
Epoch: [0][447/500]	Time 12.963 (12.963)	Loss 1.4669 (1.2513)	CeLoss 0.3027 (0.3834)	SegCLSLoss 0.0150 (0.0178)	KLLoss 0.2617 (0.2326)	MaskLoss 1.0865 (0.9826)	MaskBCELoss 0.2749 (0.1558)	MaskDICELoss 0.8116 (0.8267)
Epoch: [0][448/500]	Time 11.169 (11.169)	Loss 0.9690 (1.1860)	CeLoss 0.2754 (0.2229)	SegCLSLoss 0.0194 (0.0231)	KLLoss 0.2559 (0.2322)	MaskLoss 1.0219 (1.0531)	MaskBCELoss 0.0368 (0.2005)	MaskDICELoss 0.9851 (0.8526)
Epoch: [0][449/500]	Time  9.923 ( 9.923)	Loss 1.1267 (1.1259)	CeLoss 0.3457 (0.4619)	SegCLSLoss 0.0112 (0.0154)	KLLoss 0.2598 (0.2062)	MaskLoss 1.0176 (0.8423)	MaskBCELoss 0.0810 (0.0840)	MaskDICELoss 0.9366 (0.7583)
[2025-03-02 16:38:20,099] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=0, lr=[0.00025120408163265305], mom=[(0.9, 0.95)]
[2025-03-02 16:38:20,108] [INFO] [timer.py:215:stop] epoch=0/micro_step=4500/global_step=450, RunningAvgSamplesPerSec=0.9352882572737653, CurrSamplesPerSec=0.9483811657916257, MemAllocated=31.24GB, MaxMemAllocated=36.84GB
Epoch: [0][450/500]	Time 10.547 (10.547)	Loss 1.6066 (1.4046)	CeLoss 0.2598 (0.5144)	SegCLSLoss 0.0125 (0.0134)	KLLoss 0.2617 (0.1807)	MaskLoss 1.1596 (0.8523)	MaskBCELoss 0.3692 (0.2300)	MaskDICELoss 0.7903 (0.6223)
Epoch: [0][451/500]	Time 12.196 (12.196)	Loss 0.9399 (1.4298)	CeLoss 0.1875 (0.2508)	SegCLSLoss 0.0234 (0.0193)	KLLoss 0.2539 (0.2570)	MaskLoss 1.0419 (1.1978)	MaskBCELoss 0.0677 (0.2816)	MaskDICELoss 0.9742 (0.9162)
Epoch: [0][452/500]	Time 10.867 (10.867)	Loss 1.3573 (1.3452)	CeLoss 0.2695 (0.3130)	SegCLSLoss 0.0161 (0.0252)	KLLoss 0.2559 (0.2318)	MaskLoss 1.1816 (1.0318)	MaskBCELoss 0.2369 (0.2384)	MaskDICELoss 0.9448 (0.7935)
Epoch: [0][453/500]	Time 10.869 (10.869)	Loss 1.0002 (1.0644)	CeLoss 0.2617 (0.3575)	SegCLSLoss 0.0209 (0.0149)	KLLoss 0.2578 (0.1549)	MaskLoss 1.0258 (0.7192)	MaskBCELoss 0.0572 (0.1671)	MaskDICELoss 0.9686 (0.5521)
Epoch: [0][454/500]	Time 11.863 (11.863)	Loss 1.4113 (1.1643)	CeLoss 0.3125 (0.5205)	SegCLSLoss 0.0201 (0.0146)	KLLoss 0.2559 (0.1547)	MaskLoss 1.1727 (0.6621)	MaskBCELoss 0.2411 (0.1372)	MaskDICELoss 0.9316 (0.5249)
Epoch: [0][455/500]	Time 10.838 (10.838)	Loss 0.8847 (1.3833)	CeLoss 0.2139 (0.4440)	SegCLSLoss 0.0223 (0.0174)	KLLoss 0.2559 (0.1803)	MaskLoss 1.0187 (0.8788)	MaskBCELoss 0.0255 (0.2536)	MaskDICELoss 0.9932 (0.6252)
Epoch: [0][456/500]	Time  9.852 ( 9.852)	Loss 0.9500 (1.2251)	CeLoss 0.2129 (0.5937)	SegCLSLoss 0.0245 (0.0148)	KLLoss 0.2578 (0.1807)	MaskLoss 1.0501 (0.7195)	MaskBCELoss 0.0541 (0.1004)	MaskDICELoss 0.9960 (0.6191)
Epoch: [0][457/500]	Time 10.570 (10.570)	Loss 1.4453 (1.0787)	CeLoss 1.4453 (0.4112)	SegCLSLoss 0.0000 (0.0160)	KLLoss 0.0000 (0.1801)	MaskLoss 0.0000 (0.7643)	MaskBCELoss 0.0000 (0.1171)	MaskDICELoss 0.0000 (0.6472)
Epoch: [0][458/500]	Time 10.886 (10.886)	Loss 1.4519 (1.5072)	CeLoss 0.1206 (0.3078)	SegCLSLoss 0.0549 (0.0212)	KLLoss 0.2578 (0.1801)	MaskLoss 1.2092 (0.9948)	MaskBCELoss 0.3510 (0.3841)	MaskDICELoss 0.8582 (0.6107)
Epoch: [0][459/500]	Time 11.751 (11.751)	Loss 1.0938 (1.2717)	CeLoss 1.0938 (0.4614)	SegCLSLoss 0.0000 (0.0194)	KLLoss 0.0000 (0.2043)	MaskLoss 0.0000 (0.8995)	MaskBCELoss 0.0000 (0.1590)	MaskDICELoss 0.0000 (0.7405)
[2025-03-02 16:40:09,414] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=0, lr=[0.0002499795918367347], mom=[(0.9, 0.95)]
[2025-03-02 16:40:09,419] [INFO] [timer.py:215:stop] epoch=0/micro_step=4600/global_step=460, RunningAvgSamplesPerSec=0.9348347579722218, CurrSamplesPerSec=1.0397150286681756, MemAllocated=30.95GB, MaxMemAllocated=36.84GB
Epoch: [0][460/500]	Time  9.620 ( 9.620)	Loss 0.9938 (1.2345)	CeLoss 0.2100 (0.4448)	SegCLSLoss 0.0178 (0.0196)	KLLoss 0.2578 (0.1797)	MaskLoss 1.0402 (0.8237)	MaskBCELoss 0.0818 (0.1780)	MaskDICELoss 0.9584 (0.6458)
Epoch: [0][461/500]	Time  8.536 ( 8.536)	Loss 1.3701 (1.5019)	CeLoss 0.2637 (0.6939)	SegCLSLoss 0.0216 (0.0236)	KLLoss 0.2617 (0.1549)	MaskLoss 1.0848 (0.7420)	MaskBCELoss 0.2446 (0.2169)	MaskDICELoss 0.8401 (0.5251)
Epoch: [0][462/500]	Time 11.973 (11.973)	Loss 1.0055 (1.4210)	CeLoss 0.2520 (0.2255)	SegCLSLoss 0.0151 (0.0238)	KLLoss 0.2578 (0.2584)	MaskLoss 1.0376 (1.1671)	MaskBCELoss 0.0675 (0.2897)	MaskDICELoss 0.9701 (0.8775)
Epoch: [0][463/500]	Time 11.479 (11.479)	Loss 1.0125 (1.5780)	CeLoss 0.1924 (0.3726)	SegCLSLoss 0.0288 (0.0168)	KLLoss 0.2539 (0.2318)	MaskLoss 1.0959 (1.0922)	MaskBCELoss 0.0990 (0.3287)	MaskDICELoss 0.9969 (0.7635)
Epoch: [0][464/500]	Time  8.905 ( 8.905)	Loss 0.8906 (1.2278)	CeLoss 0.8906 (0.5558)	SegCLSLoss 0.0000 (0.0129)	KLLoss 0.0000 (0.1551)	MaskLoss 0.0000 (0.6976)	MaskBCELoss 0.0000 (0.1504)	MaskDICELoss 0.0000 (0.5472)
Epoch: [0][465/500]	Time 11.615 (11.615)	Loss 0.8977 (1.2755)	CeLoss 0.2119 (0.3652)	SegCLSLoss 0.0167 (0.0228)	KLLoss 0.2598 (0.2336)	MaskLoss 1.0259 (1.0101)	MaskBCELoss 0.0279 (0.1739)	MaskDICELoss 0.9981 (0.8362)
Epoch: [0][466/500]	Time 10.234 (10.234)	Loss 1.0547 (1.4037)	CeLoss 1.0547 (0.5073)	SegCLSLoss 0.0000 (0.0136)	KLLoss 0.0000 (0.1832)	MaskLoss 0.0000 (0.8090)	MaskBCELoss 0.0000 (0.2326)	MaskDICELoss 0.0000 (0.5764)
Epoch: [0][467/500]	Time 11.749 (11.749)	Loss 1.1925 (1.2583)	CeLoss 0.2344 (0.3324)	SegCLSLoss 0.0261 (0.0197)	KLLoss 0.2559 (0.2088)	MaskLoss 1.1152 (0.8940)	MaskBCELoss 0.1701 (0.2153)	MaskDICELoss 0.9451 (0.6786)
Epoch: [0][468/500]	Time 11.673 (11.673)	Loss 1.5080 (1.2601)	CeLoss 0.2578 (0.3088)	SegCLSLoss 0.0239 (0.0154)	KLLoss 0.2559 (0.2096)	MaskLoss 1.2435 (0.9507)	MaskBCELoss 0.3170 (0.2261)	MaskDICELoss 0.9265 (0.7246)
Epoch: [0][469/500]	Time 10.766 (10.766)	Loss 1.4555 (1.3673)	CeLoss 0.2334 (0.3530)	SegCLSLoss 0.0277 (0.0187)	KLLoss 0.2598 (0.2352)	MaskLoss 1.1668 (1.0482)	MaskBCELoss 0.3017 (0.2263)	MaskDICELoss 0.8651 (0.8218)
[2025-03-02 16:41:58,237] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=0, lr=[0.0002487551020408163], mom=[(0.9, 0.95)]
[2025-03-02 16:41:58,243] [INFO] [timer.py:215:stop] epoch=0/micro_step=4700/global_step=470, RunningAvgSamplesPerSec=0.934491648946215, CurrSamplesPerSec=0.8409417777042715, MemAllocated=31.25GB, MaxMemAllocated=36.84GB
Epoch: [0][470/500]	Time 11.893 (11.893)	Loss 1.0824 (1.1042)	CeLoss 0.2373 (0.4336)	SegCLSLoss 0.0205 (0.0196)	KLLoss 0.2559 (0.1820)	MaskLoss 1.0957 (0.7713)	MaskBCELoss 0.1132 (0.1158)	MaskDICELoss 0.9826 (0.6556)
Epoch: [0][471/500]	Time  9.917 ( 9.917)	Loss 1.3203 (1.2750)	CeLoss 1.3203 (0.4151)	SegCLSLoss 0.0000 (0.0151)	KLLoss 0.0000 (0.1828)	MaskLoss 0.0000 (0.8264)	MaskBCELoss 0.0000 (0.2124)	MaskDICELoss 0.0000 (0.6140)
Epoch: [0][472/500]	Time 10.187 (10.187)	Loss 1.4459 (1.0554)	CeLoss 0.2295 (0.3158)	SegCLSLoss 0.0220 (0.0243)	KLLoss 0.2578 (0.2082)	MaskLoss 1.2153 (0.8808)	MaskBCELoss 0.2982 (0.1173)	MaskDICELoss 0.9171 (0.7634)
Epoch: [0][473/500]	Time 12.757 (12.757)	Loss 0.8980 (1.3995)	CeLoss 0.2275 (0.2224)	SegCLSLoss 0.0245 (0.0221)	KLLoss 0.2559 (0.2580)	MaskLoss 1.0237 (1.2015)	MaskBCELoss 0.0240 (0.2790)	MaskDICELoss 0.9997 (0.9225)
Epoch: [0][474/500]	Time 10.524 (10.524)	Loss 0.9375 (1.3481)	CeLoss 0.9375 (0.3482)	SegCLSLoss 0.0000 (0.0173)	KLLoss 0.0000 (0.2059)	MaskLoss 0.0000 (0.9831)	MaskBCELoss 0.0000 (0.2534)	MaskDICELoss 0.0000 (0.7297)
Epoch: [0][475/500]	Time 10.122 (10.122)	Loss 0.2383 (0.9970)	CeLoss 0.2383 (0.6437)	SegCLSLoss 0.0000 (0.0081)	KLLoss 0.0000 (0.1020)	MaskLoss 0.0000 (0.4312)	MaskBCELoss 0.0000 (0.0537)	MaskDICELoss 0.0000 (0.3775)
Epoch: [0][476/500]	Time  9.892 ( 9.892)	Loss 1.3743 (1.1763)	CeLoss 0.1914 (0.4788)	SegCLSLoss 0.0474 (0.0182)	KLLoss 0.2578 (0.1785)	MaskLoss 1.1687 (0.7927)	MaskBCELoss 0.2774 (0.1327)	MaskDICELoss 0.8913 (0.6600)
Epoch: [0][477/500]	Time 11.694 (11.694)	Loss 1.6275 (1.0992)	CeLoss 0.1309 (0.2728)	SegCLSLoss 0.0544 (0.0290)	KLLoss 0.2559 (0.1795)	MaskLoss 1.2937 (0.8297)	MaskBCELoss 0.4349 (0.1949)	MaskDICELoss 0.8588 (0.6348)
Epoch: [0][478/500]	Time 11.110 (11.110)	Loss 1.6915 (0.9622)	CeLoss 0.3184 (0.3137)	SegCLSLoss 0.0143 (0.0150)	KLLoss 0.2637 (0.1545)	MaskLoss 1.1231 (0.6660)	MaskBCELoss 0.3830 (0.1396)	MaskDICELoss 0.7401 (0.5264)
Epoch: [0][479/500]	Time 13.031 (13.031)	Loss 0.7422 (1.0034)	CeLoss 0.7422 (0.3781)	SegCLSLoss 0.0000 (0.0138)	KLLoss 0.0000 (0.1785)	MaskLoss 0.0000 (0.7600)	MaskBCELoss 0.0000 (0.0973)	MaskDICELoss 0.0000 (0.6627)
[2025-03-02 16:43:46,980] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=0, lr=[0.00024753061224489794], mom=[(0.9, 0.95)]
[2025-03-02 16:43:46,985] [INFO] [timer.py:215:stop] epoch=0/micro_step=4800/global_step=480, RunningAvgSamplesPerSec=0.9341780919158285, CurrSamplesPerSec=1.051830495030436, MemAllocated=31.25GB, MaxMemAllocated=36.84GB
Epoch: [0][480/500]	Time  9.509 ( 9.509)	Loss 1.5603 (1.4663)	CeLoss 0.2910 (0.3259)	SegCLSLoss 0.0150 (0.0167)	KLLoss 0.2559 (0.2068)	MaskLoss 1.2114 (0.9854)	MaskBCELoss 0.3298 (0.3262)	MaskDICELoss 0.8815 (0.6592)
Epoch: [0][481/500]	Time 10.025 (10.025)	Loss 1.3096 (1.4707)	CeLoss 0.3145 (0.6062)	SegCLSLoss 0.0128 (0.0137)	KLLoss 0.2598 (0.1537)	MaskLoss 1.0008 (0.7781)	MaskBCELoss 0.1946 (0.2486)	MaskDICELoss 0.8062 (0.5295)
Epoch: [0][482/500]	Time 25.545 (25.545)	Loss 1.4110 (1.2694)	CeLoss 0.2676 (0.4077)	SegCLSLoss 0.0317 (0.0217)	KLLoss 0.2559 (0.2045)	MaskLoss 1.1780 (0.9260)	MaskBCELoss 0.2612 (0.1835)	MaskDICELoss 0.9168 (0.7425)
Epoch: [0][483/500]	Time  9.539 ( 9.539)	Loss 1.9071 (1.3870)	CeLoss 0.4883 (0.5109)	SegCLSLoss 0.0119 (0.0120)	KLLoss 0.2637 (0.1814)	MaskLoss 1.2357 (0.8388)	MaskBCELoss 0.4001 (0.2226)	MaskDICELoss 0.8356 (0.6162)
Epoch: [0][484/500]	Time 12.475 (12.475)	Loss 0.9138 (1.1363)	CeLoss 0.2168 (0.3877)	SegCLSLoss 0.0210 (0.0160)	KLLoss 0.2539 (0.1797)	MaskLoss 1.0336 (0.7987)	MaskBCELoss 0.0401 (0.1589)	MaskDICELoss 0.9935 (0.6398)
Epoch: [0][485/500]	Time 10.640 (10.640)	Loss 0.8711 (1.5830)	CeLoss 0.8711 (0.3982)	SegCLSLoss 0.0000 (0.0134)	KLLoss 0.0000 (0.2070)	MaskLoss 0.0000 (1.0287)	MaskBCELoss 0.0000 (0.3477)	MaskDICELoss 0.0000 (0.6810)
Epoch: [0][486/500]	Time 10.008 (10.008)	Loss 1.2925 (1.0711)	CeLoss 0.2051 (0.3795)	SegCLSLoss 0.0366 (0.0209)	KLLoss 0.2617 (0.1807)	MaskLoss 1.1311 (0.7777)	MaskBCELoss 0.2280 (0.1273)	MaskDICELoss 0.9031 (0.6503)
Epoch: [0][487/500]	Time 10.081 (10.081)	Loss 1.8001 (1.5807)	CeLoss 0.2344 (0.5108)	SegCLSLoss 0.0115 (0.0140)	KLLoss 0.2578 (0.2066)	MaskLoss 1.2462 (0.9446)	MaskBCELoss 0.4830 (0.2921)	MaskDICELoss 0.7632 (0.6525)
Epoch: [0][488/500]	Time 10.370 (10.370)	Loss 1.1641 (1.3032)	CeLoss 1.1641 (0.3967)	SegCLSLoss 0.0000 (0.0126)	KLLoss 0.0000 (0.1801)	MaskLoss 0.0000 (0.8132)	MaskBCELoss 0.0000 (0.2413)	MaskDICELoss 0.0000 (0.5719)
Epoch: [0][489/500]	Time 10.985 (10.985)	Loss 1.2109 (1.3992)	CeLoss 1.2109 (0.3116)	SegCLSLoss 0.0000 (0.0264)	KLLoss 0.0000 (0.2318)	MaskLoss 0.0000 (1.0669)	MaskBCELoss 0.0000 (0.2652)	MaskDICELoss 0.0000 (0.8017)
[2025-03-02 16:45:47,899] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=0, lr=[0.00024630612244897957], mom=[(0.9, 0.95)]
[2025-03-02 16:45:47,905] [INFO] [timer.py:215:stop] epoch=0/micro_step=4900/global_step=490, RunningAvgSamplesPerSec=0.9317065697908419, CurrSamplesPerSec=0.8887850093984215, MemAllocated=31.52GB, MaxMemAllocated=36.84GB
Epoch: [0][490/500]	Time 11.253 (11.253)	Loss 1.1833 (1.2428)	CeLoss 0.3438 (0.5205)	SegCLSLoss 0.0170 (0.0132)	KLLoss 0.2559 (0.1787)	MaskLoss 1.0713 (0.7816)	MaskBCELoss 0.1120 (0.1475)	MaskDICELoss 0.9593 (0.6341)
Epoch: [0][491/500]	Time 10.433 (10.433)	Loss 1.3405 (1.3768)	CeLoss 0.2793 (0.3420)	SegCLSLoss 0.0131 (0.0211)	KLLoss 0.2598 (0.2334)	MaskLoss 0.9417 (1.0089)	MaskBCELoss 0.2324 (0.2406)	MaskDICELoss 0.7092 (0.7683)
Epoch: [0][492/500]	Time 10.198 (10.198)	Loss 0.8366 (1.2300)	CeLoss 0.2002 (0.5396)	SegCLSLoss 0.0259 (0.0106)	KLLoss 0.2539 (0.1559)	MaskLoss 0.9707 (0.6694)	MaskBCELoss 0.0109 (0.1614)	MaskDICELoss 0.9598 (0.5080)
Epoch: [0][493/500]	Time 11.117 (11.117)	Loss 1.0073 (1.1925)	CeLoss 0.2559 (0.3582)	SegCLSLoss 0.0178 (0.0191)	KLLoss 0.2578 (0.2350)	MaskLoss 1.0460 (0.9326)	MaskBCELoss 0.0639 (0.1376)	MaskDICELoss 0.9821 (0.7950)
Epoch: [0][494/500]	Time 12.830 (12.830)	Loss 1.1775 (1.0732)	CeLoss 0.1953 (0.2190)	SegCLSLoss 0.0322 (0.0180)	KLLoss 0.2598 (0.2324)	MaskLoss 1.1460 (0.9994)	MaskBCELoss 0.1750 (0.1474)	MaskDICELoss 0.9711 (0.8520)
Epoch: [0][495/500]	Time 11.568 (11.568)	Loss 0.6367 (1.0774)	CeLoss 0.6367 (0.3732)	SegCLSLoss 0.0000 (0.0154)	KLLoss 0.0000 (0.2072)	MaskLoss 0.0000 (0.8687)	MaskBCELoss 0.0000 (0.1028)	MaskDICELoss 0.0000 (0.7659)
Epoch: [0][496/500]	Time  9.672 ( 9.672)	Loss 1.1031 (1.4007)	CeLoss 0.2031 (0.4532)	SegCLSLoss 0.0183 (0.0183)	KLLoss 0.2578 (0.1822)	MaskLoss 1.1233 (0.8819)	MaskBCELoss 0.1391 (0.2558)	MaskDICELoss 0.9842 (0.6261)
Epoch: [0][497/500]	Time 11.243 (11.243)	Loss 0.8916 (0.9446)	CeLoss 0.1797 (0.5543)	SegCLSLoss 0.0265 (0.0095)	KLLoss 0.2559 (0.1031)	MaskLoss 1.0195 (0.4446)	MaskBCELoss 0.0455 (0.0710)	MaskDICELoss 0.9740 (0.3736)
Epoch: [0][498/500]	Time  8.927 ( 8.927)	Loss 1.3467 (1.3497)	CeLoss 0.2012 (0.5151)	SegCLSLoss 0.0315 (0.0145)	KLLoss 0.2617 (0.1814)	MaskLoss 1.1489 (0.8203)	MaskBCELoss 0.2577 (0.2013)	MaskDICELoss 0.8912 (0.6190)
Epoch: [0][499/500]	Time 11.823 (11.823)	Loss 0.8942 (1.2290)	CeLoss 0.1973 (0.2861)	SegCLSLoss 0.0289 (0.0190)	KLLoss 0.2559 (0.2328)	MaskLoss 1.0317 (0.9920)	MaskBCELoss 0.0360 (0.1937)	MaskDICELoss 0.9957 (0.7983)
[2025-03-02 16:47:36,194] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=0, lr=[0.0002450816326530612], mom=[(0.9, 0.95)]
[2025-03-02 16:47:36,203] [INFO] [timer.py:215:stop] epoch=0/micro_step=5000/global_step=500, RunningAvgSamplesPerSec=0.931540268430771, CurrSamplesPerSec=0.9537870241063682, MemAllocated=31.24GB, MaxMemAllocated=36.84GB
Epoch: [0][500/500]	Time 10.487 (10.487)	Loss 1.5192 (1.2027)	CeLoss 0.2695 (0.5778)	SegCLSLoss 0.0148 (0.0144)	KLLoss 0.2598 (0.1551)	MaskLoss 1.1695 (0.6811)	MaskBCELoss 0.3186 (0.1260)	MaskDICELoss 0.8508 (0.5551)





























100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 199/200 [00:58<00:00,  3.23it/s]
giou: 0.0200, ciou: 0.0000
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:58<00:00,  3.41it/s]
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2025-03-02 16:48:48,564] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./runs/plum-13b_kld_2.0_dice_0.1/ckpt_model/global_step500/mp_rank_00_model_states.pt
[2025-03-02 16:48:48,565] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./runs/plum-13b_kld_2.0_dice_0.1/ckpt_model/global_step500/mp_rank_00_model_states.pt...
[2025-03-02 16:50:36,979] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./runs/plum-13b_kld_2.0_dice_0.1/ckpt_model/global_step500/mp_rank_00_model_states.pt.
[2025-03-02 16:50:37,702] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./runs/plum-13b_kld_2.0_dice_0.1/ckpt_model/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-02 16:50:48,597] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./runs/plum-13b_kld_2.0_dice_0.1/ckpt_model/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-02 16:50:48,599] [INFO] [engine.py:3244:_save_zero_checkpoint] zero checkpoint saved ./runs/plum-13b_kld_2.0_dice_0.1/ckpt_model/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-02 16:50:48,600] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step500 is ready now!
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py:71: UserWarning: Using a target size (torch.Size([333, 5120])) that is different to the input size (torch.Size([5120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return 0.5 / (sigma ** 2) * F.mse_loss(t_proj, t_ref, reduction='mean')
/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch: [1][  1/500]	Time 13.421 (13.421)	Loss 0.8914 (1.0336)	CeLoss 0.1924 (0.3011)	SegCLSLoss 0.0320 (0.0199)	KLLoss 0.2578 (0.1828)	MaskLoss 1.0266 (0.7906)	MaskBCELoss 0.0348 (0.1462)	MaskDICELoss 0.9918 (0.6443)
Epoch: [1][  2/500]	Time 10.906 (10.906)	Loss 1.5958 (1.3529)	CeLoss 0.2168 (0.2930)	SegCLSLoss 0.0157 (0.0239)	KLLoss 0.2598 (0.2342)	MaskLoss 1.2577 (1.0115)	MaskBCELoss 0.3831 (0.2522)	MaskDICELoss 0.8746 (0.7594)
Epoch: [1][  3/500]	Time  9.806 ( 9.806)	Loss 0.9514 (1.1410)	CeLoss 0.2041 (0.3994)	SegCLSLoss 0.0253 (0.0174)	KLLoss 0.2578 (0.1807)	MaskLoss 0.9886 (0.8163)	MaskBCELoss 0.0622 (0.1524)	MaskDICELoss 0.9265 (0.6639)
Epoch: [1][  4/500]	Time 10.017 (10.017)	Loss 1.1804 (1.3935)	CeLoss 0.2373 (0.3964)	SegCLSLoss 0.0164 (0.0176)	KLLoss 0.2598 (0.2086)	MaskLoss 1.1372 (0.9663)	MaskBCELoss 0.1584 (0.2494)	MaskDICELoss 0.9788 (0.7169)
Epoch: [1][  5/500]	Time 11.930 (11.930)	Loss 0.9298 (1.1514)	CeLoss 0.2334 (0.3350)	SegCLSLoss 0.0217 (0.0187)	KLLoss 0.2559 (0.2070)	MaskLoss 1.0019 (0.8914)	MaskBCELoss 0.0378 (0.1597)	MaskDICELoss 0.9641 (0.7318)
Epoch: [1][  6/500]	Time 10.414 (10.414)	Loss 1.5859 (1.2742)	CeLoss 1.5859 (0.5570)	SegCLSLoss 0.0000 (0.0148)	KLLoss 0.0000 (0.1801)	MaskLoss 0.0000 (0.7994)	MaskBCELoss 0.0000 (0.1417)	MaskDICELoss 0.0000 (0.6577)
Epoch: [1][  7/500]	Time 11.519 (11.519)	Loss 0.8686 (1.3036)	CeLoss 0.1992 (0.3625)	SegCLSLoss 0.0315 (0.0238)	KLLoss 0.2578 (0.2314)	MaskLoss 1.0150 (1.0334)	MaskBCELoss 0.0193 (0.1907)	MaskDICELoss 0.9957 (0.8427)
Epoch: [1][  8/500]	Time 12.131 (12.131)	Loss 1.1585 (1.1301)	CeLoss 0.2734 (0.4663)	SegCLSLoss 0.0170 (0.0159)	KLLoss 0.2617 (0.2051)	MaskLoss 1.0032 (0.8413)	MaskBCELoss 0.1334 (0.0852)	MaskDICELoss 0.8698 (0.7561)
Epoch: [1][  9/500]	Time 10.802 (10.802)	Loss 1.3243 (1.3659)	CeLoss 0.1748 (0.4051)	SegCLSLoss 0.0239 (0.0220)	KLLoss 0.2578 (0.2061)	MaskLoss 1.2089 (0.9570)	MaskBCELoss 0.2633 (0.2327)	MaskDICELoss 0.9457 (0.7243)
[2025-03-02 16:52:37,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=510, skipped=0, lr=[0.00024391836734693876], mom=[(0.9, 0.95)]
[2025-03-02 16:52:37,794] [INFO] [timer.py:215:stop] epoch=0/micro_step=5100/global_step=510, RunningAvgSamplesPerSec=0.931228940795497, CurrSamplesPerSec=1.2137977911986186, MemAllocated=30.8GB, MaxMemAllocated=36.84GB
Epoch: [1][ 10/500]	Time  8.241 ( 8.241)	Loss 0.9061 (1.1428)	CeLoss 0.2441 (0.7084)	SegCLSLoss 0.0204 (0.0089)	KLLoss 0.2559 (0.1277)	MaskLoss 1.0170 (0.5454)	MaskBCELoss 0.0204 (0.0631)	MaskDICELoss 0.9966 (0.4823)
Epoch: [1][ 11/500]	Time  9.457 ( 9.457)	Loss 1.5604 (1.3537)	CeLoss 0.2412 (0.4558)	SegCLSLoss 0.0225 (0.0170)	KLLoss 0.2637 (0.2072)	MaskLoss 1.0876 (0.9275)	MaskBCELoss 0.3528 (0.2013)	MaskDICELoss 0.7348 (0.7263)
Epoch: [1][ 12/500]	Time 11.426 (11.426)	Loss 1.3039 (1.0979)	CeLoss 0.3242 (0.4246)	SegCLSLoss 0.0120 (0.0173)	KLLoss 0.2578 (0.2043)	MaskLoss 1.1192 (0.8569)	MaskBCELoss 0.1833 (0.0899)	MaskDICELoss 0.9359 (0.7670)
Epoch: [1][ 13/500]	Time 11.035 (11.035)	Loss 1.5004 (1.3164)	CeLoss 0.2910 (0.2865)	SegCLSLoss 0.0228 (0.0238)	KLLoss 0.2578 (0.2332)	MaskLoss 1.2211 (1.0658)	MaskBCELoss 0.2937 (0.2344)	MaskDICELoss 0.9275 (0.8313)
Epoch: [1][ 14/500]	Time 12.334 (12.334)	Loss 0.9805 (1.2279)	CeLoss 0.2012 (0.2451)	SegCLSLoss 0.0250 (0.0225)	KLLoss 0.2539 (0.2572)	MaskLoss 1.0600 (1.1044)	MaskBCELoss 0.0799 (0.1823)	MaskDICELoss 0.9801 (0.9221)
Epoch: [1][ 15/500]	Time 12.173 (12.173)	Loss 1.0803 (1.2838)	CeLoss 0.1895 (0.2334)	SegCLSLoss 0.0320 (0.0231)	KLLoss 0.2559 (0.2580)	MaskLoss 1.0774 (1.1385)	MaskBCELoss 0.1336 (0.2152)	MaskDICELoss 0.9438 (0.9233)
Epoch: [1][ 16/500]	Time  9.543 ( 9.543)	Loss 1.5315 (1.2773)	CeLoss 0.2461 (0.4543)	SegCLSLoss 0.0178 (0.0174)	KLLoss 0.2559 (0.1811)	MaskLoss 1.2121 (0.8248)	MaskBCELoss 0.3393 (0.1949)	MaskDICELoss 0.8728 (0.6299)
Epoch: [1][ 17/500]	Time 12.038 (12.038)	Loss 1.0425 (1.1625)	CeLoss 0.2480 (0.2942)	SegCLSLoss 0.0226 (0.0220)	KLLoss 0.2539 (0.2314)	MaskLoss 1.0342 (0.9940)	MaskBCELoss 0.0913 (0.1557)	MaskDICELoss 0.9429 (0.8383)
Epoch: [1][ 18/500]	Time 11.232 (11.232)	Loss 1.2734 (1.3355)	CeLoss 1.2734 (0.4326)	SegCLSLoss 0.0000 (0.0205)	KLLoss 0.0000 (0.2082)	MaskLoss 0.0000 (0.9288)	MaskBCELoss 0.0000 (0.2017)	MaskDICELoss 0.0000 (0.7270)
Epoch: [1][ 19/500]	Time 12.402 (12.402)	Loss 0.9028 (1.1479)	CeLoss 0.1797 (0.2226)	SegCLSLoss 0.0325 (0.0226)	KLLoss 0.2559 (0.2574)	MaskLoss 1.0317 (1.0764)	MaskBCELoss 0.0487 (0.1536)	MaskDICELoss 0.9830 (0.9228)
[2025-03-02 16:54:30,894] [INFO] [logging.py:96:log_dist] [Rank 0] step=520, skipped=0, lr=[0.0002426938775510204], mom=[(0.9, 0.95)]
[2025-03-02 16:54:30,902] [INFO] [timer.py:215:stop] epoch=0/micro_step=5200/global_step=520, RunningAvgSamplesPerSec=0.930274254182488, CurrSamplesPerSec=0.8721861066165452, MemAllocated=31.42GB, MaxMemAllocated=36.84GB
Epoch: [1][ 20/500]	Time 11.467 (11.467)	Loss 1.0063 (0.9403)	CeLoss 0.1885 (0.4648)	SegCLSLoss 0.0231 (0.0128)	KLLoss 0.2559 (0.1531)	MaskLoss 1.0872 (0.6381)	MaskBCELoss 0.0982 (0.0520)	MaskDICELoss 0.9890 (0.5860)
Epoch: [1][ 21/500]	Time  8.147 ( 8.147)	Loss 0.9219 (1.4424)	CeLoss 0.9219 (0.7348)	SegCLSLoss 0.0000 (0.0117)	KLLoss 0.0000 (0.1291)	MaskLoss 0.0000 (0.6328)	MaskBCELoss 0.0000 (0.1999)	MaskDICELoss 0.0000 (0.4328)
Epoch: [1][ 22/500]	Time  9.888 ( 9.888)	Loss 1.3125 (1.1363)	CeLoss 1.3125 (0.5479)	SegCLSLoss 0.0000 (0.0148)	KLLoss 0.0000 (0.1287)	MaskLoss 0.0000 (0.5934)	MaskBCELoss 0.0000 (0.1389)	MaskDICELoss 0.0000 (0.4545)
Epoch: [1][ 23/500]	Time 11.519 (11.519)	Loss 1.4896 (1.3897)	CeLoss 0.4629 (0.3688)	SegCLSLoss 0.0121 (0.0146)	KLLoss 0.2578 (0.2051)	MaskLoss 1.0880 (0.9648)	MaskBCELoss 0.2086 (0.2670)	MaskDICELoss 0.8794 (0.6978)
Epoch: [1][ 24/500]	Time 12.041 (12.041)	Loss 1.5037 (1.3339)	CeLoss 0.1338 (0.2577)	SegCLSLoss 0.0698 (0.0228)	KLLoss 0.2578 (0.2572)	MaskLoss 1.1903 (1.1025)	MaskBCELoss 0.3690 (0.2318)	MaskDICELoss 0.8213 (0.8707)
Epoch: [1][ 25/500]	Time 10.005 (10.005)	Loss 1.5846 (1.1049)	CeLoss 0.2969 (0.5506)	SegCLSLoss 0.0148 (0.0119)	KLLoss 0.2598 (0.1535)	MaskLoss 1.1631 (0.6497)	MaskBCELoss 0.3390 (0.0928)	MaskDICELoss 0.8241 (0.5570)
Epoch: [1][ 26/500]	Time 10.846 (10.846)	Loss 0.8745 (1.2157)	CeLoss 0.2305 (0.2938)	SegCLSLoss 0.0186 (0.0191)	KLLoss 0.2598 (0.2072)	MaskLoss 1.0067 (0.9326)	MaskBCELoss 0.0084 (0.2132)	MaskDICELoss 0.9983 (0.7194)
Epoch: [1][ 27/500]	Time  9.794 ( 9.794)	Loss 1.5390 (1.1582)	CeLoss 0.2305 (0.5057)	SegCLSLoss 0.0200 (0.0119)	KLLoss 0.2637 (0.1545)	MaskLoss 0.9280 (0.6696)	MaskBCELoss 0.3582 (0.1428)	MaskDICELoss 0.5697 (0.5268)
Epoch: [1][ 28/500]	Time 13.013 (13.013)	Loss 1.9408 (1.1314)	CeLoss 0.2344 (0.2782)	SegCLSLoss 0.0146 (0.0148)	KLLoss 0.2578 (0.2066)	MaskLoss 1.4920 (0.9266)	MaskBCELoss 0.5441 (0.1788)	MaskDICELoss 0.9480 (0.7479)
Epoch: [1][ 29/500]	Time 10.094 (10.094)	Loss 1.8293 (1.3749)	CeLoss 0.2930 (0.3852)	SegCLSLoss 0.0128 (0.0168)	KLLoss 0.2559 (0.2072)	MaskLoss 1.4338 (1.0105)	MaskBCELoss 0.4597 (0.2451)	MaskDICELoss 0.9741 (0.7655)
[2025-03-02 16:56:17,653] [INFO] [logging.py:96:log_dist] [Rank 0] step=530, skipped=0, lr=[0.00024146938775510202], mom=[(0.9, 0.95)]
[2025-03-02 16:56:17,659] [INFO] [timer.py:215:stop] epoch=0/micro_step=5300/global_step=530, RunningAvgSamplesPerSec=0.9303973808520208, CurrSamplesPerSec=0.8764594007834402, MemAllocated=31.24GB, MaxMemAllocated=36.84GB
Epoch: [1][ 30/500]	Time 11.411 (11.411)	Loss 2.3036 (1.6136)	CeLoss 0.2480 (0.4287)	SegCLSLoss 0.0203 (0.0143)	KLLoss 0.2559 (0.2061)	MaskLoss 1.6918 (1.0852)	MaskBCELoss 0.7184 (0.3460)	MaskDICELoss 0.9734 (0.7392)
Epoch: [1][ 31/500]	Time 11.145 (11.145)	Loss 1.2696 (1.3777)	CeLoss 0.1572 (0.4636)	SegCLSLoss 0.0342 (0.0188)	KLLoss 0.2578 (0.1814)	MaskLoss 1.1410 (0.8817)	MaskBCELoss 0.2463 (0.2387)	MaskDICELoss 0.8946 (0.6430)
Epoch: [1][ 32/500]	Time 11.952 (11.952)	Loss 1.4343 (1.2177)	CeLoss 0.2344 (0.3470)	SegCLSLoss 0.0243 (0.0163)	KLLoss 0.2617 (0.2084)	MaskLoss 1.0390 (0.8873)	MaskBCELoss 0.2952 (0.1880)	MaskDICELoss 0.7439 (0.6994)
Epoch: [1][ 33/500]	Time 10.014 (10.014)	Loss 1.2893 (1.0767)	CeLoss 0.2383 (0.3536)	SegCLSLoss 0.0142 (0.0132)	KLLoss 0.2578 (0.1812)	MaskLoss 1.1241 (0.7933)	MaskBCELoss 0.2185 (0.1445)	MaskDICELoss 0.9056 (0.6488)
Epoch: [1][ 34/500]	Time 10.834 (10.834)	Loss 1.1188 (1.2622)	CeLoss 0.2471 (0.3711)	SegCLSLoss 0.0186 (0.0163)	KLLoss 0.2559 (0.2061)	MaskLoss 1.0984 (0.9068)	MaskBCELoss 0.1281 (0.2003)	MaskDICELoss 0.9703 (0.7065)
Epoch: [1][ 35/500]	Time 11.211 (11.211)	Loss 0.9702 (1.0841)	CeLoss 0.1777 (0.3957)	SegCLSLoss 0.0240 (0.0162)	KLLoss 0.2598 (0.1809)	MaskLoss 1.0625 (0.7806)	MaskBCELoss 0.0806 (0.1268)	MaskDICELoss 0.9820 (0.6538)
Epoch: [1][ 36/500]	Time  9.919 ( 9.919)	Loss 0.9848 (1.3052)	CeLoss 0.2109 (0.5024)	SegCLSLoss 0.0308 (0.0162)	KLLoss 0.2578 (0.1801)	MaskLoss 1.0693 (0.8188)	MaskBCELoss 0.0714 (0.1854)	MaskDICELoss 0.9979 (0.6334)
Epoch: [1][ 37/500]	Time  8.568 ( 8.568)	Loss 1.5787 (1.2975)	CeLoss 0.1816 (0.7607)	SegCLSLoss 0.0312 (0.0054)	KLLoss 0.2598 (0.0773)	MaskLoss 1.2756 (0.4040)	MaskBCELoss 0.3854 (0.1782)	MaskDICELoss 0.8902 (0.2258)
Epoch: [1][ 38/500]	Time  9.022 ( 9.022)	Loss 1.4395 (1.2147)	CeLoss 0.2070 (0.5873)	SegCLSLoss 0.0280 (0.0112)	KLLoss 0.2676 (0.1566)	MaskLoss 1.1805 (0.6689)	MaskBCELoss 0.2967 (0.1269)	MaskDICELoss 0.8838 (0.5420)
Epoch: [1][ 39/500]	Time  9.548 ( 9.548)	Loss 1.2138 (1.2336)	CeLoss 0.2520 (0.4604)	SegCLSLoss 0.0269 (0.0140)	KLLoss 0.2559 (0.1818)	MaskLoss 1.1179 (0.8196)	MaskBCELoss 0.1709 (0.1686)	MaskDICELoss 0.9470 (0.6510)
[2025-03-02 16:57:58,962] [INFO] [logging.py:96:log_dist] [Rank 0] step=540, skipped=0, lr=[0.00024024489795918368], mom=[(0.9, 0.95)]
[2025-03-02 16:57:58,970] [INFO] [timer.py:215:stop] epoch=0/micro_step=5400/global_step=540, RunningAvgSamplesPerSec=0.9313935738513088, CurrSamplesPerSec=1.0995117331602413, MemAllocated=30.7GB, MaxMemAllocated=36.84GB
Epoch: [1][ 40/500]	Time  9.097 ( 9.097)	Loss 1.3281 (1.2298)	CeLoss 1.3281 (0.4873)	SegCLSLoss 0.0000 (0.0132)	KLLoss 0.0000 (0.1818)	MaskLoss 0.0000 (0.8040)	MaskBCELoss 0.0000 (0.1539)	MaskDICELoss 0.0000 (0.6501)
Epoch: [1][ 41/500]	Time 11.094 (11.094)	Loss 1.0309 (1.3914)	CeLoss 0.2266 (0.3631)	SegCLSLoss 0.0215 (0.0131)	KLLoss 0.2559 (0.2082)	MaskLoss 1.0759 (0.9511)	MaskBCELoss 0.0912 (0.2686)	MaskDICELoss 0.9847 (0.6825)
Epoch: [1][ 42/500]	Time 12.293 (12.293)	Loss 1.2207 (1.2123)	CeLoss 0.2578 (0.3473)	SegCLSLoss 0.0165 (0.0173)	KLLoss 0.2578 (0.2061)	MaskLoss 1.1426 (0.9332)	MaskBCELoss 0.1712 (0.1843)	MaskDICELoss 0.9714 (0.7489)
Epoch: [1][ 43/500]	Time 11.518 (11.518)	Loss 1.8072 (1.2471)	CeLoss 0.2090 (0.3780)	SegCLSLoss 0.0154 (0.0162)	KLLoss 0.2715 (0.2088)	MaskLoss 1.0188 (0.8716)	MaskBCELoss 0.4987 (0.1874)	MaskDICELoss 0.5201 (0.6842)
Epoch: [1][ 44/500]	Time 10.573 (10.573)	Loss 1.0408 (0.9943)	CeLoss 0.2158 (0.4242)	SegCLSLoss 0.0262 (0.0144)	KLLoss 0.2578 (0.1533)	MaskLoss 1.0719 (0.6679)	MaskBCELoss 0.1008 (0.0998)	MaskDICELoss 0.9712 (0.5681)
Epoch: [1][ 45/500]	Time 11.178 (11.178)	Loss 1.8581 (1.3980)	CeLoss 0.2461 (0.3838)	SegCLSLoss 0.0122 (0.0228)	KLLoss 0.2598 (0.2330)	MaskLoss 1.3520 (1.0234)	MaskBCELoss 0.4997 (0.2284)	MaskDICELoss 0.8522 (0.7951)
Epoch: [1][ 46/500]	Time 11.583 (11.583)	Loss 1.5284 (1.2980)	CeLoss 0.2637 (0.3922)	SegCLSLoss 0.0145 (0.0122)	KLLoss 0.2617 (0.2070)	MaskLoss 1.1734 (0.9386)	MaskBCELoss 0.3233 (0.2060)	MaskDICELoss 0.8501 (0.7325)
Epoch: [1][ 47/500]	Time  8.895 ( 8.895)	Loss 1.6342 (1.3498)	CeLoss 0.3145 (0.5154)	SegCLSLoss 0.0124 (0.0176)	KLLoss 0.2617 (0.1818)	MaskLoss 1.1430 (0.8199)	MaskBCELoss 0.3559 (0.1997)	MaskDICELoss 0.7871 (0.6201)
Epoch: [1][ 48/500]	Time 10.173 (10.173)	Loss 1.1875 (1.2082)	CeLoss 1.1875 (0.5719)	SegCLSLoss 0.0000 (0.0124)	KLLoss 0.0000 (0.1541)	MaskLoss 0.0000 (0.6899)	MaskBCELoss 0.0000 (0.1331)	MaskDICELoss 0.0000 (0.5568)
Epoch: [1][ 49/500]	Time 10.856 (10.856)	Loss 0.9190 (1.2200)	CeLoss 0.2168 (0.5153)	SegCLSLoss 0.0215 (0.0134)	KLLoss 0.2539 (0.1834)	MaskLoss 1.0294 (0.7292)	MaskBCELoss 0.0430 (0.1360)	MaskDICELoss 0.9864 (0.5932)
[2025-03-02 16:59:46,762] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=0, lr=[0.00023902040816326528], mom=[(0.9, 0.95)]
[2025-03-02 16:59:46,770] [INFO] [timer.py:215:stop] epoch=0/micro_step=5500/global_step=550, RunningAvgSamplesPerSec=0.9313271053000883, CurrSamplesPerSec=1.0378748615060838, MemAllocated=30.93GB, MaxMemAllocated=36.84GB
Epoch: [1][ 50/500]	Time  9.637 ( 9.637)	Loss 1.8676 (1.3005)	CeLoss 0.2275 (0.4231)	SegCLSLoss 0.0120 (0.0174)	KLLoss 0.2793 (0.1820)	MaskLoss 1.1001 (0.8294)	MaskBCELoss 0.5077 (0.2218)	MaskDICELoss 0.5924 (0.6076)
Epoch: [1][ 51/500]	Time 11.246 (11.246)	Loss 1.2551 (1.1919)	CeLoss 0.2539 (0.4120)	SegCLSLoss 0.0295 (0.0213)	KLLoss 0.2539 (0.2053)	MaskLoss 1.1118 (0.8840)	MaskBCELoss 0.1929 (0.1423)	MaskDICELoss 0.9189 (0.7417)
Epoch: [1][ 52/500]	Time  9.746 ( 9.746)	Loss 1.1797 (0.8995)	CeLoss 1.1797 (0.5048)	SegCLSLoss 0.0000 (0.0105)	KLLoss 0.0000 (0.1287)	MaskLoss 0.0000 (0.5114)	MaskBCELoss 0.0000 (0.0427)	MaskDICELoss 0.0000 (0.4688)
Epoch: [1][ 53/500]	Time 12.045 (12.045)	Loss 1.0677 (1.1036)	CeLoss 0.1504 (0.3276)	SegCLSLoss 0.0488 (0.0209)	KLLoss 0.2539 (0.2049)	MaskLoss 1.0846 (0.8719)	MaskBCELoss 0.1451 (0.1412)	MaskDICELoss 0.9395 (0.7308)
Epoch: [1][ 54/500]	Time  9.571 ( 9.571)	Loss 0.0408 (1.2722)	CeLoss 0.0408 (0.3448)	SegCLSLoss 0.0000 (0.0199)	KLLoss 0.0000 (0.1791)	MaskLoss 0.0000 (0.8946)	MaskBCELoss 0.0000 (0.2470)	MaskDICELoss 0.0000 (0.6476)
Epoch: [1][ 55/500]	Time 11.507 (11.507)	Loss 0.8414 (1.2222)	CeLoss 0.2031 (0.3225)	SegCLSLoss 0.0203 (0.0198)	KLLoss 0.2539 (0.2320)	MaskLoss 1.0106 (0.9826)	MaskBCELoss 0.0114 (0.1724)	MaskDICELoss 0.9993 (0.8103)
Epoch: [1][ 56/500]	Time 14.127 (14.127)	Loss 0.9753 (1.1740)	CeLoss 0.2871 (0.2731)	SegCLSLoss 0.0216 (0.0157)	KLLoss 0.2539 (0.2559)	MaskLoss 1.0108 (1.0625)	MaskBCELoss 0.0366 (0.1448)	MaskDICELoss 0.9742 (0.9177)
Epoch: [1][ 57/500]	Time  7.603 ( 7.603)	Loss 1.6796 (1.2980)	CeLoss 0.2422 (0.7907)	SegCLSLoss 0.0115 (0.0096)	KLLoss 0.2754 (0.1049)	MaskLoss 1.0205 (0.4531)	MaskBCELoss 0.4109 (0.1303)	MaskDICELoss 0.6096 (0.3227)
Epoch: [1][ 58/500]	Time  9.179 ( 9.179)	Loss 0.9077 (1.1185)	CeLoss 0.2393 (0.7281)	SegCLSLoss 0.0243 (0.0097)	KLLoss 0.2539 (0.1021)	MaskLoss 1.0234 (0.4456)	MaskBCELoss 0.0250 (0.0720)	MaskDICELoss 0.9984 (0.3736)
Epoch: [1][ 59/500]	Time 10.473 (10.473)	Loss 1.3006 (1.4115)	CeLoss 0.2734 (0.3991)	SegCLSLoss 0.0120 (0.0168)	KLLoss 0.2559 (0.2057)	MaskLoss 1.1396 (0.9683)	MaskBCELoss 0.2092 (0.2608)	MaskDICELoss 0.9303 (0.7075)
[2025-03-02 17:01:33,569] [INFO] [logging.py:96:log_dist] [Rank 0] step=560, skipped=0, lr=[0.00023779591836734691], mom=[(0.9, 0.95)]
[2025-03-02 17:01:33,577] [INFO] [timer.py:215:stop] epoch=0/micro_step=5600/global_step=560, RunningAvgSamplesPerSec=0.9314173285930599, CurrSamplesPerSec=0.8844422656680433, MemAllocated=31.55GB, MaxMemAllocated=36.84GB
Epoch: [1][ 60/500]	Time 11.308 (11.308)	Loss 1.0726 (1.4438)	CeLoss 0.2109 (0.3759)	SegCLSLoss 0.0164 (0.0148)	KLLoss 0.2656 (0.2059)	MaskLoss 0.9055 (0.9824)	MaskBCELoss 0.1221 (0.2895)	MaskDICELoss 0.7834 (0.6929)
Epoch: [1][ 61/500]	Time  9.684 ( 9.684)	Loss 0.0732 (1.0532)	CeLoss 0.0732 (0.6814)	SegCLSLoss 0.0000 (0.0112)	KLLoss 0.0000 (0.1270)	MaskLoss 0.0000 (0.5230)	MaskBCELoss 0.0000 (0.0313)	MaskDICELoss 0.0000 (0.4916)
Epoch: [1][ 62/500]	Time 11.650 (11.650)	Loss 0.9099 (1.3077)	CeLoss 0.1924 (0.2267)	SegCLSLoss 0.0266 (0.0259)	KLLoss 0.2539 (0.2555)	MaskLoss 1.0339 (1.1515)	MaskBCELoss 0.0482 (0.2327)	MaskDICELoss 0.9857 (0.9188)
Epoch: [1][ 63/500]	Time  9.562 ( 9.562)	Loss 1.5312 (1.3947)	CeLoss 1.5312 (0.5575)	SegCLSLoss 0.0000 (0.0151)	KLLoss 0.0000 (0.1834)	MaskLoss 0.0000 (0.7744)	MaskBCELoss 0.0000 (0.2029)	MaskDICELoss 0.0000 (0.5715)
Epoch: [1][ 64/500]	Time 11.086 (11.086)	Loss 0.9442 (1.3241)	CeLoss 0.2207 (0.5146)	SegCLSLoss 0.0134 (0.0139)	KLLoss 0.2578 (0.1818)	MaskLoss 1.0369 (0.8217)	MaskBCELoss 0.0517 (0.1878)	MaskDICELoss 0.9852 (0.6340)
Epoch: [1][ 65/500]	Time  9.413 ( 9.413)	Loss 0.0718 (1.2255)	CeLoss 0.0718 (0.4512)	SegCLSLoss 0.0000 (0.0111)	KLLoss 0.0000 (0.1816)	MaskLoss 0.0000 (0.8022)	MaskBCELoss 0.0000 (0.1709)	MaskDICELoss 0.0000 (0.6312)
Epoch: [1][ 66/500]	Time  9.460 ( 9.460)	Loss 1.0509 (1.2118)	CeLoss 0.2637 (0.6785)	SegCLSLoss 0.0200 (0.0098)	KLLoss 0.2578 (0.1303)	MaskLoss 1.0574 (0.5710)	MaskBCELoss 0.0822 (0.1108)	MaskDICELoss 0.9752 (0.4602)
Epoch: [1][ 67/500]	Time 10.981 (10.981)	Loss 1.1113 (1.3192)	CeLoss 0.2148 (0.4785)	SegCLSLoss 0.0171 (0.0117)	KLLoss 0.2578 (0.1828)	MaskLoss 1.1235 (0.8283)	MaskBCELoss 0.1372 (0.2036)	MaskDICELoss 0.9863 (0.6247)
Epoch: [1][ 68/500]	Time 10.487 (10.487)	Loss 1.4766 (1.2551)	CeLoss 1.4766 (0.4875)	SegCLSLoss 0.0000 (0.0160)	KLLoss 0.0000 (0.2080)	MaskLoss 0.0000 (0.8512)	MaskBCELoss 0.0000 (0.1359)	MaskDICELoss 0.0000 (0.7153)
Epoch: [1][ 69/500]	Time 10.164 (10.164)	Loss 1.3857 (1.2966)	CeLoss 0.1914 (0.4201)	SegCLSLoss 0.0288 (0.0180)	KLLoss 0.2656 (0.2078)	MaskLoss 1.1478 (0.9075)	MaskBCELoss 0.2803 (0.1902)	MaskDICELoss 0.8675 (0.7173)
[2025-03-02 17:03:14,955] [INFO] [logging.py:96:log_dist] [Rank 0] step=570, skipped=0, lr=[0.00023657142857142854], mom=[(0.9, 0.95)]
[2025-03-02 17:03:14,965] [INFO] [timer.py:215:stop] epoch=0/micro_step=5700/global_step=570, RunningAvgSamplesPerSec=0.9323331369964978, CurrSamplesPerSec=1.123941504183498, MemAllocated=31.26GB, MaxMemAllocated=36.84GB
Epoch: [1][ 70/500]	Time  8.899 ( 8.899)	Loss 0.8934 (1.5386)	CeLoss 0.1846 (0.5648)	SegCLSLoss 0.0179 (0.0138)	KLLoss 0.2598 (0.1553)	MaskLoss 1.0232 (0.8266)	MaskBCELoss 0.0422 (0.3020)	MaskDICELoss 0.9810 (0.5246)
Epoch: [1][ 71/500]	Time 10.466 (10.466)	Loss 1.1076 (1.1081)	CeLoss 0.1816 (0.3580)	SegCLSLoss 0.0212 (0.0170)	KLLoss 0.2539 (0.2305)	MaskLoss 1.0973 (0.9613)	MaskBCELoss 0.1572 (0.0970)	MaskDICELoss 0.9401 (0.8643)
Epoch: [1][ 72/500]	Time 11.493 (11.493)	Loss 1.1624 (1.0157)	CeLoss 0.2275 (0.3539)	SegCLSLoss 0.0190 (0.0170)	KLLoss 0.3125 (0.1850)	MaskLoss 1.0906 (0.7696)	MaskBCELoss 0.1000 (0.1087)	MaskDICELoss 0.9906 (0.6609)
Epoch: [1][ 73/500]	Time 10.989 (10.989)	Loss 1.4720 (1.2080)	CeLoss 0.2500 (0.4746)	SegCLSLoss 0.0364 (0.0159)	KLLoss 0.2578 (0.1549)	MaskLoss 1.1900 (0.7214)	MaskBCELoss 0.3009 (0.1809)	MaskDICELoss 0.8891 (0.5406)
Epoch: [1][ 74/500]	Time 11.739 (11.739)	Loss 1.2874 (1.1667)	CeLoss 0.2471 (0.4674)	SegCLSLoss 0.0117 (0.0104)	KLLoss 0.2656 (0.1555)	MaskLoss 1.0041 (0.6898)	MaskBCELoss 0.2135 (0.1654)	MaskDICELoss 0.7906 (0.5244)
Epoch: [1][ 75/500]	Time 11.698 (11.698)	Loss 1.6916 (1.1860)	CeLoss 0.2334 (0.2669)	SegCLSLoss 0.0106 (0.0175)	KLLoss 0.2559 (0.2291)	MaskLoss 1.1222 (0.9920)	MaskBCELoss 0.4365 (0.1857)	MaskDICELoss 0.6857 (0.8063)
Epoch: [1][ 76/500]	Time 11.318 (11.318)	Loss 0.9432 (1.2396)	CeLoss 0.2227 (0.3544)	SegCLSLoss 0.0214 (0.0172)	KLLoss 0.2520 (0.2322)	MaskLoss 1.0316 (0.9547)	MaskBCELoss 0.0536 (0.1667)	MaskDICELoss 0.9781 (0.7880)
Epoch: [1][ 77/500]	Time  9.909 ( 9.909)	Loss 1.6734 (1.3320)	CeLoss 0.3262 (0.4040)	SegCLSLoss 0.0109 (0.0146)	KLLoss 0.2578 (0.1805)	MaskLoss 1.2185 (0.8521)	MaskBCELoss 0.3705 (0.2496)	MaskDICELoss 0.8481 (0.6025)
Epoch: [1][ 78/500]	Time 10.883 (10.883)	Loss 1.9715 (1.3648)	CeLoss 0.2617 (0.4349)	SegCLSLoss 0.0244 (0.0215)	KLLoss 0.2656 (0.2080)	MaskLoss 1.4008 (0.9184)	MaskBCELoss 0.5404 (0.2166)	MaskDICELoss 0.8604 (0.7018)
Epoch: [1][ 79/500]	Time 10.503 (10.503)	Loss 0.8772 (0.9373)	CeLoss 0.1865 (0.4354)	SegCLSLoss 0.0292 (0.0207)	KLLoss 0.2578 (0.1545)	MaskLoss 1.0257 (0.6436)	MaskBCELoss 0.0294 (0.0620)	MaskDICELoss 0.9963 (0.5816)
[2025-03-02 17:05:05,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=580, skipped=0, lr=[0.00023534693877551018], mom=[(0.9, 0.95)]
[2025-03-02 17:05:05,735] [INFO] [timer.py:215:stop] epoch=0/micro_step=5800/global_step=580, RunningAvgSamplesPerSec=0.931807164103482, CurrSamplesPerSec=0.8495745639364155, MemAllocated=31.45GB, MaxMemAllocated=36.84GB
Epoch: [1][ 80/500]	Time 11.772 (11.772)	Loss 0.9826 (1.1487)	CeLoss 0.2041 (0.2719)	SegCLSLoss 0.0211 (0.0176)	KLLoss 0.2578 (0.2340)	MaskLoss 1.0604 (0.9878)	MaskBCELoss 0.0769 (0.1583)	MaskDICELoss 0.9835 (0.8295)
Epoch: [1][ 81/500]	Time 12.621 (12.621)	Loss 0.9842 (1.1551)	CeLoss 0.3066 (0.2394)	SegCLSLoss 0.0165 (0.0193)	KLLoss 0.2617 (0.2336)	MaskLoss 1.0172 (1.0013)	MaskBCELoss 0.0245 (0.1780)	MaskDICELoss 0.9927 (0.8232)
Epoch: [1][ 82/500]	Time 11.858 (11.858)	Loss 0.2930 (1.0068)	CeLoss 0.2930 (0.2918)	SegCLSLoss 0.0000 (0.0161)	KLLoss 0.0000 (0.1822)	MaskLoss 0.0000 (0.7952)	MaskBCELoss 0.0000 (0.1383)	MaskDICELoss 0.0000 (0.6569)
Epoch: [1][ 83/500]	Time 10.077 (10.077)	Loss 1.1875 (1.3618)	CeLoss 1.1875 (0.4326)	SegCLSLoss 0.0000 (0.0140)	KLLoss 0.0000 (0.2109)	MaskLoss 0.0000 (0.8963)	MaskBCELoss 0.0000 (0.2156)	MaskDICELoss 0.0000 (0.6807)
Epoch: [1][ 84/500]	Time 12.395 (12.395)	Loss 1.8575 (1.1208)	CeLoss 0.2930 (0.2931)	SegCLSLoss 0.0124 (0.0201)	KLLoss 0.2715 (0.2092)	MaskLoss 1.2205 (0.8801)	MaskBCELoss 0.4693 (0.1638)	MaskDICELoss 0.7511 (0.7164)
Epoch: [1][ 85/500]	Time 10.686 (10.686)	Loss 0.8750 (1.0256)	CeLoss 0.8750 (0.4665)	SegCLSLoss 0.0000 (0.0142)	KLLoss 0.0000 (0.1814)	MaskLoss 0.0000 (0.7270)	MaskBCELoss 0.0000 (0.0613)	MaskDICELoss 0.0000 (0.6656)
Epoch: [1][ 86/500]	Time  9.728 ( 9.728)	Loss 2.2275 (1.4541)	CeLoss 0.2070 (0.3526)	SegCLSLoss 0.0197 (0.0161)	KLLoss 0.2637 (0.2117)	MaskLoss 1.5744 (0.9694)	MaskBCELoss 0.6989 (0.3019)	MaskDICELoss 0.8755 (0.6676)
Epoch: [1][ 87/500]	Time 11.355 (11.355)	Loss 0.8915 (1.2247)	CeLoss 0.2139 (0.2862)	SegCLSLoss 0.0239 (0.0191)	KLLoss 0.2578 (0.2334)	MaskLoss 0.9967 (1.0126)	MaskBCELoss 0.0261 (0.1900)	MaskDICELoss 0.9706 (0.8227)
Epoch: [1][ 88/500]	Time 10.764 (10.764)	Loss 1.1051 (1.3257)	CeLoss 0.2656 (0.3896)	SegCLSLoss 0.0245 (0.0180)	KLLoss 0.2578 (0.1814)	MaskLoss 1.0631 (0.8604)	MaskBCELoss 0.1083 (0.2518)	MaskDICELoss 0.9548 (0.6086)
Epoch: [1][ 89/500]	Time 10.844 (10.844)	Loss 1.2678 (1.4556)	CeLoss 0.3633 (0.4134)	SegCLSLoss 0.0154 (0.0165)	KLLoss 0.2578 (0.2340)	MaskLoss 1.1085 (1.0488)	MaskBCELoss 0.1423 (0.2428)	MaskDICELoss 0.9662 (0.8060)
[2025-03-02 17:06:55,962] [INFO] [logging.py:96:log_dist] [Rank 0] step=590, skipped=0, lr=[0.00023412244897959183], mom=[(0.9, 0.95)]
[2025-03-02 17:06:55,969] [INFO] [timer.py:215:stop] epoch=0/micro_step=5900/global_step=590, RunningAvgSamplesPerSec=0.9313787710699738, CurrSamplesPerSec=1.009670250046189, MemAllocated=30.81GB, MaxMemAllocated=36.84GB
Epoch: [1][ 90/500]	Time  9.906 ( 9.906)	Loss 1.0915 (1.1397)	CeLoss 0.1836 (0.5788)	SegCLSLoss 0.0400 (0.0132)	KLLoss 0.2539 (0.1537)	MaskLoss 1.1052 (0.6643)	MaskBCELoss 0.1421 (0.0951)	MaskDICELoss 0.9631 (0.5692)
Epoch: [1][ 91/500]	Time 11.187 (11.187)	Loss 1.6121 (1.2129)	CeLoss 0.2500 (0.3433)	SegCLSLoss 0.0114 (0.0176)	KLLoss 0.2598 (0.2314)	MaskLoss 1.2980 (0.9907)	MaskBCELoss 0.3731 (0.1574)	MaskDICELoss 0.9249 (0.8333)
Epoch: [1][ 92/500]	Time 11.151 (11.151)	Loss 1.5809 (1.3204)	CeLoss 0.2637 (0.3449)	SegCLSLoss 0.0137 (0.0190)	KLLoss 0.2578 (0.2313)	MaskLoss 1.2189 (1.0344)	MaskBCELoss 0.3526 (0.2102)	MaskDICELoss 0.8663 (0.8242)
Epoch: [1][ 93/500]	Time 13.036 (13.036)	Loss 1.5737 (1.4358)	CeLoss 0.2178 (0.2669)	SegCLSLoss 0.0278 (0.0278)	KLLoss 0.2578 (0.2566)	MaskLoss 1.2194 (1.1637)	MaskBCELoss 0.3714 (0.2765)	MaskDICELoss 0.8480 (0.8872)
Epoch: [1][ 94/500]	Time  9.300 ( 9.300)	Loss 1.5879 (1.2485)	CeLoss 0.2715 (0.6170)	SegCLSLoss 0.0134 (0.0136)	KLLoss 0.2637 (0.1545)	MaskLoss 1.0871 (0.6702)	MaskBCELoss 0.3550 (0.1313)	MaskDICELoss 0.7321 (0.5389)
Epoch: [1][ 95/500]	Time  9.910 ( 9.910)	Loss 1.4609 (1.2329)	CeLoss 1.4609 (0.6525)	SegCLSLoss 0.0000 (0.0111)	KLLoss 0.0000 (0.1527)	MaskLoss 0.0000 (0.6696)	MaskBCELoss 0.0000 (0.1068)	MaskDICELoss 0.0000 (0.5628)
Epoch: [1][ 96/500]	Time 11.212 (11.212)	Loss 1.4385 (1.1956)	CeLoss 0.2871 (0.3475)	SegCLSLoss 0.0203 (0.0212)	KLLoss 0.2539 (0.2295)	MaskLoss 1.1621 (0.9946)	MaskBCELoss 0.2724 (0.1466)	MaskDICELoss 0.8896 (0.8480)
Epoch: [1][ 97/500]	Time 11.171 (11.171)	Loss 1.0703 (1.3457)	CeLoss 0.3750 (0.4179)	SegCLSLoss 0.0135 (0.0146)	KLLoss 0.2598 (0.2057)	MaskLoss 0.8690 (0.9475)	MaskBCELoss 0.0447 (0.2180)	MaskDICELoss 0.8243 (0.7295)
Epoch: [1][ 98/500]	Time 12.056 (12.056)	Loss 0.9559 (1.1717)	CeLoss 0.2168 (0.4351)	SegCLSLoss 0.0216 (0.0173)	KLLoss 0.2539 (0.2043)	MaskLoss 1.0409 (0.8708)	MaskBCELoss 0.0618 (0.1225)	MaskDICELoss 0.9792 (0.7483)
Epoch: [1][ 99/500]	Time 12.886 (12.886)	Loss 2.1452 (1.4669)	CeLoss 0.2891 (0.2251)	SegCLSLoss 0.0121 (0.0239)	KLLoss 0.2637 (0.2566)	MaskLoss 1.4392 (1.2483)	MaskBCELoss 0.6216 (0.3114)	MaskDICELoss 0.8176 (0.9368)
[2025-03-02 17:08:48,948] [INFO] [logging.py:96:log_dist] [Rank 0] step=600, skipped=0, lr=[0.00023289795918367344], mom=[(0.9, 0.95)]
[2025-03-02 17:08:48,958] [INFO] [timer.py:215:stop] epoch=0/micro_step=6000/global_step=600, RunningAvgSamplesPerSec=0.9305661976883538, CurrSamplesPerSec=0.9028353713353843, MemAllocated=30.71GB, MaxMemAllocated=36.84GB
Epoch: [1][100/500]	Time 11.079 (11.079)	Loss 1.4219 (1.3714)	CeLoss 1.4219 (0.4613)	SegCLSLoss 0.0000 (0.0161)	KLLoss 0.0000 (0.2082)	MaskLoss 0.0000 (0.8882)	MaskBCELoss 0.0000 (0.2090)	MaskDICELoss 0.0000 (0.6792)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 656, in <module>
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 398, in main
[rank0]:   File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 511, in train
[rank0]:     mask_bce_losses.update(mask_bce_loss.item(), input_dict["images"].size(0))
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1861, in backward
[rank0]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank0]:   File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1890, in backward
[rank0]:     buf_1 = torch.empty(int(self.reduce_bucket_size),
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 954.00 MiB. GPU