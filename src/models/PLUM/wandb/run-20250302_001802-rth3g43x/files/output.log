You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
Traceback (most recent call last):
  File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 651, in <module>
    main(sys.argv[1:])
  File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/plum_train_ds.py", line 175, in main
    model = PLUMForCausalLM.from_pretrained(
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2700, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File "/shared/nas2/jk100/partonomy_private/src/models/PLUM/model/PLUM.py", line 174, in __init__
    self.use_teacher_ref = config.use_teacher_ref
  File "/shared/nas/data/m1/jk100/.conda/envs/llava/lib/python3.10/site-packages/transformers/configuration_utils.py", line 261, in __getattribute__
    return super().__getattribute__(key)
AttributeError: 'LlavaConfig' object has no attribute 'use_teacher_ref'